<rss version="2.0">
  <channel>
    <title>GitHub Python Daily Trending</title>
    <description>Daily Trending of Python in GitHub</description>
    <pubDate>Sun, 02 Nov 2025 01:36:28 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>666ghj/BettaFish</title>
      <link>https://github.com/666ghj/BettaFish</link>
      <description>&lt;p&gt;å¾®èˆ†ï¼šäººäººå¯ç”¨çš„å¤šAgentèˆ†æƒ…åˆ†æåŠ©æ‰‹ï¼Œæ‰“ç ´ä¿¡æ¯èŒ§æˆ¿ï¼Œè¿˜åŸèˆ†æƒ…åŸè²Œï¼Œé¢„æµ‹æœªæ¥èµ°å‘ï¼Œè¾…åŠ©å†³ç­–ï¼ä»0å®ç°ï¼Œä¸ä¾èµ–ä»»ä½•æ¡†æ¶ã€‚&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/666ghj/BettaFish/main/static/image/logo_compressed.png" alt="Weibo Public Opinion Analysis System Logo" width="100%" /&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/15286" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/15286" alt="666ghj%2FBettaFish | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://leaflow.net/" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/666ghj/BettaFish/main/static/image/Leaflow_logo.png" alt="666ghj%2FWeibo_PublicOpinion_AnalysisSystem | Leaflow" style="width: 150px;" width="150" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/666ghj/Weibo_PublicOpinion_AnalysisSystem/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/666ghj/Weibo_PublicOpinion_AnalysisSystem?style=flat-square" alt="GitHub Stars" /&gt;&lt;/a&gt; &lt;a href="https://github.com/666ghj/Weibo_PublicOpinion_AnalysisSystem/watchers"&gt;&lt;img src="https://img.shields.io/github/watchers/666ghj/Weibo_PublicOpinion_AnalysisSystem?style=flat-square" alt="GitHub Watchers" /&gt;&lt;/a&gt; &lt;a href="https://github.com/666ghj/Weibo_PublicOpinion_AnalysisSystem/network"&gt;&lt;img src="https://img.shields.io/github/forks/666ghj/Weibo_PublicOpinion_AnalysisSystem?style=flat-square" alt="GitHub Forks" /&gt;&lt;/a&gt; &lt;a href="https://github.com/666ghj/Weibo_PublicOpinion_AnalysisSystem/issues"&gt;&lt;img src="https://img.shields.io/github/issues/666ghj/Weibo_PublicOpinion_AnalysisSystem?style=flat-square" alt="GitHub Issues" /&gt;&lt;/a&gt; &lt;a href="https://github.com/666ghj/Weibo_PublicOpinion_AnalysisSystem/pulls"&gt;&lt;img src="https://img.shields.io/github/issues-pr/666ghj/Weibo_PublicOpinion_AnalysisSystem?style=flat-square" alt="GitHub Pull Requests" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/666ghj/Weibo_PublicOpinion_AnalysisSystem/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/666ghj/Weibo_PublicOpinion_AnalysisSystem?style=flat-square" alt="GitHub License" /&gt;&lt;/a&gt; &lt;a href="https://github.com/666ghj/Weibo_PublicOpinion_AnalysisSystem"&gt;&lt;img src="https://img.shields.io/badge/version-v1.0.0-green.svg?style=flat-square" alt="Version" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/"&gt;&lt;img src="https://img.shields.io/badge/Docker-Build-2496ED?style=flat-square&amp;amp;logo=docker&amp;amp;logoColor=white" alt="Docker" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/666ghj/BettaFish/main/README-EN.md"&gt;English&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/666ghj/BettaFish/main/README.md"&gt;ä¸­æ–‡æ–‡æ¡£&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] å‘¨ä¸€ï¼ˆ11.3ï¼‰ä¼šä¸Š&lt;strong&gt;åœ¨çº¿ä¸€é”®éƒ¨ç½²ä½“éªŒ&lt;/strong&gt;ï¼Œæ¬¢è¿æŒç»­å…³æ³¨ï¼&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;âš¡ é¡¹ç›®æ¦‚è¿°&lt;/h2&gt; 
&lt;p&gt;â€œ&lt;strong&gt;å¾®èˆ†&lt;/strong&gt;â€ æ˜¯ä¸€ä¸ªä»0å®ç°çš„åˆ›æ–°å‹ å¤šæ™ºèƒ½ä½“ èˆ†æƒ…åˆ†æç³»ç»Ÿï¼Œå¸®åŠ©å¤§å®¶ç ´é™¤ä¿¡æ¯èŒ§æˆ¿ï¼Œè¿˜åŸèˆ†æƒ…åŸè²Œï¼Œé¢„æµ‹æœªæ¥èµ°å‘ï¼Œè¾…åŠ©å†³ç­–ã€‚ç”¨æˆ·åªéœ€åƒèŠå¤©ä¸€æ ·æå‡ºåˆ†æéœ€æ±‚ï¼Œæ™ºèƒ½ä½“å¼€å§‹å…¨è‡ªåŠ¨åˆ†æ å›½å†…å¤–30+ä¸»æµç¤¾åª’ ä¸ æ•°ç™¾ä¸‡æ¡å¤§ä¼—è¯„è®ºã€‚&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;â€œå¾®èˆ†â€è°éŸ³â€œå¾®é±¼â€ï¼ŒBettaFishæ˜¯ä¸€ç§ä½“å‹å¾ˆå°ä½†éå¸¸å¥½æ–—ã€æ¼‚äº®çš„é±¼ï¼Œå®ƒè±¡å¾ç€â€œå°è€Œå¼ºå¤§ï¼Œä¸ç•æŒ‘æˆ˜â€&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;æŸ¥çœ‹ç³»ç»Ÿä»¥â€œæ­¦æ±‰å¤§å­¦èˆ†æƒ…â€ä¸ºä¾‹ï¼Œç”Ÿæˆçš„ç ”ç©¶æŠ¥å‘Šï¼š&lt;a href="https://raw.githubusercontent.com/666ghj/BettaFish/main/final_reports/final_report__20250827_131630.html"&gt;æ­¦æ±‰å¤§å­¦å“ç‰Œå£°èª‰æ·±åº¦åˆ†ææŠ¥å‘Š&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;ä¸ä»…ä»…ä½“ç°åœ¨æŠ¥å‘Šè´¨é‡ä¸Šï¼Œç›¸æ¯”åŒç±»äº§å“ï¼Œæˆ‘ä»¬æ‹¥æœ‰ğŸš€å…­å¤§ä¼˜åŠ¿ï¼š&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;AIé©±åŠ¨çš„å…¨åŸŸç›‘æ§&lt;/strong&gt;ï¼šAIçˆ¬è™«é›†ç¾¤7x24å°æ—¶ä¸é—´æ–­ä½œä¸šï¼Œå…¨é¢è¦†ç›–å¾®åšã€å°çº¢ä¹¦ã€æŠ–éŸ³ã€å¿«æ‰‹ç­‰10+å›½å†…å¤–å…³é”®ç¤¾åª’ã€‚ä¸ä»…å®æ—¶æ•è·çƒ­ç‚¹å†…å®¹ï¼Œæ›´èƒ½ä¸‹é’»è‡³æµ·é‡ç”¨æˆ·è¯„è®ºï¼Œè®©æ‚¨å¬åˆ°æœ€çœŸå®ã€æœ€å¹¿æ³›çš„å¤§ä¼—å£°éŸ³ã€‚&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;è¶…è¶ŠLLMçš„å¤åˆåˆ†æå¼•æ“&lt;/strong&gt;ï¼šæˆ‘ä»¬ä¸ä»…ä¾èµ–è®¾è®¡çš„5ç±»ä¸“ä¸šAgentï¼Œæ›´èåˆäº†å¾®è°ƒæ¨¡å‹ã€ç»Ÿè®¡æ¨¡å‹ç­‰ä¸­é—´ä»¶ã€‚é€šè¿‡å¤šæ¨¡å‹ååŒå·¥ä½œï¼Œç¡®ä¿äº†åˆ†æç»“æœçš„æ·±åº¦ã€å‡†åº¦ä¸å¤šç»´è§†è§’ã€‚&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;å¼ºå¤§çš„å¤šæ¨¡æ€èƒ½åŠ›&lt;/strong&gt;ï¼šçªç ´å›¾æ–‡é™åˆ¶ï¼Œèƒ½æ·±åº¦è§£ææŠ–éŸ³ã€å¿«æ‰‹ç­‰çŸ­è§†é¢‘å†…å®¹ï¼Œå¹¶ç²¾å‡†æå–ç°ä»£æœç´¢å¼•æ“ä¸­çš„å¤©æ°”ã€æ—¥å†ã€è‚¡ç¥¨ç­‰ç»“æ„åŒ–å¤šæ¨¡æ€ä¿¡æ¯å¡ç‰‡ï¼Œè®©æ‚¨å…¨é¢æŒæ¡èˆ†æƒ…åŠ¨æ€ã€‚&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Agentâ€œè®ºå›â€åä½œæœºåˆ¶&lt;/strong&gt;ï¼šä¸ºä¸åŒAgentèµ‹äºˆç‹¬ç‰¹çš„å·¥å…·é›†ä¸æ€ç»´æ¨¡å¼ï¼Œå¼•å…¥è¾©è®ºä¸»æŒäººæ¨¡å‹ï¼Œé€šè¿‡â€œè®ºå›â€æœºåˆ¶è¿›è¡Œé“¾å¼æ€ç»´ç¢°æ’ä¸è¾©è®ºã€‚è¿™ä¸ä»…é¿å…äº†å•ä¸€æ¨¡å‹çš„æ€ç»´å±€é™ä¸äº¤æµå¯¼è‡´çš„åŒè´¨åŒ–ï¼Œæ›´å‚¬ç”Ÿå‡ºæ›´é«˜è´¨é‡çš„é›†ä½“æ™ºèƒ½ä¸å†³ç­–æ”¯æŒã€‚&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;å…¬ç§åŸŸæ•°æ®æ— ç¼èåˆ&lt;/strong&gt;ï¼šå¹³å°ä¸ä»…åˆ†æå…¬å¼€èˆ†æƒ…ï¼Œè¿˜æä¾›é«˜å®‰å…¨æ€§çš„æ¥å£ï¼Œæ”¯æŒæ‚¨å°†å†…éƒ¨ä¸šåŠ¡æ•°æ®åº“ä¸èˆ†æƒ…æ•°æ®æ— ç¼é›†æˆã€‚æ‰“é€šæ•°æ®å£å’ï¼Œä¸ºå‚ç›´ä¸šåŠ¡æä¾›â€œå¤–éƒ¨è¶‹åŠ¿+å†…éƒ¨æ´å¯Ÿâ€çš„å¼ºå¤§åˆ†æèƒ½åŠ›ã€‚&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;è½»é‡åŒ–ä¸é«˜æ‰©å±•æ€§æ¡†æ¶&lt;/strong&gt;ï¼šåŸºäºçº¯Pythonæ¨¡å—åŒ–è®¾è®¡ï¼Œå®ç°è½»é‡åŒ–ã€ä¸€é”®å¼éƒ¨ç½²ã€‚ä»£ç ç»“æ„æ¸…æ™°ï¼Œå¼€å‘è€…å¯è½»æ¾é›†æˆè‡ªå®šä¹‰æ¨¡å‹ä¸ä¸šåŠ¡é€»è¾‘ï¼Œå®ç°å¹³å°çš„å¿«é€Ÿæ‰©å±•ä¸æ·±åº¦å®šåˆ¶ã€‚&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;å§‹äºèˆ†æƒ…ï¼Œè€Œä¸æ­¢äºèˆ†æƒ…&lt;/strong&gt;ã€‚â€œå¾®èˆ†â€çš„ç›®æ ‡ï¼Œæ˜¯æˆä¸ºé©±åŠ¨ä¸€åˆ‡ä¸šåŠ¡åœºæ™¯çš„ç®€æ´é€šç”¨çš„æ•°æ®åˆ†æå¼•æ“ã€‚&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ä¸¾ä¸ªä¾‹å­. ä½ åªéœ€ç®€å•ä¿®æ”¹Agentå·¥å…·é›†çš„apiå‚æ•°ä¸promptï¼Œå°±å¯ä»¥æŠŠä»–å˜æˆä¸€ä¸ªé‡‘èé¢†åŸŸçš„å¸‚åœºåˆ†æç³»ç»Ÿ&lt;/p&gt; 
 &lt;p&gt;é™„ä¸€ä¸ªæ¯”è¾ƒæ´»è·ƒçš„Lç«™é¡¹ç›®è®¨è®ºå¸–ï¼š&lt;a href="https://linux.do/t/topic/1009280"&gt;https://linux.do/t/topic/1009280&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/666ghj/BettaFish/main/static/image/system_schematic.png" alt="banner" width="800" /&gt; 
 &lt;p&gt;å‘Šåˆ«ä¼ ç»Ÿçš„æ•°æ®çœ‹æ¿ï¼Œåœ¨â€œå¾®èˆ†â€ï¼Œä¸€åˆ‡ç”±ä¸€ä¸ªç®€å•çš„é—®é¢˜å¼€å§‹ï¼Œæ‚¨åªéœ€åƒå¯¹è¯ä¸€æ ·ï¼Œæå‡ºæ‚¨çš„åˆ†æéœ€æ±‚&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸ—ï¸ ç³»ç»Ÿæ¶æ„&lt;/h2&gt; 
&lt;h3&gt;æ•´ä½“æ¶æ„å›¾&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Insight Agent&lt;/strong&gt; ç§æœ‰æ•°æ®åº“æŒ–æ˜ï¼šç§æœ‰èˆ†æƒ…æ•°æ®åº“æ·±åº¦åˆ†æAIä»£ç†&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Media Agent&lt;/strong&gt; å¤šæ¨¡æ€å†…å®¹åˆ†æï¼šå…·å¤‡å¼ºå¤§å¤šæ¨¡æ€èƒ½åŠ›çš„AIä»£ç†&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Query Agent&lt;/strong&gt; ç²¾å‡†ä¿¡æ¯æœç´¢ï¼šå…·å¤‡å›½å†…å¤–ç½‘é¡µæœç´¢èƒ½åŠ›çš„AIä»£ç†&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Report Agent&lt;/strong&gt; æ™ºèƒ½æŠ¥å‘Šç”Ÿæˆï¼šå†…ç½®æ¨¡æ¿çš„å¤šè½®æŠ¥å‘Šç”ŸæˆAIä»£ç†&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/666ghj/BettaFish/main/static/image/framework.png" alt="banner" width="800" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;ä¸€æ¬¡å®Œæ•´åˆ†ææµç¨‹&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;æ­¥éª¤&lt;/th&gt; 
   &lt;th&gt;é˜¶æ®µåç§°&lt;/th&gt; 
   &lt;th&gt;ä¸»è¦æ“ä½œ&lt;/th&gt; 
   &lt;th&gt;å‚ä¸ç»„ä»¶&lt;/th&gt; 
   &lt;th&gt;å¾ªç¯ç‰¹æ€§&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;1&lt;/td&gt; 
   &lt;td&gt;ç”¨æˆ·æé—®&lt;/td&gt; 
   &lt;td&gt;Flaskä¸»åº”ç”¨æ¥æ”¶æŸ¥è¯¢&lt;/td&gt; 
   &lt;td&gt;Flaskä¸»åº”ç”¨&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;å¹¶è¡Œå¯åŠ¨&lt;/td&gt; 
   &lt;td&gt;ä¸‰ä¸ªAgentåŒæ—¶å¼€å§‹å·¥ä½œ&lt;/td&gt; 
   &lt;td&gt;Query Agentã€Media Agentã€Insight Agent&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;3&lt;/td&gt; 
   &lt;td&gt;åˆæ­¥åˆ†æ&lt;/td&gt; 
   &lt;td&gt;å„Agentä½¿ç”¨ä¸“å±å·¥å…·è¿›è¡Œæ¦‚è§ˆæœç´¢&lt;/td&gt; 
   &lt;td&gt;å„Agent + ä¸“å±å·¥å…·é›†&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;4&lt;/td&gt; 
   &lt;td&gt;ç­–ç•¥åˆ¶å®š&lt;/td&gt; 
   &lt;td&gt;åŸºäºåˆæ­¥ç»“æœåˆ¶å®šåˆ†å—ç ”ç©¶ç­–ç•¥&lt;/td&gt; 
   &lt;td&gt;å„Agentå†…éƒ¨å†³ç­–æ¨¡å—&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5-N&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;å¾ªç¯é˜¶æ®µ&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;è®ºå›åä½œ + æ·±åº¦ç ”ç©¶&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;ForumEngine + æ‰€æœ‰Agent&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;å¤šè½®å¾ªç¯&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5.1&lt;/td&gt; 
   &lt;td&gt;æ·±åº¦ç ”ç©¶&lt;/td&gt; 
   &lt;td&gt;å„AgentåŸºäºè®ºå›ä¸»æŒäººå¼•å¯¼è¿›è¡Œä¸“é¡¹æœç´¢&lt;/td&gt; 
   &lt;td&gt;å„Agent + åæ€æœºåˆ¶ + è®ºå›å¼•å¯¼&lt;/td&gt; 
   &lt;td&gt;æ¯è½®å¾ªç¯&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5.2&lt;/td&gt; 
   &lt;td&gt;è®ºå›åä½œ&lt;/td&gt; 
   &lt;td&gt;ForumEngineç›‘æ§Agentå‘è¨€å¹¶ç”Ÿæˆä¸»æŒäººæ€»ç»“&lt;/td&gt; 
   &lt;td&gt;ForumEngine + LLMä¸»æŒäºº&lt;/td&gt; 
   &lt;td&gt;æ¯è½®å¾ªç¯&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5.3&lt;/td&gt; 
   &lt;td&gt;äº¤æµèåˆ&lt;/td&gt; 
   &lt;td&gt;å„Agentæ ¹æ®è®¨è®ºè°ƒæ•´ç ”ç©¶æ–¹å‘&lt;/td&gt; 
   &lt;td&gt;å„Agent + forum_readerå·¥å…·&lt;/td&gt; 
   &lt;td&gt;æ¯è½®å¾ªç¯&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;N+1&lt;/td&gt; 
   &lt;td&gt;ç»“æœæ•´åˆ&lt;/td&gt; 
   &lt;td&gt;Report Agentæ”¶é›†æ‰€æœ‰åˆ†æç»“æœå’Œè®ºå›å†…å®¹&lt;/td&gt; 
   &lt;td&gt;Report Agent&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;N+2&lt;/td&gt; 
   &lt;td&gt;æŠ¥å‘Šç”Ÿæˆ&lt;/td&gt; 
   &lt;td&gt;åŠ¨æ€é€‰æ‹©æ¨¡æ¿å’Œæ ·å¼ï¼Œå¤šè½®ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š&lt;/td&gt; 
   &lt;td&gt;Report Agent + æ¨¡æ¿å¼•æ“&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;é¡¹ç›®ä»£ç ç»“æ„æ ‘&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;Weibo_PublicOpinion_AnalysisSystem/
â”œâ”€â”€ QueryEngine/                   # å›½å†…å¤–æ–°é—»å¹¿åº¦æœç´¢Agent
â”‚   â”œâ”€â”€ agent.py                   # Agentä¸»é€»è¾‘
â”‚   â”œâ”€â”€ llms/                      # LLMæ¥å£å°è£…
â”‚   â”œâ”€â”€ nodes/                     # å¤„ç†èŠ‚ç‚¹
â”‚   â”œâ”€â”€ tools/                     # æœç´¢å·¥å…·
â”‚   â”œâ”€â”€ utils/                     # å·¥å…·å‡½æ•°
â”‚   â””â”€â”€ ...                        # å…¶ä»–æ¨¡å—
â”œâ”€â”€ MediaEngine/                   # å¼ºå¤§çš„å¤šæ¨¡æ€ç†è§£Agent
â”‚   â”œâ”€â”€ agent.py                   # Agentä¸»é€»è¾‘
â”‚   â”œâ”€â”€ nodes/                     # å¤„ç†èŠ‚ç‚¹
â”‚   â”œâ”€â”€ llms/                      # LLMæ¥å£
â”‚   â”œâ”€â”€ tools/                     # æœç´¢å·¥å…·
â”‚   â”œâ”€â”€ utils/                     # å·¥å…·å‡½æ•°
â”‚   â””â”€â”€ ...                        # å…¶ä»–æ¨¡å—
â”œâ”€â”€ InsightEngine/                 # ç§æœ‰æ•°æ®åº“æŒ–æ˜Agent
â”‚   â”œâ”€â”€ agent.py                   # Agentä¸»é€»è¾‘
â”‚   â”œâ”€â”€ llms/                      # LLMæ¥å£å°è£…
â”‚   â”‚   â””â”€â”€ base.py                # ç»Ÿä¸€çš„ OpenAI å…¼å®¹å®¢æˆ·ç«¯
â”‚   â”œâ”€â”€ nodes/                     # å¤„ç†èŠ‚ç‚¹
â”‚   â”‚   â”œâ”€â”€ base_node.py           # åŸºç¡€èŠ‚ç‚¹ç±»
â”‚   â”‚   â”œâ”€â”€ formatting_node.py     # æ ¼å¼åŒ–èŠ‚ç‚¹
â”‚   â”‚   â”œâ”€â”€ report_structure_node.py # æŠ¥å‘Šç»“æ„èŠ‚ç‚¹
â”‚   â”‚   â”œâ”€â”€ search_node.py         # æœç´¢èŠ‚ç‚¹
â”‚   â”‚   â””â”€â”€ summary_node.py        # æ€»ç»“èŠ‚ç‚¹
â”‚   â”œâ”€â”€ tools/                     # æ•°æ®åº“æŸ¥è¯¢å’Œåˆ†æå·¥å…·
â”‚   â”‚   â”œâ”€â”€ keyword_optimizer.py   # Qwenå…³é”®è¯ä¼˜åŒ–ä¸­é—´ä»¶
â”‚   â”‚   â”œâ”€â”€ search.py              # æ•°æ®åº“æ“ä½œå·¥å…·é›†
â”‚   â”‚   â””â”€â”€ sentiment_analyzer.py  # æƒ…æ„Ÿåˆ†æé›†æˆå·¥å…·
â”‚   â”œâ”€â”€ state/                     # çŠ¶æ€ç®¡ç†
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ state.py               # AgentçŠ¶æ€å®šä¹‰
â”‚   â”œâ”€â”€ prompts/                   # æç¤ºè¯æ¨¡æ¿
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ prompts.py             # å„ç±»æç¤ºè¯
â”‚   â””â”€â”€ utils/                     # å·¥å…·å‡½æ•°
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ config.py              # é…ç½®ç®¡ç†
â”‚       â””â”€â”€ text_processing.py     # æ–‡æœ¬å¤„ç†å·¥å…·
â”œâ”€â”€ ReportEngine/                  # å¤šè½®æŠ¥å‘Šç”ŸæˆAgent
â”‚   â”œâ”€â”€ agent.py                   # Agentä¸»é€»è¾‘
â”‚   â”œâ”€â”€ llms/                      # LLMæ¥å£
â”‚   â”œâ”€â”€ nodes/                     # æŠ¥å‘Šç”ŸæˆèŠ‚ç‚¹
â”‚   â”‚   â”œâ”€â”€ template_selection.py  # æ¨¡æ¿é€‰æ‹©èŠ‚ç‚¹
â”‚   â”‚   â””â”€â”€ html_generation.py     # HTMLç”ŸæˆèŠ‚ç‚¹
â”‚   â”œâ”€â”€ report_template/           # æŠ¥å‘Šæ¨¡æ¿åº“
â”‚   â”‚   â”œâ”€â”€ ç¤¾ä¼šå…¬å…±çƒ­ç‚¹äº‹ä»¶åˆ†æ.md
â”‚   â”‚   â”œâ”€â”€ å•†ä¸šå“ç‰Œèˆ†æƒ…ç›‘æµ‹.md
â”‚   â”‚   â””â”€â”€ ...                    # æ›´å¤šæ¨¡æ¿
â”‚   â””â”€â”€ flask_interface.py         # Flask APIæ¥å£
â”œâ”€â”€ ForumEngine/                   # è®ºå›å¼•æ“ç®€æ˜“å®ç°
â”‚   â”œâ”€â”€ monitor.py                 # æ—¥å¿—ç›‘æ§å’Œè®ºå›ç®¡ç†
â”‚   â””â”€â”€ llm_host.py                # è®ºå›ä¸»æŒäººLLMæ¨¡å—
â”œâ”€â”€ MindSpider/                    # å¾®åšçˆ¬è™«ç³»ç»Ÿ
â”‚   â”œâ”€â”€ main.py                    # çˆ¬è™«ä¸»ç¨‹åº
â”‚   â”œâ”€â”€ config.py                  # çˆ¬è™«é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ BroadTopicExtraction/      # è¯é¢˜æå–æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ database_manager.py    # æ•°æ®åº“ç®¡ç†å™¨
â”‚   â”‚   â”œâ”€â”€ get_today_news.py      # ä»Šæ—¥æ–°é—»è·å–
â”‚   â”‚   â”œâ”€â”€ main.py                # è¯é¢˜æå–ä¸»ç¨‹åº
â”‚   â”‚   â””â”€â”€ topic_extractor.py     # è¯é¢˜æå–å™¨
â”‚   â”œâ”€â”€ DeepSentimentCrawling/     # æ·±åº¦èˆ†æƒ…çˆ¬å–
â”‚   â”‚   â”œâ”€â”€ keyword_manager.py     # å…³é”®è¯ç®¡ç†å™¨
â”‚   â”‚   â”œâ”€â”€ main.py                # æ·±åº¦çˆ¬å–ä¸»ç¨‹åº
â”‚   â”‚   â”œâ”€â”€ MediaCrawler/          # åª’ä½“çˆ¬è™«æ ¸å¿ƒ
â”‚   â”‚   â””â”€â”€ platform_crawler.py    # å¹³å°çˆ¬è™«ç®¡ç†
â”‚   â””â”€â”€ schema/                    # æ•°æ®åº“ç»“æ„
â”‚       â”œâ”€â”€ db_manager.py          # æ•°æ®åº“ç®¡ç†å™¨
â”‚       â”œâ”€â”€ init_database.py       # æ•°æ®åº“åˆå§‹åŒ–
â”‚       â””â”€â”€ mindspider_tables.sql  # æ•°æ®åº“è¡¨ç»“æ„
â”œâ”€â”€ SentimentAnalysisModel/        # æƒ…æ„Ÿåˆ†ææ¨¡å‹é›†åˆ
â”‚   â”œâ”€â”€ WeiboSentiment_Finetuned/  # å¾®è°ƒBERT/GPT-2æ¨¡å‹
â”‚   â”œâ”€â”€ WeiboMultilingualSentiment/# å¤šè¯­è¨€æƒ…æ„Ÿåˆ†æï¼ˆæ¨èï¼‰
â”‚   â”œâ”€â”€ WeiboSentiment_SmallQwen/  # å°å‚æ•°Qwen3å¾®è°ƒ
â”‚   â””â”€â”€ WeiboSentiment_MachineLearning/ # ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•
â”œâ”€â”€ SingleEngineApp/               # å•ç‹¬Agentçš„Streamlitåº”ç”¨
â”‚   â”œâ”€â”€ query_engine_streamlit_app.py
â”‚   â”œâ”€â”€ media_engine_streamlit_app.py
â”‚   â””â”€â”€ insight_engine_streamlit_app.py
â”œâ”€â”€ templates/                     # Flaskæ¨¡æ¿
â”‚   â””â”€â”€ index.html                 # ä¸»ç•Œé¢å‰ç«¯
â”œâ”€â”€ static/                        # é™æ€èµ„æº
â”œâ”€â”€ logs/                          # è¿è¡Œæ—¥å¿—ç›®å½•
â”œâ”€â”€ final_reports/                 # æœ€ç»ˆç”Ÿæˆçš„HTMLæŠ¥å‘Šæ–‡ä»¶
â”œâ”€â”€ utils/                         # é€šç”¨å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ forum_reader.py            # Agenté—´è®ºå›é€šä¿¡
â”‚   â””â”€â”€ retry_helper.py            # ç½‘ç»œè¯·æ±‚é‡è¯•æœºåˆ¶å·¥å…·
â”œâ”€â”€ app.py                         # Flaskä¸»åº”ç”¨å…¥å£
â”œâ”€â”€ config.py                      # å…¨å±€é…ç½®æ–‡ä»¶
â””â”€â”€ requirements.txt               # Pythonä¾èµ–åŒ…æ¸…å•
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ğŸš€ å¿«é€Ÿå¼€å§‹&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;å¦‚æœä½ æ˜¯åˆæ¬¡å­¦ä¹ ä¸€ä¸ªAgentç³»ç»Ÿçš„æ­å»ºï¼Œå¯ä»¥ä»ä¸€ä¸ªéå¸¸ç®€å•çš„demoå¼€å§‹ï¼š&lt;a href="https://github.com/666ghj/DeepSearchAgent-Demo"&gt;Deep Search Agent Demo&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;ç¯å¢ƒè¦æ±‚&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;æ“ä½œç³»ç»Ÿ&lt;/strong&gt;: Windowsã€Linuxã€MacOS&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Pythonç‰ˆæœ¬&lt;/strong&gt;: 3.9+&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Conda&lt;/strong&gt;: Anacondaæˆ–Miniconda&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;æ•°æ®åº“&lt;/strong&gt;: MySQLï¼ˆå¯é€‰æ‹©æˆ‘ä»¬çš„äº‘æ•°æ®åº“æœåŠ¡ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;å†…å­˜&lt;/strong&gt;: å»ºè®®2GBä»¥ä¸Š&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;1. åˆ›å»ºCondaç¯å¢ƒ&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# åˆ›å»ºcondaç¯å¢ƒ
conda create -n your_conda_name python=3.11
conda activate your_conda_name
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. å®‰è£…ä¾èµ–åŒ…&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# åŸºç¡€ä¾èµ–å®‰è£…
pip install -r requirements.txt
# å¦‚æœä¸æƒ³ä½¿ç”¨æœ¬åœ°æƒ…æ„Ÿåˆ†ææ¨¡å‹ï¼ˆç®—åŠ›éœ€æ±‚å¾ˆå°ï¼Œé»˜è®¤å®‰è£…cpuç‰ˆæœ¬ï¼‰ï¼Œå¯ä»¥å°†è¯¥æ–‡ä»¶ä¸­çš„â€œæœºå™¨å­¦ä¹ â€éƒ¨åˆ†æ³¨é‡Šæ‰å†æ‰§è¡ŒæŒ‡ä»¤
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. å®‰è£…Playwrightæµè§ˆå™¨é©±åŠ¨&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# å®‰è£…æµè§ˆå™¨é©±åŠ¨ï¼ˆç”¨äºçˆ¬è™«åŠŸèƒ½ï¼‰
playwright install chromium
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;4. é…ç½®ç³»ç»Ÿ&lt;/h3&gt; 
&lt;h4&gt;4.1 é…ç½®APIå¯†é’¥&lt;/h4&gt; 
&lt;p&gt;ç¼–è¾‘ &lt;code&gt;config.py&lt;/code&gt; æ–‡ä»¶ï¼Œå¡«å…¥æ‚¨çš„APIå¯†é’¥ï¼ˆæ‚¨ä¹Ÿå¯ä»¥é€‰æ‹©è‡ªå·±çš„æ¨¡å‹ã€æœç´¢ä»£ç†ï¼Œè¯¦æƒ…è§configæ–‡ä»¶å†…ï¼‰ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# MySQLæ•°æ®åº“é…ç½®
DB_HOST = "localhost"
DB_PORT = 3306
DB_USER = "your_username"
DB_PASSWORD = "your_password"
DB_NAME = "your_db_name"
DB_CHARSET = "utf8mb4"

# LLMé…ç½®
# æ‚¨å¯ä»¥æ›´æ”¹æ¯ä¸ªéƒ¨åˆ†LLMä½¿ç”¨çš„APIï¼Œåªè¦å…¼å®¹OpenAIè¯·æ±‚æ ¼å¼éƒ½å¯ä»¥

# Insight Agent
INSIGHT_ENGINE_API_KEY = "your_api_key"
INSIGHT_ENGINE_BASE_URL = "https://api.moonshot.cn/v1"
INSIGHT_ENGINE_MODEL_NAME = "kimi-k2-0711-preview"
# Media Agent
...
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;4.2 æ•°æ®åº“åˆå§‹åŒ–&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;é€‰æ‹©1ï¼šä½¿ç”¨æœ¬åœ°æ•°æ®åº“&lt;/strong&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;MindSpiderçˆ¬è™«ç³»ç»Ÿè·Ÿèˆ†æƒ…ç³»ç»Ÿæ˜¯å„è‡ªç‹¬ç«‹çš„ï¼Œæ‰€ä»¥éœ€è¦å†å»&lt;code&gt;MindSpider\config.py&lt;/code&gt;é…ç½®ä¸€ä¸‹&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# æœ¬åœ°MySQLæ•°æ®åº“åˆå§‹åŒ–
cd MindSpider
python schema/init_database.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;é€‰æ‹©2ï¼šä½¿ç”¨äº‘æ•°æ®åº“æœåŠ¡ï¼ˆæ¨èï¼‰&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;æˆ‘ä»¬æä¾›ä¾¿æ·çš„äº‘æ•°æ®åº“æœåŠ¡ï¼ŒåŒ…å«æ—¥å‡10ä¸‡+çœŸå®èˆ†æƒ…æ•°æ®ï¼Œç›®å‰&lt;strong&gt;å…è´¹ç”³è¯·&lt;/strong&gt;ï¼&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;çœŸå®èˆ†æƒ…æ•°æ®ï¼Œå®æ—¶æ›´æ–°&lt;/li&gt; 
 &lt;li&gt;å¤šç»´åº¦æ ‡ç­¾åˆ†ç±»&lt;/li&gt; 
 &lt;li&gt;é«˜å¯ç”¨äº‘ç«¯æœåŠ¡&lt;/li&gt; 
 &lt;li&gt;ä¸“ä¸šæŠ€æœ¯æ”¯æŒ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;è”ç³»æˆ‘ä»¬ç”³è¯·å…è´¹äº‘æ•°æ®åº“è®¿é—®ï¼šğŸ“§ &lt;a href="mailto:670939375@qq.com"&gt;670939375@qq.com&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ä¸ºè¿›è¡Œæ•°æ®åˆè§„æ€§å®¡æŸ¥ä¸æœåŠ¡å‡çº§ï¼Œäº‘æ•°æ®åº“è‡ª2025å¹´10æœˆ1æ—¥èµ·æš‚åœæ¥æ”¶æ–°çš„ä½¿ç”¨ç”³è¯·&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;5. å¯åŠ¨ç³»ç»Ÿ&lt;/h3&gt; 
&lt;h4&gt;5.1 å®Œæ•´ç³»ç»Ÿå¯åŠ¨ï¼ˆæ¨èï¼‰&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹ï¼Œæ¿€æ´»condaç¯å¢ƒ
conda activate your_conda_name

# å¯åŠ¨ä¸»åº”ç”¨å³å¯
python app.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;æ³¨1ï¼šä¸€æ¬¡è¿è¡Œç»ˆæ­¢åï¼Œstreamlit appå¯èƒ½ç»“æŸå¼‚å¸¸ä»ç„¶å ç”¨ç«¯å£ï¼Œæ­¤æ—¶æœç´¢å ç”¨ç«¯å£çš„è¿›ç¨‹killæ‰å³å¯&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;æ³¨2ï¼šæ•°æ®çˆ¬å–éœ€è¦å•ç‹¬æ“ä½œï¼Œè§5.3æŒ‡å¼•&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;æ³¨3ï¼šå¦‚æœæœåŠ¡å™¨è¿œç¨‹éƒ¨ç½²å‡ºç°é¡µé¢æ˜¾ç¤ºé—®é¢˜ï¼Œè§&lt;a href="https://github.com/666ghj/BettaFish/pull/45"&gt;PR#45&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;è®¿é—® &lt;a href="http://localhost:5000"&gt;http://localhost:5000&lt;/a&gt; å³å¯ä½¿ç”¨å®Œæ•´ç³»ç»Ÿ&lt;/p&gt; 
&lt;h4&gt;5.2 å•ç‹¬å¯åŠ¨æŸä¸ªAgent&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# å¯åŠ¨QueryEngine
streamlit run SingleEngineApp/query_engine_streamlit_app.py --server.port 8503

# å¯åŠ¨MediaEngine  
streamlit run SingleEngineApp/media_engine_streamlit_app.py --server.port 8502

# å¯åŠ¨InsightEngine
streamlit run SingleEngineApp/insight_engine_streamlit_app.py --server.port 8501
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;5.3 çˆ¬è™«ç³»ç»Ÿå•ç‹¬ä½¿ç”¨&lt;/h4&gt; 
&lt;p&gt;è¿™éƒ¨åˆ†æœ‰è¯¦ç»†çš„é…ç½®æ–‡æ¡£ï¼š&lt;a href="https://raw.githubusercontent.com/666ghj/BettaFish/main/MindSpider/README.md"&gt;MindeSpiderä½¿ç”¨è¯´æ˜&lt;/a&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="MindSpider\img\example.png" alt="banner" width="600" /&gt; 
 &lt;p&gt;MindSpider è¿è¡Œç¤ºä¾‹&lt;/p&gt; 
&lt;/div&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# è¿›å…¥çˆ¬è™«ç›®å½•
cd MindSpider

# é¡¹ç›®åˆå§‹åŒ–
python main.py --setup

# è¿è¡Œå®Œæ•´çˆ¬è™«æµç¨‹
python main.py --complete --date 2024-01-20

# ä»…è¿è¡Œè¯é¢˜æå–
python main.py --broad-topic --date 2024-01-20

# ä»…è¿è¡Œæ·±åº¦çˆ¬å–
python main.py --deep-sentiment --platforms xhs dy wb
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;âš™ï¸ é«˜çº§é…ç½®&lt;/h2&gt; 
&lt;h3&gt;ä¿®æ”¹å…³é”®å‚æ•°&lt;/h3&gt; 
&lt;h4&gt;Agenté…ç½®å‚æ•°&lt;/h4&gt; 
&lt;p&gt;æ¯ä¸ªAgentéƒ½æœ‰ä¸“é—¨çš„é…ç½®æ–‡ä»¶ï¼Œå¯æ ¹æ®éœ€æ±‚è°ƒæ•´ï¼Œä¸‹é¢æ˜¯éƒ¨åˆ†ç¤ºä¾‹ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# QueryEngine/utils/config.py
class Config:
    max_reflections = 2           # åæ€è½®æ¬¡
    max_search_results = 15       # æœ€å¤§æœç´¢ç»“æœæ•°
    max_content_length = 8000     # æœ€å¤§å†…å®¹é•¿åº¦
    
# MediaEngine/utils/config.py  
class Config:
    comprehensive_search_limit = 10  # ç»¼åˆæœç´¢é™åˆ¶
    web_search_limit = 15           # ç½‘é¡µæœç´¢é™åˆ¶
    
# InsightEngine/utils/config.py
class Config:
    default_search_topic_globally_limit = 200    # å…¨å±€æœç´¢é™åˆ¶
    default_get_comments_limit = 500             # è¯„è®ºè·å–é™åˆ¶
    max_search_results_for_llm = 50              # ä¼ ç»™LLMçš„æœ€å¤§ç»“æœæ•°
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;æƒ…æ„Ÿåˆ†ææ¨¡å‹é…ç½®&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# InsightEngine/tools/sentiment_analyzer.py
SENTIMENT_CONFIG = {
    'model_type': 'multilingual',     # å¯é€‰: 'bert', 'multilingual', 'qwen'ç­‰
    'confidence_threshold': 0.8,      # ç½®ä¿¡åº¦é˜ˆå€¼
    'batch_size': 32,                 # æ‰¹å¤„ç†å¤§å°
    'max_sequence_length': 512,       # æœ€å¤§åºåˆ—é•¿åº¦
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;æ¥å…¥ä¸åŒçš„LLMæ¨¡å‹&lt;/h3&gt; 
&lt;p&gt;æ”¯æŒä»»æ„openAIè°ƒç”¨æ ¼å¼çš„LLMæä¾›å•†ï¼Œåªéœ€è¦åœ¨/config.pyä¸­å¡«å†™å¯¹åº”çš„KEYã€BASE_URLã€MODEL_NAMEå³å¯ã€‚&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ä»€ä¹ˆæ˜¯openAIè°ƒç”¨æ ¼å¼ï¼Ÿä¸‹é¢æä¾›ä¸€ä¸ªç®€å•çš„ä¾‹å­ï¼š&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from openai import OpenAI

client = OpenAI(api_key="your_api_key", 
               base_url="https://api.siliconflow.cn/v1")

response = client.chat.completions.create(
   model="Qwen/Qwen2.5-72B-Instruct",
   messages=[
       {'role': 'user', 
        'content': "æ¨ç†æ¨¡å‹ä¼šç»™å¸‚åœºå¸¦æ¥å“ªäº›æ–°çš„æœºä¼š"}
   ],
)

complete_response = response.choices[0].message.content
print(complete_response)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;æ›´æ”¹æƒ…æ„Ÿåˆ†ææ¨¡å‹&lt;/h3&gt; 
&lt;p&gt;ç³»ç»Ÿé›†æˆäº†å¤šç§æƒ…æ„Ÿåˆ†ææ–¹æ³•ï¼Œå¯æ ¹æ®éœ€æ±‚é€‰æ‹©ï¼š&lt;/p&gt; 
&lt;h4&gt;1. å¤šè¯­è¨€æƒ…æ„Ÿåˆ†æ&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd SentimentAnalysisModel/WeiboMultilingualSentiment
python predict.py --text "This product is amazing!" --lang "en"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;2. å°å‚æ•°Qwen3å¾®è°ƒ&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd SentimentAnalysisModel/WeiboSentiment_SmallQwen
python predict_universal.py --text "è¿™æ¬¡æ´»åŠ¨åŠå¾—å¾ˆæˆåŠŸ"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;3. åŸºäºBERTçš„å¾®è°ƒæ¨¡å‹&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# ä½¿ç”¨BERTä¸­æ–‡æ¨¡å‹
cd SentimentAnalysisModel/WeiboSentiment_Finetuned/BertChinese-Lora
python predict.py --text "è¿™ä¸ªäº§å“çœŸçš„å¾ˆä¸é”™"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;4. GPT-2 LoRAå¾®è°ƒæ¨¡å‹&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd SentimentAnalysisModel/WeiboSentiment_Finetuned/GPT2-Lora
python predict.py --text "ä»Šå¤©å¿ƒæƒ…ä¸å¤ªå¥½"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;5. ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd SentimentAnalysisModel/WeiboSentiment_MachineLearning
python predict.py --model_type "svm" --text "æœåŠ¡æ€åº¦éœ€è¦æ”¹è¿›"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;æ¥å…¥è‡ªå®šä¹‰ä¸šåŠ¡æ•°æ®åº“&lt;/h3&gt; 
&lt;h4&gt;1. ä¿®æ”¹æ•°æ®åº“è¿æ¥é…ç½®&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# config.py ä¸­æ·»åŠ æ‚¨çš„ä¸šåŠ¡æ•°æ®åº“é…ç½®
BUSINESS_DB_HOST = "your_business_db_host"
BUSINESS_DB_PORT = 3306
BUSINESS_DB_USER = "your_business_user"
BUSINESS_DB_PASSWORD = "your_business_password"
BUSINESS_DB_NAME = "your_business_database"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;2. åˆ›å»ºè‡ªå®šä¹‰æ•°æ®è®¿é—®å·¥å…·&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# InsightEngine/tools/custom_db_tool.py
class CustomBusinessDBTool:
    """è‡ªå®šä¹‰ä¸šåŠ¡æ•°æ®åº“æŸ¥è¯¢å·¥å…·"""
    
    def __init__(self):
        self.connection_config = {
            'host': config.BUSINESS_DB_HOST,
            'port': config.BUSINESS_DB_PORT,
            'user': config.BUSINESS_DB_USER,
            'password': config.BUSINESS_DB_PASSWORD,
            'database': config.BUSINESS_DB_NAME,
        }
    
    def search_business_data(self, query: str, table: str):
        """æŸ¥è¯¢ä¸šåŠ¡æ•°æ®"""
        # å®ç°æ‚¨çš„ä¸šåŠ¡é€»è¾‘
        pass
    
    def get_customer_feedback(self, product_id: str):
        """è·å–å®¢æˆ·åé¦ˆæ•°æ®"""
        # å®ç°å®¢æˆ·åé¦ˆæŸ¥è¯¢é€»è¾‘
        pass
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;3. é›†æˆåˆ°InsightEngine&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# InsightEngine/agent.py ä¸­é›†æˆè‡ªå®šä¹‰å·¥å…·
from .tools.custom_db_tool import CustomBusinessDBTool

class DeepSearchAgent:
    def __init__(self, config=None):
        # ... å…¶ä»–åˆå§‹åŒ–ä»£ç 
        self.custom_db_tool = CustomBusinessDBTool()
    
    def execute_custom_search(self, query: str):
        """æ‰§è¡Œè‡ªå®šä¹‰ä¸šåŠ¡æ•°æ®æœç´¢"""
        return self.custom_db_tool.search_business_data(query, "your_table")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;è‡ªå®šä¹‰æŠ¥å‘Šæ¨¡æ¿&lt;/h3&gt; 
&lt;h4&gt;1. åœ¨Webç•Œé¢ä¸­ä¸Šä¼ &lt;/h4&gt; 
&lt;p&gt;ç³»ç»Ÿæ”¯æŒä¸Šä¼ è‡ªå®šä¹‰æ¨¡æ¿æ–‡ä»¶ï¼ˆ.mdæˆ–.txtæ ¼å¼ï¼‰ï¼Œå¯åœ¨ç”ŸæˆæŠ¥å‘Šæ—¶é€‰æ‹©ä½¿ç”¨ã€‚&lt;/p&gt; 
&lt;h4&gt;2. åˆ›å»ºæ¨¡æ¿æ–‡ä»¶&lt;/h4&gt; 
&lt;p&gt;åœ¨ &lt;code&gt;ReportEngine/report_template/&lt;/code&gt; ç›®å½•ä¸‹åˆ›å»ºæ–°çš„æ¨¡æ¿ï¼Œæˆ‘ä»¬çš„Agentä¼šè‡ªè¡Œé€‰ç”¨æœ€åˆé€‚çš„æ¨¡æ¿ã€‚&lt;/p&gt; 
&lt;h2&gt;ğŸ¤ è´¡çŒ®æŒ‡å—&lt;/h2&gt; 
&lt;p&gt;æˆ‘ä»¬æ¬¢è¿æ‰€æœ‰å½¢å¼çš„è´¡çŒ®ï¼&lt;/p&gt; 
&lt;h3&gt;å¦‚ä½•è´¡çŒ®&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Forké¡¹ç›®&lt;/strong&gt;åˆ°æ‚¨çš„GitHubè´¦å·&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;åˆ›å»ºFeatureåˆ†æ”¯&lt;/strong&gt;ï¼š&lt;code&gt;git checkout -b feature/AmazingFeature&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;æäº¤æ›´æ”¹&lt;/strong&gt;ï¼š&lt;code&gt;git commit -m 'Add some AmazingFeature'&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;æ¨é€åˆ°åˆ†æ”¯&lt;/strong&gt;ï¼š&lt;code&gt;git push origin feature/AmazingFeature&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;å¼€å¯Pull Request&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;å¼€å‘è§„èŒƒ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ä»£ç éµå¾ªPEP8è§„èŒƒ&lt;/li&gt; 
 &lt;li&gt;æäº¤ä¿¡æ¯ä½¿ç”¨æ¸…æ™°çš„ä¸­è‹±æ–‡æè¿°&lt;/li&gt; 
 &lt;li&gt;æ–°åŠŸèƒ½éœ€è¦åŒ…å«ç›¸åº”çš„æµ‹è¯•ç”¨ä¾‹&lt;/li&gt; 
 &lt;li&gt;æ›´æ–°ç›¸å…³æ–‡æ¡£&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ¦– ä¸‹ä¸€æ­¥å¼€å‘è®¡åˆ’&lt;/h2&gt; 
&lt;p&gt;ç°åœ¨ç³»ç»Ÿåªå®Œæˆäº†"ä¸‰æ¿æ–§"ä¸­çš„å‰ä¸¤æ­¥ï¼Œå³ï¼šè¾“å…¥è¦æ±‚-&amp;gt;è¯¦ç»†åˆ†æï¼Œè¿˜ç¼ºå°‘ä¸€æ­¥é¢„æµ‹ï¼Œç›´æ¥å°†ä»–ç»§ç»­äº¤ç»™LLMæ˜¯ä¸å…·æœ‰è¯´æœåŠ›çš„ã€‚&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/666ghj/BettaFish/main/static/image/banner_compressed.png" alt="banner" width="800" /&gt; 
&lt;/div&gt; 
&lt;p&gt;ç›®å‰æˆ‘ä»¬ç»è¿‡å¾ˆé•¿ä¸€æ®µæ—¶é—´çš„çˆ¬å–æ”¶é›†ï¼Œæ‹¥æœ‰äº†å¤§é‡å…¨ç½‘è¯é¢˜çƒ­åº¦éšæ—¶é—´ã€çˆ†ç‚¹ç­‰çš„å˜åŒ–è¶‹åŠ¿çƒ­åº¦æ•°æ®ï¼Œå·²ç»å…·å¤‡äº†å¯ä»¥å¼€å‘é¢„æµ‹æ¨¡å‹çš„æ¡ä»¶ã€‚æˆ‘ä»¬å›¢é˜Ÿå°†è¿ç”¨æ—¶åºæ¨¡å‹ã€å›¾ç¥ç»ç½‘ç»œã€å¤šæ¨¡æ€èåˆç­‰é¢„æµ‹æ¨¡å‹æŠ€æœ¯å‚¨å¤‡äºæ­¤ï¼Œå®ç°çœŸæ­£åŸºäºæ•°æ®é©±åŠ¨çš„èˆ†æƒ…é¢„æµ‹åŠŸèƒ½ã€‚&lt;/p&gt; 
&lt;h2&gt;âš ï¸ å…è´£å£°æ˜&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;é‡è¦æé†’ï¼šæœ¬é¡¹ç›®ä»…ä¾›å­¦ä¹ ã€å­¦æœ¯ç ”ç©¶å’Œæ•™è‚²ç›®çš„ä½¿ç”¨&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;åˆè§„æ€§å£°æ˜&lt;/strong&gt;ï¼š&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;æœ¬é¡¹ç›®ä¸­çš„æ‰€æœ‰ä»£ç ã€å·¥å…·å’ŒåŠŸèƒ½å‡ä»…ä¾›å­¦ä¹ ã€å­¦æœ¯ç ”ç©¶å’Œæ•™è‚²ç›®çš„ä½¿ç”¨&lt;/li&gt; 
   &lt;li&gt;ä¸¥ç¦å°†æœ¬é¡¹ç›®ç”¨äºä»»ä½•å•†ä¸šç”¨é€”æˆ–ç›ˆåˆ©æ€§æ´»åŠ¨&lt;/li&gt; 
   &lt;li&gt;ä¸¥ç¦å°†æœ¬é¡¹ç›®ç”¨äºä»»ä½•è¿æ³•ã€è¿è§„æˆ–ä¾µçŠ¯ä»–äººæƒç›Šçš„è¡Œä¸º&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;çˆ¬è™«åŠŸèƒ½å…è´£&lt;/strong&gt;ï¼š&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;é¡¹ç›®ä¸­çš„çˆ¬è™«åŠŸèƒ½ä»…ç”¨äºæŠ€æœ¯å­¦ä¹ å’Œç ”ç©¶ç›®çš„&lt;/li&gt; 
   &lt;li&gt;ä½¿ç”¨è€…å¿…é¡»éµå®ˆç›®æ ‡ç½‘ç«™çš„robots.txtåè®®å’Œä½¿ç”¨æ¡æ¬¾&lt;/li&gt; 
   &lt;li&gt;ä½¿ç”¨è€…å¿…é¡»éµå®ˆç›¸å…³æ³•å¾‹æ³•è§„ï¼Œä¸å¾—è¿›è¡Œæ¶æ„çˆ¬å–æˆ–æ•°æ®æ»¥ç”¨&lt;/li&gt; 
   &lt;li&gt;å› ä½¿ç”¨çˆ¬è™«åŠŸèƒ½äº§ç”Ÿçš„ä»»ä½•æ³•å¾‹åæœç”±ä½¿ç”¨è€…è‡ªè¡Œæ‰¿æ‹…&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;æ•°æ®ä½¿ç”¨å…è´£&lt;/strong&gt;ï¼š&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;é¡¹ç›®æ¶‰åŠçš„æ•°æ®åˆ†æåŠŸèƒ½ä»…ä¾›å­¦æœ¯ç ”ç©¶ä½¿ç”¨&lt;/li&gt; 
   &lt;li&gt;ä¸¥ç¦å°†åˆ†æç»“æœç”¨äºå•†ä¸šå†³ç­–æˆ–ç›ˆåˆ©ç›®çš„&lt;/li&gt; 
   &lt;li&gt;ä½¿ç”¨è€…åº”ç¡®ä¿æ‰€åˆ†ææ•°æ®çš„åˆæ³•æ€§å’Œåˆè§„æ€§&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;æŠ€æœ¯å…è´£&lt;/strong&gt;ï¼š&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;æœ¬é¡¹ç›®æŒ‰"ç°çŠ¶"æä¾›ï¼Œä¸æä¾›ä»»ä½•æ˜ç¤ºæˆ–æš—ç¤ºçš„ä¿è¯&lt;/li&gt; 
   &lt;li&gt;ä½œè€…ä¸å¯¹ä½¿ç”¨æœ¬é¡¹ç›®é€ æˆçš„ä»»ä½•ç›´æ¥æˆ–é—´æ¥æŸå¤±æ‰¿æ‹…è´£ä»»&lt;/li&gt; 
   &lt;li&gt;ä½¿ç”¨è€…åº”è‡ªè¡Œè¯„ä¼°é¡¹ç›®çš„é€‚ç”¨æ€§å’Œé£é™©&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;è´£ä»»é™åˆ¶&lt;/strong&gt;ï¼š&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;ä½¿ç”¨è€…åœ¨ä½¿ç”¨æœ¬é¡¹ç›®å‰åº”å……åˆ†äº†è§£ç›¸å…³æ³•å¾‹æ³•è§„&lt;/li&gt; 
   &lt;li&gt;ä½¿ç”¨è€…åº”ç¡®ä¿å…¶ä½¿ç”¨è¡Œä¸ºç¬¦åˆå½“åœ°æ³•å¾‹æ³•è§„è¦æ±‚&lt;/li&gt; 
   &lt;li&gt;å› è¿åæ³•å¾‹æ³•è§„ä½¿ç”¨æœ¬é¡¹ç›®è€Œäº§ç”Ÿçš„ä»»ä½•åæœç”±ä½¿ç”¨è€…è‡ªè¡Œæ‰¿æ‹…&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;è¯·åœ¨ä½¿ç”¨æœ¬é¡¹ç›®å‰ä»”ç»†é˜…è¯»å¹¶ç†è§£ä¸Šè¿°å…è´£å£°æ˜ã€‚ä½¿ç”¨æœ¬é¡¹ç›®å³è¡¨ç¤ºæ‚¨å·²åŒæ„å¹¶æ¥å—ä¸Šè¿°æ‰€æœ‰æ¡æ¬¾ã€‚&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;ğŸ“„ è®¸å¯è¯&lt;/h2&gt; 
&lt;p&gt;æœ¬é¡¹ç›®é‡‡ç”¨ &lt;a href="https://raw.githubusercontent.com/666ghj/BettaFish/main/LICENSE"&gt;GPL-2.0è®¸å¯è¯&lt;/a&gt;ã€‚è¯¦ç»†ä¿¡æ¯è¯·å‚é˜…LICENSEæ–‡ä»¶ã€‚&lt;/p&gt; 
&lt;h2&gt;ğŸ‰ æ”¯æŒä¸è”ç³»&lt;/h2&gt; 
&lt;h3&gt;è·å–å¸®åŠ©&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;é¡¹ç›®ä¸»é¡µ&lt;/strong&gt;ï¼š&lt;a href="https://github.com/666ghj/Weibo_PublicOpinion_AnalysisSystem"&gt;GitHubä»“åº“&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;é—®é¢˜åé¦ˆ&lt;/strong&gt;ï¼š&lt;a href="https://github.com/666ghj/Weibo_PublicOpinion_AnalysisSystem/issues"&gt;Issuesé¡µé¢&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;åŠŸèƒ½å»ºè®®&lt;/strong&gt;ï¼š&lt;a href="https://github.com/666ghj/Weibo_PublicOpinion_AnalysisSystem/discussions"&gt;Discussionsé¡µé¢&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;è”ç³»æ–¹å¼&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“§ &lt;strong&gt;é‚®ç®±&lt;/strong&gt;ï¼š&lt;a href="mailto:670939375@qq.com"&gt;670939375@qq.com&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;å•†åŠ¡åˆä½œ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ä¼ä¸šå®šåˆ¶å¼€å‘&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;å¤§æ•°æ®æœåŠ¡&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;å­¦æœ¯åˆä½œ&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;æŠ€æœ¯åŸ¹è®­&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ‘¥ è´¡çŒ®è€…&lt;/h2&gt; 
&lt;p&gt;æ„Ÿè°¢ä»¥ä¸‹ä¼˜ç§€çš„è´¡çŒ®è€…ä»¬ï¼š&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/666ghj/Weibo_PublicOpinion_AnalysisSystem/graphs/contributors"&gt;&lt;img src="https://contrib.rocks/image?repo=666ghj/Weibo_PublicOpinion_AnalysisSystem" alt="Contributors" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ğŸ“ˆ é¡¹ç›®ç»Ÿè®¡&lt;/h2&gt; 
&lt;a href="https://www.star-history.com/#666ghj/BettaFish&amp;amp;type=date&amp;amp;legend=top-left"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=666ghj/BettaFish&amp;amp;type=date&amp;amp;theme=dark&amp;amp;legend=top-left" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=666ghj/BettaFish&amp;amp;type=date&amp;amp;legend=top-left" /&gt; 
  &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=666ghj/BettaFish&amp;amp;type=date&amp;amp;legend=top-left" /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;p&gt;&lt;img src="https://repobeats.axiom.co/api/embed/e04e3eea4674edc39c148a7845c8d09c1b7b1922.svg?sanitize=true" alt="Alt" title="Repobeats analytics image" /&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>kijai/ComfyUI-WanVideoWrapper</title>
      <link>https://github.com/kijai/ComfyUI-WanVideoWrapper</link>
      <description>&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;Note: Due to the stupid amount of bots or people thinking this is some of video generation service, I've blocked new accounts from posting issues for now.&lt;/h2&gt; 
&lt;h1&gt;ComfyUI wrapper nodes for &lt;a href="https://github.com/Wan-Video/Wan2.1"&gt;WanVideo&lt;/a&gt; and related models.&lt;/h1&gt; 
&lt;h1&gt;WORK IN PROGRESS (perpetually)&lt;/h1&gt; 
&lt;h1&gt;Why should I use custom nodes when WanVideo works natively?&lt;/h1&gt; 
&lt;p&gt;Short answer: Unless it's a model/feature not available yet on native, you shouldn't.&lt;/p&gt; 
&lt;p&gt;Long answer: Due to the complexity of ComfyUI core code, and my lack of coding experience, in many cases it's far easier and faster to implement new models and features to a standalone wrapper, so this is a way to test things relatively quickly. I consider this my personal sandbox (which is obviously open for everyone) to play with without having to worry about compability issues etc, but as such this code is always work in progress and prone to have issues. Also not all new models end up being worth the trouble to implement in core Comfy, though I've also made some patcher nodes to allow using them in native workflows, such as the &lt;a href="https://huggingface.co/bytedance-research/ATI"&gt;ATI&lt;/a&gt; node available in this wrapper. This is also the end goal, idea isn't to compete or even offer alternatives to everything available in native workflows. All that said (this is clearly not a sales pitch) I do appreciate everyone using these nodes to explore new releases and possibilities with WanVideo.&lt;/p&gt; 
&lt;h1&gt;Installation&lt;/h1&gt; 
&lt;ol&gt; 
 &lt;li&gt;Clone this repo into &lt;code&gt;custom_nodes&lt;/code&gt; folder.&lt;/li&gt; 
 &lt;li&gt;Install dependencies: &lt;code&gt;pip install -r requirements.txt&lt;/code&gt; or if you use the portable install, run this in ComfyUI_windows_portable -folder:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;code&gt;python_embeded\python.exe -m pip install -r ComfyUI\custom_nodes\ComfyUI-WanVideoWrapper\requirements.txt&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;Models&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/Kijai/WanVideo_comfy/tree/main"&gt;https://huggingface.co/Kijai/WanVideo_comfy/tree/main&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;fp8 scaled models (personal recommendation):&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/Kijai/WanVideo_comfy_fp8_scaled"&gt;https://huggingface.co/Kijai/WanVideo_comfy_fp8_scaled&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Text encoders to &lt;code&gt;ComfyUI/models/text_encoders&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Clip vision to &lt;code&gt;ComfyUI/models/clip_vision&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Transformer (main video model) to &lt;code&gt;ComfyUI/models/diffusion_models&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Vae to &lt;code&gt;ComfyUI/models/vae&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;You can also use the native ComfyUI text encoding and clip vision loader with the wrapper instead of the original models:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/6a2fd9a5-8163-4c93-b362-92ef34dbd3a4" alt="image" /&gt;&lt;/p&gt; 
&lt;p&gt;GGUF models can now be loaded in the main model loader as well.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;Supported extra models:&lt;/p&gt; 
&lt;p&gt;SkyReels: &lt;a href="https://huggingface.co/collections/Skywork/skyreels-v2-6801b1b93df627d441d0d0d9"&gt;https://huggingface.co/collections/Skywork/skyreels-v2-6801b1b93df627d441d0d0d9&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;WanVideoFun: &lt;a href="https://huggingface.co/collections/alibaba-pai/wan21-fun-v11-680f514c89fe7b4df9d44f17"&gt;https://huggingface.co/collections/alibaba-pai/wan21-fun-v11-680f514c89fe7b4df9d44f17&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;ReCamMaster: &lt;a href="https://github.com/KwaiVGI/ReCamMaster"&gt;https://github.com/KwaiVGI/ReCamMaster&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;VACE: &lt;a href="https://github.com/ali-vilab/VACE"&gt;https://github.com/ali-vilab/VACE&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Phantom: &lt;a href="https://huggingface.co/bytedance-research/Phantom"&gt;https://huggingface.co/bytedance-research/Phantom&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;ATI: &lt;a href="https://huggingface.co/bytedance-research/ATI"&gt;https://huggingface.co/bytedance-research/ATI&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Uni3C: &lt;a href="https://github.com/alibaba-damo-academy/Uni3C"&gt;https://github.com/alibaba-damo-academy/Uni3C&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;MiniMaxRemover: &lt;a href="https://huggingface.co/zibojia/minimax-remover"&gt;https://huggingface.co/zibojia/minimax-remover&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;MAGREF: &lt;a href="https://huggingface.co/MAGREF-Video/MAGREF"&gt;https://huggingface.co/MAGREF-Video/MAGREF&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;FantasyTalking: &lt;a href="https://github.com/Fantasy-AMAP/fantasy-talking"&gt;https://github.com/Fantasy-AMAP/fantasy-talking&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;FantasyPortrait: &lt;a href="https://github.com/Fantasy-AMAP/fantasy-portrait"&gt;https://github.com/Fantasy-AMAP/fantasy-portrait&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;MultiTalk: &lt;a href="https://github.com/MeiGen-AI/MultiTalk"&gt;https://github.com/MeiGen-AI/MultiTalk&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;EchoShot: &lt;a href="https://github.com/D2I-ai/EchoShot"&gt;https://github.com/D2I-ai/EchoShot&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Stand-In: &lt;a href="https://github.com/WeChatCV/Stand-In"&gt;https://github.com/WeChatCV/Stand-In&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;HuMo: &lt;a href="https://github.com/Phantom-video/HuMo"&gt;https://github.com/Phantom-video/HuMo&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;WanAnimate: &lt;a href="https://github.com/Wan-Video/Wan2.2/tree/main/wan/modules/animate"&gt;https://github.com/Wan-Video/Wan2.2/tree/main/wan/modules/animate&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Lynx: &lt;a href="https://github.com/bytedance/lynx"&gt;https://github.com/bytedance/lynx&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Not exactly Wan model, but close enough to work with the code base:&lt;/p&gt; 
&lt;p&gt;LongCat-Video: &lt;a href="https://meituan-longcat.github.io/LongCat-Video/"&gt;https://meituan-longcat.github.io/LongCat-Video/&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Examples:&lt;/h2&gt; 
&lt;p&gt;WanAnimate:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/f370b001-0f98-4c4c-bcb5-cfad0b330697"&gt;https://github.com/user-attachments/assets/f370b001-0f98-4c4c-bcb5-cfad0b330697&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/KwaiVGI/ReCamMaster"&gt;ReCamMaster&lt;/a&gt;:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/c58a12c2-13ba-4af8-8041-e283dbef197e"&gt;https://github.com/user-attachments/assets/c58a12c2-13ba-4af8-8041-e283dbef197e&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;TeaCache (with the old temporary WIP naive version, I2V):&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note that with the new version the threshold values should be 10x higher&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Range of 0.25-0.30 seems good when using the coefficients, start step can be 0, with more aggressive threshold values it may make sense to start later to avoid any potential step skips early on, that generally ruin the motion.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/504a9a50-3337-43d2-97b8-8e1661f29f46"&gt;https://github.com/user-attachments/assets/504a9a50-3337-43d2-97b8-8e1661f29f46&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Context window test:&lt;/p&gt; 
&lt;p&gt;1025 frames using window size of 81 frames, with 16 overlap. With the 1.3B T2V model this used under 5GB VRAM and took 10 minutes to gen on a 5090:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/89b393af-cf1b-49ae-aa29-23e57f65911e"&gt;https://github.com/user-attachments/assets/89b393af-cf1b-49ae-aa29-23e57f65911e&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;This very first test was 512x512x81&lt;/p&gt; 
&lt;p&gt;~16GB used with 20/40 blocks offloaded&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/fa6d0a4f-4a4d-4de5-84a4-877cc37b715f"&gt;https://github.com/user-attachments/assets/fa6d0a4f-4a4d-4de5-84a4-877cc37b715f&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Vid2vid example:&lt;/p&gt; 
&lt;p&gt;with 14B T2V model:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/ef228b8a-a13a-4327-8a1b-1eb343cf00d8"&gt;https://github.com/user-attachments/assets/ef228b8a-a13a-4327-8a1b-1eb343cf00d8&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;with 1.3B T2V model&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/4f35ba84-da7a-4d5b-97ee-9641296f391e"&gt;https://github.com/user-attachments/assets/4f35ba84-da7a-4d5b-97ee-9641296f391e&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ethereum/EIPs</title>
      <link>https://github.com/ethereum/EIPs</link>
      <description>&lt;p&gt;The Ethereum Improvement Proposal repository&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Ethereum Improvement Proposals (EIPs)&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;em&gt;ATTENTION&lt;/em&gt;&lt;/strong&gt;: The EIPs repository has recently &lt;a href="https://github.com/ethereum/EIPs/pull/7206"&gt;undergone&lt;/a&gt; a separation of ERCs and EIPs. ERCs are now accessible at &lt;a href="https://github.com/ethereum/ercs"&gt;https://github.com/ethereum/ercs&lt;/a&gt;. All new ERCs and updates to existing ones must be directed at this new repository. The editors apologize for this inconvenience.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;The goal of the EIP project is to standardize and provide high-quality documentation for Ethereum itself and conventions built upon it. This repository tracks past and ongoing improvements to Ethereum in the form of Ethereum Improvement Proposals (EIPs). &lt;a href="https://eips.ethereum.org/EIPS/eip-1"&gt;EIP-1&lt;/a&gt; governs how EIPs are published.&lt;/p&gt; 
&lt;p&gt;The &lt;a href="https://eips.ethereum.org/"&gt;status page&lt;/a&gt; tracks and lists EIPs, which can be divided into the following categories:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://eips.ethereum.org/core"&gt;Core EIPs&lt;/a&gt; are improvements to the Ethereum consensus protocol.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://eips.ethereum.org/networking"&gt;Networking EIPs&lt;/a&gt; specify the peer-to-peer networking layer of Ethereum.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://eips.ethereum.org/interface"&gt;Interface EIPs&lt;/a&gt; standardize interfaces to Ethereum, which determine how users and applications interact with the blockchain.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://eips.ethereum.org/erc"&gt;ERCs&lt;/a&gt; specify application layer standards, which determine how applications running on Ethereum can interact with each other.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://eips.ethereum.org/meta"&gt;Meta EIPs&lt;/a&gt; are miscellaneous improvements that nonetheless require some sort of consensus.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://eips.ethereum.org/informational"&gt;Informational EIPs&lt;/a&gt; are non-standard improvements that do not require any form of consensus.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Before you write an EIP, ideas MUST be thoroughly discussed on &lt;a href="https://ethereum-magicians.org/"&gt;Ethereum Magicians&lt;/a&gt; or &lt;a href="https://ethresear.ch/t/read-this-before-posting/8"&gt;Ethereum Research&lt;/a&gt;. Once consensus is reached, thoroughly read and review &lt;a href="https://eips.ethereum.org/EIPS/eip-1"&gt;EIP-1&lt;/a&gt;, which describes the EIP process.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Please note that this repository is for documenting standards and not for help implementing them. These types of inquiries should be directed to the &lt;a href="https://ethereum.stackexchange.com"&gt;Ethereum Stack Exchange&lt;/a&gt;. For specific questions and concerns regarding EIPs, it's best to comment on the relevant discussion thread of the EIP denoted by the &lt;code&gt;discussions-to&lt;/code&gt; tag in the EIP's preamble.&lt;/p&gt; 
&lt;p&gt;If you would like to become an EIP Editor, please read &lt;a href="https://raw.githubusercontent.com/ethereum/EIPs/master/EIPS/eip-5069.md"&gt;EIP-5069&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Preferred Citation Format&lt;/h2&gt; 
&lt;p&gt;The canonical URL for an EIP that has achieved draft status at any point is at &lt;a href="https://eips.ethereum.org/"&gt;https://eips.ethereum.org/&lt;/a&gt;. For example, the canonical URL for EIP-1 is &lt;a href="https://eips.ethereum.org/EIPS/eip-1"&gt;https://eips.ethereum.org/EIPS/eip-1&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Consider any document not published at &lt;a href="https://eips.ethereum.org/"&gt;https://eips.ethereum.org/&lt;/a&gt; as a working paper. Additionally, consider published EIPs with a status of "draft", "review", or "last call" to be incomplete drafts, and note that their specification is likely to be subject to change.&lt;/p&gt; 
&lt;h2&gt;Validation and Automerging&lt;/h2&gt; 
&lt;p&gt;All pull requests in this repository must pass automated checks before they can be automatically merged:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ethereum/eip-review-bot/"&gt;eip-review-bot&lt;/a&gt; determines when PRs can be automatically merged &lt;a href="https://github.com/ethereum/EIPs/raw/master/.github/workflows/auto-review-bot.yml"&gt;^1&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;EIP-1 rules are enforced using &lt;a href="https://github.com/ethereum/eipw"&gt;&lt;code&gt;eipw&lt;/code&gt;&lt;/a&gt;&lt;a href="https://github.com/ethereum/EIPs/raw/master/.github/workflows/ci.yml"&gt;^2&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;HTML formatting and broken links are enforced using &lt;a href="https://github.com/gjtorikian/html-proofer"&gt;HTMLProofer&lt;/a&gt;&lt;a href="https://github.com/ethereum/EIPs/raw/master/.github/workflows/ci.yml"&gt;^2&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Spelling is enforced with &lt;a href="https://github.com/codespell-project/codespell"&gt;CodeSpell&lt;/a&gt;&lt;a href="https://github.com/ethereum/EIPs/raw/master/.github/workflows/ci.yml"&gt;^2&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;False positives sometimes occur. When this happens, please submit a PR editing &lt;a href="https://github.com/ethereum/EIPs/raw/master/config/.codespell-whitelist"&gt;.codespell-whitelist&lt;/a&gt; and &lt;strong&gt;ONLY&lt;/strong&gt; .codespell-whitelist&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Markdown best practices are checked using &lt;a href="https://github.com/DavidAnson/markdownlint"&gt;markdownlint&lt;/a&gt;&lt;a href="https://github.com/ethereum/EIPs/raw/master/.github/workflows/ci.yml"&gt;^2&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;It is possible to run the EIP validator locally:&lt;/p&gt; 
&lt;p&gt;Make sure to add cargo's &lt;code&gt;bin&lt;/code&gt; directory to your environment (typically &lt;code&gt;$HOME/.cargo/bin&lt;/code&gt; in your &lt;code&gt;PATH&lt;/code&gt; environment variable)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;cargo install eipw
eipw --config ./config/eipw.toml &amp;lt;INPUT FILE / DIRECTORY&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Build the status page locally&lt;/h2&gt; 
&lt;h3&gt;Install prerequisites&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Open Terminal.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Check whether you have Ruby 3.1.4 installed. Later &lt;a href="https://stackoverflow.com/questions/14351272/undefined-method-exists-for-fileclass-nomethoderror"&gt;versions are not supported&lt;/a&gt;.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;ruby --version
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;If you don't have Ruby installed, install Ruby 3.1.4.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install Bundler:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;gem install bundler
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install dependencies:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;bundle install
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Build your local Jekyll site&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Bundle assets and start the server:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;bundle exec jekyll serve
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Preview your local Jekyll site in your web browser at &lt;code&gt;http://localhost:4000&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;More information on Jekyll and GitHub Pages &lt;a href="https://docs.github.com/en/enterprise/2.14/user/articles/setting-up-your-github-pages-site-locally-with-jekyll"&gt;here&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>emcie-co/parlant</title>
      <link>https://github.com/emcie-co/parlant</link>
      <description>&lt;p&gt;LLM agents built for control. Designed for real-world use. Deployed in minutes.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://github.com/emcie-co/parlant/blob/develop/docs/LogoTransparentLight.png?raw=true" /&gt; 
  &lt;img alt="Parlant - AI Agent Framework" src="https://github.com/emcie-co/parlant/raw/develop/docs/LogoTransparentDark.png?raw=true" width="400" /&gt; 
 &lt;/picture&gt; 
 &lt;h3&gt;Finally, LLM agents that actually follow instructions&lt;/h3&gt; 
 &lt;p&gt; &lt;a href="https://www.parlant.io/" target="_blank"&gt;ğŸŒ Website&lt;/a&gt; â€¢ &lt;a href="https://www.parlant.io/docs/quickstart/installation" target="_blank"&gt;âš¡ Quick Start&lt;/a&gt; â€¢ &lt;a href="https://discord.gg/duxWqxKk6J" target="_blank"&gt;ğŸ’¬ Discord&lt;/a&gt; â€¢ &lt;a href="https://www.parlant.io/docs/quickstart/examples" target="_blank"&gt;ğŸ“– Examples&lt;/a&gt; &lt;/p&gt; 
 &lt;p&gt; 
  &lt;!-- Keep these links. Translations will automatically update with the README. --&gt; &lt;a href="https://zdoc.app/de/emcie-co/parlant"&gt;Deutsch&lt;/a&gt; | &lt;a href="https://zdoc.app/es/emcie-co/parlant"&gt;EspaÃ±ol&lt;/a&gt; | &lt;a href="https://zdoc.app/fr/emcie-co/parlant"&gt;franÃ§ais&lt;/a&gt; | &lt;a href="https://zdoc.app/ja/emcie-co/parlant"&gt;æ—¥æœ¬èª&lt;/a&gt; | &lt;a href="https://zdoc.app/ko/emcie-co/parlant"&gt;í•œêµ­ì–´&lt;/a&gt; | &lt;a href="https://zdoc.app/pt/emcie-co/parlant"&gt;PortuguÃªs&lt;/a&gt; | &lt;a href="https://zdoc.app/ru/emcie-co/parlant"&gt;Ğ ÑƒÑÑĞºĞ¸Ğ¹&lt;/a&gt; | &lt;a href="https://zdoc.app/zh/emcie-co/parlant"&gt;ä¸­æ–‡&lt;/a&gt; &lt;/p&gt; 
 &lt;p&gt; &lt;a href="https://pypi.org/project/parlant/"&gt;&lt;img alt="PyPI" src="https://img.shields.io/pypi/v/parlant?color=blue" /&gt;&lt;/a&gt; &lt;img alt="Python 3.10+" src="https://img.shields.io/badge/python-3.10+-blue" /&gt; &lt;a href="https://opensource.org/licenses/Apache-2.0"&gt;&lt;img alt="License" src="https://img.shields.io/badge/license-Apache%202.0-green" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/duxWqxKk6J"&gt;&lt;img alt="Discord" src="https://img.shields.io/discord/1312378700993663007?color=7289da&amp;amp;logo=discord&amp;amp;logoColor=white" /&gt;&lt;/a&gt; &lt;img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/emcie-co/parlant?style=social" /&gt; &lt;/p&gt; 
 &lt;a href="https://trendshift.io/repositories/12768" target="_blank"&gt; &lt;img src="https://trendshift.io/api/badge/repositories/12768" alt="Trending on TrendShift" style="width: 250px; height: 55px;" width="250" height="55" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸ¯ The Problem Every AI Developer Faces&lt;/h2&gt; 
&lt;p&gt;You build an AI agent. It works great in testing. Then real users start talking to it and...&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;âŒ It ignores your carefully crafted system prompts&lt;/li&gt; 
 &lt;li&gt;âŒ It hallucinates responses in critical moments&lt;/li&gt; 
 &lt;li&gt;âŒ It can't handle edge cases consistently&lt;/li&gt; 
 &lt;li&gt;âŒ Each conversation feels like a roll of the dice&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Sound familiar?&lt;/strong&gt; You're not alone. This is the #1 pain point for developers building production AI agents.&lt;/p&gt; 
&lt;h2&gt;âš¡ The Solution: Stop Fighting Prompts, Teach Principles&lt;/h2&gt; 
&lt;p&gt;Parlant flips the script on AI agent development. Instead of hoping your LLM will follow instructions, &lt;strong&gt;Parlant ensures it&lt;/strong&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Traditional approach: Cross your fingers ğŸ¤
system_prompt = "You are a helpful assistant. Please follow these 47 rules..."

# Parlant approach: Ensured compliance âœ…
await agent.create_guideline(
    condition="Customer asks about refunds",
    action="Check order status first to see if eligible",
    tools=[check_order_status],
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;âœ… &lt;a href="https://www.parlant.io/blog/how-parlant-guarantees-compliance"&gt;Blog: How Parlant Ensures Agent Compliance&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ†š &lt;a href="https://www.parlant.io/blog/parlant-vs-langgraph"&gt;Blog: Parlant vs LangGraph&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ†š &lt;a href="https://www.parlant.io/blog/parlant-vs-dspy"&gt;Blog: Parlant vs DSPy&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Parlant gives you all the structure you need to build customer-facing agents that behave exactly as your business requires:&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://parlant.io/docs/concepts/customization/journeys"&gt;Journeys&lt;/a&gt;&lt;/strong&gt;: Define clear customer journeys and how your agent should respond at each step.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://parlant.io/docs/concepts/customization/guidelines"&gt;Behavioral Guidelines&lt;/a&gt;&lt;/strong&gt;: Easily craft agent behavior; Parlant will match the relevant elements contextually.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://parlant.io/docs/concepts/customization/tools"&gt;Tool Use&lt;/a&gt;&lt;/strong&gt;: Attach external APIs, data fetchers, or backend services to specific interaction events.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://parlant.io/docs/concepts/customization/glossary"&gt;Domain Adaptation&lt;/a&gt;&lt;/strong&gt;: Teach your agent domain-specific terminology and craft personalized responses.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://parlant.io/docs/concepts/customization/canned-responses"&gt;Canned Responses&lt;/a&gt;&lt;/strong&gt;: Use response templates to eliminate hallucinations and guarantee style consistency.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://parlant.io/docs/advanced/explainability"&gt;Explainability&lt;/a&gt;&lt;/strong&gt;: Understand why and when each guideline was matched and followed.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center"&gt; 
 &lt;h2&gt;ğŸš€ Get Your Agent Running in 60 Seconds&lt;/h2&gt; 
&lt;/div&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install parlant
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import parlant.sdk as p

@p.tool
async def get_weather(context: p.ToolContext, city: str) -&amp;gt; p.ToolResult:
    # Your weather API logic here
    return p.ToolResult(f"Sunny, 72Â°F in {city}")

@p.tool
async def get_datetime(context: p.ToolContext) -&amp;gt; p.ToolResult:
    from datetime import datetime
    return p.ToolResult(datetime.now())

async def main():
    async with p.Server() as server:
        agent = await server.create_agent(
            name="WeatherBot",
            description="Helpful weather assistant"
        )

        # Have the agent's context be updated on every response (though
        # update interval is customizable) using a context variable.
        await agent.create_variable(name="current-datetime", tool=get_datetime)

        # Control and guide agent behavior with natural language
        await agent.create_guideline(
            condition="User asks about weather",
            action="Get current weather and provide a friendly response with suggestions",
            tools=[get_weather]
        )

        # Add other (reliably enforced) behavioral modeling elements
        # ...

        # ğŸ‰ Test playground ready at http://localhost:8800
        # Integrate the official React widget into your app,
        # or follow the tutorial to build your own frontend!

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;That's it!&lt;/strong&gt; Your agent is running with ensured rule-following behavior.&lt;/p&gt; 
&lt;h2&gt;ğŸ¬ See It In Action&lt;/h2&gt; 
&lt;img alt="Parlant Demo" src="https://github.com/emcie-co/parlant/raw/develop/docs/demo.gif?raw=true" width="100%" /&gt; 
&lt;h2&gt;ğŸ”¥ Why Developers Are Switching to Parlant&lt;/h2&gt; 
&lt;table width="100%"&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;ğŸ—ï¸ &lt;strong&gt;Traditional AI Frameworks&lt;/strong&gt;&lt;/h3&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;âš¡ &lt;strong&gt;Parlant&lt;/strong&gt;&lt;/h3&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%"&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Write complex system prompts&lt;/li&gt; 
     &lt;li&gt;Hope the LLM follows them&lt;/li&gt; 
     &lt;li&gt;Debug unpredictable behaviors&lt;/li&gt; 
     &lt;li&gt;Scale by prompt engineering&lt;/li&gt; 
     &lt;li&gt;Cross fingers for reliability&lt;/li&gt; 
    &lt;/ul&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Define rules in natural language&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Ensured&lt;/strong&gt; rule compliance&lt;/li&gt; 
     &lt;li&gt;Predictable, consistent behavior&lt;/li&gt; 
     &lt;li&gt;Scale by adding guidelines&lt;/li&gt; 
     &lt;li&gt;Production-ready from day one&lt;/li&gt; 
    &lt;/ul&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;ğŸ¯ Perfect For Your Use Case&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;Financial Services&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;Healthcare&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;E-commerce&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;Legal Tech&lt;/strong&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;Compliance-first design&lt;/td&gt; 
    &lt;td align="center"&gt;HIPAA-ready agents&lt;/td&gt; 
    &lt;td align="center"&gt;Customer service at scale&lt;/td&gt; 
    &lt;td align="center"&gt;Precise legal guidance&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;Built-in risk management&lt;/td&gt; 
    &lt;td align="center"&gt;Patient data protection&lt;/td&gt; 
    &lt;td align="center"&gt;Order processing automation&lt;/td&gt; 
    &lt;td align="center"&gt;Document review assistance&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸ› ï¸ Enterprise-Grade Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ§­ Conversational Journeys&lt;/strong&gt; - Lead the customer step-by-step to a goal&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ¯ Dynamic Guideline Matching&lt;/strong&gt; - Context-aware rule application&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ”§ Reliable Tool Integration&lt;/strong&gt; - APIs, databases, external services&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ“Š Conversation Analytics&lt;/strong&gt; - Deep insights into agent behavior&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ”„ Iterative Refinement&lt;/strong&gt; - Continuously improve agent responses&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ›¡ï¸ Built-in Guardrails&lt;/strong&gt; - Prevent hallucination and off-topic responses&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ“± React Widget&lt;/strong&gt; - &lt;a href="https://github.com/emcie-co/parlant-chat-react"&gt;Drop-in chat UI for any web app&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ” Full Explainability&lt;/strong&gt; - Understand every decision your agent makes&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ“ˆ Join 10,000+ Developers Building Better AI&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;strong&gt;Companies using Parlant:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Financial institutions â€¢ Healthcare providers â€¢ Legal firms â€¢ E-commerce platforms&lt;/em&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://star-history.com/#emcie-co/parlant&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=emcie-co/parlant&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸŒŸ What Developers Are Saying&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;"By far the most elegant conversational AI framework that I've come across! Developing with Parlant is pure joy."&lt;/em&gt; &lt;strong&gt;â€” Vishal Ahuja, Senior Lead, Customer-Facing Conversational AI @ JPMorgan Chase&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;ğŸƒâ€â™‚ï¸ Quick Start Paths&lt;/h2&gt; 
&lt;table border="0"&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸ¯ I want to test it myself&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.parlant.io/docs/quickstart/installation"&gt;â†’ 5-minute quickstart&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸ› ï¸ I want to see an example&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.parlant.io/docs/quickstart/examples"&gt;â†’ Healthcare agent example&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸš€ I want to get involved&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://discord.gg/duxWqxKk6J"&gt;â†’ Join our Discord community&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;ğŸ¤ Community &amp;amp; Support&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ’¬ &lt;strong&gt;&lt;a href="https://discord.gg/duxWqxKk6J"&gt;Discord Community&lt;/a&gt;&lt;/strong&gt; - Get help from the team and community&lt;/li&gt; 
 &lt;li&gt;ğŸ“– &lt;strong&gt;&lt;a href="https://parlant.io/docs/quickstart/installation"&gt;Documentation&lt;/a&gt;&lt;/strong&gt; - Comprehensive guides and examples&lt;/li&gt; 
 &lt;li&gt;ğŸ› &lt;strong&gt;&lt;a href="https://github.com/emcie-co/parlant/issues"&gt;GitHub Issues&lt;/a&gt;&lt;/strong&gt; - Bug reports and feature requests&lt;/li&gt; 
 &lt;li&gt;ğŸ“§ &lt;strong&gt;&lt;a href="https://parlant.io/contact"&gt;Direct Support&lt;/a&gt;&lt;/strong&gt; - Direct line to our engineering team&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ“„ License&lt;/h2&gt; 
&lt;p&gt;Apache 2.0 - Use it anywhere, including commercial projects.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;strong&gt;Ready to build AI agents that actually work?&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;â­ &lt;strong&gt;Star this repo&lt;/strong&gt; â€¢ ğŸš€ &lt;strong&gt;&lt;a href="https://parlant.io/"&gt;Try Parlant now&lt;/a&gt;&lt;/strong&gt; â€¢ ğŸ’¬ &lt;strong&gt;&lt;a href="https://discord.gg/duxWqxKk6J"&gt;Join Discord&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Built with â¤ï¸ by the team at &lt;a href="https://emcie.co"&gt;Emcie&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>hanxi/xiaomusic</title>
      <link>https://github.com/hanxi/xiaomusic</link>
      <description>&lt;p&gt;ä½¿ç”¨å°çˆ±éŸ³ç®±æ’­æ”¾éŸ³ä¹ï¼ŒéŸ³ä¹ä½¿ç”¨ yt-dlp ä¸‹è½½ã€‚&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;XiaoMusic: æ— é™å¬æ­Œï¼Œè§£æ”¾å°çˆ±éŸ³ç®±&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/hanxi/xiaomusic"&gt;&lt;img src="https://img.shields.io/github/license/hanxi/xiaomusic" alt="GitHub License" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/hanxi/xiaomusic"&gt;&lt;img src="https://img.shields.io/docker/v/hanxi/xiaomusic?sort=semver&amp;amp;label=docker%20image" alt="Docker Image Version" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/hanxi/xiaomusic"&gt;&lt;img src="https://img.shields.io/docker/pulls/hanxi/xiaomusic" alt="Docker Pulls" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/xiaomusic/"&gt;&lt;img src="https://img.shields.io/pypi/v/xiaomusic" alt="PyPI - Version" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/xiaomusic/"&gt;&lt;img src="https://img.shields.io/pypi/dm/xiaomusic" alt="PyPI - Downloads" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/xiaomusic/"&gt;&lt;img src="https://img.shields.io/python/required-version-toml?tomlFilePath=https%3A%2F%2Fraw.githubusercontent.com%2Fhanxi%2Fxiaomusic%2Fmain%2Fpyproject.toml" alt="Python Version from PEP 621 TOML" /&gt;&lt;/a&gt; &lt;a href="https://github.com/hanxi/xiaomusic/releases"&gt;&lt;img src="https://img.shields.io/github/v/release/hanxi/xiaomusic" alt="GitHub Release" /&gt;&lt;/a&gt; &lt;a href="https://visitorbadge.io/status?path=hanxi%2Fxiaomusic"&gt;&lt;img src="https://api.visitorbadge.io/api/daily?path=hanxi%2Fxiaomusic&amp;amp;label=daily%20visitor&amp;amp;countColor=%232ccce4&amp;amp;style=flat" alt="Visitors" /&gt;&lt;/a&gt; &lt;a href="https://visitorbadge.io/status?path=hanxi%2Fxiaomusic"&gt;&lt;img src="https://api.visitorbadge.io/api/visitors?path=hanxi%2Fxiaomusic&amp;amp;label=total%20visitor&amp;amp;countColor=%232ccce4&amp;amp;style=flat" alt="Visitors" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;ä½¿ç”¨å°çˆ±éŸ³ç®±æ’­æ”¾éŸ³ä¹ï¼ŒéŸ³ä¹ä½¿ç”¨ yt-dlp ä¸‹è½½ã€‚&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/hanxi/xiaomusic"&gt;https://github.com/hanxi/xiaomusic&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;æ–‡æ¡£: &lt;a href="https://xdocs.hanxi.cc/"&gt;https://xdocs.hanxi.cc/&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] åˆæ¬¡å®‰è£…é‡åˆ°é—®é¢˜è¯·æŸ¥é˜… &lt;a href="https://github.com/hanxi/xiaomusic/issues/99"&gt;ğŸ’¬ FAQé—®é¢˜é›†åˆ&lt;/a&gt; ï¼Œä¸€èˆ¬é‡åˆ°çš„é—®é¢˜éƒ½å·²ç»æœ‰è§£å†³åŠæ³•ã€‚&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;ğŸ‘‹ æœ€ç®€é…ç½®è¿è¡Œ&lt;/h2&gt; 
&lt;p&gt;å·²ç»æ”¯æŒåœ¨ web é¡µé¢é…ç½®å…¶ä»–å‚æ•°ï¼Œdocker å¯åŠ¨å‘½ä»¤å¦‚ä¸‹:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -p 58090:8090 -e XIAOMUSIC_PUBLIC_PORT=58090 -v /xiaomusic_music:/app/music -v /xiaomusic_conf:/app/conf hanxi/xiaomusic
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ğŸ”¥ å›½å†…ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -p 58090:8090 -e XIAOMUSIC_PUBLIC_PORT=58090 -v /xiaomusic_music:/app/music -v /xiaomusic_conf:/app/conf docker.hanxi.cc/hanxi/xiaomusic
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;å¯¹åº”çš„ docker compose é…ç½®å¦‚ä¸‹ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;services:
  xiaomusic:
    image: hanxi/xiaomusic
    container_name: xiaomusic
    restart: unless-stopped
    ports:
      - 58090:8090
    environment:
      XIAOMUSIC_PUBLIC_PORT: 58090
    volumes:
      - /xiaomusic_music:/app/music
      - /xiaomusic_conf:/app/conf
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ğŸ”¥ å›½å†…ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;services:
  xiaomusic:
    image: docker.hanxi.cc/hanxi/xiaomusic
    container_name: xiaomusic
    restart: unless-stopped
    ports:
      - 58090:8090
    environment:
      XIAOMUSIC_PUBLIC_PORT: 58090
    volumes:
      - /xiaomusic_music:/app/music
      - /xiaomusic_conf:/app/conf
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;å…¶ä¸­ conf ç›®å½•ä¸ºé…ç½®æ–‡ä»¶å­˜æ”¾ç›®å½•ï¼Œmusic ç›®å½•ä¸ºéŸ³ä¹å­˜æ”¾ç›®å½•ï¼Œå»ºè®®åˆ†å¼€é…ç½®ä¸ºä¸åŒçš„ç›®å½•ã€‚&lt;/li&gt; 
 &lt;li&gt;/xiaomusic_music å’Œ /xiaomusic_conf æ˜¯ docker æ‰€åœ¨çš„ä¸»æœºçš„ç›®å½•ï¼Œå¯ä»¥ä¿®æ”¹ä¸ºå…¶ä»–ç›®å½•ã€‚å¦‚æœæŠ¥é”™æ‰¾ä¸åˆ° /xiaomusic_music ç›®å½•ï¼Œå¯ä»¥å…ˆæ‰§è¡Œ &lt;code&gt;mkdir -p /xiaomusic_{music,conf}&lt;/code&gt; å‘½ä»¤æ–°å»ºç›®å½•ã€‚&lt;/li&gt; 
 &lt;li&gt;/app/music å’Œ /app/conf æ˜¯ docker å®¹å™¨é‡Œçš„ç›®å½•ï¼Œä¸è¦å»ä¿®æ”¹ã€‚&lt;/li&gt; 
 &lt;li&gt;XIAOMUSIC_PUBLIC_PORT æ˜¯ç”¨æ¥é…ç½® NAS æœ¬åœ°ç«¯å£çš„ã€‚8090 æ˜¯å®¹å™¨ç«¯å£ï¼Œä¸è¦å»ä¿®æ”¹ã€‚&lt;/li&gt; 
 &lt;li&gt;åå°è®¿é—®åœ°å€ä¸ºï¼š http://NAS_IP:58090&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] docker å’Œ docker compose äºŒé€‰ä¸€å³å¯ï¼Œå¯åŠ¨æˆåŠŸåï¼Œåœ¨ web é¡µé¢å¯ä»¥é…ç½®å…¶ä»–å‚æ•°ï¼Œå¸¦æœ‰ &lt;code&gt;*&lt;/code&gt; å·çš„é…ç½®æ˜¯å¿…é¡»è¦é…ç½®çš„ï¼Œå…¶ä»–çš„ç”¨ä¸ä¸Šæ—¶ä¸ç”¨ä¿®æ”¹ã€‚åˆæ¬¡é…ç½®æ—¶éœ€è¦åœ¨é¡µé¢ä¸Šè¾“å…¥å°ç±³è´¦å·å’Œå¯†ç ä¿å­˜åæ‰èƒ½è·å–åˆ°è®¾å¤‡åˆ—è¡¨ã€‚&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] ç›®å‰å®‰è£…æ­¥éª¤å·²ç»æ˜¯æœ€ç®€åŒ–äº†ï¼Œå¦‚æœè¿˜æ˜¯å«Œå®‰è£…éº»çƒ¦ï¼Œå¯ä»¥å¾®ä¿¡æˆ–è€… QQ çº¦æˆ‘è¿œç¨‹å®‰è£…ï¼Œæˆ‘ä¸€èˆ¬å‘¨æœ«å’Œæ™šä¸Šæ‰æœ‰æ—¶é—´ï¼Œéœ€è¦èµåŠ©ä¸ªè¾›è‹¦è´¹ &lt;span&gt;ğŸ’°&lt;/span&gt; 50 å…ƒä¸€æ¬¡ã€‚&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;é‡åˆ°é—®é¢˜å¯ä»¥å» web è®¾ç½®é¡µé¢åº•éƒ¨ç‚¹å‡»ã€ä¸‹è½½æ—¥å¿—æ–‡ä»¶ã€‘æŒ‰é’®ï¼Œç„¶åæœç´¢ä¸€ä¸‹æ—¥å¿—æ–‡ä»¶å†…å®¹ç¡®ä¿é‡Œé¢æ²¡æœ‰è´¦å·å¯†ç ä¿¡æ¯å(æœ‰å°±åˆ é™¤è¿™äº›æ•æ„Ÿä¿¡æ¯)ï¼Œç„¶ååœ¨æ issues åé¦ˆé—®é¢˜æ—¶æŠŠä¸‹è½½çš„æ—¥å¿—æ–‡ä»¶å¸¦ä¸Šã€‚&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] ä½œè€…çš„å¦ä¸€ä¸ªé€‚ç”¨äº NAS ä¸Šå®‰è£…çš„å¼€æºå·¥å…·ï¼š &lt;a href="https://github.com/hanxi/tiny-nav"&gt;https://github.com/hanxi/tiny-nav&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP]&lt;/p&gt; 
 &lt;p&gt;å–œæ¬¢å¬ä¹¦çš„å¯ä»¥é…åˆè¿™ä¸ªå·¥å…·ä½¿ç”¨ &lt;a href="https://github.com/hanxi/epub2mp3"&gt;https://github.com/hanxi/epub2mp3&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP]&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;ğŸ”¥ã€å¹¿å‘Š:å¯ç”¨äºå®‰è£… frp å®ç°å†…ç½‘ç©¿é€ã€‘&lt;/li&gt; 
  &lt;li&gt;ğŸ”¥ æµ·å¤– RackNerd VPS æœºå™¨æ¨èï¼Œå¯æ”¯ä»˜å®ä»˜æ¬¾ã€‚&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://my.racknerd.com/aff.php?aff=11177"&gt;&lt;img src="https://racknerd.com/banners/320x50.gif" alt="RackNerd Mobile Leaderboard Banner" width="320" height="50" /&gt;&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;ä¸çŸ¥é“é€‰å“ªä¸ªå¥—é¤å¯ä»¥ç›´æ¥ä¹°è¿™ä¸ªæœ€ä¾¿å®œçš„ &lt;a href="https://my.racknerd.com/aff.php?aff=11177&amp;amp;pid=912"&gt;https://my.racknerd.com/aff.php?aff=11177&amp;amp;pid=912&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;ä¹Ÿå¯ä»¥ç”¨æ¥éƒ¨ç½²ä»£ç†ï¼Œdocker éƒ¨ç½²æ–¹æ³•è§ &lt;a href="https://github.com/hanxi/blog/issues/96"&gt;https://github.com/hanxi/blog/issues/96&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP]&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;ğŸ”¥ã€å¹¿å‘Š: æ­å»ºæ‚¨çš„ä¸“å±å¤§æ¨¡å‹ä¸»é¡µ å‘Šåˆ«ç¹çé…ç½®éš¾é¢˜ï¼Œä¸€é”®å³å¯ç•…äº«ç¨³å®šæµç•…çš„AIä½“éªŒï¼ã€‘&lt;a href="https://university.aliyun.com/mobile?userCode=szqvatm6"&gt;https://university.aliyun.com/mobile?userCode=szqvatm6&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP]&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;å…è´¹ä¸»æœº&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://dartnode.com?aff=SnappyPigeon570"&gt;&lt;img src="https://dartnode.com/branding/DN-Open-Source-sm.png" alt="Powered by DartNode - Free VPS for Open Source" width="320" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;ğŸ¤ æ”¯æŒè¯­éŸ³å£ä»¤&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ã€æ’­æ”¾æ­Œæ›²ã€‘ï¼Œæ’­æ”¾æœ¬åœ°çš„æ­Œæ›²&lt;/li&gt; 
 &lt;li&gt;ã€æ’­æ”¾æ­Œæ›²+æ­Œåã€‘ï¼Œæ¯”å¦‚ï¼šæ’­æ”¾æ­Œæ›²å‘¨æ°ä¼¦æ™´å¤©&lt;/li&gt; 
 &lt;li&gt;ã€ä¸Šä¸€é¦–ã€‘&lt;/li&gt; 
 &lt;li&gt;ã€ä¸‹ä¸€é¦–ã€‘&lt;/li&gt; 
 &lt;li&gt;ã€å•æ›²å¾ªç¯ã€‘&lt;/li&gt; 
 &lt;li&gt;ã€å…¨éƒ¨å¾ªç¯ã€‘&lt;/li&gt; 
 &lt;li&gt;ã€éšæœºæ’­æ”¾ã€‘&lt;/li&gt; 
 &lt;li&gt;ã€å…³æœºã€‘ï¼Œã€åœæ­¢æ’­æ”¾ã€‘ï¼Œä¸¤ä¸ªæ•ˆæœæ˜¯ä¸€æ ·çš„ã€‚&lt;/li&gt; 
 &lt;li&gt;ã€åˆ·æ–°åˆ—è¡¨ã€‘ï¼Œå½“å¤åˆ¶äº†æ­Œæ›²è¿› music ç›®å½•åï¼Œå¯ä»¥ç”¨è¿™ä¸ªå£ä»¤åˆ·æ–°æ­Œå•ã€‚&lt;/li&gt; 
 &lt;li&gt;ã€æ’­æ”¾åˆ—è¡¨+åˆ—è¡¨åã€‘ï¼Œæ¯”å¦‚ï¼šæ’­æ”¾åˆ—è¡¨å…¶ä»–ã€‚&lt;/li&gt; 
 &lt;li&gt;ã€åŠ å…¥æ”¶è—ã€‘ï¼ŒæŠŠå½“å‰æ’­æ”¾çš„æ­Œæ›²åŠ å…¥æ”¶è—æ­Œå•ã€‚&lt;/li&gt; 
 &lt;li&gt;ã€å–æ¶ˆæ”¶è—ã€‘ï¼ŒæŠŠå½“å‰æ’­æ”¾çš„æ­Œæ›²ä»æ”¶è—æ­Œå•é‡Œç§»é™¤ã€‚&lt;/li&gt; 
 &lt;li&gt;ã€æ’­æ”¾åˆ—è¡¨æ”¶è—ã€‘ï¼Œè¿™ä¸ªç”¨äºæ’­æ”¾æ”¶è—æ­Œå•ã€‚&lt;/li&gt; 
 &lt;li&gt;&lt;del&gt;ã€æ’­æ”¾æœ¬åœ°æ­Œæ›²+æ­Œåã€‘ï¼Œè¿™ä¸ªå£ä»¤å’Œæ’­æ”¾æ­Œæ›²çš„åŒºåˆ«æ˜¯æœ¬åœ°æ‰¾ä¸åˆ°ä¹Ÿä¸ä¼šå»ä¸‹è½½ã€‚&lt;/del&gt;&lt;/li&gt; 
 &lt;li&gt;ã€æ’­æ”¾åˆ—è¡¨ç¬¬å‡ ä¸ª+åˆ—è¡¨åã€‘ï¼Œå…·ä½“è§ï¼š &lt;a href="https://github.com/hanxi/xiaomusic/issues/158"&gt;https://github.com/hanxi/xiaomusic/issues/158&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ã€æœç´¢æ’­æ”¾+å…³é”®è¯ã€‘ï¼Œä¼šæœç´¢å…³é”®è¯ä½œä¸ºä¸´æ—¶æœç´¢åˆ—è¡¨æ’­æ”¾ï¼Œæ¯”å¦‚è¯´ã€æœç´¢æ’­æ”¾æ—ä¿Šæ°ã€‘ï¼Œä¼šæ’­æ”¾æ‰€æœ‰æ—ä¿Šæ°çš„æ­Œã€‚&lt;/li&gt; 
 &lt;li&gt;ã€æœ¬åœ°æœç´¢æ’­æ”¾+å…³é”®è¯ã€‘ï¼Œè·Ÿæœç´¢æ’­æ”¾çš„åŒºåˆ«æ˜¯æœ¬åœ°æ‰¾ä¸åˆ°ä¹Ÿä¸ä¼šå»ä¸‹è½½ã€‚&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] éšè—ç©æ³•: å¯¹å°çˆ±åŒå­¦è¯´æ’­æ”¾æ­Œæ›²å°çŒªä½©å¥‡çš„æ•…äº‹ï¼Œä¼šå…ˆä¸‹è½½å°çŒªä½©å¥‡çš„æ•…äº‹ï¼Œç„¶åå†æ’­æ”¾å°çŒªä½©å¥‡çš„æ•…äº‹ã€‚&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;ğŸ› ï¸ pip æ–¹å¼å®‰è£…è¿è¡Œ&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;&amp;gt; pip install -U xiaomusic
&amp;gt; xiaomusic --help
 __  __  _                   __  __                 _
 \ \/ / (_)   __ _    ___   |  \/  |  _   _   ___  (_)   ___
  \  /  | |  / _` |  / _ \  | |\/| | | | | | / __| | |  / __|
  /  \  | | | (_| | | (_) | | |  | | | |_| | \__ \ | | | (__
 /_/\_\ |_|  \__,_|  \___/  |_|  |_|  \__,_| |___/ |_|  \___|
          XiaoMusic v0.3.69 by: github.com/hanxi

usage: xiaomusic [-h] [--port PORT] [--hardware HARDWARE] [--account ACCOUNT]
                 [--password PASSWORD] [--cookie COOKIE] [--verbose]
                 [--config CONFIG] [--ffmpeg_location FFMPEG_LOCATION]

options:
  -h, --help            show this help message and exit
  --port PORT           ç›‘å¬ç«¯å£
  --hardware HARDWARE   å°çˆ±éŸ³ç®±å‹å·
  --account ACCOUNT     xiaomi account
  --password PASSWORD   xiaomi password
  --cookie COOKIE       xiaomi cookie
  --verbose             show info
  --config CONFIG       config file path
  --ffmpeg_location FFMPEG_LOCATION
                        ffmpeg bin path
&amp;gt; xiaomusic --config config.json
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;å…¶ä¸­ &lt;code&gt;config.json&lt;/code&gt; æ–‡ä»¶å¯ä»¥å‚è€ƒ &lt;code&gt;config-example.json&lt;/code&gt; æ–‡ä»¶é…ç½®ã€‚è§ &lt;a href="https://github.com/hanxi/xiaomusic/issues/94"&gt;https://github.com/hanxi/xiaomusic/issues/94&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;ä¸ä¿®æ”¹é»˜è®¤ç«¯å£ 8090 çš„æƒ…å†µä¸‹ï¼Œåªéœ€è¦æ‰§è¡Œ &lt;code&gt;xiaomusic&lt;/code&gt; å³å¯å¯åŠ¨ã€‚&lt;/p&gt; 
&lt;h2&gt;ğŸ”© å¼€å‘ç¯å¢ƒè¿è¡Œ&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ä½¿ç”¨ install_dependencies.sh ä¸‹è½½ä¾èµ–&lt;/li&gt; 
 &lt;li&gt;ä½¿ç”¨ pdm å®‰è£…ç¯å¢ƒ&lt;/li&gt; 
 &lt;li&gt;é»˜è®¤ç›‘å¬äº†ç«¯å£ 8090 , ä½¿ç”¨å…¶ä»–ç«¯å£è‡ªè¡Œä¿®æ”¹ã€‚&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pdm run xiaomusic.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;å¦‚æœæ˜¯å¼€å‘å‰ç«¯ç•Œé¢ï¼Œå¯ä»¥é€šè¿‡ &lt;a href="http://localhost:8090/docs"&gt;http://localhost:8090/docs&lt;/a&gt; æŸ¥çœ‹æœ‰ä»€ä¹ˆæ¥å£ã€‚ç›®å‰çš„ web æ§åˆ¶å°éå¸¸ç®€é™‹ï¼Œæ¬¢è¿æœ‰å…´è¶£çš„æœ‹å‹å¸®å¿™å®ç°ä¸€ä¸ªæ¼‚äº®çš„å‰ç«¯ï¼Œéœ€è¦ä»€ä¹ˆæ¥å£å¯ä»¥éšæ—¶æéœ€æ±‚ã€‚&lt;/p&gt; 
&lt;h3&gt;ğŸš¦ ä»£ç æäº¤è§„èŒƒ&lt;/h3&gt; 
&lt;p&gt;æäº¤å‰è¯·æ‰§è¡Œ&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pdm lintfmt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ç”¨äºæ£€æŸ¥ä»£ç å’Œæ ¼å¼åŒ–ä»£ç ã€‚&lt;/p&gt; 
&lt;h3&gt;æœ¬åœ°ç¼–è¯‘ Docker Image&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;docker build -t xiaomusic .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;æŠ€æœ¯æ ˆ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;åç«¯ä»£ç ä½¿ç”¨ Python è¯­è¨€ç¼–å†™ã€‚&lt;/li&gt; 
 &lt;li&gt;HTTP æœåŠ¡ä½¿ç”¨çš„æ˜¯ FastAPI æ¡†æ¶ï¼Œ&lt;del&gt;æ—©æœŸç‰ˆæœ¬ä½¿ç”¨çš„æ˜¯ Flask&lt;/del&gt;ã€‚&lt;/li&gt; 
 &lt;li&gt;ä½¿ç”¨äº† Docker ï¼Œåœ¨ NAS ä¸Šå®‰è£…æ›´æ–¹ä¾¿ã€‚&lt;/li&gt; 
 &lt;li&gt;é»˜è®¤çš„å‰ç«¯ä¸»é¢˜ä½¿ç”¨äº† jQuery ã€‚&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;å·²æµ‹è¯•æ”¯æŒçš„è®¾å¤‡&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;å‹å·&lt;/th&gt; 
   &lt;th&gt;åç§°&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;L06A&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://home.mi.com/baike/index.html#/detail?model=xiaomi.wifispeaker.l06a"&gt;å°çˆ±éŸ³ç®±&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;L07A&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://home.mi.com/webapp/content/baike/product/index.html?model=xiaomi.wifispeaker.l7a"&gt;Redmiå°çˆ±éŸ³ç®± Play&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;S12/S12A/MDZ-25-DA&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://home.mi.com/baike/index.html#/detail?model=xiaomi.wifispeaker.s12"&gt;å°ç±³AIéŸ³ç®±&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;LX5A&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://home.mi.com/baike/index.html#/detail?model=xiaomi.wifispeaker.lx5a"&gt;å°çˆ±éŸ³ç®± ä¸‡èƒ½é¥æ§ç‰ˆ&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;LX05&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://home.mi.com/baike/index.html#/detail?model=xiaomi.wifispeaker.lx05"&gt;å°çˆ±éŸ³ç®±Playï¼ˆ2019æ¬¾ï¼‰&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;L15A&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://home.mi.com/webapp/content/baike/product/index.html?model=xiaomi.wifispeaker.l15a#/"&gt;å°ç±³AIéŸ³ç®±ï¼ˆç¬¬äºŒä»£ï¼‰&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;L16A&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://home.mi.com/baike/index.html#/detail?model=xiaomi.wifispeaker.l16a"&gt;Xiaomi Sound&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;L17A&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://home.mi.com/baike/index.html#/detail?model=xiaomi.wifispeaker.l17a"&gt;Xiaomi Sound Pro&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;LX06&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://home.mi.com/baike/index.html#/detail?model=xiaomi.wifispeaker.lx06"&gt;å°çˆ±éŸ³ç®±Pro&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;LX01&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://home.mi.com/baike/index.html#/detail?model=xiaomi.wifispeaker.lx01"&gt;å°çˆ±éŸ³ç®±mini&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;L05B&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://home.mi.com/baike/index.html#/detail?model=xiaomi.wifispeaker.l05b"&gt;å°çˆ±éŸ³ç®±Play&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;L05C&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://home.mi.com/baike/index.html#/detail?model=xiaomi.wifispeaker.l05c"&gt;å°ç±³å°çˆ±éŸ³ç®±Play å¢å¼ºç‰ˆ&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;L09A&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://home.mi.com/webapp/content/baike/product/index.html?model=xiaomi.wifispeaker.l09a"&gt;å°ç±³éŸ³ç®±Art&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;LX04 X10A X08A&lt;/td&gt; 
   &lt;td&gt;å·²ç»æ”¯æŒçš„è§¦å±ç‰ˆ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;X08C X08E X8F&lt;/td&gt; 
   &lt;td&gt;å·²ç»ä¸éœ€è¦è®¾ç½®äº†. &lt;del&gt;éœ€è¦è®¾ç½®ã€å‹å·å…¼å®¹æ¨¡å¼ã€‘é€‰é¡¹ä¸º true&lt;/del&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;M01/XMYX01JY&lt;/td&gt; 
   &lt;td&gt;å°ç±³å°çˆ±éŸ³ç®±HD éœ€è¦è®¾ç½®ã€ç‰¹æ®Šå‹å·è·å–å¯¹è¯è®°å½•ã€‘é€‰é¡¹ä¸º true æ‰èƒ½è¯­éŸ³æ’­æ”¾&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OH2P&lt;/td&gt; 
   &lt;td&gt;XIAOMI æ™ºèƒ½éŸ³ç®± Pro&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OH2&lt;/td&gt; 
   &lt;td&gt;XIAOMI æ™ºèƒ½éŸ³ç®±&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;å‹å·ä¸äº§å“åç§°å¯¹ç…§å¯ä»¥åœ¨è¿™é‡ŒæŸ¥è¯¢ &lt;a href="https://home.miot-spec.com/s/xiaomi.wifispeaker"&gt;https://home.miot-spec.com/s/xiaomi.wifispeaker&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] å¦‚æœä½ çš„è®¾å¤‡æ”¯æŒæ’­æ”¾ï¼Œè¯·åé¦ˆç»™æˆ‘æ·»åŠ åˆ°æ”¯æŒåˆ—è¡¨é‡Œï¼Œè°¢è°¢ã€‚ ç›®å‰åº”è¯¥æ‰€æœ‰è®¾å¤‡ç±»å‹éƒ½å·²ç»æ”¯æŒæ’­æ”¾ï¼Œæœ‰é—®é¢˜éšæ—¶åé¦ˆã€‚ å…¶ä»–è§¦å±ç‰ˆä¸èƒ½æ’­æ”¾å¯ä»¥è®¾ç½®ã€å‹å·å…¼å®¹æ¨¡å¼ã€‘é€‰é¡¹ä¸º true è¯•è¯•ã€‚è§ &lt;a href="https://github.com/hanxi/xiaomusic/issues/30"&gt;https://github.com/hanxi/xiaomusic/issues/30&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;ğŸµ æ”¯æŒéŸ³ä¹æ ¼å¼&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;mp3&lt;/li&gt; 
 &lt;li&gt;flac&lt;/li&gt; 
 &lt;li&gt;wav&lt;/li&gt; 
 &lt;li&gt;ape&lt;/li&gt; 
 &lt;li&gt;ogg&lt;/li&gt; 
 &lt;li&gt;m4a&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] æœ¬åœ°éŸ³ä¹ä¼šæœç´¢ç›®å½•ä¸‹ä¸Šé¢æ ¼å¼çš„æ–‡ä»¶ï¼Œä¸‹è½½çš„æ­Œæ›²æ˜¯ mp3 æ ¼å¼çš„ã€‚ å·²çŸ¥ L05B L05C LX06 L16A ä¸æ”¯æŒ flac æ ¼å¼ã€‚ å¦‚æœæ ¼å¼ä¸èƒ½æ’­æ”¾å¯ä»¥æ‰“å¼€ã€è½¬æ¢ä¸ºMP3ã€‘å’Œã€å‹å·å…¼å®¹æ¨¡å¼ã€‘é€‰é¡¹ã€‚å…·ä½“è§ &lt;a href="https://github.com/hanxi/xiaomusic/issues/153#issuecomment-2328168689"&gt;https://github.com/hanxi/xiaomusic/issues/153#issuecomment-2328168689&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;ğŸŒ ç½‘ç»œæ­Œå•åŠŸèƒ½&lt;/h2&gt; 
&lt;p&gt;å¯ä»¥é…ç½®ä¸€ä¸ª json æ ¼å¼çš„æ­Œå•ï¼Œæ”¯æŒç”µå°å’Œæ­Œæ›²ï¼Œä¹Ÿå¯ä»¥ç›´æ¥ç”¨åˆ«äººåˆ†äº«çš„é“¾æ¥ï¼ŒåŒæ—¶é…å¤‡äº† m3u æ–‡ä»¶æ ¼å¼è½¬æ¢å·¥å…·ï¼Œå¯ä»¥å¾ˆæ–¹ä¾¿çš„æŠŠ m3u ç”µå°æ–‡ä»¶è½¬æ¢æˆç½‘ç»œæ­Œå•æ ¼å¼çš„ json æ–‡ä»¶ï¼Œå…·ä½“ç”¨æ³•è§ &lt;a href="https://github.com/hanxi/xiaomusic/issues/78"&gt;https://github.com/hanxi/xiaomusic/issues/78&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] æ¬¢è¿æœ‰æƒ³æ³•çš„æœ‹å‹ä»¬åˆ¶ä½œæ›´å¤šçš„æ­Œå•è½¬æ¢å·¥å…·ã€‚&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;ğŸº æ›´å¤šå…¶ä»–å¯é€‰é…ç½®&lt;/h2&gt; 
&lt;p&gt;è§ &lt;a href="https://github.com/hanxi/xiaomusic/issues/333"&gt;https://github.com/hanxi/xiaomusic/issues/333&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;âš ï¸ å®‰å…¨æé†’&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT]&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;å¦‚æœé…ç½®äº†å…¬ç½‘è®¿é—® xiaomusic ï¼Œè¯·ä¸€å®šè¦å¼€å¯å¯†ç ç™»é™†ï¼Œå¹¶è®¾ç½®å¤æ‚çš„å¯†ç ã€‚ä¸”ä¸è¦åœ¨å…¬å…±åœºæ‰€çš„ WiFi ç¯å¢ƒä¸‹ä½¿ç”¨ï¼Œå¦åˆ™å¯èƒ½é€ æˆå°ç±³è´¦å·å¯†ç æ³„éœ²ã€‚&lt;/li&gt; 
  &lt;li&gt;å¼ºçƒˆä¸å»ºè®®å°†å°çˆ±éŸ³ç®±çš„å°ç±³è´¦å·ç»‘å®šæ‘„åƒå¤´ï¼Œä»£ç éš¾å…ä¼šæœ‰ bug ï¼Œä¸€æ—¦å°ç±³è´¦å·å¯†ç æ³„éœ²ï¼Œå¯èƒ½ç›‘æ§å½•åƒä¹Ÿä¼šæ³„éœ²ã€‚&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;ğŸ¤” é«˜çº§ç¯‡&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;è‡ªå®šä¹‰å£ä»¤åŠŸèƒ½ &lt;a href="https://github.com/hanxi/xiaomusic/issues/105"&gt;https://github.com/hanxi/xiaomusic/issues/105&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hanxi/xiaomusic/issues/312"&gt;https://github.com/hanxi/xiaomusic/issues/312&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hanxi/xiaomusic/issues/269"&gt;https://github.com/hanxi/xiaomusic/issues/269&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hanxi/xiaomusic/issues/159"&gt;https://github.com/hanxi/xiaomusic/issues/159&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ“¢ è®¨è®ºåŒº&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://pd.qq.com/s/e2jybz0ss"&gt;ç‚¹å‡»é“¾æ¥åŠ å…¥QQé¢‘é“ã€xiaomusicã€‘&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://qm.qq.com/cgi-bin/qm/qr?_wv=1027&amp;amp;k=13St5PLVcTxYlWTAs_iAawazjtdD1l-a&amp;amp;authKey=dJWEpaT2fDBDpdUUOWj%2FLt6NS1ePBfShDfz7a6seNURi05VvVnAGQzXF%2FM%2F5HgIm&amp;amp;noverify=0&amp;amp;group_code=604526973"&gt;ç‚¹å‡»é“¾æ¥åŠ å…¥ç¾¤èŠã€æ»¡ xiaomusicå®˜æ–¹äº¤æµç¾¤1ã€‘ 604526973&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://qm.qq.com/q/BmVNqhDL3M"&gt;ç‚¹å‡»é“¾æ¥åŠ å…¥ç¾¤èŠã€æ»¡ xiaomusicå®˜æ–¹äº¤æµç¾¤2ã€‘1021062499&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://qm.qq.com/q/lxIhquqbza"&gt;ç‚¹å‡»é“¾æ¥åŠ å…¥ç¾¤èŠã€xiaomusicå®˜æ–¹äº¤æµç¾¤3ã€‘ 1072151477&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hanxi/xiaomusic/issues"&gt;https://github.com/hanxi/xiaomusic/issues&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hanxi/xiaomusic/issues/86"&gt;å¾®ä¿¡ç¾¤äºŒç»´ç &lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;â¤ï¸ æ„Ÿè°¢&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.mi.com/"&gt;xiaomi&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pdm.fming.dev/latest/"&gt;PDM&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/yihong0618/xiaogpt"&gt;xiaogpt&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/yihong0618/MiService"&gt;MiService&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/yihong0618/gitblog/issues/258"&gt;å®ç°åŸç†&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp"&gt;yt-dlp&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/zzz6519003/awesome-xiaoai"&gt;awesome-xiaoai&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/F-loat/xiaoplayer"&gt;å¾®ä¿¡å°ç¨‹åº: å¯å¯éŸ³ä¹&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/52fisher/xiaomusicUI"&gt;pure ä¸»é¢˜ xiaomusicUI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/52fisher/XMusicPlayer"&gt;ç§»åŠ¨ç«¯çš„æ’­æ”¾å™¨ä¸»é¢˜&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/clarencejh/xiaomusic"&gt;Tailwindä¸»é¢˜&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/DarrenWen/xiaomusicui"&gt;ä¸€ä¸ªç¬¬ä¸‰æ–¹çš„ä¸»é¢˜&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/umami-software/umami"&gt;Umami ç»Ÿè®¡&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/getsentry/sentry"&gt;Sentry æŠ¥é”™ç›‘æ§&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;æ‰€æœ‰å¸®å¿™è°ƒè¯•å’Œæµ‹è¯•çš„æœ‹å‹&lt;/li&gt; 
 &lt;li&gt;æ‰€æœ‰åé¦ˆé—®é¢˜å’Œå»ºè®®çš„æœ‹å‹&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ‘‰ å…¶ä»–æ•™ç¨‹&lt;/h3&gt; 
&lt;p&gt;æ›´å¤šåŠŸèƒ½è§ &lt;a href="https://github.com/hanxi/xiaomusic/issues/211"&gt;ğŸ“ æ–‡æ¡£æ±‡æ€»&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ğŸš¨ å…è´£å£°æ˜&lt;/h2&gt; 
&lt;p&gt;æœ¬é¡¹ç›®ä»…ä¾›å­¦ä¹ å’Œç ”ç©¶ç›®çš„ï¼Œä¸å¾—ç”¨äºä»»ä½•å•†ä¸šæ´»åŠ¨ã€‚ç”¨æˆ·åœ¨ä½¿ç”¨æœ¬é¡¹ç›®æ—¶åº”éµå®ˆæ‰€åœ¨åœ°åŒºçš„æ³•å¾‹æ³•è§„ï¼Œå¯¹äºè¿æ³•ä½¿ç”¨æ‰€å¯¼è‡´çš„åæœï¼Œæœ¬é¡¹ç›®åŠä½œè€…ä¸æ‰¿æ‹…ä»»ä½•è´£ä»»ã€‚ æœ¬é¡¹ç›®å¯èƒ½å­˜åœ¨æœªçŸ¥çš„ç¼ºé™·å’Œé£é™©ï¼ˆåŒ…æ‹¬ä½†ä¸é™äºè®¾å¤‡æŸåå’Œè´¦å·å°ç¦ç­‰ï¼‰ï¼Œä½¿ç”¨è€…åº”è‡ªè¡Œæ‰¿æ‹…ä½¿ç”¨æœ¬é¡¹ç›®æ‰€äº§ç”Ÿçš„æ‰€æœ‰é£é™©åŠè´£ä»»ã€‚ ä½œè€…ä¸ä¿è¯æœ¬é¡¹ç›®çš„å‡†ç¡®æ€§ã€å®Œæ•´æ€§ã€åŠæ—¶æ€§ã€å¯é æ€§ï¼Œä¹Ÿä¸æ‰¿æ‹…ä»»ä½•å› ä½¿ç”¨æœ¬é¡¹ç›®è€Œäº§ç”Ÿçš„ä»»ä½•æŸå¤±æˆ–æŸå®³è´£ä»»ã€‚ ä½¿ç”¨æœ¬é¡¹ç›®å³è¡¨ç¤ºæ‚¨å·²é˜…è¯»å¹¶åŒæ„æœ¬å…è´£å£°æ˜çš„å…¨éƒ¨å†…å®¹ã€‚&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#hanxi/xiaomusic&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=hanxi/xiaomusic&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;èµèµ&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span&gt;ğŸ’°&lt;/span&gt; çˆ±å‘ç”µ &lt;a href="https://afdian.com/a/imhanxi"&gt;https://afdian.com/a/imhanxi&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ç‚¹ä¸ª Star &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;è°¢è°¢ &lt;span&gt;â¤ï¸&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;img src="https://i.v2ex.co/7Q03axO5l.png" alt="å–æ¯å¥¶èŒ¶" /&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/hanxi/xiaomusic/raw/main/LICENSE"&gt;MIT&lt;/a&gt; License Â© 2024 æ¶µæ›¦&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>suitenumerique/docs</title>
      <link>https://github.com/suitenumerique/docs</link>
      <description>&lt;p&gt;A collaborative note taking, wiki and documentation platform that scales. Built with Django and React.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://github.com/suitenumerique/docs"&gt; &lt;img alt="Docs" src="https://raw.githubusercontent.com/suitenumerique/docs/main/docs/assets/banner-docs.png" width="100%" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/suitenumerique/docs/stargazers/"&gt; &lt;img src="https://img.shields.io/github/stars/suitenumerique/docs" alt="" /&gt; &lt;/a&gt; &lt;a href="https://github.com/suitenumerique/docs/raw/main/CONTRIBUTING.md"&gt;&lt;img alt="PRs Welcome" src="https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=shields" /&gt;&lt;/a&gt; &lt;img alt="GitHub commit activity" src="https://img.shields.io/github/commit-activity/m/suitenumerique/docs" /&gt; &lt;img alt="GitHub closed issues" src="https://img.shields.io/github/issues-closed/suitenumerique/docs" /&gt; &lt;a href="https://github.com/suitenumerique/docs/raw/main/LICENSE"&gt; &lt;img alt="MIT License" src="https://img.shields.io/github/license/suitenumerique/docs" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://matrix.to/#/#docs-official:matrix.org"&gt; Chat on Matrix &lt;/a&gt; - &lt;a href="https://raw.githubusercontent.com/suitenumerique/docs/main/docs/"&gt; Documentation &lt;/a&gt; - &lt;a href="https://raw.githubusercontent.com/suitenumerique/docs/main/#getting-started-"&gt; Getting started &lt;/a&gt; - &lt;a href="mailto:docs@numerique.gouv.fr"&gt; Reach out &lt;/a&gt; &lt;/p&gt; 
&lt;h1&gt;La Suite Docs : Collaborative Text Editing&lt;/h1&gt; 
&lt;p&gt;Docs, where your notes can become knowledge through live collaboration.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/suitenumerique/docs/main/docs/assets/docs_live_collaboration_light.gif" width="100%" align="center" /&gt; 
&lt;h2&gt;Why use Docs â“&lt;/h2&gt; 
&lt;p&gt;Docs is a collaborative text editor designed to address common challenges in knowledge building and sharing.&lt;/p&gt; 
&lt;h3&gt;Write&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ˜Œ Get simple, accessible online editing for your team.&lt;/li&gt; 
 &lt;li&gt;ğŸ’… Create clean documents with beautiful formatting options.&lt;/li&gt; 
 &lt;li&gt;ğŸ–Œï¸ Focus on your content using either the in-line editor, or &lt;a href="https://www.markdownguide.org/basic-syntax/"&gt;the Markdown syntax&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;ğŸ§± Quickly design your page thanks to the many block types, accessible from the &lt;code&gt;/&lt;/code&gt; slash commands, as well as keyboard shortcuts.&lt;/li&gt; 
 &lt;li&gt;ğŸ”Œ Write offline! Your edits will be synced once you're back online.&lt;/li&gt; 
 &lt;li&gt;âœ¨ Save time thanks to our AI actions, such as rephrasing, summarizing, fixing typos, translating, etc. You can even turn your selected text into a prompt!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Work together&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ¤ Enjoy live editing! See your team collaborate in real time.&lt;/li&gt; 
 &lt;li&gt;ğŸ”’ Keep your information secure thanks to granular access control. Only share with the right people.&lt;/li&gt; 
 &lt;li&gt;ğŸ“‘ Export your content in multiple formats (&lt;code&gt;.odt&lt;/code&gt;, &lt;code&gt;.docx&lt;/code&gt;, &lt;code&gt;.pdf&lt;/code&gt;) with customizable templates.&lt;/li&gt; 
 &lt;li&gt;ğŸ“š Turn your team's collaborative work into organized knowledge with Subpages.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Self-host&lt;/h3&gt; 
&lt;h4&gt;ğŸš€ Docs is easy to install on your own servers&lt;/h4&gt; 
&lt;p&gt;We use Kubernetes for our &lt;a href="https://docs.numerique.gouv.fr/"&gt;production instance&lt;/a&gt; but also support Docker Compose. The community contributed a couple other methods (Nix, YunoHost etc.) check out the &lt;a href="https://raw.githubusercontent.com/suitenumerique/docs/main/docs/installation/README.md"&gt;docs&lt;/a&gt; to get detailed instructions and examples.&lt;/p&gt; 
&lt;h4&gt;ğŸŒ Known instances&lt;/h4&gt; 
&lt;p&gt;We hope to see many more, here is an incomplete list of public Docs instances. Feel free to make a PR to add ones that are not listed belowğŸ™&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Url&lt;/th&gt; 
   &lt;th&gt;Org&lt;/th&gt; 
   &lt;th&gt;Public&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.numerique.gouv.fr/"&gt;docs.numerique.gouv.fr&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;DINUM&lt;/td&gt; 
   &lt;td&gt;French public agents working for the central administration and the extended public sphere. ProConnect is required to login in or sign up&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.suite.anct.gouv.fr/"&gt;docs.suite.anct.gouv.fr&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ANCT&lt;/td&gt; 
   &lt;td&gt;French public agents working for the territorial administration and the extended public sphere. ProConnect is required to login in or sign up&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://notes.demo.opendesk.eu"&gt;notes.demo.opendesk.eu&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ZenDiS&lt;/td&gt; 
   &lt;td&gt;Demo instance of OpenDesk. Request access to get credentials&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://notes.liiib.re/"&gt;notes.liiib.re&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;lasuite.coop&lt;/td&gt; 
   &lt;td&gt;Free and open demo to all. Content and accounts are reset after one month&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.federated.nexus/"&gt;docs.federated.nexus&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;federated.nexus&lt;/td&gt; 
   &lt;td&gt;Public instance, but you have to &lt;a href="https://federated.nexus/register/"&gt;sign up for a Federated Nexus account&lt;/a&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.demo.mosacloud.eu/"&gt;docs.demo.mosacloud.eu&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;mosa.cloud&lt;/td&gt; 
   &lt;td&gt;Demo instance of mosa.cloud, a dutch company providing services around La Suite apps.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h4&gt;âš ï¸ Advanced features&lt;/h4&gt; 
&lt;p&gt;For some advanced features (ex: Export as PDF) Docs relies on XL packages from BlockNote. These are licenced under GPL and are not MIT compatible. You can perfectly use Docs without these packages by setting the environment variable &lt;code&gt;PUBLISH_AS_MIT&lt;/code&gt; to true. That way you'll build an image of the application without the features that are not MIT compatible. Read the &lt;a href="https://raw.githubusercontent.com/suitenumerique/docs/main/docs/env.md"&gt;environment variables documentation&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;Getting started ğŸ”§&lt;/h2&gt; 
&lt;h3&gt;Test it&lt;/h3&gt; 
&lt;p&gt;You can test Docs on your browser by visiting this &lt;a href="https://impress-preprod.beta.numerique.gouv.fr/docs/6ee5aac4-4fb9-457d-95bf-bb56c2467713/"&gt;demo document&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Run Docs locally&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;âš ï¸ The methods described below for running Docs locally is &lt;strong&gt;for testing purposes only&lt;/strong&gt;. It is based on building Docs using &lt;a href="https://min.io/"&gt;Minio&lt;/a&gt; as an S3-compatible storage solution. Of course you can choose any S3-compatible storage solution.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Prerequisite&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Make sure you have a recent version of Docker and &lt;a href="https://docs.docker.com/compose/install"&gt;Docker Compose&lt;/a&gt; installed on your laptop, then type:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shellscript"&gt;$ docker -v

Docker version 20.10.2, build 2291f61

$ docker compose version

Docker Compose version v2.32.4
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;âš ï¸ You may need to run the following commands with &lt;code&gt;sudo&lt;/code&gt;, but this can be avoided by adding your user to the local &lt;code&gt;docker&lt;/code&gt; group.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Project bootstrap&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;The easiest way to start working on the project is to use &lt;a href="https://www.gnu.org/software/make/"&gt;GNU Make&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shellscript"&gt;$ make bootstrap FLUSH_ARGS='--no-input'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command builds the &lt;code&gt;app-dev&lt;/code&gt; and &lt;code&gt;frontend-dev&lt;/code&gt; containers, installs dependencies, performs database migrations and compiles translations. It's a good idea to use this command each time you are pulling code from the project repository to avoid dependency-related or migration-related issues.&lt;/p&gt; 
&lt;p&gt;Your Docker services should now be up and running ğŸ‰&lt;/p&gt; 
&lt;p&gt;You can access the project by going to &lt;a href="http://localhost:3000"&gt;http://localhost:3000&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You will be prompted to log in. The default credentials are:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;username: impress
password: impress
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ğŸ“ Note that if you need to run them afterwards, you can use the eponymous Make rule:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shellscript"&gt;$ make run
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;âš ï¸ For the frontend developer, it is often better to run the frontend in development mode locally.&lt;/p&gt; 
&lt;p&gt;To do so, install the frontend dependencies with the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shellscript"&gt;$ make frontend-development-install
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;And run the frontend locally in development mode with the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shellscript"&gt;$ make run-frontend-development
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To start all the services, except the frontend container, you can use the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shellscript"&gt;$ make run-backend
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To execute frontend tests &amp;amp; linting only&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shellscript"&gt;$ make frontend-test
$ make frontend-lint
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Adding content&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;You can create a basic demo site by running this command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shellscript"&gt;$ make demo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Finally, you can check all available Make rules using this command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shellscript"&gt;$ make help
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Django admin&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;You can access the Django admin site at:&lt;/p&gt; 
&lt;p&gt;&lt;a href="http://localhost:8071/admin"&gt;http://localhost:8071/admin&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You first need to create a superuser account:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shellscript"&gt;$ make superuser
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Feedback ğŸ™‹â€â™‚ï¸ğŸ™‹â€â™€ï¸&lt;/h2&gt; 
&lt;p&gt;We'd love to hear your thoughts, and hear about your experiments, so come and say hi on &lt;a href="https://matrix.to/#/#docs-official:matrix.org"&gt;Matrix&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Roadmap ğŸ’¡&lt;/h2&gt; 
&lt;p&gt;Want to know where the project is headed? &lt;a href="https://github.com/orgs/numerique-gouv/projects/13/views/11"&gt;ğŸ—ºï¸ Checkout our roadmap&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;License ğŸ“&lt;/h2&gt; 
&lt;p&gt;This work is released under the MIT License (see &lt;a href="https://github.com/suitenumerique/docs/raw/main/LICENSE"&gt;LICENSE&lt;/a&gt;).&lt;/p&gt; 
&lt;p&gt;While Docs is a public-driven initiative, our license choice is an invitation for private sector actors to use, sell and contribute to the project.&lt;/p&gt; 
&lt;h2&gt;Contributing ğŸ™Œ&lt;/h2&gt; 
&lt;p&gt;This project is intended to be community-driven, so please, do not hesitate to &lt;a href="https://matrix.to/#/#docs-official:matrix.org"&gt;get in touch&lt;/a&gt; if you have any question related to our implementation or design decisions.&lt;/p&gt; 
&lt;p&gt;You can help us with translations on &lt;a href="https://crowdin.com/project/lasuite-docs"&gt;Crowdin&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you intend to make pull requests, see &lt;a href="https://github.com/suitenumerique/docs/raw/main/CONTRIBUTING.md"&gt;CONTRIBUTING&lt;/a&gt; for guidelines.&lt;/p&gt; 
&lt;h2&gt;Directory structure:&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-markdown"&gt;docs
â”œâ”€â”€ bin - executable scripts or binaries that are used for various tasks, such as setup scripts, utility scripts, or custom commands.
â”œâ”€â”€ crowdin - for crowdin translations, a tool or service that helps manage translations for the project.
â”œâ”€â”€ docker - Dockerfiles and related configuration files used to build Docker images for the project. These images can be used for development, testing, or production environments.
â”œâ”€â”€ docs - documentation for the project, including user guides, API documentation, and other helpful resources.
â”œâ”€â”€ env.d/development - environment-specific configuration files for the development environment. These files might include environment variables, configuration settings, or other setup files needed for development.
â”œâ”€â”€ gitlint - configuration files for `gitlint`, a tool that enforces commit message guidelines to ensure consistency and quality in commit messages.
â”œâ”€â”€ playground - experimental or temporary code, where developers can test new features or ideas without affecting the main codebase.
â””â”€â”€ src - main source code directory, containing the core application code, libraries, and modules of the project.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Credits â¤ï¸&lt;/h2&gt; 
&lt;h3&gt;Stack&lt;/h3&gt; 
&lt;p&gt;Docs is built on top of &lt;a href="https://www.django-rest-framework.org/"&gt;Django Rest Framework&lt;/a&gt;, &lt;a href="https://nextjs.org/"&gt;Next.js&lt;/a&gt;, &lt;a href="https://www.blocknotejs.org/"&gt;BlockNote.js&lt;/a&gt;, &lt;a href="https://tiptap.dev/docs/hocuspocus/introduction"&gt;HocusPocus&lt;/a&gt; and &lt;a href="https://yjs.dev/"&gt;Yjs&lt;/a&gt;. We thank the contributors of all these projects for their awesome work!&lt;/p&gt; 
&lt;p&gt;We are proud sponsors of &lt;a href="https://www.blocknotejs.org/"&gt;BlockNotejs&lt;/a&gt; and &lt;a href="https://yjs.dev/"&gt;Yjs&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Gov â¤ï¸ open source&lt;/h3&gt; 
&lt;p&gt;Docs is the result of a joint effort led by the French ğŸ‡«ğŸ‡·ğŸ¥– (&lt;a href="https://www.numerique.gouv.fr/dinum/"&gt;DINUM&lt;/a&gt;) and German ğŸ‡©ğŸ‡ªğŸ¥¨ governments (&lt;a href="https://zendis.de/"&gt;ZenDiS&lt;/a&gt;).&lt;/p&gt; 
&lt;p&gt;We are always looking for new public partners (we are currently onboarding the Netherlands ğŸ‡³ğŸ‡±ğŸ§€), feel free to &lt;a href="mailto:docs@numerique.gouv.fr"&gt;reach out&lt;/a&gt; if you are interested in using or contributing to Docs.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/suitenumerique/docs/main/docs/assets/europe_opensource.png" width="50%" /&gt; &lt;/p&gt;</description>
    </item>
    
    <item>
      <title>livekit/agents</title>
      <link>https://github.com/livekit/agents</link>
      <description>&lt;p&gt;A powerful framework for building realtime voice AI agents ğŸ¤–ğŸ™ï¸ğŸ“¹&lt;/p&gt;&lt;hr&gt;&lt;picture&gt; 
 &lt;source media="(prefers-color-scheme: dark)" srcset="/.github/banner_dark.png" /&gt; 
 &lt;source media="(prefers-color-scheme: light)" srcset="/.github/banner_light.png" /&gt; 
 &lt;img style="width:100%;" alt="The LiveKit icon, the name of the repository and some sample code in the background." src="https://raw.githubusercontent.com/livekit/agents/main/.github/banner_light.png" /&gt; 
&lt;/picture&gt; 
&lt;!--END_BANNER_IMAGE--&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://img.shields.io/pypi/v/livekit-agents" alt="PyPI - Version" /&gt; &lt;a href="https://pepy.tech/projects/livekit-agents"&gt;&lt;img src="https://static.pepy.tech/badge/livekit-agents/month" alt="PyPI Downloads" /&gt;&lt;/a&gt; &lt;a href="https://livekit.io/join-slack"&gt;&lt;img src="https://img.shields.io/endpoint?url=https%3A%2F%2Flivekit.io%2Fbadges%2Fslack" alt="Slack community" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/livekit"&gt;&lt;img src="https://img.shields.io/twitter/follow/livekit" alt="Twitter Follow" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/livekit/agents"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki for understanding the codebase" /&gt;&lt;/a&gt; &lt;a href="https://github.com/livekit/livekit/raw/master/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/livekit/livekit" alt="License" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;Looking for the JS/TS library? Check out &lt;a href="https://github.com/livekit/agents-js"&gt;AgentsJS&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;What is Agents?&lt;/h2&gt; 
&lt;!--BEGIN_DESCRIPTION--&gt; 
&lt;p&gt;The Agent Framework is designed for building realtime, programmable participants that run on servers. Use it to create conversational, multi-modal voice agents that can see, hear, and understand.&lt;/p&gt; 
&lt;!--END_DESCRIPTION--&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible integrations&lt;/strong&gt;: A comprehensive ecosystem to mix and match the right STT, LLM, TTS, and Realtime API to suit your use case.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Integrated job scheduling&lt;/strong&gt;: Built-in task scheduling and distribution with &lt;a href="https://docs.livekit.io/agents/build/dispatch/"&gt;dispatch APIs&lt;/a&gt; to connect end users to agents.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Extensive WebRTC clients&lt;/strong&gt;: Build client applications using LiveKit's open-source SDK ecosystem, supporting all major platforms.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Telephony integration&lt;/strong&gt;: Works seamlessly with LiveKit's &lt;a href="https://docs.livekit.io/sip/"&gt;telephony stack&lt;/a&gt;, allowing your agent to make calls to or receive calls from phones.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Exchange data with clients&lt;/strong&gt;: Use &lt;a href="https://docs.livekit.io/home/client/data/rpc/"&gt;RPCs&lt;/a&gt; and other &lt;a href="https://docs.livekit.io/home/client/data/"&gt;Data APIs&lt;/a&gt; to seamlessly exchange data with clients.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Semantic turn detection&lt;/strong&gt;: Uses a transformer model to detect when a user is done with their turn, helps to reduce interruptions.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MCP support&lt;/strong&gt;: Native support for MCP. Integrate tools provided by MCP servers with one loc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Builtin test framework&lt;/strong&gt;: Write tests and use judges to ensure your agent is performing as expected.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Open-source&lt;/strong&gt;: Fully open-source, allowing you to run the entire stack on your own servers, including &lt;a href="https://github.com/livekit/livekit"&gt;LiveKit server&lt;/a&gt;, one of the most widely used WebRTC media servers.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;To install the core Agents library, along with plugins for popular model providers:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install "livekit-agents[openai,silero,deepgram,cartesia,turn-detector]~=1.0"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Docs and guides&lt;/h2&gt; 
&lt;p&gt;Documentation on the framework and how to use it can be found &lt;a href="https://docs.livekit.io/agents/"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Core concepts&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Agent: An LLM-based application with defined instructions.&lt;/li&gt; 
 &lt;li&gt;AgentSession: A container for agents that manages interactions with end users.&lt;/li&gt; 
 &lt;li&gt;entrypoint: The starting point for an interactive session, similar to a request handler in a web server.&lt;/li&gt; 
 &lt;li&gt;Worker: The main process that coordinates job scheduling and launches agents for user sessions.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Simple voice agent&lt;/h3&gt; 
&lt;hr /&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from livekit.agents import (
    Agent,
    AgentSession,
    JobContext,
    RunContext,
    WorkerOptions,
    cli,
    function_tool,
)
from livekit.plugins import deepgram, elevenlabs, openai, silero

@function_tool
async def lookup_weather(
    context: RunContext,
    location: str,
):
    """Used to look up weather information."""

    return {"weather": "sunny", "temperature": 70}


async def entrypoint(ctx: JobContext):
    await ctx.connect()

    agent = Agent(
        instructions="You are a friendly voice assistant built by LiveKit.",
        tools=[lookup_weather],
    )
    session = AgentSession(
        vad=silero.VAD.load(),
        # any combination of STT, LLM, TTS, or realtime API can be used
        stt=deepgram.STT(model="nova-3"),
        llm=openai.LLM(model="gpt-4o-mini"),
        tts=elevenlabs.TTS(),
    )

    await session.start(agent=agent, room=ctx.room)
    await session.generate_reply(instructions="greet the user and ask about their day")


if __name__ == "__main__":
    cli.run_app(WorkerOptions(entrypoint_fnc=entrypoint))
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You'll need the following environment variables for this example:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;DEEPGRAM_API_KEY&lt;/li&gt; 
 &lt;li&gt;OPENAI_API_KEY&lt;/li&gt; 
 &lt;li&gt;ELEVEN_API_KEY&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Multi-agent handoff&lt;/h3&gt; 
&lt;hr /&gt; 
&lt;p&gt;This code snippet is abbreviated. For the full example, see &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/voice_agents/multi_agent.py"&gt;multi_agent.py&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;...
class IntroAgent(Agent):
    def __init__(self) -&amp;gt; None:
        super().__init__(
            instructions=f"You are a story teller. Your goal is to gather a few pieces of information from the user to make the story personalized and engaging."
            "Ask the user for their name and where they are from"
        )

    async def on_enter(self):
        self.session.generate_reply(instructions="greet the user and gather information")

    @function_tool
    async def information_gathered(
        self,
        context: RunContext,
        name: str,
        location: str,
    ):
        """Called when the user has provided the information needed to make the story personalized and engaging.

        Args:
            name: The name of the user
            location: The location of the user
        """

        context.userdata.name = name
        context.userdata.location = location

        story_agent = StoryAgent(name, location)
        return story_agent, "Let's start the story!"


class StoryAgent(Agent):
    def __init__(self, name: str, location: str) -&amp;gt; None:
        super().__init__(
            instructions=f"You are a storyteller. Use the user's information in order to make the story personalized."
            f"The user's name is {name}, from {location}"
            # override the default model, switching to Realtime API from standard LLMs
            llm=openai.realtime.RealtimeModel(voice="echo"),
            chat_ctx=chat_ctx,
        )

    async def on_enter(self):
        self.session.generate_reply()


async def entrypoint(ctx: JobContext):
    await ctx.connect()

    userdata = StoryData()
    session = AgentSession[StoryData](
        vad=silero.VAD.load(),
        stt=deepgram.STT(model="nova-3"),
        llm=openai.LLM(model="gpt-4o-mini"),
        tts=openai.TTS(voice="echo"),
        userdata=userdata,
    )

    await session.start(
        agent=IntroAgent(),
        room=ctx.room,
    )
...
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Testing&lt;/h3&gt; 
&lt;p&gt;Automated tests are essential for building reliable agents, especially with the non-deterministic behavior of LLMs. LiveKit Agents include native test integration to help you create dependable agents.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;@pytest.mark.asyncio
async def test_no_availability() -&amp;gt; None:
    llm = google.LLM()
    async AgentSession(llm=llm) as sess:
        await sess.start(MyAgent())
        result = await sess.run(
            user_input="Hello, I need to place an order."
        )
        result.expect.skip_next_event_if(type="message", role="assistant")
        result.expect.next_event().is_function_call(name="start_order")
        result.expect.next_event().is_function_call_output()
        await (
            result.expect.next_event()
            .is_message(role="assistant")
            .judge(llm, intent="assistant should be asking the user what they would like")
        )

&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;ğŸ™ï¸ Starter Agent&lt;/h3&gt; &lt;p&gt;A starter agent optimized for voice conversations.&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/voice_agents/basic_agent.py"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;ğŸ”„ Multi-user push to talk&lt;/h3&gt; &lt;p&gt;Responds to multiple users in the room via push-to-talk.&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/voice_agents/push_to_talk.py"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;ğŸµ Background audio&lt;/h3&gt; &lt;p&gt;Background ambient and thinking audio to improve realism.&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/voice_agents/background_audio.py"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;ğŸ› ï¸ Dynamic tool creation&lt;/h3&gt; &lt;p&gt;Creating function tools dynamically.&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/voice_agents/dynamic_tool_creation.py"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;â˜ï¸ Outbound caller&lt;/h3&gt; &lt;p&gt;Agent that makes outbound phone calls&lt;/p&gt; &lt;p&gt; &lt;a href="https://github.com/livekit-examples/outbound-caller-python"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;ğŸ“‹ Structured output&lt;/h3&gt; &lt;p&gt;Using structured output from LLM to guide TTS tone.&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/voice_agents/structured_output.py"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;ğŸ”Œ MCP support&lt;/h3&gt; &lt;p&gt;Use tools from MCP servers&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/voice_agents/mcp"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;ğŸ’¬ Text-only agent&lt;/h3&gt; &lt;p&gt;Skip voice altogether and use the same code for text-only integrations&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/other/text_only.py"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;ğŸ“ Multi-user transcriber&lt;/h3&gt; &lt;p&gt;Produce transcriptions from all users in the room&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/other/transcription/multi-user-transcriber.py"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;ğŸ¥ Video avatars&lt;/h3&gt; &lt;p&gt;Add an AI avatar with Tavus, Beyond Presence, and Bithuman&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/avatar_agents/"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;ğŸ½ï¸ Restaurant ordering and reservations&lt;/h3&gt; &lt;p&gt;Full example of an agent that handles calls for a restaurant.&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/voice_agents/restaurant_agent.py"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;ğŸ‘ï¸ Gemini Live vision&lt;/h3&gt; &lt;p&gt;Full example (including iOS app) of Gemini Live agent that can see.&lt;/p&gt; &lt;p&gt; &lt;a href="https://github.com/livekit-examples/vision-demo"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Running your agent&lt;/h2&gt; 
&lt;h3&gt;Testing in terminal&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;python myagent.py console
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Runs your agent in terminal mode, enabling local audio input and output for testing. This mode doesn't require external servers or dependencies and is useful for quickly validating behavior.&lt;/p&gt; 
&lt;h3&gt;Developing with LiveKit clients&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;python myagent.py dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Starts the agent server and enables hot reloading when files change. This mode allows each process to host multiple concurrent agents efficiently.&lt;/p&gt; 
&lt;p&gt;The agent connects to LiveKit Cloud or your self-hosted server. Set the following environment variables:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;LIVEKIT_URL&lt;/li&gt; 
 &lt;li&gt;LIVEKIT_API_KEY&lt;/li&gt; 
 &lt;li&gt;LIVEKIT_API_SECRET&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can connect using any LiveKit client SDK or telephony integration. To get started quickly, try the &lt;a href="https://agents-playground.livekit.io/"&gt;Agents Playground&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Running for production&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;python myagent.py start
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Runs the agent with production-ready optimizations.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;The Agents framework is under active development in a rapidly evolving field. We welcome and appreciate contributions of any kind, be it feedback, bugfixes, features, new plugins and tools, or better documentation. You can file issues under this repo, open a PR, or chat with us in LiveKit's &lt;a href="https://livekit.io/join-slack"&gt;Slack community&lt;/a&gt;.&lt;/p&gt; 
&lt;!--BEGIN_REPO_NAV--&gt; 
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;table&gt; 
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th colspan="2"&gt;LiveKit Ecosystem&lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt;
   &lt;td&gt;LiveKit SDKs&lt;/td&gt;
   &lt;td&gt;&lt;a href="https://github.com/livekit/client-sdk-js"&gt;Browser&lt;/a&gt; Â· &lt;a href="https://github.com/livekit/client-sdk-swift"&gt;iOS/macOS/visionOS&lt;/a&gt; Â· &lt;a href="https://github.com/livekit/client-sdk-android"&gt;Android&lt;/a&gt; Â· &lt;a href="https://github.com/livekit/client-sdk-flutter"&gt;Flutter&lt;/a&gt; Â· &lt;a href="https://github.com/livekit/client-sdk-react-native"&gt;React Native&lt;/a&gt; Â· &lt;a href="https://github.com/livekit/rust-sdks"&gt;Rust&lt;/a&gt; Â· &lt;a href="https://github.com/livekit/node-sdks"&gt;Node.js&lt;/a&gt; Â· &lt;a href="https://github.com/livekit/python-sdks"&gt;Python&lt;/a&gt; Â· &lt;a href="https://github.com/livekit/client-sdk-unity"&gt;Unity&lt;/a&gt; Â· &lt;a href="https://github.com/livekit/client-sdk-unity-web"&gt;Unity (WebGL)&lt;/a&gt; Â· &lt;a href="https://github.com/livekit/client-sdk-esp32"&gt;ESP32&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;&lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;Server APIs&lt;/td&gt;
   &lt;td&gt;&lt;a href="https://github.com/livekit/node-sdks"&gt;Node.js&lt;/a&gt; Â· &lt;a href="https://github.com/livekit/server-sdk-go"&gt;Golang&lt;/a&gt; Â· &lt;a href="https://github.com/livekit/server-sdk-ruby"&gt;Ruby&lt;/a&gt; Â· &lt;a href="https://github.com/livekit/server-sdk-kotlin"&gt;Java/Kotlin&lt;/a&gt; Â· &lt;a href="https://github.com/livekit/python-sdks"&gt;Python&lt;/a&gt; Â· &lt;a href="https://github.com/livekit/rust-sdks"&gt;Rust&lt;/a&gt; Â· &lt;a href="https://github.com/agence104/livekit-server-sdk-php"&gt;PHP (community)&lt;/a&gt; Â· &lt;a href="https://github.com/pabloFuente/livekit-server-sdk-dotnet"&gt;.NET (community)&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;&lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;UI Components&lt;/td&gt;
   &lt;td&gt;&lt;a href="https://github.com/livekit/components-js"&gt;React&lt;/a&gt; Â· &lt;a href="https://github.com/livekit/components-android"&gt;Android Compose&lt;/a&gt; Â· &lt;a href="https://github.com/livekit/components-swift"&gt;SwiftUI&lt;/a&gt; Â· &lt;a href="https://github.com/livekit/components-flutter"&gt;Flutter&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;&lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;Agents Frameworks&lt;/td&gt;
   &lt;td&gt;&lt;b&gt;Python&lt;/b&gt; Â· &lt;a href="https://github.com/livekit/agents-js"&gt;Node.js&lt;/a&gt; Â· &lt;a href="https://github.com/livekit/agent-playground"&gt;Playground&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;&lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;Services&lt;/td&gt;
   &lt;td&gt;&lt;a href="https://github.com/livekit/livekit"&gt;LiveKit server&lt;/a&gt; Â· &lt;a href="https://github.com/livekit/egress"&gt;Egress&lt;/a&gt; Â· &lt;a href="https://github.com/livekit/ingress"&gt;Ingress&lt;/a&gt; Â· &lt;a href="https://github.com/livekit/sip"&gt;SIP&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;&lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;Resources&lt;/td&gt;
   &lt;td&gt;&lt;a href="https://docs.livekit.io"&gt;Docs&lt;/a&gt; Â· &lt;a href="https://github.com/livekit-examples"&gt;Example apps&lt;/a&gt; Â· &lt;a href="https://livekit.io/cloud"&gt;Cloud&lt;/a&gt; Â· &lt;a href="https://docs.livekit.io/home/self-hosting/deployment"&gt;Self-hosting&lt;/a&gt; Â· &lt;a href="https://github.com/livekit/livekit-cli"&gt;CLI&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;!--END_REPO_NAV--&gt;</description>
    </item>
    
    <item>
      <title>hacksider/Deep-Live-Cam</title>
      <link>https://github.com/hacksider/Deep-Live-Cam</link>
      <description>&lt;p&gt;real time face swap and one-click video deepfake with only a single image&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt;Deep-Live-Cam&lt;/h1&gt; 
&lt;p align="center"&gt; Real-time face swap and video deepfake with a single click and only a single image. &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/11395" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/11395" alt="hacksider%2FDeep-Live-Cam | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/media/demo.gif" alt="Demo GIF" width="800" /&gt; &lt;/p&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;This deepfake software is designed to be a productive tool for the AI-generated media industry. It can assist artists in animating custom characters, creating engaging content, and even using models for clothing design.&lt;/p&gt; 
&lt;p&gt;We are aware of the potential for unethical applications and are committed to preventative measures. A built-in check prevents the program from processing inappropriate media (nudity, graphic content, sensitive material like war footage, etc.). We will continue to develop this project responsibly, adhering to the law and ethics. We may shut down the project or add watermarks if legally required.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Ethical Use: Users are expected to use this software responsibly and legally. If using a real person's face, obtain their consent and clearly label any output as a deepfake when sharing online.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Content Restrictions: The software includes built-in checks to prevent processing inappropriate media, such as nudity, graphic content, or sensitive material.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Legal Compliance: We adhere to all relevant laws and ethical guidelines. If legally required, we may shut down the project or add watermarks to the output.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;User Responsibility: We are not responsible for end-user actions. Users must ensure their use of the software aligns with ethical standards and legal requirements.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;By using this software, you agree to these terms and commit to using it in a manner that respects the rights and dignity of others.&lt;/p&gt; 
&lt;p&gt;Users are expected to use this software responsibly and legally. If using a real person's face, obtain their consent and clearly label any output as a deepfake when sharing online. We are not responsible for end-user actions.&lt;/p&gt; 
&lt;h2&gt;Exclusive v2.3 Quick Start - Pre-built (Windows/Mac Silicon)&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://deeplivecam.net/index.php/quickstart"&gt; &lt;img src="https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/media/Download.png" width="285" height="77" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;a href="https://deeplivecam.net/index.php/quickstart"&gt; &lt;h5&gt;This is the fastest build you can get if you have a discrete NVIDIA or AMD GPU or Mac Silicon, And you'll receive special priority support.&lt;/h5&gt; &lt;h6&gt;These Pre-builts are perfect for non-technical users or those who don't have time to, or can't manually install all the requirements. Just a heads-up: this is an open-source project, so you can also install it manually.&lt;/h6&gt; &lt;h2&gt;TLDR; Live Deepfake in just 3 Clicks&lt;/h2&gt; &lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/af825228-852c-411b-b787-ffd9aac72fc6" alt="easysteps" /&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Select a face&lt;/li&gt; 
  &lt;li&gt;Select which camera to use&lt;/li&gt; 
  &lt;li&gt;Press live!&lt;/li&gt; 
 &lt;/ol&gt; &lt;h2&gt;Features &amp;amp; Uses - Everything is in real-time&lt;/h2&gt; &lt;h3&gt;Mouth Mask&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Retain your original mouth for accurate movement using Mouth Mask&lt;/strong&gt;&lt;/p&gt; &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/media/ludwig.gif" alt="resizable-gif" /&gt; &lt;/p&gt; &lt;h3&gt;Face Mapping&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Use different faces on multiple subjects simultaneously&lt;/strong&gt;&lt;/p&gt; &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/media/streamers.gif" alt="face_mapping_source" /&gt; &lt;/p&gt; &lt;h3&gt;Your Movie, Your Face&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Watch movies with any face in real-time&lt;/strong&gt;&lt;/p&gt; &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/media/movie.gif" alt="movie" /&gt; &lt;/p&gt; &lt;h3&gt;Live Show&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Run Live shows and performances&lt;/strong&gt;&lt;/p&gt; &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/media/live_show.gif" alt="show" /&gt; &lt;/p&gt; &lt;h3&gt;Memes&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Create Your Most Viral Meme Yet&lt;/strong&gt;&lt;/p&gt; &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/media/meme.gif" alt="show" width="450" /&gt; &lt;br /&gt; &lt;sub&gt;Created using Many Faces feature in Deep-Live-Cam&lt;/sub&gt; &lt;/p&gt; &lt;h3&gt;Omegle&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Surprise people on Omegle&lt;/strong&gt;&lt;/p&gt; &lt;p align="center"&gt; 
  &lt;video src="https://github.com/user-attachments/assets/2e9b9b82-fa04-4b70-9f56-b1f68e7672d0" width="450" controls&gt;&lt;/video&gt; &lt;/p&gt; &lt;h2&gt;Installation (Manual)&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;Please be aware that the installation requires technical skills and is not for beginners. Consider downloading the quickstart version.&lt;/strong&gt;&lt;/p&gt; &lt;/a&gt;
&lt;details&gt;
 &lt;a href="https://deeplivecam.net/index.php/quickstart"&gt; &lt;summary&gt;Click to see the process&lt;/summary&gt; &lt;h3&gt;Installation&lt;/h3&gt; &lt;p&gt;This is more likely to work on your computer but will be slower as it utilizes the CPU.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Set up Your Platform&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Python (3.11 recommended)&lt;/li&gt; 
   &lt;li&gt;pip&lt;/li&gt; 
   &lt;li&gt;git&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=OlNWCpFdVMA"&gt;ffmpeg&lt;/a&gt; - &lt;code&gt;iex (irm ffmpeg.tc.ht)&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://visualstudio.microsoft.com/visual-cpp-build-tools/"&gt;Visual Studio 2022 Runtimes (Windows)&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt;&lt;/a&gt; 
 &lt;p&gt;&lt;strong&gt;2. Clone the Repository&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/hacksider/Deep-Live-Cam.git
cd Deep-Live-Cam
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;3. Download the Models&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/hacksider/deep-live-cam/resolve/main/GFPGANv1.4.pth"&gt;GFPGANv1.4&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/hacksider/deep-live-cam/resolve/main/inswapper_128_fp16.onnx"&gt;inswapper_128_fp16.onnx&lt;/a&gt;&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;Place these files in the "&lt;strong&gt;models&lt;/strong&gt;" folder.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;4. Install Dependencies&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;We highly recommend using a &lt;code&gt;venv&lt;/code&gt; to avoid issues.&lt;/p&gt; 
 &lt;p&gt;For Windows:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python -m venv venv
venv\Scripts\activate
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;For Linux:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Ensure you use the installed Python 3.10
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;For macOS:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Apple Silicon (M1/M2/M3) requires specific setup:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Install Python 3.11 (specific version is important)
brew install python@3.11

# Install tkinter package (required for the GUI)
brew install python-tk@3.10

# Create and activate virtual environment with Python 3.11
python3.11 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;** In case something goes wrong and you need to reinstall the virtual environment **&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Deactivate the virtual environment
rm -rf venv

# Reinstall the virtual environment
python -m venv venv
source venv/bin/activate

# install the dependencies again
pip install -r requirements.txt

# gfpgan and basicsrs issue fix
pip install git+https://github.com/xinntao/BasicSR.git@master
pip uninstall gfpgan -y
pip install git+https://github.com/TencentARC/GFPGAN.git@master
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Run:&lt;/strong&gt; If you don't have a GPU, you can run Deep-Live-Cam using &lt;code&gt;python run.py&lt;/code&gt;. Note that initial execution will download models (~300MB).&lt;/p&gt; 
 &lt;h3&gt;GPU Acceleration&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;CUDA Execution Provider (Nvidia)&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Install &lt;a href="https://developer.nvidia.com/cuda-12-8-0-download-archive"&gt;CUDA Toolkit 12.8.0&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Install &lt;a href="https://developer.nvidia.com/rdp/cudnn-archive"&gt;cuDNN v8.9.7 for CUDA 12.x&lt;/a&gt; (required for onnxruntime-gpu): 
   &lt;ul&gt; 
    &lt;li&gt;Download cuDNN v8.9.7 for CUDA 12.x&lt;/li&gt; 
    &lt;li&gt;Make sure the cuDNN bin directory is in your system PATH&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;Install dependencies:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128
pip uninstall onnxruntime onnxruntime-gpu
pip install onnxruntime-gpu==1.21.0
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="3"&gt; 
  &lt;li&gt;Usage:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python run.py --execution-provider cuda
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;CoreML Execution Provider (Apple Silicon)&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Apple Silicon (M1/M2/M3) specific installation:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Make sure you've completed the macOS setup above using Python 3.10.&lt;/li&gt; 
  &lt;li&gt;Install dependencies:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip uninstall onnxruntime onnxruntime-silicon
pip install onnxruntime-silicon==1.13.1
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="3"&gt; 
  &lt;li&gt;Usage (important: specify Python 3.10):&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python3.10 run.py --execution-provider coreml
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Important Notes for macOS:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;You &lt;strong&gt;must&lt;/strong&gt; use Python 3.10, not newer versions like 3.11 or 3.13&lt;/li&gt; 
  &lt;li&gt;Always run with &lt;code&gt;python3.10&lt;/code&gt; command not just &lt;code&gt;python&lt;/code&gt; if you have multiple Python versions installed&lt;/li&gt; 
  &lt;li&gt;If you get error about &lt;code&gt;_tkinter&lt;/code&gt; missing, reinstall the tkinter package: &lt;code&gt;brew reinstall python-tk@3.10&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;If you get model loading errors, check that your models are in the correct folder&lt;/li&gt; 
  &lt;li&gt;If you encounter conflicts with other Python versions, consider uninstalling them: &lt;pre&gt;&lt;code class="language-bash"&gt;# List all installed Python versions
brew list | grep python

# Uninstall conflicting versions if needed
brew uninstall --ignore-dependencies python@3.11 python@3.13

# Keep only Python 3.11
brew cleanup
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;CoreML Execution Provider (Apple Legacy)&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Install dependencies:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip uninstall onnxruntime onnxruntime-coreml
pip install onnxruntime-coreml==1.21.0
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="2"&gt; 
  &lt;li&gt;Usage:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python run.py --execution-provider coreml
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;DirectML Execution Provider (Windows)&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Install dependencies:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip uninstall onnxruntime onnxruntime-directml
pip install onnxruntime-directml==1.21.0
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="2"&gt; 
  &lt;li&gt;Usage:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python run.py --execution-provider directml
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;OpenVINOâ„¢ Execution Provider (Intel)&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Install dependencies:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip uninstall onnxruntime onnxruntime-openvino
pip install onnxruntime-openvino==1.21.0
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="2"&gt; 
  &lt;li&gt;Usage:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python run.py --execution-provider openvino
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;1. Image/Video Mode&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Execute &lt;code&gt;python run.py&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Choose a source face image and a target image/video.&lt;/li&gt; 
 &lt;li&gt;Click "Start".&lt;/li&gt; 
 &lt;li&gt;The output will be saved in a directory named after the target video.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;2. Webcam Mode&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Execute &lt;code&gt;python run.py&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Select a source face image.&lt;/li&gt; 
 &lt;li&gt;Click "Live".&lt;/li&gt; 
 &lt;li&gt;Wait for the preview to appear (10-30 seconds).&lt;/li&gt; 
 &lt;li&gt;Use a screen capture tool like OBS to stream.&lt;/li&gt; 
 &lt;li&gt;To change the face, select a new source image.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Command Line Arguments (Unmaintained)&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;options:
  -h, --help                                               show this help message and exit
  -s SOURCE_PATH, --source SOURCE_PATH                     select a source image
  -t TARGET_PATH, --target TARGET_PATH                     select a target image or video
  -o OUTPUT_PATH, --output OUTPUT_PATH                     select output file or directory
  --frame-processor FRAME_PROCESSOR [FRAME_PROCESSOR ...]  frame processors (choices: face_swapper, face_enhancer, ...)
  --keep-fps                                               keep original fps
  --keep-audio                                             keep original audio
  --keep-frames                                            keep temporary frames
  --many-faces                                             process every face
  --map-faces                                              map source target faces
  --mouth-mask                                             mask the mouth region
  --video-encoder {libx264,libx265,libvpx-vp9}             adjust output video encoder
  --video-quality [0-51]                                   adjust output video quality
  --live-mirror                                            the live camera display as you see it in the front-facing camera frame
  --live-resizable                                         the live camera frame is resizable
  --max-memory MAX_MEMORY                                  maximum amount of RAM in GB
  --execution-provider {cpu} [{cpu} ...]                   available execution provider (choices: cpu, ...)
  --execution-threads EXECUTION_THREADS                    number of execution threads
  -v, --version                                            show program's version number and exit
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Looking for a CLI mode? Using the -s/--source argument will make the run program in cli mode.&lt;/p&gt; 
&lt;h2&gt;Press&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;We are always open to criticism and are ready to improve, that's why we didn't cherry-pick anything.&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://arstechnica.com/information-technology/2024/08/new-ai-tool-enables-real-time-face-swapping-on-webcams-raising-fraud-concerns/"&gt;&lt;em&gt;"Deep-Live-Cam goes viral, allowing anyone to become a digital doppelganger"&lt;/em&gt;&lt;/a&gt; - Ars Technica&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://dataconomy.com/2024/08/15/what-is-deep-live-cam-github-deepfake/"&gt;&lt;em&gt;"Thanks Deep Live Cam, shapeshifters are among us now"&lt;/em&gt;&lt;/a&gt; - Dataconomy&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.newsbytesapp.com/news/science/deep-live-cam-ai-impersonation-tool-goes-viral/story"&gt;&lt;em&gt;"This free AI tool lets you become anyone during video-calls"&lt;/em&gt;&lt;/a&gt; - NewsBytes&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.creativebloq.com/ai/ok-this-viral-ai-live-stream-software-is-truly-terrifying"&gt;&lt;em&gt;"OK, this viral AI live stream software is truly terrifying"&lt;/em&gt;&lt;/a&gt; - Creative Bloq&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://petapixel.com/2024/08/14/deep-live-cam-deepfake-ai-tool-lets-you-become-anyone-in-a-video-call-with-single-photo-mark-zuckerberg-jd-vance-elon-musk/"&gt;&lt;em&gt;"Deepfake AI Tool Lets You Become Anyone in a Video Call With Single Photo"&lt;/em&gt;&lt;/a&gt; - PetaPixel&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.techeblog.com/deep-live-cam-ai-transform-face/"&gt;&lt;em&gt;"Deep-Live-Cam Uses AI to Transform Your Face in Real-Time, Celebrities Included"&lt;/em&gt;&lt;/a&gt; - TechEBlog&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://telegrafi.com/en/a-tool-that-makes-you-look-like-anyone-during-a-video-call-is-going-viral-on-the-Internet/"&gt;&lt;em&gt;"An AI tool that "makes you look like anyone" during a video call is going viral online"&lt;/em&gt;&lt;/a&gt; - Telegrafi&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://decrypt.co/244565/this-deepfake-tool-turning-images-into-livestreams-is-topping-the-github-charts"&gt;&lt;em&gt;"This Deepfake Tool Turning Images Into Livestreams is Topping the GitHub Charts"&lt;/em&gt;&lt;/a&gt; - Emerge&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.digitalmusicnews.com/2024/08/15/face-swapping-ai-real-time-mimic/"&gt;&lt;em&gt;"New Real-Time Face-Swapping AI Allows Anyone to Mimic Famous Faces"&lt;/em&gt;&lt;/a&gt; - Digital Music News&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.diyphotography.net/this-real-time-webcam-deepfake-tool-raises-alarms-about-the-future-of-identity-theft/"&gt;&lt;em&gt;"This real-time webcam deepfake tool raises alarms about the future of identity theft"&lt;/em&gt;&lt;/a&gt; - DIYPhotography&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/watch?time_continue=1074&amp;amp;v=py4Tc-Y8BcY"&gt;&lt;em&gt;"That's Crazy, Oh God. That's Fucking Freaky Dude... That's So Wild Dude"&lt;/em&gt;&lt;/a&gt; - SomeOrdinaryGamers&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/live/mFsCe7AIxq8?feature=shared&amp;amp;t=2686"&gt;&lt;em&gt;"Alright look look look, now look chat, we can do any face we want to look like chat"&lt;/em&gt;&lt;/a&gt; - IShowSpeed&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=wnCghLjqv3s&amp;amp;t=551s"&gt;&lt;em&gt;"They do a pretty good job matching poses, expression and even the lighting"&lt;/em&gt;&lt;/a&gt; - TechLinked (LTT)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.golem.de/news/deepfakes-als-sean-connery-an-der-redaktionskonferenz-teilnahm-2408-188172.html"&gt;&lt;em&gt;"Als Sean Connery an der Redaktionskonferenz teilnahm"&lt;/em&gt;&lt;/a&gt; - Golem.de (German)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Credits&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://ffmpeg.org/"&gt;ffmpeg&lt;/a&gt;: for making video-related operations easy&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/deepinsight"&gt;deepinsight&lt;/a&gt;: for their &lt;a href="https://github.com/deepinsight/insightface"&gt;insightface&lt;/a&gt; project which provided a well-made library and models. Please be reminded that the &lt;a href="https://github.com/deepinsight/insightface?tab=readme-ov-file#license"&gt;use of the model is for non-commercial research purposes only&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/havok2-htwo"&gt;havok2-htwo&lt;/a&gt;: for sharing the code for webcam&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/GosuDRM"&gt;GosuDRM&lt;/a&gt;: for the open version of roop&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pereiraroland26"&gt;pereiraroland26&lt;/a&gt;: Multiple faces support&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vic4key"&gt;vic4key&lt;/a&gt;: For supporting/contributing to this project&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/kier007"&gt;kier007&lt;/a&gt;: for improving the user experience&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/qitianai"&gt;qitianai&lt;/a&gt;: for multi-lingual support&lt;/li&gt; 
 &lt;li&gt;and &lt;a href="https://github.com/hacksider/Deep-Live-Cam/graphs/contributors"&gt;all developers&lt;/a&gt; behind libraries used in this project.&lt;/li&gt; 
 &lt;li&gt;Footnote: Please be informed that the base author of the code is &lt;a href="https://github.com/s0md3v/roop"&gt;s0md3v&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;All the wonderful users who helped make this project go viral by starring the repo â¤ï¸&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://github.com/hacksider/Deep-Live-Cam/stargazers"&gt;&lt;img src="https://reporoster.com/stars/hacksider/Deep-Live-Cam" alt="Stargazers" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Contributions&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://repobeats.axiom.co/api/embed/fec8e29c45dfdb9c5916f3a7830e1249308d20e1.svg?sanitize=true" alt="Alt" title="Repobeats analytics image" /&gt;&lt;/p&gt; 
&lt;h2&gt;Stars to the Moon ğŸš€&lt;/h2&gt; 
&lt;a href="https://star-history.com/#hacksider/deep-live-cam&amp;amp;Date"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;amp;type=Date" /&gt; 
  &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;amp;type=Date" /&gt; 
 &lt;/picture&gt; &lt;/a&gt;</description>
    </item>
    
    <item>
      <title>elliottech/lighter-python</title>
      <link>https://github.com/elliottech/lighter-python</link>
      <description>&lt;p&gt;Public Python SDK for Lighter&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Lighter Python&lt;/h1&gt; 
&lt;p&gt;Python SDK for Lighter&lt;/p&gt; 
&lt;h2&gt;Requirements.&lt;/h2&gt; 
&lt;p&gt;Python 3.8+&lt;/p&gt; 
&lt;h2&gt;Installation &amp;amp; Usage&lt;/h2&gt; 
&lt;h3&gt;pip install&lt;/h3&gt; 
&lt;p&gt;If the python package is hosted on a repository, you can install directly using:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip install git+https://github.com/elliottech/lighter-python.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then import the package:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import lighter
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Tests&lt;/h3&gt; 
&lt;p&gt;Execute &lt;code&gt;pytest&lt;/code&gt; to run the tests.&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;Please follow the &lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/#installation--usage"&gt;installation procedure&lt;/a&gt; and then run the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;
import lighter
import asyncio

async def main():
    client = lighter.ApiClient()
    try:
        account_api = lighter.AccountApi(client)
        account = await account_api.account(by="index", value="1")
        print(account)
    finally:
        await client.close()  # Make sure connection is cleanly closed

if __name__ == "__main__":
    asyncio.run(main())

&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Examples&lt;/h1&gt; 
&lt;h2&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/examples/get_info.py"&gt;Read API Functions&lt;/a&gt;&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python examples/get_info.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/examples/ws.py"&gt;Websocket Sync Order Books &amp;amp; Accounts&lt;/a&gt;&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python examples/ws.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/examples/create_cancel_order.py"&gt;Create &amp;amp; Cancel Orders&lt;/a&gt;&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python examples/create_cancel_order.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Documentation for API Endpoints&lt;/h2&gt; 
&lt;p&gt;All URIs are relative to &lt;em&gt;&lt;a href="https://mainnet.zklighter.elliot.ai"&gt;https://mainnet.zklighter.elliot.ai&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Class&lt;/th&gt; 
   &lt;th&gt;Method&lt;/th&gt; 
   &lt;th&gt;HTTP request&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;em&gt;AccountApi&lt;/em&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/AccountApi.md#account"&gt;&lt;strong&gt;account&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /api/v1/account&lt;/td&gt; 
   &lt;td&gt;account&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;em&gt;AccountApi&lt;/em&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/AccountApi.md#accounts_by_l1_address"&gt;&lt;strong&gt;accounts_by_l1_address&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /api/v1/accountsByL1Address&lt;/td&gt; 
   &lt;td&gt;accountsByL1Address&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;em&gt;AccountApi&lt;/em&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/AccountApi.md#apikeys"&gt;&lt;strong&gt;apikeys&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /api/v1/apikeys&lt;/td&gt; 
   &lt;td&gt;apikeys&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;em&gt;AccountApi&lt;/em&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/AccountApi.md#pnl"&gt;&lt;strong&gt;pnl&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /api/v1/pnl&lt;/td&gt; 
   &lt;td&gt;pnl&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;em&gt;AccountApi&lt;/em&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/AccountApi.md#public_pools"&gt;&lt;strong&gt;public_pools&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /api/v1/publicPools&lt;/td&gt; 
   &lt;td&gt;publicPools&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;em&gt;BlockApi&lt;/em&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/BlockApi.md#block"&gt;&lt;strong&gt;block&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /api/v1/block&lt;/td&gt; 
   &lt;td&gt;block&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;em&gt;BlockApi&lt;/em&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/BlockApi.md#blocks"&gt;&lt;strong&gt;blocks&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /api/v1/blocks&lt;/td&gt; 
   &lt;td&gt;blocks&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;em&gt;BlockApi&lt;/em&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/BlockApi.md#current_height"&gt;&lt;strong&gt;current_height&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /api/v1/currentHeight&lt;/td&gt; 
   &lt;td&gt;currentHeight&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;em&gt;CandlestickApi&lt;/em&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/CandlestickApi.md#candlesticks"&gt;&lt;strong&gt;candlesticks&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /api/v1/candlesticks&lt;/td&gt; 
   &lt;td&gt;candlesticks&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;em&gt;CandlestickApi&lt;/em&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/CandlestickApi.md#fundings"&gt;&lt;strong&gt;fundings&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /api/v1/fundings&lt;/td&gt; 
   &lt;td&gt;fundings&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;em&gt;OrderApi&lt;/em&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/OrderApi.md#account_inactive_orders"&gt;&lt;strong&gt;account_inactive_orders&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /api/v1/accountInactiveOrders&lt;/td&gt; 
   &lt;td&gt;accountInactiveOrders&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;em&gt;OrderApi&lt;/em&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/OrderApi.md#exchange_stats"&gt;&lt;strong&gt;exchange_stats&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /api/v1/exchangeStats&lt;/td&gt; 
   &lt;td&gt;exchangeStats&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;em&gt;OrderApi&lt;/em&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/OrderApi.md#order_book_details"&gt;&lt;strong&gt;order_book_details&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /api/v1/orderBookDetails&lt;/td&gt; 
   &lt;td&gt;orderBookDetails&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;em&gt;OrderApi&lt;/em&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/OrderApi.md#order_book_orders"&gt;&lt;strong&gt;order_book_orders&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /api/v1/orderBookOrders&lt;/td&gt; 
   &lt;td&gt;orderBookOrders&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;em&gt;OrderApi&lt;/em&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/OrderApi.md#order_books"&gt;&lt;strong&gt;order_books&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /api/v1/orderBooks&lt;/td&gt; 
   &lt;td&gt;orderBooks&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;em&gt;OrderApi&lt;/em&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/OrderApi.md#recent_trades"&gt;&lt;strong&gt;recent_trades&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /api/v1/recentTrades&lt;/td&gt; 
   &lt;td&gt;recentTrades&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;em&gt;OrderApi&lt;/em&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/OrderApi.md#trades"&gt;&lt;strong&gt;trades&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /api/v1/trades&lt;/td&gt; 
   &lt;td&gt;trades&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;em&gt;RootApi&lt;/em&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/RootApi.md#info"&gt;&lt;strong&gt;info&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /info&lt;/td&gt; 
   &lt;td&gt;info&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;em&gt;RootApi&lt;/em&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/RootApi.md#status"&gt;&lt;strong&gt;status&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /&lt;/td&gt; 
   &lt;td&gt;status&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;em&gt;TransactionApi&lt;/em&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/TransactionApi.md#account_txs"&gt;&lt;strong&gt;account_txs&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /api/v1/accountTxs&lt;/td&gt; 
   &lt;td&gt;accountTxs&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;em&gt;TransactionApi&lt;/em&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/TransactionApi.md#block_txs"&gt;&lt;strong&gt;block_txs&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /api/v1/blockTxs&lt;/td&gt; 
   &lt;td&gt;blockTxs&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;em&gt;TransactionApi&lt;/em&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/TransactionApi.md#deposit_history"&gt;&lt;strong&gt;deposit_history&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /api/v1/deposit/history&lt;/td&gt; 
   &lt;td&gt;deposit_history&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;em&gt;TransactionApi&lt;/em&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/TransactionApi.md#next_nonce"&gt;&lt;strong&gt;next_nonce&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /api/v1/nextNonce&lt;/td&gt; 
   &lt;td&gt;nextNonce&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;em&gt;TransactionApi&lt;/em&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/TransactionApi.md#send_tx"&gt;&lt;strong&gt;send_tx&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;POST&lt;/strong&gt; /api/v1/sendTx&lt;/td&gt; 
   &lt;td&gt;sendTx&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;em&gt;TransactionApi&lt;/em&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/TransactionApi.md#send_tx_batch"&gt;&lt;strong&gt;send_tx_batch&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;POST&lt;/strong&gt; /api/v1/sendTxBatch&lt;/td&gt; 
   &lt;td&gt;sendTxBatch&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;em&gt;TransactionApi&lt;/em&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/TransactionApi.md#tx"&gt;&lt;strong&gt;tx&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /api/v1/tx&lt;/td&gt; 
   &lt;td&gt;tx&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;em&gt;TransactionApi&lt;/em&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/TransactionApi.md#tx_from_l1_tx_hash"&gt;&lt;strong&gt;tx_from_l1_tx_hash&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /api/v1/txFromL1TxHash&lt;/td&gt; 
   &lt;td&gt;txFromL1TxHash&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;em&gt;TransactionApi&lt;/em&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/TransactionApi.md#txs"&gt;&lt;strong&gt;txs&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /api/v1/txs&lt;/td&gt; 
   &lt;td&gt;txs&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;em&gt;TransactionApi&lt;/em&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/TransactionApi.md#withdraw_history"&gt;&lt;strong&gt;withdraw_history&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /api/v1/withdraw/history&lt;/td&gt; 
   &lt;td&gt;withdraw_history&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Documentation For Models&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/Account.md"&gt;Account&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/AccountApiKeys.md"&gt;AccountApiKeys&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/AccountMarketStats.md"&gt;AccountMarketStats&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/AccountMetadata.md"&gt;AccountMetadata&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/AccountPnL.md"&gt;AccountPnL&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/AccountPosition.md"&gt;AccountPosition&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/AccountStats.md"&gt;AccountStats&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/ApiKey.md"&gt;ApiKey&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/Block.md"&gt;Block&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/Blocks.md"&gt;Blocks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/BridgeSupportedNetwork.md"&gt;BridgeSupportedNetwork&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/Candlestick.md"&gt;Candlestick&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/Candlesticks.md"&gt;Candlesticks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/ContractAddress.md"&gt;ContractAddress&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/CurrentHeight.md"&gt;CurrentHeight&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/Cursor.md"&gt;Cursor&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/DepositHistory.md"&gt;DepositHistory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/DepositHistoryItem.md"&gt;DepositHistoryItem&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/DetailedAccount.md"&gt;DetailedAccount&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/DetailedAccounts.md"&gt;DetailedAccounts&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/DetailedCandlestick.md"&gt;DetailedCandlestick&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/EnrichedTx.md"&gt;EnrichedTx&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/ExchangeStats.md"&gt;ExchangeStats&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/Funding.md"&gt;Funding&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/Fundings.md"&gt;Fundings&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/L1ProviderInfo.md"&gt;L1ProviderInfo&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/Liquidation.md"&gt;Liquidation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/MarketInfo.md"&gt;MarketInfo&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/NextNonce.md"&gt;NextNonce&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/Order.md"&gt;Order&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/OrderBook.md"&gt;OrderBook&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/OrderBookDepth.md"&gt;OrderBookDepth&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/OrderBookDetail.md"&gt;OrderBookDetail&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/OrderBookDetails.md"&gt;OrderBookDetails&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/OrderBookOrders.md"&gt;OrderBookOrders&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/OrderBookStats.md"&gt;OrderBookStats&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/OrderBooks.md"&gt;OrderBooks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/Orders.md"&gt;Orders&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/PnLEntry.md"&gt;PnLEntry&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/PositionFunding.md"&gt;PositionFunding&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/PriceLevel.md"&gt;PriceLevel&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/PublicPool.md"&gt;PublicPool&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/PublicPoolInfo.md"&gt;PublicPoolInfo&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/PublicPoolShare.md"&gt;PublicPoolShare&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/PublicPools.md"&gt;PublicPools&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/ReqGetAccount.md"&gt;ReqGetAccount&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/ReqGetAccountApiKeys.md"&gt;ReqGetAccountApiKeys&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/ReqGetAccountByL1Address.md"&gt;ReqGetAccountByL1Address&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/ReqGetAccountInactiveOrders.md"&gt;ReqGetAccountInactiveOrders&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/ReqGetAccountPnL.md"&gt;ReqGetAccountPnL&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/ReqGetAccountTxs.md"&gt;ReqGetAccountTxs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/ReqGetBlock.md"&gt;ReqGetBlock&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/ReqGetBlockTxs.md"&gt;ReqGetBlockTxs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/ReqGetByAccount.md"&gt;ReqGetByAccount&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/ReqGetCandlesticks.md"&gt;ReqGetCandlesticks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/ReqGetDepositHistory.md"&gt;ReqGetDepositHistory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/ReqGetFundings.md"&gt;ReqGetFundings&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/ReqGetL1Tx.md"&gt;ReqGetL1Tx&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/ReqGetLatestDeposit.md"&gt;ReqGetLatestDeposit&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/ReqGetNextNonce.md"&gt;ReqGetNextNonce&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/ReqGetOrderBookDetails.md"&gt;ReqGetOrderBookDetails&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/ReqGetOrderBookOrders.md"&gt;ReqGetOrderBookOrders&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/ReqGetOrderBooks.md"&gt;ReqGetOrderBooks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/ReqGetPublicPools.md"&gt;ReqGetPublicPools&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/ReqGetRangeWithCursor.md"&gt;ReqGetRangeWithCursor&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/ReqGetRangeWithIndex.md"&gt;ReqGetRangeWithIndex&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/ReqGetRangeWithIndexSortable.md"&gt;ReqGetRangeWithIndexSortable&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/ReqGetRecentTrades.md"&gt;ReqGetRecentTrades&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/ReqGetTrades.md"&gt;ReqGetTrades&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/ReqGetTx.md"&gt;ReqGetTx&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/ReqGetWithdrawHistory.md"&gt;ReqGetWithdrawHistory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/ResultCode.md"&gt;ResultCode&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/SimpleOrder.md"&gt;SimpleOrder&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/Status.md"&gt;Status&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/SubAccounts.md"&gt;SubAccounts&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/Ticker.md"&gt;Ticker&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/Trade.md"&gt;Trade&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/Trades.md"&gt;Trades&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/Tx.md"&gt;Tx&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/TxHash.md"&gt;TxHash&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/TxHashes.md"&gt;TxHashes&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/Txs.md"&gt;Txs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/ValidatorInfo.md"&gt;ValidatorInfo&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/WithdrawHistory.md"&gt;WithdrawHistory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/WithdrawHistoryItem.md"&gt;WithdrawHistoryItem&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/elliottech/lighter-python/main/docs/ZkLighterInfo.md"&gt;ZkLighterInfo&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>eriklindernoren/ML-From-Scratch</title>
      <link>https://github.com/eriklindernoren/ML-From-Scratch</link>
      <description>&lt;p&gt;Machine Learning From Scratch. Bare bones NumPy implementations of machine learning models and algorithms with a focus on accessibility. Aims to cover everything from linear regression to deep learning.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Machine Learning From Scratch&lt;/h1&gt; 
&lt;h2&gt;About&lt;/h2&gt; 
&lt;p&gt;Python implementations of some of the fundamental Machine Learning models and algorithms from scratch.&lt;/p&gt; 
&lt;p&gt;The purpose of this project is not to produce as optimized and computationally efficient algorithms as possible but rather to present the inner workings of them in a transparent and accessible way.&lt;/p&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/#machine-learning-from-scratch"&gt;Machine Learning From Scratch&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/#about"&gt;About&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/#table-of-contents"&gt;Table of Contents&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/#examples"&gt;Examples&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/#polynomial-regression"&gt;Polynomial Regression&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/#classification-with-cnn"&gt;Classification With CNN&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/#density-based-clustering"&gt;Density-Based Clustering&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/#generating-handwritten-digits"&gt;Generating Handwritten Digits&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/#deep-reinforcement-learning"&gt;Deep Reinforcement Learning&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/#image-reconstruction-with-rbm"&gt;Image Reconstruction With RBM&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/#evolutionary-evolved-neural-network"&gt;Evolutionary Evolved Neural Network&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/#genetic-algorithm"&gt;Genetic Algorithm&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/#association-analysis"&gt;Association Analysis&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/#implementations"&gt;Implementations&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/#supervised-learning"&gt;Supervised Learning&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/#unsupervised-learning"&gt;Unsupervised Learning&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/#reinforcement-learning"&gt;Reinforcement Learning&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/#deep-learning"&gt;Deep Learning&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/#contact"&gt;Contact&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/eriklindernoren/ML-From-Scratch
$ cd ML-From-Scratch
$ python setup.py install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;h3&gt;Polynomial Regression&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/examples/polynomial_regression.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p align="center"&gt; &lt;img src="http://eriklindernoren.se/images/p_reg.gif" width="640" \ /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; Figure: Training progress of a regularized polynomial regression model fitting &lt;br /&gt; temperature data measured in LinkÃ¶ping, Sweden 2016. &lt;/p&gt; 
&lt;h3&gt;Classification With CNN&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/examples/convolutional_neural_network.py

+---------+
| ConvNet |
+---------+
Input Shape: (1, 8, 8)
+----------------------+------------+--------------+
| Layer Type           | Parameters | Output Shape |
+----------------------+------------+--------------+
| Conv2D               | 160        | (16, 8, 8)   |
| Activation (ReLU)    | 0          | (16, 8, 8)   |
| Dropout              | 0          | (16, 8, 8)   |
| BatchNormalization   | 2048       | (16, 8, 8)   |
| Conv2D               | 4640       | (32, 8, 8)   |
| Activation (ReLU)    | 0          | (32, 8, 8)   |
| Dropout              | 0          | (32, 8, 8)   |
| BatchNormalization   | 4096       | (32, 8, 8)   |
| Flatten              | 0          | (2048,)      |
| Dense                | 524544     | (256,)       |
| Activation (ReLU)    | 0          | (256,)       |
| Dropout              | 0          | (256,)       |
| BatchNormalization   | 512        | (256,)       |
| Dense                | 2570       | (10,)        |
| Activation (Softmax) | 0          | (10,)        |
+----------------------+------------+--------------+
Total Parameters: 538570

Training: 100% [------------------------------------------------------------------------] Time: 0:01:55
Accuracy: 0.987465181058
&lt;/code&gt;&lt;/pre&gt; 
&lt;p align="center"&gt; &lt;img src="http://eriklindernoren.se/images/mlfs_cnn1.png" width="640" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; Figure: Classification of the digit dataset using CNN. &lt;/p&gt; 
&lt;h3&gt;Density-Based Clustering&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/examples/dbscan.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p align="center"&gt; &lt;img src="http://eriklindernoren.se/images/mlfs_dbscan.png" width="640" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; Figure: Clustering of the moons dataset using DBSCAN. &lt;/p&gt; 
&lt;h3&gt;Generating Handwritten Digits&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/unsupervised_learning/generative_adversarial_network.py

+-----------+
| Generator |
+-----------+
Input Shape: (100,)
+------------------------+------------+--------------+
| Layer Type             | Parameters | Output Shape |
+------------------------+------------+--------------+
| Dense                  | 25856      | (256,)       |
| Activation (LeakyReLU) | 0          | (256,)       |
| BatchNormalization     | 512        | (256,)       |
| Dense                  | 131584     | (512,)       |
| Activation (LeakyReLU) | 0          | (512,)       |
| BatchNormalization     | 1024       | (512,)       |
| Dense                  | 525312     | (1024,)      |
| Activation (LeakyReLU) | 0          | (1024,)      |
| BatchNormalization     | 2048       | (1024,)      |
| Dense                  | 803600     | (784,)       |
| Activation (TanH)      | 0          | (784,)       |
+------------------------+------------+--------------+
Total Parameters: 1489936

+---------------+
| Discriminator |
+---------------+
Input Shape: (784,)
+------------------------+------------+--------------+
| Layer Type             | Parameters | Output Shape |
+------------------------+------------+--------------+
| Dense                  | 401920     | (512,)       |
| Activation (LeakyReLU) | 0          | (512,)       |
| Dropout                | 0          | (512,)       |
| Dense                  | 131328     | (256,)       |
| Activation (LeakyReLU) | 0          | (256,)       |
| Dropout                | 0          | (256,)       |
| Dense                  | 514        | (2,)         |
| Activation (Softmax)   | 0          | (2,)         |
+------------------------+------------+--------------+
Total Parameters: 533762
&lt;/code&gt;&lt;/pre&gt; 
&lt;p align="center"&gt; &lt;img src="http://eriklindernoren.se/images/gan_mnist5.gif" width="640" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; Figure: Training progress of a Generative Adversarial Network generating &lt;br /&gt; handwritten digits. &lt;/p&gt; 
&lt;h3&gt;Deep Reinforcement Learning&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/examples/deep_q_network.py

+----------------+
| Deep Q-Network |
+----------------+
Input Shape: (4,)
+-------------------+------------+--------------+
| Layer Type        | Parameters | Output Shape |
+-------------------+------------+--------------+
| Dense             | 320        | (64,)        |
| Activation (ReLU) | 0          | (64,)        |
| Dense             | 130        | (2,)         |
+-------------------+------------+--------------+
Total Parameters: 450
&lt;/code&gt;&lt;/pre&gt; 
&lt;p align="center"&gt; &lt;img src="http://eriklindernoren.se/images/mlfs_dql1.gif" width="640" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; Figure: Deep Q-Network solution to the CartPole-v1 environment in OpenAI gym. &lt;/p&gt; 
&lt;h3&gt;Image Reconstruction With RBM&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/examples/restricted_boltzmann_machine.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p align="center"&gt; &lt;img src="http://eriklindernoren.se/images/rbm_digits1.gif" width="640" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; Figure: Shows how the network gets better during training at reconstructing &lt;br /&gt; the digit 2 in the MNIST dataset. &lt;/p&gt; 
&lt;h3&gt;Evolutionary Evolved Neural Network&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/examples/neuroevolution.py

+---------------+
| Model Summary |
+---------------+
Input Shape: (64,)
+----------------------+------------+--------------+
| Layer Type           | Parameters | Output Shape |
+----------------------+------------+--------------+
| Dense                | 1040       | (16,)        |
| Activation (ReLU)    | 0          | (16,)        |
| Dense                | 170        | (10,)        |
| Activation (Softmax) | 0          | (10,)        |
+----------------------+------------+--------------+
Total Parameters: 1210

Population Size: 100
Generations: 3000
Mutation Rate: 0.01

[0 Best Individual - Fitness: 3.08301, Accuracy: 10.5%]
[1 Best Individual - Fitness: 3.08746, Accuracy: 12.0%]
...
[2999 Best Individual - Fitness: 94.08513, Accuracy: 98.5%]
Test set accuracy: 96.7%
&lt;/code&gt;&lt;/pre&gt; 
&lt;p align="center"&gt; &lt;img src="http://eriklindernoren.se/images/evo_nn4.png" width="640" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; Figure: Classification of the digit dataset by a neural network which has&lt;br /&gt; been evolutionary evolved. &lt;/p&gt; 
&lt;h3&gt;Genetic Algorithm&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/examples/genetic_algorithm.py

+--------+
|   GA   |
+--------+
Description: Implementation of a Genetic Algorithm which aims to produce
the user specified target string. This implementation calculates each
candidate's fitness based on the alphabetical distance between the candidate
and the target. A candidate is selected as a parent with probabilities proportional
to the candidate's fitness. Reproduction is implemented as a single-point
crossover between pairs of parents. Mutation is done by randomly assigning
new characters with uniform probability.

Parameters
----------
Target String: 'Genetic Algorithm'
Population Size: 100
Mutation Rate: 0.05

[0 Closest Candidate: 'CJqlJguPlqzvpoJmb', Fitness: 0.00]
[1 Closest Candidate: 'MCxZxdr nlfiwwGEk', Fitness: 0.01]
[2 Closest Candidate: 'MCxZxdm nlfiwwGcx', Fitness: 0.01]
[3 Closest Candidate: 'SmdsAklMHn kBIwKn', Fitness: 0.01]
[4 Closest Candidate: '  lotneaJOasWfu Z', Fitness: 0.01]
...
[292 Closest Candidate: 'GeneticaAlgorithm', Fitness: 1.00]
[293 Closest Candidate: 'GeneticaAlgorithm', Fitness: 1.00]
[294 Answer: 'Genetic Algorithm']
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Association Analysis&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/examples/apriori.py
+-------------+
|   Apriori   |
+-------------+
Minimum Support: 0.25
Minimum Confidence: 0.8
Transactions:
    [1, 2, 3, 4]
    [1, 2, 4]
    [1, 2]
    [2, 3, 4]
    [2, 3]
    [3, 4]
    [2, 4]
Frequent Itemsets:
    [1, 2, 3, 4, [1, 2], [1, 4], [2, 3], [2, 4], [3, 4], [1, 2, 4], [2, 3, 4]]
Rules:
    1 -&amp;gt; 2 (support: 0.43, confidence: 1.0)
    4 -&amp;gt; 2 (support: 0.57, confidence: 0.8)
    [1, 4] -&amp;gt; 2 (support: 0.29, confidence: 1.0)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Implementations&lt;/h2&gt; 
&lt;h3&gt;Supervised Learning&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/mlfromscratch/supervised_learning/adaboost.py"&gt;Adaboost&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/mlfromscratch/supervised_learning/bayesian_regression.py"&gt;Bayesian Regression&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/mlfromscratch/supervised_learning/decision_tree.py"&gt;Decision Tree&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/mlfromscratch/supervised_learning/regression.py"&gt;Elastic Net&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/mlfromscratch/supervised_learning/gradient_boosting.py"&gt;Gradient Boosting&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/mlfromscratch/supervised_learning/k_nearest_neighbors.py"&gt;K Nearest Neighbors&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/mlfromscratch/supervised_learning/regression.py"&gt;Lasso Regression&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/mlfromscratch/supervised_learning/linear_discriminant_analysis.py"&gt;Linear Discriminant Analysis&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/mlfromscratch/supervised_learning/regression.py"&gt;Linear Regression&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/mlfromscratch/supervised_learning/logistic_regression.py"&gt;Logistic Regression&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/mlfromscratch/supervised_learning/multi_class_lda.py"&gt;Multi-class Linear Discriminant Analysis&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/mlfromscratch/supervised_learning/multilayer_perceptron.py"&gt;Multilayer Perceptron&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/mlfromscratch/supervised_learning/naive_bayes.py"&gt;Naive Bayes&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/mlfromscratch/supervised_learning/neuroevolution.py"&gt;Neuroevolution&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/mlfromscratch/supervised_learning/particle_swarm_optimization.py"&gt;Particle Swarm Optimization of Neural Network&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/mlfromscratch/supervised_learning/perceptron.py"&gt;Perceptron&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/mlfromscratch/supervised_learning/regression.py"&gt;Polynomial Regression&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/mlfromscratch/supervised_learning/random_forest.py"&gt;Random Forest&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/mlfromscratch/supervised_learning/regression.py"&gt;Ridge Regression&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/mlfromscratch/supervised_learning/support_vector_machine.py"&gt;Support Vector Machine&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/mlfromscratch/supervised_learning/xgboost.py"&gt;XGBoost&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Unsupervised Learning&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/mlfromscratch/unsupervised_learning/apriori.py"&gt;Apriori&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/mlfromscratch/unsupervised_learning/autoencoder.py"&gt;Autoencoder&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/mlfromscratch/unsupervised_learning/dbscan.py"&gt;DBSCAN&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/mlfromscratch/unsupervised_learning/fp_growth.py"&gt;FP-Growth&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/mlfromscratch/unsupervised_learning/gaussian_mixture_model.py"&gt;Gaussian Mixture Model&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/mlfromscratch/unsupervised_learning/generative_adversarial_network.py"&gt;Generative Adversarial Network&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/mlfromscratch/unsupervised_learning/genetic_algorithm.py"&gt;Genetic Algorithm&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/mlfromscratch/unsupervised_learning/k_means.py"&gt;K-Means&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/mlfromscratch/unsupervised_learning/partitioning_around_medoids.py"&gt;Partitioning Around Medoids&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/mlfromscratch/unsupervised_learning/principal_component_analysis.py"&gt;Principal Component Analysis&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/mlfromscratch/unsupervised_learning/restricted_boltzmann_machine.py"&gt;Restricted Boltzmann Machine&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Reinforcement Learning&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/mlfromscratch/reinforcement_learning/deep_q_network.py"&gt;Deep Q-Network&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Deep Learning&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/mlfromscratch/deep_learning/neural_network.py"&gt;Neural Network&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/mlfromscratch/deep_learning/layers.py"&gt;Layers&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Activation Layer&lt;/li&gt; 
   &lt;li&gt;Average Pooling Layer&lt;/li&gt; 
   &lt;li&gt;Batch Normalization Layer&lt;/li&gt; 
   &lt;li&gt;Constant Padding Layer&lt;/li&gt; 
   &lt;li&gt;Convolutional Layer&lt;/li&gt; 
   &lt;li&gt;Dropout Layer&lt;/li&gt; 
   &lt;li&gt;Flatten Layer&lt;/li&gt; 
   &lt;li&gt;Fully-Connected (Dense) Layer&lt;/li&gt; 
   &lt;li&gt;Fully-Connected RNN Layer&lt;/li&gt; 
   &lt;li&gt;Max Pooling Layer&lt;/li&gt; 
   &lt;li&gt;Reshape Layer&lt;/li&gt; 
   &lt;li&gt;Up Sampling Layer&lt;/li&gt; 
   &lt;li&gt;Zero Padding Layer&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Model Types 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/mlfromscratch/examples/convolutional_neural_network.py"&gt;Convolutional Neural Network&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/mlfromscratch/examples/multilayer_perceptron.py"&gt;Multilayer Perceptron&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/mlfromscratch/examples/recurrent_neural_network.py"&gt;Recurrent Neural Network&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contact&lt;/h2&gt; 
&lt;p&gt;If there's some implementation you would like to see here or if you're just feeling social, feel free to &lt;a href="mailto:eriklindernoren@gmail.com"&gt;email&lt;/a&gt; me or connect with me on &lt;a href="https://www.linkedin.com/in/eriklindernoren/"&gt;LinkedIn&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>iam-veeramalla/aws-devops-zero-to-hero</title>
      <link>https://github.com/iam-veeramalla/aws-devops-zero-to-hero</link>
      <description>&lt;p&gt;AWS zero to hero repo for devops engineers to learn AWS in 30 Days. This repo includes projects, presentations, interview questions and real time examples.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;aws-devops-zero-to-hero&lt;/h1&gt; 
&lt;p&gt;Complete YouTube playlist - &lt;a href="https://www.youtube.com/playlist?list=PLdpzxOOAlwvLNOxX0RfndiYSt1Le9azze"&gt;https://www.youtube.com/playlist?list=PLdpzxOOAlwvLNOxX0RfndiYSt1Le9azze&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;AWS zero to hero repo for devops engineers to learn AWS in 30 Days. This repo includes projects, presentations, interview questions and real time examples. Each day's class will provide real-time knowledge on AWS services, allowing you to apply what you've learned and gain practical skills in working with AWS in a DevOps context.&lt;/p&gt; 
&lt;h2&gt;Day 1: Introduction to AWS&lt;/h2&gt; 
&lt;p&gt;You will learn what is private and public cloud. Why companies are moving to public cloud, what are the advantages of moving to cloud.&lt;/p&gt; 
&lt;p&gt;Also, you will be introduced to the basics of AWS, including the core services and their significance in DevOps practices. Finally learn how to set up an AWS account and navigate the AWS Management Console.&lt;/p&gt; 
&lt;h2&gt;Day 2: IAM (Identity and Access Management)&lt;/h2&gt; 
&lt;p&gt;You will explore IAM, which is used for managing access to AWS resources. You'll learn how to create IAM users, groups, and roles, and how to apply permissions and security best practices to ensure proper access control.&lt;/p&gt; 
&lt;h2&gt;Day 3: EC2 Instances&lt;/h2&gt; 
&lt;p&gt;You'll dive into EC2, which provides virtual servers in the cloud. You'll learn how to launch EC2 instances, connect to them using SSH, and understand key concepts such as instance types, security groups, and key pairs.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Your First AWS Project&lt;/strong&gt;: Deploy a simple web application(such as jenkins) on the ec2 instance and access the application from outside AWS.&lt;/p&gt; 
&lt;h2&gt;Day 4: AWS Networking (VPC)&lt;/h2&gt; 
&lt;p&gt;You'll explore AWS networking concepts, with a specific focus on VPC (Virtual Private Cloud). You'll learn how to create and configure VPCs, subnets, and route tables, enabling you to design and manage the network infrastructure for your applications.&lt;/p&gt; 
&lt;h2&gt;Day 5: AWS Security&lt;/h2&gt; 
&lt;p&gt;This day emphasizes security best practices in AWS. You'll learn how to implement security measures such as security groups, network ACLs (Access Control Lists), and IAM policies to ensure the confidentiality, integrity, and availability of your AWS resources.&lt;/p&gt; 
&lt;h2&gt;Day 6: AWS Route 53&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Project:&lt;/strong&gt; Configure and manage a domain name using Route 53. You'll register a domain, set up DNS records, and explore advanced features such as health checks, routing policies, and DNS-based failover.&lt;/p&gt; 
&lt;h2&gt;Day 7: Secure VPC Setup with EC2 Instances&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Project:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Design and configure a VPC: Create a VPC with custom IP ranges. Set up public and private subnets. Configure route tables and associate subnets.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Implement network security: Set up network access control lists (ACLs) to control inbound and outbound traffic. Configure security groups for EC2 instances to allow specific ports and protocols.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Provision EC2 instances: Launch EC2 instances in both the public and private subnets. Configure security groups for the instances to allow necessary traffic. Create and assign IAM roles to the instances with appropriate permissions.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Networking and routing: Set up an internet gateway to allow internet access for instances in the public subnet. Configure NAT gateway or NAT instance to enable outbound internet access for instances in the private subnet. Create appropriate route tables and associate them with the subnets.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;SSH key pair and access control: Generate an SSH key pair and securely store the private key. Configure the instances to allow SSH access only with the generated key pair. Implement IAM policies and roles to control access and permissions to AWS resources.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Test and validate the setup: SSH into the EC2 instances using the private key and verify connectivity. Test network connectivity between instances in different subnets. Validate security group rules and network ACL settings.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;By implementing this project, you'll gain hands-on experience in setting up a secure VPC with EC2 instances, implementing networking and routing, configuring security groups and IAM roles, and ensuring proper access control. This project will provide a practical understanding of how these AWS services work together to create a secure and scalable infrastructure for your applications.&lt;/p&gt; 
&lt;h2&gt;Day 8: AWS Interview Questions on EC2, IAM and VPC&lt;/h2&gt; 
&lt;h2&gt;Day 9: Amazon S3&lt;/h2&gt; 
&lt;p&gt;This day focuses on Amazon S3, a scalable object storage service. You'll learn how to create S3 buckets, upload and download objects, and organize data using S3 features like versioning, lifecycle policies, and access control.&lt;/p&gt; 
&lt;h2&gt;Day 10: AWS CLI&lt;/h2&gt; 
&lt;h2&gt;Day 11: AWS CloudFormation&lt;/h2&gt; 
&lt;p&gt;This day introduces Infrastructure as Code (IaC) using AWS CloudFormation. You'll learn how to create CloudFormation templates to automate the provisioning of resources, manage stacks, and ensure consistent infrastructure across deployments.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Project:&lt;/strong&gt; You'll work on creating a CloudFormation template that provisions a fully configured application stack, including EC2 instances, networking components, and security groups.&lt;/p&gt; 
&lt;h2&gt;Day 12: AWS CodeCommit&lt;/h2&gt; 
&lt;p&gt;This day focuses on AWS CodeCommit, a managed source control service. You'll learn how to set up a Git repository in CodeCommit, collaborate with team members, and manage version control of your codebase.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Project:&lt;/strong&gt; You'll configure a CodeCommit repository for a team project, including setting up access control and collaboration workflows.&lt;/p&gt; 
&lt;h2&gt;Day 13: AWS CodePipeline&lt;/h2&gt; 
&lt;p&gt;You'll dive into AWS CodePipeline, a fully managed continuous delivery service. You'll learn how to build end-to-end CI/CD pipelines by configuring source, build, and deployment stages, automating the entire software release process.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Project:&lt;/strong&gt; You'll create a CI/CD pipeline using CodePipeline for an application deployment, including source code integration, build, and automatic deployment to a target environment.&lt;/p&gt; 
&lt;h2&gt;Day 14: AWS CodeBuild&lt;/h2&gt; 
&lt;p&gt;This day focuses on AWS CodeBuild, a fully managed build service. You'll learn how to configure build projects in CodeBuild, define build specifications, and perform build and testing processes.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Project:&lt;/strong&gt; You'll configure and run CodeBuild for a project, including defining build specifications and integrating with other AWS services.&lt;/p&gt; 
&lt;h2&gt;Day 15: AWS CodeDeploy&lt;/h2&gt; 
&lt;p&gt;You'll explore AWS CodeDeploy, a service for automating application deployments to various compute environments. You'll learn how to create deployment groups, configure deployment strategies, and perform automatic rollbacks if necessary.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Project:&lt;/strong&gt; You'll implement a Blue/Green deployment strategy for a sample application using CodeDeploy, ensuring zero-downtime deployments and easy rollback options.&lt;/p&gt; 
&lt;h2&gt;Day 16: AWS CloudWatch&lt;/h2&gt; 
&lt;p&gt;This day focuses on monitoring AWS resources using AWS CloudWatch. You'll learn how to create alarms, set up notifications, and collect metrics to gain insights into the health and performance of your applications and infrastructure.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Project:&lt;/strong&gt; You'll set up CloudWatch alarms for critical metrics of an application, define appropriate threshold conditions, and configure notification actions.&lt;/p&gt; 
&lt;h2&gt;Day 17: AWS Lambda&lt;/h2&gt; 
&lt;p&gt;This day introduces serverless computing with AWS Lambda. You'll learn how to create and deploy serverless functions, trigger them based on events, and leverage Lambda to build scalable and event-driven architectures.&lt;/p&gt; 
&lt;h2&gt;Day 18: AWS CloudWatch Events and EventBridge&lt;/h2&gt; 
&lt;p&gt;This day focuses on AWS CloudWatch Events and EventBridge, services for event-driven architectures. You'll learn how to create event rules, configure event targets, and build serverless event-driven workflows.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Project:&lt;/strong&gt; You'll build a serverless event-driven workflow using CloudWatch Events and EventBridge, demonstrating the integration and automation of different AWS services based on events.&lt;/p&gt; 
&lt;h2&gt;Day 19: AWS CloudFront&lt;/h2&gt; 
&lt;p&gt;If you've never heard of CDN or CloudFront before, don't worry, we will start from scratch and gradually build up your understanding. By the end, you'll be well-versed in these technologies.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Project:&lt;/strong&gt; You'll configure a s3 bucket to host a static website and learn how to serve the requests to this website through CDN that is AWS Cloud Front.&lt;/p&gt; 
&lt;h2&gt;Day 20: AWS ECR (Elastic Container Registry)&lt;/h2&gt; 
&lt;p&gt;You'll explore AWS ECR, a fully managed container registry for storing and managing container images. You'll learn how to push and pull Docker images to and from ECR, enabling seamless integration with ECS and other container services.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Project:&lt;/strong&gt; You'll build a CI/CD pipeline that automatically builds, pushes, and deploys Docker images to ECR, ensuring streamlined container image management.&lt;/p&gt; 
&lt;h2&gt;Day 21: AWS ECS (Elastic Container Service)&lt;/h2&gt; 
&lt;p&gt;This day focuses on AWS ECS, a fully managed container orchestration service. You'll learn how to run and manage containers using ECS, including creating task definitions, managing services, and scaling with auto-scaling capabilities.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Project:&lt;/strong&gt; You'll deploy a multi-container application using ECS, configure auto-scaling policies, and ensure high availability and efficient resource utilization.&lt;/p&gt; 
&lt;h2&gt;Day 22: AWS EKS (Elastic Kubernetes Service)&lt;/h2&gt; 
&lt;p&gt;This day introduces AWS EKS, a fully managed Kubernetes service. You'll learn how to deploy and manage Kubernetes clusters using EKS, including launching worker nodes, configuring networking, and deploying applications using Kubernetes manifests.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Project:&lt;/strong&gt; You'll deploy a sample application on EKS using Kubernetes manifests, demonstrating the capabilities of running containerized applications on a managed Kubernetes service.&lt;/p&gt; 
&lt;h2&gt;Day 23: AWS Systems Manager&lt;/h2&gt; 
&lt;p&gt;This day focuses on AWS Secrets Manager, a service for storing and managing secrets such as database credentials, API keys, and other sensitive information. You'll learn how to store, retrieve, and rotate secrets securely in your applications.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Project:&lt;/strong&gt; You'll configure Secrets Manager to store and manage secrets, integrate secret retrieval in an application, and implement secret rotation policies.&lt;/p&gt; 
&lt;h2&gt;Day 24: Create Infrastructure using Terraform&lt;/h2&gt; 
&lt;p&gt;This day focusses on creating infrastructure using Terraform with real time example.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Project:&lt;/strong&gt; You'll create a VPC and deploy 2 applications in different availability zones. We will also create a load balancer to balance the load between the instances automatically.&lt;/p&gt; 
&lt;h2&gt;Day 25: AWS CloudTrail and Config&lt;/h2&gt; 
&lt;p&gt;You'll explore AWS CloudTrail and AWS Config, which provide auditing and compliance capabilities. You'll learn how to track API calls using CloudTrail and ensure compliance with AWS Config rules.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Project:&lt;/strong&gt; You'll configure CloudTrail to log API activities and set up AWS Config rules to enforce compliance policies for your AWS resources.&lt;/p&gt; 
&lt;h2&gt;Day 26: AWS Elastic Load Balancer&lt;/h2&gt; 
&lt;p&gt;You'll explore AWS Elastic Load Balancer, a service for distributing incoming application traffic across multiple targets. You'll learn how to configure and manage load balancers to ensure high availability, fault tolerance, and scalability.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Project:&lt;/strong&gt; You'll configure an Elastic Load Balancer for an application, define target groups, and observe the load balancing behavior across instances.&lt;/p&gt; 
&lt;h2&gt;Day 27: 500 AWS interview questions and answers topic wise for interviews.&lt;/h2&gt; 
&lt;p&gt;This day focuses on learning how to migrate applications to AWS cloud. What are the most popular strategies and tools used to achieve the cloud migration.&lt;/p&gt; 
&lt;h2&gt;Day 28: AWS Cloud Migration Strategies and Tools&lt;/h2&gt; 
&lt;p&gt;This day focuses on learning how to migrate applications to AWS cloud. What are the most popular strategies and tools used to achieve the cloud migration.&lt;/p&gt; 
&lt;h2&gt;Day 29: AWS Best Practices and Job Preparation&lt;/h2&gt; 
&lt;p&gt;On the final day, you'll review best practices for AWS services, including security, cost optimization and performance.&lt;/p&gt; 
&lt;h2&gt;Day 30: AWS Project with RDS&lt;/h2&gt;</description>
    </item>
    
    <item>
      <title>vllm-project/vllm-ascend</title>
      <link>https://github.com/vllm-project/vllm-ascend</link>
      <description>&lt;p&gt;Community maintained hardware plugin for vLLM on Ascend&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/vllm-project/vllm-ascend/main/docs/source/logos/vllm-ascend-logo-text-dark.png" /&gt; 
  &lt;img alt="vllm-ascend" src="https://raw.githubusercontent.com/vllm-project/vllm-ascend/main/docs/source/logos/vllm-ascend-logo-text-light.png" width="55%" /&gt; 
 &lt;/picture&gt; &lt;/p&gt; 
&lt;h3 align="center"&gt; vLLM Ascend Plugin &lt;/h3&gt; 
&lt;p align="center"&gt; | &lt;a href="https://www.hiascend.com/en/"&gt;&lt;b&gt;About Ascend&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://vllm-ascend.readthedocs.io/en/latest/"&gt;&lt;b&gt;Documentation&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://slack.vllm.ai"&gt;&lt;b&gt;#sig-ascend&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://discuss.vllm.ai/c/hardware-support/vllm-ascend-support"&gt;&lt;b&gt;Users Forum&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://tinyurl.com/vllm-ascend-meeting"&gt;&lt;b&gt;Weekly Meeting&lt;/b&gt;&lt;/a&gt; | &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a&gt;&lt;b&gt;English&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/vllm-project/vllm-ascend/main/README.zh.md"&gt;&lt;b&gt;ä¸­æ–‡&lt;/b&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;em&gt;Latest News&lt;/em&gt; ğŸ”¥&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;[2025/09] We released the new official version &lt;a href="https://github.com/vllm-project/vllm-ascend/releases/tag/v0.9.1"&gt;v0.9.1&lt;/a&gt;! Please follow the &lt;a href="https://vllm-ascend.readthedocs.io/en/v0.9.1-dev/tutorials/large_scale_ep.html"&gt;official guide&lt;/a&gt; to start deploy large scale Expert Parallelism (EP) on Ascend.&lt;/li&gt; 
 &lt;li&gt;[2025/08] We hosted the &lt;a href="https://mp.weixin.qq.com/s/7n8OYNrCC_I9SJaybHA_-Q"&gt;vLLM Beijing Meetup&lt;/a&gt; with vLLM and Tencent! Please find the meetup slides &lt;a href="https://drive.google.com/drive/folders/1Pid6NSFLU43DZRi0EaTcPgXsAzDvbBqF"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[2025/06] &lt;a href="https://vllm-ascend.readthedocs.io/en/latest/community/user_stories/index.html"&gt;User stories&lt;/a&gt; page is now live! It kicks off with â€ŒLLaMA-Factory/verl//TRL/GPUStackâ€Œ to demonstrate how â€ŒvLLM Ascendâ€Œ assists Ascend users in enhancing their experience across fine-tuning, evaluation, reinforcement learning (RL), and deployment scenarios.&lt;/li&gt; 
 &lt;li&gt;[2025/06] &lt;a href="https://vllm-ascend.readthedocs.io/en/latest/community/contributors.html"&gt;Contributors&lt;/a&gt; page is now live! All contributions deserve to be recorded, thanks for all contributors.&lt;/li&gt; 
 &lt;li&gt;[2025/05] We've released first official version &lt;a href="https://github.com/vllm-project/vllm-ascend/releases/tag/v0.7.3"&gt;v0.7.3&lt;/a&gt;! We collaborated with the vLLM community to publish a blog post sharing our practice: &lt;a href="https://blog.vllm.ai/2025/05/12/hardware-plugin.html"&gt;Introducing vLLM Hardware Plugin, Best Practice from Ascend NPU&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[2025/03] We hosted the &lt;a href="https://mp.weixin.qq.com/s/VtxO9WXa5fC-mKqlxNUJUQ"&gt;vLLM Beijing Meetup&lt;/a&gt; with vLLM team! Please find the meetup slides &lt;a href="https://drive.google.com/drive/folders/1Pid6NSFLU43DZRi0EaTcPgXsAzDvbBqF"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[2025/02] vLLM community officially created &lt;a href="https://github.com/vllm-project/vllm-ascend"&gt;vllm-project/vllm-ascend&lt;/a&gt; repo for running vLLM seamlessly on the Ascend NPU.&lt;/li&gt; 
 &lt;li&gt;[2024/12] We are working with the vLLM community to support &lt;a href="https://github.com/vllm-project/vllm/issues/11162"&gt;[RFC]: Hardware pluggable&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Overview&lt;/h2&gt; 
&lt;p&gt;vLLM Ascend (&lt;code&gt;vllm-ascend&lt;/code&gt;) is a community maintained hardware plugin for running vLLM seamlessly on the Ascend NPU.&lt;/p&gt; 
&lt;p&gt;It is the recommended approach for supporting the Ascend backend within the vLLM community. It adheres to the principles outlined in the &lt;a href="https://github.com/vllm-project/vllm/issues/11162"&gt;[RFC]: Hardware pluggable&lt;/a&gt;, providing a hardware-pluggable interface that decouples the integration of the Ascend NPU with vLLM.&lt;/p&gt; 
&lt;p&gt;By using vLLM Ascend plugin, popular open-source models, including Transformer-like, Mixture-of-Expert, Embedding, Multi-modal LLMs can run seamlessly on the Ascend NPU.&lt;/p&gt; 
&lt;h2&gt;Prerequisites&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Hardware: Atlas 800I A2 Inference series, Atlas A2 Training series, Atlas 800I A3 Inference series, Atlas A3 Training series, Atlas 300I Duo (Experimental)&lt;/li&gt; 
 &lt;li&gt;OS: Linux&lt;/li&gt; 
 &lt;li&gt;Software: 
  &lt;ul&gt; 
   &lt;li&gt;Python &amp;gt;= 3.9, &amp;lt; 3.12&lt;/li&gt; 
   &lt;li&gt;CANN &amp;gt;= 8.2.rc1 (Ascend HDK version refers to &lt;a href="https://www.hiascend.com/document/detail/zh/canncommercial/82RC1/releasenote/releasenote_0000.html"&gt;here&lt;/a&gt;)&lt;/li&gt; 
   &lt;li&gt;PyTorch == 2.7.1, torch-npu == 2.7.1&lt;/li&gt; 
   &lt;li&gt;vLLM (the same version as vllm-ascend)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;Please use the following recommended versions to get started quickly:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Version&lt;/th&gt; 
   &lt;th&gt;Release type&lt;/th&gt; 
   &lt;th&gt;Doc&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;v0.11.0rc0&lt;/td&gt; 
   &lt;td&gt;Latest release candidate&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://vllm-ascend.readthedocs.io/en/latest/quick_start.html"&gt;QuickStart&lt;/a&gt; and &lt;a href="https://vllm-ascend.readthedocs.io/en/latest/installation.html"&gt;Installation&lt;/a&gt; for more details&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;v0.9.1&lt;/td&gt; 
   &lt;td&gt;Latest stable version&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://vllm-ascend.readthedocs.io/en/v0.9.1-dev/quick_start.html"&gt;QuickStart&lt;/a&gt; and &lt;a href="https://vllm-ascend.readthedocs.io/en/v0.9.1-dev/installation.html"&gt;Installation&lt;/a&gt; for more details&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://vllm-ascend.readthedocs.io/en/latest/developer_guide/contribution/index.html"&gt;CONTRIBUTING&lt;/a&gt; for more details, which is a step-by-step guide to help you set up development environment, build and test.&lt;/p&gt; 
&lt;p&gt;We welcome and value any contributions and collaborations:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Please let us know if you encounter a bug by &lt;a href="https://github.com/vllm-project/vllm-ascend/issues"&gt;filing an issue&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Please use &lt;a href="https://discuss.vllm.ai/c/hardware-support/vllm-ascend-support"&gt;User forum&lt;/a&gt; for usage questions and help.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Branch&lt;/h2&gt; 
&lt;p&gt;vllm-ascend has main branch and dev branch.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;main&lt;/strong&gt;: main branchï¼Œcorresponds to the vLLM main branch, and is continuously monitored for quality through Ascend CI.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;vX.Y.Z-dev&lt;/strong&gt;: development branch, created with part of new releases of vLLM. For example, &lt;code&gt;v0.7.3-dev&lt;/code&gt; is the dev branch for vLLM &lt;code&gt;v0.7.3&lt;/code&gt; version.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Below is maintained branches:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Branch&lt;/th&gt; 
   &lt;th&gt;Status&lt;/th&gt; 
   &lt;th&gt;Note&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;main&lt;/td&gt; 
   &lt;td&gt;Maintained&lt;/td&gt; 
   &lt;td&gt;CI commitment for vLLM main branch and vLLM v0.11.0 tag&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;v0.7.1-dev&lt;/td&gt; 
   &lt;td&gt;Unmaintained&lt;/td&gt; 
   &lt;td&gt;Only doc fixed is allowed&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;v0.7.3-dev&lt;/td&gt; 
   &lt;td&gt;Maintained&lt;/td&gt; 
   &lt;td&gt;CI commitment for vLLM 0.7.3 version, only bug fix is allowed and no new release tag any more.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;v0.9.1-dev&lt;/td&gt; 
   &lt;td&gt;Maintained&lt;/td&gt; 
   &lt;td&gt;CI commitment for vLLM 0.9.1 version&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;v0.11.0-dev&lt;/td&gt; 
   &lt;td&gt;Maintained&lt;/td&gt; 
   &lt;td&gt;CI commitment for vLLM 0.11.0 version&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;rfc/feature-name&lt;/td&gt; 
   &lt;td&gt;Maintained&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://vllm-ascend.readthedocs.io/en/latest/community/versioning_policy.html#feature-branches"&gt;Feature branches&lt;/a&gt; for collaboration&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Please refer to &lt;a href="https://vllm-ascend.readthedocs.io/en/latest/community/versioning_policy.html"&gt;Versioning policy&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Weekly Meeting&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;vLLM Ascend Weekly Meeting: &lt;a href="https://tinyurl.com/vllm-ascend-meeting"&gt;https://tinyurl.com/vllm-ascend-meeting&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Wednesday, 15:00 - 16:00 (UTC+8, &lt;a href="https://dateful.com/convert/gmt8?t=15"&gt;Convert to your timezone&lt;/a&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Apache License 2.0, as found in the &lt;a href="https://raw.githubusercontent.com/vllm-project/vllm-ascend/main/LICENSE"&gt;LICENSE&lt;/a&gt; file.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Project-MONAI/MONAI</title>
      <link>https://github.com/Project-MONAI/MONAI</link>
      <description>&lt;p&gt;AI Toolkit for Healthcare Imaging&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Project-MONAI/MONAI/dev/docs/images/MONAI-logo-color.png" width="50%" alt="project-monai" /&gt; &lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;M&lt;/strong&gt;edical &lt;strong&gt;O&lt;/strong&gt;pen &lt;strong&gt;N&lt;/strong&gt;etwork for &lt;strong&gt;AI&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Project-MONAI/MONAI/dev/docs/images/python.svg?sanitize=true" alt="Supported Python versions" /&gt; &lt;a href="https://opensource.org/licenses/Apache-2.0"&gt;&lt;img src="https://img.shields.io/badge/license-Apache%202.0-green.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2211.02701"&gt;&lt;img src="https://img.shields.io/badge/dynamic/json?label=citations&amp;amp;query=%24.citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2FDOI%3A10.48550%2FarXiv.2211.02701%3Ffields%3DcitationCount" alt="auto-commit-msg" /&gt;&lt;/a&gt; &lt;a href="https://badge.fury.io/py/monai"&gt;&lt;img src="https://badge.fury.io/py/monai.svg?sanitize=true" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/projectmonai/monai"&gt;&lt;img src="https://img.shields.io/badge/docker-pull-green.svg?logo=docker&amp;amp;logoColor=white" alt="docker" /&gt;&lt;/a&gt; &lt;a href="https://anaconda.org/conda-forge/monai"&gt;&lt;img src="https://img.shields.io/conda/vn/conda-forge/monai?color=green" alt="conda" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Project-MONAI/MONAI/actions/workflows/pythonapp.yml"&gt;&lt;img src="https://github.com/Project-MONAI/MONAI/actions/workflows/pythonapp.yml/badge.svg?branch=dev" alt="premerge" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Project-MONAI/MONAI/actions?query=branch%3Adev"&gt;&lt;img src="https://img.shields.io/github/checks-status/project-monai/monai/dev?label=postmerge" alt="postmerge" /&gt;&lt;/a&gt; &lt;a href="https://docs.monai.io/en/latest/"&gt;&lt;img src="https://readthedocs.org/projects/monai/badge/?version=latest" alt="Documentation Status" /&gt;&lt;/a&gt; &lt;a href="https://codecov.io/gh/Project-MONAI/MONAI"&gt;&lt;img src="https://codecov.io/gh/Project-MONAI/MONAI/branch/dev/graph/badge.svg?token=6FTC7U1JJ4" alt="codecov" /&gt;&lt;/a&gt; &lt;a href="https://piptrends.com/package/monai"&gt;&lt;img src="https://assets.piptrends.com/get-last-month-downloads-badge/monai.svg?sanitize=true" alt="monai Downloads Last Month" title="monai Downloads Last Month by pip Trends" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;MONAI is a &lt;a href="https://pytorch.org/"&gt;PyTorch&lt;/a&gt;-based, &lt;a href="https://github.com/Project-MONAI/MONAI/raw/dev/LICENSE"&gt;open-source&lt;/a&gt; framework for deep learning in healthcare imaging, part of the &lt;a href="https://pytorch.org/ecosystem/"&gt;PyTorch Ecosystem&lt;/a&gt;. Its ambitions are as follows:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Developing a community of academic, industrial and clinical researchers collaborating on a common foundation;&lt;/li&gt; 
 &lt;li&gt;Creating state-of-the-art, end-to-end training workflows for healthcare imaging;&lt;/li&gt; 
 &lt;li&gt;Providing researchers with the optimized and standardized way to create and evaluate deep learning models.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;Please see &lt;a href="https://docs.monai.io/en/latest/highlights.html"&gt;the technical highlights&lt;/a&gt; and &lt;a href="https://docs.monai.io/en/latest/whatsnew.html"&gt;What's New&lt;/a&gt; of the milestone releases.&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;flexible pre-processing for multi-dimensional medical imaging data;&lt;/li&gt; 
 &lt;li&gt;compositional &amp;amp; portable APIs for ease of integration in existing workflows;&lt;/li&gt; 
 &lt;li&gt;domain-specific implementations for networks, losses, evaluation metrics and more;&lt;/li&gt; 
 &lt;li&gt;customizable design for varying user expertise;&lt;/li&gt; 
 &lt;li&gt;multi-GPU multi-node data parallelism support.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;p&gt;MONAI works with the &lt;a href="https://devguide.python.org/versions"&gt;currently supported versions of Python&lt;/a&gt;, and depends directly on NumPy and PyTorch with many optional dependencies.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Major releases of MONAI will have dependency versions stated for them. The current state of the &lt;code&gt;dev&lt;/code&gt; branch in this repository is the unreleased development version of MONAI which typically will support current versions of dependencies and include updates and bug fixes to do so.&lt;/li&gt; 
 &lt;li&gt;PyTorch support covers &lt;a href="https://github.com/pytorch/pytorch/releases"&gt;the current version&lt;/a&gt; plus three previous minor versions. If compatibility issues with a PyTorch version and other dependencies arise, support for a version may be delayed until a major release.&lt;/li&gt; 
 &lt;li&gt;Our support policy for other dependencies adheres for the most part to &lt;a href="https://scientific-python.org/specs/spec-0000"&gt;SPEC0&lt;/a&gt;, where dependency versions are supported where possible for up to two years. Discovered vulnerabilities or defects may require certain versions to be explicitly not supported.&lt;/li&gt; 
 &lt;li&gt;See the &lt;code&gt;requirements*.txt&lt;/code&gt; files for dependency version information.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;To install &lt;a href="https://pypi.org/project/monai/"&gt;the current release&lt;/a&gt;, you can simply run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install monai
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Please refer to &lt;a href="https://docs.monai.io/en/latest/installation.html"&gt;the installation guide&lt;/a&gt; for other installation options.&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://colab.research.google.com/github/Project-MONAI/tutorials/blob/main/2d_classification/mednist_tutorial.ipynb"&gt;MedNIST demo&lt;/a&gt; and &lt;a href="https://colab.research.google.com/github/Project-MONAI/tutorials/blob/main/modules/developer_guide.ipynb"&gt;MONAI for PyTorch Users&lt;/a&gt; are available on Colab.&lt;/p&gt; 
&lt;p&gt;Examples and notebook tutorials are located at &lt;a href="https://github.com/Project-MONAI/tutorials"&gt;Project-MONAI/tutorials&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Technical documentation is available at &lt;a href="https://docs.monai.io"&gt;docs.monai.io&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you have used MONAI in your research, please cite us! The citation can be exported from: &lt;a href="https://arxiv.org/abs/2211.02701"&gt;https://arxiv.org/abs/2211.02701&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Model Zoo&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/Project-MONAI/model-zoo"&gt;The MONAI Model Zoo&lt;/a&gt; is a place for researchers and data scientists to share the latest and great models from the community. Utilizing &lt;a href="https://docs.monai.io/en/latest/bundle_intro.html"&gt;the MONAI Bundle format&lt;/a&gt; makes it easy to &lt;a href="https://github.com/Project-MONAI/tutorials/tree/main/model_zoo"&gt;get started&lt;/a&gt; building workflows with MONAI.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;For guidance on making a contribution to MONAI, see the &lt;a href="https://github.com/Project-MONAI/MONAI/raw/dev/CONTRIBUTING.md"&gt;contributing guidelines&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;p&gt;Join the conversation on Twitter/X &lt;a href="https://twitter.com/ProjectMONAI"&gt;@ProjectMONAI&lt;/a&gt;, &lt;a href="https://www.linkedin.com/company/projectmonai"&gt;LinkedIn&lt;/a&gt;, or join our &lt;a href="https://forms.gle/QTxJq3hFictp31UM9"&gt;Slack channel&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Ask and answer questions over on &lt;a href="https://github.com/Project-MONAI/MONAI/discussions"&gt;MONAI's GitHub Discussions tab&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Links&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Website: &lt;a href="https://monai.io/"&gt;https://monai.io/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;API documentation (milestone): &lt;a href="https://docs.monai.io/"&gt;https://docs.monai.io/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;API documentation (latest dev): &lt;a href="https://docs.monai.io/en/latest/"&gt;https://docs.monai.io/en/latest/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Code: &lt;a href="https://github.com/Project-MONAI/MONAI"&gt;https://github.com/Project-MONAI/MONAI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Project tracker: &lt;a href="https://github.com/Project-MONAI/MONAI/projects"&gt;https://github.com/Project-MONAI/MONAI/projects&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Issue tracker: &lt;a href="https://github.com/Project-MONAI/MONAI/issues"&gt;https://github.com/Project-MONAI/MONAI/issues&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Wiki: &lt;a href="https://github.com/Project-MONAI/MONAI/wiki"&gt;https://github.com/Project-MONAI/MONAI/wiki&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Test status: &lt;a href="https://github.com/Project-MONAI/MONAI/actions"&gt;https://github.com/Project-MONAI/MONAI/actions&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;PyPI package: &lt;a href="https://pypi.org/project/monai/"&gt;https://pypi.org/project/monai/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;conda-forge: &lt;a href="https://anaconda.org/conda-forge/monai"&gt;https://anaconda.org/conda-forge/monai&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Weekly previews: &lt;a href="https://pypi.org/project/monai-weekly/"&gt;https://pypi.org/project/monai-weekly/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Docker Hub: &lt;a href="https://hub.docker.com/r/projectmonai/monai"&gt;https://hub.docker.com/r/projectmonai/monai&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>microsoft/agent-lightning</title>
      <link>https://github.com/microsoft/agent-lightning</link>
      <description>&lt;p&gt;The absolute trainer to light up AI agents.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/agent-lightning/main/docs/assets/readme-banner.svg?sanitize=true" alt="Agent-lightning-banner" style="width:600px" /&gt; &lt;/p&gt; 
&lt;h1&gt;Agent Lightningâš¡&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/microsoft/agent-lightning/actions/workflows/tests-full.yml"&gt;&lt;img src="https://github.com/microsoft/agent-lightning/actions/workflows/tests-full.yml/badge.svg?sanitize=true" alt="Test" /&gt;&lt;/a&gt; &lt;a href="https://microsoft.github.io/agent-lightning/"&gt;&lt;img src="https://img.shields.io/badge/GitHub%20Pages-Documentation-blue" alt="Documentation" /&gt;&lt;/a&gt; &lt;a href="https://badge.fury.io/py/agentlightning"&gt;&lt;img src="https://badge.fury.io/py/agentlightning.svg?sanitize=true" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/microsoft/agent-lightning/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/RYk7CdvDR7"&gt;&lt;img src="https://img.shields.io/badge/Discord-Join-5865F2?logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;The absolute trainer to light up AI agents.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Join our &lt;a href="https://discord.gg/RYk7CdvDR7"&gt;Discord community&lt;/a&gt; to connect with other users and contributors.&lt;/p&gt; 
&lt;h2&gt;âš¡ Core Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Turn your agent into an optimizable beast with &lt;strong&gt;ZERO CODE CHANGE&lt;/strong&gt; (almost)! ğŸ’¤&lt;/li&gt; 
 &lt;li&gt;Build with &lt;strong&gt;ANY&lt;/strong&gt; agent framework (LangChain, OpenAI Agent SDK, AutoGen, CrewAI, Microsoft Agent Framework...); or even WITHOUT agent framework (Python OpenAI). You name it! ğŸ¤–&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Selectively&lt;/strong&gt; optimize one or more agents in a multi-agent system. ğŸ¯&lt;/li&gt; 
 &lt;li&gt;Embraces &lt;strong&gt;Algorithms&lt;/strong&gt; like Reinforcement Learning, Automatic Prompt Optimization, Supervised Fine-tuning and more. ğŸ¤—&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Read more on our &lt;a href="https://microsoft.github.io/agent-lightning/"&gt;documentation website&lt;/a&gt;.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/agent-lightning/main/docs/assets/readme-diff.svg?sanitize=true" alt="Agent-Lightning Core Quickstart" style="width:100%" /&gt; &lt;/p&gt; 
&lt;h2&gt;âš¡ Installation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install agentlightning
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Please refer to our &lt;a href="https://microsoft.github.io/agent-lightning/stable/tutorials/installation/"&gt;installation guide&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;p&gt;To start using Agent-lightning, check out our &lt;a href="https://microsoft.github.io/agent-lightning/"&gt;documentation&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/microsoft/agent-lightning/main/examples"&gt;examples&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;âš¡ Articles&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;10/22/2025 &lt;a href="https://blog.vllm.ai/2025/10/22/agent-lightning.html"&gt;No More Retokenization Drift: Returning Token IDs via the OpenAI Compatible API Matters in Agent RL&lt;/a&gt; vLLM blog. See also &lt;a href="https://zhuanlan.zhihu.com/p/1965067274642785725"&gt;Zhihu writeup&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;8/11/2025 &lt;a href="https://medium.com/@yugez/training-ai-agents-to-write-and-self-correct-sql-with-reinforcement-learning-571ed31281ad"&gt;Training AI Agents to Write and Self-correct SQL with Reinforcement Learning&lt;/a&gt; Medium.&lt;/li&gt; 
 &lt;li&gt;8/5/2025 &lt;a href="https://arxiv.org/abs/2508.03680"&gt;Agent Lightning: Train ANY AI Agents with Reinforcement Learning&lt;/a&gt; arXiv paper.&lt;/li&gt; 
 &lt;li&gt;7/26/2025 &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1m9m670/we_discovered_an_approach_to_train_any_ai_agent/"&gt;We discovered an approach to train any AI agent with RL, with (almost) zero code changes.&lt;/a&gt; Reddit.&lt;/li&gt; 
 &lt;li&gt;6/6/2025 &lt;a href="https://www.microsoft.com/en-us/research/project/agent-lightning/"&gt;Agent Lightning - Microsoft Research&lt;/a&gt; Project page.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;âš¡ Community Projects&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/af-74413592/DeepWerewolf"&gt;DeepWerewolf&lt;/a&gt; â€” A case study of agent RL training for the Chinese Werewolf game built with AgentScope and Agent Lightning.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://agentflow.stanford.edu/"&gt;AgentFlow&lt;/a&gt; â€” A modular multi-agent framework that combines planner, executor, verifier, and generator agents with the Flow-GRPO algorithm to tackle long-horizon, sparse-reward tasks.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;âš¡ Architecture&lt;/h2&gt; 
&lt;p&gt;Agent Lightning keeps the moving parts to a minimum so you can focus on your idea, not the plumbing. Your agent continues to run as usual; you can still use any agent framework you like; you drop in the lightweight &lt;code&gt;agl.emit_xxx()&lt;/code&gt; helper, or let the tracer collect every prompt, tool call, and reward. Those events become structured spans that flow into the LightningStore, a central hub that keeps tasks, resources, and traces in sync.&lt;/p&gt; 
&lt;p&gt;On the other side of the store sits the algorithm you choose, or write yourself. The algorithm reads spans, learns from them, and posts updated resources such as refined prompt templates or new policy weights. The Trainer ties it all together: it streams datasets to runners, ferries resources between the store and the algorithm, and updates the inference engine when improvements land. You can either stop there, or simply let the same loop keep turning.&lt;/p&gt; 
&lt;p&gt;No rewrites, no lock-in, just a clear path from first rollout to steady improvement.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/agent-lightning/main/docs/assets/readme-architecture.svg?sanitize=true" alt="Agent-lightning Architecture" style="width:100%" /&gt; &lt;/p&gt; 
&lt;h2&gt;âš¡ CI Status&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Workflow&lt;/th&gt; 
   &lt;th&gt;Status&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;CPU Tests&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/agent-lightning/actions/workflows/tests.yml"&gt;&lt;img src="https://github.com/microsoft/agent-lightning/actions/workflows/tests.yml/badge.svg?sanitize=true" alt="tests workflow status" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;GPU Tests&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/agent-lightning/actions/workflows/tests-full.yml"&gt;&lt;img src="https://github.com/microsoft/agent-lightning/actions/workflows/tests-full.yml/badge.svg?sanitize=true" alt="tests-full workflow status" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Examples Integration&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/agent-lightning/actions/workflows/badge-examples.yml"&gt;&lt;img src="https://github.com/microsoft/agent-lightning/actions/workflows/badge-examples.yml/badge.svg?sanitize=true" alt="examples summary workflow status" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Latest Dependency Compatibility&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/agent-lightning/actions/workflows/badge-latest.yml"&gt;&lt;img src="https://github.com/microsoft/agent-lightning/actions/workflows/badge-latest.yml/badge.svg?sanitize=true" alt="latest summary workflow status" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Legacy Examples Compatibility&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/agent-lightning/actions/workflows/examples-compat.yml"&gt;&lt;img src="https://github.com/microsoft/agent-lightning/actions/workflows/examples-compat.yml/badge.svg?sanitize=true" alt="examples compatibility workflow status" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;âš¡ Citation&lt;/h2&gt; 
&lt;p&gt;If you find Agent Lightning useful in your research or projects, please cite our paper:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{luo2025agentlightningtrainai,
      title={Agent Lightning: Train ANY AI Agents with Reinforcement Learning},
      author={Xufang Luo and Yuge Zhang and Zhiyuan He and Zilong Wang and Siyun Zhao and Dongsheng Li and Luna K. Qiu and Yuqing Yang},
      year={2025},
      eprint={2508.03680},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2508.03680},
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;âš¡ Contributing&lt;/h2&gt; 
&lt;p&gt;This project welcomes contributions and suggestions. Start by reading the &lt;a href="https://raw.githubusercontent.com/microsoft/agent-lightning/main/docs/community/contributing.md"&gt;Contributing Guide&lt;/a&gt; for environment setup, branching conventions, and pull request expectations. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href="https://cla.opensource.microsoft.com"&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; 
&lt;p&gt;This project has adopted the &lt;a href="https://opensource.microsoft.com/codeofconduct/"&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href="https://opensource.microsoft.com/codeofconduct/faq/"&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href="mailto:opencode@microsoft.com"&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; 
&lt;h2&gt;âš¡ Trademarks&lt;/h2&gt; 
&lt;p&gt;This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow &lt;a href="https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general"&gt;Microsoft's Trademark &amp;amp; Brand Guidelines&lt;/a&gt;. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.&lt;/p&gt; 
&lt;h2&gt;âš¡ Responsible AI&lt;/h2&gt; 
&lt;p&gt;This project has been evaluated and certified to comply with the Microsoft Responsible AI Standard. The team will continue to monitor and maintain the repository, addressing any severe issues, including potential harms, if they arise.&lt;/p&gt; 
&lt;h2&gt;âš¡ License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the MIT License. See the &lt;a href="https://raw.githubusercontent.com/microsoft/agent-lightning/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>unitreerobotics/unitree_rl_lab</title>
      <link>https://github.com/unitreerobotics/unitree_rl_lab</link>
      <description>&lt;p&gt;This is a repository for reinforcement learning implementation for Unitree robots, based on IsaacLab.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Unitree RL Lab&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://docs.omniverse.nvidia.com/isaacsim/latest/overview.html"&gt;&lt;img src="https://img.shields.io/badge/IsaacSim-5.0.0-silver.svg?sanitize=true" alt="IsaacSim" /&gt;&lt;/a&gt; &lt;a href="https://isaac-sim.github.io/IsaacLab"&gt;&lt;img src="https://img.shields.io/badge/IsaacLab-2.2.0-silver" alt="Isaac Lab" /&gt;&lt;/a&gt; &lt;a href="https://opensource.org/license/apache-2-0"&gt;&lt;img src="https://img.shields.io/badge/license-Apache2.0-yellow.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/ZwcVwxv5rq"&gt;&lt;img src="https://img.shields.io/badge/-Discord-5865F2?style=flat&amp;amp;logo=Discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Overview&lt;/h2&gt; 
&lt;p&gt;This project provides a set of reinforcement learning environments for Unitree robots, built on top of &lt;a href="https://github.com/isaac-sim/IsaacLab"&gt;IsaacLab&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Currently supports Unitree &lt;strong&gt;Go2&lt;/strong&gt;, &lt;strong&gt;H1&lt;/strong&gt; and &lt;strong&gt;G1-29dof&lt;/strong&gt; robots.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;
     &lt;div align="center"&gt;
       Isaac Lab 
     &lt;/div&gt;&lt;/th&gt; 
    &lt;th&gt;
     &lt;div align="center"&gt;
       Mujoco 
     &lt;/div&gt;&lt;/th&gt; 
    &lt;th&gt;
     &lt;div align="center"&gt;
       Physical 
     &lt;/div&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/unitreerobotics/unitree_rl_lab/main/g1_sim.gif"&gt;&lt;img src="https://oss-global-cdn.unitree.com/static/d879adac250648c587d3681e90658b49_480x397.gif" width="240px" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/unitreerobotics/unitree_rl_lab/main/g1_mujoco.gif"&gt;&lt;img src="https://oss-global-cdn.unitree.com/static/3c88e045ab124c3ab9c761a99cb5e71f_480x397.gif" width="240px" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/unitreerobotics/unitree_rl_lab/main/g1_real.gif"&gt;&lt;img src="https://oss-global-cdn.unitree.com/static/6c17c6cf52ec4e26bbfab1fbf591adb2_480x270.gif" width="240px" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Install Isaac Lab by following the &lt;a href="https://isaac-sim.github.io/IsaacLab/main/source/setup/installation/index.html"&gt;installation guide&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install the Unitree RL IsaacLab standalone environments.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;Clone or copy this repository separately from the Isaac Lab installation (i.e. outside the &lt;code&gt;IsaacLab&lt;/code&gt; directory):&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/unitreerobotics/unitree_rl_lab.git
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Use a python interpreter that has Isaac Lab installed, install the library in editable mode using:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;conda activate env_isaaclab
./unitree_rl_lab.sh -i
# restart your shell to activate the environment changes.
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Download unitree robot description files&lt;/p&gt; &lt;p&gt;&lt;em&gt;Method 1: Using USD Files&lt;/em&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;Download unitree usd files from &lt;a href="https://huggingface.co/datasets/unitreerobotics/unitree_model/tree/main"&gt;unitree_model&lt;/a&gt;, keeping folder structure&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://huggingface.co/datasets/unitreerobotics/unitree_model
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Config &lt;code&gt;UNITREE_MODEL_DIR&lt;/code&gt; in &lt;code&gt;source/unitree_rl_lab/unitree_rl_lab/assets/robots/unitree.py&lt;/code&gt;.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;UNITREE_MODEL_DIR = "&amp;lt;/home/user/projects/unitree_usd&amp;gt;"
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;&lt;em&gt;Method 2: Using URDF Files [Recommended]&lt;/em&gt; Only for Isaacsim &amp;gt;= 5.0&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Download unitree robot urdf files from &lt;a href="https://github.com/unitreerobotics/unitree_ros"&gt;unitree_ros&lt;/a&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/unitreerobotics/unitree_ros.git
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt;Config &lt;code&gt;UNITREE_ROS_DIR&lt;/code&gt; in &lt;code&gt;source/unitree_rl_lab/unitree_rl_lab/assets/robots/unitree.py&lt;/code&gt;. &lt;pre&gt;&lt;code class="language-bash"&gt;UNITREE_ROS_DIR = "&amp;lt;/home/user/projects/unitree_ros/unitree_ros&amp;gt;"
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt;[Optional]: change &lt;em&gt;robot_cfg.spawn&lt;/em&gt; if you want to use urdf files&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Verify that the environments are correctly installed by:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;Listing the available tasks:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;./unitree_rl_lab.sh -l # This is a faster version than isaaclab
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Running a task:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;./unitree_rl_lab.sh -t --task Unitree-G1-29dof-Velocity # support for autocomplete task-name
# same as
python scripts/rsl_rl/train.py --headless --task Unitree-G1-29dof-Velocity
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Inference with a trained agent:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;./unitree_rl_lab.sh -p --task Unitree-G1-29dof-Velocity # support for autocomplete task-name
# same as
python scripts/rsl_rl/play.py --task Unitree-G1-29dof-Velocity
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Deploy&lt;/h2&gt; 
&lt;p&gt;After the model training is completed, we need to perform sim2sim on the trained strategy in Mujoco to test the performance of the model. Then deploy sim2real.&lt;/p&gt; 
&lt;h3&gt;Setup&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install dependencies
sudo apt install -y libyaml-cpp-dev libboost-all-dev libeigen3-dev libspdlog-dev libfmt-dev
# Install unitree_sdk2
git clone git@github.com:unitreerobotics/unitree_sdk2.git
cd unitree_sdk2
mkdir build &amp;amp;&amp;amp; cd build
cmake .. -DBUILD_EXAMPLES=OFF # Install on the /usr/local directory
sudo make install
# Compile the robot_controller
cd unitree_rl_lab/deploy/robots/g1_29dof # or other robots
mkdir build &amp;amp;&amp;amp; cd build
cmake .. &amp;amp;&amp;amp; make
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Sim2Sim&lt;/h3&gt; 
&lt;p&gt;Installing the &lt;a href="https://github.com/unitreerobotics/unitree_mujoco?tab=readme-ov-file#installation"&gt;unitree_mujoco&lt;/a&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Set the &lt;code&gt;robot&lt;/code&gt; at &lt;code&gt;/simulate/config.yaml&lt;/code&gt; to g1&lt;/li&gt; 
 &lt;li&gt;Set &lt;code&gt;domain_id&lt;/code&gt; to 0&lt;/li&gt; 
 &lt;li&gt;Set &lt;code&gt;enable_elastic_hand&lt;/code&gt; to 1&lt;/li&gt; 
 &lt;li&gt;Set &lt;code&gt;use_joystck&lt;/code&gt; to 1.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# start simulation
cd unitree_mujoco/simulate/build
./unitree_mujoco
# ./unitree_mujoco -i 0 -n eth0 -r g1 -s scene_29dof.xml # alternative
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd unitree_rl_lab/deploy/robots/g1_29dof/build
./g1_ctrl
# 1. press [L2 + Up] to set the robot to stand up
# 2. Click the mujoco window, and then press 8 to make the robot feet touch the ground.
# 3. Press [R1 + X] to run the policy.
# 4. Click the mujoco window, and then press 9 to disable the elastic band.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Sim2Real&lt;/h3&gt; 
&lt;p&gt;You can use this program to control the robot directly, but make sure the on-borad control program has been closed.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;./g1_ctrl --network eth0 # eth0 is the network interface name.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;p&gt;This repository is built upon the support and contributions of the following open-source projects. Special thanks to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/isaac-sim/IsaacLab"&gt;IsaacLab&lt;/a&gt;: The foundation for training and running codes.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/google-deepmind/mujoco.git"&gt;mujoco&lt;/a&gt;: Providing powerful simulation functionalities.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/fan-ziqi/robot_lab"&gt;robot_lab&lt;/a&gt;: Referenced for project structure and parts of the implementation.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/HybridRobotics/whole_body_tracking"&gt;whole_body_tracking&lt;/a&gt;: Versatile humanoid control framework for motion tracking.&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>aliasrobotics/cai</title>
      <link>https://github.com/aliasrobotics/cai</link>
      <description>&lt;p&gt;Cybersecurity AI (CAI), the framework for AI Security&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Cybersecurity AI (&lt;code&gt;CAI&lt;/code&gt;)&lt;/h1&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;a align="center" href="" target="https://github.com/aliasrobotics/CAI"&gt; &lt;img width="100%" src="https://github.com/aliasrobotics/cai/raw/main/media/cai.png" /&gt; &lt;/a&gt; &lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/14317" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/14317" alt="aliasrobotics%2Fcai | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://badge.fury.io/py/cai-framework"&gt;&lt;img src="https://badge.fury.io/py/cai-framework.svg?sanitize=true" alt="version" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/projects/cai-framework"&gt;&lt;img src="https://static.pepy.tech/badge/cai-framework" alt="downloads" /&gt;&lt;/a&gt; &lt;a href="https://github.com/aliasrobotics/cai"&gt;&lt;img src="https://img.shields.io/badge/Linux-Supported-brightgreen?logo=linux&amp;amp;logoColor=white" alt="Linux" /&gt;&lt;/a&gt; &lt;a href="https://github.com/aliasrobotics/cai"&gt;&lt;img src="https://img.shields.io/badge/OS%20X-Supported-brightgreen?logo=apple&amp;amp;logoColor=white" alt="OS X" /&gt;&lt;/a&gt; &lt;a href="https://github.com/aliasrobotics/cai"&gt;&lt;img src="https://img.shields.io/badge/Windows-Supported-brightgreen?logo=windows&amp;amp;logoColor=white" alt="Windows" /&gt;&lt;/a&gt; &lt;a href="https://github.com/aliasrobotics/cai"&gt;&lt;img src="https://img.shields.io/badge/Android-Supported-brightgreen?logo=android&amp;amp;logoColor=white" alt="Android" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/fnUFcTaQAC"&gt;&lt;img src="https://img.shields.io/badge/Discord-7289DA?logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/pdf/2504.06017"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2504.06017-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/pdf/2506.23592"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2506.23592-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/pdf/2508.13588"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2508.13588-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/pdf/2508.21669"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2508.21669-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/pdf/2509.14096"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2509.14096-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/pdf/2509.14139"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2509.14139-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/pdf/2510.17521"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2510.17521-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/pdf/2510.24317"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2510.24317-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;!-- CAI PRO - Professional Edition Banner --&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://aliasrobotics.com/cybersecurityai.php" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/media/cai-banner.svg?sanitize=true" alt="CAI - Community and Professional Editions" width="100%" style="max-width: 900px;" /&gt; &lt;/a&gt; 
 &lt;p&gt;&lt;sub&gt;&lt;i&gt;Professional Edition with unlimited &lt;code&gt;alias1&lt;/code&gt; tokens&lt;/i&gt; | &lt;a href="https://aliasrobotics.com/alias1.php#benchmarking"&gt;ğŸ“Š View Benchmarks&lt;/a&gt; | &lt;a href="https://aliasrobotics.com/cybersecurityai.php"&gt;ğŸš€ Learn More&lt;/a&gt;&lt;/sub&gt;&lt;/p&gt; 
 &lt;table style="border-collapse: collapse; width: 100%"&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td width="50%" align="center" style="padding: 0; border: none;"&gt; &lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/media/cai_poc.gif" alt="CAI Community Edition Demo" width="100%" /&gt; &lt;/td&gt; 
    &lt;td width="50%" align="center" style="padding: 0; border: none;"&gt; &lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/media/caipro_poc.gif" alt="CAI PRO Professional Edition Demo" width="100%" /&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;!-- Alternative HTML version (kept as comment for reference) --&gt; 
&lt;!--
&lt;div align="center"&gt;
  &lt;table style="border-collapse: collapse; width: 100%; max-width: 900px; box-shadow: 0 4px 12px rgba(82, 157, 134, 0.15);"&gt;
    &lt;tr&gt;
      &lt;td align="center" width="50%" style="padding: 20px; border: 3px solid #529d86; border-right: 1.5px solid #529d86; border-radius: 10px 0 0 10px; background: linear-gradient(135deg, #f0f8f6 0%, #ffffff 100%);"&gt;
        &lt;h3 style="color: #3d7b6b;"&gt;ğŸ”“ Community Edition&lt;/h3&gt;
        &lt;sub style="color: #529d86;"&gt;&lt;b&gt;Research &amp; Learning Â· Perfect for Researchers &amp; Students&lt;/b&gt;&lt;/sub&gt;&lt;br&gt;&lt;br&gt;
        &lt;code style="background: linear-gradient(135deg, #e8f5f1 0%, #d4ede5 100%); padding: 8px 16px; border-radius: 6px; font-size: 14px; border: 1px solid #529d86; color: #2d5a4d;"&gt;pip install cai-framework&lt;/code&gt;&lt;br&gt;&lt;br&gt;
        &lt;div align="left" style="margin: 10px auto; max-width: 200px; color: #2d2d2d;"&gt;
          âœ… &lt;b style="color: #529d86;"&gt;Free&lt;/b&gt; for research&lt;br&gt;
          ğŸ¤– &lt;b style="color: #529d86;"&gt;300+&lt;/b&gt; AI models&lt;br&gt;
          ğŸŒ &lt;b style="color: #529d86;"&gt;Community&lt;/b&gt; driven&lt;br&gt;
          ğŸ“š &lt;b style="color: #529d86;"&gt;Open&lt;/b&gt; source&lt;br&gt;
          ğŸ”§ &lt;b style="color: #529d86;"&gt;Extensible&lt;/b&gt; framework&lt;br&gt;
        &lt;/div&gt;
      &lt;/td&gt;
      &lt;td align="center" width="50%" style="padding: 20px; border: 3px solid #529d86; border-left: 1.5px solid #529d86; border-radius: 0 10px 10px 0; background: linear-gradient(135deg, #529d86 0%, #6bb09a 100%); position: relative; box-shadow: inset 0 0 30px rgba(255, 255, 255, 0.1);"&gt;
        &lt;h3 style="color: #ffffff; text-shadow: 0 2px 4px rgba(0, 0, 0, 0.2);"&gt;ğŸš€ &lt;a href="https://aliasrobotics.com/cybersecurityai.php" style="text-decoration: none; color: #ffffff;"&gt;Professional Edition&lt;/a&gt;&lt;/h3&gt;
        &lt;sub style="color: #e8f5f1;"&gt;&lt;b&gt;Enterprise &amp; Production Â· â‚¬350/month Â· Unlimited &lt;code style="background: rgba(255, 255, 255, 0.2); padding: 2px 6px; border-radius: 3px; color: #ffffff;"&gt;alias1&lt;/code&gt; Tokens&lt;/b&gt;&lt;/sub&gt;&lt;br&gt;&lt;br&gt;
        &lt;a href="https://aliasrobotics.com/cybersecurityai.php"&gt;
          &lt;code style="background: linear-gradient(135deg, #ffffff 0%, #f0f8f6 100%); color: #529d86; padding: 10px 20px; border-radius: 6px; font-size: 14px; font-weight: bold; border: 2px solid #ffffff; box-shadow: 0 2px 8px rgba(0, 0, 0, 0.2);"&gt;â†’ Upgrade to PRO&lt;/code&gt;
        &lt;/a&gt;&lt;br&gt;&lt;br&gt;
        &lt;div align="left" style="margin: 10px auto; max-width: 280px; color: #ffffff;"&gt;
          âš¡ &lt;b&gt;&lt;a href="https://aliasrobotics.com/alias1.php#benchmarking" style="color: #ffffff; text-decoration: underline;"&gt;alias1&lt;/a&gt;&lt;/b&gt; model - âˆ unlimited tokens&lt;br&gt;
          ğŸš« &lt;b&gt;Zero refusals&lt;/b&gt; - Unrestricted AI&lt;br&gt;
          ğŸ† &lt;b&gt;Beats GPT-5&lt;/b&gt; in CTF benchmarks&lt;br&gt;
          ğŸ›¡ï¸ &lt;b&gt;Professional&lt;/b&gt; support included&lt;br&gt;
          ğŸ‡ªğŸ‡º &lt;b&gt;European&lt;/b&gt; data sovereignty&lt;br&gt;
        &lt;/div&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td colspan="2" align="center" style="padding: 10px; background: #f6f8fa;"&gt;
        &lt;sub&gt;
          &lt;a href="https://aliasrobotics.com/cybersecurityai.php"&gt;&lt;/a&gt;&lt;br&gt;
          &lt;i&gt;CAI PRO w/ &lt;code&gt;alias1&lt;/code&gt; model outperforms GPT-5 in AI vs AI cybersecurity benchmarks&lt;/i&gt; | &lt;a href="https://aliasrobotics.com/alias1.php#benchmarking"&gt;View Full Benchmarks â†’&lt;/a&gt;
        &lt;/sub&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/table&gt;
&lt;/div&gt;
--&gt; 
&lt;p&gt;Cybersecurity AI (CAI) is a lightweight, open-source framework that empowers security professionals to build and deploy AI-powered offensive and defensive automation. CAI is the &lt;em&gt;de facto&lt;/em&gt; framework for AI Security, already used by thousands of individual users and hundreds of organizations. Whether you're a security researcher, ethical hacker, IT professional, or organization looking to enhance your security posture, CAI provides the building blocks to create specialized AI agents that can assist with mitigation, vulnerability discovery, exploitation, and security assessment.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ¤– &lt;strong&gt;300+ AI Models&lt;/strong&gt;: Support for OpenAI, Anthropic, DeepSeek, Ollama, and more&lt;/li&gt; 
 &lt;li&gt;ğŸ”§ &lt;strong&gt;Built-in Security Tools&lt;/strong&gt;: Ready-to-use tools for reconnaissance, exploitation, and privilege escalation&lt;/li&gt; 
 &lt;li&gt;ğŸ† &lt;strong&gt;Battle-tested&lt;/strong&gt;: Proven in HackTheBox CTFs, bug bounties, and real-world security &lt;a href="https://aliasrobotics.com/case-studies-robot-cybersecurity.php"&gt;case studies&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ¯ &lt;strong&gt;Agent-based Architecture&lt;/strong&gt;: Modular framework design to build specialized agents for different security tasks&lt;/li&gt; 
 &lt;li&gt;ğŸ›¡ï¸ &lt;strong&gt;Guardrails Protection&lt;/strong&gt;: Built-in defenses against prompt injection and dangerous command execution&lt;/li&gt; 
 &lt;li&gt;ğŸ“š &lt;strong&gt;Research-oriented&lt;/strong&gt;: Research foundation to democratize cybersecurity AI for the community&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Read the technical report: &lt;a href="https://arxiv.org/pdf/2504.06017"&gt;CAI: An Open, Bug Bounty-Ready Cybersecurity AI&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;For further readings, refer to our &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-impact"&gt;impact&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#citation"&gt;CAI citation&lt;/a&gt; sections.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;a href="https://aliasrobotics.com/case-study-ecoforest.php"&gt;&lt;code&gt;OT&lt;/code&gt; - CAI and alias0 on: Ecoforest Heat Pumps&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://aliasrobotics.com/case-study-cai-mir.php"&gt;&lt;code&gt;Robotics&lt;/code&gt; - CAI and alias0 on: Mobile Industrial Robots (MiR)&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;CAI discovers critical vulnerability in Ecoforest heat pumps allowing unauthorized remote access and potential catastrophic failures. AI-powered security testing reveals exposed credentials and DES encryption weaknesses affecting all of their deployed units across Europe.&lt;/td&gt; 
   &lt;td&gt;CAI-powered security testing of MiR (Mobile Industrial Robot) platform through automated ROS message injection attacks. This study demonstrates how AI-driven vulnerability discovery can expose unauthorized access to robot control systems and alarm triggers.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://aliasrobotics.com/case-study-ecoforest.php"&gt;&lt;img src="https://aliasrobotics.com/img/case-study-portada-ecoforest.png" alt="" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://aliasrobotics.com/case-study-cai-mir.php"&gt;&lt;img src="https://aliasrobotics.com/img/case-study-portada-mir-cai.png" alt="" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;a href="https://aliasrobotics.com/case-study-mercado-libre.php"&gt;&lt;code&gt;IT&lt;/code&gt; (Web) - CAI and alias0 on: Mercado Libre's e-commerce&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://aliasrobotics.com/case-study-cai-mqtt-broker.php"&gt;&lt;code&gt;OT&lt;/code&gt; - CAI and alias0 on: MQTT broker&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;CAI-powered API vulnerability discovery at Mercado Libre through automated enumeration attacks. This study demonstrates how AI-driven security testing can expose user data exposure risks in e-commerce platforms at scale.&lt;/td&gt; 
   &lt;td&gt;CAI-powered testing exposed critical flaws in an MQTT broker within a Dockerized OT network. Without authentication, CAI subscribed to temperature and humidity topics and injected false values, corrupting data shown in Grafana dashboards.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://aliasrobotics.com/case-study-mercado-libre.php"&gt;&lt;img src="https://aliasrobotics.com/img/case-study-portada-mercado-libre.png" alt="" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://aliasrobotics.com/case-study-cai-mqtt-broker.php"&gt;&lt;img src="https://aliasrobotics.com/img/case-study-portada-mqtt-broker-cai.png" alt="" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING] &lt;span&gt;âš &lt;/span&gt; CAI is in active development, so don't expect it to work flawlessly. Instead, contribute by raising an issue or &lt;a href="https://github.com/aliasrobotics/cai/pulls"&gt;sending a PR&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;Access to this library and the use of information, materials (or portions thereof), is &lt;strong&gt;&lt;u&gt;not intended&lt;/u&gt;, and is &lt;u&gt;prohibited&lt;/u&gt;, where such access or use violates applicable laws or regulations&lt;/strong&gt;. By no means the authors encourage or promote the unauthorized tampering with running systems. This can cause serious human harm and material damages.&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;By no means the authors of CAI encourage or promote the unauthorized tampering with compute systems. Please don't use the source code in here for cybercrime. &lt;u&gt;Pentest for good instead&lt;/u&gt;&lt;/em&gt;. By downloading, using, or modifying this source code, you agree to the terms of the &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/LICENSE"&gt;&lt;code&gt;LICENSE&lt;/code&gt;&lt;/a&gt; and the limitations outlined in the &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/DISCLAIMER"&gt;&lt;code&gt;DISCLAIMER&lt;/code&gt;&lt;/a&gt; file.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;&lt;span&gt;ğŸ”–&lt;/span&gt; Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#cybersecurity-ai-cai"&gt;Cybersecurity AI (&lt;code&gt;CAI&lt;/code&gt;)&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#bookmark-table-of-contents"&gt;&lt;span&gt;ğŸ”–&lt;/span&gt; Table of Contents&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-impact"&gt;ğŸ¯ Impact&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-competitions-and-challenges"&gt;ğŸ† Competitions and challenges&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-research-impact"&gt;ğŸ“Š Research Impact&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-research-products-cybersecurity-ai"&gt;ğŸ“š Research products: &lt;code&gt;Cybersecurity AI&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#pocs"&gt;PoCs&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#motivation"&gt;Motivation&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#bust_in_silhouette-why-cai"&gt;&lt;span&gt;ğŸ‘¤&lt;/span&gt; Why CAI?&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#ethical-principles-behind-cai"&gt;Ethical principles behind CAI&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#closed-source-alternatives"&gt;Closed-source alternatives&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#learn---cai-fluency"&gt;Learn - &lt;code&gt;CAI&lt;/code&gt; Fluency&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#nut_and_bolt-install"&gt;&lt;span&gt;ğŸ”©&lt;/span&gt; Install&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#os-x"&gt;OS X&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#ubuntu-2404"&gt;Ubuntu 24.04&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#ubuntu-2004"&gt;Ubuntu 20.04&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#windows-wsl"&gt;Windows WSL&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#android"&gt;Android&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#nut_and_bolt-setup-env-file"&gt;&lt;span&gt;ğŸ”©&lt;/span&gt; Setup &lt;code&gt;.env&lt;/code&gt; file&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-custom-openai-base-url-support"&gt;ğŸ”¹ Custom OpenAI Base URL Support&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#triangular_ruler-architecture"&gt;&lt;span&gt;ğŸ“&lt;/span&gt; Architecture:&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-agent"&gt;ğŸ”¹ Agent&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-tools"&gt;ğŸ”¹ Tools&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-handoffs"&gt;ğŸ”¹ Handoffs&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-patterns"&gt;ğŸ”¹ Patterns&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-turns-and-interactions"&gt;ğŸ”¹ Turns and Interactions&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-tracing"&gt;ğŸ”¹ Tracing&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-guardrails"&gt;ğŸ”¹ Guardrails&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-human-in-the-loop-hitl"&gt;ğŸ”¹ Human-In-The-Loop (HITL)&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#rocket-quickstart"&gt;&lt;span&gt;ğŸš€&lt;/span&gt; Quickstart&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#environment-variables"&gt;Environment Variables&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#openrouter-integration"&gt;OpenRouter Integration&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#azure-openai"&gt;Azure OpenAI&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#mcp"&gt;MCP&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#development"&gt;Development&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#contributions"&gt;Contributions&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#optional-requirements-caiextensions"&gt;Optional Requirements: caiextensions&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#information_source-usage-data-collection"&gt;&lt;span&gt;â„¹&lt;/span&gt; Usage Data Collection&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#reproduce-ci-setup-locally"&gt;Reproduce CI-Setup locally&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#faq"&gt;FAQ&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#citation"&gt;Citation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#acknowledgements"&gt;Acknowledgements&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#academic-collaborations"&gt;Academic Collaborations&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ¯ Impact&lt;/h2&gt; 
&lt;h3&gt;ğŸ† Competitions and challenges&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://app.hackthebox.com/users/2268644"&gt;&lt;img src="https://img.shields.io/badge/HTB_ranking-top_90_Spain_(5_days)-red.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://app.hackthebox.com/users/2268644"&gt;&lt;img src="https://img.shields.io/badge/HTB_ranking-top_50_Spain_(6_days)-red.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://app.hackthebox.com/users/2268644"&gt;&lt;img src="https://img.shields.io/badge/HTB_ranking-top_30_Spain_(7_days)-red.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://app.hackthebox.com/users/2268644"&gt;&lt;img src="https://img.shields.io/badge/HTB_ranking-top_500_World_(7_days)-red.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://ctf.hackthebox.com/event/2000/scoreboard"&gt;&lt;img src="https://img.shields.io/badge/HTB_%22Human_vs_AI%22_CTF-top_1_(AIs)_world-red.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://ctf.hackthebox.com/event/2000/scoreboard"&gt;&lt;img src="https://img.shields.io/badge/HTB_%22Human_vs_AI%22_CTF-top_1_Spain-red.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://ctf.hackthebox.com/event/2000/scoreboard"&gt;&lt;img src="https://img.shields.io/badge/HTB_%22Human_vs_AI%22_CTF-top_20_World-red.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://ctf.hackthebox.com/event/2000/scoreboard"&gt;&lt;img src="https://img.shields.io/badge/HTB_%22Human_vs_AI%22_CTF-750_$-yellow.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://lu.ma/roboticshack?tk=RuryKF"&gt;&lt;img src="https://img.shields.io/badge/Mistral_AI_Robotics_Hackathon-2500_$-yellow.svg?sanitize=true" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;ğŸ“Š Research Impact&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Pioneered LLM-powered AI Security with PentestGPT, establishing the foundation for the &lt;code&gt;Cybersecurity AI&lt;/code&gt; research domain &lt;a href="https://arxiv.org/pdf/2308.06782"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2308.06782-4a9b8e.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Established the &lt;code&gt;Cybersecurity AI&lt;/code&gt; research line with &lt;strong&gt;8 papers and technical reports&lt;/strong&gt;, with active research collaborations &lt;a href="https://arxiv.org/pdf/2504.06017"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2504.06017-63bfab.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2506.23592"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2506.23592-7dd3c0.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2508.13588"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2508.13588-52a896.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2508.21669"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2508.21669-85e0d1.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2509.14096"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2509.14096-3e8b7a.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2509.14139"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2509.14139-6bc7b5.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2510.17521"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2510.17521-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2510.24317"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2510.24317-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Demonstrated &lt;strong&gt;3,600Ã— performance improvement&lt;/strong&gt; over human penetration testers in standardized CTF benchmark evaluations &lt;a href="https://arxiv.org/pdf/2504.06017"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2504.06017-63bfab.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Identified &lt;strong&gt;CVSS 4.3-7.5 severity vulnerabilities&lt;/strong&gt; in production systems through automated security assessment &lt;a href="https://arxiv.org/pdf/2504.06017"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2504.06017-63bfab.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Democratization of AI-empowered vulnerability research&lt;/strong&gt;: CAI enables both non-security domain experts and experienced researchers to conduct more efficient vulnerability discovery, expanding the security research community while empowering small and medium enterprises to conduct autonomous security assessments &lt;a href="https://arxiv.org/pdf/2504.06017"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2504.06017-63bfab.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Systematic evaluation of large language models&lt;/strong&gt; across both proprietary and open-weight architectures, revealing &lt;u&gt;substantial gaps&lt;/u&gt; between vendor-reported capabilities and empirical cybersecurity performance metrics &lt;a href="https://arxiv.org/pdf/2504.06017"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2504.06017-63bfab.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Established the &lt;strong&gt;autonomy levels in cybersecurity&lt;/strong&gt; and argued about autonomy vs automation in the field &lt;a href="https://arxiv.org/abs/2506.23592"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2506.23592-7dd3c0.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Collaborative research initiatives&lt;/strong&gt; with international academic institutions focused on developing cybersecurity education curricula and training methodologies &lt;a href="https://arxiv.org/abs/2508.13588"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2508.13588-52a896.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Contributed a comprehensive defense framework against prompt injection in AI security agents&lt;/strong&gt;: developed and empirically validated a multi-layered defense system that addresses the identified prompt injection issues &lt;a href="https://arxiv.org/abs/2508.21669"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2508.21669-85e0d1.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Explord the Cybersecurity of Humanoid Robots with CAI and identified new attack vectors showing how it &lt;code&gt;(a)&lt;/code&gt; operates simultaneously as a covert surveillance node and &lt;code&gt;(b)&lt;/code&gt; can be purposed as an active cyber operations platform &lt;a href="https://arxiv.org/abs/2509.14096"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2509.14096-3e8b7a.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2509.14139"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2509.14139-6bc7b5.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ“š Research products: &lt;code&gt;Cybersecurity AI&lt;/code&gt;&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;CAI, An Open, Bug Bounty-Ready Cybersecurity AI &lt;a href="https://arxiv.org/pdf/2504.06017"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2504.06017-63bfab.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;The Dangerous Gap Between Automation and Autonomy &lt;a href="https://arxiv.org/abs/2506.23592"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2506.23592-7dd3c0.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;CAI Fluency, A Framework for Cybersecurity AI Fluency &lt;a href="https://arxiv.org/abs/2508.13588"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2508.13588-52a896.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;Hacking the AI Hackers via Prompt Injection &lt;a href="https://arxiv.org/abs/2508.21669"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2508.21669-85e0d1.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/2504.06017"&gt;&lt;img src="https://aliasrobotics.com/img/paper-cai.png" width="350" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.arxiv.org/pdf/2506.23592"&gt;&lt;img src="https://aliasrobotics.com/img/cai_automation_vs_autonomy.png" width="350" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/2508.13588"&gt;&lt;img src="https://aliasrobotics.com/img/cai_fluency_cover.png" width="350" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/2508.21669"&gt;&lt;img src="https://aliasrobotics.com/img/aihackers.jpeg" width="350" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Humanoid Robots as Attack Vectors &lt;a href="https://arxiv.org/abs/2509.14139"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2509.14139-6bc7b5.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;The Cybersecurity of a Humanoid Robot &lt;a href="https://arxiv.org/abs/2509.14096"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2509.14096-3e8b7a.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;Evaluating Agentic Cybersecurity in Attack/Defense CTFs &lt;a href="https://arxiv.org/abs/2510.17521"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2510.17521-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;CAIBench: A Meta-Benchmark for Evaluating Cybersecurity AI Agents &lt;a href="https://arxiv.org/abs/2510.24317"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2510.24317-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/2509.14139"&gt;&lt;img src="https://aliasrobotics.com/img/humanoids-cover.png" width="350" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/2509.14096"&gt;&lt;img src="https://aliasrobotics.com/img/humanoid.png" width="350" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/2510.17521"&gt;&lt;img src="https://aliasrobotics.com/img/cai_ad.png" width="350" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/2510.24317"&gt;&lt;img src="https://aliasrobotics.com/img/caibench_banner2.png" width="350" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;PoCs&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;CAI with &lt;code&gt;alias0&lt;/code&gt; on ROS message injection attacks in MiR-100 robot&lt;/th&gt; 
   &lt;th&gt;CAI with &lt;code&gt;alias0&lt;/code&gt; on API vulnerability discovery at Mercado Libre&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://asciinema.org/a/dNv705hZel2Rzrw0cju9HBGPh"&gt;&lt;img src="https://asciinema.org/a/dNv705hZel2Rzrw0cju9HBGPh.svg?sanitize=true" alt="asciicast" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://asciinema.org/a/9Hc9z1uFcdNjqP3bY5y7wO1Ww"&gt;&lt;img src="https://asciinema.org/a/9Hc9z1uFcdNjqP3bY5y7wO1Ww.svg?sanitize=true" alt="asciicast" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;CAI on JWT@PortSwigger CTF â€” Cybersecurity AI&lt;/th&gt; 
   &lt;th&gt;CAI on HackableII Boot2Root CTF â€” Cybersecurity AI&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://asciinema.org/a/713487"&gt;&lt;img src="https://asciinema.org/a/713487.svg?sanitize=true" alt="asciicast" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://asciinema.org/a/713485"&gt;&lt;img src="https://asciinema.org/a/713485.svg?sanitize=true" alt="asciicast" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;More case studies and PoCs are available at &lt;a href="https://aliasrobotics.com/case-studies-robot-cybersecurity.php"&gt;https://aliasrobotics.com/case-studies-robot-cybersecurity.php&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Motivation&lt;/h2&gt; 
&lt;h3&gt;&lt;span&gt;ğŸ‘¤&lt;/span&gt; Why CAI?&lt;/h3&gt; 
&lt;p&gt;The cybersecurity landscape is undergoing a dramatic transformation as AI becomes increasingly integrated into security operations. &lt;strong&gt;We predict that by 2028, AI-powered security testing tools will outnumber human pentesters&lt;/strong&gt;. This shift represents a fundamental change in how we approach cybersecurity challenges. &lt;em&gt;AI is not just another tool - it's becoming essential for addressing complex security vulnerabilities and staying ahead of sophisticated threats. As organizations face more advanced cyber attacks, AI-enhanced security testing will be crucial for maintaining robust defenses.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;This work builds upon prior efforts[^4] and similarly, we believe that democratizing access to advanced cybersecurity AI tools is vital for the entire security community. That's why we're releasing Cybersecurity AI (&lt;code&gt;CAI&lt;/code&gt;) as an open source framework. Our goal is to empower security researchers, ethical hackers, and organizations to build and deploy powerful AI-driven security tools. By making these capabilities openly available, we aim to level the playing field and ensure that cutting-edge security AI technology isn't limited to well-funded private companies or state actors.&lt;/p&gt; 
&lt;p&gt;Bug Bounty programs have become a cornerstone of modern cybersecurity, providing a crucial mechanism for organizations to identify and fix vulnerabilities in their systems before they can be exploited. These programs have proven highly effective at securing both public and private infrastructure, with researchers discovering critical vulnerabilities that might have otherwise gone unnoticed. CAI is specifically designed to enhance these efforts by providing a lightweight, ergonomic framework for building specialized AI agents that can assist in various aspects of Bug Bounty hunting - from initial reconnaissance to vulnerability validation and reporting. Our framework aims to augment human expertise with AI capabilities, helping researchers work more efficiently and thoroughly in their quest to make digital systems more secure.&lt;/p&gt; 
&lt;h3&gt;Ethical principles behind CAI&lt;/h3&gt; 
&lt;p&gt;You might be wondering if releasing CAI &lt;em&gt;in-the-wild&lt;/em&gt; given its capabilities and security implications is ethical. Our decision to open-source this framework is guided by two core ethical principles:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Democratizing Cybersecurity AI&lt;/strong&gt;: We believe that advanced cybersecurity AI tools should be accessible to the entire security community, not just well-funded private companies or state actors. By releasing CAI as an open source framework, we aim to empower security researchers, ethical hackers, and organizations to build and deploy powerful AI-driven security tools, leveling the playing field in cybersecurity.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Transparency in AI Security Capabilities&lt;/strong&gt;: Based on our research results, understanding of the technology, and dissection of top technical reports, we argue that current LLM vendors are undermining their cybersecurity capabilities. This is extremely dangerous and misleading. By developing CAI openly, we provide a transparent benchmark of what AI systems can actually do in cybersecurity contexts, enabling more informed decisions about security postures.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;CAI is built on the following core principles:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Cybersecurity oriented AI framework&lt;/strong&gt;: CAI is specifically designed for cybersecurity use cases, aiming at semi- and fully-automating offensive and defensive security tasks.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Open source, free for research&lt;/strong&gt;: CAI is open source and free for research purposes. We aim at democratizing access to AI and Cybersecurity. For professional or commercial use, including on-premise deployments, dedicated technical support and custom extensions &lt;a href="mailto:research@aliasrobotics.com"&gt;reach out&lt;/a&gt; to obtain a license.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Lightweight&lt;/strong&gt;: CAI is designed to be fast, and easy to use.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Modular and agent-centric design&lt;/strong&gt;: CAI operates on the basis of agents and agentic patterns, which allows flexibility and scalability. You can easily add the most suitable agents and pattern for your cybersecuritytarget case.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Tool-integration&lt;/strong&gt;: CAI integrates already built-in tools, and allows the user to integrate their own tools with their own logic easily.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Logging and tracing integrated&lt;/strong&gt;: using &lt;a href="https://github.com/Arize-ai/phoenix"&gt;&lt;code&gt;phoenix&lt;/code&gt;&lt;/a&gt;, the open source tracing and logging tool for LLMs. This provides the user with a detailed traceability of the agents and their execution.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-Model Support&lt;/strong&gt;: more than 300 supported and empowered by &lt;a href="https://github.com/BerriAI/litellm"&gt;LiteLLM&lt;/a&gt;. The most popular providers: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Anthropic&lt;/strong&gt;: &lt;code&gt;Claude 3.7&lt;/code&gt;, &lt;code&gt;Claude 3.5&lt;/code&gt;, &lt;code&gt;Claude 3&lt;/code&gt;, &lt;code&gt;Claude 3 Opus&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;OpenAI&lt;/strong&gt;: &lt;code&gt;O1&lt;/code&gt;, &lt;code&gt;O1 Mini&lt;/code&gt;, &lt;code&gt;O3 Mini&lt;/code&gt;, &lt;code&gt;GPT-4o&lt;/code&gt;, &lt;code&gt;GPT-4.5 Preview&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;DeepSeek&lt;/strong&gt;: &lt;code&gt;DeepSeek V3&lt;/code&gt;, &lt;code&gt;DeepSeek R1&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Ollama&lt;/strong&gt;: &lt;code&gt;Qwen2.5 72B&lt;/code&gt;, &lt;code&gt;Qwen2.5 14B&lt;/code&gt;, etc&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Closed-source alternatives&lt;/h3&gt; 
&lt;p&gt;Cybersecurity AI is a critical field, yet many groups are misguidedly pursuing it through closed-source methods for pure economic return, leveraging similar techniques and building upon existing closed-source (&lt;em&gt;often third-party owned&lt;/em&gt;) models. This approach not only squanders valuable engineering resources but also represents an economic waste and results in redundant efforts, as they often end up reinventing the wheel. Here are some of the closed-source initiatives we keep track of and attempting to leverage genAI and agentic frameworks in cybersecurity AI:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.acyber.co/"&gt;Autonomous Cyber&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://cracken.ai/"&gt;CrackenAGI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ethiack.com/"&gt;ETHIACK&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://horizon3.ai/"&gt;Horizon3&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.irregular.com/"&gt;Irregular&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.kindo.ai/"&gt;Kindo&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://lakera.ai"&gt;Lakera&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/www.mindfort.ai"&gt;Mindfort&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mindgard.ai/"&gt;Mindgard&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ndaysecurity.com/"&gt;NDAY Security&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.runsybil.com"&gt;Runsybil&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.selfhack.fi"&gt;Selfhack&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://sola.security/"&gt;Sola Security&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://squr.ai/"&gt;SQUR&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://staris.tech/"&gt;Staris&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.sxipher.com/"&gt;Sxipher&lt;/a&gt; (seems discontinued)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.terra.security"&gt;Terra Security&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://xint.io/"&gt;Xint&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.xbow.com"&gt;XBOW&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.zeropath.com"&gt;ZeroPath&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.zynap.com"&gt;Zynap&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://7ai.com"&gt;7ai&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Learn - &lt;code&gt;CAI&lt;/code&gt; Fluency&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;a align="center" href="" target="https://github.com/aliasrobotics/CAI"&gt; &lt;img width="100%" src="https://github.com/aliasrobotics/cai/raw/main/media/caiedu.PNG" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;CAI Fluency technical report (&lt;a href="https://arxiv.org/pdf/2508.13588"&gt;arXiv:2508.13588&lt;/a&gt;) establishes formal educational frameworks for cybersecurity AI literacy.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;English&lt;/th&gt; 
   &lt;th&gt;Spanish&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Episode 0&lt;/strong&gt;: What is CAI?&lt;/td&gt; 
   &lt;td&gt;Cybersecurity AI (&lt;code&gt;CAI&lt;/code&gt;) explained&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=nBdTxbKM4oo"&gt;&lt;img src="https://img.youtube.com/vi/nBdTxbKM4oo/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=FaUL9HXrQ5k"&gt;&lt;img src="https://img.youtube.com/vi/FaUL9HXrQ5k/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Episode 1&lt;/strong&gt;: The &lt;code&gt;CAI&lt;/code&gt; Framework&lt;/td&gt; 
   &lt;td&gt;Vision &amp;amp; Ethics - Explore the core motivation behind CAI and delve into the crucial ethical principles guiding its development. Understand the motivation behind CAI and how you can actively contribute to the future of cybersecurity and the CAI framework.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=QEiGdsMf29M&amp;amp;list=PLLc16OUiZWd4RuFdN5_Wx9xwjCVVbopzr&amp;amp;index=3"&gt;&lt;img src="https://img.youtube.com/vi/QEiGdsMf29M/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Episode 2&lt;/strong&gt;: From Zero to Cyber Hero&lt;/td&gt; 
   &lt;td&gt;Breaking into Cybersecurity with AI - A comprehensive guide for complete beginners to become cybersecurity practitioners using CAI and AI tools. Learn how to leverage artificial intelligence to accelerate your cybersecurity learning journey, from understanding basic security concepts to performing real-world security assessments, all without requiring prior cybersecurity experience.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=hSTLHOOcQoY&amp;amp;list=PLLc16OUiZWd4RuFdN5_Wx9xwjCVVbopzr&amp;amp;index=14"&gt;&lt;img src="https://img.youtube.com/vi/hSTLHOOcQoY/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Episode 3&lt;/strong&gt;: Vibe-Hacking Tutorial&lt;/td&gt; 
   &lt;td&gt;"My first Hack" - A Vibe-Hacking guide for newbies. We demonstrate a simple web security hack using a default agent and show how to leverage tools and interpret CAI output with the help of the CAI Python API. You'll also learn to compare different LLM models to find the best fit for your hacking endeavors.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=9vZ_Iyex7uI&amp;amp;list=PLLc16OUiZWd4RuFdN5_Wx9xwjCVVbopzr&amp;amp;index=1"&gt;&lt;img src="https://img.youtube.com/vi/9vZ_Iyex7uI/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=iAOMaI1ftiA&amp;amp;list=PLLc16OUiZWd4RuFdN5_Wx9xwjCVVbopzr&amp;amp;index=2"&gt;&lt;img src="https://img.youtube.com/vi/iAOMaI1ftiA/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Episode 4&lt;/strong&gt;: Intro ReAct&lt;/td&gt; 
   &lt;td&gt;The Evolution of LLMs - Learn how LLMs evolved from basic language models to advanced multiagency AI systems. From basic LLMs to Chain-of-Thought and Reasoning LLMs towards ReAct and Multi-Agent Architectures. Get to know the basic terms&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=tLdFO1flj_o&amp;amp;list=PLLc16OUiZWd4RuFdN5_Wx9xwjCVVbopzr&amp;amp;index=13"&gt;&lt;img src="https://img.youtube.com/vi/tLdFO1flj_o/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Episode 5&lt;/strong&gt;: CAI on CTF challenges&lt;/td&gt; 
   &lt;td&gt;Dive into Capture The Flag (CTF) competitions using CAI. Learn how to leverage AI agents to solve various cybersecurity challenges including web exploitation, cryptography, reverse engineering, and forensics. Discover how to configure CAI for competitive hacking scenarios and maximize your CTF performance with intelligent automation.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=MrXTQ0e2to4&amp;amp;list=PLLc16OUiZWd4RuFdN5_Wx9xwjCVVbopzr&amp;amp;index=13"&gt;&lt;img src="https://img.youtube.com/vi/MrXTQ0e2to4/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=r9US_JZa9_c&amp;amp;list=PLLc16OUiZWd4RuFdN5_Wx9xwjCVVbopzr&amp;amp;index=12"&gt;&lt;img src="https://img.youtube.com/vi/r9US_JZa9_c/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Annex 1&lt;/strong&gt;: &lt;code&gt;CAI&lt;/code&gt; 0.5.x release&lt;/td&gt; 
   &lt;td&gt;Introduce version 0.5 of &lt;code&gt;CAI&lt;/code&gt; including new multi-agent functionality, new commands such as &lt;code&gt;/history&lt;/code&gt;, &lt;code&gt;/compact&lt;/code&gt;, &lt;code&gt;/graph&lt;/code&gt; or &lt;code&gt;/memory&lt;/code&gt; and a case study showing how &lt;code&gt;CAI&lt;/code&gt; found a critical security flaw in OT heap pumps spread around the world.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=OPFH0ANUMMw"&gt;&lt;img src="https://img.youtube.com/vi/OPFH0ANUMMw/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=Q8AI4E4gH8k"&gt;&lt;img src="https://img.youtube.com/vi/Q8AI4E4gH8k/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Annex 2&lt;/strong&gt;: &lt;code&gt;CAI&lt;/code&gt; 0.4.x release and &lt;code&gt;alias0&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Introducing version 0.4 of &lt;code&gt;CAI&lt;/code&gt; with &lt;em&gt;streaming&lt;/em&gt; and improved MCP support. We also introduce &lt;code&gt;alias0&lt;/code&gt;, the Privacy-First Cybersecurity AI, a Model-of-Models Intelligence that implements a Privacy-by-Design architecture and obtains state-of-the-art results in cybersecurity benchmarks.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=NZjzfnvAZcc"&gt;&lt;img src="https://img.youtube.com/vi/NZjzfnvAZcc/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Annex 3&lt;/strong&gt;: Cybersecurity AI Community Meeting #1&lt;/td&gt; 
   &lt;td&gt;First Cybersecurity AI (&lt;code&gt;CAI&lt;/code&gt;) community meeting, over 40 participants from academia, industry, and defense gathered to discuss the open-source scaffolding behind CAI â€” a project designed to build agentic AI systems for cybersecurity that are open, modular, and Bug Bounty-ready.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=4JqaTiVlgsw"&gt;&lt;img src="https://img.youtube.com/vi/4JqaTiVlgsw/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Annex 4&lt;/strong&gt;: &lt;code&gt;CAI PRO&lt;/code&gt; PoC&lt;/td&gt; 
   &lt;td&gt;Short proof-of-concept demonstration of &lt;a href="https://aliasrobotics.com/cybersecurityai.php"&gt;CAI PRO&lt;/a&gt; capabilities showcasing the Professional Edition with unlimited &lt;code&gt;alias1&lt;/code&gt; tokens, unrestricted AI, and enterprise-grade security testing features.&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/media/caipro_poc.gif" alt="CAI PRO Demo" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Annex 5&lt;/strong&gt;: &lt;code&gt;CAI&lt;/code&gt; PoC&lt;/td&gt; 
   &lt;td&gt;Short proof-of-concept demonstration of CAI Community Edition showcasing the open-source framework's core capabilities for AI-powered security testing and vulnerability discovery.&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/media/cai_poc.gif" alt="CAI Demo" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Annex 6&lt;/strong&gt;: CAI in &lt;code&gt;Jaula del N00B&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;CAI (CIBERSEGURIDAD CON IA) LUIJAIT EN LA JAULA DEL N00B - Demonstration and discussion of CAI framework capabilities in the popular Spanish cybersecurity podcast/show.&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=KD2_xzIOkWg"&gt;&lt;img src="https://img.youtube.com/vi/KD2_xzIOkWg/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;&lt;span&gt;ğŸ”©&lt;/span&gt; Install&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install cai-framework
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Always create a new virtual environment to ensure proper dependency installation when updating CAI.&lt;/p&gt; 
&lt;p&gt;The following subsections provide a more detailed walkthrough on selected popular Operating Systems. Refer to the &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#development"&gt;Development&lt;/a&gt; section for developer-related install instructions.&lt;/p&gt; 
&lt;h3&gt;OS X&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;brew update &amp;amp;&amp;amp; \
    brew install git python@3.12

# Create virtual environment
python3.12 -m venv cai_env

# Install the package from the local directory
source cai_env/bin/activate &amp;amp;&amp;amp; pip install cai-framework

# Generate a .env file and set up with defaults
echo -e 'OPENAI_API_KEY="sk-1234"\nANTHROPIC_API_KEY=""\nOLLAMA=""\nPROMPT_TOOLKIT_NO_CPR=1\nCAI_STREAM=false' &amp;gt; .env

# Launch CAI
cai  # first launch it can take up to 30 seconds
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Ubuntu 24.04&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt-get update &amp;amp;&amp;amp; \
    sudo apt-get install -y git python3-pip python3.12-venv

# Create the virtual environment
python3.12 -m venv cai_env

# Install the package from the local directory
source cai_env/bin/activate &amp;amp;&amp;amp; pip install cai-framework

# Generate a .env file and set up with defaults
echo -e 'OPENAI_API_KEY="sk-1234"\nANTHROPIC_API_KEY=""\nOLLAMA=""\nPROMPT_TOOLKIT_NO_CPR=1\nCAI_STREAM=false' &amp;gt; .env

# Launch CAI
cai  # first launch it can take up to 30 seconds
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Ubuntu 20.04&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt-get update &amp;amp;&amp;amp; \
    sudo apt-get install -y software-properties-common

# Fetch Python 3.12
sudo add-apt-repository ppa:deadsnakes/ppa &amp;amp;&amp;amp; sudo apt update
sudo apt install python3.12 python3.12-venv python3.12-dev -y

# Create the virtual environment
python3.12 -m venv cai_env

# Install the package from the local directory
source cai_env/bin/activate &amp;amp;&amp;amp; pip install cai-framework

# Generate a .env file and set up with defaults
echo -e 'OPENAI_API_KEY="sk-1234"\nANTHROPIC_API_KEY=""\nOLLAMA=""\nPROMPT_TOOLKIT_NO_CPR=1\nCAI_STREAM=false' &amp;gt; .env

# Launch CAI
cai  # first launch it can take up to 30 seconds
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Windows WSL&lt;/h3&gt; 
&lt;p&gt;Go to the Microsoft page: &lt;a href="https://learn.microsoft.com/en-us/windows/wsl/install"&gt;https://learn.microsoft.com/en-us/windows/wsl/install&lt;/a&gt;. Here you will find all the instructions to install WSL&lt;/p&gt; 
&lt;p&gt;From Powershell write: wsl --install&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;
sudo apt-get update &amp;amp;&amp;amp; \
    sudo apt-get install -y git python3-pip python3-venv

# Create the virtual environment
python3 -m venv cai_env

# Install the package from the local directory
source cai_env/bin/activate &amp;amp;&amp;amp; pip install cai-framework

# Generate a .env file and set up with defaults
echo -e 'OPENAI_API_KEY="sk-1234"\nANTHROPIC_API_KEY=""\nOLLAMA=""\nPROMPT_TOOLKIT_NO_CPR=1\nCAI_STREAM=false' &amp;gt; .env

# Launch CAI
cai  # first launch it can take up to 30 seconds
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Android&lt;/h3&gt; 
&lt;p&gt;We recommend having at least 8 GB of RAM:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;First of all, install userland &lt;a href="https://play.google.com/store/apps/details?id=tech.ula&amp;amp;hl=es"&gt;https://play.google.com/store/apps/details?id=tech.ula&amp;amp;hl=es&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install Kali minimal in basic options (for free). [Or any other kali option if preferred]&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Update apt keys like in this example: &lt;a href="https://superuser.com/questions/1644520/apt-get-update-issue-in-kali"&gt;https://superuser.com/questions/1644520/apt-get-update-issue-in-kali&lt;/a&gt;, inside UserLand's Kali terminal execute&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Get new apt keys
wget http://http.kali.org/kali/pool/main/k/kali-archive-keyring/kali-archive-keyring_2024.1_all.deb

# Install new apt keys
sudo dpkg -i kali-archive-keyring_2024.1_all.deb &amp;amp;&amp;amp; rm kali-archive-keyring_2024.1_all.deb

# Update APT repository
sudo apt-get update

# CAI requieres python 3.12, lets install it (CAI for kali in Android)
sudo apt-get update &amp;amp;&amp;amp; sudo apt-get install -y git python3-pip build-essential zlib1g-dev libncurses5-dev libgdbm-dev libnss3-dev libssl-dev libreadline-dev libffi-dev libsqlite3-dev wget libbz2-dev pkg-config
wget https://www.python.org/ftp/python/3.12.4/Python-3.12.4.tar.xz
tar xf Python-3.12.4.tar.xz
cd ./configure --enable-optimizations
sudo make altinstall # This command takes long to execute

# Clone CAI's source code
git clone https://github.com/aliasrobotics/cai &amp;amp;&amp;amp; cd cai

# Create virtual environment
python3.12 -m venv cai_env

# Install the package from the local directory
source cai_env/bin/activate &amp;amp;&amp;amp; pip3 install -e .

# Generate a .env file and set up
cp .env.example .env  # edit here your keys/models

# Launch CAI
cai
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;&lt;span&gt;ğŸ”©&lt;/span&gt; Setup &lt;code&gt;.env&lt;/code&gt; file&lt;/h3&gt; 
&lt;p&gt;CAI leverages the &lt;code&gt;.env&lt;/code&gt; file to load configuration at launch. To facilitate the setup, the repo provides an exemplary &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/.env.example"&gt;&lt;code&gt;.env.example&lt;/code&gt;&lt;/a&gt; file provides a template for configuring CAI's setup and your LLM API keys to work with desired LLM models.&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;âš &lt;/span&gt; Important:&lt;/p&gt; 
&lt;p&gt;CAI does NOT provide API keys for any model by default. Don't ask us to provide keys, use your own or host your own models.&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;âš &lt;/span&gt; Note:&lt;/p&gt; 
&lt;p&gt;The OPENAI_API_KEY must not be left blank. It should contain either "sk-123" (as a placeholder) or your actual API key. See &lt;a href="https://github.com/aliasrobotics/cai/issues/27"&gt;https://github.com/aliasrobotics/cai/issues/27&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;âš &lt;/span&gt; Note:&lt;/p&gt; 
&lt;p&gt;If you are using alias1 model, make sure that CAI is &amp;gt;0.4.0 version and here you have an .env example to be able to use it.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;OPENAI_API_KEY="sk-1234"
OLLAMA=""
ALIAS_API_KEY="&amp;lt;sk-your-key&amp;gt;"  # note, add yours
CAI_STEAM=False
CAI_MODEL="alias1"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;ğŸ”¹ Custom OpenAI Base URL Support&lt;/h3&gt; 
&lt;p&gt;CAI supports configuring a custom OpenAI API base URL via the &lt;code&gt;OPENAI_BASE_URL&lt;/code&gt; environment variable. This allows users to redirect API calls to a custom endpoint, such as a proxy or self-hosted OpenAI-compatible service.&lt;/p&gt; 
&lt;p&gt;Example &lt;code&gt;.env&lt;/code&gt; entry configuration:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;OLLAMA_API_BASE="https://custom-openai-proxy.com/v1"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or directly from the command line:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;OLLAMA_API_BASE="https://custom-openai-proxy.com/v1" cai
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;&lt;span&gt;ğŸ“&lt;/span&gt; Architecture:&lt;/h2&gt; 
&lt;p&gt;CAI focuses on making cybersecurity agent &lt;strong&gt;coordination&lt;/strong&gt; and &lt;strong&gt;execution&lt;/strong&gt; lightweight, highly controllable, and useful for humans. To do so it builds upon 8 pillars: &lt;code&gt;Agent&lt;/code&gt;s, &lt;code&gt;Tools&lt;/code&gt;, &lt;code&gt;Handoffs&lt;/code&gt;, &lt;code&gt;Patterns&lt;/code&gt;, &lt;code&gt;Turns&lt;/code&gt;, &lt;code&gt;Tracing&lt;/code&gt;, &lt;code&gt;Guardrails&lt;/code&gt; and &lt;code&gt;HITL&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚      HITL     â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   Turns   â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Patterns â”‚â—€â”€â”€â”€â”€â”€â–¶â”‚  Handoffs â”‚â—€â”€â”€â”€â”€â–¶ â”‚   Agents  â”‚â—€â”€â”€â”€â”€â–¶â”‚    LLMs   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚                   â”‚
                          â”‚                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Extensions â”‚â—€â”€â”€â”€â”€â”€â–¶â”‚  Tracing  â”‚       â”‚   Tools   â”‚â—€â”€â”€â”€â–¶â”‚ Guardrails â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â–¼             â–¼          â–¼             â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ LinuxCmd  â”‚â”‚ WebSearch â”‚â”‚    Code    â”‚â”‚ SSHTunnel â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you want to dive deeper into the code, check the following files as a start point for using CAI:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/raw/main/src/cai/__init__.py"&gt;&lt;strong&gt;init&lt;/strong&gt;.py&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/raw/main/src/cai/cli.py"&gt;cli.py&lt;/a&gt; - entrypoint for command line interface&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/raw/main/src/cai/util.py"&gt;util.py&lt;/a&gt; - utility functions&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/raw/main/src/cai/agents"&gt;agents&lt;/a&gt; - Agent implementations&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/raw/main/src/cai/internal"&gt;internal&lt;/a&gt; - CAI internal functions (endpoints, metrics, logging, etc.)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/raw/main/src/cai/prompts"&gt;prompts&lt;/a&gt; - Agent Prompt Database&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/raw/main/src/cai/repl"&gt;repl&lt;/a&gt; - CLI aesthetics and commands&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/raw/main/src/cai/sdk"&gt;sdk&lt;/a&gt; - CAI command sdk&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/tree/main/src/cai/tools"&gt;tools&lt;/a&gt; - agent tools&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ”¹ Agent&lt;/h3&gt; 
&lt;p&gt;At its core, CAI abstracts its cybersecurity behavior via &lt;code&gt;Agents&lt;/code&gt; and agentic &lt;code&gt;Patterns&lt;/code&gt;. An Agent in &lt;em&gt;an intelligent system that interacts with some environment&lt;/em&gt;. More technically, within CAI we embrace a robotics-centric definition wherein an agent is anything that can be viewed as a system perceiving its environment through sensors, reasoning about its goals and and acting accordingly upon that environment through actuators (&lt;em&gt;adapted&lt;/em&gt; from Russel &amp;amp; Norvig, AI: A Modern Approach). In cybersecurity, an &lt;code&gt;Agent&lt;/code&gt; interacts with systems and networks, using peripherals and network interfaces as sensors, reasons accordingly and then executes network actions as if actuators. Correspondingly, in CAI, &lt;code&gt;Agent&lt;/code&gt;s implement the &lt;code&gt;ReACT&lt;/code&gt; (Reasoning and Action) agent model[^3]. For more information, see the &lt;a href="https://github.com/aliasrobotics/cai/raw/main/examples/basic/hello_world.py"&gt;example here&lt;/a&gt; for the full execution code, and refer to this &lt;a href="https://github.com/aliasrobotics/cai/raw/main/fluency/my-first-hack/my_first_hack.ipynb"&gt;jupyter notebook&lt;/a&gt; for a tutorial on how to use it.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from cai.sdk.agents import Agent, Runner, OpenAIChatCompletionsModel

import os
from openai import AsyncOpenAI
from dotenv import load_dotenv
load_dotenv()

agent = Agent(
      name="Custom Agent",
      instructions="""You are a Cybersecurity expert Leader""",
      model=OpenAIChatCompletionsModel(
          model=os.getenv('CAI_MODEL', "openai/gpt-4o"),
          openai_client=AsyncOpenAI(),
          )
      )

message = "Tell me about recursion in programming."
result = await Runner.run(agent, message)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;ğŸ”¹ Tools&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;Tools&lt;/code&gt; let cybersecurity agents take actions by providing interfaces to execute system commands, run security scans, analyze vulnerabilities, and interact with target systems and APIs - they are the core capabilities that enable CAI agents to perform security tasks effectively; in CAI, tools include built-in cybersecurity utilities (like LinuxCmd for command execution, WebSearch for OSINT gathering, Code for dynamic script execution, and SSHTunnel for secure remote access), function calling mechanisms that allow integration of any Python function as a security tool, and agent-as-tool functionality that enables specialized security agents (such as reconnaissance or exploit agents) to be used by other agents, creating powerful collaborative security workflows without requiring formal handoffs between agents. For more information, please refer to the &lt;a href="https://github.com/aliasrobotics/cai/raw/main/examples/basic/tools.py"&gt;example here&lt;/a&gt; for the complete configuration of custom functions.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from cai.sdk.agents import Agent, Runner, OpenAIChatCompletionsModel
from cai.tools.reconnaissance.exec_code import execute_code
from cai.tools.reconnaissance.generic_linux_command import generic_linux_command

import os
from openai import AsyncOpenAI
from dotenv import load_dotenv
load_dotenv()

agent = Agent(
      name="Custom Agent",
      instructions="""You are a Cybersecurity expert Leader""",
      tools= [
        generic_linux_command,
        execute_code
      ],
      model=OpenAIChatCompletionsModel(
          model=os.getenv('CAI_MODEL', "openai/gpt-4o"),
          openai_client=AsyncOpenAI(),
          )
      )

message = "Tell me about recursion in programming."
result = await Runner.run(agent, message)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You may find different &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/tools"&gt;tools&lt;/a&gt;. They are grouped in 6 major categories inspired by the security kill chain [^2]:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Reconnaissance and weaponization - &lt;em&gt;reconnaissance&lt;/em&gt; (crypto, listing, etc)&lt;/li&gt; 
 &lt;li&gt;Exploitation - &lt;em&gt;exploitation&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;Privilege escalation - &lt;em&gt;escalation&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;Lateral movement - &lt;em&gt;lateral&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;Data exfiltration - &lt;em&gt;exfiltration&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;Command and control - &lt;em&gt;control&lt;/em&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;ğŸ”¹ Handoffs&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;Handoffs&lt;/code&gt; allow an &lt;code&gt;Agent&lt;/code&gt; to delegate tasks to another agent, which is crucial in cybersecurity operations where specialized expertise is needed for different phases of an engagement. In our framework, &lt;code&gt;Handoffs&lt;/code&gt; are implemented as tools for the LLM, where a &lt;strong&gt;handoff/transfer function&lt;/strong&gt; like &lt;code&gt;transfer_to_flag_discriminator&lt;/code&gt; enables the &lt;code&gt;ctf_agent&lt;/code&gt; to pass control to the &lt;code&gt;flag_discriminator_agent&lt;/code&gt; once it believes it has found the flag. This creates a security validation chain where the first agent handles exploitation and flag discovery, while the second agent specializes in flag verification, ensuring proper segregation of duties and leveraging specialized capabilities of different models for distinct security tasks. For more information, please refer to the &lt;a href="https://github.com/aliasrobotics/cai/raw/main/examples/cai/agent_patterns/handoffs.py"&gt;example here&lt;/a&gt; for the full execution code.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from cai.sdk.agents import function_tool
from cai.tools.common import run_command
from cai.sdk.agents import Agent, OpenAIChatCompletionsModel, HandoffInputData, Runner, function_tool, handoff, trace
from cai.sdk.agents.extensions import handoff_filters

import os
from openai import AsyncOpenAI
from dotenv import load_dotenv
load_dotenv()

@function_tool
def execute_cli_command(command: str) -&amp;gt; str:
    return run_command(command)

flag_discriminator = Agent(
    name="Flag discriminator",
    description="Agent focused on extracting the flag from the output",
    instructions="You are an agent tailored to extract the flag from a given output.",
    model=OpenAIChatCompletionsModel(
        model=os.getenv('CAI_MODEL', "qwen2.5:14b"),
        openai_client=AsyncOpenAI(),
    ) 
)

ctf_agent = Agent(
    name="CTF agent",
    description="Agent focused on conquering security challenges",
    instructions="You are a Cybersecurity expert Leader facing a CTF",
    tools=[
        execute_cli_command,
    ],
    model=OpenAIChatCompletionsModel(
        model= os.getenv('CAI_MODEL', "qwen2.5:14b"),
        openai_client=AsyncOpenAI(),
    ), 
    handoffs = [flag_discriminator]
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;ğŸ”¹ Patterns&lt;/h3&gt; 
&lt;p&gt;An agentic &lt;code&gt;Pattern&lt;/code&gt; is a &lt;em&gt;structured design paradigm&lt;/em&gt; in artificial intelligence systems where autonomous or semi-autonomous agents operate within a defined &lt;em&gt;interaction framework&lt;/em&gt; (the pattern) to achieve a goal. These &lt;code&gt;Patterns&lt;/code&gt; specify the organization, coordination, and communication methods among agents, guiding decision-making, task execution, and delegation.&lt;/p&gt; 
&lt;p&gt;An agentic pattern (&lt;code&gt;AP&lt;/code&gt;) can be formally defined as a tuple:&lt;/p&gt; 
&lt;p&gt;\[ AP = (A, H, D, C, E) \]&lt;/p&gt; 
&lt;p&gt;wherein:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;\(A\) (Agents):&lt;/strong&gt; A set of autonomous entities, \( A = \{a_1, a_2, ..., a_n\} \), each with defined roles, capabilities, and internal states.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;\(H\) (Handoffs):&lt;/strong&gt; A function \( H: A \times T \to A \) that governs how tasks \( T \) are transferred between agents based on predefined logic (e.g., rules, negotiation, bidding).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;\(D\) (Decision Mechanism):&lt;/strong&gt; A decision function \( D: S \to A \) where \( S \) represents system states, and \( D \) determines which agent takes action at any given time.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;\(C\) (Communication Protocol):&lt;/strong&gt; A messaging function \( C: A \times A \to M \), where \( M \) is a message space, defining how agents share information.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;\(E\) (Execution Model):&lt;/strong&gt; A function \( E: A \times I \to O \) where \( I \) is the input space and \( O \) is the output space, defining how agents perform tasks.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;When building &lt;code&gt;Patterns&lt;/code&gt;, we generall y classify them among one of the following categories, though others exist:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;strong&gt;Agentic&lt;/strong&gt; &lt;code&gt;Pattern&lt;/code&gt; &lt;strong&gt;categories&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Swarm&lt;/code&gt; (Decentralized)&lt;/td&gt; 
   &lt;td&gt;Agents share tasks and self-assign responsibilities without a central orchestrator. Handoffs occur dynamically. &lt;em&gt;An example of a peer-to-peer agentic pattern is the &lt;code&gt;CTF Agentic Pattern&lt;/code&gt;, which involves a team of agents working together to solve a CTF challenge with dynamic handoffs.&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Hierarchical&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;A top-level agent (e.g., "PlannerAgent") assigns tasks via structured handoffs to specialized sub-agents. Alternatively, the structure of the agents is harcoded into the agentic pattern with pre-defined handoffs.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Chain-of-Thought&lt;/code&gt; (Sequential Workflow)&lt;/td&gt; 
   &lt;td&gt;A structured pipeline where Agent A produces an output, hands it to Agent B for reuse or refinement, and so on. Handoffs follow a linear sequence. &lt;em&gt;An example of a chain-of-thought agentic pattern is the &lt;code&gt;ReasonerAgent&lt;/code&gt;, which involves a Reasoning-type LLM that provides context to the main agent to solve a CTF challenge with a linear sequence.&lt;/em&gt;[^1]&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Auction-Based&lt;/code&gt; (Competitive Allocation)&lt;/td&gt; 
   &lt;td&gt;Agents "bid" on tasks based on priority, capability, or cost. A decision agent evaluates bids and hands off tasks to the best-fit agent.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Recursive&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;A single agent continuously refines its own output, treating itself as both executor and evaluator, with handoffs (internal or external) to itself. &lt;em&gt;An example of a recursive agentic pattern is the &lt;code&gt;CodeAgent&lt;/code&gt; (when used as a recursive agent), which continuously refines its own output by executing code and updating its own instructions.&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;For more information and examples of common agentic patterns, see the &lt;a href="https://github.com/aliasrobotics/cai/raw/main/examples/agent_patterns/README.md"&gt;examples folder&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;ğŸ”¹ Turns and Interactions&lt;/h3&gt; 
&lt;p&gt;During the agentic flow (conversation), we distinguish between &lt;strong&gt;interactions&lt;/strong&gt; and &lt;strong&gt;turns&lt;/strong&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Interactions&lt;/strong&gt; are sequential exchanges between one or multiple agents. Each agent executing its logic corresponds with one &lt;em&gt;interaction&lt;/em&gt;. Since an &lt;code&gt;Agent&lt;/code&gt; in CAI generally implements the &lt;code&gt;ReACT&lt;/code&gt; agent model[^3], each &lt;em&gt;interaction&lt;/em&gt; consists of 1) a reasoning step via an LLM inference and 2) act by calling zero-to-n &lt;code&gt;Tools&lt;/code&gt;. This is defined in&lt;code&gt;process_interaction()&lt;/code&gt; in &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/cai/core.py"&gt;core.py&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Turns&lt;/strong&gt;: A turn represents a cycle of one ore more &lt;strong&gt;interactions&lt;/strong&gt; which finishes when the &lt;code&gt;Agent&lt;/code&gt; (or &lt;code&gt;Pattern&lt;/code&gt;) executing returns &lt;code&gt;None&lt;/code&gt;, judging there're no further actions to undertake. This is defined in &lt;code&gt;run()&lt;/code&gt;, see &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/cai/core.py"&gt;core.py&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] CAI Agents are not related to Assistants in the Assistants API. They are named similarly for convenience, but are otherwise completely unrelated. CAI is entirely powered by the Chat Completions API and is hence stateless between calls.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;ğŸ”¹ Tracing&lt;/h3&gt; 
&lt;p&gt;CAI implements AI observability by adopting the OpenTelemetry standard and to do so, it leverages &lt;a href="https://github.com/Arize-ai/phoenix"&gt;Phoenix&lt;/a&gt; which provides comprehensive tracing capabilities through OpenTelemetry-based instrumentation, allowing you to monitor and analyze your security operations in real-time. This integration enables detailed visibility into agent interactions, tool usage, and attack vectors throughout penetration testing workflows, making it easier to debug complex exploitation chains, track vulnerability discovery processes, and optimize agent performance for more effective security assessments.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/media/tracing.png" alt="" /&gt;&lt;/p&gt; 
&lt;h3&gt;ğŸ”¹ Guardrails&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;Guardrails&lt;/code&gt; provide a critical security layer for CAI agents, protecting against prompt injection attacks and preventing execution of dangerous commands. These guardrails run in parallel to agents, validating both input and output to ensure safe operation. The framework includes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Input Guardrails&lt;/strong&gt;: Detect and block prompt injection attempts before they reach agents, using pattern matching, Unicode homograph detection, and AI-powered analysis&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Output Guardrails&lt;/strong&gt;: Validate agent outputs before execution, preventing dangerous commands like reverse shells, fork bombs, or data exfiltration&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-layered Defense&lt;/strong&gt;: Protection at input, processing, and execution stages with tool-level validation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Base64/Base32 Aware&lt;/strong&gt;: Automatically decodes and analyzes encoded payloads to detect hidden malicious commands&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Configurable&lt;/strong&gt;: Can be enabled/disabled via &lt;code&gt;CAI_GUARDRAILS&lt;/code&gt; environment variable&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For detailed implementation, see &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/docs/guardrails.md"&gt;docs/guardrails.md&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/docs/cai_prompt_injection.md"&gt;docs/cai_prompt_injection.md&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;ğŸ”¹ Human-In-The-Loop (HITL)&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚                                 â”‚
                      â”‚      Cybersecurity AI (CAI)     â”‚
                      â”‚                                 â”‚
                      â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
                      â”‚       â”‚  Autonomous AI  â”‚       â”‚
                      â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
                      â”‚                â”‚                â”‚
                      â”‚                â”‚                â”‚
                      â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
                      â”‚       â”‚ HITL Interaction â”‚      â”‚
                      â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
                      â”‚                â”‚                â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                                       â”‚ Ctrl+C (cli.py)
                                       â”‚
                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                           â”‚   Human Operator(s)   â”‚
                           â”‚  Expertise | Judgment â”‚
                           â”‚    Teleoperation      â”‚
                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;CAI delivers a framework for building Cybersecurity AIs with a strong emphasis on &lt;em&gt;semi-autonomous&lt;/em&gt; operation, as the reality is that &lt;strong&gt;fully-autonomous&lt;/strong&gt; cybersecurity systems remain premature and face significant challenges when tackling complex tasks. While CAI explores autonomous capabilities, we recognize that effective security operations still require human teleoperation providing expertise, judgment, and oversight in the security process.&lt;/p&gt; 
&lt;p&gt;Accordingly, the Human-In-The-Loop (&lt;code&gt;HITL&lt;/code&gt;) module is a core design principle of CAI, acknowledging that human intervention and teleoperation are essential components of responsible security testing. Through the &lt;code&gt;cli.py&lt;/code&gt; interface, users can seamlessly interact with agents at any point during execution by simply pressing &lt;code&gt;Ctrl+C&lt;/code&gt;. This is implemented across &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/cai/core.py"&gt;core.py&lt;/a&gt; and also in the REPL abstractions &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/cai/repl"&gt;REPL&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;&lt;span&gt;ğŸš€&lt;/span&gt; Quickstart&lt;/h2&gt; 
&lt;p&gt;To start CAI after installing it, just type &lt;code&gt;cai&lt;/code&gt; in the CLI:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;â””â”€# cai

          CCCCCCCCCCCCC      ++++++++   ++++++++      IIIIIIIIII
       CCC::::::::::::C  ++++++++++       ++++++++++  I::::::::I
     CC:::::::::::::::C ++++++++++         ++++++++++ I::::::::I
    C:::::CCCCCCCC::::C +++++++++    ++     +++++++++ II::::::II
   C:::::C       CCCCCC +++++++     +++++     +++++++   I::::I
  C:::::C                +++++     +++++++     +++++    I::::I
  C:::::C                ++++                   ++++    I::::I
  C:::::C                 ++                     ++     I::::I
  C:::::C                  +   +++++++++++++++   +      I::::I
  C:::::C                    +++++++++++++++++++        I::::I
  C:::::C                     +++++++++++++++++         I::::I
   C:::::C       CCCCCC        +++++++++++++++          I::::I
    C:::::CCCCCCCC::::C         +++++++++++++         II::::::II
     CC:::::::::::::::C           +++++++++           I::::::::I
       CCC::::::::::::C             +++++             I::::::::I
          CCCCCCCCCCCCC               ++              IIIIIIIIII

                      Cybersecurity AI (CAI), vX.Y.Z
                          Bug bounty-ready AI

CAI&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;That should initialize CAI and provide a prompt to execute any security task you want to perform. The navigation bar at the bottom displays important system information. This information helps you understand your environment while working with CAI.&lt;/p&gt; 
&lt;p&gt;Here's a quick &lt;a href="https://asciinema.org/a/zm7wS5DA2o0S9pu1Tb44pnlvy"&gt;demo video&lt;/a&gt; to help you get started with CAI. We'll walk through the basic steps â€” from launching the tool to running your first AI-powered task in the terminal. Whether you're a beginner or just curious, this guide will show you how easy it is to begin using CAI.&lt;/p&gt; 
&lt;p&gt;From here on, type on &lt;code&gt;CAI&lt;/code&gt; and start your security exercise. Best way to learn is by example:&lt;/p&gt; 
&lt;h3&gt;Environment Variables&lt;/h3&gt; 
&lt;p&gt;For using private models, you are given a &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/.env.example"&gt;&lt;code&gt;.env.example&lt;/code&gt;&lt;/a&gt; file. Copy it and rename it as &lt;code&gt;.env&lt;/code&gt;. Fill in your corresponding API keys, and you are ready to use CAI.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;List of Environment Variables&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Variable&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CTF_NAME&lt;/td&gt; 
    &lt;td&gt;Name of the CTF challenge to run (e.g. "picoctf_static_flag")&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CTF_CHALLENGE&lt;/td&gt; 
    &lt;td&gt;Specific challenge name within the CTF to test&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CTF_SUBNET&lt;/td&gt; 
    &lt;td&gt;Network subnet for the CTF container&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CTF_IP&lt;/td&gt; 
    &lt;td&gt;IP address for the CTF container&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CTF_INSIDE&lt;/td&gt; 
    &lt;td&gt;Whether to conquer the CTF from within container&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_MODEL&lt;/td&gt; 
    &lt;td&gt;Model to use for agents&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_DEBUG&lt;/td&gt; 
    &lt;td&gt;Set debug output level (0: Only tool outputs, 1: Verbose debug output, 2: CLI debug output)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_BRIEF&lt;/td&gt; 
    &lt;td&gt;Enable/disable brief output mode&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_MAX_TURNS&lt;/td&gt; 
    &lt;td&gt;Maximum number of turns for agent interactions&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_TRACING&lt;/td&gt; 
    &lt;td&gt;Enable/disable OpenTelemetry tracing&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_AGENT_TYPE&lt;/td&gt; 
    &lt;td&gt;Specify the agents to use (boot2root, one_tool...)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_STATE&lt;/td&gt; 
    &lt;td&gt;Enable/disable stateful mode&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_MEMORY&lt;/td&gt; 
    &lt;td&gt;Enable/disable memory mode (episodic, semantic, all)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_MEMORY_ONLINE&lt;/td&gt; 
    &lt;td&gt;Enable/disable online memory mode&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_MEMORY_OFFLINE&lt;/td&gt; 
    &lt;td&gt;Enable/disable offline memory&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_ENV_CONTEXT&lt;/td&gt; 
    &lt;td&gt;Add dirs and current env to llm context&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_MEMORY_ONLINE_INTERVAL&lt;/td&gt; 
    &lt;td&gt;Number of turns between online memory updates&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_PRICE_LIMIT&lt;/td&gt; 
    &lt;td&gt;Price limit for the conversation in dollars&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_REPORT&lt;/td&gt; 
    &lt;td&gt;Enable/disable reporter mode (ctf, nis2, pentesting)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_SUPPORT_MODEL&lt;/td&gt; 
    &lt;td&gt;Model to use for the support agent&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_SUPPORT_INTERVAL&lt;/td&gt; 
    &lt;td&gt;Number of turns between support agent executions&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_WORKSPACE&lt;/td&gt; 
    &lt;td&gt;Defines the name of the workspace&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_WORKSPACE_DIR&lt;/td&gt; 
    &lt;td&gt;Specifies the directory path where the workspace is located&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_GUARDRAILS&lt;/td&gt; 
    &lt;td&gt;Enable/disable guardrails for prompt injection protection (default: true)&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h3&gt;OpenRouter Integration&lt;/h3&gt; 
&lt;p&gt;The Cybersecurity AI (CAI) platform offers seamless integration with OpenRouter, a unified interface for Large Language Models (LLMs). This integration is crucial for users who wish to leverage advanced AI capabilities in their cybersecurity tasks. OpenRouter acts as a bridge, allowing CAI to communicate with various LLMs, thereby enhancing the flexibility and power of the AI agents used within CAI.&lt;/p&gt; 
&lt;p&gt;To enable OpenRouter support in CAI, you need to configure your environment by adding specific entries to your &lt;code&gt;.env&lt;/code&gt; file. This setup ensures that CAI can interact with the OpenRouter API, facilitating the use of sophisticated models like Meta-LLaMA. Hereâ€™s how you can configure it:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CAI_AGENT_TYPE=redteam_agent
CAI_MODEL=openrouter/meta-llama/llama-4-maverick
OPENROUTER_API_KEY=&amp;lt;sk-your-key&amp;gt;  # note, add yours
OPENROUTER_API_BASE=https://openrouter.ai/api/v1
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Azure OpenAI&lt;/h3&gt; 
&lt;p&gt;The Cybersecurity AI (CAI) platform integrates seamlessly with Azure OpenAI, enabling organizations to run CAI against enterprise-hosted models (e.g., gpt-4o). This pathway is ideal for teams that must operate within Azure governance while leveraging advanced model capabilities. To enable Azure OpenAI support in CAI, configure your environment by adding the following entries to your .env. This ensures CAI can reach your Azure deployment endpoint and authenticate correctly.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CAI_AGENT_TYPE=redteam_agent
CAI_MODEL=azure/&amp;lt;model-name-deployed&amp;gt;
# Required: keep non-empty even when using Azure
OPENAI_API_KEY=dummy
# Azure credentials and endpoint
AZURE_API_KEY=&amp;lt;your-azure-openai-key&amp;gt;
AZURE_API_BASE=https://&amp;lt;resource&amp;gt;.openai.azure.com/openai/deployments/&amp;lt;deployment-name&amp;gt;/chat/completions?api-version=2025-01-01-preview
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;MCP&lt;/h3&gt; 
&lt;p&gt;CAI supports the Model Context Protocol (MCP) for integrating external tools and services with AI agents. MCP is supported via two transport mechanisms:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;SSE (Server-Sent Events)&lt;/strong&gt; - For web-based servers that push updates over HTTP connections:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CAI&amp;gt;/mcp load http://localhost:9876/sse burp
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;&lt;strong&gt;STDIO (Standard Input/Output)&lt;/strong&gt; - For local inter-process communication:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CAI&amp;gt;/mcp load stdio myserver python mcp_server.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Once connected, you can add the MCP tools to any agent:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CAI&amp;gt;/mcp add burp redteam_agent
Adding tools from MCP server 'burp' to agent 'Red Team Agent'...
                                 Adding tools to Red Team Agent
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Tool                              â”ƒ Status â”ƒ Details                                         â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ send_http_request                 â”‚ Added  â”‚ Available as: send_http_request                 â”‚
â”‚ create_repeater_tab               â”‚ Added  â”‚ Available as: create_repeater_tab               â”‚
â”‚ send_to_intruder                  â”‚ Added  â”‚ Available as: send_to_intruder                  â”‚
â”‚ url_encode                        â”‚ Added  â”‚ Available as: url_encode                        â”‚
â”‚ url_decode                        â”‚ Added  â”‚ Available as: url_decode                        â”‚
â”‚ base64encode                      â”‚ Added  â”‚ Available as: base64encode                      â”‚
â”‚ base64decode                      â”‚ Added  â”‚ Available as: base64decode                      â”‚
â”‚ generate_random_string            â”‚ Added  â”‚ Available as: generate_random_string            â”‚
â”‚ output_project_options            â”‚ Added  â”‚ Available as: output_project_options            â”‚
â”‚ output_user_options               â”‚ Added  â”‚ Available as: output_user_options               â”‚
â”‚ set_project_options               â”‚ Added  â”‚ Available as: set_project_options               â”‚
â”‚ set_user_options                  â”‚ Added  â”‚ Available as: set_user_options                  â”‚
â”‚ get_proxy_http_history            â”‚ Added  â”‚ Available as: get_proxy_http_history            â”‚
â”‚ get_proxy_http_history_regex      â”‚ Added  â”‚ Available as: get_proxy_http_history_regex      â”‚
â”‚ get_proxy_websocket_history       â”‚ Added  â”‚ Available as: get_proxy_websocket_history       â”‚
â”‚ get_proxy_websocket_history_regex â”‚ Added  â”‚ Available as: get_proxy_websocket_history_regex â”‚
â”‚ set_task_execution_engine_state   â”‚ Added  â”‚ Available as: set_task_execution_engine_state   â”‚
â”‚ set_proxy_intercept_state         â”‚ Added  â”‚ Available as: set_proxy_intercept_state         â”‚
â”‚ get_active_editor_contents        â”‚ Added  â”‚ Available as: get_active_editor_contents        â”‚
â”‚ set_active_editor_contents        â”‚ Added  â”‚ Available as: set_active_editor_contents        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Added 20 tools from server 'burp' to agent 'Red Team Agent'.
CAI&amp;gt;/agent 13
CAI&amp;gt;Create a repeater tab
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can list all active MCP connections and their transport types:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CAI&amp;gt;/mcp list
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/386a1fd3-3469-4f84-9396-2a5236febe1f"&gt;https://github.com/user-attachments/assets/386a1fd3-3469-4f84-9396-2a5236febe1f&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Development&lt;/h2&gt; 
&lt;p&gt;Development is facilitated via VS Code dev. environments. To try out our development environment, clone the repository, open VS Code and enter de dev. container mode:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/media/cai_devenv.gif" alt="CAI Development Environment" /&gt;&lt;/p&gt; 
&lt;h3&gt;Contributions&lt;/h3&gt; 
&lt;p&gt;If you want to contribute to this project, use &lt;a href="https://pre-commit.com/"&gt;&lt;strong&gt;Pre-commit&lt;/strong&gt;&lt;/a&gt; before your MR&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install pre-commit
pre-commit # files staged
pre-commit run --all-files # all files
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Optional Requirements: caiextensions&lt;/h3&gt; 
&lt;p&gt;Currently, the extensions are not publicly available as the engineering endeavour to maintain them is significant. Instead, we're making selected custom caiextensions available for partner companies across collaborations.&lt;/p&gt; 
&lt;h3&gt;&lt;span&gt;â„¹&lt;/span&gt; Usage Data Collection&lt;/h3&gt; 
&lt;p&gt;CAI is provided free of charge for researchers. To improve CAIâ€™s detection accuracy and publish open security research, instead of payment for research use cases, we ask you to contribute to the CAI community by allowing usage data collection. This data helps us identify areas for improvement, understand how the framework is being used, and prioritize new features. Legal basis of data collection is under Art. 6 (1)(f) GDPR â€” CAIâ€™s legitimate interest in maintaining and improving security tooling, with Art. 89 safeguards for research. The collected data includes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Basic system information (OS type, Python version)&lt;/li&gt; 
 &lt;li&gt;Username and IP information&lt;/li&gt; 
 &lt;li&gt;Tool usage patterns and performance metrics&lt;/li&gt; 
 &lt;li&gt;Model interactions and token usage statistics&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We take your privacy seriously and only collect what's needed to make CAI better. For further info, reach out to researchï¼ aliasrobotics.com. You can disable some of the data collection features via the &lt;code&gt;CAI_TELEMETRY&lt;/code&gt; environment variable but we encourage you to keep it enabled and contribute back to research:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CAI_TELEMETRY=False cai
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Reproduce CI-Setup locally&lt;/h3&gt; 
&lt;p&gt;To simulate the CI/CD pipeline, you can run the following in the Gitlab runner machines:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run --rm -it \
  --privileged \
  --network=exploitflow_net \
  --add-host="host.docker.internal:host-gateway" \
  -v /cache:/cache \
  -v /var/run/docker.sock:/var/run/docker.sock:rw \
  registry.gitlab.com/aliasrobotics/alias_research/cai:latest bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;FAQ&lt;/h2&gt; 
&lt;details&gt;
 &lt;summary&gt;OLLAMA is giving me 404 errors&lt;/summary&gt; 
 &lt;p&gt;Ollama's API in OpenAI mode uses &lt;code&gt;/v1/chat/completions&lt;/code&gt; whereas the &lt;code&gt;openai&lt;/code&gt; library uses &lt;code&gt;base_url&lt;/code&gt; + &lt;code&gt;/chat/completions&lt;/code&gt;.&lt;/p&gt; 
 &lt;p&gt;We adopt the latter for overall alignment with the gen AI community and empower the former by allowing users to add the &lt;code&gt;v1&lt;/code&gt; themselves via:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;OLLAMA_API_BASE=http://IP:PORT/v1
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;See the following issues that treat this topic in more detail:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/issues/76"&gt;https://github.com/aliasrobotics/cai/issues/76&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/issues/83"&gt;https://github.com/aliasrobotics/cai/issues/83&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/issues/82"&gt;https://github.com/aliasrobotics/cai/issues/82&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Where are all the caiextensions?&lt;/summary&gt; 
 &lt;p&gt;See &lt;a href="https://gitlab.com/aliasrobotics/alias_research/caiextensions"&gt;all caiextensions&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;How do I install the report caiextension?&lt;/summary&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#optional-requirements-caiextensions"&gt;See here&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;How do I set up SSH access for Gitlab?&lt;/summary&gt; 
 &lt;p&gt;Generate a new SSH key&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;ssh-keygen -t ed25519
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Add the key to the SSH agent&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;ssh-add ~/.ssh/id_ed25519
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Add the public key to Gitlab Copy the key and add it to Gitlab under &lt;a href="https://gitlab.com/-/user_settings/ssh_keys"&gt;https://gitlab.com/-/user_settings/ssh_keys&lt;/a&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;cat ~/.ssh/id_ed25519.pub
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;To verify it:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;ssh -T git@gitlab.com
Welcome to GitLab, @vmayoral!
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;How do I clear Python cache?&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;find . -name "*.pyc" -delete &amp;amp;&amp;amp; find . -name "__pycache__" -delete
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;If host networking is not working with ollama check whether it has been disabled in Docker because you are not signed in&lt;/summary&gt; 
 &lt;p&gt;Docker in OS X behaves funny sometimes. Check if the following message has shown up:&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Host networking has been disabled because you are not signed in. Please sign in to enable it&lt;/em&gt;.&lt;/p&gt; 
 &lt;p&gt;Make sure this has been addressed and also that the Dev Container is not forwarding the 8000 port (click on x, if necessary in the ports section).&lt;/p&gt; 
 &lt;p&gt;To verify connection, from within the VSCode devcontainer:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;curl -v http://host.docker.internal:8000/api/version
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Run CAI against any target&lt;/summary&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/imgs/readme_imgs/cai-004-first-message.png" alt="cai-004-first-message" /&gt;&lt;/p&gt; 
 &lt;p&gt;The starting user prompt in this case is: &lt;code&gt;Target IP: 192.168.3.10, perform a full network scan&lt;/code&gt;.&lt;/p&gt; 
 &lt;p&gt;The agent started performing a nmap scan. You could either interact with the agent and give it more instructions, or let it run to see what it explores next.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;How do I interact with the agent? Type twice CTRL + C &lt;/summary&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/imgs/readme_imgs/cai-005-ctrl-c.png" alt="cai-005-ctrl-c" /&gt;&lt;/p&gt; 
 &lt;p&gt;If you want to use the HITL mode, you can do it by presssing twice &lt;code&gt;Ctrl + C&lt;/code&gt;. This will allow you to interact (prompt) with the agent whenever you want. The agent will not lose the previous context, as it is stored in the &lt;code&gt;history&lt;/code&gt; variable, which is passed to it and any agent that is called. This enables any agent to use the previous information and be more accurate and efficient.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; Can I change the model while CAI is running? /model &lt;/summary&gt; 
 &lt;p&gt;Use &lt;code&gt;/model&lt;/code&gt; to change the model.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/imgs/readme_imgs/cai-007-model-change.png" alt="cai-007-model-change" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;How can I list all the agents available? /agent &lt;/summary&gt; 
 &lt;p&gt;Use &lt;code&gt;/agent&lt;/code&gt; to list all the agents available.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/imgs/readme_imgs/cai-010-agents-menu.png" alt="cai-010-agents-menu" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; Where can I list all the environment variables? /config &lt;/summary&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/imgs/readme_imgs/cai-008-config.png" alt="cai-008-config" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; How can I monitor context usage and token consumption? /context or /ctx ğŸš€ CAI PRO &lt;/summary&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;strong&gt;âš¡ CAI PRO Exclusive Feature&lt;/strong&gt; The &lt;code&gt;/context&lt;/code&gt; command is available exclusively in &lt;strong&gt;&lt;a href="https://aliasrobotics.com/cybersecurityai.php"&gt;CAI PRO&lt;/a&gt;&lt;/strong&gt;. To access this feature and unlock advanced monitoring capabilities, visit &lt;a href="https://aliasrobotics.com/cybersecurityai.php"&gt;Alias Robotics&lt;/a&gt; for more information.&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;p&gt;Use &lt;code&gt;/context&lt;/code&gt; (or the short form &lt;code&gt;/ctx&lt;/code&gt;) to view your current context window usage and token statistics.&lt;/p&gt; 
 &lt;p&gt;This command displays:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Total context usage (used tokens / max tokens) with percentage&lt;/li&gt; 
  &lt;li&gt;Visual grid representation with the CAI logo showing filled context&lt;/li&gt; 
  &lt;li&gt;Detailed breakdown by category: 
   &lt;ul&gt; 
    &lt;li&gt;System prompt tokens&lt;/li&gt; 
    &lt;li&gt;Tool definitions tokens&lt;/li&gt; 
    &lt;li&gt;Memory/RAG tokens&lt;/li&gt; 
    &lt;li&gt;User prompts tokens&lt;/li&gt; 
    &lt;li&gt;Assistant responses tokens&lt;/li&gt; 
    &lt;li&gt;Tool calls tokens&lt;/li&gt; 
    &lt;li&gt;Tool results tokens&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;Free space available&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;Why this matters&lt;/strong&gt;: Different models have different context limits (e.g., GPT-4: 128k tokens, Claude: 200k tokens). Monitoring your context usage helps you avoid hitting these limits during long conversations, which could cause errors or require conversation truncation.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Show context usage
/context

# Or use the short form
/ctx
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; How to know more about the CLI? /help &lt;/summary&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/imgs/readme_imgs/cai-006-help.png" alt="cai-006-help" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;How can I trace the whole execution?&lt;/summary&gt; The environment variable `CAI_TRACING` allows the user to set it to `CAI_TRACING=true` to enable tracing, or `CAI_TRACING=false` to disable it. When CAI is prompted by the first time, the user is provided with two paths, the execution log, and the tracing log. 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/imgs/readme_imgs/cai-009-logs.png" alt="cai-009-logs" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Can I expand CAI capabilities using previous run logs?&lt;/summary&gt; 
 &lt;p&gt;Yes. Today CAI performs best by relying on Inâ€‘Context Learning (ICL). Rather than building longâ€‘term stores, the recommended workflow is to load relevant prior logs directly into the current session so the model can reason with them in context.&lt;/p&gt; 
 &lt;p&gt;Use the &lt;code&gt;/load&lt;/code&gt; command to bring JSONL logs into CAIâ€™s context (this replaces the legacy memory-loading tool):&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;CAI&amp;gt;/load logs/cai_20250408_111856.jsonl         # Load into current agent
CAI&amp;gt;/load &amp;lt;file&amp;gt; agent &amp;lt;name&amp;gt;                    # Load into a specific agent
CAI&amp;gt;/load &amp;lt;file&amp;gt; all                             # Distribute across all agents
CAI&amp;gt;/load &amp;lt;file&amp;gt; parallel                        # Match to configured parallel agents
# Tip: if you omit &amp;lt;file&amp;gt;, /load uses `logs/last`. Alias: /l
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;CAI prints the path to the current runâ€™s JSONL log at startup (highlighted in orange), which you can pass to &lt;code&gt;/load&lt;/code&gt;:&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/imgs/readme_imgs/cai-009-logs.png" alt="cai-009-logs" /&gt;&lt;/p&gt; 
 &lt;p&gt;Legacy notes: earlier â€œmemory extensionâ€ mechanisms (episodic/semantic stores and offline ingestion) are retained for reference only. See &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/src/cai/agents/memory.py"&gt;src/cai/agents/memory.py&lt;/a&gt; for background and legacy details. Our current direction prioritizes ICL over persistent memory.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Can I expand CAI capabilities using scripts or extra information?&lt;/summary&gt; 
 &lt;p&gt;Currently, CAI supports text based information. You can add any extra information on the target you are facing by copy-pasting it directly into the system or user prompt.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;How?&lt;/strong&gt; By adding it to the system (&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/cai/repl/templates/system_master_template.md"&gt;&lt;code&gt;system_master_template.md&lt;/code&gt;&lt;/a&gt;) or the user prompt (&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/cai/repl/templates/user_master_template.md"&gt;&lt;code&gt;user_master_template.md&lt;/code&gt;&lt;/a&gt;). You can always directly prompt the path to the model, and it will &lt;code&gt;cat&lt;/code&gt; it.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;How CAI licence works?&lt;/summary&gt; 
 &lt;p&gt;CAIâ€™s current license does not restrict usage for research purposes. You are free to use CAI for security assessments (pentests), to develop additional features, and to integrate it into your research activities, as long as you comply with local laws.&lt;/p&gt; 
 &lt;p&gt;If you or your organization start benefiting commercially from CAI (e.g., offering pentesting services powered by CAI), then a commercial license will be required to help sustain the project.&lt;/p&gt; 
 &lt;p&gt;CAI itself is not a profit-seeking initiative. Our goal is to build a sustainable open-source project. We simply ask that those who profit from CAI contribute back and support our ongoing development.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;I get a `Unable to locate package python3.12-venv` when installing the prerequisites on my debian based system!&lt;/summary&gt; 
 &lt;p&gt;The easiest way to get around this is to simply install &lt;a href="https://www.python.org/downloads/release/python-3120/"&gt;&lt;code&gt;python3.12&lt;/code&gt;&lt;/a&gt; from source.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you want to cite our work, please use the following (ordered by publication date):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@article{mayoral2025cai,
  title={CAI: An Open, Bug Bounty-Ready Cybersecurity AI},
  author={Mayoral-Vilches, V{\'\i}ctor and Navarrete-Lozano, Luis Javier and Sanz-G{\'o}mez, Mar{\'\i}a and Espejo, Lidia Salas and Crespo-{\'A}lvarez, Marti{\~n}o and Oca-Gonzalez, Francisco and Balassone, Francesco and Glera-Pic{\'o}n, Alfonso and Ayucar-Carbajo, Unai and Ruiz-Alcalde, Jon Ander and Rass, Stefan and Pinzger, Martin and Gil-Uriarte, Endika},
  journal={arXiv preprint arXiv:2504.06017},
  year={2025}
}

@article{mayoral2025automation,
  title={Cybersecurity AI: The Dangerous Gap Between Automation and Autonomy},
  author={Mayoral-Vilches, V{\'\i}ctor},
  journal={arXiv preprint arXiv:2506.23592},
  year={2025}
}

@article{mayoral2025fluency,
  title={CAI Fluency: A Framework for Cybersecurity AI Fluency},
  author={Mayoral-Vilches, V{\'\i}ctor and Wachter, Jasmin and Chavez, Crist{\'o}bal RJ and Schachner, Cathrin and Navarrete-Lozano, Luis Javier and Sanz-G{\'o}mez, Mar{\'\i}a},
  journal={arXiv preprint arXiv:2508.13588},
  year={2025}
}

@article{mayoral2025hacking,
  title={Cybersecurity AI: Hacking the AI Hackers via Prompt Injection},
  author={Mayoral-Vilches, V{\'\i}ctor and Rynning, Per Mannermaa},
  journal={arXiv preprint arXiv:2508.21669},
  year={2025}
}

@article{mayoral2025humanoid,
  title={Cybersecurity AI: Humanoid Robots as Attack Vectors},
  author={Mayoral-Vilches, V{\'\i}ctor},
  journal={arXiv preprint arXiv:2509.14139},
  year={2025}
}

@article{balassone2025evaluation,
  title={Cybersecurity AI: Evaluating Agentic Cybersecurity in Attack/Defense CTFs},
  author={Balassone, Francesco and Mayoral-Vilches, V{\'\i}ctor and Rass, Stefan and Pinzger, Martin and Perrone, Gaetano and Romano, Simon Pietro and Schartner, Peter},
  journal={arXiv preprint arXiv:2510.17521},
  year={2025}
}

@article{mayoral2025caibench,
  title={CAIBench: A Meta-Benchmark for Evaluating Cybersecurity AI Agents},
  author={Mayoral-Vilches, V{\'\i}ctor and Balassone, Francesco and Navarrete-Lozano, Luis Javier and Sanz-G{\'o}mez, Mar{\'\i}a and Crespo-{\'A}lvarez, Marti{\~n}o and Rass, Stefan and Pinzger, Martin},
  journal={arXiv preprint arXiv:2510.24317},
  year={2025}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;p&gt;CAI was initially developed by &lt;a href="https://aliasrobotics.com"&gt;Alias Robotics&lt;/a&gt; and co-funded by the European EIC accelerator project RIS (GA 101161136) - HORIZON-EIC-2023-ACCELERATOR-01 call. The original agentic principles are inspired from OpenAI's &lt;a href="https://github.com/openai/swarm"&gt;&lt;code&gt;swarm&lt;/code&gt;&lt;/a&gt; library and translated into newer prototypes. This project also makes use of other relevant open source building blocks including &lt;a href="https://github.com/BerriAI/litellm"&gt;&lt;code&gt;LiteLLM&lt;/code&gt;&lt;/a&gt;, and &lt;a href="https://github.com/Arize-ai/phoenix"&gt;&lt;code&gt;phoenix&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Academic Collaborations&lt;/h3&gt; 
&lt;p&gt;CAI benefits from ongoing research collaborations with academic institutions. Researchers interested in collaborative projects, dataset access, or academic licenses should contact &lt;a href="mailto:research@aliasrobotics.com"&gt;research@aliasrobotics.com&lt;/a&gt;. We provide special support for:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;PhD research projects&lt;/li&gt; 
 &lt;li&gt;Academic benchmarking studies&lt;/li&gt; 
 &lt;li&gt;Security education initiatives&lt;/li&gt; 
 &lt;li&gt;Open-source contributions from research labs&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- Footnotes --&gt; 
&lt;p&gt;[^1]: Arguably, the Chain-of-Thought agentic pattern is a special case of the Hierarchical agentic pattern. [^2]: Kamhoua, C. A., Leslie, N. O., &amp;amp; Weisman, M. J. (2018). Game theoretic modeling of advanced persistent threat in internet of things. Journal of Cyber Security and Information Systems. [^3]: Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., &amp;amp; Cao, Y. (2023, January). React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR). [^4]: Deng, G., Liu, Y., Mayoral-Vilches, V., Liu, P., Li, Y., Xu, Y., ... &amp;amp; Rass, S. (2024). {PentestGPT}: Evaluating and harnessing large language models for automated penetration testing. In 33rd USENIX Security Symposium (USENIX Security 24) (pp. 847-864).&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>opendatalab/MinerU</title>
      <link>https://github.com/opendatalab/MinerU</link>
      <description>&lt;p&gt;Transforms complex documents like PDFs into LLM-ready markdown/JSON for your Agentic workflows.&lt;/p&gt;&lt;hr&gt;&lt;div align="center" xmlns="http://www.w3.org/1999/html"&gt; 
 &lt;!-- logo --&gt; 
 &lt;p align="center"&gt; &lt;img src="https://gcore.jsdelivr.net/gh/opendatalab/MinerU@master/docs/images/MinerU-logo.png" width="300px" style="vertical-align:middle;" /&gt; &lt;/p&gt; 
 &lt;!-- icon --&gt; 
 &lt;p&gt;&lt;a href="https://github.com/opendatalab/MinerU"&gt;&lt;img src="https://img.shields.io/github/stars/opendatalab/MinerU.svg?sanitize=true" alt="stars" /&gt;&lt;/a&gt; &lt;a href="https://github.com/opendatalab/MinerU"&gt;&lt;img src="https://img.shields.io/github/forks/opendatalab/MinerU.svg?sanitize=true" alt="forks" /&gt;&lt;/a&gt; &lt;a href="https://github.com/opendatalab/MinerU/issues"&gt;&lt;img src="https://img.shields.io/github/issues-raw/opendatalab/MinerU" alt="open issues" /&gt;&lt;/a&gt; &lt;a href="https://github.com/opendatalab/MinerU/issues"&gt;&lt;img src="https://img.shields.io/github/issues-closed-raw/opendatalab/MinerU" alt="issue resolution" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/mineru/"&gt;&lt;img src="https://img.shields.io/pypi/v/mineru" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/mineru/"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/mineru" alt="PyPI - Python Version" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/mineru"&gt;&lt;img src="https://static.pepy.tech/badge/mineru" alt="Downloads" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/mineru"&gt;&lt;img src="https://static.pepy.tech/badge/mineru/month" alt="Downloads" /&gt;&lt;/a&gt; &lt;a href="https://mineru.net/OpenSourceTools/Extractor?source=github"&gt;&lt;img src="https://img.shields.io/badge/webapp_on_mineru.net-blue?logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTM0IiBoZWlnaHQ9IjEzNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBkPSJtMTIyLDljMCw1LTQsOS05LDlzLTktNC05LTksNC05LDktOSw5LDQsOSw5eiIgZmlsbD0idXJsKCNhKSIvPjxwYXRoIGQ9Im0xMjIsOWMwLDUtNCw5LTksOXMtOS00LTktOSw0LTksOS05LDksNCw5LDl6IiBmaWxsPSIjMDEwMTAxIi8+PHBhdGggZD0ibTkxLDE4YzAsNS00LDktOSw5cy05LTQtOS05LDQtOSw5LTksOSw0LDksOXoiIGZpbGw9InVybCgjYikiLz48cGF0aCBkPSJtOTEsMThjMCw1LTQsOS05LDlzLTktNC05LTksNC05LDktOSw5LDQsOSw5eiIgZmlsbD0iIzAxMDEwMSIvPjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJtMzksNjJjMCwxNiw4LDMwLDIwLDM4LDctNiwxMi0xNiwxMi0yNlY0OWMwLTQsMy03LDYtOGw0Ni0xMmM1LTEsMTEsMywxMSw4djMxYzAsMzctMzAsNjYtNjYsNjYtMzcsMC02Ni0zMC02Ni02NlY0NmMwLTQsMy03LDYtOGwyMC02YzUtMSwxMSwzLDExLDh2MjF6bS0yOSw2YzAsMTYsNiwzMCwxNyw0MCwzLDEsNSwxLDgsMSw1LDAsMTAtMSwxNS0zQzM3LDk1LDI5LDc5LDI5LDYyVjQybC0xOSw1djIweiIgZmlsbD0idXJsKCNjKSIvPjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJtMzksNjJjMCwxNiw4LDMwLDIwLDM4LDctNiwxMi0xNiwxMi0yNlY0OWMwLTQsMy03LDYtOGw0Ni0xMmM1LTEsMTEsMywxMSw4djMxYzAsMzctMzAsNjYtNjYsNjYtMzcsMC02Ni0zMC02Ni02NlY0NmMwLTQsMy03LDYtOGwyMC02YzUtMSwxMSwzLDExLDh2MjF6bS0yOSw2YzAsMTYsNiwzMCwxNyw0MCwzLDEsNSwxLDgsMSw1LDAsMTAtMSwxNS0zQzM3LDk1LDI5LDc5LDI5LDYyVjQybC0xOSw1djIweiIgZmlsbD0iIzAxMDEwMSIvPjxkZWZzPjxsaW5lYXJHcmFkaWVudCBpZD0iYSIgeDE9Ijg0IiB5MT0iNDEiIHgyPSI3NSIgeTI9IjEyMCIgZ3JhZGllbnRVbml0cz0idXNlclNwYWNlT25Vc2UiPjxzdG9wIHN0b3AtY29sb3I9IiNmZmYiLz48c3RvcCBvZmZzZXQ9IjEiIHN0b3AtY29sb3I9IiMyZTJlMmUiLz48L2xpbmVhckdyYWRpZW50PjxsaW5lYXJHcmFkaWVudCBpZD0iYiIgeDE9Ijg0IiB5MT0iNDEiIHgyPSI3NSIgeTI9IjEyMCIgZ3JhZGllbnRVbml0cz0idXNlclNwYWNlT25Vc2UiPjxzdG9wIHN0b3AtY29sb3I9IiNmZmYiLz48c3RvcCBvZmZzZXQ9IjEiIHN0b3AtY29sb3I9IiMyZTJlMmUiLz48L2xpbmVhckdyYWRpZW50PjxsaW5lYXJHcmFkaWVudCBpZD0iYyIgeDE9Ijg0IiB5MT0iNDEiIHgyPSI3NSIgeTI9IjEyMCIgZ3JhZGllbnRVbml0cz0idXNlclNwYWNlT25Vc2UiPjxzdG9wIHN0b3AtY29sb3I9IiNmZmYiLz48c3RvcCBvZmZzZXQ9IjEiIHN0b3AtY29sb3I9IiMyZTJlMmUiLz48L2xpbmVhckdyYWRpZW50PjwvZGVmcz48L3N2Zz4=&amp;amp;labelColor=white" alt="OpenDataLab" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/spaces/opendatalab/MinerU"&gt;&lt;img src="https://img.shields.io/badge/Demo_on_HuggingFace-yellow.svg?logo=data:image/png;base64,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&amp;amp;labelColor=white" alt="HuggingFace" /&gt;&lt;/a&gt; &lt;a href="https://www.modelscope.cn/studios/OpenDataLab/MinerU"&gt;&lt;img src="https://img.shields.io/badge/Demo_on_ModelScope-purple?logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIzIiBoZWlnaHQ9IjIwMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCiA8Zz4KICA8dGl0bGU+TGF5ZXIgMTwvdGl0bGU+CiAgPHBhdGggaWQ9InN2Z18xNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTAsODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTUiIGZpbGw9IiM2MjRhZmYiIGQ9Im05OS4xNCwxMTUuNDlsMjUuNjUsMGwwLDI1LjY1bC0yNS42NSwwbDAsLTI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTYiIGZpbGw9IiM2MjRhZmYiIGQ9Im0xNzYuMDksMTQxLjE0bC0yNS42NDk5OSwwbDAsMjIuMTlsNDcuODQsMGwwLC00Ny44NGwtMjIuMTksMGwwLDI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTciIGZpbGw9IiMzNmNmZDEiIGQ9Im0xMjQuNzksODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTgiIGZpbGw9IiMzNmNmZDEiIGQ9Im0wLDY0LjE5bDI1LjY1LDBsMCwyNS42NWwtMjUuNjUsMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzE5IiBmaWxsPSIjNjI0YWZmIiBkPSJtMTk4LjI4LDg5Ljg0bDI1LjY0OTk5LDBsMCwyNS42NDk5OWwtMjUuNjQ5OTksMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIwIiBmaWxsPSIjMzZjZmQxIiBkPSJtMTk4LjI4LDY0LjE5bDI1LjY0OTk5LDBsMCwyNS42NWwtMjUuNjQ5OTksMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIxIiBmaWxsPSIjNjI0YWZmIiBkPSJtMTUwLjQ0LDQybDAsMjIuMTlsMjUuNjQ5OTksMGwwLDI1LjY1bDIyLjE5LDBsMCwtNDcuODRsLTQ3Ljg0LDB6Ii8+CiAgPHBhdGggaWQ9InN2Z18yMiIgZmlsbD0iIzM2Y2ZkMSIgZD0ibTczLjQ5LDg5Ljg0bDI1LjY1LDBsMCwyNS42NDk5OWwtMjUuNjUsMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIzIiBmaWxsPSIjNjI0YWZmIiBkPSJtNDcuODQsNjQuMTlsMjUuNjUsMGwwLC0yMi4xOWwtNDcuODQsMGwwLDQ3Ljg0bDIyLjE5LDBsMCwtMjUuNjV6Ii8+CiAgPHBhdGggaWQ9InN2Z18yNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTQ3Ljg0LDExNS40OWwtMjIuMTksMGwwLDQ3Ljg0bDQ3Ljg0LDBsMCwtMjIuMTlsLTI1LjY1LDBsMCwtMjUuNjV6Ii8+CiA8L2c+Cjwvc3ZnPg==&amp;amp;labelColor=white" alt="ModelScope" /&gt;&lt;/a&gt; &lt;a href="https://colab.research.google.com/gist/myhloli/a3cb16570ab3cfeadf9d8f0ac91b4fca/mineru_demo.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Colab" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2409.18839"&gt;&lt;img src="https://img.shields.io/badge/MinerU-Technical%20Report-b31b1b.svg?logo=arXiv" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2509.22186"&gt;&lt;img src="https://img.shields.io/badge/MinerU2.5-Technical%20Report-b31b1b.svg?logo=arXiv" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/opendatalab/MinerU"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/11174" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/11174" alt="opendatalab%2FMinerU | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;!-- language --&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/opendatalab/MinerU/master/README.md"&gt;English&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/opendatalab/MinerU/master/README_zh-CN.md"&gt;ç®€ä½“ä¸­æ–‡&lt;/a&gt;&lt;/p&gt; 
 &lt;!-- hot link --&gt; 
 &lt;p align="center"&gt; ğŸš€&lt;a href="https://mineru.net/?source=github"&gt;Access MinerU Nowâ†’âœ… Zero-Install Web Version âœ… Full-Featured Desktop Client âœ… Instant API Access; Skip deployment headaches â€“ get all product formats in one click. Developers, dive in!&lt;/a&gt; &lt;/p&gt; 
 &lt;!-- join us --&gt; 
 &lt;p align="center"&gt; ğŸ‘‹ join us on &lt;a href="https://discord.gg/Tdedn9GTXq" target="_blank"&gt;Discord&lt;/a&gt; and &lt;a href="https://mineru.net/community-portal/?aliasId=3c430f94" target="_blank"&gt;WeChat&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h1&gt;Changelog&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;2025/10/31 2.6.3 Release&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Added support for a new backend &lt;code&gt;vlm-mlx-engine&lt;/code&gt;, enabling MLX-accelerated inference for the MinerU2.5 model on Apple Silicon devices. Compared to the &lt;code&gt;vlm-transformers&lt;/code&gt; backend, &lt;code&gt;vlm-mlx-engine&lt;/code&gt; delivers a 100%â€“200% speed improvement.&lt;/li&gt; 
   &lt;li&gt;Bug fixes: #3849, #3859&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025/10/24 2.6.2 Release&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;pipeline&lt;/code&gt; backend optimizations 
    &lt;ul&gt; 
     &lt;li&gt;Added experimental support for Chinese formulas, which can be enabled by setting the environment variable &lt;code&gt;export MINERU_FORMULA_CH_SUPPORT=1&lt;/code&gt;. This feature may cause a slight decrease in MFR speed and failures in recognizing some long formulas. It is recommended to enable it only when parsing Chinese formulas is needed. To disable this feature, set the environment variable to &lt;code&gt;0&lt;/code&gt;.&lt;/li&gt; 
     &lt;li&gt;&lt;code&gt;OCR&lt;/code&gt; speed significantly improved by 200%~300%, thanks to the optimization solution provided by &lt;a href="https://github.com/cjsdurj"&gt;@cjsdurj&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;code&gt;OCR&lt;/code&gt; models optimized for improved accuracy and coverage of Latin script recognition, and updated Cyrillic, Arabic, Devanagari, Telugu (te), and Tamil (ta) language systems to &lt;code&gt;ppocr-v5&lt;/code&gt; version, with accuracy improved by over 40% compared to previous models&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;vlm&lt;/code&gt; backend optimizations 
    &lt;ul&gt; 
     &lt;li&gt;&lt;code&gt;table_caption&lt;/code&gt; and &lt;code&gt;table_footnote&lt;/code&gt; matching logic optimized to improve the accuracy of table caption and footnote matching and reading order rationality in scenarios with multiple consecutive tables on a page&lt;/li&gt; 
     &lt;li&gt;Optimized CPU resource usage during high concurrency when using &lt;code&gt;vllm&lt;/code&gt; backend, reducing server pressure&lt;/li&gt; 
     &lt;li&gt;Adapted to &lt;code&gt;vllm&lt;/code&gt; version 0.11.0&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;General optimizations 
    &lt;ul&gt; 
     &lt;li&gt;Cross-page table merging effect optimized, added support for cross-page continuation table merging, improving table merging effectiveness in multi-column merge scenarios&lt;/li&gt; 
     &lt;li&gt;Added environment variable configuration option &lt;code&gt;MINERU_TABLE_MERGE_ENABLE&lt;/code&gt; for table merging feature. Table merging is enabled by default and can be disabled by setting this variable to &lt;code&gt;0&lt;/code&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025/09/26 2.5.4 released&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;ğŸ‰ğŸ‰ The MinerU2.5 &lt;a href="https://arxiv.org/abs/2509.22186"&gt;Technical Report&lt;/a&gt; is now available! We welcome you to read it for a comprehensive overview of its model architecture, training strategy, data engineering and evaluation results.&lt;/li&gt; 
   &lt;li&gt;Fixed an issue where some &lt;code&gt;PDF&lt;/code&gt; files were mistakenly identified as &lt;code&gt;AI&lt;/code&gt; files, causing parsing failures&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025/09/20 2.5.3 Released&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Dependency version range adjustment to enable Turing and earlier architecture GPUs to use vLLM acceleration for MinerU2.5 model inference.&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;pipeline&lt;/code&gt; backend compatibility fixes for torch 2.8.0.&lt;/li&gt; 
   &lt;li&gt;Reduced default concurrency for vLLM async backend to lower server pressure and avoid connection closure issues caused by high load.&lt;/li&gt; 
   &lt;li&gt;More compatibility-related details can be found in the &lt;a href="https://github.com/opendatalab/MinerU/discussions/3548"&gt;announcement&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025/09/19 2.5.2 Released&lt;/p&gt; &lt;p&gt;We are officially releasing MinerU2.5, currently the most powerful multimodal large model for document parsing. With only 1.2B parameters, MinerU2.5's accuracy on the OmniDocBench benchmark comprehensively surpasses top-tier multimodal models like Gemini 2.5 Pro, GPT-4o, and Qwen2.5-VL-72B. It also significantly outperforms leading specialized models such as dots.ocr, MonkeyOCR, and PP-StructureV3. The model has been released on &lt;a href="https://huggingface.co/opendatalab/MinerU2.5-2509-1.2B"&gt;HuggingFace&lt;/a&gt; and &lt;a href="https://modelscope.cn/models/opendatalab/MinerU2.5-2509-1.2B"&gt;ModelScope&lt;/a&gt; platforms. Welcome to download and use!&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Core Highlights: 
    &lt;ul&gt; 
     &lt;li&gt;SOTA Performance with Extreme Efficiency: As a 1.2B model, it achieves State-of-the-Art (SOTA) results that exceed models in the 10B and 100B+ classes, redefining the performance-per-parameter standard in document AI.&lt;/li&gt; 
     &lt;li&gt;Advanced Architecture for Across-the-Board Leadership: By combining a two-stage inference pipeline (decoupling layout analysis from content recognition) with a native high-resolution architecture, it achieves SOTA performance across five key areas: layout analysis, text recognition, formula recognition, table recognition, and reading order.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Key Capability Enhancements: 
    &lt;ul&gt; 
     &lt;li&gt;Layout Detection: Delivers more complete results by accurately covering non-body content like headers, footers, and page numbers. It also provides more precise element localization and natural format reconstruction for lists and references.&lt;/li&gt; 
     &lt;li&gt;Table Parsing: Drastically improves parsing for challenging cases, including rotated tables, borderless/semi-structured tables, and long/complex tables.&lt;/li&gt; 
     &lt;li&gt;Formula Recognition: Significantly boosts accuracy for complex, long-form, and hybrid Chinese-English formulas, greatly enhancing the parsing capability for mathematical documents.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;Additionally, with the release of vlm 2.5, we have made some adjustments to the repository:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;The vlm backend has been upgraded to version 2.5, supporting the MinerU2.5 model and no longer compatible with the MinerU2.0-2505-0.9B model. The last version supporting the 2.0 model is mineru-2.2.2.&lt;/li&gt; 
   &lt;li&gt;VLM inference-related code has been moved to &lt;a href="https://github.com/opendatalab/mineru-vl-utils"&gt;mineru_vl_utils&lt;/a&gt;, reducing coupling with the main mineru repository and facilitating independent iteration in the future.&lt;/li&gt; 
   &lt;li&gt;The vlm accelerated inference framework has been switched from &lt;code&gt;sglang&lt;/code&gt; to &lt;code&gt;vllm&lt;/code&gt;, achieving full compatibility with the vllm ecosystem, allowing users to use the MinerU2.5 model and accelerated inference on any platform that supports the vllm framework.&lt;/li&gt; 
   &lt;li&gt;Due to major upgrades in the vlm model supporting more layout types, we have made some adjustments to the structure of the parsing intermediate file &lt;code&gt;middle.json&lt;/code&gt; and result file &lt;code&gt;content_list.json&lt;/code&gt;. Please refer to the &lt;a href="https://opendatalab.github.io/MinerU/reference/output_files/"&gt;documentation&lt;/a&gt; for details.&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;Other repository optimizations:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Removed file extension whitelist validation for input files. When input files are PDF documents or images, there are no longer requirements for file extensions, improving usability.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;History Log&lt;/summary&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/09/10 2.2.2 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fixed the issue where the new table recognition model would affect the overall parsing task when some table parsing failed&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/09/08 2.2.1 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fixed the issue where some newly added models were not downloaded when using the model download command.&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/09/05 2.2.0 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt; Major Updates 
    &lt;ul&gt; 
     &lt;li&gt;In this version, we focused on improving table parsing accuracy by introducing a new &lt;a href="https://github.com/RapidAI/TableStructureRec"&gt;wired table recognition model&lt;/a&gt; and a brand-new hybrid table structure parsing algorithm, significantly enhancing the table recognition capabilities of the &lt;code&gt;pipeline&lt;/code&gt; backend.&lt;/li&gt; 
     &lt;li&gt;We also added support for cross-page table merging, which is supported by both &lt;code&gt;pipeline&lt;/code&gt; and &lt;code&gt;vlm&lt;/code&gt; backends, further improving the completeness and accuracy of table parsing.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; Other Updates 
    &lt;ul&gt; 
     &lt;li&gt;The &lt;code&gt;pipeline&lt;/code&gt; backend now supports 270-degree rotated table parsing, bringing support for table parsing in 0/90/270-degree orientations&lt;/li&gt; 
     &lt;li&gt;&lt;code&gt;pipeline&lt;/code&gt; added OCR capability support for Thai and Greek, and updated the English OCR model to the latest version. English recognition accuracy improved by 11%, Thai recognition model accuracy is 82.68%, and Greek recognition model accuracy is 89.28% (by PPOCRv5)&lt;/li&gt; 
     &lt;li&gt;Added &lt;code&gt;bbox&lt;/code&gt; field (mapped to 0-1000 range) in the output &lt;code&gt;content_list.json&lt;/code&gt;, making it convenient for users to directly obtain position information for each content block&lt;/li&gt; 
     &lt;li&gt;Removed the &lt;code&gt;pipeline_old_linux&lt;/code&gt; installation option, no longer supporting legacy Linux systems such as &lt;code&gt;CentOS 7&lt;/code&gt;, to provide better support for &lt;code&gt;uv&lt;/code&gt;'s &lt;code&gt;sync&lt;/code&gt;/&lt;code&gt;run&lt;/code&gt; commands&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/08/01 2.1.10 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fixed an issue in the &lt;code&gt;pipeline&lt;/code&gt; backend where block overlap caused the parsing results to deviate from expectations #3232&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/07/30 2.1.9 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;transformers&lt;/code&gt; 4.54.1 version adaptation&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/07/28 2.1.8 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;sglang&lt;/code&gt; 0.4.9.post5 version adaptation&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/07/27 2.1.7 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;transformers&lt;/code&gt; 4.54.0 version adaptation&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/07/26 2.1.6 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fixed table parsing issues in handwritten documents when using &lt;code&gt;vlm&lt;/code&gt; backend&lt;/li&gt; 
   &lt;li&gt;Fixed visualization box position drift issue when document is rotated #3175&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/07/24 2.1.5 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;sglang&lt;/code&gt; 0.4.9 version adaptation, synchronously upgrading the dockerfile base image to sglang 0.4.9.post3&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/07/23 2.1.4 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Bug Fixes&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Fixed the issue of excessive memory consumption during the &lt;code&gt;MFR&lt;/code&gt; step in the &lt;code&gt;pipeline&lt;/code&gt; backend under certain scenarios #2771&lt;/li&gt; 
     &lt;li&gt;Fixed the inaccurate matching between &lt;code&gt;image&lt;/code&gt;/&lt;code&gt;table&lt;/code&gt; and &lt;code&gt;caption&lt;/code&gt;/&lt;code&gt;footnote&lt;/code&gt; under certain conditions #3129&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/07/16 2.1.1 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Bug fixes&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Fixed text block content loss issue that could occur in certain &lt;code&gt;pipeline&lt;/code&gt; scenarios #3005&lt;/li&gt; 
     &lt;li&gt;Fixed issue where &lt;code&gt;sglang-client&lt;/code&gt; required unnecessary packages like &lt;code&gt;torch&lt;/code&gt; #2968&lt;/li&gt; 
     &lt;li&gt;Updated &lt;code&gt;dockerfile&lt;/code&gt; to fix incomplete text content parsing due to missing fonts in Linux #2915&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Usability improvements&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Updated &lt;code&gt;compose.yaml&lt;/code&gt; to facilitate direct startup of &lt;code&gt;sglang-server&lt;/code&gt;, &lt;code&gt;mineru-api&lt;/code&gt;, and &lt;code&gt;mineru-gradio&lt;/code&gt; services&lt;/li&gt; 
     &lt;li&gt;Launched brand new &lt;a href="https://opendatalab.github.io/MinerU/"&gt;online documentation site&lt;/a&gt;, simplified readme, providing better documentation experience&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/07/05 2.1.0 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;This is the first major update of MinerU 2, which includes a large number of new features and improvements, covering significant performance optimizations, user experience enhancements, and bug fixes. The detailed update contents are as follows:&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Performance Optimizations:&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Significantly improved preprocessing speed for documents with specific resolutions (around 2000 pixels on the long side).&lt;/li&gt; 
     &lt;li&gt;Greatly enhanced post-processing speed when the &lt;code&gt;pipeline&lt;/code&gt; backend handles batch processing of documents with fewer pages (&amp;lt;10 pages).&lt;/li&gt; 
     &lt;li&gt;Layout analysis speed of the &lt;code&gt;pipeline&lt;/code&gt; backend has been increased by approximately 20%.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Experience Enhancements:&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Built-in ready-to-use &lt;code&gt;fastapi service&lt;/code&gt; and &lt;code&gt;gradio webui&lt;/code&gt;. For detailed usage instructions, please refer to &lt;a href="https://opendatalab.github.io/MinerU/usage/quick_usage/#advanced-usage-via-api-webui-sglang-clientserver"&gt;Documentation&lt;/a&gt;.&lt;/li&gt; 
     &lt;li&gt;Adapted to &lt;code&gt;sglang&lt;/code&gt; version &lt;code&gt;0.4.8&lt;/code&gt;, significantly reducing the GPU memory requirements for the &lt;code&gt;vlm-sglang&lt;/code&gt; backend. It can now run on graphics cards with as little as &lt;code&gt;8GB GPU memory&lt;/code&gt; (Turing architecture or newer).&lt;/li&gt; 
     &lt;li&gt;Added transparent parameter passing for all commands related to &lt;code&gt;sglang&lt;/code&gt;, allowing the &lt;code&gt;sglang-engine&lt;/code&gt; backend to receive all &lt;code&gt;sglang&lt;/code&gt; parameters consistently with the &lt;code&gt;sglang-server&lt;/code&gt;.&lt;/li&gt; 
     &lt;li&gt;Supports feature extensions based on configuration files, including &lt;code&gt;custom formula delimiters&lt;/code&gt;, &lt;code&gt;enabling heading classification&lt;/code&gt;, and &lt;code&gt;customizing local model directories&lt;/code&gt;. For detailed usage instructions, please refer to &lt;a href="https://opendatalab.github.io/MinerU/usage/quick_usage/#extending-mineru-functionality-with-configuration-files"&gt;Documentation&lt;/a&gt;.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;New Features:&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Updated the &lt;code&gt;pipeline&lt;/code&gt; backend with the PP-OCRv5 multilingual text recognition model, supporting text recognition in 37 languages such as French, Spanish, Portuguese, Russian, and Korean, with an average accuracy improvement of over 30%. &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/algorithm/PP-OCRv5/PP-OCRv5_multi_languages.html"&gt;Details&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;Introduced limited support for vertical text layout in the &lt;code&gt;pipeline&lt;/code&gt; backend.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/06/20 2.0.6 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fixed occasional parsing interruptions caused by invalid block content in &lt;code&gt;vlm&lt;/code&gt; mode&lt;/li&gt; 
   &lt;li&gt;Fixed parsing interruptions caused by incomplete table structures in &lt;code&gt;vlm&lt;/code&gt; mode&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/06/17 2.0.5 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fixed the issue where models were still required to be downloaded in the &lt;code&gt;sglang-client&lt;/code&gt; mode&lt;/li&gt; 
   &lt;li&gt;Fixed the issue where the &lt;code&gt;sglang-client&lt;/code&gt; mode unnecessarily depended on packages like &lt;code&gt;torch&lt;/code&gt; during runtime.&lt;/li&gt; 
   &lt;li&gt;Fixed the issue where only the first instance would take effect when attempting to launch multiple &lt;code&gt;sglang-client&lt;/code&gt; instances via multiple URLs within the same process&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/06/15 2.0.3 released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fixed a configuration file key-value update error that occurred when downloading model type was set to &lt;code&gt;all&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Fixed the issue where the formula and table feature toggle switches were not working in &lt;code&gt;command line mode&lt;/code&gt;, causing the features to remain enabled.&lt;/li&gt; 
   &lt;li&gt;Fixed compatibility issues with sglang version 0.4.7 in the &lt;code&gt;sglang-engine&lt;/code&gt; mode.&lt;/li&gt; 
   &lt;li&gt;Updated Dockerfile and installation documentation for deploying the full version of MinerU in sglang environment&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/06/13 2.0.0 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;New Architecture&lt;/strong&gt;: MinerU 2.0 has been deeply restructured in code organization and interaction methods, significantly improving system usability, maintainability, and extensibility. 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Removal of Third-party Dependency Limitations&lt;/strong&gt;: Completely eliminated the dependency on &lt;code&gt;pymupdf&lt;/code&gt;, moving the project toward a more open and compliant open-source direction.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Ready-to-use, Easy Configuration&lt;/strong&gt;: No need to manually edit JSON configuration files; most parameters can now be set directly via command line or API.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Automatic Model Management&lt;/strong&gt;: Added automatic model download and update mechanisms, allowing users to complete model deployment without manual intervention.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Offline Deployment Friendly&lt;/strong&gt;: Provides built-in model download commands, supporting deployment requirements in completely offline environments.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Streamlined Code Structure&lt;/strong&gt;: Removed thousands of lines of redundant code, simplified class inheritance logic, significantly improving code readability and development efficiency.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Unified Intermediate Format Output&lt;/strong&gt;: Adopted standardized &lt;code&gt;middle_json&lt;/code&gt; format, compatible with most secondary development scenarios based on this format, ensuring seamless ecosystem business migration.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;New Model&lt;/strong&gt;: MinerU 2.0 integrates our latest small-parameter, high-performance multimodal document parsing model, achieving end-to-end high-speed, high-precision document understanding. 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Small Model, Big Capabilities&lt;/strong&gt;: With parameters under 1B, yet surpassing traditional 72B-level vision-language models (VLMs) in parsing accuracy.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Multiple Functions in One&lt;/strong&gt;: A single model covers multilingual recognition, handwriting recognition, layout analysis, table parsing, formula recognition, reading order sorting, and other core tasks.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Ultimate Inference Speed&lt;/strong&gt;: Achieves peak throughput exceeding 10,000 tokens/s through &lt;code&gt;sglang&lt;/code&gt; acceleration on a single NVIDIA 4090 card, easily handling large-scale document processing requirements.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Online Experience&lt;/strong&gt;: You can experience our brand-new VLM model on &lt;a href="https://mineru.net/OpenSourceTools/Extractor"&gt;MinerU.net&lt;/a&gt;, &lt;a href="https://huggingface.co/spaces/opendatalab/MinerU"&gt;Hugging Face&lt;/a&gt;, and &lt;a href="https://www.modelscope.cn/studios/OpenDataLab/MinerU"&gt;ModelScope&lt;/a&gt;.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Incompatible Changes Notice&lt;/strong&gt;: To improve overall architectural rationality and long-term maintainability, this version contains some incompatible changes: 
    &lt;ul&gt; 
     &lt;li&gt;Python package name changed from &lt;code&gt;magic-pdf&lt;/code&gt; to &lt;code&gt;mineru&lt;/code&gt;, and the command-line tool changed from &lt;code&gt;magic-pdf&lt;/code&gt; to &lt;code&gt;mineru&lt;/code&gt;. Please update your scripts and command calls accordingly.&lt;/li&gt; 
     &lt;li&gt;For modular system design and ecosystem consistency considerations, MinerU 2.0 no longer includes the LibreOffice document conversion module. If you need to process Office documents, we recommend converting them to PDF format through an independently deployed LibreOffice service before proceeding with subsequent parsing operations.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/05/24 Release 1.3.12&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Added support for PPOCRv5 models, updated &lt;code&gt;ch_server&lt;/code&gt; model to &lt;code&gt;PP-OCRv5_rec_server&lt;/code&gt;, and &lt;code&gt;ch_lite&lt;/code&gt; model to &lt;code&gt;PP-OCRv5_rec_mobile&lt;/code&gt; (model update required) 
    &lt;ul&gt; 
     &lt;li&gt;In testing, we found that PPOCRv5(server) has some improvement for handwritten documents, but has slightly lower accuracy than v4_server_doc for other document types, so the default ch model remains unchanged as &lt;code&gt;PP-OCRv4_server_rec_doc&lt;/code&gt;.&lt;/li&gt; 
     &lt;li&gt;Since PPOCRv5 has enhanced recognition capabilities for handwriting and special characters, you can manually choose the PPOCRv5 model for Japanese-Traditional Chinese mixed scenarios and handwritten documents&lt;/li&gt; 
     &lt;li&gt;You can select the appropriate model through the lang parameter &lt;code&gt;lang='ch_server'&lt;/code&gt; (Python API) or &lt;code&gt;--lang ch_server&lt;/code&gt; (command line): 
      &lt;ul&gt; 
       &lt;li&gt;&lt;code&gt;ch&lt;/code&gt;: &lt;code&gt;PP-OCRv4_server_rec_doc&lt;/code&gt; (default) (Chinese/English/Japanese/Traditional Chinese mixed/15K dictionary)&lt;/li&gt; 
       &lt;li&gt;&lt;code&gt;ch_server&lt;/code&gt;: &lt;code&gt;PP-OCRv5_rec_server&lt;/code&gt; (Chinese/English/Japanese/Traditional Chinese mixed + handwriting/18K dictionary)&lt;/li&gt; 
       &lt;li&gt;&lt;code&gt;ch_lite&lt;/code&gt;: &lt;code&gt;PP-OCRv5_rec_mobile&lt;/code&gt; (Chinese/English/Japanese/Traditional Chinese mixed + handwriting/18K dictionary)&lt;/li&gt; 
       &lt;li&gt;&lt;code&gt;ch_server_v4&lt;/code&gt;: &lt;code&gt;PP-OCRv4_rec_server&lt;/code&gt; (Chinese/English mixed/6K dictionary)&lt;/li&gt; 
       &lt;li&gt;&lt;code&gt;ch_lite_v4&lt;/code&gt;: &lt;code&gt;PP-OCRv4_rec_mobile&lt;/code&gt; (Chinese/English mixed/6K dictionary)&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Added support for handwritten documents through optimized layout recognition of handwritten text areas 
    &lt;ul&gt; 
     &lt;li&gt;This feature is supported by default, no additional configuration required&lt;/li&gt; 
     &lt;li&gt;You can refer to the instructions above to manually select the PPOCRv5 model for better handwritten document parsing results&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;The &lt;code&gt;huggingface&lt;/code&gt; and &lt;code&gt;modelscope&lt;/code&gt; demos have been updated to versions that support handwriting recognition and PPOCRv5 models, which you can experience online&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/04/29 Release 1.3.10&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Added support for custom formula delimiters, which can be configured by modifying the &lt;code&gt;latex-delimiter-config&lt;/code&gt; section in the &lt;code&gt;magic-pdf.json&lt;/code&gt; file in your user directory.&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/04/27 Release 1.3.9&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Optimized formula parsing functionality, improved formula rendering success rate&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/04/23 Release 1.3.8&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;The default &lt;code&gt;ocr&lt;/code&gt; model (&lt;code&gt;ch&lt;/code&gt;) has been updated to &lt;code&gt;PP-OCRv4_server_rec_doc&lt;/code&gt; (model update required) 
    &lt;ul&gt; 
     &lt;li&gt;&lt;code&gt;PP-OCRv4_server_rec_doc&lt;/code&gt; is trained on a mixture of more Chinese document data and PP-OCR training data based on &lt;code&gt;PP-OCRv4_server_rec&lt;/code&gt;, adding recognition capabilities for some traditional Chinese characters, Japanese, and special characters. It can recognize over 15,000 characters and improves both document-specific and general text recognition abilities.&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleX/latest/module_usage/tutorials/ocr_modules/text_recognition.html#_3"&gt;Performance comparison of PP-OCRv4_server_rec_doc/PP-OCRv4_server_rec/PP-OCRv4_mobile_rec&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;After verification, the &lt;code&gt;PP-OCRv4_server_rec_doc&lt;/code&gt; model shows significant accuracy improvements in Chinese/English/Japanese/Traditional Chinese in both single language and mixed language scenarios, with comparable speed to &lt;code&gt;PP-OCRv4_server_rec&lt;/code&gt;, making it suitable for most use cases.&lt;/li&gt; 
     &lt;li&gt;In some pure English scenarios, &lt;code&gt;PP-OCRv4_server_rec_doc&lt;/code&gt; may have word adhesion issues, while &lt;code&gt;PP-OCRv4_server_rec&lt;/code&gt; performs better in these cases. Therefore, we've kept the &lt;code&gt;PP-OCRv4_server_rec&lt;/code&gt; model, which users can access by adding the parameter &lt;code&gt;lang='ch_server'&lt;/code&gt; (Python API) or &lt;code&gt;--lang ch_server&lt;/code&gt; (command line).&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/04/22 Release 1.3.7&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fixed the issue where the lang parameter was ineffective during table parsing model initialization&lt;/li&gt; 
   &lt;li&gt;Fixed the significant speed reduction of OCR and table parsing in &lt;code&gt;cpu&lt;/code&gt; mode&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/04/16 Release 1.3.4&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Slightly improved OCR-det speed by removing some unnecessary blocks&lt;/li&gt; 
   &lt;li&gt;Fixed page-internal sorting errors caused by footnotes in certain cases&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/04/12 Release 1.3.2&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fixed dependency version incompatibility issues when installing on Windows with Python 3.13&lt;/li&gt; 
   &lt;li&gt;Optimized memory usage during batch inference&lt;/li&gt; 
   &lt;li&gt;Improved parsing of tables rotated 90 degrees&lt;/li&gt; 
   &lt;li&gt;Enhanced parsing of oversized tables in financial report samples&lt;/li&gt; 
   &lt;li&gt;Fixed the occasional word adhesion issue in English text areas when OCR language is not specified (model update required)&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/04/08 Release 1.3.1&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fixed several compatibility issues 
    &lt;ul&gt; 
     &lt;li&gt;Added support for Python 3.13&lt;/li&gt; 
     &lt;li&gt;Made final adaptations for outdated Linux systems (such as CentOS 7) with no guarantee of continued support in future versions, &lt;a href="https://github.com/opendatalab/MinerU/issues/1004"&gt;installation instructions&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/04/03 Release 1.3.0&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Installation and compatibility optimizations 
    &lt;ul&gt; 
     &lt;li&gt;Resolved compatibility issues caused by &lt;code&gt;detectron2&lt;/code&gt; by removing &lt;code&gt;layoutlmv3&lt;/code&gt; usage in layout&lt;/li&gt; 
     &lt;li&gt;Extended torch version compatibility to 2.2~2.6 (excluding 2.5)&lt;/li&gt; 
     &lt;li&gt;Added CUDA compatibility for versions 11.8/12.4/12.6/12.8 (CUDA version determined by torch), solving compatibility issues for users with 50-series and H-series GPUs&lt;/li&gt; 
     &lt;li&gt;Extended Python compatibility to versions 3.10~3.12, fixing the issue of automatic downgrade to version 0.6.1 when installing in non-3.10 environments&lt;/li&gt; 
     &lt;li&gt;Optimized offline deployment process, eliminating the need to download any model files after successful deployment&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Performance optimizations 
    &lt;ul&gt; 
     &lt;li&gt;Enhanced parsing speed for batches of small files by supporting batch processing of multiple PDF files (&lt;a href="https://raw.githubusercontent.com/opendatalab/MinerU/master/demo/batch_demo.py"&gt;script example&lt;/a&gt;), with formula parsing speed improved by up to 1400% and overall parsing speed improved by up to 500% compared to version 1.0.1&lt;/li&gt; 
     &lt;li&gt;Reduced memory usage and improved parsing speed by optimizing MFR model loading and usage (requires re-running the &lt;a href="https://raw.githubusercontent.com/opendatalab/MinerU/master/docs/how_to_download_models_zh_cn.md"&gt;model download process&lt;/a&gt; to get incremental updates to model files)&lt;/li&gt; 
     &lt;li&gt;Optimized GPU memory usage, requiring only 6GB minimum to run this project&lt;/li&gt; 
     &lt;li&gt;Improved running speed on MPS devices&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Parsing effect optimizations 
    &lt;ul&gt; 
     &lt;li&gt;Updated MFR model to &lt;code&gt;unimernet(2503)&lt;/code&gt;, fixing line break loss issues in multi-line formulas&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Usability optimizations 
    &lt;ul&gt; 
     &lt;li&gt;Completely replaced the &lt;code&gt;paddle&lt;/code&gt; framework and &lt;code&gt;paddleocr&lt;/code&gt; in the project by using &lt;code&gt;paddleocr2torch&lt;/code&gt;, resolving conflicts between &lt;code&gt;paddle&lt;/code&gt; and &lt;code&gt;torch&lt;/code&gt;, as well as thread safety issues caused by the &lt;code&gt;paddle&lt;/code&gt; framework&lt;/li&gt; 
     &lt;li&gt;Added real-time progress bar display during parsing, allowing precise tracking of parsing progress and making the waiting process more bearable&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/03/03 1.2.1 released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fixed the impact on punctuation marks during full-width to half-width conversion of letters and numbers&lt;/li&gt; 
   &lt;li&gt;Fixed caption matching inaccuracies in certain scenarios&lt;/li&gt; 
   &lt;li&gt;Fixed formula span loss issues in certain scenarios&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/02/24 1.2.0 released&lt;/summary&gt; 
  &lt;p&gt;This version includes several fixes and improvements to enhance parsing efficiency and accuracy:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Performance Optimization&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Increased classification speed for PDF documents in auto mode.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Parsing Optimization&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Improved parsing logic for documents containing watermarks, significantly enhancing the parsing results for such documents.&lt;/li&gt; 
     &lt;li&gt;Enhanced the matching logic for multiple images/tables and captions within a single page, improving the accuracy of image-text matching in complex layouts.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Bug Fixes&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Fixed an issue where image/table spans were incorrectly filled into text blocks under certain conditions.&lt;/li&gt; 
     &lt;li&gt;Resolved an issue where title blocks were empty in some cases.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/01/22 1.1.0 released&lt;/summary&gt; 
  &lt;p&gt;In this version we have focused on improving parsing accuracy and efficiency:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Model capability upgrade&lt;/strong&gt; (requires re-executing the &lt;a href="https://github.com/opendatalab/MinerU/raw/master/docs/how_to_download_models_en.md"&gt;model download process&lt;/a&gt; to obtain incremental updates of model files) 
    &lt;ul&gt; 
     &lt;li&gt;The layout recognition model has been upgraded to the latest &lt;code&gt;doclayout_yolo(2501)&lt;/code&gt; model, improving layout recognition accuracy.&lt;/li&gt; 
     &lt;li&gt;The formula parsing model has been upgraded to the latest &lt;code&gt;unimernet(2501)&lt;/code&gt; model, improving formula recognition accuracy.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Performance optimization&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;On devices that meet certain configuration requirements (16GB+ VRAM), by optimizing resource usage and restructuring the processing pipeline, overall parsing speed has been increased by more than 50%.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Parsing effect optimization&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Added a new heading classification feature (testing version, enabled by default) to the online demo (&lt;a href="https://mineru.net/OpenSourceTools/Extractor"&gt;mineru.net&lt;/a&gt;/&lt;a href="https://huggingface.co/spaces/opendatalab/MinerU"&gt;huggingface&lt;/a&gt;/&lt;a href="https://www.modelscope.cn/studios/OpenDataLab/MinerU"&gt;modelscope&lt;/a&gt;), which supports hierarchical classification of headings, thereby enhancing document structuring.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/01/10 1.0.1 released&lt;/summary&gt; 
  &lt;p&gt;This is our first official release, where we have introduced a completely new API interface and enhanced compatibility through extensive refactoring, as well as a brand new automatic language identification feature:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;New API Interface&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;For the data-side API, we have introduced the Dataset class, designed to provide a robust and flexible data processing framework. This framework currently supports a variety of document formats, including images (.jpg and .png), PDFs, Word documents (.doc and .docx), and PowerPoint presentations (.ppt and .pptx). It ensures effective support for data processing tasks ranging from simple to complex.&lt;/li&gt; 
     &lt;li&gt;For the user-side API, we have meticulously designed the MinerU processing workflow as a series of composable Stages. Each Stage represents a specific processing step, allowing users to define new Stages according to their needs and creatively combine these stages to customize their data processing workflows.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Enhanced Compatibility&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;By optimizing the dependency environment and configuration items, we ensure stable and efficient operation on ARM architecture Linux systems.&lt;/li&gt; 
     &lt;li&gt;We have deeply integrated with Huawei Ascend NPU acceleration, providing autonomous and controllable high-performance computing capabilities. This supports the localization and development of AI application platforms in China. &lt;a href="https://github.com/opendatalab/MinerU/raw/master/docs/README_Ascend_NPU_Acceleration_zh_CN.md"&gt;Ascend NPU Acceleration&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Automatic Language Identification&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;By introducing a new language recognition model, setting the &lt;code&gt;lang&lt;/code&gt; configuration to &lt;code&gt;auto&lt;/code&gt; during document parsing will automatically select the appropriate OCR language model, improving the accuracy of scanned document parsing.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2024/11/22 0.10.0 released&lt;/summary&gt; 
  &lt;p&gt;Introducing hybrid OCR text extraction capabilities:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Significantly improved parsing performance in complex text distribution scenarios such as dense formulas, irregular span regions, and text represented by images.&lt;/li&gt; 
   &lt;li&gt;Combines the dual advantages of accurate content extraction and faster speed in text mode, and more precise span/line region recognition in OCR mode.&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2024/11/15 0.9.3 released&lt;/summary&gt; 
  &lt;p&gt;Integrated &lt;a href="https://github.com/RapidAI/RapidTable"&gt;RapidTable&lt;/a&gt; for table recognition, improving single-table parsing speed by more than 10 times, with higher accuracy and lower GPU memory usage.&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2024/11/06 0.9.2 released&lt;/summary&gt; 
  &lt;p&gt;Integrated the &lt;a href="https://huggingface.co/U4R/StructTable-InternVL2-1B"&gt;StructTable-InternVL2-1B&lt;/a&gt; model for table recognition functionality.&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2024/10/31 0.9.0 released&lt;/summary&gt; 
  &lt;p&gt;This is a major new version with extensive code refactoring, addressing numerous issues, improving performance, reducing hardware requirements, and enhancing usability:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Refactored the sorting module code to use &lt;a href="https://github.com/ppaanngggg/layoutreader"&gt;layoutreader&lt;/a&gt; for reading order sorting, ensuring high accuracy in various layouts.&lt;/li&gt; 
   &lt;li&gt;Refactored the paragraph concatenation module to achieve good results in cross-column, cross-page, cross-figure, and cross-table scenarios.&lt;/li&gt; 
   &lt;li&gt;Refactored the list and table of contents recognition functions, significantly improving the accuracy of list blocks and table of contents blocks, as well as the parsing of corresponding text paragraphs.&lt;/li&gt; 
   &lt;li&gt;Refactored the matching logic for figures, tables, and descriptive text, greatly enhancing the accuracy of matching captions and footnotes to figures and tables, and reducing the loss rate of descriptive text to near zero.&lt;/li&gt; 
   &lt;li&gt;Added multi-language support for OCR, supporting detection and recognition of 84 languages. For the list of supported languages, see &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/ppocr/blog/multi_languages.html#5-support-languages-and-abbreviations"&gt;OCR Language Support List&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;Added memory recycling logic and other memory optimization measures, significantly reducing memory usage. The memory requirement for enabling all acceleration features except table acceleration (layout/formula/OCR) has been reduced from 16GB to 8GB, and the memory requirement for enabling all acceleration features has been reduced from 24GB to 10GB.&lt;/li&gt; 
   &lt;li&gt;Optimized configuration file feature switches, adding an independent formula detection switch to significantly improve speed and parsing results when formula detection is not needed.&lt;/li&gt; 
   &lt;li&gt;Integrated &lt;a href="https://github.com/opendatalab/PDF-Extract-Kit"&gt;PDF-Extract-Kit 1.0&lt;/a&gt;: 
    &lt;ul&gt; 
     &lt;li&gt;Added the self-developed &lt;code&gt;doclayout_yolo&lt;/code&gt; model, which speeds up processing by more than 10 times compared to the original solution while maintaining similar parsing effects, and can be freely switched with &lt;code&gt;layoutlmv3&lt;/code&gt; via the configuration file.&lt;/li&gt; 
     &lt;li&gt;Upgraded formula parsing to &lt;code&gt;unimernet 0.2.1&lt;/code&gt;, improving formula parsing accuracy while significantly reducing memory usage.&lt;/li&gt; 
     &lt;li&gt;Due to the repository change for &lt;code&gt;PDF-Extract-Kit 1.0&lt;/code&gt;, you need to re-download the model. Please refer to &lt;a href="https://github.com/opendatalab/MinerU/raw/master/docs/how_to_download_models_en.md"&gt;How to Download Models&lt;/a&gt; for detailed steps.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2024/09/27 Version 0.8.1 released&lt;/summary&gt; 
  &lt;p&gt;Fixed some bugs, and providing a &lt;a href="https://github.com/opendatalab/MinerU/raw/master/projects/web_demo/README.md"&gt;localized deployment version&lt;/a&gt; of the &lt;a href="https://opendatalab.com/OpenSourceTools/Extractor/PDF/"&gt;online demo&lt;/a&gt; and the &lt;a href="https://github.com/opendatalab/MinerU/raw/master/projects/web/README.md"&gt;front-end interface&lt;/a&gt;.&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2024/09/09 Version 0.8.0 released&lt;/summary&gt; 
  &lt;p&gt;Supporting fast deployment with Dockerfile, and launching demos on Huggingface and Modelscope.&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2024/08/30 Version 0.7.1 released&lt;/summary&gt; 
  &lt;p&gt;Add paddle tablemaster table recognition option&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2024/08/09 Version 0.7.0b1 released&lt;/summary&gt; 
  &lt;p&gt;Simplified installation process, added table recognition functionality&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2024/08/01 Version 0.6.2b1 released&lt;/summary&gt; 
  &lt;p&gt;Optimized dependency conflict issues and installation documentation&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2024/07/05 Initial open-source release&lt;/summary&gt; 
 &lt;/details&gt; 
&lt;/details&gt; 
&lt;h1&gt;MinerU&lt;/h1&gt; 
&lt;h2&gt;Project Introduction&lt;/h2&gt; 
&lt;p&gt;MinerU is a tool that converts PDFs into machine-readable formats (e.g., markdown, JSON), allowing for easy extraction into any format. MinerU was born during the pre-training process of &lt;a href="https://github.com/InternLM/InternLM"&gt;InternLM&lt;/a&gt;. We focus on solving symbol conversion issues in scientific literature and hope to contribute to technological development in the era of large models. Compared to well-known commercial products, MinerU is still young. If you encounter any issues or if the results are not as expected, please submit an issue on &lt;a href="https://github.com/opendatalab/MinerU/issues"&gt;issue&lt;/a&gt; and &lt;strong&gt;attach the relevant PDF&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/4bea02c9-6d54-4cd6-97ed-dff14340982c"&gt;https://github.com/user-attachments/assets/4bea02c9-6d54-4cd6-97ed-dff14340982c&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Remove headers, footers, footnotes, page numbers, etc., to ensure semantic coherence.&lt;/li&gt; 
 &lt;li&gt;Output text in human-readable order, suitable for single-column, multi-column, and complex layouts.&lt;/li&gt; 
 &lt;li&gt;Preserve the structure of the original document, including headings, paragraphs, lists, etc.&lt;/li&gt; 
 &lt;li&gt;Extract images, image descriptions, tables, table titles, and footnotes.&lt;/li&gt; 
 &lt;li&gt;Automatically recognize and convert formulas in the document to LaTeX format.&lt;/li&gt; 
 &lt;li&gt;Automatically recognize and convert tables in the document to HTML format.&lt;/li&gt; 
 &lt;li&gt;Automatically detect scanned PDFs and garbled PDFs and enable OCR functionality.&lt;/li&gt; 
 &lt;li&gt;OCR supports detection and recognition of 109 languages.&lt;/li&gt; 
 &lt;li&gt;Supports multiple output formats, such as multimodal and NLP Markdown, JSON sorted by reading order, and rich intermediate formats.&lt;/li&gt; 
 &lt;li&gt;Supports various visualization results, including layout visualization and span visualization, for efficient confirmation of output quality.&lt;/li&gt; 
 &lt;li&gt;Supports running in a pure CPU environment, and also supports GPU(CUDA)/NPU(CANN)/MPS acceleration&lt;/li&gt; 
 &lt;li&gt;Compatible with Windows, Linux, and Mac platforms.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Quick Start&lt;/h1&gt; 
&lt;p&gt;If you encounter any installation issues, please first consult the &lt;a href="https://raw.githubusercontent.com/opendatalab/MinerU/master/#faq"&gt;FAQ&lt;/a&gt;. &lt;br /&gt; If the parsing results are not as expected, refer to the &lt;a href="https://raw.githubusercontent.com/opendatalab/MinerU/master/#known-issues"&gt;Known Issues&lt;/a&gt;. &lt;br /&gt;&lt;/p&gt; 
&lt;h2&gt;Online Experience&lt;/h2&gt; 
&lt;h3&gt;Official online web application&lt;/h3&gt; 
&lt;p&gt;The official online version has the same functionality as the client, with a beautiful interface and rich features, requires login to use&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://mineru.net/OpenSourceTools/Extractor?source=github"&gt;&lt;img src="https://img.shields.io/badge/webapp_on_mineru.net-blue?logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTM0IiBoZWlnaHQ9IjEzNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBkPSJtMTIyLDljMCw1LTQsOS05LDlzLTktNC05LTksNC05LDktOSw5LDQsOSw5eiIgZmlsbD0idXJsKCNhKSIvPjxwYXRoIGQ9Im0xMjIsOWMwLDUtNCw5LTksOXMtOS00LTktOSw0LTksOS05LDksNCw5LDl6IiBmaWxsPSIjMDEwMTAxIi8+PHBhdGggZD0ibTkxLDE4YzAsNS00LDktOSw5cy05LTQtOS05LDQtOSw5LTksOSw0LDksOXoiIGZpbGw9InVybCgjYikiLz48cGF0aCBkPSJtOTEsMThjMCw1LTQsOS05LDlzLTktNC05LTksNC05LDktOSw5LDQsOSw5eiIgZmlsbD0iIzAxMDEwMSIvPjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJtMzksNjJjMCwxNiw4LDMwLDIwLDM4LDctNiwxMi0xNiwxMi0yNlY0OWMwLTQsMy03LDYtOGw0Ni0xMmM1LTEsMTEsMywxMSw4djMxYzAsMzctMzAsNjYtNjYsNjYtMzcsMC02Ni0zMC02Ni02NlY0NmMwLTQsMy03LDYtOGwyMC02YzUtMSwxMSwzLDExLDh2MjF6bS0yOSw2YzAsMTYsNiwzMCwxNyw0MCwzLDEsNSwxLDgsMSw1LDAsMTAtMSwxNS0zQzM3LDk1LDI5LDc5LDI5LDYyVjQybC0xOSw1djIweiIgZmlsbD0idXJsKCNjKSIvPjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJtMzksNjJjMCwxNiw4LDMwLDIwLDM4LDctNiwxMi0xNiwxMi0yNlY0OWMwLTQsMy03LDYtOGw0Ni0xMmM1LTEsMTEsMywxMSw4djMxYzAsMzctMzAsNjYtNjYsNjYtMzcsMC02Ni0zMC02Ni02NlY0NmMwLTQsMy03LDYtOGwyMC02YzUtMSwxMSwzLDExLDh2MjF6bS0yOSw2YzAsMTYsNiwzMCwxNyw0MCwzLDEsNSwxLDgsMSw1LDAsMTAtMSwxNS0zQzM3LDk1LDI5LDc5LDI5LDYyVjQybC0xOSw1djIweiIgZmlsbD0iIzAxMDEwMSIvPjxkZWZzPjxsaW5lYXJHcmFkaWVudCBpZD0iYSIgeDE9Ijg0IiB5MT0iNDEiIHgyPSI3NSIgeTI9IjEyMCIgZ3JhZGllbnRVbml0cz0idXNlclNwYWNlT25Vc2UiPjxzdG9wIHN0b3AtY29sb3I9IiNmZmYiLz48c3RvcCBvZmZzZXQ9IjEiIHN0b3AtY29sb3I9IiMyZTJlMmUiLz48L2xpbmVhckdyYWRpZW50PjxsaW5lYXJHcmFkaWVudCBpZD0iYiIgeDE9Ijg0IiB5MT0iNDEiIHgyPSI3NSIgeTI9IjEyMCIgZ3JhZGllbnRVbml0cz0idXNlclNwYWNlT25Vc2UiPjxzdG9wIHN0b3AtY29sb3I9IiNmZmYiLz48c3RvcCBvZmZzZXQ9IjEiIHN0b3AtY29sb3I9IiMyZTJlMmUiLz48L2xpbmVhckdyYWRpZW50PjxsaW5lYXJHcmFkaWVudCBpZD0iYyIgeDE9Ijg0IiB5MT0iNDEiIHgyPSI3NSIgeTI9IjEyMCIgZ3JhZGllbnRVbml0cz0idXNlclNwYWNlT25Vc2UiPjxzdG9wIHN0b3AtY29sb3I9IiNmZmYiLz48c3RvcCBvZmZzZXQ9IjEiIHN0b3AtY29sb3I9IiMyZTJlMmUiLz48L2xpbmVhckdyYWRpZW50PjwvZGVmcz48L3N2Zz4=&amp;amp;labelColor=white" alt="OpenDataLab" /&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Gradio-based online demo&lt;/h3&gt; 
&lt;p&gt;A WebUI developed based on Gradio, with a simple interface and only core parsing functionality, no login required&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.modelscope.cn/studios/OpenDataLab/MinerU"&gt;&lt;img src="https://img.shields.io/badge/Demo_on_ModelScope-purple?logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIzIiBoZWlnaHQ9IjIwMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCiA8Zz4KICA8dGl0bGU+TGF5ZXIgMTwvdGl0bGU+CiAgPHBhdGggaWQ9InN2Z18xNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTAsODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTUiIGZpbGw9IiM2MjRhZmYiIGQ9Im05OS4xNCwxMTUuNDlsMjUuNjUsMGwwLDI1LjY1bC0yNS42NSwwbDAsLTI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTYiIGZpbGw9IiM2MjRhZmYiIGQ9Im0xNzYuMDksMTQxLjE0bC0yNS42NDk5OSwwbDAsMjIuMTlsNDcuODQsMGwwLC00Ny44NGwtMjIuMTksMGwwLDI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTciIGZpbGw9IiMzNmNmZDEiIGQ9Im0xMjQuNzksODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTgiIGZpbGw9IiMzNmNmZDEiIGQ9Im0wLDY0LjE5bDI1LjY1LDBsMCwyNS42NWwtMjUuNjUsMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzE5IiBmaWxsPSIjNjI0YWZmIiBkPSJtMTk4LjI4LDg5Ljg0bDI1LjY0OTk5LDBsMCwyNS42NDk5OWwtMjUuNjQ5OTksMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIwIiBmaWxsPSIjMzZjZmQxIiBkPSJtMTk4LjI4LDY0LjE5bDI1LjY0OTk5LDBsMCwyNS42NWwtMjUuNjQ5OTksMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIxIiBmaWxsPSIjNjI0YWZmIiBkPSJtMTUwLjQ0LDQybDAsMjIuMTlsMjUuNjQ5OTksMGwwLDI1LjY1bDIyLjE5LDBsMCwtNDcuODRsLTQ3Ljg0LDB6Ii8+CiAgPHBhdGggaWQ9InN2Z18yMiIgZmlsbD0iIzM2Y2ZkMSIgZD0ibTczLjQ5LDg5Ljg0bDI1LjY1LDBsMCwyNS42NDk5OWwtMjUuNjUsMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIzIiBmaWxsPSIjNjI0YWZmIiBkPSJtNDcuODQsNjQuMTlsMjUuNjUsMGwwLC0yMi4xOWwtNDcuODQsMGwwLDQ3Ljg0bDIyLjE5LDBsMCwtMjUuNjV6Ii8+CiAgPHBhdGggaWQ9InN2Z18yNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTQ3Ljg0LDExNS40OWwtMjIuMTksMGwwLDQ3Ljg0bDQ3Ljg0LDBsMCwtMjIuMTlsLTI1LjY1LDBsMCwtMjUuNjV6Ii8+CiA8L2c+Cjwvc3ZnPg==&amp;amp;labelColor=white" alt="ModelScope" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/spaces/opendatalab/MinerU"&gt;&lt;img src="https://img.shields.io/badge/Demo_on_HuggingFace-yellow.svg?logo=data:image/png;base64,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&amp;amp;labelColor=white" alt="HuggingFace" /&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Local Deployment&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING] &lt;strong&gt;Pre-installation Noticeâ€”Hardware and Software Environment Support&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;To ensure the stability and reliability of the project, we only optimize and test for specific hardware and software environments during development. This ensures that users deploying and running the project on recommended system configurations will get the best performance with the fewest compatibility issues.&lt;/p&gt; 
 &lt;p&gt;By focusing resources on the mainline environment, our team can more efficiently resolve potential bugs and develop new features.&lt;/p&gt; 
 &lt;p&gt;In non-mainline environments, due to the diversity of hardware and software configurations, as well as third-party dependency compatibility issues, we cannot guarantee 100% project availability. Therefore, for users who wish to use this project in non-recommended environments, we suggest carefully reading the documentation and FAQ first. Most issues already have corresponding solutions in the FAQ. We also encourage community feedback to help us gradually expand support.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th rowspan="2"&gt;Parsing Backend&lt;/th&gt; 
   &lt;th rowspan="2"&gt;pipeline &lt;br /&gt; (Accuracy&lt;sup&gt;1&lt;/sup&gt; 82+)&lt;/th&gt; 
   &lt;th colspan="4"&gt;vlm (Accuracy&lt;sup&gt;1&lt;/sup&gt; 90+)&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th&gt;transformers&lt;/th&gt; 
   &lt;th&gt;mlx-engine&lt;/th&gt; 
   &lt;th&gt;vllm-engine / &lt;br /&gt;vllm-async-engine&lt;/th&gt; 
   &lt;th&gt;http-client&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Backend Features&lt;/th&gt; 
   &lt;td&gt;Fast, no hallucinations&lt;/td&gt; 
   &lt;td&gt;Good compatibility, &lt;br /&gt;but slower&lt;/td&gt; 
   &lt;td&gt;Faster than transformers&lt;/td&gt; 
   &lt;td&gt;Fast, compatible with the vLLM ecosystem&lt;/td&gt; 
   &lt;td&gt;Suitable for OpenAI-compatible servers&lt;sup&gt;5&lt;/sup&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Operating System&lt;/th&gt; 
   &lt;td colspan="2" style="text-align:center;"&gt;Linux&lt;sup&gt;2&lt;/sup&gt; / Windows / macOS&lt;/td&gt; 
   &lt;td style="text-align:center;"&gt;macOS&lt;sup&gt;3&lt;/sup&gt;&lt;/td&gt; 
   &lt;td style="text-align:center;"&gt;Linux&lt;sup&gt;2&lt;/sup&gt; / Windows&lt;sup&gt;4&lt;/sup&gt; &lt;/td&gt; 
   &lt;td&gt;Any&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th&gt;CPU inference support&lt;/th&gt; 
   &lt;td colspan="2" style="text-align:center;"&gt;âœ…&lt;/td&gt; 
   &lt;td colspan="2" style="text-align:center;"&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;Not required&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th&gt;GPU Requirements&lt;/th&gt;
   &lt;td colspan="2" style="text-align:center;"&gt;Volta or later architectures, 6 GB VRAM or more, or Apple Silicon&lt;/td&gt; 
   &lt;td&gt;Apple Silicon&lt;/td&gt; 
   &lt;td&gt;Volta or later architectures, 8 GB VRAM or more&lt;/td&gt; 
   &lt;td&gt;Not required&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Memory Requirements&lt;/th&gt; 
   &lt;td colspan="4" style="text-align:center;"&gt;Minimum 16 GB, 32 GB recommended&lt;/td&gt; 
   &lt;td&gt;8 GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Disk Space Requirements&lt;/th&gt; 
   &lt;td colspan="4" style="text-align:center;"&gt;20 GB or more, SSD recommended&lt;/td&gt; 
   &lt;td&gt;2 GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Python Version&lt;/th&gt; 
   &lt;td colspan="5" style="text-align:center;"&gt;3.10-3.13&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt; Accuracy metric is the End-to-End Evaluation Overall score of OmniDocBench (v1.5), tested on the latest &lt;code&gt;MinerU&lt;/code&gt; version.&lt;br /&gt; &lt;sup&gt;2&lt;/sup&gt; Linux supports only distributions released in 2019 or later.&lt;br /&gt; &lt;sup&gt;3&lt;/sup&gt; MLX requires macOS 13.5 or later, recommended for use with version 14.0 or higher.&lt;br /&gt; &lt;sup&gt;4&lt;/sup&gt; Windows vLLM support via WSL2(Windows Subsystem for Linux).&lt;br /&gt; &lt;sup&gt;5&lt;/sup&gt; Servers compatible with the OpenAI API, such as local or remote model services deployed via inference frameworks like &lt;code&gt;vLLM&lt;/code&gt;, &lt;code&gt;SGLang&lt;/code&gt;, or &lt;code&gt;LMDeploy&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Install MinerU&lt;/h3&gt; 
&lt;h4&gt;Install MinerU using pip or uv&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install --upgrade pip
pip install uv
uv pip install -U "mineru[core]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Install MinerU from source code&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/opendatalab/MinerU.git
cd MinerU
uv pip install -e .[core]
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] &lt;code&gt;mineru[core]&lt;/code&gt; includes all core features except &lt;code&gt;vLLM&lt;/code&gt; acceleration, compatible with Windows / Linux / macOS systems, suitable for most users. If you need to use &lt;code&gt;vLLM&lt;/code&gt; acceleration for VLM model inference or install a lightweight client on edge devices, please refer to the documentation &lt;a href="https://opendatalab.github.io/MinerU/quick_start/extension_modules/"&gt;Extension Modules Installation Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;h4&gt;Deploy MinerU using Docker&lt;/h4&gt; 
&lt;p&gt;MinerU provides a convenient Docker deployment method, which helps quickly set up the environment and solve some tricky environment compatibility issues. You can get the &lt;a href="https://opendatalab.github.io/MinerU/quick_start/docker_deployment/"&gt;Docker Deployment Instructions&lt;/a&gt; in the documentation.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Using MinerU&lt;/h3&gt; 
&lt;p&gt;The simplest command line invocation is:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mineru -p &amp;lt;input_path&amp;gt; -o &amp;lt;output_path&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can use MinerU for PDF parsing through various methods such as command line, API, and WebUI. For detailed instructions, please refer to the &lt;a href="https://opendatalab.github.io/MinerU/usage/"&gt;Usage Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;TODO&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Reading order based on the model&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Recognition of &lt;code&gt;index&lt;/code&gt; and &lt;code&gt;list&lt;/code&gt; in the main text&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Table recognition&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Heading Classification&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Handwritten Text Recognition&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Vertical Text Recognition&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Latin Accent Mark Recognition&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Code block recognition in the main text&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/opendatalab/MinerU/master/docs/chemical_knowledge_introduction/introduction.pdf"&gt;Chemical formula recognition&lt;/a&gt;(mineru.net)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Geometric shape recognition&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Known Issues&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;Reading order is determined by the model based on the spatial distribution of readable content, and may be out of order in some areas under extremely complex layouts.&lt;/li&gt; 
 &lt;li&gt;Limited support for vertical text.&lt;/li&gt; 
 &lt;li&gt;Tables of contents and lists are recognized through rules, and some uncommon list formats may not be recognized.&lt;/li&gt; 
 &lt;li&gt;Code blocks are not yet supported in the layout model.&lt;/li&gt; 
 &lt;li&gt;Comic books, art albums, primary school textbooks, and exercises cannot be parsed well.&lt;/li&gt; 
 &lt;li&gt;Table recognition may result in row/column recognition errors in complex tables.&lt;/li&gt; 
 &lt;li&gt;OCR recognition may produce inaccurate characters in PDFs of lesser-known languages (e.g., diacritical marks in Latin script, easily confused characters in Arabic script).&lt;/li&gt; 
 &lt;li&gt;Some formulas may not render correctly in Markdown.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;FAQ&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;If you encounter any issues during usage, you can first check the &lt;a href="https://opendatalab.github.io/MinerU/faq/"&gt;FAQ&lt;/a&gt; for solutions.&lt;/li&gt; 
 &lt;li&gt;If your issue remains unresolved, you may also use &lt;a href="https://deepwiki.com/opendatalab/MinerU"&gt;DeepWiki&lt;/a&gt; to interact with an AI assistant, which can address most common problems.&lt;/li&gt; 
 &lt;li&gt;If you still cannot resolve the issue, you are welcome to join our community via &lt;a href="https://discord.gg/Tdedn9GTXq"&gt;Discord&lt;/a&gt; or &lt;a href="https://mineru.net/community-portal/?aliasId=3c430f94"&gt;WeChat&lt;/a&gt; to discuss with other users and developers.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;All Thanks To Our Contributors&lt;/h1&gt; 
&lt;a href="https://github.com/opendatalab/MinerU/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=opendatalab/MinerU" /&gt; &lt;/a&gt; 
&lt;h1&gt;License Information&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/opendatalab/MinerU/master/LICENSE.md"&gt;LICENSE.md&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Currently, some models in this project are trained based on YOLO. However, since YOLO follows the AGPL license, it may impose restrictions on certain use cases. In future iterations, we plan to explore and replace these with models under more permissive licenses to enhance user-friendliness and flexibility.&lt;/p&gt; 
&lt;h1&gt;Acknowledgments&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/opendatalab/PDF-Extract-Kit"&gt;PDF-Extract-Kit&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/opendatalab/DocLayout-YOLO"&gt;DocLayout-YOLO&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/opendatalab/UniMERNet"&gt;UniMERNet&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/RapidAI/RapidTable"&gt;RapidTable&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/RapidAI/TableStructureRec"&gt;TableStructureRec&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/PaddlePaddle/PaddleOCR"&gt;PaddleOCR&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/frotms/PaddleOCR2Pytorch"&gt;PaddleOCR2Pytorch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ppaanngggg/layoutreader"&gt;layoutreader&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Sanster/xy-cut"&gt;xy-cut&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/LlmKira/fast-langdetect"&gt;fast-langdetect&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pypdfium2-team/pypdfium2"&gt;pypdfium2&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/datalab-to/pdftext"&gt;pdftext&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pdfminer/pdfminer.six"&gt;pdfminer.six&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/py-pdf/pypdf"&gt;pypdf&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/google/magika"&gt;magika&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Citation&lt;/h1&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{niu2025mineru25decoupledvisionlanguagemodel,
      title={MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing}, 
      author={Junbo Niu and Zheng Liu and Zhuangcheng Gu and Bin Wang and Linke Ouyang and Zhiyuan Zhao and Tao Chu and Tianyao He and Fan Wu and Qintong Zhang and Zhenjiang Jin and Guang Liang and Rui Zhang and Wenzheng Zhang and Yuan Qu and Zhifei Ren and Yuefeng Sun and Yuanhong Zheng and Dongsheng Ma and Zirui Tang and Boyu Niu and Ziyang Miao and Hejun Dong and Siyi Qian and Junyuan Zhang and Jingzhou Chen and Fangdong Wang and Xiaomeng Zhao and Liqun Wei and Wei Li and Shasha Wang and Ruiliang Xu and Yuanyuan Cao and Lu Chen and Qianqian Wu and Huaiyu Gu and Lindong Lu and Keming Wang and Dechen Lin and Guanlin Shen and Xuanhe Zhou and Linfeng Zhang and Yuhang Zang and Xiaoyi Dong and Jiaqi Wang and Bo Zhang and Lei Bai and Pei Chu and Weijia Li and Jiang Wu and Lijun Wu and Zhenxiang Li and Guangyu Wang and Zhongying Tu and Chao Xu and Kai Chen and Yu Qiao and Bowen Zhou and Dahua Lin and Wentao Zhang and Conghui He},
      year={2025},
      eprint={2509.22186},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2509.22186}, 
}

@misc{wang2024mineruopensourcesolutionprecise,
      title={MinerU: An Open-Source Solution for Precise Document Content Extraction}, 
      author={Bin Wang and Chao Xu and Xiaomeng Zhao and Linke Ouyang and Fan Wu and Zhiyuan Zhao and Rui Xu and Kaiwen Liu and Yuan Qu and Fukai Shang and Bo Zhang and Liqun Wei and Zhihao Sui and Wei Li and Botian Shi and Yu Qiao and Dahua Lin and Conghui He},
      year={2024},
      eprint={2409.18839},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2409.18839}, 
}

@article{he2024opendatalab,
  title={Opendatalab: Empowering general artificial intelligence with open datasets},
  author={He, Conghui and Li, Wei and Jin, Zhenjiang and Xu, Chao and Wang, Bin and Lin, Dahua},
  journal={arXiv preprint arXiv:2407.13773},
  year={2024}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Star History&lt;/h1&gt; 
&lt;a&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=opendatalab/MinerU&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=opendatalab/MinerU&amp;amp;type=Date" /&gt; 
  &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=opendatalab/MinerU&amp;amp;type=Date" /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;h1&gt;Links&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenDCAI/DataFlow"&gt;Easy Data Preparation with latest LLMs-based Operators and Pipelines&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/opendatalab/Vis3"&gt;Vis3 (OSS browser based on s3)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/opendatalab/labelU"&gt;LabelU (A Lightweight Multi-modal Data Annotation Tool)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/opendatalab/LabelLLM"&gt;LabelLLM (An Open-source LLM Dialogue Annotation Platform)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/opendatalab/PDF-Extract-Kit"&gt;PDF-Extract-Kit (A Comprehensive Toolkit for High-Quality PDF Content Extraction)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/opendatalab/OmniDocBench"&gt;OmniDocBench (A Comprehensive Benchmark for Document Parsing and Evaluation)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/opendatalab/magic-html"&gt;Magic-HTML (Mixed web page extraction tool)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/InternLM/magic-doc"&gt;Magic-Doc (Fast speed ppt/pptx/doc/docx/pdf extraction tool)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/MigoXLab/dingo"&gt;Dingo: A Comprehensive AI Data Quality Evaluation Tool&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
  </channel>
</rss>