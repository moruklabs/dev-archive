<rss version="2.0">
  <channel>
    <title>GitHub Python Daily Trending</title>
    <description>Daily Trending of Python in GitHub</description>
    <pubDate>Fri, 22 Aug 2025 01:37:45 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>sooperset/mcp-atlassian</title>
      <link>https://github.com/sooperset/mcp-atlassian</link>
      <description>&lt;p&gt;MCP server for Atlassian tools (Confluence, Jira)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MCP Atlassian&lt;/h1&gt; 
&lt;p&gt;&lt;img src="https://img.shields.io/pypi/v/mcp-atlassian" alt="PyPI Version" /&gt; &lt;img src="https://img.shields.io/pypi/dm/mcp-atlassian" alt="PyPI - Downloads" /&gt; &lt;img src="https://static.pepy.tech/personalized-badge/mcp-atlassian?period=total&amp;amp;units=international_system&amp;amp;left_color=grey&amp;amp;right_color=blue&amp;amp;left_text=Total%20Downloads" alt="PePy - Total Downloads" /&gt; &lt;a href="https://github.com/sooperset/mcp-atlassian/actions/workflows/tests.yml"&gt;&lt;img src="https://github.com/sooperset/mcp-atlassian/actions/workflows/tests.yml/badge.svg?sanitize=true" alt="Run Tests" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/github/license/sooperset/mcp-atlassian" alt="License" /&gt;&lt;/p&gt; 
&lt;p&gt;Model Context Protocol (MCP) server for Atlassian products (Confluence and Jira). This integration supports both Confluence &amp;amp; Jira Cloud and Server/Data Center deployments.&lt;/p&gt; 
&lt;h2&gt;Example Usage&lt;/h2&gt; 
&lt;p&gt;Ask your AI assistant to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;üìù Automatic Jira Updates&lt;/strong&gt; - "Update Jira from our meeting notes"&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üîç AI-Powered Confluence Search&lt;/strong&gt; - "Find our OKR guide in Confluence and summarize it"&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üêõ Smart Jira Issue Filtering&lt;/strong&gt; - "Show me urgent bugs in PROJ project from last week"&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üìÑ Content Creation &amp;amp; Management&lt;/strong&gt; - "Create a tech design doc for XYZ feature"&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Feature Demo&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/35303504-14c6-4ae4-913b-7c25ea511c3e"&gt;https://github.com/user-attachments/assets/35303504-14c6-4ae4-913b-7c25ea511c3e&lt;/a&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Confluence Demo&lt;/summary&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/7fe9c488-ad0c-4876-9b54-120b666bb785"&gt;https://github.com/user-attachments/assets/7fe9c488-ad0c-4876-9b54-120b666bb785&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h3&gt;Compatibility&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Product&lt;/th&gt; 
   &lt;th&gt;Deployment Type&lt;/th&gt; 
   &lt;th&gt;Support Status&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Confluence&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Cloud&lt;/td&gt; 
   &lt;td&gt;‚úÖ Fully supported&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Confluence&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Server/Data Center&lt;/td&gt; 
   &lt;td&gt;‚úÖ Supported (version 6.0+)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Jira&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Cloud&lt;/td&gt; 
   &lt;td&gt;‚úÖ Fully supported&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Jira&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Server/Data Center&lt;/td&gt; 
   &lt;td&gt;‚úÖ Supported (version 8.14+)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Quick Start Guide&lt;/h2&gt; 
&lt;h3&gt;üîê 1. Authentication Setup&lt;/h3&gt; 
&lt;p&gt;MCP Atlassian supports three authentication methods:&lt;/p&gt; 
&lt;h4&gt;A. API Token Authentication (Cloud) - &lt;strong&gt;Recommended&lt;/strong&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Go to &lt;a href="https://id.atlassian.com/manage-profile/security/api-tokens"&gt;https://id.atlassian.com/manage-profile/security/api-tokens&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Click &lt;strong&gt;Create API token&lt;/strong&gt;, name it&lt;/li&gt; 
 &lt;li&gt;Copy the token immediately&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;B. Personal Access Token (Server/Data Center)&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Go to your profile (avatar) ‚Üí &lt;strong&gt;Profile&lt;/strong&gt; ‚Üí &lt;strong&gt;Personal Access Tokens&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Click &lt;strong&gt;Create token&lt;/strong&gt;, name it, set expiry&lt;/li&gt; 
 &lt;li&gt;Copy the token immediately&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;C. OAuth 2.0 Authentication (Cloud) - &lt;strong&gt;Advanced&lt;/strong&gt;&lt;/h4&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] OAuth 2.0 is more complex to set up but provides enhanced security features. For most users, API Token authentication (Method A) is simpler and sufficient.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ol&gt; 
 &lt;li&gt;Go to &lt;a href="https://developer.atlassian.com/console/myapps/"&gt;Atlassian Developer Console&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Create an "OAuth 2.0 (3LO) integration" app&lt;/li&gt; 
 &lt;li&gt;Configure &lt;strong&gt;Permissions&lt;/strong&gt; (scopes) for Jira/Confluence&lt;/li&gt; 
 &lt;li&gt;Set &lt;strong&gt;Callback URL&lt;/strong&gt; (e.g., &lt;code&gt;http://localhost:8080/callback&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Run setup wizard: &lt;pre&gt;&lt;code class="language-bash"&gt;docker run --rm -i \
  -p 8080:8080 \
  -v "${HOME}/.mcp-atlassian:/home/app/.mcp-atlassian" \
  ghcr.io/sooperset/mcp-atlassian:latest --oauth-setup -v
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Follow prompts for &lt;code&gt;Client ID&lt;/code&gt;, &lt;code&gt;Secret&lt;/code&gt;, &lt;code&gt;URI&lt;/code&gt;, and &lt;code&gt;Scope&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Complete browser authorization&lt;/li&gt; 
 &lt;li&gt;Add obtained credentials to &lt;code&gt;.env&lt;/code&gt; or IDE config: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;ATLASSIAN_OAUTH_CLOUD_ID&lt;/code&gt; (from wizard)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;ATLASSIAN_OAUTH_CLIENT_ID&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;ATLASSIAN_OAUTH_CLIENT_SECRET&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;ATLASSIAN_OAUTH_REDIRECT_URI&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;ATLASSIAN_OAUTH_SCOPE&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] For the standard OAuth flow described above, include &lt;code&gt;offline_access&lt;/code&gt; in your scope (e.g., &lt;code&gt;read:jira-work write:jira-work offline_access&lt;/code&gt;). This allows the server to refresh the access token automatically.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;details&gt; 
 &lt;summary&gt;Alternative: Using a Pre-existing OAuth Access Token (BYOT)&lt;/summary&gt; 
 &lt;p&gt;If you are running mcp-atlassian part of a larger system that manages Atlassian OAuth 2.0 access tokens externally (e.g., through a central identity provider or another application), you can provide an access token directly to this MCP server. This method bypasses the interactive setup wizard and the server's internal token management (including refresh capabilities).&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Requirements:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;A valid Atlassian OAuth 2.0 Access Token with the necessary scopes for the intended operations.&lt;/li&gt; 
  &lt;li&gt;The corresponding &lt;code&gt;ATLASSIAN_OAUTH_CLOUD_ID&lt;/code&gt; for your Atlassian instance.&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;Configuration:&lt;/strong&gt; To use this method, set the following environment variables (or use the corresponding command-line flags when starting the server):&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;ATLASSIAN_OAUTH_CLOUD_ID&lt;/code&gt;: Your Atlassian Cloud ID. (CLI: &lt;code&gt;--oauth-cloud-id&lt;/code&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;ATLASSIAN_OAUTH_ACCESS_TOKEN&lt;/code&gt;: Your pre-existing OAuth 2.0 access token. (CLI: &lt;code&gt;--oauth-access-token&lt;/code&gt;)&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;Important Considerations for BYOT:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Token Lifecycle Management:&lt;/strong&gt; When using BYOT, the MCP server &lt;strong&gt;does not&lt;/strong&gt; handle token refresh. The responsibility for obtaining, refreshing (before expiry), and revoking the access token lies entirely with you or the external system providing the token.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Unused Variables:&lt;/strong&gt; The standard OAuth client variables (&lt;code&gt;ATLASSIAN_OAUTH_CLIENT_ID&lt;/code&gt;, &lt;code&gt;ATLASSIAN_OAUTH_CLIENT_SECRET&lt;/code&gt;, &lt;code&gt;ATLASSIAN_OAUTH_REDIRECT_URI&lt;/code&gt;, &lt;code&gt;ATLASSIAN_OAUTH_SCOPE&lt;/code&gt;) are &lt;strong&gt;not&lt;/strong&gt; used and can be omitted when configuring for BYOT.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;No Setup Wizard:&lt;/strong&gt; The &lt;code&gt;--oauth-setup&lt;/code&gt; wizard is not applicable and should not be used for this approach.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;No Token Cache Volume:&lt;/strong&gt; The Docker volume mount for token storage (e.g., &lt;code&gt;-v "${HOME}/.mcp-atlassian:/home/app/.mcp-atlassian"&lt;/code&gt;) is also not necessary if you are exclusively using the BYOT method, as no tokens are stored or managed by this server.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Scope:&lt;/strong&gt; The provided access token must already have the necessary permissions (scopes) for the Jira/Confluence operations you intend to perform.&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;This option is useful in scenarios where OAuth credential management is centralized or handled by other infrastructure components.&lt;/p&gt; 
&lt;/details&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] &lt;strong&gt;Multi-Cloud OAuth Support&lt;/strong&gt;: If you're building a multi-tenant application where users provide their own OAuth tokens, see the &lt;a href="https://raw.githubusercontent.com/sooperset/mcp-atlassian/main/#multi-cloud-oauth-support"&gt;Multi-Cloud OAuth Support&lt;/a&gt; section for minimal configuration setup.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;üì¶ 2. Installation&lt;/h3&gt; 
&lt;p&gt;MCP Atlassian is distributed as a Docker image. This is the recommended way to run the server, especially for IDE integration. Ensure you have Docker installed.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Pull Pre-built Image
docker pull ghcr.io/sooperset/mcp-atlassian:latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;üõ†Ô∏è IDE Integration&lt;/h2&gt; 
&lt;p&gt;MCP Atlassian is designed to be used with AI assistants through IDE integration.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] &lt;strong&gt;For Claude Desktop&lt;/strong&gt;: Locate and edit the configuration file directly:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt;: &lt;code&gt;%APPDATA%\Claude\claude_desktop_config.json&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;macOS&lt;/strong&gt;: &lt;code&gt;~/Library/Application Support/Claude/claude_desktop_config.json&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Linux&lt;/strong&gt;: &lt;code&gt;~/.config/Claude/claude_desktop_config.json&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;For Cursor&lt;/strong&gt;: Open Settings ‚Üí MCP ‚Üí + Add new global MCP server&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;‚öôÔ∏è Configuration Methods&lt;/h3&gt; 
&lt;p&gt;There are two main approaches to configure the Docker container:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Passing Variables Directly&lt;/strong&gt; (shown in examples below)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Using an Environment File&lt;/strong&gt; with &lt;code&gt;--env-file&lt;/code&gt; flag (shown in collapsible sections)&lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Common environment variables include:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;CONFLUENCE_SPACES_FILTER&lt;/code&gt;: Filter by space keys (e.g., "DEV,TEAM,DOC")&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;JIRA_PROJECTS_FILTER&lt;/code&gt;: Filter by project keys (e.g., "PROJ,DEV,SUPPORT")&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;READ_ONLY_MODE&lt;/code&gt;: Set to "true" to disable write operations&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;MCP_VERBOSE&lt;/code&gt;: Set to "true" for more detailed logging&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;MCP_LOGGING_STDOUT&lt;/code&gt;: Set to "true" to log to stdout instead of stderr&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;ENABLED_TOOLS&lt;/code&gt;: Comma-separated list of tool names to enable (e.g., "confluence_search,jira_get_issue")&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;See the &lt;a href="https://github.com/sooperset/mcp-atlassian/raw/main/.env.example"&gt;.env.example&lt;/a&gt; file for all available options.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;üìù Configuration Examples&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Method 1 (Passing Variables Directly):&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "mcp-atlassian": {
      "command": "docker",
      "args": [
        "run",
        "-i",
        "--rm",
        "-e", "CONFLUENCE_URL",
        "-e", "CONFLUENCE_USERNAME",
        "-e", "CONFLUENCE_API_TOKEN",
        "-e", "JIRA_URL",
        "-e", "JIRA_USERNAME",
        "-e", "JIRA_API_TOKEN",
        "ghcr.io/sooperset/mcp-atlassian:latest"
      ],
      "env": {
        "CONFLUENCE_URL": "https://your-company.atlassian.net/wiki",
        "CONFLUENCE_USERNAME": "your.email@company.com",
        "CONFLUENCE_API_TOKEN": "your_confluence_api_token",
        "JIRA_URL": "https://your-company.atlassian.net",
        "JIRA_USERNAME": "your.email@company.com",
        "JIRA_API_TOKEN": "your_jira_api_token"
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;Alternative: Using Environment File&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "mcp-atlassian": {
      "command": "docker",
      "args": [
        "run",
        "--rm",
        "-i",
        "--env-file",
        "/path/to/your/mcp-atlassian.env",
        "ghcr.io/sooperset/mcp-atlassian:latest"
      ]
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Server/Data Center Configuration&lt;/summary&gt; 
 &lt;p&gt;For Server/Data Center deployments, use direct variable passing:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "mcp-atlassian": {
      "command": "docker",
      "args": [
        "run",
        "--rm",
        "-i",
        "-e", "CONFLUENCE_URL",
        "-e", "CONFLUENCE_PERSONAL_TOKEN",
        "-e", "CONFLUENCE_SSL_VERIFY",
        "-e", "JIRA_URL",
        "-e", "JIRA_PERSONAL_TOKEN",
        "-e", "JIRA_SSL_VERIFY",
        "ghcr.io/sooperset/mcp-atlassian:latest"
      ],
      "env": {
        "CONFLUENCE_URL": "https://confluence.your-company.com",
        "CONFLUENCE_PERSONAL_TOKEN": "your_confluence_pat",
        "CONFLUENCE_SSL_VERIFY": "false",
        "JIRA_URL": "https://jira.your-company.com",
        "JIRA_PERSONAL_TOKEN": "your_jira_pat",
        "JIRA_SSL_VERIFY": "false"
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;[!NOTE] Set &lt;code&gt;CONFLUENCE_SSL_VERIFY&lt;/code&gt; and &lt;code&gt;JIRA_SSL_VERIFY&lt;/code&gt; to "false" only if you have self-signed certificates.&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;OAuth 2.0 Configuration (Cloud Only)&lt;/summary&gt; 
 &lt;a name="oauth-20-configuration-example-cloud-only"&gt;&lt;/a&gt; 
 &lt;p&gt;These examples show how to configure &lt;code&gt;mcp-atlassian&lt;/code&gt; in your IDE (like Cursor or Claude Desktop) when using OAuth 2.0 for Atlassian Cloud.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Example for Standard OAuth 2.0 Flow (using Setup Wizard):&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;This configuration is for when you use the server's built-in OAuth client and have completed the &lt;a href="https://raw.githubusercontent.com/sooperset/mcp-atlassian/main/#c-oauth-20-authentication-cloud---advanced"&gt;OAuth setup wizard&lt;/a&gt;.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "mcp-atlassian": {
      "command": "docker",
      "args": [
        "run",
        "--rm",
        "-i",
        "-v", "&amp;lt;path_to_your_home&amp;gt;/.mcp-atlassian:/home/app/.mcp-atlassian",
        "-e", "JIRA_URL",
        "-e", "CONFLUENCE_URL",
        "-e", "ATLASSIAN_OAUTH_CLIENT_ID",
        "-e", "ATLASSIAN_OAUTH_CLIENT_SECRET",
        "-e", "ATLASSIAN_OAUTH_REDIRECT_URI",
        "-e", "ATLASSIAN_OAUTH_SCOPE",
        "-e", "ATLASSIAN_OAUTH_CLOUD_ID",
        "ghcr.io/sooperset/mcp-atlassian:latest"
      ],
      "env": {
        "JIRA_URL": "https://your-company.atlassian.net",
        "CONFLUENCE_URL": "https://your-company.atlassian.net/wiki",
        "ATLASSIAN_OAUTH_CLIENT_ID": "YOUR_OAUTH_APP_CLIENT_ID",
        "ATLASSIAN_OAUTH_CLIENT_SECRET": "YOUR_OAUTH_APP_CLIENT_SECRET",
        "ATLASSIAN_OAUTH_REDIRECT_URI": "http://localhost:8080/callback",
        "ATLASSIAN_OAUTH_SCOPE": "read:jira-work write:jira-work read:confluence-content.all write:confluence-content offline_access",
        "ATLASSIAN_OAUTH_CLOUD_ID": "YOUR_CLOUD_ID_FROM_SETUP_WIZARD"
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;[!NOTE]&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;For the Standard Flow: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;code&gt;ATLASSIAN_OAUTH_CLOUD_ID&lt;/code&gt; is obtained from the &lt;code&gt;--oauth-setup&lt;/code&gt; wizard output or is known for your instance.&lt;/li&gt; 
     &lt;li&gt;Other &lt;code&gt;ATLASSIAN_OAUTH_*&lt;/code&gt; client variables are from your OAuth app in the Atlassian Developer Console.&lt;/li&gt; 
     &lt;li&gt;&lt;code&gt;JIRA_URL&lt;/code&gt; and &lt;code&gt;CONFLUENCE_URL&lt;/code&gt; for your Cloud instances are always required.&lt;/li&gt; 
     &lt;li&gt;The volume mount (&lt;code&gt;-v .../.mcp-atlassian:/home/app/.mcp-atlassian&lt;/code&gt;) is crucial for persisting the OAuth tokens obtained by the wizard, enabling automatic refresh.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Example for Pre-existing Access Token (BYOT - Bring Your Own Token):&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;This configuration is for when you are providing your own externally managed OAuth 2.0 access token.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "mcp-atlassian": {
      "command": "docker",
      "args": [
        "run",
        "--rm",
        "-i",
        "-e", "JIRA_URL",
        "-e", "CONFLUENCE_URL",
        "-e", "ATLASSIAN_OAUTH_CLOUD_ID",
        "-e", "ATLASSIAN_OAUTH_ACCESS_TOKEN",
        "ghcr.io/sooperset/mcp-atlassian:latest"
      ],
      "env": {
        "JIRA_URL": "https://your-company.atlassian.net",
        "CONFLUENCE_URL": "https://your-company.atlassian.net/wiki",
        "ATLASSIAN_OAUTH_CLOUD_ID": "YOUR_KNOWN_CLOUD_ID",
        "ATLASSIAN_OAUTH_ACCESS_TOKEN": "YOUR_PRE_EXISTING_OAUTH_ACCESS_TOKEN"
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;[!NOTE]&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;For the BYOT Method: 
    &lt;ul&gt; 
     &lt;li&gt;You primarily need &lt;code&gt;JIRA_URL&lt;/code&gt;, &lt;code&gt;CONFLUENCE_URL&lt;/code&gt;, &lt;code&gt;ATLASSIAN_OAUTH_CLOUD_ID&lt;/code&gt;, and &lt;code&gt;ATLASSIAN_OAUTH_ACCESS_TOKEN&lt;/code&gt;.&lt;/li&gt; 
     &lt;li&gt;Standard OAuth client variables (&lt;code&gt;ATLASSIAN_OAUTH_CLIENT_ID&lt;/code&gt;, &lt;code&gt;CLIENT_SECRET&lt;/code&gt;, &lt;code&gt;REDIRECT_URI&lt;/code&gt;, &lt;code&gt;SCOPE&lt;/code&gt;) are &lt;strong&gt;not&lt;/strong&gt; used.&lt;/li&gt; 
     &lt;li&gt;Token lifecycle (e.g., refreshing the token before it expires and restarting mcp-atlassian) is your responsibility, as the server will not refresh BYOT tokens.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/blockquote&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Proxy Configuration&lt;/summary&gt; 
 &lt;p&gt;MCP Atlassian supports routing API requests through standard HTTP/HTTPS/SOCKS proxies. Configure using environment variables:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Supports standard &lt;code&gt;HTTP_PROXY&lt;/code&gt;, &lt;code&gt;HTTPS_PROXY&lt;/code&gt;, &lt;code&gt;NO_PROXY&lt;/code&gt;, &lt;code&gt;SOCKS_PROXY&lt;/code&gt;.&lt;/li&gt; 
  &lt;li&gt;Service-specific overrides are available (e.g., &lt;code&gt;JIRA_HTTPS_PROXY&lt;/code&gt;, &lt;code&gt;CONFLUENCE_NO_PROXY&lt;/code&gt;).&lt;/li&gt; 
  &lt;li&gt;Service-specific variables override global ones for that service.&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;Add the relevant proxy variables to the &lt;code&gt;args&lt;/code&gt; (using &lt;code&gt;-e&lt;/code&gt;) and &lt;code&gt;env&lt;/code&gt; sections of your MCP configuration:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "mcp-atlassian": {
      "command": "docker",
      "args": [
        "run",
        "-i",
        "--rm",
        "-e", "... existing Confluence/Jira vars",
        "-e", "HTTP_PROXY",
        "-e", "HTTPS_PROXY",
        "-e", "NO_PROXY",
        "ghcr.io/sooperset/mcp-atlassian:latest"
      ],
      "env": {
        "... existing Confluence/Jira vars": "...",
        "HTTP_PROXY": "http://proxy.internal:8080",
        "HTTPS_PROXY": "http://proxy.internal:8080",
        "NO_PROXY": "localhost,.your-company.com"
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Credentials in proxy URLs are masked in logs. If you set &lt;code&gt;NO_PROXY&lt;/code&gt;, it will be respected for requests to matching hosts.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Custom HTTP Headers Configuration&lt;/summary&gt; 
 &lt;p&gt;MCP Atlassian supports adding custom HTTP headers to all API requests. This feature is particularly useful in corporate environments where additional headers are required for security, authentication, or routing purposes.&lt;/p&gt; 
 &lt;p&gt;Custom headers are configured using environment variables with comma-separated key=value pairs:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "mcp-atlassian": {
      "command": "docker",
      "args": [
        "run",
        "-i",
        "--rm",
        "-e", "CONFLUENCE_URL",
        "-e", "CONFLUENCE_USERNAME",
        "-e", "CONFLUENCE_API_TOKEN",
        "-e", "CONFLUENCE_CUSTOM_HEADERS",
        "-e", "JIRA_URL",
        "-e", "JIRA_USERNAME",
        "-e", "JIRA_API_TOKEN",
        "-e", "JIRA_CUSTOM_HEADERS",
        "ghcr.io/sooperset/mcp-atlassian:latest"
      ],
      "env": {
        "CONFLUENCE_URL": "https://your-company.atlassian.net/wiki",
        "CONFLUENCE_USERNAME": "your.email@company.com",
        "CONFLUENCE_API_TOKEN": "your_confluence_api_token",
        "CONFLUENCE_CUSTOM_HEADERS": "X-Confluence-Service=mcp-integration,X-Custom-Auth=confluence-token,X-ALB-Token=secret-token",
        "JIRA_URL": "https://your-company.atlassian.net",
        "JIRA_USERNAME": "your.email@company.com",
        "JIRA_API_TOKEN": "your_jira_api_token",
        "JIRA_CUSTOM_HEADERS": "X-Forwarded-User=service-account,X-Company-Service=mcp-atlassian,X-Jira-Client=mcp-integration"
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Security Considerations:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Custom header values are masked in debug logs to protect sensitive information&lt;/li&gt; 
  &lt;li&gt;Ensure custom headers don't conflict with standard HTTP or Atlassian API headers&lt;/li&gt; 
  &lt;li&gt;Avoid including sensitive authentication tokens in custom headers if already using basic auth or OAuth&lt;/li&gt; 
  &lt;li&gt;Headers are sent with every API request - verify they don't interfere with API functionality&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Multi-Cloud OAuth Support&lt;/summary&gt; 
 &lt;p&gt;MCP Atlassian supports multi-cloud OAuth scenarios where each user connects to their own Atlassian cloud instance. This is useful for multi-tenant applications, chatbots, or services where users provide their own OAuth tokens.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Minimal OAuth Configuration:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;Enable minimal OAuth mode (no client credentials required):&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;docker run -e ATLASSIAN_OAUTH_ENABLE=true -p 9000:9000 \
  ghcr.io/sooperset/mcp-atlassian:latest \
  --transport streamable-http --port 9000
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Users provide authentication via HTTP headers:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;code&gt;Authorization: Bearer &amp;lt;user_oauth_token&amp;gt;&lt;/code&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;code&gt;X-Atlassian-Cloud-Id: &amp;lt;user_cloud_id&amp;gt;&lt;/code&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;&lt;strong&gt;Example Integration (Python):&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import asyncio
from mcp.client.streamable_http import streamablehttp_client
from mcp import ClientSession

user_token = "user-specific-oauth-token"
user_cloud_id = "user-specific-cloud-id"

async def main():
    # Connect to streamable HTTP server with custom headers
    async with streamablehttp_client(
        "http://localhost:9000/mcp",
        headers={
            "Authorization": f"Bearer {user_token}",
            "X-Atlassian-Cloud-Id": user_cloud_id
        }
    ) as (read_stream, write_stream, _):
        # Create a session using the client streams
        async with ClientSession(read_stream, write_stream) as session:
            # Initialize the connection
            await session.initialize()

            # Example: Get a Jira issue
            result = await session.call_tool(
                "jira_get_issue",
                {"issue_key": "PROJ-123"}
            )
            print(result)

asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Configuration Notes:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Each request can use a different cloud instance via the &lt;code&gt;X-Atlassian-Cloud-Id&lt;/code&gt; header&lt;/li&gt; 
  &lt;li&gt;User tokens are isolated per request - no cross-tenant data leakage&lt;/li&gt; 
  &lt;li&gt;Falls back to global &lt;code&gt;ATLASSIAN_OAUTH_CLOUD_ID&lt;/code&gt; if header not provided&lt;/li&gt; 
  &lt;li&gt;Compatible with standard OAuth 2.0 bearer token authentication&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Single Service Configurations&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;For Confluence Cloud only:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "mcp-atlassian": {
      "command": "docker",
      "args": [
        "run",
        "--rm",
        "-i",
        "-e", "CONFLUENCE_URL",
        "-e", "CONFLUENCE_USERNAME",
        "-e", "CONFLUENCE_API_TOKEN",
        "ghcr.io/sooperset/mcp-atlassian:latest"
      ],
      "env": {
        "CONFLUENCE_URL": "https://your-company.atlassian.net/wiki",
        "CONFLUENCE_USERNAME": "your.email@company.com",
        "CONFLUENCE_API_TOKEN": "your_api_token"
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;For Confluence Server/DC, use:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "mcp-atlassian": {
      "command": "docker",
      "args": [
        "run",
        "--rm",
        "-i",
        "-e", "CONFLUENCE_URL",
        "-e", "CONFLUENCE_PERSONAL_TOKEN",
        "ghcr.io/sooperset/mcp-atlassian:latest"
      ],
      "env": {
        "CONFLUENCE_URL": "https://confluence.your-company.com",
        "CONFLUENCE_PERSONAL_TOKEN": "your_personal_token"
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;For Jira Cloud only:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "mcp-atlassian": {
      "command": "docker",
      "args": [
        "run",
        "--rm",
        "-i",
        "-e", "JIRA_URL",
        "-e", "JIRA_USERNAME",
        "-e", "JIRA_API_TOKEN",
        "ghcr.io/sooperset/mcp-atlassian:latest"
      ],
      "env": {
        "JIRA_URL": "https://your-company.atlassian.net",
        "JIRA_USERNAME": "your.email@company.com",
        "JIRA_API_TOKEN": "your_api_token"
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;For Jira Server/DC, use:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "mcp-atlassian": {
      "command": "docker",
      "args": [
        "run",
        "--rm",
        "-i",
        "-e", "JIRA_URL",
        "-e", "JIRA_PERSONAL_TOKEN",
        "ghcr.io/sooperset/mcp-atlassian:latest"
      ],
      "env": {
        "JIRA_URL": "https://jira.your-company.com",
        "JIRA_PERSONAL_TOKEN": "your_personal_token"
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;üë• HTTP Transport Configuration&lt;/h3&gt; 
&lt;p&gt;Instead of using &lt;code&gt;stdio&lt;/code&gt;, you can run the server as a persistent HTTP service using either:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;sse&lt;/code&gt; (Server-Sent Events) transport at &lt;code&gt;/sse&lt;/code&gt; endpoint&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;streamable-http&lt;/code&gt; transport at &lt;code&gt;/mcp&lt;/code&gt; endpoint&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Both transport types support single-user and multi-user authentication:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Authentication Options:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Single-User&lt;/strong&gt;: Use server-level authentication configured via environment variables&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-User&lt;/strong&gt;: Each user provides their own authentication: 
  &lt;ul&gt; 
   &lt;li&gt;Cloud: OAuth 2.0 Bearer tokens&lt;/li&gt; 
   &lt;li&gt;Server/Data Center: Personal Access Tokens (PATs)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;Basic HTTP Transport Setup&lt;/summary&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;Start the server with your chosen transport:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# For SSE transport
docker run --rm -p 9000:9000 \
  --env-file /path/to/your/.env \
  ghcr.io/sooperset/mcp-atlassian:latest \
  --transport sse --port 9000 -vv

# OR for streamable-http transport
docker run --rm -p 9000:9000 \
  --env-file /path/to/your/.env \
  ghcr.io/sooperset/mcp-atlassian:latest \
  --transport streamable-http --port 9000 -vv
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Configure your IDE (single-user example):&lt;/p&gt; &lt;p&gt;&lt;strong&gt;SSE Transport Example:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "mcp-atlassian-http": {
      "url": "http://localhost:9000/sse"
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Streamable-HTTP Transport Example:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "mcp-atlassian-service": {
      "url": "http://localhost:9000/mcp"
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Multi-User Authentication Setup&lt;/summary&gt; 
 &lt;p&gt;Here's a complete example of setting up multi-user authentication with streamable-HTTP transport:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;First, run the OAuth setup wizard to configure the server's OAuth credentials:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;docker run --rm -i \
  -p 8080:8080 \
  -v "${HOME}/.mcp-atlassian:/home/app/.mcp-atlassian" \
  ghcr.io/sooperset/mcp-atlassian:latest --oauth-setup -v
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Start the server with streamable-HTTP transport:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;docker run --rm -p 9000:9000 \
  --env-file /path/to/your/.env \
  ghcr.io/sooperset/mcp-atlassian:latest \
  --transport streamable-http --port 9000 -vv
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Configure your IDE's MCP settings:&lt;/p&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;&lt;strong&gt;Choose the appropriate Authorization method for your Atlassian deployment:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Cloud (OAuth 2.0):&lt;/strong&gt; Use this if your organization is on Atlassian Cloud and you have an OAuth access token for each user.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Server/Data Center (PAT):&lt;/strong&gt; Use this if you are on Atlassian Server or Data Center and each user has a Personal Access Token (PAT).&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;Cloud (OAuth 2.0) Example:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "mcp-atlassian-service": {
      "url": "http://localhost:9000/mcp",
      "headers": {
        "Authorization": "Bearer &amp;lt;USER_OAUTH_ACCESS_TOKEN&amp;gt;"
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Server/Data Center (PAT) Example:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "mcp-atlassian-service": {
      "url": "http://localhost:9000/mcp",
      "headers": {
        "Authorization": "Token &amp;lt;USER_PERSONAL_ACCESS_TOKEN&amp;gt;"
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="4"&gt; 
  &lt;li&gt;Required environment variables in &lt;code&gt;.env&lt;/code&gt;: &lt;pre&gt;&lt;code class="language-bash"&gt;JIRA_URL=https://your-company.atlassian.net
CONFLUENCE_URL=https://your-company.atlassian.net/wiki
ATLASSIAN_OAUTH_CLIENT_ID=your_oauth_app_client_id
ATLASSIAN_OAUTH_CLIENT_SECRET=your_oauth_app_client_secret
ATLASSIAN_OAUTH_REDIRECT_URI=http://localhost:8080/callback
ATLASSIAN_OAUTH_SCOPE=read:jira-work write:jira-work read:confluence-content.all write:confluence-content offline_access
ATLASSIAN_OAUTH_CLOUD_ID=your_cloud_id_from_setup_wizard
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;[!NOTE]&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;The server should have its own fallback authentication configured (e.g., via environment variables for API token, PAT, or its own OAuth setup using --oauth-setup). This is used if a request doesn't include user-specific authentication.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;OAuth&lt;/strong&gt;: Each user needs their own OAuth access token from your Atlassian OAuth app.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;PAT&lt;/strong&gt;: Each user provides their own Personal Access Token.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Multi-Cloud&lt;/strong&gt;: For OAuth users, optionally include &lt;code&gt;X-Atlassian-Cloud-Id&lt;/code&gt; header to specify which Atlassian cloud instance to use&lt;/li&gt; 
   &lt;li&gt;The server will use the user's token for API calls when provided, falling back to server auth if not&lt;/li&gt; 
   &lt;li&gt;User tokens should have appropriate scopes for their needed operations&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/blockquote&gt; 
&lt;/details&gt; 
&lt;h2&gt;Tools&lt;/h2&gt; 
&lt;h3&gt;Key Tools&lt;/h3&gt; 
&lt;h4&gt;Jira Tools&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;jira_get_issue&lt;/code&gt;: Get details of a specific issue&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;jira_search&lt;/code&gt;: Search issues using JQL&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;jira_create_issue&lt;/code&gt;: Create a new issue&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;jira_update_issue&lt;/code&gt;: Update an existing issue&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;jira_transition_issue&lt;/code&gt;: Transition an issue to a new status&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;jira_add_comment&lt;/code&gt;: Add a comment to an issue&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Confluence Tools&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;confluence_search&lt;/code&gt;: Search Confluence content using CQL&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;confluence_get_page&lt;/code&gt;: Get content of a specific page&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;confluence_create_page&lt;/code&gt;: Create a new page&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;confluence_update_page&lt;/code&gt;: Update an existing page&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;View All Tools&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Operation&lt;/th&gt; 
    &lt;th&gt;Jira Tools&lt;/th&gt; 
    &lt;th&gt;Confluence Tools&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Read&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_search&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;confluence_search&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_get_issue&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;confluence_get_page&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_get_all_projects&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;confluence_get_page_children&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_get_project_issues&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;confluence_get_comments&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_get_worklog&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;confluence_get_labels&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_get_transitions&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;confluence_search_user&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_search_fields&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_get_agile_boards&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_get_board_issues&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_get_sprints_from_board&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_get_sprint_issues&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_get_issue_link_types&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_batch_get_changelogs&lt;/code&gt;*&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_get_user_profile&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_download_attachments&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_get_project_versions&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Write&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_create_issue&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;confluence_create_page&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_update_issue&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;confluence_update_page&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_delete_issue&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;confluence_delete_page&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_batch_create_issues&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;confluence_add_label&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_add_comment&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;confluence_add_comment&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_transition_issue&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_add_worklog&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_link_to_epic&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_create_sprint&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_update_sprint&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_create_issue_link&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_remove_issue_link&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_create_version&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;jira_batch_create_versions&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;p&gt;*Tool only available on Jira Cloud&lt;/p&gt;  
&lt;h3&gt;Tool Filtering and Access Control&lt;/h3&gt; 
&lt;p&gt;The server provides two ways to control tool access:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Tool Filtering&lt;/strong&gt;: Use &lt;code&gt;--enabled-tools&lt;/code&gt; flag or &lt;code&gt;ENABLED_TOOLS&lt;/code&gt; environment variable to specify which tools should be available:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Via environment variable
ENABLED_TOOLS="confluence_search,jira_get_issue,jira_search"

# Or via command line flag
docker run ... --enabled-tools "confluence_search,jira_get_issue,jira_search" ...
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Read/Write Control&lt;/strong&gt;: Tools are categorized as read or write operations. When &lt;code&gt;READ_ONLY_MODE&lt;/code&gt; is enabled, only read operations are available regardless of &lt;code&gt;ENABLED_TOOLS&lt;/code&gt; setting.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Troubleshooting &amp;amp; Debugging&lt;/h2&gt; 
&lt;h3&gt;Common Issues&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Authentication Failures&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;For Cloud: Check your API tokens (not your account password)&lt;/li&gt; 
   &lt;li&gt;For Server/Data Center: Verify your personal access token is valid and not expired&lt;/li&gt; 
   &lt;li&gt;For older Confluence servers: Some older versions require basic authentication with &lt;code&gt;CONFLUENCE_USERNAME&lt;/code&gt; and &lt;code&gt;CONFLUENCE_API_TOKEN&lt;/code&gt; (where token is your password)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;SSL Certificate Issues&lt;/strong&gt;: If using Server/Data Center and encounter SSL errors, set &lt;code&gt;CONFLUENCE_SSL_VERIFY=false&lt;/code&gt; or &lt;code&gt;JIRA_SSL_VERIFY=false&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Permission Errors&lt;/strong&gt;: Ensure your Atlassian account has sufficient permissions to access the spaces/projects&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Custom Headers Issues&lt;/strong&gt;: See the &lt;a href="https://raw.githubusercontent.com/sooperset/mcp-atlassian/main/#debugging-custom-headers"&gt;"Debugging Custom Headers"&lt;/a&gt; section below to analyze and resolve issues with custom headers&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Debugging Custom Headers&lt;/h3&gt; 
&lt;p&gt;To verify custom headers are being applied correctly:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Enable Debug Logging&lt;/strong&gt;: Set &lt;code&gt;MCP_VERY_VERBOSE=true&lt;/code&gt; to see detailed request logs&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# In your .env file or environment
MCP_VERY_VERBOSE=true
MCP_LOGGING_STDOUT=true
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Check Header Parsing&lt;/strong&gt;: Custom headers appear in logs with masked values for security:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;DEBUG Custom headers applied: {'X-Forwarded-User': '***', 'X-ALB-Token': '***'}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Verify Service-Specific Headers&lt;/strong&gt;: Check logs to confirm the right headers are being used:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;DEBUG Jira request headers: service-specific headers applied
DEBUG Confluence request headers: service-specific headers applied
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Test Header Format&lt;/strong&gt;: Ensure your header string format is correct:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Correct format
JIRA_CUSTOM_HEADERS=X-Custom=value1,X-Other=value2
CONFLUENCE_CUSTOM_HEADERS=X-Custom=value1,X-Other=value2

# Incorrect formats (will be ignored)
JIRA_CUSTOM_HEADERS="X-Custom=value1,X-Other=value2"  # Extra quotes
JIRA_CUSTOM_HEADERS=X-Custom: value1,X-Other: value2  # Colon instead of equals
JIRA_CUSTOM_HEADERS=X-Custom = value1               # Spaces around equals
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Security Note&lt;/strong&gt;: Header values containing sensitive information (tokens, passwords) are automatically masked in logs to prevent accidental exposure.&lt;/p&gt; 
&lt;h3&gt;Debugging Tools&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Using MCP Inspector for testing
npx @modelcontextprotocol/inspector uvx mcp-atlassian ...

# For local development version
npx @modelcontextprotocol/inspector uv --directory /path/to/your/mcp-atlassian run mcp-atlassian ...

# View logs
# macOS
tail -n 20 -f ~/Library/Logs/Claude/mcp*.log
# Windows
type %APPDATA%\Claude\logs\mcp*.log | more
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Security&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Never share API tokens&lt;/li&gt; 
 &lt;li&gt;Keep .env files secure and private&lt;/li&gt; 
 &lt;li&gt;See &lt;a href="https://raw.githubusercontent.com/sooperset/mcp-atlassian/main/SECURITY.md"&gt;SECURITY.md&lt;/a&gt; for best practices&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions to MCP Atlassian! If you'd like to contribute:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Check out our &lt;a href="https://raw.githubusercontent.com/sooperset/mcp-atlassian/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; guide for detailed development setup instructions.&lt;/li&gt; 
 &lt;li&gt;Make changes and submit a pull request.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;We use pre-commit hooks for code quality and follow semantic versioning for releases.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Licensed under MIT - see &lt;a href="https://raw.githubusercontent.com/sooperset/mcp-atlassian/main/LICENSE"&gt;LICENSE&lt;/a&gt; file. This is not an official Atlassian product.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>NVIDIA-NeMo/RL</title>
      <link>https://github.com/NVIDIA-NeMo/RL</link>
      <description>&lt;p&gt;Scalable toolkit for efficient model reinforcement&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Nemo RL: A Scalable and Efficient Post-Training Library&lt;/h1&gt; 
&lt;h2&gt;üì£ News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;[7/25/2025] &lt;a href="https://github.com/NVIDIA-NeMo/RL/releases/tag/v0.3.0"&gt;Release v0.3.0!&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;üìù &lt;a href="https://nvidia-nemo.github.io/blog/2025/07/21/nemo-rl-v0.3/"&gt;v0.3.0 Blog Post&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;üìä View the release run metrics on &lt;a href="https://colab.research.google.com/drive/15kpesCV1m_C5UQFStssTEjaN2RsBMeZ0?usp=sharing"&gt;Google Colab&lt;/a&gt; to get a head start on your experimentation.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;[5/14/2025] &lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/docs/guides/grpo-deepscaler.md"&gt;Reproduce DeepscaleR with NeMo RL!&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[5/14/2025] &lt;a href="https://github.com/NVIDIA-NeMo/RL/releases/tag/v0.2.1"&gt;Release v0.2.1!&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;üìä View the release run metrics on &lt;a href="https://colab.research.google.com/drive/1o14sO0gj_Tl_ZXGsoYip3C0r5ofkU1Ey?usp=sharing"&gt;Google Colab&lt;/a&gt; to get a head start on your experimentation.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;!-- markdown all in one --&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#nemo-rl-a-scalable-and-efficient-post-training-library"&gt;Nemo RL: A Scalable and Efficient Post-Training Library&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#-news"&gt;üì£ News&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#table-of-contents"&gt;Table of Contents&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#features"&gt;Features&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#prerequisites"&gt;Prerequisites&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#training-backends"&gt;Training Backends&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#grpo"&gt;GRPO&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#grpo-single-node"&gt;GRPO Single Node&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#grpo-multi-node"&gt;GRPO Multi-node&lt;/a&gt; 
      &lt;ul&gt; 
       &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#grpo-qwen25-32b"&gt;GRPO Qwen2.5-32B&lt;/a&gt;&lt;/li&gt; 
       &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#grpo-multi-turn"&gt;GRPO Multi-Turn&lt;/a&gt;&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#supervised-fine-tuning-sft"&gt;Supervised Fine-Tuning (SFT)&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#sft-single-node"&gt;SFT Single Node&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#sft-multi-node"&gt;SFT Multi-node&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#dpo"&gt;DPO&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#dpo-single-node"&gt;DPO Single Node&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#dpo-multi-node"&gt;DPO Multi-node&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#rm"&gt;RM&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#rm-single-node"&gt;RM Single Node&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#rm-multi-node"&gt;RM Multi-node&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#evaluation"&gt;Evaluation&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#convert-model-format-optional"&gt;Convert Model Format (Optional)&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#run-evaluation"&gt;Run Evaluation&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#set-up-clusters"&gt;Set Up Clusters&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#tips-and-tricks"&gt;Tips and Tricks&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#citation"&gt;Citation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#licenses"&gt;Licenses&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Nemo RL&lt;/strong&gt; is a scalable and efficient post-training library designed for models ranging from 1 GPU to thousands, and from tiny to over 100 billion parameters.&lt;/p&gt; 
&lt;p&gt;What you can expect:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Seamless integration with Hugging Face&lt;/strong&gt; for ease of use, allowing users to leverage a wide range of pre-trained models and tools.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;High-performance implementation with Megatron Core&lt;/strong&gt;, supporting various parallelism techniques for large models (&amp;gt;100B) and large context lengths.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Efficient resource management using Ray&lt;/strong&gt;, enabling scalable and flexible deployment across different hardware configurations.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexibility&lt;/strong&gt; with a modular design that allows easy integration and customization.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Comprehensive documentation&lt;/strong&gt; that is both detailed and user-friendly, with practical examples.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;p&gt;‚úÖ &lt;em&gt;Available now&lt;/em&gt; | üîú &lt;em&gt;Coming in v0.4&lt;/em&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;Fast Generation&lt;/strong&gt; - vLLM backend for optimized inference.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;HuggingFace Integration&lt;/strong&gt; - Works with 1-70B models (Qwen, Llama).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;Distributed Training&lt;/strong&gt; - Fully Sharded Data Parallel (FSDP2) support and Ray-based infrastructure.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;Environment Support&lt;/strong&gt; - Support for multi-environment training.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;Learning Algorithms&lt;/strong&gt; - GRPO (Group Relative Policy Optimization), SFT (Supervised Fine-Tuning), and DPO (Direct Preference Optimization).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;Multi-Turn RL&lt;/strong&gt; - Multi-turn generation and training for RL with tool use, games, etc.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;Large Model Support&lt;/strong&gt; - Native PyTorch support for models up to 70B parameters.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;Advanced Parallelism&lt;/strong&gt; - PyTorch native FSDP2, TP, CP, and SP for efficient training.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;(even) Larger Model Support with Long(er) Sequences&lt;/strong&gt; - Advanced parallelisms with Megatron Core (TP/PP/CP/SP/EP).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;Worker Isolation&lt;/strong&gt; - Process isolation between RL Actors (no worries about global state).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;Environment Isolation&lt;/strong&gt; - Dependency isolation between components.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;Megatron Inference&lt;/strong&gt; - (static) Megatron Inference for day-0 support for new megatron models.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;MoE Models&lt;/strong&gt; - Support for DeepseekV3 and Qwen-3 MoE models&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;Sequence Packing&lt;/strong&gt; - Sequence packing in both DTensor and MCore for huge training perf gains&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üîú &lt;strong&gt;Improved Native Performance&lt;/strong&gt; - Improve training time for Native Pytorch Models.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üîú &lt;strong&gt;Megatron Inference&lt;/strong&gt; - (dynamic) Megatron Inference for fast day-0 support for new megatron models.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Prerequisites&lt;/h2&gt; 
&lt;p&gt;Clone &lt;strong&gt;NeMo RL&lt;/strong&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;git clone git@github.com:NVIDIA-NeMo/RL.git nemo-rl
cd nemo-rl

# If you are using the Megatron backend, download the pinned versions of Megatron-LM and NeMo submodules 
# by running (This is not necessary if you are using the pure Pytorch/DTensor path):
git submodule update --init --recursive

# Different branches of the repo can have different pinned versions of these third-party submodules. Ensure
# submodules are automatically updated after switching branches or pulling updates by configuring git with:
# git config submodule.recurse true

# **NOTE**: this setting will not download **new** or remove **old** submodules with the branch's changes.
# You will have to run the full `git submodule update --init --recursive` command in these situations.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you are using the Megatron backend on bare-metal (outside of a container), you may need to install the cudnn headers as well. Here is how you can check as well as install them:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Check if you have libcudnn installed
dpkg -l | grep cudnn.*cuda

# Find the version you need here: https://developer.nvidia.com/cudnn-downloads?target_os=Linux&amp;amp;target_arch=x86_64&amp;amp;Distribution=Ubuntu&amp;amp;target_version=20.04&amp;amp;target_type=deb_network
# As an example, these are the "Linux Ubuntu 20.04 x86_64" instructions
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-keyring_1.1-1_all.deb
sudo dpkg -i cuda-keyring_1.1-1_all.deb
sudo apt-get update
sudo apt-get install cudnn-cuda-12
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For faster setup and environment isolation, we use &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt;. Follow &lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;these instructions&lt;/a&gt; to install uv.&lt;/p&gt; 
&lt;p&gt;Then, initialize NeMo RL project virtual environment via:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;uv venv
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Please do not use &lt;code&gt;-p/--python&lt;/code&gt; and instead allow &lt;code&gt;uv venv&lt;/code&gt; to read it from &lt;code&gt;.python-version&lt;/code&gt;. This ensures that the version of python used is always what we prescribe.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;If working outside a container, it can help to build &lt;a href="https://github.com/Dao-AILab/flash-attention"&gt;flash-attn&lt;/a&gt; and warm the uv cache before your first run.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;bash tools/build-flash-attn-in-uv-cache.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] On the first install, &lt;code&gt;flash-attn&lt;/code&gt; can take a while to install (~45min with 48 CPU hyperthreads). After it is built once, it is cached in your uv's cache dir making subsequent installs much quicker.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] The NeMo RL Dockerfile will warm the uv cache with flash-attn. See &lt;a href="https://docs.nvidia.com/nemo/rl/latest/docker.html"&gt;https://docs.nvidia.com/nemo/rl/latest/docker.html&lt;/a&gt; for instructions if you are looking for the NeMo RL container.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;If sucessful, you should see &lt;code&gt;‚úÖ flash-attn successfully added to uv cache&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Use &lt;code&gt;uv run&lt;/code&gt; to launch all commands. It handles pip installing implicitly and ensures your environment is up to date with our lock file.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;It is not recommended to activate the &lt;code&gt;venv&lt;/code&gt;, and you should use &lt;code&gt;uv run &amp;lt;command&amp;gt;&lt;/code&gt; instead to execute scripts within the managed environment. This ensures consistent environment usage across different shells and sessions. Example: &lt;code&gt;uv run python examples/run_grpo_math.py&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;Ensure you have the necessary CUDA drivers and PyTorch installed compatible with your hardware.&lt;/li&gt; 
  &lt;li&gt;If you update your environment in &lt;code&gt;pyproject.toml&lt;/code&gt;, it is necessary to force a rebuild of the virtual environments by setting &lt;code&gt;NRL_FORCE_REBUILD_VENVS=true&lt;/code&gt; next time you launch a run.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Reminder&lt;/strong&gt;: Don't forget to set your &lt;code&gt;HF_HOME&lt;/code&gt;, &lt;code&gt;WANDB_API_KEY&lt;/code&gt;, and &lt;code&gt;HF_DATASETS_CACHE&lt;/code&gt; (if needed). You'll need to do a &lt;code&gt;huggingface-cli login&lt;/code&gt; as well for Llama models.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Training Backends&lt;/h2&gt; 
&lt;p&gt;NeMo RL supports multiple training backends to accommodate different model sizes and hardware configurations:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;DTensor (FSDP2)&lt;/strong&gt; - PyTorch's next-generation distributed training with improved memory efficiency&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Megatron&lt;/strong&gt; - NVIDIA's high-performance training framework for scaling to large models (&amp;gt;100B parameters)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The training backend is automatically determined based on your YAML configuration settings. For detailed information on backend selection, configuration, and examples, see the &lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/docs/design-docs/training-backends.md"&gt;Training Backends documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;GRPO&lt;/h2&gt; 
&lt;p&gt;We have a reference GRPO experiment config set up trained for math benchmarks using the &lt;a href="https://huggingface.co/datasets/nvidia/OpenMathInstruct-2"&gt;OpenInstructMath2&lt;/a&gt; dataset.&lt;/p&gt; 
&lt;p&gt;You can read about the details of the GRPO implementation &lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/docs/guides/grpo.md"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;GRPO Single Node&lt;/h3&gt; 
&lt;p&gt;To run GRPO on a single GPU for &lt;code&gt;Qwen/Qwen2.5-1.5B&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Run the GRPO math example using a 1B parameter model
uv run python examples/run_grpo_math.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;By default, this uses the configuration in &lt;code&gt;examples/configs/grpo_math_1B.yaml&lt;/code&gt;. You can customize parameters with command-line overrides. For example, to run on 8 GPUs,&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Run the GRPO math example using a 1B parameter model using 8 GPUs
uv run python examples/run_grpo_math.py \
  cluster.gpus_per_node=8
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can override any of the parameters listed in the yaml configuration file. For example,&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;uv run python examples/run_grpo_math.py \
  policy.model_name="meta-llama/Llama-3.2-1B-Instruct" \
  checkpointing.checkpoint_dir="results/llama1b_math" \
  logger.wandb_enabled=True \
  logger.wandb.name="grpo-llama1b_math" \
  logger.num_val_samples_to_print=10
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The default configuration uses the DTensor training backend. We also provide a config &lt;code&gt;examples/configs/grpo_math_1B_megatron.yaml&lt;/code&gt; which is set up to use the Megatron backend out of the box.&lt;/p&gt; 
&lt;p&gt;To train using this config on a single GPU:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Run a GRPO math example on 1 GPU using the Megatron backend
uv run python examples/run_grpo_math.py \
  --config examples/configs/grpo_math_1B_megatron.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For additional details on supported backends and how to configure the training backend to suit your setup, refer to the &lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/docs/design-docs/training-backends.md"&gt;Training Backends documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;GRPO Multi-node&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Run from the root of NeMo RL repo
NUM_ACTOR_NODES=2

# grpo_math_8b uses Llama-3.1-8B-Instruct model
COMMAND="uv run ./examples/run_grpo_math.py --config examples/configs/grpo_math_8B.yaml cluster.num_nodes=2 checkpointing.checkpoint_dir='results/llama8b_2nodes' logger.wandb_enabled=True logger.wandb.name='grpo-llama8b_math'" \
CONTAINER=YOUR_CONTAINER \
MOUNTS="$PWD:$PWD" \
sbatch \
    --nodes=${NUM_ACTOR_NODES} \
    --account=YOUR_ACCOUNT \
    --job-name=YOUR_JOBNAME \
    --partition=YOUR_PARTITION \
    --time=4:0:0 \
    --gres=gpu:8 \
    ray.sub
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The required &lt;code&gt;CONTAINER&lt;/code&gt; can be built by following the instructions in the &lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/docs/docker.md"&gt;Docker documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;GRPO Qwen2.5-32B&lt;/h4&gt; 
&lt;p&gt;This section outlines how to run GRPO for Qwen2.5-32B with a 16k sequence length.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Run from the root of NeMo RL repo
NUM_ACTOR_NODES=32

# Download Qwen before the job starts to avoid spending time downloading during the training loop
HF_HOME=/path/to/hf_home huggingface-cli download Qwen/Qwen2.5-32B

# Ensure HF_HOME is included in your MOUNTS
HF_HOME=/path/to/hf_home \
COMMAND="uv run ./examples/run_grpo_math.py --config examples/configs/grpo_math_8B.yaml policy.model_name='Qwen/Qwen2.5-32B' policy.generation.vllm_cfg.tensor_parallel_size=4 policy.max_total_sequence_length=16384 cluster.num_nodes=${NUM_ACTOR_NODES} policy.dtensor_cfg.enabled=True policy.dtensor_cfg.tensor_parallel_size=8 policy.dtensor_cfg.sequence_parallel=True policy.dtensor_cfg.activation_checkpointing=True checkpointing.checkpoint_dir='results/qwen2.5-32b' logger.wandb_enabled=True logger.wandb.name='qwen2.5-32b'" \
CONTAINER=YOUR_CONTAINER \
MOUNTS="$PWD:$PWD" \
sbatch \
    --nodes=${NUM_ACTOR_NODES} \
    --account=YOUR_ACCOUNT \
    --job-name=YOUR_JOBNAME \
    --partition=YOUR_PARTITION \
    --time=4:0:0 \
    --gres=gpu:8 \
    ray.sub
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;GRPO Multi-Turn&lt;/h4&gt; 
&lt;p&gt;We also support multi-turn generation and training (tool use, games, etc.). Reference example for training to play a Sliding Puzzle Game:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;uv run python examples/run_grpo_sliding_puzzle.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Supervised Fine-Tuning (SFT)&lt;/h2&gt; 
&lt;p&gt;We provide an example SFT experiment using the &lt;a href="https://rajpurkar.github.io/SQuAD-explorer/"&gt;SQuAD dataset&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;SFT Single Node&lt;/h3&gt; 
&lt;p&gt;The default SFT configuration is set to run on a single GPU. To start the experiment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;uv run python examples/run_sft.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This fine-tunes the &lt;code&gt;Llama3.2-1B&lt;/code&gt; model on the SQuAD dataset using a 1 GPU.&lt;/p&gt; 
&lt;p&gt;To use multiple GPUs on a single node, you can modify the cluster configuration. This adjustment will also let you potentially increase the model and batch size:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;uv run python examples/run_sft.py \
  policy.model_name="meta-llama/Meta-Llama-3-8B" \
  policy.train_global_batch_size=128 \
  sft.val_global_batch_size=128 \
  cluster.gpus_per_node=8
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Refer to &lt;code&gt;examples/configs/sft.yaml&lt;/code&gt; for a full list of parameters that can be overridden.&lt;/p&gt; 
&lt;h3&gt;SFT Multi-node&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Run from the root of NeMo RL repo
NUM_ACTOR_NODES=2

COMMAND="uv run ./examples/run_sft.py --config examples/configs/sft.yaml cluster.num_nodes=2 cluster.gpus_per_node=8 checkpointing.checkpoint_dir='results/sft_llama8b_2nodes' logger.wandb_enabled=True logger.wandb.name='sft-llama8b'" \
CONTAINER=YOUR_CONTAINER \
MOUNTS="$PWD:$PWD" \
sbatch \
    --nodes=${NUM_ACTOR_NODES} \
    --account=YOUR_ACCOUNT \
    --job-name=YOUR_JOBNAME \
    --partition=YOUR_PARTITION \
    --time=4:0:0 \
    --gres=gpu:8 \
    ray.sub
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;DPO&lt;/h2&gt; 
&lt;p&gt;We provide a sample DPO experiment that uses the &lt;a href="https://huggingface.co/datasets/nvidia/HelpSteer3"&gt;HelpSteer3 dataset&lt;/a&gt; for preference-based training.&lt;/p&gt; 
&lt;h3&gt;DPO Single Node&lt;/h3&gt; 
&lt;p&gt;The default DPO experiment is configured to run on a single GPU. To launch the experiment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;uv run python examples/run_dpo.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This trains &lt;code&gt;Llama3.2-1B-Instruct&lt;/code&gt; on one GPU.&lt;/p&gt; 
&lt;p&gt;If you have access to more GPUs, you can update the experiment accordingly. To run on 8 GPUs, we update the cluster configuration and switch to an 8B Llama3.1 Instruct model:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;uv run python examples/run_dpo.py \
  policy.model_name="meta-llama/Llama-3.1-8B-Instruct" \
  policy.train_global_batch_size=256 \
  cluster.gpus_per_node=8
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Any of the DPO parameters can be customized from the command line. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;uv run python examples/run_dpo.py \
  dpo.sft_loss_weight=0.1 \
  dpo.preference_average_log_probs=True \
  checkpointing.checkpoint_dir="results/llama_dpo_sft" \
  logger.wandb_enabled=True \
  logger.wandb.name="llama-dpo-sft"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Refer to &lt;code&gt;examples/configs/dpo.yaml&lt;/code&gt; for a full list of parameters that can be overridden. For an in-depth explanation of how to add your own DPO dataset, refer to the &lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/docs/guides/dpo.md"&gt;DPO documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;DPO Multi-node&lt;/h3&gt; 
&lt;p&gt;For distributed DPO training across multiple nodes, modify the following script for your use case:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Run from the root of NeMo RL repo
## number of nodes to use for your job
NUM_ACTOR_NODES=2

COMMAND="uv run ./examples/run_dpo.py --config examples/configs/dpo.yaml cluster.num_nodes=2 cluster.gpus_per_node=8 dpo.val_global_batch_size=32 checkpointing.checkpoint_dir='results/dpo_llama81_2nodes' logger.wandb_enabled=True logger.wandb.name='dpo-llama1b'" \
CONTAINER=YOUR_CONTAINER \
MOUNTS="$PWD:$PWD" \
sbatch \
    --nodes=${NUM_ACTOR_NODES} \
    --account=YOUR_ACCOUNT \
    --job-name=YOUR_JOBNAME \
    --partition=YOUR_PARTITION \
    --time=4:0:0 \
    --gres=gpu:8 \
    ray.sub
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;RM&lt;/h2&gt; 
&lt;p&gt;We provide a sample RM experiment that uses the &lt;a href="https://huggingface.co/datasets/nvidia/HelpSteer3"&gt;HelpSteer3 dataset&lt;/a&gt; for preference-based training.&lt;/p&gt; 
&lt;h3&gt;RM Single Node&lt;/h3&gt; 
&lt;p&gt;The default RM experiment is configured to run on a single GPU. To launch the experiment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;uv run python examples/run_rm.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This trains a RM based on &lt;code&gt;meta-llama/Llama-3.2-1B-Instruct&lt;/code&gt; on one GPU.&lt;/p&gt; 
&lt;p&gt;If you have access to more GPUs, you can update the experiment accordingly. To run on 8 GPUs, we update the cluster configuration:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;uv run python examples/run_rm.py cluster.gpus_per_node=8
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Refer to the &lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/docs/guides/rm.md"&gt;RM documentation&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h3&gt;RM Multi-node&lt;/h3&gt; 
&lt;p&gt;For distributed RM training across multiple nodes, modify the following script for your use case:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Run from the root of NeMo RL repo
## number of nodes to use for your job
NUM_ACTOR_NODES=2

COMMAND="uv run ./examples/run_rm.py --config examples/configs/rm.yaml cluster.num_nodes=2 cluster.gpus_per_node=8 checkpointing.checkpoint_dir='results/rm_llama1b_2nodes' logger.wandb_enabled=True logger.wandb.name='rm-llama1b-2nodes'" \
CONTAINER=YOUR_CONTAINER \
MOUNTS="$PWD:$PWD" \
sbatch \
    --nodes=${NUM_ACTOR_NODES} \
    --account=YOUR_ACCOUNT \
    --job-name=YOUR_JOBNAME \
    --partition=YOUR_PARTITION \
    --time=4:0:0 \
    --gres=gpu:8 \
    ray.sub
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Evaluation&lt;/h2&gt; 
&lt;p&gt;We provide evaluation tools to assess model capabilities.&lt;/p&gt; 
&lt;h3&gt;Convert Model Format (Optional)&lt;/h3&gt; 
&lt;p&gt;If you have trained a model and saved the checkpoint in the Pytorch DCP format, you first need to convert it to the Hugging Face format before running evaluation:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Example for a GRPO checkpoint at step 170
uv run python examples/converters/convert_dcp_to_hf.py \
    --config results/grpo/step_170/config.yaml \
    --dcp-ckpt-path results/grpo/step_170/policy/weights/ \
    --hf-ckpt-path results/grpo/hf
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you have a model saved in Megatron format, you can use the following command to convert it to Hugging Face format prior to running evaluation. This script requires mcore, so make sure to launch with the mcore extra:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Example for a GRPO checkpoint at step 170
uv run --extra mcore python examples/converters/convert_megatron_to_hf.py \
    --config results/grpo/step_170/config.yaml \
    --megatron-ckpt-path results/grpo/step_170/policy/weights/iter_0000000 \
    --hf-ckpt-path results/grpo/hf
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Adjust the paths according to your training output directory structure.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;For an in-depth explanation of checkpointing, refer to the &lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/docs/design-docs/checkpointing.md"&gt;Checkpointing documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Run Evaluation&lt;/h3&gt; 
&lt;p&gt;Run evaluation script with converted model:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;uv run python examples/run_eval.py generation.model_name=$PWD/results/grpo/hf
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run evaluation script with custom settings:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Example: Evaluation of DeepScaleR-1.5B-Preview on MATH-500 using 8 GPUs
#          Pass@1 accuracy averaged over 16 samples for each problem
uv run python examples/run_eval.py \
    --config examples/configs/evals/math_eval.yaml \
    generation.model_name=agentica-org/DeepScaleR-1.5B-Preview \
    generation.temperature=0.6 \
    generation.top_p=0.95 \
    generation.vllm_cfg.max_model_len=32768 \
    data.dataset_name=math500 \
    eval.num_tests_per_prompt=16 \
    cluster.gpus_per_node=8
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Evaluation results may vary slightly due to various factors, such as sampling parameters, random seed, inference engine version, and inference engine settings.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Refer to &lt;code&gt;examples/configs/evals/eval.yaml&lt;/code&gt; for a full list of parameters that can be overridden. For an in-depth explanation of evaluation, refer to the &lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/docs/guides/eval.md"&gt;Evaluation documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Set Up Clusters&lt;/h2&gt; 
&lt;p&gt;For detailed instructions on how to set up and launch NeMo RL on Slurm or Kubernetes clusters, please refer to the dedicated &lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/docs/cluster.md"&gt;Cluster Start&lt;/a&gt; documentation.&lt;/p&gt; 
&lt;h2&gt;Tips and Tricks&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;If you forget to initialize the NeMo and Megatron submodules when cloning the NeMo-RL repository, you may run into an error like this:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;ModuleNotFoundError: No module named 'megatron'
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you see this error, there is likely an issue with your virtual environments. To fix this, first intialize the submodules:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;git submodule update --init --recursive
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;and then force a rebuild of the virtual environments by setting &lt;code&gt;NRL_FORCE_REBUILD_VENVS=true&lt;/code&gt; next time you launch a run:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;NRL_FORCE_REBUILD_VENVS=true uv run examples/run_grpo.py ...
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you use NeMo RL in your research, please cite it using the following BibTeX entry:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{nemo-rl,
title = {NeMo RL: A Scalable and Efficient Post-Training Library},
howpublished = {\url{https://github.com/NVIDIA-NeMo/RL}},
year = {2025},
note = {GitHub repository},
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions to NeMo RL! Please see our &lt;a href="https://github.com/NVIDIA-NeMo/RL/raw/main/CONTRIBUTING.md"&gt;Contributing Guidelines&lt;/a&gt; for more information on how to get involved.&lt;/p&gt; 
&lt;h2&gt;Licenses&lt;/h2&gt; 
&lt;p&gt;NVIDIA NeMo RL is licensed under the &lt;a href="https://github.com/NVIDIA-NeMo/RL/raw/main/LICENSE"&gt;Apache License 2.0&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>virattt/ai-hedge-fund</title>
      <link>https://github.com/virattt/ai-hedge-fund</link>
      <description>&lt;p&gt;An AI Hedge Fund Team&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AI Hedge Fund&lt;/h1&gt; 
&lt;p&gt;This is a proof of concept for an AI-powered hedge fund. The goal of this project is to explore the use of AI to make trading decisions. This project is for &lt;strong&gt;educational&lt;/strong&gt; purposes only and is not intended for real trading or investment.&lt;/p&gt; 
&lt;p&gt;This system employs several agents working together:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Aswath Damodaran Agent - The Dean of Valuation, focuses on story, numbers, and disciplined valuation&lt;/li&gt; 
 &lt;li&gt;Ben Graham Agent - The godfather of value investing, only buys hidden gems with a margin of safety&lt;/li&gt; 
 &lt;li&gt;Bill Ackman Agent - An activist investor, takes bold positions and pushes for change&lt;/li&gt; 
 &lt;li&gt;Cathie Wood Agent - The queen of growth investing, believes in the power of innovation and disruption&lt;/li&gt; 
 &lt;li&gt;Charlie Munger Agent - Warren Buffett's partner, only buys wonderful businesses at fair prices&lt;/li&gt; 
 &lt;li&gt;Michael Burry Agent - The Big Short contrarian who hunts for deep value&lt;/li&gt; 
 &lt;li&gt;Peter Lynch Agent - Practical investor who seeks "ten-baggers" in everyday businesses&lt;/li&gt; 
 &lt;li&gt;Phil Fisher Agent - Meticulous growth investor who uses deep "scuttlebutt" research&lt;/li&gt; 
 &lt;li&gt;Rakesh Jhunjhunwala Agent - The Big Bull of India&lt;/li&gt; 
 &lt;li&gt;Stanley Druckenmiller Agent - Macro legend who hunts for asymmetric opportunities with growth potential&lt;/li&gt; 
 &lt;li&gt;Warren Buffett Agent - The oracle of Omaha, seeks wonderful companies at a fair price&lt;/li&gt; 
 &lt;li&gt;Valuation Agent - Calculates the intrinsic value of a stock and generates trading signals&lt;/li&gt; 
 &lt;li&gt;Sentiment Agent - Analyzes market sentiment and generates trading signals&lt;/li&gt; 
 &lt;li&gt;Fundamentals Agent - Analyzes fundamental data and generates trading signals&lt;/li&gt; 
 &lt;li&gt;Technicals Agent - Analyzes technical indicators and generates trading signals&lt;/li&gt; 
 &lt;li&gt;Risk Manager - Calculates risk metrics and sets position limits&lt;/li&gt; 
 &lt;li&gt;Portfolio Manager - Makes final trading decisions and generates orders&lt;/li&gt; 
&lt;/ol&gt; 
&lt;img width="1042" alt="Screenshot 2025-03-22 at 6 19 07 PM" src="https://github.com/user-attachments/assets/cbae3dcf-b571-490d-b0ad-3f0f035ac0d4" /&gt; 
&lt;p&gt;Note: the system does not actually make any trades.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://twitter.com/virattt"&gt;&lt;img src="https://img.shields.io/twitter/follow/virattt?style=social" alt="Twitter Follow" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;This project is for &lt;strong&gt;educational and research purposes only&lt;/strong&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Not intended for real trading or investment&lt;/li&gt; 
 &lt;li&gt;No investment advice or guarantees provided&lt;/li&gt; 
 &lt;li&gt;Creator assumes no liability for financial losses&lt;/li&gt; 
 &lt;li&gt;Consult a financial advisor for investment decisions&lt;/li&gt; 
 &lt;li&gt;Past performance does not indicate future results&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;By using this software, you agree to use it solely for learning purposes.&lt;/p&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#how-to-install"&gt;How to Install&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#how-to-run"&gt;How to Run&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#%EF%B8%8F-command-line-interface"&gt;‚å®Ô∏è Command Line Interface&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#%EF%B8%8F-web-application"&gt;üñ•Ô∏è Web Application (NEW!)&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#feature-requests"&gt;Feature Requests&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;How to Install&lt;/h2&gt; 
&lt;p&gt;Before you can run the AI Hedge Fund, you'll need to install it and set up your API keys. These steps are common to both the full-stack web application and command line interface.&lt;/p&gt; 
&lt;h3&gt;1. Clone the Repository&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/virattt/ai-hedge-fund.git
cd ai-hedge-fund
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Set Up Your API Keys&lt;/h3&gt; 
&lt;p&gt;Create a &lt;code&gt;.env&lt;/code&gt; file for your API keys:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Create .env file for your API keys (in the root directory)
cp .env.example .env
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Open and edit the &lt;code&gt;.env&lt;/code&gt; file to add your API keys:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# For running LLMs hosted by openai (gpt-4o, gpt-4o-mini, etc.)
OPENAI_API_KEY=your-openai-api-key

# For running GigaChat (use gigachat credentials)
GIGACHAT_API_KEY=your-gigachat-api-key

# For running LLMs hosted by groq (deepseek, llama3, etc.)
GROQ_API_KEY=your-groq-api-key

# For getting financial data to power the hedge fund
FINANCIAL_DATASETS_API_KEY=your-financial-datasets-api-key
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: You must set at least one LLM API key (&lt;code&gt;OPENAI_API_KEY&lt;/code&gt;, &lt;code&gt;GROQ_API_KEY&lt;/code&gt;, &lt;code&gt;ANTHROPIC_API_KEY&lt;/code&gt;, or &lt;code&gt;DEEPSEEK_API_KEY&lt;/code&gt;) for the hedge fund to work.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Financial Data&lt;/strong&gt;: Data for AAPL, GOOGL, MSFT, NVDA, and TSLA is free and does not require an API key. For any other ticker, you will need to set the &lt;code&gt;FINANCIAL_DATASETS_API_KEY&lt;/code&gt; in the .env file.&lt;/p&gt; 
&lt;h2&gt;How to Run&lt;/h2&gt; 
&lt;h3&gt;‚å®Ô∏è Command Line Interface&lt;/h3&gt; 
&lt;p&gt;For users who prefer working with command line tools, you can run the AI Hedge Fund directly via terminal. This approach offers more granular control and is useful for automation, scripting, and integration purposes.&lt;/p&gt; 
&lt;img width="992" alt="Screenshot 2025-01-06 at 5 50 17 PM" src="https://github.com/user-attachments/assets/e8ca04bf-9989-4a7d-a8b4-34e04666663b" /&gt; 
&lt;p&gt;Choose one of the following installation methods:&lt;/p&gt; 
&lt;h4&gt;Using Poetry&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install Poetry (if not already installed):&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -sSL https://install.python-poetry.org | python3 -
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Install dependencies:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Using Docker&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Make sure you have Docker installed on your system. If not, you can download it from &lt;a href="https://www.docker.com/get-started"&gt;Docker's official website&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Navigate to the docker directory:&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd docker
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Build the Docker image:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# On Linux/Mac:
./run.sh build

# On Windows:
run.bat build
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Running the AI Hedge Fund (with Poetry)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry run python src/main.py --ticker AAPL,MSFT,NVDA
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Running the AI Hedge Fund (with Docker)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Navigate to the docker directory first
cd docker

# On Linux/Mac:
./run.sh --ticker AAPL,MSFT,NVDA main

# On Windows:
run.bat --ticker AAPL,MSFT,NVDA main
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also specify a &lt;code&gt;--ollama&lt;/code&gt; flag to run the AI hedge fund using local LLMs.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# With Poetry:
poetry run python src/main.py --ticker AAPL,MSFT,NVDA --ollama

# With Docker (from docker/ directory):
# On Linux/Mac:
./run.sh --ticker AAPL,MSFT,NVDA --ollama main

# On Windows:
run.bat --ticker AAPL,MSFT,NVDA --ollama main
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also specify a &lt;code&gt;--show-reasoning&lt;/code&gt; flag to print the reasoning of each agent to the console.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# With Poetry:
poetry run python src/main.py --ticker AAPL,MSFT,NVDA --show-reasoning

# With Docker (from docker/ directory):
# On Linux/Mac:
./run.sh --ticker AAPL,MSFT,NVDA --show-reasoning main

# On Windows:
run.bat --ticker AAPL,MSFT,NVDA --show-reasoning main
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can optionally specify the start and end dates to make decisions for a specific time period.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# With Poetry:
poetry run python src/main.py --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01 

# With Docker (from docker/ directory):
# On Linux/Mac:
./run.sh --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01 main

# On Windows:
run.bat --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01 main
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Running the Backtester (with Poetry)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Running the Backtester (with Docker)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Navigate to the docker directory first
cd docker

# On Linux/Mac:
./run.sh --ticker AAPL,MSFT,NVDA backtest

# On Windows:
run.bat --ticker AAPL,MSFT,NVDA backtest
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Example Output:&lt;/strong&gt; &lt;img width="941" alt="Screenshot 2025-01-06 at 5 47 52 PM" src="https://github.com/user-attachments/assets/00e794ea-8628-44e6-9a84-8f8a31ad3b47" /&gt;&lt;/p&gt; 
&lt;p&gt;You can optionally specify the start and end dates to backtest over a specific time period.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# With Poetry:
poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01

# With Docker (from docker/ directory):
# On Linux/Mac:
./run.sh --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01 backtest

# On Windows:
run.bat --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01 backtest
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also specify a &lt;code&gt;--ollama&lt;/code&gt; flag to run the backtester using local LLMs.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# With Poetry:
poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA --ollama

# With Docker (from docker/ directory):
# On Linux/Mac:
./run.sh --ticker AAPL,MSFT,NVDA --ollama backtest

# On Windows:
run.bat --ticker AAPL,MSFT,NVDA --ollama backtest
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;üñ•Ô∏è Web Application&lt;/h3&gt; 
&lt;p&gt;The new way to run the AI Hedge Fund is through our web application that provides a user-friendly interface. &lt;strong&gt;This is recommended for most users, especially those who prefer visual interfaces over command line tools.&lt;/strong&gt;&lt;/p&gt; 
&lt;img width="1721" alt="Screenshot 2025-06-28 at 6 41 03‚ÄØPM" src="https://github.com/user-attachments/assets/b95ab696-c9f4-416c-9ad1-51feb1f5374b" /&gt; 
&lt;h4&gt;For Mac/Linux:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd app &amp;amp;&amp;amp; ./run.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you get a "permission denied" error, run this first:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd app &amp;amp;&amp;amp; chmod +x run.sh &amp;amp;&amp;amp; ./run.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;For Windows:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Go to /app directory
cd app

# Run the app
\.run.bat
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;That's it!&lt;/strong&gt; These scripts will:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Check for required dependencies (Node.js, Python, Poetry)&lt;/li&gt; 
 &lt;li&gt;Install all dependencies automatically&lt;/li&gt; 
 &lt;li&gt;Start both frontend and backend services&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Automatically open your web browser&lt;/strong&gt; to the application&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Detailed Setup Instructions&lt;/h4&gt; 
&lt;p&gt;For detailed setup instructions, troubleshooting, and advanced configuration options, see:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/app/README.md"&gt;Full-Stack App Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/app/frontend/README.md"&gt;Frontend Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/app/backend/README.md"&gt;Backend Documentation&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Fork the repository&lt;/li&gt; 
 &lt;li&gt;Create a feature branch&lt;/li&gt; 
 &lt;li&gt;Commit your changes&lt;/li&gt; 
 &lt;li&gt;Push to the branch&lt;/li&gt; 
 &lt;li&gt;Create a Pull Request&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: Please keep your pull requests small and focused. This will make it easier to review and merge.&lt;/p&gt; 
&lt;h2&gt;Feature Requests&lt;/h2&gt; 
&lt;p&gt;If you have a feature request, please open an &lt;a href="https://github.com/virattt/ai-hedge-fund/issues"&gt;issue&lt;/a&gt; and make sure it is tagged with &lt;code&gt;enhancement&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the MIT License - see the LICENSE file for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>pydantic/pydantic-ai</title>
      <link>https://github.com/pydantic/pydantic-ai</link>
      <description>&lt;p&gt;Agent Framework / shim to use Pydantic with LLMs&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;a href="https://ai.pydantic.dev/"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="https://ai.pydantic.dev/img/pydantic-ai-dark.svg" /&gt; 
   &lt;img src="https://ai.pydantic.dev/img/pydantic-ai-light.svg?sanitize=true" alt="Pydantic AI" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;em&gt;Agent Framework / shim to use Pydantic with LLMs&lt;/em&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/pydantic/pydantic-ai/actions/workflows/ci.yml?query=branch%3Amain"&gt;&lt;img src="https://github.com/pydantic/pydantic-ai/actions/workflows/ci.yml/badge.svg?event=push" alt="CI" /&gt;&lt;/a&gt; 
 &lt;a href="https://coverage-badge.samuelcolvin.workers.dev/redirect/pydantic/pydantic-ai"&gt;&lt;img src="https://coverage-badge.samuelcolvin.workers.dev/pydantic/pydantic-ai.svg?sanitize=true" alt="Coverage" /&gt;&lt;/a&gt; 
 &lt;a href="https://pypi.python.org/pypi/pydantic-ai"&gt;&lt;img src="https://img.shields.io/pypi/v/pydantic-ai.svg?sanitize=true" alt="PyPI" /&gt;&lt;/a&gt; 
 &lt;a href="https://github.com/pydantic/pydantic-ai"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/pydantic-ai.svg?sanitize=true" alt="versions" /&gt;&lt;/a&gt; 
 &lt;a href="https://github.com/pydantic/pydantic-ai/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/pydantic/pydantic-ai.svg?v" alt="license" /&gt;&lt;/a&gt; 
 &lt;a href="https://logfire.pydantic.dev/docs/join-slack/"&gt;&lt;img src="https://img.shields.io/badge/Slack-Join%20Slack-4A154B?logo=slack" alt="Join Slack" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;Documentation&lt;/strong&gt;: &lt;a href="https://ai.pydantic.dev/"&gt;ai.pydantic.dev&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;Pydantic AI is a Python agent framework designed to make it less painful to build production grade applications with Generative AI.&lt;/p&gt; 
&lt;p&gt;FastAPI revolutionized web development by offering an innovative and ergonomic design, built on the foundation of &lt;a href="https://docs.pydantic.dev"&gt;Pydantic Validation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Similarly, virtually every agent framework and LLM library in Python uses Pydantic Validation, yet when we began to use LLMs in &lt;a href="https://pydantic.dev/logfire"&gt;Pydantic Logfire&lt;/a&gt;, we couldn't find anything that gave us the same feeling.&lt;/p&gt; 
&lt;p&gt;We built Pydantic AI with one simple aim: to bring that FastAPI feeling to GenAI app development.&lt;/p&gt; 
&lt;h2&gt;Why use Pydantic AI&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Built by the Pydantic Team&lt;/strong&gt; Built by the team behind &lt;a href="https://docs.pydantic.dev/latest/"&gt;Pydantic Validation&lt;/a&gt; (the validation layer of the OpenAI SDK, the Anthropic SDK, LangChain, LlamaIndex, AutoGPT, Transformers, CrewAI, Instructor and many more).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Model-agnostic&lt;/strong&gt; Supports OpenAI, Anthropic, Gemini, Deepseek, Ollama, Groq, Cohere, and Mistral, and there is a simple interface to implement support for &lt;a href="https://ai.pydantic.dev/models/"&gt;other models&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Pydantic Logfire Integration&lt;/strong&gt; Seamlessly &lt;a href="https://ai.pydantic.dev/logfire/"&gt;integrates&lt;/a&gt; with &lt;a href="https://pydantic.dev/logfire"&gt;Pydantic Logfire&lt;/a&gt; for real-time debugging, performance monitoring, and behavior tracking of your LLM-powered applications.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Type-safe&lt;/strong&gt; Designed to make &lt;a href="https://ai.pydantic.dev/agents/#static-type-checking"&gt;type checking&lt;/a&gt; as powerful and informative as possible for you.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Python-centric Design&lt;/strong&gt; Leverages Python's familiar control flow and agent composition to build your AI-driven projects, making it easy to apply standard Python best practices you'd use in any other (non-AI) project.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Structured Responses&lt;/strong&gt; Harnesses the power of &lt;a href="https://docs.pydantic.dev/latest/"&gt;Pydantic Validation&lt;/a&gt; to &lt;a href="https://ai.pydantic.dev/output/#structured-output"&gt;validate and structure&lt;/a&gt; model outputs, ensuring responses are consistent across runs.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Dependency Injection System&lt;/strong&gt; Offers an optional &lt;a href="https://ai.pydantic.dev/dependencies/"&gt;dependency injection&lt;/a&gt; system to provide data and services to your agent's &lt;a href="https://ai.pydantic.dev/agents/#system-prompts"&gt;system prompts&lt;/a&gt;, &lt;a href="https://ai.pydantic.dev/tools/"&gt;tools&lt;/a&gt; and &lt;a href="https://ai.pydantic.dev/output/#output-validator-functions"&gt;output validators&lt;/a&gt;. This is useful for testing and eval-driven iterative development.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Streamed Responses&lt;/strong&gt; Provides the ability to &lt;a href="https://ai.pydantic.dev/output/#streamed-results"&gt;stream&lt;/a&gt; LLM outputs continuously, with immediate validation, ensuring rapid and accurate outputs.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Graph Support&lt;/strong&gt; &lt;a href="https://ai.pydantic.dev/graph"&gt;Pydantic Graph&lt;/a&gt; provides a powerful way to define graphs using typing hints, this is useful in complex applications where standard control flow can degrade to spaghetti code.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Hello World Example&lt;/h2&gt; 
&lt;p&gt;Here's a minimal example of Pydantic AI:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from pydantic_ai import Agent

# Define a very simple agent including the model to use, you can also set the model when running the agent.
agent = Agent(
    'google-gla:gemini-1.5-flash',
    # Register a static system prompt using a keyword argument to the agent.
    # For more complex dynamically-generated system prompts, see the example below.
    system_prompt='Be concise, reply with one sentence.',
)

# Run the agent synchronously, conducting a conversation with the LLM.
# Here the exchange should be very short: Pydantic AI will send the system prompt and the user query to the LLM,
# the model will return a text response. See below for a more complex run.
result = agent.run_sync('Where does "hello world" come from?')
print(result.output)
"""
The first known use of "hello, world" was in a 1974 textbook about the C programming language.
"""
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;(This example is complete, it can be run "as is")&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;Not very interesting yet, but we can easily add "tools", dynamic system prompts, and structured responses to build more powerful agents.&lt;/p&gt; 
&lt;h2&gt;Tools &amp;amp; Dependency Injection Example&lt;/h2&gt; 
&lt;p&gt;Here is a concise example using Pydantic AI to build a support agent for a bank:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;(Better documented example &lt;a href="https://ai.pydantic.dev/#tools-dependency-injection-example"&gt;in the docs&lt;/a&gt;)&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from dataclasses import dataclass

from pydantic import BaseModel, Field
from pydantic_ai import Agent, RunContext

from bank_database import DatabaseConn


# SupportDependencies is used to pass data, connections, and logic into the model that will be needed when running
# system prompt and tool functions. Dependency injection provides a type-safe way to customise the behavior of your agents.
@dataclass
class SupportDependencies:
    customer_id: int
    db: DatabaseConn


# This pydantic model defines the structure of the output returned by the agent.
class SupportOutput(BaseModel):
    support_advice: str = Field(description='Advice returned to the customer')
    block_card: bool = Field(description="Whether to block the customer's card")
    risk: int = Field(description='Risk level of query', ge=0, le=10)


# This agent will act as first-tier support in a bank.
# Agents are generic in the type of dependencies they accept and the type of output they return.
# In this case, the support agent has type `Agent[SupportDependencies, SupportOutput]`.
support_agent = Agent(
    'openai:gpt-4o',
    deps_type=SupportDependencies,
    # The response from the agent will, be guaranteed to be a SupportOutput,
    # if validation fails the agent is prompted to try again.
    output_type=SupportOutput,
    system_prompt=(
        'You are a support agent in our bank, give the '
        'customer support and judge the risk level of their query.'
    ),
)


# Dynamic system prompts can make use of dependency injection.
# Dependencies are carried via the `RunContext` argument, which is parameterized with the `deps_type` from above.
# If the type annotation here is wrong, static type checkers will catch it.
@support_agent.system_prompt
async def add_customer_name(ctx: RunContext[SupportDependencies]) -&amp;gt; str:
    customer_name = await ctx.deps.db.customer_name(id=ctx.deps.customer_id)
    return f"The customer's name is {customer_name!r}"


# `tool` let you register functions which the LLM may call while responding to a user.
# Again, dependencies are carried via `RunContext`, any other arguments become the tool schema passed to the LLM.
# Pydantic is used to validate these arguments, and errors are passed back to the LLM so it can retry.
@support_agent.tool
async def customer_balance(
        ctx: RunContext[SupportDependencies], include_pending: bool
) -&amp;gt; float:
    """Returns the customer's current account balance."""
    # The docstring of a tool is also passed to the LLM as the description of the tool.
    # Parameter descriptions are extracted from the docstring and added to the parameter schema sent to the LLM.
    balance = await ctx.deps.db.customer_balance(
        id=ctx.deps.customer_id,
        include_pending=include_pending,
    )
    return balance


...  # In a real use case, you'd add more tools and a longer system prompt


async def main():
    deps = SupportDependencies(customer_id=123, db=DatabaseConn())
    # Run the agent asynchronously, conducting a conversation with the LLM until a final response is reached.
    # Even in this fairly simple case, the agent will exchange multiple messages with the LLM as tools are called to retrieve an output.
    result = await support_agent.run('What is my balance?', deps=deps)
    # The `result.output` will be validated with Pydantic to guarantee it is a `SupportOutput`. Since the agent is generic,
    # it'll also be typed as a `SupportOutput` to aid with static type checking.
    print(result.output)
    """
    support_advice='Hello John, your current account balance, including pending transactions, is $123.45.' block_card=False risk=1
    """

    result = await support_agent.run('I just lost my card!', deps=deps)
    print(result.output)
    """
    support_advice="I'm sorry to hear that, John. We are temporarily blocking your card to prevent unauthorized transactions." block_card=True risk=8
    """
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Next Steps&lt;/h2&gt; 
&lt;p&gt;To try Pydantic AI yourself, follow the instructions &lt;a href="https://ai.pydantic.dev/examples/"&gt;in the examples&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Read the &lt;a href="https://ai.pydantic.dev/agents/"&gt;docs&lt;/a&gt; to learn more about building applications with Pydantic AI.&lt;/p&gt; 
&lt;p&gt;Read the &lt;a href="https://ai.pydantic.dev/api/agent/"&gt;API Reference&lt;/a&gt; to understand Pydantic AI's interface.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>emcie-co/parlant</title>
      <link>https://github.com/emcie-co/parlant</link>
      <description>&lt;p&gt;LLM agents built for control. Designed for real-world use. Deployed in minutes.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://github.com/emcie-co/parlant/blob/develop/docs/LogoTransparentLight.png?raw=true" /&gt; 
  &lt;img alt="Parlant - AI Agent Framework" src="https://github.com/emcie-co/parlant/raw/develop/docs/LogoTransparentDark.png?raw=true" width="400" /&gt; 
 &lt;/picture&gt; 
 &lt;h3&gt;Finally, LLM agents that actually follow instructions&lt;/h3&gt; 
 &lt;p&gt; &lt;a href="https://www.parlant.io/" target="_blank"&gt;üåê Website&lt;/a&gt; ‚Ä¢ &lt;a href="https://www.parlant.io/docs/quickstart/installation" target="_blank"&gt;‚ö° Quick Start&lt;/a&gt; ‚Ä¢ &lt;a href="https://discord.gg/duxWqxKk6J" target="_blank"&gt;üí¨ Discord&lt;/a&gt; ‚Ä¢ &lt;a href="https://www.parlant.io/docs/quickstart/examples" target="_blank"&gt;üìñ Examples&lt;/a&gt; &lt;/p&gt; 
 &lt;p&gt; &lt;a href="https://pypi.org/project/parlant/"&gt;&lt;img alt="PyPI" src="https://img.shields.io/pypi/v/parlant?color=blue" /&gt;&lt;/a&gt; &lt;img alt="Python 3.10+" src="https://img.shields.io/badge/python-3.10+-blue" /&gt; &lt;a href="https://opensource.org/licenses/Apache-2.0"&gt;&lt;img alt="License" src="https://img.shields.io/badge/license-Apache%202.0-green" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/duxWqxKk6J"&gt;&lt;img alt="Discord" src="https://img.shields.io/discord/1312378700993663007?color=7289da&amp;amp;logo=discord&amp;amp;logoColor=white" /&gt;&lt;/a&gt; &lt;img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/emcie-co/parlant?style=social" /&gt; &lt;/p&gt; 
 &lt;a href="https://trendshift.io/repositories/12768" target="_blank"&gt; &lt;img src="https://trendshift.io/api/badge/repositories/12768" alt="Trending on TrendShift" style="width: 250px; height: 55px;" width="250" height="55" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;üéØ The Problem Every AI Developer Faces&lt;/h2&gt; 
&lt;p&gt;You build an AI agent. It works great in testing. Then real users start talking to it and...&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚ùå It ignores your carefully crafted system prompts&lt;/li&gt; 
 &lt;li&gt;‚ùå It hallucinates responses in critical moments&lt;/li&gt; 
 &lt;li&gt;‚ùå It can't handle edge cases consistently&lt;/li&gt; 
 &lt;li&gt;‚ùå Each conversation feels like a roll of the dice&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Sound familiar?&lt;/strong&gt; You're not alone. This is the #1 pain point for developers building production AI agents.&lt;/p&gt; 
&lt;h2&gt;‚ö° The Solution: Stop Fighting Prompts, Teach Principles&lt;/h2&gt; 
&lt;p&gt;Parlant flips the script on AI agent development. Instead of hoping your LLM will follow instructions, &lt;strong&gt;Parlant ensures it&lt;/strong&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Traditional approach: Cross your fingers ü§û
system_prompt = "You are a helpful assistant. Please follow these 47 rules..."

# Parlant approach: Ensured compliance ‚úÖ
await agent.create_guideline(
    condition="Customer asks about refunds",
    action="Check order status first to see if eligible",
    tools=[check_order_status],
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Parlant gives you all the structure you need to build customer-facing agents that behave exactly as your business requires:&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://parlant.io/docs/concepts/customization/journeys"&gt;Journeys&lt;/a&gt;&lt;/strong&gt;: Define clear customer journeys and how your agent should respond at each step.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://parlant.io/docs/concepts/customization/guidelines"&gt;Behavioral Guidelines&lt;/a&gt;&lt;/strong&gt;: Easily craft agent behavior; Parlant will match the relevant elements contextually.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://parlant.io/docs/concepts/customization/tools"&gt;Tool Use&lt;/a&gt;&lt;/strong&gt;: Attach external APIs, data fetchers, or backend services to specific interaction events.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://parlant.io/docs/concepts/customization/glossary"&gt;Domain Adaptation&lt;/a&gt;&lt;/strong&gt;: Teach your agent domain-specific terminology and craft personalized responses.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://parlant.io/docs/concepts/customization/canned-responses"&gt;Canned Responses&lt;/a&gt;&lt;/strong&gt;: Use response templates to eliminate hallucinations and guarantee style consistency.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://parlant.io/docs/advanced/explainability"&gt;Explainability&lt;/a&gt;&lt;/strong&gt;: Understand why and when each guideline was matched and followed.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center"&gt; 
 &lt;h2&gt;üöÄ Get Your Agent Running in 60 Seconds&lt;/h2&gt; 
&lt;/div&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install parlant
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import parlant.sdk as p

@p.tool
async def get_weather(context: p.ToolContext, city: str) -&amp;gt; p.ToolResult:
    # Your weather API logic here
    return p.ToolResult(f"Sunny, 72¬∞F in {city}")

async def main():
    async with p.Server() as server:
        agent = await server.create_agent(
            name="WeatherBot",
            description="Helpful weather assistant"
        )

        # Define behavior with natural language
        await agent.create_guideline(
            condition="User asks about weather",
            action="Get current weather and provide a friendly response with suggestions",
            tools=[get_weather]
        )

        # üéâ Test playground ready at http://localhost:8800
        # Integrate the official React widget into your app,
        # or follow the tutorial to build your own frontend!

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;That's it!&lt;/strong&gt; Your agent is running with ensured rule-following behavior.&lt;/p&gt; 
&lt;h2&gt;üé¨ See It In Action&lt;/h2&gt; 
&lt;img alt="Parlant Demo" src="https://github.com/emcie-co/parlant/raw/develop/docs/demo.gif?raw=true" width="100%" /&gt; 
&lt;h2&gt;üî• Why Developers Are Switching to Parlant&lt;/h2&gt; 
&lt;table width="100%"&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üèóÔ∏è &lt;strong&gt;Traditional AI Frameworks&lt;/strong&gt;&lt;/h3&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;‚ö° &lt;strong&gt;Parlant&lt;/strong&gt;&lt;/h3&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%"&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Write complex system prompts&lt;/li&gt; 
     &lt;li&gt;Hope the LLM follows them&lt;/li&gt; 
     &lt;li&gt;Debug unpredictable behaviors&lt;/li&gt; 
     &lt;li&gt;Scale by prompt engineering&lt;/li&gt; 
     &lt;li&gt;Cross fingers for reliability&lt;/li&gt; 
    &lt;/ul&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Define rules in natural language&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Ensured&lt;/strong&gt; rule compliance&lt;/li&gt; 
     &lt;li&gt;Predictable, consistent behavior&lt;/li&gt; 
     &lt;li&gt;Scale by adding guidelines&lt;/li&gt; 
     &lt;li&gt;Production-ready from day one&lt;/li&gt; 
    &lt;/ul&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;üéØ Perfect For Your Use Case&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;Financial Services&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;Healthcare&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;E-commerce&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;Legal Tech&lt;/strong&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;Compliance-first design&lt;/td&gt; 
    &lt;td align="center"&gt;HIPAA-ready agents&lt;/td&gt; 
    &lt;td align="center"&gt;Customer service at scale&lt;/td&gt; 
    &lt;td align="center"&gt;Precise legal guidance&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;Built-in risk management&lt;/td&gt; 
    &lt;td align="center"&gt;Patient data protection&lt;/td&gt; 
    &lt;td align="center"&gt;Order processing automation&lt;/td&gt; 
    &lt;td align="center"&gt;Document review assistance&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;üõ†Ô∏è Enterprise-Grade Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;üß≠ Conversational Journeys&lt;/strong&gt; - Lead the customer step-by-step to a goal&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üéØ Dynamic Guideline Matching&lt;/strong&gt; - Context-aware rule application&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üîß Reliable Tool Integration&lt;/strong&gt; - APIs, databases, external services&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üìä Conversation Analytics&lt;/strong&gt; - Deep insights into agent behavior&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üîÑ Iterative Refinement&lt;/strong&gt; - Continuously improve agent responses&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üõ°Ô∏è Built-in Guardrails&lt;/strong&gt; - Prevent hallucination and off-topic responses&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üì± React Widget&lt;/strong&gt; - &lt;a href="https://github.com/emcie-co/parlant-chat-react"&gt;Drop-in chat UI for any web app&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üîç Full Explainability&lt;/strong&gt; - Understand every decision your agent makes&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìà Join 7,000+ Developers Building Better AI&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;strong&gt;Companies using Parlant:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Financial institutions ‚Ä¢ Healthcare providers ‚Ä¢ Legal firms ‚Ä¢ E-commerce platforms&lt;/em&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://star-history.com/#emcie-co/parlant&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=emcie-co/parlant&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;üåü What Developers Are Saying&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;"By far the most elegant conversational AI framework that I've come across! Developing with Parlant is pure joy."&lt;/em&gt; &lt;strong&gt;‚Äî Vishal Ahuja, Senior Lead, Customer-Facing Conversational AI @ JPMorgan Chase&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;üèÉ‚Äç‚ôÇÔ∏è Quick Start Paths&lt;/h2&gt; 
&lt;table border="0"&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üéØ I want to test it myself&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.parlant.io/docs/quickstart/installation"&gt;‚Üí 5-minute quickstart&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üõ†Ô∏è I want to see an example&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.parlant.io/docs/quickstart/examples"&gt;‚Üí Healthcare agent example&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üöÄ I want to get involved&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://discord.gg/duxWqxKk6J"&gt;‚Üí Join our Discord community&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;ü§ù Community &amp;amp; Support&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;üí¨ &lt;strong&gt;&lt;a href="https://discord.gg/duxWqxKk6J"&gt;Discord Community&lt;/a&gt;&lt;/strong&gt; - Get help from the team and community&lt;/li&gt; 
 &lt;li&gt;üìñ &lt;strong&gt;&lt;a href="https://parlant.io/docs/quickstart/installation"&gt;Documentation&lt;/a&gt;&lt;/strong&gt; - Comprehensive guides and examples&lt;/li&gt; 
 &lt;li&gt;üêõ &lt;strong&gt;&lt;a href="https://github.com/emcie-co/parlant/issues"&gt;GitHub Issues&lt;/a&gt;&lt;/strong&gt; - Bug reports and feature requests&lt;/li&gt; 
 &lt;li&gt;üìß &lt;strong&gt;&lt;a href="https://parlant.io/contact"&gt;Direct Support&lt;/a&gt;&lt;/strong&gt; - Direct line to our engineering team&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìÑ License&lt;/h2&gt; 
&lt;p&gt;Apache 2.0 - Use it anywhere, including commercial projects.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;strong&gt;Ready to build AI agents that actually work?&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;‚≠ê &lt;strong&gt;Star this repo&lt;/strong&gt; ‚Ä¢ üöÄ &lt;strong&gt;&lt;a href="https://parlant.io/"&gt;Try Parlant now&lt;/a&gt;&lt;/strong&gt; ‚Ä¢ üí¨ &lt;strong&gt;&lt;a href="https://discord.gg/duxWqxKk6J"&gt;Join Discord&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Built with ‚ù§Ô∏è by the team at &lt;a href="https://emcie.co"&gt;Emcie&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>matplotlib/matplotlib</title>
      <link>https://github.com/matplotlib/matplotlib</link>
      <description>&lt;p&gt;matplotlib: plotting with Python&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="https://pypi.org/project/matplotlib/"&gt;&lt;img src="https://img.shields.io/pypi/v/matplotlib" alt="PyPi" /&gt;&lt;/a&gt; &lt;a href="https://anaconda.org/conda-forge/matplotlib"&gt;&lt;img src="https://img.shields.io/conda/vn/conda-forge/matplotlib" alt="Conda" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/matplotlib"&gt;&lt;img src="https://img.shields.io/pypi/dm/matplotlib" alt="Downloads" /&gt;&lt;/a&gt; &lt;a href="https://numfocus.org"&gt;&lt;img src="https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&amp;amp;colorA=E1523D&amp;amp;colorB=007D8A" alt="NUMFocus" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://discourse.matplotlib.org"&gt;&lt;img src="https://img.shields.io/badge/help_forum-discourse-blue.svg?sanitize=true" alt="Discourse help forum" /&gt;&lt;/a&gt; &lt;a href="https://gitter.im/matplotlib/matplotlib"&gt;&lt;img src="https://badges.gitter.im/matplotlib/matplotlib.svg?sanitize=true" alt="Gitter" /&gt;&lt;/a&gt; &lt;a href="https://github.com/matplotlib/matplotlib/issues"&gt;&lt;img src="https://img.shields.io/badge/issue_tracking-github-blue.svg?sanitize=true" alt="GitHub issues" /&gt;&lt;/a&gt; &lt;a href="https://matplotlib.org/stable/devel/index.html"&gt;&lt;img src="https://img.shields.io/badge/PR-Welcome-%23FF8300.svg?" alt="Contributing" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/matplotlib/matplotlib/actions?query=workflow%3ATests"&gt;&lt;img src="https://github.com/matplotlib/matplotlib/workflows/Tests/badge.svg?sanitize=true" alt="GitHub actions status" /&gt;&lt;/a&gt; &lt;a href="https://dev.azure.com/matplotlib/matplotlib/_build/latest?definitionId=1&amp;amp;branchName=main"&gt;&lt;img src="https://dev.azure.com/matplotlib/matplotlib/_apis/build/status/matplotlib.matplotlib?branchName=main" alt="Azure pipelines status" /&gt;&lt;/a&gt; &lt;a href="https://ci.appveyor.com/project/matplotlib/matplotlib"&gt;&lt;img src="https://ci.appveyor.com/api/projects/status/github/matplotlib/matplotlib?branch=main&amp;amp;svg=true" alt="AppVeyor status" /&gt;&lt;/a&gt; &lt;a href="https://app.codecov.io/gh/matplotlib/matplotlib"&gt;&lt;img src="https://codecov.io/github/matplotlib/matplotlib/badge.svg?branch=main&amp;amp;service=github" alt="Codecov status" /&gt;&lt;/a&gt; &lt;a href="https://jacobtomlinson.dev/effver"&gt;&lt;img src="https://img.shields.io/badge/version_scheme-EffVer-0097a7" alt="EffVer Versioning" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://matplotlib.org/_static/logo2.svg?sanitize=true" alt="Matplotlib logotype" /&gt;&lt;/p&gt; 
&lt;p&gt;Matplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python.&lt;/p&gt; 
&lt;p&gt;Check out our &lt;a href="https://matplotlib.org/"&gt;home page&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://matplotlib.org/_static/readme_preview.png" alt="image" /&gt;&lt;/p&gt; 
&lt;p&gt;Matplotlib produces publication-quality figures in a variety of hardcopy formats and interactive environments across platforms. Matplotlib can be used in Python scripts, Python/IPython shells, web application servers, and various graphical user interface toolkits.&lt;/p&gt; 
&lt;h2&gt;Install&lt;/h2&gt; 
&lt;p&gt;See the &lt;a href="https://matplotlib.org/stable/users/installing/index.html"&gt;install documentation&lt;/a&gt;, which is generated from &lt;code&gt;/doc/install/index.rst&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;Contribute&lt;/h2&gt; 
&lt;p&gt;You've discovered a bug or something else you want to change ‚Äî excellent!&lt;/p&gt; 
&lt;p&gt;You've worked out a way to fix it ‚Äî even better!&lt;/p&gt; 
&lt;p&gt;You want to tell us about it ‚Äî best of all!&lt;/p&gt; 
&lt;p&gt;Start at the &lt;a href="https://matplotlib.org/devdocs/devel/contribute.html"&gt;contributing guide&lt;/a&gt;!&lt;/p&gt; 
&lt;h2&gt;Contact&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://discourse.matplotlib.org/"&gt;Discourse&lt;/a&gt; is the discussion forum for general questions and discussions and our recommended starting point.&lt;/p&gt; 
&lt;p&gt;Our active mailing lists (which are mirrored on Discourse) are:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://mail.python.org/mailman/listinfo/matplotlib-users"&gt;Users&lt;/a&gt; mailing list: &lt;a href="mailto:matplotlib-users@python.org"&gt;matplotlib-users@python.org&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mail.python.org/mailman/listinfo/matplotlib-announce"&gt;Announcement&lt;/a&gt; mailing list: &lt;a href="mailto:matplotlib-announce@python.org"&gt;matplotlib-announce@python.org&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mail.python.org/mailman/listinfo/matplotlib-devel"&gt;Development&lt;/a&gt; mailing list: &lt;a href="mailto:matplotlib-devel@python.org"&gt;matplotlib-devel@python.org&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://gitter.im/matplotlib/matplotlib"&gt;Gitter&lt;/a&gt; is for coordinating development and asking questions directly related to contributing to matplotlib.&lt;/p&gt; 
&lt;h2&gt;Citing Matplotlib&lt;/h2&gt; 
&lt;p&gt;If Matplotlib contributes to a project that leads to publication, please acknowledge this by citing Matplotlib.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://matplotlib.org/stable/users/project/citing.html"&gt;A ready-made citation entry&lt;/a&gt; is available.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>exo-explore/exo</title>
      <link>https://github.com/exo-explore/exo</link>
      <description>&lt;p&gt;Run your own AI cluster at home with everyday devices üì±üíª üñ•Ô∏è‚åö&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="/docs/exo-logo-black-bg.jpg" /&gt; 
  &lt;img alt="exo logo" src="https://raw.githubusercontent.com/exo-explore/exo/main/docs/exo-logo-transparent.png" width="50%" height="50%" /&gt; 
 &lt;/picture&gt; 
 &lt;p&gt;exo: Run your own AI cluster at home with everyday devices. Maintained by &lt;a href="https://x.com/exolabs"&gt;exo labs&lt;/a&gt;.&lt;/p&gt; 
 &lt;h3&gt; &lt;p&gt;&lt;a href="https://discord.gg/EUnjGpsmWw"&gt;Discord&lt;/a&gt; | &lt;a href="https://t.me/+Kh-KqHTzFYg3MGNk"&gt;Telegram&lt;/a&gt; | &lt;a href="https://x.com/exolabs"&gt;X&lt;/a&gt;&lt;/p&gt; &lt;/h3&gt; 
 &lt;p&gt;&lt;a href="https://github.com/exo-explore/exo/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/exo-explore/exo" alt="GitHub Repo stars" /&gt;&lt;/a&gt; &lt;a href="https://dl.circleci.com/status-badge/redirect/circleci/TrkofJDoGzdQAeL6yVHKsg/4i5hJuafuwZYZQxbRAWS71/tree/main"&gt;&lt;img src="https://dl.circleci.com/status-badge/img/circleci/TrkofJDoGzdQAeL6yVHKsg/4i5hJuafuwZYZQxbRAWS71/tree/main.svg?style=svg" alt="Tests" /&gt;&lt;/a&gt; &lt;a href="https://www.gnu.org/licenses/gpl-3.0"&gt;&lt;img src="https://img.shields.io/badge/License-GPLv3-blue.svg?sanitize=true" alt="License: GPL v3" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/11849" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/11849" alt="exo-explore%2Fexo | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;p&gt;Unify your existing devices into one powerful GPU: iPhone, iPad, Android, Mac, NVIDIA, Raspberry Pi, pretty much any device!&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;h2&gt;Update: exo is hiring. See &lt;a href="https://exolabs.net"&gt;here&lt;/a&gt; for more details.&lt;/h2&gt; 
 &lt;h2&gt;Interested in running exo in your business? &lt;a href="mailto:hello@exolabs.net"&gt;Contact us&lt;/a&gt; to discuss.&lt;/h2&gt; 
&lt;/div&gt; 
&lt;h2&gt;Get Involved&lt;/h2&gt; 
&lt;p&gt;exo is &lt;strong&gt;experimental&lt;/strong&gt; software. Expect bugs early on. Create issues so they can be fixed. The &lt;a href="https://x.com/exolabs"&gt;exo labs&lt;/a&gt; team will strive to resolve issues quickly.&lt;/p&gt; 
&lt;p&gt;We also welcome contributions from the community. We have a list of bounties in &lt;a href="https://docs.google.com/spreadsheets/d/1cTCpTIp48UnnIvHeLEUNg1iMy_Q6lRybgECSFCoVJpE/edit?usp=sharing"&gt;this sheet&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;h3&gt;Wide Model Support&lt;/h3&gt; 
&lt;p&gt;exo supports different models including LLaMA (&lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/inference/mlx/models/llama.py"&gt;MLX&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/inference/tinygrad/models/llama.py"&gt;tinygrad&lt;/a&gt;), Mistral, LlaVA, Qwen, and Deepseek.&lt;/p&gt; 
&lt;h3&gt;Dynamic Model Partitioning&lt;/h3&gt; 
&lt;p&gt;exo &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/topology/ring_memory_weighted_partitioning_strategy.py"&gt;optimally splits up models&lt;/a&gt; based on the current network topology and device resources available. This enables you to run larger models than you would be able to on any single device.&lt;/p&gt; 
&lt;h3&gt;Automatic Device Discovery&lt;/h3&gt; 
&lt;p&gt;exo will &lt;a href="https://github.com/exo-explore/exo/raw/945f90f676182a751d2ad7bcf20987ab7fe0181e/exo/orchestration/node.py#L154"&gt;automatically discover&lt;/a&gt; other devices using the best method available. Zero manual configuration.&lt;/p&gt; 
&lt;h3&gt;ChatGPT-compatible API&lt;/h3&gt; 
&lt;p&gt;exo provides a &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/api/chatgpt_api.py"&gt;ChatGPT-compatible API&lt;/a&gt; for running models. It's a &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/examples/chatgpt_api.sh"&gt;one-line change&lt;/a&gt; in your application to run models on your own hardware using exo.&lt;/p&gt; 
&lt;h3&gt;Device Equality&lt;/h3&gt; 
&lt;p&gt;Unlike other distributed inference frameworks, exo does not use a master-worker architecture. Instead, exo devices &lt;a href="https://github.com/exo-explore/exo/raw/945f90f676182a751d2ad7bcf20987ab7fe0181e/exo/orchestration/node.py#L161"&gt;connect p2p&lt;/a&gt;. As long as a device is connected somewhere in the network, it can be used to run models.&lt;/p&gt; 
&lt;p&gt;Exo supports different &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/topology/partitioning_strategy.py"&gt;partitioning strategies&lt;/a&gt; to split up a model across devices. The default partitioning strategy is &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/topology/ring_memory_weighted_partitioning_strategy.py"&gt;ring memory weighted partitioning&lt;/a&gt;. This runs an inference in a ring where each device runs a number of model layers proportional to the memory of the device.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/exo-explore/exo/main/docs/exo-screenshot.jpg" alt="&amp;quot;A screenshot of exo running 5 nodes" /&gt;&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;The current recommended way to install exo is from source.&lt;/p&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python&amp;gt;=3.12.0 is required because of &lt;a href="https://github.com/exo-explore/exo/issues/5"&gt;issues with asyncio&lt;/a&gt; in previous versions.&lt;/li&gt; 
 &lt;li&gt;For Linux with NVIDIA GPU support (Linux-only, skip if not using Linux or NVIDIA): 
  &lt;ul&gt; 
   &lt;li&gt;NVIDIA driver - verify with &lt;code&gt;nvidia-smi&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;CUDA toolkit - install from &lt;a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#cuda-cross-platform-installation"&gt;NVIDIA CUDA guide&lt;/a&gt;, verify with &lt;code&gt;nvcc --version&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;cuDNN library - download from &lt;a href="https://developer.nvidia.com/cudnn-downloads"&gt;NVIDIA cuDNN page&lt;/a&gt;, verify installation by following &lt;a href="https://docs.nvidia.com/deeplearning/cudnn/latest/installation/linux.html#verifying-the-install-on-linux:~:text=at%20a%20time.-,Verifying%20the%20Install%20on%20Linux,Test%20passed!,-Upgrading%20From%20Older"&gt;these steps&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Hardware Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;The only requirement to run exo is to have enough memory across all your devices to fit the entire model into memory. For example, if you are running llama 3.1 8B (fp16), you need 16GB of memory across all devices. Any of the following configurations would work since they each have more than 16GB of memory in total: 
  &lt;ul&gt; 
   &lt;li&gt;2 x 8GB M3 MacBook Airs&lt;/li&gt; 
   &lt;li&gt;1 x 16GB NVIDIA RTX 4070 Ti Laptop&lt;/li&gt; 
   &lt;li&gt;2 x Raspberry Pi 400 with 4GB of RAM each (running on CPU) + 1 x 8GB Mac Mini&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;exo is designed to run on devices with heterogeneous capabilities. For example, you can have some devices with powerful GPUs and others with integrated GPUs or even CPUs. Adding less capable devices will slow down individual inference latency but will increase the overall throughput of the cluster.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;From source&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;git clone https://github.com/exo-explore/exo.git
cd exo
pip install -e .
# alternatively, with venv
source install.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Troubleshooting&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;If running on Mac, MLX has an &lt;a href="https://ml-explore.github.io/mlx/build/html/install.html"&gt;install guide&lt;/a&gt; with troubleshooting steps.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Performance&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;There are a number of things users have empirically found to improve performance on Apple Silicon Macs:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;ol&gt; 
 &lt;li&gt;Upgrade to the latest version of macOS Sequoia.&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;./configure_mlx.sh&lt;/code&gt;. This runs commands to optimize GPU memory allocation on Apple Silicon Macs.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;h3&gt;Example Usage on Multiple macOS Devices&lt;/h3&gt; 
&lt;h4&gt;Device 1:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Device 2:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;That's it! No configuration required - exo will automatically discover the other device(s).&lt;/p&gt; 
&lt;p&gt;exo starts a ChatGPT-like WebUI (powered by &lt;a href="https://github.com/tinygrad/tinygrad/tree/master/examples/tinychat"&gt;tinygrad tinychat&lt;/a&gt;) on &lt;a href="http://localhost:52415"&gt;http://localhost:52415&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;For developers, exo also starts a ChatGPT-compatible API endpoint on &lt;a href="http://localhost:52415/v1/chat/completions"&gt;http://localhost:52415/v1/chat/completions&lt;/a&gt;. Examples with curl:&lt;/p&gt; 
&lt;h4&gt;Llama 3.2 3B:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;curl http://localhost:52415/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
     "model": "llama-3.2-3b",
     "messages": [{"role": "user", "content": "What is the meaning of exo?"}],
     "temperature": 0.7
   }'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Llama 3.1 405B:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;curl http://localhost:52415/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
     "model": "llama-3.1-405b",
     "messages": [{"role": "user", "content": "What is the meaning of exo?"}],
     "temperature": 0.7
   }'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;DeepSeek R1 (full 671B):&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;curl http://localhost:52415/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
     "model": "deepseek-r1",
     "messages": [{"role": "user", "content": "What is the meaning of exo?"}],
     "temperature": 0.7
   }'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Llava 1.5 7B (Vision Language Model):&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;curl http://localhost:52415/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
     "model": "llava-1.5-7b-hf",
     "messages": [
      {
        "role": "user",
        "content": [
          {
            "type": "text",
            "text": "What are these?"
          },
          {
            "type": "image_url",
            "image_url": {
              "url": "http://images.cocodataset.org/val2017/000000039769.jpg"
            }
          }
        ]
      }
    ],
     "temperature": 0.0
   }'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Example Usage on Multiple Heterogenous Devices (macOS + Linux)&lt;/h3&gt; 
&lt;h4&gt;Device 1 (macOS):&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note: We don't need to explicitly tell exo to use the &lt;strong&gt;tinygrad&lt;/strong&gt; inference engine. &lt;strong&gt;MLX&lt;/strong&gt; and &lt;strong&gt;tinygrad&lt;/strong&gt; are interoperable!&lt;/p&gt; 
&lt;h4&gt;Device 2 (Linux):&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Linux devices will automatically default to using the &lt;strong&gt;tinygrad&lt;/strong&gt; inference engine.&lt;/p&gt; 
&lt;p&gt;You can read about tinygrad-specific env vars &lt;a href="https://docs.tinygrad.org/env_vars/"&gt;here&lt;/a&gt;. For example, you can configure tinygrad to use the cpu by specifying &lt;code&gt;CLANG=1&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Example Usage on a single device with "exo run" command&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;exo run llama-3.2-3b
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;With a custom prompt:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;exo run llama-3.2-3b --prompt "What is the meaning of exo?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Model Storage&lt;/h3&gt; 
&lt;p&gt;Models by default are stored in &lt;code&gt;~/.cache/exo/downloads&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;You can set a different model storage location by setting the &lt;code&gt;EXO_HOME&lt;/code&gt; env var.&lt;/p&gt; 
&lt;h2&gt;Model Downloading&lt;/h2&gt; 
&lt;p&gt;Models are downloaded from Hugging Face. If you are running exo in a country with strict internet censorship, you may need to download the models manually and put them in the &lt;code&gt;~/.cache/exo/downloads&lt;/code&gt; directory.&lt;/p&gt; 
&lt;p&gt;To download models from a proxy endpoint, set the &lt;code&gt;HF_ENDPOINT&lt;/code&gt; environment variable. For example, to run exo with the huggingface mirror endpoint:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;HF_ENDPOINT=https://hf-mirror.com exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Debugging&lt;/h2&gt; 
&lt;p&gt;Enable debug logs with the DEBUG environment variable (0-9).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;DEBUG=9 exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For the &lt;strong&gt;tinygrad&lt;/strong&gt; inference engine specifically, there is a separate DEBUG flag &lt;code&gt;TINYGRAD_DEBUG&lt;/code&gt; that can be used to enable debug logs (1-6).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;TINYGRAD_DEBUG=2 exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Formatting&lt;/h2&gt; 
&lt;p&gt;We use &lt;a href="https://github.com/google/yapf"&gt;yapf&lt;/a&gt; to format the code. To format the code, first install the formatting requirements:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip3 install -e '.[formatting]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then run the formatting script:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python3 format.py ./exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Known Issues&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;On certain versions of Python on macOS, certificates may not installed correctly, potentially causing SSL errors (e.g., when accessing huggingface.co). To resolve this, run the &lt;code&gt;Install Certificates&lt;/code&gt; command, typicall as follows:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;/Applications/Python 3.x/Install Certificates.command
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;üöß As the library is evolving so quickly, the iOS implementation has fallen behind Python. We have decided for now not to put out the buggy iOS version and receive a bunch of GitHub issues for outdated code. We are working on solving this properly and will make an announcement when it's ready. If you would like access to the iOS implementation now, please email &lt;a href="mailto:alex@exolabs.net"&gt;alex@exolabs.net&lt;/a&gt; with your GitHub username explaining your use-case and you will be granted access on GitHub.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Inference Engines&lt;/h2&gt; 
&lt;p&gt;exo supports the following inference engines:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚úÖ &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/inference/mlx/sharded_inference_engine.py"&gt;MLX&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;‚úÖ &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/inference/tinygrad/inference.py"&gt;tinygrad&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üöß &lt;a href="https://github.com/exo-explore/exo/pull/139"&gt;PyTorch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üöß &lt;a href="https://github.com/exo-explore/exo/issues/167"&gt;llama.cpp&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Discovery Modules&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚úÖ &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/networking/udp"&gt;UDP&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;‚úÖ &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/networking/manual"&gt;Manual&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;‚úÖ &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/networking/tailscale"&gt;Tailscale&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üöß Radio&lt;/li&gt; 
 &lt;li&gt;üöß Bluetooth&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Peer Networking Modules&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚úÖ &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/networking/grpc"&gt;GRPC&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üöß NCCL&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>saleor/saleor</title>
      <link>https://github.com/saleor/saleor</link>
      <description>&lt;p&gt;Saleor Core: the high performance, composable, headless commerce API.&lt;/p&gt;&lt;hr&gt;&lt;div align="center" width="100px"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://github.com/user-attachments/assets/76e3079f-696a-4fcd-8658-89739647090b" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://github.com/user-attachments/assets/8477d643-a905-4c63-8ed3-03d0976f6fc3" /&gt; 
  &lt;img width="200" alt="saleor-commerce-logo" src="https://user-images.githubusercontent.com/4006792/214636328-8e4f83e8-66cb-4114-a3d8-473eb908b9c3.png" /&gt; 
 &lt;/picture&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;strong&gt;Commerce that works with your language and stack&lt;/strong&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt;
  GraphQL native, API-only platform for scalable composable commerce. 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt;
  Get to know Saleor: 
 &lt;br /&gt; 
 &lt;a href="https://saleor.typeform.com/talk-with-us?utm_source=github&amp;amp;utm_medium=readme&amp;amp;utm_campaign=repo_saleor"&gt;Talk to a human&lt;/a&gt; 
 &lt;span&gt; | &lt;/span&gt; 
 &lt;a href="https://cloud.saleor.io/signup?utm_source=github&amp;amp;utm_medium=readme&amp;amp;utm_campaign=repo_saleor"&gt;Talk to the API&lt;/a&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt;
  Join our community: 
 &lt;br /&gt; 
 &lt;a href="https://saleor.io/"&gt;Website&lt;/a&gt; 
 &lt;span&gt; | &lt;/span&gt; 
 &lt;a href="https://twitter.com/getsaleor"&gt;Twitter&lt;/a&gt; 
 &lt;span&gt; | &lt;/span&gt; 
 &lt;a href="https://saleor.io/discord"&gt;Discord&lt;/a&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://saleor.io/blog"&gt;Blog&lt;/a&gt; 
 &lt;span&gt; | &lt;/span&gt; 
 &lt;a href="https://saleor.typeform.com/to/JTJK0Nou"&gt;Subscribe to newsletter&lt;/a&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://codecov.io/gh/saleor/saleor"&gt; &lt;img src="https://codecov.io/gh/saleor/saleor/graph/badge.svg?token=qkNcTJ4TmI" alt="Coverage" /&gt; &lt;/a&gt; 
 &lt;a href="https://docs.saleor.io/"&gt; &lt;img src="https://img.shields.io/badge/docs-docs.saleor.io-brightgreen.svg?sanitize=true" alt="Documentation" /&gt; &lt;/a&gt; 
 &lt;a href="https://github.com/astral-sh/ruff"&gt; &lt;img src="https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json" alt="Linted by Ruff" /&gt; &lt;/a&gt; 
 &lt;a href="https://saleor.io/discord"&gt; &lt;img src="https://img.shields.io/discord/864066819866624010" alt="Discord" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/saleor/saleor/main/#what-makes-saleor-special"&gt;What makes Saleor special?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/saleor/saleor/main/#why-api-only-architecture"&gt;Why API-only Architecture?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/saleor/saleor/main/#features"&gt;Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/saleor/saleor/main/#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/saleor/saleor/main/#documentation"&gt;Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/saleor/saleor/main/#saleor-platform"&gt;Saleor Platform&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/saleor/saleor/main/#storefront"&gt;Storefront&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/saleor/saleor/main/#dashboard"&gt;Dashboard&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/saleor/saleor/main/#contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/saleor/saleor/main/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;What makes Saleor special?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Technology-agnostic&lt;/strong&gt; - no monolithic plugin architecture or technology lock-in.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;GraphQL only&lt;/strong&gt; - Not afterthought API design or fragmentation across different styles of API.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Headless and API only&lt;/strong&gt; - APIs are the only way to interact, configure, or extend the backend.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Open source&lt;/strong&gt; - a single version of Saleor without feature fragmentation or commercial limitations.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Cloud native&lt;/strong&gt; - battle tested on global brands.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Native-multichannel&lt;/strong&gt; - Per &lt;a href="https://docs.saleor.io/developer/channels/overview"&gt;channel&lt;/a&gt; control of pricing, currencies, stock, product, and more.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Why API-only Architecture?&lt;/h2&gt; 
&lt;p&gt;Saleor's API-first extensibility provides powerful tools for developers to extend backend using &lt;a href="https://docs.saleor.io/developer/extending/webhooks/overview"&gt;webhooks&lt;/a&gt;, attributes, &lt;a href="https://docs.saleor.io/api-usage/metadata"&gt;metadata&lt;/a&gt;, &lt;a href="https://docs.saleor.io/developer/extending/apps/overview"&gt;apps&lt;/a&gt;, &lt;a href="https://docs.saleor.io/developer/extending/webhooks/subscription-webhook-payloads"&gt;subscription queries&lt;/a&gt;, &lt;a href="https://docs.saleor.io/developer/extending/webhooks/synchronous-events/overview"&gt;API extensions&lt;/a&gt;, &lt;a href="https://docs.saleor.io/developer/extending/apps/overview"&gt;dashboard iframes&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Compared to traditional plugin architectures (monoliths) it provides the following benefits:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;There is less downtime as apps are deployed independently.&lt;/li&gt; 
 &lt;li&gt;Reliability and performance - custom logic is separated from the core.&lt;/li&gt; 
 &lt;li&gt;Simplified upgrade paths - eliminates incompatibility conflicts between extensions.&lt;/li&gt; 
 &lt;li&gt;Technology-agnostic - works with any technology, stack, or language.&lt;/li&gt; 
 &lt;li&gt;Parallel development - easier to collaborate than with a monolithic core.&lt;/li&gt; 
 &lt;li&gt;Simplified debugging - easier to narrow down bugs in independent services.&lt;/li&gt; 
 &lt;li&gt;Scalability - extensions and apps can be scaled independently.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;What are the tradeoffs?&lt;/h3&gt; 
&lt;p&gt;If you are a single developer working with a small business that doesn't have high traffic or a critical need for 24/7 availability, using a service-oriented approach might feel more complex compared to the traditional WordPress or Magento approach that provides a language-specific framework, runtime, database schema, aspect-oriented programming, and other tools to a quick start.&lt;/p&gt; 
&lt;p&gt;However, if you deploy on a daily basis, reliability and uptime is critical, you need to collaborate with other developers, or you have non-trivial requirements you might be in the right place.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Enterprise ready&lt;/strong&gt;: Secure, scalable, and stable. Battle-tested by big brands&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Dashboard&lt;/strong&gt;: User-friendly, fast, and productive. (Decoupled project &lt;a href="https://github.com/saleor/saleor-dashboard"&gt;repo&lt;/a&gt; )&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Global by design&lt;/strong&gt; Multi-currency, multi-language, multi-warehouse, tutti multi!&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;CMS&lt;/strong&gt;: Manage product or marketing content.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Product management&lt;/strong&gt;: A rich content model for large and complex catalogs.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Orders&lt;/strong&gt;: Flexible order model, split payments, multi-warehouse, returns, and more.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Customers&lt;/strong&gt;: Order history and preferences.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Promotion engine&lt;/strong&gt;: Sales, vouchers, cart rules, giftcards.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Payment orchestration&lt;/strong&gt;: multi-gateway, extensible payment API, flexible flows.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Cart&lt;/strong&gt;: Advanced payment and tax options, with full control over discounts and promotions.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Payments&lt;/strong&gt;: Flexible API architecture allows integration of any payment method.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Translations&lt;/strong&gt;: Fully translatable catalog.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;SEO&lt;/strong&gt;: Unlimited SEO freedom with headless architecture.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Apps&lt;/strong&gt;: Extend dashboard via iframe with any web stack.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/9268745/224249510-d3c7658e-6d5c-42c5-b4fb-93eaf65a5335.png" alt="Saleor Dashboard - Modern UI for managing your e-commerce" /&gt;&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://docs.saleor.io/setup/docker-compose"&gt;See the Saleor docs&lt;/a&gt; for step-by-step installation and deployment instructions. For local development without Docker, follow our &lt;a href="https://raw.githubusercontent.com/saleor/saleor/main/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Note: The &lt;code&gt;main&lt;/code&gt; branch is the development version of Saleor and it may be unstable. To use the latest stable version, download it from the &lt;a href="https://github.com/saleor/saleor/releases/"&gt;Releases&lt;/a&gt; page or switch to a release tag.&lt;/p&gt; 
&lt;p&gt;The current production-ready version is 3.x and you should use this version for all three components:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Saleor: &lt;a href="https://github.com/saleor/saleor/releases/"&gt;https://github.com/saleor/saleor/releases/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Dashboard: &lt;a href="https://github.com/saleor/saleor-dashboard/releases/"&gt;https://github.com/saleor/saleor-dashboard/releases/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Storefront: &lt;a href="https://github.com/saleor/react-storefront/releases/"&gt;https://github.com/saleor/react-storefront/releases/&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Saleor Cloud&lt;/h3&gt; 
&lt;p&gt;The fastest way to develop with Saleor is by using developer accounts in &lt;a href="https://cloud.saleor.io"&gt;Saleor Cloud&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Register &lt;a href="https://cloud.saleor.io/register"&gt;here&lt;/a&gt; or install our &lt;a href="https://github.com/saleor/saleor-cli"&gt;CLI tool&lt;/a&gt;:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;npm i -g @saleor/cli&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;and run the following command:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;saleor register&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Bootstrap your first &lt;a href="https://github.com/saleor/react-storefront"&gt;storefront&lt;/a&gt; with:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;saleor storefront create --url {your-saleor-graphql-endpoint}&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;Saleor documentation is available here: &lt;a href="https://docs.saleor.io"&gt;docs.saleor.io&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;To contribute, please see the &lt;a href="https://github.com/saleor/saleor-docs/"&gt;&lt;code&gt;saleor/saleor-docs&lt;/code&gt; repository&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Saleor Platform&lt;/h2&gt; 
&lt;p&gt;The easiest way to run all components of Saleor (API, storefront, and dashboard) together on your local machine is to use the &lt;a href="https://github.com/saleor/saleor-platform"&gt;saleor-platform&lt;/a&gt; project. Go to that repository for instructions on how to use it.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/saleor/saleor-platform"&gt;View saleor-platform&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Storefront&lt;/h2&gt; 
&lt;p&gt;An open-source storefront example built with Next.js App Router, React.js, TypeScript, GraphQL, and Tailwind CSS.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/saleor/storefront"&gt;React Storefront Repository&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://storefront.saleor.io/"&gt;View Storefront Example&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Dashboard&lt;/h2&gt; 
&lt;p&gt;For the dashboard, go to the &lt;a href="https://github.com/saleor/saleor-dashboard"&gt;saleor-dashboard&lt;/a&gt; repository.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We love your contributions and do our best to provide you with mentorship and support. If you are looking for an issue to tackle, take a look at issues labeled &lt;a href="https://github.com/saleor/saleor/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22+"&gt;&lt;code&gt;Good first issue&lt;/code&gt;&lt;/a&gt; and &lt;a href="https://github.com/saleor/saleor/issues?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22"&gt;&lt;code&gt;Help wanted&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If nothing grabs your attention, check &lt;a href="https://saleor.io/roadmap"&gt;our roadmap&lt;/a&gt; or &lt;a href="https://saleor.io/discord"&gt;start a Discord discussion&lt;/a&gt; about a feature you'd like to see. Make sure to read our &lt;a href="http://docs.saleor.io/developer/community/contributing"&gt;Contribution Guidelines&lt;/a&gt; before opening a PR or issue.&lt;/p&gt; 
&lt;p&gt;Get more details (e.g., how to run Saleor on your local machine) in our &lt;a href="https://raw.githubusercontent.com/saleor/saleor/main/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Disclaimer: Everything you see here is open and free to use as long as you comply with the &lt;a href="https://github.com/saleor/saleor/raw/master/LICENSE"&gt;license&lt;/a&gt;. There are no hidden charges. We promise to do our best to fix bugs and improve the code.&lt;/p&gt; 
&lt;h4&gt;Crafted with ‚ù§Ô∏è by &lt;a href="https://saleor.io"&gt;Saleor Commerce&lt;/a&gt;&lt;/h4&gt; 
&lt;p&gt;&lt;a href="mailto:hello@saleor.io"&gt;hello@saleor.io&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>bytedance/UI-TARS</title>
      <link>https://github.com/bytedance/UI-TARS</link>
      <description>&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/bytedance/UI-TARS/main/figures/writer.png" alt="Local Image" /&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; üåê &lt;a href="https://seed-tars.com/"&gt;Website&lt;/a&gt;&amp;nbsp;&amp;nbsp; | ü§ó &lt;a href="https://huggingface.co/ByteDance-Seed/UI-TARS-1.5-7B"&gt;Hugging Face Models&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; üîß &lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS/main/README_deploy.md"&gt;Deployment&lt;/a&gt; &amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; üìë &lt;a href="https://arxiv.org/abs/2501.12326"&gt;Paper&lt;/a&gt; &amp;nbsp;&amp;nbsp; |&amp;nbsp;&amp;nbsp; üñ•Ô∏è &lt;a href="https://github.com/bytedance/UI-TARS-desktop"&gt;UI-TARS-desktop&lt;/a&gt;&amp;nbsp;&amp;nbsp; &lt;br /&gt;üèÑ &lt;a href="https://github.com/web-infra-dev/Midscene"&gt;Midscene (Browser Automation) &lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ü´® &lt;a href="https://discord.gg/pTXwYVjfcs"&gt;Discord&lt;/a&gt;&amp;nbsp;&amp;nbsp; &lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/13561"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/13561" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;We also offer a &lt;strong&gt;UI-TARS-desktop&lt;/strong&gt; version, which can operate on your &lt;strong&gt;local personal device&lt;/strong&gt;. To use it, please visit &lt;a href="https://github.com/bytedance/UI-TARS-desktop"&gt;https://github.com/bytedance/UI-TARS-desktop&lt;/a&gt;. To use UI-TARS in web automation, you may refer to the open-source project &lt;a href="https://github.com/web-infra-dev/Midscene"&gt;Midscene.js&lt;/a&gt;. &lt;strong&gt;‚ùóNotes&lt;/strong&gt;: Since Qwen 2.5vl based models ultilizes absolute coordinates to ground objects, please kindly refer to our illustration about how to process coordinates in this &lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS/main/README_coordinates.md"&gt;guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Updates&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;üåü 2025.04.16: We shared the latest progress of the UI-TARS-1.5 model in our &lt;a href="https://seed-tars.com/1.5"&gt;blog&lt;/a&gt;, which excels in playing games and performing GUI tasks, and we open-sourced the &lt;a href="https://huggingface.co/ByteDance-Seed/UI-TARS-1.5-7B"&gt;UI-TARS-1.5-7B&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;‚ú® 2025.03.23: We updated the OSWorld inference scripts from the original official &lt;a href="https://github.com/xlang-ai/OSWorld/raw/main/run_uitars.py"&gt;OSWorld repository&lt;/a&gt;. Now, you can use the OSWorld official inference scripts to reproduce our results.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;UI-TARS-1.5, an open-source multimodal agent built upon a powerful vision-language model. It is capable of effectively performing diverse tasks within virtual worlds.&lt;/p&gt; 
&lt;p&gt;Leveraging the foundational architecture introduced in &lt;a href="https://arxiv.org/abs/2501.12326"&gt;our recent paper&lt;/a&gt;, UI-TARS-1.5 integrates advanced reasoning enabled by reinforcement learning. This allows the model to reason through its thoughts before taking action, significantly enhancing its performance and adaptability, particularly in inference-time scaling. Our new 1.5 version achieves state-of-the-art results across a variety of standard benchmarks, demonstrating strong reasoning capabilities and notable improvements over prior models.&lt;/p&gt; 
&lt;!-- ![Local Image](figures/UI-TARS.png) --&gt; 
&lt;p align="center"&gt; 
 &lt;video controls width="480"&gt; 
  &lt;source src="https://huggingface.co/datasets/JjjFangg/Demo_video/resolve/main/GUI_demo.mp4" type="video/mp4" /&gt; 
 &lt;/video&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p align="center"&gt; 
 &lt;video controls width="480"&gt; 
  &lt;source src="https://huggingface.co/datasets/JjjFangg/Demo_video/resolve/main/Game_demo.mp4" type="video/mp4" /&gt; 
 &lt;/video&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2&gt;üöÄ Quick Start Guide: Deploying and Using Our Model&lt;/h2&gt; 
&lt;p&gt;To help you get started quickly with our model, we recommend following the steps below in order. These steps will guide you through deployment, prediction post-processing to make the model take actions in your environment.&lt;/p&gt; 
&lt;h3&gt;‚úÖ Step 1: Deployment &amp;amp; Inference&lt;/h3&gt; 
&lt;p&gt;üëâ &lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS/main/README_deploy.md"&gt;Deployment and Inference&lt;/a&gt;. This includes instructions for model deployment using huggingface endpoint, and running your first prediction.&lt;/p&gt; 
&lt;h3&gt;‚úÖ Step 2: Post Processing&lt;/h3&gt; 
&lt;h4&gt;Installation&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install ui-tars
# or
uv pip install ui-tars
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Usage&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from ui_tars.action_parser import parse_action_to_structure_output, parsing_response_to_pyautogui_code

response = "Thought: Click the button\nAction: click(start_box='(100,200)')"
original_image_width, original_image_height = 1920, 1080
parsed_dict = parse_action_to_structure_output(
    response,
    factor=1000,
    origin_resized_height=original_image_height,
    origin_resized_width=original_image_width,
    model_type="qwen25vl"
)
print(parsed_dict)
parsed_pyautogui_code = parsing_response_to_pyautogui_code(
    responses=parsed_dict,
    image_height=original_image_height,
    image_width=original_image_width
)
print(parsed_pyautogui_code)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;FYI: Coordinates visualization&lt;/h5&gt; 
&lt;p&gt;To help you better understand the coordinate processing, we also provide a &lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS/main/README_coordinates.md"&gt;guide&lt;/a&gt; for coordinates processing visualization.&lt;/p&gt; 
&lt;h2&gt;Prompt Usage Guide&lt;/h2&gt; 
&lt;p&gt;To accommodate different device environments and task complexities, the following three prompt templates in &lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS/main/codes/ui_tars/prompt.py"&gt;codes/ui_tars/prompt.py&lt;/a&gt;. are designed to guide GUI agents in generating appropriate actions. Choose the template that best fits your use case:&lt;/p&gt; 
&lt;h3&gt;üñ•Ô∏è &lt;code&gt;COMPUTER_USE&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Recommended for&lt;/strong&gt;: GUI tasks on &lt;strong&gt;desktop environments&lt;/strong&gt; such as Windows, Linux, or macOS.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Supports common desktop operations: mouse clicks (single, double, right), drag actions, keyboard shortcuts, text input, scrolling, etc.&lt;/li&gt; 
 &lt;li&gt;Ideal for browser navigation, office software interaction, file management, and other desktop-based tasks.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üì± &lt;code&gt;MOBILE_USE&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Recommended for&lt;/strong&gt;: GUI tasks on &lt;strong&gt;mobile devices or Android emulators&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Includes mobile-specific actions: &lt;code&gt;long_press&lt;/code&gt;, &lt;code&gt;open_app&lt;/code&gt;, &lt;code&gt;press_home&lt;/code&gt;, &lt;code&gt;press_back&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Suitable for launching apps, scrolling views, filling input fields, and navigating within mobile apps.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üìå &lt;code&gt;GROUNDING&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Recommended for&lt;/strong&gt;: Lightweight tasks focused solely on &lt;strong&gt;action output&lt;/strong&gt;, or for use in model training and evaluation.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Only outputs the &lt;code&gt;Action&lt;/code&gt; without any reasoning (&lt;code&gt;Thought&lt;/code&gt;).&lt;/li&gt; 
 &lt;li&gt;Useful for evaluating grounding capability.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;When developing or evaluating multimodal interaction systems, choose the appropriate prompt template based on your target platform (desktop vs. mobile)&lt;/p&gt; 
&lt;h2&gt;Performance&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Online Benchmark Evaluation&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Benchmark type&lt;/th&gt; 
   &lt;th&gt;Benchmark&lt;/th&gt; 
   &lt;th&gt;UI-TARS-1.5&lt;/th&gt; 
   &lt;th&gt;OpenAI CUA&lt;/th&gt; 
   &lt;th&gt;Claude 3.7&lt;/th&gt; 
   &lt;th&gt;Previous SOTA&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Computer Use&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/abs/2404.07972"&gt;OSworld&lt;/a&gt; (100 steps)&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;42.5&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;36.4&lt;/td&gt; 
   &lt;td&gt;28&lt;/td&gt; 
   &lt;td&gt;38.1 (200 step)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/abs/2409.08264"&gt;Windows Agent Arena&lt;/a&gt; (50 steps)&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;42.1&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;29.8&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Browser Use&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/abs/2401.13919"&gt;WebVoyager&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;84.8&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;87&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;84.1&lt;/td&gt; 
   &lt;td&gt;87&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/abs/2504.01382"&gt;Online-Mind2web&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;75.8&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;71&lt;/td&gt; 
   &lt;td&gt;62.9&lt;/td&gt; 
   &lt;td&gt;71&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Phone Use&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/abs/2405.14573"&gt;Android World&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;64.2&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;59.5&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Grounding Capability Evaluation&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Benchmark&lt;/th&gt; 
   &lt;th&gt;UI-TARS-1.5&lt;/th&gt; 
   &lt;th&gt;OpenAI CUA&lt;/th&gt; 
   &lt;th&gt;Claude 3.7&lt;/th&gt; 
   &lt;th&gt;Previous SOTA&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/2410.23218"&gt;ScreenSpot-V2&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;94.2&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;87.9&lt;/td&gt; 
   &lt;td&gt;87.6&lt;/td&gt; 
   &lt;td&gt;91.6&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/2504.07981v1"&gt;ScreenSpotPro&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;61.6&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;23.4&lt;/td&gt; 
   &lt;td&gt;27.7&lt;/td&gt; 
   &lt;td&gt;43.6&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Poki Game&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://poki.com/en/g/2048"&gt;2048&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://poki.com/en/g/cubinko"&gt;cubinko&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://poki.com/en/g/energy"&gt;energy&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://poki.com/en/g/free-the-key"&gt;free-the-key&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://poki.com/en/g/gem-11"&gt;Gem-11&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://poki.com/en/g/hex-frvr"&gt;hex-frvr&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://poki.com/en/g/infinity-loop"&gt;Infinity-Loop&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://poki.com/en/g/maze-path-of-light"&gt;Maze:Path-of-Light&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://poki.com/en/g/shapes"&gt;shapes&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://poki.com/en/g/snake-solver"&gt;snake-solver&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://poki.com/en/g/wood-blocks-3d"&gt;wood-blocks-3d&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://poki.com/en/g/yarn-untangle"&gt;yarn-untangle&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://poki.com/en/g/laser-maze-puzzle"&gt;laser-maze-puzzle&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://poki.com/en/g/tiles-master"&gt;tiles-master&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenAI CUA&lt;/td&gt; 
   &lt;td&gt;31.04&lt;/td&gt; 
   &lt;td&gt;0.00&lt;/td&gt; 
   &lt;td&gt;32.80&lt;/td&gt; 
   &lt;td&gt;0.00&lt;/td&gt; 
   &lt;td&gt;46.27&lt;/td&gt; 
   &lt;td&gt;92.25&lt;/td&gt; 
   &lt;td&gt;23.08&lt;/td&gt; 
   &lt;td&gt;35.00&lt;/td&gt; 
   &lt;td&gt;52.18&lt;/td&gt; 
   &lt;td&gt;42.86&lt;/td&gt; 
   &lt;td&gt;2.02&lt;/td&gt; 
   &lt;td&gt;44.56&lt;/td&gt; 
   &lt;td&gt;80.00&lt;/td&gt; 
   &lt;td&gt;78.27&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Claude 3.7&lt;/td&gt; 
   &lt;td&gt;43.05&lt;/td&gt; 
   &lt;td&gt;0.00&lt;/td&gt; 
   &lt;td&gt;41.60&lt;/td&gt; 
   &lt;td&gt;0.00&lt;/td&gt; 
   &lt;td&gt;0.00&lt;/td&gt; 
   &lt;td&gt;30.76&lt;/td&gt; 
   &lt;td&gt;2.31&lt;/td&gt; 
   &lt;td&gt;82.00&lt;/td&gt; 
   &lt;td&gt;6.26&lt;/td&gt; 
   &lt;td&gt;42.86&lt;/td&gt; 
   &lt;td&gt;0.00&lt;/td&gt; 
   &lt;td&gt;13.77&lt;/td&gt; 
   &lt;td&gt;28.00&lt;/td&gt; 
   &lt;td&gt;52.18&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;UI-TARS-1.5&lt;/td&gt; 
   &lt;td&gt;100.00&lt;/td&gt; 
   &lt;td&gt;0.00&lt;/td&gt; 
   &lt;td&gt;100.00&lt;/td&gt; 
   &lt;td&gt;100.00&lt;/td&gt; 
   &lt;td&gt;100.00&lt;/td&gt; 
   &lt;td&gt;100.00&lt;/td&gt; 
   &lt;td&gt;100.00&lt;/td&gt; 
   &lt;td&gt;100.00&lt;/td&gt; 
   &lt;td&gt;100.00&lt;/td&gt; 
   &lt;td&gt;100.00&lt;/td&gt; 
   &lt;td&gt;100.00&lt;/td&gt; 
   &lt;td&gt;100.00&lt;/td&gt; 
   &lt;td&gt;100.00&lt;/td&gt; 
   &lt;td&gt;100.00&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Minecraft&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Task Type&lt;/th&gt; 
   &lt;th&gt;Task Name&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://openai.com/index/vpt/"&gt;VPT&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://www.nature.com/articles/s41586-025-08744-2"&gt;DreamerV3&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;Previous SOTA&lt;/th&gt; 
   &lt;th&gt;UI-TARS-1.5 w/o Thought&lt;/th&gt; 
   &lt;th&gt;UI-TARS-1.5 w/ Thought&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Mine Blocks&lt;/td&gt; 
   &lt;td&gt;(oak_log)&lt;/td&gt; 
   &lt;td&gt;0.8&lt;/td&gt; 
   &lt;td&gt;1.0&lt;/td&gt; 
   &lt;td&gt;1.0&lt;/td&gt; 
   &lt;td&gt;1.0&lt;/td&gt; 
   &lt;td&gt;1.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;(obsidian)&lt;/td&gt; 
   &lt;td&gt;0.0&lt;/td&gt; 
   &lt;td&gt;0.0&lt;/td&gt; 
   &lt;td&gt;0.0&lt;/td&gt; 
   &lt;td&gt;0.2&lt;/td&gt; 
   &lt;td&gt;0.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;(white_bed)&lt;/td&gt; 
   &lt;td&gt;0.0&lt;/td&gt; 
   &lt;td&gt;0.0&lt;/td&gt; 
   &lt;td&gt;0.1&lt;/td&gt; 
   &lt;td&gt;0.4&lt;/td&gt; 
   &lt;td&gt;0.6&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;200 Tasks Avg.&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;0.06&lt;/td&gt; 
   &lt;td&gt;0.03&lt;/td&gt; 
   &lt;td&gt;0.32&lt;/td&gt; 
   &lt;td&gt;0.35&lt;/td&gt; 
   &lt;td&gt;0.42&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Kill Mobs&lt;/td&gt; 
   &lt;td&gt;(mooshroom)&lt;/td&gt; 
   &lt;td&gt;0.0&lt;/td&gt; 
   &lt;td&gt;0.0&lt;/td&gt; 
   &lt;td&gt;0.1&lt;/td&gt; 
   &lt;td&gt;0.3&lt;/td&gt; 
   &lt;td&gt;0.4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;(zombie)&lt;/td&gt; 
   &lt;td&gt;0.4&lt;/td&gt; 
   &lt;td&gt;0.1&lt;/td&gt; 
   &lt;td&gt;0.6&lt;/td&gt; 
   &lt;td&gt;0.7&lt;/td&gt; 
   &lt;td&gt;0.9&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;(chicken)&lt;/td&gt; 
   &lt;td&gt;0.1&lt;/td&gt; 
   &lt;td&gt;0.0&lt;/td&gt; 
   &lt;td&gt;0.4&lt;/td&gt; 
   &lt;td&gt;0.5&lt;/td&gt; 
   &lt;td&gt;0.6&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;100 Tasks Avg.&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;0.04&lt;/td&gt; 
   &lt;td&gt;0.03&lt;/td&gt; 
   &lt;td&gt;0.18&lt;/td&gt; 
   &lt;td&gt;0.25&lt;/td&gt; 
   &lt;td&gt;0.31&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Model Scale Comparison&lt;/h2&gt; 
&lt;p&gt;Here we compare performance across different model scales of UI-TARS on the OSworld benchmark.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;strong&gt;Benchmark Type&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Benchmark&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;UI-TARS-72B-DPO&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;UI-TARS-1.5-7B&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;UI-TARS-1.5&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Computer Use&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/abs/2404.07972"&gt;OSWorld&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;24.6&lt;/td&gt; 
   &lt;td&gt;27.5&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;42.5&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;GUI Grounding&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/2504.07981v1"&gt;ScreenSpotPro&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;38.1&lt;/td&gt; 
   &lt;td&gt;49.6&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;61.6&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Limitations&lt;/h3&gt; 
&lt;p&gt;While UI-TARS-1.5 represents a significant advancement in multimodal agent capabilities, we acknowledge several important limitations:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Misuse:&lt;/strong&gt; Given its enhanced performance in GUI tasks, including successfully navigating authentication challenges like CAPTCHA, UI-TARS-1.5 could potentially be misused for unauthorized access or automation of protected content. To mitigate this risk, extensive internal safety evaluations are underway.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Computation:&lt;/strong&gt; UI-TARS-1.5 still requires substantial computational resources, particularly for large-scale tasks or extended gameplay scenarios.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Hallucination&lt;/strong&gt;: UI-TARS-1.5 may occasionally generate inaccurate descriptions, misidentify GUI elements, or take suboptimal actions based on incorrect inferences‚Äîespecially in ambiguous or unfamiliar environments.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Model scale:&lt;/strong&gt; The released UI-TARS-1.5-7B focuses primarily on enhancing general computer use capabilities and is not specifically optimized for game-based scenarios, where the UI-TARS-1.5 still holds a significant advantage.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;What's next&lt;/h2&gt; 
&lt;p&gt;We are providing early research access to our top-performing UI-TARS-1.5 model to facilitate collaborative research. Interested researchers can contact us at &lt;a href="mailto:TARS@bytedance.com"&gt;TARS@bytedance.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Looking ahead, we envision UI-TARS evolving into increasingly sophisticated agentic experiences capable of performing real-world actions, thereby empowering platforms such as &lt;a href="https://team.doubao.com/en/"&gt;doubao&lt;/a&gt; to accomplish more complex tasks for you :)&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#bytedance/UI-TARS&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=bytedance/UI-TARS&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you find our paper and model useful in your research, feel free to give us a cite.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-BibTeX"&gt;@article{qin2025ui,
  title={UI-TARS: Pioneering Automated GUI Interaction with Native Agents},
  author={Qin, Yujia and Ye, Yining and Fang, Junjie and Wang, Haoming and Liang, Shihao and Tian, Shizuo and Zhang, Junda and Li, Jiahao and Li, Yunxin and Huang, Shijue and others},
  journal={arXiv preprint arXiv:2501.12326},
  year={2025}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>deepseek-ai/DeepSeek-V3</title>
      <link>https://github.com/deepseek-ai/DeepSeek-V3</link>
      <description>&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://github.com/deepseek-ai/DeepSeek-V2/raw/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" /&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;div align="center" style="line-height: 1;"&gt; 
 &lt;a href="https://www.deepseek.com/"&gt;&lt;img alt="Homepage" src="https://github.com/deepseek-ai/DeepSeek-V2/raw/main/figures/badge.svg?raw=true" /&gt;&lt;/a&gt; 
 &lt;a href="https://chat.deepseek.com/"&gt;&lt;img alt="Chat" src="https://img.shields.io/badge/ü§ñ%20Chat-DeepSeek%20V3-536af5?color=536af5&amp;amp;logoColor=white" /&gt;&lt;/a&gt; 
 &lt;a href="https://huggingface.co/deepseek-ai"&gt;&lt;img alt="Hugging Face" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&amp;amp;logoColor=white" /&gt;&lt;/a&gt; 
 &lt;br /&gt; 
 &lt;a href="https://discord.gg/Tc7c45Zzu5"&gt;&lt;img alt="Discord" src="https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&amp;amp;logoColor=white&amp;amp;color=7289da" /&gt;&lt;/a&gt; 
 &lt;a href="https://github.com/deepseek-ai/DeepSeek-V2/raw/main/figures/qr.jpeg?raw=true"&gt;&lt;img alt="Wechat" src="https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&amp;amp;logoColor=white" /&gt;&lt;/a&gt; 
 &lt;a href="https://twitter.com/deepseek_ai"&gt;&lt;img alt="Twitter Follow" src="https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&amp;amp;logoColor=white" /&gt;&lt;/a&gt; 
 &lt;br /&gt; 
 &lt;a href="https://github.com/deepseek-ai/DeepSeek-V3/raw/main/LICENSE-CODE"&gt;&lt;img alt="Code License" src="https://img.shields.io/badge/Code_License-MIT-f5de53?&amp;amp;color=f5de53" /&gt;&lt;/a&gt; 
 &lt;a href="https://github.com/deepseek-ai/DeepSeek-V3/raw/main/LICENSE-MODEL"&gt;&lt;img alt="Model License" src="https://img.shields.io/badge/Model_License-Model_Agreement-f5de53?&amp;amp;color=f5de53" /&gt;&lt;/a&gt; 
 &lt;br /&gt; 
 &lt;a href="https://arxiv.org/pdf/2412.19437"&gt;&lt;b&gt;Paper Link&lt;/b&gt;üëÅÔ∏è&lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/#1-introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/#2-model-summary"&gt;Model Summary&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/#3-model-downloads"&gt;Model Downloads&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/#4-evaluation-results"&gt;Evaluation Results&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/#5-chat-website--api-platform"&gt;Chat Website &amp;amp; API Platform&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/#6-how-to-run-locally"&gt;How to Run Locally&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/#7-license"&gt;License&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/#8-citation"&gt;Citation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/#9-contact"&gt;Contact&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;1. Introduction&lt;/h2&gt; 
&lt;p&gt;We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img width="80%" src="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/figures/benchmark.png" /&gt; &lt;/p&gt; 
&lt;h2&gt;2. Model Summary&lt;/h2&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;Architecture: Innovative Load Balancing Strategy and Training Objective&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;On top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free strategy for load balancing, which minimizes the performance degradation that arises from encouraging load balancing.&lt;/li&gt; 
 &lt;li&gt;We investigate a Multi-Token Prediction (MTP) objective and prove it beneficial to model performance. It can also be used for speculative decoding for inference acceleration.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;Pre-Training: Towards Ultimate Training Efficiency&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;We design an FP8 mixed precision training framework and, for the first time, validate the feasibility and effectiveness of FP8 training on an extremely large-scale model.&lt;/li&gt; 
 &lt;li&gt;Through co-design of algorithms, frameworks, and hardware, we overcome the communication bottleneck in cross-node MoE training, nearly achieving full computation-communication overlap.&lt;br /&gt; This significantly enhances our training efficiency and reduces the training costs, enabling us to further scale up the model size without additional overhead.&lt;/li&gt; 
 &lt;li&gt;At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;Post-Training: Knowledge Distillation from DeepSeek-R1&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;We introduce an innovative methodology to distill reasoning capabilities from the long-Chain-of-Thought (CoT) model, specifically from one of the DeepSeek R1 series models, into standard LLMs, particularly DeepSeek-V3. Our pipeline elegantly incorporates the verification and reflection patterns of R1 into DeepSeek-V3 and notably improves its reasoning performance. Meanwhile, we also maintain a control over the output style and length of DeepSeek-V3.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;3. Model Downloads&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;#Total Params&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;#Activated Params&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;Context Length&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;Download&lt;/strong&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;DeepSeek-V3-Base&lt;/td&gt; 
    &lt;td align="center"&gt;671B&lt;/td&gt; 
    &lt;td align="center"&gt;37B&lt;/td&gt; 
    &lt;td align="center"&gt;128K&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3-Base"&gt;ü§ó Hugging Face&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;DeepSeek-V3&lt;/td&gt; 
    &lt;td align="center"&gt;671B&lt;/td&gt; 
    &lt;td align="center"&gt;37B&lt;/td&gt; 
    &lt;td align="center"&gt;128K&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3"&gt;ü§ó Hugging Face&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] The total size of DeepSeek-V3 models on Hugging Face is 685B, which includes 671B of the Main Model weights and 14B of the Multi-Token Prediction (MTP) Module weights.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;To ensure optimal performance and flexibility, we have partnered with open-source communities and hardware vendors to provide multiple ways to run the model locally. For step-by-step guidance, check out Section 6: &lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/#6-how-to-run-locally"&gt;How_to Run_Locally&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For developers looking to dive deeper, we recommend exploring &lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/README_WEIGHTS.md"&gt;README_WEIGHTS.md&lt;/a&gt; for details on the Main Model weights and the Multi-Token Prediction (MTP) Modules. Please note that MTP support is currently under active development within the community, and we welcome your contributions and feedback.&lt;/p&gt; 
&lt;h2&gt;4. Evaluation Results&lt;/h2&gt; 
&lt;h3&gt;Base Model&lt;/h3&gt; 
&lt;h4&gt;Standard Benchmarks&lt;/h4&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;&lt;/th&gt; 
    &lt;th&gt;Benchmark (Metric)&lt;/th&gt; 
    &lt;th&gt;# Shots&lt;/th&gt; 
    &lt;th&gt;DeepSeek-V2&lt;/th&gt; 
    &lt;th&gt;Qwen2.5 72B&lt;/th&gt; 
    &lt;th&gt;LLaMA3.1 405B&lt;/th&gt; 
    &lt;th&gt;DeepSeek-V3&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;Architecture&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;MoE&lt;/td&gt; 
    &lt;td&gt;Dense&lt;/td&gt; 
    &lt;td&gt;Dense&lt;/td&gt; 
    &lt;td&gt;MoE&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;# Activated Params&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;21B&lt;/td&gt; 
    &lt;td&gt;72B&lt;/td&gt; 
    &lt;td&gt;405B&lt;/td&gt; 
    &lt;td&gt;37B&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;# Total Params&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;236B&lt;/td&gt; 
    &lt;td&gt;72B&lt;/td&gt; 
    &lt;td&gt;405B&lt;/td&gt; 
    &lt;td&gt;671B&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;English&lt;/td&gt; 
    &lt;td&gt;Pile-test (BPB)&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;0.606&lt;/td&gt; 
    &lt;td&gt;0.638&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;0.542&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;0.548&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;BBH (EM)&lt;/td&gt; 
    &lt;td&gt;3-shot&lt;/td&gt; 
    &lt;td&gt;78.8&lt;/td&gt; 
    &lt;td&gt;79.8&lt;/td&gt; 
    &lt;td&gt;82.9&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;87.5&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;MMLU (Acc.)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;78.4&lt;/td&gt; 
    &lt;td&gt;85.0&lt;/td&gt; 
    &lt;td&gt;84.4&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;87.1&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;MMLU-Redux (Acc.)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;75.6&lt;/td&gt; 
    &lt;td&gt;83.2&lt;/td&gt; 
    &lt;td&gt;81.3&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;86.2&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;MMLU-Pro (Acc.)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;51.4&lt;/td&gt; 
    &lt;td&gt;58.3&lt;/td&gt; 
    &lt;td&gt;52.8&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;64.4&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;DROP (F1)&lt;/td&gt; 
    &lt;td&gt;3-shot&lt;/td&gt; 
    &lt;td&gt;80.4&lt;/td&gt; 
    &lt;td&gt;80.6&lt;/td&gt; 
    &lt;td&gt;86.0&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;89.0&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;ARC-Easy (Acc.)&lt;/td&gt; 
    &lt;td&gt;25-shot&lt;/td&gt; 
    &lt;td&gt;97.6&lt;/td&gt; 
    &lt;td&gt;98.4&lt;/td&gt; 
    &lt;td&gt;98.4&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;98.9&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;ARC-Challenge (Acc.)&lt;/td&gt; 
    &lt;td&gt;25-shot&lt;/td&gt; 
    &lt;td&gt;92.2&lt;/td&gt; 
    &lt;td&gt;94.5&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;95.3&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;95.3&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;HellaSwag (Acc.)&lt;/td&gt; 
    &lt;td&gt;10-shot&lt;/td&gt; 
    &lt;td&gt;87.1&lt;/td&gt; 
    &lt;td&gt;84.8&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;89.2&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;88.9&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;PIQA (Acc.)&lt;/td&gt; 
    &lt;td&gt;0-shot&lt;/td&gt; 
    &lt;td&gt;83.9&lt;/td&gt; 
    &lt;td&gt;82.6&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;85.9&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;84.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;WinoGrande (Acc.)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;86.3&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;82.3&lt;/td&gt; 
    &lt;td&gt;85.2&lt;/td&gt; 
    &lt;td&gt;84.9&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;RACE-Middle (Acc.)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;73.1&lt;/td&gt; 
    &lt;td&gt;68.1&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;74.2&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;67.1&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;RACE-High (Acc.)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;52.6&lt;/td&gt; 
    &lt;td&gt;50.3&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;56.8&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;51.3&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;TriviaQA (EM)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;80.0&lt;/td&gt; 
    &lt;td&gt;71.9&lt;/td&gt; 
    &lt;td&gt;82.7&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;82.9&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;NaturalQuestions (EM)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;38.6&lt;/td&gt; 
    &lt;td&gt;33.2&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;41.5&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;40.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;AGIEval (Acc.)&lt;/td&gt; 
    &lt;td&gt;0-shot&lt;/td&gt; 
    &lt;td&gt;57.5&lt;/td&gt; 
    &lt;td&gt;75.8&lt;/td&gt; 
    &lt;td&gt;60.6&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;79.6&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Code&lt;/td&gt; 
    &lt;td&gt;HumanEval (Pass@1)&lt;/td&gt; 
    &lt;td&gt;0-shot&lt;/td&gt; 
    &lt;td&gt;43.3&lt;/td&gt; 
    &lt;td&gt;53.0&lt;/td&gt; 
    &lt;td&gt;54.9&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;65.2&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;MBPP (Pass@1)&lt;/td&gt; 
    &lt;td&gt;3-shot&lt;/td&gt; 
    &lt;td&gt;65.0&lt;/td&gt; 
    &lt;td&gt;72.6&lt;/td&gt; 
    &lt;td&gt;68.4&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;75.4&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;LiveCodeBench-Base (Pass@1)&lt;/td&gt; 
    &lt;td&gt;3-shot&lt;/td&gt; 
    &lt;td&gt;11.6&lt;/td&gt; 
    &lt;td&gt;12.9&lt;/td&gt; 
    &lt;td&gt;15.5&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;19.4&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;CRUXEval-I (Acc.)&lt;/td&gt; 
    &lt;td&gt;2-shot&lt;/td&gt; 
    &lt;td&gt;52.5&lt;/td&gt; 
    &lt;td&gt;59.1&lt;/td&gt; 
    &lt;td&gt;58.5&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;67.3&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;CRUXEval-O (Acc.)&lt;/td&gt; 
    &lt;td&gt;2-shot&lt;/td&gt; 
    &lt;td&gt;49.8&lt;/td&gt; 
    &lt;td&gt;59.9&lt;/td&gt; 
    &lt;td&gt;59.9&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;69.8&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Math&lt;/td&gt; 
    &lt;td&gt;GSM8K (EM)&lt;/td&gt; 
    &lt;td&gt;8-shot&lt;/td&gt; 
    &lt;td&gt;81.6&lt;/td&gt; 
    &lt;td&gt;88.3&lt;/td&gt; 
    &lt;td&gt;83.5&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;89.3&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;MATH (EM)&lt;/td&gt; 
    &lt;td&gt;4-shot&lt;/td&gt; 
    &lt;td&gt;43.4&lt;/td&gt; 
    &lt;td&gt;54.4&lt;/td&gt; 
    &lt;td&gt;49.0&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;61.6&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;MGSM (EM)&lt;/td&gt; 
    &lt;td&gt;8-shot&lt;/td&gt; 
    &lt;td&gt;63.6&lt;/td&gt; 
    &lt;td&gt;76.2&lt;/td&gt; 
    &lt;td&gt;69.9&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;79.8&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;CMath (EM)&lt;/td&gt; 
    &lt;td&gt;3-shot&lt;/td&gt; 
    &lt;td&gt;78.7&lt;/td&gt; 
    &lt;td&gt;84.5&lt;/td&gt; 
    &lt;td&gt;77.3&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;90.7&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Chinese&lt;/td&gt; 
    &lt;td&gt;CLUEWSC (EM)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;82.0&lt;/td&gt; 
    &lt;td&gt;82.5&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;83.0&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;82.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;C-Eval (Acc.)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;81.4&lt;/td&gt; 
    &lt;td&gt;89.2&lt;/td&gt; 
    &lt;td&gt;72.5&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;90.1&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;CMMLU (Acc.)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;84.0&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;89.5&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;73.7&lt;/td&gt; 
    &lt;td&gt;88.8&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;CMRC (EM)&lt;/td&gt; 
    &lt;td&gt;1-shot&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;77.4&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;75.8&lt;/td&gt; 
    &lt;td&gt;76.0&lt;/td&gt; 
    &lt;td&gt;76.3&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;C3 (Acc.)&lt;/td&gt; 
    &lt;td&gt;0-shot&lt;/td&gt; 
    &lt;td&gt;77.4&lt;/td&gt; 
    &lt;td&gt;76.7&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;79.7&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;78.6&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;CCPM (Acc.)&lt;/td&gt; 
    &lt;td&gt;0-shot&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;93.0&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;88.5&lt;/td&gt; 
    &lt;td&gt;78.6&lt;/td&gt; 
    &lt;td&gt;92.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Multilingual&lt;/td&gt; 
    &lt;td&gt;MMMLU-non-English (Acc.)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;64.0&lt;/td&gt; 
    &lt;td&gt;74.8&lt;/td&gt; 
    &lt;td&gt;73.8&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;79.4&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Best results are shown in bold. Scores with a gap not exceeding 0.3 are considered to be at the same level. DeepSeek-V3 achieves the best performance on most benchmarks, especially on math and code tasks. For more evaluation details, please check our paper.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Context Window&lt;/h4&gt; 
&lt;p align="center"&gt; &lt;img width="80%" src="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/figures/niah.png" /&gt; &lt;/p&gt; 
&lt;p&gt;Evaluation results on the &lt;code&gt;Needle In A Haystack&lt;/code&gt; (NIAH) tests. DeepSeek-V3 performs well across all context window lengths up to &lt;strong&gt;128K&lt;/strong&gt;.&lt;/p&gt; 
&lt;h3&gt;Chat Model&lt;/h3&gt; 
&lt;h4&gt;Standard Benchmarks (Models larger than 67B)&lt;/h4&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;Benchmark (Metric)&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;DeepSeek V2-0506&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;DeepSeek V2.5-0905&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;Qwen2.5 72B-Inst.&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;Llama3.1 405B-Inst.&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;Claude-3.5-Sonnet-1022&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;GPT-4o 0513&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;DeepSeek V3&lt;/strong&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;Architecture&lt;/td&gt; 
    &lt;td&gt;MoE&lt;/td&gt; 
    &lt;td&gt;MoE&lt;/td&gt; 
    &lt;td&gt;Dense&lt;/td&gt; 
    &lt;td&gt;Dense&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;MoE&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;# Activated Params&lt;/td&gt; 
    &lt;td&gt;21B&lt;/td&gt; 
    &lt;td&gt;21B&lt;/td&gt; 
    &lt;td&gt;72B&lt;/td&gt; 
    &lt;td&gt;405B&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;37B&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;# Total Params&lt;/td&gt; 
    &lt;td&gt;236B&lt;/td&gt; 
    &lt;td&gt;236B&lt;/td&gt; 
    &lt;td&gt;72B&lt;/td&gt; 
    &lt;td&gt;405B&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;671B&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;English&lt;/td&gt; 
    &lt;td&gt;MMLU (EM)&lt;/td&gt; 
    &lt;td&gt;78.2&lt;/td&gt; 
    &lt;td&gt;80.6&lt;/td&gt; 
    &lt;td&gt;85.3&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;88.6&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;88.3&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;87.2&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;88.5&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;MMLU-Redux (EM)&lt;/td&gt; 
    &lt;td&gt;77.9&lt;/td&gt; 
    &lt;td&gt;80.3&lt;/td&gt; 
    &lt;td&gt;85.6&lt;/td&gt; 
    &lt;td&gt;86.2&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;88.9&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;88.0&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;89.1&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;MMLU-Pro (EM)&lt;/td&gt; 
    &lt;td&gt;58.5&lt;/td&gt; 
    &lt;td&gt;66.2&lt;/td&gt; 
    &lt;td&gt;71.6&lt;/td&gt; 
    &lt;td&gt;73.3&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;78.0&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;72.6&lt;/td&gt; 
    &lt;td&gt;75.9&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;DROP (3-shot F1)&lt;/td&gt; 
    &lt;td&gt;83.0&lt;/td&gt; 
    &lt;td&gt;87.8&lt;/td&gt; 
    &lt;td&gt;76.7&lt;/td&gt; 
    &lt;td&gt;88.7&lt;/td&gt; 
    &lt;td&gt;88.3&lt;/td&gt; 
    &lt;td&gt;83.7&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;91.6&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;IF-Eval (Prompt Strict)&lt;/td&gt; 
    &lt;td&gt;57.7&lt;/td&gt; 
    &lt;td&gt;80.6&lt;/td&gt; 
    &lt;td&gt;84.1&lt;/td&gt; 
    &lt;td&gt;86.0&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;86.5&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;84.3&lt;/td&gt; 
    &lt;td&gt;86.1&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;GPQA-Diamond (Pass@1)&lt;/td&gt; 
    &lt;td&gt;35.3&lt;/td&gt; 
    &lt;td&gt;41.3&lt;/td&gt; 
    &lt;td&gt;49.0&lt;/td&gt; 
    &lt;td&gt;51.1&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;65.0&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;49.9&lt;/td&gt; 
    &lt;td&gt;59.1&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;SimpleQA (Correct)&lt;/td&gt; 
    &lt;td&gt;9.0&lt;/td&gt; 
    &lt;td&gt;10.2&lt;/td&gt; 
    &lt;td&gt;9.1&lt;/td&gt; 
    &lt;td&gt;17.1&lt;/td&gt; 
    &lt;td&gt;28.4&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;38.2&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;24.9&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;FRAMES (Acc.)&lt;/td&gt; 
    &lt;td&gt;66.9&lt;/td&gt; 
    &lt;td&gt;65.4&lt;/td&gt; 
    &lt;td&gt;69.8&lt;/td&gt; 
    &lt;td&gt;70.0&lt;/td&gt; 
    &lt;td&gt;72.5&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;80.5&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;73.3&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;LongBench v2 (Acc.)&lt;/td&gt; 
    &lt;td&gt;31.6&lt;/td&gt; 
    &lt;td&gt;35.4&lt;/td&gt; 
    &lt;td&gt;39.4&lt;/td&gt; 
    &lt;td&gt;36.1&lt;/td&gt; 
    &lt;td&gt;41.0&lt;/td&gt; 
    &lt;td&gt;48.1&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;48.7&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Code&lt;/td&gt; 
    &lt;td&gt;HumanEval-Mul (Pass@1)&lt;/td&gt; 
    &lt;td&gt;69.3&lt;/td&gt; 
    &lt;td&gt;77.4&lt;/td&gt; 
    &lt;td&gt;77.3&lt;/td&gt; 
    &lt;td&gt;77.2&lt;/td&gt; 
    &lt;td&gt;81.7&lt;/td&gt; 
    &lt;td&gt;80.5&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;82.6&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;LiveCodeBench (Pass@1-COT)&lt;/td&gt; 
    &lt;td&gt;18.8&lt;/td&gt; 
    &lt;td&gt;29.2&lt;/td&gt; 
    &lt;td&gt;31.1&lt;/td&gt; 
    &lt;td&gt;28.4&lt;/td&gt; 
    &lt;td&gt;36.3&lt;/td&gt; 
    &lt;td&gt;33.4&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;40.5&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;LiveCodeBench (Pass@1)&lt;/td&gt; 
    &lt;td&gt;20.3&lt;/td&gt; 
    &lt;td&gt;28.4&lt;/td&gt; 
    &lt;td&gt;28.7&lt;/td&gt; 
    &lt;td&gt;30.1&lt;/td&gt; 
    &lt;td&gt;32.8&lt;/td&gt; 
    &lt;td&gt;34.2&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;37.6&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;Codeforces (Percentile)&lt;/td&gt; 
    &lt;td&gt;17.5&lt;/td&gt; 
    &lt;td&gt;35.6&lt;/td&gt; 
    &lt;td&gt;24.8&lt;/td&gt; 
    &lt;td&gt;25.3&lt;/td&gt; 
    &lt;td&gt;20.3&lt;/td&gt; 
    &lt;td&gt;23.6&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;51.6&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;SWE Verified (Resolved)&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;22.6&lt;/td&gt; 
    &lt;td&gt;23.8&lt;/td&gt; 
    &lt;td&gt;24.5&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;50.8&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;38.8&lt;/td&gt; 
    &lt;td&gt;42.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;Aider-Edit (Acc.)&lt;/td&gt; 
    &lt;td&gt;60.3&lt;/td&gt; 
    &lt;td&gt;71.6&lt;/td&gt; 
    &lt;td&gt;65.4&lt;/td&gt; 
    &lt;td&gt;63.9&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;84.2&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;72.9&lt;/td&gt; 
    &lt;td&gt;79.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;Aider-Polyglot (Acc.)&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;18.2&lt;/td&gt; 
    &lt;td&gt;7.6&lt;/td&gt; 
    &lt;td&gt;5.8&lt;/td&gt; 
    &lt;td&gt;45.3&lt;/td&gt; 
    &lt;td&gt;16.0&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;49.6&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Math&lt;/td&gt; 
    &lt;td&gt;AIME 2024 (Pass@1)&lt;/td&gt; 
    &lt;td&gt;4.6&lt;/td&gt; 
    &lt;td&gt;16.7&lt;/td&gt; 
    &lt;td&gt;23.3&lt;/td&gt; 
    &lt;td&gt;23.3&lt;/td&gt; 
    &lt;td&gt;16.0&lt;/td&gt; 
    &lt;td&gt;9.3&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;39.2&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;MATH-500 (EM)&lt;/td&gt; 
    &lt;td&gt;56.3&lt;/td&gt; 
    &lt;td&gt;74.7&lt;/td&gt; 
    &lt;td&gt;80.0&lt;/td&gt; 
    &lt;td&gt;73.8&lt;/td&gt; 
    &lt;td&gt;78.3&lt;/td&gt; 
    &lt;td&gt;74.6&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;90.2&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;CNMO 2024 (Pass@1)&lt;/td&gt; 
    &lt;td&gt;2.8&lt;/td&gt; 
    &lt;td&gt;10.8&lt;/td&gt; 
    &lt;td&gt;15.9&lt;/td&gt; 
    &lt;td&gt;6.8&lt;/td&gt; 
    &lt;td&gt;13.1&lt;/td&gt; 
    &lt;td&gt;10.8&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;43.2&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Chinese&lt;/td&gt; 
    &lt;td&gt;CLUEWSC (EM)&lt;/td&gt; 
    &lt;td&gt;89.9&lt;/td&gt; 
    &lt;td&gt;90.4&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;91.4&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;84.7&lt;/td&gt; 
    &lt;td&gt;85.4&lt;/td&gt; 
    &lt;td&gt;87.9&lt;/td&gt; 
    &lt;td&gt;90.9&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;C-Eval (EM)&lt;/td&gt; 
    &lt;td&gt;78.6&lt;/td&gt; 
    &lt;td&gt;79.5&lt;/td&gt; 
    &lt;td&gt;86.1&lt;/td&gt; 
    &lt;td&gt;61.5&lt;/td&gt; 
    &lt;td&gt;76.7&lt;/td&gt; 
    &lt;td&gt;76.0&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;86.5&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;C-SimpleQA (Correct)&lt;/td&gt; 
    &lt;td&gt;48.5&lt;/td&gt; 
    &lt;td&gt;54.1&lt;/td&gt; 
    &lt;td&gt;48.4&lt;/td&gt; 
    &lt;td&gt;50.4&lt;/td&gt; 
    &lt;td&gt;51.3&lt;/td&gt; 
    &lt;td&gt;59.3&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;64.8&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] All models are evaluated in a configuration that limits the output length to 8K. Benchmarks containing fewer than 1000 samples are tested multiple times using varying temperature settings to derive robust final results. DeepSeek-V3 stands as the best-performing open-source model, and also exhibits competitive performance against frontier closed-source models.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Open Ended Generation Evaluation&lt;/h4&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;Arena-Hard&lt;/th&gt; 
    &lt;th&gt;AlpacaEval 2.0&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;DeepSeek-V2.5-0905&lt;/td&gt; 
    &lt;td&gt;76.2&lt;/td&gt; 
    &lt;td&gt;50.5&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Qwen2.5-72B-Instruct&lt;/td&gt; 
    &lt;td&gt;81.2&lt;/td&gt; 
    &lt;td&gt;49.1&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;LLaMA-3.1 405B&lt;/td&gt; 
    &lt;td&gt;69.3&lt;/td&gt; 
    &lt;td&gt;40.5&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;GPT-4o-0513&lt;/td&gt; 
    &lt;td&gt;80.4&lt;/td&gt; 
    &lt;td&gt;51.1&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Claude-Sonnet-3.5-1022&lt;/td&gt; 
    &lt;td&gt;85.2&lt;/td&gt; 
    &lt;td&gt;52.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;DeepSeek-V3&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;85.5&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;70.0&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] English open-ended conversation evaluations. For AlpacaEval 2.0, we use the length-controlled win rate as the metric.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;5. Chat Website &amp;amp; API Platform&lt;/h2&gt; 
&lt;p&gt;You can chat with DeepSeek-V3 on DeepSeek's official website: &lt;a href="https://chat.deepseek.com/sign_in"&gt;chat.deepseek.com&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;We also provide OpenAI-Compatible API at DeepSeek Platform: &lt;a href="https://platform.deepseek.com/"&gt;platform.deepseek.com&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;6. How to Run Locally&lt;/h2&gt; 
&lt;p&gt;DeepSeek-V3 can be deployed locally using the following hardware and open-source community software:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;DeepSeek-Infer Demo&lt;/strong&gt;: We provide a simple and lightweight demo for FP8 and BF16 inference.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;SGLang&lt;/strong&gt;: Fully support the DeepSeek-V3 model in both BF16 and FP8 inference modes, with Multi-Token Prediction &lt;a href="https://github.com/sgl-project/sglang/issues/2591"&gt;coming soon&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;LMDeploy&lt;/strong&gt;: Enables efficient FP8 and BF16 inference for local and cloud deployment.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;TensorRT-LLM&lt;/strong&gt;: Currently supports BF16 inference and INT4/8 quantization, with FP8 support coming soon.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;vLLM&lt;/strong&gt;: Support DeepSeek-V3 model with FP8 and BF16 modes for tensor parallelism and pipeline parallelism.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;LightLLM&lt;/strong&gt;: Supports efficient single-node or multi-node deployment for FP8 and BF16.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;AMD GPU&lt;/strong&gt;: Enables running the DeepSeek-V3 model on AMD GPUs via SGLang in both BF16 and FP8 modes.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Huawei Ascend NPU&lt;/strong&gt;: Supports running DeepSeek-V3 on Huawei Ascend devices in both INT8 and BF16.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Since FP8 training is natively adopted in our framework, we only provide FP8 weights. If you require BF16 weights for experimentation, you can use the provided conversion script to perform the transformation.&lt;/p&gt; 
&lt;p&gt;Here is an example of converting FP8 weights to BF16:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;cd inference
python fp8_cast_bf16.py --input-fp8-hf-path /path/to/fp8_weights --output-bf16-hf-path /path/to/bf16_weights
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Hugging Face's Transformers has not been directly supported yet.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;6.1 Inference with DeepSeek-Infer Demo (example only)&lt;/h3&gt; 
&lt;h4&gt;System Requirements&lt;/h4&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Linux with Python 3.10 only. Mac and Windows are not supported.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Dependencies:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-pip-requirements"&gt;torch==2.4.1
triton==3.0.0
transformers==4.46.3
safetensors==0.4.5
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Model Weights &amp;amp; Demo Code Preparation&lt;/h4&gt; 
&lt;p&gt;First, clone our DeepSeek-V3 GitHub repository:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;git clone https://github.com/deepseek-ai/DeepSeek-V3.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Navigate to the &lt;code&gt;inference&lt;/code&gt; folder and install dependencies listed in &lt;code&gt;requirements.txt&lt;/code&gt;. Easiest way is to use a package manager like &lt;code&gt;conda&lt;/code&gt; or &lt;code&gt;uv&lt;/code&gt; to create a new virtual environment and install the dependencies.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;cd DeepSeek-V3/inference
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Download the model weights from Hugging Face, and put them into &lt;code&gt;/path/to/DeepSeek-V3&lt;/code&gt; folder.&lt;/p&gt; 
&lt;h4&gt;Model Weights Conversion&lt;/h4&gt; 
&lt;p&gt;Convert Hugging Face model weights to a specific format:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;python convert.py --hf-ckpt-path /path/to/DeepSeek-V3 --save-path /path/to/DeepSeek-V3-Demo --n-experts 256 --model-parallel 16
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Run&lt;/h4&gt; 
&lt;p&gt;Then you can chat with DeepSeek-V3:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;torchrun --nnodes 2 --nproc-per-node 8 --node-rank $RANK --master-addr $ADDR generate.py --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --interactive --temperature 0.7 --max-new-tokens 200
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or batch inference on a given file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;torchrun --nnodes 2 --nproc-per-node 8 --node-rank $RANK --master-addr $ADDR generate.py --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --input-file $FILE
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;6.2 Inference with SGLang (recommended)&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/sgl-project/sglang"&gt;SGLang&lt;/a&gt; currently supports &lt;a href="https://lmsys.org/blog/2024-09-04-sglang-v0-3/#deepseek-multi-head-latent-attention-mla-throughput-optimizations"&gt;MLA optimizations&lt;/a&gt;, &lt;a href="https://lmsys.org/blog/2024-12-04-sglang-v0-4/#data-parallelism-attention-for-deepseek-models"&gt;DP Attention&lt;/a&gt;, FP8 (W8A8), FP8 KV Cache, and Torch Compile, delivering state-of-the-art latency and throughput performance among open-source frameworks.&lt;/p&gt; 
&lt;p&gt;Notably, &lt;a href="https://github.com/sgl-project/sglang/releases/tag/v0.4.1"&gt;SGLang v0.4.1&lt;/a&gt; fully supports running DeepSeek-V3 on both &lt;strong&gt;NVIDIA and AMD GPUs&lt;/strong&gt;, making it a highly versatile and robust solution.&lt;/p&gt; 
&lt;p&gt;SGLang also supports &lt;a href="https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3#example-serving-with-2-h208"&gt;multi-node tensor parallelism&lt;/a&gt;, enabling you to run this model on multiple network-connected machines.&lt;/p&gt; 
&lt;p&gt;Multi-Token Prediction (MTP) is in development, and progress can be tracked in the &lt;a href="https://github.com/sgl-project/sglang/issues/2591"&gt;optimization plan&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Here are the launch instructions from the SGLang team: &lt;a href="https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3"&gt;https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;6.3 Inference with LMDeploy (recommended)&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/InternLM/lmdeploy"&gt;LMDeploy&lt;/a&gt;, a flexible and high-performance inference and serving framework tailored for large language models, now supports DeepSeek-V3. It offers both offline pipeline processing and online deployment capabilities, seamlessly integrating with PyTorch-based workflows.&lt;/p&gt; 
&lt;p&gt;For comprehensive step-by-step instructions on running DeepSeek-V3 with LMDeploy, please refer to here: &lt;a href="https://github.com/InternLM/lmdeploy/issues/2960"&gt;https://github.com/InternLM/lmdeploy/issues/2960&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;6.4 Inference with TRT-LLM (recommended)&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/NVIDIA/TensorRT-LLM"&gt;TensorRT-LLM&lt;/a&gt; now supports the DeepSeek-V3 model, offering precision options such as BF16 and INT4/INT8 weight-only. Support for FP8 is currently in progress and will be released soon. You can access the custom branch of TRTLLM specifically for DeepSeek-V3 support through the following link to experience the new features directly: &lt;a href="https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/deepseek_v3"&gt;https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/deepseek_v3&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;6.5 Inference with vLLM (recommended)&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/vllm-project/vllm"&gt;vLLM&lt;/a&gt; v0.6.6 supports DeepSeek-V3 inference for FP8 and BF16 modes on both NVIDIA and AMD GPUs. Aside from standard techniques, vLLM offers &lt;em&gt;pipeline parallelism&lt;/em&gt; allowing you to run this model on multiple machines connected by networks. For detailed guidance, please refer to the &lt;a href="https://docs.vllm.ai/en/latest/serving/distributed_serving.html"&gt;vLLM instructions&lt;/a&gt;. Please feel free to follow &lt;a href="https://github.com/vllm-project/vllm/issues/11539"&gt;the enhancement plan&lt;/a&gt; as well.&lt;/p&gt; 
&lt;h3&gt;6.6 Inference with LightLLM (recommended)&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/ModelTC/lightllm/tree/main"&gt;LightLLM&lt;/a&gt; v1.0.1 supports single-machine and multi-machine tensor parallel deployment for DeepSeek-R1 (FP8/BF16) and provides mixed-precision deployment, with more quantization modes continuously integrated. For more details, please refer to &lt;a href="https://lightllm-en.readthedocs.io/en/latest/getting_started/quickstart.html"&gt;LightLLM instructions&lt;/a&gt;. Additionally, LightLLM offers PD-disaggregation deployment for DeepSeek-V2, and the implementation of PD-disaggregation for DeepSeek-V3 is in development.&lt;/p&gt; 
&lt;h3&gt;6.7 Recommended Inference Functionality with AMD GPUs&lt;/h3&gt; 
&lt;p&gt;In collaboration with the AMD team, we have achieved Day-One support for AMD GPUs using SGLang, with full compatibility for both FP8 and BF16 precision. For detailed guidance, please refer to the &lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/#63-inference-with-lmdeploy-recommended"&gt;SGLang instructions&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;6.8 Recommended Inference Functionality with Huawei Ascend NPUs&lt;/h3&gt; 
&lt;p&gt;The &lt;a href="https://www.hiascend.com/en/software/mindie"&gt;MindIE&lt;/a&gt; framework from the Huawei Ascend community has successfully adapted the BF16 version of DeepSeek-V3. For step-by-step guidance on Ascend NPUs, please follow the &lt;a href="https://modelers.cn/models/MindIE/deepseekv3"&gt;instructions here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;7. License&lt;/h2&gt; 
&lt;p&gt;This code repository is licensed under &lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/LICENSE-CODE"&gt;the MIT License&lt;/a&gt;. The use of DeepSeek-V3 Base/Chat models is subject to &lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/LICENSE-MODEL"&gt;the Model License&lt;/a&gt;. DeepSeek-V3 series (including Base and Chat) supports commercial use.&lt;/p&gt; 
&lt;h2&gt;8. Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;@misc{deepseekai2024deepseekv3technicalreport,
      title={DeepSeek-V3 Technical Report}, 
      author={DeepSeek-AI},
      year={2024},
      eprint={2412.19437},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.19437}, 
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;9. Contact&lt;/h2&gt; 
&lt;p&gt;If you have any questions, please raise an issue or contact us at &lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/service@deepseek.com"&gt;service@deepseek.com&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>dataease/SQLBot</title>
      <link>https://github.com/dataease/SQLBot</link>
      <description>&lt;p&gt;Âü∫‰∫éÂ§ßÊ®°ÂûãÂíå RAG ÁöÑÊô∫ËÉΩÈóÆÊï∞Á≥ªÁªü&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt;&lt;img src="https://resource-fit2cloud-com.oss-cn-hangzhou.aliyuncs.com/sqlbot/sqlbot.png" alt="SQLBot" width="300" /&gt;&lt;/p&gt; 
&lt;h3 align="center"&gt;Âü∫‰∫éÂ§ßÊ®°ÂûãÂíå RAG ÁöÑÊô∫ËÉΩÈóÆÊï∞Á≥ªÁªü&lt;/h3&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/dataease/SQLBot/releases/latest"&gt;&lt;img src="https://img.shields.io/github/v/release/dataease/SQLBot" alt="Latest release" /&gt;&lt;/a&gt; &lt;a href="https://github.com/dataease/SQLBot"&gt;&lt;img src="https://img.shields.io/github/stars/dataease/SQLBot?color=%231890FF&amp;amp;style=flat-square" alt="Stars" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/dataease/SQLbot"&gt;&lt;img src="https://img.shields.io/docker/pulls/dataease/sqlbot?label=downloads" alt="Download" /&gt;&lt;/a&gt;&lt;br /&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;SQLBot ÊòØ‰∏ÄÊ¨æÂü∫‰∫éÂ§ßÊ®°ÂûãÂíå RAG ÁöÑÊô∫ËÉΩÈóÆÊï∞Á≥ªÁªü„ÄÇSQLBot ÁöÑ‰ºòÂäøÂåÖÊã¨Ôºö&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ÂºÄÁÆ±Âç≥Áî®&lt;/strong&gt;: Âè™ÈúÄÈÖçÁΩÆÂ§ßÊ®°ÂûãÂíåÊï∞ÊçÆÊ∫êÂç≥ÂèØÂºÄÂêØÈóÆÊï∞‰πãÊóÖÔºåÈÄöËøáÂ§ßÊ®°ÂûãÂíå RAG ÁöÑÁªìÂêàÊù•ÂÆûÁé∞È´òË¥®ÈáèÁöÑ text2sqlÔºõ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Êòì‰∫éÈõÜÊàê&lt;/strong&gt;: ÊîØÊåÅÂø´ÈÄüÂµåÂÖ•Âà∞Á¨¨‰∏âÊñπ‰∏öÂä°Á≥ªÁªüÔºå‰πüÊîØÊåÅË¢´ n8n„ÄÅMaxKB„ÄÅDify„ÄÅCoze Á≠â AI Â∫îÁî®ÂºÄÂèëÂπ≥Âè∞ÈõÜÊàêË∞ÉÁî®ÔºåËÆ©ÂêÑÁ±ªÂ∫îÁî®Âø´ÈÄüÊã•ÊúâÊô∫ËÉΩÈóÆÊï∞ËÉΩÂäõÔºõ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ÂÆâÂÖ®ÂèØÊéß&lt;/strong&gt;: Êèê‰æõÂü∫‰∫éÂ∑•‰ΩúÁ©∫Èó¥ÁöÑËµÑÊ∫êÈöîÁ¶ªÊú∫Âà∂ÔºåËÉΩÂ§üÂÆûÁé∞ÁªÜÁ≤íÂ∫¶ÁöÑÊï∞ÊçÆÊùÉÈôêÊéßÂà∂„ÄÇ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Âø´ÈÄüÂºÄÂßã&lt;/h2&gt; 
&lt;h3&gt;ÂÆâË£ÖÈÉ®ÁΩ≤&lt;/h3&gt; 
&lt;p&gt;ÂáÜÂ§á‰∏ÄÂè∞ Linux ÊúçÂä°Âô®ÔºåÊâßË°å‰ª•‰∏ã‰∏ÄÈîÆÂÆâË£ÖËÑöÊú¨„ÄÇ&lt;br /&gt; Âú®ËøêË°å SQLBot ÂâçÔºåËØ∑Á°Æ‰øùÂ∑≤ÂÆâË£ÖÂ•Ω &lt;a href="https://docs.docker.com/get-docker/"&gt;Docker&lt;/a&gt; Âíå &lt;a href="https://docs.docker.com/compose/install/"&gt;Docker Compose&lt;/a&gt;„ÄÇ&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# ÂàõÂª∫ÁõÆÂΩï
mkdir -p /opt/sqlbot
cd /opt/sqlbot

# ‰∏ãËΩΩ docker-compose.yaml
curl -o docker-compose.yaml https://raw.githubusercontent.com/dataease/SQLBot/main/docker-compose.yaml

# ÂêØÂä®ÊúçÂä°
docker compose up -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;‰Ω†‰πüÂèØ‰ª•ÈÄöËøá &lt;a href="https://apps.fit2cloud.com/1panel"&gt;1Panel Â∫îÁî®ÂïÜÂ∫ó&lt;/a&gt; Âø´ÈÄüÈÉ®ÁΩ≤ SQLBotÔºõ&lt;/p&gt; 
&lt;h3&gt;ËÆøÈóÆÊñπÂºè&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Âú®ÊµèËßàÂô®‰∏≠ÊâìÂºÄ: http://&amp;lt;‰Ω†ÁöÑÊúçÂä°Âô®IP&amp;gt;:8000/&lt;/li&gt; 
 &lt;li&gt;Áî®Êà∑Âêç: admin&lt;/li&gt; 
 &lt;li&gt;ÂØÜÁ†Å: SQLBot@123456&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ËÅîÁ≥ªÊàë‰ª¨&lt;/h3&gt; 
&lt;p&gt;Â¶Ç‰Ω†ÊúâÊõ¥Â§öÈóÆÈ¢òÔºåÂèØ‰ª•Âä†ÂÖ•Êàë‰ª¨ÁöÑÊäÄÊúØ‰∫§ÊµÅÁæ§‰∏éÊàë‰ª¨‰∫§ÊµÅ„ÄÇ&lt;/p&gt; 
&lt;img width="396" height="396" alt="contact_me_qr" src="https://github.com/user-attachments/assets/2594ff29-5426-4457-b051-279855610030" /&gt; 
&lt;h2&gt;UI Â±ïÁ§∫&lt;/h2&gt;  
&lt;img alt="q&amp;amp;a" src="https://github.com/user-attachments/assets/55526514-52f3-4cfe-98ec-08a986259280" /&gt;  
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#dataease/sqlbot&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=dataease/sqlbot&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;È£ûËá¥‰∫ëÊóó‰∏ãÁöÑÂÖ∂‰ªñÊòéÊòüÈ°πÁõÆ&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/dataease/dataease/"&gt;DataEase&lt;/a&gt; - ‰∫∫‰∫∫ÂèØÁî®ÁöÑÂºÄÊ∫ê BI Â∑•ÂÖ∑&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/1panel-dev/1panel/"&gt;1Panel&lt;/a&gt; - Áé∞‰ª£Âåñ„ÄÅÂºÄÊ∫êÁöÑ Linux ÊúçÂä°Âô®ËøêÁª¥ÁÆ°ÁêÜÈù¢Êùø&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/1panel-dev/MaxKB/"&gt;MaxKB&lt;/a&gt; - Âº∫Â§ßÊòìÁî®ÁöÑ‰ºÅ‰∏öÁ∫ßÊô∫ËÉΩ‰ΩìÂπ≥Âè∞&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/jumpserver/jumpserver/"&gt;JumpServer&lt;/a&gt; - ÂπøÂèóÊ¨¢ËøéÁöÑÂºÄÊ∫êÂ†°ÂûíÊú∫&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/halo-dev/halo/"&gt;Halo&lt;/a&gt; - Âº∫Â§ßÊòìÁî®ÁöÑÂºÄÊ∫êÂª∫Á´ôÂ∑•ÂÖ∑&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/metersphere/metersphere/"&gt;MeterSphere&lt;/a&gt; - Êñ∞‰∏Ä‰ª£ÁöÑÂºÄÊ∫êÊåÅÁª≠ÊµãËØïÂ∑•ÂÖ∑&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Êú¨‰ªìÂ∫ìÈÅµÂæ™ &lt;a href="https://raw.githubusercontent.com/dataease/SQLBot/main/LICENSE"&gt;FIT2CLOUD Open Source License&lt;/a&gt; ÂºÄÊ∫êÂçèËÆÆÔºåËØ•ËÆ∏ÂèØËØÅÊú¨Ë¥®‰∏äÊòØ GPLv3Ôºå‰ΩÜÊúâ‰∏Ä‰∫õÈ¢ùÂ§ñÁöÑÈôêÂà∂„ÄÇ&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>microsoft/BitNet</title>
      <link>https://github.com/microsoft/BitNet</link>
      <description>&lt;p&gt;Official inference framework for 1-bit LLMs&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;bitnet.cpp&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://opensource.org/licenses/MIT"&gt;&lt;img src="https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true" alt="License: MIT" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/badge/version-1.0-blue" alt="version" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/microsoft/BitNet-b1.58-2B-4T"&gt;&lt;img src="https://raw.githubusercontent.com/microsoft/BitNet/main/assets/header_model_release.png" alt="BitNet Model on Hugging Face" width="800" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Try it out via this &lt;a href="https://bitnet-demo.azurewebsites.net/"&gt;demo&lt;/a&gt;, or build and run it on your own &lt;a href="https://github.com/microsoft/BitNet?tab=readme-ov-file#build-from-source"&gt;CPU&lt;/a&gt; or &lt;a href="https://github.com/microsoft/BitNet/raw/main/gpu/README.md"&gt;GPU&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;bitnet.cpp is the official inference framework for 1-bit LLMs (e.g., BitNet b1.58). It offers a suite of optimized kernels, that support &lt;strong&gt;fast&lt;/strong&gt; and &lt;strong&gt;lossless&lt;/strong&gt; inference of 1.58-bit models on CPU and GPU (NPU support will coming next).&lt;/p&gt; 
&lt;p&gt;The first release of bitnet.cpp is to support inference on CPUs. bitnet.cpp achieves speedups of &lt;strong&gt;1.37x&lt;/strong&gt; to &lt;strong&gt;5.07x&lt;/strong&gt; on ARM CPUs, with larger models experiencing greater performance gains. Additionally, it reduces energy consumption by &lt;strong&gt;55.4%&lt;/strong&gt; to &lt;strong&gt;70.0%&lt;/strong&gt;, further boosting overall efficiency. On x86 CPUs, speedups range from &lt;strong&gt;2.37x&lt;/strong&gt; to &lt;strong&gt;6.17x&lt;/strong&gt; with energy reductions between &lt;strong&gt;71.9%&lt;/strong&gt; to &lt;strong&gt;82.2%&lt;/strong&gt;. Furthermore, bitnet.cpp can run a 100B BitNet b1.58 model on a single CPU, achieving speeds comparable to human reading (5-7 tokens per second), significantly enhancing the potential for running LLMs on local devices. Please refer to the &lt;a href="https://arxiv.org/abs/2410.16144"&gt;technical report&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/microsoft/BitNet/main/assets/m2_performance.jpg" alt="m2_performance" width="800" /&gt; 
&lt;img src="https://raw.githubusercontent.com/microsoft/BitNet/main/assets/intel_performance.jpg" alt="m2_performance" width="800" /&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;The tested models are dummy setups used in a research context to demonstrate the inference performance of bitnet.cpp.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Demo&lt;/h2&gt; 
&lt;p&gt;A demo of bitnet.cpp running a BitNet b1.58 3B model on Apple M2:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/7f46b736-edec-4828-b809-4be780a3e5b1"&gt;https://github.com/user-attachments/assets/7f46b736-edec-4828-b809-4be780a3e5b1&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;What's New:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;05/20/2025 &lt;a href="https://github.com/microsoft/BitNet/raw/main/gpu/README.md"&gt;BitNet Official GPU inference kernel&lt;/a&gt; &lt;img src="https://img.shields.io/badge/NEW-red" alt="NEW" /&gt;&lt;/li&gt; 
 &lt;li&gt;04/14/2025 &lt;a href="https://huggingface.co/microsoft/BitNet-b1.58-2B-4T"&gt;BitNet Official 2B Parameter Model on Hugging Face&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;02/18/2025 &lt;a href="https://arxiv.org/abs/2502.11880"&gt;Bitnet.cpp: Efficient Edge Inference for Ternary LLMs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;11/08/2024 &lt;a href="https://arxiv.org/abs/2411.04965"&gt;BitNet a4.8: 4-bit Activations for 1-bit LLMs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;10/21/2024 &lt;a href="https://arxiv.org/abs/2410.16144"&gt;1-bit AI Infra: Part 1.1, Fast and Lossless BitNet b1.58 Inference on CPUs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;10/17/2024 bitnet.cpp 1.0 released.&lt;/li&gt; 
 &lt;li&gt;03/21/2024 &lt;a href="https://github.com/microsoft/unilm/raw/master/bitnet/The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ.pdf"&gt;The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;02/27/2024 &lt;a href="https://arxiv.org/abs/2402.17764"&gt;The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;10/17/2023 &lt;a href="https://arxiv.org/abs/2310.11453"&gt;BitNet: Scaling 1-bit Transformers for Large Language Models&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;p&gt;This project is based on the &lt;a href="https://github.com/ggerganov/llama.cpp"&gt;llama.cpp&lt;/a&gt; framework. We would like to thank all the authors for their contributions to the open-source community. Also, bitnet.cpp's kernels are built on top of the Lookup Table methodologies pioneered in &lt;a href="https://github.com/microsoft/T-MAC/"&gt;T-MAC&lt;/a&gt;. For inference of general low-bit LLMs beyond ternary models, we recommend using T-MAC.&lt;/p&gt; 
&lt;h2&gt;Official Models&lt;/h2&gt; 
&lt;table&gt;  
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;th rowspan="2"&gt;Model&lt;/th&gt; 
   &lt;th rowspan="2"&gt;Parameters&lt;/th&gt; 
   &lt;th rowspan="2"&gt;CPU&lt;/th&gt; 
   &lt;th colspan="3"&gt;Kernel&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th&gt;I2_S&lt;/th&gt; 
   &lt;th&gt;TL1&lt;/th&gt; 
   &lt;th&gt;TL2&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/microsoft/BitNet-b1.58-2B-4T"&gt;BitNet-b1.58-2B-4T&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;2.4B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Supported Models&lt;/h2&gt; 
&lt;p&gt;‚ùóÔ∏è&lt;strong&gt;We use existing 1-bit LLMs available on &lt;a href="https://huggingface.co/"&gt;Hugging Face&lt;/a&gt; to demonstrate the inference capabilities of bitnet.cpp. We hope the release of bitnet.cpp will inspire the development of 1-bit LLMs in large-scale settings in terms of model size and training tokens.&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt;  
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;th rowspan="2"&gt;Model&lt;/th&gt; 
   &lt;th rowspan="2"&gt;Parameters&lt;/th&gt; 
   &lt;th rowspan="2"&gt;CPU&lt;/th&gt; 
   &lt;th colspan="3"&gt;Kernel&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th&gt;I2_S&lt;/th&gt; 
   &lt;th&gt;TL1&lt;/th&gt; 
   &lt;th&gt;TL2&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/1bitLLM/bitnet_b1_58-large"&gt;bitnet_b1_58-large&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;0.7B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/1bitLLM/bitnet_b1_58-3B"&gt;bitnet_b1_58-3B&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;3.3B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/HF1BitLLM/Llama3-8B-1.58-100B-tokens"&gt;Llama3-8B-1.58-100B-tokens&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;8.0B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/collections/tiiuae/falcon3-67605ae03578be86e4e87026"&gt;Falcon3 Family&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;1B-10B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/collections/tiiuae/falcon-edge-series-6804fd13344d6d8a8fa71130"&gt;Falcon-E Family&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;1B-3B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;python&amp;gt;=3.9&lt;/li&gt; 
 &lt;li&gt;cmake&amp;gt;=3.22&lt;/li&gt; 
 &lt;li&gt;clang&amp;gt;=18 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;For Windows users, install &lt;a href="https://visualstudio.microsoft.com/downloads/"&gt;Visual Studio 2022&lt;/a&gt;. In the installer, toggle on at least the following options(this also automatically installs the required additional tools like CMake):&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Desktop-development with C++&lt;/li&gt; 
     &lt;li&gt;C++-CMake Tools for Windows&lt;/li&gt; 
     &lt;li&gt;Git for Windows&lt;/li&gt; 
     &lt;li&gt;C++-Clang Compiler for Windows&lt;/li&gt; 
     &lt;li&gt;MS-Build Support for LLVM-Toolset (clang)&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;For Debian/Ubuntu users, you can download with &lt;a href="https://apt.llvm.org/"&gt;Automatic installation script&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash -c "$(wget -O - https://apt.llvm.org/llvm.sh)"&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;conda (highly recommend)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Build from source&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] If you are using Windows, please remember to always use a Developer Command Prompt / PowerShell for VS2022 for the following commands. Please refer to the FAQs below if you see any issues.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ol&gt; 
 &lt;li&gt;Clone the repo&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone --recursive https://github.com/microsoft/BitNet.git
cd BitNet
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Install the dependencies&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# (Recommended) Create a new conda environment
conda create -n bitnet-cpp python=3.9
conda activate bitnet-cpp

pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Build the project&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Manually download the model and run with local path
huggingface-cli download microsoft/BitNet-b1.58-2B-4T-gguf --local-dir models/BitNet-b1.58-2B-4T
python setup_env.py -md models/BitNet-b1.58-2B-4T -q i2_s

&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;
usage: setup_env.py [-h] [--hf-repo {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}] [--model-dir MODEL_DIR] [--log-dir LOG_DIR] [--quant-type {i2_s,tl1}] [--quant-embd]
                    [--use-pretuned]

Setup the environment for running inference

optional arguments:
  -h, --help            show this help message and exit
  --hf-repo {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}, -hr {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}
                        Model used for inference
  --model-dir MODEL_DIR, -md MODEL_DIR
                        Directory to save/load the model
  --log-dir LOG_DIR, -ld LOG_DIR
                        Directory to save the logging info
  --quant-type {i2_s,tl1}, -q {i2_s,tl1}
                        Quantization type
  --quant-embd          Quantize the embeddings to f16
  --use-pretuned, -p    Use the pretuned kernel parameters
&lt;/pre&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Basic usage&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run inference with the quantized model
python run_inference.py -m models/BitNet-b1.58-2B-4T/ggml-model-i2_s.gguf -p "You are a helpful assistant" -cnv
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;
usage: run_inference.py [-h] [-m MODEL] [-n N_PREDICT] -p PROMPT [-t THREADS] [-c CTX_SIZE] [-temp TEMPERATURE] [-cnv]

Run inference

optional arguments:
  -h, --help            show this help message and exit
  -m MODEL, --model MODEL
                        Path to model file
  -n N_PREDICT, --n-predict N_PREDICT
                        Number of tokens to predict when generating text
  -p PROMPT, --prompt PROMPT
                        Prompt to generate text from
  -t THREADS, --threads THREADS
                        Number of threads to use
  -c CTX_SIZE, --ctx-size CTX_SIZE
                        Size of the prompt context
  -temp TEMPERATURE, --temperature TEMPERATURE
                        Temperature, a hyperparameter that controls the randomness of the generated text
  -cnv, --conversation  Whether to enable chat mode or not (for instruct models.)
                        (When this option is turned on, the prompt specified by -p will be used as the system prompt.)
&lt;/pre&gt; 
&lt;h3&gt;Benchmark&lt;/h3&gt; 
&lt;p&gt;We provide scripts to run the inference benchmark providing a model.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;usage: e2e_benchmark.py -m MODEL [-n N_TOKEN] [-p N_PROMPT] [-t THREADS]  
   
Setup the environment for running the inference  
   
required arguments:  
  -m MODEL, --model MODEL  
                        Path to the model file. 
   
optional arguments:  
  -h, --help  
                        Show this help message and exit. 
  -n N_TOKEN, --n-token N_TOKEN  
                        Number of generated tokens. 
  -p N_PROMPT, --n-prompt N_PROMPT  
                        Prompt to generate text from. 
  -t THREADS, --threads THREADS  
                        Number of threads to use. 
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Here's a brief explanation of each argument:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;-m&lt;/code&gt;, &lt;code&gt;--model&lt;/code&gt;: The path to the model file. This is a required argument that must be provided when running the script.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-n&lt;/code&gt;, &lt;code&gt;--n-token&lt;/code&gt;: The number of tokens to generate during the inference. It is an optional argument with a default value of 128.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-p&lt;/code&gt;, &lt;code&gt;--n-prompt&lt;/code&gt;: The number of prompt tokens to use for generating text. This is an optional argument with a default value of 512.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-t&lt;/code&gt;, &lt;code&gt;--threads&lt;/code&gt;: The number of threads to use for running the inference. It is an optional argument with a default value of 2.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-h&lt;/code&gt;, &lt;code&gt;--help&lt;/code&gt;: Show the help message and exit. Use this argument to display usage information.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python utils/e2e_benchmark.py -m /path/to/model -n 200 -p 256 -t 4  
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command would run the inference benchmark using the model located at &lt;code&gt;/path/to/model&lt;/code&gt;, generating 200 tokens from a 256 token prompt, utilizing 4 threads.&lt;/p&gt; 
&lt;p&gt;For the model layout that do not supported by any public model, we provide scripts to generate a dummy model with the given model layout, and run the benchmark on your machine:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python utils/generate-dummy-bitnet-model.py models/bitnet_b1_58-large --outfile models/dummy-bitnet-125m.tl1.gguf --outtype tl1 --model-size 125M

# Run benchmark with the generated model, use -m to specify the model path, -p to specify the prompt processed, -n to specify the number of token to generate
python utils/e2e_benchmark.py -m models/dummy-bitnet-125m.tl1.gguf -p 512 -n 128
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Convert from &lt;code&gt;.safetensors&lt;/code&gt; Checkpoints&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Prepare the .safetensors model file
huggingface-cli download microsoft/bitnet-b1.58-2B-4T-bf16 --local-dir ./models/bitnet-b1.58-2B-4T-bf16

# Convert to gguf model
python ./utils/convert-helper-bitnet.py ./models/bitnet-b1.58-2B-4T-bf16
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;FAQ (Frequently Asked Questions)üìå&lt;/h3&gt; 
&lt;h4&gt;Q1: The build dies with errors building llama.cpp due to issues with std::chrono in log.cpp?&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; This is an issue introduced in recent version of llama.cpp. Please refer to this &lt;a href="https://github.com/tinglou/llama.cpp/commit/4e3db1e3d78cc1bcd22bcb3af54bd2a4628dd323"&gt;commit&lt;/a&gt; in the &lt;a href="https://github.com/abetlen/llama-cpp-python/issues/1942"&gt;discussion&lt;/a&gt; to fix this issue.&lt;/p&gt; 
&lt;h4&gt;Q2: How to build with clang in conda environment on windows?&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Before building the project, verify your clang installation and access to Visual Studio tools by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;clang -v
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command checks that you are using the correct version of clang and that the Visual Studio tools are available. If you see an error message such as:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;'clang' is not recognized as an internal or external command, operable program or batch file.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It indicates that your command line window is not properly initialized for Visual Studio tools.&lt;/p&gt; 
&lt;p&gt;‚Ä¢ If you are using Command Prompt, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;"C:\Program Files\Microsoft Visual Studio\2022\Professional\Common7\Tools\VsDevCmd.bat" -startdir=none -arch=x64 -host_arch=x64
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;‚Ä¢ If you are using Windows PowerShell, run the following commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Import-Module "C:\Program Files\Microsoft Visual Studio\2022\Professional\Common7\Tools\Microsoft.VisualStudio.DevShell.dll" Enter-VsDevShell 3f0e31ad -SkipAutomaticLocation -DevCmdArguments "-arch=x64 -host_arch=x64"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;These steps will initialize your environment and allow you to use the correct Visual Studio tools.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>laude-institute/terminal-bench</title>
      <link>https://github.com/laude-institute/terminal-bench</link>
      <description>&lt;p&gt;A benchmark for LLMs on complicated tasks in the terminal&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;terminal-bench&lt;/h1&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;#####################################################################
#  _____                   _             _     ______________       #
# |_   _|__ _ __ _ __ ___ (_)_ __   __ _| |   ||            ||      #
#   | |/ _ \ '__| '_ ` _ \| | '_ \ / _` | |   || &amp;gt;          ||      #
#   | |  __/ |  | | | | | | | | | | (_| | |   ||            ||      #
#   |_|\___|_|  |_| |_| |_|_|_| |_|\__,_|_|   ||____________||      #
#   ____                  _                   |______________|      #
#  | __ )  ___ _ __   ___| |__                 \\############\\     #
#  |  _ \ / _ \ '_ \ / __| '_ \                 \\############\\    # 
#  | |_) |  __/ | | | (__| | | |                 \      ____    \   #
#  |____/ \___|_| |_|\___|_| |_|                  \_____\___\____\  #
#                                                                   #
#####################################################################
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://discord.gg/6xWPKhGDbA"&gt;&lt;img src="https://img.shields.io/badge/Join_our_discord-5865F2?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://github.com/laude-institute/terminal-bench"&gt;&lt;img src="https://img.shields.io/badge/T--Bench-000000?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=000&amp;amp;logoColor=white" alt="Github" /&gt;&lt;/a&gt; &lt;a href="https://www.tbench.ai/docs"&gt;&lt;img src="https://img.shields.io/badge/Docs-000000?style=for-the-badge&amp;amp;logo=mdbook&amp;amp;color=105864" alt="Docs" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Terminal-Bench is the benchmark for testing AI agents in real terminal environments. From compiling code to training models and setting up servers, Terminal-Bench evaluates how well agents can handle real-world, end-to-end tasks - autonomously.&lt;/p&gt; 
&lt;p&gt;Whether you're building LLM agents, benchmarking frameworks, or stress-testing system-level reasoning, Terminal-Bench gives you a reproducible task suite and execution harness designed for practical, real-world evaluation.&lt;/p&gt; 
&lt;p&gt;Terminal-Bench consists of two parts: a &lt;strong&gt;dataset of tasks&lt;/strong&gt;, and an &lt;strong&gt;execution harness&lt;/strong&gt; that connects a language model to our terminal sandbox.&lt;/p&gt; 
&lt;p&gt;Terminal-Bench is currently in &lt;strong&gt;beta&lt;/strong&gt; with ~100 tasks. Over the coming months, we are going to expand Terminal-Bench into comprehensive testbed for AI agents in text-based environments. Any contributions are welcome, especially new and challenging tasks!&lt;/p&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;p&gt;Our &lt;a href="https://www.tbench.ai/docs/installation"&gt;Quickstart Guide&lt;/a&gt; will walk you through installing the repo and contributing.&lt;/p&gt; 
&lt;p&gt;Terminal-Bench is distributed as a pip package and can be run using the Terminal-Bench CLI: &lt;code&gt;tb&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv tool install terminal-bench
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install terminal-bench
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Further Documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.tbench.ai/tasks"&gt;Task Gallery&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.tbench.ai/docs/task-ideas"&gt;Task Ideas&lt;/a&gt; - Browse community-sourced task ideas&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.tbench.ai/docs/dashboard"&gt;Dashboard Documentation&lt;/a&gt; - Information about the Terminal-Bench dashboard&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Core Components&lt;/h2&gt; 
&lt;h3&gt;Dataset of Tasks&lt;/h3&gt; 
&lt;p&gt;Each task in Terminal-Bench includes&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;a instruction in English,&lt;/li&gt; 
 &lt;li&gt;a test script to verify if the language model / agent completed the task successfully,&lt;/li&gt; 
 &lt;li&gt;a reference ("oracle") solution that solves the task.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Tasks are located in the &lt;a href="https://raw.githubusercontent.com/laude-institute/terminal-bench/main/tasks"&gt;&lt;code&gt;tasks&lt;/code&gt;&lt;/a&gt; folder of the repository, and the aforementioned list of current tasks gives an overview that is easy to browse.&lt;/p&gt; 
&lt;h3&gt;Execution Harness&lt;/h3&gt; 
&lt;p&gt;The harness connects language models to a sandboxed terminal environment. After &lt;a href="https://www.tbench.ai/docs/installation"&gt;installing the terminal-bench package&lt;/a&gt; (along with the dependencies &lt;code&gt;uv&lt;/code&gt; and &lt;code&gt;Docker&lt;/code&gt;) you can view how to run the harness using:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;tb run --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For detailed information about running the harness and its options, see the &lt;a href="https://www.tbench.ai/docs/first-steps"&gt;documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Submit to Our Leaderboard&lt;/h3&gt; 
&lt;p&gt;Terminal-Bench-Core v0.1.1 is the set of tasks for Terminal-Bench's beta release and corresponds to the current leaderboard. To evaluate on it pass &lt;code&gt;--dataset-name terminal-bench-core&lt;/code&gt; and &lt;code&gt;--dataset-version 0.1.1&lt;/code&gt; to the harness. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;tb run \
    --agent terminus \
    --model-name anthropic/claude-3-7-latest \
    --dataset-name terminal-bench-core
    --dataset-version 0.1.1
    --n-concurrent 8
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more detailed instructions on submitting to the leaderboard, view our &lt;a href="https://www.tbench.ai/docs/submitting-to-leaderboard"&gt;leaderboard submission guide&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For more information on Terminal-Bench datasets and versioning view our &lt;a href="https://www.tbench.ai/docs/registry"&gt;registry overview&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Creating New Tasks&lt;/h2&gt; 
&lt;p&gt;View our &lt;a href="https://www.tbench.ai/docs/task-quickstart"&gt;task contribution quickstart&lt;/a&gt; to create a new task.&lt;/p&gt; 
&lt;h2&gt;Citing Us&lt;/h2&gt; 
&lt;p&gt;If you found Terminal-Bench useful, please cite us as:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{tbench_2025,
      title={Terminal-Bench: A Benchmark for AI Agents in Terminal Environments}, 
      url={https://github.com/laude-institute/terminal-bench}, 
      author={The Terminal-Bench Team}, year={2025}, month={Apr}} 
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>HunxByts/GhostTrack</title>
      <link>https://github.com/HunxByts/GhostTrack</link>
      <description>&lt;p&gt;Useful tool to track location or mobile number&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;GhostTrack&lt;/h1&gt; 
&lt;p&gt;Useful tool to track location or mobile number, so this tool can be called osint or also information gathering&lt;/p&gt; 
&lt;img src="https://github.com/HunxByts/GhostTrack/raw/main/asset/bn.png" /&gt; 
&lt;p&gt;New update : &lt;code&gt;Version 2.2&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Instalation on Linux (deb)&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;sudo apt-get install git
sudo apt-get install python3
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Instalation on Termux&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;pkg install git
pkg install python3
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Usage Tool&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;git clone https://github.com/HunxByts/GhostTrack.git
cd GhostTrack
pip3 install -r requirements.txt
python3 GhostTR.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Display on the menu &lt;code&gt;IP Tracker&lt;/code&gt;&lt;/p&gt; 
&lt;img src="https://github.com/HunxByts/GhostTrack/blob/main/asset/ip.png " /&gt; 
&lt;p&gt;on the IP Track menu, you can combo with the seeker tool to get the target IP&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;span&gt;‚ö°&lt;/span&gt; Install Seeker :&lt;/summary&gt; - 
 &lt;strong&gt;&lt;a href="https://github.com/thewhiteh4t/seeker"&gt;Get Seeker&lt;/a&gt;&lt;/strong&gt; 
&lt;/details&gt; 
&lt;p&gt;Display on the menu &lt;code&gt;Phone Tracker&lt;/code&gt;&lt;/p&gt; 
&lt;img src="https://github.com/HunxByts/GhostTrack/raw/main/asset/phone.png" /&gt; 
&lt;p&gt;on this menu you can search for information from the target phone number&lt;/p&gt; 
&lt;p&gt;Display on the menu &lt;code&gt;Username Tracker&lt;/code&gt;&lt;/p&gt; 
&lt;img src="https://github.com/HunxByts/GhostTrack/raw/main/asset/User.png" /&gt; on this menu you can search for information from the target username on social media 
&lt;details&gt; 
 &lt;summary&gt;&lt;span&gt;‚ö°&lt;/span&gt; Author :&lt;/summary&gt; - 
 &lt;strong&gt;&lt;a href="https://github.com/HunxByts"&gt;HunxByts&lt;/a&gt;&lt;/strong&gt; 
&lt;/details&gt;</description>
    </item>
    
  </channel>
</rss>