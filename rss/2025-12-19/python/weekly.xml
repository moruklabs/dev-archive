<rss version="2.0">
  <channel>
    <title>GitHub Python Weekly Trending</title>
    <description>Weekly Trending of Python in GitHub</description>
    <pubDate>Thu, 18 Dec 2025 01:44:42 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>theOehrly/Fast-F1</title>
      <link>https://github.com/theOehrly/Fast-F1</link>
      <description>&lt;p&gt;FastF1 is a python package for accessing and analyzing Formula 1 results, schedules, timing data and telemetry&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt; &lt;img src="https://raw.githubusercontent.com/theOehrly/Fast-F1/master/docs/_static/banner.png" width="400" /&gt; &lt;/h1&gt;
&lt;br /&gt; A python package for accessing and analyzing Formula 1 results, schedules, timing data and telemetry. 
&lt;h2&gt;Main Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Access to F1 timing data, telemetry, sessions results and more&lt;/li&gt; 
 &lt;li&gt;Full support for the Ergast compatible &lt;a href="https://github.com/jolpica/jolpica-f1/raw/main/docs/README.md"&gt;jolpica-f1&lt;/a&gt; API to access current and historical F1 data&lt;/li&gt; 
 &lt;li&gt;All data is provided in the form of extended Pandas DataFrames to make working with the data easy while having powerful tools available&lt;/li&gt; 
 &lt;li&gt;Adds custom functions to the Pandas objects specifically to make working with F1 data quick and simple&lt;/li&gt; 
 &lt;li&gt;Integration with Matplotlib to facilitate data visualization&lt;/li&gt; 
 &lt;li&gt;Implements caching for all API requests to speed up your scripts&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;It is recommended to install FastF1 using &lt;code&gt;pip&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-commandline"&gt;pip install fastf1
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, a wheel or a source distribution can be downloaded from Pypi.&lt;/p&gt; 
&lt;p&gt;You can also install using &lt;code&gt;conda&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-commandline"&gt;conda install -c conda-forge fastf1
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Installation in Pyodide, JupyterLite and other WASM-based environments&lt;/h4&gt; 
&lt;p&gt;FastF1 should be mostly compatible with Pyodide and other WASM-based environments, although this is not extensively tested. Currently, the installation and usage require some additional steps. You can find more information and a guide in &lt;a href="https://github.com/f1datajunkie/jupyterlite-fastf1"&gt;this external repository&lt;/a&gt; and the discussion in &lt;a href="https://github.com/theOehrly/Fast-F1/issues/667"&gt;this issue&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Third-party packages&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;R package that wraps FastF1: &lt;a href="https://cran.r-project.org/package=f1dataR"&gt;https://cran.r-project.org/package=f1dataR&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Third-party packages are not directly related to the FastF1 project. Questions and suggestions regarding these packages need to be directed at their respective maintainers.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;The official documentation can be found here: &lt;a href="https://docs.fastf1.dev"&gt;docs.fastf1.dev&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Supporting the Project&lt;/h2&gt; 
&lt;p&gt;If you want to support the continuous development of FastF1, you can sponsor me on GitHub or buy me a coffee.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/sponsors/theOehrly"&gt;https://github.com/sponsors/theOehrly&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.buymeacoffee.com/fastf1" target="_blank"&gt;&lt;img src="https://cdn.buymeacoffee.com/buttons/default-orange.png" alt="Buy Me A Coffee" height="41" width="174" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Notice&lt;/h2&gt; 
&lt;p&gt;FastF1 and this website are unofficial and are not associated in any way with the Formula 1 companies. F1, FORMULA ONE, FORMULA 1, FIA FORMULA ONE WORLD CHAMPIONSHIP, GRAND PRIX and related marks are trade marks of Formula One Licensing B.V.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>zhu-xlab/GlobalBuildingAtlas</title>
      <link>https://github.com/zhu-xlab/GlobalBuildingAtlas</link>
      <description>&lt;p&gt;GlobalBuildingAtlas: an open global and complete dataset of building polygons, heights and LoD1 3D models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;GlobalBuildingAtlas&lt;/h1&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;In this project, we provide the level of detail 1 (LoD1) data of buildings across the globe.&lt;/p&gt; 
&lt;p&gt;A overview of the dataset is illustrated bellow:&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/zhu-xlab/GlobalBuildingAtlas/main/figures/overview.png" width="800" /&gt; 
&lt;h2&gt;Access to the Data&lt;/h2&gt; 
&lt;h3&gt;Web Feature Service (WFS)&lt;/h3&gt; 
&lt;p&gt;A WFS is provided so that one can access the data using other websites or GIS softwares such as QGIS and ArcGIS.&lt;/p&gt; 
&lt;p&gt;Url: &lt;code&gt;https://tubvsig-so2sat-vm1.srv.mwn.de/geoserver/ows?&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Web Viewer&lt;/h3&gt; 
&lt;p&gt;A web interface for viewing the data is available at: &lt;a href="https://tubvsig-so2sat-vm1.srv.mwn.de"&gt;website&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Note: Over the past few days, our web viewer has received nearly 280,000 access requests. Due to this unusually high traffic, some data may not load completely, which may result in a significant portion of buildings not being displayed.&lt;/p&gt; 
&lt;h3&gt;Full Data Download&lt;/h3&gt; 
&lt;p&gt;The full data can be downloaded from &lt;a href="https://mediatum.ub.tum.de/1782307"&gt;mediaTUM&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Development Code&lt;/h2&gt; 
&lt;h3&gt;Global Building Polygon Generation using Satellite Data (Sec. 4.3)&lt;/h3&gt; 
&lt;p&gt;For codes related to building map extraction, regularization, polygonization, and simplification, i.e., generating building polygons from satellite images (Sec. 4.3.2, Sec. 4.3.3, and Sec. 4.3.4), please refer to &lt;code&gt;./im2bf&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Global Building Height Estimation (Sec. 4.4)&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;For codes related to monocular height estimation using HTC-DC Net (Sec. 4.4.2), please refer to &lt;code&gt;./im2bh&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;For codes related to the global inference and uncertainty quantification (Sec. 4.4.3), please refer to &lt;code&gt;./infer_height&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Global LoD1 Building Model Generation (Sec. 4.5)&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;For codes related to quality-guided building polygon fusion (Sec. 4.5.1), please refer to &lt;code&gt;./fuse_bf&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;For codes related to LoD1 building model generation (Sec. 4.5.2), please refer to &lt;code&gt;./make_lod1&lt;/code&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Visualization Code&lt;/h2&gt; 
&lt;p&gt;For codes to reproduce the plots in the manuscript, please refer to &lt;code&gt;./make_plots&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Code License&lt;/h2&gt; 
&lt;p&gt;MIT with Commons Clause (no commercial use allowed). See &lt;a href="https://github.com/zhu-xlab/GlobalBuildingAtlas/raw/main/LICENSE"&gt;LICENSE&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;How to cite&lt;/h2&gt; 
&lt;p&gt;If you find this dataset helpful in your work, please cite the following paper.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@Article{essd-17-6647-2025,
AUTHOR = {Zhu, X. X. and Chen, S. and Zhang, F. and Shi, Y. and Wang, Y.},
TITLE = {GlobalBuildingAtlas: an open global and complete dataset of building polygons, heights and LoD1 3D models},
JOURNAL = {Earth System Science Data},
VOLUME = {17},
YEAR = {2025},
NUMBER = {12},
PAGES = {6647--6668},
URL = {https://essd.copernicus.org/articles/17/6647/2025/},
DOI = {10.5194/essd-17-6647-2025}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Free-TV/IPTV</title>
      <link>https://github.com/Free-TV/IPTV</link>
      <description>&lt;p&gt;M3U Playlist for free TV channels&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Free TV&lt;/h1&gt; 
&lt;p&gt;This is an M3U playlist for free TV channels around the World.&lt;/p&gt; 
&lt;p&gt;Either free locally (over the air):&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/usa.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/us.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/canada.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/ca.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/uk.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/gb.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/ireland.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/ie.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/australia.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/au.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/india.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/in.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/japan.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/jp.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/china.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/cn.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/hong_kong.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/hk.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/macau.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/mo.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/taiwan.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/tw.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/north_korea.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/kp.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/korea.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/kr.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/denmark.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/dk.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/faroe_islands.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/fo.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/greenland.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/gl.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/finland.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/fi.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/iceland.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/is.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/norway.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/no.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/sweden.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/se.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/estonia.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/ee.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/latvia.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/lv.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/lithuania.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/lt.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/belgium.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/be.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/netherlands.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/nl.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/luxembourg.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/lu.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/germany.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/de.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/austria.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/at.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/switzerland.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/ch.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/poland.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/pl.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/czech_republic.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/cz.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/slovakia.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/sk.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/hungary.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/hu.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/romania.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/ro.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/moldova.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/md.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/bulgaria.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/bg.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/france.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/fr.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/italy.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/it.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/portugal.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/pt.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/spain.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/es.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/russia.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/ru.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/belarus.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/by.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/ukraine.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/ua.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/armenia.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/am.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/azerbaijan.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/az.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/georgia.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/ge.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/bosnia_and_herzegovina.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/ba.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/croatia.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/hr.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/montenegro.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/me.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/north_macedonia.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/mk.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/serbia.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/rs.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/slovenia.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/si.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/albania.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/al.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/kosovo.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/xk.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/greece.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/gr.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/cyprus.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/cy.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/andorra.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/ad.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/malta.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/mt.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/monaco.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/mc.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/san_marino.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/sm.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/iran.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/ir.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/iraq.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/iq.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/israel.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/il.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/qatar.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/qa.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/turkey.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/tr.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/united_arab_emirates.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/ae.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/argentina.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/ar.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/costa_rica.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/cr.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/dominican_republic.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/do.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/mexico.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/mx.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/paraguay.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/py.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/peru.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/pe.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/venezuela.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/ve.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/brazil.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/br.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/trinidad.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/tt.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/chad.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/td.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/somalia.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/so.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/indonesia.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/id.svg?sanitize=true" width="24" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Or free on the Internet:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Plex TV&lt;/li&gt; 
 &lt;li&gt;Pluto TV (English, Spanish, French, Italian)&lt;/li&gt; 
 &lt;li&gt;Redbox Live TV&lt;/li&gt; 
 &lt;li&gt;Roku TV&lt;/li&gt; 
 &lt;li&gt;Samsung TV Plus&lt;/li&gt; 
 &lt;li&gt;Youtube live channels&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To use it point your IPTV player to &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/playlist.m3u8"&gt;https://raw.githubusercontent.com/Free-TV/IPTV/master/playlist.m3u8&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Philosophy&lt;/h1&gt; 
&lt;p&gt;The main goals for this playlist are listed below.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Quality over quantity&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;The less channels we support the better.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;All channels should work well.&lt;/li&gt; 
 &lt;li&gt;As much as possible channels should be in HD, not SD.&lt;/li&gt; 
 &lt;li&gt;Only one URL per channel (no +1, no alternate feeds, no regional declinations)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Only free channels&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;If a channel is normally only available via commercial subscriptions it has nothing to do in this playlist. If on the other hand it is provided for free to everybody in a particular country, then it should be in this playlist.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;No paid channels&lt;/li&gt; 
 &lt;li&gt;Only channels which are officially provided for free (via DVB-S, DVB-T, analog, etc..)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Only mainstream channels&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;This is a playlist for everybody.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;No adult channels&lt;/li&gt; 
 &lt;li&gt;No channels dedicated to any particular religion&lt;/li&gt; 
 &lt;li&gt;No channels dedicated to any particular political party&lt;/li&gt; 
 &lt;li&gt;No channels made for a country and funded by a different country&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Feed sources&lt;/h1&gt; 
&lt;p&gt;It can be quite hard to find up to date URLs, here's a list of sources:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/iptv-org/iptv/tree/master/streams"&gt;https://github.com/iptv-org/iptv/tree/master/streams&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Youtube: As long as the channel is live and its URL doesn't change (check the age of the stream, the number of viewers..)&lt;/li&gt; 
 &lt;li&gt;Dailymotion: Same criteria as for youtube&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Format&lt;/h1&gt; 
&lt;p&gt;The m3u8 playlist is generated by &lt;code&gt;make_playlist.py&lt;/code&gt;, using the &lt;code&gt;.md&lt;/code&gt; files located in &lt;code&gt;lists&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Each .md file represesnts a group. The &lt;code&gt;&amp;lt;h1&amp;gt;&lt;/code&gt; line is used as the group title.&lt;/p&gt; 
&lt;p&gt;Only channels which URL column starts with &lt;code&gt;[&amp;gt;]&lt;/code&gt; are included in the playlist.&lt;/p&gt; 
&lt;p&gt;Channels which are not in HD are marked with an &lt;code&gt;‚ìà&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Channels which use GeoIP blocking are marked with a &lt;code&gt;‚íº&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Channels which are live Youtube channels are marked with a &lt;code&gt;‚ìé&lt;/code&gt;.&lt;/p&gt; 
&lt;h1&gt;Issues&lt;/h1&gt; 
&lt;p&gt;Only create issues for bugs and feature requests.&lt;/p&gt; 
&lt;p&gt;Do not create issues to add/edit or to remove channels. If you want to add/edit/remove channels, create a pull request directly.&lt;/p&gt; 
&lt;h1&gt;Pull Requests&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;Only modify .md files&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;If your Pull Request modifies channels, only modify .md files. Do not modify m3u8 files in your pull request.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Adding a new Channel&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;To add a new channel, make a Pull Request.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;In your Pull Request you need to provide information to show that the channel is free.&lt;/li&gt; 
 &lt;li&gt;Use imgur.com to host the channel logo and point to it.&lt;/li&gt; 
 &lt;li&gt;If you have a valid stream, add it and put &lt;code&gt;[&amp;gt;]&lt;/code&gt; in front of it.&lt;/li&gt; 
 &lt;li&gt;If you don't have an stream for the channel, add &lt;code&gt;[x]()&lt;/code&gt; in the url column and place your channel in the Invalid category.&lt;/li&gt; 
 &lt;li&gt;If you have a stream but it doesn't work well, put the channel in the Invalid category and put &lt;code&gt;[x]&lt;/code&gt; in front of the url.&lt;/li&gt; 
 &lt;li&gt;If you're adding geoblocked URLs specify it in your PR and specify which country they're working in. The PR will only be merged if these URLs can be tested.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Removing a Channel&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;To remove a channel, make a Pull Request.&lt;/p&gt; 
&lt;p&gt;In your Pull Request you need to provide information to show that the channel is only available via a private paid subscription.&lt;/p&gt; 
&lt;p&gt;Note: Public taxes (whether national or regional, whether called TV License or not) do not constitute a private paid subscription.&lt;/p&gt; 
&lt;p&gt;If a stream is broken, simply move the channel to the invalid category and replace &lt;code&gt;[&amp;gt;]&lt;/code&gt; with &lt;code&gt;[x]&lt;/code&gt; in the url column.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Mebus/cupp</title>
      <link>https://github.com/Mebus/cupp</link>
      <description>&lt;p&gt;Common User Passwords Profiler (CUPP)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CUPP - Common User Passwords Profiler&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://travis-ci.org/Mebus/cupp"&gt;&lt;img src="https://travis-ci.org/Mebus/cupp.svg?branch=master" alt="Build Status" /&gt;&lt;/a&gt; &lt;a href="https://coveralls.io/github/Mebus/cupp"&gt;&lt;img src="https://coveralls.io/repos/github/Mebus/cupp/badge.svg?sanitize=true" alt="Coverage Status" /&gt;&lt;/a&gt; &lt;a href="https://app.codacy.com/app/Mebus/cupp?utm_source=github.com&amp;amp;utm_medium=referral&amp;amp;utm_content=Mebus/cupp&amp;amp;utm_campaign=Badge_Grade_Dashboard"&gt;&lt;img src="https://api.codacy.com/project/badge/Grade/a578dde078ef481e97a0e7eac0c8d312" alt="Codacy Badge" /&gt;&lt;/a&gt; &lt;a href="https://inventory.raw.pm/"&gt;&lt;img src="https://inventory.raw.pm/img/badges/Rawsec-inventoried-FF5050_plastic.svg?sanitize=true" alt="Rawsec's CyberSecurity Inventory" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;About&lt;/h2&gt; 
&lt;p&gt;The most common form of authentication is the combination of a username and a password or passphrase. If both match values stored within a locally stored table, the user is authenticated for a connection. Password strength is a measure of the difficulty involved in guessing or breaking the password through cryptographic techniques or library-based automated testing of alternate values.&lt;/p&gt; 
&lt;p&gt;A weak password might be very short or only use alphanumberic characters, making decryption simple. A weak password can also be one that is easily guessed by someone profiling the user, such as a birthday, nickname, address, name of a pet or relative, or a common word such as God, love, money or password.&lt;/p&gt; 
&lt;p&gt;That is why CUPP was born, and it can be used in situations like legal penetration tests or forensic crime investigations.&lt;/p&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;p&gt;You need Python 3 to run CUPP.&lt;/p&gt; 
&lt;h2&gt;Quick start&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;$ python3 cupp.py -h
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Options&lt;/h2&gt; 
&lt;p&gt;Usage: cupp.py [OPTIONS]&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    -h      this menu

    -i      Interactive questions for user password profiling

    -w      Use this option to profile existing dictionary,
            or WyD.pl output to make some pwnsauce :)

    -l      Download huge wordlists from repository

    -a      Parse default usernames and passwords directly from Alecto DB.
            Project Alecto uses purified databases of Phenoelit and CIRT which where merged and enhanced.

    -v      Version of the program
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Configuration&lt;/h2&gt; 
&lt;p&gt;CUPP has configuration file cupp.cfg with instructions.&lt;/p&gt; 
&lt;h2&gt;Example (Fast forwarded)&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Mebus/cupp/master/screenshots/cupp-example.gif" alt="cupp-example" /&gt;&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This program is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 3 of the License, or any later version.&lt;/p&gt; 
&lt;p&gt;This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.&lt;/p&gt; 
&lt;p&gt;You should have received a copy of the GNU General Public License along with this program; if not, write to the Free Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA&lt;/p&gt; 
&lt;p&gt;See './LICENSE' for more information.&lt;/p&gt; 
&lt;h2&gt;Github import&lt;/h2&gt; 
&lt;p&gt;This project was imported into &lt;a href="https://github.com/Mebus/cupp"&gt;https://github.com/Mebus/cupp&lt;/a&gt; by Mebus from:&lt;br /&gt; &lt;a href="http://www.remote-exploit.org/content/cupp-3.0.tar.gz"&gt;http://www.remote-exploit.org/content/cupp-3.0.tar.gz&lt;/a&gt;&lt;br /&gt; &lt;a href="http://www.remote-exploit.org/articles/misc_research__amp_code/index.html"&gt;http://www.remote-exploit.org/articles/misc_research__amp_code/index.html&lt;/a&gt;&lt;br /&gt; to encourage further development of the tool.&lt;/p&gt; 
&lt;h2&gt;Original author&lt;/h2&gt; 
&lt;p&gt;Muris Kurgas aka j0rgan&lt;br /&gt; &lt;a href="mailto:j0rgan@remote-exploit.org"&gt;j0rgan@remote-exploit.org&lt;/a&gt;&lt;br /&gt; &lt;a href="http://www.remote-exploit.org"&gt;http://www.remote-exploit.org&lt;/a&gt;&lt;br /&gt; &lt;a href="http://www.azuzi.me"&gt;http://www.azuzi.me&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Contributors&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Bosko Petrovic aka bolexxx&lt;br /&gt; &lt;a href="mailto:bole_loser@hotmail.com"&gt;bole_loser@hotmail.com&lt;/a&gt;&lt;br /&gt; &lt;a href="http://www.offensive-security.com"&gt;http://www.offensive-security.com&lt;/a&gt;&lt;br /&gt; &lt;a href="http://www.bolexxx.net"&gt;http://www.bolexxx.net&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Mebus&lt;br /&gt; &lt;a href="https://github.com/Mebus/"&gt;https://github.com/Mebus/&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Abhro&lt;br /&gt; &lt;a href="https://github.com/Abhro/"&gt;https://github.com/Abhro/&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Andrea Giacomo&lt;br /&gt; &lt;a href="https://github.com/codepr"&gt;https://github.com/codepr&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;quantumcore&lt;br /&gt; &lt;a href="https://github.com/quantumcore"&gt;https://github.com/quantumcore&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>OpenBMB/VoxCPM</title>
      <link>https://github.com/OpenBMB/VoxCPM</link>
      <description>&lt;p&gt;VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;üéôÔ∏è VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/OpenBMB/VoxCPM/"&gt;&lt;img src="https://img.shields.io/badge/Project%20Page-GitHub-blue" alt="Project Page" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2509.24650"&gt;&lt;img src="https://img.shields.io/badge/Technical%20Report-Arxiv-red" alt="Technical Report" /&gt;&lt;/a&gt;&lt;a href="https://huggingface.co/spaces/OpenBMB/VoxCPM-Demo"&gt;&lt;img src="https://img.shields.io/badge/Live%20PlayGround-Demo-orange" alt="Live Playground" /&gt;&lt;/a&gt; &lt;a href="https://openbmb.github.io/VoxCPM-demopage"&gt;&lt;img src="https://img.shields.io/badge/Audio%20Samples-Page-green" alt="Samples" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;VoxCPM1.5 Model Weights&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/openbmb/VoxCPM1.5"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-OpenBMB-yellow" alt="Hugging Face" /&gt;&lt;/a&gt; &lt;a href="https://modelscope.cn/models/OpenBMB/VoxCPM1.5"&gt;&lt;img src="https://img.shields.io/badge/ModelScope-OpenBMB-purple" alt="ModelScope" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/VoxCPM/main/assets/voxcpm_logo.png" alt="VoxCPM Logo" width="40%" /&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;üëã Contact us on &lt;a href="https://raw.githubusercontent.com/OpenBMB/VoxCPM/main/assets/wechat.png"&gt;WeChat&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;[2025.12.05] üéâ üéâ üéâ We Open Source the VoxCPM1.5 &lt;a href="https://huggingface.co/openbmb/VoxCPM1.5"&gt;weights&lt;/a&gt;! The model now supports both full-parameter fine-tuning and efficient LoRA fine-tuning, empowering you to create your own tailored version. See &lt;a href="https://raw.githubusercontent.com/OpenBMB/VoxCPM/main/docs/release_note.md"&gt;Release Notes&lt;/a&gt; for details.&lt;/li&gt; 
 &lt;li&gt;[2025.09.30] üî• üî• üî• We Release VoxCPM &lt;a href="https://arxiv.org/abs/2509.24650"&gt;Technical Report&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;[2025.09.16] üî• üî• üî• We Open Source the VoxCPM-0.5B &lt;a href="https://huggingface.co/openbmb/VoxCPM-0.5B"&gt;weights&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;[2025.09.16] üéâ üéâ üéâ We Provide the &lt;a href="https://huggingface.co/spaces/OpenBMB/VoxCPM-Demo"&gt;Gradio PlayGround&lt;/a&gt; for VoxCPM-0.5B, try it now!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Overview&lt;/h2&gt; 
&lt;p&gt;VoxCPM is a novel tokenizer-free Text-to-Speech (TTS) system that redefines realism in speech synthesis. By modeling speech in a continuous space, it overcomes the limitations of discrete tokenization and enables two flagship capabilities: context-aware speech generation and true-to-life zero-shot voice cloning.&lt;/p&gt; 
&lt;p&gt;Unlike mainstream approaches that convert speech to discrete tokens, VoxCPM uses an end-to-end diffusion autoregressive architecture that directly generates continuous speech representations from text. Built on &lt;a href="https://huggingface.co/openbmb/MiniCPM4-0.5B"&gt;MiniCPM-4&lt;/a&gt; backbone, it achieves implicit semantic-acoustic decoupling through hierachical language modeling and FSQ constraints, greatly enhancing both expressiveness and generation stability.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/VoxCPM/main/assets/voxcpm_model.png" alt="VoxCPM Model Architecture" width="90%" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;üöÄ Key Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Context-Aware, Expressive Speech Generation&lt;/strong&gt; - VoxCPM comprehends text to infer and generate appropriate prosody, delivering speech with remarkable expressiveness and natural flow. It spontaneously adapts speaking style based on content, producing highly fitting vocal expression trained on a massive 1.8 million-hour bilingual corpus.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;True-to-Life Voice Cloning&lt;/strong&gt; - With only a short reference audio clip, VoxCPM performs accurate zero-shot voice cloning, capturing not only the speaker's timbre but also fine-grained characteristics such as accent, emotional tone, rhythm, and pacing to create a faithful and natural replica.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;High-Efficiency Synthesis&lt;/strong&gt; - VoxCPM supports streaming synthesis with a Real-Time Factor (RTF) as low as 0.17 on a consumer-grade NVIDIA RTX 4090 GPU, making it possible for real-time applications.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üì¶ Model Versions&lt;/h3&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/OpenBMB/VoxCPM/main/docs/release_note.md"&gt;Release Notes&lt;/a&gt; for details&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;VoxCPM1.5&lt;/strong&gt; (Latest):&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Model Params: 800M&lt;/li&gt; 
   &lt;li&gt;Sampling rate of AudioVAE: 44100&lt;/li&gt; 
   &lt;li&gt;Token rate in LM Backbone: 6.25Hz (patch-size=4)&lt;/li&gt; 
   &lt;li&gt;RTF in a single NVIDIA-RTX 4090 GPU: ~0.15&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;VoxCPM-0.5B&lt;/strong&gt; (Original):&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Model Params: 640M&lt;/li&gt; 
   &lt;li&gt;Sampling rate of AudioVAE: 16000&lt;/li&gt; 
   &lt;li&gt;Token rate in LM Backbone: 12.5Hz (patch-size=2)&lt;/li&gt; 
   &lt;li&gt;RTF in a single NVIDIA-RTX 4090 GPU: 0.17&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;h3&gt;üîß Install from PyPI&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip install voxcpm
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;1. Model Download (Optional)&lt;/h3&gt; 
&lt;p&gt;By default, when you first run the script, the model will be downloaded automatically, but you can also download the model in advance.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Download VoxCPM1.5&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from huggingface_hub import snapshot_download
snapshot_download("openbmb/VoxCPM1.5")
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Or Download VoxCPM-0.5B&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from huggingface_hub import snapshot_download
snapshot_download("openbmb/VoxCPM-0.5B")
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Download ZipEnhancer and SenseVoice-Small. We use ZipEnhancer to enhance speech prompts and SenseVoice-Small for speech prompt ASR in the web demo.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from modelscope import snapshot_download
snapshot_download('iic/speech_zipenhancer_ans_multiloss_16k_base')
snapshot_download('iic/SenseVoiceSmall')
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;2. Basic Usage&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import soundfile as sf
import numpy as np
from voxcpm import VoxCPM

model = VoxCPM.from_pretrained("openbmb/VoxCPM1.5")

# Non-streaming
wav = model.generate(
    text="VoxCPM is an innovative end-to-end TTS model from ModelBest, designed to generate highly expressive speech.",
    prompt_wav_path=None,      # optional: path to a prompt speech for voice cloning
    prompt_text=None,          # optional: reference text
    cfg_value=2.0,             # LM guidance on LocDiT, higher for better adherence to the prompt, but maybe worse
    inference_timesteps=10,   # LocDiT inference timesteps, higher for better result, lower for fast speed
    normalize=False,           # enable external TN tool, but will disable native raw text support
    denoise=False,             # enable external Denoise tool, but it may cause some distortion and restrict the sampling rate to 16kHz
    retry_badcase=True,        # enable retrying mode for some bad cases (unstoppable)
    retry_badcase_max_times=3,  # maximum retrying times
    retry_badcase_ratio_threshold=6.0, # maximum length restriction for bad case detection (simple but effective), it could be adjusted for slow pace speech
)

sf.write("output.wav", wav, model.tts_model.sample_rate)
print("saved: output.wav")

# Streaming
chunks = []
for chunk in model.generate_streaming(
    text = "Streaming text to speech is easy with VoxCPM!",
    # supports same args as above
):
    chunks.append(chunk)
wav = np.concatenate(chunks)

sf.write("output_streaming.wav", wav, model.tts_model.sample_rate)
print("saved: output_streaming.wav")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. CLI Usage&lt;/h3&gt; 
&lt;p&gt;After installation, the entry point is &lt;code&gt;voxcpm&lt;/code&gt; (or use &lt;code&gt;python -m voxcpm.cli&lt;/code&gt;).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 1) Direct synthesis (single text)
voxcpm --text "VoxCPM is an innovative end-to-end TTS model from ModelBest, designed to generate highly expressive speech." --output out.wav

# 2) Voice cloning (reference audio + transcript)
voxcpm --text "VoxCPM is an innovative end-to-end TTS model from ModelBest, designed to generate highly expressive speech." \
  --prompt-audio path/to/voice.wav \
  --prompt-text "reference transcript" \
  --output out.wav \
  # --denoise

# (Optinal) Voice cloning (reference audio + transcript file)
voxcpm --text "VoxCPM is an innovative end-to-end TTS model from ModelBest, designed to generate highly expressive speech." \
  --prompt-audio path/to/voice.wav \
  --prompt-file "/path/to/text-file" \
  --output out.wav \
  # --denoise

# 3) Batch processing (one text per line)
voxcpm --input examples/input.txt --output-dir outs
# (optional) Batch + cloning
voxcpm --input examples/input.txt --output-dir outs \
  --prompt-audio path/to/voice.wav \
  --prompt-text "reference transcript" \
  # --denoise

# 4) Inference parameters (quality/speed)
voxcpm --text "..." --output out.wav \
  --cfg-value 2.0 --inference-timesteps 10 --normalize

# 5) Model loading
# Prefer local path
voxcpm --text "..." --output out.wav --model-path /path/to/VoxCPM_model_dir
# Or from Hugging Face (auto download/cache)
voxcpm --text "..." --output out.wav \
  --hf-model-id openbmb/VoxCPM1.5 --cache-dir ~/.cache/huggingface --local-files-only

# 6) Denoiser control
voxcpm --text "..." --output out.wav \
  --no-denoiser --zipenhancer-path iic/speech_zipenhancer_ans_multiloss_16k_base

# 7) Help
voxcpm --help
python -m voxcpm.cli --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;4. Start web demo&lt;/h3&gt; 
&lt;p&gt;You can start the UI interface by running &lt;code&gt;python app.py&lt;/code&gt;, which allows you to perform Voice Cloning and Voice Creation.&lt;/p&gt; 
&lt;h3&gt;5. Fine-tuning&lt;/h3&gt; 
&lt;p&gt;VoxCPM1.5 supports both full fine-tuning (SFT) and LoRA fine-tuning, allowing you to train personalized voice models on your own data. See the &lt;a href="https://raw.githubusercontent.com/OpenBMB/VoxCPM/main/docs/finetune.md"&gt;Fine-tuning Guide&lt;/a&gt; for detailed instructions.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Quick Start:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Full fine-tuning
python scripts/train_voxcpm_finetune.py \
    --config_path conf/voxcpm_v1.5/voxcpm_finetune_all.yaml

# LoRA fine-tuning
python scripts/train_voxcpm_finetune.py \
    --config_path conf/voxcpm_v1.5/voxcpm_finetune_lora.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;üìö Documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/VoxCPM/main/docs/usage_guide.md"&gt;Usage Guide&lt;/a&gt;&lt;/strong&gt; - Detailed guide on how to use VoxCPM effectively, including text input modes, voice cloning tips, and parameter tuning&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/VoxCPM/main/docs/finetune.md"&gt;Fine-tuning Guide&lt;/a&gt;&lt;/strong&gt; - Complete guide for fine-tuning VoxCPM models with SFT and LoRA&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/VoxCPM/main/docs/release_note.md"&gt;Release Notes&lt;/a&gt;&lt;/strong&gt; - Version history and updates&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/VoxCPM/main/docs/performance.md"&gt;Performance Benchmarks&lt;/a&gt;&lt;/strong&gt; - Detailed performance comparisons on public benchmarks&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìö More Information&lt;/h2&gt; 
&lt;h3&gt;üåü Community Projects&lt;/h3&gt; 
&lt;p&gt;We're excited to see the VoxCPM community growing! Here are some amazing projects and features built by our community:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/wildminder/ComfyUI-VoxCPM"&gt;ComfyUI-VoxCPM&lt;/a&gt;&lt;/strong&gt; A VoxCPM extension for ComfyUI.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/1038lab/ComfyUI-VoxCPMTTS"&gt;ComfyUI-VoxCPMTTS&lt;/a&gt;&lt;/strong&gt; A VoxCPM extension for ComfyUI.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rsxdalv/tts_webui_extension.vox_cpm"&gt;WebUI-VoxCPM&lt;/a&gt;&lt;/strong&gt; A template extension for TTS WebUI.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/OpenBMB/VoxCPM/pull/26"&gt;PR: Streaming API Support (by AbrahamSanders)&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/a710128/nanovllm-voxcpm"&gt;VoxCPM-NanoVLLM&lt;/a&gt;&lt;/strong&gt; NanoVLLM integration for VoxCPM for faster, high-throughput inference on GPU.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/bluryar/VoxCPM-ONNX"&gt;VoxCPM-ONNX&lt;/a&gt;&lt;/strong&gt; ONNX export for VoxCPM supports faster CPU inference.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/0seba/VoxCPMANE"&gt;VoxCPMANE&lt;/a&gt;&lt;/strong&gt; VoxCPM TTS with Apple Neural Engine backend server.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/OpenBMB/VoxCPM/pull/100"&gt;PR: LoRA finetune web UI (by Ayin1412)&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/madushan1000/voxcpm_rs"&gt;voxcpm_rs&lt;/a&gt;&lt;/strong&gt; A re-implementation of VoxCPM-0.5B in Rust.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;em&gt;Note: The projects are not officially maintained by OpenBMB.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Have you built something cool with VoxCPM? We'd love to feature it here! Please open an issue or pull request to add your project.&lt;/em&gt;&lt;/p&gt; 
&lt;h3&gt;üìä Performance Highlights&lt;/h3&gt; 
&lt;p&gt;VoxCPM achieves competitive results on public zero-shot TTS benchmarks. See &lt;a href="https://raw.githubusercontent.com/OpenBMB/VoxCPM/main/docs/performance.md"&gt;Performance Benchmarks&lt;/a&gt; for detailed comparison tables.&lt;/p&gt; 
&lt;h2&gt;‚ö†Ô∏è Risks and limitations&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;General Model Behavior: While VoxCPM has been trained on a large-scale dataset, it may still produce outputs that are unexpected, biased, or contain artifacts.&lt;/li&gt; 
 &lt;li&gt;Potential for Misuse of Voice Cloning: VoxCPM's powerful zero-shot voice cloning capability can generate highly realistic synthetic speech. This technology could be misused for creating convincing deepfakes for purposes of impersonation, fraud, or spreading disinformation. Users of this model must not use it to create content that infringes upon the rights of individuals. It is strictly forbidden to use VoxCPM for any illegal or unethical purposes. We strongly recommend that any publicly shared content generated with this model be clearly marked as AI-generated.&lt;/li&gt; 
 &lt;li&gt;Current Technical Limitations: Although generally stable, the model may occasionally exhibit instability, especially with very long or expressive inputs. Furthermore, the current version offers limited direct control over specific speech attributes like emotion or speaking style.&lt;/li&gt; 
 &lt;li&gt;Bilingual Model: VoxCPM is trained primarily on Chinese and English data. Performance on other languages is not guaranteed and may result in unpredictable or low-quality audio.&lt;/li&gt; 
 &lt;li&gt;This model is released for research and development purposes only. We do not recommend its use in production or commercial applications without rigorous testing and safety evaluations. Please use VoxCPM responsibly.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìù TO-DO List&lt;/h2&gt; 
&lt;p&gt;Please stay tuned for updates!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Release the VoxCPM technical report.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Support higher sampling rate (44.1kHz in VoxCPM-1.5).&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Support SFT and LoRA fine-tuning.&lt;/li&gt; 
 &lt;li&gt;[] Multilingual Support (besides ZH/EN).&lt;/li&gt; 
 &lt;li&gt;[] Controllable Speech Generation by Human Instruction.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìÑ License&lt;/h2&gt; 
&lt;p&gt;The VoxCPM model weights and code are open-sourced under the &lt;a href="https://raw.githubusercontent.com/OpenBMB/VoxCPM/main/LICENSE"&gt;Apache-2.0&lt;/a&gt; license.&lt;/p&gt; 
&lt;h2&gt;üôè Acknowledgments&lt;/h2&gt; 
&lt;p&gt;We extend our sincere gratitude to the following works and resources for their inspiration and contributions:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2502.03930"&gt;DiTAR&lt;/a&gt; for the diffusion autoregressive backbone used in speech generation&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenBMB/MiniCPM"&gt;MiniCPM-4&lt;/a&gt; for serving as the language model foundation&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/FunAudioLLM/CosyVoice"&gt;CosyVoice&lt;/a&gt; for the implementation of Flow Matching-based LocDiT&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/descriptinc/descript-audio-codec"&gt;DAC&lt;/a&gt; for providing the Audio VAE backbone&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Institutions&lt;/h2&gt; 
&lt;p&gt;This project is developed by the following institutions:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/VoxCPM/main/assets/modelbest_logo.png" width="28px" /&gt; &lt;a href="https://modelbest.cn/"&gt;ModelBest&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/VoxCPM/main/assets/thuhcsi_logo.png" width="28px" /&gt; &lt;a href="https://github.com/thuhcsi"&gt;THUHCSI&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;‚≠ê Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#OpenBMB/VoxCPM&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=OpenBMB/VoxCPM&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üìö Citation&lt;/h2&gt; 
&lt;p&gt;If you find our model helpful, please consider citing our projects üìù and staring us ‚≠êÔ∏èÔºÅ&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bib"&gt;@article{voxcpm2025,
  title        = {VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning},
  author       = {Zhou, Yixuan and Zeng, Guoyang and Liu, Xin and Li, Xiang and Yu, Renjie and Wang, Ziyang and Ye, Runchuan and Sun, Weiyue and Gui, Jiancheng and Li, Kehan and Wu, Zhiyong  and Liu, Zhiyuan},
  journal      = {arXiv preprint arXiv:2509.24650},
  year         = {2025},
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>microsoft/VibeVoice</title>
      <link>https://github.com/microsoft/VibeVoice</link>
      <description>&lt;p&gt;Open-Source Frontier Voice AI&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h2&gt;üéôÔ∏è VibeVoice: Open-Source Frontier Voice AI&lt;/h2&gt; 
 &lt;p&gt;&lt;a href="https://microsoft.github.io/VibeVoice"&gt;&lt;img src="https://img.shields.io/badge/Project-Page-blue?logo=microsoft" alt="Project Page" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/collections/microsoft/vibevoice-68a2ef24a875c44be47b034f"&gt;&lt;img src="https://img.shields.io/badge/HuggingFace-Collection-orange?logo=huggingface" alt="Hugging Face" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/pdf/2508.19205"&gt;&lt;img src="https://img.shields.io/badge/Technical-Report-red?logo=adobeacrobatreader" alt="Technical Report" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="Figures/VibeVoice_logo_white.png" /&gt; 
  &lt;img src="https://raw.githubusercontent.com/microsoft/VibeVoice/main/Figures/VibeVoice_logo.png" alt="VibeVoice Logo" width="300" /&gt; 
 &lt;/picture&gt; 
&lt;/div&gt; 
&lt;div align="left"&gt; 
 &lt;h3&gt;üì∞ News&lt;/h3&gt; 
 &lt;img src="https://img.shields.io/badge/Status-New-brightgreen?style=flat" alt="New" /&gt; 
 &lt;img src="https://img.shields.io/badge/Feature-Realtime_TTS-blue?style=flat&amp;amp;logo=soundcharts" alt="Realtime TTS" /&gt; 
 &lt;p&gt;&lt;strong&gt;2025-12-16: üì£ We added more experimental speakers for exploration, including multilingual voices and 11 distinct English style voices. &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-realtime-0.5b.md#optional-more-experimental-voices"&gt;Try it&lt;/a&gt;. More speaker types will be added over time.&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;2025-12-09: üì£ We added experimental speakers in nine languages (DE, FR, IT, JP, KR, NL, PL, PT, ES) for exploration‚Äîwelcome to try them out and share your feedback.&lt;/p&gt; 
 &lt;p&gt;2025-12-03: üì£ We open-sourced &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-realtime-0.5b.md"&gt;&lt;strong&gt;VibeVoice‚ÄëRealtime‚Äë0.5B&lt;/strong&gt;&lt;/a&gt;, a real‚Äëtime text‚Äëto‚Äëspeech model that supports streaming text input and robust long-form speech generation. Try it on &lt;a href="https://colab.research.google.com/github/microsoft/VibeVoice/blob/main/demo/vibevoice_realtime_colab.ipynb"&gt;Colab&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;To mitigate deepfake risks and ensure low latency for the first speech chunk, voice prompts are provided in an embedded format. For users requiring voice customization, please reach out to our team. We will also be expanding the range of available speakers. &lt;br /&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/0901d274-f6ae-46ef-a0fd-3c4fba4f76dc"&gt;https://github.com/user-attachments/assets/0901d274-f6ae-46ef-a0fd-3c4fba4f76dc&lt;/a&gt;&lt;/p&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;(Launch your own realtime demo via the websocket example in &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-realtime-0.5b.md#usage-1-launch-real-time-websocket-demo"&gt;Usage&lt;/a&gt;).&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/div&gt; 
&lt;p&gt;2025-09-05: VibeVoice is an open-source research framework intended to advance collaboration in the speech synthesis community. After release, we discovered instances where the tool was used in ways inconsistent with the stated intent. Since responsible use of AI is one of Microsoft‚Äôs guiding principles, we have disabled this repo until we are confident that out-of-scope use is no longer possible.&lt;/p&gt; 
&lt;h3&gt;Overview&lt;/h3&gt; 
&lt;p&gt;VibeVoice is a novel framework designed for generating &lt;strong&gt;expressive&lt;/strong&gt;, &lt;strong&gt;long-form&lt;/strong&gt;, &lt;strong&gt;multi-speaker&lt;/strong&gt; conversational audio, such as podcasts, from text. It addresses significant challenges in traditional Text-to-Speech (TTS) systems, particularly in scalability, speaker consistency, and natural turn-taking.&lt;/p&gt; 
&lt;p&gt;VibeVoice currently includes two model variants:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Long-form multi-speaker model&lt;/strong&gt;: Synthesizes conversational/single-speaker speech up to &lt;strong&gt;90 minutes&lt;/strong&gt; with up to &lt;strong&gt;4 distinct speakers&lt;/strong&gt;, surpassing the typical 1‚Äì2 speaker limits of many prior models.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-realtime-0.5b.md"&gt;Realtime streaming TTS model&lt;/a&gt;&lt;/strong&gt;: Produces initial audible speech in ~&lt;strong&gt;300 ms&lt;/strong&gt; and supports &lt;strong&gt;streaming text input&lt;/strong&gt; for single-speaker &lt;strong&gt;real-time&lt;/strong&gt; speech generation; designed for low-latency generation.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;A core innovation of VibeVoice is its use of continuous speech tokenizers (Acoustic and Semantic) operating at an ultra-low frame rate of 7.5 Hz. These tokenizers efficiently preserve audio fidelity while significantly boosting computational efficiency for processing long sequences. VibeVoice employs a &lt;a href="https://arxiv.org/abs/2412.08635"&gt;next-token diffusion&lt;/a&gt; framework, leveraging a Large Language Model (LLM) to understand textual context and dialogue flow, and a diffusion head to generate high-fidelity acoustic details.&lt;/p&gt; 
&lt;p align="left"&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/VibeVoice/main/Figures/MOS-preference.png" alt="MOS Preference Results" height="260px" /&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/VibeVoice/main/Figures/VibeVoice.jpg" alt="VibeVoice Overview" height="250px" style="margin-right: 10px;" /&gt; &lt;/p&gt; 
&lt;h3&gt;üéµ Demo Examples&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Video Demo&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;We produced this video with &lt;a href="https://github.com/Wan-Video/Wan2.2"&gt;Wan2.2&lt;/a&gt;. We sincerely appreciate the Wan-Video team for their great work.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;English&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/0967027c-141e-4909-bec8-091558b1b784"&gt;https://github.com/user-attachments/assets/0967027c-141e-4909-bec8-091558b1b784&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Chinese&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/322280b7-3093-4c67-86e3-10be4746c88f"&gt;https://github.com/user-attachments/assets/322280b7-3093-4c67-86e3-10be4746c88f&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Cross-Lingual&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/838d8ad9-a201-4dde-bb45-8cd3f59ce722"&gt;https://github.com/user-attachments/assets/838d8ad9-a201-4dde-bb45-8cd3f59ce722&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Spontaneous Singing&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/6f27a8a5-0c60-4f57-87f3-7dea2e11c730"&gt;https://github.com/user-attachments/assets/6f27a8a5-0c60-4f57-87f3-7dea2e11c730&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Long Conversation with 4 people&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/a357c4b6-9768-495c-a576-1618f6275727"&gt;https://github.com/user-attachments/assets/a357c4b6-9768-495c-a576-1618f6275727&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;For more examples, see the &lt;a href="https://microsoft.github.io/VibeVoice"&gt;Project Page&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Risks and limitations&lt;/h2&gt; 
&lt;p&gt;While efforts have been made to optimize it through various techniques, it may still produce outputs that are unexpected, biased, or inaccurate. VibeVoice inherits any biases, errors, or omissions produced by its base model (specifically, Qwen2.5 1.5b in this release). Potential for Deepfakes and Disinformation: High-quality synthetic speech can be misused to create convincing fake audio content for impersonation, fraud, or spreading disinformation. Users must ensure transcripts are reliable, check content accuracy, and avoid using generated content in misleading ways. Users are expected to use the generated content and to deploy the models in a lawful manner, in full compliance with all applicable laws and regulations in the relevant jurisdictions. It is best practice to disclose the use of AI when sharing AI-generated content.&lt;/p&gt; 
&lt;p&gt;English and Chinese only: Transcripts in languages other than English or Chinese may result in unexpected audio outputs.&lt;/p&gt; 
&lt;p&gt;Non-Speech Audio: The model focuses solely on speech synthesis and does not handle background noise, music, or other sound effects.&lt;/p&gt; 
&lt;p&gt;Overlapping Speech: The current model does not explicitly model or generate overlapping speech segments in conversations.&lt;/p&gt; 
&lt;p&gt;We do not recommend using VibeVoice in commercial or real-world applications without further testing and development. This model is intended for research and development purposes only. Please use responsibly.&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://api.star-history.com/svg?repos=Microsoft/vibevoice&amp;amp;type=date&amp;amp;legend=top-left" alt="Star History Chart" /&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>datawhalechina/hello-agents</title>
      <link>https://github.com/datawhalechina/hello-agents</link>
      <description>&lt;p&gt;üìö „Ää‰ªéÈõ∂ÂºÄÂßãÊûÑÂª∫Êô∫ËÉΩ‰Ωì„Äã‚Äî‚Äî‰ªéÈõ∂ÂºÄÂßãÁöÑÊô∫ËÉΩ‰ΩìÂéüÁêÜ‰∏éÂÆûË∑µÊïôÁ®ã&lt;/p&gt;&lt;hr&gt;&lt;div align="right"&gt; 
 &lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/README_EN.md"&gt;English&lt;/a&gt; | ‰∏≠Êñá 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/images/hello-agents.png" alt="alt text" width="100%" /&gt; 
 &lt;h1&gt;Hello-Agents&lt;/h1&gt; 
 &lt;h3&gt;ü§ñ „Ää‰ªéÈõ∂ÂºÄÂßãÊûÑÂª∫Êô∫ËÉΩ‰Ωì„Äã&lt;/h3&gt; 
 &lt;div align="center"&gt; 
  &lt;a href="https://trendshift.io/repositories/15520" target="_blank"&gt; &lt;img src="https://trendshift.io/api/badge/repositories/15520" alt="datawhalechina%2Fhello-agents | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt; &lt;/a&gt; 
 &lt;/div&gt; 
 &lt;p&gt;&lt;em&gt;‰ªéÂü∫Á°ÄÁêÜËÆ∫Âà∞ÂÆûÈôÖÂ∫îÁî®ÔºåÂÖ®Èù¢ÊéåÊè°Êô∫ËÉΩ‰ΩìÁ≥ªÁªüÁöÑËÆæËÆ°‰∏éÂÆûÁé∞&lt;/em&gt;&lt;/p&gt; 
 &lt;img src="https://img.shields.io/github/stars/datawhalechina/Hello-Agents?style=flat&amp;amp;logo=github" alt="GitHub stars" /&gt; 
 &lt;img src="https://img.shields.io/github/forks/datawhalechina/Hello-Agents?style=flat&amp;amp;logo=github" alt="GitHub forks" /&gt; 
 &lt;img src="https://img.shields.io/badge/language-Chinese-brightgreen?style=flat" alt="Language" /&gt; 
 &lt;a href="https://github.com/datawhalechina/Hello-Agents"&gt;&lt;img src="https://img.shields.io/badge/GitHub-Project-blue?style=flat&amp;amp;logo=github" alt="GitHub Project" /&gt;&lt;/a&gt; 
 &lt;a href="https://datawhalechina.github.io/hello-agents/"&gt;&lt;img src="https://img.shields.io/badge/Âú®Á∫øÈòÖËØª-Online%20Reading-green?style=flat&amp;amp;logo=gitbook" alt="Online Reading" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üéØ È°πÁõÆ‰ªãÁªç&lt;/h2&gt; 
&lt;p&gt;‚ÄÉ‚ÄÉÂ¶ÇÊûúËØ¥ 2024 Âπ¥ÊòØ"ÁôæÊ®°Â§ßÊàò"ÁöÑÂÖÉÂπ¥ÔºåÈÇ£‰πà 2025 Âπ¥Êó†ÁñëÂºÄÂêØ‰∫Ü"Agent ÂÖÉÂπ¥"„ÄÇÊäÄÊúØÁöÑÁÑ¶ÁÇπÊ≠£‰ªéËÆ≠ÁªÉÊõ¥Â§ßÁöÑÂü∫Á°ÄÊ®°ÂûãÔºåËΩ¨ÂêëÊûÑÂª∫Êõ¥ËÅ™ÊòéÁöÑÊô∫ËÉΩ‰ΩìÂ∫îÁî®„ÄÇÁÑ∂ËÄåÔºåÂΩìÂâçÁ≥ªÁªüÊÄß„ÄÅÈáçÂÆûË∑µÁöÑÊïôÁ®ãÂç¥ÊûÅÂ∫¶ÂåÆ‰πè„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨ÂèëËµ∑‰∫Ü Hello-Agents È°πÁõÆÔºåÂ∏åÊúõËÉΩ‰∏∫Á§æÂå∫Êèê‰æõ‰∏ÄÊú¨‰ªéÈõ∂ÂºÄÂßã„ÄÅÁêÜËÆ∫‰∏éÂÆûÊàòÂπ∂ÈáçÁöÑÊô∫ËÉΩ‰ΩìÁ≥ªÁªüÊûÑÂª∫ÊåáÂçó„ÄÇ&lt;/p&gt; 
&lt;p&gt;‚ÄÉ‚ÄÉHello-Agents ÊòØ Datawhale Á§æÂå∫ÁöÑ&lt;strong&gt;Á≥ªÁªüÊÄßÊô∫ËÉΩ‰ΩìÂ≠¶‰π†ÊïôÁ®ã&lt;/strong&gt;„ÄÇÂ¶Ç‰ªä Agent ÊûÑÂª∫‰∏ªË¶ÅÂàÜ‰∏∫‰∏§Ê¥æÔºå‰∏ÄÊ¥æÊòØ DifyÔºåCozeÔºån8n ËøôÁ±ªËΩØ‰ª∂Â∑•Á®ãÁ±ª AgentÔºåÂÖ∂Êú¨Ë¥®ÊòØÊµÅÁ®ãÈ©±Âä®ÁöÑËΩØ‰ª∂ÂºÄÂèëÔºåLLM ‰Ωú‰∏∫Êï∞ÊçÆÂ§ÑÁêÜÁöÑÂêéÁ´ØÔºõÂè¶‰∏ÄÊ¥æÂàôÊòØ AI ÂéüÁîüÁöÑ AgentÔºåÂç≥ÁúüÊ≠£‰ª• AI È©±Âä®ÁöÑ Agent„ÄÇÊú¨ÊïôÁ®ãÊó®Âú®Â∏¶È¢ÜÂ§ßÂÆ∂Ê∑±ÂÖ•ÁêÜËß£Âπ∂ÊûÑÂª∫ÂêéËÄÖ‚Äî‚ÄîÁúüÊ≠£ÁöÑ AI Native Agent„ÄÇÊïôÁ®ãÂ∞ÜÂ∏¶È¢Ü‰Ω†Á©øÈÄèÊ°ÜÊû∂Ë°®Ë±°Ôºå‰ªéÊô∫ËÉΩ‰ΩìÁöÑÊ†∏ÂøÉÂéüÁêÜÂá∫ÂèëÔºåÊ∑±ÂÖ•ÂÖ∂Ê†∏ÂøÉÊû∂ÊûÑÔºåÁêÜËß£ÂÖ∂ÁªèÂÖ∏ËåÉÂºèÔºåÂπ∂ÊúÄÁªà‰∫≤ÊâãÊûÑÂª∫Ëµ∑Â±û‰∫éËá™Â∑±ÁöÑÂ§öÊô∫ËÉΩ‰ΩìÂ∫îÁî®„ÄÇÊàë‰ª¨Áõ∏‰ø°ÔºåÊúÄÂ•ΩÁöÑÂ≠¶‰π†ÊñπÂºèÂ∞±ÊòØÂä®ÊâãÂÆûË∑µ„ÄÇÂ∏åÊúõËøôÊú¨ÊïôÁ®ãËÉΩÊàê‰∏∫‰Ω†Êé¢Á¥¢Êô∫ËÉΩ‰Ωì‰∏ñÁïåÁöÑËµ∑ÁÇπÔºåËÉΩÂ§ü‰ªé‰∏ÄÂêçÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑ"‰ΩøÁî®ËÄÖ"ÔºåËúïÂèò‰∏∫‰∏ÄÂêçÊô∫ËÉΩ‰ΩìÁ≥ªÁªüÁöÑ"ÊûÑÂª∫ËÄÖ"„ÄÇ&lt;/p&gt; 
&lt;h2&gt;üìö Âø´ÈÄüÂºÄÂßã&lt;/h2&gt; 
&lt;h3&gt;Âú®Á∫øÈòÖËØª&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://datawhalechina.github.io/hello-agents/"&gt;üåê ÁÇπÂáªËøôÈáåÂºÄÂßãÂú®Á∫øÈòÖËØª&lt;/a&gt;&lt;/strong&gt; - Êó†ÈúÄ‰∏ãËΩΩÔºåÈöèÊó∂ÈöèÂú∞Â≠¶‰π†&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://book.heterocat.com.cn/"&gt;üìñ Cookbook(ÊµãËØïÁâà)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h3&gt;Êú¨Âú∞ÈòÖËØª&lt;/h3&gt; 
&lt;p&gt;Â¶ÇÊûúÊÇ®Â∏åÊúõÂú®Êú¨Âú∞ÈòÖËØªÊàñË¥°ÁåÆÂÜÖÂÆπÔºåËØ∑ÂèÇËÄÉ‰∏ãÊñπÁöÑÂ≠¶‰π†ÊåáÂçó„ÄÇ&lt;/p&gt; 
&lt;h3&gt;‚ú® ‰Ω†Â∞ÜÊî∂Ëé∑‰ªÄ‰πàÔºü&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìñ &lt;strong&gt;Datawhale ÂºÄÊ∫êÂÖçË¥π&lt;/strong&gt; ÂÆåÂÖ®ÂÖçË¥πÂ≠¶‰π†Êú¨È°πÁõÆÊâÄÊúâÂÜÖÂÆπÔºå‰∏éÁ§æÂå∫ÂÖ±ÂêåÊàêÈïø&lt;/li&gt; 
 &lt;li&gt;üîç &lt;strong&gt;ÁêÜËß£Ê†∏ÂøÉÂéüÁêÜ&lt;/strong&gt; Ê∑±ÂÖ•ÁêÜËß£Êô∫ËÉΩ‰ΩìÁöÑÊ¶ÇÂøµ„ÄÅÂéÜÂè≤‰∏éÁªèÂÖ∏ËåÉÂºè&lt;/li&gt; 
 &lt;li&gt;üèóÔ∏è &lt;strong&gt;‰∫≤ÊâãÂÆûÁé∞&lt;/strong&gt; ÊéåÊè°ÁÉ≠Èó®‰Ωé‰ª£Á†ÅÂπ≥Âè∞ÂíåÊô∫ËÉΩ‰Ωì‰ª£Á†ÅÊ°ÜÊû∂ÁöÑ‰ΩøÁî®&lt;/li&gt; 
 &lt;li&gt;üõ†Ô∏è &lt;strong&gt;Ëá™Á†îÊ°ÜÊû∂&lt;a href="https://github.com/jjyaoao/helloagents"&gt;HelloAgents&lt;/a&gt;&lt;/strong&gt; Âü∫‰∫é Openai ÂéüÁîü API ‰ªéÈõ∂ÊûÑÂª∫‰∏Ä‰∏™Ëá™Â∑±ÁöÑÊô∫ËÉΩ‰ΩìÊ°ÜÊû∂&lt;/li&gt; 
 &lt;li&gt;‚öôÔ∏è &lt;strong&gt;ÊéåÊè°È´òÁ∫ßÊäÄËÉΩ&lt;/strong&gt; ‰∏ÄÊ≠•Ê≠•ÂÆûÁé∞‰∏ä‰∏ãÊñáÂ∑•Á®ã„ÄÅMemory„ÄÅÂçèËÆÆ„ÄÅËØÑ‰º∞Á≠âÁ≥ªÁªüÊÄßÊäÄÊúØ&lt;/li&gt; 
 &lt;li&gt;ü§ù &lt;strong&gt;Ê®°ÂûãËÆ≠ÁªÉ&lt;/strong&gt; ÊéåÊè° Agentic RLÔºå‰ªé SFT Âà∞ GRPO ÁöÑÂÖ®ÊµÅÁ®ãÂÆûÊàòËÆ≠ÁªÉ LLM&lt;/li&gt; 
 &lt;li&gt;üöÄ &lt;strong&gt;È©±Âä®ÁúüÂÆûÊ°à‰æã&lt;/strong&gt; ÂÆûÊàòÂºÄÂèëÊô∫ËÉΩÊóÖË°åÂä©Êâã„ÄÅËµõÂçöÂ∞èÈïáÁ≠âÁªºÂêàÈ°πÁõÆ&lt;/li&gt; 
 &lt;li&gt;üìñ &lt;strong&gt;Ê±ÇËÅåÈù¢ËØï&lt;/strong&gt; Â≠¶‰π†Êô∫ËÉΩ‰ΩìÊ±ÇËÅåÁõ∏ÂÖ≥Èù¢ËØïÈóÆÈ¢ò&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìñ ÂÜÖÂÆπÂØºËà™&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Á´†ËäÇ&lt;/th&gt; 
   &lt;th&gt;ÂÖ≥ÈîÆÂÜÖÂÆπ&lt;/th&gt; 
   &lt;th&gt;Áä∂ÊÄÅ&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/%E5%89%8D%E8%A8%80.md"&gt;ÂâçË®Ä&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;È°πÁõÆÁöÑÁºòËµ∑„ÄÅËÉåÊôØÂèäËØªËÄÖÂª∫ËÆÆ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Á¨¨‰∏ÄÈÉ®ÂàÜÔºöÊô∫ËÉΩ‰Ωì‰∏éËØ≠Ë®ÄÊ®°ÂûãÂü∫Á°Ä&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter1/%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%88%9D%E8%AF%86%E6%99%BA%E8%83%BD%E4%BD%93.md"&gt;Á¨¨‰∏ÄÁ´† ÂàùËØÜÊô∫ËÉΩ‰Ωì&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Êô∫ËÉΩ‰ΩìÂÆö‰πâ„ÄÅÁ±ªÂûã„ÄÅËåÉÂºè‰∏éÂ∫îÁî®&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter2/%E7%AC%AC%E4%BA%8C%E7%AB%A0%20%E6%99%BA%E8%83%BD%E4%BD%93%E5%8F%91%E5%B1%95%E5%8F%B2.md"&gt;Á¨¨‰∫åÁ´† Êô∫ËÉΩ‰ΩìÂèëÂ±ïÂè≤&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‰ªéÁ¨¶Âè∑‰∏ª‰πâÂà∞ LLM È©±Âä®ÁöÑÊô∫ËÉΩ‰ΩìÊºîËøõ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter3/%E7%AC%AC%E4%B8%89%E7%AB%A0%20%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80.md"&gt;Á¨¨‰∏âÁ´† Â§ßËØ≠Ë®ÄÊ®°ÂûãÂü∫Á°Ä&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Transformer„ÄÅÊèêÁ§∫„ÄÅ‰∏ªÊµÅ LLM ÂèäÂÖ∂Â±ÄÈôê&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Á¨¨‰∫åÈÉ®ÂàÜÔºöÊûÑÂª∫‰Ω†ÁöÑÂ§ßËØ≠Ë®ÄÊ®°ÂûãÊô∫ËÉΩ‰Ωì&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter4/%E7%AC%AC%E5%9B%9B%E7%AB%A0%20%E6%99%BA%E8%83%BD%E4%BD%93%E7%BB%8F%E5%85%B8%E8%8C%83%E5%BC%8F%E6%9E%84%E5%BB%BA.md"&gt;Á¨¨ÂõõÁ´† Êô∫ËÉΩ‰ΩìÁªèÂÖ∏ËåÉÂºèÊûÑÂª∫&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ÊâãÊääÊâãÂÆûÁé∞ ReAct„ÄÅPlan-and-Solve„ÄÅReflection&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter5/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%9F%BA%E4%BA%8E%E4%BD%8E%E4%BB%A3%E7%A0%81%E5%B9%B3%E5%8F%B0%E7%9A%84%E6%99%BA%E8%83%BD%E4%BD%93%E6%90%AD%E5%BB%BA.md"&gt;Á¨¨‰∫îÁ´† Âü∫‰∫é‰Ωé‰ª£Á†ÅÂπ≥Âè∞ÁöÑÊô∫ËÉΩ‰ΩìÊê≠Âª∫&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‰∫ÜËß£ Coze„ÄÅDify„ÄÅn8n Á≠â‰Ωé‰ª£Á†ÅÊô∫ËÉΩ‰ΩìÂπ≥Âè∞‰ΩøÁî®&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter6/%E7%AC%AC%E5%85%AD%E7%AB%A0%20%E6%A1%86%E6%9E%B6%E5%BC%80%E5%8F%91%E5%AE%9E%E8%B7%B5.md"&gt;Á¨¨ÂÖ≠Á´† Ê°ÜÊû∂ÂºÄÂèëÂÆûË∑µ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;AutoGen„ÄÅAgentScope„ÄÅLangGraph Á≠â‰∏ªÊµÅÊ°ÜÊû∂Â∫îÁî®&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter7/%E7%AC%AC%E4%B8%83%E7%AB%A0%20%E6%9E%84%E5%BB%BA%E4%BD%A0%E7%9A%84Agent%E6%A1%86%E6%9E%B6.md"&gt;Á¨¨‰∏ÉÁ´† ÊûÑÂª∫‰Ω†ÁöÑAgentÊ°ÜÊû∂&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‰ªé 0 ÂºÄÂßãÊûÑÂª∫Êô∫ËÉΩ‰ΩìÊ°ÜÊû∂&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Á¨¨‰∏âÈÉ®ÂàÜÔºöÈ´òÁ∫ßÁü•ËØÜÊâ©Â±ï&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter8/%E7%AC%AC%E5%85%AB%E7%AB%A0%20%E8%AE%B0%E5%BF%86%E4%B8%8E%E6%A3%80%E7%B4%A2.md"&gt;Á¨¨ÂÖ´Á´† ËÆ∞ÂøÜ‰∏éÊ£ÄÁ¥¢&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ËÆ∞ÂøÜÁ≥ªÁªüÔºåRAGÔºåÂ≠òÂÇ®&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter9/%E7%AC%AC%E4%B9%9D%E7%AB%A0%20%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B.md"&gt;Á¨¨‰πùÁ´† ‰∏ä‰∏ãÊñáÂ∑•Á®ã&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ÊåÅÁª≠‰∫§‰∫íÁöÑ"ÊÉÖÂ¢ÉÁêÜËß£"&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter10/%E7%AC%AC%E5%8D%81%E7%AB%A0%20%E6%99%BA%E8%83%BD%E4%BD%93%E9%80%9A%E4%BF%A1%E5%8D%8F%E8%AE%AE.md"&gt;Á¨¨ÂçÅÁ´† Êô∫ËÉΩ‰ΩìÈÄö‰ø°ÂçèËÆÆ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;MCP„ÄÅA2A„ÄÅANP Á≠âÂçèËÆÆËß£Êûê&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter11/%E7%AC%AC%E5%8D%81%E4%B8%80%E7%AB%A0%20Agentic-RL.md"&gt;Á¨¨ÂçÅ‰∏ÄÁ´† Agentic-RL&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‰ªé SFT Âà∞ GRPO ÁöÑ LLM ËÆ≠ÁªÉÂÆûÊàò&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter12/%E7%AC%AC%E5%8D%81%E4%BA%8C%E7%AB%A0%20%E6%99%BA%E8%83%BD%E4%BD%93%E6%80%A7%E8%83%BD%E8%AF%84%E4%BC%B0.md"&gt;Á¨¨ÂçÅ‰∫åÁ´† Êô∫ËÉΩ‰ΩìÊÄßËÉΩËØÑ‰º∞&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Ê†∏ÂøÉÊåáÊ†á„ÄÅÂü∫ÂáÜÊµãËØï‰∏éËØÑ‰º∞Ê°ÜÊû∂&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Á¨¨ÂõõÈÉ®ÂàÜÔºöÁªºÂêàÊ°à‰æãËøõÈò∂&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter13/%E7%AC%AC%E5%8D%81%E4%B8%89%E7%AB%A0%20%E6%99%BA%E8%83%BD%E6%97%85%E8%A1%8C%E5%8A%A9%E6%89%8B.md"&gt;Á¨¨ÂçÅ‰∏âÁ´† Êô∫ËÉΩÊóÖË°åÂä©Êâã&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;MCP ‰∏éÂ§öÊô∫ËÉΩ‰ΩìÂçè‰ΩúÁöÑÁúüÂÆû‰∏ñÁïåÂ∫îÁî®&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter14/%E7%AC%AC%E5%8D%81%E5%9B%9B%E7%AB%A0%20%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A0%94%E7%A9%B6%E6%99%BA%E8%83%BD%E4%BD%93.md"&gt;Á¨¨ÂçÅÂõõÁ´† Ëá™Âä®ÂåñÊ∑±Â∫¶Á†îÁ©∂Êô∫ËÉΩ‰Ωì&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;DeepResearch Agent Â§çÁé∞‰∏éËß£Êûê&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter15/%E7%AC%AC%E5%8D%81%E4%BA%94%E7%AB%A0%20%E6%9E%84%E5%BB%BA%E8%B5%9B%E5%8D%9A%E5%B0%8F%E9%95%87.md"&gt;Á¨¨ÂçÅ‰∫îÁ´† ÊûÑÂª∫ËµõÂçöÂ∞èÈïá&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Agent ‰∏éÊ∏∏ÊàèÁöÑÁªìÂêàÔºåÊ®°ÊãüÁ§æ‰ºöÂä®ÊÄÅ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Á¨¨‰∫îÈÉ®ÂàÜÔºöÊØï‰∏öËÆæËÆ°ÂèäÊú™Êù•Â±ïÊúõ&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter16/%E7%AC%AC%E5%8D%81%E5%85%AD%E7%AB%A0%20%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1.md"&gt;Á¨¨ÂçÅÂÖ≠Á´† ÊØï‰∏öËÆæËÆ°&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ÊûÑÂª∫Â±û‰∫é‰Ω†ÁöÑÂÆåÊï¥Â§öÊô∫ËÉΩ‰ΩìÂ∫îÁî®&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Á§æÂå∫Ë¥°ÁåÆÁ≤æÈÄâ (Community Blog)&lt;/h3&gt; 
&lt;p&gt;‚ÄÉ‚ÄÉÊ¨¢ËøéÂ§ßÂÆ∂Â∞ÜÂú®Â≠¶‰π† Hello-Agents Êàñ Agent Áõ∏ÂÖ≥ÊäÄÊúØ‰∏≠ÁöÑÁã¨Âà∞ËßÅËß£„ÄÅÂÆûË∑µÊÄªÁªìÔºå‰ª• PR ÁöÑÂΩ¢ÂºèË¥°ÁåÆÂà∞Á§æÂå∫Á≤æÈÄâ„ÄÇÂ¶ÇÊûúÊòØÁã¨Á´ã‰∫éÊ≠£ÊñáÁöÑÂÜÖÂÆπÔºå‰πüÂèØ‰ª•ÊäïÁ®øËá≥ Extra-ChapterÔºÅ&lt;strong&gt;ÊúüÂæÖ‰Ω†ÁöÑÁ¨¨‰∏ÄÊ¨°Ë¥°ÁåÆÔºÅ&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Á§æÂå∫Á≤æÈÄâ&lt;/th&gt; 
   &lt;th&gt;ÂÜÖÂÆπÊÄªÁªì&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/datawhalechina/hello-agents/raw/main/Extra-Chapter/Extra01-%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93.md"&gt;01-AgentÈù¢ËØïÈ¢òÊÄªÁªì&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Agent Â≤ó‰ΩçÁõ∏ÂÖ≥Èù¢ËØïÈóÆÈ¢ò&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/datawhalechina/hello-agents/raw/main/Extra-Chapter/Extra01-%E5%8F%82%E8%80%83%E7%AD%94%E6%A1%88.md"&gt;01-AgentÈù¢ËØïÈ¢òÁ≠îÊ°à&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Áõ∏ÂÖ≥Èù¢ËØïÈóÆÈ¢òÁ≠îÊ°à&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/datawhalechina/hello-agents/raw/main/Extra-Chapter/Extra02-%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%E8%A1%A5%E5%85%85%E7%9F%A5%E8%AF%86.md"&gt;02-‰∏ä‰∏ãÊñáÂ∑•Á®ãÂÜÖÂÆπË°•ÂÖÖ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‰∏ä‰∏ãÊñáÂ∑•Á®ãÂÜÖÂÆπÊâ©Â±ï&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/datawhalechina/hello-agents/raw/main/Extra-Chapter/Extra03-Dify%E6%99%BA%E8%83%BD%E4%BD%93%E5%88%9B%E5%BB%BA%E4%BF%9D%E5%A7%86%E7%BA%A7%E6%93%8D%E4%BD%9C%E6%B5%81%E7%A8%8B.md"&gt;03-DifyÊô∫ËÉΩ‰ΩìÂàõÂª∫‰øùÂßÜÁ∫ßÊïôÁ®ã&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;DifyÊô∫ËÉΩ‰ΩìÂàõÂª∫‰øùÂßÜÁ∫ßÊïôÁ®ã&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/datawhalechina/hello-agents/raw/main/Extra-Chapter/Extra04-DatawhaleFAQ.md"&gt;04-Hello-agentsËØæÁ®ãÂ∏∏ËßÅÈóÆÈ¢ò&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;DatawhaleËØæÁ®ãÂ∏∏ËßÅÈóÆÈ¢ò&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;PDF ÁâàÊú¨‰∏ãËΩΩ&lt;/h3&gt; 
&lt;p&gt;‚ÄÉ‚ÄÉ&lt;em&gt;&lt;strong&gt;Êú¨ Hello-Agents PDF ÊïôÁ®ãÂÆåÂÖ®ÂºÄÊ∫êÂÖçË¥π„ÄÇ‰∏∫Èò≤Ê≠¢ÂêÑÁ±ªËê•ÈîÄÂè∑Âä†Ê∞¥Âç∞ÂêéË¥©ÂçñÁªôÂ§öÊô∫ËÉΩ‰ΩìÁ≥ªÁªüÂàùÂ≠¶ËÄÖÔºåÊàë‰ª¨ÁâπÂú∞Âú® PDF Êñá‰ª∂‰∏≠È¢ÑÂÖàÊ∑ªÂä†‰∫Ü‰∏çÂΩ±ÂìçÈòÖËØªÁöÑ Datawhale ÂºÄÊ∫êÊ†áÂøóÊ∞¥Âç∞ÔºåÊï¨ËØ∑Ë∞ÖËß£ÔΩû&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;Hello-Agents PDF : &lt;a href="https://github.com/datawhalechina/hello-agents/releases/tag/V1.0.0"&gt;https://github.com/datawhalechina/hello-agents/releases/tag/V1.0.0&lt;/a&gt;&lt;/em&gt;&lt;br /&gt; &lt;em&gt;Hello-Agents PDF ÂõΩÂÜÖ‰∏ãËΩΩÂú∞ÂùÄ : &lt;a href="https://www.datawhale.cn/learn/summary/239"&gt;https://www.datawhale.cn/learn/summary/239&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;üí° Â¶Ç‰ΩïÂ≠¶‰π†&lt;/h2&gt; 
&lt;p&gt;‚ÄÉ‚ÄÉÊ¨¢Ëøé‰Ω†ÔºåÊú™Êù•ÁöÑÊô∫ËÉΩÁ≥ªÁªüÊûÑÂª∫ËÄÖÔºÅÂú®ÂºÄÂêØËøôÊÆµÊøÄÂä®‰∫∫ÂøÉÁöÑÊóÖÁ®ã‰πãÂâçÔºåËØ∑ÂÖÅËÆ∏Êàë‰ª¨Áªô‰Ω†‰∏Ä‰∫õÊ∏ÖÊô∞ÁöÑÊåáÂºï„ÄÇ&lt;/p&gt; 
&lt;p&gt;‚ÄÉ‚ÄÉÊú¨È°πÁõÆÂÜÖÂÆπÂÖºÈ°æÁêÜËÆ∫‰∏éÂÆûÊàòÔºåÊó®Âú®Â∏ÆÂä©‰Ω†Á≥ªÁªüÊÄßÂú∞ÊéåÊè°‰ªéÂçï‰∏™Êô∫ËÉΩ‰ΩìÂà∞Â§öÊô∫ËÉΩ‰ΩìÁ≥ªÁªüÁöÑËÆæËÆ°‰∏éÂºÄÂèëÂÖ®ÊµÅÁ®ã„ÄÇÂõ†Ê≠§ÔºåÂ∞§ÂÖ∂ÈÄÇÂêàÊúâ‰∏ÄÂÆöÁºñÁ®ãÂü∫Á°ÄÁöÑ &lt;strong&gt;AI ÂºÄÂèëËÄÖ„ÄÅËΩØ‰ª∂Â∑•Á®ãÂ∏à„ÄÅÂú®Ê†°Â≠¶Áîü&lt;/strong&gt; ‰ª•ÂèäÂØπÂâçÊ≤ø AI ÊäÄÊúØÊä±ÊúâÊµìÂéöÂÖ¥Ë∂£ÁöÑ &lt;strong&gt;Ëá™Â≠¶ËÄÖ&lt;/strong&gt;„ÄÇÂú®Â≠¶‰π†Êú¨È°πÁõÆ‰πãÂâçÔºåÊàë‰ª¨Â∏åÊúõ‰Ω†ÂÖ∑Â§áÂü∫Á°ÄÁöÑ Python ÁºñÁ®ãËÉΩÂäõÔºåÂπ∂ÂØπÂ§ßËØ≠Ë®ÄÊ®°ÂûãÊúâÂü∫Êú¨ÁöÑÊ¶ÇÂøµÊÄß‰∫ÜËß£Ôºà‰æãÂ¶ÇÔºåÁü•ÈÅìÂ¶Ç‰ΩïÈÄöËøá API Ë∞ÉÁî®‰∏Ä‰∏™ LLMÔºâ„ÄÇÈ°πÁõÆÁöÑÈáçÁÇπÊòØÂ∫îÁî®‰∏éÊûÑÂª∫ÔºåÂõ†Ê≠§‰Ω†Êó†ÈúÄÂÖ∑Â§áÊ∑±ÂéöÁöÑÁÆóÊ≥ïÊàñÊ®°ÂûãËÆ≠ÁªÉËÉåÊôØ„ÄÇ&lt;/p&gt; 
&lt;p&gt;‚ÄÉ‚ÄÉÈ°πÁõÆÂàÜ‰∏∫‰∫îÂ§ßÈÉ®ÂàÜÔºåÊØè‰∏ÄÈÉ®ÂàÜÈÉΩÊòØÈÄöÂæÄ‰∏ã‰∏ÄÈò∂ÊÆµÁöÑÂùöÂÆûÈò∂Ê¢ØÔºö&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Á¨¨‰∏ÄÈÉ®ÂàÜÔºöÊô∫ËÉΩ‰Ωì‰∏éËØ≠Ë®ÄÊ®°ÂûãÂü∫Á°Ä&lt;/strong&gt;ÔºàÁ¨¨‰∏ÄÁ´†ÔΩûÁ¨¨‰∏âÁ´†ÔºâÔºåÊàë‰ª¨Â∞Ü‰ªéÊô∫ËÉΩ‰ΩìÁöÑÂÆö‰πâ„ÄÅÁ±ªÂûã‰∏éÂèëÂ±ïÂéÜÂè≤ËÆ≤Ëµ∑Ôºå‰∏∫‰Ω†Ê¢≥ÁêÜ"Êô∫ËÉΩ‰Ωì"Ëøô‰∏ÄÊ¶ÇÂøµÁöÑÊù•ÈæôÂéªËÑâ„ÄÇÈöèÂêéÔºåÊàë‰ª¨‰ºöÂø´ÈÄüÂ∑©Âõ∫Â§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÊ†∏ÂøÉÁü•ËØÜÔºå‰∏∫‰Ω†ÁöÑÂÆûË∑µ‰πãÊóÖÊâì‰∏ãÂùöÂÆûÁöÑÁêÜËÆ∫Âú∞Âü∫„ÄÇ&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Á¨¨‰∫åÈÉ®ÂàÜÔºöÊûÑÂª∫‰Ω†ÁöÑÂ§ßËØ≠Ë®ÄÊ®°ÂûãÊô∫ËÉΩ‰Ωì&lt;/strong&gt;ÔºàÁ¨¨ÂõõÁ´†ÔΩûÁ¨¨‰∏ÉÁ´†ÔºâÔºåËøôÊòØ‰Ω†Âä®ÊâãÂÆûË∑µÁöÑËµ∑ÁÇπ„ÄÇ‰Ω†Â∞Ü‰∫≤ÊâãÂÆûÁé∞ ReAct Á≠âÁªèÂÖ∏ËåÉÂºèÔºå‰ΩìÈ™å Coze Á≠â‰Ωé‰ª£Á†ÅÂπ≥Âè∞ÁöÑ‰æøÊç∑ÔºåÂπ∂ÊéåÊè° Langgraph Á≠â‰∏ªÊµÅÊ°ÜÊû∂ÁöÑÂ∫îÁî®„ÄÇÊúÄÁªàÔºåÊàë‰ª¨Ëøò‰ºöÂ∏¶‰Ω†‰ªéÈõ∂ÂºÄÂßãÊûÑÂª∫‰∏Ä‰∏™Â±û‰∫éËá™Â∑±ÁöÑÊô∫ËÉΩ‰ΩìÊ°ÜÊû∂ÔºåËÆ©‰Ω†ÂÖºÂÖ∑‚ÄúÁî®ËΩÆÂ≠ê‚Äù‰∏é‚ÄúÈÄ†ËΩÆÂ≠ê‚ÄùÁöÑËÉΩÂäõ„ÄÇ&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Á¨¨‰∏âÈÉ®ÂàÜÔºöÈ´òÁ∫ßÁü•ËØÜÊâ©Â±ï&lt;/strong&gt;ÔºàÁ¨¨ÂÖ´Á´†ÔΩûÁ¨¨ÂçÅ‰∫åÁ´†ÔºâÔºåÂú®Ëøô‰∏ÄÈÉ®ÂàÜÔºå‰Ω†ÁöÑÊô∫ËÉΩ‰ΩìÂ∞Ü‚ÄúÂ≠¶‰ºö‚ÄùÊÄùËÄÉ‰∏éÂçè‰Ωú„ÄÇÊàë‰ª¨Â∞Ü‰ΩøÁî®Á¨¨‰∫åÈÉ®ÂàÜÁöÑËá™Á†îÊ°ÜÊû∂ÔºåÊ∑±ÂÖ•Êé¢Á¥¢ËÆ∞ÂøÜ‰∏éÊ£ÄÁ¥¢„ÄÅ‰∏ä‰∏ãÊñáÂ∑•Á®ã„ÄÅAgent ËÆ≠ÁªÉÁ≠âÊ†∏ÂøÉÊäÄÊúØÔºåÂπ∂Â≠¶‰π†Â§öÊô∫ËÉΩ‰ΩìÈó¥ÁöÑÈÄö‰ø°ÂçèËÆÆ„ÄÇÊúÄÁªàÔºå‰Ω†Â∞ÜÊéåÊè°ËØÑ‰º∞Êô∫ËÉΩ‰ΩìÁ≥ªÁªüÊÄßËÉΩÁöÑ‰∏ì‰∏öÊñπÊ≥ï„ÄÇ&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Á¨¨ÂõõÈÉ®ÂàÜÔºöÁªºÂêàÊ°à‰æãËøõÈò∂&lt;/strong&gt;ÔºàÁ¨¨ÂçÅ‰∏âÁ´†ÔΩûÁ¨¨ÂçÅ‰∫îÁ´†ÔºâÔºåËøôÈáåÊòØÁêÜËÆ∫‰∏éÂÆûË∑µÁöÑ‰∫§Ê±áÁÇπ„ÄÇ‰Ω†Â∞ÜÊääÊâÄÂ≠¶Ëûç‰ºöË¥ØÈÄöÔºå‰∫≤ÊâãÊâìÈÄ†Êô∫ËÉΩÊóÖË°åÂä©Êâã„ÄÅËá™Âä®ÂåñÊ∑±Â∫¶Á†îÁ©∂Êô∫ËÉΩ‰ΩìÔºå‰πÉËá≥‰∏Ä‰∏™Ê®°ÊãüÁ§æ‰ºöÂä®ÊÄÅÁöÑËµõÂçöÂ∞èÈïáÔºåÂú®ÁúüÂÆûÊúâË∂£ÁöÑÈ°πÁõÆ‰∏≠Ê∑¨ÁÇº‰Ω†ÁöÑÊûÑÂª∫ËÉΩÂäõ„ÄÇ&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Á¨¨‰∫îÈÉ®ÂàÜÔºöÊØï‰∏öËÆæËÆ°ÂèäÊú™Êù•Â±ïÊúõ&lt;/strong&gt;ÔºàÁ¨¨ÂçÅÂÖ≠Á´†ÔºâÔºåÂú®ÊóÖÁ®ãÁöÑÁªàÁÇπÔºå‰Ω†Â∞ÜËøéÊù•‰∏Ä‰∏™ÊØï‰∏öËÆæËÆ°ÔºåÊûÑÂª∫‰∏Ä‰∏™ÂÆåÊï¥ÁöÑ„ÄÅÂ±û‰∫é‰Ω†Ëá™Â∑±ÁöÑÂ§öÊô∫ËÉΩ‰ΩìÂ∫îÁî®ÔºåÂÖ®Èù¢Ê£ÄÈ™å‰Ω†ÁöÑÂ≠¶‰π†ÊàêÊûú„ÄÇÊàë‰ª¨ËøòÂ∞Ü‰∏é‰Ω†‰∏ÄÂêåÂ±ïÊúõÊô∫ËÉΩ‰ΩìÁöÑÊú™Êù•ÔºåÊé¢Á¥¢ÊøÄÂä®‰∫∫ÂøÉÁöÑÂâçÊ≤øÊñπÂêë„ÄÇ&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;‚ÄÉ‚ÄÉÊô∫ËÉΩ‰ΩìÊòØ‰∏Ä‰∏™È£ûÈÄüÂèëÂ±ï‰∏îÊûÅÂ∫¶‰æùËµñÂÆûË∑µÁöÑÈ¢ÜÂüü„ÄÇ‰∏∫‰∫ÜËé∑ÂæóÊúÄ‰Ω≥ÁöÑÂ≠¶‰π†ÊïàÊûúÔºåÊàë‰ª¨Âú®È°πÁõÆÁöÑ&lt;code&gt;code&lt;/code&gt;Êñá‰ª∂Â§πÂÜÖÊèê‰æõ‰∫ÜÈÖçÂ•óÁöÑÂÖ®ÈÉ®‰ª£Á†ÅÔºåÂº∫ÁÉàÂª∫ËÆÆ‰Ω†&lt;strong&gt;Â∞ÜÁêÜËÆ∫‰∏éÂÆûË∑µÁõ∏ÁªìÂêà&lt;/strong&gt;„ÄÇËØ∑Âä°ÂøÖ‰∫≤ÊâãËøêË°å„ÄÅË∞ÉËØïÁîöËá≥‰øÆÊîπÈ°πÁõÆÈáåÊèê‰æõÁöÑÊØè‰∏Ä‰ªΩ‰ª£Á†Å„ÄÇÊ¨¢Ëøé‰Ω†ÈöèÊó∂ÂÖ≥Ê≥® Datawhale ‰ª•ÂèäÂÖ∂‰ªñ Agent Áõ∏ÂÖ≥Á§æÂå∫ÔºåÂΩìÈÅáÂà∞ÈóÆÈ¢òÊó∂Ôºå‰Ω†ÂèØ‰ª•ÈöèÊó∂Âú®Êú¨È°πÁõÆÁöÑ issue Âå∫ÊèêÈóÆ„ÄÇ&lt;/p&gt; 
&lt;p&gt;‚ÄÉ‚ÄÉÁé∞Âú®ÔºåÂáÜÂ§áÂ•ΩËøõÂÖ•Êô∫ËÉΩ‰ΩìÁöÑÂ•áÂ¶ô‰∏ñÁïå‰∫ÜÂêóÔºüËÆ©Êàë‰ª¨Âç≥ÂàªÂêØÁ®ãÔºÅ&lt;/p&gt; 
&lt;h2&gt;‰∏ã‰∏ÄÊ≠•ËßÑÂàí&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;[]Ëã±ÊñáÁâàÊïôÁ®ã&lt;/li&gt; 
 &lt;li&gt;[]ÂèåËØ≠ËßÜÈ¢ëËØæÁ®ã[Ëã±Êñá+‰∏≠Êñá]ÔºàÂ∞Ü‰ºöÊõ¥Âä†ÁªÜËá¥ÔºåÂÆûË∑µËØæÂ∏¶È¢ÜÂ§ßÂÆ∂‰ªéËÆæËÆ°ÊÄùË∑ØÂà∞ÂÆûÊñΩÔºåÊéà‰∫∫‰ª•È±º‰πüÊéà‰∫∫‰ª•Ê∏îÔºâ&lt;/li&gt; 
 &lt;li&gt;[]ÂÖ±ÂàõÁ¨¨16Á´†ÔºàÊâìÈÄ†ÂêÑÁ±ªAgentÂ∫îÁî®,Êõ¥ÊâìÈÄ†AgentÁîüÊÄÅÔºâ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ü§ù Â¶Ç‰ΩïË¥°ÁåÆ&lt;/h2&gt; 
&lt;p&gt;Êàë‰ª¨ÊòØ‰∏Ä‰∏™ÂºÄÊîæÁöÑÂºÄÊ∫êÁ§æÂå∫ÔºåÊ¨¢Ëøé‰ªª‰ΩïÂΩ¢ÂºèÁöÑË¥°ÁåÆÔºÅ&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üêõ &lt;strong&gt;Êä•Âëä Bug&lt;/strong&gt; - ÂèëÁé∞ÂÜÖÂÆπÊàñ‰ª£Á†ÅÈóÆÈ¢òÔºåËØ∑Êèê‰∫§ Issue&lt;/li&gt; 
 &lt;li&gt;üí° &lt;strong&gt;ÊèêÂá∫Âª∫ËÆÆ&lt;/strong&gt; - ÂØπÈ°πÁõÆÊúâÂ•ΩÊÉ≥Ê≥ïÔºåÊ¨¢ËøéÂèëËµ∑ËÆ®ËÆ∫&lt;/li&gt; 
 &lt;li&gt;üìù &lt;strong&gt;ÂÆåÂñÑÂÜÖÂÆπ&lt;/strong&gt; - Â∏ÆÂä©ÊîπËøõÊïôÁ®ãÔºåÊèê‰∫§‰Ω†ÁöÑ Pull Request&lt;/li&gt; 
 &lt;li&gt;‚úçÔ∏è &lt;strong&gt;ÂàÜ‰∫´ÂÆûË∑µ&lt;/strong&gt; - Âú®"Á§æÂå∫Ë¥°ÁåÆÁ≤æÈÄâ"‰∏≠ÂàÜ‰∫´‰Ω†ÁöÑÂ≠¶‰π†Á¨îËÆ∞ÂíåÈ°πÁõÆ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üôè Ëá¥Ë∞¢&lt;/h2&gt; 
&lt;h3&gt;Ê†∏ÂøÉË¥°ÁåÆËÄÖ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/jjyaoao"&gt;ÈôàÊÄùÂ∑û-È°πÁõÆË¥üË¥£‰∫∫&lt;/a&gt; (Datawhale ÊàêÂëò, ÂÖ®ÊñáÂÜô‰ΩúÂíåÊ†°ÂØπ)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/fengju0213"&gt;Â≠ôÈü¨-È°πÁõÆË¥üË¥£‰∫∫&lt;/a&gt; (Datawhale ÊàêÂëò, Á¨¨‰πùÁ´†ÂÜÖÂÆπÂíåÊ†°ÂØπ)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Tsumugii24"&gt;ÂßúËàíÂá°-È°πÁõÆË¥üË¥£‰∫∫&lt;/a&gt;ÔºàDatawhale ÊàêÂëò, Á´†ËäÇ‰π†È¢òËÆæËÆ°ÂíåÊ†°ÂØπÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/HeteroCat"&gt;ÈªÑ‰Ω©Êûó-DatawhaleÊÑèÂêëÊàêÂëò&lt;/a&gt; (Agent ÂºÄÂèëÂ∑•Á®ãÂ∏à, Á¨¨‰∫îÁ´†ÂÜÖÂÆπË¥°ÁåÆËÄÖ)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/fancyboi999"&gt;ÊõæÈë´Ê∞ë-AgentÂ∑•Á®ãÂ∏à&lt;/a&gt; (ÁâõÂÆ¢ÁßëÊäÄ, Á¨¨ÂçÅÂõõÁ´†Ê°à‰æãÂºÄÂèë)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://xinzhongzhu.github.io/"&gt;Êú±‰ø°Âø†-ÊåáÂØº‰∏ìÂÆ∂&lt;/a&gt; (DatawhaleÈ¶ñÂ∏≠ÁßëÂ≠¶ÂÆ∂-ÊµôÊ±üÂ∏àËåÉÂ§ßÂ≠¶Êù≠Â∑û‰∫∫Â∑•Êô∫ËÉΩÁ†îÁ©∂Èô¢ÊïôÊéà)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Extra-Chapter Ë¥°ÁåÆËÄÖ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/WHQAQ11"&gt;WH&lt;/a&gt; (ÂÜÖÂÆπË¥°ÁåÆËÄÖ)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/thunderbolt-fire"&gt;Âë®Â••Êù∞-DWË¥°ÁåÆËÄÖÂõ¢Èòü&lt;/a&gt; (Ë•øÂÆâ‰∫§ÈÄöÂ§ßÂ≠¶, Extra02 ÂÜÖÂÆπË¥°ÁåÆ)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Tasselszcx"&gt;Âº†ÂÆ∏Êó≠-‰∏™‰∫∫ÂºÄÂèëËÄÖ&lt;/a&gt;(Â∏ùÂõΩÁêÜÂ∑•Â≠¶Èô¢, Extra03 ÂÜÖÂÆπË¥°ÁåÆ)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/XiaoMa-PM"&gt;ÈªÑÂÆèÊôó-DWË¥°ÁåÆËÄÖÂõ¢Èòü&lt;/a&gt; (Ê∑±Âú≥Â§ßÂ≠¶, Extra04 ÂÜÖÂÆπË¥°ÁåÆ)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ÁâπÂà´ÊÑüË∞¢&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ÊÑüË∞¢ &lt;a href="https://github.com/Sm1les"&gt;@Sm1les&lt;/a&gt; ÂØπÊú¨È°πÁõÆÁöÑÂ∏ÆÂä©‰∏éÊîØÊåÅ&lt;/li&gt; 
 &lt;li&gt;ÊÑüË∞¢ÊâÄÊúâ‰∏∫Êú¨È°πÁõÆÂÅöÂá∫Ë¥°ÁåÆÁöÑÂºÄÂèëËÄÖ‰ª¨ ‚ù§Ô∏è&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center" style="margin-top: 30px;"&gt; 
 &lt;a href="https://github.com/datawhalechina/Hello-Agents/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=datawhalechina/Hello-Agents" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/images/star-history-20251217.png" alt="Datawhale" width="90%" /&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;‚≠ê Â¶ÇÊûúËøô‰∏™È°πÁõÆÂØπ‰Ω†ÊúâÂ∏ÆÂä©ÔºåËØ∑ÁªôÊàë‰ª¨‰∏Ä‰∏™ StarÔºÅ&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;ÂÖ≥‰∫é Datawhale&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/images/datawhale.png" alt="Datawhale" width="30%" /&gt; 
 &lt;p&gt;Êâ´Êèè‰∫åÁª¥Á†ÅÂÖ≥Ê≥® Datawhale ÂÖ¨‰ºóÂè∑ÔºåËé∑ÂèñÊõ¥Â§ö‰ºòË¥®ÂºÄÊ∫êÂÜÖÂÆπ&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìú ÂºÄÊ∫êÂçèËÆÆ&lt;/h2&gt; 
&lt;p&gt;Êú¨‰ΩúÂìÅÈááÁî®&lt;a href="http://creativecommons.org/licenses/by-nc-sa/4.0/"&gt;Áü•ËØÜÂÖ±‰∫´ÁΩ≤Âêç-ÈùûÂïÜ‰∏öÊÄß‰ΩøÁî®-Áõ∏ÂêåÊñπÂºèÂÖ±‰∫´ 4.0 ÂõΩÈôÖËÆ∏ÂèØÂçèËÆÆ&lt;/a&gt;ËøõË°åËÆ∏ÂèØ„ÄÇ&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>zhaochenyang20/Awesome-ML-SYS-Tutorial</title>
      <link>https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial</link>
      <description>&lt;p&gt;My learning notes for ML SYS.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Awesome-ML-SYS-Tutorial&lt;/h1&gt; 
&lt;h2&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/README.md"&gt;English Version&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/README-cn.md"&gt;Chinese Version&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;My learning notes for ML SYS.&lt;/p&gt; 
&lt;p&gt;I've been writing this blog series intermittently for over a year now, and it's almost become an RL Infra Learning Note üòÇ&lt;/p&gt; 
&lt;p&gt;I often see discussions about whether ML SYS or AI Infra is worth getting into, and how to start. Everyone's choice is different. For me, I simply want to &lt;strong&gt;pursue the truth in algorithms&lt;/strong&gt;:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;A large number of RL conclusions derived from papers are based on RL infrastructure in the open-source community that may be extremely flawed. I've been involved in RL infra development for over a year, and I've seen numerous community experts diligently working, but the fact is that RL infra, whether open-source or within major companies, still has many problems. It is absolutely worth questioning whether the high-level conclusions drawn from this flawed infrastructure are correct. When I was reviewing for ICLR this year, I often asked the papers assigned to me, "If the framework you are using has implementation issues itself, can your conclusions still hold?" Although I never deducted points for this reason, no one could provide an answer that resolved my fundamental doubt.&lt;/p&gt; 
 &lt;p&gt;Therefore, some excellent researchers I know are keen to participate in infra development, spending most of their time on foundational work to rigorously ensure that the algorithm they plan to develop next has a correct basis. I greatly admire them and agree with such rigor‚Äîthey are my role models. The same is true for our SGLang RL community. With so much human power and time, we all hope to provide the most correct and concise RL foundation possible, whether it's for companies training models or researchers developing new algorithms, with the goal of genuinely serving everyone in the community. Thank you for your recognition, and I look forward to hearing from interested friends who wish to contact me and join us!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;After a year of going around in circles, this is the resolve that keeps me going in Infra: &lt;strong&gt;to make a contribution to the community by building a correct foundation, thereby helping to ensure correct conclusions.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Coming back to the topic, this series of podcasts started in August 2024, when I began learning ML SYS notes following the opportunity to use &lt;a href="https://github.com/sgl-project/sglang"&gt;SGLang&lt;/a&gt; during my research. It's largely written by me, with content focusing on &lt;strong&gt;RL infra, online/offline inference systems, and some fundamentals of AI Infra&lt;/strong&gt;. Over the past year, starting from two or three articles and thirty to fifty Github Stars, to now exceeding 4.5K Stars, I have become a minor technical influencer. I am deeply honored and grateful for the support.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;I would like to thank my advisors, Professor Quanquan Gu, Dr. Ying Sheng, and Dr. Linmin Zheng&lt;/strong&gt;, for the immense help and guidance they gave me in my study of AI Infra, career choices, and life path. Although I am no longer pursuing a Ph.D. at UCLA due to personal reasons, this journey after my undergraduate graduation has been an incredibly valuable experience. I have now joined RadixArk full-time, continuing my research in RL Infra. We will continue to share AI Infra-related technology and thoughts through my blog, via unofficial channels. &lt;strong&gt;I also hope readers interested in AI Infra reach out to us, join the SGLang open-source community, and together build open-source AI Infra that changes the world and is worth being proud of for a lifetime!&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;RLHF System Development Notes&lt;/h2&gt; 
&lt;h3&gt;slime Framework&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/slime/mismatch/blog-en.md"&gt;Achieving Speed and Accuracy: A Comprehensive Solution to Train-Inference Mismatch in RL&lt;/a&gt;: Introduces two solutions provided by the slime framework for the train-inference mismatch problem: achieving perfect True On-Policy training through kernel-level alignment, and mitigating the mismatch using algorithms like TIS/MIS. Also available in &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/slime/mismatch/blog-cn.md"&gt;Chinese version&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/slime/fsdp/readme_en.md"&gt;Support FSDP2 as A Training Backend for slime&lt;/a&gt;: Added FSDP as a training backend to slime, and aligned it with Megatron. FSDP is more flexible in supporting models with architectural innovations like Qwen3-Next/gpt-oss and helps us further support VLM RL. Also available in &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/slime/fsdp/readme.md"&gt;Chinese version&lt;/a&gt; and on &lt;a href="https://zhuanlan.zhihu.com/p/1979141713449742500"&gt;Zhihu&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/slime/fp8/readme_en.md"&gt;Unified FP8: Moving Beyond Mixed Precision for Stable and Accelerated MoE RL&lt;/a&gt;: Fully utilizing FP8 for both sampling (Rollout) and training (Training) in RL. Also available in &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/slime/fp8/readme.md"&gt;Chinese version&lt;/a&gt; and on &lt;a href="https://zhuanlan.zhihu.com/p/1974681194017865986"&gt;Zhihu&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/slime/spec/readme-en.md"&gt;Power Up Speculative Decoding In Reinforcement Learning&lt;/a&gt;: Introduces speculative decoding into the RL sampling process, significantly boosting sampling speed when the batch size is appropriate; moreover, the draft model is updated during training. Compared to freezing the draft model, the accepted length remains consistently high, yielding long-term stable positive returns. Also available in &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/slime/spec/readme.md"&gt;Chinese version&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/slime/code-walk-through/readme_en.md"&gt;An In-Depth Look at the Elegant Design and Source Code of the slime RL Framework&lt;/a&gt;: slime source code appreciation. Also available on &lt;a href="https://zhuanlan.zhihu.com/p/1946402397409740613"&gt;Zhihu&lt;/a&gt; and in &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/slime/code-walk-through/readme.md"&gt;Chinese version&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[Pending Review] &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/slime/fsdp/release_log/setup_fsdp.md"&gt;slime FSDP Setup Guide&lt;/a&gt;: Records how to test FSDP on slime, including H-cards and B-cards, and both Colocate and Disaggregated placement methods.&lt;/li&gt; 
 &lt;li&gt;[Pending Review] &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/slime/batch-GAE/ppo-gae-chunk.md"&gt;Chunked Parallel Computation of GAE in PPO (slime Implementation)&lt;/a&gt;: Rewrites the standard backward recurrence of GAE into chunk-based parallel prefix scanning, significantly mitigating the GAE computation bottleneck in long sequence scenarios, achieving about $100\times‚Äì300\times$ acceleration in slime. Also available on &lt;a href="https://zhuanlan.zhihu.com/p/1975237289425798560"&gt;Zhihu&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;AReal Framework&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/areal/code-walk-through_EN.md"&gt;AReal Code Walk Through&lt;/a&gt; AReal source code appreciation. Also available on &lt;a href="https://zhuanlan.zhihu.com/p/1983417813080236770"&gt;Zhihu&lt;/a&gt; and in &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/areal/code-walk-through_CN.md"&gt;Chinese version&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;verl Framework&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/torch/mem-snapshot/readme-en.md"&gt;Analyzing VLM RL Training Memory Leaks via Torch Memory Snapshot&lt;/a&gt;: Analysis of SGLang memory leak issues and solutions. Also available on &lt;a href="https://zhuanlan.zhihu.com/p/1943202817247519535"&gt;Zhihu&lt;/a&gt; and in &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/torch/mem-snapshot/readme.md"&gt;Chinese version&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/sglang/latency-accelerate-for-weight-updates/readme.md"&gt;Latency optimization for weight updates&lt;/a&gt;: A debug process for efficiency. Also available on &lt;a href="https://zhuanlan.zhihu.com/p/9908228168"&gt;Zhihu: A record of optimizing SGLang weight update latency&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/verl/multi-turn/code-walk-through/readme_EN.md"&gt;In-Depth Understanding of verl Source Code (Initialization)&lt;/a&gt;: Also available on &lt;a href="https://zhuanlan.zhihu.com/p/1920751852749849692"&gt;Zhihu&lt;/a&gt; and in &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/verl/multi-turn/code-walk-through/readme.md"&gt;Chinese version&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/verl/multi-turn/code-walk-through/readme-2-EN.md"&gt;In-Depth Understanding of verl Source Code (Rollout)&lt;/a&gt;: Also available on &lt;a href="https://zhuanlan.zhihu.com/p/1923349757566388159"&gt;Zhihu&lt;/a&gt; and in &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/verl/multi-turn/code-walk-through/readme-2.md"&gt;Chinese version&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[Pending Review] &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/verl/multi-turn/code-walk-through/readme-3.md"&gt;In-Depth Understanding of verl Source Code (Make Experience)&lt;/a&gt;: Analysis of the logic for the make experience part in verl.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/verl/multi-turn/code-walk-through/readme-6.md"&gt;AgentLoop Source Code Analysis&lt;/a&gt;: Analysis of the multi-turn RL implementation based on AgentLoop in verl.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/verl/multi-turn/code-walk-through/readme-5-EN.md"&gt;verl Parameter Quick Reference&lt;/a&gt;: Quick reference for verl parameters. Also available on &lt;a href="https://zhuanlan.zhihu.com/p/1925041836998783250"&gt;Zhihu&lt;/a&gt; and in &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/verl/multi-turn/code-walk-through/readme-5.md"&gt;Chinese version&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/verl/multi-turn/fast_tokenization/multiturn_tokenization_and_masking.md"&gt;Analyzing the Complexity of Agentic Multi-Turn Training from a Tokenizer Perspective&lt;/a&gt;: Also available on &lt;a href="https://zhuanlan.zhihu.com/p/1917126584806139373"&gt;Zhihu&lt;/a&gt; and in &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/verl/multi-turn/fast_tokenization/multiturn_tokenization_and_masking_ZH.md"&gt;Chinese version&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[Pending Review] &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/verl/multi-turn/code-walk-through/dapo.md"&gt;DAPO Dynamic Filtering Implementation and Batch Size Analysis&lt;/a&gt;: Exploring how to achieve higher parallelism by padding prompts to a smaller batch size.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/verl/multi-turn/tool_examples/profile_en.md"&gt;Systematic Analysis of Time Consumption in verl Multi-Turn Training&lt;/a&gt;: verl multi-turn interaction and tool call profile analysis. Also available in &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/verl/multi-turn/tool_examples/profile.md"&gt;Chinese version&lt;/a&gt; and on &lt;a href="https://zhuanlan.zhihu.com/p/1929748460212552414"&gt;Zhihu&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/verl/multi-turn/release_log/verl-multiturn-rollout-Release_ZH.md"&gt;SGLang, verl, OpenBMB, and Tsinghua University Team Jointly Open Source: First Support for Multi-Turn Interaction and Tool Calling in Mainstream RLHF Frameworks&lt;/a&gt;: First support for multi-turn interaction and tool calling in mainstream RLHF frameworks. Also available on &lt;a href="https://zhuanlan.zhihu.com/p/1906007821889283171"&gt;Zhihu&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/verl/multi-turn/tool_examples/verl-multiturn-searchR1-like_ZH.md"&gt;Search-R1 &amp;amp; veRL-SGLang: Train LLMs with Multi-Turn RL to Reason and Call a Search Engine&lt;/a&gt;: Integrating the Search-R1 framework into the verl-sglang ecosystem. Also available on &lt;a href="https://zhuanlan.zhihu.com/p/1912156329751081620"&gt;Zhihu&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/verl/server-based/veRL-server-based-rollout.md"&gt;SGLang-veRL Server: From Engine to Server, We Need More Flexible RLHF Rollout Interfaces&lt;/a&gt;: To implement more complex RLHF systems, we are gradually replacing the rollout engine in veRL with a rollout server. Also available on &lt;a href="https://zhuanlan.zhihu.com/p/1890631652486665464"&gt;Zhihu: SGLang-veRL Server&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/verl/readme.md"&gt;HybridFlow veRL Original Paper Analysis&lt;/a&gt;: Principles and implementation of SGLang's hybrid engine. Also available on &lt;a href="https://zhuanlan.zhihu.com/p/24682036412"&gt;Zhihu: HybridFlow veRL Original Paper Analysis&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;OpenRLHF Framework&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/677607581"&gt;Illustrated Series on LLM RLHF: PPO Principles and Source Code Interpretation for Everyone&lt;/a&gt; and &lt;a href="https://zhuanlan.zhihu.com/p/12871616401"&gt;Illustrated Distributed Training Process based on Ray in OpenRLHF&lt;/a&gt;: Excellent RLHF introductory resources by Ms. Mengyuan. After reading, you will have a good understanding of RLHF's computational flow and the OpenRLHF PPO framework. I have also added my own understanding in &lt;a href="https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/tree/main/rlhf/OpenRLHF#rlhf-%E7%9A%84%E8%AE%A1%E7%AE%97%E6%B5%81"&gt;RLHF Computational Flow&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/OpenRLHF/readme.md"&gt;Brief Analysis of the Computational Flow of Post-Training Systems Represented by OpenRLHF&lt;/a&gt;: Further complement to Ms. Mengyuan's article. The Github native rendering is terrible; you might as well look at &lt;a href="https://zhuanlan.zhihu.com/p/16370000391"&gt;Zhihu&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;System Design and Optimization&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/sys-design/readme-1-EN.md"&gt;Deep Thoughts on RL Systems: In-Depth Understanding of Weight Update Mechanism&lt;/a&gt;: Summary of half a year's work, in-depth understanding of the weight update mechanism. Also available on &lt;a href="https://zhuanlan.zhihu.com/p/1925210722704531547"&gt;Zhihu&lt;/a&gt; and in &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/sys-design/readme-1.md"&gt;Chinese version&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/sys-design/readme-2-en.md"&gt;Deep Thoughts on RL Systems: FSDP Training Backend&lt;/a&gt;: Discusses the principles and implementation of FSDP, and analyzes verl's use of FSDP. Also available on &lt;a href="https://zhuanlan.zhihu.com/p/1929115059113693341"&gt;Zhihu&lt;/a&gt; and in &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/sys-design/readme-2.md"&gt;Chinese version&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[Pending Review] &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/sys-design/readme-3.md"&gt;Deep Thoughts on RL Systems: Megatron&lt;/a&gt;: Brief analysis of Megatron's basic features, focusing on its use in the RL framework.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/OpenRLHF/develop-log.md"&gt;Extending the OpenRLHF Inference Engine&lt;/a&gt;: Development notes on integrating SGLang into OpenRLHF. The entire process was very painful, and there's still an nccl hang error that a DeepSpeed core contributor is currently fixing.&lt;/li&gt; 
 &lt;li&gt;[Pending Review] &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/GRPO/SGLang_GRPO.md"&gt;SGLang as rollout engine of GRPO trainer&lt;/a&gt;: Introduction on how to use SGLang as the inference backend for the GRPO Trainer in TRL. GRPO is a PPO variant that optimizes PPO's memory usage while improving mathematical reasoning capabilities.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Algorithms and Theory&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;[Pending Review] &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/partial-rollout/Learning_to_Reason_under_Off-Policy_Guidance.md"&gt;Learning to Reason under Off-Policy Guidance&lt;/a&gt;: The LUFFY framework uses off-policy assistance for on-policy learning, dynamically balancing imitation and exploration by combining off-policy inference trajectories with on-policy rollouts.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/partial-rollout/readme.md"&gt;Kimi K1.5: Successful Practice of Long Context RL&lt;/a&gt;: Industrial implementation of Long Context RLHF. I have always liked the technical reports from the Kimi team. Also available on &lt;a href="https://zhuanlan.zhihu.com/p/1894282607325344277"&gt;Zhihu: Kimi K1.5: Successful Practice of Long Context RL&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/13211508979"&gt;Rule-based Reward&lt;/a&gt;: Only on Zhihu, a brief write-up. Honestly, I didn't particularly like the original paper, but determined reward is indeed charming.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/16292266518"&gt;SWE-Bench: How to Construct an Excellent Benchmark in the LLM Era&lt;/a&gt;: Reading notes on the SWE-Bench paper. How to construct a good benchmark to provide fine-grained reward for post-training is an eternal and beautiful topic.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/5220718268"&gt;Brief Analysis of Mainstream Alignment Algorithms and the NeMo-Aligner Framework&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;SGLang Learning Notes&lt;/h2&gt; 
&lt;h3&gt;SGLang Diffusion Learning Notes&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/sglang/code-walk-through/sgl_diffusion_en.md"&gt;SGLang Diffusion Code Walk Through&lt;/a&gt;: Basic principles of the diffusion model, and the entire process of a request being handled by SGLang-Diffusion. Also available on &lt;a href="https://zhuanlan.zhihu.com/p/1982441236066480797"&gt;Zhihu&lt;/a&gt; and in &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/sglang/code-walk-through/sgl_diffusion.md"&gt;Chinese version&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Core Architecture and Optimization&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/sglang/code-walk-through/readme.md"&gt;SGLang Code Walk Through&lt;/a&gt;: The entire process of a request being handled by the SGLang Engine. Some parts are unfinished, but most are okay and have served as a starting point for many SGLang beginners. &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/sglang/code-walk-through/readme-CN.md"&gt;Chinese version is here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/sglang/sglang-worker/readme.md"&gt;Walk Through SGLang / VLLM Worker&lt;/a&gt;: Incomplete analysis of SGLang code. Also available on &lt;a href="https://zhuanlan.zhihu.com/p/6363614076"&gt;Walk Through SGLang / VLLM Worker&lt;/a&gt;. We also thoughtfully provide an &lt;a href="https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/raw/main/sglang/sglang-worker/readme.md"&gt;English version&lt;/a&gt;. For a more detailed analysis, refer to &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/sglang/code-walk-through/readme.md"&gt;SGLang Code Walk Through&lt;/a&gt;; this one is just supplementary.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/sglang/sglang-scheduler/readme-CN.md"&gt;Walk Through SGLang Scheduler&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[Pending Review] &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/sglang/scheduler-evolution/SGLang%20Scheduler%20%E6%8A%80%E6%9C%AF%E5%8F%98%E8%BF%81.md"&gt;SGLang Scheduler Evolution&lt;/a&gt;: Detailed introduction to the technical evolution of the SGLang Scheduler from serial to CPU/GPU overlap, and related components, comparing the previous overlap Scheduler with the current one introducing multiple CUDA streams and FutureMap. Can be viewed on &lt;a href="https://zhuanlan.zhihu.com/p/1969077475129688722"&gt;Zhihu article&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[Pending Review] &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/sglang/kvcache-code-walk-through/readme.md"&gt;KV Cache Code Walkthrough&lt;/a&gt;: Overview of KV cache management implementation, starting from the Scheduler component, detailing the update process of KV cache and memory pool during prefill and decode stages.&lt;/li&gt; 
 &lt;li&gt;[Pending Review] &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/sglang/code-walk-through/multimodal_request_lifecycle.md"&gt;SGLang Multimodal Request Lifecycle: A Deep Architectural Analysis with Qwen2.5-VL as an Example&lt;/a&gt;: Provides a detailed analysis of the multimodal request processing flow within the SGLang framework, using Qwen2.5-VL as a reference model.&lt;/li&gt; 
 &lt;li&gt;[Pending Review] &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/sglang/how-model-is-loaded/readme.md"&gt;How A Model is Loaded in Hugging Face and SGLang&lt;/a&gt;: Documents the process of loading models in Hugging Face and SGLang to help understand the weight loading mechanism.&lt;/li&gt; 
 &lt;li&gt;[Pending Review] &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/sglang/speculative-decoding/speculative-decoding.md"&gt;Speculative Decoding&lt;/a&gt;: Introduces the speculative decoding optimization technique, which uses a smaller draft model to predict the next $K$ tokens, achieving up to $K$-fold acceleration.&lt;/li&gt; 
 &lt;li&gt;[Pending Review] &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/sglang/zero-overhead-scheduler/zero-overhead-batch-scheduler.md"&gt;Zero-Overhead Batch Scheduler&lt;/a&gt;: Introduces the zero-overhead batch scheduler, which solves the GPU Bubble problem caused by serial execution of CPU scheduling and GPU computation in traditional inference systems.&lt;/li&gt; 
 &lt;li&gt;[Pending Review] &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/sglang/dp-attention/readme.md"&gt;Data Parallelism Attention&lt;/a&gt;: Detailed introduction to the principles and implementation of DP Attention, specifically for models like DeepSeek that use MLA and only have one KV head, to avoid KV cache duplication caused by tensor parallelism.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/sglang/quantization/quantization_architecture_en.md"&gt;Brief Analysis of SGLang Framework's Quantization Design and Ideas&lt;/a&gt;: Also available on &lt;a href="https://zhuanlan.zhihu.com/p/1971183020338832111"&gt;Zhihu: Brief Analysis of SGLang Framework's Quantization Design and Ideas&lt;/a&gt; and in &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/sglang/quantization/quantization_architecture.md"&gt;Chinese version&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/sglang/constraint-decoding/readme.md"&gt;Constraint Decoding: Concepts, Methods, and Optimization&lt;/a&gt;: Also available on &lt;a href="https://zhuanlan.zhihu.com/p/18336995950"&gt;Zhihu: Understanding Constraint Decoding: Concepts, Methods, and Optimization in one article&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[Pending Review] &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/sglang/online-update-weights/readme.md"&gt;Online Update Weights&lt;/a&gt;: Introduction to the implementation of the &lt;code&gt;online_update_weights&lt;/code&gt; interface in SGLang. Unlike &lt;code&gt;update_weights&lt;/code&gt; which reads weights from the disk, this interface broadcasts new weights directly from the training engine via NCCL.&lt;/li&gt; 
 &lt;li&gt;[Pending Review] &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/sglang/sglang-verl-engine/readme.md"&gt;SGLang Verl Engine Optimization Analysis&lt;/a&gt;: Analysis of optimizations in the SGLang verl engine, including the implementation of interfaces like &lt;code&gt;update_weights_from_tensor&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/sglang/latency-accelerate-for-weight-updates/readme-CN.md"&gt;Latency Accelerate For Weight Updates&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[üî• Related Debugging] &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/torch/mem-snapshot/readme-en.md"&gt;Analyzing VLM RL Training Memory Leaks via Torch Memory Snapshot&lt;/a&gt;&lt;/strong&gt;: Analysis of SGLang memory leak issues and solutions. Also available on &lt;a href="https://zhuanlan.zhihu.com/p/1943202817247519535"&gt;Zhihu&lt;/a&gt; and in &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/torch/mem-snapshot/readme.md"&gt;Chinese version&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Usage and Practice&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;[Pending Review] &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/sglang/qwen/coder.md"&gt;Qwen3-Coder Usage&lt;/a&gt;: Introduction to using Qwen3-coder in SGLang, including the use of tool-parser.&lt;/li&gt; 
 &lt;li&gt;[Pending Review] &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/sglang/nvidia-dynamo/dynamo.md"&gt;NVIDIA Dynamo&lt;/a&gt;: Introduction to NVIDIA Dynamo, a high-throughput, low-latency inference framework designed for generative AI and inference model serving in multi-node distributed environments.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/9912733791"&gt;Viewing HuggingFace Model Structure&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/716543182"&gt;SGLang Backend Original Paper Analysis&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/4148050391"&gt;Brief Analysis of the Status Quo of Reward / Embed Model Server Engine&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/714833359"&gt;Newbie Perspective: Experience and Gains from Migrating vllm to SGLang&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/715805386"&gt;Newbie Perspective: Using SGL to Serve Embedding Model&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/715857723"&gt;Newbie Perspective: Using vllm to serve a new Embedding Model&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Scheduling and Routing&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/1711346141"&gt;Mooncake: Carrying the P/D Separation to the End&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/1280567902"&gt;Should Prefill and Decode be Separated onto Different Cards?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/718715866"&gt;Understanding Prefill and Decode Computation Characteristics Based on Chunked Prefill&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/718015016"&gt;ModelServer: A Frontend Distribution System Based on SGLang&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ML System Fundamentals&lt;/h2&gt; 
&lt;h3&gt;Transformers &amp;amp; Model Architecture&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;[Pending Review] &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/transformers/attention/cross_attention_en.md"&gt;Cross-Attention Mechanism in Transformer&lt;/a&gt;: Introduction to the cross-attention mechanism in Transformers, allowing the decoder to access and use relevant information from the encoder. Also available in &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/transformers/attention/cross_attention.md"&gt;Chinese version&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/transformers/special_tokens/special_tokens.md"&gt;Understanding Special Tokens and Chat Templates in One Article&lt;/a&gt;: Also recorded on Zhihu &lt;a href="https://zhuanlan.zhihu.com/p/17052593700"&gt;Understanding Special Tokens and Chat Templates in One Article&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;CUDA &amp;amp; GPU&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/torch/cuda-graph/readme_en.md"&gt;Brief Analysis of CUDA Graph Based on torch-memory-savor&lt;/a&gt;: Also available on &lt;a href="https://zhuanlan.zhihu.com/p/1921726788574360686"&gt;Zhihu: Brief Analysis of CUDA Graph Based on torch-memory-savor&lt;/a&gt; and in &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/torch/cuda-graph/readme.md"&gt;Chinese version&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Distributed Training &amp;amp; Communication&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;[Pending Review] &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/torch/tensor-parallelism/readme.md"&gt;Implementing Tensor Parallelism From Scratch&lt;/a&gt;: Implementation and practice of Tensor Parallelism.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/torch/nccl/readme.md"&gt;NCCL and NVIDIA TOPO&lt;/a&gt;: Introduction to NCCL and NVIDIA GPU detection. Also available on &lt;a href="https://zhuanlan.zhihu.com/p/6160835906"&gt;NCCL and NVIDIA TOPO&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/torch/nccl/readme_en.md"&gt;NCCL and SGLang&lt;/a&gt;: Application of NCCL in SGLang. This is very similar to the Chinese content but includes some additional notes on parallel strategies. I probably won't complete this note and will write a separate one to record parallel strategies.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/torch/torch-distributed/readme.md"&gt;PyTorch Distributed&lt;/a&gt;: Communication practice with &lt;code&gt;torch.distributed&lt;/code&gt;, details on GIL and &lt;code&gt;all_reduce&lt;/code&gt;. This part is also available on &lt;a href="https://zhuanlan.zhihu.com/p/5853094319"&gt;Zhihu: PyTorch Communication Practice&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/178402798"&gt;[Original][In-Depth][PyTorch] DDP Series Part 1: Introductory Tutorial&lt;/a&gt;: Although I didn't fully grasp the DDP content, I used this to learn about GIL and ring all reduce. This step is recorded in the &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/torch/torch-distributed/readme.md#gil"&gt;Postscript of torch-distributed&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.yourmetaverse.cn/deep_learning/199/"&gt;Detailed Explanation of nvidia-smi Command and Some Advanced Tips&lt;/a&gt;: Mainly about network topology; my local results are recorded in the &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/torch/nccl/readme.md#nvlink-%E6%9F%A5%E8%AF%A2"&gt;NCCL section&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Quantization&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/5485556270"&gt;Give me BF16 or Give Me Death: Comprehensive Evaluation of Current Quantization Methods&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/942485319"&gt;AWQ: Model Quantization Should Focus on Activation Values&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Developer Guide&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/engineer/how-to-use-docker/readme_en.md"&gt;How to use docker&lt;/a&gt;: How to use Docker to manage development environments. Please note that to collectively foster a good research environment and prevent others from being annoyed by the baseline "it runs on my machine," learning Docker is essential for everyone. We also have a &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/engineer/how-to-use-docker/readme.md"&gt;Chinese version&lt;/a&gt; and &lt;a href="https://zhuanlan.zhihu.com/p/1916764175230801287"&gt;Zhihu&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/engineer/uv/readme.md"&gt;Setting up a Clean Development Environment&lt;/a&gt;: Setting up a clean development environment. Also available on &lt;a href="https://zhuanlan.zhihu.com/p/23440683394"&gt;Zhihu: Setting up a Clean Development Environment&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/2382351079"&gt;Compiling and Deploying Jupyter Notebooks as Documentation on CI&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>SkyworkAI/SkyReels-V2</title>
      <link>https://github.com/SkyworkAI/SkyReels-V2</link>
      <description>&lt;p&gt;SkyReels-V2: Infinite-length Film Generative model&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/assets/logo2.png" alt="SkyReels Logo" width="50%" /&gt; &lt;/p&gt; 
&lt;h1 align="center"&gt;SkyReels V2: Infinite-Length Film Generative Model&lt;/h1&gt; 
&lt;p align="center"&gt; üìë &lt;a href="https://arxiv.org/pdf/2504.13074"&gt;Technical Report&lt;/a&gt; ¬∑ üëã &lt;a href="https://www.skyreels.ai/home?utm_campaign=github_SkyReels_V2" target="_blank"&gt;Playground&lt;/a&gt; ¬∑ üí¨ &lt;a href="https://discord.gg/PwM6NYtccQ" target="_blank"&gt;Discord&lt;/a&gt; ¬∑ ü§ó &lt;a href="https://huggingface.co/collections/Skywork/skyreels-v2-6801b1b93df627d441d0d0d9" target="_blank"&gt;Hugging Face&lt;/a&gt; ¬∑ ü§ñ &lt;a href="https://www.modelscope.cn/collections/SkyReels-V2-f665650130b144" target="_blank"&gt;ModelScope&lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;Welcome to the &lt;strong&gt;SkyReels V2&lt;/strong&gt; repository! Here, you'll find the model weights and inference code for our infinite-length film generative models. To the best of our knowledge, it represents the first open-source video generative model employing &lt;strong&gt;AutoRegressive Diffusion-Forcing architecture&lt;/strong&gt; that achieves the &lt;strong&gt;SOTA performance&lt;/strong&gt; among publicly available models.&lt;/p&gt; 
&lt;h2&gt;üî•üî•üî• News!!&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Jun 1, 2025: üéâ We published the technical report, &lt;a href="https://arxiv.org/pdf/2506.00830"&gt;SkyReels-Audio: Omni Audio-Conditioned Talking Portraits in Video Diffusion Transformers&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;May 16, 2025: üî• We release the inference code for &lt;a href="https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#ve"&gt;video extension&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#se"&gt;start/end frame control&lt;/a&gt; in diffusion forcing model.&lt;/li&gt; 
 &lt;li&gt;Apr 24, 2025: üî• We release the 720P models, &lt;a href="https://huggingface.co/Skywork/SkyReels-V2-DF-14B-720P"&gt;SkyReels-V2-DF-14B-720P&lt;/a&gt; and &lt;a href="https://huggingface.co/Skywork/SkyReels-V2-I2V-14B-720P"&gt;SkyReels-V2-I2V-14B-720P&lt;/a&gt;. The former facilitates infinite-length autoregressive video generation, and the latter focuses on Image2Video synthesis.&lt;/li&gt; 
 &lt;li&gt;Apr 21, 2025: üëã We release the inference code and model weights of &lt;a href="https://huggingface.co/collections/Skywork/skyreels-v2-6801b1b93df627d441d0d0d9"&gt;SkyReels-V2&lt;/a&gt; Series Models and the video captioning model &lt;a href="https://huggingface.co/Skywork/SkyCaptioner-V1"&gt;SkyCaptioner-V1&lt;/a&gt; .&lt;/li&gt; 
 &lt;li&gt;Apr 3, 2025: üî• We also release &lt;a href="https://github.com/SkyworkAI/SkyReels-A2"&gt;SkyReels-A2&lt;/a&gt;. This is an open-sourced controllable video generation framework capable of assembling arbitrary visual elements.&lt;/li&gt; 
 &lt;li&gt;Feb 18, 2025: üî• we released &lt;a href="https://github.com/SkyworkAI/SkyReels-A1"&gt;SkyReels-A1&lt;/a&gt;. This is an open-sourced and effective framework for portrait image animation.&lt;/li&gt; 
 &lt;li&gt;Feb 18, 2025: üî• We released &lt;a href="https://github.com/SkyworkAI/SkyReels-V1"&gt;SkyReels-V1&lt;/a&gt;. This is the first and most advanced open-source human-centric video foundation model.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üé• Demos&lt;/h2&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td align="center"&gt; 
    &lt;video src="https://github.com/user-attachments/assets/f6f9f9a7-5d5f-433c-9d73-d8d593b7ad25" width="100%"&gt;&lt;/video&gt; &lt;/td&gt; 
   &lt;td align="center"&gt; 
    &lt;video src="https://github.com/user-attachments/assets/0eb13415-f4d9-4aaf-bcd3-3031851109b9" width="100%"&gt;&lt;/video&gt; &lt;/td&gt; 
   &lt;td align="center"&gt; 
    &lt;video src="https://github.com/user-attachments/assets/dcd16603-5bf4-4786-8e4d-1ed23889d07a" width="100%"&gt;&lt;/video&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; The demos above showcase 30-second videos generated using our SkyReels-V2 Diffusion Forcing model. 
&lt;h2&gt;üìë TODO List&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://arxiv.org/pdf/2504.13074"&gt;Technical Report&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Checkpoints of the 14B and 1.3B Models Series&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Single-GPU &amp;amp; Multi-GPU Inference Code&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://huggingface.co/Skywork/SkyCaptioner-V1"&gt;SkyCaptioner-V1&lt;/a&gt;: A Video Captioning Model&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Prompt Enhancer&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Diffusers integration&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Checkpoints of the 5B Models Series&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Checkpoints of the Camera Director Models&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Checkpoints of the Step &amp;amp; Guidance Distill Model&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üöÄ Quickstart&lt;/h2&gt; 
&lt;h4&gt;Installation&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# clone the repository.
git clone https://github.com/SkyworkAI/SkyReels-V2
cd SkyReels-V2
# Install dependencies. Test environment uses Python 3.10.12.
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Model Download&lt;/h4&gt; 
&lt;p&gt;You can download our models from Hugging Face:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Model Variant&lt;/th&gt; 
   &lt;th&gt;Recommended Height/Width/Frame&lt;/th&gt; 
   &lt;th&gt;Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="5"&gt;Diffusion Forcing&lt;/td&gt; 
   &lt;td&gt;1.3B-540P&lt;/td&gt; 
   &lt;td&gt;544 * 960 * 97f&lt;/td&gt; 
   &lt;td&gt;ü§ó &lt;a href="https://huggingface.co/Skywork/SkyReels-V2-DF-1.3B-540P"&gt;Huggingface&lt;/a&gt; ü§ñ &lt;a href="https://www.modelscope.cn/models/Skywork/SkyReels-V2-DF-1.3B-540P"&gt;ModelScope&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5B-540P&lt;/td&gt; 
   &lt;td&gt;544 * 960 * 97f&lt;/td&gt; 
   &lt;td&gt;Coming Soon&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5B-720P&lt;/td&gt; 
   &lt;td&gt;720 * 1280 * 121f&lt;/td&gt; 
   &lt;td&gt;Coming Soon&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;14B-540P&lt;/td&gt; 
   &lt;td&gt;544 * 960 * 97f&lt;/td&gt; 
   &lt;td&gt;ü§ó &lt;a href="https://huggingface.co/Skywork/SkyReels-V2-DF-14B-540P"&gt;Huggingface&lt;/a&gt; ü§ñ &lt;a href="https://www.modelscope.cn/models/Skywork/SkyReels-V2-DF-14B-540P"&gt;ModelScope&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;14B-720P&lt;/td&gt; 
   &lt;td&gt;720 * 1280 * 121f&lt;/td&gt; 
   &lt;td&gt;ü§ó &lt;a href="https://huggingface.co/Skywork/SkyReels-V2-DF-14B-720P"&gt;Huggingface&lt;/a&gt; ü§ñ &lt;a href="https://www.modelscope.cn/models/Skywork/SkyReels-V2-DF-14B-720P"&gt;ModelScope&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="5"&gt;Text-to-Video&lt;/td&gt; 
   &lt;td&gt;1.3B-540P&lt;/td&gt; 
   &lt;td&gt;544 * 960 * 97f&lt;/td&gt; 
   &lt;td&gt;Coming Soon&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5B-540P&lt;/td&gt; 
   &lt;td&gt;544 * 960 * 97f&lt;/td&gt; 
   &lt;td&gt;Coming Soon&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5B-720P&lt;/td&gt; 
   &lt;td&gt;720 * 1280 * 121f&lt;/td&gt; 
   &lt;td&gt;Coming Soon&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;14B-540P&lt;/td&gt; 
   &lt;td&gt;544 * 960 * 97f&lt;/td&gt; 
   &lt;td&gt;ü§ó &lt;a href="https://huggingface.co/Skywork/SkyReels-V2-T2V-14B-540P"&gt;Huggingface&lt;/a&gt; ü§ñ &lt;a href="https://www.modelscope.cn/models/Skywork/SkyReels-V2-T2V-14B-540P"&gt;ModelScope&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;14B-720P&lt;/td&gt; 
   &lt;td&gt;720 * 1280 * 121f&lt;/td&gt; 
   &lt;td&gt;ü§ó &lt;a href="https://huggingface.co/Skywork/SkyReels-V2-T2V-14B-720P"&gt;Huggingface&lt;/a&gt; ü§ñ &lt;a href="https://www.modelscope.cn/models/Skywork/SkyReels-V2-T2V-14B-720P"&gt;ModelScope&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="5"&gt;Image-to-Video&lt;/td&gt; 
   &lt;td&gt;1.3B-540P&lt;/td&gt; 
   &lt;td&gt;544 * 960 * 97f&lt;/td&gt; 
   &lt;td&gt;ü§ó &lt;a href="https://huggingface.co/Skywork/SkyReels-V2-I2V-1.3B-540P"&gt;Huggingface&lt;/a&gt; ü§ñ &lt;a href="https://www.modelscope.cn/models/Skywork/SkyReels-V2-I2V-1.3B-540P"&gt;ModelScope&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5B-540P&lt;/td&gt; 
   &lt;td&gt;544 * 960 * 97f&lt;/td&gt; 
   &lt;td&gt;Coming Soon&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5B-720P&lt;/td&gt; 
   &lt;td&gt;720 * 1280 * 121f&lt;/td&gt; 
   &lt;td&gt;Coming Soon&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;14B-540P&lt;/td&gt; 
   &lt;td&gt;544 * 960 * 97f&lt;/td&gt; 
   &lt;td&gt;ü§ó &lt;a href="https://huggingface.co/Skywork/SkyReels-V2-I2V-14B-540P"&gt;Huggingface&lt;/a&gt; ü§ñ &lt;a href="https://www.modelscope.cn/models/Skywork/SkyReels-V2-I2V-14B-540P"&gt;ModelScope&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;14B-720P&lt;/td&gt; 
   &lt;td&gt;720 * 1280 * 121f&lt;/td&gt; 
   &lt;td&gt;ü§ó &lt;a href="https://huggingface.co/Skywork/SkyReels-V2-I2V-14B-720P"&gt;Huggingface&lt;/a&gt; ü§ñ &lt;a href="https://www.modelscope.cn/models/Skywork/SkyReels-V2-I2V-14B-720P"&gt;ModelScope&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="3"&gt;Camera Director&lt;/td&gt; 
   &lt;td&gt;5B-540P&lt;/td&gt; 
   &lt;td&gt;544 * 960 * 97f&lt;/td&gt; 
   &lt;td&gt;Coming Soon&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5B-720P&lt;/td&gt; 
   &lt;td&gt;720 * 1280 * 121f&lt;/td&gt; 
   &lt;td&gt;Coming Soon&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;14B-720P&lt;/td&gt; 
   &lt;td&gt;720 * 1280 * 121f&lt;/td&gt; 
   &lt;td&gt;Coming Soon&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;After downloading, set the model path in your generation commands:&lt;/p&gt; 
&lt;h4&gt;Single GPU Inference&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Diffusion Forcing for Long Video Generation&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The &lt;a href="https://arxiv.org/abs/2407.01392"&gt;&lt;strong&gt;Diffusion Forcing&lt;/strong&gt;&lt;/a&gt; version model allows us to generate Infinite-Length videos. This model supports both &lt;strong&gt;text-to-video (T2V)&lt;/strong&gt; and &lt;strong&gt;image-to-video (I2V)&lt;/strong&gt; tasks, and it can perform inference in both synchronous and asynchronous modes. Here we demonstrate 2 running scripts as examples for long video generation. If you want to adjust the inference parameters, e.g., the duration of video, inference mode, read the Note below first.&lt;/p&gt; 
&lt;p&gt;synchronous generation for 10s video&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;model_id=Skywork/SkyReels-V2-DF-14B-540P
# synchronous inference
python3 generate_video_df.py \
  --model_id ${model_id} \
  --resolution 540P \
  --ar_step 0 \
  --base_num_frames 97 \
  --num_frames 257 \
  --overlap_history 17 \
  --prompt "A graceful white swan with a curved neck and delicate feathers swimming in a serene lake at dawn, its reflection perfectly mirrored in the still water as mist rises from the surface, with the swan occasionally dipping its head into the water to feed." \
  --addnoise_condition 20 \
  --offload \
  --teacache \
  --use_ret_steps \
  --teacache_thresh 0.3
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;asynchronous generation for 30s video&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;model_id=Skywork/SkyReels-V2-DF-14B-540P
# asynchronous inference
python3 generate_video_df.py \
  --model_id ${model_id} \
  --resolution 540P \
  --ar_step 5 \
  --causal_block_size 5 \
  --base_num_frames 97 \
  --num_frames 737 \
  --overlap_history 17 \
  --prompt "A graceful white swan with a curved neck and delicate feathers swimming in a serene lake at dawn, its reflection perfectly mirrored in the still water as mist rises from the surface, with the swan occasionally dipping its head into the water to feed." \
  --addnoise_condition 20 \
  --offload
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Text-to-video with &lt;code&gt;diffusers&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;import torch
from diffusers import AutoModel, SkyReelsV2DiffusionForcingPipeline, UniPCMultistepScheduler
from diffusers.utils import export_to_video

vae = AutoModel.from_pretrained("Skywork/SkyReels-V2-DF-14B-540P-Diffusers", subfolder="vae", torch_dtype=torch.float32)

pipeline = SkyReelsV2DiffusionForcingPipeline.from_pretrained(
    "Skywork/SkyReels-V2-DF-14B-540P-Diffusers",
    vae=vae,
    torch_dtype=torch.bfloat16
)
flow_shift = 8.0  # 8.0 for T2V, 5.0 for I2V
pipeline.scheduler = UniPCMultistepScheduler.from_config(pipeline.scheduler.config, flow_shift=flow_shift)
pipeline = pipeline.to("cuda")

prompt = "A cat and a dog baking a cake together in a kitchen. The cat is carefully measuring flour, while the dog is stirring the batter with a wooden spoon. The kitchen is cozy, with sunlight streaming through the window."

output = pipeline(
    prompt=prompt,
    num_inference_steps=30,
    height=544,  # 720 for 720P
    width=960,   # 1280 for 720P
    num_frames=97,
    base_num_frames=97,  # 121 for 720P
    ar_step=5,  # Controls asynchronous inference (0 for synchronous mode)
    causal_block_size=5,  # Number of frames in each block for asynchronous processing
    overlap_history=None,  # Number of frames to overlap for smooth transitions in long videos; 17 for long video generations
    addnoise_condition=20,  # Improves consistency in long video generation
).frames[0]
export_to_video(output, "T2V.mp4", fps=24, quality=8)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Image-to-video with &lt;code&gt;diffusers&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;import numpy as np
import torch
import torchvision.transforms.functional as TF
from diffusers import AutoencoderKLWan, SkyReelsV2DiffusionForcingImageToVideoPipeline, UniPCMultistepScheduler
from diffusers.utils import export_to_video, load_image

model_id = "Skywork/SkyReels-V2-DF-14B-720P-Diffusers"
vae = AutoencoderKLWan.from_pretrained(model_id, subfolder="vae", torch_dtype=torch.float32)
pipeline = SkyReelsV2DiffusionForcingImageToVideoPipeline.from_pretrained(
    model_id, vae=vae, torch_dtype=torch.bfloat16
)
flow_shift = 5.0  # 8.0 for T2V, 5.0 for I2V
pipeline.scheduler = UniPCMultistepScheduler.from_config(pipeline.scheduler.config, flow_shift=flow_shift)
pipeline.to("cuda")

first_frame = load_image("https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/flf2v_input_first_frame.png")
last_frame = load_image("https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/flf2v_input_last_frame.png")

def aspect_ratio_resize(image, pipeline, max_area=720 * 1280):
    aspect_ratio = image.height / image.width
    mod_value = pipeline.vae_scale_factor_spatial * pipeline.transformer.config.patch_size[1]
    height = round(np.sqrt(max_area * aspect_ratio)) // mod_value * mod_value
    width = round(np.sqrt(max_area / aspect_ratio)) // mod_value * mod_value
    image = image.resize((width, height))
    return image, height, width

def center_crop_resize(image, height, width):
    # Calculate resize ratio to match first frame dimensions
    resize_ratio = max(width / image.width, height / image.height)

    # Resize the image
    width = round(image.width * resize_ratio)
    height = round(image.height * resize_ratio)
    size = [width, height]
    image = TF.center_crop(image, size)

    return image, height, width

first_frame, height, width = aspect_ratio_resize(first_frame, pipeline)
if last_frame.size != first_frame.size:
    last_frame, _, _ = center_crop_resize(last_frame, height, width)

prompt = "CG animation style, a small blue bird takes off from the ground, flapping its wings. The bird's feathers are delicate, with a unique pattern on its chest. The background shows a blue sky with white clouds under bright sunshine. The camera follows the bird upward, capturing its flight and the vastness of the sky from a close-up, low-angle perspective."

output = pipeline(
    image=first_frame, last_image=last_frame, prompt=prompt, height=height, width=width, guidance_scale=5.0
).frames[0]
export_to_video(output, "output.mp4", fps=24, quality=8)
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;If you want to run the &lt;strong&gt;image-to-video (I2V)&lt;/strong&gt; task, add &lt;code&gt;--image ${image_path}&lt;/code&gt; to your command and it is also better to use &lt;strong&gt;text-to-video (T2V)&lt;/strong&gt;-like prompt which includes some descriptions of the first-frame image.&lt;/li&gt; 
  &lt;li&gt;For long video generation, you can just switch the &lt;code&gt;--num_frames&lt;/code&gt;, e.g., &lt;code&gt;--num_frames 257&lt;/code&gt; for 10s video, &lt;code&gt;--num_frames 377&lt;/code&gt; for 15s video, &lt;code&gt;--num_frames 737&lt;/code&gt; for 30s video, &lt;code&gt;--num_frames 1457&lt;/code&gt; for 60s video. The number is not strictly aligned with the logical frame number for specified time duration, but it is aligned with some training parameters, which means it may perform better. When you use asynchronous inference with causal_block_size &amp;gt; 1, the &lt;code&gt;--num_frames&lt;/code&gt; should be carefully set.&lt;/li&gt; 
  &lt;li&gt;You can use &lt;code&gt;--ar_step 5&lt;/code&gt; to enable asynchronous inference. When asynchronous inference, &lt;code&gt;--causal_block_size 5&lt;/code&gt; is recommended while it is not supposed to be set for synchronous generation. REMEMBER that the frame latent number inputted into the model in every iteration, e.g., base frame latent number (e.g., (97-1)//4+1=25 for base_num_frames=97) and (e.g., (237-97-(97-17)x1+17-1)//4+1=20 for base_num_frames=97, num_frames=237, overlap_history=17) for the last iteration, MUST be divided by causal_block_size. If you find it too hard to calculate and set proper values, just use our recommended setting above :). Asynchronous inference will take more steps to diffuse the whole sequence which means it will be SLOWER than synchronous mode. In our experiments, asynchronous inference may improve the instruction following and visual consistent performance.&lt;/li&gt; 
  &lt;li&gt;To reduce peak VRAM, just lower the &lt;code&gt;--base_num_frames&lt;/code&gt;, e.g., to 77 or 57, while keeping the same generative length &lt;code&gt;--num_frames&lt;/code&gt; you want to generate. This may slightly reduce video quality, and it should not be set too small.&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;--addnoise_condition&lt;/code&gt; is used to help smooth the long video generation by adding some noise to the clean condition. Too large noise can cause the inconsistency as well. 20 is a recommended value, and you may try larger ones, but it is recommended to not exceed 50.&lt;/li&gt; 
  &lt;li&gt;Generating a 540P video using the 1.3B model requires approximately 14.7GB peak VRAM, while the same resolution video using the 14B model demands around 51.2GB peak VRAM.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;span id="ve"&gt;Video Extention&lt;/span&gt;&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;model_id=Skywork/SkyReels-V2-DF-14B-540P
# video extention
python3 generate_video_df.py \
  --model_id ${model_id} \
  --resolution 540P \
  --ar_step 0 \
  --base_num_frames 97 \
  --num_frames 120 \
  --overlap_history 17 \
  --prompt ${prompt} \
  --addnoise_condition 20 \
  --offload \
  --use_ret_steps \
  --teacache \
  --teacache_thresh 0.3 \
  --video_path ${video_path}
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;When performing video extension, you need to pass the &lt;code&gt;--video_path ${video_path}&lt;/code&gt; parameter to specify the video to be extended.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;span id="se"&gt;Start/End Frame Control&lt;/span&gt;&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;model_id=Skywork/SkyReels-V2-DF-14B-540P
# start/end frame control
python3 generate_video_df.py \
  --model_id ${model_id} \
  --resolution 540P \
  --ar_step 0 \
  --base_num_frames 97 \
  --num_frames 97 \
  --overlap_history 17 \
  --prompt ${prompt} \
  --addnoise_condition 20 \
  --offload \
  --use_ret_steps \
  --teacache \
  --teacache_thresh 0.3 \
  --image ${image} \
  --end_image ${end_image}
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;When controlling the start and end frames, you need to pass the &lt;code&gt;--image ${image}&lt;/code&gt; parameter to control the generation of the start frame and the &lt;code&gt;--end_image ${end_image}&lt;/code&gt; parameter to control the generation of the end frame.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Video extension with &lt;code&gt;diffusers&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;import numpy as np
import torch
import torchvision.transforms.functional as TF
from diffusers import AutoencoderKLWan, SkyReelsV2DiffusionForcingVideoToVideoPipeline, UniPCMultistepScheduler
from diffusers.utils import export_to_video, load_video

model_id = "Skywork/SkyReels-V2-DF-14B-540P-Diffusers"
vae = AutoencoderKLWan.from_pretrained(model_id, subfolder="vae", torch_dtype=torch.float32)
pipeline = SkyReelsV2DiffusionForcingVideoToVideoPipeline.from_pretrained(
    model_id, vae=vae, torch_dtype=torch.bfloat16
)
flow_shift = 5.0  # 8.0 for T2V, 5.0 for I2V
pipeline.scheduler = UniPCMultistepScheduler.from_config(pipeline.scheduler.config, flow_shift=flow_shift)
pipeline.to("cuda")

video = load_video("input_video.mp4")

prompt = "CG animation style, a small blue bird takes off from the ground, flapping its wings. The bird's feathers are delicate, with a unique pattern on its chest. The background shows a blue sky with white clouds under bright sunshine. The camera follows the bird upward, capturing its flight and the vastness of the sky from a close-up, low-angle perspective."

output = pipeline(
    video=video, prompt=prompt, height=544, width=960, guidance_scale=5.0,
    num_inference_steps=30, num_frames=257, base_num_frames=97#, ar_step=5, causal_block_size=5,
).frames[0]
export_to_video(output, "output.mp4", fps=24, quality=8)
# Total frames will be the number of frames of given video + 257
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Text To Video &amp;amp; Image To Video&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# run Text-to-Video Generation
model_id=Skywork/SkyReels-V2-T2V-14B-540P
python3 generate_video.py \
  --model_id ${model_id} \
  --resolution 540P \
  --num_frames 97 \
  --guidance_scale 6.0 \
  --shift 8.0 \
  --fps 24 \
  --prompt "A serene lake surrounded by towering mountains, with a few swans gracefully gliding across the water and sunlight dancing on the surface." \
  --offload \
  --teacache \
  --use_ret_steps \
  --teacache_thresh 0.3
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;When using an &lt;strong&gt;image-to-video (I2V)&lt;/strong&gt; model, you must provide an input image using the &lt;code&gt;--image ${image_path}&lt;/code&gt; parameter. The &lt;code&gt;--guidance_scale 5.0&lt;/code&gt; and &lt;code&gt;--shift 3.0&lt;/code&gt; is recommended for I2V model.&lt;/li&gt; 
  &lt;li&gt;Generating a 540P video using the 1.3B model requires approximately 14.7GB peak VRAM, while the same resolution video using the 14B model demands around 43.4GB peak VRAM.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;T2V models with &lt;code&gt;diffusers&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;import torch
from diffusers import (
    SkyReelsV2Pipeline,
    UniPCMultistepScheduler,
    AutoencoderKLWan,
)
from diffusers.utils import export_to_video

# Load the pipeline
# Available models:
# - Skywork/SkyReels-V2-T2V-14B-540P-Diffusers
# - Skywork/SkyReels-V2-T2V-14B-720P-Diffusers
vae = AutoencoderKLWan.from_pretrained(
    "Skywork/SkyReels-V2-T2V-14B-720P-Diffusers",
    subfolder="vae",
    torch_dtype=torch.float32,
)
pipe = SkyReelsV2Pipeline.from_pretrained(
    "Skywork/SkyReels-V2-T2V-14B-720P-Diffusers",
    vae=vae,
    torch_dtype=torch.bfloat16,
)
flow_shift = 8.0  # 8.0 for T2V, 5.0 for I2V
pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config, flow_shift=flow_shift)
pipe = pipe.to("cuda")

prompt = "A cat and a dog baking a cake together in a kitchen. The cat is carefully measuring flour, while the dog is stirring the batter with a wooden spoon. The kitchen is cozy, with sunlight streaming through the window."

output = pipe(
    prompt=prompt,
    num_inference_steps=50,
    height=544,
    width=960,
    guidance_scale=6.0,  # 6.0 for T2V, 5.0 for I2V
    num_frames=97,
).frames[0]
export_to_video(output, "video.mp4", fps=24, quality=8)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;I2V models with &lt;code&gt;diffusers&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;import torch
from diffusers import (
    SkyReelsV2ImageToVideoPipeline,
    UniPCMultistepScheduler,
    AutoencoderKLWan,
)
from diffusers.utils import export_to_video
from PIL import Image

# Load the pipeline
# Available models:
# - Skywork/SkyReels-V2-I2V-1.3B-540P-Diffusers
# - Skywork/SkyReels-V2-I2V-14B-540P-Diffusers
# - Skywork/SkyReels-V2-I2V-14B-720P-Diffusers
vae = AutoencoderKLWan.from_pretrained(
    "Skywork/SkyReels-V2-I2V-14B-720P-Diffusers",
    subfolder="vae",
    torch_dtype=torch.float32,
)
pipe = SkyReelsV2ImageToVideoPipeline.from_pretrained(
    "Skywork/SkyReels-V2-I2V-14B-720P-Diffusers",
    vae=vae,
    torch_dtype=torch.bfloat16,
)
flow_shift = 5.0  # 8.0 for T2V, 5.0 for I2V
pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config, flow_shift=flow_shift)
pipe = pipe.to("cuda")

prompt = "A cat and a dog baking a cake together in a kitchen. The cat is carefully measuring flour, while the dog is stirring the batter with a wooden spoon. The kitchen is cozy, with sunlight streaming through the window."
image = Image.open("path/to/image.png")

output = pipe(
    image=image,
    prompt=prompt,
    num_inference_steps=50,
    height=544,
    width=960,
    guidance_scale=5.0,  # 6.0 for T2V, 5.0 for I2V
    num_frames=97,
).frames[0]
export_to_video(output, "video.mp4", fps=24, quality=8)
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Prompt Enhancer&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The prompt enhancer is implemented based on &lt;a href="https://huggingface.co/Qwen/Qwen2.5-32B-Instruct"&gt;Qwen2.5-32B-Instruct&lt;/a&gt; and is utilized via the &lt;code&gt;--prompt_enhancer&lt;/code&gt; parameter. It works ideally for short prompts, while for long prompts, it might generate an excessively lengthy prompt that could lead to over-saturation in the generative video. Note the peak memory of GPU is 64G+ if you use &lt;code&gt;--prompt_enhancer&lt;/code&gt;. If you want to obtain the enhanced prompt separately, you can also run the prompt_enhancer script separately for testing. The steps are as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;cd skyreels_v2_infer/pipelines
python3 prompt_enhancer.py --prompt "A serene lake surrounded by towering mountains, with a few swans gracefully gliding across the water and sunlight dancing on the surface."
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;--prompt_enhancer&lt;/code&gt; is not allowed if using &lt;code&gt;--use_usp&lt;/code&gt;. We recommend running the skyreels_v2_infer/pipelines/prompt_enhancer.py script first to generate enhanced prompt before enabling the &lt;code&gt;--use_usp&lt;/code&gt; parameter.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Advanced Configuration Options&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Below are the key parameters you can customize for video generation:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Parameter&lt;/th&gt; 
   &lt;th align="center"&gt;Recommended Value&lt;/th&gt; 
   &lt;th align="center"&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--prompt&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;Text description for generating your video&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--image&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;Path to input image for image-to-video generation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--resolution&lt;/td&gt; 
   &lt;td align="center"&gt;540P or 720P&lt;/td&gt; 
   &lt;td align="center"&gt;Output video resolution (select based on model type)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--num_frames&lt;/td&gt; 
   &lt;td align="center"&gt;97 or 121&lt;/td&gt; 
   &lt;td align="center"&gt;Total frames to generate (&lt;strong&gt;97 for 540P models&lt;/strong&gt;, &lt;strong&gt;121 for 720P models&lt;/strong&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--inference_steps&lt;/td&gt; 
   &lt;td align="center"&gt;50&lt;/td&gt; 
   &lt;td align="center"&gt;Number of denoising steps&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--fps&lt;/td&gt; 
   &lt;td align="center"&gt;24&lt;/td&gt; 
   &lt;td align="center"&gt;Frames per second in the output video&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--shift&lt;/td&gt; 
   &lt;td align="center"&gt;8.0 or 5.0&lt;/td&gt; 
   &lt;td align="center"&gt;Flow matching scheduler parameter (&lt;strong&gt;8.0 for T2V&lt;/strong&gt;, &lt;strong&gt;5.0 for I2V&lt;/strong&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--guidance_scale&lt;/td&gt; 
   &lt;td align="center"&gt;6.0 or 5.0&lt;/td&gt; 
   &lt;td align="center"&gt;Controls text adherence strength (&lt;strong&gt;6.0 for T2V&lt;/strong&gt;, &lt;strong&gt;5.0 for I2V&lt;/strong&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--seed&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;Fixed seed for reproducible results (omit for random generation)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--offload&lt;/td&gt; 
   &lt;td align="center"&gt;True&lt;/td&gt; 
   &lt;td align="center"&gt;Offloads model components to CPU to reduce VRAM usage (recommended)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--use_usp&lt;/td&gt; 
   &lt;td align="center"&gt;True&lt;/td&gt; 
   &lt;td align="center"&gt;Enables multi-GPU acceleration with xDiT USP&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--outdir&lt;/td&gt; 
   &lt;td align="center"&gt;./video_out&lt;/td&gt; 
   &lt;td align="center"&gt;Directory where generated videos will be saved&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--prompt_enhancer&lt;/td&gt; 
   &lt;td align="center"&gt;True&lt;/td&gt; 
   &lt;td align="center"&gt;Expand the prompt into a more detailed description&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--teacache&lt;/td&gt; 
   &lt;td align="center"&gt;False&lt;/td&gt; 
   &lt;td align="center"&gt;Enables teacache for faster inference&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--teacache_thresh&lt;/td&gt; 
   &lt;td align="center"&gt;0.2&lt;/td&gt; 
   &lt;td align="center"&gt;Higher speedup will cause to worse quality&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--use_ret_steps&lt;/td&gt; 
   &lt;td align="center"&gt;False&lt;/td&gt; 
   &lt;td align="center"&gt;Retention Steps for teacache&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Diffusion Forcing Additional Parameters&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Parameter&lt;/th&gt; 
   &lt;th align="center"&gt;Recommended Value&lt;/th&gt; 
   &lt;th align="center"&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--ar_step&lt;/td&gt; 
   &lt;td align="center"&gt;0&lt;/td&gt; 
   &lt;td align="center"&gt;Controls asynchronous inference (0 for synchronous mode)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--base_num_frames&lt;/td&gt; 
   &lt;td align="center"&gt;97 or 121&lt;/td&gt; 
   &lt;td align="center"&gt;Base frame count (&lt;strong&gt;97 for 540P&lt;/strong&gt;, &lt;strong&gt;121 for 720P&lt;/strong&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--overlap_history&lt;/td&gt; 
   &lt;td align="center"&gt;17&lt;/td&gt; 
   &lt;td align="center"&gt;Number of frames to overlap for smooth transitions in long videos&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--addnoise_condition&lt;/td&gt; 
   &lt;td align="center"&gt;20&lt;/td&gt; 
   &lt;td align="center"&gt;Improves consistency in long video generation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--causal_block_size&lt;/td&gt; 
   &lt;td align="center"&gt;5&lt;/td&gt; 
   &lt;td align="center"&gt;Recommended when using asynchronous inference (--ar_step &amp;gt; 0)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--video_path&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;Path to input video for video extension&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--end_image&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;Path to input image for end frame control&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h4&gt;Multi-GPU inference using xDiT USP&lt;/h4&gt; 
&lt;p&gt;We use &lt;a href="https://github.com/xdit-project/xDiT"&gt;xDiT&lt;/a&gt; USP to accelerate inference. For example, to generate a video with 2 GPUs, you can use the following command:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Diffusion Forcing&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;model_id=Skywork/SkyReels-V2-DF-14B-540P
# diffusion forcing synchronous inference
torchrun --nproc_per_node=2 generate_video_df.py \
  --model_id ${model_id} \
  --resolution 540P \
  --ar_step 0 \
  --base_num_frames 97 \
  --num_frames 257 \
  --overlap_history 17 \
  --prompt "A graceful white swan with a curved neck and delicate feathers swimming in a serene lake at dawn, its reflection perfectly mirrored in the still water as mist rises from the surface, with the swan occasionally dipping its head into the water to feed." \
  --addnoise_condition 20 \
  --use_usp \
  --offload \
  --seed 42
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Text To Video &amp;amp; Image To Video&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# run Text-to-Video Generation
model_id=Skywork/SkyReels-V2-T2V-14B-540P
torchrun --nproc_per_node=2 generate_video.py \
  --model_id ${model_id} \
  --resolution 540P \
  --num_frames 97 \
  --guidance_scale 6.0 \
  --shift 8.0 \
  --fps 24 \
  --offload \
  --prompt "A serene lake surrounded by towering mountains, with a few swans gracefully gliding across the water and sunlight dancing on the surface." \
  --use_usp \
  --seed 42
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;When using an &lt;strong&gt;image-to-video (I2V)&lt;/strong&gt; model, you must provide an input image using the &lt;code&gt;--image ${image_path}&lt;/code&gt; parameter. The &lt;code&gt;--guidance_scale 5.0&lt;/code&gt; and &lt;code&gt;--shift 3.0&lt;/code&gt; is recommended for I2V model.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#abstract"&gt;Abstract&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#methodology-of-skyreels-v2"&gt;Methodology of SkyReels-V2&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#key-contributions-of-skyreels-v2"&gt;Key Contributions of SkyReels-V2&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#video-captioner"&gt;Video Captioner&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#reinforcement-learning"&gt;Reinforcement Learning&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#diffusion-forcing"&gt;Diffusion Forcing&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#high-quality-supervised-fine-tuning-sft"&gt;High-Quality Supervised Fine-Tuning(SFT)&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#performance"&gt;Performance&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#acknowledgements"&gt;Acknowledgements&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#citation"&gt;Citation&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Abstract&lt;/h2&gt; 
&lt;p&gt;Recent advances in video generation have been driven by diffusion models and autoregressive frameworks, yet critical challenges persist in harmonizing prompt adherence, visual quality, motion dynamics, and duration: compromises in motion dynamics to enhance temporal visual quality, constrained video duration (5-10 seconds) to prioritize resolution, and inadequate shot-aware generation stemming from general-purpose MLLMs' inability to interpret cinematic grammar, such as shot composition, actor expressions, and camera motions. These intertwined limitations hinder realistic long-form synthesis and professional film-style generation.&lt;/p&gt; 
&lt;p&gt;To address these limitations, we introduce SkyReels-V2, the world's first infinite-length film generative model using a Diffusion Forcing framework. Our approach synergizes Multi-modal Large Language Models (MLLM), Multi-stage Pretraining, Reinforcement Learning, and Diffusion Forcing techniques to achieve comprehensive optimization. Beyond its technical innovations, SkyReels-V2 enables multiple practical applications, including Story Generation, Image-to-Video Synthesis, Camera Director functionality, and multi-subject consistent video generation through our &lt;a href="https://github.com/SkyworkAI/SkyReels-A2"&gt;Skyreels-A2&lt;/a&gt; system.&lt;/p&gt; 
&lt;h2&gt;Methodology of SkyReels-V2&lt;/h2&gt; 
&lt;p&gt;The SkyReels-V2 methodology consists of several interconnected components. It starts with a comprehensive data processing pipeline that prepares various quality training data. At its core is the Video Captioner architecture, which provides detailed annotations for video content. The system employs a multi-task pretraining strategy to build fundamental video generation capabilities. Post-training optimization includes Reinforcement Learning to enhance motion quality, Diffusion Forcing Training for generating extended videos, and High-quality Supervised Fine-Tuning (SFT) stages for visual refinement. The model runs on optimized computational infrastructure for efficient training and inference. SkyReels-V2 supports multiple applications, including Story Generation, Image-to-Video Synthesis, Camera Director functionality, and Elements-to-Video Generation.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/assets/main_pipeline.jpg" alt="mainpipeline" width="100%" /&gt; &lt;/p&gt; 
&lt;h2&gt;Key Contributions of SkyReels-V2&lt;/h2&gt; 
&lt;h4&gt;Video Captioner&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/Skywork/SkyCaptioner-V1"&gt;SkyCaptioner-V1&lt;/a&gt; serves as our video captioning model for data annotation. This model is trained on the captioning result from the base model &lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct"&gt;Qwen2.5-VL-72B-Instruct&lt;/a&gt; and the sub-expert captioners on a balanced video data. The balanced video data is a carefully curated dataset of approximately 2 million videos to ensure conceptual balance and annotation quality. Built upon the &lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct"&gt;Qwen2.5-VL-7B-Instruct&lt;/a&gt; foundation model, &lt;a href="https://huggingface.co/Skywork/SkyCaptioner-V1"&gt;SkyCaptioner-V1&lt;/a&gt; is fine-tuned to enhance performance in domain-specific video captioning tasks. To compare the performance with the SOTA models, we conducted a manual assessment of accuracy across different captioning fields using a test set of 1,000 samples. The proposed &lt;a href="https://huggingface.co/Skywork/SkyCaptioner-V1"&gt;SkyCaptioner-V1&lt;/a&gt; achieves the highest average accuracy among the baseline models, and show a dramatic result in the shot related fields&lt;/p&gt; 
&lt;p align="center"&gt; &lt;/p&gt;
&lt;table align="center"&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;model&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct"&gt;Qwen2.5-VL-7B-Ins.&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct"&gt;Qwen2.5-VL-72B-Ins.&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://huggingface.co/omni-research/Tarsier2-Recap-7b"&gt;Tarsier2-Recap-7b&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://huggingface.co/Skywork/SkyCaptioner-V1"&gt;SkyCaptioner-V1&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Avg accuracy&lt;/td&gt; 
   &lt;td&gt;51.4%&lt;/td&gt; 
   &lt;td&gt;58.7%&lt;/td&gt; 
   &lt;td&gt;49.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;76.3%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;shot type&lt;/td&gt; 
   &lt;td&gt;76.8%&lt;/td&gt; 
   &lt;td&gt;82.5%&lt;/td&gt; 
   &lt;td&gt;60.2%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;93.7%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;shot angle&lt;/td&gt; 
   &lt;td&gt;60.0%&lt;/td&gt; 
   &lt;td&gt;73.7%&lt;/td&gt; 
   &lt;td&gt;52.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;89.8%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;shot position&lt;/td&gt; 
   &lt;td&gt;28.4%&lt;/td&gt; 
   &lt;td&gt;32.7%&lt;/td&gt; 
   &lt;td&gt;23.6%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;83.1%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;camera motion&lt;/td&gt; 
   &lt;td&gt;62.0%&lt;/td&gt; 
   &lt;td&gt;61.2%&lt;/td&gt; 
   &lt;td&gt;45.3%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;85.3%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;expression&lt;/td&gt; 
   &lt;td&gt;43.6%&lt;/td&gt; 
   &lt;td&gt;51.5%&lt;/td&gt; 
   &lt;td&gt;54.3%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;68.8%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td colspan="5" style="text-align: center; border-bottom: 1px solid #ddd; padding: 8px;"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;TYPES_type&lt;/td&gt; 
   &lt;td&gt;43.5%&lt;/td&gt; 
   &lt;td&gt;49.7%&lt;/td&gt; 
   &lt;td&gt;47.6%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;82.5%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;TYPES_sub_type&lt;/td&gt; 
   &lt;td&gt;38.9%&lt;/td&gt; 
   &lt;td&gt;44.9%&lt;/td&gt; 
   &lt;td&gt;45.9%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;75.4%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;appearance&lt;/td&gt; 
   &lt;td&gt;40.9%&lt;/td&gt; 
   &lt;td&gt;52.0%&lt;/td&gt; 
   &lt;td&gt;45.6%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;59.3%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;action&lt;/td&gt; 
   &lt;td&gt;32.4%&lt;/td&gt; 
   &lt;td&gt;52.0%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;69.8%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;68.8%&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;position&lt;/td&gt; 
   &lt;td&gt;35.4%&lt;/td&gt; 
   &lt;td&gt;48.6%&lt;/td&gt; 
   &lt;td&gt;45.5%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;57.5%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;is_main_subject&lt;/td&gt; 
   &lt;td&gt;58.5%&lt;/td&gt; 
   &lt;td&gt;68.7%&lt;/td&gt; 
   &lt;td&gt;69.7%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;80.9%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;environment&lt;/td&gt; 
   &lt;td&gt;70.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;72.7%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;61.4%&lt;/td&gt; 
   &lt;td&gt;70.5%&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;lighting&lt;/td&gt; 
   &lt;td&gt;77.1%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;80.0%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;21.2%&lt;/td&gt; 
   &lt;td&gt;76.5%&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;/p&gt; 
&lt;h4&gt;Reinforcement Learning&lt;/h4&gt; 
&lt;p&gt;Inspired by the previous success in LLM, we propose to enhance the performance of the generative model by Reinforcement Learning. Specifically, we focus on the motion quality because we find that the main drawback of our generative model is:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;the generative model does not handle well with large, deformable motions.&lt;/li&gt; 
 &lt;li&gt;the generated videos may violate the physical law.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To avoid the degradation in other metrics, such as text alignment and video quality, we ensure the preference data pairs have comparable text alignment and video quality, while only the motion quality varies. This requirement poses greater challenges in obtaining preference annotations due to the inherently higher costs of human annotation. To address this challenge, we propose a semi-automatic pipeline that strategically combines automatically generated motion pairs and human annotation results. This hybrid approach not only enhances the data scale but also improves alignment with human preferences through curated quality control. Leveraging this enhanced dataset, we first train a specialized reward model to capture the generic motion quality differences between paired samples. This learned reward function subsequently guides the sample selection process for Direct Preference Optimization (DPO), enhancing the motion quality of the generative model.&lt;/p&gt; 
&lt;h4&gt;Diffusion Forcing&lt;/h4&gt; 
&lt;p&gt;We introduce the Diffusion Forcing Transformer to unlock our model‚Äôs ability to generate long videos. Diffusion Forcing is a training and sampling strategy where each token is assigned an independent noise level. This allows tokens to be denoised according to arbitrary, per-token schedules. Conceptually, this approach functions as a form of partial masking: a token with zero noise is fully unmasked, while complete noise fully masks it. Diffusion Forcing trains the model to "unmask" any combination of variably noised tokens, using the cleaner tokens as conditional information to guide the recovery of noisy ones. Building on this, our Diffusion Forcing Transformer can extend video generation indefinitely based on the last frames of the previous segment. Note that the synchronous full sequence diffusion is a special case of Diffusion Forcing, where all tokens share the same noise level. This relationship allows us to fine-tune the Diffusion Forcing Transformer from a full-sequence diffusion model.&lt;/p&gt; 
&lt;h4&gt;High-Quality Supervised Fine-Tuning (SFT)&lt;/h4&gt; 
&lt;p&gt;We implement two sequential high-quality supervised fine-tuning (SFT) stages at 540p and 720p resolutions respectively, with the initial SFT phase conducted immediately after pretraining but prior to reinforcement learning (RL) stage.This first-stage SFT serves as a conceptual equilibrium trainer, building upon the foundation model‚Äôs pretraining outcomes that utilized only fps24 video data, while strategically removing FPS embedding components to streamline thearchitecture. Trained with the high-quality concept-balanced samples, this phase establishes optimized initialization parameters for subsequent training processes. Following this, we execute a secondary high-resolution SFT at 720p after completing the diffusion forcing stage, incorporating identical loss formulations and the higher-quality concept-balanced datasets by the manually filter. This final refinement phase focuses on resolution increase such that the overall video quality will be further enhanced.&lt;/p&gt; 
&lt;h2&gt;Performance&lt;/h2&gt; 
&lt;p&gt;To comprehensively evaluate our proposed method, we construct the SkyReels-Bench for human assessment and leveraged the open-source &lt;a href="https://github.com/Vchitect/VBench"&gt;V-Bench&lt;/a&gt; for automated evaluation. This allows us to compare our model with the state-of-the-art (SOTA) baselines, including both open-source and proprietary models.&lt;/p&gt; 
&lt;h4&gt;Human Evaluation&lt;/h4&gt; 
&lt;p&gt;For human evaluation, we design SkyReels-Bench with 1,020 text prompts, systematically assessing three dimensions: Instruction Adherence, Motion Quality, Consistency and Visual Quality. This benchmark is designed to evaluate both text-to-video (T2V) and image-to-video (I2V) generation models, providing comprehensive assessment across different generation paradigms. To ensure fairness, all models were evaluated under default settings with consistent resolutions, and no post-generation filtering was applied.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Text To Video Models&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="center"&gt; &lt;/p&gt;
&lt;table align="center"&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model Name&lt;/th&gt; 
   &lt;th&gt;Average&lt;/th&gt; 
   &lt;th&gt;Instruction Adherence&lt;/th&gt; 
   &lt;th&gt;Consistency&lt;/th&gt; 
   &lt;th&gt;Visual Quality&lt;/th&gt; 
   &lt;th&gt;Motion Quality&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://runwayml.com/research/introducing-gen-3-alpha"&gt;Runway-Gen3 Alpha&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2.53&lt;/td&gt; 
   &lt;td&gt;2.19&lt;/td&gt; 
   &lt;td&gt;2.57&lt;/td&gt; 
   &lt;td&gt;3.23&lt;/td&gt; 
   &lt;td&gt;2.11&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Tencent/HunyuanVideo"&gt;HunyuanVideo-13B&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2.82&lt;/td&gt; 
   &lt;td&gt;2.64&lt;/td&gt; 
   &lt;td&gt;2.81&lt;/td&gt; 
   &lt;td&gt;3.20&lt;/td&gt; 
   &lt;td&gt;2.61&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://klingai.com"&gt;Kling-1.6 STD Mode&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2.99&lt;/td&gt; 
   &lt;td&gt;2.77&lt;/td&gt; 
   &lt;td&gt;3.05&lt;/td&gt; 
   &lt;td&gt;3.39&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;2.76&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://hailuoai.video"&gt;Hailuo-01&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;3.0&lt;/td&gt; 
   &lt;td&gt;2.8&lt;/td&gt; 
   &lt;td&gt;3.08&lt;/td&gt; 
   &lt;td&gt;3.29&lt;/td&gt; 
   &lt;td&gt;2.74&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Wan-Video/Wan2.1"&gt;Wan2.1-14B&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;3.12&lt;/td&gt; 
   &lt;td&gt;2.91&lt;/td&gt; 
   &lt;td&gt;3.31&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;3.54&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;2.71&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;SkyReels-V2&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;3.14&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;3.15&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;3.35&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;3.34&lt;/td&gt; 
   &lt;td&gt;2.74&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;/p&gt; 
&lt;p&gt;The evaluation demonstrates that our model achieves significant advancements in &lt;strong&gt;instruction adherence (3.15)&lt;/strong&gt; compared to baseline methods, while maintaining competitive performance in &lt;strong&gt;motion quality (2.74)&lt;/strong&gt; without sacrificing the &lt;strong&gt;consistency (3.35)&lt;/strong&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Image To Video Models&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="center"&gt; &lt;/p&gt;
&lt;table align="center"&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Average&lt;/th&gt; 
   &lt;th&gt;Instruction Adherence&lt;/th&gt; 
   &lt;th&gt;Consistency&lt;/th&gt; 
   &lt;th&gt;Visual Quality&lt;/th&gt; 
   &lt;th&gt;Motion Quality&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Tencent/HunyuanVideo"&gt;HunyuanVideo-13B&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2.84&lt;/td&gt; 
   &lt;td&gt;2.97&lt;/td&gt; 
   &lt;td&gt;2.95&lt;/td&gt; 
   &lt;td&gt;2.87&lt;/td&gt; 
   &lt;td&gt;2.56&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Wan-Video/Wan2.1"&gt;Wan2.1-14B&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2.85&lt;/td&gt; 
   &lt;td&gt;3.10&lt;/td&gt; 
   &lt;td&gt;2.81&lt;/td&gt; 
   &lt;td&gt;3.00&lt;/td&gt; 
   &lt;td&gt;2.48&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://hailuoai.video"&gt;Hailuo-01&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;3.05&lt;/td&gt; 
   &lt;td&gt;3.31&lt;/td&gt; 
   &lt;td&gt;2.58&lt;/td&gt; 
   &lt;td&gt;3.55&lt;/td&gt; 
   &lt;td&gt;2.74&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://klingai.com"&gt;Kling-1.6 Pro Mode&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;3.4&lt;/td&gt; 
   &lt;td&gt;3.56&lt;/td&gt; 
   &lt;td&gt;3.03&lt;/td&gt; 
   &lt;td&gt;3.58&lt;/td&gt; 
   &lt;td&gt;3.41&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://runwayml.com/research/introducing-runway-gen-4"&gt;Runway-Gen4&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;3.39&lt;/td&gt; 
   &lt;td&gt;3.75&lt;/td&gt; 
   &lt;td&gt;3.2&lt;/td&gt; 
   &lt;td&gt;3.4&lt;/td&gt; 
   &lt;td&gt;3.37&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;SkyReels-V2-DF&lt;/td&gt; 
   &lt;td&gt;3.24&lt;/td&gt; 
   &lt;td&gt;3.64&lt;/td&gt; 
   &lt;td&gt;3.21&lt;/td&gt; 
   &lt;td&gt;3.18&lt;/td&gt; 
   &lt;td&gt;2.93&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;SkyReels-V2-I2V&lt;/td&gt; 
   &lt;td&gt;3.29&lt;/td&gt; 
   &lt;td&gt;3.42&lt;/td&gt; 
   &lt;td&gt;3.18&lt;/td&gt; 
   &lt;td&gt;3.56&lt;/td&gt; 
   &lt;td&gt;3.01&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;/p&gt; 
&lt;p&gt;Our results demonstrate that both &lt;strong&gt;SkyReels-V2-I2V (3.29)&lt;/strong&gt; and &lt;strong&gt;SkyReels-V2-DF (3.24)&lt;/strong&gt; achieve state-of-the-art performance among open-source models, significantly outperforming HunyuanVideo-13B (2.84) and Wan2.1-14B (2.85) across all quality dimensions. With an average score of 3.29, SkyReels-V2-I2V demonstrates comparable performance to proprietary models Kling-1.6 (3.4) and Runway-Gen4 (3.39).&lt;/p&gt; 
&lt;h4&gt;VBench&lt;/h4&gt; 
&lt;p&gt;To objectively compare SkyReels-V2 Model against other leading open-source Text-To-Video models, we conduct comprehensive evaluations using the public benchmark &lt;a href="https://github.com/Vchitect/VBench"&gt;V-Bench&lt;/a&gt;. Our evaluation specifically leverages the benchmark‚Äôs longer version prompt. For fair comparison with baseline models, we strictly follow their recommended setting for inference.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;/p&gt;
&lt;table align="center"&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Total Score&lt;/th&gt; 
   &lt;th&gt;Quality Score&lt;/th&gt; 
   &lt;th&gt;Semantic Score&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/hpcaitech/Open-Sora"&gt;OpenSora 2.0&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;81.5 %&lt;/td&gt; 
   &lt;td&gt;82.1 %&lt;/td&gt; 
   &lt;td&gt;78.2 %&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/THUDM/CogVideo"&gt;CogVideoX1.5-5B&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;80.3 %&lt;/td&gt; 
   &lt;td&gt;80.9 %&lt;/td&gt; 
   &lt;td&gt;77.9 %&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Tencent/HunyuanVideo"&gt;HunyuanVideo-13B&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;82.7 %&lt;/td&gt; 
   &lt;td&gt;84.4 %&lt;/td&gt; 
   &lt;td&gt;76.2 %&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Wan-Video/Wan2.1"&gt;Wan2.1-14B&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;83.7 %&lt;/td&gt; 
   &lt;td&gt;84.2 %&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;81.4 %&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;SkyReels-V2&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;83.9 %&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;84.7 %&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;80.8 %&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;/p&gt; 
&lt;p&gt;The VBench results demonstrate that SkyReels-V2 outperforms all compared models including HunyuanVideo-13B and Wan2.1-14B, With the highest &lt;strong&gt;total score (83.9%)&lt;/strong&gt; and &lt;strong&gt;quality score (84.7%)&lt;/strong&gt;. In this evaluation, the semantic score is slightly lower than Wan2.1-14B, while we outperform Wan2.1-14B in human evaluations, with the primary gap attributed to V-Bench‚Äôs insufficient evaluation of shot-scenario semantic adherence.&lt;/p&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;p&gt;We would like to thank the contributors of &lt;a href="https://github.com/Wan-Video/Wan2.1"&gt;Wan 2.1&lt;/a&gt;, &lt;a href="https://github.com/xdit-project/xDiT"&gt;XDit&lt;/a&gt; and &lt;a href="https://qwenlm.github.io/blog/qwen2.5/"&gt;Qwen 2.5&lt;/a&gt; repositories, for their open research and contributions.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{chen2025skyreelsv2infinitelengthfilmgenerative,
      title={SkyReels-V2: Infinite-length Film Generative Model}, 
      author={Guibin Chen and Dixuan Lin and Jiangping Yang and Chunze Lin and Junchen Zhu and Mingyuan Fan and Hao Zhang and Sheng Chen and Zheng Chen and Chengcheng Ma and Weiming Xiong and Wei Wang and Nuo Pang and Kang Kang and Zhiheng Xu and Yuzhe Jin and Yupeng Liang and Yubing Song and Peng Zhao and Boyuan Xu and Di Qiu and Debang Li and Zhengcong Fei and Yang Li and Yahui Zhou},
      year={2025},
      eprint={2504.13074},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2504.13074}, 
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>srbhr/Resume-Matcher</title>
      <link>https://github.com/srbhr/Resume-Matcher</link>
      <description>&lt;p&gt;Improve your resumes with Resume Matcher. Get insights, keyword suggestions and tune your resumes to job descriptions.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://www.resumematcher.fyi"&gt;&lt;img src="https://raw.githubusercontent.com/srbhr/Resume-Matcher/main/assets/page_2.png" alt="Resume Matcher" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;h1&gt;Resume Matcher&lt;/h1&gt; 
 &lt;p&gt;&lt;a href="https://dsc.gg/resume-matcher"&gt;ùôπùöòùöíùöó ùô≥ùöíùöúùöåùöòùöõùöç&lt;/a&gt; ‚ú¶ &lt;a href="https://resumematcher.fyi"&gt;ùöÜùöéùöãùöúùöíùöùùöé&lt;/a&gt; ‚ú¶ &lt;a href="https://raw.githubusercontent.com/srbhr/Resume-Matcher/main/#how-to-install"&gt;ùô∑ùöòùö† ùöùùöò ùô∏ùöóùöúùöùùöäùöïùöï&lt;/a&gt; ‚ú¶ &lt;a href="https://raw.githubusercontent.com/srbhr/Resume-Matcher/main/#contributors"&gt;ùô≤ùöòùöóùöùùöõùöíùöãùöûùöùùöòùöõùöú&lt;/a&gt; ‚ú¶ &lt;a href="https://raw.githubusercontent.com/srbhr/Resume-Matcher/main/#support-the-development-by-donating"&gt;ùô≥ùöòùöóùöäùöùùöé&lt;/a&gt; ‚ú¶ &lt;a href="https://twitter.com/ssrbhr"&gt;ùöÉùö†ùöíùöùùöùùöéùöõ/ùöá&lt;/a&gt; ‚ú¶ &lt;a href="https://www.linkedin.com/company/resume-matcher/"&gt;ùôªùöíùöóùöîùöéùöçùô∏ùöó&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Stop getting auto-rejected by ATS bots.&lt;/strong&gt; Resume Matcher is the AI-powered platform that reverse-engineers hiring algorithms to show you exactly how to tailor your resume. Get the keywords, formatting, and insights that actually get you past the first screen and into human hands.&lt;/p&gt; 
 &lt;p&gt;Hoping to make this, &lt;strong&gt;VS Code for making resumes&lt;/strong&gt;.&lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;img src="https://img.shields.io/github/stars/srbhr/Resume-Matcher?labelColor=black&amp;amp;style=for-the-badge&amp;amp;color=c20a71" alt="Stars" /&gt; &lt;img src="https://img.shields.io/github/license/srbhr/Resume-Matcher?labelColor=black&amp;amp;style=for-the-badge&amp;amp;color=c20a71" alt="Apache 2.0" /&gt; &lt;img src="https://img.shields.io/github/forks/srbhr/Resume-Matcher?labelColor=black&amp;amp;style=for-the-badge&amp;amp;color=c20a71" alt="Forks" /&gt; &lt;img src="https://img.shields.io/badge/Version-0.1%20Veridis%20Quo-FFF?labelColor=black&amp;amp;logo=LinkedIn&amp;amp;style=for-the-badge&amp;amp;color=c20a71" alt="version" /&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://dsc.gg/resume-matcher"&gt;&lt;img src="https://img.shields.io/discord/1122069176962531400?labelColor=black&amp;amp;logo=discord&amp;amp;logoColor=c20a71&amp;amp;style=for-the-badge&amp;amp;color=c20a71" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://resumematcher.fyi"&gt;&lt;img src="https://img.shields.io/badge/website-Resume%20Matcher-FFF?labelColor=black&amp;amp;style=for-the-badge&amp;amp;color=c20a71" alt="Website" /&gt;&lt;/a&gt; &lt;a href="https://www.linkedin.com/company/resume-matcher/"&gt;&lt;img src="https://img.shields.io/badge/LinkedIn-Resume%20Matcher-FFF?labelColor=black&amp;amp;logo=LinkedIn&amp;amp;style=for-the-badge&amp;amp;color=c20a71" alt="LinkedIn" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/565" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/565" alt="srbhr%2FResume-Matcher | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://vercel.com/oss/program-badge.svg?sanitize=true" alt="Vercel OSS Program" /&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT]&lt;/p&gt; 
 &lt;p&gt;This project is in active development. New features are being added continuously, and we welcome contributions from the community. There are some breaking changes on the &lt;code&gt;main&lt;/code&gt; branch. If you have any suggestions or feature requests, please feel free to open an issue on GitHub or discuss it on our &lt;a href="https://dsc.gg/resume-matcher"&gt;Discord&lt;/a&gt; server.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Getting started with Resume Matcher&lt;/h2&gt; 
&lt;p&gt;Resume Matcher is designed to help you optimize your resume with the aim to highlight your skills and experience in a way that resonates with potential employers.&lt;/p&gt; 
&lt;p&gt;We're actively working on improving the platform, building towards a &lt;strong&gt;VS Code for making resumes&lt;/strong&gt;, and adding new features. The best way to stay updated is to join the Discord discussion and be part of the active development community.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Join our &lt;a href="https://dsc.gg/resume-matcher"&gt;Discord&lt;/a&gt; community üëá &lt;a href="https://dsc.gg/resume-matcher"&gt;&lt;img src="https://raw.githubusercontent.com/srbhr/Resume-Matcher/main/assets/resume_matcher_discord.png" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Follow us on &lt;a href="https://www.linkedin.com/company/resume-matcher/"&gt;LinkedIn&lt;/a&gt; ‚ú® &lt;a href="https://www.linkedin.com/company/resume-matcher/"&gt;&lt;img src="https://raw.githubusercontent.com/srbhr/Resume-Matcher/main/assets/resume_matcher_linkedin.png" alt="LinkedIn" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;‚≠ê Star Resume Matcher to support the development and get updates on GitHub. &lt;img src="https://raw.githubusercontent.com/srbhr/Resume-Matcher/main/assets/star_resume_matcher.png" alt="Star Resume Matcher" /&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/srbhr/Resume-Matcher/main/assets/resume_matcher_features.png" alt="resume_matcher_features" /&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Works locally&lt;/strong&gt;: No need to upload your resume to a server. Everything runs on your machine with open source AI models by Ollama.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ATS Compatibility&lt;/strong&gt;: Get a detailed analysis of your resume's compatibility with ATS systems.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Instant Match Score&lt;/strong&gt;: Upload resume &amp;amp; job description for a quick match score and key improvement areas.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Keyword Optimizer&lt;/strong&gt;: Align your resume with job keywords and identify critical content gaps.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Guided Improvements&lt;/strong&gt;: Get clear suggestions to make your resume stand out.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Roadmap&lt;/h3&gt; 
&lt;p&gt;If you have any suggestions or feature requests, please feel free to open an issue on GitHub. And discuss it on our &lt;a href="https://dsc.gg/resume-matcher"&gt;Discord&lt;/a&gt; server.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Visual keyword highlighting.&lt;/li&gt; 
 &lt;li&gt;AI Canvas, which can help to craft impactful, metric-driven resume content.&lt;/li&gt; 
 &lt;li&gt;Multi-job description optimization.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;How to Install&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/srbhr/Resume-Matcher/main/assets/how_to_install_resumematcher.png" alt="Installation" /&gt;&lt;/p&gt; 
&lt;p&gt;Follow the instructions in the &lt;a href="https://raw.githubusercontent.com/srbhr/Resume-Matcher/main/SETUP.md"&gt;SETUP.md&lt;/a&gt; file to set up the project locally. The setup script will install all the necessary dependencies and configure your environment.&lt;/p&gt; 
&lt;p&gt;The project is built using:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;FastAPI for the backend.&lt;/li&gt; 
 &lt;li&gt;Next.js for the frontend.&lt;/li&gt; 
 &lt;li&gt;Ollama for local AI model serving.&lt;/li&gt; 
 &lt;li&gt;Tailwind CSS for styling.&lt;/li&gt; 
 &lt;li&gt;SQLite for the database.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Technology&lt;/th&gt; 
   &lt;th&gt;Info/Version&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Python&lt;/td&gt; 
   &lt;td&gt;3.12+&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Next.js&lt;/td&gt; 
   &lt;td&gt;15+&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ollama&lt;/td&gt; 
   &lt;td&gt;0.6.7&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Join Us and Contribute&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/srbhr/Resume-Matcher/main/assets/how_to_contribute.png" alt="how to contribute" /&gt;&lt;/p&gt; 
&lt;p&gt;We welcome contributions from everyone! Whether you're a developer, designer, or just someone who wants to help out. All the contributors are listed in the &lt;a href="https://resumematcher.fyi/about"&gt;about page&lt;/a&gt; on our website and on the GitHub Readme here.&lt;/p&gt; 
&lt;p&gt;Check out the roadmap if you would like to work on the features that are planned for the future. If you have any suggestions or feature requests, please feel free to open an issue on GitHub and discuss it on our &lt;a href="https://dsc.gg/resume-matcher"&gt;Discord&lt;/a&gt; server.&lt;/p&gt; 
&lt;h2&gt;Contributors&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/srbhr/Resume-Matcher/main/assets/contributors.png" alt="Contributors" /&gt;&lt;/p&gt; 
&lt;a href="https://github.com/srbhr/Resume-Matcher/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=srbhr/Resume-Matcher" /&gt; &lt;/a&gt; 
&lt;h2&gt;Support the Development by Donating&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/srbhr/Resume-Matcher/main/assets/supporting_resume_matcher.png" alt="donate" /&gt;&lt;/p&gt; 
&lt;p&gt;If you would like to support the development of Resume Matcher, you can do so by donating. Your contributions will help us keep the project alive and continue adding new features.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Platform&lt;/th&gt; 
   &lt;th&gt;Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;GitHub&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/sponsors/srbhr"&gt;&lt;img src="https://img.shields.io/github/sponsors/srbhr?style=for-the-badge&amp;amp;color=c20a71&amp;amp;labelColor=black&amp;amp;logo=github" alt="GitHub Sponsors" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Buy Me a Coffee&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.buymeacoffee.com/srbhr"&gt;&lt;img src="https://img.shields.io/badge/Buy%20Me%20a%20Coffee-ffdd00?style=for-the-badge&amp;amp;logo=buy-me-a-coffee&amp;amp;color=c20a72&amp;amp;logoColor=white" alt="BuyMeACoffee" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;kbd&gt;Star History&lt;/kbd&gt;&lt;/summary&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=srbhr/resume-matcher&amp;amp;theme=dark&amp;amp;type=Date" /&gt; 
  &lt;img width="100%" src="https://api.star-history.com/svg?repos=srbhr/resume-matcher&amp;amp;theme=dark&amp;amp;type=Date" /&gt; 
 &lt;/picture&gt; 
&lt;/details&gt; 
&lt;h2&gt;Resume Matcher is a part of &lt;a href="https://vercel.com/oss"&gt;Vercel Open Source Program&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://vercel.com/oss/program-badge.svg?sanitize=true" alt="Vercel OSS Program" /&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>virattt/ai-hedge-fund</title>
      <link>https://github.com/virattt/ai-hedge-fund</link>
      <description>&lt;p&gt;An AI Hedge Fund Team&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AI Hedge Fund&lt;/h1&gt; 
&lt;p&gt;This is a proof of concept for an AI-powered hedge fund. The goal of this project is to explore the use of AI to make trading decisions. This project is for &lt;strong&gt;educational&lt;/strong&gt; purposes only and is not intended for real trading or investment.&lt;/p&gt; 
&lt;p&gt;This system employs several agents working together:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Aswath Damodaran Agent - The Dean of Valuation, focuses on story, numbers, and disciplined valuation&lt;/li&gt; 
 &lt;li&gt;Ben Graham Agent - The godfather of value investing, only buys hidden gems with a margin of safety&lt;/li&gt; 
 &lt;li&gt;Bill Ackman Agent - An activist investor, takes bold positions and pushes for change&lt;/li&gt; 
 &lt;li&gt;Cathie Wood Agent - The queen of growth investing, believes in the power of innovation and disruption&lt;/li&gt; 
 &lt;li&gt;Charlie Munger Agent - Warren Buffett's partner, only buys wonderful businesses at fair prices&lt;/li&gt; 
 &lt;li&gt;Michael Burry Agent - The Big Short contrarian who hunts for deep value&lt;/li&gt; 
 &lt;li&gt;Mohnish Pabrai Agent - The Dhandho investor, who looks for doubles at low risk&lt;/li&gt; 
 &lt;li&gt;Peter Lynch Agent - Practical investor who seeks "ten-baggers" in everyday businesses&lt;/li&gt; 
 &lt;li&gt;Phil Fisher Agent - Meticulous growth investor who uses deep "scuttlebutt" research&lt;/li&gt; 
 &lt;li&gt;Rakesh Jhunjhunwala Agent - The Big Bull of India&lt;/li&gt; 
 &lt;li&gt;Stanley Druckenmiller Agent - Macro legend who hunts for asymmetric opportunities with growth potential&lt;/li&gt; 
 &lt;li&gt;Warren Buffett Agent - The oracle of Omaha, seeks wonderful companies at a fair price&lt;/li&gt; 
 &lt;li&gt;Valuation Agent - Calculates the intrinsic value of a stock and generates trading signals&lt;/li&gt; 
 &lt;li&gt;Sentiment Agent - Analyzes market sentiment and generates trading signals&lt;/li&gt; 
 &lt;li&gt;Fundamentals Agent - Analyzes fundamental data and generates trading signals&lt;/li&gt; 
 &lt;li&gt;Technicals Agent - Analyzes technical indicators and generates trading signals&lt;/li&gt; 
 &lt;li&gt;Risk Manager - Calculates risk metrics and sets position limits&lt;/li&gt; 
 &lt;li&gt;Portfolio Manager - Makes final trading decisions and generates orders&lt;/li&gt; 
&lt;/ol&gt; 
&lt;img width="1042" alt="Screenshot 2025-03-22 at 6 19 07 PM" src="https://github.com/user-attachments/assets/cbae3dcf-b571-490d-b0ad-3f0f035ac0d4" /&gt; 
&lt;p&gt;Note: the system does not actually make any trades.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://twitter.com/virattt"&gt;&lt;img src="https://img.shields.io/twitter/follow/virattt?style=social" alt="Twitter Follow" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;This project is for &lt;strong&gt;educational and research purposes only&lt;/strong&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Not intended for real trading or investment&lt;/li&gt; 
 &lt;li&gt;No investment advice or guarantees provided&lt;/li&gt; 
 &lt;li&gt;Creator assumes no liability for financial losses&lt;/li&gt; 
 &lt;li&gt;Consult a financial advisor for investment decisions&lt;/li&gt; 
 &lt;li&gt;Past performance does not indicate future results&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;By using this software, you agree to use it solely for learning purposes.&lt;/p&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#how-to-install"&gt;How to Install&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#how-to-run"&gt;How to Run&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#%EF%B8%8F-command-line-interface"&gt;‚å®Ô∏è Command Line Interface&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#%EF%B8%8F-web-application"&gt;üñ•Ô∏è Web Application&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#how-to-contribute"&gt;How to Contribute&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#feature-requests"&gt;Feature Requests&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;How to Install&lt;/h2&gt; 
&lt;p&gt;Before you can run the AI Hedge Fund, you'll need to install it and set up your API keys. These steps are common to both the full-stack web application and command line interface.&lt;/p&gt; 
&lt;h3&gt;1. Clone the Repository&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/virattt/ai-hedge-fund.git
cd ai-hedge-fund
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Set up API keys&lt;/h3&gt; 
&lt;p&gt;Create a &lt;code&gt;.env&lt;/code&gt; file for your API keys:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Create .env file for your API keys (in the root directory)
cp .env.example .env
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Open and edit the &lt;code&gt;.env&lt;/code&gt; file to add your API keys:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# For running LLMs hosted by openai (gpt-4o, gpt-4o-mini, etc.)
OPENAI_API_KEY=your-openai-api-key

# For getting financial data to power the hedge fund
FINANCIAL_DATASETS_API_KEY=your-financial-datasets-api-key
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: You must set at least one LLM API key (e.g. &lt;code&gt;OPENAI_API_KEY&lt;/code&gt;, &lt;code&gt;GROQ_API_KEY&lt;/code&gt;, &lt;code&gt;ANTHROPIC_API_KEY&lt;/code&gt;, or &lt;code&gt;DEEPSEEK_API_KEY&lt;/code&gt;) for the hedge fund to work.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Financial Data&lt;/strong&gt;: Data for AAPL, GOOGL, MSFT, NVDA, and TSLA is free and does not require an API key. For any other ticker, you will need to set the &lt;code&gt;FINANCIAL_DATASETS_API_KEY&lt;/code&gt; in the .env file.&lt;/p&gt; 
&lt;h2&gt;How to Run&lt;/h2&gt; 
&lt;h3&gt;‚å®Ô∏è Command Line Interface&lt;/h3&gt; 
&lt;p&gt;You can run the AI Hedge Fund directly via terminal. This approach offers more granular control and is useful for automation, scripting, and integration purposes.&lt;/p&gt; 
&lt;img width="992" alt="Screenshot 2025-01-06 at 5 50 17 PM" src="https://github.com/user-attachments/assets/e8ca04bf-9989-4a7d-a8b4-34e04666663b" /&gt; 
&lt;h4&gt;Quick Start&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install Poetry (if not already installed):&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -sSL https://install.python-poetry.org | python3 -
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Install dependencies:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Run the AI Hedge Fund&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry run python src/main.py --ticker AAPL,MSFT,NVDA
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also specify a &lt;code&gt;--ollama&lt;/code&gt; flag to run the AI hedge fund using local LLMs.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry run python src/main.py --ticker AAPL,MSFT,NVDA --ollama
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can optionally specify the start and end dates to make decisions over a specific time period.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry run python src/main.py --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Run the Backtester&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Example Output:&lt;/strong&gt; &lt;img width="941" alt="Screenshot 2025-01-06 at 5 47 52 PM" src="https://github.com/user-attachments/assets/00e794ea-8628-44e6-9a84-8f8a31ad3b47" /&gt;&lt;/p&gt; 
&lt;p&gt;Note: The &lt;code&gt;--ollama&lt;/code&gt;, &lt;code&gt;--start-date&lt;/code&gt;, and &lt;code&gt;--end-date&lt;/code&gt; flags work for the backtester, as well!&lt;/p&gt; 
&lt;h3&gt;üñ•Ô∏è Web Application&lt;/h3&gt; 
&lt;p&gt;The new way to run the AI Hedge Fund is through our web application that provides a user-friendly interface. This is recommended for users who prefer visual interfaces over command line tools.&lt;/p&gt; 
&lt;p&gt;Please see detailed instructions on how to install and run the web application &lt;a href="https://github.com/virattt/ai-hedge-fund/tree/main/app"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;img width="1721" alt="Screenshot 2025-06-28 at 6 41 03‚ÄØPM" src="https://github.com/user-attachments/assets/b95ab696-c9f4-416c-9ad1-51feb1f5374b" /&gt; 
&lt;h2&gt;How to Contribute&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Fork the repository&lt;/li&gt; 
 &lt;li&gt;Create a feature branch&lt;/li&gt; 
 &lt;li&gt;Commit your changes&lt;/li&gt; 
 &lt;li&gt;Push to the branch&lt;/li&gt; 
 &lt;li&gt;Create a Pull Request&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: Please keep your pull requests small and focused. This will make it easier to review and merge.&lt;/p&gt; 
&lt;h2&gt;Feature Requests&lt;/h2&gt; 
&lt;p&gt;If you have a feature request, please open an &lt;a href="https://github.com/virattt/ai-hedge-fund/issues"&gt;issue&lt;/a&gt; and make sure it is tagged with &lt;code&gt;enhancement&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the MIT License - see the LICENSE file for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>mindsdb/mindsdb</title>
      <link>https://github.com/mindsdb/mindsdb</link>
      <description>&lt;p&gt;Federated query engine for AI - The only MCP Server you'll ever need&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a name="readme-top"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://pypi.org/project/MindsDB/" target="_blank"&gt;&lt;img src="https://badge.fury.io/py/MindsDB.svg?sanitize=true" alt="MindsDB Release" /&gt;&lt;/a&gt; 
 &lt;a href="https://www.python.org/downloads/" target="_blank"&gt;&lt;img src="https://img.shields.io/badge/python-3.10.x%7C%203.11.x%7C%203.12.x%7C%203.13.x-brightgreen.svg?sanitize=true" alt="Python supported" /&gt;&lt;/a&gt; 
 &lt;a href="https://hub.docker.com/u/mindsdb" target="_blank"&gt;&lt;img src="https://img.shields.io/docker/pulls/mindsdb/mindsdb" alt="Docker pulls" /&gt;&lt;/a&gt; 
 &lt;br /&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/3068" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/3068" alt="mindsdb%2Fmindsdb | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;a href="https://github.com/mindsdb/mindsdb"&gt; &lt;img src="https://raw.githubusercontent.com/mindsdb/mindsdb/main/docs/assets/mindsdb_logo.png" alt="MindsDB" width="300" /&gt; &lt;/a&gt; 
 &lt;p align="center"&gt; &lt;br /&gt; &lt;a href="https://www.mindsdb.com?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo"&gt;Website&lt;/a&gt; ¬∑ &lt;a href="https://docs.mindsdb.com?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo"&gt;Docs&lt;/a&gt; ¬∑ &lt;a href="https://mindsdb.com/contact"&gt;Contact us for a Demo&lt;/a&gt; ¬∑ &lt;a href="https://mindsdb.com/joincommunity?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo"&gt;Community Slack&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;p&gt;MindsDB enables humans, AI, agents, and applications to get highly accurate answers across large scale data sources.&lt;/p&gt; 
&lt;a href="https://www.youtube.com/watch?v=MX3OKpnsoLM" target="_blank"&gt; &lt;img src="https://github.com/user-attachments/assets/119e7b82-f901-4214-a26f-ff7c5ad86064" alt="MindsDB Demo" /&gt; &lt;/a&gt; 
&lt;h2&gt;Install MindsDB Server&lt;/h2&gt; 
&lt;p&gt;MindsDB is an open-source server that can be deployed anywhere - from your laptop to the cloud, and everywhere in between. And yes, you can customize it to your heart's content.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.mindsdb.com/setup/self-hosted/docker-desktop"&gt;Using Docker Desktop&lt;/a&gt;. This is the fastest and recommended way to get started and have it all running.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.mindsdb.com/setup/self-hosted/docker"&gt;Using Docker&lt;/a&gt;. This is also simple, but gives you more flexibility on how to further customize your server.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://docs.mindsdb.com/mcp/overview"&gt;MindsDB has an MCP server built in&lt;/a&gt; that enables your MCP applications to connect, unify and respond to questions over large-scale federated data‚Äîspanning databases, data warehouses, and SaaS applications.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h1&gt;Core Philosophy: Connect, Unify, Respond&lt;/h1&gt; 
&lt;p&gt;MindsDB's architecture is built around three fundamental capabilities:&lt;/p&gt; 
&lt;h2&gt;&lt;a href="https://docs.mindsdb.com/integrations/data-overview"&gt;Connect&lt;/a&gt; Your Data&lt;/h2&gt; 
&lt;p&gt;You can connect to hundreds of enterprise &lt;a href="https://docs.mindsdb.com/integrations/data-overview"&gt;data sources (learn more)&lt;/a&gt;. These integrations allow MindsDB to access data wherever it resides, forming the foundation for all other capabilities.&lt;/p&gt; 
&lt;h2&gt;&lt;a href="https://docs.mindsdb.com/mindsdb_sql/overview"&gt;Unify&lt;/a&gt; Your Data&lt;/h2&gt; 
&lt;p&gt;In many situations, it‚Äôs important to be able to prepare and unify data before generating responses from it. MindsDB SQL offers knowledge bases and views that allow indexing and organizing structured and unstructured data as if it were unified in a single system.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.mindsdb.com/mindsdb_sql/knowledge-bases"&gt;&lt;strong&gt;KNOWLEDGE BASES&lt;/strong&gt;&lt;/a&gt; ‚Äì Index and organize unstructured data for efficient Q&amp;amp;A.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.mindsdb.com/mindsdb_sql/sql/create/view"&gt;&lt;strong&gt;VIEWS&lt;/strong&gt;&lt;/a&gt; ‚Äì Simplify data access by creating unified views across different sources (no-ETL).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Unification of data can be automated using JOBs&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.mindsdb.com/mindsdb_sql/sql/create/jobs"&gt;&lt;strong&gt;JOBS&lt;/strong&gt;&lt;/a&gt; ‚Äì Schedule synchronization and transformation tasks for real-time processing.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;a href="https://docs.mindsdb.com/mindsdb_sql/agents/agent"&gt;Respond&lt;/a&gt; From Your Data&lt;/h2&gt; 
&lt;p&gt;Chat with Your Data&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.mindsdb.com/mindsdb_sql/agents/agent"&gt;&lt;strong&gt;AGENTS&lt;/strong&gt;&lt;/a&gt; ‚Äì Configure built-in agents specialized in answering questions over your connected and unified data.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.mindsdb.com/mcp/overview"&gt;&lt;strong&gt;MCP&lt;/strong&gt;&lt;/a&gt; ‚Äì Connect to MindsDB through the MCP (Model Context Protocol) for seamless interaction.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ü§ù Contribute&lt;/h2&gt; 
&lt;p&gt;Interested in contributing to MindsDB? Follow our &lt;a href="https://docs.mindsdb.com/contribute/install?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo"&gt;installation guide for development&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You can find our &lt;a href="https://docs.mindsdb.com/contribute/contribute?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo"&gt;contribution guide here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We welcome suggestions! Feel free to open new issues with your ideas, and we‚Äôll guide you.&lt;/p&gt; 
&lt;p&gt;This project adheres to a &lt;a href="https://github.com/mindsdb/mindsdb/raw/main/CODE_OF_CONDUCT.md"&gt;Contributor Code of Conduct&lt;/a&gt;. By participating, you agree to follow its terms.&lt;/p&gt; 
&lt;p&gt;Also, check out our &lt;a href="https://mindsdb.com/community?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo"&gt;community rewards and programs&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ü§ç Support&lt;/h2&gt; 
&lt;p&gt;If you find a bug, please submit an &lt;a href="https://github.com/mindsdb/mindsdb/issues/new/choose"&gt;issue on GitHub&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Here‚Äôs how you can get community support:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Ask a question in our &lt;a href="https://mindsdb.com/joincommunity?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo"&gt;Slack Community&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Join our &lt;a href="https://github.com/mindsdb/mindsdb/discussions"&gt;GitHub Discussions&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Post on &lt;a href="https://stackoverflow.com/questions/tagged/mindsdb"&gt;Stack Overflow&lt;/a&gt; with the MindsDB tag.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For commercial support, please &lt;a href="https://mindsdb.com/contact?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo"&gt;contact the MindsDB team&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;üíö Current Contributors&lt;/h2&gt; 
&lt;a href="https://github.com/mindsdb/mindsdb/graphs/contributors"&gt; &lt;img src="https://contributors-img.web.app/image?repo=mindsdb/mindsdb" /&gt; &lt;/a&gt; 
&lt;p&gt;Generated with &lt;a href="https://contributors-img.web.app"&gt;contributors-img&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;üîî Subscribe for Updates&lt;/h2&gt; 
&lt;p&gt;Join our &lt;a href="https://mindsdb.com/joincommunity"&gt;Slack community&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>DevilXD/TwitchDropsMiner</title>
      <link>https://github.com/DevilXD/TwitchDropsMiner</link>
      <description>&lt;p&gt;An app that allows you to AFK mine timed Twitch drops, with automatic drop claiming and channel switching.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Twitch Drops Miner&lt;/h1&gt; 
&lt;p&gt;This application allows you to AFK mine timed Twitch drops, without having to worry about switching channels when the one you were watching goes offline, claiming the drops, or even receiving the stream data itself. This helps you save on bandwidth and hassle.&lt;/p&gt; 
&lt;h3&gt;How It Works:&lt;/h3&gt; 
&lt;p&gt;Every several seconds, the application pretends to watch a particular stream by fetching stream metadata - this is enough to advance the drops. Note that this completely bypasses the need to download any actual stream video and sound. To keep the status (ONLINE or OFFLINE) of the channels up-to-date, there's a websocket connection established that receives events about streams going up or down, or updates regarding the current amount of viewers.&lt;/p&gt; 
&lt;h3&gt;Features:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Stream-less drop mining - save on bandwidth.&lt;/li&gt; 
 &lt;li&gt;Game priority and exclusion lists, allowing you to focus on mining what you want, in the order you want, and ignore what you don't want.&lt;/li&gt; 
 &lt;li&gt;Sharded websocket connection, allowing for tracking up to &lt;code&gt;199&lt;/code&gt; channels at the same time.&lt;/li&gt; 
 &lt;li&gt;Automatic drop campaigns discovery based on linked accounts (requires you to do &lt;a href="https://www.twitch.tv/drops/campaigns"&gt;account linking&lt;/a&gt; yourself though).&lt;/li&gt; 
 &lt;li&gt;Stream tags and drop campaign validation, to ensure you won't end up mining a stream that can't earn you the drop.&lt;/li&gt; 
 &lt;li&gt;Automatic channel stream switching, when the one you were currently watching goes offline, as well as when a channel streaming a higher priority game goes online.&lt;/li&gt; 
 &lt;li&gt;Login session is saved in a cookies file, so you don't need to login every time.&lt;/li&gt; 
 &lt;li&gt;Mining is automatically started as new campaigns appear, and stopped when the last available drops have been mined.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Usage:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Download and unzip &lt;a href="https://github.com/DevilXD/TwitchDropsMiner/releases"&gt;the latest release&lt;/a&gt; - it's recommended to keep it in the folder it comes in.&lt;/li&gt; 
 &lt;li&gt;Run it and login/connect the miner to your Twitch account by using the in-app login form.&lt;/li&gt; 
 &lt;li&gt;After a successful login, the app should fetch a list of all available campaigns and games you can mine drops for - you can then select and add games of choice to the Priority List available on the Settings tab, and then press on the &lt;code&gt;Reload&lt;/code&gt; button to start processing. It will fetch a list of all applicable streams it can watch, and start mining right away. You can also manually switch to a different channel as needed.&lt;/li&gt; 
 &lt;li&gt;If you wish to keep the miner occupied with mining anything it can, beyond what you've selected via the Priority List, you can use the Priority Mode setting to specify the mining order for the rest of the games.&lt;/li&gt; 
 &lt;li&gt;Make sure to link your Twitch account to game accounts on the &lt;a href="https://www.twitch.tv/drops/campaigns"&gt;campaigns page&lt;/a&gt;, to enable more games to be mined.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Pictures:&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/4180725/164298155-c0880ad7-6423-4419-8d73-f3c053730a1b.png" alt="Main" /&gt; &lt;img src="https://user-images.githubusercontent.com/4180725/164298315-81cae0d2-24a4-4822-a056-154fd763c284.png" alt="Inventory" /&gt; &lt;img src="https://user-images.githubusercontent.com/4180725/164298391-b13ad40d-3881-436c-8d4c-34e2bbe33a78.png" alt="Settings" /&gt;&lt;/p&gt; 
&lt;h3&gt;Notes:&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING]&lt;br /&gt; Due to how Twitch handles the drop progression on their side, watching a stream in the browser (or by any other means) on the same account that is actively being used by the miner, will usually cause the miner to misbehave, reporting false progress and getting stuck mining the current drop.&lt;/p&gt; 
 &lt;p&gt;Using the same account to watch other streams during mining is thus discouraged, in order to avoid any problems arising from it.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!CAUTION]&lt;br /&gt; Persistent cookies will be stored in the &lt;code&gt;cookies.jar&lt;/code&gt; file, from which the authorization (login) information will be restored on each subsequent run. Make sure to keep your cookies file safe, as the authorization information it stores can give another person access to your Twitch account, even without them knowing your password!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT]&lt;br /&gt; Successfully logging into your Twitch account in the application may cause Twitch to send you a "New Login" notification email. This is normal - you can verify that it comes from your own IP address. The detected browser during the login will be "Chrome", as that's what the miner currently presents itself to the Twitch server.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;br /&gt; The time remaining timer always countdowns a single minute and then stops - it is then restarted only after the application redetermines the remaining time. This "redetermination" can happen at any time Twitch decides to report on the drop's progress, but not later than 20 seconds after the timer reaches zero. The seconds timer is only an approximation and does not represent nor affect actual mining speed. The time variations are due to Twitch sometimes not reporting drop progress at all, or reporting progress for the wrong drop - these cases have all been accounted for in the application though.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;br /&gt; The source code requires Python 3.10 or higher to run.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Notes about the Windows build:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;To achieve a portable-executable format, the application is packaged with PyInstaller into an &lt;code&gt;EXE&lt;/code&gt;. Some antivirus engines (including Windows Defender) might report the packaged executable as a trojan, because PyInstaller has been used by others to package malicious Python code in the past. These reports can be safely ignored. If you absolutely do not trust the executable, you'll have to install Python yourself and run everything from source.&lt;/li&gt; 
 &lt;li&gt;The executable uses the &lt;code&gt;%TEMP%&lt;/code&gt; directory for temporary runtime storage of files, that don't need to be exposed to the user (like compiled code and translation files). For persistent storage, the directory the executable resides in is used instead.&lt;/li&gt; 
 &lt;li&gt;The autostart feature is implemented as a registry entry to the current user's (&lt;code&gt;HKCU&lt;/code&gt;) autostart key. It is only altered when toggling the respective option. If you relocate the app to a different directory, the autostart feature will stop working, until you toggle the option off and back on again&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Notes about the Linux build:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;The Linux app is built and distributed using two distinct portable-executable formats: &lt;a href="https://appimage.org/"&gt;AppImage&lt;/a&gt; and &lt;a href="https://pyinstaller.org/"&gt;PyInstaller&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;There are no major differences between the two formats, but if you're looking for a recommendation, use the AppImage.&lt;/li&gt; 
 &lt;li&gt;The Linux app should work out of the box on any modern distribution, as long as it has &lt;code&gt;glibc&amp;gt;=2.35&lt;/code&gt;, plus a working display server.&lt;/li&gt; 
 &lt;li&gt;Every feature of the app is expected to work on Linux just as well as it does on Windows. If you find something that's broken, please &lt;a href="https://github.com/DevilXD/TwitchDropsMiner/issues/new"&gt;open a new issue&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;The size of the Linux app is significantly larger than the Windows app due to the inclusion of the &lt;code&gt;gtk3&lt;/code&gt; library (and its dependencies), which is required for proper system tray/notifications support.&lt;/li&gt; 
 &lt;li&gt;As an alternative to the native Linux app, you can run the Windows app via &lt;a href="https://www.winehq.org/"&gt;Wine&lt;/a&gt; instead. It works really well!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Advanced Usage:&lt;/h3&gt; 
&lt;p&gt;If you'd be interested in running the latest master from source or building your own executable, see the wiki page explaining how to do so: &lt;a href="https://github.com/DevilXD/TwitchDropsMiner/wiki/Setting-up-the-environment,-building-and-running"&gt;https://github.com/DevilXD/TwitchDropsMiner/wiki/Setting-up-the-environment,-building-and-running&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Support&lt;/h3&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://www.buymeacoffee.com/DevilXD"&gt;&lt;img src="https://i.imgur.com/cL95gzE.png" alt="Buy me a coffee" /&gt;&lt;/a&gt; &lt;a href="https://www.patreon.com/bePatron?u=26937862"&gt;&lt;img src="https://i.imgur.com/Mdkb9jq.png" alt="Support me on Patreon" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h3&gt;Project goals:&lt;/h3&gt; 
&lt;p&gt;Twitch Drops Miner (TDM for short) has been designed with a couple of simple goals in mind. These are, specifically:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Twitch Drops oriented - it's in the name. That's what I made it for.&lt;/li&gt; 
 &lt;li&gt;Easy to use for an average person. Includes a nice looking GUI and is packaged as a ready-to-go executable, without requiring an existing Python installation to work.&lt;/li&gt; 
 &lt;li&gt;Intended as a helper tool that starts together with your PC, runs in the background through out the day, and then closes together with your PC shutting down at the end of the day. If it can run continuously for 24 hours at minimum, and not run into any errors, I'd call that good enough already.&lt;/li&gt; 
 &lt;li&gt;Requiring a minimum amount of attention during operation - check it once or twice through out the day to see if everything's fine with it.&lt;/li&gt; 
 &lt;li&gt;Underlying service friendly - the amount of interactions done with the Twitch site is kept to the minimum required for reliable operation, at a level achievable by a diligent site user.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;TDM is not intended for/as:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Mining channel points - again, it's about the drops: only.&lt;/li&gt; 
 &lt;li&gt;Mining anything else besides Twitch drops - no, I won't be adding support for a random 3rd party site that also happens to rely on watching Twitch streams.&lt;/li&gt; 
 &lt;li&gt;Unattended operation: worst case scenario, it'll stop working and you'll hopefully notice that at some point. Hopefully.&lt;/li&gt; 
 &lt;li&gt;100% uptime application, due to the underlying nature of it, expect fatal errors to happen every so often.&lt;/li&gt; 
 &lt;li&gt;Being hosted on a remote server as a 24/7 miner.&lt;/li&gt; 
 &lt;li&gt;Being used with more than one managed account.&lt;/li&gt; 
 &lt;li&gt;Mining campaigns the managed account isn't linked to.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This means that features such as:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;It being possible to run it without a GUI, or with only a console attached.&lt;/li&gt; 
 &lt;li&gt;Any form of automatic restart when an error happens.&lt;/li&gt; 
 &lt;li&gt;Docker or any other form of remote deployment.&lt;/li&gt; 
 &lt;li&gt;Using it with more than one managed account.&lt;/li&gt; 
 &lt;li&gt;Making it possible to mine campaigns that the managed account isn't linked to.&lt;/li&gt; 
 &lt;li&gt;Anything that increases the site processing load caused by the application.&lt;/li&gt; 
 &lt;li&gt;Any form of additional notifications system (email, webhook, etc.), beyond what's already implemented.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;..., are most likely not going to be a feature, ever. You're welcome to search through the existing issues to comment on your point of view on the relevant matters, where applicable. Otherwise, most of the new issues that go against these goals will be closed and the user will be pointed to this paragraph.&lt;/p&gt; 
&lt;p&gt;For more context about these goals, please check out these issues: &lt;a href="https://github.com/DevilXD/TwitchDropsMiner/issues/161"&gt;#161&lt;/a&gt;, &lt;a href="https://github.com/DevilXD/TwitchDropsMiner/issues/105"&gt;#105&lt;/a&gt;, &lt;a href="https://github.com/DevilXD/TwitchDropsMiner/issues/84"&gt;#84&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Credits:&lt;/h3&gt; 
&lt;!--
Note: The translations credits are sorted alphabetically, based on their English language name.
When adding a new entry, please ensure to insert it in the correct place in the second section.
Non-translations related credits should be added to the first section instead.

Note: When adding a new credits line below, please add two trailing spaces at the end
of the previous line, if they aren't already there. Doing so ensures proper markdown
rendering on Github. In short: Each credits line should end with two trailing spaces,
placed past the period character at the end.

‚Ä¢ Last line can have the two trailing spaces omitted.
‚Ä¢ Please ensure your editor won't trim the trailing spaces upon saving the file.
‚Ä¢ Please ensure to leave a single empty new line at the end of the file.
--&gt; 
&lt;p&gt;@guihkx - For the CI script, CI maintenance, and everything related to Linux builds.&lt;br /&gt; @kWAYTV - For the implementation of the dark mode theme.&lt;/p&gt; 
&lt;p&gt;@Bamboozul - For the entirety of the Arabic (ÿßŸÑÿπÿ±ÿ®Ÿäÿ©) translation.&lt;br /&gt; @Suz1e - For the entirety of the Chinese (ÁÆÄ‰Ωì‰∏≠Êñá) translation and revisions.&lt;br /&gt; @wwj010, @zhangminghao1989, @Self4215 - For the Chinese (ÁÆÄ‰Ωì‰∏≠Êñá) translation corrections and revisions.&lt;br /&gt; @Ricky103403 - For the entirety of the Traditional Chinese (ÁπÅÈ´î‰∏≠Êñá) translation.&lt;br /&gt; @LusTerCsI - For the Traditional Chinese (ÁπÅÈ´î‰∏≠Êñá) translation corrections and revisions.&lt;br /&gt; @nwvh - For the entirety of the Czech (ƒåe≈°tina) translation.&lt;br /&gt; @Kjerne - For the entirety of the Danish (Dansk) translation.&lt;br /&gt; @roobini-gamer - For the entirety of the French (Fran√ßais) translation.&lt;br /&gt; @Calvineries - For the French (Fran√ßais) translation revisions.&lt;br /&gt; @ThisIsCyreX - For the entirety of the German (Deutsch) translation.&lt;br /&gt; @Eriza-Z - For the entirety of the Indonesian translation.&lt;br /&gt; @casungo - For the entirety of the Italian (Italiano) translation.&lt;br /&gt; @ShimadaNanaki - For the entirety of the Japanese (Êó•Êú¨Ë™û) translation.&lt;br /&gt; @biroman - For the entirety of the Norwegian (Norsk) translation.&lt;br /&gt; @Patriot99 - For the Polish (Polski) translation and revisions (co-authored with @DevilXD).&lt;br /&gt; @zarigata - For the entirety of the Portuguese (Portugu√™s) translation.&lt;br /&gt; @Sergo1217 - For the entirety of the Russian (–†—É—Å—Å–∫–∏–π) translation.&lt;br /&gt; @kilroy98, @flamesv - For the Russian (–†—É—Å—Å–∫–∏–π) translation corrections and revisions.&lt;br /&gt; @Shofuu - For the entirety of the Spanish (Espa√±ol) translation and revisions.&lt;br /&gt; @Forero-0 = For the Spanish (Espa√±ol) translation revisions.&lt;br /&gt; @alikdb - For the entirety of the Turkish (T√ºrk√ße) translation.&lt;br /&gt; @DogancanYr - For the Turkish (T√ºrk√ße) translation revisions.&lt;br /&gt; @Elderly-Emre - For the Turkish (T√ºrk√ße) translation revisions.&lt;br /&gt; @Nollasko - For the entirety of the Ukrainian (–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞) translation and revisions.&lt;br /&gt; @kilroy98 - For the Ukrainian (–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞) translation corrections and revisions.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>polarsource/polar</title>
      <link>https://github.com/polarsource/polar</link>
      <description>&lt;p&gt;Turn your software into a business.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://polar.sh"&gt; &lt;img src="https://github.com/user-attachments/assets/89a588e5-0c58-429a-8bbe-20f70af41372" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://www.producthunt.com/posts/polar-5?embed=true&amp;amp;utm_source=badge-top-post-badge&amp;amp;utm_medium=badge&amp;amp;utm_souce=badge-polar-5" target="_blank"&gt;&lt;img src="https://api.producthunt.com/widgets/embed-image/v1/top-post-badge.svg?post_id=484271&amp;amp;theme=dark&amp;amp;period=daily" alt="Polar - An open source monetization platform for developers | Product Hunt" style="width: 250px; height: 54px;" width="250" height="54" /&gt;&lt;/a&gt; &lt;a href="https://www.producthunt.com/posts/polar-5?embed=true&amp;amp;utm_source=badge-top-post-topic-badge&amp;amp;utm_medium=badge&amp;amp;utm_souce=badge-polar-5" target="_blank"&gt;&lt;img src="https://api.producthunt.com/widgets/embed-image/v1/top-post-topic-badge.svg?post_id=484271&amp;amp;theme=dark&amp;amp;period=monthly&amp;amp;topic_id=267" alt="Polar - An open source monetization platform for developers | Product Hunt" style="width: 250px; height: 54px;" width="250" height="54" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://polar.sh"&gt;Website&lt;/a&gt; &lt;span&gt;&amp;nbsp;&amp;nbsp;‚Ä¢&amp;nbsp;&amp;nbsp;&lt;/span&gt; &lt;a href="https://polar.sh/blog"&gt;Blog&lt;/a&gt; &lt;span&gt;&amp;nbsp;&amp;nbsp;‚Ä¢&amp;nbsp;&amp;nbsp;&lt;/span&gt; &lt;a href="https://polar.sh/docs"&gt;Docs&lt;/a&gt; &lt;span&gt;&amp;nbsp;&amp;nbsp;‚Ä¢&amp;nbsp;&amp;nbsp;&lt;/span&gt; &lt;a href="https://polar.sh/docs/api-reference"&gt;API Reference&lt;/a&gt;&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;a href="https://discord.gg/Pnhfz3UThd"&gt; &lt;img src="https://img.shields.io/badge/chat-on%20discord-7289DA.svg?sanitize=true" alt="Discord Chat" /&gt; &lt;/a&gt; &lt;a href="https://twitter.com/intent/follow?screen_name=polar_sh"&gt; &lt;img src="https://img.shields.io/twitter/follow/polar_sh.svg?label=Follow%20@polar_sh" alt="Follow @polar_sh" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Polar: Open Source payments infrastructure for the 21st century&lt;/h2&gt; 
&lt;p&gt;Focus on building your passion, while we focus on the infrastructure to get you paid.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Sell SaaS and digital products in minutes&lt;/li&gt; 
 &lt;li&gt;All-in-one funding &amp;amp; monetization platform for developers.&lt;/li&gt; 
 &lt;li&gt;Sell access to GitHub repositories, Discord Support channels, File Downloads, License Keys &amp;amp; much more with Digital Products &amp;amp; Subscriptions.&lt;/li&gt; 
 &lt;li&gt;We're the merchant of record handling the... 
  &lt;ul&gt; 
   &lt;li&gt;...boilerplate (billing, receipts, customer accounts etc)&lt;/li&gt; 
   &lt;li&gt;...headaches (sales tax, VAT)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Pricing&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;4% + 40¬¢&lt;/li&gt; 
 &lt;li&gt;No fixed monthly costs&lt;/li&gt; 
 &lt;li&gt;Additional fees may apply. &lt;a href="https://polar.sh/docs/documentation/polar-as-merchant-of-record/fees"&gt;Read more&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Roadmap, Issues &amp;amp; Feature Requests&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;üéØ Upcoming milestones.&lt;/strong&gt; &lt;a href="https://github.com/polarsource/polar/issues/3242"&gt;Check out what we're building towards&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;üí¨ Shape the future of Polar with us.&lt;/strong&gt; &lt;a href="https://discord.gg/Pnhfz3UThd"&gt;Join our Discord&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;üêõ Found a bug?&lt;/strong&gt; &lt;a href="https://github.com/polarsource/polar/issues"&gt;Submit it here&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;üîì Found a security vulnerability?&lt;/strong&gt; We greatly appreciate responsible and private disclosures. See &lt;a href="https://raw.githubusercontent.com/polarsource/polar/main/SECURITY.md"&gt;Security&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Polar API &amp;amp; SDK&lt;/h3&gt; 
&lt;p&gt;You can integrate Polar on your docs, sites or services using our &lt;a href="https://polar.sh/docs/api-reference"&gt;Public API&lt;/a&gt; and &lt;a href="https://polar.sh/docs/integrate/webhooks/endpoints"&gt;Webhook API&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We also maintain SDKs for the following languages:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;JavaScript (Node.js and browsers): &lt;a href="https://github.com/polarsource/polar-js"&gt;polarsource/polar-js&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Python: &lt;a href="https://github.com/polarsource/polar-python"&gt;polarsource/polar-python&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributions&lt;/h2&gt; 
&lt;p&gt;Our &lt;a href="https://raw.githubusercontent.com/polarsource/polar/main/DEVELOPMENT.md"&gt;&lt;code&gt;DEVELOPMENT.md&lt;/code&gt;&lt;/a&gt; file contains everything you need to know to configure your development environment.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] Want to get started quickly? Use GitHub Codespaces.&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://codespaces.new/polarsource/polar?machine=standardLinux32gb"&gt;&lt;img src="https://github.com/codespaces/badge.svg?sanitize=true" alt="Open in GitHub Codespaces" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Contributors&lt;/h3&gt; 
&lt;a href="https://github.com/polarsource/polar/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=polarsource/polar" /&gt; &lt;/a&gt; 
&lt;h2&gt;Monorepo&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/polarsource/polar/main/server/README.md"&gt;server&lt;/a&gt;&lt;/strong&gt; ‚Äì Python / FastAPI / Dramatiq / SQLAlchemy (PostgreSQL) / Redis&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/polarsource/polar/main/clients/README.md"&gt;clients&lt;/a&gt;&lt;/strong&gt; ‚Äì Turborepo 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/polarsource/polar/main/clients/apps/web"&gt;web&lt;/a&gt; (Dashboard) ‚Äì NextJS (TypeScript)&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/polarsource/polar/main/clients/packages/polarkit"&gt;polarkit&lt;/a&gt; - Shared React components&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;sub&gt;‚ô•Ô∏èüôè To our &lt;code&gt;pyproject.toml&lt;/code&gt; friends: &lt;a href="https://github.com/tiangolo/fastapi"&gt;FastAPI&lt;/a&gt;, &lt;a href="https://github.com/pydantic/pydantic"&gt;Pydantic&lt;/a&gt;, &lt;a href="https://github.com/Bogdanp/dramatiq"&gt;Dramatiq&lt;/a&gt;, &lt;a href="https://github.com/sqlalchemy/sqlalchemy"&gt;SQLAlchemy&lt;/a&gt;, &lt;a href="https://github.com/yanyongyu/githubkit"&gt;Githubkit&lt;/a&gt;, &lt;a href="https://github.com/sysid/sse-starlette"&gt;sse-starlette&lt;/a&gt;, &lt;a href="https://github.com/encode/uvicorn"&gt;Uvicorn&lt;/a&gt;, &lt;a href="https://github.com/frankie567/httpx-oauth"&gt;httpx-oauth&lt;/a&gt;, &lt;a href="https://github.com/pallets/jinja"&gt;jinja&lt;/a&gt;, &lt;a href="https://github.com/pallets-eco/blinker"&gt;blinker&lt;/a&gt;, &lt;a href="https://github.com/jpadilla/pyjwt"&gt;pyjwt&lt;/a&gt;, &lt;a href="https://github.com/getsentry/sentry"&gt;Sentry&lt;/a&gt; + more&lt;/sub&gt;&lt;br /&gt; &lt;sub&gt;‚ô•Ô∏èüôè To our &lt;code&gt;package.json&lt;/code&gt; friends: &lt;a href="https://github.com/vercel/next.js/"&gt;Next.js&lt;/a&gt;, &lt;a href="https://github.com/TanStack/query"&gt;TanStack Query&lt;/a&gt;, &lt;a href="https://github.com/tailwindlabs/tailwindcss"&gt;tailwindcss&lt;/a&gt;, &lt;a href="https://github.com/ferdikoomen/openapi-typescript-codegen"&gt;openapi-typescript-codegen&lt;/a&gt;, &lt;a href="https://github.com/axios/axios"&gt;axios&lt;/a&gt;, &lt;a href="https://github.com/radix-ui/primitives"&gt;radix-ui&lt;/a&gt;, &lt;a href="https://github.com/pacocoursey/cmdk"&gt;cmdk&lt;/a&gt;, &lt;a href="https://github.com/framer/motion"&gt;framer-motion&lt;/a&gt; + more&lt;/sub&gt;&lt;br /&gt; &lt;sub&gt;‚ô•Ô∏èüôè To &lt;a href="https://ipinfo.io"&gt;IPinfo&lt;/a&gt; that provides IP address data to help us geolocate customers during checkout.&lt;/sub&gt;&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Licensed under &lt;a href="https://www.apache.org/licenses/LICENSE-2.0"&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>GoogleCloudPlatform/agent-starter-pack</title>
      <link>https://github.com/GoogleCloudPlatform/agent-starter-pack</link>
      <description>&lt;p&gt;Ship AI Agents to Google Cloud in minutes, not months. Production-ready templates with built-in CI/CD, evaluation, and observability.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;üöÄ Agent Starter Pack&lt;/h1&gt; 
&lt;p&gt;&lt;img src="https://img.shields.io/pypi/v/agent-starter-pack?color=blue" alt="Version" /&gt; &lt;a href="https://youtu.be/jHt-ZVD660g"&gt;&lt;img src="https://img.shields.io/badge/1--Minute%20Overview-gray" alt="1-Minute Video Overview" /&gt;&lt;/a&gt; &lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/"&gt;&lt;img src="https://img.shields.io/badge/Documentation-gray" alt="Docs" /&gt;&lt;/a&gt; &lt;a href="https://studio.firebase.google.com/new?template=https%3A%2F%2Fgithub.com%2FGoogleCloudPlatform%2Fagent-starter-pack%2Ftree%2Fmain%2Fagent_starter_pack%2Fresources%2Fidx"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="https://cdn.firebasestudio.dev/btn/try_light_20.svg" /&gt; 
   &lt;source media="(prefers-color-scheme: light)" srcset="https://cdn.firebasestudio.dev/btn/try_dark_20.svg" /&gt; 
   &lt;img height="20" alt="Try in Firebase Studio" src="https://cdn.firebasestudio.dev/btn/try_blue_20.svg?sanitize=true" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; &lt;a href="https://shell.cloud.google.com/cloudshell/editor?cloudshell_git_repo=https%3A%2F%2Fgithub.com%2Feliasecchig%2Fasp-open-in-cloud-shell&amp;amp;cloudshell_print=open-in-cs"&gt;&lt;img src="https://img.shields.io/badge/Launch-in_Cloud_Shell-white" alt="Launch in Cloud Shell" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/github/stars/GoogleCloudPlatform/agent-starter-pack?color=yellow" alt="Stars" /&gt;&lt;/p&gt; 
&lt;p&gt;A Python package that provides &lt;strong&gt;production-ready templates&lt;/strong&gt; for GenAI agents on Google Cloud.&lt;/p&gt; 
&lt;p&gt;Focus on your agent logic‚Äîthe starter pack provides everything else: infrastructure, CI/CD, observability, and security.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;‚ö°Ô∏è Launch&lt;/th&gt; 
   &lt;th&gt;üß™ Experiment&lt;/th&gt; 
   &lt;th&gt;‚úÖ Deploy&lt;/th&gt; 
   &lt;th&gt;üõ†Ô∏è Customize&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/GoogleCloudPlatform/agent-starter-pack/main/agent_starter_pack/agents/"&gt;Pre-built agent templates&lt;/a&gt; (ReAct, RAG, multi-agent, Live API).&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://cloud.google.com/vertex-ai/generative-ai/docs/models/evaluation-overview"&gt;Vertex AI evaluation&lt;/a&gt; and an interactive playground.&lt;/td&gt; 
   &lt;td&gt;Production-ready infra with &lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/guide/observability"&gt;monitoring, observability&lt;/a&gt;, and &lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/guide/deployment"&gt;CI/CD&lt;/a&gt; on &lt;a href="https://cloud.google.com/run"&gt;Cloud Run&lt;/a&gt; or &lt;a href="https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/overview"&gt;Agent Engine&lt;/a&gt;.&lt;/td&gt; 
   &lt;td&gt;Extend and customize templates according to your needs. üÜï Now integrating with &lt;a href="https://github.com/google-gemini/gemini-cli"&gt;Gemini CLI&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h2&gt;‚ö° Get Started in 1 Minute&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;From zero to production-ready agent in 60 seconds using &lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;&lt;code&gt;uv&lt;/code&gt;&lt;/a&gt;:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uvx agent-starter-pack create
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt; ‚ú® Alternative: Using pip&lt;/summary&gt; 
 &lt;p&gt;If you don't have &lt;a href="https://github.com/astral-sh/uv"&gt;&lt;code&gt;uv&lt;/code&gt;&lt;/a&gt; installed, you can use pip:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Create and activate a Python virtual environment
python -m venv .venv &amp;amp;&amp;amp; source .venv/bin/activate

# Install the agent starter pack
pip install --upgrade agent-starter-pack

# Create a new agent project
agent-starter-pack create
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p&gt;&lt;strong&gt;That's it!&lt;/strong&gt; You now have a fully functional agent project‚Äîcomplete with backend, frontend, and deployment infrastructure‚Äîready for you to explore and customize.&lt;/p&gt; 
&lt;h3&gt;üîß Enhance Existing Agents&lt;/h3&gt; 
&lt;p&gt;Already have an agent? Add production-ready deployment and infrastructure by running this command in your project's root folder:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uvx agent-starter-pack enhance
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/guide/installation"&gt;Installation Guide&lt;/a&gt; for more options, or try with zero setup in &lt;a href="https://studio.firebase.google.com/new?template=https%3A%2F%2Fgithub.com%2FGoogleCloudPlatform%2Fagent-starter-pack%2Ftree%2Fmain%2Fsrc%2Fresources%2Fidx"&gt;Firebase Studio&lt;/a&gt; or &lt;a href="https://shell.cloud.google.com/cloudshell/editor?cloudshell_git_repo=https%3A%2F%2Fgithub.com%2Feliasecchig%2Fasp-open-in-cloud-shell&amp;amp;cloudshell_print=open-in-cs"&gt;Cloud Shell&lt;/a&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ü§ñ Agents&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Agent Name&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;adk_base&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;A base ReAct agent implemented using Google's &lt;a href="https://github.com/google/adk-python"&gt;Agent Development Kit&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;adk_a2a_base&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;An ADK agent with &lt;a href="https://a2a-protocol.org/"&gt;Agent2Agent (A2A) Protocol&lt;/a&gt; support for distributed agent communication and interoperability&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;agentic_rag&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;A RAG agent for document retrieval and Q&amp;amp;A. Supporting &lt;a href="https://cloud.google.com/generative-ai-app-builder/docs/enterprise-search-introduction"&gt;Vertex AI Search&lt;/a&gt; and &lt;a href="https://cloud.google.com/vertex-ai/docs/vector-search/overview"&gt;Vector Search&lt;/a&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;langgraph_base&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;A base ReAct agent implemented using LangChain's &lt;a href="https://github.com/langchain-ai/langgraph"&gt;LangGraph&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;adk_live&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;A real-time multimodal RAG agent powered by Gemini, supporting audio/video/text chat&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;More agents are on the way!&lt;/strong&gt; We are continuously expanding our &lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/agents/overview"&gt;agent library&lt;/a&gt;. Have a specific agent type in mind? &lt;a href="https://github.com/GoogleCloudPlatform/agent-starter-pack/issues/new?labels=enhancement"&gt;Raise an issue as a feature request!&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;üîç ADK Samples&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Looking to explore more ADK examples? Check out the &lt;a href="https://github.com/google/adk-samples"&gt;ADK Samples Repository&lt;/a&gt; for additional examples and use cases demonstrating ADK's capabilities.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üåü Community Showcase&lt;/h2&gt; 
&lt;p&gt;Explore amazing projects built with the Agent Starter Pack!&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/guide/community-showcase"&gt;View Community Showcase ‚Üí&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;p&gt;The &lt;code&gt;agent-starter-pack&lt;/code&gt; offers key features to accelerate and simplify the development of your agent:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;üîÑ &lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/cli/setup_cicd"&gt;CI/CD Automation&lt;/a&gt;&lt;/strong&gt; - A single command to set up a complete CI/CD pipeline for all environments, supporting both &lt;strong&gt;Google Cloud Build&lt;/strong&gt; and &lt;strong&gt;GitHub Actions&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üì• &lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/guide/data-ingestion"&gt;Data Pipeline for RAG with Terraform/CI-CD&lt;/a&gt;&lt;/strong&gt; - Seamlessly integrate a data pipeline to process embeddings for RAG into your agent system. Supporting &lt;a href="https://cloud.google.com/generative-ai-app-builder/docs/enterprise-search-introduction"&gt;Vertex AI Search&lt;/a&gt; and &lt;a href="https://cloud.google.com/vertex-ai/docs/vector-search/overview"&gt;Vector Search&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/GoogleCloudPlatform/agent-starter-pack/main/docs/guide/remote-templating.md"&gt;Remote Templates&lt;/a&gt;&lt;/strong&gt;: Create and share your own agent starter packs templates from any Git repository.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ü§ñ Gemini CLI Integration&lt;/strong&gt; - Use the &lt;a href="https://github.com/google-gemini/gemini-cli"&gt;Gemini CLI&lt;/a&gt; and the included &lt;code&gt;GEMINI.md&lt;/code&gt; context file to ask questions about your template, agent architecture, and the path to production. Get instant guidance and code examples directly in your terminal.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;High-Level Architecture&lt;/h2&gt; 
&lt;p&gt;This starter pack covers all aspects of Agent development, from prototyping and evaluation to deployment and monitoring.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/GoogleCloudPlatform/agent-starter-pack/main/docs/images/ags_high_level_architecture.png" alt="High Level Architecture" title="Architecture" /&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üîß Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.10+&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://cloud.google.com/sdk/docs/install"&gt;Google Cloud SDK&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://developer.hashicorp.com/terraform/downloads"&gt;Terraform&lt;/a&gt; (for deployment)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.gnu.org/software/make/"&gt;Make&lt;/a&gt; (for development tasks)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìö Documentation&lt;/h2&gt; 
&lt;p&gt;Visit our &lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/"&gt;documentation site&lt;/a&gt; for comprehensive guides and references!&lt;/p&gt; 
&lt;p&gt;üîç &lt;strong&gt;New to the codebase?&lt;/strong&gt; Explore the &lt;a href="https://codewiki.google/github.com/googlecloudplatform/agent-starter-pack"&gt;CodeWiki&lt;/a&gt; for AI-powered code understanding and navigation.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/guide/getting-started"&gt;Getting Started Guide&lt;/a&gt; - First steps with agent-starter-pack&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/guide/installation"&gt;Installation Guide&lt;/a&gt; - Setting up your environment&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/guide/deployment"&gt;Deployment Guide&lt;/a&gt; - Taking your agent to production&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/agents/overview"&gt;Agent Templates Overview&lt;/a&gt; - Explore available agent patterns&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/cli/"&gt;CLI Reference&lt;/a&gt; - Command-line tool documentation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Video Walkthrough:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.youtube.com/watch?v=9zqwym-N3lg"&gt;Exploring the Agent Starter Pack&lt;/a&gt;&lt;/strong&gt;: A comprehensive tutorial demonstrating how to rapidly deploy AI Agents using the Agent Starter Pack, covering architecture, templates, and step-by-step deployment.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.youtube.com/live/eZ-8UQ_t4YM?feature=shared&amp;amp;t=2791"&gt;6-minute introduction&lt;/a&gt;&lt;/strong&gt; (April 2024): Explaining the Agent Starter Pack and demonstrating its key features. Part of the Kaggle GenAI intensive course.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Looking for more examples and resources for Generative AI on Google Cloud? Check out the &lt;a href="https://github.com/GoogleCloudPlatform/generative-ai"&gt;GoogleCloudPlatform/generative-ai&lt;/a&gt; repository for notebooks, code samples, and more!&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions are welcome! See the &lt;a href="https://raw.githubusercontent.com/GoogleCloudPlatform/agent-starter-pack/main/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Feedback&lt;/h2&gt; 
&lt;p&gt;We value your input! Your feedback helps us improve this starter pack and make it more useful for the community.&lt;/p&gt; 
&lt;h3&gt;Getting Help&lt;/h3&gt; 
&lt;p&gt;If you encounter any issues or have specific suggestions, please first consider &lt;a href="https://github.com/GoogleCloudPlatform/generative-ai/issues"&gt;raising an issue&lt;/a&gt; on our GitHub repository.&lt;/p&gt; 
&lt;h3&gt;Share Your Experience&lt;/h3&gt; 
&lt;p&gt;For other types of feedback, or if you'd like to share a positive experience or success story using this starter pack, we'd love to hear from you! You can reach out to us at &lt;a href="mailto:agent-starter-pack@google.com"&gt;&lt;/a&gt;&lt;a href="mailto:agent-starter-pack@google.com"&gt;agent-starter-pack@google.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Thank you for your contributions!&lt;/p&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;This repository is for demonstrative purposes only and is not an officially supported Google product.&lt;/p&gt; 
&lt;h2&gt;Terms of Service&lt;/h2&gt; 
&lt;p&gt;The agent-starter-pack templating CLI and the templates in this starter pack leverage Google Cloud APIs. When you use this starter pack, you'll be deploying resources in your own Google Cloud project and will be responsible for those resources. Please review the &lt;a href="https://cloud.google.com/terms/service-terms"&gt;Google Cloud Service Terms&lt;/a&gt; for details on the terms of service associated with these APIs.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>HKUDS/DeepCode</title>
      <link>https://github.com/HKUDS/DeepCode</link>
      <description>&lt;p&gt;"DeepCode: Open Agentic Coding (Paper2Code &amp; Text2Web &amp; Text2Backend)"&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;table style="border: none; margin: 0 auto; padding: 0; border-collapse: collapse;"&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td align="center" style="vertical-align: middle; padding: 10px; border: none; width: 250px;"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/DeepCode/main/assets/logo.png" alt="DeepCode Logo" width="200" style="margin: 0; padding: 0; display: block;" /&gt; &lt;/td&gt; 
    &lt;td align="left" style="vertical-align: middle; padding: 10px 0 10px 30px; border: none;"&gt; &lt;pre style="font-family: 'Courier New', monospace; font-size: 16px; color: #0EA5E9; margin: 0; padding: 0; text-shadow: 0 0 10px #0EA5E9, 0 0 20px rgba(14,165,233,0.5); line-height: 1.2; transform: skew(-1deg, 0deg); display: block;"&gt;    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó
    ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù
    ‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó
    ‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù  ‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù  ‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïù ‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù
    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë     ‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó
    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïù      ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù&lt;/pre&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
 &lt;div align="center"&gt; 
  &lt;a href="https://trendshift.io/repositories/14665" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/14665" alt="HKUDS%2FDeepCode | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; 
 &lt;/div&gt; 
 &lt;!-- &lt;img src="https://readme-typing-svg.herokuapp.com?font=Russo+One&amp;size=28&amp;duration=2000&amp;pause=800&amp;color=06B6D4&amp;background=00000000&amp;center=true&amp;vCenter=true&amp;width=800&amp;height=50&amp;lines=%E2%9A%A1+OPEN+AGENTIC+CODING+%E2%9A%A1" alt="DeepCode Tech Subtitle" style="margin-top: 5px; filter: drop-shadow(0 0 12px #06B6D4) drop-shadow(0 0 24px rgba(6,182,212,0.4));"/&gt; --&gt; 
 &lt;h1&gt;&lt;img src="https://github.com/Zongwei9888/Experiment_Images/raw/43c585dca3d21b8e4b6390d835cdd34dc4b4b23d/DeepCode_images/title_logo.svg?sanitize=true" alt="DeepCode Logo" width="32" height="32" style="vertical-align: middle; margin-right: 8px;" /&gt; DeepCode: Open Agentic Coding&lt;/h1&gt; 
 &lt;h3&gt;&lt;em&gt;Advancing Code Generation with Multi-Agent Systems&lt;/em&gt;&lt;/h3&gt; 
 &lt;!-- &lt;p align="center"&gt;
  &lt;img src="https://img.shields.io/badge/Version-1.0.0-00d4ff?style=for-the-badge&amp;logo=rocket&amp;logoColor=white" alt="Version"&gt;

  &lt;img src="https://img.shields.io/badge/License-MIT-4ecdc4?style=for-the-badge&amp;logo=opensourceinitiative&amp;logoColor=white" alt="License"&gt;
  &lt;img src="https://img.shields.io/badge/AI-Multi--Agent-9b59b6?style=for-the-badge&amp;logo=brain&amp;logoColor=white" alt="AI"&gt;
  &lt;img src="https://img.shields.io/badge/HKU-Data_Intelligence_Lab-f39c12?style=for-the-badge&amp;logo=university&amp;logoColor=white" alt="HKU"&gt;
&lt;/p&gt; --&gt; 
 &lt;p&gt; &lt;a href="https://github.com/HKUDS/DeepCode/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/HKUDS/DeepCode?color=00d9ff&amp;amp;style=for-the-badge&amp;amp;logo=star&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2512.07921"&gt;&lt;img src="https://img.shields.io/badge/Paper-arXiv-orange?style=for-the-badge&amp;amp;logo=arxiv&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/badge/üêçPython-3.13-4ecdc4?style=for-the-badge&amp;amp;logo=python&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt; 
  &lt;!-- &lt;a href="https://pypi.org/project/deepcode-hku/"&gt;&lt;img src="https://img.shields.io/pypi/v/deepcode-hku.svg?style=for-the-badge&amp;logo=pypi&amp;logoColor=white&amp;labelColor=1a1a2e&amp;color=ff6b6b"&gt;&lt;/a&gt; --&gt; &lt;/p&gt; 
 &lt;p&gt; &lt;a href="https://discord.gg/yF2MmDJyGJ"&gt;&lt;img src="https://img.shields.io/badge/üí¨Discord-Community-7289da?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;a href="https://github.com/HKUDS/DeepCode/issues/11"&gt;&lt;img src="https://img.shields.io/badge/üí¨WeChat-Group-07c160?style=for-the-badge&amp;amp;logo=wechat&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;div style="width: 100%; height: 2px; margin: 20px 0; background: linear-gradient(90deg, transparent, #00d9ff, transparent);"&gt;&lt;/div&gt; 
 &lt;/div&gt; 
 &lt;div align="center"&gt; 
  &lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-quick-start" style="text-decoration: none;"&gt; &lt;img src="https://img.shields.io/badge/Quick%20Start-Get%20Started%20Now-00d9ff?style=for-the-badge&amp;amp;logo=rocket&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt; &lt;/a&gt; 
 &lt;/div&gt; 
 &lt;div align="center" style="margin-top: 10px;"&gt; 
  &lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/README.md"&gt; &lt;img src="https://img.shields.io/badge/English-00d4ff?style=for-the-badge&amp;amp;logo=readme&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" alt="English" /&gt; &lt;/a&gt; 
  &lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/README_ZH.md"&gt; &lt;img src="https://img.shields.io/badge/‰∏≠Êñá-00d4ff?style=for-the-badge&amp;amp;logo=readme&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" alt="‰∏≠Êñá" /&gt; &lt;/a&gt; 
 &lt;/div&gt; 
 &lt;h3&gt;üñ•Ô∏è &lt;strong&gt;Interface Showcase&lt;/strong&gt;&lt;/h3&gt; 
 &lt;table align="center" width="100%" style="border: none; border-collapse: collapse; margin: 30px 0;"&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td width="50%" align="center" style="vertical-align: top; padding: 20px;"&gt; &lt;h4&gt;üñ•Ô∏è &lt;strong&gt;CLI Interface&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;Terminal-Based Development&lt;/strong&gt;&lt;/p&gt; 
     &lt;div align="center"&gt; 
      &lt;img src="https://github.com/Zongwei9888/Experiment_Images/raw/8882a7313c504ca97ead6e7b36c51aa761b6a4f3/DeepCode_images/CLI.gif" alt="CLI Interface Demo" width="100%" style="border-radius: 10px; box-shadow: 0 8px 20px rgba(45,55,72,0.3); margin: 15px 0;" /&gt; 
      &lt;div style="background: linear-gradient(135deg, #2D3748 0%, #4A5568 100%); border-radius: 12px; padding: 15px; margin: 15px 0; color: white;"&gt; 
       &lt;strong&gt;üöÄ Advanced Terminal Experience&lt;/strong&gt;
       &lt;br /&gt; 
       &lt;small&gt;‚ö° Fast command-line workflow&lt;br /&gt;üîß Developer-friendly interface&lt;br /&gt;üìä Real-time progress tracking&lt;/small&gt; 
      &lt;/div&gt; 
      &lt;p&gt;&lt;em&gt;Professional terminal interface for advanced users and CI/CD integration&lt;/em&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
    &lt;td width="50%" align="center" style="vertical-align: top; padding: 20px;"&gt; &lt;h4&gt;üåê &lt;strong&gt;Web Interface&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;Visual Interactive Experience&lt;/strong&gt;&lt;/p&gt; 
     &lt;div align="center"&gt; 
      &lt;img src="https://github.com/Zongwei9888/Experiment_Images/raw/8882a7313c504ca97ead6e7b36c51aa761b6a4f3/DeepCode_images/UI.gif" alt="Web Interface Demo" width="100%" style="border-radius: 10px; box-shadow: 0 8px 20px rgba(14,165,233,0.3); margin: 15px 0;" /&gt; 
      &lt;div style="background: linear-gradient(135deg, #0EA5E9 0%, #00D4FF 100%); border-radius: 12px; padding: 15px; margin: 15px 0; color: white;"&gt; 
       &lt;strong&gt;üé® Modern Web Dashboard&lt;/strong&gt;
       &lt;br /&gt; 
       &lt;small&gt;üñ±Ô∏è Intuitive drag-and-drop&lt;br /&gt;üì± Responsive design&lt;br /&gt;üéØ Visual progress tracking&lt;/small&gt; 
      &lt;/div&gt; 
      &lt;p&gt;&lt;em&gt;Beautiful web interface with streamlined workflow for all skill levels&lt;/em&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
 &lt;hr /&gt; 
 &lt;div align="center"&gt; 
  &lt;h3&gt;üé¨ &lt;strong&gt;Introduction Video&lt;/strong&gt;&lt;/h3&gt; 
  &lt;div style="margin: 20px 0;"&gt; 
   &lt;a href="https://youtu.be/PRgmP8pOI08" target="_blank"&gt; &lt;img src="https://img.youtube.com/vi/PRgmP8pOI08/maxresdefault.jpg" alt="DeepCode Introduction Video" width="75%" style="border-radius: 12px; box-shadow: 0 8px 25px rgba(0,0,0,0.15); transition: transform 0.3s ease;" /&gt; &lt;/a&gt; 
  &lt;/div&gt; 
  &lt;p&gt;&lt;em&gt;üéØ &lt;strong&gt;Watch our complete introduction&lt;/strong&gt; - See how DeepCode transforms research papers and natural language into production-ready code&lt;/em&gt;&lt;/p&gt; 
  &lt;p&gt; &lt;a href="https://youtu.be/PRgmP8pOI08" target="_blank"&gt; &lt;img src="https://img.shields.io/badge/‚ñ∂Ô∏è_Watch_Video-FF0000?style=for-the-badge&amp;amp;logo=youtube&amp;amp;logoColor=white" alt="Watch Video" /&gt; &lt;/a&gt; &lt;/p&gt; 
 &lt;/div&gt; 
 &lt;hr /&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;em&gt;"Where AI Agents Transform Ideas into Production-Ready Code"&lt;/em&gt;&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìë Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-news"&gt;üì∞ News&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-key-features"&gt;üöÄ Key Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#%EF%B8%8F-architecture"&gt;üèóÔ∏è Architecture&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-experimental-results"&gt;üìä Experimental Results&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-quick-start"&gt;üöÄ Quick Start&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-examples"&gt;üí° Examples&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-live-demonstrations"&gt;üé¨ Live Demonstrations&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-star-history"&gt;‚≠ê Star History&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-license"&gt;üìÑ License&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üì∞ News&lt;/h2&gt; 
&lt;p&gt;üéâ &lt;strong&gt;[2025-10] üéâ [2025-10-28] DeepCode Achieves SOTA on PaperBench!&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;DeepCode sets new benchmarks on OpenAI's PaperBench Code-Dev across all categories:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üèÜ &lt;strong&gt;Surpasses Human Experts&lt;/strong&gt;: &lt;strong&gt;75.9%&lt;/strong&gt; (DeepCode) vs Top Machine Learning PhDs 72.4% (+3.5%).&lt;/li&gt; 
 &lt;li&gt;ü•á &lt;strong&gt;Outperforms SOTA Commercial Code Agents&lt;/strong&gt;: &lt;strong&gt;84.8%&lt;/strong&gt; (DeepCode) vs Leading Commercial Code Agents (+26.1%) (Cursor, Claude Code, and Codex).&lt;/li&gt; 
 &lt;li&gt;üî¨ &lt;strong&gt;Advances Scientific Coding&lt;/strong&gt;: &lt;strong&gt;73.5%&lt;/strong&gt; (DeepCode) vs PaperCoder 51.1% (+22.4%).&lt;/li&gt; 
 &lt;li&gt;üöÄ &lt;strong&gt;Beats LLM Agents&lt;/strong&gt;: &lt;strong&gt;73.5%&lt;/strong&gt; (DeepCode) vs best LLM frameworks 43.3% (+30.2%).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üöÄ Key Features&lt;/h2&gt; 
&lt;br /&gt; 
&lt;table align="center" width="100%" style="border: none; table-layout: fixed;"&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td width="30%" align="center" style="vertical-align: top; padding: 20px;"&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;h3 style="margin: 0; padding: 0;"&gt;üöÄ &lt;strong&gt;Paper2Code&lt;/strong&gt;&lt;/h3&gt; 
    &lt;/div&gt; 
    &lt;div align="center" style="margin: 15px 0;"&gt; 
     &lt;img src="https://img.shields.io/badge/ALGORITHM-IMPLEMENTATION-ff6b6b?style=for-the-badge&amp;amp;logo=algorithm&amp;amp;logoColor=white" alt="Algorithm Badge" /&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;&lt;strong&gt;Automated Implementation of Complex Algorithms&lt;/strong&gt;&lt;/p&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 60px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;Effortlessly converts complex algorithms from research papers into &lt;strong&gt;high-quality&lt;/strong&gt;, &lt;strong&gt;production-ready&lt;/strong&gt; code, accelerating algorithm reproduction.&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
   &lt;td width="30%" align="center" style="vertical-align: top; padding: 20px;"&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;h3 style="margin: 0; padding: 0;"&gt;üé® &lt;strong&gt;Text2Web&lt;/strong&gt;&lt;/h3&gt; 
    &lt;/div&gt; 
    &lt;div align="center" style="margin: 15px 0;"&gt; 
     &lt;img src="https://img.shields.io/badge/FRONTEND-DEVELOPMENT-4ecdc4?style=for-the-badge&amp;amp;logo=react&amp;amp;logoColor=white" alt="Frontend Badge" /&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;&lt;strong&gt;Automated Front-End Web Development&lt;/strong&gt;&lt;/p&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 60px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;Translates plain textual descriptions into &lt;strong&gt;fully functional&lt;/strong&gt;, &lt;strong&gt;visually appealing&lt;/strong&gt; front-end web code for rapid interface creation.&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
   &lt;td width="30%" align="center" style="vertical-align: top; padding: 20px;"&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;h3 style="margin: 0; padding: 0;"&gt;‚öôÔ∏è &lt;strong&gt;Text2Backend&lt;/strong&gt;&lt;/h3&gt; 
    &lt;/div&gt; 
    &lt;div align="center" style="margin: 15px 0;"&gt; 
     &lt;img src="https://img.shields.io/badge/BACKEND-DEVELOPMENT-9b59b6?style=for-the-badge&amp;amp;logo=server&amp;amp;logoColor=white" alt="Backend Badge" /&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;&lt;strong&gt;Automated Back-End Development&lt;/strong&gt;&lt;/p&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 60px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;Generates &lt;strong&gt;efficient&lt;/strong&gt;, &lt;strong&gt;scalable&lt;/strong&gt;, and &lt;strong&gt;feature-rich&lt;/strong&gt; back-end code from simple text inputs, streamlining server-side development.&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;br /&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìä Experimental Results&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/HKUDS/DeepCode/main/assets/result_main02.jpg" /&gt;
 &lt;br /&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;p&gt;We evaluate &lt;strong&gt;DeepCode&lt;/strong&gt; on the &lt;a href="https://openai.com/index/paperbench/"&gt;&lt;em&gt;PaperBench&lt;/em&gt;&lt;/a&gt; benchmark (released by OpenAI), a rigorous testbed requiring AI agents to independently reproduce 20 ICML 2024 papers from scratch. The benchmark comprises 8,316 gradable components assessed using SimpleJudge with hierarchical weighting.&lt;/p&gt; 
&lt;p&gt;Our experiments compare DeepCode against four baseline categories: &lt;strong&gt;(1) Human Experts&lt;/strong&gt;, &lt;strong&gt;(2) State-of-the-Art Commercial Code Agents&lt;/strong&gt;, &lt;strong&gt;(3) Scientific Code Agents&lt;/strong&gt;, and &lt;strong&gt;(4) LLM-Based Agents&lt;/strong&gt;.&lt;/p&gt; 
&lt;h3&gt;‚ë† üß† Human Expert Performance (Top Machine Learning PhD)&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;DeepCode: 75.9% vs. Top Machine Learning PhD: 72.4% (+3.5%)&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;DeepCode achieves &lt;strong&gt;75.9%&lt;/strong&gt; on the 3-paper human evaluation subset, &lt;strong&gt;surpassing the best-of-3 human expert baseline (72.4%) by +3.5 percentage points&lt;/strong&gt;. This demonstrates that our framework not only matches but exceeds expert-level code reproduction capabilities, representing a significant milestone in autonomous scientific software engineering.&lt;/p&gt; 
&lt;h3&gt;‚ë° üíº State-of-the-Art Commercial Code Agents&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;DeepCode: 84.8% vs. Best Commercial Agent: 58.7% (+26.1%)&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;On the 5-paper subset, DeepCode substantially outperforms leading commercial coding tools:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Cursor: 58.4%&lt;/li&gt; 
 &lt;li&gt;Claude Code: 58.7%&lt;/li&gt; 
 &lt;li&gt;Codex: 40.0%&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;DeepCode: 84.8%&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This represents a &lt;strong&gt;+26.1% improvement&lt;/strong&gt; over the leading commercial code agent. All commercial agents utilize Claude Sonnet 4.5 or GPT-5 Codex-high, highlighting that &lt;strong&gt;DeepCode's superior architecture&lt;/strong&gt;‚Äîrather than base model capability‚Äîdrives this performance gap.&lt;/p&gt; 
&lt;h3&gt;‚ë¢ üî¨ Scientific Code Agents&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;DeepCode: 73.5% vs. PaperCoder: 51.1% (+22.4%)&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Compared to PaperCoder (&lt;strong&gt;51.1%&lt;/strong&gt;), the state-of-the-art scientific code reproduction framework, DeepCode achieves &lt;strong&gt;73.5%&lt;/strong&gt;, demonstrating a &lt;strong&gt;+22.4% relative improvement&lt;/strong&gt;. This substantial margin validates our multi-module architecture combining planning, hierarchical task decomposition, code generation, and iterative debugging over simpler pipeline-based approaches.&lt;/p&gt; 
&lt;h3&gt;‚ë£ ü§ñ LLM-Based Agents&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;DeepCode: 73.5% vs. Best LLM Agent: 43.3% (+30.2%)&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;DeepCode significantly outperforms all tested LLM agents:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Claude 3.5 Sonnet + IterativeAgent: 27.5%&lt;/li&gt; 
 &lt;li&gt;o1 + IterativeAgent (36 hours): 42.4%&lt;/li&gt; 
 &lt;li&gt;o1 BasicAgent: 43.3%&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;DeepCode: 73.5%&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The &lt;strong&gt;+30.2% improvement&lt;/strong&gt; over the best-performing LLM agent demonstrates that sophisticated agent scaffolding, rather than extended inference time or larger models, is critical for complex code reproduction tasks.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;üéØ &lt;strong&gt;Autonomous Self-Orchestrating Multi-Agent Architecture&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;The Challenges&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;üìÑ &lt;strong&gt;Implementation Complexity&lt;/strong&gt;: Converting academic papers and complex algorithms into working code requires significant technical effort and domain expertise&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üî¨ &lt;strong&gt;Research Bottleneck&lt;/strong&gt;: Researchers spend valuable time implementing algorithms instead of focusing on their core research and discovery work&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;‚è±Ô∏è &lt;strong&gt;Development Delays&lt;/strong&gt;: Product teams experience long wait times between concept and testable prototypes, slowing down innovation cycles&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üîÑ &lt;strong&gt;Repetitive Coding&lt;/strong&gt;: Developers repeatedly implement similar patterns and functionality instead of building on existing solutions&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;DeepCode&lt;/strong&gt; addresses these workflow inefficiencies by providing reliable automation for common development tasks, streamlining your development workflow from concept to code.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;pre&gt;&lt;code class="language-mermaid"&gt;flowchart LR
    A["üìÑ Research Papers&amp;lt;br/&amp;gt;üí¨ Text Prompts&amp;lt;br/&amp;gt;üåê URLs &amp;amp; Document&amp;lt;br/&amp;gt;üìé Files: PDF, DOC, PPTX, TXT, HTML"] --&amp;gt; B["üß† DeepCode&amp;lt;br/&amp;gt;Multi-Agent Engine"]
    B --&amp;gt; C["üöÄ Algorithm Implementation &amp;lt;br/&amp;gt;üé® Frontend Development &amp;lt;br/&amp;gt;‚öôÔ∏è Backend Development"]

    style A fill:#ff6b6b,stroke:#c0392b,stroke-width:2px,color:#000
    style B fill:#00d4ff,stroke:#0984e3,stroke-width:3px,color:#000
    style C fill:#00b894,stroke:#00a085,stroke-width:2px,color:#000
&lt;/code&gt;&lt;/pre&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üèóÔ∏è Architecture&lt;/h2&gt; 
&lt;h3&gt;üìä &lt;strong&gt;System Overview&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;DeepCode&lt;/strong&gt; is an AI-powered development platform that automates code generation and implementation tasks. Our multi-agent system handles the complexity of translating requirements into functional, well-structured code, allowing you to focus on innovation rather than implementation details.&lt;/p&gt; 
&lt;p&gt;üéØ &lt;strong&gt;Technical Capabilities&lt;/strong&gt;:&lt;/p&gt; 
&lt;p&gt;üß¨ &lt;strong&gt;Research-to-Production Pipeline&lt;/strong&gt;&lt;br /&gt; Multi-modal document analysis engine that extracts algorithmic logic and mathematical models from academic papers. Generates optimized implementations with proper data structures while preserving computational complexity characteristics.&lt;/p&gt; 
&lt;p&gt;ü™Ñ &lt;strong&gt;Natural Language Code Synthesis&lt;/strong&gt;&lt;br /&gt; Context-aware code generation using fine-tuned language models trained on curated code repositories. Maintains architectural consistency across modules while supporting multiple programming languages and frameworks.&lt;/p&gt; 
&lt;p&gt;‚ö° &lt;strong&gt;Automated Prototyping Engine&lt;/strong&gt;&lt;br /&gt; Intelligent scaffolding system generating complete application structures including database schemas, API endpoints, and frontend components. Uses dependency analysis to ensure scalable architecture from initial generation.&lt;/p&gt; 
&lt;p&gt;üíé &lt;strong&gt;Quality Assurance Automation&lt;/strong&gt;&lt;br /&gt; Integrated static analysis with automated unit test generation and documentation synthesis. Employs AST analysis for code correctness and property-based testing for comprehensive coverage.&lt;/p&gt; 
&lt;p&gt;üîÆ &lt;strong&gt;CodeRAG Integration System&lt;/strong&gt;&lt;br /&gt; Advanced retrieval-augmented generation combining semantic vector embeddings with graph-based dependency analysis. Automatically discovers optimal libraries and implementation patterns from large-scale code corpus.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;üîß &lt;strong&gt;Core Techniques&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;üß† &lt;strong&gt;Intelligent Orchestration Agent&lt;/strong&gt;: Central decision-making system that coordinates workflow phases and analyzes requirements. Employs dynamic planning algorithms to adapt execution strategies in real-time based on evolving project complexity. Dynamically selects optimal processing strategies for each implementation step. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üíæ &lt;strong&gt;Efficient Memory Mechanism&lt;/strong&gt;: Advanced context engineering system that manages large-scale code contexts efficiently. Implements hierarchical memory structures with intelligent compression for handling complex codebases. This component enables instant retrieval of implementation patterns and maintains semantic coherence across extended development sessions. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üîç &lt;strong&gt;Advanced CodeRAG System&lt;/strong&gt;: Global code comprehension engine that analyzes complex inter-dependencies across repositories. Performs cross-codebase relationship mapping to understand architectural patterns from a holistic perspective. This module leverages dependency graphs and semantic analysis to provide globally-aware code recommendations during implementation.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;ü§ñ &lt;strong&gt;Multi-Agent Architecture of DeepCode&lt;/strong&gt;:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üéØ Central Orchestrating Agent&lt;/strong&gt;: Orchestrates entire workflow execution and makes strategic decisions. Coordinates specialized agents based on input complexity analysis. Implements dynamic task planning and resource allocation algorithms. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üìù Intent Understanding Agent&lt;/strong&gt;: Performs deep semantic analysis of user requirements to decode complex intentions. Extracts functional specifications and technical constraints through advanced NLP processing. Transforms ambiguous human descriptions into precise, actionable development specifications with structured task decomposition. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üìÑ Document Parsing Agent&lt;/strong&gt;: Processes complex technical documents and research papers with advanced parsing capabilities. Extracts algorithms and methodologies using document understanding models. Converts academic concepts into practical implementation specifications through intelligent content analysis. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üèóÔ∏è Code Planning Agent&lt;/strong&gt;: Performs architectural design and technology stack optimization. Dynamic planning for adaptive development roadmaps. Enforces coding standards and generates modular structures through automated design pattern selection.&lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üîç Code Reference Mining Agent&lt;/strong&gt;: Discovers relevant repositories and frameworks through intelligent search algorithms. Analyzes codebases for compatibility and integration potential. Provides recommendations based on similarity metrics and automated dependency analysis. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üìö Code Indexing Agent&lt;/strong&gt;: Builds comprehensive knowledge graphs of discovered codebases. Maintains semantic relationships between code components. Enables intelligent retrieval and cross-reference capabilities. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üß¨ Code Generation Agent&lt;/strong&gt;: Synthesizes gathered information into executable code implementations. Creates functional interfaces and integrates discovered components. Generates comprehensive test suites and documentation for reproducibility.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h4&gt;üõ†Ô∏è &lt;strong&gt;Implementation Tools Matrix&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;üîß Powered by MCP (Model Context Protocol)&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;DeepCode leverages the &lt;strong&gt;Model Context Protocol (MCP)&lt;/strong&gt; standard to seamlessly integrate with various tools and services. This standardized approach ensures reliable communication between AI agents and external systems, enabling powerful automation capabilities.&lt;/p&gt; 
&lt;h5&gt;üì° &lt;strong&gt;MCP Servers &amp;amp; Tools&lt;/strong&gt;&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;üõ†Ô∏è &lt;strong&gt;MCP Server&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;üîß &lt;strong&gt;Primary Function&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;üí° &lt;strong&gt;Purpose &amp;amp; Capabilities&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üîç brave&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Web Search Engine&lt;/td&gt; 
   &lt;td&gt;Real-time information retrieval via Brave Search API&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üåê bocha-mcp&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Alternative Search&lt;/td&gt; 
   &lt;td&gt;Secondary search option with independent API access&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üìÇ filesystem&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;File System Operations&lt;/td&gt; 
   &lt;td&gt;Local file and directory management, read/write operations&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üåê fetch&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Web Content Retrieval&lt;/td&gt; 
   &lt;td&gt;Fetch and extract content from URLs and web resources&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üì• github-downloader&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Repository Management&lt;/td&gt; 
   &lt;td&gt;Clone and download GitHub repositories for analysis&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üìã file-downloader&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Document Processing&lt;/td&gt; 
   &lt;td&gt;Download and convert files (PDF, DOCX, etc.) to Markdown&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;‚ö° command-executor&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;System Commands&lt;/td&gt; 
   &lt;td&gt;Execute bash/shell commands for environment management&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üß¨ code-implementation&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Code Generation Hub&lt;/td&gt; 
   &lt;td&gt;Comprehensive code reproduction with execution and testing&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üìö code-reference-indexer&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Smart Code Search&lt;/td&gt; 
   &lt;td&gt;Intelligent indexing and search of code repositories&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üìÑ document-segmentation&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Smart Document Analysis&lt;/td&gt; 
   &lt;td&gt;Intelligent document segmentation for large papers and technical documents&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h5&gt;üîß &lt;strong&gt;Legacy Tool Functions&lt;/strong&gt; &lt;em&gt;(for reference)&lt;/em&gt;&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;üõ†Ô∏è &lt;strong&gt;Function&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;üéØ &lt;strong&gt;Usage Context&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üìÑ read_code_mem&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Efficient code context retrieval from memory&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;‚úçÔ∏è write_file&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Direct file content generation and modification&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üêç execute_python&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Python code testing and validation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üìÅ get_file_structure&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Project structure analysis and organization&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;‚öôÔ∏è set_workspace&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Dynamic workspace and environment configuration&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üìä get_operation_history&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Process monitoring and operation tracking&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;p&gt;üéõÔ∏è &lt;strong&gt;Multi-Interface Framework&lt;/strong&gt;&lt;br /&gt; RESTful API with CLI and web frontends featuring real-time code streaming, interactive debugging, and extensible plugin architecture for CI/CD integration.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;üöÄ Multi-Agent Intelligent Pipeline:&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;h3&gt;üåü &lt;strong&gt;Intelligence Processing Flow&lt;/strong&gt;&lt;/h3&gt; 
 &lt;table align="center" width="100%" style="border: none; border-collapse: collapse;"&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td colspan="3" align="center" style="padding: 20px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; color: white; font-weight: bold;"&gt; üí° &lt;strong&gt;INPUT LAYER&lt;/strong&gt;&lt;br /&gt; üìÑ Research Papers ‚Ä¢ üí¨ Natural Language ‚Ä¢ üåê URLs ‚Ä¢ üìã Requirements &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="20"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td colspan="3" align="center" style="padding: 15px; background: linear-gradient(135deg, #ff6b6b 0%, #ee5a24 100%); border-radius: 12px; color: white; font-weight: bold;"&gt; üéØ &lt;strong&gt;CENTRAL ORCHESTRATION&lt;/strong&gt;&lt;br /&gt; Strategic Decision Making ‚Ä¢ Workflow Coordination ‚Ä¢ Agent Management &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="15"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center" style="padding: 12px; background: linear-gradient(135deg, #3742fa 0%, #2f3542 100%); border-radius: 10px; color: white; width: 50%;"&gt; üìù &lt;strong&gt;TEXT ANALYSIS&lt;/strong&gt;&lt;br /&gt; &lt;small&gt;Requirement Processing&lt;/small&gt; &lt;/td&gt; 
    &lt;td width="10"&gt;&lt;/td&gt; 
    &lt;td align="center" style="padding: 12px; background: linear-gradient(135deg, #8c7ae6 0%, #9c88ff 100%); border-radius: 10px; color: white; width: 50%;"&gt; üìÑ &lt;strong&gt;DOCUMENT ANALYSIS&lt;/strong&gt;&lt;br /&gt; &lt;small&gt;Paper &amp;amp; Spec Processing&lt;/small&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="15"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td colspan="3" align="center" style="padding: 15px; background: linear-gradient(135deg, #00d2d3 0%, #54a0ff 100%); border-radius: 12px; color: white; font-weight: bold;"&gt; üìã &lt;strong&gt;REPRODUCTION PLANNING&lt;/strong&gt;&lt;br /&gt; Deep Paper Analysis ‚Ä¢ Code Requirements Parsing ‚Ä¢ Reproduction Strategy Development &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="15"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center" style="padding: 12px; background: linear-gradient(135deg, #ffa726 0%, #ff7043 100%); border-radius: 10px; color: white; width: 50%;"&gt; üîç &lt;strong&gt;REFERENCE ANALYSIS&lt;/strong&gt;&lt;br /&gt; &lt;small&gt;Repository Discovery&lt;/small&gt; &lt;/td&gt; 
    &lt;td width="10"&gt;&lt;/td&gt; 
    &lt;td align="center" style="padding: 12px; background: linear-gradient(135deg, #e056fd 0%, #f368e0 100%); border-radius: 10px; color: white; width: 50%;"&gt; üìö &lt;strong&gt;CODE INDEXING&lt;/strong&gt;&lt;br /&gt; &lt;small&gt;Knowledge Graph Building&lt;/small&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="15"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td colspan="3" align="center" style="padding: 15px; background: linear-gradient(135deg, #26de81 0%, #20bf6b 100%); border-radius: 12px; color: white; font-weight: bold;"&gt; üß¨ &lt;strong&gt;CODE IMPLEMENTATION&lt;/strong&gt;&lt;br /&gt; Implementation Generation ‚Ä¢ Testing ‚Ä¢ Documentation &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="15"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td colspan="3" align="center" style="padding: 20px; background: linear-gradient(135deg, #045de9 0%, #09c6f9 100%); border-radius: 15px; color: white; font-weight: bold;"&gt; ‚ö° &lt;strong&gt;OUTPUT DELIVERY&lt;/strong&gt;&lt;br /&gt; üì¶ Complete Codebase ‚Ä¢ üß™ Test Suite ‚Ä¢ üìö Documentation ‚Ä¢ üöÄ Deployment Ready &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;br /&gt; 
 &lt;h3&gt;üîÑ &lt;strong&gt;Process Intelligence Features&lt;/strong&gt;&lt;/h3&gt; 
 &lt;table align="center" style="border: none;"&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td align="center" width="25%" style="padding: 15px;"&gt; 
     &lt;div style="background: #f8f9fa; border-radius: 10px; padding: 15px; border-left: 4px solid #ff6b6b;"&gt; 
      &lt;h4&gt;üéØ Adaptive Flow&lt;/h4&gt; 
      &lt;p&gt;&lt;small&gt;Dynamic agent selection based on input complexity&lt;/small&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
    &lt;td align="center" width="25%" style="padding: 15px;"&gt; 
     &lt;div style="background: #f8f9fa; border-radius: 10px; padding: 15px; border-left: 4px solid #4ecdc4;"&gt; 
      &lt;h4&gt;üß† Smart Coordination&lt;/h4&gt; 
      &lt;p&gt;&lt;small&gt;Intelligent task distribution and parallel processing&lt;/small&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
    &lt;td align="center" width="25%" style="padding: 15px;"&gt; 
     &lt;div style="background: #f8f9fa; border-radius: 10px; padding: 15px; border-left: 4px solid #45b7d1;"&gt; 
      &lt;h4&gt;üîç Context Awareness&lt;/h4&gt; 
      &lt;p&gt;&lt;small&gt;Deep understanding through CodeRAG integration&lt;/small&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
    &lt;td align="center" width="25%" style="padding: 15px;"&gt; 
     &lt;div style="background: #f8f9fa; border-radius: 10px; padding: 15px; border-left: 4px solid #96ceb4;"&gt; 
      &lt;h4&gt;‚ö° Quality Assurance&lt;/h4&gt; 
      &lt;p&gt;&lt;small&gt;Automated testing and validation throughout&lt;/small&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üöÄ Quick Start&lt;/h2&gt; 
&lt;h3&gt;üì¶ &lt;strong&gt;Step 1: Installation&lt;/strong&gt;&lt;/h3&gt; 
&lt;h4&gt;‚ö° &lt;strong&gt;Direct Installation (Recommended)&lt;/strong&gt;&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# üöÄ Install DeepCode package directly
pip install deepcode-hku

# üîë Download configuration files
curl -O https://raw.githubusercontent.com/HKUDS/DeepCode/main/mcp_agent.config.yaml
curl -O https://raw.githubusercontent.com/HKUDS/DeepCode/main/mcp_agent.secrets.yaml

# üîë Configure API keys (required)
# Edit mcp_agent.secrets.yaml with your API keys and base_url:
# - openai: api_key, base_url (for OpenAI/custom endpoints)
# - anthropic: api_key (for Claude models)
# - google: api_key (for Gemini models)

# ü§ñ Select your preferred LLM provider (optional)
# Edit mcp_agent.config.yaml to choose your LLM (line ~106):
# - llm_provider: "google"    # Use Google Gemini models
# - llm_provider: "anthropic" # Use Anthropic Claude models
# - llm_provider: "openai"    # Use OpenAI/compatible models
# Note: If not set or unavailable, will automatically fallback to first available provider

# üîë Configure search API keys for web search (optional)
# Edit mcp_agent.config.yaml to set your API keys:
# - For Brave Search: Set BRAVE_API_KEY: "your_key_here" in brave.env section (line ~28)
# - For Bocha-MCP: Set BOCHA_API_KEY: "your_key_here" in bocha-mcp.env section (line ~74)

# üìÑ Configure document segmentation (optional)
# Edit mcp_agent.config.yaml to control document processing:
# - enabled: true/false (whether to use intelligent document segmentation)
# - size_threshold_chars: 50000 (document size threshold to trigger segmentation)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;üîß &lt;strong&gt;Development Installation (From Source)&lt;/strong&gt;&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìÇ Click to expand development installation options&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h5&gt;üî• &lt;strong&gt;Using UV (Recommended for Development)&lt;/strong&gt;&lt;/h5&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# üîΩ Clone the repository
git clone https://github.com/HKUDS/DeepCode.git
cd DeepCode/

# üì¶ Install UV package manager
curl -LsSf https://astral.sh/uv/install.sh | sh

# üîß Install dependencies with UV
uv venv --python=3.13
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
uv pip install -r requirements.txt

# üîë Configure API keys (required)
# Edit mcp_agent.secrets.yaml with your API keys and base_url:
# - openai: api_key, base_url (for OpenAI/custom endpoints)
# - anthropic: api_key (for Claude models)
# - google: api_key (for Gemini models)

# ü§ñ Select your preferred LLM provider (optional)
# Edit mcp_agent.config.yaml to choose your LLM (line ~106):
# - llm_provider: "google"    # Use Google Gemini models
# - llm_provider: "anthropic" # Use Anthropic Claude models
# - llm_provider: "openai"    # Use OpenAI/compatible models
# Note: If not set or unavailable, will automatically fallback to first available provider

# üîë Configure search API keys for web search (optional)
# Edit mcp_agent.config.yaml to set your API keys:
# - For Brave Search: Set BRAVE_API_KEY: "your_key_here" in brave.env section (line ~28)
# - For Bocha-MCP: Set BOCHA_API_KEY: "your_key_here" in bocha-mcp.env section (line ~74)

# üìÑ Configure document segmentation (optional)
# Edit mcp_agent.config.yaml to control document processing:
# - enabled: true/false (whether to use intelligent document segmentation)
# - size_threshold_chars: 50000 (document size threshold to trigger segmentation)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h5&gt;üêç &lt;strong&gt;Using Traditional pip&lt;/strong&gt;&lt;/h5&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# üîΩ Clone the repository
git clone https://github.com/HKUDS/DeepCode.git
cd DeepCode/

# üì¶ Install dependencies
pip install -r requirements.txt

# üîë Configure API keys (required)
# Edit mcp_agent.secrets.yaml with your API keys and base_url:
# - openai: api_key, base_url (for OpenAI/custom endpoints)
# - anthropic: api_key (for Claude models)
# - google: api_key (for Gemini models)

# ü§ñ Select your preferred LLM provider (optional)
# Edit mcp_agent.config.yaml to choose your LLM (line ~106):
# - llm_provider: "google"    # Use Google Gemini models
# - llm_provider: "anthropic" # Use Anthropic Claude models
# - llm_provider: "openai"    # Use OpenAI/compatible models
# Note: If not set or unavailable, will automatically fallback to first available provider

# üîë Configure search API keys for web search (optional)
# Edit mcp_agent.config.yaml to set your API keys:
# - For Brave Search: Set BRAVE_API_KEY: "your_key_here" in brave.env section (line ~28)
# - For Bocha-MCP: Set BOCHA_API_KEY: "your_key_here" in bocha-mcp.env section (line ~74)

# üìÑ Configure document segmentation (optional)
# Edit mcp_agent.config.yaml to control document processing:
# - enabled: true/false (whether to use intelligent document segmentation)
# - size_threshold_chars: 50000 (document size threshold to trigger segmentation)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h4&gt;ü™ü &lt;strong&gt;Windows Users: Additional MCP Server Configuration&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;If you're using Windows, you may need to configure MCP servers manually in &lt;code&gt;mcp_agent.config.yaml&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 1. Install MCP servers globally
npm i -g @modelcontextprotocol/server-brave-search
npm i -g @modelcontextprotocol/server-filesystem

# 2. Find your global node_modules path
npm -g root
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then update your &lt;code&gt;mcp_agent.config.yaml&lt;/code&gt; to use absolute paths:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;mcp:
  servers:
    brave:
      command: "node"
      args: ["C:/Program Files/nodejs/node_modules/@modelcontextprotocol/server-brave-search/dist/index.js"]
    filesystem:
      command: "node"
      args: ["C:/Program Files/nodejs/node_modules/@modelcontextprotocol/server-filesystem/dist/index.js", "."]
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Replace the path with your actual global node_modules path from step 2.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;üîç &lt;strong&gt;Search Server Configuration (Optional)&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;DeepCode supports multiple search servers for web search functionality. You can configure your preferred option in &lt;code&gt;mcp_agent.config.yaml&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;# Default search server configuration
# Options: "brave" or "bocha-mcp"
default_search_server: "brave"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Available Options:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üîç Brave Search&lt;/strong&gt; (&lt;code&gt;"brave"&lt;/code&gt;):&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Default option with high-quality search results&lt;/li&gt; 
   &lt;li&gt;Requires BRAVE_API_KEY configuration&lt;/li&gt; 
   &lt;li&gt;Recommended for most users&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üåê Bocha-MCP&lt;/strong&gt; (&lt;code&gt;"bocha-mcp"&lt;/code&gt;):&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Alternative search server option&lt;/li&gt; 
   &lt;li&gt;Requires BOCHA_API_KEY configuration&lt;/li&gt; 
   &lt;li&gt;Uses local Python server implementation&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;API Key Configuration in mcp_agent.config.yaml:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;# For Brave Search (default) - around line 28
brave:
  command: "npx"
  args: ["-y", "@modelcontextprotocol/server-brave-search"]
  env:
    BRAVE_API_KEY: "your_brave_api_key_here"

# For Bocha-MCP (alternative) - around line 74
bocha-mcp:
  command: "python"
  args: ["tools/bocha_search_server.py"]
  env:
    PYTHONPATH: "."
    BOCHA_API_KEY: "your_bocha_api_key_here"
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;üí° Tip&lt;/strong&gt;: Both search servers require API key configuration. Choose the one that best fits your API access and requirements.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;‚ö° &lt;strong&gt;Step 2: Launch Application&lt;/strong&gt;&lt;/h3&gt; 
&lt;h4&gt;üöÄ &lt;strong&gt;Using Installed Package (Recommended)&lt;/strong&gt;&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# üåê Launch web interface directly
deepcode

# The application will automatically start at http://localhost:8501
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;üõ†Ô∏è &lt;strong&gt;Using Source Code&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;Choose your preferred interface:&lt;/p&gt; 
&lt;h5&gt;üåê &lt;strong&gt;Web Interface&lt;/strong&gt; (Recommended)&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Using UV
uv run streamlit run ui/streamlit_app.py
# Or using traditional Python
streamlit run ui/streamlit_app.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://img.shields.io/badge/Access-localhost:8501-00d4ff?style=flat-square&amp;amp;logo=streamlit&amp;amp;logoColor=white" alt="Web Access" /&gt; 
&lt;/div&gt; 
&lt;h5&gt;üñ•Ô∏è &lt;strong&gt;CLI Interface&lt;/strong&gt; (Advanced Users)&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Using UV
uv run python cli/main_cli.py
# Or using traditional Python
python cli/main_cli.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://img.shields.io/badge/Mode-Interactive_Terminal-9b59b6?style=flat-square&amp;amp;logo=terminal&amp;amp;logoColor=white" alt="CLI Mode" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;üéØ &lt;strong&gt;Step 3: Generate Code&lt;/strong&gt;&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;üìÑ Input&lt;/strong&gt;: Upload your research paper, provide requirements, or paste a URL&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ü§ñ Processing&lt;/strong&gt;: Watch the multi-agent system analyze and plan&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;‚ö° Output&lt;/strong&gt;: Receive production-ready code with tests and documentation&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üí° Examples&lt;/h2&gt; 
&lt;h3&gt;üé¨ &lt;strong&gt;Live Demonstrations&lt;/strong&gt;&lt;/h3&gt; 
&lt;table align="center"&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td width="33%" align="center"&gt; &lt;h4&gt;üìÑ &lt;strong&gt;Paper2Code Demo&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;Research to Implementation&lt;/strong&gt;&lt;/p&gt; 
    &lt;div align="center"&gt; 
     &lt;a href="https://www.youtube.com/watch?v=MQZYpLkzsbw"&gt; &lt;img src="https://img.youtube.com/vi/MQZYpLkzsbw/maxresdefault.jpg" alt="Paper2Code Demo" width="100%" style="border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);" /&gt; &lt;/a&gt; 
     &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.youtube.com/watch?v=MQZYpLkzsbw"&gt;‚ñ∂Ô∏è Watch Demo&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
     &lt;p&gt;&lt;em&gt;Transform academic papers into production-ready code automatically&lt;/em&gt;&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
   &lt;td width="33%" align="center"&gt; &lt;h4&gt;üñºÔ∏è &lt;strong&gt;Image Processing Demo&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;AI-Powered Image Tools&lt;/strong&gt;&lt;/p&gt; 
    &lt;div align="center"&gt; 
     &lt;a href="https://www.youtube.com/watch?v=nFt5mLaMEac"&gt; &lt;img src="https://img.youtube.com/vi/nFt5mLaMEac/maxresdefault.jpg" alt="Image Processing Demo" width="100%" style="border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);" /&gt; &lt;/a&gt; 
     &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.youtube.com/watch?v=nFt5mLaMEac"&gt;‚ñ∂Ô∏è Watch Demo&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
     &lt;p&gt;&lt;em&gt;Intelligent image processing with background removal and enhancement&lt;/em&gt;&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
   &lt;td width="33%" align="center"&gt; &lt;h4&gt;üåê &lt;strong&gt;Frontend Implementation&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;Complete Web Application&lt;/strong&gt;&lt;/p&gt; 
    &lt;div align="center"&gt; 
     &lt;a href="https://www.youtube.com/watch?v=78wx3dkTaAU"&gt; &lt;img src="https://img.youtube.com/vi/78wx3dkTaAU/maxresdefault.jpg" alt="Frontend Demo" width="100%" style="border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);" /&gt; &lt;/a&gt; 
     &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.youtube.com/watch?v=78wx3dkTaAU"&gt;‚ñ∂Ô∏è Watch Demo&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
     &lt;p&gt;&lt;em&gt;Full-stack web development from concept to deployment&lt;/em&gt;&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;üÜï &lt;strong&gt;Recent Updates&lt;/strong&gt;&lt;/h3&gt; 
&lt;h4&gt;üìÑ &lt;strong&gt;Smart Document Segmentation (v1.2.0)&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Intelligent Processing&lt;/strong&gt;: Automatically handles large research papers and technical documents that exceed LLM token limits&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Configurable Control&lt;/strong&gt;: Toggle segmentation via configuration with size-based thresholds&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Semantic Analysis&lt;/strong&gt;: Advanced content understanding with algorithm, concept, and formula preservation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Backward Compatibility&lt;/strong&gt;: Seamlessly falls back to traditional processing for smaller documents&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üöÄ &lt;strong&gt;Coming Soon&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;We're continuously enhancing DeepCode with exciting new features:&lt;/p&gt; 
&lt;h4&gt;üîß &lt;strong&gt;Enhanced Code Reliability &amp;amp; Validation&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Automated Testing&lt;/strong&gt;: Comprehensive functionality testing with execution verification and error detection.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Code Quality Assurance&lt;/strong&gt;: Multi-level validation through static analysis, dynamic testing, and performance benchmarking.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Smart Debugging&lt;/strong&gt;: AI-powered error detection with automatic correction suggestions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;üìä &lt;strong&gt;PaperBench Performance Showcase&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Benchmark Dashboard&lt;/strong&gt;: Comprehensive performance metrics on the PaperBench evaluation suite.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Accuracy Metrics&lt;/strong&gt;: Detailed comparison with state-of-the-art paper reproduction systems.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Success Analytics&lt;/strong&gt;: Statistical analysis across paper categories and complexity levels.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;‚ö° &lt;strong&gt;System-wide Optimizations&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Performance Boost&lt;/strong&gt;: Multi-threaded processing and optimized agent coordination for faster generation.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Enhanced Reasoning&lt;/strong&gt;: Advanced reasoning capabilities with improved context understanding.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Expanded Support&lt;/strong&gt;: Extended compatibility with additional programming languages and frameworks.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;‚≠ê Star History&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;em&gt;Community Growth Trajectory&lt;/em&gt;&lt;/p&gt; 
 &lt;a href="https://star-history.com/#HKUDS/DeepCode&amp;amp;Date"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=HKUDS/DeepCode&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
   &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=HKUDS/DeepCode&amp;amp;type=Date" /&gt; 
   &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=HKUDS/DeepCode&amp;amp;type=Date" style="border-radius: 15px; box-shadow: 0 0 30px rgba(0, 217, 255, 0.3);" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h3&gt;üöÄ &lt;strong&gt;Ready to Transform Development?&lt;/strong&gt;&lt;/h3&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-quick-start"&gt;&lt;img src="https://img.shields.io/badge/üöÄ_Get_Started-00d4ff?style=for-the-badge&amp;amp;logo=rocket&amp;amp;logoColor=white" alt="Get Started" /&gt;&lt;/a&gt; &lt;a href="https://github.com/HKUDS"&gt;&lt;img src="https://img.shields.io/badge/üèõÔ∏è_View_on_GitHub-00d4ff?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white" alt="View on GitHub" /&gt;&lt;/a&gt; &lt;a href="https://github.com/HKUDS/deepcode-agent"&gt;&lt;img src="https://img.shields.io/badge/‚≠ê_Star_Project-00d4ff?style=for-the-badge&amp;amp;logo=star&amp;amp;logoColor=white" alt="Star Project" /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;hr /&gt; 
 &lt;div align="left"&gt; 
  &lt;h3&gt;üìñ &lt;strong&gt;Citation&lt;/strong&gt;&lt;/h3&gt; 
  &lt;p&gt;If you find DeepCode useful in your research or applications, please kindly cite:&lt;/p&gt; 
  &lt;pre&gt;&lt;code&gt;@misc{li2025deepcodeopenagenticcoding,
      title={DeepCode: Open Agentic Coding}, 
      author={Zongwei Li and Zhonghang Li and Zirui Guo and Xubin Ren and Chao Huang},
      year={2025},
      eprint={2512.07921},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2512.07921}, 
}
&lt;/code&gt;&lt;/pre&gt; 
  &lt;hr /&gt; 
  &lt;h3&gt;üìÑ &lt;strong&gt;License&lt;/strong&gt;&lt;/h3&gt; 
  &lt;div align="center"&gt; 
   &lt;img src="https://img.shields.io/badge/License-MIT-4ecdc4?style=for-the-badge&amp;amp;logo=opensourceinitiative&amp;amp;logoColor=white" alt="MIT License" /&gt; 
   &lt;p&gt;&lt;strong&gt;MIT License&lt;/strong&gt; - Copyright (c) 2025 Data Intelligence Lab, The University of Hong Kong&lt;/p&gt; 
   &lt;hr /&gt; 
   &lt;img src="https://visitor-badge.laobi.icu/badge?page_id=deepcode.readme&amp;amp;style=for-the-badge&amp;amp;color=00d4ff" alt="Visitors" /&gt; 
  &lt;/div&gt; 
 &lt;/div&gt;
&lt;/div&gt;</description>
    </item>
    
  </channel>
</rss>