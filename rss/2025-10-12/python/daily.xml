<rss version="2.0">
  <channel>
    <title>GitHub Python Daily Trending</title>
    <description>Daily Trending of Python in GitHub</description>
    <pubDate>Sat, 11 Oct 2025 01:35:53 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>mem0ai/mem0</title>
      <link>https://github.com/mem0ai/mem0</link>
      <description>&lt;p&gt;Universal memory layer for AI Agents; Announcing OpenMemory MCP - local and secure memory management.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://github.com/mem0ai/mem0"&gt; &lt;img src="https://raw.githubusercontent.com/mem0ai/mem0/main/docs/images/banner-sm.png" width="800px" alt="Mem0 - The Memory Layer for Personalized AI" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center" style="display: flex; justify-content: center; gap: 20px; align-items: center;"&gt; &lt;a href="https://trendshift.io/repositories/11194" target="blank"&gt; &lt;img src="https://trendshift.io/api/badge/repositories/11194" alt="mem0ai%2Fmem0 | Trendshift" width="250" height="55" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://mem0.ai"&gt;Learn more&lt;/a&gt; Â· &lt;a href="https://mem0.dev/DiG"&gt;Join Discord&lt;/a&gt; Â· &lt;a href="https://mem0.dev/demo"&gt;Demo&lt;/a&gt; Â· &lt;a href="https://mem0.dev/openmemory"&gt;OpenMemory&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://mem0.dev/DiG"&gt; &lt;img src="https://img.shields.io/badge/Discord-%235865F2.svg?&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Mem0 Discord" /&gt; &lt;/a&gt; &lt;a href="https://pepy.tech/project/mem0ai"&gt; &lt;img src="https://img.shields.io/pypi/dm/mem0ai" alt="Mem0 PyPI - Downloads" /&gt; &lt;/a&gt; &lt;a href="https://github.com/mem0ai/mem0"&gt; &lt;img src="https://img.shields.io/github/commit-activity/m/mem0ai/mem0?style=flat-square" alt="GitHub commit activity" /&gt; &lt;/a&gt; &lt;a href="https://pypi.org/project/mem0ai" target="blank"&gt; &lt;img src="https://img.shields.io/pypi/v/mem0ai?color=%2334D058&amp;amp;label=pypi%20package" alt="Package version" /&gt; &lt;/a&gt; &lt;a href="https://www.npmjs.com/package/mem0ai" target="blank"&gt; &lt;img src="https://img.shields.io/npm/v/mem0ai" alt="Npm package" /&gt; &lt;/a&gt; &lt;a href="https://www.ycombinator.com/companies/mem0"&gt; &lt;img src="https://img.shields.io/badge/Y%20Combinator-S24-orange?style=flat-square" alt="Y Combinator S24" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://mem0.ai/research"&gt;&lt;strong&gt;ğŸ“„ Building Production-Ready AI Agents with Scalable Long-Term Memory â†’&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;strong&gt;âš¡ +26% Accuracy vs. OpenAI Memory â€¢ ğŸš€ 91% Faster â€¢ ğŸ’° 90% Fewer Tokens&lt;/strong&gt; &lt;/p&gt; 
&lt;h2&gt;ğŸ”¥ Research Highlights&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;+26% Accuracy&lt;/strong&gt; over OpenAI Memory on the LOCOMO benchmark&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;91% Faster Responses&lt;/strong&gt; than full-context, ensuring low-latency at scale&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;90% Lower Token Usage&lt;/strong&gt; than full-context, cutting costs without compromise&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mem0.ai/research"&gt;Read the full paper&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Introduction&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://mem0.ai"&gt;Mem0&lt;/a&gt; ("mem-zero") enhances AI assistants and agents with an intelligent memory layer, enabling personalized AI interactions. It remembers user preferences, adapts to individual needs, and continuously learns over timeâ€”ideal for customer support chatbots, AI assistants, and autonomous systems.&lt;/p&gt; 
&lt;h3&gt;Key Features &amp;amp; Use Cases&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Core Capabilities:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-Level Memory&lt;/strong&gt;: Seamlessly retains User, Session, and Agent state with adaptive personalization&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Developer-Friendly&lt;/strong&gt;: Intuitive API, cross-platform SDKs, and a fully managed service option&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Applications:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;AI Assistants&lt;/strong&gt;: Consistent, context-rich conversations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Customer Support&lt;/strong&gt;: Recall past tickets and user history for tailored help&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Healthcare&lt;/strong&gt;: Track patient preferences and history for personalized care&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Productivity &amp;amp; Gaming&lt;/strong&gt;: Adaptive workflows and environments based on user behavior&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸš€ Quickstart Guide &lt;a name="quickstart"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;Choose between our hosted platform or self-hosted package:&lt;/p&gt; 
&lt;h3&gt;Hosted Platform&lt;/h3&gt; 
&lt;p&gt;Get up and running in minutes with automatic updates, analytics, and enterprise security.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Sign up on &lt;a href="https://app.mem0.ai"&gt;Mem0 Platform&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Embed the memory layer via SDK or API keys&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Self-Hosted (Open Source)&lt;/h3&gt; 
&lt;p&gt;Install the sdk via pip:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install mem0ai
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Install sdk via npm:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npm install mem0ai
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Basic Usage&lt;/h3&gt; 
&lt;p&gt;Mem0 requires an LLM to function, with &lt;code&gt;gpt-4o-mini&lt;/code&gt; from OpenAI as the default. However, it supports a variety of LLMs; for details, refer to our &lt;a href="https://docs.mem0.ai/components/llms/overview"&gt;Supported LLMs documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;First step is to instantiate the memory:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openai import OpenAI
from mem0 import Memory

openai_client = OpenAI()
memory = Memory()

def chat_with_memories(message: str, user_id: str = "default_user") -&amp;gt; str:
    # Retrieve relevant memories
    relevant_memories = memory.search(query=message, user_id=user_id, limit=3)
    memories_str = "\n".join(f"- {entry['memory']}" for entry in relevant_memories["results"])

    # Generate Assistant response
    system_prompt = f"You are a helpful AI. Answer the question based on query and memories.\nUser Memories:\n{memories_str}"
    messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": message}]
    response = openai_client.chat.completions.create(model="gpt-4o-mini", messages=messages)
    assistant_response = response.choices[0].message.content

    # Create new memories from the conversation
    messages.append({"role": "assistant", "content": assistant_response})
    memory.add(messages, user_id=user_id)

    return assistant_response

def main():
    print("Chat with AI (type 'exit' to quit)")
    while True:
        user_input = input("You: ").strip()
        if user_input.lower() == 'exit':
            print("Goodbye!")
            break
        print(f"AI: {chat_with_memories(user_input)}")

if __name__ == "__main__":
    main()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For detailed integration steps, see the &lt;a href="https://docs.mem0.ai/quickstart"&gt;Quickstart&lt;/a&gt; and &lt;a href="https://docs.mem0.ai/api-reference"&gt;API Reference&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ğŸ”— Integrations &amp;amp; Demos&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ChatGPT with Memory&lt;/strong&gt;: Personalized chat powered by Mem0 (&lt;a href="https://mem0.dev/demo"&gt;Live Demo&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Browser Extension&lt;/strong&gt;: Store memories across ChatGPT, Perplexity, and Claude (&lt;a href="https://chromewebstore.google.com/detail/onihkkbipkfeijkadecaafbgagkhglop?utm_source=item-share-cb"&gt;Chrome Extension&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Langgraph Support&lt;/strong&gt;: Build a customer bot with Langgraph + Mem0 (&lt;a href="https://docs.mem0.ai/integrations/langgraph"&gt;Guide&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;CrewAI Integration&lt;/strong&gt;: Tailor CrewAI outputs with Mem0 (&lt;a href="https://docs.mem0.ai/integrations/crewai"&gt;Example&lt;/a&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ“š Documentation &amp;amp; Support&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Full docs: &lt;a href="https://docs.mem0.ai"&gt;https://docs.mem0.ai&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Community: &lt;a href="https://mem0.dev/DiG"&gt;Discord&lt;/a&gt; Â· &lt;a href="https://x.com/mem0ai"&gt;Twitter&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Contact: &lt;a href="mailto:founders@mem0.ai"&gt;founders@mem0.ai&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;We now have a paper you can cite:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@article{mem0,
  title={Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory},
  author={Chhikara, Prateek and Khant, Dev and Aryan, Saket and Singh, Taranjeet and Yadav, Deshraj},
  journal={arXiv preprint arXiv:2504.19413},
  year={2025}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;âš–ï¸ License&lt;/h2&gt; 
&lt;p&gt;Apache 2.0 â€” see the &lt;a href="https://github.com/mem0ai/mem0/raw/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>WECENG/ticket-purchase</title>
      <link>https://github.com/WECENG/ticket-purchase</link>
      <description>&lt;p&gt;å¤§éº¦è‡ªåŠ¨æŠ¢ç¥¨ï¼Œæ”¯æŒäººå‘˜ã€åŸå¸‚ã€æ—¥æœŸåœºæ¬¡ã€ä»·æ ¼é€‰æ‹©&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;å¤§éº¦æŠ¢ç¥¨è„šæœ¬ V1.0&lt;/h1&gt; 
&lt;h3&gt;ç‰¹å¾&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;è‡ªåŠ¨æ— å»¶æ—¶æŠ¢ç¥¨&lt;/li&gt; 
 &lt;li&gt;æ”¯æŒäººå‘˜ã€åŸå¸‚ã€æ—¥æœŸåœºæ¬¡ã€ä»·æ ¼é€‰æ‹©&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;åŠŸèƒ½ä»‹ç»&lt;/h2&gt; 
&lt;p&gt;é€šè¿‡seleniumæ‰“å¼€é¡µé¢è¿›è¡Œç™»å½•ï¼Œæ¨¡æ‹Ÿç”¨æˆ·è´­ç¥¨æµç¨‹è‡ªåŠ¨è´­ç¥¨&lt;/p&gt; 
&lt;p&gt;å…¶æµç¨‹å›¾å¦‚ä¸‹:&lt;/p&gt; 
&lt;img src="img/å¤§éº¦æŠ¢ç¥¨æµç¨‹.png" width="50%" height="50%" /&gt; 
&lt;h2&gt;å‡†å¤‡å·¥ä½œ&lt;/h2&gt; 
&lt;h3&gt;1. é…ç½®ç¯å¢ƒ&lt;/h3&gt; 
&lt;h4&gt;1.1å®‰è£…python3ç¯å¢ƒ&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;Windows&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;è®¿é—®Pythonå®˜æ–¹ç½‘ç«™ï¼š&lt;a href="https://www.python.org/downloads/windows/"&gt;https://www.python.org/downloads/windows/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ä¸‹è½½æœ€æ–°çš„Python 3.9+ç‰ˆæœ¬çš„å®‰è£…ç¨‹åºã€‚&lt;/li&gt; 
 &lt;li&gt;è¿è¡Œå®‰è£…ç¨‹åºã€‚&lt;/li&gt; 
 &lt;li&gt;åœ¨å®‰è£…ç¨‹åºä¸­ï¼Œç¡®ä¿å‹¾é€‰ "Add Python X.X to PATH" é€‰é¡¹ï¼Œè¿™å°†è‡ªåŠ¨å°†Pythonæ·»åŠ åˆ°ç³»ç»Ÿç¯å¢ƒå˜é‡ä¸­ï¼Œæ–¹ä¾¿åœ¨å‘½ä»¤è¡Œä¸­ä½¿ç”¨Pythonã€‚&lt;/li&gt; 
 &lt;li&gt;å®Œæˆå®‰è£…åï¼Œä½ å¯ä»¥åœ¨å‘½ä»¤æç¤ºç¬¦æˆ–PowerShellä¸­è¾“å…¥ &lt;code&gt;python3&lt;/code&gt; æ¥å¯åŠ¨Pythonè§£é‡Šå™¨ã€‚&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;macOS&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;ä½ å¯ä»¥ä½¿ç”¨Homebrewæ¥å®‰è£…Python 3ã€‚&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;å®‰è£…Homebrewï¼ˆå¦‚æœæœªå®‰è£…ï¼‰ï¼šæ‰“å¼€ç»ˆç«¯å¹¶è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;å®‰è£…Python 3ï¼šè¿è¡Œä»¥ä¸‹å‘½ä»¤æ¥å®‰è£…Python 3ï¼š&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;brew install python@3
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;1.2 å®‰è£…æ‰€éœ€è¦çš„ç¯å¢ƒ&lt;/h4&gt; 
&lt;p&gt;åœ¨å‘½ä»¤çª—å£è¾“å…¥å¦‚ä¸‹æŒ‡ä»¤&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip3 install selenium
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;1.3 ä¸‹è½½google chromeæµè§ˆå™¨&lt;/h4&gt; 
&lt;p&gt;ä¸‹è½½åœ°å€: &lt;a href="https://www.google.cn/intl/zh-CN/chrome/?brand=YTUH&amp;amp;gclid=Cj0KCQjwj5mpBhDJARIsAOVjBdoV_1sBwdqKGHV3rUU1vJmNKZdy5QNzbRT8F5O0-_jq1WHXurE8a7MaAkWrEALw_wcB&amp;amp;gclsrc=aw.ds"&gt;https://www.google.cn/intl/zh-CN/chrome/?brand=YTUH&amp;amp;gclid=Cj0KCQjwj5mpBhDJARIsAOVjBdoV_1sBwdqKGHV3rUU1vJmNKZdy5QNzbRT8F5O0-_jq1WHXurE8a7MaAkWrEALw_wcB&amp;amp;gclsrc=aw.ds&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;2. ä¿®æ”¹é…ç½®æ–‡ä»¶&lt;/h3&gt; 
&lt;p&gt;åœ¨è¿è¡Œç¨‹åºä¹‹å‰ï¼Œéœ€è¦å…ˆä¿®æ”¹&lt;code&gt;config.json&lt;/code&gt;æ–‡ä»¶ã€‚è¯¥æ–‡ä»¶ç”¨äºæŒ‡å®šç”¨æˆ·éœ€è¦æŠ¢ç¥¨çš„ç›¸å…³ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ¼”å”±ä¼šçš„åœºæ¬¡ã€è§‚æ¼”çš„äººå‘˜ã€åŸå¸‚ã€æ—¥æœŸã€ä»·æ ¼ç­‰ã€‚æ–‡ä»¶ç»“æœå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/WECENG/ticket-purchase/master/img/config_json.png" width="50%" height="50%" /&gt; 
&lt;h4&gt;2.1 æ–‡ä»¶å†…å®¹è¯´æ˜&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;index_url&lt;/code&gt;ä¸ºå¤§éº¦ç½‘çš„åœ°å€ï¼Œ&lt;strong&gt;æ— éœ€ä¿®æ”¹&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;login_url&lt;/code&gt;ä¸ºå¤§éº¦ç½‘çš„ç™»å½•åœ°å€ï¼Œ&lt;strong&gt;æ— éœ€ä¿®æ”¹&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;target_url&lt;/code&gt;ä¸ºç”¨æˆ·éœ€è¦æŠ¢çš„æ¼”å”±ä¼šç¥¨çš„ç›®æ ‡åœ°å€ï¼Œ&lt;strong&gt;å¾…ä¿®æ”¹&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;users&lt;/code&gt;ä¸ºè§‚æ¼”äººçš„å§“åï¼Œ&lt;strong&gt;è§‚æ¼”äººéœ€è¦ç”¨æˆ·åœ¨æ‰‹æœºå¤§éº¦APPä¸­å…ˆå¡«å†™å¥½ï¼Œç„¶åå†å¡«å…¥è¯¥é…ç½®æ–‡ä»¶ä¸­&lt;/strong&gt;ï¼Œ&lt;strong&gt;å¾…ä¿®æ”¹&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;city&lt;/code&gt;ä¸ºåŸå¸‚ï¼Œ&lt;strong&gt;å¦‚æœç”¨æˆ·éœ€è¦æŠ¢çš„æ¼”å”±ä¼šç¥¨éœ€è¦é€‰æ‹©åŸå¸‚ï¼Œè¯·æŠŠåŸå¸‚å¡«å…¥æ­¤å¤„ã€‚å¦‚æ— éœ€é€‰æ‹©ï¼Œåˆ™ä¸å¡«&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;date&lt;/code&gt;ä¸ºåœºæ¬¡æ—¥æœŸï¼Œ&lt;strong&gt;å¾…ä¿®æ”¹ï¼Œå¯å¤šé€‰&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;price&lt;/code&gt;ä¸ºç¥¨æ¡£çš„ä»·æ ¼ï¼Œ&lt;strong&gt;å¾…ä¿®æ”¹ï¼Œå¯å¤šé€‰&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;if_commit_order&lt;/code&gt;ä¸ºæ˜¯å¦è¦è‡ªåŠ¨æäº¤è®¢å•ï¼Œ&lt;strong&gt;æ”¹æˆ true&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;if_listenä¸ºæ˜¯å¦å›æµç›‘å¬ï¼Œ&lt;strong&gt;æ”¹æˆtrue&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;2.2 ç¤ºä¾‹è¯´æ˜&lt;/h4&gt; 
&lt;p&gt;è¿›å…¥å¤§éº¦ç½‘&lt;a href="https://www.damai.cn/%EF%BC%8C%E9%80%89%E6%8B%A9%E4%BD%A0%E9%9C%80%E8%A6%81%E6%8A%A2%E7%A5%A8%E7%9A%84%E6%BC%94%E5%94%B1%E4%BC%9A%E3%80%82%E5%81%87%E8%AE%BE%E5%A6%82%E4%B8%8B%E5%9B%BE%E6%89%80%E7%A4%BA%EF%BC%9A"&gt;https://www.damai.cn/ï¼Œé€‰æ‹©ä½ éœ€è¦æŠ¢ç¥¨çš„æ¼”å”±ä¼šã€‚å‡è®¾å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š&lt;/a&gt;&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/WECENG/ticket-purchase/master/img/example.png" width="50%" height="50%" /&gt; 
&lt;p&gt;æ¥ä¸‹æ¥æŒ‰ç…§ä¸‹å›¾çš„æ ‡æ³¨å¯¹é…ç½®æ–‡ä»¶è¿›è¡Œä¿®æ”¹ï¼š&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/WECENG/ticket-purchase/master/img/example_detail.png" width="50%" height="50%" /&gt; 
&lt;p&gt;æœ€ç»ˆ&lt;code&gt;config.json&lt;/code&gt;çš„æ–‡ä»¶å†…å®¹å¦‚ä¸‹ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "index_url": "https://www.damai.cn/",
  "login_url": "https://passport.damai.cn/login?ru=https%3A%2F%2Fwww.damai.cn%2F",
  "target_url": "https://detail.damai.cn/item.htm?spm=a2oeg.home.card_0.ditem_1.591b23e1JQGWHg&amp;amp;id=740680932762",
  "users": [
    "åå­—1",
    "åå­—2"
  ],
  "city": "å¹¿å·",
  "date": "2023-10-28",
  "price": "1039",
  "if_listen":true,
  "if_commit_order": true
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3.è¿è¡Œç¨‹åº&lt;/h3&gt; 
&lt;p&gt;è¿è¡Œç¨‹åºå¼€å§‹æŠ¢ç¥¨ï¼Œè¿›å…¥å‘½ä»¤çª—å£ï¼Œæ‰§è¡Œå¦‚ä¸‹å‘½ä»¤ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;cd damai
python3 damai.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;å¤§éº¦appæŠ¢ç¥¨&lt;/h1&gt; 
&lt;p&gt;å¤§éº¦appæŠ¢ç¥¨è„šæœ¬éœ€è¦ä¾èµ–appiumï¼Œå› æ­¤éœ€è¦ç°åœ¨å®‰è£…appium server&amp;amp;clientç¯å¢ƒï¼Œæ­¥éª¤å¦‚ä¸‹ï¼š&lt;/p&gt; 
&lt;h2&gt;appium server&lt;/h2&gt; 
&lt;h3&gt;ä¸‹è½½&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;å…ˆå®‰è£…å¥½nodeç¯å¢ƒï¼ˆå…·å¤‡npmï¼‰nodeç‰ˆæœ¬å·18.0.0&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;å…ˆä¸‹è½½å¹¶å®‰è£…å¥½android sdkï¼Œå¹¶é…ç½®ç¯å¢ƒå˜é‡ï¼ˆappium serverè¿è¡Œéœ€ä¾èµ–android sdk)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ä¸‹è½½appium&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;npm install -g appium
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;æŸ¥çœ‹appiumæ˜¯å¦å®‰è£…æˆåŠŸ&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;appium -v
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ä¸‹è½½UiAutomator2é©±åŠ¨&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;npm install appium-uiautomator2-driver
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;â€‹ å¯èƒ½ä¼šé‡åˆ°å¦‚ä¸‹é”™è¯¯ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-tex"&gt;âœ  xcode git:(master) âœ— npm install appium-uiautomator2-driver

npm ERR! code 1
npm ERR! path /Users/chenweicheng/Documents/xcode/node_modules/appium-uiautomator2-driver/node_modules/appium-chromedriver
npm ERR! command failed
npm ERR! command sh -c node install-npm.js
npm ERR! [11:57:54] Error installing Chromedriver: Request failed with status code 404
npm ERR! [11:57:54] AxiosError: Request failed with status code 404
npm ERR!     at settle (/Users/chenweicheng/Documents/xcode/node_modules/appium-uiautomator2-driver/node_modules/axios/lib/core/settle.js:19:12)
npm ERR!     at IncomingMessage.handleStreamEnd (/Users/chenweicheng/Documents/xcode/node_modules/appium-uiautomator2-driver/node_modules/axios/lib/adapters/http.js:572:11)
npm ERR!     at IncomingMessage.emit (node:events:539:35)
npm ERR!     at endReadableNT (node:internal/streams/readable:1344:12)
npm ERR!     at processTicksAndRejections (node:internal/process/task_queues:82:21)
npm ERR! [11:57:54] Downloading Chromedriver can be skipped by setting the'APPIUM_SKIP_CHROMEDRIVER_INSTALL' environment variable.

npm ERR! A complete log of this run can be found in:
npm ERR!     /Users/chenweicheng/.npm/_logs/2023-10-26T03_57_35_950Z-debug-0.log
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;â€‹ è§£å†³åŠæ³•ï¼ˆæ·»åŠ ç¯å¢ƒå˜é‡ï¼Œé”™è¯¯åŸå› æ˜¯æ²¡æœ‰æ‰¾åˆ°chromeæµè§ˆå™¨é©±åŠ¨ï¼Œå¿½ç•¥å³å¯ï¼‰&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;export APPIUM_SKIP_CHROMEDRIVER_INSTALL=true
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;å¯åŠ¨&lt;/h3&gt; 
&lt;p&gt;å¯åŠ¨appium serverå¹¶ä½¿ç”¨uiautomator2é©±åŠ¨&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;appium --use-plugins uiautomator2
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;å¯åŠ¨æˆåŠŸå°†å‡ºç°å¦‚ä¸‹ä¿¡æ¯ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;[Appium] Welcome to Appium v2.2.1 (REV 2176894a5be5da17a362bf3f20678641a78f4b69)
[Appium] Non-default server args:
[Appium] {
[Appium]   usePlugins: [
[Appium]     'uiautomator2'
[Appium]   ]
[Appium] }
[Appium] Attempting to load driver uiautomator2...
[Appium] Requiring driver at /Users/chenweicheng/Documents/xcode/node_modules/appium-uiautomator2-driver
[Appium] Appium REST http interface listener started on http://0.0.0.0:4723
[Appium] You can provide the following URLs in your client code to connect to this server:
[Appium] 	http://127.0.0.1:4723/ (only accessible from the same host)
[Appium] 	http://172.31.102.45:4723/
[Appium] 	http://198.18.0.1:4723/
[Appium] Available drivers:
[Appium]   - uiautomator2@2.32.3 (automationName 'UiAutomator2')
[Appium] No plugins have been installed. Use the "appium plugin" command to install the one(s) you want to use.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;å…¶ä¸­&lt;code&gt;[Appium] http://127.0.0.1:4723/ (only accessible from the same host) [Appium] http://172.31.102.45:4723/ [Appium] http://198.18.0.1:4723/&lt;/code&gt;ä¸ºappium serverè¿æ¥åœ°å€&lt;/p&gt; 
&lt;h2&gt;appium client&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;å…ˆä¸‹è½½å¹¶å®‰è£…å¥½python3å’Œpip3&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;å®‰è£…&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;pip3 install appium-python-client
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;åœ¨ä»£ç ä¸­å¼•å…¥å¹¶ä½¿ç”¨appium&lt;/p&gt; &lt;pre&gt;&lt;code class="language-python"&gt;from appium import webdriver
from appium.options.common.base import AppiumOptions

device_app_info = AppiumOptions()
device_app_info.set_capability('platformName', 'Android')
device_app_info.set_capability('platformVersion', '10')
device_app_info.set_capability('deviceName', 'YourDeviceName')
device_app_info.set_capability('appPackage', 'cn.damai')
device_app_info.set_capability('appActivity', '.launcher.splash.SplashMainActivity')
device_app_info.set_capability('unicodeKeyboard', True)
device_app_info.set_capability('resetKeyboard', True)
device_app_info.set_capability('noReset', True)
device_app_info.set_capability('newCommandTimeout', 6000)
device_app_info.set_capability('automationName', 'UiAutomator2')

# è¿æ¥appium serverï¼Œserveråœ°å€æŸ¥çœ‹appiumå¯åŠ¨ä¿¡æ¯
driver = webdriver.Remote('http://127.0.0.1:4723', options=device_app_info)

&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;å¯åŠ¨è„šæœ¬ç¨‹åº&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;cd damai_appium
python3 damai_appium.py
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>MODSetter/SurfSense</title>
      <link>https://github.com/MODSetter/SurfSense</link>
      <description>&lt;p&gt;Open Source Alternative to NotebookLM / Perplexity, connected to external sources such as Search Engines, Slack, Linear, Jira, ClickUp, Confluence, Notion, YouTube, GitHub, Discord and more. Join our discord: https://discord.gg/ejRNvftDp9&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/e236b764-0ddc-42ff-a1f1-8fbb3d2e0e65" alt="new_header" /&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://discord.gg/ejRNvftDp9"&gt; &lt;img src="https://img.shields.io/discord/1359368468260192417" alt="Discord" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h1&gt;SurfSense&lt;/h1&gt; 
&lt;p&gt;While tools like NotebookLM and Perplexity are impressive and highly effective for conducting research on any topic/query, SurfSense elevates this capability by integrating with your personal knowledge base. It is a highly customizable AI research agent, connected to external sources such as Search Engines (Tavily, LinkUp), Slack, Linear, Jira, ClickUp, Confluence, Gmail, Notion, YouTube, GitHub, Discord, Airtable, Google Calendar, Luma and more to come.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://trendshift.io/repositories/13606" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/13606" alt="MODSetter%2FSurfSense | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;h1&gt;Video&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/d9221908-e0de-4b2f-ac3a-691cf4b202da"&gt;https://github.com/user-attachments/assets/d9221908-e0de-4b2f-ac3a-691cf4b202da&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Podcast Sample&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/a0a16566-6967-4374-ac51-9b3e07fbecd7"&gt;https://github.com/user-attachments/assets/a0a16566-6967-4374-ac51-9b3e07fbecd7&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;h3&gt;ğŸ’¡ &lt;strong&gt;Idea&lt;/strong&gt;:&lt;/h3&gt; 
&lt;p&gt;Have your own highly customizable private NotebookLM and Perplexity integrated with external sources.&lt;/p&gt; 
&lt;h3&gt;ğŸ“ &lt;strong&gt;Multiple File Format Uploading Support&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Save content from your own personal files &lt;em&gt;(Documents, images, videos and supports &lt;strong&gt;50+ file extensions&lt;/strong&gt;)&lt;/em&gt; to your own personal knowledge base .&lt;/p&gt; 
&lt;h3&gt;ğŸ” &lt;strong&gt;Powerful Search&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Quickly research or find anything in your saved content .&lt;/p&gt; 
&lt;h3&gt;ğŸ’¬ &lt;strong&gt;Chat with your Saved Content&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Interact in Natural Language and get cited answers.&lt;/p&gt; 
&lt;h3&gt;ğŸ“„ &lt;strong&gt;Cited Answers&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Get Cited answers just like Perplexity.&lt;/p&gt; 
&lt;h3&gt;ğŸ”” &lt;strong&gt;Privacy &amp;amp; Local LLM Support&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Works Flawlessly with Ollama local LLMs.&lt;/p&gt; 
&lt;h3&gt;ğŸ  &lt;strong&gt;Self Hostable&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Open source and easy to deploy locally.&lt;/p&gt; 
&lt;h3&gt;ğŸ™ï¸ Podcasts&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Blazingly fast podcast generation agent. (Creates a 3-minute podcast in under 20 seconds.)&lt;/li&gt; 
 &lt;li&gt;Convert your chat conversations into engaging audio content&lt;/li&gt; 
 &lt;li&gt;Support for local TTS providers (Kokoro TTS)&lt;/li&gt; 
 &lt;li&gt;Support for multiple TTS providers (OpenAI, Azure, Google Vertex AI)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ“Š &lt;strong&gt;Advanced RAG Techniques&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Supports 100+ LLM's&lt;/li&gt; 
 &lt;li&gt;Supports 6000+ Embedding Models.&lt;/li&gt; 
 &lt;li&gt;Supports all major Rerankers (Pinecode, Cohere, Flashrank etc)&lt;/li&gt; 
 &lt;li&gt;Uses Hierarchical Indices (2 tiered RAG setup).&lt;/li&gt; 
 &lt;li&gt;Utilizes Hybrid Search (Semantic + Full Text Search combined with Reciprocal Rank Fusion).&lt;/li&gt; 
 &lt;li&gt;RAG as a Service API Backend.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;â„¹ï¸ &lt;strong&gt;External Sources&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Search Engines (Tavily, LinkUp)&lt;/li&gt; 
 &lt;li&gt;Slack&lt;/li&gt; 
 &lt;li&gt;Linear&lt;/li&gt; 
 &lt;li&gt;Jira&lt;/li&gt; 
 &lt;li&gt;ClickUp&lt;/li&gt; 
 &lt;li&gt;Confluence&lt;/li&gt; 
 &lt;li&gt;Notion&lt;/li&gt; 
 &lt;li&gt;Gmail&lt;/li&gt; 
 &lt;li&gt;Youtube Videos&lt;/li&gt; 
 &lt;li&gt;GitHub&lt;/li&gt; 
 &lt;li&gt;Discord&lt;/li&gt; 
 &lt;li&gt;Airtable&lt;/li&gt; 
 &lt;li&gt;Google Calendar&lt;/li&gt; 
 &lt;li&gt;Luma&lt;/li&gt; 
 &lt;li&gt;and more to come.....&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ“„ &lt;strong&gt;Supported File Extensions&lt;/strong&gt;&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: File format support depends on your ETL service configuration. LlamaCloud supports 50+ formats, Unstructured supports 34+ core formats, and Docling (core formats, local processing, privacy-focused, no API key).&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Documents &amp;amp; Text&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;LlamaCloud&lt;/strong&gt;: &lt;code&gt;.pdf&lt;/code&gt;, &lt;code&gt;.doc&lt;/code&gt;, &lt;code&gt;.docx&lt;/code&gt;, &lt;code&gt;.docm&lt;/code&gt;, &lt;code&gt;.dot&lt;/code&gt;, &lt;code&gt;.dotm&lt;/code&gt;, &lt;code&gt;.rtf&lt;/code&gt;, &lt;code&gt;.txt&lt;/code&gt;, &lt;code&gt;.xml&lt;/code&gt;, &lt;code&gt;.epub&lt;/code&gt;, &lt;code&gt;.odt&lt;/code&gt;, &lt;code&gt;.wpd&lt;/code&gt;, &lt;code&gt;.pages&lt;/code&gt;, &lt;code&gt;.key&lt;/code&gt;, &lt;code&gt;.numbers&lt;/code&gt;, &lt;code&gt;.602&lt;/code&gt;, &lt;code&gt;.abw&lt;/code&gt;, &lt;code&gt;.cgm&lt;/code&gt;, &lt;code&gt;.cwk&lt;/code&gt;, &lt;code&gt;.hwp&lt;/code&gt;, &lt;code&gt;.lwp&lt;/code&gt;, &lt;code&gt;.mw&lt;/code&gt;, &lt;code&gt;.mcw&lt;/code&gt;, &lt;code&gt;.pbd&lt;/code&gt;, &lt;code&gt;.sda&lt;/code&gt;, &lt;code&gt;.sdd&lt;/code&gt;, &lt;code&gt;.sdp&lt;/code&gt;, &lt;code&gt;.sdw&lt;/code&gt;, &lt;code&gt;.sgl&lt;/code&gt;, &lt;code&gt;.sti&lt;/code&gt;, &lt;code&gt;.sxi&lt;/code&gt;, &lt;code&gt;.sxw&lt;/code&gt;, &lt;code&gt;.stw&lt;/code&gt;, &lt;code&gt;.sxg&lt;/code&gt;, &lt;code&gt;.uof&lt;/code&gt;, &lt;code&gt;.uop&lt;/code&gt;, &lt;code&gt;.uot&lt;/code&gt;, &lt;code&gt;.vor&lt;/code&gt;, &lt;code&gt;.wps&lt;/code&gt;, &lt;code&gt;.zabw&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Unstructured&lt;/strong&gt;: &lt;code&gt;.doc&lt;/code&gt;, &lt;code&gt;.docx&lt;/code&gt;, &lt;code&gt;.odt&lt;/code&gt;, &lt;code&gt;.rtf&lt;/code&gt;, &lt;code&gt;.pdf&lt;/code&gt;, &lt;code&gt;.xml&lt;/code&gt;, &lt;code&gt;.txt&lt;/code&gt;, &lt;code&gt;.md&lt;/code&gt;, &lt;code&gt;.markdown&lt;/code&gt;, &lt;code&gt;.rst&lt;/code&gt;, &lt;code&gt;.html&lt;/code&gt;, &lt;code&gt;.org&lt;/code&gt;, &lt;code&gt;.epub&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Docling&lt;/strong&gt;: &lt;code&gt;.pdf&lt;/code&gt;, &lt;code&gt;.docx&lt;/code&gt;, &lt;code&gt;.html&lt;/code&gt;, &lt;code&gt;.htm&lt;/code&gt;, &lt;code&gt;.xhtml&lt;/code&gt;, &lt;code&gt;.adoc&lt;/code&gt;, &lt;code&gt;.asciidoc&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Presentations&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;LlamaCloud&lt;/strong&gt;: &lt;code&gt;.ppt&lt;/code&gt;, &lt;code&gt;.pptx&lt;/code&gt;, &lt;code&gt;.pptm&lt;/code&gt;, &lt;code&gt;.pot&lt;/code&gt;, &lt;code&gt;.potm&lt;/code&gt;, &lt;code&gt;.potx&lt;/code&gt;, &lt;code&gt;.odp&lt;/code&gt;, &lt;code&gt;.key&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Unstructured&lt;/strong&gt;: &lt;code&gt;.ppt&lt;/code&gt;, &lt;code&gt;.pptx&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Docling&lt;/strong&gt;: &lt;code&gt;.pptx&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Spreadsheets &amp;amp; Data&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;LlamaCloud&lt;/strong&gt;: &lt;code&gt;.xlsx&lt;/code&gt;, &lt;code&gt;.xls&lt;/code&gt;, &lt;code&gt;.xlsm&lt;/code&gt;, &lt;code&gt;.xlsb&lt;/code&gt;, &lt;code&gt;.xlw&lt;/code&gt;, &lt;code&gt;.csv&lt;/code&gt;, &lt;code&gt;.tsv&lt;/code&gt;, &lt;code&gt;.ods&lt;/code&gt;, &lt;code&gt;.fods&lt;/code&gt;, &lt;code&gt;.numbers&lt;/code&gt;, &lt;code&gt;.dbf&lt;/code&gt;, &lt;code&gt;.123&lt;/code&gt;, &lt;code&gt;.dif&lt;/code&gt;, &lt;code&gt;.sylk&lt;/code&gt;, &lt;code&gt;.slk&lt;/code&gt;, &lt;code&gt;.prn&lt;/code&gt;, &lt;code&gt;.et&lt;/code&gt;, &lt;code&gt;.uos1&lt;/code&gt;, &lt;code&gt;.uos2&lt;/code&gt;, &lt;code&gt;.wk1&lt;/code&gt;, &lt;code&gt;.wk2&lt;/code&gt;, &lt;code&gt;.wk3&lt;/code&gt;, &lt;code&gt;.wk4&lt;/code&gt;, &lt;code&gt;.wks&lt;/code&gt;, &lt;code&gt;.wq1&lt;/code&gt;, &lt;code&gt;.wq2&lt;/code&gt;, &lt;code&gt;.wb1&lt;/code&gt;, &lt;code&gt;.wb2&lt;/code&gt;, &lt;code&gt;.wb3&lt;/code&gt;, &lt;code&gt;.qpw&lt;/code&gt;, &lt;code&gt;.xlr&lt;/code&gt;, &lt;code&gt;.eth&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Unstructured&lt;/strong&gt;: &lt;code&gt;.xls&lt;/code&gt;, &lt;code&gt;.xlsx&lt;/code&gt;, &lt;code&gt;.csv&lt;/code&gt;, &lt;code&gt;.tsv&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Docling&lt;/strong&gt;: &lt;code&gt;.xlsx&lt;/code&gt;, &lt;code&gt;.csv&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Images&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;LlamaCloud&lt;/strong&gt;: &lt;code&gt;.jpg&lt;/code&gt;, &lt;code&gt;.jpeg&lt;/code&gt;, &lt;code&gt;.png&lt;/code&gt;, &lt;code&gt;.gif&lt;/code&gt;, &lt;code&gt;.bmp&lt;/code&gt;, &lt;code&gt;.svg&lt;/code&gt;, &lt;code&gt;.tiff&lt;/code&gt;, &lt;code&gt;.webp&lt;/code&gt;, &lt;code&gt;.html&lt;/code&gt;, &lt;code&gt;.htm&lt;/code&gt;, &lt;code&gt;.web&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Unstructured&lt;/strong&gt;: &lt;code&gt;.jpg&lt;/code&gt;, &lt;code&gt;.jpeg&lt;/code&gt;, &lt;code&gt;.png&lt;/code&gt;, &lt;code&gt;.bmp&lt;/code&gt;, &lt;code&gt;.tiff&lt;/code&gt;, &lt;code&gt;.heic&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Docling&lt;/strong&gt;: &lt;code&gt;.jpg&lt;/code&gt;, &lt;code&gt;.jpeg&lt;/code&gt;, &lt;code&gt;.png&lt;/code&gt;, &lt;code&gt;.bmp&lt;/code&gt;, &lt;code&gt;.tiff&lt;/code&gt;, &lt;code&gt;.tif&lt;/code&gt;, &lt;code&gt;.webp&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Audio &amp;amp; Video &lt;em&gt;(Always Supported)&lt;/em&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;.mp3&lt;/code&gt;, &lt;code&gt;.mpga&lt;/code&gt;, &lt;code&gt;.m4a&lt;/code&gt;, &lt;code&gt;.wav&lt;/code&gt;, &lt;code&gt;.mp4&lt;/code&gt;, &lt;code&gt;.mpeg&lt;/code&gt;, &lt;code&gt;.webm&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Email &amp;amp; Communication&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Unstructured&lt;/strong&gt;: &lt;code&gt;.eml&lt;/code&gt;, &lt;code&gt;.msg&lt;/code&gt;, &lt;code&gt;.p7s&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;ğŸ”– Cross Browser Extension&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;The SurfSense extension can be used to save any webpage you like.&lt;/li&gt; 
 &lt;li&gt;Its main usecase is to save any webpages protected beyond authentication.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;FEATURE REQUESTS AND FUTURE&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;SurfSense is actively being developed.&lt;/strong&gt; While it's not yet production-ready, you can help us speed up the process.&lt;/p&gt; 
&lt;p&gt;Join the &lt;a href="https://discord.gg/ejRNvftDp9"&gt;SurfSense Discord&lt;/a&gt; and help shape the future of SurfSense!&lt;/p&gt; 
&lt;h2&gt;ğŸš€ Roadmap&lt;/h2&gt; 
&lt;p&gt;Stay up to date with our development progress and upcoming features!&lt;br /&gt; Check out our public roadmap and contribute your ideas or feedback:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;View the Roadmap:&lt;/strong&gt; &lt;a href="https://github.com/users/MODSetter/projects/2"&gt;SurfSense Roadmap on GitHub Projects&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;How to get started?&lt;/h2&gt; 
&lt;h3&gt;Installation Options&lt;/h3&gt; 
&lt;p&gt;SurfSense provides two installation methods:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.surfsense.net/docs/docker-installation"&gt;Docker Installation&lt;/a&gt;&lt;/strong&gt; - The easiest way to get SurfSense up and running with all dependencies containerized.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Includes pgAdmin for database management through a web UI&lt;/li&gt; 
   &lt;li&gt;Supports environment variable customization via &lt;code&gt;.env&lt;/code&gt; file&lt;/li&gt; 
   &lt;li&gt;Flexible deployment options (full stack or core services only)&lt;/li&gt; 
   &lt;li&gt;No need to manually edit configuration files between environments&lt;/li&gt; 
   &lt;li&gt;See &lt;a href="https://raw.githubusercontent.com/MODSetter/SurfSense/main/DOCKER_SETUP.md"&gt;Docker Setup Guide&lt;/a&gt; for detailed instructions&lt;/li&gt; 
   &lt;li&gt;For deployment scenarios and options, see &lt;a href="https://raw.githubusercontent.com/MODSetter/SurfSense/main/DEPLOYMENT_GUIDE.md"&gt;Deployment Guide&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.surfsense.net/docs/manual-installation"&gt;Manual Installation (Recommended)&lt;/a&gt;&lt;/strong&gt; - For users who prefer more control over their setup or need to customize their deployment.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Both installation guides include detailed OS-specific instructions for Windows, macOS, and Linux.&lt;/p&gt; 
&lt;p&gt;Before installation, make sure to complete the &lt;a href="https://www.surfsense.net/docs/"&gt;prerequisite setup steps&lt;/a&gt; including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;PGVector setup&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;File Processing ETL Service&lt;/strong&gt; (choose one): 
  &lt;ul&gt; 
   &lt;li&gt;Unstructured.io API key (supports 34+ formats)&lt;/li&gt; 
   &lt;li&gt;LlamaIndex API key (enhanced parsing, supports 50+ formats)&lt;/li&gt; 
   &lt;li&gt;Docling (local processing, no API key required, supports PDF, Office docs, images, HTML, CSV)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Other required API keys&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Screenshots&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Research Agent&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/e22c5d86-f511-4c72-8c50-feba0c1561b4" alt="updated_researcher" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Search Spaces&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/e254c38c-f937-44b6-9e9d-770db583d099" alt="search_spaces" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Manage Documents&lt;/strong&gt; &lt;img src="https://github.com/user-attachments/assets/7001e306-eb06-4009-89c6-8fadfdc3fc4d" alt="documents" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Podcast Agent&lt;/strong&gt; &lt;img src="https://github.com/user-attachments/assets/6cb82ffd-9e14-4172-bc79-67faf34c4c1c" alt="podcasts" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Agent Chat&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/bb352d52-1c6d-4020-926b-722d0b98b491" alt="git_chat" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Browser Extension&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/1f042b7a-6349-422b-94fb-d40d0df16c40" alt="ext1" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/a9b9f1aa-2677-404d-b0a0-c1b2dddf24a7" alt="ext2" /&gt;&lt;/p&gt; 
&lt;h2&gt;Tech Stack&lt;/h2&gt; 
&lt;h3&gt;&lt;strong&gt;BackEnd&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;FastAPI&lt;/strong&gt;: Modern, fast web framework for building APIs with Python&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;PostgreSQL with pgvector&lt;/strong&gt;: Database with vector search capabilities for similarity searches&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;SQLAlchemy&lt;/strong&gt;: SQL toolkit and ORM (Object-Relational Mapping) for database interactions&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Alembic&lt;/strong&gt;: A database migrations tool for SQLAlchemy.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;FastAPI Users&lt;/strong&gt;: Authentication and user management with JWT and OAuth support&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;LangGraph&lt;/strong&gt;: Framework for developing AI-agents.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;LangChain&lt;/strong&gt;: Framework for developing AI-powered applications.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;LLM Integration&lt;/strong&gt;: Integration with LLM models through LiteLLM&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Rerankers&lt;/strong&gt;: Advanced result ranking for improved search relevance&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Hybrid Search&lt;/strong&gt;: Combines vector similarity and full-text search for optimal results using Reciprocal Rank Fusion (RRF)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Vector Embeddings&lt;/strong&gt;: Document and text embeddings for semantic search&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;pgvector&lt;/strong&gt;: PostgreSQL extension for efficient vector similarity operations&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Chonkie&lt;/strong&gt;: Advanced document chunking and embedding library&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Uses &lt;code&gt;AutoEmbeddings&lt;/code&gt; for flexible embedding model selection&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;LateChunker&lt;/code&gt; for optimized document chunking based on embedding model's max sequence length&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;&lt;strong&gt;FrontEnd&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Next.js 15.2.3&lt;/strong&gt;: React framework featuring App Router, server components, automatic code-splitting, and optimized rendering.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;React 19.0.0&lt;/strong&gt;: JavaScript library for building user interfaces.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;TypeScript&lt;/strong&gt;: Static type-checking for JavaScript, enhancing code quality and developer experience.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Vercel AI SDK Kit UI Stream Protocol&lt;/strong&gt;: To create scalable chat UI.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Tailwind CSS 4.x&lt;/strong&gt;: Utility-first CSS framework for building custom UI designs.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Shadcn&lt;/strong&gt;: Headless components library.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Lucide React&lt;/strong&gt;: Icon set implemented as React components.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Framer Motion&lt;/strong&gt;: Animation library for React.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Sonner&lt;/strong&gt;: Toast notification library.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Geist&lt;/strong&gt;: Font family from Vercel.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;React Hook Form&lt;/strong&gt;: Form state management and validation.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Zod&lt;/strong&gt;: TypeScript-first schema validation with static type inference.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;@hookform/resolvers&lt;/strong&gt;: Resolvers for using validation libraries with React Hook Form.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;@tanstack/react-table&lt;/strong&gt;: Headless UI for building powerful tables &amp;amp; datagrids.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;strong&gt;DevOps&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Docker&lt;/strong&gt;: Container platform for consistent deployment across environments&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Docker Compose&lt;/strong&gt;: Tool for defining and running multi-container Docker applications&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;pgAdmin&lt;/strong&gt;: Web-based PostgreSQL administration tool included in Docker setup&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;strong&gt;Extension&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Manifest v3 on Plasmo&lt;/p&gt; 
&lt;h2&gt;Future Work&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Add More Connectors.&lt;/li&gt; 
 &lt;li&gt;Patch minor bugs.&lt;/li&gt; 
 &lt;li&gt;Document Podcasts&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contribute&lt;/h2&gt; 
&lt;p&gt;Contributions are very welcome! A contribution can be as small as a â­ or even finding and creating issues. Fine-tuning the Backend is always desired.&lt;/p&gt; 
&lt;p&gt;For detailed contribution guidelines, please see our &lt;a href="https://raw.githubusercontent.com/MODSetter/SurfSense/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; file.&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;a href="https://www.star-history.com/#MODSetter/SurfSense&amp;amp;Date"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=MODSetter/SurfSense&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=MODSetter/SurfSense&amp;amp;type=Date" /&gt; 
  &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=MODSetter/SurfSense&amp;amp;type=Date" /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;hr /&gt; 
&lt;hr /&gt; 
&lt;p align="center"&gt; &lt;img src="https://github.com/user-attachments/assets/329c9bc2-6005-4aed-a629-700b5ae296b4" alt="Catalyst Project" width="200" /&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;hr /&gt;</description>
    </item>
    
    <item>
      <title>intruder-io/autoswagger</title>
      <link>https://github.com/intruder-io/autoswagger</link>
      <description>&lt;p&gt;Autoswagger by Intruder - detect API auth weaknesses&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;a href="https://www.intruder.io/research/broken-authorization-apis-autoswagger"&gt;Autoswagger&lt;/a&gt; by &lt;a href="https://intruder.io/"&gt;Intruder&lt;/a&gt;&lt;/h1&gt; 
&lt;a href="https://intruder.io/"&gt; &lt;img width="966" alt="output" src="https://github.com/user-attachments/assets/e502abaf-426c-4fab-ad60-d7b5dcd730d8" /&gt; &lt;/a&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://www.intruder.io/research/broken-authorization-apis-autoswagger"&gt;Autoswagger&lt;/a&gt;&lt;/strong&gt; is a command-line tool designed to discover, parse, and test for unauthenticated endpoints using &lt;strong&gt;Swagger/OpenAPI&lt;/strong&gt; documentation. It helps identify potential security issues in unprotected endpoints of APIs, such as PII leaks and common secret exposures.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Please note that this initial release of Autoswagger is by no means complete, and there are some types of specification which the tool does not currently handle. Please feel free to use it as you wish, and extend its detection capabilities or add detection regexes to cover your specific use-case!&lt;/strong&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/intruder-io/autoswagger/main/#introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/intruder-io/autoswagger/main/#key-features"&gt;Key Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/intruder-io/autoswagger/main/#installation--usage"&gt;Installation &amp;amp; Usage&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/intruder-io/autoswagger/main/#discovery-phases"&gt;Discovery Phases&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/intruder-io/autoswagger/main/#endpoint-testing"&gt;Endpoint Testing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/intruder-io/autoswagger/main/#pii-detection"&gt;PII Detection&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/intruder-io/autoswagger/main/#output"&gt;Output Examples&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/intruder-io/autoswagger/main/#stats--reporting"&gt;Stats &amp;amp; Reporting&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/intruder-io/autoswagger/main/#acknowledgments"&gt;Acknowledgments&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;Autoswagger automates the process of finding &lt;strong&gt;OpenAPI/Swagger&lt;/strong&gt; specifications, extracting API endpoints, and systematically testing them for &lt;strong&gt;PII&lt;/strong&gt; exposure, &lt;strong&gt;secrets&lt;/strong&gt;, and large or interesting responses. It leverages &lt;strong&gt;Presidio&lt;/strong&gt; for PII recognition and &lt;strong&gt;regex&lt;/strong&gt; for sensitive key/token detection.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Multiple Discovery Phases&lt;/strong&gt;&lt;br /&gt; Discovers OpenAPI specs in three ways:&lt;/p&gt; 
  &lt;ol&gt; 
   &lt;li&gt;&lt;strong&gt;Direct Spec&lt;/strong&gt;: If a full URL with a path ending in &lt;code&gt;.json&lt;/code&gt;, &lt;code&gt;.yaml&lt;/code&gt;, or &lt;code&gt;.yml&lt;/code&gt; is provided, parse that file directly.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Swagger UI&lt;/strong&gt;: Parse known paths of Swagger UI (e.g. &lt;code&gt;/swagger-ui.html&lt;/code&gt;), and extract spec from HTML or JavaScript.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Direct Spec by Bruteforce&lt;/strong&gt;: Attempt discovery using common OpenAPI schema locations (&lt;code&gt;/swagger.json&lt;/code&gt;, &lt;code&gt;/openapi.json&lt;/code&gt;, etc.). Only attempt this if 1. and 2. did not yield a result.&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Parallel Endpoint Testing&lt;/strong&gt;&lt;br /&gt; Multi-threaded concurrent testing of many endpoints, respecting a configurable rate limit (&lt;code&gt;-rate&lt;/code&gt;).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Brute-Force of Parameter Values&lt;/strong&gt;&lt;br /&gt; If &lt;code&gt;-b&lt;/code&gt; or &lt;code&gt;--brute&lt;/code&gt; is used, try using various data types with a few example values in an attempt to bypass parameter-specific validations.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Presidio PII Detection&lt;/strong&gt;&lt;br /&gt; Check output for phone numbers, emails, addresses, and names (with context validation to reduce false positives). Also parse CSV rows and naive â€œkey: valueâ€ lines.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Secrets Detection&lt;/strong&gt;&lt;br /&gt; Leverages a set of regex patterns to detect tokens, keys, and debugging artifacts (like environment variables).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Command Line or JSON Output&lt;/strong&gt;&lt;br /&gt; In default mode, displays results in a table. With &lt;code&gt;-json&lt;/code&gt;, output a JSON structure. &lt;code&gt;-product&lt;/code&gt; mode filters output to only show those that contain PII, secrets, or large responses.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Installation &amp;amp; Usage&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Clone&lt;/strong&gt; or &lt;strong&gt;download&lt;/strong&gt; the repository containing Autoswagger.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone git@github.com:intruder-io/autoswagger.git
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install dependencies&lt;/strong&gt; (e.g., using Python 3.7+):&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;(It's recommended to use a virtual environment for this: &lt;code&gt;python3 -m venv venv;source venv/bin/activate&lt;/code&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Check installation, show help:&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python3 autoswagger.py -h
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Flags&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Flag&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;urls&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;List of base URLs or direct spec URLs.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-v, --verbose&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Enables verbose logging. Creates a log file under &lt;code&gt;~/.autoswagger/logs&lt;/code&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-risk&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Includes non-GET methods (POST, PUT, PATCH, DELETE) in testing.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-all&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Includes 200 and 404 endpoints in output (excludes 401/403).&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-product&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Outputs only endpoints with PII or large responses, in JSON format.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-stats&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Displays scan statistics (e.g. requests, RPS, hosts with PII).&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-rate &amp;lt;N&amp;gt;&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Throttles requests to N requests per second. Default is 30. Use 0 to disable rate limiting.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-b, --brute&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Enables brute-forcing of parameter values (multiple test combos).&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-json&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Outputs results in JSON format instead of a Rich table in default mode.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Help&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;

      /   | __  __/ /_____  ______      ______ _____ _____ ____  _____
     / /| |/ / / / __/ __ \/ ___/ | /| / / __ `/ __ `/ __ `/ _ \/ ___/
    / ___ / /_/ / /_/ /_/ (__  )| |/ |/ / /_/ / /_/ / /_/ /  __/ /
    /_/  |_\__,_/\__/\____/____/ |__/|__/_\__,_/\__, /\__, /\___/_/
                                              /____//____/
                              https://intruder.io
                          Find unauthenticated endpoints

usage: autoswagger.py [-h] [-v] [-risk] [-all] [-product] [-stats] [-rate RATE] [-b] [-json] [urls ...]

Autoswagger: Detect unauthenticated access control issues via Swagger/OpenAPI documentation.

positional arguments:
  urls           Base URL(s) or spec URL(s) of the target API(s)

options:
  -h, --help     show this help message and exit
  -v, --verbose  Enable verbose output
  -risk          Include non-GET requests in testing
  -all           Include all HTTP status codes in the results, excluding 401 and 403
  -product       Output all endpoints in JSON, flagging those that contain PII or have large responses.
  -stats         Display scan statistics. Included in JSON if -product or -json is used.
  -rate RATE     Set the rate limit in requests per second (default: 30). Use 0 to disable rate limiting.
  -b, --brute    Enable exhaustive testing of parameter values.
  -json          Output results in JSON format in default mode.

Example usage:
  python autoswagger.py https://api.example.com -v

&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Discovery Phases&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Direct Spec&lt;/strong&gt;&lt;br /&gt; If a provided URL ends with &lt;code&gt;.json/.yaml/.yml&lt;/code&gt;, Autoswagger &lt;strong&gt;directly&lt;/strong&gt; attempts to parse the OpenAPI schema.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Swagger-UI Detection&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Tries known UI paths (e.g., &lt;code&gt;/swagger-ui.html&lt;/code&gt;).&lt;/li&gt; 
   &lt;li&gt;If found, parses the HTML or local JavaScript files for a &lt;code&gt;swagger.json&lt;/code&gt; or &lt;code&gt;openapi.json&lt;/code&gt;.&lt;/li&gt; 
   &lt;li&gt;Can detect embedded configs like &lt;code&gt;window.swashbuckleConfig&lt;/code&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Direct Spec by Bruteforce&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;If no spec is found so far, Autoswagger attempts a list of default endpoints like &lt;code&gt;/swagger.json&lt;/code&gt;, &lt;code&gt;/openapi.json&lt;/code&gt;, etc.&lt;/li&gt; 
   &lt;li&gt;Stops when a valid spec is discovered or none are found.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Endpoint Testing&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Collect Endpoints&lt;/strong&gt;&lt;br /&gt; After loading a spec, Autoswagger extracts each path and method under the &lt;code&gt;paths&lt;/code&gt; key.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;HTTP Methods&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;By default, tests &lt;code&gt;GET&lt;/code&gt; only.&lt;/li&gt; 
   &lt;li&gt;Use &lt;code&gt;-risk&lt;/code&gt; to include other methods (&lt;code&gt;POST&lt;/code&gt;, &lt;code&gt;PUT&lt;/code&gt;, &lt;code&gt;PATCH&lt;/code&gt;, &lt;code&gt;DELETE&lt;/code&gt;).&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Parameter Values&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fill path/query parameters with defaults or values to enumerate.&lt;/li&gt; 
   &lt;li&gt;Optionally builds request bodies from the specâ€™s &lt;code&gt;requestBody&lt;/code&gt; (OpenAPI 3) or body parameters (Swagger 2).&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Rate Limiting &amp;amp; Concurrency&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Supports threading with a cap on requests per second (&lt;code&gt;-rate&lt;/code&gt;).&lt;/li&gt; 
   &lt;li&gt;Each endpoint is tested in a dedicated job.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Response Analysis&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Decodes responses, checks for PII, secrets, and large content.&lt;/li&gt; 
   &lt;li&gt;Logs relevant findings.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr /&gt; 
&lt;h2&gt;PII Detection&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Presidio-Based Analysis&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Searches for phone numbers, emails, addresses, names.&lt;/li&gt; 
   &lt;li&gt;Context-based scanning (e.g., CSV headers, key-value lines).&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Secrets &amp;amp; Debug Info&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;TruffleHog-like regex checks for API keys, tokens, environment variables.&lt;/li&gt; 
   &lt;li&gt;Merges any matches into the PII data structure for final reporting.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Large Response Check&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Flags responses with 100+ JSON elements or large XML structures as â€œinteresting.â€&lt;/li&gt; 
   &lt;li&gt;Also checks raw size threshold (e.g., &amp;gt;100k bytes).&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Output&lt;/h2&gt; 
&lt;p&gt;By default, output is shown in a table.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;-json&lt;/code&gt; produces JSON objects, grouping results by endpoint.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-product&lt;/code&gt; filters down to only â€œinterestingâ€ endpoints (PII, large responses and responses with secrets).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Interpreting Results&lt;/h2&gt; 
&lt;p&gt;For most use cases, interpreting results involves looking at the output (endpoints resulting in Status Code 200s), and paying particular attention to endpoints which are marked as 'PII or Secret Detected'. These endpoints are the ones that contain impactful exposures, but they should be manually checked to confirm. You may also wish to look at other 200s that do not contain PII, and determine whether it's intended for these endpoints to be public or not.&lt;/p&gt; 
&lt;p&gt;Simple GET endpoints can be triaged using command line tools like curl, but we would recommend using your usual API testing suite (tools such as Postman or Burp Suite) to replay requests and read responses to confirm whether an exposure is present.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Stats &amp;amp; Reporting&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;-stats&lt;/code&gt; appends or prints overall statistics, such as: 
  &lt;ul&gt; 
   &lt;li&gt;Hosts with valid specs&lt;/li&gt; 
   &lt;li&gt;Hosts with PII&lt;/li&gt; 
   &lt;li&gt;Total requests sent, average RPS&lt;/li&gt; 
   &lt;li&gt;Percentage of endpoints responding with 2xx or 4xx&lt;/li&gt; 
   &lt;li&gt;Shown in either a Rich table in default mode or embedded in JSON if &lt;code&gt;-json&lt;/code&gt; or &lt;code&gt;-product&lt;/code&gt; is used.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Acknowledgments&lt;/h2&gt; 
&lt;p&gt;Autoswagger is maintained and owned by &lt;strong&gt;&lt;a href="https://intruder.io/"&gt;Intruder&lt;/a&gt;&lt;/strong&gt;. It was primarily developed by Cale Anderson&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>dataease/SQLBot</title>
      <link>https://github.com/dataease/SQLBot</link>
      <description>&lt;p&gt;ğŸ”¥ åŸºäºå¤§æ¨¡å‹å’Œ RAG çš„æ™ºèƒ½é—®æ•°ç³»ç»Ÿã€‚Text-to-SQL Generation via LLMs using RAG.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt;&lt;img src="https://resource-fit2cloud-com.oss-cn-hangzhou.aliyuncs.com/sqlbot/sqlbot.png" alt="SQLBot" width="300" /&gt;&lt;/p&gt; 
&lt;h3 align="center"&gt;åŸºäºå¤§æ¨¡å‹å’Œ RAG çš„æ™ºèƒ½é—®æ•°ç³»ç»Ÿ&lt;/h3&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/dataease/SQLBot/releases/latest"&gt;&lt;img src="https://img.shields.io/github/v/release/dataease/SQLBot" alt="Latest release" /&gt;&lt;/a&gt; &lt;a href="https://github.com/dataease/SQLBot"&gt;&lt;img src="https://img.shields.io/github/stars/dataease/SQLBot?color=%231890FF&amp;amp;style=flat-square" alt="Stars" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/dataease/SQLbot"&gt;&lt;img src="https://img.shields.io/docker/pulls/dataease/sqlbot?label=downloads" alt="Download" /&gt;&lt;/a&gt;&lt;br /&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;SQLBot æ˜¯ä¸€æ¬¾åŸºäºå¤§æ¨¡å‹å’Œ RAG çš„æ™ºèƒ½é—®æ•°ç³»ç»Ÿã€‚SQLBot çš„ä¼˜åŠ¿åŒ…æ‹¬ï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;å¼€ç®±å³ç”¨&lt;/strong&gt;: åªéœ€é…ç½®å¤§æ¨¡å‹å’Œæ•°æ®æºå³å¯å¼€å¯é—®æ•°ä¹‹æ—…ï¼Œé€šè¿‡å¤§æ¨¡å‹å’Œ RAG çš„ç»“åˆæ¥å®ç°é«˜è´¨é‡çš„ text2sqlï¼›&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;æ˜“äºé›†æˆ&lt;/strong&gt;: æ”¯æŒå¿«é€ŸåµŒå…¥åˆ°ç¬¬ä¸‰æ–¹ä¸šåŠ¡ç³»ç»Ÿï¼Œä¹Ÿæ”¯æŒè¢« n8nã€MaxKBã€Difyã€Coze ç­‰ AI åº”ç”¨å¼€å‘å¹³å°é›†æˆè°ƒç”¨ï¼Œè®©å„ç±»åº”ç”¨å¿«é€Ÿæ‹¥æœ‰æ™ºèƒ½é—®æ•°èƒ½åŠ›ï¼›&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;å®‰å…¨å¯æ§&lt;/strong&gt;: æä¾›åŸºäºå·¥ä½œç©ºé—´çš„èµ„æºéš”ç¦»æœºåˆ¶ï¼Œèƒ½å¤Ÿå®ç°ç»†ç²’åº¦çš„æ•°æ®æƒé™æ§åˆ¶ã€‚&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;å·¥ä½œåŸç†&lt;/h2&gt; 
&lt;img width="1105" height="577" alt="system-arch" src="https://github.com/user-attachments/assets/462603fc-980b-4b8b-a6d4-a821c070a048" /&gt; 
&lt;h2&gt;å¿«é€Ÿå¼€å§‹&lt;/h2&gt; 
&lt;h3&gt;å®‰è£…éƒ¨ç½²&lt;/h3&gt; 
&lt;p&gt;å‡†å¤‡ä¸€å° Linux æœåŠ¡å™¨ï¼Œå®‰è£…å¥½ &lt;a href="https://docs.docker.com/get-docker/"&gt;Docker&lt;/a&gt;ï¼Œæ‰§è¡Œä»¥ä¸‹ä¸€é”®å®‰è£…è„šæœ¬ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -d \
  --name sqlbot \
  --restart unless-stopped \
  -p 8000:8000 \
  -p 8001:8001 \
  -v ./data/sqlbot/excel:/opt/sqlbot/data/excel \
  -v ./data/sqlbot/file:/opt/sqlbot/data/file \
  -v ./data/sqlbot/images:/opt/sqlbot/images \
  -v ./data/sqlbot/logs:/opt/sqlbot/app/logs \
  -v ./data/postgresql:/var/lib/postgresql/data \
  --privileged=true \
  dataease/sqlbot
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ä½ ä¹Ÿå¯ä»¥é€šè¿‡ &lt;a href="https://apps.fit2cloud.com/1panel"&gt;1Panel åº”ç”¨å•†åº—&lt;/a&gt; å¿«é€Ÿéƒ¨ç½² SQLBotã€‚&lt;/p&gt; 
&lt;p&gt;å¦‚æœæ˜¯å†…ç½‘ç¯å¢ƒï¼Œä½ å¯ä»¥é€šè¿‡ &lt;a href="https://community.fit2cloud.com/#/products/sqlbot/downloads"&gt;ç¦»çº¿å®‰è£…åŒ…æ–¹å¼&lt;/a&gt; éƒ¨ç½² SQLBotã€‚&lt;/p&gt; 
&lt;h3&gt;è®¿é—®æ–¹å¼&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€: http://&amp;lt;ä½ çš„æœåŠ¡å™¨IP&amp;gt;:8000/&lt;/li&gt; 
 &lt;li&gt;ç”¨æˆ·å: admin&lt;/li&gt; 
 &lt;li&gt;å¯†ç : SQLBot@123456&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;è”ç³»æˆ‘ä»¬&lt;/h3&gt; 
&lt;p&gt;å¦‚ä½ æœ‰æ›´å¤šé—®é¢˜ï¼Œå¯ä»¥åŠ å…¥æˆ‘ä»¬çš„æŠ€æœ¯äº¤æµç¾¤ä¸æˆ‘ä»¬äº¤æµã€‚&lt;/p&gt; 
&lt;img width="180" height="180" alt="contact_me_qr" src="https://github.com/user-attachments/assets/2594ff29-5426-4457-b051-279855610030" /&gt; 
&lt;h2&gt;UI å±•ç¤º&lt;/h2&gt;  
&lt;img alt="q&amp;amp;a" src="https://github.com/user-attachments/assets/55526514-52f3-4cfe-98ec-08a986259280" /&gt;  
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#dataease/sqlbot&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=dataease/sqlbot&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;é£è‡´äº‘æ——ä¸‹çš„å…¶ä»–æ˜æ˜Ÿé¡¹ç›®&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/dataease/dataease/"&gt;DataEase&lt;/a&gt; - äººäººå¯ç”¨çš„å¼€æº BI å·¥å…·&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/1panel-dev/1panel/"&gt;1Panel&lt;/a&gt; - ç°ä»£åŒ–ã€å¼€æºçš„ Linux æœåŠ¡å™¨è¿ç»´ç®¡ç†é¢æ¿&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/1panel-dev/MaxKB/"&gt;MaxKB&lt;/a&gt; - å¼ºå¤§æ˜“ç”¨çš„ä¼ä¸šçº§æ™ºèƒ½ä½“å¹³å°&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/jumpserver/jumpserver/"&gt;JumpServer&lt;/a&gt; - å¹¿å—æ¬¢è¿çš„å¼€æºå ¡å’æœº&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/1Panel-dev/CordysCRM"&gt;Cordys CRM&lt;/a&gt; - æ–°ä¸€ä»£çš„å¼€æº AI CRM ç³»ç»Ÿ&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/halo-dev/halo/"&gt;Halo&lt;/a&gt; - å¼ºå¤§æ˜“ç”¨çš„å¼€æºå»ºç«™å·¥å…·&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/metersphere/metersphere/"&gt;MeterSphere&lt;/a&gt; - æ–°ä¸€ä»£çš„å¼€æºæŒç»­æµ‹è¯•å·¥å…·&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;æœ¬ä»“åº“éµå¾ª &lt;a href="https://raw.githubusercontent.com/dataease/SQLBot/main/LICENSE"&gt;FIT2CLOUD Open Source License&lt;/a&gt; å¼€æºåè®®ï¼Œè¯¥è®¸å¯è¯æœ¬è´¨ä¸Šæ˜¯ GPLv3ï¼Œä½†æœ‰ä¸€äº›é¢å¤–çš„é™åˆ¶ã€‚&lt;/p&gt; 
&lt;p&gt;ä½ å¯ä»¥åŸºäº SQLBot çš„æºä»£ç è¿›è¡ŒäºŒæ¬¡å¼€å‘ï¼Œä½†æ˜¯éœ€è¦éµå®ˆä»¥ä¸‹è§„å®šï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ä¸èƒ½æ›¿æ¢å’Œä¿®æ”¹ SQLBot çš„ Logo å’Œç‰ˆæƒä¿¡æ¯ï¼›&lt;/li&gt; 
 &lt;li&gt;äºŒæ¬¡å¼€å‘åçš„è¡ç”Ÿä½œå“å¿…é¡»éµå®ˆ GPL V3 çš„å¼€æºä¹‰åŠ¡ã€‚&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;å¦‚éœ€å•†ä¸šæˆæƒï¼Œè¯·è”ç³» &lt;a href="mailto:support@fit2cloud.com"&gt;support@fit2cloud.com&lt;/a&gt; ã€‚&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>vllm-project/vllm</title>
      <link>https://github.com/vllm-project/vllm</link>
      <description>&lt;p&gt;A high-throughput and memory-efficient inference and serving engine for LLMs&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/vllm-project/vllm/main/docs/assets/logos/vllm-logo-text-dark.png" /&gt; 
  &lt;img alt="vLLM" src="https://raw.githubusercontent.com/vllm-project/vllm/main/docs/assets/logos/vllm-logo-text-light.png" width="55%" /&gt; 
 &lt;/picture&gt; &lt;/p&gt; 
&lt;h3 align="center"&gt; Easy, fast, and cheap LLM serving for everyone &lt;/h3&gt; 
&lt;p align="center"&gt; | &lt;a href="https://docs.vllm.ai"&gt;&lt;b&gt;Documentation&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://blog.vllm.ai/"&gt;&lt;b&gt;Blog&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://arxiv.org/abs/2309.06180"&gt;&lt;b&gt;Paper&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://x.com/vllm_project"&gt;&lt;b&gt;Twitter/X&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://discuss.vllm.ai"&gt;&lt;b&gt;User Forum&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://slack.vllm.ai"&gt;&lt;b&gt;Developer Slack&lt;/b&gt;&lt;/a&gt; | &lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;Join us at the &lt;a href="https://events.linuxfoundation.org/pytorch-conference/"&gt;PyTorch Conference, October 22-23&lt;/a&gt; and &lt;a href="https://www.anyscale.com/ray-summit/2025"&gt;Ray Summit, November 3-5&lt;/a&gt; in San Francisco for our latest updates on vLLM and to meet the vLLM team! Register now for the largest vLLM community events of the year!&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;em&gt;Latest News&lt;/em&gt; ğŸ”¥&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;[2025/09] We hosted &lt;a href="https://luma.com/e80e0ymm"&gt;vLLM Toronto Meetup&lt;/a&gt; focused on tackling inference at scale and speculative decoding with speakers from NVIDIA and Red Hat! Please find the meetup slides &lt;a href="https://docs.google.com/presentation/d/1IYJYmJcu9fLpID5N5RbW_vO0XLo0CGOR14IXOjB61V8/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[2025/08] We hosted &lt;a href="https://mp.weixin.qq.com/s/k8ZBO1u2_2odgiKWH_GVTQ"&gt;vLLM Shenzhen Meetup&lt;/a&gt; focusing on the ecosystem around vLLM! Please find the meetup slides &lt;a href="https://drive.google.com/drive/folders/1Ua2SVKVSu-wp5vou_6ElraDt2bnKhiEA"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[2025/08] We hosted &lt;a href="https://www.sginnovate.com/event/vllm-sg-meet"&gt;vLLM Singapore Meetup&lt;/a&gt;. We shared V1 updates, disaggregated serving and MLLM speedups with speakers from Embedded LLM, AMD, WekaIO, and A*STAR. Please find the meetup slides &lt;a href="https://drive.google.com/drive/folders/1ncf3GyqLdqFaB6IeB834E5TZJPLAOiXZ?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[2025/08] We hosted &lt;a href="https://mp.weixin.qq.com/s/pDmAXHcN7Iqc8sUKgJgGtg"&gt;vLLM Shanghai Meetup&lt;/a&gt; focusing on building, developing, and integrating with vLLM! Please find the meetup slides &lt;a href="https://drive.google.com/drive/folders/1OvLx39wnCGy_WKq8SiVKf7YcxxYI3WCH"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[2025/05] vLLM is now a hosted project under PyTorch Foundation! Please find the announcement &lt;a href="https://pytorch.org/blog/pytorch-foundation-welcomes-vllm/"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[2025/01] We are excited to announce the alpha release of vLLM V1: A major architectural upgrade with 1.7x speedup! Clean code, optimized execution loop, zero-overhead prefix caching, enhanced multimodal support, and more. Please check out our blog post &lt;a href="https://blog.vllm.ai/2025/01/27/v1-alpha-release.html"&gt;here&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;Previous News&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;[2025/08] We hosted &lt;a href="https://luma.com/cgcgprmh"&gt;vLLM Korea Meetup&lt;/a&gt; with Red Hat and Rebellions! We shared the latest advancements in vLLM along with project spotlights from the vLLM Korea community. Please find the meetup slides &lt;a href="https://drive.google.com/file/d/1bcrrAE1rxUgx0mjIeOWT6hNe2RefC5Hm/view"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2025/08] We hosted &lt;a href="https://mp.weixin.qq.com/s/dgkWg1WFpWGO2jCdTqQHxA"&gt;vLLM Beijing Meetup&lt;/a&gt; focusing on large-scale LLM deployment! Please find the meetup slides &lt;a href="https://drive.google.com/drive/folders/1Pid6NSFLU43DZRi0EaTcPgXsAzDvbBqF"&gt;here&lt;/a&gt; and the recording &lt;a href="https://www.chaspark.com/#/live/1166916873711665152"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2025/05] We hosted &lt;a href="https://lu.ma/c1rqyf1f"&gt;NYC vLLM Meetup&lt;/a&gt;! Please find the meetup slides &lt;a href="https://docs.google.com/presentation/d/1_q_aW_ioMJWUImf1s1YM-ZhjXz8cUeL0IJvaquOYBeA/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2025/04] We hosted &lt;a href="https://www.sginnovate.com/event/limited-availability-morning-evening-slots-remaining-inaugural-vllm-asia-developer-day"&gt;Asia Developer Day&lt;/a&gt;! Please find the meetup slides from the vLLM team &lt;a href="https://docs.google.com/presentation/d/19cp6Qu8u48ihB91A064XfaXruNYiBOUKrBxAmDOllOo/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2025/03] We hosted &lt;a href="https://lu.ma/vllm-ollama"&gt;vLLM x Ollama Inference Night&lt;/a&gt;! Please find the meetup slides from the vLLM team &lt;a href="https://docs.google.com/presentation/d/16T2PDD1YwRnZ4Tu8Q5r6n53c5Lr5c73UV9Vd2_eBo4U/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2025/03] We hosted &lt;a href="https://mp.weixin.qq.com/s/n77GibL2corAtQHtVEAzfg"&gt;the first vLLM China Meetup&lt;/a&gt;! Please find the meetup slides from vLLM team &lt;a href="https://docs.google.com/presentation/d/1REHvfQMKGnvz6p3Fd23HhSO4c8j5WPGZV0bKYLwnHyQ/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2025/03] We hosted &lt;a href="https://lu.ma/7mu4k4xx"&gt;the East Coast vLLM Meetup&lt;/a&gt;! Please find the meetup slides &lt;a href="https://docs.google.com/presentation/d/1NHiv8EUFF1NLd3fEYODm56nDmL26lEeXCaDgyDlTsRs/edit#slide=id.g31441846c39_0_0"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2025/02] We hosted &lt;a href="https://lu.ma/h7g3kuj9"&gt;the ninth vLLM meetup&lt;/a&gt; with Meta! Please find the meetup slides from vLLM team &lt;a href="https://docs.google.com/presentation/d/1jzC_PZVXrVNSFVCW-V4cFXb6pn7zZ2CyP_Flwo05aqg/edit?usp=sharing"&gt;here&lt;/a&gt; and AMD &lt;a href="https://drive.google.com/file/d/1Zk5qEJIkTmlQ2eQcXQZlljAx3m9s7nwn/view?usp=sharing"&gt;here&lt;/a&gt;. The slides from Meta will not be posted.&lt;/li&gt; 
  &lt;li&gt;[2025/01] We hosted &lt;a href="https://lu.ma/zep56hui"&gt;the eighth vLLM meetup&lt;/a&gt; with Google Cloud! Please find the meetup slides from vLLM team &lt;a href="https://docs.google.com/presentation/d/1epVkt4Zu8Jz_S5OhEHPc798emsYh2BwYfRuDDVEF7u4/edit?usp=sharing"&gt;here&lt;/a&gt;, and Google Cloud team &lt;a href="https://drive.google.com/file/d/1h24pHewANyRL11xy5dXUbvRC9F9Kkjix/view?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2024/12] vLLM joins &lt;a href="https://pytorch.org/blog/vllm-joins-pytorch"&gt;pytorch ecosystem&lt;/a&gt;! Easy, Fast, and Cheap LLM Serving for Everyone!&lt;/li&gt; 
  &lt;li&gt;[2024/11] We hosted &lt;a href="https://lu.ma/h0qvrajz"&gt;the seventh vLLM meetup&lt;/a&gt; with Snowflake! Please find the meetup slides from vLLM team &lt;a href="https://docs.google.com/presentation/d/1e3CxQBV3JsfGp30SwyvS3eM_tW-ghOhJ9PAJGK6KR54/edit?usp=sharing"&gt;here&lt;/a&gt;, and Snowflake team &lt;a href="https://docs.google.com/presentation/d/1qF3RkDAbOULwz9WK5TOltt2fE9t6uIc_hVNLFAaQX6A/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2024/10] We have just created a developer slack (&lt;a href="https://slack.vllm.ai"&gt;slack.vllm.ai&lt;/a&gt;) focusing on coordinating contributions and discussing features. Please feel free to join us there!&lt;/li&gt; 
  &lt;li&gt;[2024/10] Ray Summit 2024 held a special track for vLLM! Please find the opening talk slides from the vLLM team &lt;a href="https://docs.google.com/presentation/d/1B_KQxpHBTRa_mDF-tR6i8rWdOU5QoTZNcEg2MKZxEHM/edit?usp=sharing"&gt;here&lt;/a&gt;. Learn more from the &lt;a href="https://www.youtube.com/playlist?list=PLzTswPQNepXl6AQwifuwUImLPFRVpksjR"&gt;talks&lt;/a&gt; from other vLLM contributors and users!&lt;/li&gt; 
  &lt;li&gt;[2024/09] We hosted &lt;a href="https://lu.ma/87q3nvnh"&gt;the sixth vLLM meetup&lt;/a&gt; with NVIDIA! Please find the meetup slides &lt;a href="https://docs.google.com/presentation/d/1wrLGwytQfaOTd5wCGSPNhoaW3nq0E-9wqyP7ny93xRs/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2024/07] We hosted &lt;a href="https://lu.ma/lp0gyjqr"&gt;the fifth vLLM meetup&lt;/a&gt; with AWS! Please find the meetup slides &lt;a href="https://docs.google.com/presentation/d/1RgUD8aCfcHocghoP3zmXzck9vX3RCI9yfUAB2Bbcl4Y/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2024/07] In partnership with Meta, vLLM officially supports Llama 3.1 with FP8 quantization and pipeline parallelism! Please check out our blog post &lt;a href="https://blog.vllm.ai/2024/07/23/llama31.html"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2024/06] We hosted &lt;a href="https://lu.ma/agivllm"&gt;the fourth vLLM meetup&lt;/a&gt; with Cloudflare and BentoML! Please find the meetup slides &lt;a href="https://docs.google.com/presentation/d/1iJ8o7V2bQEi0BFEljLTwc5G1S10_Rhv3beed5oB0NJ4/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2024/04] We hosted &lt;a href="https://robloxandvllmmeetup2024.splashthat.com/"&gt;the third vLLM meetup&lt;/a&gt; with Roblox! Please find the meetup slides &lt;a href="https://docs.google.com/presentation/d/1A--47JAK4BJ39t954HyTkvtfwn0fkqtsL8NGFuslReM/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2024/01] We hosted &lt;a href="https://lu.ma/ygxbpzhl"&gt;the second vLLM meetup&lt;/a&gt; with IBM! Please find the meetup slides &lt;a href="https://docs.google.com/presentation/d/12mI2sKABnUw5RBWXDYY-HtHth4iMSNcEoQ10jDQbxgA/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2023/10] We hosted &lt;a href="https://lu.ma/first-vllm-meetup"&gt;the first vLLM meetup&lt;/a&gt; with a16z! Please find the meetup slides &lt;a href="https://docs.google.com/presentation/d/1QL-XPFXiFpDBh86DbEegFXBXFXjix4v032GhShbKf3s/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2023/08] We would like to express our sincere gratitude to &lt;a href="https://a16z.com/2023/08/30/supporting-the-open-source-ai-community/"&gt;Andreessen Horowitz&lt;/a&gt; (a16z) for providing a generous grant to support the open-source development and research of vLLM.&lt;/li&gt; 
  &lt;li&gt;[2023/06] We officially released vLLM! FastChat-vLLM integration has powered &lt;a href="https://chat.lmsys.org"&gt;LMSYS Vicuna and Chatbot Arena&lt;/a&gt; since mid-April. Check out our &lt;a href="https://vllm.ai"&gt;blog post&lt;/a&gt;.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h2&gt;About&lt;/h2&gt; 
&lt;p&gt;vLLM is a fast and easy-to-use library for LLM inference and serving.&lt;/p&gt; 
&lt;p&gt;Originally developed in the &lt;a href="https://sky.cs.berkeley.edu"&gt;Sky Computing Lab&lt;/a&gt; at UC Berkeley, vLLM has evolved into a community-driven project with contributions from both academia and industry.&lt;/p&gt; 
&lt;p&gt;vLLM is fast with:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;State-of-the-art serving throughput&lt;/li&gt; 
 &lt;li&gt;Efficient management of attention key and value memory with &lt;a href="https://blog.vllm.ai/2023/06/20/vllm.html"&gt;&lt;strong&gt;PagedAttention&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Continuous batching of incoming requests&lt;/li&gt; 
 &lt;li&gt;Fast model execution with CUDA/HIP graph&lt;/li&gt; 
 &lt;li&gt;Quantizations: &lt;a href="https://arxiv.org/abs/2210.17323"&gt;GPTQ&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/2306.00978"&gt;AWQ&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/2309.05516"&gt;AutoRound&lt;/a&gt;, INT4, INT8, and FP8&lt;/li&gt; 
 &lt;li&gt;Optimized CUDA kernels, including integration with FlashAttention and FlashInfer&lt;/li&gt; 
 &lt;li&gt;Speculative decoding&lt;/li&gt; 
 &lt;li&gt;Chunked prefill&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;vLLM is flexible and easy to use with:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Seamless integration with popular Hugging Face models&lt;/li&gt; 
 &lt;li&gt;High-throughput serving with various decoding algorithms, including &lt;em&gt;parallel sampling&lt;/em&gt;, &lt;em&gt;beam search&lt;/em&gt;, and more&lt;/li&gt; 
 &lt;li&gt;Tensor, pipeline, data and expert parallelism support for distributed inference&lt;/li&gt; 
 &lt;li&gt;Streaming outputs&lt;/li&gt; 
 &lt;li&gt;OpenAI-compatible API server&lt;/li&gt; 
 &lt;li&gt;Support for NVIDIA GPUs, AMD CPUs and GPUs, Intel CPUs and GPUs, PowerPC CPUs, and TPU. Additionally, support for diverse hardware plugins such as Intel Gaudi, IBM Spyre and Huawei Ascend.&lt;/li&gt; 
 &lt;li&gt;Prefix caching support&lt;/li&gt; 
 &lt;li&gt;Multi-LoRA support&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;vLLM seamlessly supports most popular open-source models on HuggingFace, including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Transformer-like LLMs (e.g., Llama)&lt;/li&gt; 
 &lt;li&gt;Mixture-of-Expert LLMs (e.g., Mixtral, Deepseek-V2 and V3)&lt;/li&gt; 
 &lt;li&gt;Embedding Models (e.g., E5-Mistral)&lt;/li&gt; 
 &lt;li&gt;Multi-modal LLMs (e.g., LLaVA)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Find the full list of supported models &lt;a href="https://docs.vllm.ai/en/latest/models/supported_models.html"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;Install vLLM with &lt;code&gt;pip&lt;/code&gt; or &lt;a href="https://docs.vllm.ai/en/latest/getting_started/installation/gpu/index.html#build-wheel-from-source"&gt;from source&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install vllm
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Visit our &lt;a href="https://docs.vllm.ai/en/latest/"&gt;documentation&lt;/a&gt; to learn more.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.vllm.ai/en/latest/getting_started/installation.html"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.vllm.ai/en/latest/getting_started/quickstart.html"&gt;Quickstart&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.vllm.ai/en/latest/models/supported_models.html"&gt;List of Supported Models&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome and value any contributions and collaborations. Please check out &lt;a href="https://docs.vllm.ai/en/latest/contributing/index.html"&gt;Contributing to vLLM&lt;/a&gt; for how to get involved.&lt;/p&gt; 
&lt;h2&gt;Sponsors&lt;/h2&gt; 
&lt;p&gt;vLLM is a community project. Our compute resources for development and testing are supported by the following organizations. Thank you for your support!&lt;/p&gt; 
&lt;!-- Note: Please sort them in alphabetical order. --&gt; 
&lt;!-- Note: Please keep these consistent with docs/community/sponsors.md --&gt; 
&lt;p&gt;Cash Donations:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;a16z&lt;/li&gt; 
 &lt;li&gt;Dropbox&lt;/li&gt; 
 &lt;li&gt;Sequoia Capital&lt;/li&gt; 
 &lt;li&gt;Skywork AI&lt;/li&gt; 
 &lt;li&gt;ZhenFund&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Compute Resources:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Alibaba Cloud&lt;/li&gt; 
 &lt;li&gt;AMD&lt;/li&gt; 
 &lt;li&gt;Anyscale&lt;/li&gt; 
 &lt;li&gt;AWS&lt;/li&gt; 
 &lt;li&gt;Crusoe Cloud&lt;/li&gt; 
 &lt;li&gt;Databricks&lt;/li&gt; 
 &lt;li&gt;DeepInfra&lt;/li&gt; 
 &lt;li&gt;Google Cloud&lt;/li&gt; 
 &lt;li&gt;Intel&lt;/li&gt; 
 &lt;li&gt;Lambda Lab&lt;/li&gt; 
 &lt;li&gt;Nebius&lt;/li&gt; 
 &lt;li&gt;Novita AI&lt;/li&gt; 
 &lt;li&gt;NVIDIA&lt;/li&gt; 
 &lt;li&gt;Replicate&lt;/li&gt; 
 &lt;li&gt;Roblox&lt;/li&gt; 
 &lt;li&gt;RunPod&lt;/li&gt; 
 &lt;li&gt;Trainy&lt;/li&gt; 
 &lt;li&gt;UC Berkeley&lt;/li&gt; 
 &lt;li&gt;UC San Diego&lt;/li&gt; 
 &lt;li&gt;Volcengine&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Slack Sponsor: Anyscale&lt;/p&gt; 
&lt;p&gt;We also have an official fundraising venue through &lt;a href="https://opencollective.com/vllm"&gt;OpenCollective&lt;/a&gt;. We plan to use the fund to support the development, maintenance, and adoption of vLLM.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you use vLLM for your research, please cite our &lt;a href="https://arxiv.org/abs/2309.06180"&gt;paper&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@inproceedings{kwon2023efficient,
  title={Efficient Memory Management for Large Language Model Serving with PagedAttention},
  author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},
  booktitle={Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles},
  year={2023}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contact Us&lt;/h2&gt; 
&lt;!-- --8&lt;-- [start:contact-us] --&gt; 
&lt;ul&gt; 
 &lt;li&gt;For technical questions and feature requests, please use GitHub &lt;a href="https://github.com/vllm-project/vllm/issues"&gt;Issues&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;For discussing with fellow users, please use the &lt;a href="https://discuss.vllm.ai"&gt;vLLM Forum&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;For coordinating contributions and development, please use &lt;a href="https://slack.vllm.ai"&gt;Slack&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;For security disclosures, please use GitHub's &lt;a href="https://github.com/vllm-project/vllm/security/advisories"&gt;Security Advisories&lt;/a&gt; feature&lt;/li&gt; 
 &lt;li&gt;For collaborations and partnerships, please contact us at &lt;a href="mailto:vllm-questions@lists.berkeley.edu"&gt;vllm-questions@lists.berkeley.edu&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- --8&lt;-- [end:contact-us] --&gt; 
&lt;h2&gt;Media Kit&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;If you wish to use vLLM's logo, please refer to &lt;a href="https://github.com/vllm-project/media-kit"&gt;our media kit repo&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>bytedance/deer-flow</title>
      <link>https://github.com/bytedance/deer-flow</link>
      <description>&lt;p&gt;DeerFlow is a community-driven Deep Research framework, combining language models with tools like web search, crawling, and Python execution, while contributing back to the open-source community.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ğŸ¦Œ DeerFlow&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://www.python.org/downloads/"&gt;&lt;img src="https://img.shields.io/badge/python-3.12+-blue.svg?sanitize=true" alt="Python 3.12+" /&gt;&lt;/a&gt; &lt;a href="https://opensource.org/licenses/MIT"&gt;&lt;img src="https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true" alt="License: MIT" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/bytedance/deer-flow"&gt;&lt;img src="https://img.shields.io/badge/DeepWiki-bytedance%2Fdeer--flow-blue.svg?logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACwAAAAyCAYAAAAnWDnqAAAAAXNSR0IArs4c6QAAA05JREFUaEPtmUtyEzEQhtWTQyQLHNak2AB7ZnyXZMEjXMGeK/AIi+QuHrMnbChYY7MIh8g01fJoopFb0uhhEqqcbWTp06/uv1saEDv4O3n3dV60RfP947Mm9/SQc0ICFQgzfc4CYZoTPAswgSJCCUJUnAAoRHOAUOcATwbmVLWdGoH//PB8mnKqScAhsD0kYP3j/Yt5LPQe2KvcXmGvRHcDnpxfL2zOYJ1mFwrryWTz0advv1Ut4CJgf5uhDuDj5eUcAUoahrdY/56ebRWeraTjMt/00Sh3UDtjgHtQNHwcRGOC98BJEAEymycmYcWwOprTgcB6VZ5JK5TAJ+fXGLBm3FDAmn6oPPjR4rKCAoJCal2eAiQp2x0vxTPB3ALO2CRkwmDy5WohzBDwSEFKRwPbknEggCPB/imwrycgxX2NzoMCHhPkDwqYMr9tRcP5qNrMZHkVnOjRMWwLCcr8ohBVb1OMjxLwGCvjTikrsBOiA6fNyCrm8V1rP93iVPpwaE+gO0SsWmPiXB+jikdf6SizrT5qKasx5j8ABbHpFTx+vFXp9EnYQmLx02h1QTTrl6eDqxLnGjporxl3NL3agEvXdT0WmEost648sQOYAeJS9Q7bfUVoMGnjo4AZdUMQku50McCcMWcBPvr0SzbTAFDfvJqwLzgxwATnCgnp4wDl6Aa+Ax283gghmj+vj7feE2KBBRMW3FzOpLOADl0Isb5587h/U4gGvkt5v60Z1VLG8BhYjbzRwyQZemwAd6cCR5/XFWLYZRIMpX39AR0tjaGGiGzLVyhse5C9RKC6ai42ppWPKiBagOvaYk8lO7DajerabOZP46Lby5wKjw1HCRx7p9sVMOWGzb/vA1hwiWc6jm3MvQDTogQkiqIhJV0nBQBTU+3okKCFDy9WwferkHjtxib7t3xIUQtHxnIwtx4mpg26/HfwVNVDb4oI9RHmx5WGelRVlrtiw43zboCLaxv46AZeB3IlTkwouebTr1y2NjSpHz68WNFjHvupy3q8TFn3Hos2IAk4Ju5dCo8B3wP7VPr/FGaKiG+T+v+TQqIrOqMTL1VdWV1DdmcbO8KXBz6esmYWYKPwDL5b5FA1a0hwapHiom0r/cKaoqr+27/XcrS5UwSMbQAAAABJRU5ErkJggg==" alt="DeepWiki" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;!-- DeepWiki badge generated by https://deepwiki.ryoppippi.com/ --&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/bytedance/deer-flow/main/README.md"&gt;English&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/bytedance/deer-flow/main/README_zh.md"&gt;ç®€ä½“ä¸­æ–‡&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/bytedance/deer-flow/main/README_ja.md"&gt;æ—¥æœ¬èª&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/bytedance/deer-flow/main/README_de.md"&gt;Deutsch&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/bytedance/deer-flow/main/README_es.md"&gt;EspaÃ±ol&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/bytedance/deer-flow/main/README_ru.md"&gt;Ğ ÑƒÑÑĞºĞ¸Ğ¹&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/bytedance/deer-flow/main/README_pt.md"&gt;Portuguese&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Originated from Open Source, give back to Open Source.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;DeerFlow&lt;/strong&gt; (&lt;strong&gt;D&lt;/strong&gt;eep &lt;strong&gt;E&lt;/strong&gt;xploration and &lt;strong&gt;E&lt;/strong&gt;fficient &lt;strong&gt;R&lt;/strong&gt;esearch &lt;strong&gt;Flow&lt;/strong&gt;) is a community-driven Deep Research framework that builds upon the incredible work of the open source community. Our goal is to combine language models with specialized tools for tasks like web search, crawling, and Python code execution, while giving back to the community that made this possible.&lt;/p&gt; 
&lt;p&gt;Currently, DeerFlow has officially entered the &lt;a href="https://console.volcengine.com/vefaas/region:vefaas+cn-beijing/market"&gt;FaaS Application Center of Volcengine&lt;/a&gt;. Users can experience it online through the &lt;a href="https://console.volcengine.com/vefaas/region:vefaas+cn-beijing/market/deerflow/?channel=github&amp;amp;source=deerflow"&gt;experience link&lt;/a&gt; to intuitively feel its powerful functions and convenient operations. At the same time, to meet the deployment needs of different users, DeerFlow supports one-click deployment based on Volcengine. Click the &lt;a href="https://console.volcengine.com/vefaas/region:vefaas+cn-beijing/application/create?templateId=683adf9e372daa0008aaed5c&amp;amp;channel=github&amp;amp;source=deerflow"&gt;deployment link&lt;/a&gt; to quickly complete the deployment process and start an efficient research journey.&lt;/p&gt; 
&lt;p&gt;Please visit &lt;a href="https://deerflow.tech/"&gt;our official website&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Demo&lt;/h2&gt; 
&lt;h3&gt;Video&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/f3786598-1f2a-4d07-919e-8b99dfa1de3e"&gt;https://github.com/user-attachments/assets/f3786598-1f2a-4d07-919e-8b99dfa1de3e&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;In this demo, we showcase how to use DeerFlow to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Seamlessly integrate with MCP services&lt;/li&gt; 
 &lt;li&gt;Conduct the Deep Research process and produce a comprehensive report with images&lt;/li&gt; 
 &lt;li&gt;Create podcast audio based on the generated report&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Replays&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://deerflow.tech/chat?replay=eiffel-tower-vs-tallest-building"&gt;How tall is Eiffel Tower compared to tallest building?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://deerflow.tech/chat?replay=github-top-trending-repo"&gt;What are the top trending repositories on GitHub?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://deerflow.tech/chat?replay=nanjing-traditional-dishes"&gt;Write an article about Nanjing's traditional dishes&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://deerflow.tech/chat?replay=rental-apartment-decoration"&gt;How to decorate a rental apartment?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://deerflow.tech/#case-studies"&gt;Visit our official website to explore more replays.&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ“‘ Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/bytedance/deer-flow/main/#quick-start"&gt;ğŸš€ Quick Start&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/bytedance/deer-flow/main/#features"&gt;ğŸŒŸ Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/bytedance/deer-flow/main/#architecture"&gt;ğŸ—ï¸ Architecture&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/bytedance/deer-flow/main/#development"&gt;ğŸ› ï¸ Development&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/bytedance/deer-flow/main/#docker"&gt;ğŸ³ Docker&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/bytedance/deer-flow/main/#text-to-speech-integration"&gt;ğŸ—£ï¸ Text-to-Speech Integration&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/bytedance/deer-flow/main/#examples"&gt;ğŸ“š Examples&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/bytedance/deer-flow/main/#faq"&gt;â“ FAQ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/bytedance/deer-flow/main/#license"&gt;ğŸ“œ License&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/bytedance/deer-flow/main/#acknowledgments"&gt;ğŸ’– Acknowledgments&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/bytedance/deer-flow/main/#star-history"&gt;â­ Star History&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;DeerFlow is developed in Python, and comes with a web UI written in Node.js. To ensure a smooth setup process, we recommend using the following tools:&lt;/p&gt; 
&lt;h3&gt;Recommended Tools&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;&lt;code&gt;uv&lt;/code&gt;&lt;/a&gt;:&lt;/strong&gt; Simplify Python environment and dependency management. &lt;code&gt;uv&lt;/code&gt; automatically creates a virtual environment in the root directory and installs all required packages for youâ€”no need to manually install Python environments.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/nvm-sh/nvm"&gt;&lt;code&gt;nvm&lt;/code&gt;&lt;/a&gt;:&lt;/strong&gt; Manage multiple versions of the Node.js runtime effortlessly.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://pnpm.io/installation"&gt;&lt;code&gt;pnpm&lt;/code&gt;&lt;/a&gt;:&lt;/strong&gt; Install and manage dependencies of Node.js project.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Environment Requirements&lt;/h3&gt; 
&lt;p&gt;Make sure your system meets the following minimum requirements:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://www.python.org/downloads/"&gt;Python&lt;/a&gt;:&lt;/strong&gt; Version &lt;code&gt;3.12+&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://nodejs.org/en/download/"&gt;Node.js&lt;/a&gt;:&lt;/strong&gt; Version &lt;code&gt;22+&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone the repository
git clone https://github.com/bytedance/deer-flow.git
cd deer-flow

# Install dependencies, uv will take care of the python interpreter and venv creation, and install the required packages
uv sync

# Configure .env with your API keys
# Tavily: https://app.tavily.com/home
# Brave_SEARCH: https://brave.com/search/api/
# volcengine TTS: Add your TTS credentials if you have them
cp .env.example .env

# See the 'Supported Search Engines' and 'Text-to-Speech Integration' sections below for all available options

# Configure conf.yaml for your LLM model and API keys
# Please refer to 'docs/configuration_guide.md' for more details
# For local development, you can use Ollama or other local models
cp conf.yaml.example conf.yaml

# Install marp for ppt generation
# https://github.com/marp-team/marp-cli?tab=readme-ov-file#use-package-manager
brew install marp-cli
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Optionally, install web UI dependencies via &lt;a href="https://pnpm.io/installation"&gt;pnpm&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd deer-flow/web
pnpm install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Configurations&lt;/h3&gt; 
&lt;p&gt;Please refer to the &lt;a href="https://raw.githubusercontent.com/bytedance/deer-flow/main/docs/configuration_guide.md"&gt;Configuration Guide&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Before you start the project, read the guide carefully, and update the configurations to match your specific settings and requirements.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Console UI&lt;/h3&gt; 
&lt;p&gt;The quickest way to run the project is to use the console UI.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run the project in a bash-like shell
uv run main.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Web UI&lt;/h3&gt; 
&lt;p&gt;This project also includes a Web UI, offering a more dynamic and engaging interactive experience.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] You need to install the dependencies of web UI first.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run both the backend and frontend servers in development mode
# On macOS/Linux
./bootstrap.sh -d

# On Windows
bootstrap.bat -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Note] By default, the backend server binds to 127.0.0.1 (localhost) for security reasons. If you need to allow external connections (e.g., when deploying on Linux server), you can modify the server host to 0.0.0.0 in the bootstrap script(uv run server.py --host 0.0.0.0). Please ensure your environment is properly secured before exposing the service to external networks.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Open your browser and visit &lt;a href="http://localhost:3000"&gt;&lt;code&gt;http://localhost:3000&lt;/code&gt;&lt;/a&gt; to explore the web UI.&lt;/p&gt; 
&lt;p&gt;Explore more details in the &lt;a href="https://raw.githubusercontent.com/bytedance/deer-flow/main/web/"&gt;&lt;code&gt;web&lt;/code&gt;&lt;/a&gt; directory.&lt;/p&gt; 
&lt;h2&gt;Supported Search Engines&lt;/h2&gt; 
&lt;h3&gt;Web Search&lt;/h3&gt; 
&lt;p&gt;DeerFlow supports multiple search engines that can be configured in your &lt;code&gt;.env&lt;/code&gt; file using the &lt;code&gt;SEARCH_API&lt;/code&gt; variable:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Tavily&lt;/strong&gt; (default): A specialized search API for AI applications&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Requires &lt;code&gt;TAVILY_API_KEY&lt;/code&gt; in your &lt;code&gt;.env&lt;/code&gt; file&lt;/li&gt; 
   &lt;li&gt;Sign up at: &lt;a href="https://app.tavily.com/home"&gt;https://app.tavily.com/home&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;DuckDuckGo&lt;/strong&gt;: Privacy-focused search engine&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;No API key required&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Brave Search&lt;/strong&gt;: Privacy-focused search engine with advanced features&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Requires &lt;code&gt;BRAVE_SEARCH_API_KEY&lt;/code&gt; in your &lt;code&gt;.env&lt;/code&gt; file&lt;/li&gt; 
   &lt;li&gt;Sign up at: &lt;a href="https://brave.com/search/api/"&gt;https://brave.com/search/api/&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Arxiv&lt;/strong&gt;: Scientific paper search for academic research&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;No API key required&lt;/li&gt; 
   &lt;li&gt;Specialized for scientific and academic papers&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Searx/SearxNG&lt;/strong&gt;: Self-hosted metasearch engine&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Requires &lt;code&gt;SEARX_HOST&lt;/code&gt; to be set in the &lt;code&gt;.env&lt;/code&gt; file&lt;/li&gt; 
   &lt;li&gt;Supports connecting to either Searx or SearxNG&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To configure your preferred search engine, set the &lt;code&gt;SEARCH_API&lt;/code&gt; variable in your &lt;code&gt;.env&lt;/code&gt; file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Choose one: tavily, duckduckgo, brave_search, arxiv
SEARCH_API=tavily
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Private Knowledgebase&lt;/h3&gt; 
&lt;p&gt;DeerFlow support private knowledgebase such as ragflow and vikingdb, so that you can use your private documents to answer questions.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://ragflow.io/docs/dev/"&gt;RAGFlow&lt;/a&gt;&lt;/strong&gt;ï¼šopen source RAG engine &lt;pre&gt;&lt;code&gt;# examples in .env.example
RAG_PROVIDER=ragflow
RAGFLOW_API_URL="http://localhost:9388"
RAGFLOW_API_KEY="ragflow-xxx"
RAGFLOW_RETRIEVAL_SIZE=10
RAGFLOW_CROSS_LANGUAGES=English,Chinese,Spanish,French,German,Japanese,Korean
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;h3&gt;Core Capabilities&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ¤– &lt;strong&gt;LLM Integration&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;It supports the integration of most models through &lt;a href="https://docs.litellm.ai/docs/providers"&gt;litellm&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;Support for open source models like Qwen, you need to read the &lt;a href="https://raw.githubusercontent.com/bytedance/deer-flow/main/docs/configuration_guide.md"&gt;configuration&lt;/a&gt; for more details.&lt;/li&gt; 
   &lt;li&gt;OpenAI-compatible API interface&lt;/li&gt; 
   &lt;li&gt;Multi-tier LLM system for different task complexities&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Tools and MCP Integrations&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;ğŸ” &lt;strong&gt;Search and Retrieval&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Web search via Tavily, Brave Search and more&lt;/li&gt; 
   &lt;li&gt;Crawling with Jina&lt;/li&gt; 
   &lt;li&gt;Advanced content extraction&lt;/li&gt; 
   &lt;li&gt;Support for private knowledgebase&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ğŸ“ƒ &lt;strong&gt;RAG Integration&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Supports mentioning files from &lt;a href="https://github.com/infiniflow/ragflow"&gt;RAGFlow&lt;/a&gt; within the input box. &lt;a href="https://ragflow.io/docs/dev/"&gt;Start up RAGFlow server&lt;/a&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ğŸ”— &lt;strong&gt;MCP Seamless Integration&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Expand capabilities for private domain access, knowledge graph, web browsing and more&lt;/li&gt; 
   &lt;li&gt;Facilitates integration of diverse research tools and methodologies&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Human Collaboration&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;ğŸ§  &lt;strong&gt;Human-in-the-loop&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Supports interactive modification of research plans using natural language&lt;/li&gt; 
   &lt;li&gt;Supports auto-acceptance of research plans&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ğŸ“ &lt;strong&gt;Report Post-Editing&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Supports Notion-like block editing&lt;/li&gt; 
   &lt;li&gt;Allows AI refinements, including AI-assisted polishing, sentence shortening, and expansion&lt;/li&gt; 
   &lt;li&gt;Powered by &lt;a href="https://tiptap.dev/"&gt;tiptap&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Content Creation&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ™ï¸ &lt;strong&gt;Podcast and Presentation Generation&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;AI-powered podcast script generation and audio synthesis&lt;/li&gt; 
   &lt;li&gt;Automated creation of simple PowerPoint presentations&lt;/li&gt; 
   &lt;li&gt;Customizable templates for tailored content&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Architecture&lt;/h2&gt; 
&lt;p&gt;DeerFlow implements a modular multi-agent system architecture designed for automated research and code analysis. The system is built on LangGraph, enabling a flexible state-based workflow where components communicate through a well-defined message passing system.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/bytedance/deer-flow/main/assets/architecture.png" alt="Architecture Diagram" /&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;See it live at &lt;a href="https://deerflow.tech/#multi-agent-architecture"&gt;deerflow.tech&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;The system employs a streamlined workflow with the following components:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Coordinator&lt;/strong&gt;: The entry point that manages the workflow lifecycle&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Initiates the research process based on user input&lt;/li&gt; 
   &lt;li&gt;Delegates tasks to the planner when appropriate&lt;/li&gt; 
   &lt;li&gt;Acts as the primary interface between the user and the system&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Planner&lt;/strong&gt;: Strategic component for task decomposition and planning&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Analyzes research objectives and creates structured execution plans&lt;/li&gt; 
   &lt;li&gt;Determines if enough context is available or if more research is needed&lt;/li&gt; 
   &lt;li&gt;Manages the research flow and decides when to generate the final report&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Research Team&lt;/strong&gt;: A collection of specialized agents that execute the plan:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Researcher&lt;/strong&gt;: Conducts web searches and information gathering using tools like web search engines, crawling and even MCP services.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Coder&lt;/strong&gt;: Handles code analysis, execution, and technical tasks using Python REPL tool. Each agent has access to specific tools optimized for their role and operates within the LangGraph framework&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Reporter&lt;/strong&gt;: Final stage processor for research outputs&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Aggregates findings from the research team&lt;/li&gt; 
   &lt;li&gt;Processes and structures the collected information&lt;/li&gt; 
   &lt;li&gt;Generates comprehensive research reports&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Text-to-Speech Integration&lt;/h2&gt; 
&lt;p&gt;DeerFlow now includes a Text-to-Speech (TTS) feature that allows you to convert research reports to speech. This feature uses the volcengine TTS API to generate high-quality audio from text. Features like speed, volume, and pitch are also customizable.&lt;/p&gt; 
&lt;h3&gt;Using the TTS API&lt;/h3&gt; 
&lt;p&gt;You can access the TTS functionality through the &lt;code&gt;/api/tts&lt;/code&gt; endpoint:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Example API call using curl
curl --location 'http://localhost:8000/api/tts' \
--header 'Content-Type: application/json' \
--data '{
    "text": "This is a test of the text-to-speech functionality.",
    "speed_ratio": 1.0,
    "volume_ratio": 1.0,
    "pitch_ratio": 1.0
}' \
--output speech.mp3
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Development&lt;/h2&gt; 
&lt;h3&gt;Testing&lt;/h3&gt; 
&lt;p&gt;Run the test suite:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run all tests
make test

# Run specific test file
pytest tests/integration/test_workflow.py

# Run with coverage
make coverage
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Code Quality&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run linting
make lint

# Format code
make format
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Debugging with LangGraph Studio&lt;/h3&gt; 
&lt;p&gt;DeerFlow uses LangGraph for its workflow architecture. You can use LangGraph Studio to debug and visualize the workflow in real-time.&lt;/p&gt; 
&lt;h4&gt;Running LangGraph Studio Locally&lt;/h4&gt; 
&lt;p&gt;DeerFlow includes a &lt;code&gt;langgraph.json&lt;/code&gt; configuration file that defines the graph structure and dependencies for the LangGraph Studio. This file points to the workflow graphs defined in the project and automatically loads environment variables from the &lt;code&gt;.env&lt;/code&gt; file.&lt;/p&gt; 
&lt;h5&gt;Mac&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install uv package manager if you don't have it
curl -LsSf https://astral.sh/uv/install.sh | sh

# Install dependencies and start the LangGraph server
uvx --refresh --from "langgraph-cli[inmem]" --with-editable . --python 3.12 langgraph dev --allow-blocking
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;Windows / Linux&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install dependencies
pip install -e .
pip install -U "langgraph-cli[inmem]"

# Start the LangGraph server
langgraph dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After starting the LangGraph server, you'll see several URLs in the terminal:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;API: &lt;a href="http://127.0.0.1:2024"&gt;http://127.0.0.1:2024&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Studio UI: &lt;a href="https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024"&gt;https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;API Docs: &lt;a href="http://127.0.0.1:2024/docs"&gt;http://127.0.0.1:2024/docs&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Open the Studio UI link in your browser to access the debugging interface.&lt;/p&gt; 
&lt;h4&gt;Using LangGraph Studio&lt;/h4&gt; 
&lt;p&gt;In the Studio UI, you can:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Visualize the workflow graph and see how components connect&lt;/li&gt; 
 &lt;li&gt;Trace execution in real-time to see how data flows through the system&lt;/li&gt; 
 &lt;li&gt;Inspect the state at each step of the workflow&lt;/li&gt; 
 &lt;li&gt;Debug issues by examining inputs and outputs of each component&lt;/li&gt; 
 &lt;li&gt;Provide feedback during the planning phase to refine research plans&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;When you submit a research topic in the Studio UI, you'll be able to see the entire workflow execution, including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;The planning phase where the research plan is created&lt;/li&gt; 
 &lt;li&gt;The feedback loop where you can modify the plan&lt;/li&gt; 
 &lt;li&gt;The research and writing phases for each section&lt;/li&gt; 
 &lt;li&gt;The final report generation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Enabling LangSmith Tracing&lt;/h3&gt; 
&lt;p&gt;DeerFlow supports LangSmith tracing to help you debug and monitor your workflows. To enable LangSmith tracing:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Make sure your &lt;code&gt;.env&lt;/code&gt; file has the following configurations (see &lt;code&gt;.env.example&lt;/code&gt;):&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;LANGSMITH_TRACING=true
LANGSMITH_ENDPOINT="https://api.smith.langchain.com"
LANGSMITH_API_KEY="xxx"
LANGSMITH_PROJECT="xxx"
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Start tracing and visualize the graph locally with LangSmith by running:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;langgraph dev
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;This will enable trace visualization in LangGraph Studio and send your traces to LangSmith for monitoring and analysis.&lt;/p&gt; 
&lt;h3&gt;Checkpointing&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Postgres and MonogDB implementation of LangGraph checkpoint saver.&lt;/li&gt; 
 &lt;li&gt;In-memory store is used to caching the streaming messages before persisting to database, If finish_reason is "stop" or "interrupt", it triggers persistence.&lt;/li&gt; 
 &lt;li&gt;Supports saving and loading checkpoints for workflow execution.&lt;/li&gt; 
 &lt;li&gt;Supports saving chat stream events for replaying conversations.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;em&gt;Note: About langgraph issue #5557&lt;/em&gt; The latest langgraph-checkpoint-postgres-2.0.23 have checkpointing issue, you can check the open issue:"TypeError: Object of type HumanMessage is not JSON serializable" [https://github.com/langchain-ai/langgraph/issues/5557].&lt;/p&gt; 
&lt;p&gt;To use postgres checkpoint you should install langgraph-checkpoint-postgres-2.0.21&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Note: About psycopg dependencies&lt;/em&gt; Please read the following document before using postgres: &lt;a href="https://www.psycopg.org/psycopg3/docs/basic/install.html"&gt;https://www.psycopg.org/psycopg3/docs/basic/install.html&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;BY default, psycopg needs libpq to be installed on your system. If you don't have libpq installed, you can install psycopg with the &lt;code&gt;binary&lt;/code&gt; extra to include a statically linked version of libpq mannually:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install psycopg[binary]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will install a self-contained package with all the libraries needed, but binary not supported for all platform, you check the supported platform : &lt;a href="https://pypi.org/project/psycopg-binary/#files"&gt;https://pypi.org/project/psycopg-binary/#files&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;if not supported, you can select local-installation: &lt;a href="https://www.psycopg.org/psycopg3/docs/basic/install.html#local-installation"&gt;https://www.psycopg.org/psycopg3/docs/basic/install.html#local-installation&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;The default database and collection will be automatically created if not exists. Default database: checkpoing_db Default collection: checkpoint_writes_aio (langgraph checkpoint writes) Default collection: checkpoints_aio (langgraph checkpoints) Default collection: chat_streams (chat stream events for replaying conversations)&lt;/p&gt; 
&lt;p&gt;You need to set the following environment variables in your &lt;code&gt;.env&lt;/code&gt; file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Enable LangGraph checkpoint saver, supports MongoDB, Postgres
LANGGRAPH_CHECKPOINT_SAVER=true
# Set the database URL for saving checkpoints
LANGGRAPH_CHECKPOINT_DB_URL="mongodb://localhost:27017/"
#LANGGRAPH_CHECKPOINT_DB_URL=postgresql://localhost:5432/postgres
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Docker&lt;/h2&gt; 
&lt;p&gt;You can also run this project with Docker.&lt;/p&gt; 
&lt;p&gt;First, you need read the &lt;a href="https://raw.githubusercontent.com/bytedance/deer-flow/main/docs/configuration_guide.md"&gt;configuration&lt;/a&gt; below. Make sure &lt;code&gt;.env&lt;/code&gt;, &lt;code&gt;.conf.yaml&lt;/code&gt; files are ready.&lt;/p&gt; 
&lt;p&gt;Second, to build a Docker image of your own web server:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker build -t deer-flow-api .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Final, start up a docker container running the web server:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Replace deer-flow-api-app with your preferred container name
# Start the server then bind to localhost:8000
docker run -d -t -p 127.0.0.1:8000:8000 --env-file .env --name deer-flow-api-app deer-flow-api

# stop the server
docker stop deer-flow-api-app
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Docker Compose (include both backend and frontend)&lt;/h3&gt; 
&lt;p&gt;DeerFlow provides a docker-compose setup to easily run both the backend and frontend together:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# building docker image
docker compose build

# start the server
docker compose up
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING] If you want to deploy the deer flow into production environments, please add authentication to the website and evaluate your security check of the MCPServer and Python Repl.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;p&gt;The following examples demonstrate the capabilities of DeerFlow:&lt;/p&gt; 
&lt;h3&gt;Research Reports&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;OpenAI Sora Report&lt;/strong&gt; - Analysis of OpenAI's Sora AI tool&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Discusses features, access, prompt engineering, limitations, and ethical considerations&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/bytedance/deer-flow/main/examples/openai_sora_report.md"&gt;View full report&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Google's Agent to Agent Protocol Report&lt;/strong&gt; - Overview of Google's Agent to Agent (A2A) protocol&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Discusses its role in AI agent communication and its relationship with Anthropic's Model Context Protocol (MCP)&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/bytedance/deer-flow/main/examples/what_is_agent_to_agent_protocol.md"&gt;View full report&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;What is MCP?&lt;/strong&gt; - A comprehensive analysis of the term "MCP" across multiple contexts&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Explores Model Context Protocol in AI, Monocalcium Phosphate in chemistry, and Micro-channel Plate in electronics&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/bytedance/deer-flow/main/examples/what_is_mcp.md"&gt;View full report&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Bitcoin Price Fluctuations&lt;/strong&gt; - Analysis of recent Bitcoin price movements&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Examines market trends, regulatory influences, and technical indicators&lt;/li&gt; 
   &lt;li&gt;Provides recommendations based on historical data&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/bytedance/deer-flow/main/examples/bitcoin_price_fluctuation.md"&gt;View full report&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;What is LLM?&lt;/strong&gt; - An in-depth exploration of Large Language Models&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Discusses architecture, training, applications, and ethical considerations&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/bytedance/deer-flow/main/examples/what_is_llm.md"&gt;View full report&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;How to Use Claude for Deep Research?&lt;/strong&gt; - Best practices and workflows for using Claude in deep research&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Covers prompt engineering, data analysis, and integration with other tools&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/bytedance/deer-flow/main/examples/how_to_use_claude_deep_research.md"&gt;View full report&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;AI Adoption in Healthcare: Influencing Factors&lt;/strong&gt; - Analysis of factors driving AI adoption in healthcare&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Discusses AI technologies, data quality, ethical considerations, economic evaluations, organizational readiness, and digital infrastructure&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/bytedance/deer-flow/main/examples/AI_adoption_in_healthcare.md"&gt;View full report&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Quantum Computing Impact on Cryptography&lt;/strong&gt; - Analysis of quantum computing's impact on cryptography&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Discusses vulnerabilities of classical cryptography, post-quantum cryptography, and quantum-resistant cryptographic solutions&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/bytedance/deer-flow/main/examples/Quantum_Computing_Impact_on_Cryptography.md"&gt;View full report&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Cristiano Ronaldo's Performance Highlights&lt;/strong&gt; - Analysis of Cristiano Ronaldo's performance highlights&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Discusses his career achievements, international goals, and performance in various matches&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/bytedance/deer-flow/main/examples/Cristiano_Ronaldo's_Performance_Highlights.md"&gt;View full report&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;To run these examples or create your own research reports, you can use the following commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run with a specific query
uv run main.py "What factors are influencing AI adoption in healthcare?"

# Run with custom planning parameters
uv run main.py --max_plan_iterations 3 "How does quantum computing impact cryptography?"

# Run in interactive mode with built-in questions
uv run main.py --interactive

# Or run with basic interactive prompt
uv run main.py

# View all available options
uv run main.py --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Interactive Mode&lt;/h3&gt; 
&lt;p&gt;The application now supports an interactive mode with built-in questions in both English and Chinese:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Launch the interactive mode:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;uv run main.py --interactive
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Select your preferred language (English or ä¸­æ–‡)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Choose from a list of built-in questions or select the option to ask your own question&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;The system will process your question and generate a comprehensive research report&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Human in the Loop&lt;/h3&gt; 
&lt;p&gt;DeerFlow includes a human in the loop mechanism that allows you to review, edit, and approve research plans before they are executed:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Plan Review&lt;/strong&gt;: When human in the loop is enabled, the system will present the generated research plan for your review before execution&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Providing Feedback&lt;/strong&gt;: You can:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Accept the plan by responding with &lt;code&gt;[ACCEPTED]&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Edit the plan by providing feedback (e.g., &lt;code&gt;[EDIT PLAN] Add more steps about technical implementation&lt;/code&gt;)&lt;/li&gt; 
   &lt;li&gt;The system will incorporate your feedback and generate a revised plan&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Auto-acceptance&lt;/strong&gt;: You can enable auto-acceptance to skip the review process:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Via API: Set &lt;code&gt;auto_accepted_plan: true&lt;/code&gt; in your request&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;API Integration&lt;/strong&gt;: When using the API, you can provide feedback through the &lt;code&gt;feedback&lt;/code&gt; parameter:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-json"&gt;{
  "messages": [{ "role": "user", "content": "What is quantum computing?" }],
  "thread_id": "my_thread_id",
  "auto_accepted_plan": false,
  "feedback": "[EDIT PLAN] Include more about quantum algorithms"
}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Command Line Arguments&lt;/h3&gt; 
&lt;p&gt;The application supports several command-line arguments to customize its behavior:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;query&lt;/strong&gt;: The research query to process (can be multiple words)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;--interactive&lt;/strong&gt;: Run in interactive mode with built-in questions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;--max_plan_iterations&lt;/strong&gt;: Maximum number of planning cycles (default: 1)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;--max_step_num&lt;/strong&gt;: Maximum number of steps in a research plan (default: 3)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;--debug&lt;/strong&gt;: Enable detailed debug logging&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;FAQ&lt;/h2&gt; 
&lt;p&gt;Please refer to the &lt;a href="https://raw.githubusercontent.com/bytedance/deer-flow/main/docs/FAQ.md"&gt;FAQ.md&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is open source and available under the &lt;a href="https://raw.githubusercontent.com/bytedance/deer-flow/main/LICENSE"&gt;MIT License&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Acknowledgments&lt;/h2&gt; 
&lt;p&gt;DeerFlow is built upon the incredible work of the open-source community. We are deeply grateful to all the projects and contributors whose efforts have made DeerFlow possible. Truly, we stand on the shoulders of giants.&lt;/p&gt; 
&lt;p&gt;We would like to extend our sincere appreciation to the following projects for their invaluable contributions:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/langchain-ai/langchain"&gt;LangChain&lt;/a&gt;&lt;/strong&gt;: Their exceptional framework powers our LLM interactions and chains, enabling seamless integration and functionality.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/langchain-ai/langgraph"&gt;LangGraph&lt;/a&gt;&lt;/strong&gt;: Their innovative approach to multi-agent orchestration has been instrumental in enabling DeerFlow's sophisticated workflows.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/steven-tey/novel"&gt;Novel&lt;/a&gt;&lt;/strong&gt;: Their Notion-style WYSIWYG editor supports our report editing and AI-assisted rewriting.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/infiniflow/ragflow"&gt;RAGFlow&lt;/a&gt;&lt;/strong&gt;: We have achieved support for research on users' private knowledge bases through integration with RAGFlow.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;These projects exemplify the transformative power of open-source collaboration, and we are proud to build upon their foundations.&lt;/p&gt; 
&lt;h3&gt;Key Contributors&lt;/h3&gt; 
&lt;p&gt;A heartfelt thank you goes out to the core authors of &lt;code&gt;DeerFlow&lt;/code&gt;, whose vision, passion, and dedication have brought this project to life:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/hetaoBackend/"&gt;Daniel Walnut&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/magiccube/"&gt;Henry Li&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Your unwavering commitment and expertise have been the driving force behind DeerFlow's success. We are honored to have you at the helm of this journey.&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#bytedance/deer-flow&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=bytedance/deer-flow&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>microsoft/RD-Agent</title>
      <link>https://github.com/microsoft/RD-Agent</link>
      <description>&lt;p&gt;Research and development (R&amp;D) is crucial for the enhancement of industrial productivity, especially in the AI era, where the core aspects of R&amp;D are mainly focused on data and models. We are committed to automating these high-value generic R&amp;D processes through R&amp;D-Agent, which lets AI drive data-driven AI. ğŸ”—https://aka.ms/RD-Agent-Tech-Report&lt;/p&gt;&lt;hr&gt;&lt;h4 align="center"&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/RD-Agent/main/docs/_static/logo.png" alt="RA-Agent logo" style="width:70%; " /&gt; &lt;p&gt;&lt;a href="https://rdagent.azurewebsites.net" target="_blank"&gt;ğŸ–¥ï¸ Live Demo&lt;/a&gt; | &lt;a href="https://rdagent.azurewebsites.net/factor_loop" target="_blank"&gt;ğŸ¥ Demo Video&lt;/a&gt; &lt;a href="https://www.youtube.com/watch?v=JJ4JYO3HscM&amp;amp;list=PLALmKB0_N3_i52fhUmPQiL4jsO354uopR" target="_blank"&gt;â–¶ï¸YouTube&lt;/a&gt; | &lt;a href="https://rdagent.readthedocs.io/en/latest/index.html" target="_blank"&gt;ğŸ“– Documentation&lt;/a&gt; | &lt;a href="https://aka.ms/RD-Agent-Tech-Report" target="_blank"&gt;ğŸ“„ Tech Report&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/RD-Agent/main/#-paperwork-list"&gt; ğŸ“ƒ Papers &lt;/a&gt;&lt;/p&gt; &lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://github.com/microsoft/RD-Agent/actions/workflows/ci.yml"&gt;&lt;img src="https://github.com/microsoft/RD-Agent/actions/workflows/ci.yml/badge.svg?sanitize=true" alt="CI" /&gt;&lt;/a&gt; &lt;a href="https://github.com/microsoft/RD-Agent/actions/workflows/github-code-scanning/codeql"&gt;&lt;img src="https://github.com/microsoft/RD-Agent/actions/workflows/github-code-scanning/codeql/badge.svg?sanitize=true" alt="CodeQL" /&gt;&lt;/a&gt; &lt;a href="https://github.com/microsoft/RD-Agent/actions/workflows/dependabot/dependabot-updates"&gt;&lt;img src="https://github.com/microsoft/RD-Agent/actions/workflows/dependabot/dependabot-updates/badge.svg?sanitize=true" alt="Dependabot Updates" /&gt;&lt;/a&gt; &lt;a href="https://github.com/microsoft/RD-Agent/actions/workflows/pr.yml"&gt;&lt;img src="https://github.com/microsoft/RD-Agent/actions/workflows/pr.yml/badge.svg?sanitize=true" alt="Lint PR Title" /&gt;&lt;/a&gt; &lt;a href="https://github.com/microsoft/RD-Agent/actions/workflows/release.yml"&gt;&lt;img src="https://github.com/microsoft/RD-Agent/actions/workflows/release.yml/badge.svg?sanitize=true" alt="Release.yml" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/rdagent/#files"&gt;&lt;img src="https://img.shields.io/badge/platform-Linux-blue" alt="Platform" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/rdagent/"&gt;&lt;img src="https://img.shields.io/pypi/v/rdagent" alt="PyPI" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/rdagent/"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/rdagent" alt="PyPI - Python Version" /&gt;&lt;/a&gt; &lt;a href="https://github.com/microsoft/RD-Agent/releases"&gt;&lt;img src="https://img.shields.io/github/v/release/microsoft/RD-Agent" alt="Release" /&gt;&lt;/a&gt; &lt;a href="https://github.com/microsoft/RD-Agent/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/microsoft/RD-Agent" alt="GitHub" /&gt;&lt;/a&gt; &lt;a href="https://github.com/pre-commit/pre-commit"&gt;&lt;img src="https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit" alt="pre-commit" /&gt;&lt;/a&gt; &lt;a href="http://mypy-lang.org/"&gt;&lt;img src="https://www.mypy-lang.org/static/mypy_badge.svg?sanitize=true" alt="Checked with mypy" /&gt;&lt;/a&gt; &lt;a href="https://github.com/astral-sh/ruff"&gt;&lt;img src="https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json" alt="Ruff" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/ybQ97B6Jjy"&gt;&lt;img src="https://img.shields.io/badge/chat-discord-blue" alt="Chat" /&gt;&lt;/a&gt; &lt;a href="https://rdagent.readthedocs.io/en/latest/?badge=latest"&gt;&lt;img src="https://readthedocs.org/projects/rdagent/badge/?version=latest" alt="Documentation Status" /&gt;&lt;/a&gt; &lt;a href="https://github.com/microsoft/RD-Agent/actions/workflows/readthedocs-preview.yml"&gt;&lt;img src="https://github.com/microsoft/RD-Agent/actions/workflows/readthedocs-preview.yml/badge.svg?sanitize=true" alt="Readthedocs Preview" /&gt;&lt;/a&gt; 
 &lt;!-- this badge is too long, please place it in the last one to make it pretty --&gt; &lt;a href="https://arxiv.org/abs/2505.14738"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2505.14738-00ff00.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;ğŸ“° News&lt;/h1&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;ğŸ—ï¸ News&lt;/th&gt; 
   &lt;th&gt;ğŸ“ Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;NeurIPS 2025 Acceptance&lt;/td&gt; 
   &lt;td&gt;We are thrilled to announce that our paper &lt;a href="https://arxiv.org/abs/2505.15155"&gt;R&amp;amp;D-Agent-Quant&lt;/a&gt; has been accepted to NeurIPS 2025&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/RD-Agent/main/#overall-technical-report"&gt;Technical Report Release&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Overall framework description and results on MLE-bench&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/RD-Agent/main/#deep-application-in-diverse-scenarios"&gt;R&amp;amp;D-Agent-Quant Release&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Apply R&amp;amp;D-Agent to quant trading&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MLE-Bench Results Released&lt;/td&gt; 
   &lt;td&gt;R&amp;amp;D-Agent currently leads as the &lt;a href="https://raw.githubusercontent.com/microsoft/RD-Agent/main/#-the-best-machine-learning-engineering-agent"&gt;top-performing machine learning engineering agent&lt;/a&gt; on MLE-bench&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Support LiteLLM Backend&lt;/td&gt; 
   &lt;td&gt;We now fully support &lt;strong&gt;&lt;a href="https://github.com/BerriAI/litellm"&gt;LiteLLM&lt;/a&gt;&lt;/strong&gt; as our default backend for integration with multiple LLM providers.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;General Data Science Agent&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://rdagent.readthedocs.io/en/latest/scens/data_science.html"&gt;Data Science Agent&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Kaggle Scenario release&lt;/td&gt; 
   &lt;td&gt;We release &lt;strong&gt;&lt;a href="https://rdagent.readthedocs.io/en/latest/scens/data_science.html"&gt;Kaggle Agent&lt;/a&gt;&lt;/strong&gt;, try the new features!&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Official WeChat group release&lt;/td&gt; 
   &lt;td&gt;We created a WeChat group, welcome to join! (ğŸ—ª&lt;a href="https://github.com/microsoft/RD-Agent/issues/880"&gt;QR Code&lt;/a&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Official Discord release&lt;/td&gt; 
   &lt;td&gt;We launch our first chatting channel in Discord (ğŸ—ª&lt;a href="https://discord.gg/ybQ97B6Jjy"&gt;&lt;img src="https://img.shields.io/badge/chat-discord-blue" alt="Chat" /&gt;&lt;/a&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;First release&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;R&amp;amp;D-Agent&lt;/strong&gt; is released on GitHub&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;ğŸ† The Best Machine Learning Engineering Agent!&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/openai/mle-bench"&gt;MLE-bench&lt;/a&gt; is a comprehensive benchmark evaluating the performance of AI agents on machine learning engineering tasks. Utilizing datasets from 75 Kaggle competitions, MLE-bench provides robust assessments of AI systems' capabilities in real-world ML engineering scenarios.&lt;/p&gt; 
&lt;p&gt;R&amp;amp;D-Agent currently leads as the top-performing machine learning engineering agent on MLE-bench:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Agent&lt;/th&gt; 
   &lt;th&gt;Low == Lite (%)&lt;/th&gt; 
   &lt;th&gt;Medium (%)&lt;/th&gt; 
   &lt;th&gt;High (%)&lt;/th&gt; 
   &lt;th&gt;All (%)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;R&amp;amp;D-Agent o3(R)+GPT-4.1(D)&lt;/td&gt; 
   &lt;td&gt;51.52 Â± 6.9&lt;/td&gt; 
   &lt;td&gt;19.3 Â± 5.5&lt;/td&gt; 
   &lt;td&gt;26.67 Â± 0&lt;/td&gt; 
   &lt;td&gt;30.22 Â± 1.5&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;R&amp;amp;D-Agent o1-preview&lt;/td&gt; 
   &lt;td&gt;48.18 Â± 2.49&lt;/td&gt; 
   &lt;td&gt;8.95 Â± 2.36&lt;/td&gt; 
   &lt;td&gt;18.67 Â± 2.98&lt;/td&gt; 
   &lt;td&gt;22.4 Â± 1.1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;AIDE o1-preview&lt;/td&gt; 
   &lt;td&gt;34.3 Â± 2.4&lt;/td&gt; 
   &lt;td&gt;8.8 Â± 1.1&lt;/td&gt; 
   &lt;td&gt;10.0 Â± 1.9&lt;/td&gt; 
   &lt;td&gt;16.9 Â± 1.1&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Notes:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;O3(R)+GPT-4.1(D)&lt;/strong&gt;: This version is designed to both reduce average time per loop and leverage a cost-effective combination of backend LLMs by seamlessly integrating Research Agent (o3) with Development Agent (GPT-4.1).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;AIDE o1-preview&lt;/strong&gt;: Represents the previously best public result on MLE-bench as reported in the original MLE-bench paper.&lt;/li&gt; 
 &lt;li&gt;Average and standard deviation results for R&amp;amp;D-Agent o1-preview is based on a independent of 5 seeds and for R&amp;amp;D-Agent o3(R)+GPT-4.1(D) is based on 6 seeds.&lt;/li&gt; 
 &lt;li&gt;According to MLE-Bench, the 75 competitions are categorized into three levels of complexity: &lt;strong&gt;Low==Lite&lt;/strong&gt; if we estimate that an experienced ML engineer can produce a sensible solution in under 2 hours, excluding the time taken to train any models; &lt;strong&gt;Medium&lt;/strong&gt; if it takes between 2 and 10 hours; and &lt;strong&gt;High&lt;/strong&gt; if it takes more than 10 hours.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can inspect the detailed runs of the above results online.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://aka.ms/RD-Agent_MLE-Bench_O1-preview"&gt;R&amp;amp;D-Agent o1-preview detailed runs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aka.ms/RD-Agent_MLE-Bench_O3_GPT41"&gt;R&amp;amp;D-Agent o3(R)+GPT-4.1(D) detailed runs&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For running R&amp;amp;D-Agent on MLE-bench, refer to &lt;strong&gt;&lt;a href="https://rdagent.readthedocs.io/en/latest/scens/data_science.html"&gt;MLE-bench Guide: Running ML Engineering via MLE-bench&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h1&gt;ğŸ¥‡ The First Data-Centric Quant Multi-Agent Framework!&lt;/h1&gt; 
&lt;p&gt;R&amp;amp;D-Agent for Quantitative Finance, in short &lt;strong&gt;RD-Agent(Q)&lt;/strong&gt;, is the first data-centric, multi-agent framework designed to automate the full-stack research and development of quantitative strategies via coordinated factor-model co-optimization.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/3198bc10-47ba-4ee0-8a8e-46d5ce44f45d" alt="image" /&gt;&lt;/p&gt; 
&lt;p&gt;Extensive experiments in real stock markets show that, at a cost under $10, RD-Agent(Q) achieves approximately 2Ã— higher ARR than benchmark factor libraries while using over 70% fewer factors. It also surpasses state-of-the-art deep time-series models under smaller resource budgets. Its alternating factorâ€“model optimization further delivers excellent trade-off between predictive accuracy and strategy robustness.&lt;/p&gt; 
&lt;p&gt;You can learn more details about &lt;strong&gt;RD-Agent(Q)&lt;/strong&gt; through the &lt;a href="https://arxiv.org/abs/2505.15155"&gt;paper&lt;/a&gt; and reproduce it through the &lt;a href="https://rdagent.readthedocs.io/en/latest/scens/quant_agent_fin.html"&gt;documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Data Science Agent Preview&lt;/h1&gt; 
&lt;p&gt;Check out our demo video showcasing the current progress of our Data Science Agent under development:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/3eccbecb-34a4-4c81-bce4-d3f8862f7305"&gt;https://github.com/user-attachments/assets/3eccbecb-34a4-4c81-bce4-d3f8862f7305&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;ğŸŒŸ Introduction&lt;/h1&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/microsoft/RD-Agent/main/docs/_static/scen.png" alt="Our focused scenario" style="width:80%; " /&gt; 
&lt;/div&gt; 
&lt;p&gt;R&amp;amp;D-Agent aims to automate the most critical and valuable aspects of the industrial R&amp;amp;D process, and we begin with focusing on the data-driven scenarios to streamline the development of models and data. Methodologically, we have identified a framework with two key components: 'R' for proposing new ideas and 'D' for implementing them. We believe that the automatic evolution of R&amp;amp;D will lead to solutions of significant industrial value.&lt;/p&gt; 
&lt;!-- Tag Cloud --&gt; 
&lt;p&gt;R&amp;amp;D is a very general scenario. The advent of R&amp;amp;D-Agent can be your&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ’° &lt;strong&gt;Automatic Quant Factory&lt;/strong&gt; (&lt;a href="https://rdagent.azurewebsites.net/factor_loop"&gt;ğŸ¥Demo Video&lt;/a&gt;|&lt;a href="https://www.youtube.com/watch?v=X4DK2QZKaKY&amp;amp;t=6s"&gt;â–¶ï¸YouTube&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;ğŸ¤– &lt;strong&gt;Data Mining Agent:&lt;/strong&gt; Iteratively proposing data &amp;amp; models (&lt;a href="https://rdagent.azurewebsites.net/model_loop"&gt;ğŸ¥Demo Video 1&lt;/a&gt;|&lt;a href="https://www.youtube.com/watch?v=dm0dWL49Bc0&amp;amp;t=104s"&gt;â–¶ï¸YouTube&lt;/a&gt;) (&lt;a href="https://rdagent.azurewebsites.net/dmm"&gt;ğŸ¥Demo Video 2&lt;/a&gt;|&lt;a href="https://www.youtube.com/watch?v=VIaSTZuoZg4"&gt;â–¶ï¸YouTube&lt;/a&gt;) and implementing them by gaining knowledge from data.&lt;/li&gt; 
 &lt;li&gt;ğŸ¦¾ &lt;strong&gt;Research Copilot:&lt;/strong&gt; Auto read research papers (&lt;a href="https://rdagent.azurewebsites.net/report_model"&gt;ğŸ¥Demo Video&lt;/a&gt;|&lt;a href="https://www.youtube.com/watch?v=BiA2SfdKQ7o"&gt;â–¶ï¸YouTube&lt;/a&gt;) / financial reports (&lt;a href="https://rdagent.azurewebsites.net/report_factor"&gt;ğŸ¥Demo Video&lt;/a&gt;|&lt;a href="https://www.youtube.com/watch?v=ECLTXVcSx-c"&gt;â–¶ï¸YouTube&lt;/a&gt;) and implement model structures or building datasets.&lt;/li&gt; 
 &lt;li&gt;ğŸ¤– &lt;strong&gt;Kaggle Agent:&lt;/strong&gt; Auto Model Tuning and Feature Engineering(&lt;a href=""&gt;ğŸ¥Demo Video Coming Soon...&lt;/a&gt;) and implementing them to achieve more in competitions.&lt;/li&gt; 
 &lt;li&gt;...&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can click the links above to view the demo. We're continuously adding more methods and scenarios to the project to enhance your R&amp;amp;D processes and boost productivity.&lt;/p&gt; 
&lt;p&gt;Additionally, you can take a closer look at the examples in our &lt;strong&gt;&lt;a href="https://rdagent.azurewebsites.net/"&gt;ğŸ–¥ï¸ Live Demo&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://rdagent.azurewebsites.net/" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/RD-Agent/main/docs/_static/demo.png" alt="Watch the demo" width="80%" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h1&gt;âš¡ Quick start&lt;/h1&gt; 
&lt;h3&gt;RD-Agent currently only supports Linux.&lt;/h3&gt; 
&lt;p&gt;You can try above demos by running the following command:&lt;/p&gt; 
&lt;h3&gt;ğŸ³ Docker installation.&lt;/h3&gt; 
&lt;p&gt;Users must ensure Docker is installed before attempting most scenarios. Please refer to the &lt;a href="https://docs.docker.com/engine/install/"&gt;official ğŸ³Docker page&lt;/a&gt; for installation instructions. Ensure the current user can run Docker commands &lt;strong&gt;without using sudo&lt;/strong&gt;. You can verify this by executing &lt;code&gt;docker run hello-world&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;ğŸ Create a Conda Environment&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Create a new conda environment with Python (3.10 and 3.11 are well-tested in our CI): &lt;pre&gt;&lt;code class="language-sh"&gt;conda create -n rdagent python=3.10
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Activate the environment: &lt;pre&gt;&lt;code class="language-sh"&gt;conda activate rdagent
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ› ï¸ Install the R&amp;amp;D-Agent&lt;/h3&gt; 
&lt;h4&gt;For Users&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;You can directly install the R&amp;amp;D-Agent package from PyPI: &lt;pre&gt;&lt;code class="language-sh"&gt;pip install rdagent
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;For Developers&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;If you want to try the latest version or contribute to RD-Agent, you can install it from the source and follow the development setup: &lt;pre&gt;&lt;code class="language-sh"&gt;git clone https://github.com/microsoft/RD-Agent
cd RD-Agent
make dev
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;More details can be found in the &lt;a href="https://rdagent.readthedocs.io/en/latest/development.html"&gt;development setup&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;ğŸ’Š Health check&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;rdagent provides a health check that currently checks two things. 
  &lt;ul&gt; 
   &lt;li&gt;whether the docker installation was successful.&lt;/li&gt; 
   &lt;li&gt;whether the default port used by the &lt;a href="https://github.com/microsoft/RD-Agent?tab=readme-ov-file#%EF%B8%8F-monitor-the-application-results"&gt;rdagent ui&lt;/a&gt; is occupied.&lt;/li&gt; 
  &lt;/ul&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;rdagent health_check --no-check-env
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;âš™ï¸ Configuration&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;The demos requires following ability:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;ChatCompletion&lt;/li&gt; 
   &lt;li&gt;json_mode&lt;/li&gt; 
   &lt;li&gt;embedding query&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;You can set your Chat Model and Embedding Model in the following ways:&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;&lt;strong&gt;ğŸ”¥ Attention&lt;/strong&gt;: We now provide experimental support for &lt;strong&gt;DeepSeek&lt;/strong&gt; models! You can use DeepSeek's official API for cost-effective and high-performance inference. See the configuration example below for DeepSeek setup.&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Using LiteLLM (Default)&lt;/strong&gt;: We now support LiteLLM as a backend for integration with multiple LLM providers. You can configure in multiple ways:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Option 1: Unified API base for both models&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Configuration Example: &lt;code&gt;OpenAI&lt;/code&gt; Setup :&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cat &amp;lt;&amp;lt; EOF  &amp;gt; .env
# Set to any model supported by LiteLLM.
CHAT_MODEL=gpt-4o 
EMBEDDING_MODEL=text-embedding-3-small
# Configure unified API base
OPENAI_API_BASE=&amp;lt;your_unified_api_base&amp;gt;
OPENAI_API_KEY=&amp;lt;replace_with_your_openai_api_key&amp;gt;
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;Configuration Example: &lt;code&gt;Azure OpenAI&lt;/code&gt; Setup :&lt;/em&gt;&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;Before using this configuration, please confirm in advance that your &lt;code&gt;Azure OpenAI API key&lt;/code&gt; supports &lt;code&gt;embedded models&lt;/code&gt;.&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cat &amp;lt;&amp;lt; EOF  &amp;gt; .env
EMBEDDING_MODEL=azure/&amp;lt;Model deployment supporting embedding&amp;gt;
CHAT_MODEL=azure/&amp;lt;your deployment name&amp;gt;
AZURE_API_KEY=&amp;lt;replace_with_your_openai_api_key&amp;gt;
AZURE_API_BASE=&amp;lt;your_unified_api_base&amp;gt;
AZURE_API_VERSION=&amp;lt;azure api version&amp;gt;
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Option 2: Separate API bases for Chat and Embedding models&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cat &amp;lt;&amp;lt; EOF  &amp;gt; .env
# Set to any model supported by LiteLLM.
# Configure separate API bases for chat and embedding

# CHAT MODEL:
CHAT_MODEL=gpt-4o 
OPENAI_API_BASE=&amp;lt;your_chat_api_base&amp;gt;
OPENAI_API_KEY=&amp;lt;replace_with_your_openai_api_key&amp;gt;

# EMBEDDING MODEL:
# TAKE siliconflow as an example, you can use other providers.
# Note: embedding requires litellm_proxy prefix
EMBEDDING_MODEL=litellm_proxy/BAAI/bge-large-en-v1.5
LITELLM_PROXY_API_KEY=&amp;lt;replace_with_your_siliconflow_api_key&amp;gt;
LITELLM_PROXY_API_BASE=https://api.siliconflow.cn/v1
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;Configuration Example: &lt;code&gt;DeepSeek&lt;/code&gt; Setup :&lt;/em&gt;&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;Since many users encounter configuration errors when setting up DeepSeek. Here's a complete working example for DeepSeek Setup:&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cat &amp;lt;&amp;lt; EOF  &amp;gt; .env
# CHAT MODEL: Using DeepSeek Official API
CHAT_MODEL=deepseek/deepseek-chat 
DEEPSEEK_API_KEY=&amp;lt;replace_with_your_deepseek_api_key&amp;gt;

# EMBEDDING MODEL: Using SiliconFlow for embedding since deepseek has no embedding model.
# Note: embedding requires litellm_proxy prefix
EMBEDDING_MODEL=litellm_proxy/BAAI/bge-m3
LITELLM_PROXY_API_KEY=&amp;lt;replace_with_your_siliconflow_api_key&amp;gt;
LITELLM_PROXY_API_BASE=https://api.siliconflow.cn/v1
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Notice: If you are using reasoning models that include thought processes in their responses (such as &amp;lt;think&amp;gt; tags), you need to set the following environment variable:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;REASONING_THINK_RM=True
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can also use a deprecated backend if you only use &lt;code&gt;OpenAI API&lt;/code&gt; or &lt;code&gt;Azure OpenAI&lt;/code&gt; directly. For this deprecated setting and more configuration information, please refer to the &lt;a href="https://rdagent.readthedocs.io/en/latest/installation_and_configuration.html"&gt;documentation&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;If your environment configuration is complete, please execute the following commands to check if your configuration is valid. This step is necessary.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;rdagent health_check
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸš€ Run the Application&lt;/h3&gt; 
&lt;p&gt;The &lt;strong&gt;&lt;a href="https://rdagent.azurewebsites.net/"&gt;ğŸ–¥ï¸ Live Demo&lt;/a&gt;&lt;/strong&gt; is implemented by the following commands(each item represents one demo, you can select the one you prefer):&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Run the &lt;strong&gt;Automated Quantitative Trading &amp;amp; Iterative Factors Model Joint Evolution&lt;/strong&gt;: &lt;a href="http://github.com/microsoft/qlib"&gt;Qlib&lt;/a&gt; self-loop factor &amp;amp; model proposal and implementation application&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;rdagent fin_quant
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run the &lt;strong&gt;Automated Quantitative Trading &amp;amp; Iterative Factors Evolution&lt;/strong&gt;: &lt;a href="http://github.com/microsoft/qlib"&gt;Qlib&lt;/a&gt; self-loop factor proposal and implementation application&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;rdagent fin_factor
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run the &lt;strong&gt;Automated Quantitative Trading &amp;amp; Iterative Model Evolution&lt;/strong&gt;: &lt;a href="http://github.com/microsoft/qlib"&gt;Qlib&lt;/a&gt; self-loop model proposal and implementation application&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;rdagent fin_model
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run the &lt;strong&gt;Automated Quantitative Trading &amp;amp; Factors Extraction from Financial Reports&lt;/strong&gt;: Run the &lt;a href="http://github.com/microsoft/qlib"&gt;Qlib&lt;/a&gt; factor extraction and implementation application based on financial reports&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;# 1. Generally, you can run this scenario using the following command:
rdagent fin_factor_report --report-folder=&amp;lt;Your financial reports folder path&amp;gt;

# 2. Specifically, you need to prepare some financial reports first. You can follow this concrete example:
wget https://github.com/SunsetWolf/rdagent_resource/releases/download/reports/all_reports.zip
unzip all_reports.zip -d git_ignore_folder/reports
rdagent fin_factor_report --report-folder=git_ignore_folder/reports
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run the &lt;strong&gt;Automated Model Research &amp;amp; Development Copilot&lt;/strong&gt;: model extraction and implementation application&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;# 1. Generally, you can run your own papers/reports with the following command:
rdagent general_model &amp;lt;Your paper URL&amp;gt;

# 2. Specifically, you can do it like this. For more details and additional paper examples, use `rdagent general_model -h`:
rdagent general_model  "https://arxiv.org/pdf/2210.09789"
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run the &lt;strong&gt;Automated Medical Prediction Model Evolution&lt;/strong&gt;: Medical self-loop model proposal and implementation application&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Generally, you can run the data science program with the following command:
rdagent data_science --competition &amp;lt;your competition name&amp;gt;

# Specifically, you need to create a folder for storing competition files (e.g., competition description file, competition datasets, etc.), and configure the path to the folder in your environment. In addition, you need to use chromedriver when you download the competition descriptors, which you can follow for this specific example:

# 1. Download the dataset, extract it to the target folder.
wget https://github.com/SunsetWolf/rdagent_resource/releases/download/ds_data/arf-12-hours-prediction-task.zip
unzip arf-12-hours-prediction-task.zip -d ./git_ignore_folder/ds_data/

# 2. Configure environment variables in the `.env` file
dotenv set DS_LOCAL_DATA_PATH "$(pwd)/git_ignore_folder/ds_data"
dotenv set DS_CODER_ON_WHOLE_PIPELINE True
dotenv set DS_IF_USING_MLE_DATA False
dotenv set DS_SAMPLE_DATA_BY_LLM False
dotenv set DS_SCEN rdagent.scenarios.data_science.scen.DataScienceScen

# 3. run the application
rdagent data_science --competition arf-12-hours-prediction-task
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; For more information about the dataset, please refer to the &lt;a href="https://rdagent.readthedocs.io/en/latest/scens/data_science.html"&gt;documentation&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run the &lt;strong&gt;Automated Kaggle Model Tuning &amp;amp; Feature Engineering&lt;/strong&gt;: self-loop model proposal and feature engineering implementation application &lt;br /&gt;&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;Using &lt;strong&gt;tabular-playground-series-dec-2021&lt;/strong&gt; as an example. &lt;br /&gt;&lt;/p&gt; 
   &lt;ol&gt; 
    &lt;li&gt;Register and login on the &lt;a href="https://www.kaggle.com/"&gt;Kaggle&lt;/a&gt; website. &lt;br /&gt;&lt;/li&gt; 
    &lt;li&gt;Configuring the Kaggle API. &lt;br /&gt; (1) Click on the avatar (usually in the top right corner of the page) -&amp;gt; &lt;code&gt;Settings&lt;/code&gt; -&amp;gt; &lt;code&gt;Create New Token&lt;/code&gt;, A file called &lt;code&gt;kaggle.json&lt;/code&gt; will be downloaded. &lt;br /&gt; (2) Move &lt;code&gt;kaggle.json&lt;/code&gt; to &lt;code&gt;~/.config/kaggle/&lt;/code&gt; &lt;br /&gt; (3) Modify the permissions of the kaggle.json file. Reference command: &lt;code&gt;chmod 600 ~/.config/kaggle/kaggle.json&lt;/code&gt; &lt;br /&gt;&lt;/li&gt; 
    &lt;li&gt;Join the competition: Click &lt;code&gt;Join the competition&lt;/code&gt; -&amp;gt; &lt;code&gt;I Understand and Accept&lt;/code&gt; at the bottom of the &lt;a href="https://www.kaggle.com/competitions/tabular-playground-series-dec-2021/data"&gt;competition details page&lt;/a&gt;.&lt;/li&gt; 
   &lt;/ol&gt; 
  &lt;/blockquote&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Generally, you can run the Kaggle competition program with the following command:
rdagent data_science --competition &amp;lt;your competition name&amp;gt;

# 1. Configure environment variables in the `.env` file
mkdir -p ./git_ignore_folder/ds_data
dotenv set DS_LOCAL_DATA_PATH "$(pwd)/git_ignore_folder/ds_data"
dotenv set DS_CODER_ON_WHOLE_PIPELINE True
dotenv set DS_IF_USING_MLE_DATA True
dotenv set DS_SAMPLE_DATA_BY_LLM True
dotenv set DS_SCEN rdagent.scenarios.data_science.scen.KaggleScen

# 2. run the application
rdagent data_science --competition tabular-playground-series-dec-2021
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ–¥ï¸ Monitor the Application Results&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;You can run the following command for our demo program to see the run logs.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;rdagent ui --port 19899 --log-dir &amp;lt;your log folder like "log/"&amp;gt; --data-science
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;About the &lt;code&gt;data_science&lt;/code&gt; parameter: If you want to see the logs of the data science scenario, set the &lt;code&gt;data_science&lt;/code&gt; parameter to &lt;code&gt;True&lt;/code&gt;; otherwise set it to &lt;code&gt;False&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Although port 19899 is not commonly used, but before you run this demo, you need to check if port 19899 is occupied. If it is, please change it to another port that is not occupied.&lt;/p&gt; &lt;p&gt;You can check if a port is occupied by running the following command.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;rdagent health_check --no-check-env --no-check-docker
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;ğŸ­ Scenarios&lt;/h1&gt; 
&lt;p&gt;We have applied R&amp;amp;D-Agent to multiple valuable data-driven industrial scenarios.&lt;/p&gt; 
&lt;h2&gt;ğŸ¯ Goal: Agent for Data-driven R&amp;amp;D&lt;/h2&gt; 
&lt;p&gt;In this project, we are aiming to build an Agent to automate Data-Driven R&amp;amp;D that can&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“„ Read real-world material (reports, papers, etc.) and &lt;strong&gt;extract&lt;/strong&gt; key formulas, descriptions of interested &lt;strong&gt;features&lt;/strong&gt; and &lt;strong&gt;models&lt;/strong&gt;, which are the key components of data-driven R&amp;amp;D .&lt;/li&gt; 
 &lt;li&gt;ğŸ› ï¸ &lt;strong&gt;Implement&lt;/strong&gt; the extracted formulas (e.g., features, factors, and models) in runnable codes. 
  &lt;ul&gt; 
   &lt;li&gt;Due to the limited ability of LLM in implementing at once, build an evolving process for the agent to improve performance by learning from feedback and knowledge.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;ğŸ’¡ Propose &lt;strong&gt;new ideas&lt;/strong&gt; based on current knowledge and observations.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- ![Data-Centric R&amp;D Overview](docs/_static/overview.png) --&gt; 
&lt;h2&gt;ğŸ“ˆ Scenarios/Demos&lt;/h2&gt; 
&lt;p&gt;In the two key areas of data-driven scenarios, model implementation and data building, our system aims to serve two main roles: ğŸ¦¾Copilot and ğŸ¤–Agent.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;The ğŸ¦¾Copilot follows human instructions to automate repetitive tasks.&lt;/li&gt; 
 &lt;li&gt;The ğŸ¤–Agent, being more autonomous, actively proposes ideas for better results in the future.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The supported scenarios are listed below:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Scenario/Target&lt;/th&gt; 
   &lt;th&gt;Model Implementation&lt;/th&gt; 
   &lt;th&gt;Data Building&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸ’¹ Finance&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;ğŸ¤– &lt;a href="https://rdagent.azurewebsites.net/model_loop"&gt;Iteratively Proposing Ideas &amp;amp; Evolving&lt;/a&gt;&lt;a href="https://www.youtube.com/watch?v=dm0dWL49Bc0&amp;amp;t=104s"&gt;â–¶ï¸YouTube&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ğŸ¤– &lt;a href="https://rdagent.azurewebsites.net/factor_loop"&gt;Iteratively Proposing Ideas &amp;amp; Evolving&lt;/a&gt; &lt;a href="https://www.youtube.com/watch?v=X4DK2QZKaKY&amp;amp;t=6s"&gt;â–¶ï¸YouTube&lt;/a&gt; &lt;br /&gt; ğŸ¦¾ &lt;a href="https://rdagent.azurewebsites.net/report_factor"&gt;Auto reports reading &amp;amp; implementation&lt;/a&gt;&lt;a href="https://www.youtube.com/watch?v=ECLTXVcSx-c"&gt;â–¶ï¸YouTube&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸ©º Medical&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;ğŸ¤– &lt;a href="https://rdagent.azurewebsites.net/dmm"&gt;Iteratively Proposing Ideas &amp;amp; Evolving&lt;/a&gt;&lt;a href="https://www.youtube.com/watch?v=VIaSTZuoZg4"&gt;â–¶ï¸YouTube&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸ­ General&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;ğŸ¦¾ &lt;a href="https://rdagent.azurewebsites.net/report_model"&gt;Auto paper reading &amp;amp; implementation&lt;/a&gt;&lt;a href="https://www.youtube.com/watch?v=BiA2SfdKQ7o"&gt;â–¶ï¸YouTube&lt;/a&gt; &lt;br /&gt; ğŸ¤– Auto Kaggle Model Tuning&lt;/td&gt; 
   &lt;td&gt;ğŸ¤–Auto Kaggle feature Engineering&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://rdagent.readthedocs.io/en/latest/scens/data_science.html#roadmap"&gt;RoadMap&lt;/a&gt;&lt;/strong&gt;: Currently, we are working hard to add new features to the Kaggle scenario.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Different scenarios vary in entrance and configuration. Please check the detailed setup tutorial in the scenarios documents.&lt;/p&gt; 
&lt;p&gt;Here is a gallery of &lt;a href="https://github.com/SunsetWolf/rdagent_resource/releases/download/demo_traces/demo_traces.zip"&gt;successful explorations&lt;/a&gt; (5 traces showed in &lt;strong&gt;&lt;a href="https://rdagent.azurewebsites.net/"&gt;ğŸ–¥ï¸ Live Demo&lt;/a&gt;&lt;/strong&gt;). You can download and view the execution trace using &lt;a href="https://github.com/microsoft/RD-Agent?tab=readme-ov-file#%EF%B8%8F-monitor-the-application-results"&gt;this command&lt;/a&gt; from the documentation.&lt;/p&gt; 
&lt;p&gt;Please refer to &lt;strong&gt;&lt;a href="https://rdagent.readthedocs.io/en/latest/scens/catalog.html"&gt;ğŸ“–readthedocs_scen&lt;/a&gt;&lt;/strong&gt; for more details of the scenarios.&lt;/p&gt; 
&lt;h1&gt;âš™ï¸ Framework&lt;/h1&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/microsoft/RD-Agent/main/docs/_static/Framework-RDAgent.png" alt="Framework-RDAgent" width="85%" /&gt; 
&lt;/div&gt; 
&lt;p&gt;Automating the R&amp;amp;D process in data science is a highly valuable yet underexplored area in industry. We propose a framework to push the boundaries of this important research field.&lt;/p&gt; 
&lt;p&gt;The research questions within this framework can be divided into three main categories:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Research Area&lt;/th&gt; 
   &lt;th&gt;Paper/Work List&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Benchmark the R&amp;amp;D abilities&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/RD-Agent/main/#benchmark"&gt;Benchmark&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Idea proposal:&lt;/strong&gt; Explore new ideas or refine existing ones&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/RD-Agent/main/#research"&gt;Research&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Ability to realize ideas:&lt;/strong&gt; Implement and execute ideas&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/RD-Agent/main/#development"&gt;Development&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;We believe that the key to delivering high-quality solutions lies in the ability to evolve R&amp;amp;D capabilities. Agents should learn like human experts, continuously improving their R&amp;amp;D skills.&lt;/p&gt; 
&lt;p&gt;More documents can be found in the &lt;strong&gt;&lt;a href="https://rdagent.readthedocs.io/"&gt;ğŸ“– readthedocs&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; 
&lt;h1&gt;ğŸ“ƒ Paper/Work list&lt;/h1&gt; 
&lt;h2&gt;Overall Technical Report&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2505.14738"&gt;R&amp;amp;D-Agent: Automating Data-Driven AI Solution Building Through LLM-Powered Automated Research, Development, and Evolution&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-BibTeX"&gt;@misc{yang2024rdagent,
    title={R\&amp;amp;D-Agent: Automating Data-Driven AI Solution Building Through LLM-Powered Automated Research, Development, and Evolution},
    author={Xu Yang and Xiao Yang and Shikai Fang and Bowen Xian and Yuante Li and Jian Wang and Minrui Xu and Haoran Pan and Xinpeng Hong and Weiqing Liu and Yelong Shen and Weizhu Chen and Jiang Bian},
    year={2025},
    eprint={2505.14738},
    archivePrefix={arXiv},
    primaryClass={cs.AI},
    url={https://arxiv.org/abs/2505.14738}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/28b0488d-a546-4fef-8dc5-563ed64a9b4d" alt="image" /&gt;&lt;/p&gt; 
&lt;h2&gt;ğŸ“Š Benchmark&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2404.11276"&gt;Towards Data-Centric Automatic R&amp;amp;D&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-BibTeX"&gt;@misc{chen2024datacentric,
    title={Towards Data-Centric Automatic R&amp;amp;D},
    author={Haotian Chen and Xinjie Shen and Zeqi Ye and Wenjun Feng and Haoxue Wang and Xiao Yang and Xu Yang and Weiqing Liu and Jiang Bian},
    year={2024},
    eprint={2404.11276},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/494f55d3-de9e-4e73-ba3d-a787e8f9e841" alt="image" /&gt;&lt;/p&gt; 
&lt;h2&gt;ğŸ” Research&lt;/h2&gt; 
&lt;p&gt;In a data mining expert's daily research and development process, they propose a hypothesis (e.g., a model structure like RNN can capture patterns in time-series data), design experiments (e.g., finance data contains time-series and we can verify the hypothesis in this scenario), implement the experiment as code (e.g., Pytorch model structure), and then execute the code to get feedback (e.g., metrics, loss curve, etc.). The experts learn from the feedback and improve in the next iteration.&lt;/p&gt; 
&lt;p&gt;Based on the principles above, we have established a basic method framework that continuously proposes hypotheses, verifies them, and gets feedback from the real-world practice. This is the first scientific research automation framework that supports linking with real-world verification.&lt;/p&gt; 
&lt;p&gt;For more detail, please refer to our &lt;strong&gt;&lt;a href="https://rdagent.azurewebsites.net"&gt;ğŸ–¥ï¸ Live Demo page&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; 
&lt;h2&gt;ğŸ› ï¸ Development&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2407.18690"&gt;Collaborative Evolving Strategy for Automatic Data-Centric Development&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-BibTeX"&gt;@misc{yang2024collaborative,
    title={Collaborative Evolving Strategy for Automatic Data-Centric Development},
    author={Xu Yang and Haotian Chen and Wenjun Feng and Haoxue Wang and Zeqi Ye and Xinjie Shen and Xiao Yang and Shizhao Sun and Weiqing Liu and Jiang Bian},
    year={2024},
    eprint={2407.18690},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/75d9769b-0edd-4caf-9d45-57d1e577054b" alt="image" /&gt;&lt;/p&gt; 
&lt;h2&gt;Deep Application in Diverse Scenarios&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2505.15155"&gt;R&amp;amp;D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-BibTeX"&gt;@misc{li2025rdagentquant,
    title={R\&amp;amp;D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization},
    author={Yuante Li and Xu Yang and Xiao Yang and Minrui Xu and Xisen Wang and Weiqing Liu and Jiang Bian},
    year={2025},
    eprint={2505.15155},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/3186f67a-c2f8-4b6b-8bb9-a9b959c13866" alt="image" /&gt;&lt;/p&gt; 
&lt;h1&gt;ğŸ¤ Contributing&lt;/h1&gt; 
&lt;p&gt;We welcome contributions and suggestions to improve R&amp;amp;D-Agent. Please refer to the &lt;a href="https://raw.githubusercontent.com/microsoft/RD-Agent/main/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt; for more details on how to contribute.&lt;/p&gt; 
&lt;p&gt;Before submitting a pull request, ensure that your code passes the automatic CI checks.&lt;/p&gt; 
&lt;h2&gt;ğŸ“ Guidelines&lt;/h2&gt; 
&lt;p&gt;This project welcomes contributions and suggestions. Contributing to this project is straightforward and rewarding. Whether it's solving an issue, addressing a bug, enhancing documentation, or even correcting a typo, every contribution is valuable and helps improve R&amp;amp;D-Agent.&lt;/p&gt; 
&lt;p&gt;To get started, you can explore the issues list, or search for &lt;code&gt;TODO:&lt;/code&gt; comments in the codebase by running the command &lt;code&gt;grep -r "TODO:"&lt;/code&gt;.&lt;/p&gt; 
&lt;img src="https://img.shields.io/github/contributors-anon/microsoft/RD-Agent" /&gt; 
&lt;a href="https://github.com/microsoft/RD-Agent/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=microsoft/RD-Agent&amp;amp;max=100&amp;amp;columns=15" /&gt; &lt;/a&gt; 
&lt;p&gt;Before we released R&amp;amp;D-Agent as an open-source project on GitHub, it was an internal project within our group. Unfortunately, the internal commit history was not preserved when we removed some confidential code. As a result, some contributions from our group members, including Haotian Chen, Wenjun Feng, Haoxue Wang, Zeqi Ye, Xinjie Shen, and Jinhui Li, were not included in the public commits.&lt;/p&gt; 
&lt;h1&gt;âš–ï¸ Legal disclaimer&lt;/h1&gt; 
&lt;p style="line-height: 1; font-style: italic;"&gt;The RD-agent is provided â€œas isâ€, without warranty of any kind, express or implied, including but not limited to the warranties of merchantability, fitness for a particular purpose and noninfringement. The RD-agent is aimed to facilitate research and development process in the financial industry and not ready-to-use for any financial investment or advice. Users shall independently assess and test the risks of the RD-agent in a specific use scenario, ensure the responsible use of AI technology, including but not limited to developing and integrating risk mitigation measures, and comply with all applicable laws and regulations in all applicable jurisdictions. The RD-agent does not provide financial opinions or reflect the opinions of Microsoft, nor is it designed to replace the role of qualified financial professionals in formulating, assessing, and approving finance products. The inputs and outputs of the RD-agent belong to the users and users shall assume all liability under any theory of liability, whether in contract, torts, regulatory, negligence, products liability, or otherwise, associated with use of the RD-agent and any inputs and outputs thereof.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>THUDM/slime</title>
      <link>https://github.com/THUDM/slime</link>
      <description>&lt;p&gt;slime is an LLM post-training framework for RL Scaling.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;slime&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/THUDM/slime/main/README_zh.md"&gt;ä¸­æ–‡ç‰ˆ&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://thudm.github.io/slime/"&gt;&lt;img src="https://img.shields.io/badge/docs-latest-brightgreen.svg?style=flat" alt="Documentation" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/THUDM/slime"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;slime&lt;/strong&gt; is an LLM post-training framework for RL scaling, providing two core capabilities:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;High-Performance Training&lt;/strong&gt;: Supports efficient training in various modes by connecting Megatron with SGLang;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible Data Generation&lt;/strong&gt;: Enables arbitrary training data generation workflows through custom data generation interfaces and server-based engines.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;slime is the RL-framework behind &lt;a href="https://z.ai/blog/glm-4.5"&gt;GLM-4.5&lt;/a&gt; and &lt;a href="https://z.ai/blog/glm-4.6"&gt;GLM-4.6&lt;/a&gt; and apart from models from Z.ai, we also supports the following models:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Qwen3 series (Qwen3Next, Qwen3MoE, Qwen3), Qwen2.5 series;&lt;/li&gt; 
 &lt;li&gt;DeepSeek V3 series (DeepSeek V3, V3.1, DeepSeek R1);&lt;/li&gt; 
 &lt;li&gt;Llama 3.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Blogs&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Our vision: &lt;a href="https://lmsys.org/blog/2025-07-09-slime/"&gt;slime: An SGLang-Native Post-Training Framework for RL Scaling&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Our ideas on agentic training: &lt;a href="https://www.notion.so/Agent-Oriented-Design-An-Asynchronous-and-Decoupled-Framework-for-Agentic-RL-2278e692d081802cbdd5d37cef76a547"&gt;Agent-Oriented Design: An Asynchronous and Decoupled Framework for Agentic RL&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;v0.1.0 release note: &lt;a href="https://thudm.github.io/slime/blogs/release_v0.1.0.html"&gt;v0.1.0: Redefining High-Performance RL Training Frameworks&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/THUDM/slime/main/#architecture-overview"&gt;Architecture Overview&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/THUDM/slime/main/#quick-start"&gt;Quick Start&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/THUDM/slime/main/#projects-built-with-slime"&gt;Projects Built with slime&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/THUDM/slime/main/#arguments-walkthrough"&gt;Arguments Walkthrough&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/THUDM/slime/main/#developer-guide"&gt;Developer Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/THUDM/slime/main/#faq--acknowledgements"&gt;FAQ &amp;amp; Acknowledgements&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Architecture Overview&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/THUDM/slime/main/imgs/arch.png" alt="arch" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Module Descriptions&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;training (Megatron)&lt;/strong&gt;: Responsible for the main training process, reads data from the Data Buffer, and synchronizes parameters to the rollout module after training.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;rollout (SGLang + router)&lt;/strong&gt;: Generates new data (including rewards/verifier outputs) and stores it in the Data Buffer.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;data buffer&lt;/strong&gt;: A bridge module that manages prompt initialization, custom data, and rollout generation methods.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;For a comprehensive quick start guide covering environment setup, data preparation, training startup, and key code analysis, please refer to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/THUDM/slime/main/docs/en/get_started/quick_start.md"&gt;Quick Start Guide&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We also provide examples for some use cases not covered in the quick start guide; please check &lt;a href="https://raw.githubusercontent.com/THUDM/slime/main/examples/"&gt;examples&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Projects Built upon slime&lt;/h2&gt; 
&lt;p&gt;slime has powered several novel research projects and production systems. Here are some notable examples:&lt;/p&gt; 
&lt;h3&gt;âš¡ TritonForge: Agentic RL Training Framework for Kernel Generation&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/RLsys-Foundation/TritonForge"&gt;&lt;strong&gt;TritonForge&lt;/strong&gt;&lt;/a&gt; leverages slime's SFT &amp;amp; RL capabilities to train LLMs that automatically generate optimized GPU kernels. By using a two-stage training approachâ€”supervised fine-tuning followed by reinforcement learning with multi-turn compilation feedbackâ€”TritonForge achieves remarkable results in converting PyTorch operations into high-performance Triton kernels.&lt;/p&gt; 
&lt;h3&gt;ğŸš€ APRIL: Accelerating RL Training with Active Partial Rollouts&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/RLsys-Foundation/APRIL"&gt;&lt;strong&gt;APRIL&lt;/strong&gt;&lt;/a&gt; introduces a system-level optimization that seamlessly integrates with slime to accelerate the rollout generation phase in RL training. By intelligently over-provisioning requests and actively managing partial completions, APRIL addresses the long-tail generation bottleneck that typically consumes over 90% of RL training time.&lt;/p&gt; 
&lt;p&gt;These projects showcase slime's versatilityâ€”from training code-generation models to optimizing RL training systemsâ€”making it a powerful foundation for both research and production deployments.&lt;/p&gt; 
&lt;h2&gt;Arguments Walkthrough&lt;/h2&gt; 
&lt;p&gt;Arguments in slime are divided into three categories:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Megatron arguments&lt;/strong&gt;: slime reads all arguments set in Megatron via &lt;code&gt;PYTHONPATH&lt;/code&gt;. You can configure Megatron by passing arguments like &lt;code&gt;--tensor-model-parallel-size 2&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;SGLang arguments&lt;/strong&gt;: All arguments for the installed SGLang are supported. These arguments must be prefixed with &lt;code&gt;--sglang-&lt;/code&gt;. For example, &lt;code&gt;--mem-fraction-static&lt;/code&gt; should be passed as &lt;code&gt;--sglang-mem-fraction-static&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;slime-specific arguments&lt;/strong&gt;: Please refer to: &lt;a href="https://raw.githubusercontent.com/THUDM/slime/main/slime/utils/arguments.py"&gt;slime/utils/arguments.py&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;For complete usage instructions, please refer to the &lt;a href="https://raw.githubusercontent.com/THUDM/slime/main/docs/en/get_started/usage.md"&gt;Usage Documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Developer Guide&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Contributions are welcome!&lt;/strong&gt; If you have suggestions for new features, performance tuning, or feedback on user experience, feel free to submit an Issue or PR ğŸ˜Š&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Use &lt;a href="https://pre-commit.com/"&gt;pre-commit&lt;/a&gt; to ensure code style consistency for your commits:&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;apt install pre-commit -y
pre-commit install

# run pre-commit to ensure code style consistency
pre-commit run --all-files --show-diff-on-failure --color=always
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;For debugging tips, please refer to the &lt;a href="https://raw.githubusercontent.com/THUDM/slime/main/docs/en/developer_guide/debug.md"&gt;Debugging Guide&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;FAQ &amp;amp; Acknowledgements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;For frequently asked questions, please see the &lt;a href="https://raw.githubusercontent.com/THUDM/slime/main/docs/en/get_started/qa.md"&gt;Q&amp;amp;A&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Special thanks to the following projects &amp;amp; communities: SGLang, Megatronâ€‘LM, mbridge, OpenRLHF, veRL, Pai-Megatron-Patch and others.&lt;/li&gt; 
 &lt;li&gt;To quote slime, please use:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bibtext"&gt;@misc{slime_github,
  author       = {Zilin Zhu and Chengxing Xie and Xin Lv and slime Contributors},
  title        = {slime: An LLM post-training framework for RL Scaling},
  year         = {2025},
  howpublished = {\url{https://github.com/THUDM/slime}},
  note         = {GitHub repository. Corresponding author: Xin Lv},
  urldate      = {2025-06-19}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>jiji262/douyin-downloader</title>
      <link>https://github.com/jiji262/douyin-downloader</link>
      <description>&lt;p&gt;æŠ–éŸ³æ‰¹é‡ä¸‹è½½å·¥å…·ï¼Œå»æ°´å°ï¼Œæ”¯æŒè§†é¢‘ã€å›¾é›†ã€åˆé›†ã€éŸ³ä¹(åŸå£°)ã€‚å…è´¹ï¼å…è´¹ï¼å…è´¹ï¼&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;æŠ–éŸ³ä¸‹è½½å™¨ - æ— æ°´å°æ‰¹é‡ä¸‹è½½å·¥å…·&lt;/h1&gt; 
&lt;p&gt;&lt;img src="https://socialify.git.ci/jiji262/douyin-downloader/image?custom_description=%E6%8A%96%E9%9F%B3%E6%89%B9%E9%87%8F%E4%B8%8B%E8%BD%BD%E5%B7%A5%E5%85%B7%EF%BC%8C%E5%8E%BB%E6%B0%B4%E5%8D%B0%EF%BC%8C%E6%94%AF%E6%8C%81%E8%A7%86%E9%A2%91%E3%80%81%E5%9B%BE%E9%9B%86%E3%80%81%E5%90%88%E9%9B%86%E3%80%81%E9%9F%B3%E4%B9%90%28%E5%8E%9F%E5%A3%B0%29%E3%80%82%0A%E5%85%8D%E8%B4%B9%EF%BC%81%E5%85%8D%E8%B4%B9%EF%BC%81%E5%85%8D%E8%B4%B9%EF%BC%81&amp;amp;description=1&amp;amp;font=Jost&amp;amp;forks=1&amp;amp;logo=https%3A%2F%2Fraw.githubusercontent.com%2Fjiji262%2Fdouyin-downloader%2Frefs%2Fheads%2Fmain%2Fimg%2Flogo.png&amp;amp;name=1&amp;amp;owner=1&amp;amp;pattern=Circuit+Board&amp;amp;pulls=1&amp;amp;stargazers=1&amp;amp;theme=Light" alt="douyin-downloader" /&gt;&lt;/p&gt; 
&lt;p&gt;ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„æŠ–éŸ³å†…å®¹æ‰¹é‡ä¸‹è½½å·¥å…·ï¼Œæ”¯æŒè§†é¢‘ã€å›¾é›†ã€éŸ³ä¹ã€ç›´æ’­ç­‰å¤šç§å†…å®¹ç±»å‹çš„ä¸‹è½½ã€‚æä¾›ä¸¤ä¸ªç‰ˆæœ¬ï¼šV1.0ï¼ˆç¨³å®šç‰ˆï¼‰å’Œ V2.0ï¼ˆå¢å¼ºç‰ˆï¼‰ã€‚&lt;/p&gt; 
&lt;h2&gt;ğŸ“‹ ç›®å½•&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jiji262/douyin-downloader/main/#-%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B"&gt;å¿«é€Ÿå¼€å§‹&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jiji262/douyin-downloader/main/#-%E7%89%88%E6%9C%AC%E8%AF%B4%E6%98%8E"&gt;ç‰ˆæœ¬è¯´æ˜&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jiji262/douyin-downloader/main/#-v10-%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97"&gt;V1.0 ä½¿ç”¨æŒ‡å—&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jiji262/douyin-downloader/main/#-v20-%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97"&gt;V2.0 ä½¿ç”¨æŒ‡å—&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jiji262/douyin-downloader/main/#-cookie-%E9%85%8D%E7%BD%AE%E5%B7%A5%E5%85%B7"&gt;Cookie é…ç½®å·¥å…·&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jiji262/douyin-downloader/main/#-%E6%94%AF%E6%8C%81%E7%9A%84%E9%93%BE%E6%8E%A5%E7%B1%BB%E5%9E%8B"&gt;æ”¯æŒçš„é“¾æ¥ç±»å‹&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jiji262/douyin-downloader/main/#-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98"&gt;å¸¸è§é—®é¢˜&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jiji262/douyin-downloader/main/#-%E6%9B%B4%E6%96%B0%E6%97%A5%E5%BF%97"&gt;æ›´æ–°æ—¥å¿—&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;âš¡ å¿«é€Ÿå¼€å§‹&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/jiji262/douyin-downloader/main/img/fuye.jpg" alt="qun" /&gt;&lt;/p&gt; 
&lt;h3&gt;ç¯å¢ƒè¦æ±‚&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Python 3.9+&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;æ“ä½œç³»ç»Ÿ&lt;/strong&gt;ï¼šWindowsã€macOSã€Linux&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;å®‰è£…æ­¥éª¤&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;å…‹éš†é¡¹ç›®&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/jiji262/douyin-downloader.git
cd douyin-downloader
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;&lt;strong&gt;å®‰è£…ä¾èµ–&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;&lt;strong&gt;é…ç½® Cookie&lt;/strong&gt;ï¼ˆé¦–æ¬¡ä½¿ç”¨éœ€è¦ï¼‰&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# æ–¹å¼1ï¼šè‡ªåŠ¨è·å–ï¼ˆæ¨èï¼‰
python cookie_extractor.py

# æ–¹å¼2ï¼šæ‰‹åŠ¨è·å–
python get_cookies_manual.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ğŸ“¦ ç‰ˆæœ¬è¯´æ˜&lt;/h2&gt; 
&lt;h3&gt;V1.0 (DouYinCommand.py) - ç¨³å®šç‰ˆ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;âœ… &lt;strong&gt;ç»è¿‡éªŒè¯&lt;/strong&gt;ï¼šç¨³å®šå¯é ï¼Œç»è¿‡å¤§é‡æµ‹è¯•&lt;/li&gt; 
 &lt;li&gt;âœ… &lt;strong&gt;ç®€å•æ˜“ç”¨&lt;/strong&gt;ï¼šé…ç½®æ–‡ä»¶é©±åŠ¨ï¼Œä½¿ç”¨ç®€å•&lt;/li&gt; 
 &lt;li&gt;âœ… &lt;strong&gt;åŠŸèƒ½å®Œæ•´&lt;/strong&gt;ï¼šæ”¯æŒæ‰€æœ‰å†…å®¹ç±»å‹ä¸‹è½½&lt;/li&gt; 
 &lt;li&gt;âœ… &lt;strong&gt;å•ä¸ªè§†é¢‘ä¸‹è½½&lt;/strong&gt;ï¼šå®Œå…¨æ­£å¸¸å·¥ä½œ&lt;/li&gt; 
 &lt;li&gt;âš ï¸ &lt;strong&gt;éœ€è¦æ‰‹åŠ¨é…ç½®&lt;/strong&gt;ï¼šéœ€è¦æ‰‹åŠ¨è·å–å’Œé…ç½® Cookie&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;V2.0 (downloader.py) - å¢å¼ºç‰ˆ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸš€ &lt;strong&gt;è‡ªåŠ¨ Cookie ç®¡ç†&lt;/strong&gt;ï¼šæ”¯æŒè‡ªåŠ¨è·å–å’Œåˆ·æ–° Cookie&lt;/li&gt; 
 &lt;li&gt;ğŸš€ &lt;strong&gt;ç»Ÿä¸€å…¥å£&lt;/strong&gt;ï¼šæ•´åˆæ‰€æœ‰åŠŸèƒ½åˆ°å•ä¸€è„šæœ¬&lt;/li&gt; 
 &lt;li&gt;ğŸš€ &lt;strong&gt;å¼‚æ­¥æ¶æ„&lt;/strong&gt;ï¼šæ€§èƒ½æ›´ä¼˜ï¼Œæ”¯æŒå¹¶å‘ä¸‹è½½&lt;/li&gt; 
 &lt;li&gt;ğŸš€ &lt;strong&gt;æ™ºèƒ½é‡è¯•&lt;/strong&gt;ï¼šè‡ªåŠ¨é‡è¯•å’Œé”™è¯¯æ¢å¤&lt;/li&gt; 
 &lt;li&gt;ğŸš€ &lt;strong&gt;å¢é‡ä¸‹è½½&lt;/strong&gt;ï¼šæ”¯æŒå¢é‡æ›´æ–°ï¼Œé¿å…é‡å¤ä¸‹è½½&lt;/li&gt; 
 &lt;li&gt;âš ï¸ &lt;strong&gt;å•ä¸ªè§†é¢‘ä¸‹è½½&lt;/strong&gt;ï¼šç›®å‰ API è¿”å›ç©ºå“åº”ï¼ˆå·²çŸ¥é—®é¢˜ï¼‰&lt;/li&gt; 
 &lt;li&gt;âœ… &lt;strong&gt;ç”¨æˆ·ä¸»é¡µä¸‹è½½&lt;/strong&gt;ï¼šå®Œå…¨æ­£å¸¸å·¥ä½œ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ¯ V1.0 ä½¿ç”¨æŒ‡å—&lt;/h2&gt; 
&lt;h3&gt;é…ç½®æ–‡ä»¶è®¾ç½®&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;ç¼–è¾‘é…ç½®æ–‡ä»¶&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cp config.example.yml config.yml
# ç¼–è¾‘ config.yml æ–‡ä»¶
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;&lt;strong&gt;é…ç½®ç¤ºä¾‹&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;# ä¸‹è½½é“¾æ¥
link:
  - https://v.douyin.com/xxxxx/                    # å•ä¸ªè§†é¢‘
  - https://www.douyin.com/user/xxxxx              # ç”¨æˆ·ä¸»é¡µ
  - https://www.douyin.com/collection/xxxxx        # åˆé›†

# ä¿å­˜è·¯å¾„
path: ./Downloaded/

# Cookieé…ç½®ï¼ˆå¿…å¡«ï¼‰
cookies:
  msToken: YOUR_MS_TOKEN_HERE
  ttwid: YOUR_TTWID_HERE
  odin_tt: YOUR_ODIN_TT_HERE
  passport_csrf_token: YOUR_PASSPORT_CSRF_TOKEN_HERE
  sid_guard: YOUR_SID_GUARD_HERE

# ä¸‹è½½é€‰é¡¹
music: True    # ä¸‹è½½éŸ³ä¹
cover: True    # ä¸‹è½½å°é¢
avatar: True   # ä¸‹è½½å¤´åƒ
json: True     # ä¿å­˜JSONæ•°æ®

# ä¸‹è½½æ¨¡å¼
mode:
  - post       # ä¸‹è½½å‘å¸ƒçš„ä½œå“
  # - like     # ä¸‹è½½å–œæ¬¢çš„ä½œå“
  # - mix      # ä¸‹è½½åˆé›†

# ä¸‹è½½æ•°é‡ï¼ˆ0è¡¨ç¤ºå…¨éƒ¨ï¼‰
number:
  post: 0      # å‘å¸ƒä½œå“æ•°é‡
  like: 0      # å–œæ¬¢ä½œå“æ•°é‡
  allmix: 0    # åˆé›†æ•°é‡
  mix: 0       # å•ä¸ªåˆé›†å†…ä½œå“æ•°é‡

# å…¶ä»–è®¾ç½®
thread: 5      # ä¸‹è½½çº¿ç¨‹æ•°
database: True # ä½¿ç”¨æ•°æ®åº“è®°å½•
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;è¿è¡Œç¨‹åº&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# ä½¿ç”¨é…ç½®æ–‡ä»¶è¿è¡Œ
python DouYinCommand.py

# æˆ–è€…ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°
python DouYinCommand.py --cmd False
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;ä½¿ç”¨ç¤ºä¾‹&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# ä¸‹è½½å•ä¸ªè§†é¢‘
# åœ¨ config.yml ä¸­è®¾ç½® link ä¸ºå•ä¸ªè§†é¢‘é“¾æ¥
python DouYinCommand.py

# ä¸‹è½½ç”¨æˆ·ä¸»é¡µ
# åœ¨ config.yml ä¸­è®¾ç½® link ä¸ºç”¨æˆ·ä¸»é¡µé“¾æ¥
python DouYinCommand.py

# ä¸‹è½½åˆé›†
# åœ¨ config.yml ä¸­è®¾ç½® link ä¸ºåˆé›†é“¾æ¥
python DouYinCommand.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ğŸš€ V2.0 ä½¿ç”¨æŒ‡å—&lt;/h2&gt; 
&lt;h3&gt;å‘½ä»¤è¡Œä½¿ç”¨&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# ä¸‹è½½å•ä¸ªè§†é¢‘ï¼ˆéœ€è¦å…ˆé…ç½® Cookieï¼‰
python downloader.py -u "https://v.douyin.com/xxxxx/"

# ä¸‹è½½ç”¨æˆ·ä¸»é¡µï¼ˆæ¨èï¼‰
python downloader.py -u "https://www.douyin.com/user/xxxxx"

# è‡ªåŠ¨è·å– Cookie å¹¶ä¸‹è½½
python downloader.py --auto-cookie -u "https://www.douyin.com/user/xxxxx"

# æŒ‡å®šä¿å­˜è·¯å¾„
python downloader.py -u "é“¾æ¥" --path "./my_videos/"

# ä½¿ç”¨é…ç½®æ–‡ä»¶
python downloader.py --config
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;é…ç½®æ–‡ä»¶ä½¿ç”¨&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;åˆ›å»ºé…ç½®æ–‡ä»¶&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cp config.example.yml config_simple.yml
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;&lt;strong&gt;é…ç½®ç¤ºä¾‹&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;# ä¸‹è½½é“¾æ¥
link:
  - https://www.douyin.com/user/xxxxx

# ä¿å­˜è·¯å¾„
path: ./Downloaded/

# è‡ªåŠ¨ Cookie ç®¡ç†
auto_cookie: true

# ä¸‹è½½é€‰é¡¹
music: true
cover: true
avatar: true
json: true

# ä¸‹è½½æ¨¡å¼
mode:
  - post

# ä¸‹è½½æ•°é‡
number:
  post: 10

# å¢é‡ä¸‹è½½
increase:
  post: false

# æ•°æ®åº“
database: true
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;&lt;strong&gt;è¿è¡Œç¨‹åº&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python downloader.py --config
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;å‘½ä»¤è¡Œå‚æ•°&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python downloader.py [é€‰é¡¹] [é“¾æ¥...]

é€‰é¡¹ï¼š
  -u, --url URL          ä¸‹è½½é“¾æ¥
  -p, --path PATH        ä¿å­˜è·¯å¾„
  -c, --config           ä½¿ç”¨é…ç½®æ–‡ä»¶
  --auto-cookie          è‡ªåŠ¨è·å– Cookie
  --cookies COOKIES      æ‰‹åŠ¨æŒ‡å®š Cookie
  -h, --help            æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ğŸª Cookie é…ç½®å·¥å…·&lt;/h2&gt; 
&lt;h3&gt;1. cookie_extractor.py - è‡ªåŠ¨è·å–å·¥å…·&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;åŠŸèƒ½&lt;/strong&gt;ï¼šä½¿ç”¨ Playwright è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨ï¼Œè‡ªåŠ¨è·å– Cookie&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;ä½¿ç”¨æ–¹å¼&lt;/strong&gt;ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# å®‰è£… Playwright
pip install playwright
playwright install chromium

# è¿è¡Œè‡ªåŠ¨è·å–
python cookie_extractor.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;ç‰¹ç‚¹&lt;/strong&gt;ï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;âœ… è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨&lt;/li&gt; 
 &lt;li&gt;âœ… æ”¯æŒæ‰«ç ç™»å½•&lt;/li&gt; 
 &lt;li&gt;âœ… è‡ªåŠ¨æ£€æµ‹ç™»å½•çŠ¶æ€&lt;/li&gt; 
 &lt;li&gt;âœ… è‡ªåŠ¨ä¿å­˜åˆ°é…ç½®æ–‡ä»¶&lt;/li&gt; 
 &lt;li&gt;âœ… æ”¯æŒå¤šç§ç™»å½•æ–¹å¼&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;ä½¿ç”¨æ­¥éª¤&lt;/strong&gt;ï¼š&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;è¿è¡Œ &lt;code&gt;python cookie_extractor.py&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;é€‰æ‹©æå–æ–¹å¼ï¼ˆæ¨èé€‰æ‹©1ï¼‰&lt;/li&gt; 
 &lt;li&gt;åœ¨æ‰“å¼€çš„æµè§ˆå™¨ä¸­å®Œæˆç™»å½•&lt;/li&gt; 
 &lt;li&gt;ç¨‹åºè‡ªåŠ¨æå–å¹¶ä¿å­˜ Cookie&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;2. get_cookies_manual.py - æ‰‹åŠ¨è·å–å·¥å…·&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;åŠŸèƒ½&lt;/strong&gt;ï¼šé€šè¿‡æµè§ˆå™¨å¼€å‘è€…å·¥å…·æ‰‹åŠ¨è·å– Cookie&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;ä½¿ç”¨æ–¹å¼&lt;/strong&gt;ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python get_cookies_manual.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;ç‰¹ç‚¹&lt;/strong&gt;ï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;âœ… æ— éœ€å®‰è£… Playwright&lt;/li&gt; 
 &lt;li&gt;âœ… è¯¦ç»†çš„æ“ä½œæ•™ç¨‹&lt;/li&gt; 
 &lt;li&gt;âœ… æ”¯æŒ Cookie éªŒè¯&lt;/li&gt; 
 &lt;li&gt;âœ… è‡ªåŠ¨ä¿å­˜åˆ°é…ç½®æ–‡ä»¶&lt;/li&gt; 
 &lt;li&gt;âœ… æ”¯æŒå¤‡ä»½å’Œæ¢å¤&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;ä½¿ç”¨æ­¥éª¤&lt;/strong&gt;ï¼š&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;è¿è¡Œ &lt;code&gt;python get_cookies_manual.py&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;é€‰æ‹©"è·å–æ–°çš„Cookie"&lt;/li&gt; 
 &lt;li&gt;æŒ‰ç…§æ•™ç¨‹åœ¨æµè§ˆå™¨ä¸­è·å– Cookie&lt;/li&gt; 
 &lt;li&gt;ç²˜è´´ Cookie å†…å®¹&lt;/li&gt; 
 &lt;li&gt;ç¨‹åºè‡ªåŠ¨è§£æå¹¶ä¿å­˜&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Cookie è·å–æ•™ç¨‹&lt;/h3&gt; 
&lt;h4&gt;æ–¹æ³•ä¸€ï¼šæµè§ˆå™¨å¼€å‘è€…å·¥å…·&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;æ‰“å¼€æµè§ˆå™¨ï¼Œè®¿é—® &lt;a href="https://www.douyin.com"&gt;æŠ–éŸ³ç½‘é¡µç‰ˆ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ç™»å½•ä½ çš„æŠ–éŸ³è´¦å·&lt;/li&gt; 
 &lt;li&gt;æŒ‰ &lt;code&gt;F12&lt;/code&gt; æ‰“å¼€å¼€å‘è€…å·¥å…·&lt;/li&gt; 
 &lt;li&gt;åˆ‡æ¢åˆ° &lt;code&gt;Network&lt;/code&gt; æ ‡ç­¾é¡µ&lt;/li&gt; 
 &lt;li&gt;åˆ·æ–°é¡µé¢ï¼Œæ‰¾åˆ°ä»»æ„è¯·æ±‚&lt;/li&gt; 
 &lt;li&gt;åœ¨è¯·æ±‚å¤´ä¸­æ‰¾åˆ° &lt;code&gt;Cookie&lt;/code&gt; å­—æ®µ&lt;/li&gt; 
 &lt;li&gt;å¤åˆ¶ä»¥ä¸‹å…³é”® cookie å€¼ï¼š 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;msToken&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;ttwid&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;odin_tt&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;passport_csrf_token&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;sid_guard&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;æ–¹æ³•äºŒï¼šä½¿ç”¨è‡ªåŠ¨å·¥å…·&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# æ¨èä½¿ç”¨è‡ªåŠ¨å·¥å…·
python cookie_extractor.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ğŸ“‹ æ”¯æŒçš„é“¾æ¥ç±»å‹&lt;/h2&gt; 
&lt;h3&gt;ğŸ¬ è§†é¢‘å†…å®¹&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;å•ä¸ªè§†é¢‘åˆ†äº«é“¾æ¥&lt;/strong&gt;ï¼š&lt;code&gt;https://v.douyin.com/xxxxx/&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;å•ä¸ªè§†é¢‘ç›´é“¾&lt;/strong&gt;ï¼š&lt;code&gt;https://www.douyin.com/video/xxxxx&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;å›¾é›†ä½œå“&lt;/strong&gt;ï¼š&lt;code&gt;https://www.douyin.com/note/xxxxx&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ‘¤ ç”¨æˆ·å†…å®¹&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ç”¨æˆ·ä¸»é¡µ&lt;/strong&gt;ï¼š&lt;code&gt;https://www.douyin.com/user/xxxxx&lt;/code&gt; 
  &lt;ul&gt; 
   &lt;li&gt;æ”¯æŒä¸‹è½½ç”¨æˆ·å‘å¸ƒçš„æ‰€æœ‰ä½œå“&lt;/li&gt; 
   &lt;li&gt;æ”¯æŒä¸‹è½½ç”¨æˆ·å–œæ¬¢çš„ä½œå“ï¼ˆéœ€è¦æƒé™ï¼‰&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ“š åˆé›†å†…å®¹&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ç”¨æˆ·åˆé›†&lt;/strong&gt;ï¼š&lt;code&gt;https://www.douyin.com/collection/xxxxx&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;éŸ³ä¹åˆé›†&lt;/strong&gt;ï¼š&lt;code&gt;https://www.douyin.com/music/xxxxx&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ”´ ç›´æ’­å†…å®¹&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ç›´æ’­é—´&lt;/strong&gt;ï¼š&lt;code&gt;https://live.douyin.com/xxxxx&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ”§ å¸¸è§é—®é¢˜&lt;/h2&gt; 
&lt;h3&gt;Q: ä¸ºä»€ä¹ˆå•ä¸ªè§†é¢‘ä¸‹è½½å¤±è´¥ï¼Ÿ&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;A&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;V1.0ï¼šè¯·æ£€æŸ¥ Cookie æ˜¯å¦æœ‰æ•ˆï¼Œç¡®ä¿åŒ…å«å¿…è¦çš„å­—æ®µ&lt;/li&gt; 
 &lt;li&gt;V2.0ï¼šç›®å‰å·²çŸ¥é—®é¢˜ï¼ŒAPI è¿”å›ç©ºå“åº”ï¼Œå»ºè®®ä½¿ç”¨ç”¨æˆ·ä¸»é¡µä¸‹è½½&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Q: Cookie è¿‡æœŸæ€ä¹ˆåŠï¼Ÿ&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;A&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ä½¿ç”¨ &lt;code&gt;python cookie_extractor.py&lt;/code&gt; é‡æ–°è·å–&lt;/li&gt; 
 &lt;li&gt;æˆ–ä½¿ç”¨ &lt;code&gt;python get_cookies_manual.py&lt;/code&gt; æ‰‹åŠ¨è·å–&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Q: ä¸‹è½½é€Ÿåº¦æ…¢æ€ä¹ˆåŠï¼Ÿ&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;A&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;è°ƒæ•´ &lt;code&gt;thread&lt;/code&gt; å‚æ•°å¢åŠ å¹¶å‘æ•°&lt;/li&gt; 
 &lt;li&gt;æ£€æŸ¥ç½‘ç»œè¿æ¥&lt;/li&gt; 
 &lt;li&gt;é¿å…åŒæ—¶ä¸‹è½½è¿‡å¤šå†…å®¹&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Q: å¦‚ä½•æ‰¹é‡ä¸‹è½½ï¼Ÿ&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;A&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;V1.0ï¼šåœ¨ &lt;code&gt;config.yml&lt;/code&gt; ä¸­æ·»åŠ å¤šä¸ªé“¾æ¥&lt;/li&gt; 
 &lt;li&gt;V2.0ï¼šä½¿ç”¨å‘½ä»¤è¡Œä¼ å…¥å¤šä¸ªé“¾æ¥æˆ–ä½¿ç”¨é…ç½®æ–‡ä»¶&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Q: æ”¯æŒå“ªäº›æ ¼å¼ï¼Ÿ&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;A&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;è§†é¢‘ï¼šMP4 æ ¼å¼ï¼ˆæ— æ°´å°ï¼‰&lt;/li&gt; 
 &lt;li&gt;å›¾ç‰‡ï¼šJPG æ ¼å¼&lt;/li&gt; 
 &lt;li&gt;éŸ³é¢‘ï¼šMP3 æ ¼å¼&lt;/li&gt; 
 &lt;li&gt;æ•°æ®ï¼šJSON æ ¼å¼&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ“ æ›´æ–°æ—¥å¿—&lt;/h2&gt; 
&lt;h3&gt;V2.0 (2025-08)&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;âœ… &lt;strong&gt;ç»Ÿä¸€å…¥å£&lt;/strong&gt;ï¼šæ•´åˆæ‰€æœ‰åŠŸèƒ½åˆ° &lt;code&gt;downloader.py&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;âœ… &lt;strong&gt;è‡ªåŠ¨ Cookie ç®¡ç†&lt;/strong&gt;ï¼šæ”¯æŒè‡ªåŠ¨è·å–å’Œåˆ·æ–°&lt;/li&gt; 
 &lt;li&gt;âœ… &lt;strong&gt;å¼‚æ­¥æ¶æ„&lt;/strong&gt;ï¼šæ€§èƒ½ä¼˜åŒ–ï¼Œæ”¯æŒå¹¶å‘ä¸‹è½½&lt;/li&gt; 
 &lt;li&gt;âœ… &lt;strong&gt;æ™ºèƒ½é‡è¯•&lt;/strong&gt;ï¼šè‡ªåŠ¨é‡è¯•å’Œé”™è¯¯æ¢å¤&lt;/li&gt; 
 &lt;li&gt;âœ… &lt;strong&gt;å¢é‡ä¸‹è½½&lt;/strong&gt;ï¼šæ”¯æŒå¢é‡æ›´æ–°&lt;/li&gt; 
 &lt;li&gt;âœ… &lt;strong&gt;ç”¨æˆ·ä¸»é¡µä¸‹è½½&lt;/strong&gt;ï¼šå®Œå…¨æ­£å¸¸å·¥ä½œ&lt;/li&gt; 
 &lt;li&gt;âš ï¸ &lt;strong&gt;å•ä¸ªè§†é¢‘ä¸‹è½½&lt;/strong&gt;ï¼šAPI è¿”å›ç©ºå“åº”ï¼ˆå·²çŸ¥é—®é¢˜ï¼‰&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;V1.0 (2024-12)&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;âœ… &lt;strong&gt;ç¨³å®šå¯é &lt;/strong&gt;ï¼šç»è¿‡å¤§é‡æµ‹è¯•éªŒè¯&lt;/li&gt; 
 &lt;li&gt;âœ… &lt;strong&gt;åŠŸèƒ½å®Œæ•´&lt;/strong&gt;ï¼šæ”¯æŒæ‰€æœ‰å†…å®¹ç±»å‹&lt;/li&gt; 
 &lt;li&gt;âœ… &lt;strong&gt;å•ä¸ªè§†é¢‘ä¸‹è½½&lt;/strong&gt;ï¼šå®Œå…¨æ­£å¸¸å·¥ä½œ&lt;/li&gt; 
 &lt;li&gt;âœ… &lt;strong&gt;é…ç½®æ–‡ä»¶é©±åŠ¨&lt;/strong&gt;ï¼šç®€å•æ˜“ç”¨&lt;/li&gt; 
 &lt;li&gt;âœ… &lt;strong&gt;æ•°æ®åº“æ”¯æŒ&lt;/strong&gt;ï¼šè®°å½•ä¸‹è½½å†å²&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;âš–ï¸ æ³•å¾‹å£°æ˜&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;æœ¬é¡¹ç›®ä»…ä¾›&lt;strong&gt;å­¦ä¹ äº¤æµ&lt;/strong&gt;ä½¿ç”¨&lt;/li&gt; 
 &lt;li&gt;è¯·éµå®ˆç›¸å…³æ³•å¾‹æ³•è§„å’Œå¹³å°æœåŠ¡æ¡æ¬¾&lt;/li&gt; 
 &lt;li&gt;ä¸å¾—ç”¨äºå•†ä¸šç”¨é€”æˆ–ä¾µçŠ¯ä»–äººæƒç›Š&lt;/li&gt; 
 &lt;li&gt;ä¸‹è½½å†…å®¹è¯·å°Šé‡åŸä½œè€…ç‰ˆæƒ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ¤ è´¡çŒ®æŒ‡å—&lt;/h2&gt; 
&lt;p&gt;æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼&lt;/p&gt; 
&lt;h3&gt;æŠ¥å‘Šé—®é¢˜&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ä½¿ç”¨ &lt;a href="https://github.com/jiji262/douyin-downloader/issues"&gt;Issues&lt;/a&gt; æŠ¥å‘Š bug&lt;/li&gt; 
 &lt;li&gt;è¯·æä¾›è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯å’Œå¤ç°æ­¥éª¤&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;åŠŸèƒ½å»ºè®®&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;åœ¨ Issues ä¸­æå‡ºæ–°åŠŸèƒ½å»ºè®®&lt;/li&gt; 
 &lt;li&gt;è¯¦ç»†æè¿°åŠŸèƒ½éœ€æ±‚å’Œä½¿ç”¨åœºæ™¯&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ“„ è®¸å¯è¯&lt;/h2&gt; 
&lt;p&gt;æœ¬é¡¹ç›®é‡‡ç”¨ &lt;a href="https://raw.githubusercontent.com/jiji262/douyin-downloader/main/LICENSE"&gt;MIT License&lt;/a&gt; å¼€æºè®¸å¯è¯ã€‚&lt;/p&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;strong&gt;å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™ä¸ª â­ Star æ”¯æŒä¸€ä¸‹ï¼&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/jiji262/douyin-downloader/issues"&gt;ğŸ› æŠ¥å‘Šé—®é¢˜&lt;/a&gt; â€¢ &lt;a href="https://github.com/jiji262/douyin-downloader/issues"&gt;ğŸ’¡ åŠŸèƒ½å»ºè®®&lt;/a&gt; â€¢ &lt;a href="https://github.com/jiji262/douyin-downloader/wiki"&gt;ğŸ“– æŸ¥çœ‹æ–‡æ¡£&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;Made with â¤ï¸ by &lt;a href="https://github.com/jiji262"&gt;jiji262&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>NanmiCoder/MediaCrawler</title>
      <link>https://github.com/NanmiCoder/MediaCrawler</link>
      <description>&lt;p&gt;å°çº¢ä¹¦ç¬”è®° | è¯„è®ºçˆ¬è™«ã€æŠ–éŸ³è§†é¢‘ | è¯„è®ºçˆ¬è™«ã€å¿«æ‰‹è§†é¢‘ | è¯„è®ºçˆ¬è™«ã€B ç«™è§†é¢‘ ï½œ è¯„è®ºçˆ¬è™«ã€å¾®åšå¸–å­ ï½œ è¯„è®ºçˆ¬è™«ã€ç™¾åº¦è´´å§å¸–å­ ï½œ ç™¾åº¦è´´å§è¯„è®ºå›å¤çˆ¬è™« | çŸ¥ä¹é—®ç­”æ–‡ç« ï½œè¯„è®ºçˆ¬è™«&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ğŸ”¥ MediaCrawler - è‡ªåª’ä½“å¹³å°çˆ¬è™« ğŸ•·ï¸&lt;/h1&gt; 
&lt;div align="center" markdown="1"&gt; 
 &lt;sup&gt;Special thanks to:&lt;/sup&gt; 
 &lt;br /&gt; 
 &lt;br /&gt; 
 &lt;a href="https://go.warp.dev/MediaCrawler"&gt; &lt;img alt="Warp sponsorship" width="400" src="https://github.com/warpdotdev/brand-assets/raw/main/Github/Sponsor/Warp-Github-LG-02.png?raw=true" /&gt; &lt;/a&gt; 
 &lt;h3&gt;&lt;a href="https://go.warp.dev/MediaCrawler"&gt;Warp is built for coding with multiple AI agents&lt;/a&gt;&lt;/h3&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://trendshift.io/repositories/8291" target="_blank"&gt; &lt;img src="https://trendshift.io/api/badge/repositories/8291" alt="NanmiCoder%2FMediaCrawler | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt; &lt;/a&gt; 
 &lt;p&gt;&lt;a href="https://github.com/NanmiCoder/MediaCrawler/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/NanmiCoder/MediaCrawler?style=social" alt="GitHub Stars" /&gt;&lt;/a&gt; &lt;a href="https://github.com/NanmiCoder/MediaCrawler/network/members"&gt;&lt;img src="https://img.shields.io/github/forks/NanmiCoder/MediaCrawler?style=social" alt="GitHub Forks" /&gt;&lt;/a&gt; &lt;a href="https://github.com/NanmiCoder/MediaCrawler/issues"&gt;&lt;img src="https://img.shields.io/github/issues/NanmiCoder/MediaCrawler" alt="GitHub Issues" /&gt;&lt;/a&gt; &lt;a href="https://github.com/NanmiCoder/MediaCrawler/pulls"&gt;&lt;img src="https://img.shields.io/github/issues-pr/NanmiCoder/MediaCrawler" alt="GitHub Pull Requests" /&gt;&lt;/a&gt; &lt;a href="https://github.com/NanmiCoder/MediaCrawler/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/NanmiCoder/MediaCrawler" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/NanmiCoder/MediaCrawler/main/README.md"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%87%A8%F0%9F%87%B3_%E4%B8%AD%E6%96%87-%E5%BD%93%E5%89%8D-blue" alt="ä¸­æ–‡" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/NanmiCoder/MediaCrawler/main/README_en.md"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%87%BA%F0%9F%87%B8_English-Available-green" alt="English" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/NanmiCoder/MediaCrawler/main/README_es.md"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%87%AA%F0%9F%87%B8_Espa%C3%B1ol-Available-green" alt="EspaÃ±ol" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;å…è´£å£°æ˜ï¼š&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;å¤§å®¶è¯·ä»¥å­¦ä¹ ä¸ºç›®çš„ä½¿ç”¨æœ¬ä»“åº“âš ï¸âš ï¸âš ï¸âš ï¸ï¼Œ&lt;a href="https://github.com/HiddenStrawberry/Crawler_Illegal_Cases_In_China"&gt;çˆ¬è™«è¿æ³•è¿è§„çš„æ¡ˆä»¶&lt;/a&gt; &lt;br /&gt;&lt;/p&gt; 
 &lt;p&gt;æœ¬ä»“åº“çš„æ‰€æœ‰å†…å®¹ä»…ä¾›å­¦ä¹ å’Œå‚è€ƒä¹‹ç”¨ï¼Œç¦æ­¢ç”¨äºå•†ä¸šç”¨é€”ã€‚ä»»ä½•äººæˆ–ç»„ç»‡ä¸å¾—å°†æœ¬ä»“åº“çš„å†…å®¹ç”¨äºéæ³•ç”¨é€”æˆ–ä¾µçŠ¯ä»–äººåˆæ³•æƒç›Šã€‚æœ¬ä»“åº“æ‰€æ¶‰åŠçš„çˆ¬è™«æŠ€æœ¯ä»…ç”¨äºå­¦ä¹ å’Œç ”ç©¶ï¼Œä¸å¾—ç”¨äºå¯¹å…¶ä»–å¹³å°è¿›è¡Œå¤§è§„æ¨¡çˆ¬è™«æˆ–å…¶ä»–éæ³•è¡Œä¸ºã€‚å¯¹äºå› ä½¿ç”¨æœ¬ä»“åº“å†…å®¹è€Œå¼•èµ·çš„ä»»ä½•æ³•å¾‹è´£ä»»ï¼Œæœ¬ä»“åº“ä¸æ‰¿æ‹…ä»»ä½•è´£ä»»ã€‚ä½¿ç”¨æœ¬ä»“åº“çš„å†…å®¹å³è¡¨ç¤ºæ‚¨åŒæ„æœ¬å…è´£å£°æ˜çš„æ‰€æœ‰æ¡æ¬¾å’Œæ¡ä»¶ã€‚&lt;/p&gt; 
 &lt;p&gt;ç‚¹å‡»æŸ¥çœ‹æ›´ä¸ºè¯¦ç»†çš„å…è´£å£°æ˜ã€‚&lt;a href="https://raw.githubusercontent.com/NanmiCoder/MediaCrawler/main/#disclaimer"&gt;ç‚¹å‡»è·³è½¬&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;ğŸ“– é¡¹ç›®ç®€ä»‹&lt;/h2&gt; 
&lt;p&gt;ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„&lt;strong&gt;å¤šå¹³å°è‡ªåª’ä½“æ•°æ®é‡‡é›†å·¥å…·&lt;/strong&gt;ï¼Œæ”¯æŒå°çº¢ä¹¦ã€æŠ–éŸ³ã€å¿«æ‰‹ã€Bç«™ã€å¾®åšã€è´´å§ã€çŸ¥ä¹ç­‰ä¸»æµå¹³å°çš„å…¬å¼€ä¿¡æ¯æŠ“å–ã€‚&lt;/p&gt; 
&lt;h3&gt;ğŸ”§ æŠ€æœ¯åŸç†&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;æ ¸å¿ƒæŠ€æœ¯&lt;/strong&gt;ï¼šåŸºäº &lt;a href="https://playwright.dev/"&gt;Playwright&lt;/a&gt; æµè§ˆå™¨è‡ªåŠ¨åŒ–æ¡†æ¶ç™»å½•ä¿å­˜ç™»å½•æ€&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;æ— éœ€JSé€†å‘&lt;/strong&gt;ï¼šåˆ©ç”¨ä¿ç•™ç™»å½•æ€çš„æµè§ˆå™¨ä¸Šä¸‹æ–‡ç¯å¢ƒï¼Œé€šè¿‡ JS è¡¨è¾¾å¼è·å–ç­¾åå‚æ•°&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ä¼˜åŠ¿ç‰¹ç‚¹&lt;/strong&gt;ï¼šæ— éœ€é€†å‘å¤æ‚çš„åŠ å¯†ç®—æ³•ï¼Œå¤§å¹…é™ä½æŠ€æœ¯é—¨æ§›&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;âœ¨ åŠŸèƒ½ç‰¹æ€§&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;å¹³å°&lt;/th&gt; 
   &lt;th&gt;å…³é”®è¯æœç´¢&lt;/th&gt; 
   &lt;th&gt;æŒ‡å®šå¸–å­IDçˆ¬å–&lt;/th&gt; 
   &lt;th&gt;äºŒçº§è¯„è®º&lt;/th&gt; 
   &lt;th&gt;æŒ‡å®šåˆ›ä½œè€…ä¸»é¡µ&lt;/th&gt; 
   &lt;th&gt;ç™»å½•æ€ç¼“å­˜&lt;/th&gt; 
   &lt;th&gt;IPä»£ç†æ± &lt;/th&gt; 
   &lt;th&gt;ç”Ÿæˆè¯„è®ºè¯äº‘å›¾&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;å°çº¢ä¹¦&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;æŠ–éŸ³&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;å¿«æ‰‹&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;B ç«™&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;å¾®åš&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;è´´å§&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;çŸ¥ä¹&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;ğŸš€ MediaCrawlerPro é‡ç£…å‘å¸ƒï¼&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ä¸“æ³¨äºå­¦ä¹ æˆç†Ÿé¡¹ç›®çš„æ¶æ„è®¾è®¡ï¼Œä¸ä»…ä»…æ˜¯çˆ¬è™«æŠ€æœ¯ï¼ŒPro ç‰ˆæœ¬çš„ä»£ç è®¾è®¡æ€è·¯åŒæ ·å€¼å¾—æ·±å…¥å­¦ä¹ ï¼&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;a href="https://github.com/MediaCrawlerPro"&gt;MediaCrawlerPro&lt;/a&gt; ç›¸è¾ƒäºå¼€æºç‰ˆæœ¬çš„æ ¸å¿ƒä¼˜åŠ¿ï¼š&lt;/p&gt; 
&lt;h4&gt;ğŸ¯ æ ¸å¿ƒåŠŸèƒ½å‡çº§&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;âœ… &lt;strong&gt;æ–­ç‚¹ç»­çˆ¬åŠŸèƒ½&lt;/strong&gt;ï¼ˆé‡ç‚¹ç‰¹æ€§ï¼‰&lt;/li&gt; 
 &lt;li&gt;âœ… &lt;strong&gt;å¤šè´¦å· + IPä»£ç†æ± æ”¯æŒ&lt;/strong&gt;ï¼ˆé‡ç‚¹ç‰¹æ€§ï¼‰&lt;/li&gt; 
 &lt;li&gt;âœ… &lt;strong&gt;å»é™¤ Playwright ä¾èµ–&lt;/strong&gt;ï¼Œä½¿ç”¨æ›´ç®€å•&lt;/li&gt; 
 &lt;li&gt;âœ… &lt;strong&gt;å®Œæ•´ Linux ç¯å¢ƒæ”¯æŒ&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;ğŸ—ï¸ æ¶æ„è®¾è®¡ä¼˜åŒ–&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;âœ… &lt;strong&gt;ä»£ç é‡æ„ä¼˜åŒ–&lt;/strong&gt;ï¼Œæ›´æ˜“è¯»æ˜“ç»´æŠ¤ï¼ˆè§£è€¦ JS ç­¾åé€»è¾‘ï¼‰&lt;/li&gt; 
 &lt;li&gt;âœ… &lt;strong&gt;ä¼ä¸šçº§ä»£ç è´¨é‡&lt;/strong&gt;ï¼Œé€‚åˆæ„å»ºå¤§å‹çˆ¬è™«é¡¹ç›®&lt;/li&gt; 
 &lt;li&gt;âœ… &lt;strong&gt;å®Œç¾æ¶æ„è®¾è®¡&lt;/strong&gt;ï¼Œé«˜æ‰©å±•æ€§ï¼Œæºç å­¦ä¹ ä»·å€¼æ›´å¤§&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;ğŸ é¢å¤–åŠŸèƒ½&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;âœ… &lt;strong&gt;è‡ªåª’ä½“è§†é¢‘ä¸‹è½½å™¨æ¡Œé¢ç«¯&lt;/strong&gt;ï¼ˆé€‚åˆå­¦ä¹ å…¨æ ˆå¼€å‘ï¼‰&lt;/li&gt; 
 &lt;li&gt;âœ… &lt;strong&gt;å¤šå¹³å°é¦–é¡µä¿¡æ¯æµæ¨è&lt;/strong&gt;ï¼ˆHomeFeedï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;strong&gt;åŸºäºè‡ªåª’ä½“å¹³å°çš„AI Agentæ­£åœ¨å¼€å‘ä¸­ ğŸš€ğŸš€&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;ç‚¹å‡»æŸ¥çœ‹ï¼š&lt;a href="https://github.com/MediaCrawlerPro"&gt;MediaCrawlerPro é¡¹ç›®ä¸»é¡µ&lt;/a&gt; æ›´å¤šä»‹ç»&lt;/p&gt; 
&lt;h2&gt;ğŸš€ å¿«é€Ÿå¼€å§‹&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ğŸ’¡ &lt;strong&gt;å¼€æºä¸æ˜“ï¼Œå¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·ç»™ä¸ª â­ Star æ”¯æŒä¸€ä¸‹ï¼&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;ğŸ“‹ å‰ç½®ä¾èµ–&lt;/h2&gt; 
&lt;h3&gt;ğŸš€ uv å®‰è£…ï¼ˆæ¨èï¼‰&lt;/h3&gt; 
&lt;p&gt;åœ¨è¿›è¡Œä¸‹ä¸€æ­¥æ“ä½œä¹‹å‰ï¼Œè¯·ç¡®ä¿ç”µè„‘ä¸Šå·²ç»å®‰è£…äº† uvï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;å®‰è£…åœ°å€&lt;/strong&gt;ï¼š&lt;a href="https://docs.astral.sh/uv/getting-started/installation"&gt;uv å®˜æ–¹å®‰è£…æŒ‡å—&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;éªŒè¯å®‰è£…&lt;/strong&gt;ï¼šç»ˆç«¯è¾“å…¥å‘½ä»¤ &lt;code&gt;uv --version&lt;/code&gt;ï¼Œå¦‚æœæ­£å¸¸æ˜¾ç¤ºç‰ˆæœ¬å·ï¼Œè¯æ˜å·²ç»å®‰è£…æˆåŠŸ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;æ¨èç†ç”±&lt;/strong&gt;ï¼šuv æ˜¯ç›®å‰æœ€å¼ºçš„ Python åŒ…ç®¡ç†å·¥å…·ï¼Œé€Ÿåº¦å¿«ã€ä¾èµ–è§£æå‡†ç¡®&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸŸ¢ Node.js å®‰è£…&lt;/h3&gt; 
&lt;p&gt;é¡¹ç›®ä¾èµ– Node.jsï¼Œè¯·å‰å¾€å®˜ç½‘ä¸‹è½½å®‰è£…ï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ä¸‹è½½åœ°å€&lt;/strong&gt;ï¼š&lt;a href="https://nodejs.org/en/download/"&gt;https://nodejs.org/en/download/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ç‰ˆæœ¬è¦æ±‚&lt;/strong&gt;ï¼š&amp;gt;= 16.0.0&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ“¦ Python åŒ…å®‰è£…&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# è¿›å…¥é¡¹ç›®ç›®å½•
cd MediaCrawler

# ä½¿ç”¨ uv sync å‘½ä»¤æ¥ä¿è¯ python ç‰ˆæœ¬å’Œç›¸å…³ä¾èµ–åŒ…çš„ä¸€è‡´æ€§
uv sync
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;ğŸŒ æµè§ˆå™¨é©±åŠ¨å®‰è£…&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# å®‰è£…æµè§ˆå™¨é©±åŠ¨
uv run playwright install
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;ğŸ’¡ æç¤º&lt;/strong&gt;ï¼šMediaCrawler ç›®å‰å·²ç»æ”¯æŒä½¿ç”¨ playwright è¿æ¥ä½ æœ¬åœ°çš„ Chrome æµè§ˆå™¨äº†ï¼Œä¸€äº›å› ä¸º Webdriver å¯¼è‡´çš„é—®é¢˜è¿åˆƒè€Œè§£äº†ã€‚&lt;/p&gt; 
 &lt;p&gt;ç›®å‰å¼€æ”¾äº† &lt;code&gt;xhs&lt;/code&gt; å’Œ &lt;code&gt;dy&lt;/code&gt; è¿™ä¸¤ä¸ªä½¿ç”¨ CDP çš„æ–¹å¼è¿æ¥æœ¬åœ°æµè§ˆå™¨ï¼Œå¦‚æœ‰éœ€è¦ï¼ŒæŸ¥çœ‹ &lt;code&gt;config/base_config.py&lt;/code&gt; ä¸­çš„é…ç½®é¡¹ã€‚&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;ğŸš€ è¿è¡Œçˆ¬è™«ç¨‹åº&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# é¡¹ç›®é»˜è®¤æ˜¯æ²¡æœ‰å¼€å¯è¯„è®ºçˆ¬å–æ¨¡å¼ï¼Œå¦‚éœ€è¯„è®ºè¯·åœ¨ config/base_config.py ä¸­çš„ ENABLE_GET_COMMENTS å˜é‡ä¿®æ”¹
# ä¸€äº›å…¶ä»–æ”¯æŒé¡¹ï¼Œä¹Ÿå¯ä»¥åœ¨ config/base_config.py æŸ¥çœ‹åŠŸèƒ½ï¼Œå†™çš„æœ‰ä¸­æ–‡æ³¨é‡Š

# ä»é…ç½®æ–‡ä»¶ä¸­è¯»å–å…³é”®è¯æœç´¢ç›¸å…³çš„å¸–å­å¹¶çˆ¬å–å¸–å­ä¿¡æ¯ä¸è¯„è®º
uv run main.py --platform xhs --lt qrcode --type search

# ä»é…ç½®æ–‡ä»¶ä¸­è¯»å–æŒ‡å®šçš„å¸–å­IDåˆ—è¡¨è·å–æŒ‡å®šå¸–å­çš„ä¿¡æ¯ä¸è¯„è®ºä¿¡æ¯
uv run main.py --platform xhs --lt qrcode --type detail

# æ‰“å¼€å¯¹åº”APPæ‰«äºŒç»´ç ç™»å½•

# å…¶ä»–å¹³å°çˆ¬è™«ä½¿ç”¨ç¤ºä¾‹ï¼Œæ‰§è¡Œä¸‹é¢çš„å‘½ä»¤æŸ¥çœ‹
uv run main.py --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;ğŸ”— &lt;strong&gt;ä½¿ç”¨ Python åŸç”Ÿ venv ç®¡ç†ç¯å¢ƒï¼ˆä¸æ¨èï¼‰&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h4&gt;åˆ›å»ºå¹¶æ¿€æ´» Python è™šæ‹Ÿç¯å¢ƒ&lt;/h4&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;å¦‚æœæ˜¯çˆ¬å–æŠ–éŸ³å’ŒçŸ¥ä¹ï¼Œéœ€è¦æå‰å®‰è£… nodejs ç¯å¢ƒï¼Œç‰ˆæœ¬å¤§äºç­‰äºï¼š&lt;code&gt;16&lt;/code&gt; å³å¯&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;# è¿›å…¥é¡¹ç›®æ ¹ç›®å½•
cd MediaCrawler

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
# æˆ‘çš„ python ç‰ˆæœ¬æ˜¯ï¼š3.9.6ï¼Œrequirements.txt ä¸­çš„åº“æ˜¯åŸºäºè¿™ä¸ªç‰ˆæœ¬çš„
# å¦‚æœæ˜¯å…¶ä»– python ç‰ˆæœ¬ï¼Œå¯èƒ½ requirements.txt ä¸­çš„åº“ä¸å…¼å®¹ï¼Œéœ€è‡ªè¡Œè§£å†³
python -m venv venv

# macOS &amp;amp; Linux æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source venv/bin/activate

# Windows æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
venv\Scripts\activate
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;å®‰è£…ä¾èµ–åº“&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;å®‰è£… playwright æµè§ˆå™¨é©±åŠ¨&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;playwright install
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;è¿è¡Œçˆ¬è™«ç¨‹åºï¼ˆåŸç”Ÿç¯å¢ƒï¼‰&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;# é¡¹ç›®é»˜è®¤æ˜¯æ²¡æœ‰å¼€å¯è¯„è®ºçˆ¬å–æ¨¡å¼ï¼Œå¦‚éœ€è¯„è®ºè¯·åœ¨ config/base_config.py ä¸­çš„ ENABLE_GET_COMMENTS å˜é‡ä¿®æ”¹
# ä¸€äº›å…¶ä»–æ”¯æŒé¡¹ï¼Œä¹Ÿå¯ä»¥åœ¨ config/base_config.py æŸ¥çœ‹åŠŸèƒ½ï¼Œå†™çš„æœ‰ä¸­æ–‡æ³¨é‡Š

# ä»é…ç½®æ–‡ä»¶ä¸­è¯»å–å…³é”®è¯æœç´¢ç›¸å…³çš„å¸–å­å¹¶çˆ¬å–å¸–å­ä¿¡æ¯ä¸è¯„è®º
python main.py --platform xhs --lt qrcode --type search

# ä»é…ç½®æ–‡ä»¶ä¸­è¯»å–æŒ‡å®šçš„å¸–å­IDåˆ—è¡¨è·å–æŒ‡å®šå¸–å­çš„ä¿¡æ¯ä¸è¯„è®ºä¿¡æ¯
python main.py --platform xhs --lt qrcode --type detail

# æ‰“å¼€å¯¹åº”APPæ‰«äºŒç»´ç ç™»å½•

# å…¶ä»–å¹³å°çˆ¬è™«ä½¿ç”¨ç¤ºä¾‹ï¼Œæ‰§è¡Œä¸‹é¢çš„å‘½ä»¤æŸ¥çœ‹
python main.py --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;ğŸ’¾ æ•°æ®ä¿å­˜&lt;/h2&gt; 
&lt;p&gt;æ”¯æŒå¤šç§æ•°æ®å­˜å‚¨æ–¹å¼ï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;CSV æ–‡ä»¶&lt;/strong&gt;ï¼šæ”¯æŒä¿å­˜åˆ° CSV ä¸­ï¼ˆ&lt;code&gt;data/&lt;/code&gt; ç›®å½•ä¸‹ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;JSON æ–‡ä»¶&lt;/strong&gt;ï¼šæ”¯æŒä¿å­˜åˆ° JSON ä¸­ï¼ˆ&lt;code&gt;data/&lt;/code&gt; ç›®å½•ä¸‹ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;æ•°æ®åº“å­˜å‚¨&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;ä½¿ç”¨å‚æ•° &lt;code&gt;--init_db&lt;/code&gt; è¿›è¡Œæ•°æ®åº“åˆå§‹åŒ–ï¼ˆä½¿ç”¨&lt;code&gt;--init_db&lt;/code&gt;æ—¶ä¸éœ€è¦æºå¸¦å…¶ä»–optionalï¼‰&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;SQLite æ•°æ®åº“&lt;/strong&gt;ï¼šè½»é‡çº§æ•°æ®åº“ï¼Œæ— éœ€æœåŠ¡å™¨ï¼Œé€‚åˆä¸ªäººä½¿ç”¨ï¼ˆæ¨èï¼‰ 
    &lt;ol&gt; 
     &lt;li&gt;åˆå§‹åŒ–ï¼š&lt;code&gt;--init_db sqlite&lt;/code&gt;&lt;/li&gt; 
     &lt;li&gt;æ•°æ®å­˜å‚¨ï¼š&lt;code&gt;--save_data_option sqlite&lt;/code&gt;&lt;/li&gt; 
    &lt;/ol&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;MySQL æ•°æ®åº“&lt;/strong&gt;ï¼šæ”¯æŒå…³ç³»å‹æ•°æ®åº“ MySQL ä¸­ä¿å­˜ï¼ˆéœ€è¦æå‰åˆ›å»ºæ•°æ®åº“ï¼‰ 
    &lt;ol&gt; 
     &lt;li&gt;åˆå§‹åŒ–ï¼š&lt;code&gt;--init_db mysql&lt;/code&gt;&lt;/li&gt; 
     &lt;li&gt;æ•°æ®å­˜å‚¨ï¼š&lt;code&gt;--save_data_option db&lt;/code&gt;ï¼ˆdb å‚æ•°ä¸ºå…¼å®¹å†å²æ›´æ–°ä¿ç•™ï¼‰&lt;/li&gt; 
    &lt;/ol&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ä½¿ç”¨ç¤ºä¾‹ï¼š&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# åˆå§‹åŒ– SQLite æ•°æ®åº“ï¼ˆä½¿ç”¨'--init_db'æ—¶ä¸éœ€è¦æºå¸¦å…¶ä»–optionalï¼‰
uv run main.py --init_db sqlite
# ä½¿ç”¨ SQLite å­˜å‚¨æ•°æ®ï¼ˆæ¨èä¸ªäººç”¨æˆ·ä½¿ç”¨ï¼‰
uv run main.py --platform xhs --lt qrcode --type search --save_data_option sqlite
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# åˆå§‹åŒ– MySQL æ•°æ®åº“
uv run main.py --init_db mysql
# ä½¿ç”¨ MySQL å­˜å‚¨æ•°æ®ï¼ˆä¸ºé€‚é…å†å²æ›´æ–°ï¼Œdbå‚æ•°è¿›è¡Œæ²¿ç”¨ï¼‰
uv run main.py --platform xhs --lt qrcode --type search --save_data_option db
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://github.com/MediaCrawlerPro"&gt;ğŸš€ MediaCrawlerPro é‡ç£…å‘å¸ƒ ğŸš€ï¼æ›´å¤šçš„åŠŸèƒ½ï¼Œæ›´å¥½çš„æ¶æ„è®¾è®¡ï¼&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;ğŸ’° èµåŠ©å•†å±•ç¤º&lt;/h3&gt; 
&lt;a href="https://www.swiftproxy.net/?ref=nanmi"&gt; &lt;img src="https://raw.githubusercontent.com/NanmiCoder/MediaCrawler/main/docs/static/images/img_5.png" /&gt; &lt;br /&gt; Swiftproxy - 90M+ å…¨çƒé«˜è´¨é‡çº¯å‡€ä½å®…IPï¼Œæ³¨å†Œå¯é¢†å…è´¹ 500MB æµ‹è¯•æµé‡ï¼ŒåŠ¨æ€æµé‡ä¸è¿‡æœŸï¼ &amp;gt; ä¸“å±æŠ˜æ‰£ç ï¼š**GHB5** ç«‹äº«ä¹æŠ˜ä¼˜æƒ ï¼ &lt;/a&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;a href="https://h.wandouip.com"&gt; &lt;img src="https://raw.githubusercontent.com/NanmiCoder/MediaCrawler/main/docs/static/images/img_8.jpg" /&gt; &lt;br /&gt; è±Œè±†HTTPè‡ªè¥åƒä¸‡çº§IPèµ„æºæ± ï¼ŒIPçº¯å‡€åº¦â‰¥99.8%ï¼Œæ¯æ—¥ä¿æŒIPé«˜é¢‘æ›´æ–°ï¼Œå¿«é€Ÿå“åº”ï¼Œç¨³å®šè¿æ¥ï¼Œæ»¡è¶³å¤šç§ä¸šåŠ¡åœºæ™¯ï¼Œæ”¯æŒæŒ‰éœ€å®šåˆ¶ï¼Œæ³¨å†Œå…è´¹æå–10000ipã€‚ &lt;/a&gt; 
&lt;h3&gt;ğŸ¤ æˆä¸ºèµåŠ©è€…&lt;/h3&gt; 
&lt;p&gt;æˆä¸ºèµåŠ©è€…ï¼Œå¯ä»¥å°†æ‚¨çš„äº§å“å±•ç¤ºåœ¨è¿™é‡Œï¼Œæ¯å¤©è·å¾—å¤§é‡æ›å…‰ï¼&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;è”ç³»æ–¹å¼&lt;/strong&gt;ï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;å¾®ä¿¡ï¼š&lt;code&gt;yzglan&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;é‚®ç®±ï¼š&lt;code&gt;relakkes@gmail.com&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ¤ ç¤¾åŒºä¸æ”¯æŒ&lt;/h2&gt; 
&lt;h3&gt;ğŸ’¬ äº¤æµç¾¤ç»„&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;å¾®ä¿¡äº¤æµç¾¤&lt;/strong&gt;ï¼š&lt;a href="https://nanmicoder.github.io/MediaCrawler/%E5%BE%AE%E4%BF%A1%E4%BA%A4%E6%B5%81%E7%BE%A4.html"&gt;ç‚¹å‡»åŠ å…¥&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ“š æ–‡æ¡£ä¸æ•™ç¨‹&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;åœ¨çº¿æ–‡æ¡£&lt;/strong&gt;ï¼š&lt;a href="https://nanmicoder.github.io/MediaCrawler/"&gt;MediaCrawler å®Œæ•´æ–‡æ¡£&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;çˆ¬è™«æ•™ç¨‹&lt;/strong&gt;ï¼š&lt;a href="https://github.com/NanmiCoder/CrawlerTutorial"&gt;CrawlerTutorial å…è´¹æ•™ç¨‹&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;å…¶ä»–å¸¸è§é—®é¢˜å¯ä»¥æŸ¥çœ‹åœ¨çº¿æ–‡æ¡£&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;åœ¨çº¿æ–‡æ¡£åŒ…å«ä½¿ç”¨æ–¹æ³•ã€å¸¸è§é—®é¢˜ã€åŠ å…¥é¡¹ç›®äº¤æµç¾¤ç­‰ã€‚ &lt;a href="https://nanmicoder.github.io/MediaCrawler/"&gt;MediaCrawleråœ¨çº¿æ–‡æ¡£&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h1&gt;ä½œè€…æä¾›çš„çŸ¥è¯†æœåŠ¡&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;å¦‚æœæƒ³å¿«é€Ÿå…¥é—¨å’Œå­¦ä¹ è¯¥é¡¹ç›®çš„ä½¿ç”¨ã€æºç æ¶æ„è®¾è®¡ç­‰ã€å­¦ä¹ ç¼–ç¨‹æŠ€æœ¯ã€äº¦æˆ–è€…æƒ³äº†è§£MediaCrawlerProçš„æºä»£ç è®¾è®¡å¯ä»¥çœ‹ä¸‹æˆ‘çš„çŸ¥è¯†ä»˜è´¹æ ç›®ã€‚&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;a href="https://nanmicoder.github.io/MediaCrawler/%E7%9F%A5%E8%AF%86%E4%BB%98%E8%B4%B9%E4%BB%8B%E7%BB%8D.html"&gt;ä½œè€…çš„çŸ¥è¯†ä»˜è´¹æ ç›®ä»‹ç»&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;â­ Star è¶‹åŠ¿å›¾&lt;/h2&gt; 
&lt;p&gt;å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·ç»™ä¸ª â­ Star æ”¯æŒä¸€ä¸‹ï¼Œè®©æ›´å¤šçš„äººçœ‹åˆ° MediaCrawlerï¼&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#NanmiCoder/MediaCrawler&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=NanmiCoder/MediaCrawler&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ğŸ“š å‚è€ƒ&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;å°çº¢ä¹¦å®¢æˆ·ç«¯&lt;/strong&gt;ï¼š&lt;a href="https://github.com/ReaJason/xhs"&gt;ReaJason çš„ xhs ä»“åº“&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;çŸ­ä¿¡è½¬å‘&lt;/strong&gt;ï¼š&lt;a href="https://github.com/pppscn/SmsForwarder"&gt;SmsForwarder å‚è€ƒä»“åº“&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;å†…ç½‘ç©¿é€å·¥å…·&lt;/strong&gt;ï¼š&lt;a href="https://ngrok.com/docs/"&gt;ngrok å®˜æ–¹æ–‡æ¡£&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;å…è´£å£°æ˜&lt;/h1&gt; 
&lt;div id="disclaimer"&gt; 
 &lt;h2&gt;1. é¡¹ç›®ç›®çš„ä¸æ€§è´¨&lt;/h2&gt; 
 &lt;p&gt;æœ¬é¡¹ç›®ï¼ˆä»¥ä¸‹ç®€ç§°â€œæœ¬é¡¹ç›®â€ï¼‰æ˜¯ä½œä¸ºä¸€ä¸ªæŠ€æœ¯ç ”ç©¶ä¸å­¦ä¹ å·¥å…·è€Œåˆ›å»ºçš„ï¼Œæ—¨åœ¨æ¢ç´¢å’Œå­¦ä¹ ç½‘ç»œæ•°æ®é‡‡é›†æŠ€æœ¯ã€‚æœ¬é¡¹ç›®ä¸“æ³¨äºè‡ªåª’ä½“å¹³å°çš„æ•°æ®çˆ¬å–æŠ€æœ¯ç ”ç©¶ï¼Œæ—¨åœ¨æä¾›ç»™å­¦ä¹ è€…å’Œç ”ç©¶è€…ä½œä¸ºæŠ€æœ¯äº¤æµä¹‹ç”¨ã€‚&lt;/p&gt; 
 &lt;h2&gt;2. æ³•å¾‹åˆè§„æ€§å£°æ˜&lt;/h2&gt; 
 &lt;p&gt;æœ¬é¡¹ç›®å¼€å‘è€…ï¼ˆä»¥ä¸‹ç®€ç§°â€œå¼€å‘è€…â€ï¼‰éƒ‘é‡æé†’ç”¨æˆ·åœ¨ä¸‹è½½ã€å®‰è£…å’Œä½¿ç”¨æœ¬é¡¹ç›®æ—¶ï¼Œä¸¥æ ¼éµå®ˆä¸­åäººæ°‘å…±å’Œå›½ç›¸å…³æ³•å¾‹æ³•è§„ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºã€Šä¸­åäººæ°‘å…±å’Œå›½ç½‘ç»œå®‰å…¨æ³•ã€‹ã€ã€Šä¸­åäººæ°‘å…±å’Œå›½åé—´è°æ³•ã€‹ç­‰æ‰€æœ‰é€‚ç”¨çš„å›½å®¶æ³•å¾‹å’Œæ”¿ç­–ã€‚ç”¨æˆ·åº”è‡ªè¡Œæ‰¿æ‹…ä¸€åˆ‡å› ä½¿ç”¨æœ¬é¡¹ç›®è€Œå¯èƒ½å¼•èµ·çš„æ³•å¾‹è´£ä»»ã€‚&lt;/p&gt; 
 &lt;h2&gt;3. ä½¿ç”¨ç›®çš„é™åˆ¶&lt;/h2&gt; 
 &lt;p&gt;æœ¬é¡¹ç›®ä¸¥ç¦ç”¨äºä»»ä½•éæ³•ç›®çš„æˆ–éå­¦ä¹ ã€éç ”ç©¶çš„å•†ä¸šè¡Œä¸ºã€‚æœ¬é¡¹ç›®ä¸å¾—ç”¨äºä»»ä½•å½¢å¼çš„éæ³•ä¾µå…¥ä»–äººè®¡ç®—æœºç³»ç»Ÿï¼Œä¸å¾—ç”¨äºä»»ä½•ä¾µçŠ¯ä»–äººçŸ¥è¯†äº§æƒæˆ–å…¶ä»–åˆæ³•æƒç›Šçš„è¡Œä¸ºã€‚ç”¨æˆ·åº”ä¿è¯å…¶ä½¿ç”¨æœ¬é¡¹ç›®çš„ç›®çš„çº¯å±ä¸ªäººå­¦ä¹ å’ŒæŠ€æœ¯ç ”ç©¶ï¼Œä¸å¾—ç”¨äºä»»ä½•å½¢å¼çš„éæ³•æ´»åŠ¨ã€‚&lt;/p&gt; 
 &lt;h2&gt;4. å…è´£å£°æ˜&lt;/h2&gt; 
 &lt;p&gt;å¼€å‘è€…å·²å°½æœ€å¤§åŠªåŠ›ç¡®ä¿æœ¬é¡¹ç›®çš„æ­£å½“æ€§åŠå®‰å…¨æ€§ï¼Œä½†ä¸å¯¹ç”¨æˆ·ä½¿ç”¨æœ¬é¡¹ç›®å¯èƒ½å¼•èµ·çš„ä»»ä½•å½¢å¼çš„ç›´æ¥æˆ–é—´æ¥æŸå¤±æ‰¿æ‹…è´£ä»»ã€‚åŒ…æ‹¬ä½†ä¸é™äºç”±äºä½¿ç”¨æœ¬é¡¹ç›®è€Œå¯¼è‡´çš„ä»»ä½•æ•°æ®ä¸¢å¤±ã€è®¾å¤‡æŸåã€æ³•å¾‹è¯‰è®¼ç­‰ã€‚&lt;/p&gt; 
 &lt;h2&gt;5. çŸ¥è¯†äº§æƒå£°æ˜&lt;/h2&gt; 
 &lt;p&gt;æœ¬é¡¹ç›®çš„çŸ¥è¯†äº§æƒå½’å¼€å‘è€…æ‰€æœ‰ã€‚æœ¬é¡¹ç›®å—åˆ°è‘—ä½œæƒæ³•å’Œå›½é™…è‘—ä½œæƒæ¡çº¦ä»¥åŠå…¶ä»–çŸ¥è¯†äº§æƒæ³•å¾‹å’Œæ¡çº¦çš„ä¿æŠ¤ã€‚ç”¨æˆ·åœ¨éµå®ˆæœ¬å£°æ˜åŠç›¸å…³æ³•å¾‹æ³•è§„çš„å‰æä¸‹ï¼Œå¯ä»¥ä¸‹è½½å’Œä½¿ç”¨æœ¬é¡¹ç›®ã€‚&lt;/p&gt; 
 &lt;h2&gt;6. æœ€ç»ˆè§£é‡Šæƒ&lt;/h2&gt; 
 &lt;p&gt;å…³äºæœ¬é¡¹ç›®çš„æœ€ç»ˆè§£é‡Šæƒå½’å¼€å‘è€…æ‰€æœ‰ã€‚å¼€å‘è€…ä¿ç•™éšæ—¶æ›´æ”¹æˆ–æ›´æ–°æœ¬å…è´£å£°æ˜çš„æƒåˆ©ï¼Œæ•ä¸å¦è¡Œé€šçŸ¥ã€‚&lt;/p&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>OpenMind/OM1</title>
      <link>https://github.com/OpenMind/OM1</link>
      <description>&lt;p&gt;Modular AI runtime for robots&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/853153b7-351a-433d-9e1a-d257b781f93c" alt="OM_Banner_X2 (1)" /&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://arxiv.org/abs/2412.18588"&gt;Technical Paper&lt;/a&gt; | &lt;a href="https://docs.openmind.org/"&gt;Documentation&lt;/a&gt; | &lt;a href="https://x.com/openmind_agi"&gt;X&lt;/a&gt; | &lt;a href="https://discord.gg/VUjpg4ef5n"&gt;Discord&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;OpenMind's OM1 is a modular AI runtime that empowers developers to create and deploy multimodal AI agents across digital environments and physical robots&lt;/strong&gt;, including Humanoids, Phone Apps, websites, Quadrupeds, and educational robots such as TurtleBot 4. OM1 agents can process diverse inputs like web data, social media, camera feeds, and LIDAR, while enabling physical actions including motion, autonomous navigation, and natural conversations. The goal of OM1 is to make it easy to create highly capable human-focused robots, that are easy to upgrade and (re)configure to accommodate different physical form factors.&lt;/p&gt; 
&lt;h2&gt;Capabilities of OM1&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Modular Architecture&lt;/strong&gt;: Designed with Python for simplicity and seamless integration.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Data Input&lt;/strong&gt;: Easily handles new data and sensors.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Hardware Support via Plugins&lt;/strong&gt;: Supports new hardware through plugins for API endpoints and specific robot hardware connections to &lt;code&gt;ROS2&lt;/code&gt;, &lt;code&gt;Zenoh&lt;/code&gt;, and &lt;code&gt;CycloneDDS&lt;/code&gt;. (We recommend &lt;code&gt;Zenoh&lt;/code&gt; for all new development).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Web-Based Debugging Display&lt;/strong&gt;: Monitor the system in action with WebSim (available at &lt;a href="http://localhost:8000/"&gt;http://localhost:8000/&lt;/a&gt;) for easy visual debugging.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Pre-configured Endpoints&lt;/strong&gt;: Supports Voice-to-Speech, OpenAIâ€™s &lt;code&gt;gpt-4o&lt;/code&gt;, DeepSeek, and multiple Visual Language Models (VLMs) with pre-configured endpoints for each service.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Architecture Overview&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/14e9b916-4df7-4700-9336-2983c85be311" alt="Artboard 1@4x 1 (1)" /&gt;&lt;/p&gt; 
&lt;h2&gt;Getting Started - Hello World&lt;/h2&gt; 
&lt;p&gt;To get started with OM1, let's run the Spot agent. Spot uses your webcam to capture and label objects. These text captions are then sent to &lt;code&gt;OpenAI 4o&lt;/code&gt;, which returns &lt;code&gt;movement&lt;/code&gt;, &lt;code&gt;speech&lt;/code&gt; and &lt;code&gt;face&lt;/code&gt; action commands. These commands are displayed on WebSim along with basic timing and other debugging information.&lt;/p&gt; 
&lt;h3&gt;Package Management and VENV&lt;/h3&gt; 
&lt;p&gt;You will need the &lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;&lt;code&gt;uv&lt;/code&gt; package manager&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Clone the Repo&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/openmind/OM1.git
cd OM1
git submodule update --init
uv venv
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Install Dependencies&lt;/h3&gt; 
&lt;p&gt;For MacOS&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;brew install portaudio ffmpeg
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For Linux&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt-get update
sudo apt-get install portaudio19-dev python-dev ffmpeg
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Obtain an OpenMind API Key&lt;/h3&gt; 
&lt;p&gt;Obtain your API Key at &lt;a href="https://portal.openmind.org/"&gt;OpenMind Portal&lt;/a&gt;. Copy it to &lt;code&gt;config/spot.json5&lt;/code&gt;, replacing the &lt;code&gt;openmind_free&lt;/code&gt; placeholder. Or, &lt;code&gt;cp env.example .env&lt;/code&gt; and add your key to the &lt;code&gt;.env&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Launching OM1&lt;/h3&gt; 
&lt;p&gt;Run&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run src/run.py spot
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After launching OM1, the Spot agent will interact with you and perform (simulated) actions. For more help connecting OM1 to your robot hardware, see &lt;a href="https://docs.openmind.org/getting-started"&gt;getting started&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;What's Next?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Try out some &lt;a href="https://docs.openmind.org/examples"&gt;examples&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Add new &lt;code&gt;inputs&lt;/code&gt; and &lt;code&gt;actions&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Design custom agents and robots by creating your own &lt;code&gt;json5&lt;/code&gt; config files with custom combinations of inputs and actions.&lt;/li&gt; 
 &lt;li&gt;Change the system prompts in the configuration files (located in &lt;code&gt;/config/&lt;/code&gt;) to create new behaviors.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Interfacing with New Robot Hardware&lt;/h2&gt; 
&lt;p&gt;OM1 assumes that robot hardware provides a high-level SDK that accepts elemental movement and action commands such as &lt;code&gt;backflip&lt;/code&gt;, &lt;code&gt;run&lt;/code&gt;, &lt;code&gt;gently pick up the red apple&lt;/code&gt;, &lt;code&gt;move(0.37, 0, 0)&lt;/code&gt;, and &lt;code&gt;smile&lt;/code&gt;. An example is provided in &lt;code&gt;actions/move_safe/connector/ros2.py&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;...
elif output_interface.action == "shake paw":
    if self.sport_client:
        self.sport_client.Hello()
...
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If your robot hardware does not yet provide a suitable HAL (hardware abstraction layer), traditional robotics approaches such as RL (reinforcement learning) in concert with suitable simulation environments (Unity, Gazebo), sensors (such as hand mounted ZED depth cameras), and custom VLAs will be needed for you to create one. It is further assumed that your HAL accepts motion trajectories, provides battery and thermal management/monitoring, and calibrates and tunes sensors such as IMUs, LIDARs, and magnetometers.&lt;/p&gt; 
&lt;p&gt;OM1 can interface with your HAL via USB, serial, ROS2, CycloneDDS, Zenoh, or websockets. For an example of an advanced humanoid HAL, please see &lt;a href="https://github.com/unitreerobotics/unitree_sdk2/raw/adee312b081c656ecd0bb4e936eed96325546296/example/g1/high_level/g1_loco_client_example.cpp#L159"&gt;Unitree's C++ SDK&lt;/a&gt;. Frequently, a HAL, especially ROS2 code, will be dockerized and can then interface with OM1 through DDS middleware or websockets.&lt;/p&gt; 
&lt;h2&gt;Recommended Development Platforms&lt;/h2&gt; 
&lt;p&gt;OM1 is developed on:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Jetson AGX Orin 64GB (running Ubuntu 22.04 and JetPack 6.1)&lt;/li&gt; 
 &lt;li&gt;Mac Studio with Apple M2 Ultra with 48 GB unified memory (running MacOS Sequoia)&lt;/li&gt; 
 &lt;li&gt;Mac Mini with Apple M4 Pro with 48 GB unified memory (running MacOS Sequoia)&lt;/li&gt; 
 &lt;li&gt;Generic Linux machines (running Ubuntu 22.04)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;OM1 &lt;em&gt;should&lt;/em&gt; run on other platforms (such as Windows) and microcontrollers such as the Raspberry Pi 5 16GB.&lt;/p&gt; 
&lt;h2&gt;Full Autonomy Guidance&lt;/h2&gt; 
&lt;p&gt;We're excited to introduce &lt;strong&gt;full autonomy mode&lt;/strong&gt;, where three services work together in a loop without manual intervention:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;om1&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;unitree_go2_ros2_sdk&lt;/strong&gt; â€“ A ROS 2 package that provides SLAM (Simultaneous Localization and Mapping) capabilities for the Unitree Go2 robot using an RPLiDAR sensor, the SLAM Toolbox and the Nav2 stack.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;om1-avatar&lt;/strong&gt; â€“ A modern React-based frontend application that provides the user interface and avatar display system for OM1 robotics software.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Intro to Backpack?&lt;/h2&gt; 
&lt;p&gt;From research to real-world autonomy, a platform that learns, moves, and builds with you. We'll shortly be releasing the &lt;strong&gt;BOM&lt;/strong&gt; and details on &lt;strong&gt;DIY&lt;/strong&gt; for the it. Stay tuned!&lt;/p&gt; 
&lt;p&gt;Clone the following repos -&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenMind/OM1.git"&gt;https://github.com/OpenMind/OM1.git&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenMind/unitree_go2_ros2_sdk.git"&gt;https://github.com/OpenMind/unitree_go2_ros2_sdk.git&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenMind/OM1-avatar.git"&gt;https://github.com/OpenMind/OM1-avatar.git&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Starting the system&lt;/h2&gt; 
&lt;p&gt;To start all services, run the following commands:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;For OM1&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Setup the API key&lt;/p&gt; 
&lt;p&gt;For Bash: vim ~/.bashrc or ~/.bash_profile.&lt;/p&gt; 
&lt;p&gt;For Zsh: vim ~/.zshrc.&lt;/p&gt; 
&lt;p&gt;Add&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export OM_API_KEY="your_api_key"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Update the docker-compose file. Replace "unitree_go2_autonomy_advance" with the agent you want to run.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;command: ["unitree_go2_autonomy_advance"]
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd OM1
docker-compose up om1 -d --no-build
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;For unitree_go2_ros2_sdk&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd unitree_go2_ros2_sdk
docker-compose up orchestrator -d --no-build
docker-compose up om1_sensor -d --no-build
docker-compose up watchdog -d --no-build
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;For OM1-avatar&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd OM1-avatar
docker-compose up om1_avatar -d --no-build
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Detailed Documentation&lt;/h2&gt; 
&lt;p&gt;More detailed documentation can be accessed at &lt;a href="https://docs.openmind.org/"&gt;docs.openmind.org&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Please make sure to read the &lt;a href="https://raw.githubusercontent.com/OpenMind/OM1/main/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt; before making a pull request.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the terms of the MIT License, which is a permissive free software license that allows users to freely use, modify, and distribute the software. The MIT License is a widely used and well-established license that is known for its simplicity and flexibility. By using the MIT License, this project aims to encourage collaboration, modification, and distribution of the software.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>open-compass/VLMEvalKit</title>
      <link>https://github.com/open-compass/VLMEvalKit</link>
      <description>&lt;p&gt;Open-source evaluation toolkit of large multi-modality models (LMMs), support 220+ LMMs, 80+ benchmarks&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="http://opencompass.openxlab.space/utils/MMLB.jpg" alt="LOGO" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;b&gt;A Toolkit for Evaluating Large Vision-Language Models. &lt;/b&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/open-compass/VLMEvalKit/graphs/contributors"&gt;&lt;img src="https://img.shields.io/github/contributors/open-compass/VLMEvalKit?color=c4f042&amp;amp;labelColor=black&amp;amp;style=flat-square" alt="" /&gt;&lt;/a&gt; â€¢ &lt;a href="https://github.com/open-compass/VLMEvalKit/network/members"&gt;&lt;img src="https://img.shields.io/github/forks/open-compass/VLMEvalKit?color=8ae8ff&amp;amp;labelColor=black&amp;amp;style=flat-square" alt="" /&gt;&lt;/a&gt; â€¢ &lt;a href="https://github.com/open-compass/VLMEvalKit/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/open-compass/VLMEvalKit?color=ffcb47&amp;amp;labelColor=black&amp;amp;style=flat-square" alt="" /&gt;&lt;/a&gt; â€¢ &lt;a href="https://github.com/open-compass/VLMEvalKit/issues"&gt;&lt;img src="https://img.shields.io/github/issues/open-compass/VLMEvalKit?color=ff80eb&amp;amp;labelColor=black&amp;amp;style=flat-square" alt="" /&gt;&lt;/a&gt; â€¢ &lt;a href="https://github.com/open-compass/VLMEvalKit/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/open-compass/VLMEvalKit?color=white&amp;amp;labelColor=black&amp;amp;style=flat-square" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;English | &lt;a href="https://raw.githubusercontent.com/open-compass/VLMEvalKit/main/docs/zh-CN/README_zh-CN.md"&gt;ç®€ä½“ä¸­æ–‡&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/open-compass/VLMEvalKit/main/docs/ja/README_ja.md"&gt;æ—¥æœ¬èª&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://rank.opencompass.org.cn/leaderboard-multimodal"&gt;ğŸ† OC Learderboard &lt;/a&gt; â€¢ &lt;a href="https://raw.githubusercontent.com/open-compass/VLMEvalKit/main/#%EF%B8%8F-quickstart"&gt;ğŸ—ï¸Quickstart &lt;/a&gt; â€¢ &lt;a href="https://raw.githubusercontent.com/open-compass/VLMEvalKit/main/#-datasets-models-and-evaluation-results"&gt;ğŸ“ŠDatasets &amp;amp; Models &lt;/a&gt; â€¢ &lt;a href="https://raw.githubusercontent.com/open-compass/VLMEvalKit/main/#%EF%B8%8F-development-guide"&gt;ğŸ› ï¸Development &lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/spaces/opencompass/open_vlm_leaderboard"&gt;ğŸ¤— HF Leaderboard&lt;/a&gt; â€¢ &lt;a href="https://huggingface.co/datasets/VLMEval/OpenVLMRecords"&gt;ğŸ¤— Evaluation Records&lt;/a&gt; â€¢ &lt;a href="https://huggingface.co/spaces/opencompass/openvlm_video_leaderboard"&gt;ğŸ¤— HF Video Leaderboard&lt;/a&gt; â€¢&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://discord.gg/evDT4GZmxN"&gt;ğŸ”Š Discord&lt;/a&gt; â€¢ &lt;a href="https://www.arxiv.org/abs/2407.11691"&gt;ğŸ“ Report&lt;/a&gt; â€¢ &lt;a href="https://raw.githubusercontent.com/open-compass/VLMEvalKit/main/#-the-goal-of-vlmevalkit"&gt;ğŸ¯Goal &lt;/a&gt; â€¢ &lt;a href="https://raw.githubusercontent.com/open-compass/VLMEvalKit/main/#%EF%B8%8F-citation"&gt;ğŸ–Šï¸Citation &lt;/a&gt;&lt;/p&gt;  
&lt;p&gt;&lt;strong&gt;VLMEvalKit&lt;/strong&gt; (the python package name is &lt;strong&gt;vlmeval&lt;/strong&gt;) is an &lt;strong&gt;open-source evaluation toolkit&lt;/strong&gt; of &lt;strong&gt;large vision-language models (LVLMs)&lt;/strong&gt;. It enables &lt;strong&gt;one-command evaluation&lt;/strong&gt; of LVLMs on various benchmarks, without the heavy workload of data preparation under multiple repositories. In VLMEvalKit, we adopt &lt;strong&gt;generation-based evaluation&lt;/strong&gt; for all LVLMs, and provide the evaluation results obtained with both &lt;strong&gt;exact matching&lt;/strong&gt; and &lt;strong&gt;LLM-based answer extraction&lt;/strong&gt;.&lt;/p&gt; 
&lt;h2&gt;Recent Codebase Changes&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;[2025-09-12]&lt;/strong&gt; &lt;strong&gt;Major Update: Improved Handling for Models with Thinking Mode&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A new feature in &lt;a href="https://github.com/open-compass/VLMEvalKit/pull/1175"&gt;PR 1229&lt;/a&gt; that improves support for models with thinking mode. VLMEvalKit now allows for the use of a custom &lt;code&gt;split_thinking&lt;/code&gt; function. &lt;strong&gt;We strongly recommend this for models with thinking mode to ensure the accuracy of evaluation&lt;/strong&gt;. To use this new functionality, please enable the following settings: &lt;code&gt;SPLIT_THINK=True&lt;/code&gt;. By default, the function will parse content within &lt;code&gt;&amp;lt;think&amp;gt;...&amp;lt;/think&amp;gt;&lt;/code&gt; tags and store it in the &lt;code&gt;thinking&lt;/code&gt; key of the output. For more advanced customization, you can also create a &lt;code&gt;split_think&lt;/code&gt; function for model. Please see the InternVL implementation for an example.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;[2025-09-12]&lt;/strong&gt; &lt;strong&gt;Major Update: Improved Handling for Long Response(More than 16k/32k)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A new feature in &lt;a href="https://github.com/open-compass/VLMEvalKit/pull/1175"&gt;PR 1229&lt;/a&gt; that improves support for models with long response outputs. VLMEvalKit can now save prediction files in TSV format. &lt;strong&gt;Since individual cells in an &lt;code&gt;.xlsx&lt;/code&gt; file are limited to 32,767 characters, we strongly recommend using this feature for models that generate long responses (e.g., exceeding 16k or 32k tokens) to prevent data truncation.&lt;/strong&gt;. To use this new functionality, please enable the following settings: &lt;code&gt;PRED_FORMAT=tsv&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;[2025-08-04]&lt;/strong&gt; In &lt;a href="https://github.com/open-compass/VLMEvalKit/pull/1175"&gt;PR 1175&lt;/a&gt;, we refine the &lt;code&gt;can_infer_option&lt;/code&gt; and &lt;code&gt;can_infer_text&lt;/code&gt;, which increasingly route the evaluation to LLM choice extractors and empirically leads to slight performance improvement for MCQ benchmarks.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ†• News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;[2025-07-07]&lt;/strong&gt; Supported &lt;a href="https://seephys.github.io/"&gt;&lt;strong&gt;SeePhys&lt;/strong&gt;&lt;/a&gt;, which is a â€‹full spectrum multimodal benchmark for evaluating physics reasoning across different knowledge levels. thanks to &lt;a href="https://github.com/Quinn777"&gt;&lt;strong&gt;Quinn777&lt;/strong&gt;&lt;/a&gt; ğŸ”¥ğŸ”¥ğŸ”¥&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025-07-02]&lt;/strong&gt; Supported &lt;a href="https://huggingface.co/AIDC-AI/Ovis-U1-3B"&gt;&lt;strong&gt;OvisU1&lt;/strong&gt;&lt;/a&gt;, thanks to &lt;a href="https://github.com/liyang-7"&gt;&lt;strong&gt;liyang-7&lt;/strong&gt;&lt;/a&gt; ğŸ”¥ğŸ”¥ğŸ”¥&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025-06-16]&lt;/strong&gt; Supported &lt;a href="https://phyx-bench.github.io/"&gt;&lt;strong&gt;PhyX&lt;/strong&gt;&lt;/a&gt;, a benchmark aiming to assess capacity for physics-grounded reasoning in visual scenarios. ğŸ”¥ğŸ”¥ğŸ”¥&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025-05-24]&lt;/strong&gt; To facilitate faster evaluations for large-scale or thinking models, &lt;strong&gt;VLMEvalKit supports multi-node distributed inference&lt;/strong&gt; using &lt;strong&gt;LMDeploy&lt;/strong&gt; (supports &lt;em&gt;InternVL Series, QwenVL Series, LLaMa4&lt;/em&gt;) or &lt;strong&gt;VLLM&lt;/strong&gt;(supports &lt;em&gt;QwenVL Series, LLaMa4&lt;/em&gt;). You can activate this feature by adding the &lt;code&gt;use_lmdeploy&lt;/code&gt; or &lt;code&gt;use_vllm&lt;/code&gt; flag to your custom model configuration in &lt;a href="https://raw.githubusercontent.com/open-compass/VLMEvalKit/main/vlmeval/config.py"&gt;config.py&lt;/a&gt; . Leverage these tools to significantly speed up your evaluation workflows ğŸ”¥ğŸ”¥ğŸ”¥&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025-05-24]&lt;/strong&gt; Supported Models: &lt;strong&gt;InternVL3 Series, Gemini-2.5-Pro, Kimi-VL, LLaMA4, NVILA, Qwen2.5-Omni, Phi4, SmolVLM2, Grok, SAIL-VL-1.5, WeThink-Qwen2.5VL-7B, Bailingmm, VLM-R1, Taichu-VLR&lt;/strong&gt;. Supported Benchmarks: &lt;strong&gt;HLE-Bench, MMVP, MM-AlignBench, Creation-MMBench, MM-IFEval, OmniDocBench, OCR-Reasoning, EMMA, ChaXivï¼ŒMedXpertQA, Physics, MSEarthMCQ, MicroBench, MMSci, VGRP-Bench, wildDoc, TDBench, VisuLogic, CVBench, LEGO-Puzzles, Video-MMLU, QBench-Video, MME-CoT, VLM2Bench, VMCBench, MOAT, Spatial457 Benchmark&lt;/strong&gt;. Please refer to &lt;a href="https://aicarrier.feishu.cn/wiki/Qp7wwSzQ9iK1Y6kNUJVcr6zTnPe?table=tblsdEpLieDoCxtb"&gt;&lt;strong&gt;VLMEvalKit Features&lt;/strong&gt;&lt;/a&gt; for more details. Thanks to all contributors ğŸ”¥ğŸ”¥ğŸ”¥&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025-02-20]&lt;/strong&gt; Supported Models: &lt;strong&gt;InternVL2.5 Series, Qwen2.5VL Series, QVQ-72B, Doubao-VL, Janus-Pro-7B, MiniCPM-o-2.6, InternVL2-MPO, LLaVA-CoT, Hunyuan-Standard-Vision, Ovis2, Valley, SAIL-VL, Ross, Long-VITA, EMU3, SmolVLM&lt;/strong&gt;. Supported Benchmarks: &lt;strong&gt;MMMU-Pro, WeMath, 3DSRBench, LogicVista, VL-RewardBench, CC-OCR, CG-Bench, CMMMU, WorldSense&lt;/strong&gt;. Thanks to all contributors ğŸ”¥ğŸ”¥ğŸ”¥&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2024-12-11]&lt;/strong&gt; Supported &lt;a href="https://huggingface.co/datasets/BaiqiL/NaturalBench"&gt;&lt;strong&gt;NaturalBench&lt;/strong&gt;&lt;/a&gt;, a vision-centric VQA benchmark (NeurIPS'24) that challenges vision-language models with simple questions about natural imagery.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2024-12-02]&lt;/strong&gt; Supported &lt;a href="https://github.com/psunlpgroup/VisOnlyQA/"&gt;&lt;strong&gt;VisOnlyQA&lt;/strong&gt;&lt;/a&gt;, a benchmark for evaluating the visual perception capabilities ğŸ”¥ğŸ”¥ğŸ”¥&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2024-11-26]&lt;/strong&gt; Supported &lt;a href="https://huggingface.co/AIDC-AI/Ovis1.6-Gemma2-27B"&gt;&lt;strong&gt;Ovis1.6-Gemma2-27B&lt;/strong&gt;&lt;/a&gt;, thanks to &lt;a href="https://github.com/runninglsy"&gt;&lt;strong&gt;runninglsy&lt;/strong&gt;&lt;/a&gt; ğŸ”¥ğŸ”¥ğŸ”¥&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2024-11-25]&lt;/strong&gt; Create a new flag &lt;code&gt;VLMEVALKIT_USE_MODELSCOPE&lt;/code&gt;. By setting this environment variable, you can download the video benchmarks supported from &lt;a href="https://www.modelscope.cn"&gt;&lt;strong&gt;modelscope&lt;/strong&gt;&lt;/a&gt; ğŸ”¥ğŸ”¥ğŸ”¥&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ—ï¸ QuickStart&lt;/h2&gt; 
&lt;p&gt;See [&lt;a href="https://raw.githubusercontent.com/open-compass/VLMEvalKit/main/docs/en/Quickstart.md"&gt;QuickStart&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/open-compass/VLMEvalKit/main/docs/zh-CN/Quickstart.md"&gt;å¿«é€Ÿå¼€å§‹&lt;/a&gt;] for a quick start guide.&lt;/p&gt; 
&lt;h2&gt;ğŸ“Š Datasets, Models, and Evaluation Results&lt;/h2&gt; 
&lt;h3&gt;Evaluation Results&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;The performance numbers on our official multi-modal leaderboards can be downloaded from here!&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/spaces/opencompass/open_vlm_leaderboard"&gt;&lt;strong&gt;OpenVLM Leaderboard&lt;/strong&gt;&lt;/a&gt;: &lt;a href="http://opencompass.openxlab.space/assets/OpenVLM.json"&gt;&lt;strong&gt;Download All DETAILED Results&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Check &lt;strong&gt;Supported Benchmarks&lt;/strong&gt; Tab in &lt;a href="https://aicarrier.feishu.cn/wiki/Qp7wwSzQ9iK1Y6kNUJVcr6zTnPe?table=tblsdEpLieDoCxtb"&gt;&lt;strong&gt;VLMEvalKit Features&lt;/strong&gt;&lt;/a&gt; to view all supported image &amp;amp; video benchmarks (70+).&lt;/p&gt; 
&lt;p&gt;Check &lt;strong&gt;Supported LMMs&lt;/strong&gt; Tab in &lt;a href="https://aicarrier.feishu.cn/wiki/Qp7wwSzQ9iK1Y6kNUJVcr6zTnPe?table=tblsdEpLieDoCxtb"&gt;&lt;strong&gt;VLMEvalKit Features&lt;/strong&gt;&lt;/a&gt; to view all supported LMMs, including commercial APIs, open-source models, and more (200+).&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Transformers Version Recommendation:&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Note that some VLMs may not be able to run under certain transformer versions, we recommend the following settings to evaluate each VLM:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Please use&lt;/strong&gt; &lt;code&gt;transformers==4.33.0&lt;/code&gt; &lt;strong&gt;for&lt;/strong&gt;: &lt;code&gt;Qwen series&lt;/code&gt;, &lt;code&gt;Monkey series&lt;/code&gt;, &lt;code&gt;InternLM-XComposer Series&lt;/code&gt;, &lt;code&gt;mPLUG-Owl2&lt;/code&gt;, &lt;code&gt;OpenFlamingo v2&lt;/code&gt;, &lt;code&gt;IDEFICS series&lt;/code&gt;, &lt;code&gt;VisualGLM&lt;/code&gt;, &lt;code&gt;MMAlaya&lt;/code&gt;, &lt;code&gt;ShareCaptioner&lt;/code&gt;, &lt;code&gt;MiniGPT-4 series&lt;/code&gt;, &lt;code&gt;InstructBLIP series&lt;/code&gt;, &lt;code&gt;PandaGPT&lt;/code&gt;, &lt;code&gt;VXVERSE&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Please use&lt;/strong&gt; &lt;code&gt;transformers==4.36.2&lt;/code&gt; &lt;strong&gt;for&lt;/strong&gt;: &lt;code&gt;Moondream1&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Please use&lt;/strong&gt; &lt;code&gt;transformers==4.37.0&lt;/code&gt; &lt;strong&gt;for&lt;/strong&gt;: &lt;code&gt;LLaVA series&lt;/code&gt;, &lt;code&gt;ShareGPT4V series&lt;/code&gt;, &lt;code&gt;TransCore-M&lt;/code&gt;, &lt;code&gt;LLaVA (XTuner)&lt;/code&gt;, &lt;code&gt;CogVLM Series&lt;/code&gt;, &lt;code&gt;EMU2 Series&lt;/code&gt;, &lt;code&gt;Yi-VL Series&lt;/code&gt;, &lt;code&gt;MiniCPM-[V1/V2]&lt;/code&gt;, &lt;code&gt;OmniLMM-12B&lt;/code&gt;, &lt;code&gt;DeepSeek-VL series&lt;/code&gt;, &lt;code&gt;InternVL series&lt;/code&gt;, &lt;code&gt;Cambrian Series&lt;/code&gt;, &lt;code&gt;VILA Series&lt;/code&gt;, &lt;code&gt;Llama-3-MixSenseV1_1&lt;/code&gt;, &lt;code&gt;Parrot-7B&lt;/code&gt;, &lt;code&gt;PLLaVA Series&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Please use&lt;/strong&gt; &lt;code&gt;transformers==4.40.0&lt;/code&gt; &lt;strong&gt;for&lt;/strong&gt;: &lt;code&gt;IDEFICS2&lt;/code&gt;, &lt;code&gt;Bunny-Llama3&lt;/code&gt;, &lt;code&gt;MiniCPM-Llama3-V2.5&lt;/code&gt;, &lt;code&gt;360VL-70B&lt;/code&gt;, &lt;code&gt;Phi-3-Vision&lt;/code&gt;, &lt;code&gt;WeMM&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Please use&lt;/strong&gt; &lt;code&gt;transformers==4.42.0&lt;/code&gt; &lt;strong&gt;for&lt;/strong&gt;: &lt;code&gt;AKI&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Please use&lt;/strong&gt; &lt;code&gt;transformers==4.44.0&lt;/code&gt; &lt;strong&gt;for&lt;/strong&gt;: &lt;code&gt;Moondream2&lt;/code&gt;, &lt;code&gt;H2OVL series&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Please use&lt;/strong&gt; &lt;code&gt;transformers==4.45.0&lt;/code&gt; &lt;strong&gt;for&lt;/strong&gt;: &lt;code&gt;Aria&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Please use&lt;/strong&gt; &lt;code&gt;transformers==latest&lt;/code&gt; &lt;strong&gt;for&lt;/strong&gt;: &lt;code&gt;LLaVA-Next series&lt;/code&gt;, &lt;code&gt;PaliGemma-3B&lt;/code&gt;, &lt;code&gt;Chameleon series&lt;/code&gt;, &lt;code&gt;Video-LLaVA-7B-HF&lt;/code&gt;, &lt;code&gt;Ovis series&lt;/code&gt;, &lt;code&gt;Mantis series&lt;/code&gt;, &lt;code&gt;MiniCPM-V2.6&lt;/code&gt;, &lt;code&gt;OmChat-v2.0-13B-sinlge-beta&lt;/code&gt;, &lt;code&gt;Idefics-3&lt;/code&gt;, &lt;code&gt;GLM-4v-9B&lt;/code&gt;, &lt;code&gt;VideoChat2-HD&lt;/code&gt;, &lt;code&gt;RBDash_72b&lt;/code&gt;, &lt;code&gt;Llama-3.2 series&lt;/code&gt;, &lt;code&gt;Kosmos series&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Torchvision Version Recommendation:&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Note that some VLMs may not be able to run under certain torchvision versions, we recommend the following settings to evaluate each VLM:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Please use&lt;/strong&gt; &lt;code&gt;torchvision&amp;gt;=0.16&lt;/code&gt; &lt;strong&gt;for&lt;/strong&gt;: &lt;code&gt;Moondream series&lt;/code&gt; and &lt;code&gt;Aria&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Flash-attn Version Recommendation:&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Note that some VLMs may not be able to run under certain flash-attention versions, we recommend the following settings to evaluate each VLM:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Please use&lt;/strong&gt; &lt;code&gt;pip install flash-attn --no-build-isolation&lt;/code&gt; &lt;strong&gt;for&lt;/strong&gt;: &lt;code&gt;Aria&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Demo
from vlmeval.config import supported_VLM
model = supported_VLM['idefics_9b_instruct']()
# Forward Single Image
ret = model.generate(['assets/apple.jpg', 'What is in this image?'])
print(ret)  # The image features a red apple with a leaf on it.
# Forward Multiple Images
ret = model.generate(['assets/apple.jpg', 'assets/apple.jpg', 'How many apples are there in the provided images? '])
print(ret)  # There are two apples in the provided images.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ğŸ› ï¸ Development Guide&lt;/h2&gt; 
&lt;p&gt;To develop custom benchmarks, VLMs, or simply contribute other codes to &lt;strong&gt;VLMEvalKit&lt;/strong&gt;, please refer to [&lt;a href="https://raw.githubusercontent.com/open-compass/VLMEvalKit/main/docs/en/Development.md"&gt;Development_Guide&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/open-compass/VLMEvalKit/main/docs/zh-CN/Development.md"&gt;å¼€å‘æŒ‡å—&lt;/a&gt;].&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Call for contributions&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;To promote the contribution from the community and share the corresponding credit (in the next report update):&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;All Contributions will be acknowledged in the report.&lt;/li&gt; 
 &lt;li&gt;Contributors with 3 or more major contributions (implementing an MLLM, benchmark, or major feature) can join the author list of &lt;a href="https://www.arxiv.org/abs/2407.11691"&gt;VLMEvalKit Technical Report&lt;/a&gt; on ArXiv. Eligible contributors can create an issue or dm kennyutc in &lt;a href="https://discord.com/invite/evDT4GZmxN"&gt;VLMEvalKit Discord Channel&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Here is a &lt;a href="https://raw.githubusercontent.com/open-compass/VLMEvalKit/main/docs/en/Contributors.md"&gt;contributor list&lt;/a&gt; we curated based on the records.&lt;/p&gt; 
&lt;h2&gt;ğŸ¯ The Goal of VLMEvalKit&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;The codebase is designed to:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Provide an &lt;strong&gt;easy-to-use&lt;/strong&gt;, &lt;strong&gt;opensource evaluation toolkit&lt;/strong&gt; to make it convenient for researchers &amp;amp; developers to evaluate existing LVLMs and make evaluation results &lt;strong&gt;easy to reproduce&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;Make it easy for VLM developers to evaluate their own models. To evaluate the VLM on multiple supported benchmarks, one just need to &lt;strong&gt;implement a single &lt;code&gt;generate_inner()&lt;/code&gt; function&lt;/strong&gt;, all other workloads (data downloading, data preprocessing, prediction inference, metric calculation) are handled by the codebase.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;The codebase is not designed to:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Reproduce the exact accuracy number reported in the original papers of all &lt;strong&gt;3rd party benchmarks&lt;/strong&gt;. The reason can be two-fold: 
  &lt;ol&gt; 
   &lt;li&gt;VLMEvalKit uses &lt;strong&gt;generation-based evaluation&lt;/strong&gt; for all VLMs (and optionally with &lt;strong&gt;LLM-based answer extraction&lt;/strong&gt;). Meanwhile, some benchmarks may use different approaches (SEEDBench uses PPL-based evaluation, &lt;em&gt;eg.&lt;/em&gt;). For those benchmarks, we compare both scores in the corresponding result. We encourage developers to support other evaluation paradigms in the codebase.&lt;/li&gt; 
   &lt;li&gt;By default, we use the same prompt template for all VLMs to evaluate on a benchmark. Meanwhile, &lt;strong&gt;some VLMs may have their specific prompt templates&lt;/strong&gt; (some may not covered by the codebase at this time). We encourage VLM developers to implement their own prompt template in VLMEvalKit, if that is not covered currently. That will help to improve the reproducibility.&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;ğŸ–Šï¸ Citation&lt;/h2&gt; 
&lt;p&gt;If you find this work helpful, please consider to &lt;strong&gt;starğŸŒŸ&lt;/strong&gt; this repo. Thanks for your support!&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/open-compass/VLMEvalKit/stargazers"&gt;&lt;img src="https://reporoster.com/stars/open-compass/VLMEvalKit" alt="Stargazers repo roster for @open-compass/VLMEvalKit" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;If you use VLMEvalKit in your research or wish to refer to published OpenSource evaluation results, please use the following BibTeX entry and the BibTex entry corresponding to the specific VLM / benchmark you used.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bib"&gt;@inproceedings{duan2024vlmevalkit,
  title={Vlmevalkit: An open-source toolkit for evaluating large multi-modality models},
  author={Duan, Haodong and Yang, Junming and Qiao, Yuxuan and Fang, Xinyu and Chen, Lin and Liu, Yuan and Dong, Xiaoyi and Zang, Yuhang and Zhang, Pan and Wang, Jiaqi and others},
  booktitle={Proceedings of the 32nd ACM International Conference on Multimedia},
  pages={11198--11201},
  year={2024}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p align="right"&gt;&lt;a href="https://raw.githubusercontent.com/open-compass/VLMEvalKit/main/#top"&gt;ğŸ”Back to top&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>MetaCubeX/mihomo</title>
      <link>https://github.com/MetaCubeX/mihomo</link>
      <description>&lt;p&gt;A simple Python Pydantic model for Honkai: Star Rail parsed data from the Mihomo API.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;mihomo&lt;/h1&gt; 
&lt;p&gt;A simple python pydantic model (type hint and autocompletion support) for Honkai: Star Rail parsed data from the Mihomo API.&lt;/p&gt; 
&lt;p&gt;API url: &lt;a href="https://api.mihomo.me/sr_info_parsed/%7BUID%7D?lang=%7BLANG%7D"&gt;https://api.mihomo.me/sr_info_parsed/{UID}?lang={LANG}&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;pip install -U git+https://github.com/KT-Yeh/mihomo.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Basic&lt;/h3&gt; 
&lt;p&gt;There are two parsed data formats:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;V1: 
  &lt;ul&gt; 
   &lt;li&gt;URL: &lt;a href="https://api.mihomo.me/sr_info_parsed/800333171?lang=en&amp;amp;version=v1"&gt;https://api.mihomo.me/sr_info_parsed/800333171?lang=en&amp;amp;version=v1&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Fetching: use &lt;code&gt;client.fetch_user_v1(800333171)&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Data model: &lt;code&gt;mihomo.models.v1.StarrailInfoParsedV1&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;All models defined in &lt;code&gt;mihomo/models/v1&lt;/code&gt; directory.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;V2: 
  &lt;ul&gt; 
   &lt;li&gt;URL: &lt;a href="https://api.mihomo.me/sr_info_parsed/800333171?lang=en"&gt;https://api.mihomo.me/sr_info_parsed/800333171?lang=en&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Fetching: use &lt;code&gt;client.fetch_user(800333171)&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Data model: &lt;code&gt;mihomo.models.StarrailInfoParsed&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;All models defined in &lt;code&gt;mihomo/models&lt;/code&gt; directory.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If you don't want to use &lt;code&gt;client.get_icon_url&lt;/code&gt; to get the image url everytime, you can use &lt;code&gt;client.fetch_user(800333171, replace_icon_name_with_url=True)&lt;/code&gt; to get the parsed data with asset urls.&lt;/p&gt; 
&lt;h3&gt;Example&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;import asyncio

from mihomo import Language, MihomoAPI
from mihomo.models import StarrailInfoParsed
from mihomo.models.v1 import StarrailInfoParsedV1

client = MihomoAPI(language=Language.EN)


async def v1():
    data: StarrailInfoParsedV1 = await client.fetch_user_v1(800333171)

    print(f"Name: {data.player.name}")
    print(f"Level: {data.player.level}")
    print(f"Signature: {data.player.signature}")
    print(f"Achievements: {data.player_details.achievements}")
    print(f"Characters count: {data.player_details.characters}")
    print(f"Profile picture url: {client.get_icon_url(data.player.icon)}")
    for character in data.characters:
        print("-----------")
        print(f"Name: {character.name}")
        print(f"Rarity: {character.rarity}")
        print(f"Level: {character.level}")
        print(f"Avatar url: {client.get_icon_url(character.icon)}")
        print(f"Preview url: {client.get_icon_url(character.preview)}")
        print(f"Portrait url: {client.get_icon_url(character.portrait)}")


async def v2():
    data: StarrailInfoParsed = await client.fetch_user(800333171, replace_icon_name_with_url=True)

    print(f"Name: {data.player.name}")
    print(f"Level: {data.player.level}")
    print(f"Signature: {data.player.signature}")
    print(f"Profile picture url: {data.player.avatar.icon}")
    for character in data.characters:
        print("-----------")
        print(f"Name: {character.name}")
        print(f"Rarity: {character.rarity}")
        print(f"Portrait url: {character.portrait}")

asyncio.run(v1())
asyncio.run(v2())
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Tools&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;from mihomo import tools&lt;/code&gt;&lt;/p&gt; 
&lt;h4&gt;Remove Duplicate Character&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;    data = await client.fetch_user(800333171)
    data = tools.remove_duplicate_character(data)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Merge Character Data&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;    old_data = await client.fetch_user(800333171)

    # Change characters in game and wait for the API to refresh
    # ...

    new_data = await client.fetch_user(800333171)
    data = tools.merge_character_data(new_data, old_data)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Data Persistence&lt;/h3&gt; 
&lt;p&gt;Take pickle and json as an example&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;import pickle
import zlib
from mihomo import MihomoAPI, Language, StarrailInfoParsed

client = MihomoAPI(language=Language.EN)
data = await client.fetch_user(800333171)

# Save
pickle_data = zlib.compress(pickle.dumps(data))
print(len(pickle_data))
json_data = data.json(by_alias=True, ensure_ascii=False)
print(len(json_data))

# Load
data_from_pickle = pickle.loads(zlib.decompress(pickle_data))
data_from_json = StarrailInfoParsed.parse_raw(json_data)
print(type(data_from_pickle))
print(type(data_from_json))
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>mindsdb/mindsdb</title>
      <link>https://github.com/mindsdb/mindsdb</link>
      <description>&lt;p&gt;AI Analytics and Knowledge Engine for RAG over large-scale, heterogeneous data. - The only MCP Server you'll ever need&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a name="readme-top"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://pypi.org/project/MindsDB/" target="_blank"&gt;&lt;img src="https://badge.fury.io/py/MindsDB.svg?sanitize=true" alt="MindsDB Release" /&gt;&lt;/a&gt; 
 &lt;a href="https://www.python.org/downloads/" target="_blank"&gt;&lt;img src="https://img.shields.io/badge/python-3.10.x%7C%203.11.x-brightgreen.svg?sanitize=true" alt="Python supported" /&gt;&lt;/a&gt; 
 &lt;a href="https://hub.docker.com/u/mindsdb" target="_blank"&gt;&lt;img src="https://img.shields.io/docker/pulls/mindsdb/mindsdb" alt="Docker pulls" /&gt;&lt;/a&gt; 
 &lt;br /&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/3068" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/3068" alt="mindsdb%2Fmindsdb | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;a href="https://github.com/mindsdb/mindsdb"&gt; &lt;img src="https://raw.githubusercontent.com/mindsdb/mindsdb/main/docs/assets/mindsdb_logo.png" alt="MindsDB" width="300" /&gt; &lt;/a&gt; 
 &lt;p align="center"&gt; &lt;br /&gt; &lt;a href="https://www.mindsdb.com?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo"&gt;Website&lt;/a&gt; Â· &lt;a href="https://docs.mindsdb.com?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo"&gt;Docs&lt;/a&gt; Â· &lt;a href="https://mindsdb.com/contact"&gt;Contact us for a Demo&lt;/a&gt; Â· &lt;a href="https://mindsdb.com/joincommunity?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo"&gt;Community Slack&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;p&gt;MindsDB enables humans, AI, agents, and applications to get highly accurate answers across large scale data sources.&lt;/p&gt; 
&lt;a href="https://www.youtube.com/watch?v=MX3OKpnsoLM" target="_blank"&gt; &lt;img src="https://github.com/user-attachments/assets/119e7b82-f901-4214-a26f-ff7c5ad86064" alt="MindsDB Demo" /&gt; &lt;/a&gt; 
&lt;h2&gt;Install MindsDB Server&lt;/h2&gt; 
&lt;p&gt;MindsDB is an open-source server that can be deployed anywhere - from your laptop to the cloud, and everywhere in between. And yes, you can customize it to your heart's content.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.mindsdb.com/setup/self-hosted/docker-desktop"&gt;Using Docker Desktop&lt;/a&gt;. This is the fastest and recommended way to get started and have it all running.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.mindsdb.com/setup/self-hosted/docker"&gt;Using Docker&lt;/a&gt;. This is also simple, but gives you more flexibility on how to further customize your server.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://docs.mindsdb.com/mcp/overview"&gt;MindsDB has an MCP server built in&lt;/a&gt; that enables your MCP applications to connect, unify and respond to questions over large-scale federated dataâ€”spanning databases, data warehouses, and SaaS applications.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h1&gt;Core Philosophy: Connect, Unify, Respond&lt;/h1&gt; 
&lt;p&gt;MindsDB's architecture is built around three fundamental capabilities:&lt;/p&gt; 
&lt;h2&gt;&lt;a href="https://docs.mindsdb.com/integrations/data-overview"&gt;Connect&lt;/a&gt; Your Data&lt;/h2&gt; 
&lt;p&gt;You can connect to hundreds of enterprise &lt;a href="https://docs.mindsdb.com/integrations/data-overview"&gt;data sources (learn more)&lt;/a&gt;. These integrations allow MindsDB to access data wherever it resides, forming the foundation for all other capabilities.&lt;/p&gt; 
&lt;h2&gt;&lt;a href="https://docs.mindsdb.com/mindsdb_sql/overview"&gt;Unify&lt;/a&gt; Your Data&lt;/h2&gt; 
&lt;p&gt;In many situations, itâ€™s important to be able to prepare and unify data before generating responses from it. MindsDB SQL offers knowledge bases and views that allow indexing and organizing structured and unstructured data as if it were unified in a single system.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.mindsdb.com/mindsdb_sql/knowledge-bases"&gt;&lt;strong&gt;KNOWLEDGE BASES&lt;/strong&gt;&lt;/a&gt; â€“ Index and organize unstructured data for efficient Q&amp;amp;A.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.mindsdb.com/mindsdb_sql/sql/create/view"&gt;&lt;strong&gt;VIEWS&lt;/strong&gt;&lt;/a&gt; â€“ Simplify data access by creating unified views across different sources (no-ETL).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Unification of data can be automated using JOBs&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.mindsdb.com/mindsdb_sql/sql/create/jobs"&gt;&lt;strong&gt;JOBS&lt;/strong&gt;&lt;/a&gt; â€“ Schedule synchronization and transformation tasks for real-time processing.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;a href="https://docs.mindsdb.com/mindsdb_sql/agents/agent"&gt;Respond&lt;/a&gt; From Your Data&lt;/h2&gt; 
&lt;p&gt;Chat with Your Data&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.mindsdb.com/mindsdb_sql/agents/agent"&gt;&lt;strong&gt;AGENTS&lt;/strong&gt;&lt;/a&gt; â€“ Configure built-in agents specialized in answering questions over your connected and unified data.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.mindsdb.com/mcp/overview"&gt;&lt;strong&gt;MCP&lt;/strong&gt;&lt;/a&gt; â€“ Connect to MindsDB through the MCP (Model Context Protocol) for seamless interaction.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ¤ Contribute&lt;/h2&gt; 
&lt;p&gt;Interested in contributing to MindsDB? Follow our &lt;a href="https://docs.mindsdb.com/contribute/install?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo"&gt;installation guide for development&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You can find our &lt;a href="https://docs.mindsdb.com/contribute/contribute?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo"&gt;contribution guide here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We welcome suggestions! Feel free to open new issues with your ideas, and weâ€™ll guide you.&lt;/p&gt; 
&lt;p&gt;This project adheres to a &lt;a href="https://github.com/mindsdb/mindsdb/raw/main/CODE_OF_CONDUCT.md"&gt;Contributor Code of Conduct&lt;/a&gt;. By participating, you agree to follow its terms.&lt;/p&gt; 
&lt;p&gt;Also, check out our &lt;a href="https://mindsdb.com/community?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo"&gt;community rewards and programs&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ğŸ¤ Support&lt;/h2&gt; 
&lt;p&gt;If you find a bug, please submit an &lt;a href="https://github.com/mindsdb/mindsdb/issues/new/choose"&gt;issue on GitHub&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Hereâ€™s how you can get community support:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Ask a question in our &lt;a href="https://mindsdb.com/joincommunity?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo"&gt;Slack Community&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Join our &lt;a href="https://github.com/mindsdb/mindsdb/discussions"&gt;GitHub Discussions&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Post on &lt;a href="https://stackoverflow.com/questions/tagged/mindsdb"&gt;Stack Overflow&lt;/a&gt; with the MindsDB tag.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For commercial support, please &lt;a href="https://mindsdb.com/contact?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo"&gt;contact the MindsDB team&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ğŸ’š Current Contributors&lt;/h2&gt; 
&lt;a href="https://github.com/mindsdb/mindsdb/graphs/contributors"&gt; &lt;img src="https://contributors-img.web.app/image?repo=mindsdb/mindsdb" /&gt; &lt;/a&gt; 
&lt;p&gt;Generated with &lt;a href="https://contributors-img.web.app"&gt;contributors-img&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ğŸ”” Subscribe for Updates&lt;/h2&gt; 
&lt;p&gt;Join our &lt;a href="https://mindsdb.com/joincommunity"&gt;Slack community&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Zie619/n8n-workflows</title>
      <link>https://github.com/Zie619/n8n-workflows</link>
      <description>&lt;p&gt;all of the workflows of n8n i could find (also from the site itself)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;âš¡ N8N Workflow Collection &amp;amp; Documentation&lt;/h1&gt; 
&lt;p&gt;A professionally organized collection of &lt;strong&gt;2,057 n8n workflows&lt;/strong&gt; with a lightning-fast documentation system that provides instant search, analysis, and browsing capabilities.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;âš ï¸ IMPORTANT NOTICE (Aug 14, 2025):&lt;/strong&gt; Repository history has been rewritten due to DMCA compliance. If you have a fork or local clone, please see &lt;a href="https://github.com/Zie619/n8n-workflows/issues/85"&gt;Issue 85&lt;/a&gt; for instructions on syncing your copy.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Support My Work&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.buymeacoffee.com/zie619"&gt;&lt;img src="https://img.shields.io/badge/-Buy%20Me%20a%20Coffee-ffdd00?logo=buy-me-a-coffee&amp;amp;logoColor=black&amp;amp;style=flat" alt="Buy Me a Coffee" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;If you'd like to say thanks, consider buying me a coffeeâ€”your support helps me keep improving this project!&lt;/p&gt; 
&lt;h2&gt;ğŸš€ &lt;strong&gt;NEW: Public Search Interface &amp;amp; High-Performance Documentation&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;ğŸŒ &lt;a href="https://zie619.github.io/n8n-workflows"&gt;Browse workflows online&lt;/a&gt; - No installation required!&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Or run locally for development with 100x performance improvement:&lt;/strong&gt;&lt;/p&gt; 
&lt;h3&gt;Option 1: Online Search (Recommended for Users)&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;ğŸ”— Visit: &lt;a href="https://zie619.github.io/n8n-workflows"&gt;zie619.github.io/n8n-workflows&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;âš¡ &lt;strong&gt;Instant access&lt;/strong&gt; - No setup required&lt;/li&gt; 
 &lt;li&gt;ğŸ” &lt;strong&gt;Search 2,057+ workflows&lt;/strong&gt; directly in browser&lt;/li&gt; 
 &lt;li&gt;ğŸ“± &lt;strong&gt;Mobile-friendly&lt;/strong&gt; interface&lt;/li&gt; 
 &lt;li&gt;ğŸ·ï¸ &lt;strong&gt;Category filtering&lt;/strong&gt; across 15 categories&lt;/li&gt; 
 &lt;li&gt;ğŸ“¥ &lt;strong&gt;Direct download&lt;/strong&gt; of workflow JSON files&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Option 2: Local Development System&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install dependencies
pip install -r requirements.txt

# Start the fast API server
python run.py

# Open in browser
http://localhost:8000
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Features:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;âš¡ &lt;strong&gt;Sub-100ms response times&lt;/strong&gt; with SQLite FTS5 search&lt;/li&gt; 
 &lt;li&gt;ğŸ” &lt;strong&gt;Instant full-text search&lt;/strong&gt; with advanced filtering&lt;/li&gt; 
 &lt;li&gt;ğŸ“± &lt;strong&gt;Responsive design&lt;/strong&gt; - works perfectly on mobile&lt;/li&gt; 
 &lt;li&gt;ğŸŒ™ &lt;strong&gt;Dark/light themes&lt;/strong&gt; with system preference detection&lt;/li&gt; 
 &lt;li&gt;ğŸ“Š &lt;strong&gt;Live statistics&lt;/strong&gt; - 365 unique integrations, 29,445 total nodes&lt;/li&gt; 
 &lt;li&gt;ğŸ¯ &lt;strong&gt;Smart categorization&lt;/strong&gt; by trigger type and complexity&lt;/li&gt; 
 &lt;li&gt;ğŸ¯ &lt;strong&gt;Use case categorization&lt;/strong&gt; by service name mapped to categories&lt;/li&gt; 
 &lt;li&gt;ğŸ“„ &lt;strong&gt;On-demand JSON viewing&lt;/strong&gt; and download&lt;/li&gt; 
 &lt;li&gt;ğŸ”— &lt;strong&gt;Mermaid diagram generation&lt;/strong&gt; for workflow visualization&lt;/li&gt; 
 &lt;li&gt;ğŸ”„ &lt;strong&gt;Real-time workflow naming&lt;/strong&gt; with intelligent formatting&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Performance Comparison&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Metric&lt;/th&gt; 
   &lt;th&gt;Old System&lt;/th&gt; 
   &lt;th&gt;New System&lt;/th&gt; 
   &lt;th&gt;Improvement&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;File Size&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;71MB HTML&lt;/td&gt; 
   &lt;td&gt;&amp;lt;100KB&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;700x smaller&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Load Time&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;10+ seconds&lt;/td&gt; 
   &lt;td&gt;&amp;lt;1 second&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;10x faster&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Search&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Client-side only&lt;/td&gt; 
   &lt;td&gt;Full-text with FTS5&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Instant&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Memory Usage&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;~2GB RAM&lt;/td&gt; 
   &lt;td&gt;&amp;lt;50MB RAM&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;40x less&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Mobile Support&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Poor&lt;/td&gt; 
   &lt;td&gt;Excellent&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Fully responsive&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ“‚ Repository Organization&lt;/h2&gt; 
&lt;h3&gt;Workflow Collection&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;2,057 workflows&lt;/strong&gt; with meaningful, searchable names&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;365 unique integrations&lt;/strong&gt; across popular platforms&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;29,445 total nodes&lt;/strong&gt; with professional categorization&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Quality assurance&lt;/strong&gt; - All workflows analyzed and categorized&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Advanced Naming System âœ¨&lt;/h3&gt; 
&lt;p&gt;Our intelligent naming system converts technical filenames into readable titles:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Before&lt;/strong&gt;: &lt;code&gt;2051_Telegram_Webhook_Automation_Webhook.json&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;After&lt;/strong&gt;: &lt;code&gt;Telegram Webhook Automation&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;100% meaningful names&lt;/strong&gt; with smart capitalization&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Automatic integration detection&lt;/strong&gt; from node analysis&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Use Case Category âœ¨&lt;/h3&gt; 
&lt;p&gt;The search interface includes a dropdown filter that lets you browse 2,057+ workflows by category.&lt;/p&gt; 
&lt;p&gt;The system includes an automated categorization feature that organizes workflows by service categories to make them easier to discover and filter.&lt;/p&gt; 
&lt;h3&gt;How Categorization Works&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Run the categorization script&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python create_categories.py
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Service Name Recognition&lt;/strong&gt; The script analyzes each workflow JSON filename to identify recognized service names (e.g., "Twilio", "Slack", "Gmail", etc.)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Category Mapping&lt;/strong&gt; Each recognized service name is matched to its corresponding category using the definitions in &lt;code&gt;context/def_categories.json&lt;/code&gt;. For example:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Twilio â†’ Communication &amp;amp; Messaging&lt;/li&gt; 
   &lt;li&gt;Gmail â†’ Communication &amp;amp; Messaging&lt;/li&gt; 
   &lt;li&gt;Airtable â†’ Data Processing &amp;amp; Analysis&lt;/li&gt; 
   &lt;li&gt;Salesforce â†’ CRM &amp;amp; Sales&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Search Categories Generation&lt;/strong&gt; The script produces a &lt;code&gt;search_categories.json&lt;/code&gt; file that contains the categorized workflow data&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Filter Interface&lt;/strong&gt; Users can then filter workflows by category in the search interface, making it easier to find workflows for specific use cases&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Available Categories&lt;/h3&gt; 
&lt;p&gt;The categorization system includes the following main categories:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;AI Agent Development&lt;/li&gt; 
 &lt;li&gt;Business Process Automation&lt;/li&gt; 
 &lt;li&gt;Cloud Storage &amp;amp; File Management&lt;/li&gt; 
 &lt;li&gt;Communication &amp;amp; Messaging&lt;/li&gt; 
 &lt;li&gt;Creative Content &amp;amp; Video Automation&lt;/li&gt; 
 &lt;li&gt;Creative Design Automation&lt;/li&gt; 
 &lt;li&gt;CRM &amp;amp; Sales&lt;/li&gt; 
 &lt;li&gt;Data Processing &amp;amp; Analysis&lt;/li&gt; 
 &lt;li&gt;E-commerce &amp;amp; Retail&lt;/li&gt; 
 &lt;li&gt;Financial &amp;amp; Accounting&lt;/li&gt; 
 &lt;li&gt;Marketing &amp;amp; Advertising Automation&lt;/li&gt; 
 &lt;li&gt;Project Management&lt;/li&gt; 
 &lt;li&gt;Social Media Management&lt;/li&gt; 
 &lt;li&gt;Technical Infrastructure &amp;amp; DevOps&lt;/li&gt; 
 &lt;li&gt;Web Scraping &amp;amp; Data Extraction&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Contribute Categories&lt;/h3&gt; 
&lt;p&gt;You can help expand the categorization by adding more service-to-category mappings (e.g., Twilio â†’ Communication &amp;amp; Messaging) in context/defs_categories.json.&lt;/p&gt; 
&lt;p&gt;Many workflow JSON files are conveniently named with the service name, often separated by underscores (_).&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ›  Usage Instructions&lt;/h2&gt; 
&lt;h3&gt;Option 1: Modern Fast System (Recommended)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone repository
git clone &amp;lt;repo-url&amp;gt;
cd n8n-workflows

# Install Python dependencies
pip install -r requirements.txt

# Start the documentation server
python run.py

# Browse workflows at http://localhost:8000
# - Instant search across 2,057 workflows
# - Professional responsive interface
# - Real-time workflow statistics
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Option 2: Development Mode&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Start with auto-reload for development
python run.py --dev

# Or specify custom host/port
python run.py --host 0.0.0.0 --port 3000

# Force database reindexing
python run.py --reindex
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Import Workflows into n8n&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Use the Python importer (recommended)
python import_workflows.py

# Or manually import individual workflows:
# 1. Open your n8n Editor UI
# 2. Click menu (â˜°) â†’ Import workflow
# 3. Choose any .json file from the workflows/ folder
# 4. Update credentials/webhook URLs before running
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ“Š Workflow Statistics&lt;/h2&gt; 
&lt;h3&gt;Current Collection Stats&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Total Workflows&lt;/strong&gt;: 2,057 automation workflows&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Active Workflows&lt;/strong&gt;: 215 (10.5% active rate)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Total Nodes&lt;/strong&gt;: 29,528 (avg 14.4 nodes per workflow)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Unique Integrations&lt;/strong&gt;: 367 different services and APIs&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Database&lt;/strong&gt;: SQLite with FTS5 full-text search&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Trigger Distribution&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Complex&lt;/strong&gt;: 832 workflows (40.4%) - Multi-trigger systems&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Webhook&lt;/strong&gt;: 521 workflows (25.3%) - API-triggered automations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Manual&lt;/strong&gt;: 478 workflows (23.2%) - User-initiated workflows&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Scheduled&lt;/strong&gt;: 226 workflows (11.0%) - Time-based executions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Complexity Analysis&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Low (â‰¤5 nodes)&lt;/strong&gt;: ~35% - Simple automations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Medium (6-15 nodes)&lt;/strong&gt;: ~45% - Standard workflows&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;High (16+ nodes)&lt;/strong&gt;: ~20% - Complex enterprise systems&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Popular Integrations&lt;/h3&gt; 
&lt;p&gt;Top services by usage frequency:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Communication&lt;/strong&gt;: Telegram, Discord, Slack, WhatsApp&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Cloud Storage&lt;/strong&gt;: Google Drive, Google Sheets, Dropbox&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Databases&lt;/strong&gt;: PostgreSQL, MySQL, MongoDB, Airtable&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;AI/ML&lt;/strong&gt;: OpenAI, Anthropic, Hugging Face&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Development&lt;/strong&gt;: HTTP Request, Webhook, GraphQL&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ” Advanced Search Features&lt;/h2&gt; 
&lt;h3&gt;Smart Search Categories&lt;/h3&gt; 
&lt;p&gt;Our system automatically categorizes workflows into 15 main categories:&lt;/p&gt; 
&lt;h4&gt;Available Categories:&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;AI Agent Development&lt;/strong&gt;: OpenAI, Anthropic, Hugging Face, CalcsLive&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Business Process Automation&lt;/strong&gt;: Workflow utilities, scheduling, data processing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Cloud Storage &amp;amp; File Management&lt;/strong&gt;: Google Drive, Dropbox, OneDrive, Box&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Communication &amp;amp; Messaging&lt;/strong&gt;: Telegram, Discord, Slack, WhatsApp, Email&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Creative Content &amp;amp; Video Automation&lt;/strong&gt;: YouTube, Vimeo, content creation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Creative Design Automation&lt;/strong&gt;: Canva, Figma, image processing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;CRM &amp;amp; Sales&lt;/strong&gt;: Salesforce, HubSpot, Pipedrive, customer management&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Data Processing &amp;amp; Analysis&lt;/strong&gt;: Database operations, analytics, data transformation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;E-commerce &amp;amp; Retail&lt;/strong&gt;: Shopify, Stripe, PayPal, online stores&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Financial &amp;amp; Accounting&lt;/strong&gt;: Financial tools, payment processing, accounting&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Marketing &amp;amp; Advertising Automation&lt;/strong&gt;: Email marketing, campaigns, lead generation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Project Management&lt;/strong&gt;: Jira, Trello, Asana, task management&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Social Media Management&lt;/strong&gt;: LinkedIn, Twitter/X, Facebook, Instagram&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Technical Infrastructure &amp;amp; DevOps&lt;/strong&gt;: GitHub, deployment, monitoring&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Web Scraping &amp;amp; Data Extraction&lt;/strong&gt;: HTTP requests, webhooks, data collection&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;API Usage Examples&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Search workflows by text
curl "http://localhost:8000/api/workflows?q=telegram+automation"

# Filter by trigger type and complexity
curl "http://localhost:8000/api/workflows?trigger=Webhook&amp;amp;complexity=high"

# Find all messaging workflows
curl "http://localhost:8000/api/workflows/category/messaging"

# Get database statistics
curl "http://localhost:8000/api/stats"

# Browse available categories
curl "http://localhost:8000/api/categories"
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ— Technical Architecture&lt;/h2&gt; 
&lt;h3&gt;Modern Stack&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;SQLite Database&lt;/strong&gt; - FTS5 full-text search with 365 indexed integrations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;FastAPI Backend&lt;/strong&gt; - RESTful API with automatic OpenAPI documentation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Responsive Frontend&lt;/strong&gt; - Modern HTML5 with embedded CSS/JavaScript&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Smart Analysis&lt;/strong&gt; - Automatic workflow categorization and naming&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Key Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Change Detection&lt;/strong&gt; - MD5 hashing for efficient re-indexing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Background Processing&lt;/strong&gt; - Non-blocking workflow analysis&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Compressed Responses&lt;/strong&gt; - Gzip middleware for optimal speed&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Error Handling&lt;/strong&gt; - Graceful degradation and comprehensive logging&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Mobile Optimization&lt;/strong&gt; - Touch-friendly interface design&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Database Performance&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sql"&gt;-- Optimized schema for lightning-fast queries
CREATE TABLE workflows (
    id INTEGER PRIMARY KEY,
    filename TEXT UNIQUE,
    name TEXT,
    active BOOLEAN,
    trigger_type TEXT,
    complexity TEXT,
    node_count INTEGER,
    integrations TEXT,  -- JSON array of 365 unique services
    description TEXT,
    file_hash TEXT,     -- MD5 for change detection
    analyzed_at TIMESTAMP
);

-- Full-text search with ranking
CREATE VIRTUAL TABLE workflows_fts USING fts5(
    filename, name, description, integrations, tags,
    content='workflows', content_rowid='id'
);
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ”§ Setup &amp;amp; Requirements&lt;/h2&gt; 
&lt;h3&gt;System Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Python 3.7+&lt;/strong&gt; - For running the documentation system&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Modern Browser&lt;/strong&gt; - Chrome, Firefox, Safari, Edge&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;50MB Storage&lt;/strong&gt; - For SQLite database and indexes&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;n8n Instance&lt;/strong&gt; - For importing and running workflows&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone repository
git clone &amp;lt;repo-url&amp;gt;
cd n8n-workflows

# Install dependencies
pip install -r requirements.txt

# Start documentation server
python run.py

# Access at http://localhost:8000
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Development Setup&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Create virtual environment
python3 -m venv .venv
source .venv/bin/activate  # Linux/Mac
# or .venv\Scripts\activate  # Windows

# Install dependencies
pip install -r requirements.txt

# Run with auto-reload for development
python api_server.py --reload

# Force database reindexing
python workflow_db.py --index --force
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ“‹ Naming Convention&lt;/h2&gt; 
&lt;h3&gt;Intelligent Formatting System&lt;/h3&gt; 
&lt;p&gt;Our system automatically converts technical filenames to user-friendly names:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Automatic transformations:
2051_Telegram_Webhook_Automation_Webhook.json â†’ "Telegram Webhook Automation"
0250_HTTP_Discord_Import_Scheduled.json â†’ "HTTP Discord Import Scheduled"  
0966_OpenAI_Data_Processing_Manual.json â†’ "OpenAI Data Processing Manual"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Technical Format&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;[ID]_[Service1]_[Service2]_[Purpose]_[Trigger].json
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Smart Capitalization Rules&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;HTTP&lt;/strong&gt; â†’ HTTP (not Http)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;API&lt;/strong&gt; â†’ API (not Api)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;webhook&lt;/strong&gt; â†’ Webhook&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;automation&lt;/strong&gt; â†’ Automation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;scheduled&lt;/strong&gt; â†’ Scheduled&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸš€ API Documentation&lt;/h2&gt; 
&lt;h3&gt;Core Endpoints&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;GET /&lt;/code&gt; - Main workflow browser interface&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;GET /api/stats&lt;/code&gt; - Database statistics and metrics&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;GET /api/workflows&lt;/code&gt; - Search with filters and pagination&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;GET /api/workflows/{filename}&lt;/code&gt; - Detailed workflow information&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;GET /api/workflows/{filename}/download&lt;/code&gt; - Download workflow JSON&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;GET /api/workflows/{filename}/diagram&lt;/code&gt; - Generate Mermaid diagram&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Advanced Search&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;GET /api/workflows/category/{category}&lt;/code&gt; - Search by service category&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;GET /api/categories&lt;/code&gt; - List all available categories&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;GET /api/integrations&lt;/code&gt; - Get integration statistics&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;POST /api/reindex&lt;/code&gt; - Trigger background reindexing&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Response Examples&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;// GET /api/stats
{
  "total": 2053,
  "active": 215,
  "inactive": 1838,
  "triggers": {
    "Complex": 831,
    "Webhook": 519,
    "Manual": 477,
    "Scheduled": 226
  },
  "total_nodes": 29445,
  "unique_integrations": 365
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ¤ Contributing&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;ğŸ‰ This project solves &lt;a href="https://github.com/Zie619/n8n-workflows/issues/84"&gt;Issue #84&lt;/a&gt; - providing online access to workflows without requiring local setup!&lt;/strong&gt;&lt;/p&gt; 
&lt;h3&gt;Adding New Workflows&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Export workflow&lt;/strong&gt; as JSON from n8n&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Name descriptively&lt;/strong&gt; following the established pattern: &lt;code&gt;[ID]_[Service]_[Purpose]_[Trigger].json&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Add to workflows/&lt;/strong&gt; directory (create service folder if needed)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Remove sensitive data&lt;/strong&gt; (credentials, personal URLs)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Add tags&lt;/strong&gt; for better searchability (calculation, automation, etc.)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;GitHub Actions automatically&lt;/strong&gt; updates the public search interface&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Quality Standards&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;âœ… Workflow must be functional and tested&lt;/li&gt; 
 &lt;li&gt;âœ… Remove all credentials and sensitive data&lt;/li&gt; 
 &lt;li&gt;âœ… Follow naming convention for consistency&lt;/li&gt; 
 &lt;li&gt;âœ… Verify compatibility with recent n8n versions&lt;/li&gt; 
 &lt;li&gt;âœ… Include meaningful description or comments&lt;/li&gt; 
 &lt;li&gt;âœ… Add relevant tags for search optimization&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Custom Node Workflows&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;âœ… Include npm package links in descriptions&lt;/li&gt; 
 &lt;li&gt;âœ… Document custom node requirements&lt;/li&gt; 
 &lt;li&gt;âœ… Add installation instructions&lt;/li&gt; 
 &lt;li&gt;âœ… Use descriptive tags (like CalcsLive example)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Reindexing (for local development)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Force database reindexing after adding workflows
python run.py --reindex

# Or update search index only
python scripts/generate_search_index.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;âš ï¸ Important Notes&lt;/h2&gt; 
&lt;h3&gt;Security &amp;amp; Privacy&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Review before use&lt;/strong&gt; - All workflows shared as-is for educational purposes&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Update credentials&lt;/strong&gt; - Replace API keys, tokens, and webhooks&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Test safely&lt;/strong&gt; - Verify in development environment first&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Check permissions&lt;/strong&gt; - Ensure proper access rights for integrations&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Compatibility&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;n8n Version&lt;/strong&gt; - Compatible with n8n 1.0+ (most workflows)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Community Nodes&lt;/strong&gt; - Some workflows may require additional node installations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;API Changes&lt;/strong&gt; - External services may have updated their APIs since creation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Dependencies&lt;/strong&gt; - Verify required integrations before importing&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ“š Resources &amp;amp; References&lt;/h2&gt; 
&lt;h3&gt;Workflow Sources&lt;/h3&gt; 
&lt;p&gt;This comprehensive collection includes workflows from:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Official n8n.io&lt;/strong&gt; - Documentation and community examples&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;GitHub repositories&lt;/strong&gt; - Open source community contributions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Blog posts &amp;amp; tutorials&lt;/strong&gt; - Real-world automation patterns&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;User submissions&lt;/strong&gt; - Tested and verified workflows&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Enterprise use cases&lt;/strong&gt; - Business process automations&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Learn More&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.n8n.io/"&gt;n8n Documentation&lt;/a&gt; - Official documentation&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://community.n8n.io/"&gt;n8n Community&lt;/a&gt; - Community forum and support&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://n8n.io/workflows/"&gt;Workflow Templates&lt;/a&gt; - Official template library&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.n8n.io/integrations/"&gt;Integration Docs&lt;/a&gt; - Service-specific guides&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ† Project Achievements&lt;/h2&gt; 
&lt;h3&gt;Repository Transformation&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;2,053 workflows&lt;/strong&gt; professionally organized and named&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;365 unique integrations&lt;/strong&gt; automatically detected and categorized&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;100% meaningful names&lt;/strong&gt; (improved from basic filename patterns)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Zero data loss&lt;/strong&gt; during intelligent renaming process&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Advanced search&lt;/strong&gt; with 15 service categories&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Performance Revolution&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Sub-100ms search&lt;/strong&gt; with SQLite FTS5 full-text indexing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Instant filtering&lt;/strong&gt; across 29,445 workflow nodes&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Mobile-optimized&lt;/strong&gt; responsive design for all devices&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Real-time statistics&lt;/strong&gt; with live database queries&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Professional interface&lt;/strong&gt; with modern UX principles&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;System Reliability&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Robust error handling&lt;/strong&gt; with graceful degradation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Change detection&lt;/strong&gt; for efficient database updates&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Background processing&lt;/strong&gt; for non-blocking operations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Comprehensive logging&lt;/strong&gt; for debugging and monitoring&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Production-ready&lt;/strong&gt; with proper middleware and security&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;em&gt;This repository represents the most comprehensive and well-organized collection of n8n workflows available, featuring cutting-edge search technology and professional documentation that makes workflow discovery and usage a delightful experience.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;ğŸ¯ Perfect for&lt;/strong&gt;: Developers, automation engineers, business analysts, and anyone looking to streamline their workflows with proven n8n automations.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/Zie619/n8n-workflows/main/README_ZH.md"&gt;ä¸­æ–‡&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>hiyouga/LLaMA-Factory</title>
      <link>https://github.com/hiyouga/LLaMA-Factory</link>
      <description>&lt;p&gt;Unified Efficient Fine-Tuning of 100+ LLMs &amp; VLMs (ACL 2024)&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/assets/logo.png" alt="# LLaMA Factory" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/hiyouga/LLaMA-Factory/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/hiyouga/LLaMA-Factory?style=social" alt="GitHub Repo stars" /&gt;&lt;/a&gt; &lt;a href="https://github.com/hiyouga/LLaMA-Factory/commits/main"&gt;&lt;img src="https://img.shields.io/github/last-commit/hiyouga/LLaMA-Factory" alt="GitHub last commit" /&gt;&lt;/a&gt; &lt;a href="https://github.com/hiyouga/LLaMA-Factory/graphs/contributors"&gt;&lt;img src="https://img.shields.io/github/contributors/hiyouga/LLaMA-Factory?color=orange" alt="GitHub contributors" /&gt;&lt;/a&gt; &lt;a href="https://github.com/hiyouga/LLaMA-Factory/actions/workflows/tests.yml"&gt;&lt;img src="https://github.com/hiyouga/LLaMA-Factory/actions/workflows/tests.yml/badge.svg?sanitize=true" alt="GitHub workflow" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/llamafactory/"&gt;&lt;img src="https://img.shields.io/pypi/v/llamafactory" alt="PyPI" /&gt;&lt;/a&gt; &lt;a href="https://scholar.google.com/scholar?cites=12620864006390196564"&gt;&lt;img src="https://img.shields.io/badge/citation-840-green" alt="Citation" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/hiyouga/llamafactory/tags"&gt;&lt;img src="https://img.shields.io/docker/pulls/hiyouga/llamafactory" alt="Docker Pulls" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://twitter.com/llamafactory_ai"&gt;&lt;img src="https://img.shields.io/twitter/follow/llamafactory_ai" alt="Twitter" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/rKfvV9r9FK"&gt;&lt;img src="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/assets/thirdparty/discord.svg?sanitize=true" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing"&gt;&lt;img src="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/assets/thirdparty/colab.svg?sanitize=true" alt="Open in Colab" /&gt;&lt;/a&gt; &lt;a href="https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory"&gt;&lt;img src="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/assets/thirdparty/dsw.svg?sanitize=true" alt="Open in DSW" /&gt;&lt;/a&gt; &lt;a href="https://www.lab4ai.cn/course/detail?id=7c13e60f6137474eb40f6fd3983c0f46&amp;amp;utm_source=LLaMA-Factory"&gt;&lt;img src="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/assets/thirdparty/lab4ai.svg?sanitize=true" alt="Open in Lab4ai" /&gt;&lt;/a&gt; &lt;a href="https://www.llamafactory.com.cn/?utm_source=LLaMA-Factory"&gt;&lt;img src="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/assets/thirdparty/online.svg?sanitize=true" alt="Open in Online" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/spaces/hiyouga/LLaMA-Board"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%A4%97-Open%20in%20Spaces-blue" alt="Open in Spaces" /&gt;&lt;/a&gt; &lt;a href="https://modelscope.cn/studios/hiyouga/LLaMA-Board"&gt;&lt;img src="https://img.shields.io/badge/ModelScope-Open%20in%20Studios-blue" alt="Open in Studios" /&gt;&lt;/a&gt; &lt;a href="https://novita.ai/templates-library/105981?sharer=88115474-394e-4bda-968e-b88e123d0c47"&gt;&lt;img src="https://img.shields.io/badge/Novita-Deploy%20Template-blue" alt="Open in Novita" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Used by &lt;a href="https://aws.amazon.com/cn/blogs/machine-learning/how-apoidea-group-enhances-visual-information-extraction-from-banking-documents-with-multimodal-models-using-llama-factory-on-amazon-sagemaker-hyperpod/"&gt;Amazon&lt;/a&gt;, &lt;a href="https://developer.nvidia.com/rtx/ai-toolkit"&gt;NVIDIA&lt;/a&gt;, &lt;a href="https://help.aliyun.com/zh/pai/use-cases/fine-tune-a-llama-3-model-with-llama-factory"&gt;Aliyun&lt;/a&gt;, etc.&lt;/h3&gt; 
&lt;div align="center" markdown="1"&gt; 
 &lt;h3&gt;Supporters â¤ï¸&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;
     &lt;div style="text-align: center;"&gt;
      &lt;a href="https://warp.dev/llama-factory"&gt;&lt;img alt="Warp sponsorship" width="400" src="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/assets/sponsors/warp.jpg" /&gt;&lt;/a&gt;
      &lt;br /&gt;
      &lt;a href="https://warp.dev/llama-factory" style="font-size:larger;"&gt;Warp, the agentic terminal for developers&lt;/a&gt;
      &lt;br /&gt;
      &lt;a href="https://warp.dev/llama-factory"&gt;Available for MacOS, Linux, &amp;amp; Windows&lt;/a&gt;
     &lt;/div&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;a href="https://serpapi.com"&gt;&lt;img alt="SerpAPI sponsorship" width="250" src="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/assets/sponsors/serpapi.svg?sanitize=true" /&gt; &lt;/a&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
 &lt;/table&gt; 
 &lt;hr /&gt; 
 &lt;h3&gt;Easily fine-tune 100+ large language models with zero-code &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#quickstart"&gt;CLI&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#fine-tuning-with-llama-board-gui-powered-by-gradio"&gt;Web UI&lt;/a&gt;&lt;/h3&gt; 
 &lt;p&gt;&lt;img src="https://trendshift.io/api/badge/repositories/4535" alt="GitHub Trend" /&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;ğŸ‘‹ Join our &lt;a href="https://github.com/hiyouga/llamafactory-community/raw/main/wechat/main.jpg"&gt;WeChat&lt;/a&gt;, &lt;a href="https://github.com/hiyouga/llamafactory-community/raw/main/wechat/npu.jpg"&gt;NPU&lt;/a&gt;, &lt;a href="https://github.com/hiyouga/llamafactory-community/raw/main/wechat/lab4ai.jpg"&gt;Lab4AI&lt;/a&gt;, &lt;a href="https://github.com/hiyouga/llamafactory-community/raw/main/wechat/online.jpg"&gt;LLaMA Factory Online&lt;/a&gt; user group.&lt;/p&gt; 
&lt;p&gt;[ English | &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/README_zh.md"&gt;ä¸­æ–‡&lt;/a&gt; ]&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Fine-tuning a large language model can be easy as...&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/3991a3a8-4276-4d30-9cab-4cb0c4b9b99e"&gt;https://github.com/user-attachments/assets/3991a3a8-4276-4d30-9cab-4cb0c4b9b99e&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Choose your path:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Documentation (WIP)&lt;/strong&gt;: &lt;a href="https://llamafactory.readthedocs.io/en/latest/"&gt;https://llamafactory.readthedocs.io/en/latest/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Documentation (AMD GPU)&lt;/strong&gt;: &lt;a href="https://rocm.docs.amd.com/projects/ai-developer-hub/en/latest/notebooks/fine_tune/llama_factory_llama3.html"&gt;https://rocm.docs.amd.com/projects/ai-developer-hub/en/latest/notebooks/fine_tune/llama_factory_llama3.html&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Colab (free)&lt;/strong&gt;: &lt;a href="https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing"&gt;https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Local machine&lt;/strong&gt;: Please refer to &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#getting-started"&gt;usage&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;PAI-DSW (free trial)&lt;/strong&gt;: &lt;a href="https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory"&gt;https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Alaya NeW (cloud GPU deal)&lt;/strong&gt;: &lt;a href="https://docs.alayanew.com/docs/documents/useGuide/LLaMAFactory/mutiple/?utm_source=LLaMA-Factory"&gt;https://docs.alayanew.com/docs/documents/useGuide/LLaMAFactory/mutiple/?utm_source=LLaMA-Factory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Official Course&lt;/strong&gt;: &lt;a href="https://www.lab4ai.cn/course/detail?id=7c13e60f6137474eb40f6fd3983c0f46&amp;amp;utm_source=LLaMA-Factory"&gt;https://www.lab4ai.cn/course/detail?id=7c13e60f6137474eb40f6fd3983c0f46&amp;amp;utm_source=LLaMA-Factory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;LLaMA Factory Online&lt;/strong&gt;: &lt;a href="https://www.llamafactory.com.cn/?utm_source=LLaMA-Factory"&gt;https://www.llamafactory.com.cn/?utm_source=LLaMA-Factory&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Except for the above links, all other websites are unauthorized third-party websites. Please carefully use them.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#features"&gt;Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#blogs"&gt;Blogs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#changelog"&gt;Changelog&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#supported-models"&gt;Supported Models&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#supported-training-approaches"&gt;Supported Training Approaches&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#provided-datasets"&gt;Provided Datasets&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#requirement"&gt;Requirement&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#getting-started"&gt;Getting Started&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#data-preparation"&gt;Data Preparation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#quickstart"&gt;Quickstart&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#fine-tuning-with-llama-board-gui-powered-by-gradio"&gt;Fine-Tuning with LLaMA Board GUI&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#llama-factory-online"&gt;LLaMA Factory Online&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#build-docker"&gt;Build Docker&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#deploy-with-openai-style-api-and-vllm"&gt;Deploy with OpenAI-style API and vLLM&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#download-from-modelscope-hub"&gt;Download from ModelScope Hub&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#download-from-modelers-hub"&gt;Download from Modelers Hub&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#use-wb-logger"&gt;Use W&amp;amp;B Logger&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#use-swanlab-logger"&gt;Use SwanLab Logger&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#projects-using-llama-factory"&gt;Projects using LLaMA Factory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#citation"&gt;Citation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#acknowledgement"&gt;Acknowledgement&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Various models&lt;/strong&gt;: LLaMA, LLaVA, Mistral, Mixtral-MoE, Qwen, Qwen2-VL, DeepSeek, Yi, Gemma, ChatGLM, Phi, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Integrated methods&lt;/strong&gt;: (Continuous) pre-training, (multimodal) supervised fine-tuning, reward modeling, PPO, DPO, KTO, ORPO, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Scalable resources&lt;/strong&gt;: 16-bit full-tuning, freeze-tuning, LoRA and 2/3/4/5/6/8-bit QLoRA via AQLM/AWQ/GPTQ/LLM.int8/HQQ/EETQ.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Advanced algorithms&lt;/strong&gt;: &lt;a href="https://github.com/jiaweizzhao/GaLore"&gt;GaLore&lt;/a&gt;, &lt;a href="https://github.com/Ledzy/BAdam"&gt;BAdam&lt;/a&gt;, &lt;a href="https://github.com/zhuhanqing/APOLLO"&gt;APOLLO&lt;/a&gt;, &lt;a href="https://github.com/zyushun/Adam-mini"&gt;Adam-mini&lt;/a&gt;, &lt;a href="https://github.com/KellerJordan/Muon"&gt;Muon&lt;/a&gt;, &lt;a href="https://github.com/huggingface/peft/tree/main/src/peft/tuners/oft"&gt;OFT&lt;/a&gt;, DoRA, LongLoRA, LLaMA Pro, Mixture-of-Depths, LoRA+, LoftQ and PiSSA.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Practical tricks&lt;/strong&gt;: &lt;a href="https://github.com/Dao-AILab/flash-attention"&gt;FlashAttention-2&lt;/a&gt;, &lt;a href="https://github.com/unslothai/unsloth"&gt;Unsloth&lt;/a&gt;, &lt;a href="https://github.com/linkedin/Liger-Kernel"&gt;Liger Kernel&lt;/a&gt;, RoPE scaling, NEFTune and rsLoRA.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Wide tasks&lt;/strong&gt;: Multi-turn dialogue, tool using, image understanding, visual grounding, video recognition, audio understanding, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Experiment monitors&lt;/strong&gt;: LlamaBoard, TensorBoard, Wandb, MLflow, &lt;a href="https://github.com/SwanHubX/SwanLab"&gt;SwanLab&lt;/a&gt;, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Faster inference&lt;/strong&gt;: OpenAI-style API, Gradio UI and CLI with &lt;a href="https://github.com/vllm-project/vllm"&gt;vLLM worker&lt;/a&gt; or &lt;a href="https://github.com/sgl-project/sglang"&gt;SGLang worker&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Day-N Support for Fine-Tuning Cutting-Edge Models&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Support Date&lt;/th&gt; 
   &lt;th&gt;Model Name&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Day 0&lt;/td&gt; 
   &lt;td&gt;Qwen3 / Qwen2.5-VL / Gemma 3 / GLM-4.1V / InternLM 3 / MiniCPM-o-2.6&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Day 1&lt;/td&gt; 
   &lt;td&gt;Llama 3 / GLM-4 / Mistral Small / PaliGemma2 / Llama 4&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Blogs&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ’¡ &lt;a href="https://buaa-act.feishu.cn/wiki/GVzlwYcRFiR8OLkHbL6cQpYin7g"&gt;Easy Dataset Ã— LLaMA Factory: Enabling LLMs to Efficiently Learn Domain Knowledge&lt;/a&gt; (English)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.lab4ai.cn/project/detail?id=25cce32ec131497b9e06a93336a0817f&amp;amp;type=project&amp;amp;utm_source=LLaMA-Factory"&gt;Fine-tune a mental health LLM using LLaMA-Factory&lt;/a&gt; (Chinese)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.llamafactory.com.cn/docs/documents/best-practice/gptroleplay/?utm_source=LLaMA-Factory"&gt;Fine-tune GPT-OSS for Role-Playing using LLaMA-Factory&lt;/a&gt; (Chinese)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/cn/blogs/china/building-llm-model-hub-based-on-llamafactory-and-easyr1/"&gt;A One-Stop Code-Free Model Reinforcement Learning and Deployment Platform based on LLaMA-Factory and EasyR1&lt;/a&gt; (Chinese)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/cn/blogs/machine-learning/how-apoidea-group-enhances-visual-information-extraction-from-banking-documents-with-multimodal-models-using-llama-factory-on-amazon-sagemaker-hyperpod/"&gt;How Apoidea Group enhances visual information extraction from banking documents with multimodal models using LLaMA-Factory on Amazon SageMaker HyperPod&lt;/a&gt; (English)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt;
 &lt;summary&gt;All Blogs&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://docs.alayanew.com/docs/documents/bestPractice/bigModel/llama70B/?utm_source=LLaMA-Factory"&gt;Fine-tune Llama3.1-70B for Medical Diagnosis using LLaMA-Factory&lt;/a&gt; (Chinese)&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://docs.alayanew.com/docs/documents/useGuide/LLaMAFactory/mutiple/?utm_source=LLaMA-Factory"&gt;Fine-tune Qwen2.5-VL for Autonomous Driving using LLaMA-Factory&lt;/a&gt; (Chinese)&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory_deepseek_r1_distill_7b"&gt;LLaMA Factory: Fine-tuning the DeepSeek-R1-Distill-Qwen-7B Model for News Classifier&lt;/a&gt; (Chinese)&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://aws.amazon.com/cn/blogs/china/a-one-stop-code-free-model-fine-tuning-deployment-platform-based-on-sagemaker-and-llama-factory/"&gt;A One-Stop Code-Free Model Fine-Tuning &amp;amp; Deployment Platform based on SageMaker and LLaMA-Factory&lt;/a&gt; (Chinese)&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory_qwen2vl"&gt;LLaMA Factory Multi-Modal Fine-Tuning Practice: Fine-Tuning Qwen2-VL for Personal Tourist Guide&lt;/a&gt; (Chinese)&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory"&gt;LLaMA Factory: Fine-tuning Llama3 for Role-Playing&lt;/a&gt; (Chinese)&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;Changelog&lt;/h2&gt; 
&lt;p&gt;[25/08/22] We supported &lt;strong&gt;&lt;a href="https://arxiv.org/abs/2306.07280"&gt;OFT&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href="https://arxiv.org/abs/2506.19847"&gt;OFTv2&lt;/a&gt;&lt;/strong&gt;. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
&lt;p&gt;[25/08/20] We supported fine-tuning the &lt;strong&gt;&lt;a href="https://huggingface.co/internlm/Intern-S1-mini"&gt;Intern-S1-mini&lt;/a&gt;&lt;/strong&gt; models. See &lt;a href="https://github.com/hiyouga/LLaMA-Factory/pull/8976"&gt;PR #8976&lt;/a&gt; to get started.&lt;/p&gt; 
&lt;p&gt;[25/08/06] We supported fine-tuning the &lt;strong&gt;&lt;a href="https://github.com/openai/gpt-oss"&gt;GPT-OSS&lt;/a&gt;&lt;/strong&gt; models. See &lt;a href="https://github.com/hiyouga/LLaMA-Factory/pull/8826"&gt;PR #8826&lt;/a&gt; to get started.&lt;/p&gt; 
&lt;details&gt;
 &lt;summary&gt;Full Changelog&lt;/summary&gt; 
 &lt;p&gt;[25/07/02] We supported fine-tuning the &lt;strong&gt;&lt;a href="https://github.com/THUDM/GLM-4.1V-Thinking"&gt;GLM-4.1V-9B-Thinking&lt;/a&gt;&lt;/strong&gt; model.&lt;/p&gt; 
 &lt;p&gt;[25/04/28] We supported fine-tuning the &lt;strong&gt;&lt;a href="https://qwenlm.github.io/blog/qwen3/"&gt;Qwen3&lt;/a&gt;&lt;/strong&gt; model family.&lt;/p&gt; 
 &lt;p&gt;[25/04/21] We supported the &lt;strong&gt;&lt;a href="https://github.com/KellerJordan/Muon"&gt;Muon&lt;/a&gt;&lt;/strong&gt; optimizer. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples&lt;/a&gt; for usage. Thank &lt;a href="https://github.com/tianshijing"&gt;@tianshijing&lt;/a&gt;'s PR.&lt;/p&gt; 
 &lt;p&gt;[25/04/16] We supported fine-tuning the &lt;strong&gt;&lt;a href="https://huggingface.co/OpenGVLab/InternVL3-8B"&gt;InternVL3&lt;/a&gt;&lt;/strong&gt; model. See &lt;a href="https://github.com/hiyouga/LLaMA-Factory/pull/7258"&gt;PR #7258&lt;/a&gt; to get started.&lt;/p&gt; 
 &lt;p&gt;[25/04/14] We supported fine-tuning the &lt;strong&gt;&lt;a href="https://huggingface.co/THUDM/GLM-Z1-9B-0414"&gt;GLM-Z1&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href="https://huggingface.co/moonshotai/Kimi-VL-A3B-Instruct"&gt;Kimi-VL&lt;/a&gt;&lt;/strong&gt; models.&lt;/p&gt; 
 &lt;p&gt;[25/04/06] We supported fine-tuning the &lt;strong&gt;&lt;a href="https://ai.meta.com/blog/llama-4-multimodal-intelligence/"&gt;Llama 4&lt;/a&gt;&lt;/strong&gt; model. See &lt;a href="https://github.com/hiyouga/LLaMA-Factory/pull/7611"&gt;PR #7611&lt;/a&gt; to get started.&lt;/p&gt; 
 &lt;p&gt;[25/03/31] We supported fine-tuning the &lt;strong&gt;&lt;a href="https://qwenlm.github.io/blog/qwen2.5-omni/"&gt;Qwen2.5 Omni&lt;/a&gt;&lt;/strong&gt; model. See &lt;a href="https://github.com/hiyouga/LLaMA-Factory/pull/7537"&gt;PR #7537&lt;/a&gt; to get started.&lt;/p&gt; 
 &lt;p&gt;[25/03/15] We supported &lt;strong&gt;&lt;a href="https://github.com/sgl-project/sglang"&gt;SGLang&lt;/a&gt;&lt;/strong&gt; as inference backend. Try &lt;code&gt;infer_backend: sglang&lt;/code&gt; to accelerate inference.&lt;/p&gt; 
 &lt;p&gt;[25/03/12] We supported fine-tuning the &lt;strong&gt;&lt;a href="https://huggingface.co/blog/gemma3"&gt;Gemma 3&lt;/a&gt;&lt;/strong&gt; model.&lt;/p&gt; 
 &lt;p&gt;[25/02/24] Announcing &lt;strong&gt;&lt;a href="https://github.com/hiyouga/EasyR1"&gt;EasyR1&lt;/a&gt;&lt;/strong&gt;, an efficient, scalable and multi-modality RL training framework for efficient GRPO training.&lt;/p&gt; 
 &lt;p&gt;[25/02/11] We supported saving the &lt;strong&gt;&lt;a href="https://github.com/ollama/ollama"&gt;Ollama&lt;/a&gt;&lt;/strong&gt; modelfile when exporting the model checkpoints. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[25/02/05] We supported fine-tuning the &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/Qwen/Qwen2-Audio-7B-Instruct"&gt;Qwen2-Audio&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href="https://huggingface.co/openbmb/MiniCPM-o-2_6"&gt;MiniCPM-o-2.6&lt;/a&gt;&lt;/strong&gt; on audio understanding tasks.&lt;/p&gt; 
 &lt;p&gt;[25/01/31] We supported fine-tuning the &lt;strong&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1"&gt;DeepSeek-R1&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct"&gt;Qwen2.5-VL&lt;/a&gt;&lt;/strong&gt; models.&lt;/p&gt; 
 &lt;p&gt;[25/01/15] We supported &lt;strong&gt;&lt;a href="https://arxiv.org/abs/2412.05270"&gt;APOLLO&lt;/a&gt;&lt;/strong&gt; optimizer. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[25/01/14] We supported fine-tuning the &lt;strong&gt;&lt;a href="https://huggingface.co/openbmb/MiniCPM-o-2_6"&gt;MiniCPM-o-2.6&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href="https://huggingface.co/openbmb/MiniCPM-V-2_6"&gt;MiniCPM-V-2.6&lt;/a&gt;&lt;/strong&gt; models. Thank &lt;a href="https://github.com/BUAADreamer"&gt;@BUAADreamer&lt;/a&gt;'s PR.&lt;/p&gt; 
 &lt;p&gt;[25/01/14] We supported fine-tuning the &lt;strong&gt;&lt;a href="https://huggingface.co/collections/internlm/"&gt;InternLM 3&lt;/a&gt;&lt;/strong&gt; models. Thank &lt;a href="https://github.com/hhaAndroid"&gt;@hhaAndroid&lt;/a&gt;'s PR.&lt;/p&gt; 
 &lt;p&gt;[25/01/10] We supported fine-tuning the &lt;strong&gt;&lt;a href="https://huggingface.co/microsoft/phi-4"&gt;Phi-4&lt;/a&gt;&lt;/strong&gt; model.&lt;/p&gt; 
 &lt;p&gt;[24/12/21] We supported using &lt;strong&gt;&lt;a href="https://github.com/SwanHubX/SwanLab"&gt;SwanLab&lt;/a&gt;&lt;/strong&gt; for experiment tracking and visualization. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#use-swanlab-logger"&gt;this section&lt;/a&gt; for details.&lt;/p&gt; 
 &lt;p&gt;[24/11/27] We supported fine-tuning the &lt;strong&gt;&lt;a href="https://huggingface.co/Skywork/Skywork-o1-Open-Llama-3.1-8B"&gt;Skywork-o1&lt;/a&gt;&lt;/strong&gt; model and the &lt;strong&gt;&lt;a href="https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT"&gt;OpenO1&lt;/a&gt;&lt;/strong&gt; dataset.&lt;/p&gt; 
 &lt;p&gt;[24/10/09] We supported downloading pre-trained models and datasets from the &lt;strong&gt;&lt;a href="https://modelers.cn/models"&gt;Modelers Hub&lt;/a&gt;&lt;/strong&gt;. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#download-from-modelers-hub"&gt;this tutorial&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/09/19] We supported fine-tuning the &lt;strong&gt;&lt;a href="https://qwenlm.github.io/blog/qwen2.5/"&gt;Qwen2.5&lt;/a&gt;&lt;/strong&gt; models.&lt;/p&gt; 
 &lt;p&gt;[24/08/30] We supported fine-tuning the &lt;strong&gt;&lt;a href="https://qwenlm.github.io/blog/qwen2-vl/"&gt;Qwen2-VL&lt;/a&gt;&lt;/strong&gt; models. Thank &lt;a href="https://github.com/simonJJJ"&gt;@simonJJJ&lt;/a&gt;'s PR.&lt;/p&gt; 
 &lt;p&gt;[24/08/27] We supported &lt;strong&gt;&lt;a href="https://github.com/linkedin/Liger-Kernel"&gt;Liger Kernel&lt;/a&gt;&lt;/strong&gt;. Try &lt;code&gt;enable_liger_kernel: true&lt;/code&gt; for efficient training.&lt;/p&gt; 
 &lt;p&gt;[24/08/09] We supported &lt;strong&gt;&lt;a href="https://github.com/zyushun/Adam-mini"&gt;Adam-mini&lt;/a&gt;&lt;/strong&gt; optimizer. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples&lt;/a&gt; for usage. Thank &lt;a href="https://github.com/relic-yuexi"&gt;@relic-yuexi&lt;/a&gt;'s PR.&lt;/p&gt; 
 &lt;p&gt;[24/07/04] We supported &lt;a href="https://github.com/MeetKai/functionary/tree/main/functionary/train/packing"&gt;contamination-free packed training&lt;/a&gt;. Use &lt;code&gt;neat_packing: true&lt;/code&gt; to activate it. Thank &lt;a href="https://github.com/chuan298"&gt;@chuan298&lt;/a&gt;'s PR.&lt;/p&gt; 
 &lt;p&gt;[24/06/16] We supported &lt;strong&gt;&lt;a href="https://arxiv.org/abs/2404.02948"&gt;PiSSA&lt;/a&gt;&lt;/strong&gt; algorithm. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/06/07] We supported fine-tuning the &lt;strong&gt;&lt;a href="https://qwenlm.github.io/blog/qwen2/"&gt;Qwen2&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href="https://github.com/THUDM/GLM-4"&gt;GLM-4&lt;/a&gt;&lt;/strong&gt; models.&lt;/p&gt; 
 &lt;p&gt;[24/05/26] We supported &lt;strong&gt;&lt;a href="https://arxiv.org/abs/2405.14734"&gt;SimPO&lt;/a&gt;&lt;/strong&gt; algorithm for preference learning. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/05/20] We supported fine-tuning the &lt;strong&gt;PaliGemma&lt;/strong&gt; series models. Note that the PaliGemma models are pre-trained models, you need to fine-tune them with &lt;code&gt;paligemma&lt;/code&gt; template for chat completion.&lt;/p&gt; 
 &lt;p&gt;[24/05/18] We supported &lt;strong&gt;&lt;a href="https://arxiv.org/abs/2402.01306"&gt;KTO&lt;/a&gt;&lt;/strong&gt; algorithm for preference learning. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/05/14] We supported training and inference on the Ascend NPU devices. Check &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#installation"&gt;installation&lt;/a&gt; section for details.&lt;/p&gt; 
 &lt;p&gt;[24/04/26] We supported fine-tuning the &lt;strong&gt;LLaVA-1.5&lt;/strong&gt; multimodal LLMs. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/04/22] We provided a &lt;strong&gt;&lt;a href="https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing"&gt;Colab notebook&lt;/a&gt;&lt;/strong&gt; for fine-tuning the Llama-3 model on a free T4 GPU. Two Llama-3-derived models fine-tuned using LLaMA Factory are available at Hugging Face, check &lt;a href="https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat"&gt;Llama3-8B-Chinese-Chat&lt;/a&gt; and &lt;a href="https://huggingface.co/zhichen/Llama3-Chinese"&gt;Llama3-Chinese&lt;/a&gt; for details.&lt;/p&gt; 
 &lt;p&gt;[24/04/21] We supported &lt;strong&gt;&lt;a href="https://arxiv.org/abs/2404.02258"&gt;Mixture-of-Depths&lt;/a&gt;&lt;/strong&gt; according to &lt;a href="https://github.com/astramind-ai/Mixture-of-depths"&gt;AstraMindAI's implementation&lt;/a&gt;. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/04/16] We supported &lt;strong&gt;&lt;a href="https://arxiv.org/abs/2404.02827"&gt;BAdam&lt;/a&gt;&lt;/strong&gt; optimizer. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/04/16] We supported &lt;strong&gt;&lt;a href="https://github.com/unslothai/unsloth"&gt;unsloth&lt;/a&gt;&lt;/strong&gt;'s long-sequence training (Llama-2-7B-56k within 24GB). It achieves &lt;strong&gt;117%&lt;/strong&gt; speed and &lt;strong&gt;50%&lt;/strong&gt; memory compared with FlashAttention-2, more benchmarks can be found in &lt;a href="https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-comparison"&gt;this page&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;[24/03/31] We supported &lt;strong&gt;&lt;a href="https://arxiv.org/abs/2403.07691"&gt;ORPO&lt;/a&gt;&lt;/strong&gt;. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/03/21] Our paper "&lt;a href="https://arxiv.org/abs/2403.13372"&gt;LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models&lt;/a&gt;" is available at arXiv!&lt;/p&gt; 
 &lt;p&gt;[24/03/20] We supported &lt;strong&gt;FSDP+QLoRA&lt;/strong&gt; that fine-tunes a 70B model on 2x24GB GPUs. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/03/13] We supported &lt;strong&gt;&lt;a href="https://arxiv.org/abs/2402.12354"&gt;LoRA+&lt;/a&gt;&lt;/strong&gt;. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/03/07] We supported &lt;strong&gt;&lt;a href="https://arxiv.org/abs/2403.03507"&gt;GaLore&lt;/a&gt;&lt;/strong&gt; optimizer. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/03/07] We integrated &lt;strong&gt;&lt;a href="https://github.com/vllm-project/vllm"&gt;vLLM&lt;/a&gt;&lt;/strong&gt; for faster and concurrent inference. Try &lt;code&gt;infer_backend: vllm&lt;/code&gt; to enjoy &lt;strong&gt;270%&lt;/strong&gt; inference speed.&lt;/p&gt; 
 &lt;p&gt;[24/02/28] We supported weight-decomposed LoRA (&lt;strong&gt;&lt;a href="https://arxiv.org/abs/2402.09353"&gt;DoRA&lt;/a&gt;&lt;/strong&gt;). Try &lt;code&gt;use_dora: true&lt;/code&gt; to activate DoRA training.&lt;/p&gt; 
 &lt;p&gt;[24/02/15] We supported &lt;strong&gt;block expansion&lt;/strong&gt; proposed by &lt;a href="https://github.com/TencentARC/LLaMA-Pro"&gt;LLaMA Pro&lt;/a&gt;. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/02/05] Qwen1.5 (Qwen2 beta version) series models are supported in LLaMA-Factory. Check this &lt;a href="https://qwenlm.github.io/blog/qwen1.5/"&gt;blog post&lt;/a&gt; for details.&lt;/p&gt; 
 &lt;p&gt;[24/01/18] We supported &lt;strong&gt;agent tuning&lt;/strong&gt; for most models, equipping model with tool using abilities by fine-tuning with &lt;code&gt;dataset: glaive_toolcall_en&lt;/code&gt;.&lt;/p&gt; 
 &lt;p&gt;[23/12/23] We supported &lt;strong&gt;&lt;a href="https://github.com/unslothai/unsloth"&gt;unsloth&lt;/a&gt;&lt;/strong&gt;'s implementation to boost LoRA tuning for the LLaMA, Mistral and Yi models. Try &lt;code&gt;use_unsloth: true&lt;/code&gt; argument to activate unsloth patch. It achieves &lt;strong&gt;170%&lt;/strong&gt; speed in our benchmark, check &lt;a href="https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-comparison"&gt;this page&lt;/a&gt; for details.&lt;/p&gt; 
 &lt;p&gt;[23/12/12] We supported fine-tuning the latest MoE model &lt;strong&gt;&lt;a href="https://huggingface.co/mistralai/Mixtral-8x7B-v0.1"&gt;Mixtral 8x7B&lt;/a&gt;&lt;/strong&gt; in our framework. See hardware requirement &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#hardware-requirement"&gt;here&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;[23/12/01] We supported downloading pre-trained models and datasets from the &lt;strong&gt;&lt;a href="https://modelscope.cn/models"&gt;ModelScope Hub&lt;/a&gt;&lt;/strong&gt;. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#download-from-modelscope-hub"&gt;this tutorial&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[23/10/21] We supported &lt;strong&gt;&lt;a href="https://arxiv.org/abs/2310.05914"&gt;NEFTune&lt;/a&gt;&lt;/strong&gt; trick for fine-tuning. Try &lt;code&gt;neftune_noise_alpha: 5&lt;/code&gt; argument to activate NEFTune.&lt;/p&gt; 
 &lt;p&gt;[23/09/27] We supported &lt;strong&gt;$S^2$-Attn&lt;/strong&gt; proposed by &lt;a href="https://github.com/dvlab-research/LongLoRA"&gt;LongLoRA&lt;/a&gt; for the LLaMA models. Try &lt;code&gt;shift_attn: true&lt;/code&gt; argument to enable shift short attention.&lt;/p&gt; 
 &lt;p&gt;[23/09/23] We integrated MMLU, C-Eval and CMMLU benchmarks in this repo. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[23/09/10] We supported &lt;strong&gt;&lt;a href="https://github.com/Dao-AILab/flash-attention"&gt;FlashAttention-2&lt;/a&gt;&lt;/strong&gt;. Try &lt;code&gt;flash_attn: fa2&lt;/code&gt; argument to enable FlashAttention-2 if you are using RTX4090, A100 or H100 GPUs.&lt;/p&gt; 
 &lt;p&gt;[23/08/12] We supported &lt;strong&gt;RoPE scaling&lt;/strong&gt; to extend the context length of the LLaMA models. Try &lt;code&gt;rope_scaling: linear&lt;/code&gt; argument in training and &lt;code&gt;rope_scaling: dynamic&lt;/code&gt; argument at inference to extrapolate the position embeddings.&lt;/p&gt; 
 &lt;p&gt;[23/08/11] We supported &lt;strong&gt;&lt;a href="https://arxiv.org/abs/2305.18290"&gt;DPO training&lt;/a&gt;&lt;/strong&gt; for instruction-tuned models. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[23/07/31] We supported &lt;strong&gt;dataset streaming&lt;/strong&gt;. Try &lt;code&gt;streaming: true&lt;/code&gt; and &lt;code&gt;max_steps: 10000&lt;/code&gt; arguments to load your dataset in streaming mode.&lt;/p&gt; 
 &lt;p&gt;[23/07/29] We released two instruction-tuned 13B models at Hugging Face. See these Hugging Face Repos (&lt;a href="https://huggingface.co/hiyouga/Llama-2-Chinese-13b-chat"&gt;LLaMA-2&lt;/a&gt; / &lt;a href="https://huggingface.co/hiyouga/Baichuan-13B-sft"&gt;Baichuan&lt;/a&gt;) for details.&lt;/p&gt; 
 &lt;p&gt;[23/07/18] We developed an &lt;strong&gt;all-in-one Web UI&lt;/strong&gt; for training, evaluation and inference. Try &lt;code&gt;train_web.py&lt;/code&gt; to fine-tune models in your Web browser. Thank &lt;a href="https://github.com/KanadeSiina"&gt;@KanadeSiina&lt;/a&gt; and &lt;a href="https://github.com/codemayq"&gt;@codemayq&lt;/a&gt; for their efforts in the development.&lt;/p&gt; 
 &lt;p&gt;[23/07/09] We released &lt;strong&gt;&lt;a href="https://github.com/hiyouga/FastEdit"&gt;FastEdit&lt;/a&gt;&lt;/strong&gt; âš¡ğŸ©¹, an easy-to-use package for editing the factual knowledge of large language models efficiently. Please follow &lt;a href="https://github.com/hiyouga/FastEdit"&gt;FastEdit&lt;/a&gt; if you are interested.&lt;/p&gt; 
 &lt;p&gt;[23/06/29] We provided a &lt;strong&gt;reproducible example&lt;/strong&gt; of training a chat model using instruction-following datasets, see &lt;a href="https://huggingface.co/hiyouga/Baichuan-7B-sft"&gt;Baichuan-7B-sft&lt;/a&gt; for details.&lt;/p&gt; 
 &lt;p&gt;[23/06/22] We aligned the &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/src/api_demo.py"&gt;demo API&lt;/a&gt; with the &lt;a href="https://platform.openai.com/docs/api-reference/chat"&gt;OpenAI's&lt;/a&gt; format where you can insert the fine-tuned model in &lt;strong&gt;arbitrary ChatGPT-based applications&lt;/strong&gt;.&lt;/p&gt; 
 &lt;p&gt;[23/06/03] We supported quantized training and inference (aka &lt;strong&gt;&lt;a href="https://github.com/artidoro/qlora"&gt;QLoRA&lt;/a&gt;&lt;/strong&gt;). See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
&lt;/details&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] If you cannot use the latest feature, please pull the latest code and install LLaMA-Factory again.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Supported Models&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Model size&lt;/th&gt; 
   &lt;th&gt;Template&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/baichuan-inc"&gt;Baichuan 2&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B/13B&lt;/td&gt; 
   &lt;td&gt;baichuan2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/bigscience"&gt;BLOOM/BLOOMZ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;560M/1.1B/1.7B/3B/7.1B/176B&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/THUDM"&gt;ChatGLM3&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;6B&lt;/td&gt; 
   &lt;td&gt;chatglm3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/CohereForAI"&gt;Command R&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;35B/104B&lt;/td&gt; 
   &lt;td&gt;cohere&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/deepseek-ai"&gt;DeepSeek (Code/MoE)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B/16B/67B/236B&lt;/td&gt; 
   &lt;td&gt;deepseek&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/deepseek-ai"&gt;DeepSeek 2.5/3&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;236B/671B&lt;/td&gt; 
   &lt;td&gt;deepseek3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/deepseek-ai"&gt;DeepSeek R1 (Distill)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1.5B/7B/8B/14B/32B/70B/671B&lt;/td&gt; 
   &lt;td&gt;deepseekr1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/baidu"&gt;ERNIE-4.5&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;0.3B/21B/300B&lt;/td&gt; 
   &lt;td&gt;ernie/ernie_nothink&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/tiiuae"&gt;Falcon&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B/11B/40B/180B&lt;/td&gt; 
   &lt;td&gt;falcon&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/tiiuae"&gt;Falcon-H1&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;0.5B/1.5B/3B/7B/34B&lt;/td&gt; 
   &lt;td&gt;falcon_h1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/google"&gt;Gemma/Gemma 2/CodeGemma&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2B/7B/9B/27B&lt;/td&gt; 
   &lt;td&gt;gemma/gemma2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/google"&gt;Gemma 3/Gemma 3n&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;270M/1B/4B/6B/8B/12B/27B&lt;/td&gt; 
   &lt;td&gt;gemma3/gemma3n&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/zai-org"&gt;GLM-4/GLM-4-0414/GLM-Z1&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;9B/32B&lt;/td&gt; 
   &lt;td&gt;glm4/glmz1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/zai-org"&gt;GLM-4.1V&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;9B&lt;/td&gt; 
   &lt;td&gt;glm4v&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/zai-org"&gt;GLM-4.5/GLM-4.5V&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;106B/355B&lt;/td&gt; 
   &lt;td&gt;glm4_moe/glm4v_moe&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/openai-community"&gt;GPT-2&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;0.1B/0.4B/0.8B/1.5B&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/openai"&gt;GPT-OSS&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;20B/120B&lt;/td&gt; 
   &lt;td&gt;gpt&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/ibm-granite"&gt;Granite 3.0-3.3&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1B/2B/3B/8B&lt;/td&gt; 
   &lt;td&gt;granite3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/ibm-granite"&gt;Granite 4&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B&lt;/td&gt; 
   &lt;td&gt;granite4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/tencent/"&gt;Hunyuan&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B&lt;/td&gt; 
   &lt;td&gt;hunyuan&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/IndexTeam"&gt;Index&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1.9B&lt;/td&gt; 
   &lt;td&gt;index&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/internlm"&gt;InternLM 2-3&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B/8B/20B&lt;/td&gt; 
   &lt;td&gt;intern2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/OpenGVLab"&gt;InternVL 2.5-3.5&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1B/2B/4B/8B/14B/30B/38B/78B/241B&lt;/td&gt; 
   &lt;td&gt;intern_vl&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/internlm/"&gt;InternLM/Intern-S1-mini&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;8B&lt;/td&gt; 
   &lt;td&gt;intern_s1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/moonshotai"&gt;Kimi-VL&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;16B&lt;/td&gt; 
   &lt;td&gt;kimi_vl&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/inclusionAI"&gt;Ling 2.0 (mini/flash)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;16B/100B&lt;/td&gt; 
   &lt;td&gt;bailing_v2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/facebookresearch/llama"&gt;Llama&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B/13B/33B/65B&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/meta-llama"&gt;Llama 2&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B/13B/70B&lt;/td&gt; 
   &lt;td&gt;llama2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/meta-llama"&gt;Llama 3-3.3&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1B/3B/8B/70B&lt;/td&gt; 
   &lt;td&gt;llama3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/meta-llama"&gt;Llama 4&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;109B/402B&lt;/td&gt; 
   &lt;td&gt;llama4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/meta-llama"&gt;Llama 3.2 Vision&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;11B/90B&lt;/td&gt; 
   &lt;td&gt;mllama&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/llava-hf"&gt;LLaVA-1.5&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B/13B&lt;/td&gt; 
   &lt;td&gt;llava&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/llava-hf"&gt;LLaVA-NeXT&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B/8B/13B/34B/72B/110B&lt;/td&gt; 
   &lt;td&gt;llava_next&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/llava-hf"&gt;LLaVA-NeXT-Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B/34B&lt;/td&gt; 
   &lt;td&gt;llava_next_video&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/XiaomiMiMo"&gt;MiMo&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B&lt;/td&gt; 
   &lt;td&gt;mimo&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/openbmb"&gt;MiniCPM 1-4.1&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;0.5B/1B/2B/4B/8B&lt;/td&gt; 
   &lt;td&gt;cpm/cpm3/cpm4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/openbmb"&gt;MiniCPM-o-2.6/MiniCPM-V-2.6&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;8B&lt;/td&gt; 
   &lt;td&gt;minicpm_o/minicpm_v&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mistralai"&gt;Ministral/Mistral-Nemo&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;8B/12B&lt;/td&gt; 
   &lt;td&gt;ministral&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mistralai"&gt;Mistral/Mixtral&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B/8x7B/8x22B&lt;/td&gt; 
   &lt;td&gt;mistral&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mistralai"&gt;Mistral Small&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;24B&lt;/td&gt; 
   &lt;td&gt;mistral_small&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/allenai"&gt;OLMo&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1B/7B&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/google"&gt;PaliGemma/PaliGemma2&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;3B/10B/28B&lt;/td&gt; 
   &lt;td&gt;paligemma&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/microsoft"&gt;Phi-1.5/Phi-2&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1.3B/2.7B&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/microsoft"&gt;Phi-3/Phi-3.5&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;4B/14B&lt;/td&gt; 
   &lt;td&gt;phi&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/microsoft"&gt;Phi-3-small&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B&lt;/td&gt; 
   &lt;td&gt;phi_small&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/microsoft"&gt;Phi-4&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;14B&lt;/td&gt; 
   &lt;td&gt;phi4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mistralai"&gt;Pixtral&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;12B&lt;/td&gt; 
   &lt;td&gt;pixtral&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/Qwen"&gt;Qwen (1-2.5) (Code/Math/MoE/QwQ)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;0.5B/1.5B/3B/7B/14B/32B/72B/110B&lt;/td&gt; 
   &lt;td&gt;qwen&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/Qwen"&gt;Qwen3 (MoE/Instruct/Thinking/Next)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;0.6B/1.7B/4B/8B/14B/32B/80B/235B&lt;/td&gt; 
   &lt;td&gt;qwen3/qwen3_nothink&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/Qwen"&gt;Qwen2-Audio&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B&lt;/td&gt; 
   &lt;td&gt;qwen2_audio&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/Qwen"&gt;Qwen2.5-Omni&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;3B/7B&lt;/td&gt; 
   &lt;td&gt;qwen2_omni&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/Qwen"&gt;Qwen3-Omni&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;30B&lt;/td&gt; 
   &lt;td&gt;qwen3_omni&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/Qwen"&gt;Qwen2-VL/Qwen2.5-VL/QVQ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2B/3B/7B/32B/72B&lt;/td&gt; 
   &lt;td&gt;qwen2_vl&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/Qwen"&gt;Qwen3-VL&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;235B&lt;/td&gt; 
   &lt;td&gt;qwen3_vl&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/ByteDance-Seed"&gt;Seed (OSS/Coder)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;8B/36B&lt;/td&gt; 
   &lt;td&gt;seed_oss/seed_coder&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/Skywork"&gt;Skywork o1&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;8B&lt;/td&gt; 
   &lt;td&gt;skywork_o1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/bigcode"&gt;StarCoder 2&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;3B/7B/15B&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/Tele-AI"&gt;TeleChat2&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;3B/7B/35B/115B&lt;/td&gt; 
   &lt;td&gt;telechat2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/xverse"&gt;XVERSE&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B/13B/65B&lt;/td&gt; 
   &lt;td&gt;xverse&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/01-ai"&gt;Yi/Yi-1.5 (Code)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1.5B/6B/9B/34B&lt;/td&gt; 
   &lt;td&gt;yi&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/01-ai"&gt;Yi-VL&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;6B/34B&lt;/td&gt; 
   &lt;td&gt;yi_vl&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/IEITYuan"&gt;Yuan 2&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2B/51B/102B&lt;/td&gt; 
   &lt;td&gt;yuan&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] For the "base" models, the &lt;code&gt;template&lt;/code&gt; argument can be chosen from &lt;code&gt;default&lt;/code&gt;, &lt;code&gt;alpaca&lt;/code&gt;, &lt;code&gt;vicuna&lt;/code&gt; etc. But make sure to use the &lt;strong&gt;corresponding template&lt;/strong&gt; for the "instruct/chat" models.&lt;/p&gt; 
 &lt;p&gt;Remember to use the &lt;strong&gt;SAME&lt;/strong&gt; template in training and inference.&lt;/p&gt; 
 &lt;p&gt;*: You should install the &lt;code&gt;transformers&lt;/code&gt; from main branch and use &lt;code&gt;DISABLE_VERSION_CHECK=1&lt;/code&gt; to skip version check.&lt;/p&gt; 
 &lt;p&gt;**: You need to install a specific version of &lt;code&gt;transformers&lt;/code&gt; to use the corresponding model.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Please refer to &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/src/llamafactory/extras/constants.py"&gt;constants.py&lt;/a&gt; for a full list of models we supported.&lt;/p&gt; 
&lt;p&gt;You also can add a custom chat template to &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/src/llamafactory/data/template.py"&gt;template.py&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Supported Training Approaches&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Approach&lt;/th&gt; 
   &lt;th&gt;Full-tuning&lt;/th&gt; 
   &lt;th&gt;Freeze-tuning&lt;/th&gt; 
   &lt;th&gt;LoRA&lt;/th&gt; 
   &lt;th&gt;QLoRA&lt;/th&gt; 
   &lt;th&gt;OFT&lt;/th&gt; 
   &lt;th&gt;QOFT&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Pre-Training&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Supervised Fine-Tuning&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Reward Modeling&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;PPO Training&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;DPO Training&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;KTO Training&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ORPO Training&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;SimPO Training&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] The implementation details of PPO can be found in &lt;a href="https://newfacade.github.io/notes-on-reinforcement-learning/17-ppo-trl.html"&gt;this blog&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Provided Datasets&lt;/h2&gt; 
&lt;details&gt;
 &lt;summary&gt;Pre-training datasets&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/data/wiki_demo.txt"&gt;Wiki Demo (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/tiiuae/falcon-refinedweb"&gt;RefinedWeb (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/togethercomputer/RedPajama-Data-V2"&gt;RedPajama V2 (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/olm/olm-wikipedia-20221220"&gt;Wikipedia (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/pleisto/wikipedia-cn-20230720-filtered"&gt;Wikipedia (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/EleutherAI/pile"&gt;Pile (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/Skywork/SkyPile-150B"&gt;SkyPile (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/HuggingFaceFW/fineweb"&gt;FineWeb (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu"&gt;FineWeb-Edu (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/BAAI/CCI3-HQ"&gt;CCI3-HQ (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/BAAI/CCI3-Data"&gt;CCI3-Data (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/BAAI/CCI4.0-M2-Base-v1"&gt;CCI4.0-M2-Base-v1 (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/BAAI/CCI4.0-M2-CoT-v1"&gt;CCI4.0-M2-CoT-v1 (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/BAAI/CCI4.0-M2-Extra-v1"&gt;CCI4.0-M2-Extra-v1 (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/bigcode/the-stack"&gt;The Stack (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/bigcode/starcoderdata"&gt;StarCoder (en)&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Supervised fine-tuning datasets&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/data/identity.json"&gt;Identity (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/tatsu-lab/stanford_alpaca"&gt;Stanford Alpaca (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/ymcui/Chinese-LLaMA-Alpaca-3"&gt;Stanford Alpaca (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM"&gt;Alpaca GPT4 (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/glaiveai/glaive-function-calling-v2"&gt;Glaive Function Calling V2 (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/GAIR/lima"&gt;LIMA (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/JosephusCheung/GuanacoDataset"&gt;Guanaco Dataset (multilingual)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/BelleGroup/train_2M_CN"&gt;BELLE 2M (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/BelleGroup/train_1M_CN"&gt;BELLE 1M (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/BelleGroup/train_0.5M_CN"&gt;BELLE 0.5M (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/BelleGroup/generated_chat_0.4M"&gt;BELLE Dialogue 0.4M (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/BelleGroup/school_math_0.25M"&gt;BELLE School Math 0.25M (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M"&gt;BELLE Multiturn Chat 0.8M (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/thunlp/UltraChat"&gt;UltraChat (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/garage-bAInd/Open-Platypus"&gt;OpenPlatypus (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k"&gt;CodeAlpaca 20k (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/QingyiSi/Alpaca-CoT"&gt;Alpaca CoT (multilingual)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/Open-Orca/OpenOrca"&gt;OpenOrca (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/Open-Orca/SlimOrca"&gt;SlimOrca (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/TIGER-Lab/MathInstruct"&gt;MathInstruct (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M"&gt;Firefly 1.1M (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/wiki_qa"&gt;Wiki QA (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/suolyer/webqa"&gt;Web QA (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/zxbsmk/webnovel_cn"&gt;WebNovel (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/berkeley-nest/Nectar"&gt;Nectar (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data"&gt;deepctrl (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/HasturOfficial/adgen"&gt;Advertise Generating (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/totally-not-an-llm/sharegpt-hyperfiltered-3k"&gt;ShareGPT Hyperfiltered (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/shibing624/sharegpt_gpt4"&gt;ShareGPT4 (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k"&gt;UltraChat 200k (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/BAAI/Infinity-Instruct"&gt;Infinity Instruct (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/THUDM/AgentInstruct"&gt;AgentInstruct (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/lmsys/lmsys-chat-1m"&gt;LMSYS Chat 1M (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_V2_196k"&gt;Evol Instruct V2 (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/HuggingFaceTB/cosmopedia"&gt;Cosmopedia (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/hfl/stem_zh_instruction"&gt;STEM (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/hfl/ruozhiba_gpt4_turbo"&gt;Ruozhiba (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/m-a-p/neo_sft_phase2"&gt;Neo-sft (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/Magpie-Align/Magpie-Pro-300K-Filtered"&gt;Magpie-Pro-300K-Filtered (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/argilla/magpie-ultra-v0.1"&gt;Magpie-ultra-v0.1 (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/TIGER-Lab/WebInstructSub"&gt;WebInstructSub (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT"&gt;OpenO1-SFT (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k"&gt;Open-Thoughts (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/open-r1/OpenR1-Math-220k"&gt;Open-R1-Math (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/Congliu/Chinese-DeepSeek-R1-Distill-data-110k-SFT"&gt;Chinese-DeepSeek-R1-Distill (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/BUAADreamer/llava-en-zh-300k"&gt;LLaVA mixed (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/jugg1024/pokemon-gpt4o-captions"&gt;Pokemon-gpt4o-captions (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/mayflowergmbh/oasst_de"&gt;Open Assistant (de)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/mayflowergmbh/dolly-15k_de"&gt;Dolly 15k (de)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/mayflowergmbh/alpaca-gpt4_de"&gt;Alpaca GPT4 (de)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/mayflowergmbh/openschnabeltier_de"&gt;OpenSchnabeltier (de)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/mayflowergmbh/evol-instruct_de"&gt;Evol Instruct (de)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/mayflowergmbh/dolphin_de"&gt;Dolphin (de)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/mayflowergmbh/booksum_de"&gt;Booksum (de)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/mayflowergmbh/airoboros-3.0_de"&gt;Airoboros (de)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/mayflowergmbh/ultra-chat_de"&gt;Ultrachat (de)&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Preference datasets&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/hiyouga/DPO-En-Zh-20k"&gt;DPO mixed (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized"&gt;UltraFeedback (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/m-a-p/COIG-P"&gt;COIG-P (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/openbmb/RLHF-V-Dataset"&gt;RLHF-V (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/Zhihui/VLFeedback"&gt;VLFeedback (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/openbmb/RLAIF-V-Dataset"&gt;RLAIF-V (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/Intel/orca_dpo_pairs"&gt;Orca DPO Pairs (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/Anthropic/hh-rlhf"&gt;HH-RLHF (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/berkeley-nest/Nectar"&gt;Nectar (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/mayflowergmbh/intel_orca_dpo_pairs_de"&gt;Orca DPO (de)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/argilla/kto-mix-15k"&gt;KTO mixed (en)&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;p&gt;Some datasets require confirmation before using them, so we recommend logging in with your Hugging Face account using these commands.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install --upgrade huggingface_hub
huggingface-cli login
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Requirement&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Mandatory&lt;/th&gt; 
   &lt;th&gt;Minimum&lt;/th&gt; 
   &lt;th&gt;Recommend&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;python&lt;/td&gt; 
   &lt;td&gt;3.9&lt;/td&gt; 
   &lt;td&gt;3.10&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;torch&lt;/td&gt; 
   &lt;td&gt;2.0.0&lt;/td&gt; 
   &lt;td&gt;2.6.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;torchvision&lt;/td&gt; 
   &lt;td&gt;0.15.0&lt;/td&gt; 
   &lt;td&gt;0.21.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;transformers&lt;/td&gt; 
   &lt;td&gt;4.49.0&lt;/td&gt; 
   &lt;td&gt;4.50.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;datasets&lt;/td&gt; 
   &lt;td&gt;2.16.0&lt;/td&gt; 
   &lt;td&gt;3.2.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;accelerate&lt;/td&gt; 
   &lt;td&gt;0.34.0&lt;/td&gt; 
   &lt;td&gt;1.2.1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;peft&lt;/td&gt; 
   &lt;td&gt;0.14.0&lt;/td&gt; 
   &lt;td&gt;0.15.1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;trl&lt;/td&gt; 
   &lt;td&gt;0.8.6&lt;/td&gt; 
   &lt;td&gt;0.9.6&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Optional&lt;/th&gt; 
   &lt;th&gt;Minimum&lt;/th&gt; 
   &lt;th&gt;Recommend&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;CUDA&lt;/td&gt; 
   &lt;td&gt;11.6&lt;/td&gt; 
   &lt;td&gt;12.2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;deepspeed&lt;/td&gt; 
   &lt;td&gt;0.10.0&lt;/td&gt; 
   &lt;td&gt;0.16.4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;bitsandbytes&lt;/td&gt; 
   &lt;td&gt;0.39.0&lt;/td&gt; 
   &lt;td&gt;0.43.1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;vllm&lt;/td&gt; 
   &lt;td&gt;0.4.3&lt;/td&gt; 
   &lt;td&gt;0.8.2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;flash-attn&lt;/td&gt; 
   &lt;td&gt;2.5.6&lt;/td&gt; 
   &lt;td&gt;2.7.2&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Hardware Requirement&lt;/h3&gt; 
&lt;p&gt;* &lt;em&gt;estimated&lt;/em&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Method&lt;/th&gt; 
   &lt;th&gt;Bits&lt;/th&gt; 
   &lt;th&gt;7B&lt;/th&gt; 
   &lt;th&gt;14B&lt;/th&gt; 
   &lt;th&gt;30B&lt;/th&gt; 
   &lt;th&gt;70B&lt;/th&gt; 
   &lt;th&gt;&lt;code&gt;x&lt;/code&gt;B&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Full (&lt;code&gt;bf16&lt;/code&gt; or &lt;code&gt;fp16&lt;/code&gt;)&lt;/td&gt; 
   &lt;td&gt;32&lt;/td&gt; 
   &lt;td&gt;120GB&lt;/td&gt; 
   &lt;td&gt;240GB&lt;/td&gt; 
   &lt;td&gt;600GB&lt;/td&gt; 
   &lt;td&gt;1200GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;18x&lt;/code&gt;GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Full (&lt;code&gt;pure_bf16&lt;/code&gt;)&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;60GB&lt;/td&gt; 
   &lt;td&gt;120GB&lt;/td&gt; 
   &lt;td&gt;300GB&lt;/td&gt; 
   &lt;td&gt;600GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;8x&lt;/code&gt;GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Freeze/LoRA/GaLore/APOLLO/BAdam/OFT&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;16GB&lt;/td&gt; 
   &lt;td&gt;32GB&lt;/td&gt; 
   &lt;td&gt;64GB&lt;/td&gt; 
   &lt;td&gt;160GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;2x&lt;/code&gt;GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;QLoRA / QOFT&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;10GB&lt;/td&gt; 
   &lt;td&gt;20GB&lt;/td&gt; 
   &lt;td&gt;40GB&lt;/td&gt; 
   &lt;td&gt;80GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;x&lt;/code&gt;GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;QLoRA / QOFT&lt;/td&gt; 
   &lt;td&gt;4&lt;/td&gt; 
   &lt;td&gt;6GB&lt;/td&gt; 
   &lt;td&gt;12GB&lt;/td&gt; 
   &lt;td&gt;24GB&lt;/td&gt; 
   &lt;td&gt;48GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;x/2&lt;/code&gt;GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;QLoRA / QOFT&lt;/td&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;4GB&lt;/td&gt; 
   &lt;td&gt;8GB&lt;/td&gt; 
   &lt;td&gt;16GB&lt;/td&gt; 
   &lt;td&gt;24GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;x/4&lt;/code&gt;GB&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] Installation is mandatory.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Install from Source&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install -e ".[torch,metrics]" --no-build-isolation
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Extra dependencies available: torch, torch-npu, metrics, deepspeed, liger-kernel, bitsandbytes, hqq, eetq, gptq, aqlm, vllm, sglang, galore, apollo, badam, adam-mini, qwen, minicpm_v, openmind, swanlab, dev&lt;/p&gt; 
&lt;h4&gt;Install from Docker Image&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -it --rm --gpus=all --ipc=host hiyouga/llamafactory:latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This image is built on Ubuntu 22.04 (x86_64), CUDA 12.4, Python 3.11, PyTorch 2.6.0, and Flash-attn 2.7.4.&lt;/p&gt; 
&lt;p&gt;Find the pre-built images: &lt;a href="https://hub.docker.com/r/hiyouga/llamafactory/tags"&gt;https://hub.docker.com/r/hiyouga/llamafactory/tags&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Please refer to &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#build-docker"&gt;build docker&lt;/a&gt; to build the image yourself.&lt;/p&gt; 
&lt;details&gt;
 &lt;summary&gt;Setting up a virtual environment with &lt;b&gt;uv&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;Create an isolated Python environment with &lt;a href="https://github.com/astral-sh/uv"&gt;uv&lt;/a&gt;:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;uv sync --extra torch --extra metrics --prerelease=allow
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Run LLaMA-Factory in the isolated environment:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;uv run --prerelease=allow llamafactory-cli train examples/train_lora/llama3_lora_pretrain.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;For Windows users&lt;/summary&gt; 
 &lt;h4&gt;Install PyTorch&lt;/h4&gt; 
 &lt;p&gt;You need to manually install the GPU version of PyTorch on the Windows platform. Please refer to the &lt;a href="https://pytorch.org/get-started/locally/"&gt;official website&lt;/a&gt; and the following command to install PyTorch with CUDA support:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip uninstall torch torchvision torchaudio
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126
python -c "import torch; print(torch.cuda.is_available())"
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;If you see &lt;code&gt;True&lt;/code&gt; then you have successfully installed PyTorch with CUDA support.&lt;/p&gt; 
 &lt;p&gt;Try &lt;code&gt;dataloader_num_workers: 0&lt;/code&gt; if you encounter &lt;code&gt;Can't pickle local object&lt;/code&gt; error.&lt;/p&gt; 
 &lt;h4&gt;Install BitsAndBytes&lt;/h4&gt; 
 &lt;p&gt;If you want to enable the quantized LoRA (QLoRA) on the Windows platform, you need to install a pre-built version of &lt;code&gt;bitsandbytes&lt;/code&gt; library, which supports CUDA 11.1 to 12.2, please select the appropriate &lt;a href="https://github.com/jllllll/bitsandbytes-windows-webui/releases/tag/wheels"&gt;release version&lt;/a&gt; based on your CUDA version.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.2.post2-py3-none-win_amd64.whl
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Install Flash Attention-2&lt;/h4&gt; 
 &lt;p&gt;To enable FlashAttention-2 on the Windows platform, please use the script from &lt;a href="https://huggingface.co/lldacing/flash-attention-windows-wheel"&gt;flash-attention-windows-wheel&lt;/a&gt; to compile and install it by yourself.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;For Ascend NPU users&lt;/summary&gt; 
 &lt;p&gt;To install LLaMA Factory on Ascend NPU devices, please upgrade Python to version 3.10 or higher and specify extra dependencies: &lt;code&gt;pip install -e ".[torch-npu,metrics]"&lt;/code&gt;. Additionally, you need to install the &lt;strong&gt;&lt;a href="https://www.hiascend.com/developer/download/community/result?module=cann"&gt;Ascend CANN Toolkit and Kernels&lt;/a&gt;&lt;/strong&gt;. Please follow the &lt;a href="https://www.hiascend.com/document/detail/en/CANNCommunityEdition/600alphaX/softwareinstall/instg/atlasdeploy_03_0031.html"&gt;installation tutorial&lt;/a&gt; or use the following commands:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# replace the url according to your CANN version and devices
# install CANN Toolkit
wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C20SPC702/Ascend-cann-toolkit_8.0.0.alpha002_linux-"$(uname -i)".run
bash Ascend-cann-toolkit_8.0.0.alpha002_linux-"$(uname -i)".run --install

# install CANN Kernels
wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C20SPC702/Ascend-cann-kernels-910b_8.0.0.alpha002_linux-"$(uname -i)".run
bash Ascend-cann-kernels-910b_8.0.0.alpha002_linux-"$(uname -i)".run --install

# set env variables
source /usr/local/Ascend/ascend-toolkit/set_env.sh
&lt;/code&gt;&lt;/pre&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Requirement&lt;/th&gt; 
    &lt;th&gt;Minimum&lt;/th&gt; 
    &lt;th&gt;Recommend&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CANN&lt;/td&gt; 
    &lt;td&gt;8.0.RC1&lt;/td&gt; 
    &lt;td&gt;8.0.0.alpha002&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;torch&lt;/td&gt; 
    &lt;td&gt;2.1.0&lt;/td&gt; 
    &lt;td&gt;2.4.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;torch-npu&lt;/td&gt; 
    &lt;td&gt;2.1.0&lt;/td&gt; 
    &lt;td&gt;2.4.0.post2&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;deepspeed&lt;/td&gt; 
    &lt;td&gt;0.13.2&lt;/td&gt; 
    &lt;td&gt;0.13.2&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;vllm-ascend&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;0.7.3&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p&gt;Remember to use &lt;code&gt;ASCEND_RT_VISIBLE_DEVICES&lt;/code&gt; instead of &lt;code&gt;CUDA_VISIBLE_DEVICES&lt;/code&gt; to specify the device to use.&lt;/p&gt; 
 &lt;p&gt;If you cannot infer model on NPU devices, try setting &lt;code&gt;do_sample: false&lt;/code&gt; in the configurations.&lt;/p&gt; 
 &lt;p&gt;Download the pre-built Docker images: &lt;a href="http://mirrors.cn-central-221.ovaijisuan.com/detail/130.html"&gt;32GB&lt;/a&gt; | &lt;a href="http://mirrors.cn-central-221.ovaijisuan.com/detail/131.html"&gt;64GB&lt;/a&gt;&lt;/p&gt; 
 &lt;h4&gt;Install BitsAndBytes&lt;/h4&gt; 
 &lt;p&gt;To use QLoRA based on bitsandbytes on Ascend NPU, please follow these 3 steps:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Manually compile bitsandbytes: Refer to &lt;a href="https://huggingface.co/docs/bitsandbytes/installation?backend=Ascend+NPU&amp;amp;platform=Ascend+NPU"&gt;the installation documentation&lt;/a&gt; for the NPU version of bitsandbytes to complete the compilation and installation. The compilation requires a cmake version of at least 3.22.1 and a g++ version of at least 12.x.&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Install bitsandbytes from source
# Clone bitsandbytes repo, Ascend NPU backend is currently enabled on multi-backend-refactor branch
git clone -b multi-backend-refactor https://github.com/bitsandbytes-foundation/bitsandbytes.git
cd bitsandbytes/

# Install dependencies
pip install -r requirements-dev.txt

# Install the dependencies for the compilation tools. Note that the commands for this step may vary depending on the operating system. The following are provided for reference
apt-get install -y build-essential cmake

# Compile &amp;amp; install  
cmake -DCOMPUTE_BACKEND=npu -S .
make
pip install .
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="2"&gt; 
  &lt;li&gt;Install transformers from the main branch.&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;git clone -b main https://github.com/huggingface/transformers.git
cd transformers
pip install .
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="3"&gt; 
  &lt;li&gt;Set &lt;code&gt;double_quantization: false&lt;/code&gt; in the configuration. You can refer to the &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/train_qlora/llama3_lora_sft_bnb_npu.yaml"&gt;example&lt;/a&gt;.&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;h3&gt;Data Preparation&lt;/h3&gt; 
&lt;p&gt;Please refer to &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/data/README.md"&gt;data/README.md&lt;/a&gt; for checking the details about the format of dataset files. You can use datasets on HuggingFace / ModelScope / Modelers hub, load the dataset in local disk, or specify a path to s3/gcs cloud storage.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Please update &lt;code&gt;data/dataset_info.json&lt;/code&gt; to use your custom dataset.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;You can also use &lt;strong&gt;&lt;a href="https://github.com/ConardLi/easy-dataset"&gt;Easy Dataset&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://github.com/OpenDCAI/DataFlow"&gt;DataFlow&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href="https://github.com/open-sciencelab/GraphGen"&gt;GraphGen&lt;/a&gt;&lt;/strong&gt; to create synthetic data for fine-tuning.&lt;/p&gt; 
&lt;h3&gt;Quickstart&lt;/h3&gt; 
&lt;p&gt;Use the following 3 commands to run LoRA &lt;strong&gt;fine-tuning&lt;/strong&gt;, &lt;strong&gt;inference&lt;/strong&gt; and &lt;strong&gt;merging&lt;/strong&gt; of the Llama3-8B-Instruct model, respectively.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;llamafactory-cli train examples/train_lora/llama3_lora_sft.yaml
llamafactory-cli chat examples/inference/llama3_lora_sft.yaml
llamafactory-cli export examples/merge_lora/llama3_lora_sft.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples/README.md&lt;/a&gt; for advanced usage (including distributed training).&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] Use &lt;code&gt;llamafactory-cli help&lt;/code&gt; to show help information.&lt;/p&gt; 
 &lt;p&gt;Read &lt;a href="https://github.com/hiyouga/LLaMA-Factory/issues/4614"&gt;FAQs&lt;/a&gt; first if you encounter any problems.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Fine-Tuning with LLaMA Board GUI (powered by &lt;a href="https://github.com/gradio-app/gradio"&gt;Gradio&lt;/a&gt;)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;llamafactory-cli webui
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;LLaMA Factory Online&lt;/h3&gt; 
&lt;p&gt;Read our &lt;a href="https://docs.llamafactory.com.cn/docs/documents/quickstart/getstarted/?utm_source=LLaMA-Factory"&gt;documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Build Docker&lt;/h3&gt; 
&lt;p&gt;For CUDA users:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd docker/docker-cuda/
docker compose up -d
docker compose exec llamafactory bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For Ascend NPU users:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd docker/docker-npu/
docker compose up -d
docker compose exec llamafactory bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For AMD ROCm users:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd docker/docker-rocm/
docker compose up -d
docker compose exec llamafactory bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt;
 &lt;summary&gt;Build without Docker Compose&lt;/summary&gt; 
 &lt;p&gt;For CUDA users:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;docker build -f ./docker/docker-cuda/Dockerfile \
    --build-arg PIP_INDEX=https://pypi.org/simple \
    --build-arg EXTRAS=metrics \
    -t llamafactory:latest .

docker run -dit --ipc=host --gpus=all \
    -p 7860:7860 \
    -p 8000:8000 \
    --name llamafactory \
    llamafactory:latest

docker exec -it llamafactory bash
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;For Ascend NPU users:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;docker build -f ./docker/docker-npu/Dockerfile \
    --build-arg PIP_INDEX=https://pypi.org/simple \
    --build-arg EXTRAS=torch-npu,metrics \
    -t llamafactory:latest .

docker run -dit --ipc=host \
    -v /usr/local/dcmi:/usr/local/dcmi \
    -v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \
    -v /usr/local/Ascend/driver:/usr/local/Ascend/driver \
    -v /etc/ascend_install.info:/etc/ascend_install.info \
    -p 7860:7860 \
    -p 8000:8000 \
    --device /dev/davinci0 \
    --device /dev/davinci_manager \
    --device /dev/devmm_svm \
    --device /dev/hisi_hdc \
    --name llamafactory \
    llamafactory:latest

docker exec -it llamafactory bash
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;For AMD ROCm users:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;docker build -f ./docker/docker-rocm/Dockerfile \
    --build-arg PIP_INDEX=https://pypi.org/simple \
    --build-arg EXTRAS=metrics \
    -t llamafactory:latest .

docker run -dit --ipc=host \
    -p 7860:7860 \
    -p 8000:8000 \
    --device /dev/kfd \
    --device /dev/dri \
    --name llamafactory \
    llamafactory:latest

docker exec -it llamafactory bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Use Docker volumes&lt;/summary&gt; 
 &lt;p&gt;You can uncomment &lt;code&gt;VOLUME [ "/root/.cache/huggingface", "/app/shared_data", "/app/output" ]&lt;/code&gt; in the Dockerfile to use data volumes.&lt;/p&gt; 
 &lt;p&gt;When building the Docker image, use &lt;code&gt;-v ./hf_cache:/root/.cache/huggingface&lt;/code&gt; argument to mount the local directory to the container. The following data volumes are available.&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;hf_cache&lt;/code&gt;: Utilize Hugging Face cache on the host machine.&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;shared_data&lt;/code&gt;: The directionary to store datasets on the host machine.&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;output&lt;/code&gt;: Set export dir to this location so that the merged result can be accessed directly on the host machine.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;Deploy with OpenAI-style API and vLLM&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;API_PORT=8000 llamafactory-cli api examples/inference/llama3.yaml infer_backend=vllm vllm_enforce_eager=true
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] Visit &lt;a href="https://platform.openai.com/docs/api-reference/chat/create"&gt;this page&lt;/a&gt; for API document.&lt;/p&gt; 
 &lt;p&gt;Examples: &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/scripts/api_example/test_image.py"&gt;Image understanding&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/scripts/api_example/test_toolcall.py"&gt;Function calling&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Download from ModelScope Hub&lt;/h3&gt; 
&lt;p&gt;If you have trouble with downloading models and datasets from Hugging Face, you can use ModelScope.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export USE_MODELSCOPE_HUB=1 # `set USE_MODELSCOPE_HUB=1` for Windows
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Train the model by specifying a model ID of the ModelScope Hub as the &lt;code&gt;model_name_or_path&lt;/code&gt;. You can find a full list of model IDs at &lt;a href="https://modelscope.cn/models"&gt;ModelScope Hub&lt;/a&gt;, e.g., &lt;code&gt;LLM-Research/Meta-Llama-3-8B-Instruct&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Download from Modelers Hub&lt;/h3&gt; 
&lt;p&gt;You can also use Modelers Hub to download models and datasets.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export USE_OPENMIND_HUB=1 # `set USE_OPENMIND_HUB=1` for Windows
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Train the model by specifying a model ID of the Modelers Hub as the &lt;code&gt;model_name_or_path&lt;/code&gt;. You can find a full list of model IDs at &lt;a href="https://modelers.cn/models"&gt;Modelers Hub&lt;/a&gt;, e.g., &lt;code&gt;TeleAI/TeleChat-7B-pt&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Use W&amp;amp;B Logger&lt;/h3&gt; 
&lt;p&gt;To use &lt;a href="https://wandb.ai"&gt;Weights &amp;amp; Biases&lt;/a&gt; for logging experimental results, you need to add the following arguments to yaml files.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;report_to: wandb
run_name: test_run # optional
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Set &lt;code&gt;WANDB_API_KEY&lt;/code&gt; to &lt;a href="https://wandb.ai/authorize"&gt;your key&lt;/a&gt; when launching training tasks to log in with your W&amp;amp;B account.&lt;/p&gt; 
&lt;h3&gt;Use SwanLab Logger&lt;/h3&gt; 
&lt;p&gt;To use &lt;a href="https://github.com/SwanHubX/SwanLab"&gt;SwanLab&lt;/a&gt; for logging experimental results, you need to add the following arguments to yaml files.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;use_swanlab: true
swanlab_run_name: test_run # optional
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;When launching training tasks, you can log in to SwanLab in three ways:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Add &lt;code&gt;swanlab_api_key=&amp;lt;your_api_key&amp;gt;&lt;/code&gt; to the yaml file, and set it to your &lt;a href="https://swanlab.cn/settings"&gt;API key&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Set the environment variable &lt;code&gt;SWANLAB_API_KEY&lt;/code&gt; to your &lt;a href="https://swanlab.cn/settings"&gt;API key&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Use the &lt;code&gt;swanlab login&lt;/code&gt; command to complete the login.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Projects using LLaMA Factory&lt;/h2&gt; 
&lt;p&gt;If you have a project that should be incorporated, please contact via email or create a pull request.&lt;/p&gt; 
&lt;details&gt;
 &lt;summary&gt;Click to show&lt;/summary&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Wang et al. ESRL: Efficient Sampling-based Reinforcement Learning for Sequence Generation. 2023. &lt;a href="https://arxiv.org/abs/2308.02223"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Yu et al. Open, Closed, or Small Language Models for Text Classification? 2023. &lt;a href="https://arxiv.org/abs/2308.10092"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Wang et al. UbiPhysio: Support Daily Functioning, Fitness, and Rehabilitation with Action Understanding and Feedback in Natural Language. 2023. &lt;a href="https://arxiv.org/abs/2308.10526"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Luceri et al. Leveraging Large Language Models to Detect Influence Campaigns in Social Media. 2023. &lt;a href="https://arxiv.org/abs/2311.07816"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zhang et al. Alleviating Hallucinations of Large Language Models through Induced Hallucinations. 2023. &lt;a href="https://arxiv.org/abs/2312.15710"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Wang et al. Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs. KDD 2024. &lt;a href="https://arxiv.org/abs/2401.04319"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Wang et al. CANDLE: Iterative Conceptualization and Instantiation Distillation from Large Language Models for Commonsense Reasoning. ACL 2024. &lt;a href="https://arxiv.org/abs/2401.07286"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Choi et al. FACT-GPT: Fact-Checking Augmentation via Claim Matching with LLMs. 2024. &lt;a href="https://arxiv.org/abs/2402.05904"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zhang et al. AutoMathText: Autonomous Data Selection with Language Models for Mathematical Texts. 2024. &lt;a href="https://arxiv.org/abs/2402.07625"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Lyu et al. KnowTuning: Knowledge-aware Fine-tuning for Large Language Models. 2024. &lt;a href="https://arxiv.org/abs/2402.11176"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Yang et al. LaCo: Large Language Model Pruning via Layer Collaps. 2024. &lt;a href="https://arxiv.org/abs/2402.11187"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Bhardwaj et al. Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic. 2024. &lt;a href="https://arxiv.org/abs/2402.11746"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Yang et al. Enhancing Empathetic Response Generation by Augmenting LLMs with Small-scale Empathetic Models. 2024. &lt;a href="https://arxiv.org/abs/2402.11801"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Yi et al. Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding. ACL 2024 Findings. &lt;a href="https://arxiv.org/abs/2402.11809"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Cao et al. Head-wise Shareable Attention for Large Language Models. 2024. &lt;a href="https://arxiv.org/abs/2402.11819"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zhang et al. Enhancing Multilingual Capabilities of Large Language Models through Self-Distillation from Resource-Rich Languages. 2024. &lt;a href="https://arxiv.org/abs/2402.12204"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Kim et al. Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models. 2024. &lt;a href="https://arxiv.org/abs/2402.14714"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Yu et al. KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models. ACL 2024. &lt;a href="https://arxiv.org/abs/2402.15043"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Huang et al. Key-Point-Driven Data Synthesis with its Enhancement on Mathematical Reasoning. 2024. &lt;a href="https://arxiv.org/abs/2403.02333"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Duan et al. Negating Negatives: Alignment without Human Positive Samples via Distributional Dispreference Optimization. 2024. &lt;a href="https://arxiv.org/abs/2403.03419"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Xie and Schwertfeger. Empowering Robotics with Large Language Models: osmAG Map Comprehension with LLMs. 2024. &lt;a href="https://arxiv.org/abs/2403.08228"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Wu et al. Large Language Models are Parallel Multilingual Learners. 2024. &lt;a href="https://arxiv.org/abs/2403.09073"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zhang et al. EDT: Improving Large Language Models' Generation by Entropy-based Dynamic Temperature Sampling. 2024. &lt;a href="https://arxiv.org/abs/2403.14541"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Weller et al. FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions. 2024. &lt;a href="https://arxiv.org/abs/2403.15246"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Hongbin Na. CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral Therapy-based Mental Health Question Answering. COLING 2024. &lt;a href="https://arxiv.org/abs/2403.16008"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zan et al. CodeS: Natural Language to Code Repository via Multi-Layer Sketch. 2024. &lt;a href="https://arxiv.org/abs/2403.16443"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Liu et al. Extensive Self-Contrast Enables Feedback-Free Language Model Alignment. 2024. &lt;a href="https://arxiv.org/abs/2404.00604"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Luo et al. BAdam: A Memory Efficient Full Parameter Training Method for Large Language Models. 2024. &lt;a href="https://arxiv.org/abs/2404.02827"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Du et al. Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model. 2024. &lt;a href="https://arxiv.org/abs/2404.04167"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Ma et al. Parameter Efficient Quasi-Orthogonal Fine-Tuning via Givens Rotation. ICML 2024. &lt;a href="https://arxiv.org/abs/2404.04316"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Liu et al. Dynamic Generation of Personalities with Large Language Models. 2024. &lt;a href="https://arxiv.org/abs/2404.07084"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Shang et al. How Far Have We Gone in Stripped Binary Code Understanding Using Large Language Models. 2024. &lt;a href="https://arxiv.org/abs/2404.09836"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Huang et al. LLMTune: Accelerate Database Knob Tuning with Large Language Models. 2024. &lt;a href="https://arxiv.org/abs/2404.11581"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Deng et al. Text-Tuple-Table: Towards Information Integration in Text-to-Table Generation via Global Tuple Extraction. 2024. &lt;a href="https://arxiv.org/abs/2404.14215"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Acikgoz et al. Hippocrates: An Open-Source Framework for Advancing Large Language Models in Healthcare. 2024. &lt;a href="https://arxiv.org/abs/2404.16621"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zhang et al. Small Language Models Need Strong Verifiers to Self-Correct Reasoning. ACL 2024 Findings. &lt;a href="https://arxiv.org/abs/2404.17140"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zhou et al. FREB-TQA: A Fine-Grained Robustness Evaluation Benchmark for Table Question Answering. NAACL 2024. &lt;a href="https://arxiv.org/abs/2404.18585"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Xu et al. Large Language Models for Cyber Security: A Systematic Literature Review. 2024. &lt;a href="https://arxiv.org/abs/2405.04760"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Dammu et al. "They are uncultured": Unveiling Covert Harms and Social Threats in LLM Generated Conversations. 2024. &lt;a href="https://arxiv.org/abs/2405.05378"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Yi et al. A safety realignment framework via subspace-oriented model fusion for large language models. 2024. &lt;a href="https://arxiv.org/abs/2405.09055"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Lou et al. SPO: Multi-Dimensional Preference Sequential Alignment With Implicit Reward Modeling. 2024. &lt;a href="https://arxiv.org/abs/2405.12739"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zhang et al. Getting More from Less: Large Language Models are Good Spontaneous Multilingual Learners. 2024. &lt;a href="https://arxiv.org/abs/2405.13816"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zhang et al. TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models. 2024. &lt;a href="https://arxiv.org/abs/2405.20215"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zihong Chen. Sentence Segmentation and Sentence Punctuation Based on XunziALLM. 2024. &lt;a href="https://aclanthology.org/2024.lt4hala-1.30"&gt;[paper]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Gao et al. The Best of Both Worlds: Toward an Honest and Helpful Large Language Model. 2024. &lt;a href="https://arxiv.org/abs/2406.00380"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Wang and Song. MARS: Benchmarking the Metaphysical Reasoning Abilities of Language Models with a Multi-task Evaluation Dataset. 2024. &lt;a href="https://arxiv.org/abs/2406.02106"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Hu et al. Computational Limits of Low-Rank Adaptation (LoRA) for Transformer-Based Models. 2024. &lt;a href="https://arxiv.org/abs/2406.03136"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Ge et al. Time Sensitive Knowledge Editing through Efficient Finetuning. ACL 2024. &lt;a href="https://arxiv.org/abs/2406.04496"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Tan et al. Peer Review as A Multi-Turn and Long-Context Dialogue with Role-Based Interactions. 2024. &lt;a href="https://arxiv.org/abs/2406.05688"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Song et al. Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters. 2024. &lt;a href="https://arxiv.org/abs/2406.05955"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Gu et al. RWKV-CLIP: A Robust Vision-Language Representation Learner. 2024. &lt;a href="https://arxiv.org/abs/2406.06973"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Chen et al. Advancing Tool-Augmented Large Language Models: Integrating Insights from Errors in Inference Trees. 2024. &lt;a href="https://arxiv.org/abs/2406.07115"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zhu et al. Are Large Language Models Good Statisticians?. 2024. &lt;a href="https://arxiv.org/abs/2406.07815"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Li et al. Know the Unknown: An Uncertainty-Sensitive Method for LLM Instruction Tuning. 2024. &lt;a href="https://arxiv.org/abs/2406.10099"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Ding et al. IntentionQA: A Benchmark for Evaluating Purchase Intention Comprehension Abilities of Language Models in E-commerce. 2024. &lt;a href="https://arxiv.org/abs/2406.10173"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;He et al. COMMUNITY-CROSS-INSTRUCT: Unsupervised Instruction Generation for Aligning Large Language Models to Online Communities. 2024. &lt;a href="https://arxiv.org/abs/2406.12074"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Lin et al. FVEL: Interactive Formal Verification Environment with Large Language Models via Theorem Proving. 2024. &lt;a href="https://arxiv.org/abs/2406.14408"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Treutlein et al. Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from Disparate Training Data. 2024. &lt;a href="https://arxiv.org/abs/2406.14546"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Feng et al. SS-Bench: A Benchmark for Social Story Generation and Evaluation. 2024. &lt;a href="https://arxiv.org/abs/2406.15695"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Feng et al. Self-Constructed Context Decompilation with Fined-grained Alignment Enhancement. 2024. &lt;a href="https://arxiv.org/abs/2406.17233"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Liu et al. Large Language Models for Cuffless Blood Pressure Measurement From Wearable Biosignals. 2024. &lt;a href="https://arxiv.org/abs/2406.18069"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Iyer et al. Exploring Very Low-Resource Translation with LLMs: The University of Edinburgh's Submission to AmericasNLP 2024 Translation Task. AmericasNLP 2024. &lt;a href="https://aclanthology.org/2024.americasnlp-1.25"&gt;[paper]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Li et al. Calibrating LLMs with Preference Optimization on Thought Trees for Generating Rationale in Science Question Scoring. 2024. &lt;a href="https://arxiv.org/abs/2406.19949"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Yang et al. Financial Knowledge Large Language Model. 2024. &lt;a href="https://arxiv.org/abs/2407.00365"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Lin et al. DogeRM: Equipping Reward Models with Domain Knowledge through Model Merging. 2024. &lt;a href="https://arxiv.org/abs/2407.01470"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Bako et al. Evaluating the Semantic Profiling Abilities of LLMs for Natural Language Utterances in Data Visualization. 2024. &lt;a href="https://arxiv.org/abs/2407.06129"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Huang et al. RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization. 2024. &lt;a href="https://arxiv.org/abs/2407.08044"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Jiang et al. LLM-Collaboration on Automatic Science Journalism for the General Audience. 2024. &lt;a href="https://arxiv.org/abs/2407.09756"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Inouye et al. Applied Auto-tuning on LoRA Hyperparameters. 2024. &lt;a href="https://scholarcommons.scu.edu/cseng_senior/272/"&gt;[paper]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Qi et al. Research on Tibetan Tourism Viewpoints information generation system based on LLM. 2024. &lt;a href="https://arxiv.org/abs/2407.13561"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Xu et al. Course-Correction: Safety Alignment Using Synthetic Preferences. 2024. &lt;a href="https://arxiv.org/abs/2407.16637"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Sun et al. LAMBDA: A Large Model Based Data Agent. 2024. &lt;a href="https://arxiv.org/abs/2407.17535"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zhu et al. CollectiveSFT: Scaling Large Language Models for Chinese Medical Benchmark with Collective Instructions in Healthcare. 2024. &lt;a href="https://arxiv.org/abs/2407.19705"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Yu et al. Correcting Negative Bias in Large Language Models through Negative Attention Score Alignment. 2024. &lt;a href="https://arxiv.org/abs/2408.00137"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Xie et al. The Power of Personalized Datasets: Advancing Chinese Composition Writing for Elementary School through Targeted Model Fine-Tuning. IALP 2024. &lt;a href="https://www.asianlp.sg/conferences/ialp2024/proceedings/papers/IALP2024_P055.pdf"&gt;[paper]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Liu et al. Instruct-Code-Llama: Improving Capabilities of Language Model in Competition Level Code Generation by Online Judge Feedback. ICIC 2024. &lt;a href="https://link.springer.com/chapter/10.1007/978-981-97-5669-8_11"&gt;[paper]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Wang et al. Cybernetic Sentinels: Unveiling the Impact of Safety Data Selection on Model Security in Supervised Fine-Tuning. ICIC 2024. &lt;a href="https://link.springer.com/chapter/10.1007/978-981-97-5669-8_23"&gt;[paper]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Xia et al. Understanding the Performance and Estimating the Cost of LLM Fine-Tuning. 2024. &lt;a href="https://arxiv.org/abs/2408.04693"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zeng et al. Perceive, Reflect, and Plan: Designing LLM Agent for Goal-Directed City Navigation without Instructions. 2024. &lt;a href="https://arxiv.org/abs/2408.04168"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Xia et al. Using Pre-trained Language Model for Accurate ESG Prediction. FinNLP 2024. &lt;a href="https://aclanthology.org/2024.finnlp-2.1/"&gt;[paper]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Liang et al. I-SHEEP: Self-Alignment of LLM from Scratch through an Iterative Self-Enhancement Paradigm. 2024. &lt;a href="https://arxiv.org/abs/2408.08072"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Bai et al. Aligning Large Language Model with Direct Multi-Preference Optimization for Recommendation. CIKM 2024. &lt;a href="https://dl.acm.org/doi/10.1145/3627673.3679611"&gt;[paper]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zhang et al. CPsyCoun: A Report-based Multi-turn Dialogue Reconstruction and Evaluation Framework for Chinese Psychological Counseling. ACL 2024. &lt;a href="https://aclanthology.org/2024.findings-acl.830.pdf"&gt;[paper]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/Yu-Yang-Li/StarWhisper"&gt;StarWhisper&lt;/a&gt;&lt;/strong&gt;: A large language model for Astronomy, based on ChatGLM2-6B and Qwen-14B.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/FudanDISC/DISC-LawLLM"&gt;DISC-LawLLM&lt;/a&gt;&lt;/strong&gt;: A large language model specialized in Chinese legal domain, based on Baichuan-13B, is capable of retrieving and reasoning on legal knowledge.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/X-D-Lab/Sunsimiao"&gt;Sunsimiao&lt;/a&gt;&lt;/strong&gt;: A large language model specialized in Chinese medical domain, based on Baichuan-7B and ChatGLM-6B.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/WangRongsheng/CareGPT"&gt;CareGPT&lt;/a&gt;&lt;/strong&gt;: A series of large language models for Chinese medical domain, based on LLaMA2-7B and Baichuan-13B.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/PKU-YuanGroup/Machine-Mindset/"&gt;MachineMindset&lt;/a&gt;&lt;/strong&gt;: A series of MBTI Personality large language models, capable of giving any LLM 16 different personality types based on different datasets and training methods.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://huggingface.co/Nekochu/Luminia-13B-v3"&gt;Luminia-13B-v3&lt;/a&gt;&lt;/strong&gt;: A large language model specialized in generate metadata for stable diffusion. &lt;a href="https://huggingface.co/spaces/Nekochu/Luminia-13B_SD_Prompt"&gt;[demo]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/BUAADreamer/Chinese-LLaVA-Med"&gt;Chinese-LLaVA-Med&lt;/a&gt;&lt;/strong&gt;: A multimodal large language model specialized in Chinese medical domain, based on LLaVA-1.5-7B.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/THUDM/AutoRE"&gt;AutoRE&lt;/a&gt;&lt;/strong&gt;: A document-level relation extraction system based on large language models.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/NVIDIA/RTX-AI-Toolkit"&gt;NVIDIA RTX AI Toolkit&lt;/a&gt;&lt;/strong&gt;: SDKs for fine-tuning LLMs on Windows PC for NVIDIA RTX.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/LazyAGI/LazyLLM"&gt;LazyLLM&lt;/a&gt;&lt;/strong&gt;: An easy and lazy way for building multi-agent LLMs applications and supports model fine-tuning via LLaMA Factory.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/NLPJCL/RAG-Retrieval"&gt;RAG-Retrieval&lt;/a&gt;&lt;/strong&gt;: A full pipeline for RAG retrieval model fine-tuning, inference, and distillation. &lt;a href="https://zhuanlan.zhihu.com/p/987727357"&gt;[blog]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/Qihoo360/360-LLaMA-Factory"&gt;360-LLaMA-Factory&lt;/a&gt;&lt;/strong&gt;: A modified library that supports long sequence SFT &amp;amp; DPO using ring attention.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://novasky-ai.github.io/posts/sky-t1/"&gt;Sky-T1&lt;/a&gt;&lt;/strong&gt;: An o1-like model fine-tuned by NovaSky AI with very small cost.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/xming521/WeClone"&gt;WeClone&lt;/a&gt;&lt;/strong&gt;: One-stop solution for creating your digital avatar from chat logs.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/SmartFlowAI/EmoLLM"&gt;EmoLLM&lt;/a&gt;&lt;/strong&gt;: A project about large language models (LLMs) and mental health.&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This repository is licensed under the &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/LICENSE"&gt;Apache-2.0 License&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Please follow the model licenses to use the corresponding model weights: &lt;a href="https://huggingface.co/baichuan-inc/Baichuan2-7B-Base/blob/main/Community%20License%20for%20Baichuan%202%20Model.pdf"&gt;Baichuan 2&lt;/a&gt; / &lt;a href="https://huggingface.co/spaces/bigscience/license"&gt;BLOOM&lt;/a&gt; / &lt;a href="https://github.com/THUDM/ChatGLM3/raw/main/MODEL_LICENSE"&gt;ChatGLM3&lt;/a&gt; / &lt;a href="https://cohere.com/c4ai-cc-by-nc-license"&gt;Command R&lt;/a&gt; / &lt;a href="https://github.com/deepseek-ai/DeepSeek-LLM/raw/main/LICENSE-MODEL"&gt;DeepSeek&lt;/a&gt; / &lt;a href="https://huggingface.co/tiiuae/falcon-180B/blob/main/LICENSE.txt"&gt;Falcon&lt;/a&gt; / &lt;a href="https://ai.google.dev/gemma/terms"&gt;Gemma&lt;/a&gt; / &lt;a href="https://huggingface.co/THUDM/glm-4-9b/blob/main/LICENSE"&gt;GLM-4&lt;/a&gt; / &lt;a href="https://github.com/openai/gpt-2/raw/master/LICENSE"&gt;GPT-2&lt;/a&gt; / &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/LICENSE"&gt;Granite&lt;/a&gt; / &lt;a href="https://huggingface.co/IndexTeam/Index-1.9B/blob/main/LICENSE"&gt;Index&lt;/a&gt; / &lt;a href="https://github.com/InternLM/InternLM#license"&gt;InternLM&lt;/a&gt; / &lt;a href="https://github.com/facebookresearch/llama/raw/main/MODEL_CARD.md"&gt;Llama&lt;/a&gt; / &lt;a href="https://ai.meta.com/llama/license/"&gt;Llama 2&lt;/a&gt; / &lt;a href="https://llama.meta.com/llama3/license/"&gt;Llama 3&lt;/a&gt; / &lt;a href="https://github.com/meta-llama/llama-models/raw/main/models/llama4/LICENSE"&gt;Llama 4&lt;/a&gt; / &lt;a href="https://github.com/OpenBMB/MiniCPM/raw/main/MiniCPM%20Model%20License.md"&gt;MiniCPM&lt;/a&gt; / &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/LICENSE"&gt;Mistral/Mixtral/Pixtral&lt;/a&gt; / &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/LICENSE"&gt;OLMo&lt;/a&gt; / &lt;a href="https://huggingface.co/microsoft/phi-1_5/resolve/main/Research%20License.docx"&gt;Phi-1.5/Phi-2&lt;/a&gt; / &lt;a href="https://huggingface.co/microsoft/Phi-3-mini-4k-instruct/blob/main/LICENSE"&gt;Phi-3/Phi-4&lt;/a&gt; / &lt;a href="https://github.com/QwenLM/Qwen/raw/main/Tongyi%20Qianwen%20LICENSE%20AGREEMENT"&gt;Qwen&lt;/a&gt; / &lt;a href="https://huggingface.co/Skywork/Skywork-13B-base/blob/main/Skywork%20Community%20License.pdf"&gt;Skywork&lt;/a&gt; / &lt;a href="https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement"&gt;StarCoder 2&lt;/a&gt; / &lt;a href="https://huggingface.co/Tele-AI/telechat-7B/blob/main/TeleChat%E6%A8%A1%E5%9E%8B%E7%A4%BE%E5%8C%BA%E8%AE%B8%E5%8F%AF%E5%8D%8F%E8%AE%AE.pdf"&gt;TeleChat2&lt;/a&gt; / &lt;a href="https://github.com/xverse-ai/XVERSE-13B/raw/main/MODEL_LICENSE.pdf"&gt;XVERSE&lt;/a&gt; / &lt;a href="https://huggingface.co/01-ai/Yi-6B/blob/main/LICENSE"&gt;Yi&lt;/a&gt; / &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/LICENSE"&gt;Yi-1.5&lt;/a&gt; / &lt;a href="https://github.com/IEIT-Yuan/Yuan-2.0/raw/main/LICENSE-Yuan"&gt;Yuan 2&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If this work is helpful, please kindly cite as:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@inproceedings{zheng2024llamafactory,
  title={LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models},
  author={Yaowei Zheng and Richong Zhang and Junhao Zhang and Yanhan Ye and Zheyan Luo and Zhangchi Feng and Yongqiang Ma},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)},
  address={Bangkok, Thailand},
  publisher={Association for Computational Linguistics},
  year={2024},
  url={http://arxiv.org/abs/2403.13372}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Acknowledgement&lt;/h2&gt; 
&lt;p&gt;This repo benefits from &lt;a href="https://github.com/huggingface/peft"&gt;PEFT&lt;/a&gt;, &lt;a href="https://github.com/huggingface/trl"&gt;TRL&lt;/a&gt;, &lt;a href="https://github.com/artidoro/qlora"&gt;QLoRA&lt;/a&gt; and &lt;a href="https://github.com/lm-sys/FastChat"&gt;FastChat&lt;/a&gt;. Thanks for their wonderful works.&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://api.star-history.com/svg?repos=hiyouga/LLaMA-Factory&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>