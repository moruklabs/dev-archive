<rss version="2.0">
  <channel>
    <title>GitHub All Languages Weekly Trending</title>
    <description>Weekly Trending of All Languages in GitHub</description>
    <pubDate>Fri, 15 Aug 2025 01:41:50 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>menloresearch/jan</title>
      <link>https://github.com/menloresearch/jan</link>
      <description>&lt;p&gt;Jan is an open source alternative to ChatGPT that runs 100% offline on your computer&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Jan - Local AI Assistant&lt;/h1&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/menloresearch/jan/dev/docs/src/pages/docs/_assets/jan-app.png" alt="Jan AI" /&gt;&lt;/p&gt; 
&lt;p align="center"&gt; 
 &lt;!-- ALL-CONTRIBUTORS-BADGE:START - Do not remove or modify this section --&gt; &lt;img alt="GitHub commit activity" src="https://img.shields.io/github/commit-activity/m/menloresearch/jan" /&gt; &lt;img alt="Github Last Commit" src="https://img.shields.io/github/last-commit/menloresearch/jan" /&gt; &lt;img alt="Github Contributors" src="https://img.shields.io/github/contributors/menloresearch/jan" /&gt; &lt;img alt="GitHub closed issues" src="https://img.shields.io/github/issues-closed/menloresearch/jan" /&gt; &lt;img alt="Discord" src="https://img.shields.io/discord/1107178041848909847?label=discord" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://jan.ai/docs/quickstart"&gt;Getting Started&lt;/a&gt; - &lt;a href="https://jan.ai/docs"&gt;Docs&lt;/a&gt; - &lt;a href="https://jan.ai/changelog"&gt;Changelog&lt;/a&gt; - &lt;a href="https://github.com/menloresearch/jan/issues"&gt;Bug reports&lt;/a&gt; - &lt;a href="https://discord.gg/AsJ8krTT3N"&gt;Discord&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;Jan is an AI assistant that can run 100% offline on your device. Download and run LLMs with &lt;strong&gt;full control&lt;/strong&gt; and &lt;strong&gt;privacy&lt;/strong&gt;.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;The easiest way to get started is by downloading one of the following versions for your respective operating system:&lt;/p&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;Platform&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;b&gt;Stable&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;b&gt;Nightly&lt;/b&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;Windows&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://app.jan.ai/download/latest/win-x64"&gt;jan.exe&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://app.jan.ai/download/nightly/win-x64"&gt;jan.exe&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;macOS&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://app.jan.ai/download/latest/mac-universal"&gt;jan.dmg&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://app.jan.ai/download/nightly/mac-universal"&gt;jan.dmg&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;Linux (deb)&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://app.jan.ai/download/latest/linux-amd64-deb"&gt;jan.deb&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://app.jan.ai/download/nightly/linux-amd64-deb"&gt;jan.deb&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;Linux (AppImage)&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://app.jan.ai/download/latest/linux-amd64-appimage"&gt;jan.AppImage&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://app.jan.ai/download/nightly/linux-amd64-appimage"&gt;jan.AppImage&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;p&gt;Download from &lt;a href="https://jan.ai/"&gt;jan.ai&lt;/a&gt; or &lt;a href="https://github.com/menloresearch/jan/releases"&gt;GitHub Releases&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Local AI Models&lt;/strong&gt;: Download and run LLMs (Llama, Gemma, Qwen, etc.) from HuggingFace&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Cloud Integration&lt;/strong&gt;: Connect to OpenAI, Anthropic, Mistral, Groq, and others&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Custom Assistants&lt;/strong&gt;: Create specialized AI assistants for your tasks&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;OpenAI-Compatible API&lt;/strong&gt;: Local server at &lt;code&gt;localhost:1337&lt;/code&gt; for other applications&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Model Context Protocol&lt;/strong&gt;: MCP integration for enhanced capabilities&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Privacy First&lt;/strong&gt;: Everything runs locally when you want it to&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Build from Source&lt;/h2&gt; 
&lt;p&gt;For those who enjoy the scenic route:&lt;/p&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Node.js ≥ 20.0.0&lt;/li&gt; 
 &lt;li&gt;Yarn ≥ 1.22.0&lt;/li&gt; 
 &lt;li&gt;Make ≥ 3.81&lt;/li&gt; 
 &lt;li&gt;Rust (for Tauri)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Run with Make&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/menloresearch/jan
cd jan
make dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This handles everything: installs dependencies, builds core components, and launches the app.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Available make targets:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;make dev&lt;/code&gt; - Full development setup and launch&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make build&lt;/code&gt; - Production build&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make test&lt;/code&gt; - Run tests and linting&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make clean&lt;/code&gt; - Delete everything and start fresh&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Run with Mise (easier)&lt;/h3&gt; 
&lt;p&gt;You can also run with &lt;a href="https://mise.jdx.dev/"&gt;mise&lt;/a&gt;, which is a bit easier as it ensures Node.js, Rust, and other dependency versions are automatically managed:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/menloresearch/jan
cd jan

# Install mise (if not already installed)
curl https://mise.run | sh

# Install tools and start development
mise install    # installs Node.js, Rust, and other tools
mise dev        # runs the full development setup
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Available mise commands:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;mise dev&lt;/code&gt; - Full development setup and launch&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;mise build&lt;/code&gt; - Production build&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;mise test&lt;/code&gt; - Run tests and linting&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;mise clean&lt;/code&gt; - Delete everything and start fresh&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;mise tasks&lt;/code&gt; - List all available tasks&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Manual Commands&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;yarn install
yarn build:core
yarn build:extensions
yarn dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;System Requirements&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Minimum specs for a decent experience:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;macOS&lt;/strong&gt;: 13.6+ (8GB RAM for 3B models, 16GB for 7B, 32GB for 13B)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt;: 10+ with GPU support for NVIDIA/AMD/Intel Arc&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Linux&lt;/strong&gt;: Most distributions work, GPU acceleration available&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For detailed compatibility, check our &lt;a href="https://jan.ai/docs/desktop/mac"&gt;installation guides&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Troubleshooting&lt;/h2&gt; 
&lt;p&gt;If things go sideways:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Check our &lt;a href="https://jan.ai/docs/troubleshooting"&gt;troubleshooting docs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Copy your error logs and system specs&lt;/li&gt; 
 &lt;li&gt;Ask for help in our &lt;a href="https://discord.gg/FTk2MvZwJH"&gt;Discord&lt;/a&gt; &lt;code&gt;#🆘|jan-help&lt;/code&gt; channel&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions welcome. See &lt;a href="https://raw.githubusercontent.com/menloresearch/jan/dev/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for the full spiel.&lt;/p&gt; 
&lt;h2&gt;Links&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://jan.ai/docs"&gt;Documentation&lt;/a&gt; - The manual you should read&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://jan.ai/api-reference"&gt;API Reference&lt;/a&gt; - For the technically inclined&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://jan.ai/changelog"&gt;Changelog&lt;/a&gt; - What we broke and fixed&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://discord.gg/FTk2MvZwJH"&gt;Discord&lt;/a&gt; - Where the community lives&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contact&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Bugs&lt;/strong&gt;: &lt;a href="https://github.com/menloresearch/jan/issues"&gt;GitHub Issues&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Business&lt;/strong&gt;: &lt;a href="mailto:hello@jan.ai"&gt;hello@jan.ai&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Jobs&lt;/strong&gt;: &lt;a href="mailto:hr@jan.ai"&gt;hr@jan.ai&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;General Discussion&lt;/strong&gt;: &lt;a href="https://discord.gg/FTk2MvZwJH"&gt;Discord&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Apache 2.0 - Because sharing is caring.&lt;/p&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;p&gt;Built on the shoulders of giants:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ggerganov/llama.cpp"&gt;Llama.cpp&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://tauri.app/"&gt;Tauri&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/scalar/scalar"&gt;Scalar&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>bytedance/UI-TARS-desktop</title>
      <link>https://github.com/bytedance/UI-TARS-desktop</link>
      <description>&lt;p&gt;The Open-sourced Multimodal AI Agent Stack connecting Cutting-edge AI Models and Agent Infra.&lt;/p&gt;&lt;hr&gt;&lt;picture&gt; 
 &lt;img alt="Agent TARS Banner" src="https://raw.githubusercontent.com/bytedance/UI-TARS-desktop/main/images/tars.png" /&gt; 
&lt;/picture&gt; 
&lt;br /&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;English | &lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS-desktop/main/README.zh-CN.md"&gt;简体中文&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://trendshift.io/repositories/13584"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/13584" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;b&gt;TARS&lt;sup&gt;*&lt;/sup&gt;&lt;/b&gt; is a Multimodal AI Agent stack, currently shipping two projects: &lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS-desktop/main/#agent-tars"&gt;Agent TARS&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS-desktop/main/#ui-tars-desktop"&gt;UI-TARS-desktop&lt;/a&gt;:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th width="50%" align="center"&gt;&lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS-desktop/main/#agent-tars"&gt;Agent TARS&lt;/a&gt;&lt;/th&gt; 
   &lt;th width="50%" align="center"&gt;&lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS-desktop/main/#ui-tars-desktop"&gt;UI-TARS-desktop&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt; 
    &lt;video src="https://github.com/user-attachments/assets/c9489936-afdc-4d12-adda-d4b90d2a869d" width="50%"&gt;&lt;/video&gt; &lt;/td&gt; 
   &lt;td align="center"&gt; 
    &lt;video src="https://github.com/user-attachments/assets/e0914ce9-ad33-494b-bdec-0c25c1b01a27" width="50%"&gt;&lt;/video&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt; &lt;b&gt;Agent TARS&lt;/b&gt; is a general multimodal AI Agent stack, it brings the power of GUI Agent and Vision into your terminal, computer, browser and product. &lt;br /&gt; &lt;br /&gt; It primarily ships with a &lt;a href="https://agent-tars.com/guide/basic/cli.html" target="_blank"&gt;CLI&lt;/a&gt; and &lt;a href="https://agent-tars.com/guide/basic/web-ui.html" target="_blank"&gt;Web UI&lt;/a&gt; for usage. It aims to provide a workflow that is closer to human-like task completion through cutting-edge multimodal LLMs and seamless integration with various real-world &lt;a href="https://agent-tars.com/guide/basic/mcp.html" target="_blank"&gt;MCP&lt;/a&gt; tools. &lt;/td&gt; 
   &lt;td align="left"&gt; &lt;b&gt;UI-TARS Desktop&lt;/b&gt; is a desktop application that provides a native GUI Agent based on the &lt;a href="https://github.com/bytedance/UI-TARS" target="_blank"&gt;UI-TARS&lt;/a&gt; model. &lt;br /&gt; &lt;br /&gt; It primarily ships a &lt;a href="https://github.com/bytedance/UI-TARS-desktop/raw/main/docs/quick-start.md#get-model-and-run-local-operator" target="_blank"&gt;local&lt;/a&gt; and &lt;a href="https://github.com/bytedance/UI-TARS-desktop/raw/main/docs/quick-start.md#run-remote-operator" target="_blank"&gt;remote&lt;/a&gt; computer as well as browser operators. &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;!-- START doctoc generated TOC please keep comment here to allow auto update --&gt; 
&lt;!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE --&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS-desktop/main/#news"&gt;News&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS-desktop/main/#agent-tars"&gt;Agent TARS&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS-desktop/main/#showcase"&gt;Showcase&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS-desktop/main/#core-features"&gt;Core Features&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS-desktop/main/#quick-start"&gt;Quick Start&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS-desktop/main/#documentation"&gt;Documentation&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS-desktop/main/#ui-tars-desktop"&gt;UI-TARS Desktop&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS-desktop/main/#showcase-1"&gt;Showcase&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS-desktop/main/#features"&gt;Features&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS-desktop/main/#quick-start-1"&gt;Quick Start&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS-desktop/main/#contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS-desktop/main/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS-desktop/main/#citation"&gt;Citation&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- END doctoc generated TOC please keep comment here to allow auto update --&gt; 
&lt;h2&gt;News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;[2025-06-25]&lt;/strong&gt; We released a Agent TARS Beta and Agent TARS CLI - &lt;a href="https://agent-tars.com/blog/2025-06-25-introducing-agent-tars-beta.html"&gt;Introducing Agent TARS Beta&lt;/a&gt;, a multimodal AI agent that aims to explore a work form that is closer to human-like task completion through rich multimodal capabilities (such as GUI Agent, Vision) and seamless integration with various real-world tools.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025-06-12]&lt;/strong&gt; - 🎁 We are thrilled to announce the release of UI-TARS Desktop v0.2.0! This update introduces two powerful new features: &lt;strong&gt;Remote Computer Operator&lt;/strong&gt; and &lt;strong&gt;Remote Browser Operator&lt;/strong&gt;—both completely free. No configuration required: simply click to remotely control any computer or browser, and experience a new level of convenience and intelligence.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025-04-17]&lt;/strong&gt; - 🎉 We're thrilled to announce the release of new UI-TARS Desktop application v0.1.0, featuring a redesigned Agent UI. The application enhances the computer using experience, introduces new browser operation features, and supports &lt;a href="https://seed-tars.com/1.5"&gt;the advanced UI-TARS-1.5 model&lt;/a&gt; for improved performance and precise control.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025-02-20]&lt;/strong&gt; - 📦 Introduced &lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS-desktop/main/docs/sdk.md"&gt;UI TARS SDK&lt;/a&gt;, is a powerful cross-platform toolkit for building GUI automation agents.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025-01-23]&lt;/strong&gt; - 🚀 We updated the &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS-desktop/main/docs/deployment.md#cloud-deployment"&gt;Cloud Deployment&lt;/a&gt;&lt;/strong&gt; section in the 中文版: &lt;a href="https://bytedance.sg.larkoffice.com/docx/TCcudYwyIox5vyxiSDLlgIsTgWf#U94rdCxzBoJMLex38NPlHL21gNb"&gt;GUI模型部署教程&lt;/a&gt; with new information related to the ModelScope platform. You can now use the ModelScope platform for deployment.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;h2&gt;Agent TARS&lt;/h2&gt; 
&lt;p&gt; &lt;a href="https://npmjs.com/package/@agent-tars/cli?activeTab=readme"&gt;&lt;img src="https://img.shields.io/npm/v/@agent-tars/cli?style=for-the-badge&amp;amp;colorA=1a1a2e&amp;amp;colorB=3B82F6&amp;amp;logo=npm&amp;amp;logoColor=white" alt="npm version" /&gt;&lt;/a&gt; &lt;a href="https://npmcharts.com/compare/@agent-tars/cli?minimal=true"&gt;&lt;img src="https://img.shields.io/npm/dm/@agent-tars/cli.svg?style=for-the-badge&amp;amp;colorA=1a1a2e&amp;amp;colorB=0EA5E9&amp;amp;logo=npm&amp;amp;logoColor=white" alt="downloads" /&gt;&lt;/a&gt; &lt;a href="https://nodejs.org/en/about/previous-releases"&gt;&lt;img src="https://img.shields.io/node/v/@agent-tars/cli.svg?style=for-the-badge&amp;amp;colorA=1a1a2e&amp;amp;colorB=06B6D4&amp;amp;logo=node.js&amp;amp;logoColor=white" alt="node version" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/HnKcSBgTVx"&gt;&lt;img src="https://img.shields.io/badge/Discord-Join%20Community-5865F2?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Discord Community" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/agent_tars"&gt;&lt;img src="https://img.shields.io/badge/Twitter-Follow%20%40agent__tars-1DA1F2?style=for-the-badge&amp;amp;logo=twitter&amp;amp;logoColor=white" alt="Official Twitter" /&gt;&lt;/a&gt; &lt;a href="https://applink.larkoffice.com/client/chat/chatter/add_by_link?link_token=279h3365-b0fa-407f-89f3-0f96f36cd4d8"&gt;&lt;img src="https://img.shields.io/badge/飞书群-加入交流群-00D4AA?style=for-the-badge&amp;amp;logo=lark&amp;amp;logoColor=white" alt="飞书交流群" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/bytedance/UI-TARS-desktop"&gt;&lt;img src="https://img.shields.io/badge/DeepWiki-Ask%20AI-8B5CF6?style=for-the-badge&amp;amp;logo=gitbook&amp;amp;logoColor=white" alt="Ask DeepWiki" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;&lt;b&gt;Agent TARS&lt;/b&gt; is a general multimodal AI Agent stack, it brings the power of GUI Agent and Vision into your terminal, computer, browser and product. &lt;br /&gt; &lt;br /&gt; It primarily ships with a &lt;a href="https://agent-tars.com/guide/basic/cli.html" target="_blank"&gt;CLI&lt;/a&gt; and &lt;a href="https://agent-tars.com/guide/basic/web-ui.html" target="_blank"&gt;Web UI&lt;/a&gt; for usage. It aims to provide a workflow that is closer to human-like task completion through cutting-edge multimodal LLMs and seamless integration with various real-world &lt;a href="https://agent-tars.com/guide/basic/mcp.html" target="_blank"&gt;MCP&lt;/a&gt; tools.&lt;/p&gt; 
&lt;h3&gt;Showcase&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;Please help me book the earliest flight from San Jose to New York on September 1st and the last return flight on September 6th on Priceline
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/772b0eef-aef7-4ab9-8cb0-9611820539d8"&gt;https://github.com/user-attachments/assets/772b0eef-aef7-4ab9-8cb0-9611820539d8&lt;/a&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th width="50%" align="center"&gt;Booking Hotel&lt;/th&gt; 
   &lt;th width="50%" align="center"&gt;Generate Chart with extra MCP Servers&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt; 
    &lt;video src="https://github.com/user-attachments/assets/c9489936-afdc-4d12-adda-d4b90d2a869d" width="50%"&gt;&lt;/video&gt; &lt;/td&gt; 
   &lt;td align="center"&gt; 
    &lt;video src="https://github.com/user-attachments/assets/a9fd72d0-01bb-4233-aa27-ca95194bbce9" width="50%"&gt;&lt;/video&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt; &lt;b&gt;Instruction:&lt;/b&gt; &lt;i&gt;I am in Los Angeles from September 1st to September 6th, with a budget of $5,000. Please help me book a Ritz-Carlton hotel closest to the airport on booking.com and compile a transportation guide for me&lt;/i&gt; &lt;/td&gt; 
   &lt;td align="left"&gt; &lt;b&gt;Instruction:&lt;/b&gt; &lt;i&gt;Draw me a chart of Hangzhou's weather for one month&lt;/i&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;For more use cases, please check out &lt;a href="https://github.com/bytedance/UI-TARS-desktop/issues/842"&gt;#842&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Core Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;🖱️ &lt;strong&gt;One-Click Out-of-the-box CLI&lt;/strong&gt; - Supports both &lt;strong&gt;headful&lt;/strong&gt; &lt;a href="https://agent-tars.com/guide/basic/web-ui.html"&gt;Web UI&lt;/a&gt; and &lt;strong&gt;headless&lt;/strong&gt; &lt;a href="https://agent-tars.com/guide/advanced/server.html"&gt;server&lt;/a&gt;) &lt;a href="https://agent-tars.com/guide/basic/cli.html"&gt;execution&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;🌐 &lt;strong&gt;Hybrid Browser Agent&lt;/strong&gt; - Control browsers using &lt;a href="https://agent-tars.com/guide/basic/browser.html#visual-grounding"&gt;GUI Agent&lt;/a&gt;, &lt;a href="https://agent-tars.com/guide/basic/browser.html#dom"&gt;DOM&lt;/a&gt;, or a hybrid strategy.&lt;/li&gt; 
 &lt;li&gt;🔄 &lt;strong&gt;Event Stream&lt;/strong&gt; - Protocol-driven Event Stream drives &lt;a href="https://agent-tars.com/beta#context-engineering"&gt;Context Engineering&lt;/a&gt; and &lt;a href="https://agent-tars.com/blog/2025-06-25-introducing-agent-tars-beta.html#easy-to-build-applications"&gt;Agent UI&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;🧰 &lt;strong&gt;MCP Integration&lt;/strong&gt; - The kernel is built on MCP and also supports mounting &lt;a href="https://agent-tars.com/guide/basic/mcp.html"&gt;MCP Servers&lt;/a&gt; to connect to real-world tools.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Quick Start&lt;/h3&gt; 
&lt;img alt="Agent TARS CLI" src="https://agent-tars.com/agent-tars-cli.png" /&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Luanch with `npx`.
npx @agent-tars/cli@latest

# Install globally, required Node.js &amp;gt;= 22
npm install @agent-tars/cli@latest -g

# Run with your preferred model provider
agent-tars --provider volcengine --model doubao-1-5-thinking-vision-pro-250428 --apiKey your-api-key
agent-tars --provider anthropic --model claude-3-7-sonnet-latest --apiKey your-api-key
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Visit the comprehensive &lt;a href="https://agent-tars.com/guide/get-started/quick-start.html"&gt;Quick Start&lt;/a&gt; guide for detailed setup instructions.&lt;/p&gt; 
&lt;h3&gt;Documentation&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;🌟 &lt;strong&gt;Explore Agent TARS Universe&lt;/strong&gt; 🌟&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th width="20%" align="center"&gt;Category&lt;/th&gt; 
   &lt;th width="30%" align="center"&gt;Resource Link&lt;/th&gt; 
   &lt;th width="50%" align="left"&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;🏠 &lt;strong&gt;Central Hub&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt; &lt;a href="https://agent-tars.com"&gt; &lt;img src="https://img.shields.io/badge/Visit-Website-4F46E5?style=for-the-badge&amp;amp;logo=globe&amp;amp;logoColor=white" alt="Website" /&gt; &lt;/a&gt; &lt;/td&gt; 
   &lt;td align="left"&gt;Your gateway to Agent TARS ecosystem&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;📚 &lt;strong&gt;Quick Start&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt; &lt;a href="https://agent-tars.com/guide/get-started/quick-start.html"&gt; &lt;img src="https://img.shields.io/badge/Get-Started-06B6D4?style=for-the-badge&amp;amp;logo=rocket&amp;amp;logoColor=white" alt="Quick Start" /&gt; &lt;/a&gt; &lt;/td&gt; 
   &lt;td align="left"&gt;Zero to hero in 5 minutes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;🚀 &lt;strong&gt;What's New&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt; &lt;a href="https://agent-tars.com/beta"&gt; &lt;img src="https://img.shields.io/badge/Read-Blog-F59E0B?style=for-the-badge&amp;amp;logo=rss&amp;amp;logoColor=white" alt="Blog" /&gt; &lt;/a&gt; &lt;/td&gt; 
   &lt;td align="left"&gt;Discover cutting-edge features &amp;amp; vision&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;🛠️ &lt;strong&gt;Developer Zone&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt; &lt;a href="https://agent-tars.com/guide/get-started/introduction.html"&gt; &lt;img src="https://img.shields.io/badge/View-Docs-10B981?style=for-the-badge&amp;amp;logo=gitbook&amp;amp;logoColor=white" alt="Docs" /&gt; &lt;/a&gt; &lt;/td&gt; 
   &lt;td align="left"&gt;Master every command &amp;amp; features&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;🎯 &lt;strong&gt;Showcase&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt; &lt;a href="https://github.com/bytedance/UI-TARS-desktop/issues/842"&gt; &lt;img src="https://img.shields.io/badge/View-Examples-8B5CF6?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white" alt="Examples" /&gt; &lt;/a&gt; &lt;/td&gt; 
   &lt;td align="left"&gt;View use cases built by the official and community&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;🔧 &lt;strong&gt;Reference&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt; &lt;a href="https://agent-tars.com/api/"&gt; &lt;img src="https://img.shields.io/badge/API-Reference-EF4444?style=for-the-badge&amp;amp;logo=book&amp;amp;logoColor=white" alt="API" /&gt; &lt;/a&gt; &lt;/td&gt; 
   &lt;td align="left"&gt;Complete technical reference&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;h2&gt;UI-TARS Desktop&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;img alt="UI-TARS" width="260" src="https://raw.githubusercontent.com/bytedance/UI-TARS-desktop/main/apps/ui-tars/resources/icon.png" /&gt; &lt;/p&gt; 
&lt;p&gt;UI-TARS Desktop is a native GUI agent driven by &lt;a href="https://github.com/bytedance/UI-TARS"&gt;UI-TARS&lt;/a&gt; and Seed-1.5-VL/1.6 series models, available on your local computer and remote VM sandbox on cloud.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &amp;nbsp;&amp;nbsp; 📑 &lt;a href="https://arxiv.org/abs/2501.12326"&gt;Paper&lt;/a&gt; &amp;nbsp;&amp;nbsp; | 🤗 &lt;a href="https://huggingface.co/ByteDance-Seed/UI-TARS-1.5-7B"&gt;Hugging Face Models&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;🫨 &lt;a href="https://discord.gg/pTXwYVjfcs"&gt;Discord&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;🤖 &lt;a href="https://www.modelscope.cn/collections/UI-TARS-bccb56fa1ef640"&gt;ModelScope&lt;/a&gt;&amp;nbsp;&amp;nbsp; &lt;br /&gt; 🖥️ Desktop Application &amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; 👓 &lt;a href="https://github.com/web-infra-dev/midscene"&gt;Midscene (use in browser)&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h3&gt;Showcase&lt;/h3&gt; 
&lt;!-- // FIXME: Choose only two demo, one local computer and one remote computer showcase. --&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Instruction&lt;/th&gt; 
   &lt;th align="center"&gt;Local Operator&lt;/th&gt; 
   &lt;th align="center"&gt;Remote Operator&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Please help me open the autosave feature of VS Code and delay AutoSave operations for 500 milliseconds in the VS Code setting.&lt;/td&gt; 
   &lt;td align="center"&gt;
    &lt;video src="https://github.com/user-attachments/assets/e0914ce9-ad33-494b-bdec-0c25c1b01a27" height="300"&gt;&lt;/video&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;
    &lt;video src="https://github.com/user-attachments/assets/01e49b69-7070-46c8-b3e3-2aaaaec71800" height="300"&gt;&lt;/video&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Could you help me check the latest open issue of the UI-TARS-Desktop project on GitHub?&lt;/td&gt; 
   &lt;td align="center"&gt;
    &lt;video src="https://github.com/user-attachments/assets/3d159f54-d24a-4268-96c0-e149607e9199" height="300"&gt;&lt;/video&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;
    &lt;video src="https://github.com/user-attachments/assets/072fb72d-7394-4bfa-95f5-4736e29f7e58" height="300"&gt;&lt;/video&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;🤖 Natural language control powered by Vision-Language Model&lt;/li&gt; 
 &lt;li&gt;🖥️ Screenshot and visual recognition support&lt;/li&gt; 
 &lt;li&gt;🎯 Precise mouse and keyboard control&lt;/li&gt; 
 &lt;li&gt;💻 Cross-platform support (Windows/MacOS/Browser)&lt;/li&gt; 
 &lt;li&gt;🔄 Real-time feedback and status display&lt;/li&gt; 
 &lt;li&gt;🔐 Private and secure - fully local processing&lt;/li&gt; 
 &lt;li&gt;🛠️ Effortless setup and intuitive remote operators&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Quick Start&lt;/h3&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS-desktop/main/docs/quick-start.md"&gt;Quick Start&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS-desktop/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the Apache License 2.0.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you find our paper and code useful in your research, please consider giving a star &lt;span&gt;⭐&lt;/span&gt; and citation &lt;span&gt;📝&lt;/span&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-BibTeX"&gt;@article{qin2025ui,
  title={UI-TARS: Pioneering Automated GUI Interaction with Native Agents},
  author={Qin, Yujia and Ye, Yining and Fang, Junjie and Wang, Haoming and Liang, Shihao and Tian, Shizuo and Zhang, Junda and Li, Jiahao and Li, Yunxin and Huang, Shijue and others},
  journal={arXiv preprint arXiv:2501.12326},
  year={2025}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>umami-software/umami</title>
      <link>https://github.com/umami-software/umami</link>
      <description>&lt;p&gt;Umami is a modern, privacy-focused alternative to Google Analytics.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://content.umami.is/website/images/umami-logo.png" alt="Umami Logo" width="100" /&gt; &lt;/p&gt; 
&lt;h1 align="center"&gt;Umami&lt;/h1&gt; 
&lt;p align="center"&gt; &lt;i&gt;Umami is a simple, fast, privacy-focused alternative to Google Analytics.&lt;/i&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/umami-software/umami/releases"&gt; &lt;img src="https://img.shields.io/github/release/umami-software/umami.svg?sanitize=true" alt="GitHub Release" /&gt; &lt;/a&gt; &lt;a href="https://github.com/umami-software/umami/raw/master/LICENSE"&gt; &lt;img src="https://img.shields.io/github/license/umami-software/umami.svg?sanitize=true" alt="MIT License" /&gt; &lt;/a&gt; &lt;a href="https://github.com/umami-software/umami/actions"&gt; &lt;img src="https://img.shields.io/github/actions/workflow/status/umami-software/umami/ci.yml" alt="Build Status" /&gt; &lt;/a&gt; &lt;a href="https://analytics.umami.is/share/LGazGOecbDtaIwDr/umami.is" style="text-decoration: none;"&gt; &lt;img src="https://img.shields.io/badge/Try%20Demo%20Now-Click%20Here-brightgreen" alt="Umami Demo" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🚀 Getting Started&lt;/h2&gt; 
&lt;p&gt;A detailed getting started guide can be found at &lt;a href="https://umami.is/docs/"&gt;umami.is/docs&lt;/a&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🛠 Installing from Source&lt;/h2&gt; 
&lt;h3&gt;Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;A server with Node.js version 18.18 or newer&lt;/li&gt; 
 &lt;li&gt;A database. Umami supports &lt;a href="https://www.mariadb.org/"&gt;MariaDB&lt;/a&gt; (minimum v10.5), &lt;a href="https://www.mysql.com/"&gt;MySQL&lt;/a&gt; (minimum v8.0) and &lt;a href="https://www.postgresql.org/"&gt;PostgreSQL&lt;/a&gt; (minimum v12.14) databases.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Get the Source Code and Install Packages&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/umami-software/umami.git
cd umami
npm install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Configure Umami&lt;/h3&gt; 
&lt;p&gt;Create an &lt;code&gt;.env&lt;/code&gt; file with the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;DATABASE_URL=connection-url
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The connection URL format:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;postgresql://username:mypassword@localhost:5432/mydb
mysql://username:mypassword@localhost:3306/mydb
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Build the Application&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npm run build
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;The build step will create tables in your database if you are installing for the first time. It will also create a login user with username &lt;strong&gt;admin&lt;/strong&gt; and password &lt;strong&gt;umami&lt;/strong&gt;.&lt;/em&gt;&lt;/p&gt; 
&lt;h3&gt;Start the Application&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npm run start
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;By default, this will launch the application on &lt;code&gt;http://localhost:3000&lt;/code&gt;. You will need to either &lt;a href="https://docs.nginx.com/nginx/admin-guide/web-server/reverse-proxy/"&gt;proxy&lt;/a&gt; requests from your web server or change the &lt;a href="https://nextjs.org/docs/api-reference/cli#production"&gt;port&lt;/a&gt; to serve the application directly.&lt;/em&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🐳 Installing with Docker&lt;/h2&gt; 
&lt;p&gt;To build the Umami container and start up a Postgres database, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker compose up -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, to pull just the Umami Docker image with PostgreSQL support:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker pull docker.umami.is/umami-software/umami:postgresql-latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or with MySQL support:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker pull docker.umami.is/umami-software/umami:mysql-latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🔄 Getting Updates&lt;/h2&gt; 
&lt;p&gt;To get the latest features, simply do a pull, install any new dependencies, and rebuild:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git pull
npm install
npm run build
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To update the Docker image, simply pull the new images and rebuild:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker compose pull
docker compose up --force-recreate -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🛟 Support&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/umami-software/umami"&gt; &lt;img src="https://img.shields.io/badge/GitHub--blue?style=social&amp;amp;logo=github" alt="GitHub" /&gt; &lt;/a&gt; &lt;a href="https://twitter.com/umami_software"&gt; &lt;img src="https://img.shields.io/badge/Twitter--blue?style=social&amp;amp;logo=twitter" alt="Twitter" /&gt; &lt;/a&gt; &lt;a href="https://linkedin.com/company/umami-software"&gt; &lt;img src="https://img.shields.io/badge/LinkedIn--blue?style=social&amp;amp;logo=linkedin" alt="LinkedIn" /&gt; &lt;/a&gt; &lt;a href="https://umami.is/discord"&gt; &lt;img src="https://img.shields.io/badge/Discord--blue?style=social&amp;amp;logo=discord" alt="Discord" /&gt; &lt;/a&gt; &lt;/p&gt;</description>
    </item>
    
    <item>
      <title>exo-explore/exo</title>
      <link>https://github.com/exo-explore/exo</link>
      <description>&lt;p&gt;Run your own AI cluster at home with everyday devices 📱💻 🖥️⌚&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="/docs/exo-logo-black-bg.jpg" /&gt; 
  &lt;img alt="exo logo" src="https://raw.githubusercontent.com/exo-explore/exo/main/docs/exo-logo-transparent.png" width="50%" height="50%" /&gt; 
 &lt;/picture&gt; 
 &lt;p&gt;exo: Run your own AI cluster at home with everyday devices. Maintained by &lt;a href="https://x.com/exolabs"&gt;exo labs&lt;/a&gt;.&lt;/p&gt; 
 &lt;h3&gt; &lt;p&gt;&lt;a href="https://discord.gg/EUnjGpsmWw"&gt;Discord&lt;/a&gt; | &lt;a href="https://t.me/+Kh-KqHTzFYg3MGNk"&gt;Telegram&lt;/a&gt; | &lt;a href="https://x.com/exolabs"&gt;X&lt;/a&gt;&lt;/p&gt; &lt;/h3&gt; 
 &lt;p&gt;&lt;a href="https://github.com/exo-explore/exo/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/exo-explore/exo" alt="GitHub Repo stars" /&gt;&lt;/a&gt; &lt;a href="https://dl.circleci.com/status-badge/redirect/circleci/TrkofJDoGzdQAeL6yVHKsg/4i5hJuafuwZYZQxbRAWS71/tree/main"&gt;&lt;img src="https://dl.circleci.com/status-badge/img/circleci/TrkofJDoGzdQAeL6yVHKsg/4i5hJuafuwZYZQxbRAWS71/tree/main.svg?style=svg" alt="Tests" /&gt;&lt;/a&gt; &lt;a href="https://www.gnu.org/licenses/gpl-3.0"&gt;&lt;img src="https://img.shields.io/badge/License-GPLv3-blue.svg?sanitize=true" alt="License: GPL v3" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/11849" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/11849" alt="exo-explore%2Fexo | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;p&gt;Unify your existing devices into one powerful GPU: iPhone, iPad, Android, Mac, NVIDIA, Raspberry Pi, pretty much any device!&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;h2&gt;Update: exo is hiring. See &lt;a href="https://exolabs.net"&gt;here&lt;/a&gt; for more details.&lt;/h2&gt; 
 &lt;h2&gt;Interested in running exo in your business? &lt;a href="mailto:hello@exolabs.net"&gt;Contact us&lt;/a&gt; to discuss.&lt;/h2&gt; 
&lt;/div&gt; 
&lt;h2&gt;Get Involved&lt;/h2&gt; 
&lt;p&gt;exo is &lt;strong&gt;experimental&lt;/strong&gt; software. Expect bugs early on. Create issues so they can be fixed. The &lt;a href="https://x.com/exolabs"&gt;exo labs&lt;/a&gt; team will strive to resolve issues quickly.&lt;/p&gt; 
&lt;p&gt;We also welcome contributions from the community. We have a list of bounties in &lt;a href="https://docs.google.com/spreadsheets/d/1cTCpTIp48UnnIvHeLEUNg1iMy_Q6lRybgECSFCoVJpE/edit?usp=sharing"&gt;this sheet&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;h3&gt;Wide Model Support&lt;/h3&gt; 
&lt;p&gt;exo supports different models including LLaMA (&lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/inference/mlx/models/llama.py"&gt;MLX&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/inference/tinygrad/models/llama.py"&gt;tinygrad&lt;/a&gt;), Mistral, LlaVA, Qwen, and Deepseek.&lt;/p&gt; 
&lt;h3&gt;Dynamic Model Partitioning&lt;/h3&gt; 
&lt;p&gt;exo &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/topology/ring_memory_weighted_partitioning_strategy.py"&gt;optimally splits up models&lt;/a&gt; based on the current network topology and device resources available. This enables you to run larger models than you would be able to on any single device.&lt;/p&gt; 
&lt;h3&gt;Automatic Device Discovery&lt;/h3&gt; 
&lt;p&gt;exo will &lt;a href="https://github.com/exo-explore/exo/raw/945f90f676182a751d2ad7bcf20987ab7fe0181e/exo/orchestration/node.py#L154"&gt;automatically discover&lt;/a&gt; other devices using the best method available. Zero manual configuration.&lt;/p&gt; 
&lt;h3&gt;ChatGPT-compatible API&lt;/h3&gt; 
&lt;p&gt;exo provides a &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/api/chatgpt_api.py"&gt;ChatGPT-compatible API&lt;/a&gt; for running models. It's a &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/examples/chatgpt_api.sh"&gt;one-line change&lt;/a&gt; in your application to run models on your own hardware using exo.&lt;/p&gt; 
&lt;h3&gt;Device Equality&lt;/h3&gt; 
&lt;p&gt;Unlike other distributed inference frameworks, exo does not use a master-worker architecture. Instead, exo devices &lt;a href="https://github.com/exo-explore/exo/raw/945f90f676182a751d2ad7bcf20987ab7fe0181e/exo/orchestration/node.py#L161"&gt;connect p2p&lt;/a&gt;. As long as a device is connected somewhere in the network, it can be used to run models.&lt;/p&gt; 
&lt;p&gt;Exo supports different &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/topology/partitioning_strategy.py"&gt;partitioning strategies&lt;/a&gt; to split up a model across devices. The default partitioning strategy is &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/topology/ring_memory_weighted_partitioning_strategy.py"&gt;ring memory weighted partitioning&lt;/a&gt;. This runs an inference in a ring where each device runs a number of model layers proportional to the memory of the device.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/exo-explore/exo/main/docs/exo-screenshot.jpg" alt="&amp;quot;A screenshot of exo running 5 nodes" /&gt;&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;The current recommended way to install exo is from source.&lt;/p&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python&amp;gt;=3.12.0 is required because of &lt;a href="https://github.com/exo-explore/exo/issues/5"&gt;issues with asyncio&lt;/a&gt; in previous versions.&lt;/li&gt; 
 &lt;li&gt;For Linux with NVIDIA GPU support (Linux-only, skip if not using Linux or NVIDIA): 
  &lt;ul&gt; 
   &lt;li&gt;NVIDIA driver - verify with &lt;code&gt;nvidia-smi&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;CUDA toolkit - install from &lt;a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#cuda-cross-platform-installation"&gt;NVIDIA CUDA guide&lt;/a&gt;, verify with &lt;code&gt;nvcc --version&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;cuDNN library - download from &lt;a href="https://developer.nvidia.com/cudnn-downloads"&gt;NVIDIA cuDNN page&lt;/a&gt;, verify installation by following &lt;a href="https://docs.nvidia.com/deeplearning/cudnn/latest/installation/linux.html#verifying-the-install-on-linux:~:text=at%20a%20time.-,Verifying%20the%20Install%20on%20Linux,Test%20passed!,-Upgrading%20From%20Older"&gt;these steps&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Hardware Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;The only requirement to run exo is to have enough memory across all your devices to fit the entire model into memory. For example, if you are running llama 3.1 8B (fp16), you need 16GB of memory across all devices. Any of the following configurations would work since they each have more than 16GB of memory in total: 
  &lt;ul&gt; 
   &lt;li&gt;2 x 8GB M3 MacBook Airs&lt;/li&gt; 
   &lt;li&gt;1 x 16GB NVIDIA RTX 4070 Ti Laptop&lt;/li&gt; 
   &lt;li&gt;2 x Raspberry Pi 400 with 4GB of RAM each (running on CPU) + 1 x 8GB Mac Mini&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;exo is designed to run on devices with heterogeneous capabilities. For example, you can have some devices with powerful GPUs and others with integrated GPUs or even CPUs. Adding less capable devices will slow down individual inference latency but will increase the overall throughput of the cluster.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;From source&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;git clone https://github.com/exo-explore/exo.git
cd exo
pip install -e .
# alternatively, with venv
source install.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Troubleshooting&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;If running on Mac, MLX has an &lt;a href="https://ml-explore.github.io/mlx/build/html/install.html"&gt;install guide&lt;/a&gt; with troubleshooting steps.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Performance&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;There are a number of things users have empirically found to improve performance on Apple Silicon Macs:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;ol&gt; 
 &lt;li&gt;Upgrade to the latest version of macOS Sequoia.&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;./configure_mlx.sh&lt;/code&gt;. This runs commands to optimize GPU memory allocation on Apple Silicon Macs.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;h3&gt;Example Usage on Multiple macOS Devices&lt;/h3&gt; 
&lt;h4&gt;Device 1:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Device 2:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;That's it! No configuration required - exo will automatically discover the other device(s).&lt;/p&gt; 
&lt;p&gt;exo starts a ChatGPT-like WebUI (powered by &lt;a href="https://github.com/tinygrad/tinygrad/tree/master/examples/tinychat"&gt;tinygrad tinychat&lt;/a&gt;) on &lt;a href="http://localhost:52415"&gt;http://localhost:52415&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;For developers, exo also starts a ChatGPT-compatible API endpoint on &lt;a href="http://localhost:52415/v1/chat/completions"&gt;http://localhost:52415/v1/chat/completions&lt;/a&gt;. Examples with curl:&lt;/p&gt; 
&lt;h4&gt;Llama 3.2 3B:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;curl http://localhost:52415/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
     "model": "llama-3.2-3b",
     "messages": [{"role": "user", "content": "What is the meaning of exo?"}],
     "temperature": 0.7
   }'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Llama 3.1 405B:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;curl http://localhost:52415/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
     "model": "llama-3.1-405b",
     "messages": [{"role": "user", "content": "What is the meaning of exo?"}],
     "temperature": 0.7
   }'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;DeepSeek R1 (full 671B):&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;curl http://localhost:52415/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
     "model": "deepseek-r1",
     "messages": [{"role": "user", "content": "What is the meaning of exo?"}],
     "temperature": 0.7
   }'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Llava 1.5 7B (Vision Language Model):&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;curl http://localhost:52415/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
     "model": "llava-1.5-7b-hf",
     "messages": [
      {
        "role": "user",
        "content": [
          {
            "type": "text",
            "text": "What are these?"
          },
          {
            "type": "image_url",
            "image_url": {
              "url": "http://images.cocodataset.org/val2017/000000039769.jpg"
            }
          }
        ]
      }
    ],
     "temperature": 0.0
   }'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Example Usage on Multiple Heterogenous Devices (macOS + Linux)&lt;/h3&gt; 
&lt;h4&gt;Device 1 (macOS):&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note: We don't need to explicitly tell exo to use the &lt;strong&gt;tinygrad&lt;/strong&gt; inference engine. &lt;strong&gt;MLX&lt;/strong&gt; and &lt;strong&gt;tinygrad&lt;/strong&gt; are interoperable!&lt;/p&gt; 
&lt;h4&gt;Device 2 (Linux):&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Linux devices will automatically default to using the &lt;strong&gt;tinygrad&lt;/strong&gt; inference engine.&lt;/p&gt; 
&lt;p&gt;You can read about tinygrad-specific env vars &lt;a href="https://docs.tinygrad.org/env_vars/"&gt;here&lt;/a&gt;. For example, you can configure tinygrad to use the cpu by specifying &lt;code&gt;CLANG=1&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Example Usage on a single device with "exo run" command&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;exo run llama-3.2-3b
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;With a custom prompt:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;exo run llama-3.2-3b --prompt "What is the meaning of exo?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Model Storage&lt;/h3&gt; 
&lt;p&gt;Models by default are stored in &lt;code&gt;~/.cache/exo/downloads&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;You can set a different model storage location by setting the &lt;code&gt;EXO_HOME&lt;/code&gt; env var.&lt;/p&gt; 
&lt;h2&gt;Model Downloading&lt;/h2&gt; 
&lt;p&gt;Models are downloaded from Hugging Face. If you are running exo in a country with strict internet censorship, you may need to download the models manually and put them in the &lt;code&gt;~/.cache/exo/downloads&lt;/code&gt; directory.&lt;/p&gt; 
&lt;p&gt;To download models from a proxy endpoint, set the &lt;code&gt;HF_ENDPOINT&lt;/code&gt; environment variable. For example, to run exo with the huggingface mirror endpoint:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;HF_ENDPOINT=https://hf-mirror.com exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Debugging&lt;/h2&gt; 
&lt;p&gt;Enable debug logs with the DEBUG environment variable (0-9).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;DEBUG=9 exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For the &lt;strong&gt;tinygrad&lt;/strong&gt; inference engine specifically, there is a separate DEBUG flag &lt;code&gt;TINYGRAD_DEBUG&lt;/code&gt; that can be used to enable debug logs (1-6).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;TINYGRAD_DEBUG=2 exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Formatting&lt;/h2&gt; 
&lt;p&gt;We use &lt;a href="https://github.com/google/yapf"&gt;yapf&lt;/a&gt; to format the code. To format the code, first install the formatting requirements:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip3 install -e '.[formatting]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then run the formatting script:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python3 format.py ./exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Known Issues&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;On certain versions of Python on macOS, certificates may not installed correctly, potentially causing SSL errors (e.g., when accessing huggingface.co). To resolve this, run the &lt;code&gt;Install Certificates&lt;/code&gt; command, typicall as follows:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;/Applications/Python 3.x/Install Certificates.command
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;🚧 As the library is evolving so quickly, the iOS implementation has fallen behind Python. We have decided for now not to put out the buggy iOS version and receive a bunch of GitHub issues for outdated code. We are working on solving this properly and will make an announcement when it's ready. If you would like access to the iOS implementation now, please email &lt;a href="mailto:alex@exolabs.net"&gt;alex@exolabs.net&lt;/a&gt; with your GitHub username explaining your use-case and you will be granted access on GitHub.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Inference Engines&lt;/h2&gt; 
&lt;p&gt;exo supports the following inference engines:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;✅ &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/inference/mlx/sharded_inference_engine.py"&gt;MLX&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;✅ &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/inference/tinygrad/inference.py"&gt;tinygrad&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;🚧 &lt;a href="https://github.com/exo-explore/exo/pull/139"&gt;PyTorch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;🚧 &lt;a href="https://github.com/exo-explore/exo/issues/167"&gt;llama.cpp&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Discovery Modules&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;✅ &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/networking/udp"&gt;UDP&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;✅ &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/networking/manual"&gt;Manual&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;✅ &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/networking/tailscale"&gt;Tailscale&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;🚧 Radio&lt;/li&gt; 
 &lt;li&gt;🚧 Bluetooth&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Peer Networking Modules&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;✅ &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/networking/grpc"&gt;GRPC&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;🚧 NCCL&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>tadata-org/fastapi_mcp</title>
      <link>https://github.com/tadata-org/fastapi_mcp</link>
      <description>&lt;p&gt;Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt;&lt;a href="https://github.com/tadata-org/fastapi_mcp"&gt;&lt;img src="https://github.com/user-attachments/assets/7e44e98b-a0ba-4aff-a68a-4ffee3a6189c" alt="fastapi-to-mcp" height="100/" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;span style="font-size: 0.85em; font-weight: normal;"&gt;Built by &lt;a href="https://tadata.com"&gt;Tadata&lt;/a&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;h1 align="center"&gt; FastAPI-MCP &lt;/h1&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://trendshift.io/repositories/14064" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/14064" alt="tadata-org%2Ffastapi_mcp | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;p align="center"&gt;Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://pypi.org/project/fastapi-mcp/"&gt;&lt;img src="https://img.shields.io/pypi/v/fastapi-mcp?color=%2334D058&amp;amp;label=pypi%20package" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/fastapi-mcp/"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/fastapi-mcp.svg?sanitize=true" alt="Python Versions" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/tadata-org/fastapi_mcp/main/#"&gt;&lt;img src="https://img.shields.io/badge/FastAPI-009485.svg?logo=fastapi&amp;amp;logoColor=white" alt="FastAPI" /&gt;&lt;/a&gt; &lt;a href="https://github.com/tadata-org/fastapi_mcp/actions/workflows/ci.yml"&gt;&lt;img src="https://github.com/tadata-org/fastapi_mcp/actions/workflows/ci.yml/badge.svg?sanitize=true" alt="CI" /&gt;&lt;/a&gt; &lt;a href="https://codecov.io/gh/tadata-org/fastapi_mcp"&gt;&lt;img src="https://codecov.io/gh/tadata-org/fastapi_mcp/branch/main/graph/badge.svg?sanitize=true" alt="Coverage" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p align="center"&gt;&lt;a href="https://github.com/tadata-org/fastapi_mcp"&gt;&lt;img src="https://github.com/user-attachments/assets/b205adc6-28c0-4e3c-a68b-9c1a80eb7d0c" alt="fastapi-mcp-usage" height="400" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Authentication&lt;/strong&gt; built in, using your existing FastAPI dependencies!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;FastAPI-native:&lt;/strong&gt; Not just another OpenAPI -&amp;gt; MCP converter&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Zero/Minimal configuration&lt;/strong&gt; required - just point it at your FastAPI app and it works&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Preserving schemas&lt;/strong&gt; of your request models and response models&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Preserve documentation&lt;/strong&gt; of all your endpoints, just as it is in Swagger&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Flexible deployment&lt;/strong&gt; - Mount your MCP server to the same app, or deploy separately&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ASGI transport&lt;/strong&gt; - Uses FastAPI's ASGI interface directly for efficient communication&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Hosted Solution&lt;/h2&gt; 
&lt;p&gt;If you prefer a managed hosted solution check out &lt;a href="https://tadata.com"&gt;tadata.com&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;We recommend using &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt;, a fast Python package installer:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv add fastapi-mcp
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, you can install with pip:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install fastapi-mcp
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Basic Usage&lt;/h2&gt; 
&lt;p&gt;The simplest way to use FastAPI-MCP is to add an MCP server directly to your FastAPI application:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from fastapi import FastAPI
from fastapi_mcp import FastApiMCP

app = FastAPI()

mcp = FastApiMCP(app)

# Mount the MCP server directly to your FastAPI app
mcp.mount()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;That's it! Your auto-generated MCP server is now available at &lt;code&gt;https://app.base.url/mcp&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Documentation, Examples and Advanced Usage&lt;/h2&gt; 
&lt;p&gt;FastAPI-MCP provides &lt;a href="https://fastapi-mcp.tadata.com/"&gt;comprehensive documentation&lt;/a&gt;. Additionaly, check out the &lt;a href="https://raw.githubusercontent.com/tadata-org/fastapi_mcp/main/examples"&gt;examples directory&lt;/a&gt; for code samples demonstrating these features in action.&lt;/p&gt; 
&lt;h2&gt;FastAPI-first Approach&lt;/h2&gt; 
&lt;p&gt;FastAPI-MCP is designed as a native extension of FastAPI, not just a converter that generates MCP tools from your API. This approach offers several key advantages:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Native dependencies&lt;/strong&gt;: Secure your MCP endpoints using familiar FastAPI &lt;code&gt;Depends()&lt;/code&gt; for authentication and authorization&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ASGI transport&lt;/strong&gt;: Communicates directly with your FastAPI app using its ASGI interface, eliminating the need for HTTP calls from the MCP to your API&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Unified infrastructure&lt;/strong&gt;: Your FastAPI app doesn't need to run separately from the MCP server (though &lt;a href="https://fastapi-mcp.tadata.com/advanced/deploy#deploying-separately-from-original-fastapi-app"&gt;separate deployment&lt;/a&gt; is also supported)&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This design philosophy ensures minimum friction when adding MCP capabilities to your existing FastAPI services.&lt;/p&gt; 
&lt;h2&gt;Development and Contributing&lt;/h2&gt; 
&lt;p&gt;Thank you for considering contributing to FastAPI-MCP! We encourage the community to post Issues and create Pull Requests.&lt;/p&gt; 
&lt;p&gt;Before you get started, please see our &lt;a href="https://raw.githubusercontent.com/tadata-org/fastapi_mcp/main/CONTRIBUTING.md"&gt;Contribution Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;p&gt;Join &lt;a href="https://join.slack.com/t/themcparty/shared_invite/zt-30yxr1zdi-2FG~XjBA0xIgYSYuKe7~Xg"&gt;MCParty Slack community&lt;/a&gt; to connect with other MCP enthusiasts, ask questions, and share your experiences with FastAPI-MCP.&lt;/p&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.10+ (Recommended 3.12)&lt;/li&gt; 
 &lt;li&gt;uv&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;MIT License. Copyright (c) 2025 Tadata Inc.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>patchy631/ai-engineering-hub</title>
      <link>https://github.com/patchy631/ai-engineering-hub</link>
      <description>&lt;p&gt;In-depth tutorials on LLMs, RAGs and real-world AI agent applications.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/12800"&gt; &lt;img src="https://raw.githubusercontent.com/patchy631/ai-engineering-hub/main/assets/TRENDING-BADGE.png" alt="Trending Badge" style="width: 250px; height: 55px;" width="250" height="55" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/patchy631/ai-engineering-hub/main/assets/ai-eng-hub.gif" alt="AI Engineering Hub Banner" /&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h1&gt;AI Engineering Hub 🚀&lt;/h1&gt; 
&lt;p&gt;Welcome to the &lt;strong&gt;AI Engineering Hub&lt;/strong&gt;!&lt;/p&gt; 
&lt;h2&gt;🌟 Why This Repo?&lt;/h2&gt; 
&lt;p&gt;AI Engineering is advancing rapidly, and staying at the forefront requires both deep understanding and hands-on experience. Here, you will find:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;In-depth tutorials on &lt;strong&gt;LLMs and RAGs&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Real-world &lt;strong&gt;AI agent&lt;/strong&gt; applications&lt;/li&gt; 
 &lt;li&gt;Examples to implement, adapt, and scale in your projects&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Whether you’re a beginner, practitioner, or researcher, this repo provides resources for all skill levels to experiment and succeed in AI engineering.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;📬 Stay Updated with Our Newsletter!&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Get a FREE Data Science eBook&lt;/strong&gt; 📖 with 150+ essential lessons in Data Science when you subscribe to our newsletter! Stay in the loop with the latest tutorials, insights, and exclusive resources. &lt;a href="https://join.dailydoseofds.com"&gt;Subscribe now!&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://join.dailydoseofds.com"&gt;&lt;img src="https://github.com/patchy631/ai-engineering/raw/main/resources/join_ddods.png" alt="Daily Dose of Data Science Newsletter" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;📢 Contribute to the AI Engineering Hub!&lt;/h2&gt; 
&lt;p&gt;We welcome contributors! Whether you want to add new tutorials, improve existing code, or report issues, your contributions make this community thrive. Here’s how to get involved:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Fork&lt;/strong&gt; the repository.&lt;/li&gt; 
 &lt;li&gt;Create a new branch for your contribution.&lt;/li&gt; 
 &lt;li&gt;Submit a &lt;strong&gt;Pull Request&lt;/strong&gt; and describe the improvements.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr /&gt; 
&lt;h2&gt;📜 License&lt;/h2&gt; 
&lt;p&gt;This repository is licensed under the MIT License - see the &lt;a href="https://raw.githubusercontent.com/patchy631/ai-engineering-hub/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h2&gt;💬 Connect&lt;/h2&gt; 
&lt;p&gt;For discussions, suggestions, and more, feel free to &lt;a href="https://github.com/patchy631/ai-engineering/issues"&gt;create an issue&lt;/a&gt; or reach out directly!&lt;/p&gt; 
&lt;p&gt;Happy Coding! 🎉&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>nautechsystems/nautilus_trader</title>
      <link>https://github.com/nautechsystems/nautilus_trader</link>
      <description>&lt;p&gt;A high-performance algorithmic trading platform and event-driven backtester&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;img src="https://github.com/nautechsystems/nautilus_trader/raw/develop/assets/nautilus-trader-logo.png" width="500" /&gt;&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://codecov.io/gh/nautechsystems/nautilus_trader"&gt;&lt;img src="https://codecov.io/gh/nautechsystems/nautilus_trader/branch/master/graph/badge.svg?token=DXO9QQI40H" alt="codecov" /&gt;&lt;/a&gt; &lt;a href="https://codspeed.io/nautechsystems/nautilus_trader"&gt;&lt;img src="https://img.shields.io/endpoint?url=https://codspeed.io/badge.json" alt="codspeed" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/pypi/pyversions/nautilus_trader" alt="pythons" /&gt; &lt;img src="https://img.shields.io/pypi/v/nautilus_trader" alt="pypi-version" /&gt; &lt;img src="https://img.shields.io/pypi/format/nautilus_trader?color=blue" alt="pypi-format" /&gt; &lt;a href="https://pepy.tech/project/nautilus-trader"&gt;&lt;img src="https://pepy.tech/badge/nautilus-trader" alt="Downloads" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/NautilusTrader"&gt;&lt;img src="https://img.shields.io/badge/Discord-%235865F2.svg?logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Branch&lt;/th&gt; 
   &lt;th align="left"&gt;Version&lt;/th&gt; 
   &lt;th align="left"&gt;Status&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;master&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://packages.nautechsystems.io/simple/nautilus-trader/index.html"&gt;&lt;img src="https://img.shields.io/endpoint?url=https%3A%2F%2Fraw.githubusercontent.com%2Fnautechsystems%2Fnautilus_trader%2Fmaster%2Fversion.json" alt="version" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml"&gt;&lt;img src="https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml/badge.svg?branch=nightly" alt="build" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;nightly&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://packages.nautechsystems.io/simple/nautilus-trader/index.html"&gt;&lt;img src="https://img.shields.io/endpoint?url=https%3A%2F%2Fraw.githubusercontent.com%2Fnautechsystems%2Fnautilus_trader%2Fnightly%2Fversion.json" alt="version" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml"&gt;&lt;img src="https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml/badge.svg?branch=nightly" alt="build" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;develop&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://packages.nautechsystems.io/simple/nautilus-trader/index.html"&gt;&lt;img src="https://img.shields.io/endpoint?url=https%3A%2F%2Fraw.githubusercontent.com%2Fnautechsystems%2Fnautilus_trader%2Fdevelop%2Fversion.json" alt="version" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml"&gt;&lt;img src="https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml/badge.svg?branch=develop" alt="build" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Platform&lt;/th&gt; 
   &lt;th align="left"&gt;Rust&lt;/th&gt; 
   &lt;th align="left"&gt;Python&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;Linux (x86_64)&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;1.89.0&lt;/td&gt; 
   &lt;td align="left"&gt;3.11-3.13&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;Linux (ARM64)&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;1.89.0&lt;/td&gt; 
   &lt;td align="left"&gt;3.11-3.13&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;macOS (ARM64)&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;1.89.0&lt;/td&gt; 
   &lt;td align="left"&gt;3.11-3.13&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;Windows (x86_64)&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;1.89.0&lt;/td&gt; 
   &lt;td align="left"&gt;3.11-3.13*&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;* Windows builds are currently pinned to CPython 3.13.2, see &lt;a href="https://github.com/nautechsystems/nautilus_trader/raw/develop/docs/getting_started/installation.md"&gt;installation guide&lt;/a&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Docs&lt;/strong&gt;: &lt;a href="https://nautilustrader.io/docs/"&gt;https://nautilustrader.io/docs/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Website&lt;/strong&gt;: &lt;a href="https://nautilustrader.io"&gt;https://nautilustrader.io&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Support&lt;/strong&gt;: &lt;a href="mailto:support@nautilustrader.io"&gt;support@nautilustrader.io&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;NautilusTrader is an open-source, high-performance, production-grade algorithmic trading platform, providing quantitative traders with the ability to backtest portfolios of automated trading strategies on historical data with an event-driven engine, and also deploy those same strategies live, with no code changes.&lt;/p&gt; 
&lt;p&gt;The platform is &lt;em&gt;AI-first&lt;/em&gt;, designed to develop and deploy algorithmic trading strategies within a highly performant and robust Python-native environment. This helps to address the parity challenge of keeping the Python research/backtest environment consistent with the production live trading environment.&lt;/p&gt; 
&lt;p&gt;NautilusTrader's design, architecture, and implementation philosophy prioritizes software correctness and safety at the highest level, with the aim of supporting Python-native, mission-critical, trading system backtesting and live deployment workloads.&lt;/p&gt; 
&lt;p&gt;The platform is also universal, and asset-class-agnostic — with any REST API or WebSocket feed able to be integrated via modular adapters. It supports high-frequency trading across a wide range of asset classes and instrument types including FX, Equities, Futures, Options, Crypto, DeFi, and Betting, enabling seamless operations across multiple venues simultaneously.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/nautechsystems/nautilus_trader/raw/develop/assets/nautilus-trader.png" alt="nautilus-trader" title="nautilus-trader" /&gt;&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Fast&lt;/strong&gt;: Core is written in Rust with asynchronous networking using &lt;a href="https://crates.io/crates/tokio"&gt;tokio&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Reliable&lt;/strong&gt;: Rust-powered type- and thread-safety, with optional Redis-backed state persistence.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Portable&lt;/strong&gt;: OS independent, runs on Linux, macOS, and Windows. Deploy using Docker.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible&lt;/strong&gt;: Modular adapters mean any REST API or WebSocket feed can be integrated.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Advanced&lt;/strong&gt;: Time in force &lt;code&gt;IOC&lt;/code&gt;, &lt;code&gt;FOK&lt;/code&gt;, &lt;code&gt;GTC&lt;/code&gt;, &lt;code&gt;GTD&lt;/code&gt;, &lt;code&gt;DAY&lt;/code&gt;, &lt;code&gt;AT_THE_OPEN&lt;/code&gt;, &lt;code&gt;AT_THE_CLOSE&lt;/code&gt;, advanced order types and conditional triggers. Execution instructions &lt;code&gt;post-only&lt;/code&gt;, &lt;code&gt;reduce-only&lt;/code&gt;, and icebergs. Contingency orders including &lt;code&gt;OCO&lt;/code&gt;, &lt;code&gt;OUO&lt;/code&gt;, &lt;code&gt;OTO&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Customizable&lt;/strong&gt;: Add user-defined custom components, or assemble entire systems from scratch leveraging the &lt;a href="https://nautilustrader.io/docs/latest/concepts/cache"&gt;cache&lt;/a&gt; and &lt;a href="https://nautilustrader.io/docs/latest/concepts/message_bus"&gt;message bus&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Backtesting&lt;/strong&gt;: Run with multiple venues, instruments and strategies simultaneously using historical quote tick, trade tick, bar, order book and custom data with nanosecond resolution.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Live&lt;/strong&gt;: Use identical strategy implementations between backtesting and live deployments.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-venue&lt;/strong&gt;: Multiple venue capabilities facilitate market-making and statistical arbitrage strategies.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;AI Training&lt;/strong&gt;: Backtest engine fast enough to be used to train AI trading agents (RL/ES).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://github.com/nautechsystems/nautilus_trader/raw/develop/assets/nautilus-art.png" alt="Alt text" title="nautilus" /&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;nautilus - from ancient Greek 'sailor' and naus 'ship'.&lt;/em&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;The nautilus shell consists of modular chambers with a growth factor which approximates a logarithmic spiral. The idea is that this can be translated to the aesthetics of design and architecture.&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Why NautilusTrader?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Highly performant event-driven Python&lt;/strong&gt;: Native binary core components.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Parity between backtesting and live trading&lt;/strong&gt;: Identical strategy code.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Reduced operational risk&lt;/strong&gt;: Enhanced risk management functionality, logical accuracy, and type safety.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Highly extendable&lt;/strong&gt;: Message bus, custom components and actors, custom data, custom adapters.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Traditionally, trading strategy research and backtesting might be conducted in Python using vectorized methods, with the strategy then needing to be reimplemented in a more event-driven way using C++, C#, Java or other statically typed language(s). The reasoning here is that vectorized backtesting code cannot express the granular time and event dependent complexity of real-time trading, where compiled languages have proven to be more suitable due to their inherently higher performance, and type safety.&lt;/p&gt; 
&lt;p&gt;One of the key advantages of NautilusTrader here, is that this reimplementation step is now circumvented - as the critical core components of the platform have all been written entirely in &lt;a href="https://www.rust-lang.org/"&gt;Rust&lt;/a&gt; or &lt;a href="https://cython.org/"&gt;Cython&lt;/a&gt;. This means we're using the right tools for the job, where systems programming languages compile performant binaries, with CPython C extension modules then able to offer a Python-native environment, suitable for professional quantitative traders and trading firms.&lt;/p&gt; 
&lt;h2&gt;Why Python?&lt;/h2&gt; 
&lt;p&gt;Python was originally created decades ago as a simple scripting language with a clean straightforward syntax. It has since evolved into a fully fledged general purpose object-oriented programming language. Based on the TIOBE index, Python is currently the most popular programming language in the world. Not only that, Python has become the &lt;em&gt;de facto lingua franca&lt;/em&gt; of data science, machine learning, and artificial intelligence.&lt;/p&gt; 
&lt;p&gt;developer/user communities. However, Python has performance and typing limitations for large-scale, latency-sensitive systems. Cython addresses many of these issues by introducing static typing into Python's rich ecosystem of libraries and communities.&lt;/p&gt; 
&lt;h2&gt;Why Rust?&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.rust-lang.org/"&gt;Rust&lt;/a&gt; is a multi-paradigm programming language designed for performance and safety, especially safe concurrency. Rust is "blazingly fast" and memory-efficient (comparable to C and C++) with no garbage collector. It can power mission-critical systems, run on embedded devices, and easily integrates with other languages.&lt;/p&gt; 
&lt;p&gt;Rust’s rich type system and ownership model guarantees memory-safety and thread-safety deterministically — eliminating many classes of bugs at compile-time.&lt;/p&gt; 
&lt;p&gt;The project increasingly utilizes Rust for core performance-critical components. Python bindings are implemented via Cython and &lt;a href="https://pyo3.rs"&gt;PyO3&lt;/a&gt;—no Rust toolchain is required at install time.&lt;/p&gt; 
&lt;p&gt;This project makes the &lt;a href="https://raphlinus.github.io/rust/2020/01/18/soundness-pledge.html"&gt;Soundness Pledge&lt;/a&gt;:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;“The intent of this project is to be free of soundness bugs. The developers will do their best to avoid them, and welcome help in analyzing and fixing them.”&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;MSRV:&lt;/strong&gt; NautilusTrader relies heavily on improvements in the Rust language and compiler. As a result, the Minimum Supported Rust Version (MSRV) is generally equal to the latest stable release of Rust.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Integrations&lt;/h2&gt; 
&lt;p&gt;NautilusTrader is modularly designed to work with &lt;em&gt;adapters&lt;/em&gt;, enabling connectivity to trading venues and data providers by translating their raw APIs into a unified interface and normalized domain model.&lt;/p&gt; 
&lt;p&gt;The following integrations are currently supported; see &lt;a href="https://nautilustrader.io/docs/latest/integrations/"&gt;docs/integrations/&lt;/a&gt; for details:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Name&lt;/th&gt; 
   &lt;th align="left"&gt;ID&lt;/th&gt; 
   &lt;th align="left"&gt;Type&lt;/th&gt; 
   &lt;th align="left"&gt;Status&lt;/th&gt; 
   &lt;th align="left"&gt;Docs&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://betfair.com"&gt;Betfair&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;BETFAIR&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Sports Betting Exchange&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/betfair.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://binance.com"&gt;Binance&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;BINANCE&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Exchange (CEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/binance.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://binance.us"&gt;Binance US&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;BINANCE&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Exchange (CEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/binance.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://www.binance.com/en/futures"&gt;Binance Futures&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;BINANCE&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Exchange (CEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/binance.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://www.bitmex.com"&gt;BitMEX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;BITMEX&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Exchange (CEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/building-orange" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/bitmex.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://www.bybit.com"&gt;Bybit&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;BYBIT&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Exchange (CEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/bybit.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://www.coinbase.com/en/international-exchange"&gt;Coinbase International&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;COINBASE_INTX&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Exchange (CEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/coinbase_intx.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://databento.com"&gt;Databento&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;DATABENTO&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Data Provider&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/databento.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://dydx.exchange/"&gt;dYdX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;DYDX&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Exchange (DEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/dydx.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://hyperliquid.xyz"&gt;Hyperliquid&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;HYPERLIQUID&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Exchange (DEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/building-orange" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/hyperliquid.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://www.interactivebrokers.com"&gt;Interactive Brokers&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;INTERACTIVE_BROKERS&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Brokerage (multi-venue)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/ib.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://okx.com"&gt;OKX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;OKX&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Exchange (CEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/beta-yellow" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/okx.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://polymarket.com"&gt;Polymarket&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;POLYMARKET&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Prediction Market (DEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/polymarket.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://tardis.dev"&gt;Tardis&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;TARDIS&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Data Provider&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/tardis.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ID&lt;/strong&gt;: The default client ID for the integrations adapter clients.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Type&lt;/strong&gt;: The type of integration (often the venue type).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Status&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;building&lt;/code&gt;: Under construction and likely not in a usable state.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;beta&lt;/code&gt;: Completed to a minimally working state and in a beta testing phase.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;stable&lt;/code&gt;: Stabilized feature set and API, the integration has been tested by both developers and users to a reasonable level (some bugs may still remain).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See the &lt;a href="https://nautilustrader.io/docs/latest/integrations/index.html"&gt;Integrations&lt;/a&gt; documentation for further details.&lt;/p&gt; 
&lt;h2&gt;Versioning and releases&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;NautilusTrader is still under active development&lt;/strong&gt;. Some features may be incomplete, and while the API is becoming more stable, breaking changes can occur between releases. We strive to document these changes in the release notes on a &lt;strong&gt;best-effort basis&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;We aim to follow a &lt;strong&gt;bi-weekly release schedule&lt;/strong&gt;, though experimental or larger features may cause delays.&lt;/p&gt; 
&lt;h3&gt;Branches&lt;/h3&gt; 
&lt;p&gt;We aim to maintain a stable, passing build across all branches.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;master&lt;/code&gt;: Reflects the source code for the latest released version; recommended for production use.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;nightly&lt;/code&gt;: Daily snapshots of the &lt;code&gt;develop&lt;/code&gt; branch for early testing; merged at &lt;strong&gt;14:00 UTC&lt;/strong&gt; or on demand.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;develop&lt;/code&gt;: Active development branch for contributors and feature work.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;Our &lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/ROADMAP.md"&gt;roadmap&lt;/a&gt; aims to achieve a &lt;strong&gt;stable API for version 2.x&lt;/strong&gt; (likely after the Rust port). Once this milestone is reached, we plan to implement a formal deprecation process for any API changes. This approach allows us to maintain a rapid development pace for now.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Precision mode&lt;/h2&gt; 
&lt;p&gt;NautilusTrader supports two precision modes for its core value types (&lt;code&gt;Price&lt;/code&gt;, &lt;code&gt;Quantity&lt;/code&gt;, &lt;code&gt;Money&lt;/code&gt;), which differ in their internal bit-width and maximum decimal precision.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;High-precision&lt;/strong&gt;: 128-bit integers with up to 16 decimals of precision, and a larger value range.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Standard-precision&lt;/strong&gt;: 64-bit integers with up to 9 decimals of precision, and a smaller value range.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;By default, the official Python wheels &lt;strong&gt;ship&lt;/strong&gt; in high-precision (128-bit) mode on Linux and macOS. On Windows, only standard-precision (64-bit) is available due to the lack of native 128-bit integer support. For the Rust crates, the default is standard-precision unless you explicitly enable the &lt;code&gt;high-precision&lt;/code&gt; feature flag.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;See the &lt;a href="https://nautilustrader.io/docs/latest/getting_started/installation"&gt;Installation Guide&lt;/a&gt; for further details.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Rust feature flag&lt;/strong&gt;: To enable high-precision mode in Rust, add the &lt;code&gt;high-precision&lt;/code&gt; feature to your Cargo.toml:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-toml"&gt;[dependencies]
nautilus_model = { version = "*", features = ["high-precision"] }
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;We recommend using the latest supported version of Python and installing &lt;a href="https://pypi.org/project/nautilus_trader/"&gt;nautilus_trader&lt;/a&gt; inside a virtual environment to isolate dependencies.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;There are two supported ways to install&lt;/strong&gt;:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Pre-built binary wheel from PyPI &lt;em&gt;or&lt;/em&gt; the Nautech Systems package index.&lt;/li&gt; 
 &lt;li&gt;Build from source.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP]&lt;/p&gt; 
 &lt;p&gt;We highly recommend installing using the &lt;a href="https://docs.astral.sh/uv"&gt;uv&lt;/a&gt; package manager with a "vanilla" CPython.&lt;/p&gt; 
 &lt;p&gt;Conda and other Python distributions &lt;em&gt;may&lt;/em&gt; work but aren’t officially supported.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;From PyPI&lt;/h3&gt; 
&lt;p&gt;To install the latest binary wheel (or sdist package) from PyPI using Python's pip package manager:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -U nautilus_trader
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;From the Nautech Systems package index&lt;/h3&gt; 
&lt;p&gt;The Nautech Systems package index (&lt;code&gt;packages.nautechsystems.io&lt;/code&gt;) complies with &lt;a href="https://peps.python.org/pep-0503/"&gt;PEP-503&lt;/a&gt; and hosts both stable and development binary wheels for &lt;code&gt;nautilus_trader&lt;/code&gt;. This enables users to install either the latest stable release or pre-release versions for testing.&lt;/p&gt; 
&lt;h4&gt;Stable wheels&lt;/h4&gt; 
&lt;p&gt;Stable wheels correspond to official releases of &lt;code&gt;nautilus_trader&lt;/code&gt; on PyPI, and use standard versioning.&lt;/p&gt; 
&lt;p&gt;To install the latest stable release:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -U nautilus_trader --index-url=https://packages.nautechsystems.io/simple
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Development wheels&lt;/h4&gt; 
&lt;p&gt;Development wheels are published from both the &lt;code&gt;nightly&lt;/code&gt; and &lt;code&gt;develop&lt;/code&gt; branches, allowing users to test features and fixes ahead of stable releases.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Wheels from the &lt;code&gt;develop&lt;/code&gt; branch are only built for the Linux x86_64 platform to save time and compute resources, while &lt;code&gt;nightly&lt;/code&gt; wheels support additional platforms as shown below.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Platform&lt;/th&gt; 
   &lt;th align="left"&gt;Nightly&lt;/th&gt; 
   &lt;th align="left"&gt;Develop&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;Linux (x86_64)&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;✓&lt;/td&gt; 
   &lt;td align="left"&gt;✓&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;Linux (ARM64)&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;✓&lt;/td&gt; 
   &lt;td align="left"&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;macOS (ARM64)&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;✓&lt;/td&gt; 
   &lt;td align="left"&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;Windows (x86_64)&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;✓&lt;/td&gt; 
   &lt;td align="left"&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;This process also helps preserve compute resources and ensures easy access to the exact binaries tested in CI pipelines, while adhering to &lt;a href="https://peps.python.org/pep-0440/"&gt;PEP-440&lt;/a&gt; versioning standards:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;develop&lt;/code&gt; wheels use the version format &lt;code&gt;dev{date}+{build_number}&lt;/code&gt; (e.g., &lt;code&gt;1.208.0.dev20241212+7001&lt;/code&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;nightly&lt;/code&gt; wheels use the version format &lt;code&gt;a{date}&lt;/code&gt; (alpha) (e.g., &lt;code&gt;1.208.0a20241212&lt;/code&gt;).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING]&lt;/p&gt; 
 &lt;p&gt;We do not recommend using development wheels in production environments, such as live trading controlling real capital.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Installation commands&lt;/h4&gt; 
&lt;p&gt;By default, pip will install the latest stable release. Adding the &lt;code&gt;--pre&lt;/code&gt; flag ensures that pre-release versions, including development wheels, are considered.&lt;/p&gt; 
&lt;p&gt;To install the latest available pre-release (including development wheels):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -U nautilus_trader --pre --index-url=https://packages.nautechsystems.io/simple
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To install a specific development wheel (e.g., &lt;code&gt;1.208.0a20241212&lt;/code&gt; for December 12, 2024):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install nautilus_trader==1.208.0a20241212 --index-url=https://packages.nautechsystems.io/simple
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Available versions&lt;/h4&gt; 
&lt;p&gt;You can view all available versions of &lt;code&gt;nautilus_trader&lt;/code&gt; on the &lt;a href="https://packages.nautechsystems.io/simple/nautilus-trader/index.html"&gt;package index&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To programmatically fetch and list available versions:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -s https://packages.nautechsystems.io/simple/nautilus-trader/index.html | grep -oP '(?&amp;lt;=&amp;lt;a href=")[^"]+(?=")' | awk -F'#' '{print $1}' | sort
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Branch updates&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;develop&lt;/code&gt; branch wheels (&lt;code&gt;.dev&lt;/code&gt;): Build and publish continuously with every merged commit.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;nightly&lt;/code&gt; branch wheels (&lt;code&gt;a&lt;/code&gt;): Build and publish daily when we automatically merge the &lt;code&gt;develop&lt;/code&gt; branch at &lt;strong&gt;14:00 UTC&lt;/strong&gt; (if there are changes).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Retention policies&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;develop&lt;/code&gt; branch wheels (&lt;code&gt;.dev&lt;/code&gt;): We retain only the most recent wheel build.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;nightly&lt;/code&gt; branch wheels (&lt;code&gt;a&lt;/code&gt;): We retain only the 10 most recent wheel builds.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;From Source&lt;/h3&gt; 
&lt;p&gt;It's possible to install from source using pip if you first install the build dependencies as specified in the &lt;code&gt;pyproject.toml&lt;/code&gt;.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Install &lt;a href="https://rustup.rs/"&gt;rustup&lt;/a&gt; (the Rust toolchain installer):&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;Linux and macOS:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;curl https://sh.rustup.rs -sSf | sh
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Windows:&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Download and install &lt;a href="https://win.rustup.rs/x86_64"&gt;&lt;code&gt;rustup-init.exe&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;Install "Desktop development with C++" with &lt;a href="https://visualstudio.microsoft.com/thank-you-downloading-visual-studio/?sku=BuildTools&amp;amp;rel=16"&gt;Build Tools for Visual Studio 2019&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Verify (any system): from a terminal session run: &lt;code&gt;rustc --version&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Enable &lt;code&gt;cargo&lt;/code&gt; in the current shell:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;Linux and macOS:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;source $HOME/.cargo/env
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Windows:&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Start a new PowerShell&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install &lt;a href="https://clang.llvm.org/"&gt;clang&lt;/a&gt; (a C language frontend for LLVM):&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;Linux:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt-get install clang
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Windows:&lt;/p&gt; 
    &lt;ol&gt; 
     &lt;li&gt; &lt;p&gt;Add Clang to your &lt;a href="https://visualstudio.microsoft.com/thank-you-downloading-visual-studio/?sku=BuildTools&amp;amp;rel=16"&gt;Build Tools for Visual Studio 2019&lt;/a&gt;:&lt;/p&gt; 
      &lt;ul&gt; 
       &lt;li&gt;Start | Visual Studio Installer | Modify | C++ Clang tools for Windows (12.0.0 - x64…) = checked | Modify&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;Enable &lt;code&gt;clang&lt;/code&gt; in the current shell:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-powershell"&gt;[System.Environment]::SetEnvironmentVariable('path', "C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\VC\Tools\Llvm\x64\bin\;" + $env:Path,"User")
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
    &lt;/ol&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Verify (any system): from a terminal session run: &lt;code&gt;clang --version&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install uv (see the &lt;a href="https://docs.astral.sh/uv/getting-started/installation"&gt;uv installation guide&lt;/a&gt; for more details):&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;curl -LsSf https://astral.sh/uv/install.sh | sh
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Clone the source with &lt;code&gt;git&lt;/code&gt;, and install from the project's root directory:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone --branch develop --depth 1 https://github.com/nautechsystems/nautilus_trader
cd nautilus_trader
uv sync --all-extras
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;The &lt;code&gt;--depth 1&lt;/code&gt; flag fetches just the latest commit for a faster, lightweight clone.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ol start="6"&gt; 
 &lt;li&gt; &lt;p&gt;Set environment variables for PyO3 compilation (Linux and macOS only):&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Set the library path for the Python interpreter (in this case Python 3.13.4)
export LD_LIBRARY_PATH="$HOME/.local/share/uv/python/cpython-3.13.4-linux-x86_64-gnu/lib:$LD_LIBRARY_PATH"

# Set the Python executable path for PyO3
export PYO3_PYTHON=$(pwd)/.venv/bin/python
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;Adjust the Python version and architecture in the &lt;code&gt;LD_LIBRARY_PATH&lt;/code&gt; to match your system. Use &lt;code&gt;uv python list&lt;/code&gt; to find the exact path for your Python installation.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;See the &lt;a href="https://nautilustrader.io/docs/latest/getting_started/installation"&gt;Installation Guide&lt;/a&gt; for other options and further details.&lt;/p&gt; 
&lt;h2&gt;Redis&lt;/h2&gt; 
&lt;p&gt;Using &lt;a href="https://redis.io"&gt;Redis&lt;/a&gt; with NautilusTrader is &lt;strong&gt;optional&lt;/strong&gt; and only required if configured as the backend for a &lt;a href="https://nautilustrader.io/docs/latest/concepts/cache"&gt;cache&lt;/a&gt; database or &lt;a href="https://nautilustrader.io/docs/latest/concepts/message_bus"&gt;message bus&lt;/a&gt;. See the &lt;strong&gt;Redis&lt;/strong&gt; section of the &lt;a href="https://nautilustrader.io/docs/latest/getting_started/installation#redis"&gt;Installation Guide&lt;/a&gt; for further details.&lt;/p&gt; 
&lt;h2&gt;Makefile&lt;/h2&gt; 
&lt;p&gt;A &lt;code&gt;Makefile&lt;/code&gt; is provided to automate most installation and build tasks for development. Some of the targets include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;make install&lt;/code&gt;: Installs in &lt;code&gt;release&lt;/code&gt; build mode with all dependency groups and extras.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make install-debug&lt;/code&gt;: Same as &lt;code&gt;make install&lt;/code&gt; but with &lt;code&gt;debug&lt;/code&gt; build mode.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make install-just-deps&lt;/code&gt;: Installs just the &lt;code&gt;main&lt;/code&gt;, &lt;code&gt;dev&lt;/code&gt; and &lt;code&gt;test&lt;/code&gt; dependencies (does not install package).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make build&lt;/code&gt;: Runs the build script in &lt;code&gt;release&lt;/code&gt; build mode (default).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make build-debug&lt;/code&gt;: Runs the build script in &lt;code&gt;debug&lt;/code&gt; build mode.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make build-wheel&lt;/code&gt;: Runs uv build with a wheel format in &lt;code&gt;release&lt;/code&gt; mode.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make build-wheel-debug&lt;/code&gt;: Runs uv build with a wheel format in &lt;code&gt;debug&lt;/code&gt; mode.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make cargo-test&lt;/code&gt;: Runs all Rust crate tests using &lt;code&gt;cargo-nextest&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make clean&lt;/code&gt;: Deletes all build results, such as &lt;code&gt;.so&lt;/code&gt; or &lt;code&gt;.dll&lt;/code&gt; files.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make distclean&lt;/code&gt;: &lt;strong&gt;CAUTION&lt;/strong&gt; Removes all artifacts not in the git index from the repository. This includes source files which have not been &lt;code&gt;git add&lt;/code&gt;ed.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make docs&lt;/code&gt;: Builds the documentation HTML using Sphinx.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make pre-commit&lt;/code&gt;: Runs the pre-commit checks over all files.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make ruff&lt;/code&gt;: Runs ruff over all files using the &lt;code&gt;pyproject.toml&lt;/code&gt; config (with autofix).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make pytest&lt;/code&gt;: Runs all tests with &lt;code&gt;pytest&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make test-performance&lt;/code&gt;: Runs performance tests with &lt;a href="https://codspeed.io"&gt;codspeed&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP]&lt;/p&gt; 
 &lt;p&gt;Run &lt;code&gt;make help&lt;/code&gt; for documentation on all available make targets.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP]&lt;/p&gt; 
 &lt;p&gt;See the &lt;a href="https://github.com/nautechsystems/nautilus_trader/raw/develop/crates/infrastructure/TESTS.md"&gt;crates/infrastructure/TESTS.md&lt;/a&gt; file for running the infrastructure integration tests.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;p&gt;Indicators and strategies can be developed in both Python and Cython. For performance and latency-sensitive applications, we recommend using Cython. Below are some examples:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/nautilus_trader/examples/indicators/ema_python.py"&gt;indicator&lt;/a&gt; example written in Python.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/nautilus_trader/indicators/"&gt;indicator&lt;/a&gt; examples written in Cython.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/nautilus_trader/examples/strategies/"&gt;strategy&lt;/a&gt; examples written in Python.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/examples/backtest/"&gt;backtest&lt;/a&gt; examples using a &lt;code&gt;BacktestEngine&lt;/code&gt; directly.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Docker&lt;/h2&gt; 
&lt;p&gt;Docker containers are built using the base image &lt;code&gt;python:3.12-slim&lt;/code&gt; with the following variant tags:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;nautilus_trader:latest&lt;/code&gt; has the latest release version installed.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;nautilus_trader:nightly&lt;/code&gt; has the head of the &lt;code&gt;nightly&lt;/code&gt; branch installed.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;jupyterlab:latest&lt;/code&gt; has the latest release version installed along with &lt;code&gt;jupyterlab&lt;/code&gt; and an example backtest notebook with accompanying data.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;jupyterlab:nightly&lt;/code&gt; has the head of the &lt;code&gt;nightly&lt;/code&gt; branch installed along with &lt;code&gt;jupyterlab&lt;/code&gt; and an example backtest notebook with accompanying data.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can pull the container images as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker pull ghcr.io/nautechsystems/&amp;lt;image_variant_tag&amp;gt; --platform linux/amd64
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can launch the backtest example container by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker pull ghcr.io/nautechsystems/jupyterlab:nightly --platform linux/amd64
docker run -p 8888:8888 ghcr.io/nautechsystems/jupyterlab:nightly
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then open your browser at the following address:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;http://127.0.0.1:8888/lab
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING]&lt;/p&gt; 
 &lt;p&gt;NautilusTrader currently exceeds the rate limit for Jupyter notebook logging (stdout output). Therefore, we set the &lt;code&gt;log_level&lt;/code&gt; to &lt;code&gt;ERROR&lt;/code&gt; in the examples. Lowering this level to see more logging will cause the notebook to hang during cell execution. We are investigating a fix that may involve either raising the configured rate limits for Jupyter or throttling the log flushing from Nautilus.&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://github.com/jupyterlab/jupyterlab/issues/12845"&gt;https://github.com/jupyterlab/jupyterlab/issues/12845&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/deshaw/jupyterlab-limit-output"&gt;https://github.com/deshaw/jupyterlab-limit-output&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Development&lt;/h2&gt; 
&lt;p&gt;We aim to provide the most pleasant developer experience possible for this hybrid codebase of Python, Cython and Rust. See the &lt;a href="https://nautilustrader.io/docs/latest/developer_guide/index.html"&gt;Developer Guide&lt;/a&gt; for helpful information.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP]&lt;/p&gt; 
 &lt;p&gt;Run &lt;code&gt;make build-debug&lt;/code&gt; to compile after changes to Rust or Cython code for the most efficient development workflow.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Testing with Rust&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://nexte.st"&gt;cargo-nextest&lt;/a&gt; is the standard Rust test runner for NautilusTrader. Its key benefit is isolating each test in its own process, ensuring test reliability by avoiding interference.&lt;/p&gt; 
&lt;p&gt;You can install cargo-nextest by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cargo install cargo-nextest
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP]&lt;/p&gt; 
 &lt;p&gt;Run Rust tests with &lt;code&gt;make cargo-test&lt;/code&gt;, which uses &lt;strong&gt;cargo-nextest&lt;/strong&gt; with an efficient profile.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Thank you for considering contributing to NautilusTrader! We welcome any and all help to improve the project. If you have an idea for an enhancement or a bug fix, the first step is to open an &lt;a href="https://github.com/nautechsystems/nautilus_trader/issues"&gt;issue&lt;/a&gt; on GitHub to discuss it with the team. This helps to ensure that your contribution will be well-aligned with the goals of the project and avoids duplication of effort.&lt;/p&gt; 
&lt;p&gt;Before getting started, be sure to review the &lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/ROADMAP.md#open-source-scope"&gt;open-source scope&lt;/a&gt; outlined in the project’s roadmap to understand what’s in and out of scope.&lt;/p&gt; 
&lt;p&gt;Once you're ready to start working on your contribution, make sure to follow the guidelines outlined in the &lt;a href="https://github.com/nautechsystems/nautilus_trader/raw/develop/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; file. This includes signing a Contributor License Agreement (CLA) to ensure that your contributions can be included in the project.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;Pull requests should target the &lt;code&gt;develop&lt;/code&gt; branch (the default branch). This is where new features and improvements are integrated before release.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Thank you again for your interest in NautilusTrader! We look forward to reviewing your contributions and working with you to improve the project.&lt;/p&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;p&gt;Join our community of users and contributors on &lt;a href="https://discord.gg/NautilusTrader"&gt;Discord&lt;/a&gt; to chat and stay up-to-date with the latest announcements and features of NautilusTrader. Whether you're a developer looking to contribute or just want to learn more about the platform, all are welcome on our Discord server.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING]&lt;/p&gt; 
 &lt;p&gt;NautilusTrader does not issue, promote, or endorse any cryptocurrency tokens. Any claims or communications suggesting otherwise are unauthorized and false.&lt;/p&gt; 
 &lt;p&gt;All official updates and communications from NautilusTrader will be shared exclusively through &lt;a href="https://nautilustrader.io"&gt;https://nautilustrader.io&lt;/a&gt;, our &lt;a href="https://discord.gg/NautilusTrader"&gt;Discord server&lt;/a&gt;, or our X (Twitter) account: &lt;a href="https://x.com/NautilusTrader"&gt;@NautilusTrader&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;If you encounter any suspicious activity, please report it to the appropriate platform and contact us at &lt;a href="mailto:info@nautechsystems.io"&gt;info@nautechsystems.io&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;The source code for NautilusTrader is available on GitHub under the &lt;a href="https://www.gnu.org/licenses/lgpl-3.0.en.html"&gt;GNU Lesser General Public License v3.0&lt;/a&gt;. Contributions to the project are welcome and require the completion of a standard &lt;a href="https://github.com/nautechsystems/nautilus_trader/raw/develop/CLA.md"&gt;Contributor License Agreement (CLA)&lt;/a&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;NautilusTrader™ is developed and maintained by Nautech Systems, a technology company specializing in the development of high-performance trading systems. For more information, visit &lt;a href="https://nautilustrader.io"&gt;https://nautilustrader.io&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;© 2015-2025 Nautech Systems Pty Ltd. All rights reserved.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/nautechsystems/nautilus_trader/raw/develop/assets/ns-logo.png" alt="nautechsystems" title="nautechsystems" /&gt; &lt;img src="https://github.com/nautechsystems/nautilus_trader/raw/develop/assets/ferris.png" width="128" /&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>openai/openai-go</title>
      <link>https://github.com/openai/openai-go</link>
      <description>&lt;p&gt;The official Go library for the OpenAI API&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OpenAI Go API Library&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://pkg.go.dev/github.com/openai/openai-go/v2"&gt;&lt;img src="https://pkg.go.dev/badge/github.com/openai/openai-go.svg?sanitize=true" alt="Go Reference" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;The OpenAI Go library provides convenient access to the &lt;a href="https://platform.openai.com/docs"&gt;OpenAI REST API&lt;/a&gt; from applications written in Go.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING] The latest version of this package has small and limited breaking changes. See the &lt;a href="https://raw.githubusercontent.com/openai/openai-go/main/CHANGELOG.md"&gt;changelog&lt;/a&gt; for details.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;!-- x-release-please-start-version --&gt; 
&lt;pre&gt;&lt;code class="language-go"&gt;import (
	"github.com/openai/openai-go/v2" // imported as openai
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;!-- x-release-please-end --&gt; 
&lt;p&gt;Or to pin the version:&lt;/p&gt; 
&lt;!-- x-release-please-start-version --&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;go get -u 'github.com/openai/openai-go@v2.0.2'
&lt;/code&gt;&lt;/pre&gt; 
&lt;!-- x-release-please-end --&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;p&gt;This library requires Go 1.21+.&lt;/p&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;p&gt;The full API of this library can be found in &lt;a href="https://raw.githubusercontent.com/openai/openai-go/main/api.md"&gt;api.md&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-go"&gt;package main

import (
	"context"
	"fmt"

	"github.com/openai/openai-go/v2"
	"github.com/openai/openai-go/v2/option"
	"github.com/openai/openai-go/v2/shared"
)

func main() {
	client := openai.NewClient(
		option.WithAPIKey("My API Key"), // defaults to os.LookupEnv("OPENAI_API_KEY")
	)
	chatCompletion, err := client.Chat.Completions.New(context.TODO(), openai.ChatCompletionNewParams{
		Messages: []openai.ChatCompletionMessageParamUnion{
			openai.UserMessage("Say this is a test"),
		},
		Model: openai.ChatModelGPT4o,
	})
	if err != nil {
		panic(err.Error())
	}
	println(chatCompletion.Choices[0].Message.Content)
}

&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;Conversations&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-go"&gt;param := openai.ChatCompletionNewParams{
	Messages: []openai.ChatCompletionMessageParamUnion{
		openai.UserMessage("What kind of houseplant is easy to take care of?"),
	},
	Seed:     openai.Int(1),
	Model:    openai.ChatModelGPT4o,
}

completion, err := client.Chat.Completions.New(ctx, param)

param.Messages = append(param.Messages, completion.Choices[0].Message.ToParam())
param.Messages = append(param.Messages, openai.UserMessage("How big are those?"))

// continue the conversation
completion, err = client.Chat.Completions.New(ctx, param)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Streaming responses&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-go"&gt;question := "Write an epic"

stream := client.Chat.Completions.NewStreaming(ctx, openai.ChatCompletionNewParams{
	Messages: []openai.ChatCompletionMessageParamUnion{
		openai.UserMessage(question),
	},
	Seed:  openai.Int(0),
	Model: openai.ChatModelGPT4o,
})

// optionally, an accumulator helper can be used
acc := openai.ChatCompletionAccumulator{}

for stream.Next() {
	chunk := stream.Current()
	acc.AddChunk(chunk)

	if content, ok := acc.JustFinishedContent(); ok {
		println("Content stream finished:", content)
	}

	// if using tool calls
	if tool, ok := acc.JustFinishedToolCall(); ok {
		println("Tool call stream finished:", tool.Index, tool.Name, tool.Arguments)
	}

	if refusal, ok := acc.JustFinishedRefusal(); ok {
		println("Refusal stream finished:", refusal)
	}

	// it's best to use chunks after handling JustFinished events
	if len(chunk.Choices) &amp;gt; 0 {
		println(chunk.Choices[0].Delta.Content)
	}
}

if stream.Err() != nil {
	panic(stream.Err())
}

// After the stream is finished, acc can be used like a ChatCompletion
_ = acc.Choices[0].Message.Content
&lt;/code&gt;&lt;/pre&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/openai/openai-go/main/examples/chat-completion-accumulating/main.go"&gt;full streaming and accumulation example&lt;/a&gt;&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Tool calling&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-go"&gt;import (
	"encoding/json"
	// ...
)

// ...

question := "What is the weather in New York City?"

params := openai.ChatCompletionNewParams{
	Messages: []openai.ChatCompletionMessageParamUnion{
		openai.UserMessage(question),
	},
	Tools: []openai.ChatCompletionToolParam{
		{
			Function: openai.FunctionDefinitionParam{
				Name:        "get_weather",
				Description: openai.String("Get weather at the given location"),
				Parameters: openai.FunctionParameters{
					"type": "object",
					"properties": map[string]interface{}{
						"location": map[string]string{
							"type": "string",
						},
					},
					"required": []string{"location"},
				},
			},
		},
	},
	Model: openai.ChatModelGPT4o,
}

// If there is a was a function call, continue the conversation
params.Messages = append(params.Messages, completion.Choices[0].Message.ToParam())
for _, toolCall := range toolCalls {
	if toolCall.Function.Name == "get_weather" {
		// Extract the location from the function call arguments
		var args map[string]interface{}
		err := json.Unmarshal([]byte(toolCall.Function.Arguments), &amp;amp;args)
		if err != nil {
			panic(err)
		}
		location := args["location"].(string)

		// Simulate getting weather data
		weatherData := getWeather(location)

		// Print the weather data
		fmt.Printf("Weather in %s: %s\n", location, weatherData)

		params.Messages = append(params.Messages, openai.ToolMessage(weatherData, toolCall.ID))
	}
}

// ... continue the conversation with the information provided by the tool
&lt;/code&gt;&lt;/pre&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/openai/openai-go/main/examples/chat-completion-tool-calling/main.go"&gt;full tool calling example&lt;/a&gt;&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Structured outputs&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-go"&gt;import (
	"encoding/json"
	"github.com/invopop/jsonschema"
	// ...
)

// A struct that will be converted to a Structured Outputs response schema
type HistoricalComputer struct {
	Origin       Origin   `json:"origin" jsonschema_description:"The origin of the computer"`
	Name         string   `json:"full_name" jsonschema_description:"The name of the device model"`
	Legacy       string   `json:"legacy" jsonschema:"enum=positive,enum=neutral,enum=negative" jsonschema_description:"Its influence on the field of computing"`
	NotableFacts []string `json:"notable_facts" jsonschema_description:"A few key facts about the computer"`
}

type Origin struct {
	YearBuilt    int64  `json:"year_of_construction" jsonschema_description:"The year it was made"`
	Organization string `json:"organization" jsonschema_description:"The organization that was in charge of its development"`
}

func GenerateSchema[T any]() interface{} {
	// Structured Outputs uses a subset of JSON schema
	// These flags are necessary to comply with the subset
	reflector := jsonschema.Reflector{
		AllowAdditionalProperties: false,
		DoNotReference:            true,
	}
	var v T
	schema := reflector.Reflect(v)
	return schema
}

// Generate the JSON schema at initialization time
var HistoricalComputerResponseSchema = GenerateSchema[HistoricalComputer]()

func main() {

	// ...

	question := "What computer ran the first neural network?"

	schemaParam := openai.ResponseFormatJSONSchemaJSONSchemaParam{
		Name:        "historical_computer",
		Description: openai.String("Notable information about a computer"),
		Schema:      HistoricalComputerResponseSchema,
		Strict:      openai.Bool(true),
	}

	chat, _ := client.Chat.Completions.New(ctx, openai.ChatCompletionNewParams{
		// ...
		ResponseFormat: openai.ChatCompletionNewParamsResponseFormatUnion{
			OfJSONSchema: &amp;amp;openai.ResponseFormatJSONSchemaParam{
				JSONSchema: schemaParam,
			},
		},
		// only certain models can perform structured outputs
		Model: openai.ChatModelGPT4o2024_08_06,
	})

	// extract into a well-typed struct
	var historicalComputer HistoricalComputer
	_ = json.Unmarshal([]byte(chat.Choices[0].Message.Content), &amp;amp;historicalComputer)

	historicalComputer.Name
	historicalComputer.Origin.YearBuilt
	historicalComputer.Origin.Organization
	for i, fact := range historicalComputer.NotableFacts {
		// ...
	}
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/openai/openai-go/main/examples/structured-outputs/main.go"&gt;full structured outputs example&lt;/a&gt;&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/details&gt; 
&lt;h3&gt;Request fields&lt;/h3&gt; 
&lt;p&gt;The openai library uses the &lt;a href="https://tip.golang.org/doc/go1.24#encodingjsonpkgencodingjson"&gt;&lt;code&gt;omitzero&lt;/code&gt;&lt;/a&gt; semantics from the Go 1.24+ &lt;code&gt;encoding/json&lt;/code&gt; release for request fields.&lt;/p&gt; 
&lt;p&gt;Required primitive fields (&lt;code&gt;int64&lt;/code&gt;, &lt;code&gt;string&lt;/code&gt;, etc.) feature the tag &lt;code&gt;`json:"...,required"`&lt;/code&gt;. These fields are always serialized, even their zero values.&lt;/p&gt; 
&lt;p&gt;Optional primitive types are wrapped in a &lt;code&gt;param.Opt[T]&lt;/code&gt;. These fields can be set with the provided constructors, &lt;code&gt;openai.String(string)&lt;/code&gt;, &lt;code&gt;openai.Int(int64)&lt;/code&gt;, etc.&lt;/p&gt; 
&lt;p&gt;Any &lt;code&gt;param.Opt[T]&lt;/code&gt;, map, slice, struct or string enum uses the tag &lt;code&gt;`json:"...,omitzero"`&lt;/code&gt;. Its zero value is considered omitted.&lt;/p&gt; 
&lt;p&gt;The &lt;code&gt;param.IsOmitted(any)&lt;/code&gt; function can confirm the presence of any &lt;code&gt;omitzero&lt;/code&gt; field.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-go"&gt;p := openai.ExampleParams{
	ID:   "id_xxx",             // required property
	Name: openai.String("..."), // optional property

	Point: openai.Point{
		X: 0,             // required field will serialize as 0
		Y: openai.Int(1), // optional field will serialize as 1
		// ... omitted non-required fields will not be serialized
	},

	Origin: openai.Origin{}, // the zero value of [Origin] is considered omitted
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To send &lt;code&gt;null&lt;/code&gt; instead of a &lt;code&gt;param.Opt[T]&lt;/code&gt;, use &lt;code&gt;param.Null[T]()&lt;/code&gt;. To send &lt;code&gt;null&lt;/code&gt; instead of a struct &lt;code&gt;T&lt;/code&gt;, use &lt;code&gt;param.NullStruct[T]()&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-go"&gt;p.Name = param.Null[string]()       // 'null' instead of string
p.Point = param.NullStruct[Point]() // 'null' instead of struct

param.IsNull(p.Name)  // true
param.IsNull(p.Point) // true
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Request structs contain a &lt;code&gt;.SetExtraFields(map[string]any)&lt;/code&gt; method which can send non-conforming fields in the request body. Extra fields overwrite any struct fields with a matching key. For security reasons, only use &lt;code&gt;SetExtraFields&lt;/code&gt; with trusted data.&lt;/p&gt; 
&lt;p&gt;To send a custom value instead of a struct, use &lt;code&gt;param.Override[T](value)&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-go"&gt;// In cases where the API specifies a given type,
// but you want to send something else, use [SetExtraFields]:
p.SetExtraFields(map[string]any{
	"x": 0.01, // send "x" as a float instead of int
})

// Send a number instead of an object
custom := param.Override[openai.FooParams](12)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Request unions&lt;/h3&gt; 
&lt;p&gt;Unions are represented as a struct with fields prefixed by "Of" for each of it's variants, only one field can be non-zero. The non-zero field will be serialized.&lt;/p&gt; 
&lt;p&gt;Sub-properties of the union can be accessed via methods on the union struct. These methods return a mutable pointer to the underlying data, if present.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-go"&gt;// Only one field can be non-zero, use param.IsOmitted() to check if a field is set
type AnimalUnionParam struct {
	OfCat *Cat `json:",omitzero,inline`
	OfDog *Dog `json:",omitzero,inline`
}

animal := AnimalUnionParam{
	OfCat: &amp;amp;Cat{
		Name: "Whiskers",
		Owner: PersonParam{
			Address: AddressParam{Street: "3333 Coyote Hill Rd", Zip: 0},
		},
	},
}

// Mutating a field
if address := animal.GetOwner().GetAddress(); address != nil {
	address.ZipCode = 94304
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Response objects&lt;/h3&gt; 
&lt;p&gt;All fields in response structs are ordinary value types (not pointers or wrappers). Response structs also include a special &lt;code&gt;JSON&lt;/code&gt; field containing metadata about each property.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-go"&gt;type Animal struct {
	Name   string `json:"name,nullable"`
	Owners int    `json:"owners"`
	Age    int    `json:"age"`
	JSON   struct {
		Name        respjson.Field
		Owner       respjson.Field
		Age         respjson.Field
		ExtraFields map[string]respjson.Field
	} `json:"-"`
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To handle optional data, use the &lt;code&gt;.Valid()&lt;/code&gt; method on the JSON field. &lt;code&gt;.Valid()&lt;/code&gt; returns true if a field is not &lt;code&gt;null&lt;/code&gt;, not present, or couldn't be marshaled.&lt;/p&gt; 
&lt;p&gt;If &lt;code&gt;.Valid()&lt;/code&gt; is false, the corresponding field will simply be its zero value.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-go"&gt;raw := `{"owners": 1, "name": null}`

var res Animal
json.Unmarshal([]byte(raw), &amp;amp;res)

// Accessing regular fields

res.Owners // 1
res.Name   // ""
res.Age    // 0

// Optional field checks

res.JSON.Owners.Valid() // true
res.JSON.Name.Valid()   // false
res.JSON.Age.Valid()    // false

// Raw JSON values

res.JSON.Owners.Raw()                  // "1"
res.JSON.Name.Raw() == "null"          // true
res.JSON.Name.Raw() == respjson.Null   // true
res.JSON.Age.Raw() == ""               // true
res.JSON.Age.Raw() == respjson.Omitted // true
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;These &lt;code&gt;.JSON&lt;/code&gt; structs also include an &lt;code&gt;ExtraFields&lt;/code&gt; map containing any properties in the json response that were not specified in the struct. This can be useful for API features not yet present in the SDK.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-go"&gt;body := res.JSON.ExtraFields["my_unexpected_field"].Raw()
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Response Unions&lt;/h3&gt; 
&lt;p&gt;In responses, unions are represented by a flattened struct containing all possible fields from each of the object variants. To convert it to a variant use the &lt;code&gt;.AsFooVariant()&lt;/code&gt; method or the &lt;code&gt;.AsAny()&lt;/code&gt; method if present.&lt;/p&gt; 
&lt;p&gt;If a response value union contains primitive values, primitive fields will be alongside the properties but prefixed with &lt;code&gt;Of&lt;/code&gt; and feature the tag &lt;code&gt;json:"...,inline"&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-go"&gt;type AnimalUnion struct {
	// From variants [Dog], [Cat]
	Owner Person `json:"owner"`
	// From variant [Dog]
	DogBreed string `json:"dog_breed"`
	// From variant [Cat]
	CatBreed string `json:"cat_breed"`
	// ...

	JSON struct {
		Owner respjson.Field
		// ...
	} `json:"-"`
}

// If animal variant
if animal.Owner.Address.ZipCode == "" {
	panic("missing zip code")
}

// Switch on the variant
switch variant := animal.AsAny().(type) {
case Dog:
case Cat:
default:
	panic("unexpected type")
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;RequestOptions&lt;/h3&gt; 
&lt;p&gt;This library uses the functional options pattern. Functions defined in the &lt;code&gt;option&lt;/code&gt; package return a &lt;code&gt;RequestOption&lt;/code&gt;, which is a closure that mutates a &lt;code&gt;RequestConfig&lt;/code&gt;. These options can be supplied to the client or at individual requests. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-go"&gt;client := openai.NewClient(
	// Adds a header to every request made by the client
	option.WithHeader("X-Some-Header", "custom_header_info"),
)

client.Chat.Completions.New(context.TODO(), ...,
	// Override the header
	option.WithHeader("X-Some-Header", "some_other_custom_header_info"),
	// Add an undocumented field to the request body, using sjson syntax
	option.WithJSONSet("some.json.path", map[string]string{"my": "object"}),
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The request option &lt;code&gt;option.WithDebugLog(nil)&lt;/code&gt; may be helpful while debugging.&lt;/p&gt; 
&lt;p&gt;See the &lt;a href="https://pkg.go.dev/github.com/openai/openai-go/option"&gt;full list of request options&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Pagination&lt;/h3&gt; 
&lt;p&gt;This library provides some conveniences for working with paginated list endpoints.&lt;/p&gt; 
&lt;p&gt;You can use &lt;code&gt;.ListAutoPaging()&lt;/code&gt; methods to iterate through items across all pages:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-go"&gt;iter := client.FineTuning.Jobs.ListAutoPaging(context.TODO(), openai.FineTuningJobListParams{
	Limit: openai.Int(20),
})
// Automatically fetches more pages as needed.
for iter.Next() {
	fineTuningJob := iter.Current()
	fmt.Printf("%+v\n", fineTuningJob)
}
if err := iter.Err(); err != nil {
	panic(err.Error())
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or you can use simple &lt;code&gt;.List()&lt;/code&gt; methods to fetch a single page and receive a standard response object with additional helper methods like &lt;code&gt;.GetNextPage()&lt;/code&gt;, e.g.:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-go"&gt;page, err := client.FineTuning.Jobs.List(context.TODO(), openai.FineTuningJobListParams{
	Limit: openai.Int(20),
})
for page != nil {
	for _, job := range page.Data {
		fmt.Printf("%+v\n", job)
	}
	page, err = page.GetNextPage()
}
if err != nil {
	panic(err.Error())
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Errors&lt;/h3&gt; 
&lt;p&gt;When the API returns a non-success status code, we return an error with type &lt;code&gt;*openai.Error&lt;/code&gt;. This contains the &lt;code&gt;StatusCode&lt;/code&gt;, &lt;code&gt;*http.Request&lt;/code&gt;, and &lt;code&gt;*http.Response&lt;/code&gt; values of the request, as well as the JSON of the error body (much like other response objects in the SDK).&lt;/p&gt; 
&lt;p&gt;To handle errors, we recommend that you use the &lt;code&gt;errors.As&lt;/code&gt; pattern:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-go"&gt;_, err := client.FineTuning.Jobs.New(context.TODO(), openai.FineTuningJobNewParams{
	Model:        openai.FineTuningJobNewParamsModelBabbage002,
	TrainingFile: "file-abc123",
})
if err != nil {
	var apierr *openai.Error
	if errors.As(err, &amp;amp;apierr) {
		println(string(apierr.DumpRequest(true)))  // Prints the serialized HTTP request
		println(string(apierr.DumpResponse(true))) // Prints the serialized HTTP response
	}
	panic(err.Error()) // GET "/fine_tuning/jobs": 400 Bad Request { ... }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;When other errors occur, they are returned unwrapped; for example, if HTTP transport fails, you might receive &lt;code&gt;*url.Error&lt;/code&gt; wrapping &lt;code&gt;*net.OpError&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Timeouts&lt;/h3&gt; 
&lt;p&gt;Requests do not time out by default; use context to configure a timeout for a request lifecycle.&lt;/p&gt; 
&lt;p&gt;Note that if a request is &lt;a href="https://raw.githubusercontent.com/openai/openai-go/main/#retries"&gt;retried&lt;/a&gt;, the context timeout does not start over. To set a per-retry timeout, use &lt;code&gt;option.WithRequestTimeout()&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-go"&gt;// This sets the timeout for the request, including all the retries.
ctx, cancel := context.WithTimeout(context.Background(), 5*time.Minute)
defer cancel()
client.Chat.Completions.New(
	ctx,
	openai.ChatCompletionNewParams{
		Messages: []openai.ChatCompletionMessageParamUnion{{
			OfUser: &amp;amp;openai.ChatCompletionUserMessageParam{
				Content: openai.ChatCompletionUserMessageParamContentUnion{
					OfString: openai.String("How can I list all files in a directory using Python?"),
				},
			},
		}},
		Model: shared.ChatModelGPT5,
	},
	// This sets the per-retry timeout
	option.WithRequestTimeout(20*time.Second),
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;File uploads&lt;/h3&gt; 
&lt;p&gt;Request parameters that correspond to file uploads in multipart requests are typed as &lt;code&gt;io.Reader&lt;/code&gt;. The contents of the &lt;code&gt;io.Reader&lt;/code&gt; will by default be sent as a multipart form part with the file name of "anonymous_file" and content-type of "application/octet-stream".&lt;/p&gt; 
&lt;p&gt;The file name and content-type can be customized by implementing &lt;code&gt;Name() string&lt;/code&gt; or &lt;code&gt;ContentType() string&lt;/code&gt; on the run-time type of &lt;code&gt;io.Reader&lt;/code&gt;. Note that &lt;code&gt;os.File&lt;/code&gt; implements &lt;code&gt;Name() string&lt;/code&gt;, so a file returned by &lt;code&gt;os.Open&lt;/code&gt; will be sent with the file name on disk.&lt;/p&gt; 
&lt;p&gt;We also provide a helper &lt;code&gt;openai.File(reader io.Reader, filename string, contentType string)&lt;/code&gt; which can be used to wrap any &lt;code&gt;io.Reader&lt;/code&gt; with the appropriate file name and content type.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-go"&gt;// A file from the file system
file, err := os.Open("input.jsonl")
openai.FileNewParams{
	File:    file,
	Purpose: openai.FilePurposeFineTune,
}

// A file from a string
openai.FileNewParams{
	File:    strings.NewReader("my file contents"),
	Purpose: openai.FilePurposeFineTune,
}

// With a custom filename and contentType
openai.FileNewParams{
	File:    openai.File(strings.NewReader(`{"hello": "foo"}`), "file.go", "application/json"),
	Purpose: openai.FilePurposeFineTune,
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Webhook Verification&lt;/h2&gt; 
&lt;p&gt;Verifying webhook signatures is &lt;em&gt;optional but encouraged&lt;/em&gt;.&lt;/p&gt; 
&lt;p&gt;For more information about webhooks, see &lt;a href="https://platform.openai.com/docs/guides/webhooks"&gt;the API docs&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Parsing webhook payloads&lt;/h3&gt; 
&lt;p&gt;For most use cases, you will likely want to verify the webhook and parse the payload at the same time. To achieve this, we provide the method &lt;code&gt;client.Webhooks.Unwrap()&lt;/code&gt;, which parses a webhook request and verifies that it was sent by OpenAI. This method will return an error if the signature is invalid.&lt;/p&gt; 
&lt;p&gt;Note that the &lt;code&gt;body&lt;/code&gt; parameter should be the raw JSON bytes sent from the server (do not parse it first). The &lt;code&gt;Unwrap()&lt;/code&gt; method will parse this JSON for you into an event object after verifying the webhook was sent from OpenAI.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-go"&gt;package main

import (
	"io"
	"log"
	"net/http"
	"os"

	"github.com/gin-gonic/gin"
	"github.com/openai/openai-go/v2"
	"github.com/openai/openai-go/v2/option"
	"github.com/openai/openai-go/v2/webhooks"
)

func main() {
	client := openai.NewClient(
		option.WithWebhookSecret(os.Getenv("OPENAI_WEBHOOK_SECRET")), // env var used by default; explicit here.
	)

	r := gin.Default()

	r.POST("/webhook", func(c *gin.Context) {
		body, err := io.ReadAll(c.Request.Body)
		if err != nil {
			c.JSON(http.StatusInternalServerError, gin.H{"error": "Error reading request body"})
			return
		}
		defer c.Request.Body.Close()

		webhookEvent, err := client.Webhooks.Unwrap(body, c.Request.Header)
		if err != nil {
			log.Printf("Invalid webhook signature: %v", err)
			c.JSON(http.StatusBadRequest, gin.H{"error": "invalid signature"})
			return
		}

		switch event := webhookEvent.AsAny().(type) {
		case webhooks.ResponseCompletedWebhookEvent:
			log.Printf("Response completed: %+v", event.Data)
		case webhooks.ResponseFailedWebhookEvent:
			log.Printf("Response failed: %+v", event.Data)
		default:
			log.Printf("Unhandled event type: %T", event)
		}

		c.JSON(http.StatusOK, gin.H{"message": "ok"})
	})

	r.Run(":8000")
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Verifying webhook payloads directly&lt;/h3&gt; 
&lt;p&gt;In some cases, you may want to verify the webhook separately from parsing the payload. If you prefer to handle these steps separately, we provide the method &lt;code&gt;client.Webhooks.VerifySignature()&lt;/code&gt; to &lt;em&gt;only verify&lt;/em&gt; the signature of a webhook request. Like &lt;code&gt;Unwrap()&lt;/code&gt;, this method will return an error if the signature is invalid.&lt;/p&gt; 
&lt;p&gt;Note that the &lt;code&gt;body&lt;/code&gt; parameter should be the raw JSON bytes sent from the server (do not parse it first). You will then need to parse the body after verifying the signature.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-go"&gt;package main

import (
	"encoding/json"
	"io"
	"log"
	"net/http"
	"os"

	"github.com/gin-gonic/gin"
	"github.com/openai/openai-go/v2"
	"github.com/openai/openai-go/v2/option"
)

func main() {
	client := openai.NewClient(
		option.WithWebhookSecret(os.Getenv("OPENAI_WEBHOOK_SECRET")), // env var used by default; explicit here.
	)

	r := gin.Default()

	r.POST("/webhook", func(c *gin.Context) {
		body, err := io.ReadAll(c.Request.Body)
		if err != nil {
			c.JSON(http.StatusInternalServerError, gin.H{"error": "Error reading request body"})
			return
		}
		defer c.Request.Body.Close()

		err = client.Webhooks.VerifySignature(body, c.Request.Header)
		if err != nil {
			log.Printf("Invalid webhook signature: %v", err)
			c.JSON(http.StatusBadRequest, gin.H{"error": "invalid signature"})
			return
		}

		c.JSON(http.StatusOK, gin.H{"message": "ok"})
	})

	r.Run(":8000")
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Retries&lt;/h3&gt; 
&lt;p&gt;Certain errors will be automatically retried 2 times by default, with a short exponential backoff. We retry by default all connection errors, 408 Request Timeout, 409 Conflict, 429 Rate Limit, and &amp;gt;=500 Internal errors.&lt;/p&gt; 
&lt;p&gt;You can use the &lt;code&gt;WithMaxRetries&lt;/code&gt; option to configure or disable this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-go"&gt;// Configure the default for all requests:
client := openai.NewClient(
	option.WithMaxRetries(0), // default is 2
)

// Override per-request:
client.Chat.Completions.New(
	context.TODO(),
	openai.ChatCompletionNewParams{
		Messages: []openai.ChatCompletionMessageParamUnion{{
			OfUser: &amp;amp;openai.ChatCompletionUserMessageParam{
				Content: openai.ChatCompletionUserMessageParamContentUnion{
					OfString: openai.String("How can I get the name of the current day in JavaScript?"),
				},
			},
		}},
		Model: shared.ChatModelGPT5,
	},
	option.WithMaxRetries(5),
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Accessing raw response data (e.g. response headers)&lt;/h3&gt; 
&lt;p&gt;You can access the raw HTTP response data by using the &lt;code&gt;option.WithResponseInto()&lt;/code&gt; request option. This is useful when you need to examine response headers, status codes, or other details.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-go"&gt;// Create a variable to store the HTTP response
var response *http.Response
chatCompletion, err := client.Chat.Completions.New(
	context.TODO(),
	openai.ChatCompletionNewParams{
		Messages: []openai.ChatCompletionMessageParamUnion{{
			OfUser: &amp;amp;openai.ChatCompletionUserMessageParam{
				Content: openai.ChatCompletionUserMessageParamContentUnion{
					OfString: openai.String("Say this is a test"),
				},
			},
		}},
		Model: shared.ChatModelGPT5,
	},
	option.WithResponseInto(&amp;amp;response),
)
if err != nil {
	// handle error
}
fmt.Printf("%+v\n", chatCompletion)

fmt.Printf("Status Code: %d\n", response.StatusCode)
fmt.Printf("Headers: %+#v\n", response.Header)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Making custom/undocumented requests&lt;/h3&gt; 
&lt;p&gt;This library is typed for convenient access to the documented API. If you need to access undocumented endpoints, params, or response properties, the library can still be used.&lt;/p&gt; 
&lt;h4&gt;Undocumented endpoints&lt;/h4&gt; 
&lt;p&gt;To make requests to undocumented endpoints, you can use &lt;code&gt;client.Get&lt;/code&gt;, &lt;code&gt;client.Post&lt;/code&gt;, and other HTTP verbs. &lt;code&gt;RequestOptions&lt;/code&gt; on the client, such as retries, will be respected when making these requests.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-go"&gt;var (
    // params can be an io.Reader, a []byte, an encoding/json serializable object,
    // or a "…Params" struct defined in this library.
    params map[string]any

    // result can be an []byte, *http.Response, a encoding/json deserializable object,
    // or a model defined in this library.
    result *http.Response
)
err := client.Post(context.Background(), "/unspecified", params, &amp;amp;result)
if err != nil {
    …
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Undocumented request params&lt;/h4&gt; 
&lt;p&gt;To make requests using undocumented parameters, you may use either the &lt;code&gt;option.WithQuerySet()&lt;/code&gt; or the &lt;code&gt;option.WithJSONSet()&lt;/code&gt; methods.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-go"&gt;params := FooNewParams{
    ID:   "id_xxxx",
    Data: FooNewParamsData{
        FirstName: openai.String("John"),
    },
}
client.Foo.New(context.Background(), params, option.WithJSONSet("data.last_name", "Doe"))
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Undocumented response properties&lt;/h4&gt; 
&lt;p&gt;To access undocumented response properties, you may either access the raw JSON of the response as a string with &lt;code&gt;result.JSON.RawJSON()&lt;/code&gt;, or get the raw JSON of a particular field on the result with &lt;code&gt;result.JSON.Foo.Raw()&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Any fields that are not present on the response struct will be saved and can be accessed by &lt;code&gt;result.JSON.ExtraFields()&lt;/code&gt; which returns the extra fields as a &lt;code&gt;map[string]Field&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Middleware&lt;/h3&gt; 
&lt;p&gt;We provide &lt;code&gt;option.WithMiddleware&lt;/code&gt; which applies the given middleware to requests.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-go"&gt;func Logger(req *http.Request, next option.MiddlewareNext) (res *http.Response, err error) {
	// Before the request
	start := time.Now()
	LogReq(req)

	// Forward the request to the next handler
	res, err = next(req)

	// Handle stuff after the request
	end := time.Now()
	LogRes(res, err, start - end)

    return res, err
}

client := openai.NewClient(
	option.WithMiddleware(Logger),
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;When multiple middlewares are provided as variadic arguments, the middlewares are applied left to right. If &lt;code&gt;option.WithMiddleware&lt;/code&gt; is given multiple times, for example first in the client then the method, the middleware in the client will run first and the middleware given in the method will run next.&lt;/p&gt; 
&lt;p&gt;You may also replace the default &lt;code&gt;http.Client&lt;/code&gt; with &lt;code&gt;option.WithHTTPClient(client)&lt;/code&gt;. Only one http client is accepted (this overwrites any previous client) and receives requests after any middleware has been applied.&lt;/p&gt; 
&lt;h2&gt;Microsoft Azure OpenAI&lt;/h2&gt; 
&lt;p&gt;To use this library with [Azure OpenAI]&lt;a href="https://learn.microsoft.com/azure/ai-services/openai/overview"&gt;https://learn.microsoft.com/azure/ai-services/openai/overview&lt;/a&gt;), use the option.RequestOption functions in the &lt;code&gt;azure&lt;/code&gt; package.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-go"&gt;package main

import (
	"github.com/Azure/azure-sdk-for-go/sdk/azidentity"
	"github.com/openai/openai-go/v2"
	"github.com/openai/openai-go/v2/azure"
)

func main() {
	const azureOpenAIEndpoint = "https://&amp;lt;azure-openai-resource&amp;gt;.openai.azure.com"

	// The latest API versions, including previews, can be found here:
	// ttps://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versionng
	const azureOpenAIAPIVersion = "2024-06-01"

	tokenCredential, err := azidentity.NewDefaultAzureCredential(nil)

	if err != nil {
		fmt.Printf("Failed to create the DefaultAzureCredential: %s", err)
		os.Exit(1)
	}

	client := openai.NewClient(
		azure.WithEndpoint(azureOpenAIEndpoint, azureOpenAIAPIVersion),

		// Choose between authenticating using a TokenCredential or an API Key
		azure.WithTokenCredential(tokenCredential),
		// or azure.WithAPIKey(azureOpenAIAPIKey),
	)
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Semantic versioning&lt;/h2&gt; 
&lt;p&gt;This package generally follows &lt;a href="https://semver.org/spec/v2.0.0.html"&gt;SemVer&lt;/a&gt; conventions, though certain backwards-incompatible changes may be released as minor versions:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Changes to library internals which are technically public but not intended or documented for external use. &lt;em&gt;(Please open a GitHub issue to let us know if you are relying on such internals.)&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;Changes that we do not expect to impact the vast majority of users in practice.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;We take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.&lt;/p&gt; 
&lt;p&gt;We are keen for your feedback; please open an &lt;a href="https://www.github.com/openai/openai-go/issues"&gt;issue&lt;/a&gt; with questions, bugs, or suggestions.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/openai/openai-go/main/CONTRIBUTING.md"&gt;the contributing documentation&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>filamentphp/filament</title>
      <link>https://github.com/filamentphp/filament</link>
      <description>&lt;p&gt;A powerful open source UI framework for Laravel • Build and ship admin panels &amp; apps fast with Livewire&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://github.com/filamentphp/filament/assets/41773797/8d5a0b12-4643-4b5c-964a-56f0db91b90a" alt="Banner" style="width: 100%; max-width: 800px;" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/filamentphp/filament/actions"&gt;&lt;img alt="Tests passing" src="https://img.shields.io/badge/Tests-passing-green?style=for-the-badge&amp;amp;logo=github" /&gt;&lt;/a&gt; &lt;a href="https://laravel.com"&gt;&lt;img alt="Laravel v11+" src="https://img.shields.io/badge/Laravel-v11+-FF2D20?style=for-the-badge&amp;amp;logo=laravel" /&gt;&lt;/a&gt; &lt;a href="https://livewire.laravel.com"&gt;&lt;img alt="Livewire v3" src="https://img.shields.io/badge/Livewire-v3-FB70A9?style=for-the-badge" /&gt;&lt;/a&gt; &lt;a href="https://php.net"&gt;&lt;img alt="PHP 8.2+" src="https://img.shields.io/badge/PHP-8.2+-777BB4?style=for-the-badge&amp;amp;logo=php" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/238" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/238" alt="filamentphp%2Ffilament | Trendshift" style="width: 250px; height: 55px;" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Filament is a powerful open source UI framework for Laravel, built with Livewire to help you ship admin panels &amp;amp; apps fast.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;It includes a collection of beautifully designed, fully extensible components that handle the hard parts, so you can focus on what matters.&lt;/p&gt; 
&lt;p&gt;Why rebuild dashboards, forms, and tables from scratch every time?&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://filamentphp.com/docs"&gt;Read the documentation&lt;/a&gt; • &lt;a href="https://demo.filamentphp.com"&gt;Check out the demo&lt;/a&gt; • &lt;a href="https://filamentphp.com/discord"&gt;Chat with us on Discord&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Please see our &lt;a href="https://filamentphp.com/docs/support/contributing"&gt;contributing guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Need Help?&lt;/h2&gt; 
&lt;p&gt;🐞 If you spot a bug, please &lt;a href="https://github.com/filamentphp/filament/issues/new?template=bug_report.yml"&gt;submit a detailed issue&lt;/a&gt;, and wait for assistance.&lt;/p&gt; 
&lt;p&gt;🤔 If you have a question or feature request, please &lt;a href="https://github.com/filamentphp/filament/discussions/new"&gt;start a new discussion&lt;/a&gt;. We also have a &lt;a href="https://filamentphp.com/discord"&gt;Discord community&lt;/a&gt;. For quick help, ask questions in the appropriate channel.&lt;/p&gt; 
&lt;p&gt;🔐 If you discover a vulnerability, please review our &lt;a href="https://github.com/filamentphp/filament/raw/4.x/SECURITY.md"&gt;security policy&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>FiloSottile/mkcert</title>
      <link>https://github.com/FiloSottile/mkcert</link>
      <description>&lt;p&gt;A simple zero-config tool to make locally trusted development certificates with any names you'd like.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;mkcert&lt;/h1&gt; 
&lt;p&gt;mkcert is a simple tool for making locally-trusted development certificates. It requires no configuration.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ mkcert -install
Created a new local CA 💥
The local CA is now installed in the system trust store! ⚡️
The local CA is now installed in the Firefox trust store (requires browser restart)! 🦊

$ mkcert example.com "*.example.com" example.test localhost 127.0.0.1 ::1

Created a new certificate valid for the following names 📜
 - "example.com"
 - "*.example.com"
 - "example.test"
 - "localhost"
 - "127.0.0.1"
 - "::1"

The certificate is at "./example.com+5.pem" and the key at "./example.com+5-key.pem" ✅
&lt;/code&gt;&lt;/pre&gt; 
&lt;p align="center"&gt;&lt;img width="498" alt="Chrome and Firefox screenshot" src="https://user-images.githubusercontent.com/1225294/51066373-96d4aa80-15be-11e9-91e2-f4e44a3a4458.png" /&gt;&lt;/p&gt; 
&lt;p&gt;Using certificates from real certificate authorities (CAs) for development can be dangerous or impossible (for hosts like &lt;code&gt;example.test&lt;/code&gt;, &lt;code&gt;localhost&lt;/code&gt; or &lt;code&gt;127.0.0.1&lt;/code&gt;), but self-signed certificates cause trust errors. Managing your own CA is the best solution, but usually involves arcane commands, specialized knowledge and manual steps.&lt;/p&gt; 
&lt;p&gt;mkcert automatically creates and installs a local CA in the system root store, and generates locally-trusted certificates. mkcert does not automatically configure servers to use the certificates, though, that's up to you.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;: the &lt;code&gt;rootCA-key.pem&lt;/code&gt; file that mkcert automatically generates gives complete power to intercept secure requests from your machine. Do not share it.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;macOS&lt;/h3&gt; 
&lt;p&gt;On macOS, use &lt;a href="https://brew.sh/"&gt;Homebrew&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;brew install mkcert
brew install nss # if you use Firefox
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or &lt;a href="https://www.macports.org/"&gt;MacPorts&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;sudo port selfupdate
sudo port install mkcert
sudo port install nss # if you use Firefox
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Linux&lt;/h3&gt; 
&lt;p&gt;On Linux, first install &lt;code&gt;certutil&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;sudo apt install libnss3-tools
    -or-
sudo yum install nss-tools
    -or-
sudo pacman -S nss
    -or-
sudo zypper install mozilla-nss-tools
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then you can install using &lt;a href="https://docs.brew.sh/Homebrew-on-Linux"&gt;Homebrew on Linux&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;brew install mkcert
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or build from source (requires Go 1.13+)&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;git clone https://github.com/FiloSottile/mkcert &amp;amp;&amp;amp; cd mkcert
go build -ldflags "-X main.Version=$(git describe --tags)"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or use &lt;a href="https://github.com/FiloSottile/mkcert/releases"&gt;the pre-built binaries&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;curl -JLO "https://dl.filippo.io/mkcert/latest?for=linux/amd64"
chmod +x mkcert-v*-linux-amd64
sudo cp mkcert-v*-linux-amd64 /usr/local/bin/mkcert
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For Arch Linux users, &lt;a href="https://archlinux.org/packages/extra/x86_64/mkcert/"&gt;&lt;code&gt;mkcert&lt;/code&gt;&lt;/a&gt; is available on the official Arch Linux repository.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;sudo pacman -Syu mkcert
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Windows&lt;/h3&gt; 
&lt;p&gt;On Windows, use &lt;a href="https://chocolatey.org"&gt;Chocolatey&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;choco install mkcert
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or use Scoop&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;scoop bucket add extras
scoop install mkcert
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or build from source (requires Go 1.10+), or use &lt;a href="https://github.com/FiloSottile/mkcert/releases"&gt;the pre-built binaries&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you're running into permission problems try running &lt;code&gt;mkcert&lt;/code&gt; as an Administrator.&lt;/p&gt; 
&lt;h2&gt;Supported root stores&lt;/h2&gt; 
&lt;p&gt;mkcert supports the following root stores:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;macOS system store&lt;/li&gt; 
 &lt;li&gt;Windows system store&lt;/li&gt; 
 &lt;li&gt;Linux variants that provide either 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;update-ca-trust&lt;/code&gt; (Fedora, RHEL, CentOS) or&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;update-ca-certificates&lt;/code&gt; (Ubuntu, Debian, OpenSUSE, SLES) or&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;trust&lt;/code&gt; (Arch)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Firefox (macOS and Linux only)&lt;/li&gt; 
 &lt;li&gt;Chrome and Chromium&lt;/li&gt; 
 &lt;li&gt;Java (when &lt;code&gt;JAVA_HOME&lt;/code&gt; is set)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To only install the local root CA into a subset of them, you can set the &lt;code&gt;TRUST_STORES&lt;/code&gt; environment variable to a comma-separated list. Options are: "system", "java" and "nss" (includes Firefox).&lt;/p&gt; 
&lt;h2&gt;Advanced topics&lt;/h2&gt; 
&lt;h3&gt;Advanced options&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;	-cert-file FILE, -key-file FILE, -p12-file FILE
	    Customize the output paths.

	-client
	    Generate a certificate for client authentication.

	-ecdsa
	    Generate a certificate with an ECDSA key.

	-pkcs12
	    Generate a ".p12" PKCS #12 file, also know as a ".pfx" file,
	    containing certificate and key for legacy applications.

	-csr CSR
	    Generate a certificate based on the supplied CSR. Conflicts with
	    all other flags and arguments except -install and -cert-file.
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; You &lt;em&gt;must&lt;/em&gt; place these options before the domain names list.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Example&lt;/h4&gt; 
&lt;pre&gt;&lt;code&gt;mkcert -key-file key.pem -cert-file cert.pem example.com *.example.com
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;S/MIME&lt;/h3&gt; 
&lt;p&gt;mkcert automatically generates an S/MIME certificate if one of the supplied names is an email address.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;mkcert filippo@example.com
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Mobile devices&lt;/h3&gt; 
&lt;p&gt;For the certificates to be trusted on mobile devices, you will have to install the root CA. It's the &lt;code&gt;rootCA.pem&lt;/code&gt; file in the folder printed by &lt;code&gt;mkcert -CAROOT&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;On iOS, you can either use AirDrop, email the CA to yourself, or serve it from an HTTP server. After opening it, you need to &lt;a href="https://github.com/FiloSottile/mkcert/issues/233#issuecomment-690110809"&gt;install the profile in Settings &amp;gt; Profile Downloaded&lt;/a&gt; and then &lt;a href="https://support.apple.com/en-nz/HT204477"&gt;enable full trust in it&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For Android, you will have to install the CA and then enable user roots in the development build of your app. See &lt;a href="https://stackoverflow.com/a/22040887/749014"&gt;this StackOverflow answer&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Using the root with Node.js&lt;/h3&gt; 
&lt;p&gt;Node does not use the system root store, so it won't accept mkcert certificates automatically. Instead, you will have to set the &lt;a href="https://nodejs.org/api/cli.html#cli_node_extra_ca_certs_file"&gt;&lt;code&gt;NODE_EXTRA_CA_CERTS&lt;/code&gt;&lt;/a&gt; environment variable.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;export NODE_EXTRA_CA_CERTS="$(mkcert -CAROOT)/rootCA.pem"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Changing the location of the CA files&lt;/h3&gt; 
&lt;p&gt;The CA certificate and its key are stored in an application data folder in the user home. You usually don't have to worry about it, as installation is automated, but the location is printed by &lt;code&gt;mkcert -CAROOT&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;If you want to manage separate CAs, you can use the environment variable &lt;code&gt;$CAROOT&lt;/code&gt; to set the folder where mkcert will place and look for the local CA files.&lt;/p&gt; 
&lt;h3&gt;Installing the CA on other systems&lt;/h3&gt; 
&lt;p&gt;Installing in the trust store does not require the CA key, so you can export the CA certificate and use mkcert to install it in other machines.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Look for the &lt;code&gt;rootCA.pem&lt;/code&gt; file in &lt;code&gt;mkcert -CAROOT&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;copy it to a different machine&lt;/li&gt; 
 &lt;li&gt;set &lt;code&gt;$CAROOT&lt;/code&gt; to its directory&lt;/li&gt; 
 &lt;li&gt;run &lt;code&gt;mkcert -install&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Remember that mkcert is meant for development purposes, not production, so it should not be used on end users' machines, and that you should &lt;em&gt;not&lt;/em&gt; export or share &lt;code&gt;rootCA-key.pem&lt;/code&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>openai/openai-cookbook</title>
      <link>https://github.com/openai/openai-cookbook</link>
      <description>&lt;p&gt;Examples and guides for using the OpenAI API&lt;/p&gt;&lt;hr&gt;&lt;a href="https://cookbook.openai.com" target="_blank"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="/images/openai-cookbook-white.png" style="max-width: 100%; width: 400px; margin-bottom: 20px" /&gt; 
  &lt;img alt="OpenAI Cookbook Logo" src="https://raw.githubusercontent.com/openai/openai-cookbook/main/images/openai-cookbook.png" width="400px" /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;h3&gt;&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;✨ Navigate at &lt;a href="https://cookbook.openai.com"&gt;cookbook.openai.com&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Example code and guides for accomplishing common tasks with the &lt;a href="https://platform.openai.com/docs/introduction"&gt;OpenAI API&lt;/a&gt;. To run these examples, you'll need an OpenAI account and associated API key (&lt;a href="https://platform.openai.com/signup"&gt;create a free account here&lt;/a&gt;). Set an environment variable called &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; with your API key. Alternatively, in most IDEs such as Visual Studio Code, you can create an &lt;code&gt;.env&lt;/code&gt; file at the root of your repo containing &lt;code&gt;OPENAI_API_KEY=&amp;lt;your API key&amp;gt;&lt;/code&gt;, which will be picked up by the notebooks.&lt;/p&gt; 
&lt;p&gt;Most code examples are written in Python, though the concepts can be applied in any language.&lt;/p&gt; 
&lt;p&gt;For other useful tools, guides and courses, check out these &lt;a href="https://cookbook.openai.com/related_resources"&gt;related resources from around the web&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;MIT License&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>lvgl/lvgl</title>
      <link>https://github.com/lvgl/lvgl</link>
      <description>&lt;p&gt;Embedded graphics library to create beautiful UIs for any MCU, MPU and display type.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="https://github.com/sponsors/lvgl" target="_blank"&gt;&lt;img align="left" src="https://lvgl.io/github-assets/sponsor.png" height="32px" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p align="right"&gt; &lt;b&gt;English&lt;/b&gt; | &lt;a href="https://raw.githubusercontent.com/lvgl/lvgl/master/docs/README_zh.md"&gt;中文&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/lvgl/lvgl/master/docs/README_pt_BR.md"&gt;Português do Brasil&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/lvgl/lvgl/master/docs/README_jp.md"&gt;日本語&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/lvgl/lvgl/master/docs/README_he.md"&gt;עברית&lt;/a&gt; &lt;/p&gt; 
&lt;br /&gt; 
&lt;p align="center"&gt; &amp;nbsp; &lt;img src="https://lvgl.io/github-assets/logo-colored.png" width="300px" /&gt; &lt;/p&gt; 
&lt;p&gt;&amp;nbsp; &lt;/p&gt;
&lt;h1 align="center"&gt;Light and Versatile Graphics Library&lt;/h1&gt; &amp;nbsp; 
&lt;br /&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://lvgl.io/github-assets/smartwatch-demo.gif" /&gt; &amp;nbsp; &amp;nbsp; 
 &lt;img border="1px" src="https://lvgl.io/github-assets/widgets-demo.gif" /&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;p align="center"&gt; &lt;a href="https://lvgl.io" title="Homepage of LVGL"&gt;Website &lt;/a&gt; | &lt;a href="https://docs.lvgl.io/" title="Detailed documentation with 100+ examples"&gt;Docs&lt;/a&gt; | &lt;a href="https://forum.lvgl.io" title="Get help and help others"&gt;Forum&lt;/a&gt; | &lt;a href="https://lvgl.io/demos" title="Demos running in your browser"&gt;Demos&lt;/a&gt; | &lt;a href="https://lvgl.io/services" title="Graphics design, UI implementation and consulting"&gt;Services&lt;/a&gt; &lt;/p&gt; 
&lt;br /&gt; 
&lt;h2&gt;&lt;span&gt;📒&lt;/span&gt; Overview&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Mature and Well-known&lt;/strong&gt;&lt;br /&gt; LVGL is the most popular free and open source embedded graphics library to create beautiful UIs for any MCU, MPU and display type. It's supported by industry leading vendors and projects like &amp;nbsp;Arm, STM32, NXP, Espressif, Nuvoton, Arduino, RT-Thread, Zephyr, NuttX, Adafruit and many more.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Feature Rich&lt;/strong&gt;&lt;br /&gt; It has all the features to create modern and beautiful GUIs: 30+ built-in widgets, a powerful style system, web inspired layout managers, and a typography system supporting many languages. To integrate LVGL into your platform, all you need is at least 32kB RAM and 128 kB Flash, a C compiler, a frame buffer, and at least an 1/10 screen sized buffer for rendering.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Services&lt;/strong&gt;&lt;br /&gt; Our team is ready to help you with graphics design, UI implementation and consulting services. Contact us if you need some support during the development of your next GUI project.&lt;/p&gt; 
&lt;h2&gt;&lt;span&gt;🚀&lt;/span&gt; Features&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Free and Portable&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;A fully portable C (C++ compatible) library with no external dependencies.&lt;/li&gt; 
 &lt;li&gt;Can be compiled to any MCU or MPU, with any (RT)OS.&lt;/li&gt; 
 &lt;li&gt;Supports monochrome, ePaper, OLED or TFT displays, or even monitors. &lt;a href="https://docs.lvgl.io/master/details/main-modules/display/index.html"&gt;Displays&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Distributed under the MIT license, so you can easily use it in commercial projects too.&lt;/li&gt; 
 &lt;li&gt;Needs only 32kB RAM and 128 kB Flash, a frame buffer, and at least an 1/10 screen sized buffer for rendering.&lt;/li&gt; 
 &lt;li&gt;OS, External memory and GPU are supported but not required.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Widgets, Styles, Layouts and more&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;30+ built-in &lt;a href="https://docs.lvgl.io/master/details/widgets/index.html"&gt;Widgets&lt;/a&gt;: &amp;nbsp;Button, Label, Slider, Chart, Keyboard, Meter, Arc, Table and many more.&lt;/li&gt; 
 &lt;li&gt;Flexible &lt;a href="https://docs.lvgl.io/master/details/common-widget-features/styles/index.html"&gt;Style system&lt;/a&gt; with &amp;nbsp;~100 style properties to customize any part of the widgets in any state.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.lvgl.io/master/details/common-widget-features/layouts/flex.html"&gt;Flexbox&lt;/a&gt; and &lt;a href="https://docs.lvgl.io/master/details/common-widget-features/layouts/grid.html"&gt;Grid&lt;/a&gt;-like layouts engines to automatically size and position the widgets in a responsive way.&lt;/li&gt; 
 &lt;li&gt;Texts are rendered with UTF-8 encoding supporting CJK, Thai, Hindi, Arabic, Persian writing systems.&lt;/li&gt; 
 &lt;li&gt;Word wrapping, kerning, text scrolling, sub-pixel rendering, Pinyin-IME Chinese input, Emojis in texts.&lt;/li&gt; 
 &lt;li&gt;Rendering engine supporting animations, anti-aliasing, opacity, smooth scrolling, shadows, image transformation, etc &amp;nbsp;&lt;/li&gt; 
 &lt;li&gt;Supports Mouse, Touchpad, Keypad, Keyboard, External buttons, Encoder &lt;a href="https://docs.lvgl.io/master/details/main-modules/indev.html"&gt;Input devices&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.lvgl.io/master/details/main-modules/display/overview.html#how-many-displays-can-lvgl-use"&gt;Multiple display&lt;/a&gt; support.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Binding and Build Support&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://blog.lvgl.io/2019-02-20/micropython-bindings"&gt;MicroPython Binding&lt;/a&gt; exposes LVGL API&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://blog.lvgl.io/2022-08-24/pikascript-and-lvgl"&gt;PikaScript Binding&lt;/a&gt; python on MCU lighter and easier.&lt;/li&gt; 
 &lt;li&gt;No custom build system is used. You can build LVGL as you build the other files of your project.&lt;/li&gt; 
 &lt;li&gt;Support for Make and &lt;a href="https://docs.lvgl.io/master/details/integration/building/cmake.html"&gt;CMake&lt;/a&gt; is included out of the box.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.lvgl.io/master/details/integration/ide/pc-simulator.html"&gt;Develop on PC&lt;/a&gt; and use the same UI code on embedded hardware.&lt;/li&gt; 
 &lt;li&gt;Convert the C UI code to HTML file with our &lt;a href="https://github.com/lvgl/lv_web_emscripten"&gt;Emscripten port&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Docs, Tools, and Services&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Detailed &lt;a href="https://docs.lvgl.io/"&gt;Documentation&lt;/a&gt; with &lt;a href="https://docs.lvgl.io/master/examples.html"&gt;100+ simple examples&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://lvgl.io/services"&gt;Services&lt;/a&gt; such as User interface design, Implementation and Consulting to make UI development simpler and faster.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;span&gt;❤️&lt;/span&gt; Sponsor&lt;/h2&gt; 
&lt;p&gt;If LVGL saved you a lot of time and money or you just had fun using it, consider &lt;a href="https://github.com/sponsors/lvgl"&gt;Supporting its Development&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;How do we spend the donations?&lt;/strong&gt;&lt;br /&gt; Our goal is to provide financial compensation for people who do the most for LVGL. It means not only the maintainers but anyone who implements a great feature should get a payment from the accumulated money. We use the donations to cover our operational costs like servers and related services.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;How to donate?&lt;/strong&gt;&lt;br /&gt; We use &lt;a href="https://github.com/sponsors/lvgl"&gt;GitHub Sponsors&lt;/a&gt; where you can easily send one time or recurring donations. You can also see all of our expenses in a transparent way.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;How to get paid for your contribution?&lt;/strong&gt;&lt;br /&gt; If someone implements or fixes an issue labeled as &lt;a href="https://github.com/lvgl/lvgl/labels/Sponsored"&gt;Sponsored&lt;/a&gt; he or she will get a payment for that work. We estimate the required time, complexity and importance of the issue and set a price accordingly. To jump in just comment on a &lt;a href="https://github.com/lvgl/lvgl/labels/Sponsored"&gt;Sponsored&lt;/a&gt; issue saying "Hi, I'd like to deal with it. This is how I'm planning to fix/implement it...". A work is considered ready when it's approved and merged by a maintainer. After that you can submit and expense at &lt;a href="https://opencollective.com/lvgl"&gt;opencollective.com&lt;/a&gt; and you will receive the payment in a few days.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Organizations supporting LVGL&lt;/strong&gt;&lt;br /&gt; &lt;a href="https://opencollective.com/lvgl"&gt;&lt;img src="https://opencollective.com/lvgl/organizations.svg?width=600" alt="Sponsors of LVGL" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Individuals supporting LVGL&lt;/strong&gt;&lt;br /&gt; &lt;a href="https://opencollective.com/lvgl"&gt;&lt;img src="https://contrib.rocks/image?repo=lvgl/lvgl&amp;amp;max=48" alt="Backers of LVGL" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;span&gt;📦&lt;/span&gt; Packages&lt;/h2&gt; 
&lt;p&gt;LVGL is available as:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.lvgl.io/master/details/integration/framework/arduino.html"&gt;Arduino library&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://registry.platformio.org/libraries/lvgl/lvgl"&gt;PlatformIO package&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.lvgl.io/master/details/integration/os/zephyr.html"&gt;Zephyr library&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://components.espressif.com/components/lvgl/lvgl"&gt;ESP-IDF(ESP32) component&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.nxp.com/design/software/embedded-software/lvgl-open-source-graphics-library:LITTLEVGL-OPEN-SOURCE-GRAPHICS-LIBRARY"&gt;NXP MCUXpresso component&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.lvgl.io/master/details/integration/os/nuttx.html"&gt;NuttX library&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.lvgl.io/master/details/integration/os/rt-thread.html"&gt;RT-Thread RTOS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;CMSIS-Pack&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://doc.riot-os.org/group__pkg__lvgl.html#details"&gt;RIOT OS package&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;span&gt;🤖&lt;/span&gt; Examples&lt;/h2&gt; 
&lt;p&gt;See some examples of creating widgets, using layouts and applying styles. You will find C and MicroPython code, and links to try out or edit the examples in an online MicroPython editor.&lt;/p&gt; 
&lt;p&gt;For more examples check out the &lt;a href="https://github.com/lvgl/lvgl/tree/master/examples"&gt;Examples&lt;/a&gt; folder.&lt;/p&gt; 
&lt;h3&gt;Hello world label&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://github.com/kisvegabor/test/raw/master/readme_example_1.png" alt="Simple Hello world label example in LVGL" /&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;C code&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-c"&gt;/*Change the active screen's background color*/
lv_obj_set_style_bg_color(lv_screen_active(), lv_color_hex(0x003a57), LV_PART_MAIN);

/*Create a white label, set its text and align it to the center*/
lv_obj_t * label = lv_label_create(lv_screen_active());
lv_label_set_text(label, "Hello world");
lv_obj_set_style_text_color(label, lv_color_hex(0xffffff), LV_PART_MAIN);
lv_obj_align(label, LV_ALIGN_CENTER, 0, 0);
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;MicroPython code | &lt;a href="https://sim.lvgl.io/v8.3/micropython/ports/javascript/index.html?script_direct=4ab7c40c35b0dc349aa2f0c3b00938d7d8e8ac9f" target="_blank"&gt;Online Simulator&lt;/a&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Change the active screen's background color
scr = lv.screen_active()
scr.set_style_bg_color(lv.color_hex(0x003a57), lv.PART.MAIN)

# Create a white label, set its text and align it to the center
label = lv.label(lv.screen_active())
label.set_text("Hello world")
label.set_style_text_color(lv.color_hex(0xffffff), lv.PART.MAIN)
label.align(lv.ALIGN.CENTER, 0, 0)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;br /&gt; 
&lt;h3&gt;Button with Click Event&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://github.com/kisvegabor/test/raw/master/readme_example_2.gif" alt="LVGL button with label example" /&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;C code&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-c"&gt;lv_obj_t * button = lv_button_create(lv_screen_active()); &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; /*Add a button to the current screen*/
lv_obj_center(button); &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;         /*Set its position*/
lv_obj_set_size(button, 100, 50); &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;/*Set its size*/
lv_obj_add_event_cb(button, button_event_cb, LV_EVENT_CLICKED, NULL); /*Assign a callback to the button*/

lv_obj_t * label = lv_label_create(button); &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;/*Add a label to the button*/
lv_label_set_text(label, "Button"); &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; /*Set the labels text*/
lv_obj_center(label); &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; /*Align the label to the center*/
...

void button_event_cb(lv_event_t * e)
{
&amp;nbsp; printf("Clicked\n");
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;MicroPython code | &lt;a href="https://sim.lvgl.io/v8.3/micropython/ports/javascript/index.html?script_startup=https://raw.githubusercontent.com/lvgl/lvgl/0d9ab4ee0e591aad1970e3c9164fd7c544ecce70/examples/header.py&amp;amp;script=https://raw.githubusercontent.com/lvgl/lvgl/0d9ab4ee0e591aad1970e3c9164fd7c544ecce70/examples/widgets/slider/lv_example_slider_2.py&amp;amp;script_direct=926bde43ec7af0146c486de470c53f11f167491e" target="_blank"&gt;Online Simulator&lt;/a&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;def button_event_cb(e):
&amp;nbsp; print("Clicked")

# Create a Button and a Label
button = lv.button(lv.screen_active())
button.center()
button.set_size(100, 50)
button.add_event_cb(button_event_cb, lv.EVENT.CLICKED, None)

label = lv.label(button)
label.set_text("Button")
label.center()
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;br /&gt; 
&lt;h3&gt;Checkboxes with Layout&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://github.com/kisvegabor/test/raw/master/readme_example_3.gif" alt="Checkboxes with layout in LVGL" /&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;C code&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-c"&gt;
lv_obj_set_flex_flow(lv_screen_active(), LV_FLEX_FLOW_COLUMN);
lv_obj_set_flex_align(lv_screen_active(), LV_FLEX_ALIGN_CENTER, LV_FLEX_ALIGN_START, LV_FLEX_ALIGN_CENTER);

lv_obj_t * cb;
cb = lv_checkbox_create(lv_screen_active());
lv_checkbox_set_text(cb, "Apple");
lv_obj_add_event_cb(cb, event_handler, LV_EVENT_ALL, NULL);

cb = lv_checkbox_create(lv_screen_active());
lv_checkbox_set_text(cb, "Banana");
lv_obj_add_state(cb, LV_STATE_CHECKED);
lv_obj_add_event_cb(cb, event_handler, LV_EVENT_ALL, NULL);

cb = lv_checkbox_create(lv_screen_active());
lv_checkbox_set_text(cb, "Lemon");
lv_obj_add_state(cb, LV_STATE_DISABLED);
lv_obj_add_event_cb(cb, event_handler, LV_EVENT_ALL, NULL);

cb = lv_checkbox_create(lv_screen_active());
lv_obj_add_state(cb, LV_STATE_CHECKED);
lv_obj_add_state(cb, LV_STATE_DISABLED);
lv_checkbox_set_text(cb, "Melon\nand a new line");
lv_obj_add_event_cb(cb, event_handler, LV_EVENT_ALL, NULL);
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;MicroPython code | &lt;a href="https://sim.lvgl.io/v8.3/micropython/ports/javascript/index.html?script_startup=https://raw.githubusercontent.com/lvgl/lvgl/0d9ab4ee0e591aad1970e3c9164fd7c544ecce70/examples/header.py&amp;amp;script=https://raw.githubusercontent.com/lvgl/lvgl/0d9ab4ee0e591aad1970e3c9164fd7c544ecce70/examples/widgets/slider/lv_example_slider_2.py&amp;amp;script_direct=311d37e5f70daf1cb0d2cad24c7f72751b5f1792" target="_blank"&gt;Online Simulator&lt;/a&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;def event_handler(e):
    code = e.get_code()
    obj = e.get_target_obj()
    if code == lv.EVENT.VALUE_CHANGED:
        txt = obj.get_text()
        if obj.get_state() &amp;amp; lv.STATE.CHECKED:
            state = "Checked"
        else:
            state = "Unchecked"
        print(txt + ":" + state)


lv.screen_active().set_flex_flow(lv.FLEX_FLOW.COLUMN)
lv.screen_active().set_flex_align(lv.FLEX_ALIGN.CENTER, lv.FLEX_ALIGN.START, lv.FLEX_ALIGN.CENTER)

cb = lv.checkbox(lv.screen_active())
cb.set_text("Apple")
cb.add_event_cb(event_handler, lv.EVENT.ALL, None)

cb = lv.checkbox(lv.screen_active())
cb.set_text("Banana")
cb.add_state(lv.STATE.CHECKED)
cb.add_event_cb(event_handler, lv.EVENT.ALL, None)

cb = lv.checkbox(lv.screen_active())
cb.set_text("Lemon")
cb.add_state(lv.STATE.DISABLED)
cb.add_event_cb(event_handler, lv.EVENT.ALL, None)

cb = lv.checkbox(lv.screen_active())
cb.add_state(lv.STATE.CHECKED | lv.STATE.DISABLED)
cb.set_text("Melon")
cb.add_event_cb(event_handler, lv.EVENT.ALL, None)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;br /&gt; 
&lt;h3&gt;Styling a Slider&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://github.com/kisvegabor/test/raw/master/readme_example_4.gif" alt="Styling a slider with LVGL" /&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;C code&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-c"&gt;lv_obj_t * slider = lv_slider_create(lv_screen_active());
lv_slider_set_value(slider, 70, LV_ANIM_OFF);
lv_obj_set_size(slider, 300, 20);
lv_obj_center(slider);

/*Add local styles to MAIN part (background rectangle)*/
lv_obj_set_style_bg_color(slider, lv_color_hex(0x0F1215), LV_PART_MAIN);
lv_obj_set_style_bg_opa(slider, 255, LV_PART_MAIN);
lv_obj_set_style_border_color(slider, lv_color_hex(0x333943), LV_PART_MAIN);
lv_obj_set_style_border_width(slider, 5, LV_PART_MAIN);
lv_obj_set_style_pad_all(slider, 5, LV_PART_MAIN);

/*Create a reusable style sheet for the INDICATOR part*/
static lv_style_t style_indicator;
lv_style_init(&amp;amp;style_indicator);
lv_style_set_bg_color(&amp;amp;style_indicator, lv_color_hex(0x37B9F5));
lv_style_set_bg_grad_color(&amp;amp;style_indicator, lv_color_hex(0x1464F0));
lv_style_set_bg_grad_dir(&amp;amp;style_indicator, LV_GRAD_DIR_HOR);
lv_style_set_shadow_color(&amp;amp;style_indicator, lv_color_hex(0x37B9F5));
lv_style_set_shadow_width(&amp;amp;style_indicator, 15);
lv_style_set_shadow_spread(&amp;amp;style_indicator, 5);
4
/*Add the style sheet to the slider's INDICATOR part*/
lv_obj_add_style(slider, &amp;amp;style_indicator, LV_PART_INDICATOR);

/*Add the same style to the KNOB part too and locally overwrite some properties*/
lv_obj_add_style(slider, &amp;amp;style_indicator, LV_PART_KNOB);

lv_obj_set_style_outline_color(slider, lv_color_hex(0x0096FF), LV_PART_KNOB);
lv_obj_set_style_outline_width(slider, 3, LV_PART_KNOB);
lv_obj_set_style_outline_pad(slider, -5, LV_PART_KNOB);
lv_obj_set_style_shadow_spread(slider, 2, LV_PART_KNOB);
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;MicroPython code | &lt;a href="https://sim.lvgl.io/v8.3/micropython/ports/javascript/index.html?script_startup=https://raw.githubusercontent.com/lvgl/lvgl/0d9ab4ee0e591aad1970e3c9164fd7c544ecce70/examples/header.py&amp;amp;script=https://raw.githubusercontent.com/lvgl/lvgl/0d9ab4ee0e591aad1970e3c9164fd7c544ecce70/examples/widgets/slider/lv_example_slider_2.py&amp;amp;script_direct=c431c7b4dfd2cc0dd9c392b74365d5af6ea986f0" target="_blank"&gt;Online Simulator&lt;/a&gt; &lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Create a slider and add the style
slider = lv.slider(lv.screen_active())
slider.set_value(70, lv.ANIM.OFF)
slider.set_size(300, 20)
slider.center()

# Add local styles to MAIN part (background rectangle)
slider.set_style_bg_color(lv.color_hex(0x0F1215), lv.PART.MAIN)
slider.set_style_bg_opa(255, lv.PART.MAIN)
slider.set_style_border_color(lv.color_hex(0x333943), lv.PART.MAIN)
slider.set_style_border_width(5, lv.PART.MAIN)
slider.set_style_pad_all(5, lv.PART.MAIN)

# Create a reusable style sheet for the INDICATOR part
style_indicator = lv.style_t()
style_indicator.init()
style_indicator.set_bg_color(lv.color_hex(0x37B9F5))
style_indicator.set_bg_grad_color(lv.color_hex(0x1464F0))
style_indicator.set_bg_grad_dir(lv.GRAD_DIR.HOR)
style_indicator.set_shadow_color(lv.color_hex(0x37B9F5))
style_indicator.set_shadow_width(15)
style_indicator.set_shadow_spread(5)

# Add the style sheet to the slider's INDICATOR part
slider.add_style(style_indicator, lv.PART.INDICATOR)
slider.add_style(style_indicator, lv.PART.KNOB)

# Add the same style to the KNOB part too and locally overwrite some properties
slider.set_style_outline_color(lv.color_hex(0x0096FF), lv.PART.KNOB)
slider.set_style_outline_width(3, lv.PART.KNOB)
slider.set_style_outline_pad(-5, lv.PART.KNOB)
slider.set_style_shadow_spread(2, lv.PART.KNOB)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;br /&gt; 
&lt;h3&gt;English, Hebrew (mixed LTR-RTL) and Chinese texts&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://github.com/kisvegabor/test/raw/master/readme_example_5.png" alt="English, Hebrew and Chinese texts with LVGL" /&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;C code&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-c"&gt;lv_obj_t * ltr_label = lv_label_create(lv_screen_active());
lv_label_set_text(ltr_label, "In modern terminology, a microcontroller is similar to a system on a chip (SoC).");
lv_obj_set_style_text_font(ltr_label, &amp;amp;lv_font_montserrat_16, 0);
lv_obj_set_width(ltr_label, 310);
lv_obj_align(ltr_label, LV_ALIGN_TOP_LEFT, 5, 5);

lv_obj_t * rtl_label = lv_label_create(lv_screen_active());
lv_label_set_text(rtl_label,"מעבד, או בשמו המלא יחידת עיבוד מרכזית (באנגלית: CPU - Central Processing Unit).");
lv_obj_set_style_base_dir(rtl_label, LV_BASE_DIR_RTL, 0);
lv_obj_set_style_text_font(rtl_label, &amp;amp;lv_font_dejavu_16_persian_hebrew, 0);
lv_obj_set_width(rtl_label, 310);
lv_obj_align(rtl_label, LV_ALIGN_LEFT_MID, 5, 0);

lv_obj_t * cz_label = lv_label_create(lv_screen_active());
lv_label_set_text(cz_label,
                  "嵌入式系统（Embedded System），\n是一种嵌入机械或电气系统内部、具有专一功能和实时计算性能的计算机系统。");
lv_obj_set_style_text_font(cz_label, &amp;amp;lv_font_source_han_sans_sc_16_cjk, 0);
lv_obj_set_width(cz_label, 310);
lv_obj_align(cz_label, LV_ALIGN_BOTTOM_LEFT, 5, -5);
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;MicroPython code | &lt;a href="https://sim.lvgl.io/v8.3/micropython/ports/javascript/index.html?script_startup=https://raw.githubusercontent.com/lvgl/lvgl/0d9ab4ee0e591aad1970e3c9164fd7c544ecce70/examples/header.py&amp;amp;script=https://raw.githubusercontent.com/lvgl/lvgl/0d9ab4ee0e591aad1970e3c9164fd7c544ecce70/examples/widgets/slider/lv_example_slider_2.py&amp;amp;script_direct=18bb38200a64e10ead1aa17a65c977fc18131842" target="_blank"&gt;Online Simulator&lt;/a&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;ltr_label = lv.label(lv.screen_active())
ltr_label.set_text("In modern terminology, a microcontroller is similar to a system on a chip (SoC).")
ltr_label.set_style_text_font(lv.font_montserrat_16, 0);

ltr_label.set_width(310)
ltr_label.align(lv.ALIGN.TOP_LEFT, 5, 5)

rtl_label = lv.label(lv.screen_active())
rtl_label.set_text("מעבד, או בשמו המלא יחידת עיבוד מרכזית (באנגלית: CPU - Central Processing Unit).")
rtl_label.set_style_base_dir(lv.BASE_DIR.RTL, 0)
rtl_label.set_style_text_font(lv.font_dejavu_16_persian_hebrew, 0)
rtl_label.set_width(310)
rtl_label.align(lv.ALIGN.LEFT_MID, 5, 0)

font_hans_sans_16_cjk = lv.font_load("S:../../assets/font/lv_font_source_han_sans_sc_16_cjk.fnt")

cz_label = lv.label(lv.screen_active())
cz_label.set_style_text_font(font_hans_sans_16_cjk, 0)
cz_label.set_text("嵌入式系统（Embedded System），\n是一种嵌入机械或电气系统内部、具有专一功能和实时计算性能的计算机系统。")
cz_label.set_width(310)
cz_label.align(lv.ALIGN.BOTTOM_LEFT, 5, -5)

&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;&lt;span&gt;▶&lt;/span&gt; Get started&lt;/h2&gt; 
&lt;p&gt;This list will guide you to get started with LVGL step-by-step.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Get Familiar with LVGL&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Check the &lt;a href="https://lvgl.io/demos"&gt;Online demos&lt;/a&gt; to see LVGL in action (3 minutes).&lt;/li&gt; 
 &lt;li&gt;Read the &lt;a href="https://docs.lvgl.io/master/intro/introduction/index.html"&gt;Introduction&lt;/a&gt; page of the documentation (5 minutes).&lt;/li&gt; 
 &lt;li&gt;Get familiar with the basics on the &lt;a href="https://docs.lvgl.io/master/intro/getting_started/learn_the_basics.html"&gt;Quick overview&lt;/a&gt; page (15 minutes).&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Start to Use LVGL&lt;/strong&gt;&lt;/p&gt; 
&lt;ol start="4"&gt; 
 &lt;li&gt;Set up a &lt;a href="https://docs.lvgl.io/master/details/integration/ide/pc-simulator.html#simulator"&gt;Simulator&lt;/a&gt; (10 minutes).&lt;/li&gt; 
 &lt;li&gt;Try out some &lt;a href="https://github.com/lvgl/lvgl/tree/master/examples"&gt;Examples&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Port LVGL to a board. See the &lt;a href="https://docs.lvgl.io/master/details/integration/adding-lvgl-to-your-project/index.html"&gt;Porting&lt;/a&gt; guide or check out the ready-to-use &lt;a href="https://github.com/lvgl?q=lv_port_"&gt;Projects&lt;/a&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Become a Pro&lt;/strong&gt;&lt;/p&gt; 
&lt;ol start="7"&gt; 
 &lt;li&gt;Read the &lt;a href="https://docs.lvgl.io/master/details/main-modules/index.html"&gt;Main-Modules&lt;/a&gt; page to get a better understanding of the library (2-3 hours)&lt;/li&gt; 
 &lt;li&gt;Check the documentation of the &lt;a href="https://docs.lvgl.io/master/details/widgets/index.html"&gt;Widgets&lt;/a&gt; to see their features and usage&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Get Help and Help Others&lt;/strong&gt;&lt;/p&gt; 
&lt;ol start="9"&gt; 
 &lt;li&gt;If you have questions go to the &lt;a href="http://forum.lvgl.io/"&gt;Forum&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Read the &lt;a href="https://docs.lvgl.io/master/contributing/index.html"&gt;Contributing&lt;/a&gt; guide to see how you can help to improve LVGL (15 minutes)&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;&lt;span&gt;🤝&lt;/span&gt; Services&lt;/h2&gt; 
&lt;p&gt;LVGL LLC was established to provide a solid background for LVGL library and to offer several type of services to help you in UI development. With 15+ years of experience in the user interface and graphics industry we can help you the bring your UI to the next level.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Graphics design&lt;/strong&gt; Our in-house graphics designers are experts in creating beautiful modern designs which fit to your product and the resources of your hardware.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;UI implementation&lt;/strong&gt; We can also implement your UI based on the design you or we have created. You can be sure that we will make the most out of your hardware and LVGL. If a feature or widget is missing from LVGL, don't worry, we will implement it for you.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Consulting and Support&lt;/strong&gt; We can support you with consulting as well to avoid pricey and time consuming mistakes during the UI development.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Board certification&lt;/strong&gt; For companies who are offering development boards, or production ready kits we do board certification which shows how board can run LVGL.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Check out our &lt;a href="https://lvgl.io/demos"&gt;Demos&lt;/a&gt; as reference. For more information take look at the &lt;a href="https://lvgl.io/services"&gt;Services page&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://lvgl.io/#contact"&gt;Contact us&lt;/a&gt; and tell how we can help.&lt;/p&gt; 
&lt;h2&gt;&lt;span&gt;🌟&lt;/span&gt; Contributing&lt;/h2&gt; 
&lt;p&gt;LVGL is an open project and contribution is very welcome. There are many ways to contribute from simply speaking about your project, through writing examples, improving the documentation, fixing bugs or even hosting your own project under the LVGL organization.&lt;/p&gt; 
&lt;p&gt;For a detailed description of contribution opportunities visit the &lt;a href="https://docs.lvgl.io/master/contributing/index.html"&gt;Contributing&lt;/a&gt; section of the documentation.&lt;/p&gt; 
&lt;p&gt;More than 300 people already left their fingerprint in LVGL. Be one them! See you here! &lt;span&gt;🙂&lt;/span&gt;&lt;/p&gt; 
&lt;a href="https://github.com/lvgl/lvgl/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=lvgl/lvgl&amp;amp;max=48" /&gt; &lt;/a&gt; 
&lt;p&gt;... and many other.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>syncthing/syncthing</title>
      <link>https://github.com/syncthing/syncthing</link>
      <description>&lt;p&gt;Open Source Continuous File Synchronization&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="https://syncthing.net/"&gt;&lt;img src="https://raw.githubusercontent.com/syncthing/syncthing/main/assets/logo-text-128.png" alt="Syncthing" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;a href="https://www.mozilla.org/MPL/2.0/"&gt;&lt;img src="https://img.shields.io/badge/license-MPLv2-blue.svg?style=flat-square" alt="MPLv2 License" /&gt;&lt;/a&gt; &lt;a href="https://bestpractices.coreinfrastructure.org/projects/88"&gt;&lt;img src="https://bestpractices.coreinfrastructure.org/projects/88/badge" alt="CII Best Practices" /&gt;&lt;/a&gt; &lt;a href="https://goreportcard.com/report/github.com/syncthing/syncthing"&gt;&lt;img src="https://goreportcard.com/badge/github.com/syncthing/syncthing" alt="Go Report Card" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Goals&lt;/h2&gt; 
&lt;p&gt;Syncthing is a &lt;strong&gt;continuous file synchronization program&lt;/strong&gt;. It synchronizes files between two or more computers. We strive to fulfill the goals below. The goals are listed in order of importance, the most important ones first. This is the summary version of the goal list - for more commentary, see the full &lt;a href="https://github.com/syncthing/syncthing/raw/main/GOALS.md"&gt;Goals document&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Syncthing should be:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Safe From Data Loss&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Protecting the user's data is paramount. We take every reasonable precaution to avoid corrupting the user's files.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Secure Against Attackers&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Again, protecting the user's data is paramount. Regardless of our other goals, we must never allow the user's data to be susceptible to eavesdropping or modification by unauthorized parties.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Easy to Use&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Syncthing should be approachable, understandable, and inclusive.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Automatic&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;User interaction should be required only when absolutely necessary.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Universally Available&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Syncthing should run on every common computer. We are mindful that the latest technology is not always available to every individual.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;For Individuals&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Syncthing is primarily about empowering the individual user with safe, secure, and easy to use file synchronization.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Everything Else&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;There are many things we care about that don't make it on to the list. It is fine to optimize for these values, as long as they are not in conflict with the stated goals above.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;Take a look at the &lt;a href="https://docs.syncthing.net/intro/getting-started.html"&gt;getting started guide&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;There are a few examples for keeping Syncthing running in the background on your system in &lt;a href="https://github.com/syncthing/syncthing/raw/main/etc"&gt;the etc directory&lt;/a&gt;. There are also several &lt;a href="https://docs.syncthing.net/users/contrib.html#gui-wrappers"&gt;GUI implementations&lt;/a&gt; for Windows, Mac, and Linux.&lt;/p&gt; 
&lt;h2&gt;Docker&lt;/h2&gt; 
&lt;p&gt;To run Syncthing in Docker, see &lt;a href="https://github.com/syncthing/syncthing/raw/main/README-Docker.md"&gt;the Docker README&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Getting in Touch&lt;/h2&gt; 
&lt;p&gt;The first and best point of contact is the &lt;a href="https://forum.syncthing.net/"&gt;Forum&lt;/a&gt;. If you've found something that is clearly a bug, feel free to report it in the &lt;a href="https://github.com/syncthing/syncthing/issues"&gt;GitHub issue tracker&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you believe that you’ve found a Syncthing-related security vulnerability, please report it by emailing &lt;a href="mailto:security@syncthing.net"&gt;security@syncthing.net&lt;/a&gt;. Do not report it in the Forum or issue tracker.&lt;/p&gt; 
&lt;h2&gt;Building&lt;/h2&gt; 
&lt;p&gt;Building Syncthing from source is easy. After extracting the source bundle from a release or checking out git, you just need to run &lt;code&gt;go run build.go&lt;/code&gt; and the binaries are created in &lt;code&gt;./bin&lt;/code&gt;. There's &lt;a href="https://docs.syncthing.net/dev/building.html"&gt;a guide&lt;/a&gt; with more details on the build process.&lt;/p&gt; 
&lt;h2&gt;Signed Releases&lt;/h2&gt; 
&lt;p&gt;Release binaries are GPG signed with the key available from &lt;a href="https://syncthing.net/security/"&gt;https://syncthing.net/security/&lt;/a&gt;. There is also a built-in automatic upgrade mechanism (disabled in some distribution channels) which uses a compiled in ECDSA signature. macOS and Windows binaries are also code-signed.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;Please see the Syncthing &lt;a href="https://docs.syncthing.net/"&gt;documentation site&lt;/a&gt; &lt;a href="https://github.com/syncthing/docs"&gt;[source]&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;All code is licensed under the &lt;a href="https://github.com/syncthing/syncthing/raw/main/LICENSE"&gt;MPLv2 License&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>openai/codex</title>
      <link>https://github.com/openai/codex</link>
      <description>&lt;p&gt;Lightweight coding agent that runs in your terminal&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt;OpenAI Codex CLI&lt;/h1&gt; 
&lt;p align="center"&gt;&lt;code&gt;npm i -g @openai/codex&lt;/code&gt;&lt;br /&gt;or &lt;code&gt;brew install codex&lt;/code&gt;&lt;/p&gt; 
&lt;p align="center"&gt;&lt;strong&gt;Codex CLI&lt;/strong&gt; is a coding agent from OpenAI that runs locally on your computer.&lt;br /&gt;If you are looking for the &lt;em&gt;cloud-based agent&lt;/em&gt; from OpenAI, &lt;strong&gt;Codex Web&lt;/strong&gt;, see &lt;a href="https://chatgpt.com/codex"&gt;chatgpt.com/codex&lt;/a&gt;.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/openai/codex/main/.github/codex-cli-splash.png" alt="Codex CLI splash" width="50%" /&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Table of contents&lt;/strong&gt;&lt;/summary&gt; 
 &lt;!-- Begin ToC --&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#quickstart"&gt;Quickstart&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#installing-and-running-codex-cli"&gt;Installing and running Codex CLI&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#using-codex-with-your-chatgpt-plan"&gt;Using Codex with your ChatGPT plan&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#connecting-on-a-headless-machine"&gt;Connecting on a "Headless" Machine&lt;/a&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#authenticate-locally-and-copy-your-credentials-to-the-headless-machine"&gt;Authenticate locally and copy your credentials to the "headless" machine&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#connecting-through-vps-or-remote"&gt;Connecting through VPS or remote&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#usage-based-billing-alternative-use-an-openai-api-key"&gt;Usage-based billing alternative: Use an OpenAI API key&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#choosing-codexs-level-of-autonomy"&gt;Choosing Codex's level of autonomy&lt;/a&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#1-readwrite"&gt;&lt;strong&gt;1. Read/write&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#2-read-only"&gt;&lt;strong&gt;2. Read-only&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#3-advanced-configuration"&gt;&lt;strong&gt;3. Advanced configuration&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#can-i-run-without-any-approvals"&gt;Can I run without ANY approvals?&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#fine-tuning-in-configtoml"&gt;Fine-tuning in &lt;code&gt;config.toml&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#example-prompts"&gt;Example prompts&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#running-with-a-prompt-as-input"&gt;Running with a prompt as input&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#using-open-source-models"&gt;Using Open Source Models&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#platform-sandboxing-details"&gt;Platform sandboxing details&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#experimental-technology-disclaimer"&gt;Experimental technology disclaimer&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#system-requirements"&gt;System requirements&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#cli-reference"&gt;CLI reference&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#memory--project-docs"&gt;Memory &amp;amp; project docs&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#non-interactive--ci-mode"&gt;Non-interactive / CI mode&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#model-context-protocol-mcp"&gt;Model Context Protocol (MCP)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#tracing--verbose-logging"&gt;Tracing / verbose logging&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#dotslash"&gt;DotSlash&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#configuration"&gt;Configuration&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#faq"&gt;FAQ&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#zero-data-retention-zdr-usage"&gt;Zero data retention (ZDR) usage&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#codex-open-source-fund"&gt;Codex open source fund&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#contributing"&gt;Contributing&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#development-workflow"&gt;Development workflow&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#writing-high-impact-code-changes"&gt;Writing high-impact code changes&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#opening-a-pull-request"&gt;Opening a pull request&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#review-process"&gt;Review process&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#community-values"&gt;Community values&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#getting-help"&gt;Getting help&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#contributor-license-agreement-cla"&gt;Contributor license agreement (CLA)&lt;/a&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#quick-fixes"&gt;Quick fixes&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#releasing-codex"&gt;Releasing &lt;code&gt;codex&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#security--responsible-ai"&gt;Security &amp;amp; responsible AI&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;!-- End ToC --&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;h3&gt;Installing and running Codex CLI&lt;/h3&gt; 
&lt;p&gt;Install globally with your preferred package manager:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;npm install -g @openai/codex  # Alternatively: `brew install codex`
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then simply run &lt;code&gt;codex&lt;/code&gt; to get started:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;codex
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;You can also go to the &lt;a href="https://github.com/openai/codex/releases/latest"&gt;latest GitHub Release&lt;/a&gt; and download the appropriate binary for your platform.&lt;/summary&gt; 
 &lt;p&gt;Each GitHub Release contains many executables, but in practice, you likely want one of these:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;macOS 
   &lt;ul&gt; 
    &lt;li&gt;Apple Silicon/arm64: &lt;code&gt;codex-aarch64-apple-darwin.tar.gz&lt;/code&gt;&lt;/li&gt; 
    &lt;li&gt;x86_64 (older Mac hardware): &lt;code&gt;codex-x86_64-apple-darwin.tar.gz&lt;/code&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;Linux 
   &lt;ul&gt; 
    &lt;li&gt;x86_64: &lt;code&gt;codex-x86_64-unknown-linux-musl.tar.gz&lt;/code&gt;&lt;/li&gt; 
    &lt;li&gt;arm64: &lt;code&gt;codex-aarch64-unknown-linux-musl.tar.gz&lt;/code&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;Each archive contains a single entry with the platform baked into the name (e.g., &lt;code&gt;codex-x86_64-unknown-linux-musl&lt;/code&gt;), so you likely want to rename it to &lt;code&gt;codex&lt;/code&gt; after extracting it.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h3&gt;Using Codex with your ChatGPT plan&lt;/h3&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/openai/codex/main/.github/codex-cli-login.png" alt="Codex CLI login" width="50%" /&gt; &lt;/p&gt; 
&lt;p&gt;Run &lt;code&gt;codex&lt;/code&gt; and select &lt;strong&gt;Sign in with ChatGPT&lt;/strong&gt;. You'll need a Plus, Pro, or Team ChatGPT account, and will get access to our latest models, including &lt;code&gt;gpt-5&lt;/code&gt;, at no extra cost to your plan. (Enterprise is coming soon.)&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Important: If you've used the Codex CLI before, follow these steps to migrate from usage-based billing with your API key:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Update the CLI and ensure &lt;code&gt;codex --version&lt;/code&gt; is &lt;code&gt;0.20.0&lt;/code&gt; or later&lt;/li&gt; 
  &lt;li&gt;Delete &lt;code&gt;~/.codex/auth.json&lt;/code&gt; (this should be &lt;code&gt;C:\Users\USERNAME\.codex\auth.json&lt;/code&gt; on Windows)&lt;/li&gt; 
  &lt;li&gt;Run &lt;code&gt;codex login&lt;/code&gt; again&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;If you encounter problems with the login flow, please comment on &lt;a href="https://github.com/openai/codex/issues/1243"&gt;this issue&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Connecting on a "Headless" Machine&lt;/h3&gt; 
&lt;p&gt;Today, the login process entails running a server on &lt;code&gt;localhost:1455&lt;/code&gt;. If you are on a "headless" server, such as a Docker container or are &lt;code&gt;ssh&lt;/code&gt;'d into a remote machine, loading &lt;code&gt;localhost:1455&lt;/code&gt; in the browser on your local machine will not automatically connect to the webserver running on the &lt;em&gt;headless&lt;/em&gt; machine, so you must use one of the following workarounds:&lt;/p&gt; 
&lt;h4&gt;Authenticate locally and copy your credentials to the "headless" machine&lt;/h4&gt; 
&lt;p&gt;The easiest solution is likely to run through the &lt;code&gt;codex login&lt;/code&gt; process on your local machine such that &lt;code&gt;localhost:1455&lt;/code&gt; &lt;em&gt;is&lt;/em&gt; accessible in your web browser. When you complete the authentication process, an &lt;code&gt;auth.json&lt;/code&gt; file should be available at &lt;code&gt;$CODEX_HOME/auth.json&lt;/code&gt; (on Mac/Linux, &lt;code&gt;$CODEX_HOME&lt;/code&gt; defaults to &lt;code&gt;~/.codex&lt;/code&gt; whereas on Windows, it defaults to &lt;code&gt;%USERPROFILE%\.codex&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;Because the &lt;code&gt;auth.json&lt;/code&gt; file is not tied to a specific host, once you complete the authentication flow locally, you can copy the &lt;code&gt;$CODEX_HOME/auth.json&lt;/code&gt; file to the headless machine and then &lt;code&gt;codex&lt;/code&gt; should "just work" on that machine. Note to copy a file to a Docker container, you can do:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# substitute MY_CONTAINER with the name or id of your Docker container:
CONTAINER_HOME=$(docker exec MY_CONTAINER printenv HOME)
docker exec MY_CONTAINER mkdir -p "$CONTAINER_HOME/.codex"
docker cp auth.json MY_CONTAINER:"$CONTAINER_HOME/.codex/auth.json"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;whereas if you are &lt;code&gt;ssh&lt;/code&gt;'d into a remote machine, you likely want to use &lt;a href="https://en.wikipedia.org/wiki/Secure_copy_protocol"&gt;&lt;code&gt;scp&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;ssh user@remote 'mkdir -p ~/.codex'
scp ~/.codex/auth.json user@remote:~/.codex/auth.json
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or try this one-liner:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;ssh user@remote 'mkdir -p ~/.codex &amp;amp;&amp;amp; cat &amp;gt; ~/.codex/auth.json' &amp;lt; ~/.codex/auth.json
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Connecting through VPS or remote&lt;/h4&gt; 
&lt;p&gt;If you run Codex on a remote machine (VPS/server) without a local browser, the login helper starts a server on &lt;code&gt;localhost:1455&lt;/code&gt; on the remote host. To complete login in your local browser, forward that port to your machine before starting the login flow:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# From your local machine
ssh -L 1455:localhost:1455 &amp;lt;user&amp;gt;@&amp;lt;remote-host&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then, in that SSH session, run &lt;code&gt;codex&lt;/code&gt; and select "Sign in with ChatGPT". When prompted, open the printed URL (it will be &lt;code&gt;http://localhost:1455/...&lt;/code&gt;) in your local browser. The traffic will be tunneled to the remote server.&lt;/p&gt; 
&lt;h3&gt;Usage-based billing alternative: Use an OpenAI API key&lt;/h3&gt; 
&lt;p&gt;If you prefer to pay-as-you-go, you can still authenticate with your OpenAI API key by setting it as an environment variable:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;export OPENAI_API_KEY="your-api-key-here"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Notes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;This command only sets the key for your current terminal session, which we recommend. To set it for all future sessions, you can also add the &lt;code&gt;export&lt;/code&gt; line to your shell's configuration file (e.g., &lt;code&gt;~/.zshrc&lt;/code&gt;).&lt;/li&gt; 
 &lt;li&gt;If you have signed in with ChatGPT, Codex will default to using your ChatGPT credits. If you wish to use your API key, use the &lt;code&gt;/logout&lt;/code&gt; command to clear your ChatGPT authentication.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Choosing Codex's level of autonomy&lt;/h3&gt; 
&lt;p&gt;We always recommend running Codex in its default sandbox that gives you strong guardrails around what the agent can do. The default sandbox prevents it from editing files outside its workspace, or from accessing the network.&lt;/p&gt; 
&lt;p&gt;When you launch Codex in a new folder, it detects whether the folder is version controlled and recommends one of two levels of autonomy:&lt;/p&gt; 
&lt;h4&gt;&lt;strong&gt;1. Read/write&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Codex can run commands and write files in the workspace without approval.&lt;/li&gt; 
 &lt;li&gt;To write files in other folders, access network, update git or perform other actions protected by the sandbox, Codex will need your permission.&lt;/li&gt; 
 &lt;li&gt;By default, the workspace includes the current directory, as well as temporary directories like &lt;code&gt;/tmp&lt;/code&gt;. You can see what directories are in the workspace with the &lt;code&gt;/status&lt;/code&gt; command. See the docs for how to customize this behavior.&lt;/li&gt; 
 &lt;li&gt;Advanced: You can manually specify this configuration by running &lt;code&gt;codex --sandbox workspace-write --ask-for-approval on-request&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;This is the recommended default for version-controlled folders.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;&lt;strong&gt;2. Read-only&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Codex can run read-only commands without approval.&lt;/li&gt; 
 &lt;li&gt;To edit files, access network, or perform other actions protected by the sandbox, Codex will need your permission.&lt;/li&gt; 
 &lt;li&gt;Advanced: You can manually specify this configuration by running &lt;code&gt;codex --sandbox read-only --ask-for-approval on-request&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;This is the recommended default non-version-controlled folders.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;&lt;strong&gt;3. Advanced configuration&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;Codex gives you fine-grained control over the sandbox with the &lt;code&gt;--sandbox&lt;/code&gt; option, and over when it requests approval with the &lt;code&gt;--ask-for-approval&lt;/code&gt; option. Run &lt;code&gt;codex help&lt;/code&gt; for more on these options.&lt;/p&gt; 
&lt;h4&gt;Can I run without ANY approvals?&lt;/h4&gt; 
&lt;p&gt;Yes, run codex non-interactively with &lt;code&gt;--ask-for-approval never&lt;/code&gt;. This option works with all &lt;code&gt;--sandbox&lt;/code&gt; options, so you still have full control over Codex's level of autonomy. It will make its best attempt with whatever contrainsts you provide. For example:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Use &lt;code&gt;codex --ask-for-approval never --sandbox read-only&lt;/code&gt; when you are running many agents to answer questions in parallel in the same workspace.&lt;/li&gt; 
 &lt;li&gt;Use &lt;code&gt;codex --ask-for-approval never --sandbox workspace-write&lt;/code&gt; when you want the agent to non-interactively take time to produce the best outcome, with strong guardrails around its behavior.&lt;/li&gt; 
 &lt;li&gt;Use &lt;code&gt;codex --ask-for-approval never --sandbox danger-full-access&lt;/code&gt; to dangerously give the agent full autonomy. Because this disables important safety mechanisms, we recommend against using this unless running Codex in an isolated environment.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Fine-tuning in &lt;code&gt;config.toml&lt;/code&gt;&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-toml"&gt;# approval mode
approval_policy = "untrusted"
sandbox_mode    = "read-only"

# full-auto mode
approval_policy = "on-request"
sandbox_mode    = "workspace-write"

# Optional: allow network in workspace-write mode
[sandbox_workspace_write]
network_access = true
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also save presets as &lt;strong&gt;profiles&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-toml"&gt;[profiles.full_auto]
approval_policy = "on-request"
sandbox_mode    = "workspace-write"

[profiles.readonly_quiet]
approval_policy = "never"
sandbox_mode    = "read-only"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Example prompts&lt;/h3&gt; 
&lt;p&gt;Below are a few bite-size examples you can copy-paste. Replace the text in quotes with your own task. See the &lt;a href="https://github.com/openai/codex/raw/main/codex-cli/examples/prompting_guide.md"&gt;prompting guide&lt;/a&gt; for more tips and usage patterns.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;✨&lt;/th&gt; 
   &lt;th&gt;What you type&lt;/th&gt; 
   &lt;th&gt;What happens&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;1&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;codex "Refactor the Dashboard component to React Hooks"&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Codex rewrites the class component, runs &lt;code&gt;npm test&lt;/code&gt;, and shows the diff.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;codex "Generate SQL migrations for adding a users table"&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Infers your ORM, creates migration files, and runs them in a sandboxed DB.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;3&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;codex "Write unit tests for utils/date.ts"&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Generates tests, executes them, and iterates until they pass.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;4&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;codex "Bulk-rename *.jpeg -&amp;gt; *.jpg with git mv"&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Safely renames files and updates imports/usages.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;codex "Explain what this regex does: ^(?=.*[A-Z]).{8,}$"&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Outputs a step-by-step human explanation.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;6&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;codex "Carefully review this repo, and propose 3 high impact well-scoped PRs"&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Suggests impactful PRs in the current codebase.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;7&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;codex "Look for vulnerabilities and create a security review report"&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Finds and explains security bugs.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Running with a prompt as input&lt;/h2&gt; 
&lt;p&gt;You can also run Codex CLI with a prompt as input:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;codex "explain this codebase to me"
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;codex --full-auto "create the fanciest todo-list app"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;That's it - Codex will scaffold a file, run it inside a sandbox, install any missing dependencies, and show you the live result. Approve the changes and they'll be committed to your working directory.&lt;/p&gt; 
&lt;h2&gt;Using Open Source Models&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Use &lt;code&gt;--profile&lt;/code&gt; to use other models&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Codex also allows you to use other providers that support the OpenAI Chat Completions (or Responses) API.&lt;/p&gt; 
 &lt;p&gt;To do so, you must first define custom &lt;a href="https://raw.githubusercontent.com/openai/codex/main/config.md#model_providers"&gt;providers&lt;/a&gt; in &lt;code&gt;~/.codex/config.toml&lt;/code&gt;. For example, the provider for a standard Ollama setup would be defined as follows:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-toml"&gt;[model_providers.ollama]
name = "Ollama"
base_url = "http://localhost:11434/v1"
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;The &lt;code&gt;base_url&lt;/code&gt; will have &lt;code&gt;/chat/completions&lt;/code&gt; appended to it to build the full URL for the request.&lt;/p&gt; 
 &lt;p&gt;For providers that also require an &lt;code&gt;Authorization&lt;/code&gt; header of the form &lt;code&gt;Bearer: SECRET&lt;/code&gt;, an &lt;code&gt;env_key&lt;/code&gt; can be specified, which indicates the environment variable to read to use as the value of &lt;code&gt;SECRET&lt;/code&gt; when making a request:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-toml"&gt;[model_providers.openrouter]
name = "OpenRouter"
base_url = "https://openrouter.ai/api/v1"
env_key = "OPENROUTER_API_KEY"
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Providers that speak the Responses API are also supported by adding &lt;code&gt;wire_api = "responses"&lt;/code&gt; as part of the definition. Accessing OpenAI models via Azure is an example of such a provider, though it also requires specifying additional &lt;code&gt;query_params&lt;/code&gt; that need to be appended to the request URL:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-toml"&gt;[model_providers.azure]
name = "Azure"
# Make sure you set the appropriate subdomain for this URL.
base_url = "https://YOUR_PROJECT_NAME.openai.azure.com/openai"
env_key = "AZURE_OPENAI_API_KEY"  # Or "OPENAI_API_KEY", whichever you use.
# Newer versions appear to support the responses API, see https://github.com/openai/codex/pull/1321
query_params = { api-version = "2025-04-01-preview" }
wire_api = "responses"
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Once you have defined a provider you wish to use, you can configure it as your default provider as follows:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-toml"&gt;model_provider = "azure"
&lt;/code&gt;&lt;/pre&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;[!TIP] If you find yourself experimenting with a variety of models and providers, then you likely want to invest in defining a &lt;em&gt;profile&lt;/em&gt; for each configuration like so:&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;pre&gt;&lt;code class="language-toml"&gt;[profiles.o3]
model_provider = "azure"
model = "o3"

[profiles.mistral]
model_provider = "ollama"
model = "mistral"
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;This way, you can specify one command-line argument (.e.g., &lt;code&gt;--profile o3&lt;/code&gt;, &lt;code&gt;--profile mistral&lt;/code&gt;) to override multiple settings together.&lt;/p&gt; 
&lt;/details&gt; 
&lt;p&gt;Codex can run fully locally against an OpenAI-compatible OSS host (like Ollama) using the &lt;code&gt;--oss&lt;/code&gt; flag:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Interactive UI: 
  &lt;ul&gt; 
   &lt;li&gt;codex --oss&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Non-interactive (programmatic) mode: 
  &lt;ul&gt; 
   &lt;li&gt;echo "Refactor utils" | codex exec --oss&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Model selection when using &lt;code&gt;--oss&lt;/code&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;If you omit &lt;code&gt;-m/--model&lt;/code&gt;, Codex defaults to -m gpt-oss:20b and will verify it exists locally (downloading if needed).&lt;/li&gt; 
 &lt;li&gt;To pick a different size, pass one of: 
  &lt;ul&gt; 
   &lt;li&gt;-m "gpt-oss:20b"&lt;/li&gt; 
   &lt;li&gt;-m "gpt-oss:120b"&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Point Codex at your own OSS host:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;By default, &lt;code&gt;--oss&lt;/code&gt; talks to &lt;a href="http://localhost:11434/v1"&gt;http://localhost:11434/v1&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;To use a different host, set one of these environment variables before running Codex: 
  &lt;ul&gt; 
   &lt;li&gt;CODEX_OSS_BASE_URL, for example: 
    &lt;ul&gt; 
     &lt;li&gt;CODEX_OSS_BASE_URL="&lt;a href="http://my-ollama.example.com:11434/v1"&gt;http://my-ollama.example.com:11434/v1&lt;/a&gt;" codex --oss -m gpt-oss:20b&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;or CODEX_OSS_PORT (when the host is localhost): 
    &lt;ul&gt; 
     &lt;li&gt;CODEX_OSS_PORT=11434 codex --oss&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Advanced: you can persist this in your config instead of environment variables by overriding the built-in &lt;code&gt;oss&lt;/code&gt; provider in &lt;code&gt;~/.codex/config.toml&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-toml"&gt;[model_providers.oss]
name = "Open Source"
base_url = "http://my-ollama.example.com:11434/v1"
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Platform sandboxing details&lt;/h3&gt; 
&lt;p&gt;The mechanism Codex uses to implement the sandbox policy depends on your OS:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;macOS 12+&lt;/strong&gt; uses &lt;strong&gt;Apple Seatbelt&lt;/strong&gt; and runs commands using &lt;code&gt;sandbox-exec&lt;/code&gt; with a profile (&lt;code&gt;-p&lt;/code&gt;) that corresponds to the &lt;code&gt;--sandbox&lt;/code&gt; that was specified.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Linux&lt;/strong&gt; uses a combination of Landlock/seccomp APIs to enforce the &lt;code&gt;sandbox&lt;/code&gt; configuration.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Note that when running Linux in a containerized environment such as Docker, sandboxing may not work if the host/container configuration does not support the necessary Landlock/seccomp APIs. In such cases, we recommend configuring your Docker container so that it provides the sandbox guarantees you are looking for and then running &lt;code&gt;codex&lt;/code&gt; with &lt;code&gt;--sandbox danger-full-access&lt;/code&gt; (or, more simply, the &lt;code&gt;--dangerously-bypass-approvals-and-sandbox&lt;/code&gt; flag) within your container.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Experimental technology disclaimer&lt;/h2&gt; 
&lt;p&gt;Codex CLI is an experimental project under active development. It is not yet stable, may contain bugs, incomplete features, or undergo breaking changes. We're building it in the open with the community and welcome:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Bug reports&lt;/li&gt; 
 &lt;li&gt;Feature requests&lt;/li&gt; 
 &lt;li&gt;Pull requests&lt;/li&gt; 
 &lt;li&gt;Good vibes&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Help us improve by filing issues or submitting PRs (see the section below for how to contribute)!&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;System requirements&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Requirement&lt;/th&gt; 
   &lt;th&gt;Details&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Operating systems&lt;/td&gt; 
   &lt;td&gt;macOS 12+, Ubuntu 20.04+/Debian 10+, or Windows 11 &lt;strong&gt;via WSL2&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Git (optional, recommended)&lt;/td&gt; 
   &lt;td&gt;2.23+ for built-in PR helpers&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;RAM&lt;/td&gt; 
   &lt;td&gt;4-GB minimum (8-GB recommended)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h2&gt;CLI reference&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Command&lt;/th&gt; 
   &lt;th&gt;Purpose&lt;/th&gt; 
   &lt;th&gt;Example&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;codex&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Interactive TUI&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;codex&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;codex "..."&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Initial prompt for interactive TUI&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;codex "fix lint errors"&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;codex exec "..."&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Non-interactive "automation mode"&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;codex exec "explain utils.ts"&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Key flags: &lt;code&gt;--model/-m&lt;/code&gt;, &lt;code&gt;--ask-for-approval/-a&lt;/code&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Memory &amp;amp; project docs&lt;/h2&gt; 
&lt;p&gt;You can give Codex extra instructions and guidance using &lt;code&gt;AGENTS.md&lt;/code&gt; files. Codex looks for &lt;code&gt;AGENTS.md&lt;/code&gt; files in the following places, and merges them top-down:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;code&gt;~/.codex/AGENTS.md&lt;/code&gt; - personal global guidance&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;AGENTS.md&lt;/code&gt; at repo root - shared project notes&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;AGENTS.md&lt;/code&gt; in the current working directory - sub-folder/feature specifics&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Non-interactive / CI mode&lt;/h2&gt; 
&lt;p&gt;Run Codex head-less in pipelines. Example GitHub Action step:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;- name: Update changelog via Codex
  run: |
    npm install -g @openai/codex
    export OPENAI_API_KEY="${{ secrets.OPENAI_KEY }}"
    codex exec --full-auto "update CHANGELOG for next release"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Model Context Protocol (MCP)&lt;/h2&gt; 
&lt;p&gt;The Codex CLI can be configured to leverage MCP servers by defining an &lt;a href="https://raw.githubusercontent.com/openai/codex/main/codex-rs/config.md#mcp_servers"&gt;&lt;code&gt;mcp_servers&lt;/code&gt;&lt;/a&gt; section in &lt;code&gt;~/.codex/config.toml&lt;/code&gt;. It is intended to mirror how tools such as Claude and Cursor define &lt;code&gt;mcpServers&lt;/code&gt; in their respective JSON config files, though the Codex format is slightly different since it uses TOML rather than JSON, e.g.:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-toml"&gt;# IMPORTANT: the top-level key is `mcp_servers` rather than `mcpServers`.
[mcp_servers.server-name]
command = "npx"
args = ["-y", "mcp-server"]
env = { "API_KEY" = "value" }
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] It is somewhat experimental, but the Codex CLI can also be run as an MCP &lt;em&gt;server&lt;/em&gt; via &lt;code&gt;codex mcp&lt;/code&gt;. If you launch it with an MCP client such as &lt;code&gt;npx @modelcontextprotocol/inspector codex mcp&lt;/code&gt; and send it a &lt;code&gt;tools/list&lt;/code&gt; request, you will see that there is only one tool, &lt;code&gt;codex&lt;/code&gt;, that accepts a grab-bag of inputs, including a catch-all &lt;code&gt;config&lt;/code&gt; map for anything you might want to override. Feel free to play around with it and provide feedback via GitHub issues.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Tracing / verbose logging&lt;/h2&gt; 
&lt;p&gt;Because Codex is written in Rust, it honors the &lt;code&gt;RUST_LOG&lt;/code&gt; environment variable to configure its logging behavior.&lt;/p&gt; 
&lt;p&gt;The TUI defaults to &lt;code&gt;RUST_LOG=codex_core=info,codex_tui=info&lt;/code&gt; and log messages are written to &lt;code&gt;~/.codex/log/codex-tui.log&lt;/code&gt;, so you can leave the following running in a separate terminal to monitor log messages as they are written:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;tail -F ~/.codex/log/codex-tui.log
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;By comparison, the non-interactive mode (&lt;code&gt;codex exec&lt;/code&gt;) defaults to &lt;code&gt;RUST_LOG=error&lt;/code&gt;, but messages are printed inline, so there is no need to monitor a separate file.&lt;/p&gt; 
&lt;p&gt;See the Rust documentation on &lt;a href="https://docs.rs/env_logger/latest/env_logger/#enabling-logging"&gt;&lt;code&gt;RUST_LOG&lt;/code&gt;&lt;/a&gt; for more information on the configuration options.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;DotSlash&lt;/h3&gt; 
&lt;p&gt;The GitHub Release also contains a &lt;a href="https://dotslash-cli.com/"&gt;DotSlash&lt;/a&gt; file for the Codex CLI named &lt;code&gt;codex&lt;/code&gt;. Using a DotSlash file makes it possible to make a lightweight commit to source control to ensure all contributors use the same version of an executable, regardless of what platform they use for development.&lt;/p&gt;  
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Build from source&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Clone the repository and navigate to the root of the Cargo workspace.
git clone https://github.com/openai/codex.git
cd codex/codex-rs

# Install the Rust toolchain, if necessary.
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y
source "$HOME/.cargo/env"
rustup component add rustfmt
rustup component add clippy

# Build Codex.
cargo build

# Launch the TUI with a sample prompt.
cargo run --bin codex -- "explain this codebase to me"

# After making changes, ensure the code is clean.
cargo fmt -- --config imports_granularity=Item
cargo clippy --tests

# Run the tests.
cargo test
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Configuration&lt;/h2&gt; 
&lt;p&gt;Codex supports a rich set of configuration options documented in &lt;a href="https://raw.githubusercontent.com/openai/codex/main/codex-rs/config.md"&gt;&lt;code&gt;codex-rs/config.md&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;By default, Codex loads its configuration from &lt;code&gt;~/.codex/config.toml&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Though &lt;code&gt;--config&lt;/code&gt; can be used to set/override ad-hoc config values for individual invocations of &lt;code&gt;codex&lt;/code&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;FAQ&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;OpenAI released a model called Codex in 2021 - is this related?&lt;/summary&gt; 
 &lt;p&gt;In 2021, OpenAI released Codex, an AI system designed to generate code from natural language prompts. That original Codex model was deprecated as of March 2023 and is separate from the CLI tool.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Which models are supported?&lt;/summary&gt; 
 &lt;p&gt;Any model available with &lt;a href="https://platform.openai.com/docs/api-reference/responses"&gt;Responses API&lt;/a&gt;. The default is &lt;code&gt;o4-mini&lt;/code&gt;, but pass &lt;code&gt;--model gpt-4.1&lt;/code&gt; or set &lt;code&gt;model: gpt-4.1&lt;/code&gt; in your config file to override.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Why does &lt;code&gt;o3&lt;/code&gt; or &lt;code&gt;o4-mini&lt;/code&gt; not work for me?&lt;/summary&gt; 
 &lt;p&gt;It's possible that your &lt;a href="https://help.openai.com/en/articles/10910291-api-organization-verification"&gt;API account needs to be verified&lt;/a&gt; in order to start streaming responses and seeing chain of thought summaries from the API. If you're still running into issues, please let us know!&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;How do I stop Codex from editing my files?&lt;/summary&gt; 
 &lt;p&gt;Codex runs model-generated commands in a sandbox. If a proposed command or file change doesn't look right, you can simply type &lt;strong&gt;n&lt;/strong&gt; to deny the command or give the model feedback.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Does it work on Windows?&lt;/summary&gt; 
 &lt;p&gt;Not directly. It requires &lt;a href="https://learn.microsoft.com/en-us/windows/wsl/install"&gt;Windows Subsystem for Linux (WSL2)&lt;/a&gt; - Codex has been tested on macOS and Linux with Node 22.&lt;/p&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Zero data retention (ZDR) usage&lt;/h2&gt; 
&lt;p&gt;Codex CLI &lt;strong&gt;does&lt;/strong&gt; support OpenAI organizations with &lt;a href="https://platform.openai.com/docs/guides/your-data#zero-data-retention"&gt;Zero Data Retention (ZDR)&lt;/a&gt; enabled. If your OpenAI organization has Zero Data Retention enabled and you still encounter errors such as:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;OpenAI rejected the request. Error details: Status: 400, Code: unsupported_parameter, Type: invalid_request_error, Message: 400 Previous response cannot be used for this organization due to Zero Data Retention.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Ensure you are running &lt;code&gt;codex&lt;/code&gt; with &lt;code&gt;--config disable_response_storage=true&lt;/code&gt; or add this line to &lt;code&gt;~/.codex/config.toml&lt;/code&gt; to avoid specifying the command line option each time:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-toml"&gt;disable_response_storage = true
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/openai/codex/main/codex-rs/config.md#disable_response_storage"&gt;the configuration documentation on &lt;code&gt;disable_response_storage&lt;/code&gt;&lt;/a&gt; for details.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Codex open source fund&lt;/h2&gt; 
&lt;p&gt;We're excited to launch a &lt;strong&gt;$1 million initiative&lt;/strong&gt; supporting open source projects that use Codex CLI and other OpenAI models.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Grants are awarded up to &lt;strong&gt;$25,000&lt;/strong&gt; API credits.&lt;/li&gt; 
 &lt;li&gt;Applications are reviewed &lt;strong&gt;on a rolling basis&lt;/strong&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Interested? &lt;a href="https://openai.com/form/codex-open-source-fund/"&gt;Apply here&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;This project is under active development and the code will likely change pretty significantly.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;At the moment, we only plan to prioritize reviewing external contributions for bugs or security fixes.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;If you want to add a new feature or change the behavior of an existing one, please open an issue proposing the feature and get approval from an OpenAI team member before spending time building it.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;New contributions that don't go through this process may be closed&lt;/strong&gt; if they aren't aligned with our current roadmap or conflict with other priorities/upcoming features.&lt;/p&gt; 
&lt;h3&gt;Development workflow&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Create a &lt;em&gt;topic branch&lt;/em&gt; from &lt;code&gt;main&lt;/code&gt; - e.g. &lt;code&gt;feat/interactive-prompt&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Keep your changes focused. Multiple unrelated fixes should be opened as separate PRs.&lt;/li&gt; 
 &lt;li&gt;Following the &lt;a href="https://raw.githubusercontent.com/openai/codex/main/#development-workflow"&gt;development setup&lt;/a&gt; instructions above, ensure your change is free of lint warnings and test failures.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Writing high-impact code changes&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Start with an issue.&lt;/strong&gt; Open a new one or comment on an existing discussion so we can agree on the solution before code is written.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Add or update tests.&lt;/strong&gt; Every new feature or bug-fix should come with test coverage that fails before your change and passes afterwards. 100% coverage is not required, but aim for meaningful assertions.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Document behaviour.&lt;/strong&gt; If your change affects user-facing behaviour, update the README, inline help (&lt;code&gt;codex --help&lt;/code&gt;), or relevant example projects.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Keep commits atomic.&lt;/strong&gt; Each commit should compile and the tests should pass. This makes reviews and potential rollbacks easier.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Opening a pull request&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Fill in the PR template (or include similar information) - &lt;strong&gt;What? Why? How?&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Run &lt;strong&gt;all&lt;/strong&gt; checks locally (&lt;code&gt;cargo test &amp;amp;&amp;amp; cargo clippy --tests &amp;amp;&amp;amp; cargo fmt -- --config imports_granularity=Item&lt;/code&gt;). CI failures that could have been caught locally slow down the process.&lt;/li&gt; 
 &lt;li&gt;Make sure your branch is up-to-date with &lt;code&gt;main&lt;/code&gt; and that you have resolved merge conflicts.&lt;/li&gt; 
 &lt;li&gt;Mark the PR as &lt;strong&gt;Ready for review&lt;/strong&gt; only when you believe it is in a merge-able state.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Review process&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;One maintainer will be assigned as a primary reviewer.&lt;/li&gt; 
 &lt;li&gt;If your PR adds a new feature that was not previously discussed and approved, we may choose to close your PR (see &lt;a href="https://raw.githubusercontent.com/openai/codex/main/#contributing"&gt;Contributing&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;We may ask for changes - please do not take this personally. We value the work, but we also value consistency and long-term maintainability.&lt;/li&gt; 
 &lt;li&gt;When there is consensus that the PR meets the bar, a maintainer will squash-and-merge.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Community values&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Be kind and inclusive.&lt;/strong&gt; Treat others with respect; we follow the &lt;a href="https://www.contributor-covenant.org/"&gt;Contributor Covenant&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Assume good intent.&lt;/strong&gt; Written communication is hard - err on the side of generosity.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Teach &amp;amp; learn.&lt;/strong&gt; If you spot something confusing, open an issue or PR with improvements.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Getting help&lt;/h3&gt; 
&lt;p&gt;If you run into problems setting up the project, would like feedback on an idea, or just want to say &lt;em&gt;hi&lt;/em&gt; - please open a Discussion or jump into the relevant issue. We are happy to help.&lt;/p&gt; 
&lt;p&gt;Together we can make Codex CLI an incredible tool. &lt;strong&gt;Happy hacking!&lt;/strong&gt; &lt;span&gt;🚀&lt;/span&gt;&lt;/p&gt; 
&lt;h3&gt;Contributor license agreement (CLA)&lt;/h3&gt; 
&lt;p&gt;All contributors &lt;strong&gt;must&lt;/strong&gt; accept the CLA. The process is lightweight:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Open your pull request.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Paste the following comment (or reply &lt;code&gt;recheck&lt;/code&gt; if you've signed before):&lt;/p&gt; &lt;pre&gt;&lt;code class="language-text"&gt;I have read the CLA Document and I hereby sign the CLA
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;The CLA-Assistant bot records your signature in the repo and marks the status check as passed.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;No special Git commands, email attachments, or commit footers required.&lt;/p&gt; 
&lt;h4&gt;Quick fixes&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Scenario&lt;/th&gt; 
   &lt;th&gt;Command&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Amend last commit&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;git commit --amend -s --no-edit &amp;amp;&amp;amp; git push -f&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;The &lt;strong&gt;DCO check&lt;/strong&gt; blocks merges until every commit in the PR carries the footer (with squash this is just the one).&lt;/p&gt; 
&lt;h3&gt;Releasing &lt;code&gt;codex&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;em&gt;For admins only.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;Make sure you are on &lt;code&gt;main&lt;/code&gt; and have no local changes. Then run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;VERSION=0.2.0  # Can also be 0.2.0-alpha.1 or any valid Rust version.
./codex-rs/scripts/create_github_release.sh "$VERSION"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will make a local commit on top of &lt;code&gt;main&lt;/code&gt; with &lt;code&gt;version&lt;/code&gt; set to &lt;code&gt;$VERSION&lt;/code&gt; in &lt;code&gt;codex-rs/Cargo.toml&lt;/code&gt; (note that on &lt;code&gt;main&lt;/code&gt;, we leave the version as &lt;code&gt;version = "0.0.0"&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;This will push the commit using the tag &lt;code&gt;rust-v${VERSION}&lt;/code&gt;, which in turn kicks off &lt;a href="https://raw.githubusercontent.com/openai/codex/main/.github/workflows/rust-release.yml"&gt;the release workflow&lt;/a&gt;. This will create a new GitHub Release named &lt;code&gt;$VERSION&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;If everything looks good in the generated GitHub Release, uncheck the &lt;strong&gt;pre-release&lt;/strong&gt; box so it is the latest release.&lt;/p&gt; 
&lt;p&gt;Create a PR to update &lt;a href="https://github.com/Homebrew/homebrew-core/raw/main/Formula/c/codex.rb"&gt;&lt;code&gt;Formula/c/codex.rb&lt;/code&gt;&lt;/a&gt; on Homebrew.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Security &amp;amp; responsible AI&lt;/h2&gt; 
&lt;p&gt;Have you discovered a vulnerability or have concerns about model output? Please e-mail &lt;strong&gt;&lt;a href="mailto:security@openai.com"&gt;security@openai.com&lt;/a&gt;&lt;/strong&gt; and we will respond promptly.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This repository is licensed under the &lt;a href="https://raw.githubusercontent.com/openai/codex/main/LICENSE"&gt;Apache-2.0 License&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>