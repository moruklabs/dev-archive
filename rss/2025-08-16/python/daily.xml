<rss version="2.0">
  <channel>
    <title>GitHub Python Daily Trending</title>
    <description>Daily Trending of Python in GitHub</description>
    <pubDate>Fri, 15 Aug 2025 01:39:05 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>paperless-ngx/paperless-ngx</title>
      <link>https://github.com/paperless-ngx/paperless-ngx</link>
      <description>&lt;p&gt;A community-supported supercharged document management system: scan, index and archive all your documents&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="https://github.com/paperless-ngx/paperless-ngx/actions"&gt;&lt;img src="https://github.com/paperless-ngx/paperless-ngx/workflows/ci/badge.svg?sanitize=true" alt="ci" /&gt;&lt;/a&gt; &lt;a href="https://crowdin.com/project/paperless-ngx"&gt;&lt;img src="https://badges.crowdin.net/paperless-ngx/localized.svg?sanitize=true" alt="Crowdin" /&gt;&lt;/a&gt; &lt;a href="https://docs.paperless-ngx.com"&gt;&lt;img src="https://img.shields.io/github/deployments/paperless-ngx/paperless-ngx/github-pages?label=docs" alt="Documentation Status" /&gt;&lt;/a&gt; &lt;a href="https://codecov.io/gh/paperless-ngx/paperless-ngx"&gt;&lt;img src="https://codecov.io/gh/paperless-ngx/paperless-ngx/branch/main/graph/badge.svg?token=VK6OUPJ3TY" alt="codecov" /&gt;&lt;/a&gt; &lt;a href="https://matrix.to/#/%23paperlessngx%3Amatrix.org"&gt;&lt;img src="https://matrix.to/img/matrix-badge.svg?sanitize=true" alt="Chat on Matrix" /&gt;&lt;/a&gt; &lt;a href="https://demo.paperless-ngx.com"&gt;&lt;img src="https://cronitor.io/badges/ve7ItY/production/W5E_B9jkelG9ZbDiNHUPQEVH3MY.svg?sanitize=true" alt="demo" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://github.com/paperless-ngx/paperless-ngx/blob/main/resources/logo/web/png/White%20logo%20-%20no%20background.png" width="50%" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://github.com/paperless-ngx/paperless-ngx/raw/main/resources/logo/web/png/Black%20logo%20-%20no%20background.png" width="50%" /&gt; 
  &lt;img src="https://github.com/paperless-ngx/paperless-ngx/raw/main/resources/logo/web/png/Black%20logo%20-%20no%20background.png" width="50%" /&gt; 
 &lt;/picture&gt; &lt;/p&gt; 
&lt;!-- omit in toc --&gt; 
&lt;h1&gt;Paperless-ngx&lt;/h1&gt; 
&lt;p&gt;Paperless-ngx is a document management system that transforms your physical documents into a searchable online archive so you can keep, well, &lt;em&gt;less paper&lt;/em&gt;.&lt;/p&gt; 
&lt;p&gt;Paperless-ngx is the official successor to the original &lt;a href="https://github.com/the-paperless-project/paperless"&gt;Paperless&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/jonaswinkler/paperless-ng"&gt;Paperless-ng&lt;/a&gt; projects and is designed to distribute the responsibility of advancing and supporting the project among a team of people. &lt;a href="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/dev/#community-support"&gt;Consider joining us!&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Thanks to the generous folks at &lt;a href="https://m.do.co/c/8d70b916d462"&gt;DigitalOcean&lt;/a&gt;, a demo is available at &lt;a href="https://demo.paperless-ngx.com"&gt;demo.paperless-ngx.com&lt;/a&gt; using login &lt;code&gt;demo&lt;/code&gt; / &lt;code&gt;demo&lt;/code&gt;. &lt;em&gt;Note: demo content is reset frequently and confidential information should not be uploaded.&lt;/em&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/dev/#features"&gt;Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/dev/#getting-started"&gt;Getting started&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/dev/#contributing"&gt;Contributing&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/dev/#community-support"&gt;Community Support&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/dev/#translation"&gt;Translation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/dev/#feature-requests"&gt;Feature Requests&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/dev/#bugs"&gt;Bugs&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/dev/#related-projects"&gt;Related Projects&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/dev/#important-note"&gt;Important Note&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="right"&gt;This project is supported by:&lt;br /&gt; &lt;a href="https://m.do.co/c/8d70b916d462" style="padding-top: 4px; display: block;"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="https://opensource.nyc3.cdn.digitaloceanspaces.com/attribution/assets/SVG/DO_Logo_horizontal_white.svg" width="140px" /&gt; 
   &lt;source media="(prefers-color-scheme: light)" srcset="https://opensource.nyc3.cdn.digitaloceanspaces.com/attribution/assets/SVG/DO_Logo_horizontal_blue.svg" width="140px" /&gt; 
   &lt;img src="https://opensource.nyc3.cdn.digitaloceanspaces.com/attribution/assets/SVG/DO_Logo_horizontal_black_.svg?sanitize=true" width="140px" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h1&gt;Features&lt;/h1&gt; 
&lt;picture&gt; 
 &lt;source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/docs/assets/screenshots/documents-smallcards-dark.png" /&gt; 
 &lt;source media="(prefers-color-scheme: light)" srcset="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/docs/assets/screenshots/documents-smallcards.png" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/docs/assets/screenshots/documents-smallcards.png" /&gt; 
&lt;/picture&gt; 
&lt;p&gt;A full list of &lt;a href="https://docs.paperless-ngx.com/#features"&gt;features&lt;/a&gt; and &lt;a href="https://docs.paperless-ngx.com/#screenshots"&gt;screenshots&lt;/a&gt; are available in the &lt;a href="https://docs.paperless-ngx.com/"&gt;documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Getting started&lt;/h1&gt; 
&lt;p&gt;The easiest way to deploy paperless is &lt;code&gt;docker compose&lt;/code&gt;. The files in the &lt;a href="https://github.com/paperless-ngx/paperless-ngx/tree/main/docker/compose"&gt;&lt;code&gt;/docker/compose&lt;/code&gt; directory&lt;/a&gt; are configured to pull the image from the GitHub container registry.&lt;/p&gt; 
&lt;p&gt;If you'd like to jump right in, you can configure a &lt;code&gt;docker compose&lt;/code&gt; environment with our install script:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;bash -c "$(curl -L https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/install-paperless-ngx.sh)"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;More details and step-by-step guides for alternative installation methods can be found in &lt;a href="https://docs.paperless-ngx.com/setup/#installation"&gt;the documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Migrating from Paperless-ng is easy, just drop in the new docker image! See the &lt;a href="https://docs.paperless-ngx.com/setup/#migrating-to-paperless-ngx"&gt;documentation on migrating&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;!-- omit in toc --&gt; 
&lt;h3&gt;Documentation&lt;/h3&gt; 
&lt;p&gt;The documentation for Paperless-ngx is available at &lt;a href="https://docs.paperless-ngx.com/"&gt;https://docs.paperless-ngx.com&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Contributing&lt;/h1&gt; 
&lt;p&gt;If you feel like contributing to the project, please do! Bug fixes, enhancements, visual fixes etc. are always welcome. If you want to implement something big: Please start a discussion about that! The &lt;a href="https://docs.paperless-ngx.com/development/"&gt;documentation&lt;/a&gt; has some basic information on how to get started.&lt;/p&gt; 
&lt;h2&gt;Community Support&lt;/h2&gt; 
&lt;p&gt;People interested in continuing the work on paperless-ngx are encouraged to reach out here on github and in the &lt;a href="https://matrix.to/#/#paperless:matrix.org"&gt;Matrix Room&lt;/a&gt;. If you would like to contribute to the project on an ongoing basis there are multiple &lt;a href="https://github.com/orgs/paperless-ngx/people"&gt;teams&lt;/a&gt; (frontend, ci/cd, etc) that could use your help so please reach out!&lt;/p&gt; 
&lt;h2&gt;Translation&lt;/h2&gt; 
&lt;p&gt;Paperless-ngx is available in many languages that are coordinated on Crowdin. If you want to help out by translating paperless-ngx into your language, please head over to &lt;a href="https://crowdin.com/project/paperless-ngx"&gt;https://crowdin.com/project/paperless-ngx&lt;/a&gt;, and thank you! More details can be found in &lt;a href="https://github.com/paperless-ngx/paperless-ngx/raw/main/CONTRIBUTING.md#translating-paperless-ngx"&gt;CONTRIBUTING.md&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Feature Requests&lt;/h2&gt; 
&lt;p&gt;Feature requests can be submitted via &lt;a href="https://github.com/paperless-ngx/paperless-ngx/discussions/categories/feature-requests"&gt;GitHub Discussions&lt;/a&gt;, you can search for existing ideas, add your own and vote for the ones you care about.&lt;/p&gt; 
&lt;h2&gt;Bugs&lt;/h2&gt; 
&lt;p&gt;For bugs please &lt;a href="https://github.com/paperless-ngx/paperless-ngx/issues"&gt;open an issue&lt;/a&gt; or &lt;a href="https://github.com/paperless-ngx/paperless-ngx/discussions"&gt;start a discussion&lt;/a&gt; if you have questions.&lt;/p&gt; 
&lt;h1&gt;Related Projects&lt;/h1&gt; 
&lt;p&gt;Please see &lt;a href="https://github.com/paperless-ngx/paperless-ngx/wiki/Related-Projects"&gt;the wiki&lt;/a&gt; for a user-maintained list of related projects and software that is compatible with Paperless-ngx.&lt;/p&gt; 
&lt;h1&gt;Important Note&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Document scanners are typically used to scan sensitive documents like your social insurance number, tax records, invoices, etc. &lt;strong&gt;Paperless-ngx should never be run on an untrusted host&lt;/strong&gt; because information is stored in clear text without encryption. No guarantees are made regarding security (but we do try!) and you use the app at your own risk. &lt;strong&gt;The safest way to run Paperless-ngx is on a local server in your own home with backups in place&lt;/strong&gt;.&lt;/p&gt; 
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>browser-use/browser-use</title>
      <link>https://github.com/browser-use/browser-use</link>
      <description>&lt;p&gt;🌐 Make websites accessible for AI agents. Automate tasks online with ease.&lt;/p&gt;&lt;hr&gt;&lt;picture&gt; 
 &lt;source media="(prefers-color-scheme: dark)" srcset="./static/browser-use-dark.png" /&gt; 
 &lt;source media="(prefers-color-scheme: light)" srcset="./static/browser-use.png" /&gt; 
 &lt;img alt="Shows a black Browser Use Logo in light color mode and a white one in dark color mode." src="https://raw.githubusercontent.com/browser-use/browser-use/main/static/browser-use.png" width="full" /&gt; 
&lt;/picture&gt; 
&lt;h1 align="center"&gt;Enable AI to control your browser 🤖&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/gregpr07/browser-use/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/gregpr07/browser-use?style=social" alt="GitHub stars" /&gt;&lt;/a&gt; &lt;a href="https://link.browser-use.com/discord"&gt;&lt;img src="https://img.shields.io/discord/1303749220842340412?color=7289DA&amp;amp;label=Discord&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://cloud.browser-use.com"&gt;&lt;img src="https://img.shields.io/badge/Cloud-%E2%98%81%EF%B8%8F-blue" alt="Cloud" /&gt;&lt;/a&gt; &lt;a href="https://docs.browser-use.com"&gt;&lt;img src="https://img.shields.io/badge/Documentation-%F0%9F%93%95-blue" alt="Documentation" /&gt;&lt;/a&gt; &lt;a href="https://x.com/intent/user?screen_name=gregpr07"&gt;&lt;img src="https://img.shields.io/twitter/follow/Gregor?style=social" alt="Twitter Follow" /&gt;&lt;/a&gt; &lt;a href="https://x.com/intent/user?screen_name=mamagnus00"&gt;&lt;img src="https://img.shields.io/twitter/follow/Magnus?style=social" alt="Twitter Follow" /&gt;&lt;/a&gt; &lt;a href="https://app.workweave.ai/reports/repository/org_T5Pvn3UBswTHIsN1dWS3voPg/881458615"&gt;&lt;img src="https://img.shields.io/endpoint?url=https%3A%2F%2Fapp.workweave.ai%2Fapi%2Frepository%2Fbadge%2Forg_T5Pvn3UBswTHIsN1dWS3voPg%2F881458615&amp;amp;labelColor=#EC6341" alt="Weave Badge" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;🌐 Browser-use is the easiest way to connect your AI agents with the browser.&lt;/p&gt; 
&lt;p&gt;💡 See what others are building and share your projects in our &lt;a href="https://link.browser-use.com/discord"&gt;Discord&lt;/a&gt;! Want Swag? Check out our &lt;a href="https://browsermerch.com"&gt;Merch store&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;🌤️ Skip the setup - try our &lt;b&gt;hosted version&lt;/b&gt; for instant browser automation! &lt;b&gt;&lt;a href="https://cloud.browser-use.com"&gt;Try the cloud ☁︎&lt;/a&gt;&lt;/b&gt;.&lt;/p&gt; 
&lt;h1&gt;Quick start&lt;/h1&gt; 
&lt;p&gt;With pip (Python&amp;gt;=3.11):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install browser-use
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Install the browser:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;playwright install chromium --with-deps --no-shell
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Spin up your agent:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import asyncio
from dotenv import load_dotenv
load_dotenv()
from browser_use import Agent
from browser_use.llm import ChatOpenAI

async def main():
    agent = Agent(
        task="Compare the price of gpt-4o and DeepSeek-V3",
        llm=ChatOpenAI(model="o4-mini", temperature=1.0),
    )
    await agent.run()

asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Add your API keys for the provider you want to use to your &lt;code&gt;.env&lt;/code&gt; file.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;OPENAI_API_KEY=
ANTHROPIC_API_KEY=
AZURE_OPENAI_ENDPOINT=
AZURE_OPENAI_KEY=
GOOGLE_API_KEY=
DEEPSEEK_API_KEY=
GROK_API_KEY=
NOVITA_API_KEY=
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For other settings, models, and more, check out the &lt;a href="https://docs.browser-use.com"&gt;documentation 📕&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Test with UI&lt;/h3&gt; 
&lt;p&gt;You can test browser-use using its &lt;a href="https://github.com/browser-use/web-ui"&gt;Web UI&lt;/a&gt; or &lt;a href="https://github.com/browser-use/desktop"&gt;Desktop App&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Test with an interactive CLI&lt;/h3&gt; 
&lt;p&gt;You can also use our &lt;code&gt;browser-use&lt;/code&gt; interactive CLI (similar to &lt;code&gt;claude&lt;/code&gt; code):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install "browser-use[cli]"
browser-use
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;MCP Integration&lt;/h2&gt; 
&lt;p&gt;Browser-use supports the &lt;a href="https://modelcontextprotocol.io/"&gt;Model Context Protocol (MCP)&lt;/a&gt;, enabling integration with Claude Desktop and other MCP-compatible clients.&lt;/p&gt; 
&lt;h3&gt;Use as MCP Server with Claude Desktop&lt;/h3&gt; 
&lt;p&gt;Add browser-use to your Claude Desktop configuration:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "browser-use": {
      "command": "uvx",
      "args": ["browser-use[cli]", "--mcp"],
      "env": {
        "OPENAI_API_KEY": "sk-..."
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This gives Claude Desktop access to browser automation tools for web scraping, form filling, and more.&lt;/p&gt; 
&lt;h3&gt;Connect External MCP Servers to Browser-Use Agent&lt;/h3&gt; 
&lt;p&gt;Browser-use agents can connect to multiple external MCP servers to extend their capabilities:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import asyncio
from browser_use import Agent, Controller
from browser_use.mcp.client import MCPClient
from browser_use.llm import ChatOpenAI

async def main():
    # Initialize controller
    controller = Controller()
    
    # Connect to multiple MCP servers
    filesystem_client = MCPClient(
        server_name="filesystem",
        command="npx",
        args=["-y", "@modelcontextprotocol/server-filesystem", "/Users/me/documents"]
    )
    
    github_client = MCPClient(
        server_name="github", 
        command="npx",
        args=["-y", "@modelcontextprotocol/server-github"],
        env={"GITHUB_TOKEN": "your-github-token"}
    )
    
    # Connect and register tools from both servers
    await filesystem_client.connect()
    await filesystem_client.register_to_controller(controller)
    
    await github_client.connect()
    await github_client.register_to_controller(controller)
    
    # Create agent with MCP-enabled controller
    agent = Agent(
        task="Find the latest report.pdf in my documents and create a GitHub issue about it",
        llm=ChatOpenAI(model="gpt-4o"),
        controller=controller  # Controller has tools from both MCP servers
    )
    
    # Run the agent
    await agent.run()
    
    # Cleanup
    await filesystem_client.disconnect()
    await github_client.disconnect()

asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See the &lt;a href="https://docs.browser-use.com/customize/mcp-server"&gt;MCP documentation&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h1&gt;Demos&lt;/h1&gt; 
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/browser-use/browser-use/raw/main/examples/use-cases/shopping.py"&gt;Task&lt;/a&gt;: Add grocery items to cart, and checkout.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=L2Ya9PYNns8"&gt;&lt;img src="https://github.com/user-attachments/assets/a0ffd23d-9a11-4368-8893-b092703abc14" alt="AI Did My Groceries" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt; 
&lt;p&gt;Prompt: Add my latest LinkedIn follower to my leads in Salesforce.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/50d6e691-b66b-4077-a46c-49e9d4707e07" alt="LinkedIn to Salesforce" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/browser-use/browser-use/raw/main/examples/use-cases/find_and_apply_to_jobs.py"&gt;Prompt&lt;/a&gt;: Read my CV &amp;amp; find ML jobs, save them to a file, and then start applying for them in new tabs, if you need help, ask me.'&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/171fb4d6-0355-46f2-863e-edb04a828d04"&gt;https://github.com/user-attachments/assets/171fb4d6-0355-46f2-863e-edb04a828d04&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/browser-use/browser-use/raw/main/examples/browser/real_browser.py"&gt;Prompt&lt;/a&gt;: Write a letter in Google Docs to my Papa, thanking him for everything, and save the document as a PDF.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/242ade3e-15bc-41c2-988f-cbc5415a66aa" alt="Letter to Papa" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/browser-use/browser-use/raw/main/examples/custom-functions/save_to_file_hugging_face.py"&gt;Prompt&lt;/a&gt;: Look up models with a license of cc-by-sa-4.0 and sort by most likes on Hugging face, save top 5 to file.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/de73ee39-432c-4b97-b4e8-939fd7f323b3"&gt;https://github.com/user-attachments/assets/de73ee39-432c-4b97-b4e8-939fd7f323b3&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt; 
&lt;h2&gt;More examples&lt;/h2&gt; 
&lt;p&gt;For more examples see the &lt;a href="https://raw.githubusercontent.com/browser-use/browser-use/main/examples"&gt;examples&lt;/a&gt; folder or join the &lt;a href="https://link.browser-use.com/discord"&gt;Discord&lt;/a&gt; and show off your project. You can also see our &lt;a href="https://github.com/browser-use/awesome-prompts"&gt;&lt;code&gt;awesome-prompts&lt;/code&gt;&lt;/a&gt; repo for prompting inspiration.&lt;/p&gt; 
&lt;h1&gt;Vision&lt;/h1&gt; 
&lt;p&gt;Tell your computer what to do, and it gets it done.&lt;/p&gt; 
&lt;h2&gt;Roadmap&lt;/h2&gt; 
&lt;h3&gt;Agent&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Improve agent memory to handle +100 steps&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Enhance planning capabilities (load website specific context)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Reduce token consumption (system prompt, DOM state)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;DOM Extraction&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Enable detection for all possible UI elements&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Improve state representation for UI elements so that all LLMs can understand what's on the page&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Workflows&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Let user record a workflow - which we can rerun with browser-use as a fallback&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Make rerunning of workflows work, even if pages change&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;User Experience&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Create various templates for tutorial execution, job application, QA testing, social media, etc. which users can just copy &amp;amp; paste.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Improve docs&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Make it faster&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Parallelization&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Human work is sequential. The real power of a browser agent comes into reality if we can parallelize similar tasks. For example, if you want to find contact information for 100 companies, this can all be done in parallel and reported back to a main agent, which processes the results and kicks off parallel subtasks again.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We love contributions! Feel free to open issues for bugs or feature requests. To contribute to the docs, check out the &lt;code&gt;/docs&lt;/code&gt; folder.&lt;/p&gt; 
&lt;h2&gt;🧪 How to make your agents robust?&lt;/h2&gt; 
&lt;p&gt;We offer to run your tasks in our CI—automatically, on every update!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Add your task:&lt;/strong&gt; Add a YAML file in &lt;code&gt;tests/agent_tasks/&lt;/code&gt; (see the &lt;a href="https://raw.githubusercontent.com/browser-use/browser-use/main/tests/agent_tasks/README.md"&gt;&lt;code&gt;README there&lt;/code&gt;&lt;/a&gt; for details).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Automatic validation:&lt;/strong&gt; Every time we push updates, your task will be run by the agent and evaluated using your criteria.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Local Setup&lt;/h2&gt; 
&lt;p&gt;To learn more about the library, check out the &lt;a href="https://docs.browser-use.com/development/local-setup"&gt;local setup 📕&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;main&lt;/code&gt; is the primary development branch with frequent changes. For production use, install a stable &lt;a href="https://github.com/browser-use/browser-use/releases"&gt;versioned release&lt;/a&gt; instead.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Swag&lt;/h2&gt; 
&lt;p&gt;Want to show off your Browser-use swag? Check out our &lt;a href="https://browsermerch.com"&gt;Merch store&lt;/a&gt;. Good contributors will receive swag for free 👀.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you use Browser Use in your research or project, please cite:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@software{browser_use2024,
  author = {Müller, Magnus and Žunič, Gregor},
  title = {Browser Use: Enable AI to control your browser},
  year = {2024},
  publisher = {GitHub},
  url = {https://github.com/browser-use/browser-use}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://github.com/user-attachments/assets/06fa3078-8461-4560-b434-445510c1766f" width="400" /&gt; 
 &lt;p&gt;&lt;a href="https://x.com/intent/user?screen_name=gregpr07"&gt;&lt;img src="https://img.shields.io/twitter/follow/Gregor?style=social" alt="Twitter Follow" /&gt;&lt;/a&gt; &lt;a href="https://x.com/intent/user?screen_name=mamagnus00"&gt;&lt;img src="https://img.shields.io/twitter/follow/Magnus?style=social" alt="Twitter Follow" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt;
  Made with ❤️ in Zurich and San Francisco 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>PacktPublishing/LLM-Engineers-Handbook</title>
      <link>https://github.com/PacktPublishing/LLM-Engineers-Handbook</link>
      <description>&lt;p&gt;The LLM's practical guide: From the fundamentals to deploying advanced LLM and RAG apps to AWS using LLMOps best practices&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h1&gt;👷 LLM Engineer's Handbook&lt;/h1&gt; 
 &lt;p class="tagline"&gt;Official repository of the &lt;a href="https://www.amazon.com/LLM-Engineers-Handbook-engineering-production/dp/1836200072/"&gt;LLM Engineer's Handbook&lt;/a&gt; by &lt;a href="https://github.com/iusztinpaul"&gt;Paul Iusztin&lt;/a&gt; and &lt;a href="https://github.com/mlabonne"&gt;Maxime Labonne&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;p align="center"&gt; &lt;a href="https://www.amazon.com/LLM-Engineers-Handbook-engineering-production/dp/1836200072/"&gt; &lt;img src="https://raw.githubusercontent.com/PacktPublishing/LLM-Engineers-Handbook/main/images/cover_plus.png" alt="Book cover" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; Find the book on &lt;a href="https://www.amazon.com/LLM-Engineers-Handbook-engineering-production/dp/1836200072/"&gt;Amazon&lt;/a&gt; or &lt;a href="https://www.packtpub.com/en-us/product/llm-engineers-handbook-9781836200062"&gt;Packt&lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;🌟 Features&lt;/h2&gt; 
&lt;p&gt;The goal of this book is to create your own end-to-end LLM-based system using best practices:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;📝 Data collection &amp;amp; generation&lt;/li&gt; 
 &lt;li&gt;🔄 LLM training pipeline&lt;/li&gt; 
 &lt;li&gt;📊 Simple RAG system&lt;/li&gt; 
 &lt;li&gt;🚀 Production-ready AWS deployment&lt;/li&gt; 
 &lt;li&gt;🔍 Comprehensive monitoring&lt;/li&gt; 
 &lt;li&gt;🧪 Testing and evaluation framework&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can download and use the final trained model on &lt;a href="https://huggingface.co/mlabonne/TwinLlama-3.1-8B-DPO"&gt;Hugging Face&lt;/a&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] The code in this GitHub repository is actively maintained and may contain updates not reflected in the book. &lt;strong&gt;Always refer to this repository for the latest version of the code.&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;🔗 Dependencies&lt;/h2&gt; 
&lt;h3&gt;Local dependencies&lt;/h3&gt; 
&lt;p&gt;To install and run the project locally, you need the following dependencies.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Tool&lt;/th&gt; 
   &lt;th&gt;Version&lt;/th&gt; 
   &lt;th&gt;Purpose&lt;/th&gt; 
   &lt;th&gt;Installation Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;pyenv&lt;/td&gt; 
   &lt;td&gt;≥2.3.36&lt;/td&gt; 
   &lt;td&gt;Multiple Python versions (optional)&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/pyenv/pyenv?tab=readme-ov-file#installation"&gt;Install Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Python&lt;/td&gt; 
   &lt;td&gt;3.11&lt;/td&gt; 
   &lt;td&gt;Runtime environment&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.python.org/downloads/"&gt;Download&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Poetry&lt;/td&gt; 
   &lt;td&gt;&amp;gt;= 1.8.3 and &amp;lt; 2.0&lt;/td&gt; 
   &lt;td&gt;Package management&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://python-poetry.org/docs/#installation"&gt;Install Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Docker&lt;/td&gt; 
   &lt;td&gt;≥27.1.1&lt;/td&gt; 
   &lt;td&gt;Containerization&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.docker.com/engine/install/"&gt;Install Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;AWS CLI&lt;/td&gt; 
   &lt;td&gt;≥2.15.42&lt;/td&gt; 
   &lt;td&gt;Cloud management&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html"&gt;Install Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Git&lt;/td&gt; 
   &lt;td&gt;≥2.44.0&lt;/td&gt; 
   &lt;td&gt;Version control&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://git-scm.com/downloads"&gt;Download&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Cloud services&lt;/h3&gt; 
&lt;p&gt;The code also uses and depends on the following cloud services. For now, you don't have to do anything. We will guide you in the installation and deployment sections on how to use them:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Service&lt;/th&gt; 
   &lt;th&gt;Purpose&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.com/"&gt;HuggingFace&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Model registry&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.comet.com/site/products/opik/?utm_source=llm_handbook&amp;amp;utm_medium=github&amp;amp;utm_campaign=opik"&gt;Comet ML&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Experiment tracker&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.comet.com/site/products/opik/?utm_source=llm_handbook&amp;amp;utm_medium=github&amp;amp;utm_campaign=opik"&gt;Opik&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Prompt monitoring&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.zenml.io/"&gt;ZenML&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Orchestrator and artifacts layer&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://aws.amazon.com/"&gt;AWS&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Compute and storage&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.mongodb.com/"&gt;MongoDB&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;NoSQL database&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://qdrant.tech/"&gt;Qdrant&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Vector database&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/features/actions"&gt;GitHub Actions&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;CI/CD pipeline&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;In the &lt;a href="https://www.amazon.com/LLM-Engineers-Handbook-engineering-production/dp/1836200072/"&gt;LLM Engineer's Handbook&lt;/a&gt;, Chapter 2 will walk you through each tool. Chapters 10 and 11 provide step-by-step guides on how to set up everything you need.&lt;/p&gt; 
&lt;h2&gt;🗂️ Project Structure&lt;/h2&gt; 
&lt;p&gt;Here is the directory overview:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;.
├── code_snippets/       # Standalone example code
├── configs/             # Pipeline configuration files
├── llm_engineering/     # Core project package
│   ├── application/    
│   ├── domain/         
│   ├── infrastructure/ 
│   ├── model/         
├── pipelines/           # ML pipeline definitions
├── steps/               # Pipeline components
├── tests/               # Test examples
├── tools/               # Utility scripts
│   ├── run.py
│   ├── ml_service.py
│   ├── rag.py
│   ├── data_warehouse.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;code&gt;llm_engineering/&lt;/code&gt; is the main Python package implementing LLM and RAG functionality. It follows Domain-Driven Design (DDD) principles:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;domain/&lt;/code&gt;: Core business entities and structures&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;application/&lt;/code&gt;: Business logic, crawlers, and RAG implementation&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;model/&lt;/code&gt;: LLM training and inference&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;infrastructure/&lt;/code&gt;: External service integrations (AWS, Qdrant, MongoDB, FastAPI)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The code logic and imports flow as follows: &lt;code&gt;infrastructure&lt;/code&gt; → &lt;code&gt;model&lt;/code&gt; → &lt;code&gt;application&lt;/code&gt; → &lt;code&gt;domain&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pipelines/&lt;/code&gt;: Contains the ZenML ML pipelines, which serve as the entry point for all the ML pipelines. Coordinates the data processing and model training stages of the ML lifecycle.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;steps/&lt;/code&gt;: Contains individual ZenML steps, which are reusable components for building and customizing ZenML pipelines. Steps perform specific tasks (e.g., data loading, preprocessing) and can be combined within the ML pipelines.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;tests/&lt;/code&gt;: Covers a few sample tests used as examples within the CI pipeline.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;tools/&lt;/code&gt;: Utility scripts used to call the ZenML pipelines and inference code:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;run.py&lt;/code&gt;: Entry point script to run ZenML pipelines.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ml_service.py&lt;/code&gt;: Starts the REST API inference server.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;rag.py&lt;/code&gt;: Demonstrates usage of the RAG retrieval module.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;data_warehouse.py&lt;/code&gt;: Used to export or import data from the MongoDB data warehouse through JSON files.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;code&gt;configs/&lt;/code&gt;: ZenML YAML configuration files to control the execution of pipelines and steps.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;code_snippets/&lt;/code&gt;: Independent code examples that can be executed independently.&lt;/p&gt; 
&lt;h2&gt;💻 Installation&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] If you are experiencing issues while installing and running the repository, consider checking the &lt;a href="https://github.com/PacktPublishing/LLM-Engineers-Handbook/issues"&gt;Issues&lt;/a&gt; GitHub section for other people who solved similar problems or directly asking us for help.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;1. Clone the Repository&lt;/h3&gt; 
&lt;p&gt;Start by cloning the repository and navigating to the project directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/PacktPublishing/LLM-Engineers-Handbook.git
cd LLM-Engineers-Handbook 
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Next, we have to prepare your Python environment and its adjacent dependencies.&lt;/p&gt; 
&lt;h3&gt;2. Set Up Python Environment&lt;/h3&gt; 
&lt;p&gt;The project requires Python 3.11. You can either use your global Python installation or set up a project-specific version using pyenv.&lt;/p&gt; 
&lt;h4&gt;Option A: Using Global Python (if version 3.11 is installed)&lt;/h4&gt; 
&lt;p&gt;Verify your Python version:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python --version  # Should show Python 3.11.x
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Option B: Using pyenv (recommended)&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Verify pyenv installation:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pyenv --version   # Should show pyenv 2.3.36 or later
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Install Python 3.11.8:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pyenv install 3.11.8
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Verify the installation:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python --version  # Should show Python 3.11.8
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="4"&gt; 
 &lt;li&gt;Confirm Python version in the project directory:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python --version
# Output: Python 3.11.8
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;br /&gt; The project includes a &lt;code&gt;.python-version&lt;/code&gt; file that automatically sets the correct Python version when you're in the project directory.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;3. Install Dependencies&lt;/h3&gt; 
&lt;p&gt;The project uses Poetry for dependency management.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Verify Poetry installation:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry --version  # Should show Poetry version 1.8.3 or later
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Set up the project environment and install dependencies:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry env use 3.11
poetry install --without aws
poetry run pre-commit install
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Configure Poetry to use Python 3.11&lt;/li&gt; 
 &lt;li&gt;Install project dependencies (excluding AWS-specific packages)&lt;/li&gt; 
 &lt;li&gt;Set up pre-commit hooks for code verification&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;4. Activate the Environment&lt;/h3&gt; 
&lt;p&gt;As our task manager, we run all the scripts using &lt;a href="https://poethepoet.natn.io/index.html"&gt;Poe the Poet&lt;/a&gt;.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Start a Poetry shell:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry shell
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Run project commands using Poe the Poet:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry poe ...
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;🔧 Troubleshooting Poe the Poet Installation&lt;/summary&gt; 
 &lt;h3&gt;Alternative Command Execution&lt;/h3&gt; 
 &lt;p&gt;If you're experiencing issues with &lt;code&gt;poethepoet&lt;/code&gt;, you can still run the project commands directly through Poetry. Here's how:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Look up the command definition in &lt;code&gt;pyproject.toml&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;Use &lt;code&gt;poetry run&lt;/code&gt; with the underlying command&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;h4&gt;Example:&lt;/h4&gt; 
 &lt;p&gt;Instead of:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;poetry poe local-infrastructure-up
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Use the direct command from pyproject.toml:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;poetry run &amp;lt;actual-command-from-pyproject-toml&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Note: All project commands are defined in the [tool.poe.tasks] section of pyproject.toml&lt;/p&gt; 
&lt;/details&gt; 
&lt;p&gt;Now, let's configure our local project with all the necessary credentials and tokens to run the code locally.&lt;/p&gt; 
&lt;h3&gt;5. Local Development Setup&lt;/h3&gt; 
&lt;p&gt;After you have installed all the dependencies, you must create and fill a&amp;nbsp;&lt;code&gt;.env&lt;/code&gt; file with your credentials to appropriately interact with other services and run the project. Setting your sensitive credentials in a &lt;code&gt;.env&lt;/code&gt; file is a good security practice, as this file won't be committed to GitHub or shared with anyone else.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;First, copy our example by running the following:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cp .env.example .env # The file must be at your repository's root!
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Now, let's understand how to fill in all the essential variables within the &lt;code&gt;.env&lt;/code&gt; file to get you started. The following are the mandatory settings we must complete when working locally:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;OpenAI&lt;/h4&gt; 
&lt;p&gt;To authenticate to OpenAI's API, you must fill out the &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; env var with an authentication token.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-env"&gt;OPENAI_API_KEY=your_api_key_here
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;→ Check out this &lt;a href="https://platform.openai.com/docs/quickstart"&gt;tutorial&lt;/a&gt; to learn how to provide one from OpenAI.&lt;/p&gt; 
&lt;h4&gt;Hugging Face&lt;/h4&gt; 
&lt;p&gt;To authenticate to Hugging Face, you must fill out the &lt;code&gt;HUGGINGFACE_ACCESS_TOKEN&lt;/code&gt; env var with an authentication token.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-env"&gt;HUGGINGFACE_ACCESS_TOKEN=your_token_here
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;→ Check out this &lt;a href="https://huggingface.co/docs/hub/en/security-tokens"&gt;tutorial&lt;/a&gt; to learn how to provide one from Hugging Face.&lt;/p&gt; 
&lt;h4&gt;Comet ML &amp;amp; Opik&lt;/h4&gt; 
&lt;p&gt;To authenticate to Comet ML (required only during training) and Opik, you must fill out the &lt;code&gt;COMET_API_KEY&lt;/code&gt; env var with your authentication token.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-env"&gt;COMET_API_KEY=your_api_key_here
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;→ Check out this &lt;a href="https://www.comet.com/docs/opik/?utm_source=llm_handbook&amp;amp;utm_medium=github&amp;amp;utm_campaign=opik"&gt;tutorial&lt;/a&gt; to learn how to get started with Opik. You can also access Opik's dashboard using 🔗&lt;a href="https://www.comet.com/opik?utm_source=llm_handbook&amp;amp;utm_medium=github&amp;amp;utm_content=opik"&gt;this link&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;6. Deployment Setup&lt;/h3&gt; 
&lt;p&gt;When deploying the project to the cloud, we must set additional settings for Mongo, Qdrant, and AWS. If you are just working locally, the default values of these env vars will work out of the box. Detailed deployment instructions are available in Chapter 11 of the &lt;a href="https://www.amazon.com/LLM-Engineers-Handbook-engineering-production/dp/1836200072/"&gt;LLM Engineer's Handbook&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;MongoDB&lt;/h4&gt; 
&lt;p&gt;We must change the &lt;code&gt;DATABASE_HOST&lt;/code&gt; env var with the URL pointing to your cloud MongoDB cluster.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-env"&gt;DATABASE_HOST=your_mongodb_url
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;→ Check out this &lt;a href="https://www.mongodb.com/resources/products/fundamentals/mongodb-cluster-setup"&gt;tutorial&lt;/a&gt; to learn how to create and host a MongoDB cluster for free.&lt;/p&gt; 
&lt;h4&gt;Qdrant&lt;/h4&gt; 
&lt;p&gt;Change &lt;code&gt;USE_QDRANT_CLOUD&lt;/code&gt; to &lt;code&gt;true&lt;/code&gt;, &lt;code&gt;QDRANT_CLOUD_URL&lt;/code&gt; with the URL point to your cloud Qdrant cluster, and &lt;code&gt;QDRANT_APIKEY&lt;/code&gt; with its API key.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-env"&gt;USE_QDRANT_CLOUD=true
QDRANT_CLOUD_URL=your_qdrant_cloud_url
QDRANT_APIKEY=your_qdrant_api_key
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;→ Check out this &lt;a href="https://qdrant.tech/documentation/cloud/create-cluster/"&gt;tutorial&lt;/a&gt; to learn how to create a Qdrant cluster for free&lt;/p&gt; 
&lt;h4&gt;AWS&lt;/h4&gt; 
&lt;p&gt;For your AWS set-up to work correctly, you need the AWS CLI installed on your local machine and properly configured with an admin user (or a user with enough permissions to create new SageMaker, ECR, and S3 resources; using an admin user will make everything more straightforward).&lt;/p&gt; 
&lt;p&gt;Chapter 2 provides step-by-step instructions on how to install the AWS CLI, create an admin user on AWS, and get an access key to set up the &lt;code&gt;AWS_ACCESS_KEY&lt;/code&gt; and &lt;code&gt;AWS_SECRET_KEY&lt;/code&gt; environment variables. If you already have an AWS admin user in place, you have to configure the following env vars in your &lt;code&gt;.env&lt;/code&gt; file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;AWS_REGION=eu-central-1 # Change it with your AWS region.
AWS_ACCESS_KEY=your_aws_access_key
AWS_SECRET_KEY=your_aws_secret_key
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;AWS credentials are typically stored in &lt;code&gt;~/.aws/credentials&lt;/code&gt;. You can view this file directly using &lt;code&gt;cat&lt;/code&gt; or similar commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cat ~/.aws/credentials
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] Additional configuration options are available in &lt;a href="https://github.com/PacktPublishing/LLM-Engineers-Handbook/raw/main/llm_engineering/settings.py"&gt;settings.py&lt;/a&gt;. Any variable in the &lt;code&gt;Settings&lt;/code&gt; class can be configured through the &lt;code&gt;.env&lt;/code&gt; file.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;🏗️ Infrastructure&lt;/h2&gt; 
&lt;h3&gt;Local infrastructure (for testing and development)&lt;/h3&gt; 
&lt;p&gt;When running the project locally, we host a MongoDB and Qdrant database using Docker. Also, a testing ZenML server is made available through their Python package.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING] You need Docker installed (&amp;gt;= v27.1.1)&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;For ease of use, you can start the whole local development infrastructure with the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry poe local-infrastructure-up
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Also, you can stop the ZenML server and all the Docker containers using the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry poe local-infrastructure-down
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING]&lt;br /&gt; When running on MacOS, before starting the server, export the following environment variable: &lt;code&gt;export OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES&lt;/code&gt; Otherwise, the connection between the local server and pipeline will break. 🔗 More details in &lt;a href="https://github.com/zenml-io/zenml/issues/2369"&gt;this issue&lt;/a&gt;. This is done by default when using Poe the Poet.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Start the inference real-time RESTful API:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry poe run-inference-ml-service
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] The LLM microservice, called by the RESTful API, will work only after deploying the LLM to AWS SageMaker.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;ZenML&lt;/h4&gt; 
&lt;p&gt;Dashboard URL: &lt;code&gt;localhost:8237&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Default credentials:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;username&lt;/code&gt;: default&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;password&lt;/code&gt;:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;→ Find out more about using and setting up &lt;a href="https://docs.zenml.io/"&gt;ZenML&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;Qdrant&lt;/h4&gt; 
&lt;p&gt;REST API URL: &lt;code&gt;localhost:6333&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Dashboard URL: &lt;code&gt;localhost:6333/dashboard&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;→ Find out more about using and setting up &lt;a href="https://qdrant.tech/documentation/quick-start/"&gt;Qdrant with Docker&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;MongoDB&lt;/h4&gt; 
&lt;p&gt;Database URI: &lt;code&gt;mongodb://llm_engineering:llm_engineering@127.0.0.1:27017&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Database name: &lt;code&gt;twin&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Default credentials:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;username&lt;/code&gt;: llm_engineering&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;password&lt;/code&gt;: llm_engineering&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;→ Find out more about using and setting up &lt;a href="https://www.mongodb.com/docs/manual/tutorial/install-mongodb-community-with-docker"&gt;MongoDB with Docker&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You can search your MongoDB collections using your &lt;strong&gt;IDEs MongoDB plugin&lt;/strong&gt; (which you have to install separately), where you have to use the database URI to connect to the MongoDB database hosted within the Docker container: &lt;code&gt;mongodb://llm_engineering:llm_engineering@127.0.0.1:27017&lt;/code&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] Everything related to training or running the LLMs (e.g., training, evaluation, inference) can only be run if you set up AWS SageMaker, as explained in the next section on cloud infrastructure.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Cloud infrastructure (for production)&lt;/h3&gt; 
&lt;p&gt;Here we will quickly present how to deploy the project to AWS and other serverless services. We won't go into the details (as everything is presented in the book) but only point out the main steps you have to go through.&lt;/p&gt; 
&lt;p&gt;First, reinstall your Python dependencies with the AWS group:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry install --with aws
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;AWS SageMaker&lt;/h4&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Chapter 10 provides step-by-step instructions in the section "Implementing the LLM microservice using AWS SageMaker".&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;By this point, we expect you to have AWS CLI installed and your AWS CLI and project's env vars (within the &lt;code&gt;.env&lt;/code&gt; file) properly configured with an AWS admin user.&lt;/p&gt; 
&lt;p&gt;To ensure best practices, we must create a new AWS user restricted to creating and deleting only resources related to AWS SageMaker. Create it by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry poe create-sagemaker-role
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It will create a &lt;code&gt;sagemaker_user_credentials.json&lt;/code&gt; file at the root of your repository with your new &lt;code&gt;AWS_ACCESS_KEY&lt;/code&gt; and &lt;code&gt;AWS_SECRET_KEY&lt;/code&gt; values. &lt;strong&gt;But before replacing your new AWS credentials, also run the following command to create the execution role (to create it using your admin credentials).&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;To create the IAM execution role used by AWS SageMaker to access other AWS resources on our behalf, run the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry poe create-sagemaker-execution-role
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It will create a &lt;code&gt;sagemaker_execution_role.json&lt;/code&gt; file at the root of your repository with your new &lt;code&gt;AWS_ARN_ROLE&lt;/code&gt; value. Add it to your &lt;code&gt;.env&lt;/code&gt; file.&lt;/p&gt; 
&lt;p&gt;Once you've updated the &lt;code&gt;AWS_ACCESS_KEY&lt;/code&gt;, &lt;code&gt;AWS_SECRET_KEY&lt;/code&gt;, and &lt;code&gt;AWS_ARN_ROLE&lt;/code&gt; values in your &lt;code&gt;.env&lt;/code&gt; file, you can use AWS SageMaker. &lt;strong&gt;Note that this step is crucial to complete the AWS setup.&lt;/strong&gt;&lt;/p&gt; 
&lt;h4&gt;Training&lt;/h4&gt; 
&lt;p&gt;We start the training pipeline through ZenML by running the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry poe run-training-pipeline
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will start the training code using the configs from &lt;code&gt;configs/training.yaml&lt;/code&gt; directly in SageMaker. You can visualize the results in Comet ML's dashboard.&lt;/p&gt; 
&lt;p&gt;We start the evaluation pipeline through ZenML by running the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry poe run-evaluation-pipeline
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will start the evaluation code using the configs from &lt;code&gt;configs/evaluating.yaml&lt;/code&gt; directly in SageMaker. You can visualize the results in &lt;code&gt;*-results&lt;/code&gt; datasets saved to your Hugging Face profile.&lt;/p&gt; 
&lt;h4&gt;Inference&lt;/h4&gt; 
&lt;p&gt;To create an AWS SageMaker Inference Endpoint, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry poe deploy-inference-endpoint
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To test it out, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry poe test-sagemaker-endpoint
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To delete it, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry poe delete-inference-endpoint
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;AWS: ML pipelines, artifacts, and containers&lt;/h4&gt; 
&lt;p&gt;The ML pipelines, artifacts, and containers are deployed to AWS by leveraging ZenML's deployment features. Thus, you must create an account with ZenML Cloud and follow their guide on deploying a ZenML stack to AWS. Otherwise, we provide step-by-step instructions in &lt;strong&gt;Chapter 11&lt;/strong&gt;, section &lt;strong&gt;Deploying the LLM Twin's pipelines to the cloud&lt;/strong&gt; on what you must do.&lt;/p&gt; 
&lt;h4&gt;Qdrant &amp;amp; MongoDB&lt;/h4&gt; 
&lt;p&gt;We leverage Qdrant's and MongoDB's serverless options when deploying the project. Thus, you can either follow &lt;a href="https://qdrant.tech/documentation/cloud/create-cluster/"&gt;Qdrant's&lt;/a&gt; and &lt;a href="https://www.mongodb.com/resources/products/fundamentals/mongodb-cluster-setup"&gt;MongoDB's&lt;/a&gt; tutorials on how to create a freemium cluster for each or go through &lt;strong&gt;Chapter 11&lt;/strong&gt;, section &lt;strong&gt;Deploying the LLM Twin's pipelines to the cloud&lt;/strong&gt; and follow our step-by-step instructions.&lt;/p&gt; 
&lt;h4&gt;GitHub Actions&lt;/h4&gt; 
&lt;p&gt;We use GitHub Actions to implement our CI/CD pipelines. To implement your own, you have to fork our repository and set the following env vars as Actions secrets in your forked repository:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;AWS_ACCESS_KEY_ID&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;AWS_SECRET_ACCESS_KEY&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;AWS_ECR_NAME&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;AWS_REGION&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Also, we provide instructions on how to set everything up in &lt;strong&gt;Chapter 11&lt;/strong&gt;, section &lt;strong&gt;Adding LLMOps to the LLM Twin&lt;/strong&gt;.&lt;/p&gt; 
&lt;h4&gt;Comet ML &amp;amp; Opik&lt;/h4&gt; 
&lt;p&gt;You can visualize the results on their self-hosted dashboards if you create a Comet account and correctly set the &lt;code&gt;COMET_API_KEY&lt;/code&gt; env var. As Opik is powered by Comet, you don't have to set up anything else along Comet:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.comet.com/?utm_source=llm_handbook&amp;amp;utm_medium=github&amp;amp;utm_campaign=opik"&gt;Comet ML (for experiment tracking)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.comet.com/opik?utm_source=llm_handbook&amp;amp;utm_medium=github&amp;amp;utm_campaign=opik"&gt;Opik (for prompt monitoring)&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;💰 Running the Project Costs&lt;/h3&gt; 
&lt;p&gt;We will mostly stick to free tiers for all the services except for AWS and OpenAI's API, which are both pay-as-you-go services. The cost of running the project once, with our default values, will be roughly ~$25 (most of it comes from using AWS SageMaker for training and inference).&lt;/p&gt; 
&lt;h2&gt;⚡ Pipelines&lt;/h2&gt; 
&lt;p&gt;All the ML pipelines will be orchestrated behind the scenes by &lt;a href="https://www.zenml.io/"&gt;ZenML&lt;/a&gt;. A few exceptions exist when running utility scrips, such as exporting or importing from the data warehouse.&lt;/p&gt; 
&lt;p&gt;The ZenML pipelines are the entry point for most processes throughout this project. They are under the &lt;code&gt;pipelines/&lt;/code&gt; folder. Thus, when you want to understand or debug a workflow, starting with the ZenML pipeline is the best approach.&lt;/p&gt; 
&lt;p&gt;To see the pipelines running and their results:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;go to your ZenML dashboard&lt;/li&gt; 
 &lt;li&gt;go to the &lt;code&gt;Pipelines&lt;/code&gt; section&lt;/li&gt; 
 &lt;li&gt;click on a specific pipeline (e.g., &lt;code&gt;feature_engineering&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;click on a specific run (e.g., &lt;code&gt;feature_engineering_run_2024_06_20_18_40_24&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;click on a specific step or artifact of the DAG to find more details about it&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Now, let's explore all the pipelines you can run. From data collection to training, we will present them in their natural order to go through the LLM project end-to-end.&lt;/p&gt; 
&lt;h3&gt;Data pipelines&lt;/h3&gt; 
&lt;p&gt;Run the data collection ETL:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry poe run-digital-data-etl
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING] You must have Chrome (or another Chromium-based browser) installed on your system for LinkedIn and Medium crawlers to work (which use Selenium under the hood). Based on your Chrome version, the Chromedriver will be automatically installed to enable Selenium support. Another option is to run everything using our Docker image if you don't want to install Chrome. For example, to run all the pipelines combined you can run &lt;code&gt;poetry poe run-docker-end-to-end-data-pipeline&lt;/code&gt;. Note that the command can be tweaked to support any other pipeline.&lt;/p&gt; 
 &lt;p&gt;If, for any other reason, you don't have a Chromium-based browser installed and don't want to use Docker, you have two other options to bypass this Selenium issue:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Comment out all the code related to Selenium, Chrome and all the links that use Selenium to crawl them (e.g., Medium), such as the &lt;code&gt;chromedriver_autoinstaller.install()&lt;/code&gt; command from &lt;a href="https://github.com/PacktPublishing/LLM-Engineers-Handbook/raw/main/llm_engineering/application/crawlers/base.py"&gt;application.crawlers.base&lt;/a&gt; and other static calls that check for Chrome drivers and Selenium.&lt;/li&gt; 
  &lt;li&gt;Install Google Chrome using your CLI in environments such as GitHub Codespaces or other cloud VMs using the same command as in our &lt;a href="https://github.com/PacktPublishing/LLM-Engineers-Handbook/raw/main/Dockerfile#L10"&gt;Docker file&lt;/a&gt;.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;To add additional links to collect from, go to &lt;code&gt;configs/digital_data_etl_[author_name].yaml&lt;/code&gt; and add them to the &lt;code&gt;links&lt;/code&gt; field. Also, you can create a completely new file and specify it at run time, like this: &lt;code&gt;python -m llm_engineering.interfaces.orchestrator.run --run-etl --etl-config-filename configs/digital_data_etl_[your_name].yaml&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Run the feature engineering pipeline:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry poe run-feature-engineering-pipeline
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Generate the instruct dataset:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry poe run-generate-instruct-datasets-pipeline
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Generate the preference dataset:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry poe run-generate-preference-datasets-pipeline
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run all of the above compressed into a single pipeline:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry poe run-end-to-end-data-pipeline
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Utility pipelines&lt;/h3&gt; 
&lt;p&gt;Export the data from the data warehouse to JSON files:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry poe run-export-data-warehouse-to-json
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Import data to the data warehouse from JSON files (by default, it imports the data from the &lt;code&gt;data/data_warehouse_raw_data&lt;/code&gt; directory):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry poe run-import-data-warehouse-from-json
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Export ZenML artifacts to JSON:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry poe run-export-artifact-to-json-pipeline
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will export the following ZenML artifacts to the &lt;code&gt;output&lt;/code&gt; folder as JSON files (it will take their latest version):&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;cleaned_documents.json&lt;/li&gt; 
 &lt;li&gt;instruct_datasets.json&lt;/li&gt; 
 &lt;li&gt;preference_datasets.json&lt;/li&gt; 
 &lt;li&gt;raw_documents.json&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can configure what artifacts to export by tweaking the &lt;code&gt;configs/export_artifact_to_json.yaml&lt;/code&gt; configuration file.&lt;/p&gt; 
&lt;h3&gt;Training pipelines&lt;/h3&gt; 
&lt;p&gt;Run the training pipeline:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry poe run-training-pipeline
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run the evaluation pipeline:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry poe run-evaluation-pipeline
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING] For this to work, make sure you properly configured AWS SageMaker as described in &lt;a href="https://raw.githubusercontent.com/PacktPublishing/LLM-Engineers-Handbook/main/#set-up-cloud-infrastructure-for-production"&gt;Set up cloud infrastructure (for production)&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Inference pipelines&lt;/h3&gt; 
&lt;p&gt;Call the RAG retrieval module with a test query:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry poe call-rag-retrieval-module
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Start the inference real-time RESTful API:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry poe run-inference-ml-service
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Call the inference real-time RESTful API with a test query:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry poe call-inference-ml-service
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Remember that you can monitor the prompt traces on &lt;a href="https://www.comet.com/opik"&gt;Opik&lt;/a&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING] For the inference service to work, you must have the LLM microservice deployed to AWS SageMaker, as explained in the setup cloud infrastructure section.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Linting &amp;amp; formatting (QA)&lt;/h3&gt; 
&lt;p&gt;Check or fix your linting issues:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry poe lint-check
poetry poe lint-fix
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Check or fix your formatting issues:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry poe format-check
poetry poe format-fix
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Check the code for leaked credentials:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry poe gitleaks-check
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Tests&lt;/h3&gt; 
&lt;p&gt;Run all the tests using the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry poe test
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;🏃 Run project&lt;/h2&gt; 
&lt;p&gt;Based on the setup and usage steps described above, assuming the local and cloud infrastructure works and the &lt;code&gt;.env&lt;/code&gt; is filled as expected, follow the next steps to run the LLM system end-to-end:&lt;/p&gt; 
&lt;h3&gt;Data&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Collect data: &lt;code&gt;poetry poe run-digital-data-etl&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Compute features: &lt;code&gt;poetry poe run-feature-engineering-pipeline&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Compute instruct dataset: &lt;code&gt;poetry poe run-generate-instruct-datasets-pipeline&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Compute preference alignment dataset: &lt;code&gt;poetry poe run-generate-preference-datasets-pipeline&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Training&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] From now on, for these steps to work, you need to properly set up AWS SageMaker, such as running &lt;code&gt;poetry install --with aws&lt;/code&gt; and filling in the AWS-related environment variables and configs.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ol start="5"&gt; 
 &lt;li&gt; &lt;p&gt;SFT fine-tuning Llamma 3.1: &lt;code&gt;poetry poe run-training-pipeline&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;For DPO, go to &lt;code&gt;configs/training.yaml&lt;/code&gt;, change &lt;code&gt;finetuning_type&lt;/code&gt; to &lt;code&gt;dpo&lt;/code&gt;, and run &lt;code&gt;poetry poe run-training-pipeline&lt;/code&gt; again&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Evaluate fine-tuned models: &lt;code&gt;poetry poe run-evaluation-pipeline&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Inference&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] From now on, for these steps to work, you need to properly set up AWS SageMaker, such as running &lt;code&gt;poetry install --with aws&lt;/code&gt; and filling in the AWS-related environment variables and configs.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ol start="8"&gt; 
 &lt;li&gt; &lt;p&gt;Call only the RAG retrieval module: &lt;code&gt;poetry poe call-rag-retrieval-module&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Deploy the LLM Twin microservice to SageMaker: &lt;code&gt;poetry poe deploy-inference-endpoint&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Test the LLM Twin microservice: &lt;code&gt;poetry poe test-sagemaker-endpoint&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Start end-to-end RAG server: &lt;code&gt;poetry poe run-inference-ml-service&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Test RAG server: &lt;code&gt;poetry poe call-inference-ml-service&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;📄 License&lt;/h2&gt; 
&lt;p&gt;This course is an open-source project released under the MIT license. Thus, as long you distribute our LICENSE and acknowledge our work, you can safely clone or fork this project and use it as a source of inspiration for whatever you want (e.g., university projects, college degree projects, personal projects, etc.).&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>martinvigo/email2phonenumber</title>
      <link>https://github.com/martinvigo/email2phonenumber</link>
      <description>&lt;p&gt;A OSINT tool to obtain a target's phone number just by having his email address&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;email2phonenumber&lt;/h1&gt; 
&lt;p&gt;email2phonenumber is an OSINT tool that allows you to obtain a target's phone number just by having his email address.&lt;/p&gt; 
&lt;p&gt;For full details check: &lt;a href="https://www.martinvigo.com/email2phonenumber"&gt;https://www.martinvigo.com/email2phonenumber&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Demo: &lt;a href="https://www.youtube.com/watch?v=dfvqhDUn81s"&gt;https://www.youtube.com/watch?v=dfvqhDUn81s&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;&lt;strong&gt;IMPORTANT:&lt;/strong&gt;&lt;/em&gt; *email2phonenumber is a proof-of-concept tool I wrote during my research on new OSINT methodologies to obtain a target's phone number. The supported services (Ebay, Lastpass, Amazon and Twitter) have long added protections to protect from these type of scraping like having to receive a code over email first or simply adding captchas. There are of course many other sites that are still leaking phone number digits but I am focused on other research projects. Feel free to submit pull request if you want to add support for new sites.&lt;/p&gt; 
&lt;p&gt;Please check out my newer tool "&lt;a href="https://www.martinvigo.com/tools/phonerator/"&gt;Phonerator&lt;/a&gt;", which is maintained and focuses on the novel aspect of this research, generating valid phone numbers. &lt;a href="https://www.martinvigo.com/phonerator-an-advanced-valid-phone-number-generator/"&gt;See more details&lt;/a&gt;. There is also a small OSINT challenge in there... ;)&lt;/p&gt; 
&lt;h2&gt;Basic info&lt;/h2&gt; 
&lt;p&gt;This tool helps automate discovering someone's phone number by abusing password reset design weaknesses and publicly available data. It supports 3 main functions:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;"scrape" - scrapes websites for phone number digits by initiating password reset using the target's email address&lt;/li&gt; 
 &lt;li&gt;"generate" - creates a list of valid phone numbers based on the country's Phone Numbering Plan publicly available information&lt;/li&gt; 
 &lt;li&gt;"bruteforce" - iterates over a list of phone numbers and initiates password reset on different websites to obtain associated masked emails and correlate it to the victim's one&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Setup&lt;/h2&gt; 
&lt;p&gt;email2phonenumber was developed on Python 3.x&lt;/p&gt; 
&lt;p&gt;You will need couple 3rd party libraries: BeautifulSoup and requests. These can be easily installed with pip&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip3 install beautifulsoup4 requests
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;p&gt;Scrape websites for phone number digits&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python3 email2phonenumber.py scrape -e target@email.com
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Generate a dictionary of valid phone numbers based on a phone number mask&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python3 email2phonenumber.py generate -m 555XXX1234 -o /tmp/dic.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Find target's phone number by resetting passwords on websites that do not alert the target using a phone number mask and proxies to avoid captchas and other abuse protections&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python3 email2phonenumber.py bruteforce -m 555XXX1234 -e target@email.com -p /tmp/proxies.txt -q
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Demo video&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=dfvqhDUn81s"&gt;&lt;img src="https://img.youtube.com/vi/dfvqhDUn81s/0.jpg" alt="email2phonenumber demo video" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Tool presentation at BSides Las Vegas 2019&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=1zssBR85vDA"&gt;&lt;img src="https://img.youtube.com/vi/1zssBR85vDA/0.jpg" alt="Tool presentation at Bsides Las Vegas 2019" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Authors&lt;/h2&gt; 
&lt;p&gt;Martin Vigo - @martin_vigo - &lt;a href="https://www.martinvigo.com"&gt;martinvigo.com&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>microsoft/magentic-ui</title>
      <link>https://github.com/microsoft/magentic-ui</link>
      <description>&lt;p&gt;A research prototype of a human-centered web agent&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/microsoft/magentic-ui/main/docs/img/magui-readme-logo.svg?sanitize=true" alt="Magentic-UI Logo" /&gt; 
 &lt;p&gt;&lt;em&gt;Automate your web tasks while you stay in control&lt;/em&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://pypi.python.org/pypi/magentic_ui"&gt;&lt;img src="https://img.shields.io/pypi/v/magentic_ui.svg?sanitize=true" alt="image" /&gt;&lt;/a&gt; &lt;a href="https://pypi.python.org/pypi/magentic_ui"&gt;&lt;img src="https://img.shields.io/pypi/l/magentic_ui.svg?sanitize=true" alt="image" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/badge/python-3.10%20%7C%203.11%20%7C%203.12%20%7C%203.13-blue" alt="Python Versions" /&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;p&gt;Magentic-UI is a &lt;strong&gt;research prototype&lt;/strong&gt; of a human-centered interface powered by a multi-agent system that can browse and perform actions on the web, generate and execute code, and generate and analyze files.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/7975fc26-1a18-4acb-8bf9-321171eeade7"&gt;https://github.com/user-attachments/assets/7975fc26-1a18-4acb-8bf9-321171eeade7&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;🚀 Quick Start&lt;/h2&gt; 
&lt;p&gt;Here's how you can get started with Magentic-UI:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 1. Setup environment
python3 -m venv .venv
source .venv/bin/activate
pip install magentic-ui --upgrade

# 2. Set your API key
export OPENAI_API_KEY="your-api-key-here"

# 3. Launch Magentic-UI
magentic-ui --port 8081
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then open &lt;a href="http://localhost:8081"&gt;http://localhost:8081&lt;/a&gt; in your browser to interact with Magentic-UI!&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Prerequisites&lt;/strong&gt;: Requires Docker and Python 3.10+. Windows users should use WSL2. See &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/#%EF%B8%8F-installation"&gt;detailed installation&lt;/a&gt; for more info.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;✨ What's New&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;File Upload Support&lt;/strong&gt;: Upload any file through the UI for analysis or modification&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MCP Agents&lt;/strong&gt;: Extend capabilities with your favorite MCP servers&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Easier Installation&lt;/strong&gt;: We have uploaded our docker containers to GHCR so you no longer need to build any containers! Installation time now is much quicker.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Alternative Usage Options&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Without Docker&lt;/strong&gt; (limited functionality: no code execution):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;magentic-ui --run-without-docker --port 8081
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Command Line Interface&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;magentic-cli --work-dir PATH/TO/STORE/DATA
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Custom LLM Clients&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Azure
pip install magentic-ui[azure]

# Ollama (local models)
pip install magentic-ui[ollama]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can then pass a config file to the &lt;code&gt;magentic-ui&lt;/code&gt; command (&lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/#model-client-configuration"&gt; client config&lt;/a&gt;) or change the model client inside the UI settings.&lt;/p&gt; 
&lt;p&gt;For further details on installation please read the &lt;a href="#️-installation"&gt;🛠️ Installation&lt;/a&gt; section. For common installation issues and their solutions, please refer to the &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/TROUBLESHOOTING.md"&gt;troubleshooting document&lt;/a&gt;. See advanced usage instructions with the command &lt;code&gt;magentic-ui --help&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Quick Navigation:&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/#-how-it-works"&gt;🟪 How it Works&lt;/a&gt; &amp;nbsp;|&amp;nbsp; &lt;a href="#️-installation"&gt;🛠️ Installation&lt;/a&gt; &amp;nbsp;|&amp;nbsp; &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/#troubleshooting"&gt;⚠️ Troubleshooting&lt;/a&gt; &amp;nbsp;|&amp;nbsp; &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/#contributing"&gt;🤝 Contributing&lt;/a&gt; &amp;nbsp;|&amp;nbsp; &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/#license"&gt;📄 License&lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🟪 How it Works&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/magentic-ui/main/docs/img/magenticui_running.png" alt="Magentic-UI" height="400" /&gt; &lt;/p&gt; 
&lt;p&gt;Magentic-UI is especially useful for web tasks that require actions on the web (e.g., filling a form, customizing a food order), deep navigation through websites not indexed by search engines (e.g., filtering flights, finding a link from a personal site) or tasks that need web navigation and code execution (e.g., generate a chart from online data).&lt;/p&gt; 
&lt;p&gt;The interface of Magentic-UI is displayed in the screenshot above and consists of two panels. The left side panel is the sessions navigator where users can create new sessions to solve new tasks, switch between sessions and check on session progress with the session status indicators (🔴 needs input, ✅ task done, ↺ task in progress).&lt;/p&gt; 
&lt;p&gt;The right-side panel displays the session selected. This is where you can type your query to Magentic-UI alongside any file attachments and observe detailed task progress as well as interact with the agents. The session display itself is split in two panels: the left side is where Magentic-UI presents the plan, task progress and asks for action approvals, the right side is a browser view where you can see web agent actions in real time and interact with the browser. Finally, at the top of the session display is a progress bar that updates as Magentic-UI makes progress.&lt;/p&gt; 
&lt;p&gt;The example below shows a step by step user interaction with Magentic-UI:&lt;/p&gt; 
&lt;!-- Screenshots --&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/magentic-ui/main/docs/img/magui-landing.png" alt="Magentic-UI Landing" width="45%" style="margin:10px;" /&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/magentic-ui/main/docs/img/magui-coplanning.png" alt="Co-Planning UI" width="45%" style="margin:10px;" /&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/magentic-ui/main/docs/img/magui-cotasking.png" alt="Co-Tasking UI" width="45%" style="margin:10px;" /&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/magentic-ui/main/docs/img/magui-actionguard.png" alt="Action Guard UI" width="45%" style="margin:10px;" /&gt; &lt;/p&gt; 
&lt;p&gt;What differentiates Magentic-UI from other browser use offerings is its transparent and controllable interface that allows for efficient human-in-the-loop involvement. Magentic-UI is built using &lt;a href="https://github.com/microsoft/autogen"&gt;AutoGen&lt;/a&gt; and provides a platform to study human-agent interaction and experiment with web agents. Key features include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;🧑‍🤝‍🧑 &lt;strong&gt;Co-Planning&lt;/strong&gt;: Collaboratively create and approve step-by-step plans using chat and the plan editor.&lt;/li&gt; 
 &lt;li&gt;🤝 &lt;strong&gt;Co-Tasking&lt;/strong&gt;: Interrupt and guide the task execution using the web browser directly or through chat. Magentic-UI can also ask for clarifications and help when needed.&lt;/li&gt; 
 &lt;li&gt;🛡️ &lt;strong&gt;Action Guards&lt;/strong&gt;: Sensitive actions are only executed with explicit user approvals.&lt;/li&gt; 
 &lt;li&gt;🧠 &lt;strong&gt;Plan Learning and Retrieval&lt;/strong&gt;: Learn from previous runs to improve future task automation and save them in a plan gallery. Automatically or manually retrieve saved plans in future tasks.&lt;/li&gt; 
 &lt;li&gt;🔀 &lt;strong&gt;Parallel Task Execution&lt;/strong&gt;: You can run multiple tasks in parallel and session status indicators will let you know when Magentic-UI needs your input or has completed the task.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://www.youtube.com/watch?v=wOs-5SR8xOc" target="_blank"&gt; &lt;img src="https://img.youtube.com/vi/wOs-5SR8xOc/maxresdefault.jpg" alt="Watch the demo video" width="600" /&gt; &lt;/a&gt; 
 &lt;br /&gt; ▶️ 
 &lt;em&gt; Click to watch a video and learn more about Magentic-UI &lt;/em&gt; 
&lt;/div&gt; 
&lt;h3&gt;Autonomous Evaluation&lt;/h3&gt; 
&lt;p&gt;To evaluate its autonomous capabilities, Magentic-UI has been tested against several benchmarks when running with o4-mini: &lt;a href="https://huggingface.co/datasets/gaia-benchmark/GAIA"&gt;GAIA&lt;/a&gt; test set (42.52%), which assesses general AI assistants across reasoning, tool use, and web interaction tasks ; &lt;a href="https://huggingface.co/AssistantBench"&gt;AssistantBench&lt;/a&gt; test set (27.60%), focusing on realistic, time-consuming web tasks; &lt;a href="https://github.com/MinorJerry/WebVoyager"&gt;WebVoyager&lt;/a&gt; (82.2%), measuring end-to-end web navigation in real-world scenarios; and &lt;a href="https://webgames.convergence.ai/"&gt;WebGames&lt;/a&gt; (45.5%), evaluating general-purpose web-browsing agents through interactive challenges. To reproduce these experimental results, please see the following &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/experiments/eval/README.md"&gt;instructions&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you're interested in reading more checkout our &lt;a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/magentic-ui-report.pdf"&gt;technical report&lt;/a&gt; and &lt;a href="https://www.microsoft.com/en-us/research/blog/magentic-ui-an-experimental-human-centered-web-agent/"&gt;blog post&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;🛠️ Installation&lt;/h2&gt; 
&lt;h3&gt;Pre-Requisites&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: If you're using Windows, we highly recommend using &lt;a href="https://docs.microsoft.com/en-us/windows/wsl/install"&gt;WSL2&lt;/a&gt; (Windows Subsystem for Linux).&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;If running on &lt;strong&gt;Windows&lt;/strong&gt; or &lt;strong&gt;Mac&lt;/strong&gt; you should use &lt;a href="https://www.docker.com/products/docker-desktop/"&gt;Docker Desktop&lt;/a&gt; or if inside WSL2 you can install Docker directly inside WSL &lt;a href="https://gist.github.com/dehsilvadeveloper/c3bdf0f4cdcc5c177e2fe9be671820c7"&gt;docker in WSL2 guide&lt;/a&gt;. If running on &lt;strong&gt;Linux&lt;/strong&gt;, you should use &lt;a href="https://docs.docker.com/engine/install/"&gt;Docker Engine&lt;/a&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;If using Docker Desktop, make sure it is set up to use WSL2: - Go to Settings &amp;gt; Resources &amp;gt; WSL Integration - Enable integration with your development distro You can find more detailed instructions about this step &lt;a href="https://docs.microsoft.com/en-us/windows/wsl/tutorials/wsl-containers"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt; &lt;p&gt;During the Installation step, you will need to set up your &lt;code&gt;OPENAI_API_KEY&lt;/code&gt;. To use other models, review the &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/#model-client-configuration"&gt;Model Client Configuration&lt;/a&gt; section below.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;You need at least &lt;a href="https://www.python.org/downloads/"&gt;Python 3.10&lt;/a&gt; installed.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;If you are on Windows, we recommend to run Magentic-UI inside &lt;a href="https://docs.microsoft.com/en-us/windows/wsl/install"&gt;WSL2&lt;/a&gt; (Windows Subsystem for Linux) for correct Docker and file path compatibility.&lt;/p&gt; 
&lt;h3&gt;PyPI Installation&lt;/h3&gt; 
&lt;p&gt;Magentic-UI is available on PyPI. We recommend using a virtual environment to avoid conflicts with other packages.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python3 -m venv .venv
source .venv/bin/activate
pip install magentic-ui
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, if you use &lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;&lt;code&gt;uv&lt;/code&gt;&lt;/a&gt; for dependency management, you can install Magentic-UI with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv venv --python=3.12 .venv
. .venv/bin/activate
uv pip install magentic-ui
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Running Magentic-UI&lt;/h3&gt; 
&lt;p&gt;To run Magentic-UI, make sure that Docker is running, then run the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;magentic-ui --port 8081
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Running this command for the first time will pull two docker images required for the Magentic-UI agents. If you encounter problems, you can build them directly with the following command:&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd docker
sh build-all.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you face issues with Docker, please refer to the &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/TROUBLESHOOTING.md"&gt;TROUBLESHOOTING.md&lt;/a&gt; document.&lt;/p&gt; 
&lt;p&gt;Once the server is running, you can access the UI at &lt;a href="http://localhost:8081"&gt;http://localhost:8081&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Configuration&lt;/h3&gt; 
&lt;h4&gt;Model Client Configuration&lt;/h4&gt; 
&lt;p&gt;If you want to use a different OpenAI key, or if you want to configure use with Azure OpenAI or Ollama, you can do so inside the UI by navigating to settings (top right icon) and changing model configuration. Another option is to pass a yaml config file when you start Magentic-UI which will override any settings in the UI:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;magentic-ui --port 8081 --config config.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Where the &lt;code&gt;config.yaml&lt;/code&gt; should look as follows with an AutoGen model client configuration:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;gpt4o_client: &amp;amp;gpt4o_client
    provider: OpenAIChatCompletionClient
    config:
      model: gpt-4o-2024-08-06
      api_key: null
      base_url: null
      max_retries: 5

orchestrator_client: *gpt4o_client
coder_client: *gpt4o_client
web_surfer_client: *gpt4o_client
file_surfer_client: *gpt4o_client
action_guard_client: *gpt4o_client
plan_learning_client: *gpt4o_client
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can change the client for each of the agents using the config file and use AzureOpenAI (&lt;code&gt;AzureOpenAIChatCompletionClient&lt;/code&gt;), Ollama and other clients.&lt;/p&gt; 
&lt;h4&gt;MCP Server Configuration&lt;/h4&gt; 
&lt;p&gt;You can also extend Magentic-UI's capabilities by adding custom "McpAgents" to the multi-agent team. Each McpAgent can have access to one or more MCP Servers. You can specify these agents via the &lt;code&gt;mcp_agent_configs&lt;/code&gt; parameter in your &lt;code&gt;config.yaml&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;For example, here's an agent called "airbnb_surfer" that has access to the OpenBnb MCP Server running locally via Stdio.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;mcp_agent_configs:
  - name: airbnb_surfer
    description: "The airbnb_surfer has direct access to AirBnB."
    model_client: 
      provider: OpenAIChatCompletionClient
      config:
        model: gpt-4.1-2025-04-14
      max_retries: 10
    system_message: |-
      You are AirBnb Surfer, a helpful digital assistant that can help users acces AirBnB.

      You have access to a suite of tools provided by the AirBnB API. Use those tools to satisfy the users requests.
    reflect_on_tool_use: false
    mcp_servers:
      - server_name: AirBnB
        server_params:
          type: StdioServerParams
          command: npx
          args:
            - -y
            - "@openbnb/mcp-server-airbnb"
            - --ignore-robots-txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Under the hood, each &lt;code&gt;McpAgent&lt;/code&gt; is just a &lt;code&gt;autogen_agentchat.agents.AssistantAgent&lt;/code&gt; with the set of MCP Servers exposed as an &lt;code&gt;AggregateMcpWorkbench&lt;/code&gt; which is simply a named collection of &lt;code&gt;autogen_ext.tools.mcp.McpWorkbench&lt;/code&gt; objects (one per MCP Server).&lt;/p&gt; 
&lt;p&gt;Currently the supported MCP Server types are &lt;code&gt;autogen_ext.tools.mcp.StdioServerParams&lt;/code&gt; and &lt;code&gt;autogen_ext.tools.mcp.SseServerParams&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Building Magentic-UI from source&lt;/h3&gt; 
&lt;p&gt;This step is primarily for users seeking to make modifications to the code, are having trouble with the pypi installation or want the latest code before a pypi version release.&lt;/p&gt; 
&lt;h4&gt;1. Make sure the above prerequisites are installed, and that Docker is running.&lt;/h4&gt; 
&lt;h4&gt;2. Clone the repository to your local machine:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/microsoft/magentic-ui.git
cd magentic-ui
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;3. Install Magentic-UI's dependencies with uv or your favorite package manager:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# install uv through https://docs.astral.sh/uv/getting-started/installation/
uv venv --python=3.12 .venv
uv sync --all-extras
source .venv/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;4. Build the frontend:&lt;/h4&gt; 
&lt;p&gt;First make sure to install node:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# install nvm to install node
curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.1/install.sh | bash
nvm install node
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then install the frontend:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd frontend
npm install -g gatsby-cli
npm install --global yarn
yarn install
yarn build
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;5. Run Magentic-UI, as usual.&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;magentic-ui --port 8081
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Running the UI from source&lt;/h4&gt; 
&lt;p&gt;If you are making changes to the source code of the UI, you can run the frontend in development mode so that it will automatically update when you make changes for faster development.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Open a separate terminal and change directory to the frontend&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd frontend
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Create a &lt;code&gt;.env.development&lt;/code&gt; file.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cp .env.default .env.development
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Launch frontend server&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npm run start
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="4"&gt; 
 &lt;li&gt;Then run the UI:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;magentic-ui --port 8081
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The frontend from source will be available at &lt;a href="http://localhost:8000"&gt;http://localhost:8000&lt;/a&gt;, and the compiled frontend will be available at &lt;a href="http://localhost:8081"&gt;http://localhost:8081&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Troubleshooting&lt;/h2&gt; 
&lt;p&gt;If you were unable to get Magentic-UI running, do not worry! The first step is to make sure you have followed the steps outlined above, particularly with the &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/#pre-requisites"&gt;pre-requisites&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For common issues and their solutions, please refer to the &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/TROUBLESHOOTING.md"&gt;TROUBLESHOOTING.md&lt;/a&gt; file in this repository. If you do not see your problem there, please open a &lt;code&gt;GitHub Issue&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;This project welcomes contributions and suggestions. For information about contributing to Magentic-UI, please see our &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; guide, which includes current issues to be resolved and other forms of contributing.&lt;/p&gt; 
&lt;p&gt;This project has adopted the &lt;a href="https://opensource.microsoft.com/codeofconduct/"&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information, see the &lt;a href="https://opensource.microsoft.com/codeofconduct/faq/"&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href="mailto:opencode@microsoft.com"&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Microsoft, and any contributors, grant you a license to any code in the repository under the &lt;a href="https://opensource.org/licenses/MIT"&gt;MIT License&lt;/a&gt;. See the &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/LICENSE"&gt;LICENSE&lt;/a&gt; file.&lt;/p&gt; 
&lt;p&gt;Microsoft, Windows, Microsoft Azure, and/or other Microsoft products and services referenced in the documentation may be either trademarks or registered trademarks of Microsoft in the United States and/or other countries. The licenses for this project do not grant you rights to use any Microsoft names, logos, or trademarks. Microsoft's general trademark guidelines can be found at &lt;a href="http://go.microsoft.com/fwlink/?LinkID=254653"&gt;http://go.microsoft.com/fwlink/?LinkID=254653&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Any use of third-party trademarks or logos are subject to those third-party's policies.&lt;/p&gt; 
&lt;p&gt;Privacy information can be found at &lt;a href="https://go.microsoft.com/fwlink/?LinkId=521839"&gt;https://go.microsoft.com/fwlink/?LinkId=521839&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Microsoft and any contributors reserve all other rights, whether under their respective copyrights, patents, or trademarks, whether by implication, estoppel, or otherwise.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>hiyouga/LLaMA-Factory</title>
      <link>https://github.com/hiyouga/LLaMA-Factory</link>
      <description>&lt;p&gt;Unified Efficient Fine-Tuning of 100+ LLMs &amp; VLMs (ACL 2024)&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/assets/logo.png" alt="# LLaMA Factory" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/hiyouga/LLaMA-Factory/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/hiyouga/LLaMA-Factory?style=social" alt="GitHub Repo stars" /&gt;&lt;/a&gt; &lt;a href="https://github.com/hiyouga/LLaMA-Factory/commits/main"&gt;&lt;img src="https://img.shields.io/github/last-commit/hiyouga/LLaMA-Factory" alt="GitHub last commit" /&gt;&lt;/a&gt; &lt;a href="https://github.com/hiyouga/LLaMA-Factory/graphs/contributors"&gt;&lt;img src="https://img.shields.io/github/contributors/hiyouga/LLaMA-Factory?color=orange" alt="GitHub contributors" /&gt;&lt;/a&gt; &lt;a href="https://github.com/hiyouga/LLaMA-Factory/actions/workflows/tests.yml"&gt;&lt;img src="https://github.com/hiyouga/LLaMA-Factory/actions/workflows/tests.yml/badge.svg?sanitize=true" alt="GitHub workflow" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/llamafactory/"&gt;&lt;img src="https://img.shields.io/pypi/v/llamafactory" alt="PyPI" /&gt;&lt;/a&gt; &lt;a href="https://scholar.google.com/scholar?cites=12620864006390196564"&gt;&lt;img src="https://img.shields.io/badge/citation-760-green" alt="Citation" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/hiyouga/llamafactory/tags"&gt;&lt;img src="https://img.shields.io/docker/pulls/hiyouga/llamafactory" alt="Docker Pulls" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://twitter.com/llamafactory_ai"&gt;&lt;img src="https://img.shields.io/twitter/follow/llamafactory_ai" alt="Twitter" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/rKfvV9r9FK"&gt;&lt;img src="https://dcbadge.vercel.app/api/server/rKfvV9r9FK?compact=true&amp;amp;style=flat" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://gitcode.com/zhengyaowei/LLaMA-Factory"&gt;&lt;img src="https://gitcode.com/zhengyaowei/LLaMA-Factory/star/badge.svg?sanitize=true" alt="GitCode" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open in Colab" /&gt;&lt;/a&gt; &lt;a href="https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory"&gt;&lt;img src="https://gallery.pai-ml.com/assets/open-in-dsw.svg?sanitize=true" alt="Open in DSW" /&gt;&lt;/a&gt; &lt;a href="https://docs.alayanew.com/docs/documents/newActivities/llamafactory/?utm_source=LLaMA-Factory"&gt;&lt;img src="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/assets/alaya_new.svg?sanitize=true" alt="Open in Alaya" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/spaces/hiyouga/LLaMA-Board"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%A4%97-Open%20in%20Spaces-blue" alt="Open in Spaces" /&gt;&lt;/a&gt; &lt;a href="https://modelscope.cn/studios/hiyouga/LLaMA-Board"&gt;&lt;img src="https://img.shields.io/badge/ModelScope-Open%20in%20Studios-blue" alt="Open in Studios" /&gt;&lt;/a&gt; &lt;a href="https://novita.ai/templates-library/105981?sharer=88115474-394e-4bda-968e-b88e123d0c47"&gt;&lt;img src="https://img.shields.io/badge/Novita-Deploy%20Template-blue" alt="Open in Novita" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Used by &lt;a href="https://aws.amazon.com/cn/blogs/machine-learning/how-apoidea-group-enhances-visual-information-extraction-from-banking-documents-with-multimodal-models-using-llama-factory-on-amazon-sagemaker-hyperpod/"&gt;Amazon&lt;/a&gt;, &lt;a href="https://developer.nvidia.com/rtx/ai-toolkit"&gt;NVIDIA&lt;/a&gt;, &lt;a href="https://help.aliyun.com/zh/pai/use-cases/fine-tune-a-llama-3-model-with-llama-factory"&gt;Aliyun&lt;/a&gt;, etc.&lt;/h3&gt; 
&lt;div align="center" markdown="1"&gt; 
 &lt;h3&gt;Supporters ❤️&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;
     &lt;div style="text-align: center;"&gt;
      &lt;a href="https://warp.dev/llama-factory"&gt;&lt;img alt="Warp sponsorship" width="400" src="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/assets/warp.jpg" /&gt;&lt;/a&gt;
      &lt;br /&gt;
      &lt;a href="https://warp.dev/llama-factory" style="font-size:larger;"&gt;Warp, the agentic terminal for developers&lt;/a&gt;
      &lt;br /&gt;
      &lt;a href="https://warp.dev/llama-factory"&gt;Available for MacOS, Linux, &amp;amp; Windows&lt;/a&gt;
     &lt;/div&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;a href="https://serpapi.com"&gt;&lt;img alt="SerpAPI sponsorship" width="250" src="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/assets/serpapi.svg?sanitize=true" /&gt; &lt;/a&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
 &lt;/table&gt; 
 &lt;hr /&gt; 
 &lt;h3&gt;Easily fine-tune 100+ large language models with zero-code &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#quickstart"&gt;CLI&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#fine-tuning-with-llama-board-gui-powered-by-gradio"&gt;Web UI&lt;/a&gt;&lt;/h3&gt; 
 &lt;p&gt;&lt;img src="https://trendshift.io/api/badge/repositories/4535" alt="GitHub Trend" /&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;👋 Join our &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/assets/wechat.jpg"&gt;WeChat group&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/assets/wechat_npu.jpg"&gt;NPU user group&lt;/a&gt; or &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/assets/wechat_alaya.png"&gt;Alaya NeW user group&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;[ English | &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/README_zh.md"&gt;中文&lt;/a&gt; ]&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Fine-tuning a large language model can be easy as...&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/3991a3a8-4276-4d30-9cab-4cb0c4b9b99e"&gt;https://github.com/user-attachments/assets/3991a3a8-4276-4d30-9cab-4cb0c4b9b99e&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Choose your path:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Documentation (WIP)&lt;/strong&gt;: &lt;a href="https://llamafactory.readthedocs.io/en/latest/"&gt;https://llamafactory.readthedocs.io/en/latest/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Documentation (AMD GPU)&lt;/strong&gt;: &lt;a href="https://rocm.docs.amd.com/projects/ai-developer-hub/en/latest/notebooks/fine_tune/llama_factory_llama3.html"&gt;https://rocm.docs.amd.com/projects/ai-developer-hub/en/latest/notebooks/fine_tune/llama_factory_llama3.html&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Colab (free)&lt;/strong&gt;: &lt;a href="https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing"&gt;https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Local machine&lt;/strong&gt;: Please refer to &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#getting-started"&gt;usage&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;PAI-DSW (free trial)&lt;/strong&gt;: &lt;a href="https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory"&gt;https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Alaya NeW (cloud GPU deal)&lt;/strong&gt;: &lt;a href="https://docs.alayanew.com/docs/documents/useGuide/LLaMAFactory/mutiple/?utm_source=LLaMA-Factory"&gt;https://docs.alayanew.com/docs/documents/useGuide/LLaMAFactory/mutiple/?utm_source=LLaMA-Factory&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Except for the above links, all other websites are unauthorized third-party websites. Please carefully use them.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#features"&gt;Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#blogs"&gt;Blogs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#changelog"&gt;Changelog&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#supported-models"&gt;Supported Models&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#supported-training-approaches"&gt;Supported Training Approaches&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#provided-datasets"&gt;Provided Datasets&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#requirement"&gt;Requirement&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#getting-started"&gt;Getting Started&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#data-preparation"&gt;Data Preparation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#quickstart"&gt;Quickstart&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#fine-tuning-with-llama-board-gui-powered-by-gradio"&gt;Fine-Tuning with LLaMA Board GUI&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#build-docker"&gt;Build Docker&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#deploy-with-openai-style-api-and-vllm"&gt;Deploy with OpenAI-style API and vLLM&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#download-from-modelscope-hub"&gt;Download from ModelScope Hub&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#download-from-modelers-hub"&gt;Download from Modelers Hub&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#use-wb-logger"&gt;Use W&amp;amp;B Logger&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#use-swanlab-logger"&gt;Use SwanLab Logger&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#projects-using-llama-factory"&gt;Projects using LLaMA Factory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#citation"&gt;Citation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#acknowledgement"&gt;Acknowledgement&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Various models&lt;/strong&gt;: LLaMA, LLaVA, Mistral, Mixtral-MoE, Qwen, Qwen2-VL, DeepSeek, Yi, Gemma, ChatGLM, Phi, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Integrated methods&lt;/strong&gt;: (Continuous) pre-training, (multimodal) supervised fine-tuning, reward modeling, PPO, DPO, KTO, ORPO, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Scalable resources&lt;/strong&gt;: 16-bit full-tuning, freeze-tuning, LoRA and 2/3/4/5/6/8-bit QLoRA via AQLM/AWQ/GPTQ/LLM.int8/HQQ/EETQ.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Advanced algorithms&lt;/strong&gt;: &lt;a href="https://github.com/jiaweizzhao/GaLore"&gt;GaLore&lt;/a&gt;, &lt;a href="https://github.com/Ledzy/BAdam"&gt;BAdam&lt;/a&gt;, &lt;a href="https://github.com/zhuhanqing/APOLLO"&gt;APOLLO&lt;/a&gt;, &lt;a href="https://github.com/zyushun/Adam-mini"&gt;Adam-mini&lt;/a&gt;, &lt;a href="https://github.com/KellerJordan/Muon"&gt;Muon&lt;/a&gt;, DoRA, LongLoRA, LLaMA Pro, Mixture-of-Depths, LoRA+, LoftQ and PiSSA.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Practical tricks&lt;/strong&gt;: &lt;a href="https://github.com/Dao-AILab/flash-attention"&gt;FlashAttention-2&lt;/a&gt;, &lt;a href="https://github.com/unslothai/unsloth"&gt;Unsloth&lt;/a&gt;, &lt;a href="https://github.com/linkedin/Liger-Kernel"&gt;Liger Kernel&lt;/a&gt;, RoPE scaling, NEFTune and rsLoRA.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Wide tasks&lt;/strong&gt;: Multi-turn dialogue, tool using, image understanding, visual grounding, video recognition, audio understanding, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Experiment monitors&lt;/strong&gt;: LlamaBoard, TensorBoard, Wandb, MLflow, &lt;a href="https://github.com/SwanHubX/SwanLab"&gt;SwanLab&lt;/a&gt;, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Faster inference&lt;/strong&gt;: OpenAI-style API, Gradio UI and CLI with &lt;a href="https://github.com/vllm-project/vllm"&gt;vLLM worker&lt;/a&gt; or &lt;a href="https://github.com/sgl-project/sglang"&gt;SGLang worker&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Day-N Support for Fine-Tuning Cutting-Edge Models&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Support Date&lt;/th&gt; 
   &lt;th&gt;Model Name&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Day 0&lt;/td&gt; 
   &lt;td&gt;Qwen3 / Qwen2.5-VL / Gemma 3 / GLM-4.1V / InternLM 3 / MiniCPM-o-2.6&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Day 1&lt;/td&gt; 
   &lt;td&gt;Llama 3 / GLM-4 / Mistral Small / PaliGemma2 / Llama 4&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Blogs&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.llamafactory.com.cn/docs/documents/best-practice/gptoss/?utm_source=LLaMA-Factory"&gt;Fine-tune GPT-OSS for Role-Playing using LLaMA-Factory&lt;/a&gt; (Chinese)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.alayanew.com/docs/documents/bestPractice/bigModel/llama70B/?utm_source=LLaMA-Factory"&gt;Fine-tune Llama3.1-70B for Medical Diagnosis using LLaMA-Factory&lt;/a&gt; (Chinese)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/cn/blogs/china/building-llm-model-hub-based-on-llamafactory-and-easyr1/"&gt;A One-Stop Code-Free Model Reinforcement Learning and Deployment Platform based on LLaMA-Factory and EasyR1&lt;/a&gt; (Chinese)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/cn/blogs/machine-learning/how-apoidea-group-enhances-visual-information-extraction-from-banking-documents-with-multimodal-models-using-llama-factory-on-amazon-sagemaker-hyperpod/"&gt;How Apoidea Group enhances visual information extraction from banking documents with multimodal models using LLaMA-Factory on Amazon SageMaker HyperPod&lt;/a&gt; (English)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://buaa-act.feishu.cn/wiki/GVzlwYcRFiR8OLkHbL6cQpYin7g"&gt;Easy Dataset × LLaMA Factory: Enabling LLMs to Efficiently Learn Domain Knowledge&lt;/a&gt; (English)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt;
 &lt;summary&gt;All Blogs&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://docs.alayanew.com/docs/documents/useGuide/LLaMAFactory/mutiple/?utm_source=LLaMA-Factory"&gt;Fine-tune Qwen2.5-VL for Autonomous Driving using LLaMA-Factory&lt;/a&gt; (Chinese)&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory_deepseek_r1_distill_7b"&gt;LLaMA Factory: Fine-tuning the DeepSeek-R1-Distill-Qwen-7B Model for News Classifier&lt;/a&gt; (Chinese)&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://aws.amazon.com/cn/blogs/china/a-one-stop-code-free-model-fine-tuning-deployment-platform-based-on-sagemaker-and-llama-factory/"&gt;A One-Stop Code-Free Model Fine-Tuning &amp;amp; Deployment Platform based on SageMaker and LLaMA-Factory&lt;/a&gt; (Chinese)&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory_qwen2vl"&gt;LLaMA Factory Multi-Modal Fine-Tuning Practice: Fine-Tuning Qwen2-VL for Personal Tourist Guide&lt;/a&gt; (Chinese)&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory"&gt;LLaMA Factory: Fine-tuning Llama3 for Role-Playing&lt;/a&gt; (Chinese)&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;Changelog&lt;/h2&gt; 
&lt;p&gt;[25/08/06] We supported fine-tuning the &lt;strong&gt;&lt;a href="https://github.com/openai/gpt-oss"&gt;GPT-OSS&lt;/a&gt;&lt;/strong&gt; models. See &lt;a href="https://github.com/hiyouga/LLaMA-Factory/pull/8826"&gt;PR #8826&lt;/a&gt; to get started.&lt;/p&gt; 
&lt;p&gt;[25/07/02] We supported fine-tuning the &lt;strong&gt;&lt;a href="https://github.com/THUDM/GLM-4.1V-Thinking"&gt;GLM-4.1V-9B-Thinking&lt;/a&gt;&lt;/strong&gt; model.&lt;/p&gt; 
&lt;p&gt;[25/04/28] We supported fine-tuning the &lt;strong&gt;&lt;a href="https://qwenlm.github.io/blog/qwen3/"&gt;Qwen3&lt;/a&gt;&lt;/strong&gt; model family.&lt;/p&gt; 
&lt;details&gt;
 &lt;summary&gt;Full Changelog&lt;/summary&gt; 
 &lt;p&gt;[25/04/21] We supported the &lt;strong&gt;&lt;a href="https://github.com/KellerJordan/Muon"&gt;Muon&lt;/a&gt;&lt;/strong&gt; optimizer. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples&lt;/a&gt; for usage. Thank &lt;a href="https://github.com/tianshijing"&gt;@tianshijing&lt;/a&gt;'s PR.&lt;/p&gt; 
 &lt;p&gt;[25/04/16] We supported fine-tuning the &lt;strong&gt;&lt;a href="https://huggingface.co/OpenGVLab/InternVL3-8B"&gt;InternVL3&lt;/a&gt;&lt;/strong&gt; model. See &lt;a href="https://github.com/hiyouga/LLaMA-Factory/pull/7258"&gt;PR #7258&lt;/a&gt; to get started.&lt;/p&gt; 
 &lt;p&gt;[25/04/14] We supported fine-tuning the &lt;strong&gt;&lt;a href="https://huggingface.co/THUDM/GLM-Z1-9B-0414"&gt;GLM-Z1&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href="https://huggingface.co/moonshotai/Kimi-VL-A3B-Instruct"&gt;Kimi-VL&lt;/a&gt;&lt;/strong&gt; models.&lt;/p&gt; 
 &lt;p&gt;[25/04/06] We supported fine-tuning the &lt;strong&gt;&lt;a href="https://ai.meta.com/blog/llama-4-multimodal-intelligence/"&gt;Llama 4&lt;/a&gt;&lt;/strong&gt; model. See &lt;a href="https://github.com/hiyouga/LLaMA-Factory/pull/7611"&gt;PR #7611&lt;/a&gt; to get started.&lt;/p&gt; 
 &lt;p&gt;[25/03/31] We supported fine-tuning the &lt;strong&gt;&lt;a href="https://qwenlm.github.io/blog/qwen2.5-omni/"&gt;Qwen2.5 Omni&lt;/a&gt;&lt;/strong&gt; model. See &lt;a href="https://github.com/hiyouga/LLaMA-Factory/pull/7537"&gt;PR #7537&lt;/a&gt; to get started.&lt;/p&gt; 
 &lt;p&gt;[25/03/15] We supported &lt;strong&gt;&lt;a href="https://github.com/sgl-project/sglang"&gt;SGLang&lt;/a&gt;&lt;/strong&gt; as inference backend. Try &lt;code&gt;infer_backend: sglang&lt;/code&gt; to accelerate inference.&lt;/p&gt; 
 &lt;p&gt;[25/03/12] We supported fine-tuning the &lt;strong&gt;&lt;a href="https://huggingface.co/blog/gemma3"&gt;Gemma 3&lt;/a&gt;&lt;/strong&gt; model.&lt;/p&gt; 
 &lt;p&gt;[25/02/24] Announcing &lt;strong&gt;&lt;a href="https://github.com/hiyouga/EasyR1"&gt;EasyR1&lt;/a&gt;&lt;/strong&gt;, an efficient, scalable and multi-modality RL training framework for efficient GRPO training.&lt;/p&gt; 
 &lt;p&gt;[25/02/11] We supported saving the &lt;strong&gt;&lt;a href="https://github.com/ollama/ollama"&gt;Ollama&lt;/a&gt;&lt;/strong&gt; modelfile when exporting the model checkpoints. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[25/02/05] We supported fine-tuning the &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/Qwen/Qwen2-Audio-7B-Instruct"&gt;Qwen2-Audio&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href="https://huggingface.co/openbmb/MiniCPM-o-2_6"&gt;MiniCPM-o-2.6&lt;/a&gt;&lt;/strong&gt; on audio understanding tasks.&lt;/p&gt; 
 &lt;p&gt;[25/01/31] We supported fine-tuning the &lt;strong&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1"&gt;DeepSeek-R1&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct"&gt;Qwen2.5-VL&lt;/a&gt;&lt;/strong&gt; models.&lt;/p&gt; 
 &lt;p&gt;[25/01/15] We supported &lt;strong&gt;&lt;a href="https://arxiv.org/abs/2412.05270"&gt;APOLLO&lt;/a&gt;&lt;/strong&gt; optimizer. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[25/01/14] We supported fine-tuning the &lt;strong&gt;&lt;a href="https://huggingface.co/openbmb/MiniCPM-o-2_6"&gt;MiniCPM-o-2.6&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href="https://huggingface.co/openbmb/MiniCPM-V-2_6"&gt;MiniCPM-V-2.6&lt;/a&gt;&lt;/strong&gt; models. Thank &lt;a href="https://github.com/BUAADreamer"&gt;@BUAADreamer&lt;/a&gt;'s PR.&lt;/p&gt; 
 &lt;p&gt;[25/01/14] We supported fine-tuning the &lt;strong&gt;&lt;a href="https://huggingface.co/collections/internlm/"&gt;InternLM 3&lt;/a&gt;&lt;/strong&gt; models. Thank &lt;a href="https://github.com/hhaAndroid"&gt;@hhaAndroid&lt;/a&gt;'s PR.&lt;/p&gt; 
 &lt;p&gt;[25/01/10] We supported fine-tuning the &lt;strong&gt;&lt;a href="https://huggingface.co/microsoft/phi-4"&gt;Phi-4&lt;/a&gt;&lt;/strong&gt; model.&lt;/p&gt; 
 &lt;p&gt;[24/12/21] We supported using &lt;strong&gt;&lt;a href="https://github.com/SwanHubX/SwanLab"&gt;SwanLab&lt;/a&gt;&lt;/strong&gt; for experiment tracking and visualization. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#use-swanlab-logger"&gt;this section&lt;/a&gt; for details.&lt;/p&gt; 
 &lt;p&gt;[24/11/27] We supported fine-tuning the &lt;strong&gt;&lt;a href="https://huggingface.co/Skywork/Skywork-o1-Open-Llama-3.1-8B"&gt;Skywork-o1&lt;/a&gt;&lt;/strong&gt; model and the &lt;strong&gt;&lt;a href="https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT"&gt;OpenO1&lt;/a&gt;&lt;/strong&gt; dataset.&lt;/p&gt; 
 &lt;p&gt;[24/10/09] We supported downloading pre-trained models and datasets from the &lt;strong&gt;&lt;a href="https://modelers.cn/models"&gt;Modelers Hub&lt;/a&gt;&lt;/strong&gt;. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#download-from-modelers-hub"&gt;this tutorial&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/09/19] We supported fine-tuning the &lt;strong&gt;&lt;a href="https://qwenlm.github.io/blog/qwen2.5/"&gt;Qwen2.5&lt;/a&gt;&lt;/strong&gt; models.&lt;/p&gt; 
 &lt;p&gt;[24/08/30] We supported fine-tuning the &lt;strong&gt;&lt;a href="https://qwenlm.github.io/blog/qwen2-vl/"&gt;Qwen2-VL&lt;/a&gt;&lt;/strong&gt; models. Thank &lt;a href="https://github.com/simonJJJ"&gt;@simonJJJ&lt;/a&gt;'s PR.&lt;/p&gt; 
 &lt;p&gt;[24/08/27] We supported &lt;strong&gt;&lt;a href="https://github.com/linkedin/Liger-Kernel"&gt;Liger Kernel&lt;/a&gt;&lt;/strong&gt;. Try &lt;code&gt;enable_liger_kernel: true&lt;/code&gt; for efficient training.&lt;/p&gt; 
 &lt;p&gt;[24/08/09] We supported &lt;strong&gt;&lt;a href="https://github.com/zyushun/Adam-mini"&gt;Adam-mini&lt;/a&gt;&lt;/strong&gt; optimizer. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples&lt;/a&gt; for usage. Thank &lt;a href="https://github.com/relic-yuexi"&gt;@relic-yuexi&lt;/a&gt;'s PR.&lt;/p&gt; 
 &lt;p&gt;[24/07/04] We supported &lt;a href="https://github.com/MeetKai/functionary/tree/main/functionary/train/packing"&gt;contamination-free packed training&lt;/a&gt;. Use &lt;code&gt;neat_packing: true&lt;/code&gt; to activate it. Thank &lt;a href="https://github.com/chuan298"&gt;@chuan298&lt;/a&gt;'s PR.&lt;/p&gt; 
 &lt;p&gt;[24/06/16] We supported &lt;strong&gt;&lt;a href="https://arxiv.org/abs/2404.02948"&gt;PiSSA&lt;/a&gt;&lt;/strong&gt; algorithm. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/06/07] We supported fine-tuning the &lt;strong&gt;&lt;a href="https://qwenlm.github.io/blog/qwen2/"&gt;Qwen2&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href="https://github.com/THUDM/GLM-4"&gt;GLM-4&lt;/a&gt;&lt;/strong&gt; models.&lt;/p&gt; 
 &lt;p&gt;[24/05/26] We supported &lt;strong&gt;&lt;a href="https://arxiv.org/abs/2405.14734"&gt;SimPO&lt;/a&gt;&lt;/strong&gt; algorithm for preference learning. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/05/20] We supported fine-tuning the &lt;strong&gt;PaliGemma&lt;/strong&gt; series models. Note that the PaliGemma models are pre-trained models, you need to fine-tune them with &lt;code&gt;paligemma&lt;/code&gt; template for chat completion.&lt;/p&gt; 
 &lt;p&gt;[24/05/18] We supported &lt;strong&gt;&lt;a href="https://arxiv.org/abs/2402.01306"&gt;KTO&lt;/a&gt;&lt;/strong&gt; algorithm for preference learning. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/05/14] We supported training and inference on the Ascend NPU devices. Check &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#installation"&gt;installation&lt;/a&gt; section for details.&lt;/p&gt; 
 &lt;p&gt;[24/04/26] We supported fine-tuning the &lt;strong&gt;LLaVA-1.5&lt;/strong&gt; multimodal LLMs. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/04/22] We provided a &lt;strong&gt;&lt;a href="https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing"&gt;Colab notebook&lt;/a&gt;&lt;/strong&gt; for fine-tuning the Llama-3 model on a free T4 GPU. Two Llama-3-derived models fine-tuned using LLaMA Factory are available at Hugging Face, check &lt;a href="https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat"&gt;Llama3-8B-Chinese-Chat&lt;/a&gt; and &lt;a href="https://huggingface.co/zhichen/Llama3-Chinese"&gt;Llama3-Chinese&lt;/a&gt; for details.&lt;/p&gt; 
 &lt;p&gt;[24/04/21] We supported &lt;strong&gt;&lt;a href="https://arxiv.org/abs/2404.02258"&gt;Mixture-of-Depths&lt;/a&gt;&lt;/strong&gt; according to &lt;a href="https://github.com/astramind-ai/Mixture-of-depths"&gt;AstraMindAI's implementation&lt;/a&gt;. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/04/16] We supported &lt;strong&gt;&lt;a href="https://arxiv.org/abs/2404.02827"&gt;BAdam&lt;/a&gt;&lt;/strong&gt; optimizer. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/04/16] We supported &lt;strong&gt;&lt;a href="https://github.com/unslothai/unsloth"&gt;unsloth&lt;/a&gt;&lt;/strong&gt;'s long-sequence training (Llama-2-7B-56k within 24GB). It achieves &lt;strong&gt;117%&lt;/strong&gt; speed and &lt;strong&gt;50%&lt;/strong&gt; memory compared with FlashAttention-2, more benchmarks can be found in &lt;a href="https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-comparison"&gt;this page&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;[24/03/31] We supported &lt;strong&gt;&lt;a href="https://arxiv.org/abs/2403.07691"&gt;ORPO&lt;/a&gt;&lt;/strong&gt;. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/03/21] Our paper "&lt;a href="https://arxiv.org/abs/2403.13372"&gt;LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models&lt;/a&gt;" is available at arXiv!&lt;/p&gt; 
 &lt;p&gt;[24/03/20] We supported &lt;strong&gt;FSDP+QLoRA&lt;/strong&gt; that fine-tunes a 70B model on 2x24GB GPUs. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/03/13] We supported &lt;strong&gt;&lt;a href="https://arxiv.org/abs/2402.12354"&gt;LoRA+&lt;/a&gt;&lt;/strong&gt;. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/03/07] We supported &lt;strong&gt;&lt;a href="https://arxiv.org/abs/2403.03507"&gt;GaLore&lt;/a&gt;&lt;/strong&gt; optimizer. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/03/07] We integrated &lt;strong&gt;&lt;a href="https://github.com/vllm-project/vllm"&gt;vLLM&lt;/a&gt;&lt;/strong&gt; for faster and concurrent inference. Try &lt;code&gt;infer_backend: vllm&lt;/code&gt; to enjoy &lt;strong&gt;270%&lt;/strong&gt; inference speed.&lt;/p&gt; 
 &lt;p&gt;[24/02/28] We supported weight-decomposed LoRA (&lt;strong&gt;&lt;a href="https://arxiv.org/abs/2402.09353"&gt;DoRA&lt;/a&gt;&lt;/strong&gt;). Try &lt;code&gt;use_dora: true&lt;/code&gt; to activate DoRA training.&lt;/p&gt; 
 &lt;p&gt;[24/02/15] We supported &lt;strong&gt;block expansion&lt;/strong&gt; proposed by &lt;a href="https://github.com/TencentARC/LLaMA-Pro"&gt;LLaMA Pro&lt;/a&gt;. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/02/05] Qwen1.5 (Qwen2 beta version) series models are supported in LLaMA-Factory. Check this &lt;a href="https://qwenlm.github.io/blog/qwen1.5/"&gt;blog post&lt;/a&gt; for details.&lt;/p&gt; 
 &lt;p&gt;[24/01/18] We supported &lt;strong&gt;agent tuning&lt;/strong&gt; for most models, equipping model with tool using abilities by fine-tuning with &lt;code&gt;dataset: glaive_toolcall_en&lt;/code&gt;.&lt;/p&gt; 
 &lt;p&gt;[23/12/23] We supported &lt;strong&gt;&lt;a href="https://github.com/unslothai/unsloth"&gt;unsloth&lt;/a&gt;&lt;/strong&gt;'s implementation to boost LoRA tuning for the LLaMA, Mistral and Yi models. Try &lt;code&gt;use_unsloth: true&lt;/code&gt; argument to activate unsloth patch. It achieves &lt;strong&gt;170%&lt;/strong&gt; speed in our benchmark, check &lt;a href="https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-comparison"&gt;this page&lt;/a&gt; for details.&lt;/p&gt; 
 &lt;p&gt;[23/12/12] We supported fine-tuning the latest MoE model &lt;strong&gt;&lt;a href="https://huggingface.co/mistralai/Mixtral-8x7B-v0.1"&gt;Mixtral 8x7B&lt;/a&gt;&lt;/strong&gt; in our framework. See hardware requirement &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#hardware-requirement"&gt;here&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;[23/12/01] We supported downloading pre-trained models and datasets from the &lt;strong&gt;&lt;a href="https://modelscope.cn/models"&gt;ModelScope Hub&lt;/a&gt;&lt;/strong&gt;. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#download-from-modelscope-hub"&gt;this tutorial&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[23/10/21] We supported &lt;strong&gt;&lt;a href="https://arxiv.org/abs/2310.05914"&gt;NEFTune&lt;/a&gt;&lt;/strong&gt; trick for fine-tuning. Try &lt;code&gt;neftune_noise_alpha: 5&lt;/code&gt; argument to activate NEFTune.&lt;/p&gt; 
 &lt;p&gt;[23/09/27] We supported &lt;strong&gt;$S^2$-Attn&lt;/strong&gt; proposed by &lt;a href="https://github.com/dvlab-research/LongLoRA"&gt;LongLoRA&lt;/a&gt; for the LLaMA models. Try &lt;code&gt;shift_attn: true&lt;/code&gt; argument to enable shift short attention.&lt;/p&gt; 
 &lt;p&gt;[23/09/23] We integrated MMLU, C-Eval and CMMLU benchmarks in this repo. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[23/09/10] We supported &lt;strong&gt;&lt;a href="https://github.com/Dao-AILab/flash-attention"&gt;FlashAttention-2&lt;/a&gt;&lt;/strong&gt;. Try &lt;code&gt;flash_attn: fa2&lt;/code&gt; argument to enable FlashAttention-2 if you are using RTX4090, A100 or H100 GPUs.&lt;/p&gt; 
 &lt;p&gt;[23/08/12] We supported &lt;strong&gt;RoPE scaling&lt;/strong&gt; to extend the context length of the LLaMA models. Try &lt;code&gt;rope_scaling: linear&lt;/code&gt; argument in training and &lt;code&gt;rope_scaling: dynamic&lt;/code&gt; argument at inference to extrapolate the position embeddings.&lt;/p&gt; 
 &lt;p&gt;[23/08/11] We supported &lt;strong&gt;&lt;a href="https://arxiv.org/abs/2305.18290"&gt;DPO training&lt;/a&gt;&lt;/strong&gt; for instruction-tuned models. See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[23/07/31] We supported &lt;strong&gt;dataset streaming&lt;/strong&gt;. Try &lt;code&gt;streaming: true&lt;/code&gt; and &lt;code&gt;max_steps: 10000&lt;/code&gt; arguments to load your dataset in streaming mode.&lt;/p&gt; 
 &lt;p&gt;[23/07/29] We released two instruction-tuned 13B models at Hugging Face. See these Hugging Face Repos (&lt;a href="https://huggingface.co/hiyouga/Llama-2-Chinese-13b-chat"&gt;LLaMA-2&lt;/a&gt; / &lt;a href="https://huggingface.co/hiyouga/Baichuan-13B-sft"&gt;Baichuan&lt;/a&gt;) for details.&lt;/p&gt; 
 &lt;p&gt;[23/07/18] We developed an &lt;strong&gt;all-in-one Web UI&lt;/strong&gt; for training, evaluation and inference. Try &lt;code&gt;train_web.py&lt;/code&gt; to fine-tune models in your Web browser. Thank &lt;a href="https://github.com/KanadeSiina"&gt;@KanadeSiina&lt;/a&gt; and &lt;a href="https://github.com/codemayq"&gt;@codemayq&lt;/a&gt; for their efforts in the development.&lt;/p&gt; 
 &lt;p&gt;[23/07/09] We released &lt;strong&gt;&lt;a href="https://github.com/hiyouga/FastEdit"&gt;FastEdit&lt;/a&gt;&lt;/strong&gt; ⚡🩹, an easy-to-use package for editing the factual knowledge of large language models efficiently. Please follow &lt;a href="https://github.com/hiyouga/FastEdit"&gt;FastEdit&lt;/a&gt; if you are interested.&lt;/p&gt; 
 &lt;p&gt;[23/06/29] We provided a &lt;strong&gt;reproducible example&lt;/strong&gt; of training a chat model using instruction-following datasets, see &lt;a href="https://huggingface.co/hiyouga/Baichuan-7B-sft"&gt;Baichuan-7B-sft&lt;/a&gt; for details.&lt;/p&gt; 
 &lt;p&gt;[23/06/22] We aligned the &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/src/api_demo.py"&gt;demo API&lt;/a&gt; with the &lt;a href="https://platform.openai.com/docs/api-reference/chat"&gt;OpenAI's&lt;/a&gt; format where you can insert the fine-tuned model in &lt;strong&gt;arbitrary ChatGPT-based applications&lt;/strong&gt;.&lt;/p&gt; 
 &lt;p&gt;[23/06/03] We supported quantized training and inference (aka &lt;strong&gt;&lt;a href="https://github.com/artidoro/qlora"&gt;QLoRA&lt;/a&gt;&lt;/strong&gt;). See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
&lt;/details&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] If you cannot use the latest feature, please pull the latest code and install LLaMA-Factory again.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Supported Models&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Model size&lt;/th&gt; 
   &lt;th&gt;Template&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/baichuan-inc"&gt;Baichuan 2&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B/13B&lt;/td&gt; 
   &lt;td&gt;baichuan2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/bigscience"&gt;BLOOM/BLOOMZ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;560M/1.1B/1.7B/3B/7.1B/176B&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/THUDM"&gt;ChatGLM3&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;6B&lt;/td&gt; 
   &lt;td&gt;chatglm3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/CohereForAI"&gt;Command R&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;35B/104B&lt;/td&gt; 
   &lt;td&gt;cohere&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/deepseek-ai"&gt;DeepSeek (Code/MoE)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B/16B/67B/236B&lt;/td&gt; 
   &lt;td&gt;deepseek&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/deepseek-ai"&gt;DeepSeek 2.5/3&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;236B/671B&lt;/td&gt; 
   &lt;td&gt;deepseek3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/deepseek-ai"&gt;DeepSeek R1 (Distill)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1.5B/7B/8B/14B/32B/70B/671B&lt;/td&gt; 
   &lt;td&gt;deepseekr1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/tiiuae"&gt;Falcon&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B/11B/40B/180B&lt;/td&gt; 
   &lt;td&gt;falcon&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/tiiuae"&gt;Falcon-H1&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;0.5B/1.5B/3B/7B/34B&lt;/td&gt; 
   &lt;td&gt;falcon_h1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/google"&gt;Gemma/Gemma 2/CodeGemma&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2B/7B/9B/27B&lt;/td&gt; 
   &lt;td&gt;gemma/gemma2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/google"&gt;Gemma 3/Gemma 3n&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1B/4B/6B/8B/12B/27B&lt;/td&gt; 
   &lt;td&gt;gemma3/gemma3n&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/zai-org"&gt;GLM-4/GLM-4-0414/GLM-Z1&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;9B/32B&lt;/td&gt; 
   &lt;td&gt;glm4/glmz1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/zai-org"&gt;GLM-4.1V&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;9B&lt;/td&gt; 
   &lt;td&gt;glm4v&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/zai-org"&gt;GLM-4.5/GLM-4.5V&lt;/a&gt;*&lt;/td&gt; 
   &lt;td&gt;106B/355B&lt;/td&gt; 
   &lt;td&gt;glm4_moe/glm4v_moe&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/openai-community"&gt;GPT-2&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;0.1B/0.4B/0.8B/1.5B&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/openai"&gt;GPT-OSS&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;20B/120B&lt;/td&gt; 
   &lt;td&gt;gpt&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/ibm-granite"&gt;Granite 3.0-3.3&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1B/2B/3B/8B&lt;/td&gt; 
   &lt;td&gt;granite3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/ibm-granite"&gt;Granite 4&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B&lt;/td&gt; 
   &lt;td&gt;granite4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/tencent/"&gt;Hunyuan&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B&lt;/td&gt; 
   &lt;td&gt;hunyuan&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/IndexTeam"&gt;Index&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1.9B&lt;/td&gt; 
   &lt;td&gt;index&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/internlm"&gt;InternLM 2-3&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B/8B/20B&lt;/td&gt; 
   &lt;td&gt;intern2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/OpenGVLab"&gt;InternVL 2.5-3&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1B/2B/8B/14B/38B/78B&lt;/td&gt; 
   &lt;td&gt;intern_vl&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/moonshotai"&gt;Kimi-VL&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;16B&lt;/td&gt; 
   &lt;td&gt;kimi_vl&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/facebookresearch/llama"&gt;Llama&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B/13B/33B/65B&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/meta-llama"&gt;Llama 2&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B/13B/70B&lt;/td&gt; 
   &lt;td&gt;llama2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/meta-llama"&gt;Llama 3-3.3&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1B/3B/8B/70B&lt;/td&gt; 
   &lt;td&gt;llama3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/meta-llama"&gt;Llama 4&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;109B/402B&lt;/td&gt; 
   &lt;td&gt;llama4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/meta-llama"&gt;Llama 3.2 Vision&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;11B/90B&lt;/td&gt; 
   &lt;td&gt;mllama&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/llava-hf"&gt;LLaVA-1.5&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B/13B&lt;/td&gt; 
   &lt;td&gt;llava&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/llava-hf"&gt;LLaVA-NeXT&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B/8B/13B/34B/72B/110B&lt;/td&gt; 
   &lt;td&gt;llava_next&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/llava-hf"&gt;LLaVA-NeXT-Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B/34B&lt;/td&gt; 
   &lt;td&gt;llava_next_video&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/XiaomiMiMo"&gt;MiMo&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B&lt;/td&gt; 
   &lt;td&gt;mimo&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/openbmb"&gt;MiniCPM&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;0.5B/1B/2B/4B/8B&lt;/td&gt; 
   &lt;td&gt;cpm/cpm3/cpm4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/openbmb"&gt;MiniCPM-o-2.6/MiniCPM-V-2.6&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;8B&lt;/td&gt; 
   &lt;td&gt;minicpm_o/minicpm_v&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mistralai"&gt;Ministral/Mistral-Nemo&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;8B/12B&lt;/td&gt; 
   &lt;td&gt;ministral&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mistralai"&gt;Mistral/Mixtral&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B/8x7B/8x22B&lt;/td&gt; 
   &lt;td&gt;mistral&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mistralai"&gt;Mistral Small&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;24B&lt;/td&gt; 
   &lt;td&gt;mistral_small&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/allenai"&gt;OLMo&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1B/7B&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/google"&gt;PaliGemma/PaliGemma2&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;3B/10B/28B&lt;/td&gt; 
   &lt;td&gt;paligemma&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/microsoft"&gt;Phi-1.5/Phi-2&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1.3B/2.7B&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/microsoft"&gt;Phi-3/Phi-3.5&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;4B/14B&lt;/td&gt; 
   &lt;td&gt;phi&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/microsoft"&gt;Phi-3-small&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B&lt;/td&gt; 
   &lt;td&gt;phi_small&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/microsoft"&gt;Phi-4&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;14B&lt;/td&gt; 
   &lt;td&gt;phi4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mistralai"&gt;Pixtral&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;12B&lt;/td&gt; 
   &lt;td&gt;pixtral&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/Qwen"&gt;Qwen (1-2.5) (Code/Math/MoE/QwQ)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;0.5B/1.5B/3B/7B/14B/32B/72B/110B&lt;/td&gt; 
   &lt;td&gt;qwen&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/Qwen"&gt;Qwen3 (MoE/Instruct/Thinking)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;0.6B/1.7B/4B/8B/14B/32B/235B&lt;/td&gt; 
   &lt;td&gt;qwen3/qwen3_nothink&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/Qwen"&gt;Qwen2-Audio&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B&lt;/td&gt; 
   &lt;td&gt;qwen2_audio&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/Qwen"&gt;Qwen2.5-Omni&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;3B/7B&lt;/td&gt; 
   &lt;td&gt;qwen2_omni&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/Qwen"&gt;Qwen2-VL/Qwen2.5-VL/QVQ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2B/3B/7B/32B/72B&lt;/td&gt; 
   &lt;td&gt;qwen2_vl&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/ByteDance-Seed"&gt;Seed Coder&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;8B&lt;/td&gt; 
   &lt;td&gt;seed_coder&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/Skywork"&gt;Skywork o1&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;8B&lt;/td&gt; 
   &lt;td&gt;skywork_o1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/bigcode"&gt;StarCoder 2&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;3B/7B/15B&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/Tele-AI"&gt;TeleChat2&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;3B/7B/35B/115B&lt;/td&gt; 
   &lt;td&gt;telechat2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/xverse"&gt;XVERSE&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B/13B/65B&lt;/td&gt; 
   &lt;td&gt;xverse&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/01-ai"&gt;Yi/Yi-1.5 (Code)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1.5B/6B/9B/34B&lt;/td&gt; 
   &lt;td&gt;yi&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/01-ai"&gt;Yi-VL&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;6B/34B&lt;/td&gt; 
   &lt;td&gt;yi_vl&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/IEITYuan"&gt;Yuan 2&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2B/51B/102B&lt;/td&gt; 
   &lt;td&gt;yuan&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] For the "base" models, the &lt;code&gt;template&lt;/code&gt; argument can be chosen from &lt;code&gt;default&lt;/code&gt;, &lt;code&gt;alpaca&lt;/code&gt;, &lt;code&gt;vicuna&lt;/code&gt; etc. But make sure to use the &lt;strong&gt;corresponding template&lt;/strong&gt; for the "instruct/chat" models.&lt;/p&gt; 
 &lt;p&gt;Remember to use the &lt;strong&gt;SAME&lt;/strong&gt; template in training and inference.&lt;/p&gt; 
 &lt;p&gt;*: You should install the &lt;code&gt;transformers&lt;/code&gt; from main branch and use &lt;code&gt;DISABLE_VERSION_CHECK=1&lt;/code&gt; to skip version check.&lt;/p&gt; 
 &lt;p&gt;**: You need to install a specific version of &lt;code&gt;transformers&lt;/code&gt; to use the corresponding model.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Please refer to &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/src/llamafactory/extras/constants.py"&gt;constants.py&lt;/a&gt; for a full list of models we supported.&lt;/p&gt; 
&lt;p&gt;You also can add a custom chat template to &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/src/llamafactory/data/template.py"&gt;template.py&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Supported Training Approaches&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Approach&lt;/th&gt; 
   &lt;th&gt;Full-tuning&lt;/th&gt; 
   &lt;th&gt;Freeze-tuning&lt;/th&gt; 
   &lt;th&gt;LoRA&lt;/th&gt; 
   &lt;th&gt;QLoRA&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Pre-Training&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Supervised Fine-Tuning&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Reward Modeling&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;PPO Training&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;DPO Training&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;KTO Training&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ORPO Training&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;SimPO Training&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] The implementation details of PPO can be found in &lt;a href="https://newfacade.github.io/notes-on-reinforcement-learning/17-ppo-trl.html"&gt;this blog&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Provided Datasets&lt;/h2&gt; 
&lt;details&gt;
 &lt;summary&gt;Pre-training datasets&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/data/wiki_demo.txt"&gt;Wiki Demo (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/tiiuae/falcon-refinedweb"&gt;RefinedWeb (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/togethercomputer/RedPajama-Data-V2"&gt;RedPajama V2 (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/olm/olm-wikipedia-20221220"&gt;Wikipedia (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/pleisto/wikipedia-cn-20230720-filtered"&gt;Wikipedia (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/EleutherAI/pile"&gt;Pile (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/Skywork/SkyPile-150B"&gt;SkyPile (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/HuggingFaceFW/fineweb"&gt;FineWeb (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu"&gt;FineWeb-Edu (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/bigcode/the-stack"&gt;The Stack (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/bigcode/starcoderdata"&gt;StarCoder (en)&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Supervised fine-tuning datasets&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/data/identity.json"&gt;Identity (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/tatsu-lab/stanford_alpaca"&gt;Stanford Alpaca (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/ymcui/Chinese-LLaMA-Alpaca-3"&gt;Stanford Alpaca (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM"&gt;Alpaca GPT4 (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/glaiveai/glaive-function-calling-v2"&gt;Glaive Function Calling V2 (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/GAIR/lima"&gt;LIMA (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/JosephusCheung/GuanacoDataset"&gt;Guanaco Dataset (multilingual)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/BelleGroup/train_2M_CN"&gt;BELLE 2M (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/BelleGroup/train_1M_CN"&gt;BELLE 1M (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/BelleGroup/train_0.5M_CN"&gt;BELLE 0.5M (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/BelleGroup/generated_chat_0.4M"&gt;BELLE Dialogue 0.4M (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/BelleGroup/school_math_0.25M"&gt;BELLE School Math 0.25M (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M"&gt;BELLE Multiturn Chat 0.8M (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/thunlp/UltraChat"&gt;UltraChat (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/garage-bAInd/Open-Platypus"&gt;OpenPlatypus (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k"&gt;CodeAlpaca 20k (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/QingyiSi/Alpaca-CoT"&gt;Alpaca CoT (multilingual)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/Open-Orca/OpenOrca"&gt;OpenOrca (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/Open-Orca/SlimOrca"&gt;SlimOrca (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/TIGER-Lab/MathInstruct"&gt;MathInstruct (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M"&gt;Firefly 1.1M (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/wiki_qa"&gt;Wiki QA (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/suolyer/webqa"&gt;Web QA (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/zxbsmk/webnovel_cn"&gt;WebNovel (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/berkeley-nest/Nectar"&gt;Nectar (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data"&gt;deepctrl (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/HasturOfficial/adgen"&gt;Advertise Generating (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/totally-not-an-llm/sharegpt-hyperfiltered-3k"&gt;ShareGPT Hyperfiltered (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/shibing624/sharegpt_gpt4"&gt;ShareGPT4 (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k"&gt;UltraChat 200k (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/THUDM/AgentInstruct"&gt;AgentInstruct (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/lmsys/lmsys-chat-1m"&gt;LMSYS Chat 1M (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_V2_196k"&gt;Evol Instruct V2 (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/HuggingFaceTB/cosmopedia"&gt;Cosmopedia (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/hfl/stem_zh_instruction"&gt;STEM (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/hfl/ruozhiba_gpt4_turbo"&gt;Ruozhiba (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/m-a-p/neo_sft_phase2"&gt;Neo-sft (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/Magpie-Align/Magpie-Pro-300K-Filtered"&gt;Magpie-Pro-300K-Filtered (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/argilla/magpie-ultra-v0.1"&gt;Magpie-ultra-v0.1 (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/TIGER-Lab/WebInstructSub"&gt;WebInstructSub (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT"&gt;OpenO1-SFT (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k"&gt;Open-Thoughts (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/open-r1/OpenR1-Math-220k"&gt;Open-R1-Math (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/Congliu/Chinese-DeepSeek-R1-Distill-data-110k-SFT"&gt;Chinese-DeepSeek-R1-Distill (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/BUAADreamer/llava-en-zh-300k"&gt;LLaVA mixed (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/jugg1024/pokemon-gpt4o-captions"&gt;Pokemon-gpt4o-captions (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/mayflowergmbh/oasst_de"&gt;Open Assistant (de)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/mayflowergmbh/dolly-15k_de"&gt;Dolly 15k (de)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/mayflowergmbh/alpaca-gpt4_de"&gt;Alpaca GPT4 (de)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/mayflowergmbh/openschnabeltier_de"&gt;OpenSchnabeltier (de)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/mayflowergmbh/evol-instruct_de"&gt;Evol Instruct (de)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/mayflowergmbh/dolphin_de"&gt;Dolphin (de)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/mayflowergmbh/booksum_de"&gt;Booksum (de)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/mayflowergmbh/airoboros-3.0_de"&gt;Airoboros (de)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/mayflowergmbh/ultra-chat_de"&gt;Ultrachat (de)&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Preference datasets&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/hiyouga/DPO-En-Zh-20k"&gt;DPO mixed (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized"&gt;UltraFeedback (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/m-a-p/COIG-P"&gt;COIG-P (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/openbmb/RLHF-V-Dataset"&gt;RLHF-V (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/Zhihui/VLFeedback"&gt;VLFeedback (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/openbmb/RLAIF-V-Dataset"&gt;RLAIF-V (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/Intel/orca_dpo_pairs"&gt;Orca DPO Pairs (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/Anthropic/hh-rlhf"&gt;HH-RLHF (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/berkeley-nest/Nectar"&gt;Nectar (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/mayflowergmbh/intel_orca_dpo_pairs_de"&gt;Orca DPO (de)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/argilla/kto-mix-15k"&gt;KTO mixed (en)&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;p&gt;Some datasets require confirmation before using them, so we recommend logging in with your Hugging Face account using these commands.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install --upgrade huggingface_hub
huggingface-cli login
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Requirement&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Mandatory&lt;/th&gt; 
   &lt;th&gt;Minimum&lt;/th&gt; 
   &lt;th&gt;Recommend&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;python&lt;/td&gt; 
   &lt;td&gt;3.9&lt;/td&gt; 
   &lt;td&gt;3.10&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;torch&lt;/td&gt; 
   &lt;td&gt;2.0.0&lt;/td&gt; 
   &lt;td&gt;2.6.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;torchvision&lt;/td&gt; 
   &lt;td&gt;0.15.0&lt;/td&gt; 
   &lt;td&gt;0.21.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;transformers&lt;/td&gt; 
   &lt;td&gt;4.49.0&lt;/td&gt; 
   &lt;td&gt;4.50.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;datasets&lt;/td&gt; 
   &lt;td&gt;2.16.0&lt;/td&gt; 
   &lt;td&gt;3.2.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;accelerate&lt;/td&gt; 
   &lt;td&gt;0.34.0&lt;/td&gt; 
   &lt;td&gt;1.2.1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;peft&lt;/td&gt; 
   &lt;td&gt;0.14.0&lt;/td&gt; 
   &lt;td&gt;0.15.1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;trl&lt;/td&gt; 
   &lt;td&gt;0.8.6&lt;/td&gt; 
   &lt;td&gt;0.9.6&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Optional&lt;/th&gt; 
   &lt;th&gt;Minimum&lt;/th&gt; 
   &lt;th&gt;Recommend&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;CUDA&lt;/td&gt; 
   &lt;td&gt;11.6&lt;/td&gt; 
   &lt;td&gt;12.2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;deepspeed&lt;/td&gt; 
   &lt;td&gt;0.10.0&lt;/td&gt; 
   &lt;td&gt;0.16.4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;bitsandbytes&lt;/td&gt; 
   &lt;td&gt;0.39.0&lt;/td&gt; 
   &lt;td&gt;0.43.1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;vllm&lt;/td&gt; 
   &lt;td&gt;0.4.3&lt;/td&gt; 
   &lt;td&gt;0.8.2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;flash-attn&lt;/td&gt; 
   &lt;td&gt;2.5.6&lt;/td&gt; 
   &lt;td&gt;2.7.2&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Hardware Requirement&lt;/h3&gt; 
&lt;p&gt;* &lt;em&gt;estimated&lt;/em&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Method&lt;/th&gt; 
   &lt;th&gt;Bits&lt;/th&gt; 
   &lt;th&gt;7B&lt;/th&gt; 
   &lt;th&gt;14B&lt;/th&gt; 
   &lt;th&gt;30B&lt;/th&gt; 
   &lt;th&gt;70B&lt;/th&gt; 
   &lt;th&gt;&lt;code&gt;x&lt;/code&gt;B&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Full (&lt;code&gt;bf16&lt;/code&gt; or &lt;code&gt;fp16&lt;/code&gt;)&lt;/td&gt; 
   &lt;td&gt;32&lt;/td&gt; 
   &lt;td&gt;120GB&lt;/td&gt; 
   &lt;td&gt;240GB&lt;/td&gt; 
   &lt;td&gt;600GB&lt;/td&gt; 
   &lt;td&gt;1200GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;18x&lt;/code&gt;GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Full (&lt;code&gt;pure_bf16&lt;/code&gt;)&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;60GB&lt;/td&gt; 
   &lt;td&gt;120GB&lt;/td&gt; 
   &lt;td&gt;300GB&lt;/td&gt; 
   &lt;td&gt;600GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;8x&lt;/code&gt;GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Freeze/LoRA/GaLore/APOLLO/BAdam&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;16GB&lt;/td&gt; 
   &lt;td&gt;32GB&lt;/td&gt; 
   &lt;td&gt;64GB&lt;/td&gt; 
   &lt;td&gt;160GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;2x&lt;/code&gt;GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;QLoRA&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;10GB&lt;/td&gt; 
   &lt;td&gt;20GB&lt;/td&gt; 
   &lt;td&gt;40GB&lt;/td&gt; 
   &lt;td&gt;80GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;x&lt;/code&gt;GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;QLoRA&lt;/td&gt; 
   &lt;td&gt;4&lt;/td&gt; 
   &lt;td&gt;6GB&lt;/td&gt; 
   &lt;td&gt;12GB&lt;/td&gt; 
   &lt;td&gt;24GB&lt;/td&gt; 
   &lt;td&gt;48GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;x/2&lt;/code&gt;GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;QLoRA&lt;/td&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;4GB&lt;/td&gt; 
   &lt;td&gt;8GB&lt;/td&gt; 
   &lt;td&gt;16GB&lt;/td&gt; 
   &lt;td&gt;24GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;x/4&lt;/code&gt;GB&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] Installation is mandatory.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Install from Source&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install -e ".[torch,metrics]" --no-build-isolation
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Extra dependencies available: torch, torch-npu, metrics, deepspeed, liger-kernel, bitsandbytes, hqq, eetq, gptq, aqlm, vllm, sglang, galore, apollo, badam, adam-mini, qwen, minicpm_v, openmind, swanlab, dev&lt;/p&gt; 
&lt;h4&gt;Install from Docker Image&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -it --rm --gpus=all --ipc=host hiyouga/llamafactory:latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This image is built on Ubuntu 22.04 (x86_64), CUDA 12.4, Python 3.11, PyTorch 2.6.0, and Flash-attn 2.7.4.&lt;/p&gt; 
&lt;p&gt;Find the pre-built images: &lt;a href="https://hub.docker.com/r/hiyouga/llamafactory/tags"&gt;https://hub.docker.com/r/hiyouga/llamafactory/tags&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Please refer to &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#build-docker"&gt;build docker&lt;/a&gt; to build the image yourself.&lt;/p&gt; 
&lt;details&gt;
 &lt;summary&gt;Setting up a virtual environment with &lt;b&gt;uv&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;Create an isolated Python environment with &lt;a href="https://github.com/astral-sh/uv"&gt;uv&lt;/a&gt;:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;uv sync --extra torch --extra metrics --prerelease=allow
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Run LLaMA-Factory in the isolated environment:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;uv run --prerelease=allow llamafactory-cli train examples/train_lora/llama3_lora_pretrain.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;For Windows users&lt;/summary&gt; 
 &lt;h4&gt;Install PyTorch&lt;/h4&gt; 
 &lt;p&gt;You need to manually install the GPU version of PyTorch on the Windows platform. Please refer to the &lt;a href="https://pytorch.org/get-started/locally/"&gt;official website&lt;/a&gt; and the following command to install PyTorch with CUDA support:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip uninstall torch torchvision torchaudio
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126
python -c "import torch; print(torch.cuda.is_available())"
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;If you see &lt;code&gt;True&lt;/code&gt; then you have successfully installed PyTorch with CUDA support.&lt;/p&gt; 
 &lt;p&gt;Try &lt;code&gt;dataloader_num_workers: 0&lt;/code&gt; if you encounter &lt;code&gt;Can't pickle local object&lt;/code&gt; error.&lt;/p&gt; 
 &lt;h4&gt;Install BitsAndBytes&lt;/h4&gt; 
 &lt;p&gt;If you want to enable the quantized LoRA (QLoRA) on the Windows platform, you need to install a pre-built version of &lt;code&gt;bitsandbytes&lt;/code&gt; library, which supports CUDA 11.1 to 12.2, please select the appropriate &lt;a href="https://github.com/jllllll/bitsandbytes-windows-webui/releases/tag/wheels"&gt;release version&lt;/a&gt; based on your CUDA version.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.2.post2-py3-none-win_amd64.whl
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Install Flash Attention-2&lt;/h4&gt; 
 &lt;p&gt;To enable FlashAttention-2 on the Windows platform, please use the script from &lt;a href="https://huggingface.co/lldacing/flash-attention-windows-wheel"&gt;flash-attention-windows-wheel&lt;/a&gt; to compile and install it by yourself.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;For Ascend NPU users&lt;/summary&gt; 
 &lt;p&gt;To install LLaMA Factory on Ascend NPU devices, please upgrade Python to version 3.10 or higher and specify extra dependencies: &lt;code&gt;pip install -e ".[torch-npu,metrics]"&lt;/code&gt;. Additionally, you need to install the &lt;strong&gt;&lt;a href="https://www.hiascend.com/developer/download/community/result?module=cann"&gt;Ascend CANN Toolkit and Kernels&lt;/a&gt;&lt;/strong&gt;. Please follow the &lt;a href="https://www.hiascend.com/document/detail/en/CANNCommunityEdition/600alphaX/softwareinstall/instg/atlasdeploy_03_0031.html"&gt;installation tutorial&lt;/a&gt; or use the following commands:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# replace the url according to your CANN version and devices
# install CANN Toolkit
wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C20SPC702/Ascend-cann-toolkit_8.0.0.alpha002_linux-"$(uname -i)".run
bash Ascend-cann-toolkit_8.0.0.alpha002_linux-"$(uname -i)".run --install

# install CANN Kernels
wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C20SPC702/Ascend-cann-kernels-910b_8.0.0.alpha002_linux-"$(uname -i)".run
bash Ascend-cann-kernels-910b_8.0.0.alpha002_linux-"$(uname -i)".run --install

# set env variables
source /usr/local/Ascend/ascend-toolkit/set_env.sh
&lt;/code&gt;&lt;/pre&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Requirement&lt;/th&gt; 
    &lt;th&gt;Minimum&lt;/th&gt; 
    &lt;th&gt;Recommend&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CANN&lt;/td&gt; 
    &lt;td&gt;8.0.RC1&lt;/td&gt; 
    &lt;td&gt;8.0.0.alpha002&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;torch&lt;/td&gt; 
    &lt;td&gt;2.1.0&lt;/td&gt; 
    &lt;td&gt;2.4.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;torch-npu&lt;/td&gt; 
    &lt;td&gt;2.1.0&lt;/td&gt; 
    &lt;td&gt;2.4.0.post2&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;deepspeed&lt;/td&gt; 
    &lt;td&gt;0.13.2&lt;/td&gt; 
    &lt;td&gt;0.13.2&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;vllm-ascend&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;0.7.3&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p&gt;Remember to use &lt;code&gt;ASCEND_RT_VISIBLE_DEVICES&lt;/code&gt; instead of &lt;code&gt;CUDA_VISIBLE_DEVICES&lt;/code&gt; to specify the device to use.&lt;/p&gt; 
 &lt;p&gt;If you cannot infer model on NPU devices, try setting &lt;code&gt;do_sample: false&lt;/code&gt; in the configurations.&lt;/p&gt; 
 &lt;p&gt;Download the pre-built Docker images: &lt;a href="http://mirrors.cn-central-221.ovaijisuan.com/detail/130.html"&gt;32GB&lt;/a&gt; | &lt;a href="http://mirrors.cn-central-221.ovaijisuan.com/detail/131.html"&gt;64GB&lt;/a&gt;&lt;/p&gt; 
 &lt;h4&gt;Install BitsAndBytes&lt;/h4&gt; 
 &lt;p&gt;To use QLoRA based on bitsandbytes on Ascend NPU, please follow these 3 steps:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Manually compile bitsandbytes: Refer to &lt;a href="https://huggingface.co/docs/bitsandbytes/installation?backend=Ascend+NPU&amp;amp;platform=Ascend+NPU"&gt;the installation documentation&lt;/a&gt; for the NPU version of bitsandbytes to complete the compilation and installation. The compilation requires a cmake version of at least 3.22.1 and a g++ version of at least 12.x.&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Install bitsandbytes from source
# Clone bitsandbytes repo, Ascend NPU backend is currently enabled on multi-backend-refactor branch
git clone -b multi-backend-refactor https://github.com/bitsandbytes-foundation/bitsandbytes.git
cd bitsandbytes/

# Install dependencies
pip install -r requirements-dev.txt

# Install the dependencies for the compilation tools. Note that the commands for this step may vary depending on the operating system. The following are provided for reference
apt-get install -y build-essential cmake

# Compile &amp;amp; install  
cmake -DCOMPUTE_BACKEND=npu -S .
make
pip install .
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="2"&gt; 
  &lt;li&gt;Install transformers from the main branch.&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;git clone -b main https://github.com/huggingface/transformers.git
cd transformers
pip install .
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="3"&gt; 
  &lt;li&gt;Set &lt;code&gt;double_quantization: false&lt;/code&gt; in the configuration. You can refer to the &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/train_qlora/llama3_lora_sft_bnb_npu.yaml"&gt;example&lt;/a&gt;.&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;h3&gt;Data Preparation&lt;/h3&gt; 
&lt;p&gt;Please refer to &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/data/README.md"&gt;data/README.md&lt;/a&gt; for checking the details about the format of dataset files. You can use datasets on HuggingFace / ModelScope / Modelers hub, load the dataset in local disk, or specify a path to s3/gcs cloud storage.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Please update &lt;code&gt;data/dataset_info.json&lt;/code&gt; to use your custom dataset.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;You can also use &lt;strong&gt;&lt;a href="https://github.com/ConardLi/easy-dataset"&gt;Easy Dataset&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://github.com/OpenDCAI/DataFlow"&gt;DataFlow&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href="https://github.com/open-sciencelab/GraphGen"&gt;GraphGen&lt;/a&gt;&lt;/strong&gt; to create synthetic data for fine-tuning.&lt;/p&gt; 
&lt;h3&gt;Quickstart&lt;/h3&gt; 
&lt;p&gt;Use the following 3 commands to run LoRA &lt;strong&gt;fine-tuning&lt;/strong&gt;, &lt;strong&gt;inference&lt;/strong&gt; and &lt;strong&gt;merging&lt;/strong&gt; of the Llama3-8B-Instruct model, respectively.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;llamafactory-cli train examples/train_lora/llama3_lora_sft.yaml
llamafactory-cli chat examples/inference/llama3_lora_sft.yaml
llamafactory-cli export examples/merge_lora/llama3_lora_sft.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md"&gt;examples/README.md&lt;/a&gt; for advanced usage (including distributed training).&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] Use &lt;code&gt;llamafactory-cli help&lt;/code&gt; to show help information.&lt;/p&gt; 
 &lt;p&gt;Read &lt;a href="https://github.com/hiyouga/LLaMA-Factory/issues/4614"&gt;FAQs&lt;/a&gt; first if you encounter any problems.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Fine-Tuning with LLaMA Board GUI (powered by &lt;a href="https://github.com/gradio-app/gradio"&gt;Gradio&lt;/a&gt;)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;llamafactory-cli webui
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Build Docker&lt;/h3&gt; 
&lt;p&gt;For CUDA users:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd docker/docker-cuda/
docker compose up -d
docker compose exec llamafactory bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For Ascend NPU users:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd docker/docker-npu/
docker compose up -d
docker compose exec llamafactory bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For AMD ROCm users:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd docker/docker-rocm/
docker compose up -d
docker compose exec llamafactory bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt;
 &lt;summary&gt;Build without Docker Compose&lt;/summary&gt; 
 &lt;p&gt;For CUDA users:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;docker build -f ./docker/docker-cuda/Dockerfile \
    --build-arg PIP_INDEX=https://pypi.org/simple \
    --build-arg EXTRAS=metrics \
    -t llamafactory:latest .

docker run -dit --ipc=host --gpus=all \
    -p 7860:7860 \
    -p 8000:8000 \
    --name llamafactory \
    llamafactory:latest

docker exec -it llamafactory bash
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;For Ascend NPU users:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;docker build -f ./docker/docker-npu/Dockerfile \
    --build-arg PIP_INDEX=https://pypi.org/simple \
    --build-arg EXTRAS=torch-npu,metrics \
    -t llamafactory:latest .

docker run -dit --ipc=host \
    -v /usr/local/dcmi:/usr/local/dcmi \
    -v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \
    -v /usr/local/Ascend/driver:/usr/local/Ascend/driver \
    -v /etc/ascend_install.info:/etc/ascend_install.info \
    -p 7860:7860 \
    -p 8000:8000 \
    --device /dev/davinci0 \
    --device /dev/davinci_manager \
    --device /dev/devmm_svm \
    --device /dev/hisi_hdc \
    --name llamafactory \
    llamafactory:latest

docker exec -it llamafactory bash
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;For AMD ROCm users:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;docker build -f ./docker/docker-rocm/Dockerfile \
    --build-arg PIP_INDEX=https://pypi.org/simple \
    --build-arg EXTRAS=metrics \
    -t llamafactory:latest .

docker run -dit --ipc=host \
    -p 7860:7860 \
    -p 8000:8000 \
    --device /dev/kfd \
    --device /dev/dri \
    --name llamafactory \
    llamafactory:latest

docker exec -it llamafactory bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Use Docker volumes&lt;/summary&gt; 
 &lt;p&gt;You can uncomment &lt;code&gt;VOLUME [ "/root/.cache/huggingface", "/app/shared_data", "/app/output" ]&lt;/code&gt; in the Dockerfile to use data volumes.&lt;/p&gt; 
 &lt;p&gt;When building the Docker image, use &lt;code&gt;-v ./hf_cache:/root/.cache/huggingface&lt;/code&gt; argument to mount the local directory to the container. The following data volumes are available.&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;hf_cache&lt;/code&gt;: Utilize Hugging Face cache on the host machine.&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;shared_data&lt;/code&gt;: The directionary to store datasets on the host machine.&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;output&lt;/code&gt;: Set export dir to this location so that the merged result can be accessed directly on the host machine.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;Deploy with OpenAI-style API and vLLM&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;API_PORT=8000 llamafactory-cli api examples/inference/llama3.yaml infer_backend=vllm vllm_enforce_eager=true
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] Visit &lt;a href="https://platform.openai.com/docs/api-reference/chat/create"&gt;this page&lt;/a&gt; for API document.&lt;/p&gt; 
 &lt;p&gt;Examples: &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/scripts/api_example/test_image.py"&gt;Image understanding&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/scripts/api_example/test_toolcall.py"&gt;Function calling&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Download from ModelScope Hub&lt;/h3&gt; 
&lt;p&gt;If you have trouble with downloading models and datasets from Hugging Face, you can use ModelScope.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export USE_MODELSCOPE_HUB=1 # `set USE_MODELSCOPE_HUB=1` for Windows
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Train the model by specifying a model ID of the ModelScope Hub as the &lt;code&gt;model_name_or_path&lt;/code&gt;. You can find a full list of model IDs at &lt;a href="https://modelscope.cn/models"&gt;ModelScope Hub&lt;/a&gt;, e.g., &lt;code&gt;LLM-Research/Meta-Llama-3-8B-Instruct&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Download from Modelers Hub&lt;/h3&gt; 
&lt;p&gt;You can also use Modelers Hub to download models and datasets.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export USE_OPENMIND_HUB=1 # `set USE_OPENMIND_HUB=1` for Windows
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Train the model by specifying a model ID of the Modelers Hub as the &lt;code&gt;model_name_or_path&lt;/code&gt;. You can find a full list of model IDs at &lt;a href="https://modelers.cn/models"&gt;Modelers Hub&lt;/a&gt;, e.g., &lt;code&gt;TeleAI/TeleChat-7B-pt&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Use W&amp;amp;B Logger&lt;/h3&gt; 
&lt;p&gt;To use &lt;a href="https://wandb.ai"&gt;Weights &amp;amp; Biases&lt;/a&gt; for logging experimental results, you need to add the following arguments to yaml files.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;report_to: wandb
run_name: test_run # optional
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Set &lt;code&gt;WANDB_API_KEY&lt;/code&gt; to &lt;a href="https://wandb.ai/authorize"&gt;your key&lt;/a&gt; when launching training tasks to log in with your W&amp;amp;B account.&lt;/p&gt; 
&lt;h3&gt;Use SwanLab Logger&lt;/h3&gt; 
&lt;p&gt;To use &lt;a href="https://github.com/SwanHubX/SwanLab"&gt;SwanLab&lt;/a&gt; for logging experimental results, you need to add the following arguments to yaml files.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;use_swanlab: true
swanlab_run_name: test_run # optional
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;When launching training tasks, you can log in to SwanLab in three ways:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Add &lt;code&gt;swanlab_api_key=&amp;lt;your_api_key&amp;gt;&lt;/code&gt; to the yaml file, and set it to your &lt;a href="https://swanlab.cn/settings"&gt;API key&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Set the environment variable &lt;code&gt;SWANLAB_API_KEY&lt;/code&gt; to your &lt;a href="https://swanlab.cn/settings"&gt;API key&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Use the &lt;code&gt;swanlab login&lt;/code&gt; command to complete the login.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Projects using LLaMA Factory&lt;/h2&gt; 
&lt;p&gt;If you have a project that should be incorporated, please contact via email or create a pull request.&lt;/p&gt; 
&lt;details&gt;
 &lt;summary&gt;Click to show&lt;/summary&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Wang et al. ESRL: Efficient Sampling-based Reinforcement Learning for Sequence Generation. 2023. &lt;a href="https://arxiv.org/abs/2308.02223"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Yu et al. Open, Closed, or Small Language Models for Text Classification? 2023. &lt;a href="https://arxiv.org/abs/2308.10092"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Wang et al. UbiPhysio: Support Daily Functioning, Fitness, and Rehabilitation with Action Understanding and Feedback in Natural Language. 2023. &lt;a href="https://arxiv.org/abs/2308.10526"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Luceri et al. Leveraging Large Language Models to Detect Influence Campaigns in Social Media. 2023. &lt;a href="https://arxiv.org/abs/2311.07816"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zhang et al. Alleviating Hallucinations of Large Language Models through Induced Hallucinations. 2023. &lt;a href="https://arxiv.org/abs/2312.15710"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Wang et al. Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs. KDD 2024. &lt;a href="https://arxiv.org/abs/2401.04319"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Wang et al. CANDLE: Iterative Conceptualization and Instantiation Distillation from Large Language Models for Commonsense Reasoning. ACL 2024. &lt;a href="https://arxiv.org/abs/2401.07286"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Choi et al. FACT-GPT: Fact-Checking Augmentation via Claim Matching with LLMs. 2024. &lt;a href="https://arxiv.org/abs/2402.05904"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zhang et al. AutoMathText: Autonomous Data Selection with Language Models for Mathematical Texts. 2024. &lt;a href="https://arxiv.org/abs/2402.07625"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Lyu et al. KnowTuning: Knowledge-aware Fine-tuning for Large Language Models. 2024. &lt;a href="https://arxiv.org/abs/2402.11176"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Yang et al. LaCo: Large Language Model Pruning via Layer Collaps. 2024. &lt;a href="https://arxiv.org/abs/2402.11187"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Bhardwaj et al. Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic. 2024. &lt;a href="https://arxiv.org/abs/2402.11746"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Yang et al. Enhancing Empathetic Response Generation by Augmenting LLMs with Small-scale Empathetic Models. 2024. &lt;a href="https://arxiv.org/abs/2402.11801"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Yi et al. Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding. ACL 2024 Findings. &lt;a href="https://arxiv.org/abs/2402.11809"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Cao et al. Head-wise Shareable Attention for Large Language Models. 2024. &lt;a href="https://arxiv.org/abs/2402.11819"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zhang et al. Enhancing Multilingual Capabilities of Large Language Models through Self-Distillation from Resource-Rich Languages. 2024. &lt;a href="https://arxiv.org/abs/2402.12204"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Kim et al. Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models. 2024. &lt;a href="https://arxiv.org/abs/2402.14714"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Yu et al. KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models. ACL 2024. &lt;a href="https://arxiv.org/abs/2402.15043"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Huang et al. Key-Point-Driven Data Synthesis with its Enhancement on Mathematical Reasoning. 2024. &lt;a href="https://arxiv.org/abs/2403.02333"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Duan et al. Negating Negatives: Alignment without Human Positive Samples via Distributional Dispreference Optimization. 2024. &lt;a href="https://arxiv.org/abs/2403.03419"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Xie and Schwertfeger. Empowering Robotics with Large Language Models: osmAG Map Comprehension with LLMs. 2024. &lt;a href="https://arxiv.org/abs/2403.08228"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Wu et al. Large Language Models are Parallel Multilingual Learners. 2024. &lt;a href="https://arxiv.org/abs/2403.09073"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zhang et al. EDT: Improving Large Language Models' Generation by Entropy-based Dynamic Temperature Sampling. 2024. &lt;a href="https://arxiv.org/abs/2403.14541"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Weller et al. FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions. 2024. &lt;a href="https://arxiv.org/abs/2403.15246"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Hongbin Na. CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral Therapy-based Mental Health Question Answering. COLING 2024. &lt;a href="https://arxiv.org/abs/2403.16008"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zan et al. CodeS: Natural Language to Code Repository via Multi-Layer Sketch. 2024. &lt;a href="https://arxiv.org/abs/2403.16443"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Liu et al. Extensive Self-Contrast Enables Feedback-Free Language Model Alignment. 2024. &lt;a href="https://arxiv.org/abs/2404.00604"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Luo et al. BAdam: A Memory Efficient Full Parameter Training Method for Large Language Models. 2024. &lt;a href="https://arxiv.org/abs/2404.02827"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Du et al. Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model. 2024. &lt;a href="https://arxiv.org/abs/2404.04167"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Ma et al. Parameter Efficient Quasi-Orthogonal Fine-Tuning via Givens Rotation. ICML 2024. &lt;a href="https://arxiv.org/abs/2404.04316"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Liu et al. Dynamic Generation of Personalities with Large Language Models. 2024. &lt;a href="https://arxiv.org/abs/2404.07084"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Shang et al. How Far Have We Gone in Stripped Binary Code Understanding Using Large Language Models. 2024. &lt;a href="https://arxiv.org/abs/2404.09836"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Huang et al. LLMTune: Accelerate Database Knob Tuning with Large Language Models. 2024. &lt;a href="https://arxiv.org/abs/2404.11581"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Deng et al. Text-Tuple-Table: Towards Information Integration in Text-to-Table Generation via Global Tuple Extraction. 2024. &lt;a href="https://arxiv.org/abs/2404.14215"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Acikgoz et al. Hippocrates: An Open-Source Framework for Advancing Large Language Models in Healthcare. 2024. &lt;a href="https://arxiv.org/abs/2404.16621"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zhang et al. Small Language Models Need Strong Verifiers to Self-Correct Reasoning. ACL 2024 Findings. &lt;a href="https://arxiv.org/abs/2404.17140"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zhou et al. FREB-TQA: A Fine-Grained Robustness Evaluation Benchmark for Table Question Answering. NAACL 2024. &lt;a href="https://arxiv.org/abs/2404.18585"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Xu et al. Large Language Models for Cyber Security: A Systematic Literature Review. 2024. &lt;a href="https://arxiv.org/abs/2405.04760"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Dammu et al. "They are uncultured": Unveiling Covert Harms and Social Threats in LLM Generated Conversations. 2024. &lt;a href="https://arxiv.org/abs/2405.05378"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Yi et al. A safety realignment framework via subspace-oriented model fusion for large language models. 2024. &lt;a href="https://arxiv.org/abs/2405.09055"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Lou et al. SPO: Multi-Dimensional Preference Sequential Alignment With Implicit Reward Modeling. 2024. &lt;a href="https://arxiv.org/abs/2405.12739"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zhang et al. Getting More from Less: Large Language Models are Good Spontaneous Multilingual Learners. 2024. &lt;a href="https://arxiv.org/abs/2405.13816"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zhang et al. TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models. 2024. &lt;a href="https://arxiv.org/abs/2405.20215"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zihong Chen. Sentence Segmentation and Sentence Punctuation Based on XunziALLM. 2024. &lt;a href="https://aclanthology.org/2024.lt4hala-1.30"&gt;[paper]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Gao et al. The Best of Both Worlds: Toward an Honest and Helpful Large Language Model. 2024. &lt;a href="https://arxiv.org/abs/2406.00380"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Wang and Song. MARS: Benchmarking the Metaphysical Reasoning Abilities of Language Models with a Multi-task Evaluation Dataset. 2024. &lt;a href="https://arxiv.org/abs/2406.02106"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Hu et al. Computational Limits of Low-Rank Adaptation (LoRA) for Transformer-Based Models. 2024. &lt;a href="https://arxiv.org/abs/2406.03136"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Ge et al. Time Sensitive Knowledge Editing through Efficient Finetuning. ACL 2024. &lt;a href="https://arxiv.org/abs/2406.04496"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Tan et al. Peer Review as A Multi-Turn and Long-Context Dialogue with Role-Based Interactions. 2024. &lt;a href="https://arxiv.org/abs/2406.05688"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Song et al. Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters. 2024. &lt;a href="https://arxiv.org/abs/2406.05955"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Gu et al. RWKV-CLIP: A Robust Vision-Language Representation Learner. 2024. &lt;a href="https://arxiv.org/abs/2406.06973"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Chen et al. Advancing Tool-Augmented Large Language Models: Integrating Insights from Errors in Inference Trees. 2024. &lt;a href="https://arxiv.org/abs/2406.07115"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zhu et al. Are Large Language Models Good Statisticians?. 2024. &lt;a href="https://arxiv.org/abs/2406.07815"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Li et al. Know the Unknown: An Uncertainty-Sensitive Method for LLM Instruction Tuning. 2024. &lt;a href="https://arxiv.org/abs/2406.10099"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Ding et al. IntentionQA: A Benchmark for Evaluating Purchase Intention Comprehension Abilities of Language Models in E-commerce. 2024. &lt;a href="https://arxiv.org/abs/2406.10173"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;He et al. COMMUNITY-CROSS-INSTRUCT: Unsupervised Instruction Generation for Aligning Large Language Models to Online Communities. 2024. &lt;a href="https://arxiv.org/abs/2406.12074"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Lin et al. FVEL: Interactive Formal Verification Environment with Large Language Models via Theorem Proving. 2024. &lt;a href="https://arxiv.org/abs/2406.14408"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Treutlein et al. Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from Disparate Training Data. 2024. &lt;a href="https://arxiv.org/abs/2406.14546"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Feng et al. SS-Bench: A Benchmark for Social Story Generation and Evaluation. 2024. &lt;a href="https://arxiv.org/abs/2406.15695"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Feng et al. Self-Constructed Context Decompilation with Fined-grained Alignment Enhancement. 2024. &lt;a href="https://arxiv.org/abs/2406.17233"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Liu et al. Large Language Models for Cuffless Blood Pressure Measurement From Wearable Biosignals. 2024. &lt;a href="https://arxiv.org/abs/2406.18069"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Iyer et al. Exploring Very Low-Resource Translation with LLMs: The University of Edinburgh's Submission to AmericasNLP 2024 Translation Task. AmericasNLP 2024. &lt;a href="https://aclanthology.org/2024.americasnlp-1.25"&gt;[paper]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Li et al. Calibrating LLMs with Preference Optimization on Thought Trees for Generating Rationale in Science Question Scoring. 2024. &lt;a href="https://arxiv.org/abs/2406.19949"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Yang et al. Financial Knowledge Large Language Model. 2024. &lt;a href="https://arxiv.org/abs/2407.00365"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Lin et al. DogeRM: Equipping Reward Models with Domain Knowledge through Model Merging. 2024. &lt;a href="https://arxiv.org/abs/2407.01470"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Bako et al. Evaluating the Semantic Profiling Abilities of LLMs for Natural Language Utterances in Data Visualization. 2024. &lt;a href="https://arxiv.org/abs/2407.06129"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Huang et al. RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization. 2024. &lt;a href="https://arxiv.org/abs/2407.08044"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Jiang et al. LLM-Collaboration on Automatic Science Journalism for the General Audience. 2024. &lt;a href="https://arxiv.org/abs/2407.09756"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Inouye et al. Applied Auto-tuning on LoRA Hyperparameters. 2024. &lt;a href="https://scholarcommons.scu.edu/cseng_senior/272/"&gt;[paper]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Qi et al. Research on Tibetan Tourism Viewpoints information generation system based on LLM. 2024. &lt;a href="https://arxiv.org/abs/2407.13561"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Xu et al. Course-Correction: Safety Alignment Using Synthetic Preferences. 2024. &lt;a href="https://arxiv.org/abs/2407.16637"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Sun et al. LAMBDA: A Large Model Based Data Agent. 2024. &lt;a href="https://arxiv.org/abs/2407.17535"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zhu et al. CollectiveSFT: Scaling Large Language Models for Chinese Medical Benchmark with Collective Instructions in Healthcare. 2024. &lt;a href="https://arxiv.org/abs/2407.19705"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Yu et al. Correcting Negative Bias in Large Language Models through Negative Attention Score Alignment. 2024. &lt;a href="https://arxiv.org/abs/2408.00137"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Xie et al. The Power of Personalized Datasets: Advancing Chinese Composition Writing for Elementary School through Targeted Model Fine-Tuning. IALP 2024. &lt;a href="https://www.asianlp.sg/conferences/ialp2024/proceedings/papers/IALP2024_P055.pdf"&gt;[paper]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Liu et al. Instruct-Code-Llama: Improving Capabilities of Language Model in Competition Level Code Generation by Online Judge Feedback. ICIC 2024. &lt;a href="https://link.springer.com/chapter/10.1007/978-981-97-5669-8_11"&gt;[paper]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Wang et al. Cybernetic Sentinels: Unveiling the Impact of Safety Data Selection on Model Security in Supervised Fine-Tuning. ICIC 2024. &lt;a href="https://link.springer.com/chapter/10.1007/978-981-97-5669-8_23"&gt;[paper]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Xia et al. Understanding the Performance and Estimating the Cost of LLM Fine-Tuning. 2024. &lt;a href="https://arxiv.org/abs/2408.04693"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zeng et al. Perceive, Reflect, and Plan: Designing LLM Agent for Goal-Directed City Navigation without Instructions. 2024. &lt;a href="https://arxiv.org/abs/2408.04168"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Xia et al. Using Pre-trained Language Model for Accurate ESG Prediction. FinNLP 2024. &lt;a href="https://aclanthology.org/2024.finnlp-2.1/"&gt;[paper]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Liang et al. I-SHEEP: Self-Alignment of LLM from Scratch through an Iterative Self-Enhancement Paradigm. 2024. &lt;a href="https://arxiv.org/abs/2408.08072"&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Bai et al. Aligning Large Language Model with Direct Multi-Preference Optimization for Recommendation. CIKM 2024. &lt;a href="https://dl.acm.org/doi/10.1145/3627673.3679611"&gt;[paper]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zhang et al. CPsyCoun: A Report-based Multi-turn Dialogue Reconstruction and Evaluation Framework for Chinese Psychological Counseling. ACL 2024. &lt;a href="https://aclanthology.org/2024.findings-acl.830.pdf"&gt;[paper]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/Yu-Yang-Li/StarWhisper"&gt;StarWhisper&lt;/a&gt;&lt;/strong&gt;: A large language model for Astronomy, based on ChatGLM2-6B and Qwen-14B.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/FudanDISC/DISC-LawLLM"&gt;DISC-LawLLM&lt;/a&gt;&lt;/strong&gt;: A large language model specialized in Chinese legal domain, based on Baichuan-13B, is capable of retrieving and reasoning on legal knowledge.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/X-D-Lab/Sunsimiao"&gt;Sunsimiao&lt;/a&gt;&lt;/strong&gt;: A large language model specialized in Chinese medical domain, based on Baichuan-7B and ChatGLM-6B.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/WangRongsheng/CareGPT"&gt;CareGPT&lt;/a&gt;&lt;/strong&gt;: A series of large language models for Chinese medical domain, based on LLaMA2-7B and Baichuan-13B.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/PKU-YuanGroup/Machine-Mindset/"&gt;MachineMindset&lt;/a&gt;&lt;/strong&gt;: A series of MBTI Personality large language models, capable of giving any LLM 16 different personality types based on different datasets and training methods.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://huggingface.co/Nekochu/Luminia-13B-v3"&gt;Luminia-13B-v3&lt;/a&gt;&lt;/strong&gt;: A large language model specialized in generate metadata for stable diffusion. &lt;a href="https://huggingface.co/spaces/Nekochu/Luminia-13B_SD_Prompt"&gt;[demo]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/BUAADreamer/Chinese-LLaVA-Med"&gt;Chinese-LLaVA-Med&lt;/a&gt;&lt;/strong&gt;: A multimodal large language model specialized in Chinese medical domain, based on LLaVA-1.5-7B.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/THUDM/AutoRE"&gt;AutoRE&lt;/a&gt;&lt;/strong&gt;: A document-level relation extraction system based on large language models.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/NVIDIA/RTX-AI-Toolkit"&gt;NVIDIA RTX AI Toolkit&lt;/a&gt;&lt;/strong&gt;: SDKs for fine-tuning LLMs on Windows PC for NVIDIA RTX.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/LazyAGI/LazyLLM"&gt;LazyLLM&lt;/a&gt;&lt;/strong&gt;: An easy and lazy way for building multi-agent LLMs applications and supports model fine-tuning via LLaMA Factory.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/NLPJCL/RAG-Retrieval"&gt;RAG-Retrieval&lt;/a&gt;&lt;/strong&gt;: A full pipeline for RAG retrieval model fine-tuning, inference, and distillation. &lt;a href="https://zhuanlan.zhihu.com/p/987727357"&gt;[blog]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/Qihoo360/360-LLaMA-Factory"&gt;360-LLaMA-Factory&lt;/a&gt;&lt;/strong&gt;: A modified library that supports long sequence SFT &amp;amp; DPO using ring attention.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://novasky-ai.github.io/posts/sky-t1/"&gt;Sky-T1&lt;/a&gt;&lt;/strong&gt;: An o1-like model fine-tuned by NovaSky AI with very small cost.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/xming521/WeClone"&gt;WeClone&lt;/a&gt;&lt;/strong&gt;: One-stop solution for creating your digital avatar from chat logs.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/SmartFlowAI/EmoLLM"&gt;EmoLLM&lt;/a&gt;&lt;/strong&gt;: A project about large language models (LLMs) and mental health.&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This repository is licensed under the &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/LICENSE"&gt;Apache-2.0 License&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Please follow the model licenses to use the corresponding model weights: &lt;a href="https://huggingface.co/baichuan-inc/Baichuan2-7B-Base/blob/main/Community%20License%20for%20Baichuan%202%20Model.pdf"&gt;Baichuan 2&lt;/a&gt; / &lt;a href="https://huggingface.co/spaces/bigscience/license"&gt;BLOOM&lt;/a&gt; / &lt;a href="https://github.com/THUDM/ChatGLM3/raw/main/MODEL_LICENSE"&gt;ChatGLM3&lt;/a&gt; / &lt;a href="https://cohere.com/c4ai-cc-by-nc-license"&gt;Command R&lt;/a&gt; / &lt;a href="https://github.com/deepseek-ai/DeepSeek-LLM/raw/main/LICENSE-MODEL"&gt;DeepSeek&lt;/a&gt; / &lt;a href="https://huggingface.co/tiiuae/falcon-180B/blob/main/LICENSE.txt"&gt;Falcon&lt;/a&gt; / &lt;a href="https://ai.google.dev/gemma/terms"&gt;Gemma&lt;/a&gt; / &lt;a href="https://huggingface.co/THUDM/glm-4-9b/blob/main/LICENSE"&gt;GLM-4&lt;/a&gt; / &lt;a href="https://github.com/openai/gpt-2/raw/master/LICENSE"&gt;GPT-2&lt;/a&gt; / &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/LICENSE"&gt;Granite&lt;/a&gt; / &lt;a href="https://huggingface.co/IndexTeam/Index-1.9B/blob/main/LICENSE"&gt;Index&lt;/a&gt; / &lt;a href="https://github.com/InternLM/InternLM#license"&gt;InternLM&lt;/a&gt; / &lt;a href="https://github.com/facebookresearch/llama/raw/main/MODEL_CARD.md"&gt;Llama&lt;/a&gt; / &lt;a href="https://ai.meta.com/llama/license/"&gt;Llama 2&lt;/a&gt; / &lt;a href="https://llama.meta.com/llama3/license/"&gt;Llama 3&lt;/a&gt; / &lt;a href="https://github.com/meta-llama/llama-models/raw/main/models/llama4/LICENSE"&gt;Llama 4&lt;/a&gt; / &lt;a href="https://github.com/OpenBMB/MiniCPM/raw/main/MiniCPM%20Model%20License.md"&gt;MiniCPM&lt;/a&gt; / &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/LICENSE"&gt;Mistral/Mixtral/Pixtral&lt;/a&gt; / &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/LICENSE"&gt;OLMo&lt;/a&gt; / &lt;a href="https://huggingface.co/microsoft/phi-1_5/resolve/main/Research%20License.docx"&gt;Phi-1.5/Phi-2&lt;/a&gt; / &lt;a href="https://huggingface.co/microsoft/Phi-3-mini-4k-instruct/blob/main/LICENSE"&gt;Phi-3/Phi-4&lt;/a&gt; / &lt;a href="https://github.com/QwenLM/Qwen/raw/main/Tongyi%20Qianwen%20LICENSE%20AGREEMENT"&gt;Qwen&lt;/a&gt; / &lt;a href="https://huggingface.co/Skywork/Skywork-13B-base/blob/main/Skywork%20Community%20License.pdf"&gt;Skywork&lt;/a&gt; / &lt;a href="https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement"&gt;StarCoder 2&lt;/a&gt; / &lt;a href="https://huggingface.co/Tele-AI/telechat-7B/blob/main/TeleChat%E6%A8%A1%E5%9E%8B%E7%A4%BE%E5%8C%BA%E8%AE%B8%E5%8F%AF%E5%8D%8F%E8%AE%AE.pdf"&gt;TeleChat2&lt;/a&gt; / &lt;a href="https://github.com/xverse-ai/XVERSE-13B/raw/main/MODEL_LICENSE.pdf"&gt;XVERSE&lt;/a&gt; / &lt;a href="https://huggingface.co/01-ai/Yi-6B/blob/main/LICENSE"&gt;Yi&lt;/a&gt; / &lt;a href="https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/LICENSE"&gt;Yi-1.5&lt;/a&gt; / &lt;a href="https://github.com/IEIT-Yuan/Yuan-2.0/raw/main/LICENSE-Yuan"&gt;Yuan 2&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If this work is helpful, please kindly cite as:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@inproceedings{zheng2024llamafactory,
  title={LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models},
  author={Yaowei Zheng and Richong Zhang and Junhao Zhang and Yanhan Ye and Zheyan Luo and Zhangchi Feng and Yongqiang Ma},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)},
  address={Bangkok, Thailand},
  publisher={Association for Computational Linguistics},
  year={2024},
  url={http://arxiv.org/abs/2403.13372}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Acknowledgement&lt;/h2&gt; 
&lt;p&gt;This repo benefits from &lt;a href="https://github.com/huggingface/peft"&gt;PEFT&lt;/a&gt;, &lt;a href="https://github.com/huggingface/trl"&gt;TRL&lt;/a&gt;, &lt;a href="https://github.com/artidoro/qlora"&gt;QLoRA&lt;/a&gt; and &lt;a href="https://github.com/lm-sys/FastChat"&gt;FastChat&lt;/a&gt;. Thanks for their wonderful works.&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://api.star-history.com/svg?repos=hiyouga/LLaMA-Factory&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>modelcontextprotocol/python-sdk</title>
      <link>https://github.com/modelcontextprotocol/python-sdk</link>
      <description>&lt;p&gt;The official Python SDK for Model Context Protocol servers and clients&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MCP Python SDK&lt;/h1&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;strong&gt;Python implementation of the Model Context Protocol (MCP)&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://pypi.org/project/mcp/"&gt;&lt;img src="https://img.shields.io/pypi/v/mcp.svg?sanitize=true" alt="PyPI" /&gt;&lt;/a&gt; &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/pypi/l/mcp.svg?sanitize=true" alt="MIT licensed" /&gt;&lt;/a&gt; &lt;a href="https://www.python.org/downloads/"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/mcp.svg?sanitize=true" alt="Python Version" /&gt;&lt;/a&gt; &lt;a href="https://modelcontextprotocol.io"&gt;&lt;img src="https://img.shields.io/badge/docs-modelcontextprotocol.io-blue.svg?sanitize=true" alt="Documentation" /&gt;&lt;/a&gt; &lt;a href="https://spec.modelcontextprotocol.io"&gt;&lt;img src="https://img.shields.io/badge/spec-spec.modelcontextprotocol.io-blue.svg?sanitize=true" alt="Specification" /&gt;&lt;/a&gt; &lt;a href="https://github.com/modelcontextprotocol/python-sdk/discussions"&gt;&lt;img src="https://img.shields.io/github/discussions/modelcontextprotocol/python-sdk" alt="GitHub Discussions" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;!-- omit in toc --&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#mcp-python-sdk"&gt;MCP Python SDK&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#overview"&gt;Overview&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#installation"&gt;Installation&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#adding-mcp-to-your-python-project"&gt;Adding MCP to your python project&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#running-the-standalone-mcp-development-tools"&gt;Running the standalone MCP development tools&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#quickstart"&gt;Quickstart&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#what-is-mcp"&gt;What is MCP?&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#core-concepts"&gt;Core Concepts&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#server"&gt;Server&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#resources"&gt;Resources&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#tools"&gt;Tools&lt;/a&gt; 
      &lt;ul&gt; 
       &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#structured-output"&gt;Structured Output&lt;/a&gt;&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#prompts"&gt;Prompts&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#images"&gt;Images&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#context"&gt;Context&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#completions"&gt;Completions&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#elicitation"&gt;Elicitation&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#sampling"&gt;Sampling&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#logging-and-notifications"&gt;Logging and Notifications&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#authentication"&gt;Authentication&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#fastmcp-properties"&gt;FastMCP Properties&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#session-properties-and-methods"&gt;Session Properties&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#request-context-properties"&gt;Request Context Properties&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#running-your-server"&gt;Running Your Server&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#development-mode"&gt;Development Mode&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#claude-desktop-integration"&gt;Claude Desktop Integration&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#direct-execution"&gt;Direct Execution&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#streamable-http-transport"&gt;Streamable HTTP Transport&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#mounting-to-an-existing-asgi-server"&gt;Mounting to an Existing ASGI Server&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#advanced-usage"&gt;Advanced Usage&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#low-level-server"&gt;Low-Level Server&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#writing-mcp-clients"&gt;Writing MCP Clients&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#client-display-utilities"&gt;Client Display Utilities&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#oauth-authentication-for-clients"&gt;OAuth Authentication for Clients&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#parsing-tool-results"&gt;Parsing Tool Results&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#mcp-primitives"&gt;MCP Primitives&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#server-capabilities"&gt;Server Capabilities&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#documentation"&gt;Documentation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Overview&lt;/h2&gt; 
&lt;p&gt;The Model Context Protocol allows applications to provide context for LLMs in a standardized way, separating the concerns of providing context from the actual LLM interaction. This Python SDK implements the full MCP specification, making it easy to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Build MCP clients that can connect to any MCP server&lt;/li&gt; 
 &lt;li&gt;Create MCP servers that expose resources, prompts and tools&lt;/li&gt; 
 &lt;li&gt;Use standard transports like stdio, SSE, and Streamable HTTP&lt;/li&gt; 
 &lt;li&gt;Handle all MCP protocol messages and lifecycle events&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;Adding MCP to your python project&lt;/h3&gt; 
&lt;p&gt;We recommend using &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt; to manage your Python projects.&lt;/p&gt; 
&lt;p&gt;If you haven't created a uv-managed project yet, create one:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv init mcp-server-demo
cd mcp-server-demo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then add MCP to your project dependencies:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv add "mcp[cli]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, for projects using pip for dependencies:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install "mcp[cli]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Running the standalone MCP development tools&lt;/h3&gt; 
&lt;p&gt;To run the mcp command with uv:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run mcp
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;p&gt;Let's create a simple MCP server that exposes a calculator tool and some data:&lt;/p&gt; 
&lt;!-- snippet-source examples/snippets/servers/fastmcp_quickstart.py --&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;"""
FastMCP quickstart example.

cd to the `examples/snippets/clients` directory and run:
    uv run server fastmcp_quickstart stdio
"""

from mcp.server.fastmcp import FastMCP

# Create an MCP server
mcp = FastMCP("Demo")


# Add an addition tool
@mcp.tool()
def add(a: int, b: int) -&amp;gt; int:
    """Add two numbers"""
    return a + b


# Add a dynamic greeting resource
@mcp.resource("greeting://{name}")
def get_greeting(name: str) -&amp;gt; str:
    """Get a personalized greeting"""
    return f"Hello, {name}!"


# Add a prompt
@mcp.prompt()
def greet_user(name: str, style: str = "friendly") -&amp;gt; str:
    """Generate a greeting prompt"""
    styles = {
        "friendly": "Please write a warm, friendly greeting",
        "formal": "Please write a formal, professional greeting",
        "casual": "Please write a casual, relaxed greeting",
    }

    return f"{styles.get(style, styles['friendly'])} for someone named {name}."
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Full example: &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/examples/snippets/servers/fastmcp_quickstart.py"&gt;examples/snippets/servers/fastmcp_quickstart.py&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;!-- /snippet-source --&gt; 
&lt;p&gt;You can install this server in &lt;a href="https://claude.ai/download"&gt;Claude Desktop&lt;/a&gt; and interact with it right away by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run mcp install server.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, you can test it with the MCP Inspector:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run mcp dev server.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;What is MCP?&lt;/h2&gt; 
&lt;p&gt;The &lt;a href="https://modelcontextprotocol.io"&gt;Model Context Protocol (MCP)&lt;/a&gt; lets you build servers that expose data and functionality to LLM applications in a secure, standardized way. Think of it like a web API, but specifically designed for LLM interactions. MCP servers can:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Expose data through &lt;strong&gt;Resources&lt;/strong&gt; (think of these sort of like GET endpoints; they are used to load information into the LLM's context)&lt;/li&gt; 
 &lt;li&gt;Provide functionality through &lt;strong&gt;Tools&lt;/strong&gt; (sort of like POST endpoints; they are used to execute code or otherwise produce a side effect)&lt;/li&gt; 
 &lt;li&gt;Define interaction patterns through &lt;strong&gt;Prompts&lt;/strong&gt; (reusable templates for LLM interactions)&lt;/li&gt; 
 &lt;li&gt;And more!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Core Concepts&lt;/h2&gt; 
&lt;h3&gt;Server&lt;/h3&gt; 
&lt;p&gt;The FastMCP server is your core interface to the MCP protocol. It handles connection management, protocol compliance, and message routing:&lt;/p&gt; 
&lt;!-- snippet-source examples/snippets/servers/lifespan_example.py --&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;"""Example showing lifespan support for startup/shutdown with strong typing."""

from collections.abc import AsyncIterator
from contextlib import asynccontextmanager
from dataclasses import dataclass

from mcp.server.fastmcp import Context, FastMCP
from mcp.server.session import ServerSession


# Mock database class for example
class Database:
    """Mock database class for example."""

    @classmethod
    async def connect(cls) -&amp;gt; "Database":
        """Connect to database."""
        return cls()

    async def disconnect(self) -&amp;gt; None:
        """Disconnect from database."""
        pass

    def query(self) -&amp;gt; str:
        """Execute a query."""
        return "Query result"


@dataclass
class AppContext:
    """Application context with typed dependencies."""

    db: Database


@asynccontextmanager
async def app_lifespan(server: FastMCP) -&amp;gt; AsyncIterator[AppContext]:
    """Manage application lifecycle with type-safe context."""
    # Initialize on startup
    db = await Database.connect()
    try:
        yield AppContext(db=db)
    finally:
        # Cleanup on shutdown
        await db.disconnect()


# Pass lifespan to server
mcp = FastMCP("My App", lifespan=app_lifespan)


# Access type-safe lifespan context in tools
@mcp.tool()
def query_db(ctx: Context[ServerSession, AppContext]) -&amp;gt; str:
    """Tool that uses initialized resources."""
    db = ctx.request_context.lifespan_context.db
    return db.query()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Full example: &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/examples/snippets/servers/lifespan_example.py"&gt;examples/snippets/servers/lifespan_example.py&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;!-- /snippet-source --&gt; 
&lt;h3&gt;Resources&lt;/h3&gt; 
&lt;p&gt;Resources are how you expose data to LLMs. They're similar to GET endpoints in a REST API - they provide data but shouldn't perform significant computation or have side effects:&lt;/p&gt; 
&lt;!-- snippet-source examples/snippets/servers/basic_resource.py --&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mcp.server.fastmcp import FastMCP

mcp = FastMCP(name="Resource Example")


@mcp.resource("file://documents/{name}")
def read_document(name: str) -&amp;gt; str:
    """Read a document by name."""
    # This would normally read from disk
    return f"Content of {name}"


@mcp.resource("config://settings")
def get_settings() -&amp;gt; str:
    """Get application settings."""
    return """{
  "theme": "dark",
  "language": "en",
  "debug": false
}"""
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Full example: &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/examples/snippets/servers/basic_resource.py"&gt;examples/snippets/servers/basic_resource.py&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;!-- /snippet-source --&gt; 
&lt;h3&gt;Tools&lt;/h3&gt; 
&lt;p&gt;Tools let LLMs take actions through your server. Unlike resources, tools are expected to perform computation and have side effects:&lt;/p&gt; 
&lt;!-- snippet-source examples/snippets/servers/basic_tool.py --&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mcp.server.fastmcp import FastMCP

mcp = FastMCP(name="Tool Example")


@mcp.tool()
def sum(a: int, b: int) -&amp;gt; int:
    """Add two numbers together."""
    return a + b


@mcp.tool()
def get_weather(city: str, unit: str = "celsius") -&amp;gt; str:
    """Get weather for a city."""
    # This would normally call a weather API
    return f"Weather in {city}: 22degrees{unit[0].upper()}"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Full example: &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/examples/snippets/servers/basic_tool.py"&gt;examples/snippets/servers/basic_tool.py&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;!-- /snippet-source --&gt; 
&lt;p&gt;Tools can optionally receive a Context object by including a parameter with the &lt;code&gt;Context&lt;/code&gt; type annotation. This context is automatically injected by the FastMCP framework and provides access to MCP capabilities:&lt;/p&gt; 
&lt;!-- snippet-source examples/snippets/servers/tool_progress.py --&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mcp.server.fastmcp import Context, FastMCP
from mcp.server.session import ServerSession

mcp = FastMCP(name="Progress Example")


@mcp.tool()
async def long_running_task(task_name: str, ctx: Context[ServerSession, None], steps: int = 5) -&amp;gt; str:
    """Execute a task with progress updates."""
    await ctx.info(f"Starting: {task_name}")

    for i in range(steps):
        progress = (i + 1) / steps
        await ctx.report_progress(
            progress=progress,
            total=1.0,
            message=f"Step {i + 1}/{steps}",
        )
        await ctx.debug(f"Completed step {i + 1}")

    return f"Task '{task_name}' completed"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Full example: &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/examples/snippets/servers/tool_progress.py"&gt;examples/snippets/servers/tool_progress.py&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;!-- /snippet-source --&gt; 
&lt;h4&gt;Structured Output&lt;/h4&gt; 
&lt;p&gt;Tools will return structured results by default, if their return type annotation is compatible. Otherwise, they will return unstructured results.&lt;/p&gt; 
&lt;p&gt;Structured output supports these return types:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Pydantic models (BaseModel subclasses)&lt;/li&gt; 
 &lt;li&gt;TypedDicts&lt;/li&gt; 
 &lt;li&gt;Dataclasses and other classes with type hints&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;dict[str, T]&lt;/code&gt; (where T is any JSON-serializable type)&lt;/li&gt; 
 &lt;li&gt;Primitive types (str, int, float, bool, bytes, None) - wrapped in &lt;code&gt;{"result": value}&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Generic types (list, tuple, Union, Optional, etc.) - wrapped in &lt;code&gt;{"result": value}&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Classes without type hints cannot be serialized for structured output. Only classes with properly annotated attributes will be converted to Pydantic models for schema generation and validation.&lt;/p&gt; 
&lt;p&gt;Structured results are automatically validated against the output schema generated from the annotation. This ensures the tool returns well-typed, validated data that clients can easily process.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; For backward compatibility, unstructured results are also returned. Unstructured results are provided for backward compatibility with previous versions of the MCP specification, and are quirks-compatible with previous versions of FastMCP in the current version of the SDK.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; In cases where a tool function's return type annotation causes the tool to be classified as structured &lt;em&gt;and this is undesirable&lt;/em&gt;, the classification can be suppressed by passing &lt;code&gt;structured_output=False&lt;/code&gt; to the &lt;code&gt;@tool&lt;/code&gt; decorator.&lt;/p&gt; 
&lt;!-- snippet-source examples/snippets/servers/structured_output.py --&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;"""Example showing structured output with tools."""

from typing import TypedDict

from pydantic import BaseModel, Field

from mcp.server.fastmcp import FastMCP

mcp = FastMCP("Structured Output Example")


# Using Pydantic models for rich structured data
class WeatherData(BaseModel):
    """Weather information structure."""

    temperature: float = Field(description="Temperature in Celsius")
    humidity: float = Field(description="Humidity percentage")
    condition: str
    wind_speed: float


@mcp.tool()
def get_weather(city: str) -&amp;gt; WeatherData:
    """Get weather for a city - returns structured data."""
    # Simulated weather data
    return WeatherData(
        temperature=72.5,
        humidity=45.0,
        condition="sunny",
        wind_speed=5.2,
    )


# Using TypedDict for simpler structures
class LocationInfo(TypedDict):
    latitude: float
    longitude: float
    name: str


@mcp.tool()
def get_location(address: str) -&amp;gt; LocationInfo:
    """Get location coordinates"""
    return LocationInfo(latitude=51.5074, longitude=-0.1278, name="London, UK")


# Using dict[str, Any] for flexible schemas
@mcp.tool()
def get_statistics(data_type: str) -&amp;gt; dict[str, float]:
    """Get various statistics"""
    return {"mean": 42.5, "median": 40.0, "std_dev": 5.2}


# Ordinary classes with type hints work for structured output
class UserProfile:
    name: str
    age: int
    email: str | None = None

    def __init__(self, name: str, age: int, email: str | None = None):
        self.name = name
        self.age = age
        self.email = email


@mcp.tool()
def get_user(user_id: str) -&amp;gt; UserProfile:
    """Get user profile - returns structured data"""
    return UserProfile(name="Alice", age=30, email="alice@example.com")


# Classes WITHOUT type hints cannot be used for structured output
class UntypedConfig:
    def __init__(self, setting1, setting2):  # type: ignore[reportMissingParameterType]
        self.setting1 = setting1
        self.setting2 = setting2


@mcp.tool()
def get_config() -&amp;gt; UntypedConfig:
    """This returns unstructured output - no schema generated"""
    return UntypedConfig("value1", "value2")


# Lists and other types are wrapped automatically
@mcp.tool()
def list_cities() -&amp;gt; list[str]:
    """Get a list of cities"""
    return ["London", "Paris", "Tokyo"]
    # Returns: {"result": ["London", "Paris", "Tokyo"]}


@mcp.tool()
def get_temperature(city: str) -&amp;gt; float:
    """Get temperature as a simple float"""
    return 22.5
    # Returns: {"result": 22.5}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Full example: &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/examples/snippets/servers/structured_output.py"&gt;examples/snippets/servers/structured_output.py&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;!-- /snippet-source --&gt; 
&lt;h3&gt;Prompts&lt;/h3&gt; 
&lt;p&gt;Prompts are reusable templates that help LLMs interact with your server effectively:&lt;/p&gt; 
&lt;!-- snippet-source examples/snippets/servers/basic_prompt.py --&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mcp.server.fastmcp import FastMCP
from mcp.server.fastmcp.prompts import base

mcp = FastMCP(name="Prompt Example")


@mcp.prompt(title="Code Review")
def review_code(code: str) -&amp;gt; str:
    return f"Please review this code:\n\n{code}"


@mcp.prompt(title="Debug Assistant")
def debug_error(error: str) -&amp;gt; list[base.Message]:
    return [
        base.UserMessage("I'm seeing this error:"),
        base.UserMessage(error),
        base.AssistantMessage("I'll help debug that. What have you tried so far?"),
    ]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Full example: &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/examples/snippets/servers/basic_prompt.py"&gt;examples/snippets/servers/basic_prompt.py&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;!-- /snippet-source --&gt; 
&lt;h3&gt;Images&lt;/h3&gt; 
&lt;p&gt;FastMCP provides an &lt;code&gt;Image&lt;/code&gt; class that automatically handles image data:&lt;/p&gt; 
&lt;!-- snippet-source examples/snippets/servers/images.py --&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;"""Example showing image handling with FastMCP."""

from PIL import Image as PILImage

from mcp.server.fastmcp import FastMCP, Image

mcp = FastMCP("Image Example")


@mcp.tool()
def create_thumbnail(image_path: str) -&amp;gt; Image:
    """Create a thumbnail from an image"""
    img = PILImage.open(image_path)
    img.thumbnail((100, 100))
    return Image(data=img.tobytes(), format="png")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Full example: &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/examples/snippets/servers/images.py"&gt;examples/snippets/servers/images.py&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;!-- /snippet-source --&gt; 
&lt;h3&gt;Context&lt;/h3&gt; 
&lt;p&gt;The Context object is automatically injected into tool and resource functions that request it via type hints. It provides access to MCP capabilities like logging, progress reporting, resource reading, user interaction, and request metadata.&lt;/p&gt; 
&lt;h4&gt;Getting Context in Functions&lt;/h4&gt; 
&lt;p&gt;To use context in a tool or resource function, add a parameter with the &lt;code&gt;Context&lt;/code&gt; type annotation:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mcp.server.fastmcp import Context, FastMCP

mcp = FastMCP(name="Context Example")


@mcp.tool()
async def my_tool(x: int, ctx: Context) -&amp;gt; str:
    """Tool that uses context capabilities."""
    # The context parameter can have any name as long as it's type-annotated
    return await process_with_context(x, ctx)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Context Properties and Methods&lt;/h4&gt; 
&lt;p&gt;The Context object provides the following capabilities:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;ctx.request_id&lt;/code&gt; - Unique ID for the current request&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ctx.client_id&lt;/code&gt; - Client ID if available&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ctx.fastmcp&lt;/code&gt; - Access to the FastMCP server instance (see &lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#fastmcp-properties"&gt;FastMCP Properties&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ctx.session&lt;/code&gt; - Access to the underlying session for advanced communication (see &lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#session-properties-and-methods"&gt;Session Properties and Methods&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ctx.request_context&lt;/code&gt; - Access to request-specific data and lifespan resources (see &lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#request-context-properties"&gt;Request Context Properties&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;await ctx.debug(message)&lt;/code&gt; - Send debug log message&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;await ctx.info(message)&lt;/code&gt; - Send info log message&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;await ctx.warning(message)&lt;/code&gt; - Send warning log message&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;await ctx.error(message)&lt;/code&gt; - Send error log message&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;await ctx.log(level, message, logger_name=None)&lt;/code&gt; - Send log with custom level&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;await ctx.report_progress(progress, total=None, message=None)&lt;/code&gt; - Report operation progress&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;await ctx.read_resource(uri)&lt;/code&gt; - Read a resource by URI&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;await ctx.elicit(message, schema)&lt;/code&gt; - Request additional information from user with validation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- snippet-source examples/snippets/servers/tool_progress.py --&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mcp.server.fastmcp import Context, FastMCP
from mcp.server.session import ServerSession

mcp = FastMCP(name="Progress Example")


@mcp.tool()
async def long_running_task(task_name: str, ctx: Context[ServerSession, None], steps: int = 5) -&amp;gt; str:
    """Execute a task with progress updates."""
    await ctx.info(f"Starting: {task_name}")

    for i in range(steps):
        progress = (i + 1) / steps
        await ctx.report_progress(
            progress=progress,
            total=1.0,
            message=f"Step {i + 1}/{steps}",
        )
        await ctx.debug(f"Completed step {i + 1}")

    return f"Task '{task_name}' completed"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Full example: &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/examples/snippets/servers/tool_progress.py"&gt;examples/snippets/servers/tool_progress.py&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;!-- /snippet-source --&gt; 
&lt;h3&gt;Completions&lt;/h3&gt; 
&lt;p&gt;MCP supports providing completion suggestions for prompt arguments and resource template parameters. With the context parameter, servers can provide completions based on previously resolved values:&lt;/p&gt; 
&lt;p&gt;Client usage:&lt;/p&gt; 
&lt;!-- snippet-source examples/snippets/clients/completion_client.py --&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;"""
cd to the `examples/snippets` directory and run:
    uv run completion-client
"""

import asyncio
import os

from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
from mcp.types import PromptReference, ResourceTemplateReference

# Create server parameters for stdio connection
server_params = StdioServerParameters(
    command="uv",  # Using uv to run the server
    args=["run", "server", "completion", "stdio"],  # Server with completion support
    env={"UV_INDEX": os.environ.get("UV_INDEX", "")},
)


async def run():
    """Run the completion client example."""
    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write) as session:
            # Initialize the connection
            await session.initialize()

            # List available resource templates
            templates = await session.list_resource_templates()
            print("Available resource templates:")
            for template in templates.resourceTemplates:
                print(f"  - {template.uriTemplate}")

            # List available prompts
            prompts = await session.list_prompts()
            print("\nAvailable prompts:")
            for prompt in prompts.prompts:
                print(f"  - {prompt.name}")

            # Complete resource template arguments
            if templates.resourceTemplates:
                template = templates.resourceTemplates[0]
                print(f"\nCompleting arguments for resource template: {template.uriTemplate}")

                # Complete without context
                result = await session.complete(
                    ref=ResourceTemplateReference(type="ref/resource", uri=template.uriTemplate),
                    argument={"name": "owner", "value": "model"},
                )
                print(f"Completions for 'owner' starting with 'model': {result.completion.values}")

                # Complete with context - repo suggestions based on owner
                result = await session.complete(
                    ref=ResourceTemplateReference(type="ref/resource", uri=template.uriTemplate),
                    argument={"name": "repo", "value": ""},
                    context_arguments={"owner": "modelcontextprotocol"},
                )
                print(f"Completions for 'repo' with owner='modelcontextprotocol': {result.completion.values}")

            # Complete prompt arguments
            if prompts.prompts:
                prompt_name = prompts.prompts[0].name
                print(f"\nCompleting arguments for prompt: {prompt_name}")

                result = await session.complete(
                    ref=PromptReference(type="ref/prompt", name=prompt_name),
                    argument={"name": "style", "value": ""},
                )
                print(f"Completions for 'style' argument: {result.completion.values}")


def main():
    """Entry point for the completion client."""
    asyncio.run(run())


if __name__ == "__main__":
    main()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Full example: &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/examples/snippets/clients/completion_client.py"&gt;examples/snippets/clients/completion_client.py&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;!-- /snippet-source --&gt; 
&lt;h3&gt;Elicitation&lt;/h3&gt; 
&lt;p&gt;Request additional information from users. This example shows an Elicitation during a Tool Call:&lt;/p&gt; 
&lt;!-- snippet-source examples/snippets/servers/elicitation.py --&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from pydantic import BaseModel, Field

from mcp.server.fastmcp import Context, FastMCP
from mcp.server.session import ServerSession

mcp = FastMCP(name="Elicitation Example")


class BookingPreferences(BaseModel):
    """Schema for collecting user preferences."""

    checkAlternative: bool = Field(description="Would you like to check another date?")
    alternativeDate: str = Field(
        default="2024-12-26",
        description="Alternative date (YYYY-MM-DD)",
    )


@mcp.tool()
async def book_table(date: str, time: str, party_size: int, ctx: Context[ServerSession, None]) -&amp;gt; str:
    """Book a table with date availability check."""
    # Check if date is available
    if date == "2024-12-25":
        # Date unavailable - ask user for alternative
        result = await ctx.elicit(
            message=(f"No tables available for {party_size} on {date}. Would you like to try another date?"),
            schema=BookingPreferences,
        )

        if result.action == "accept" and result.data:
            if result.data.checkAlternative:
                return f"[SUCCESS] Booked for {result.data.alternativeDate}"
            return "[CANCELLED] No booking made"
        return "[CANCELLED] Booking cancelled"

    # Date available
    return f"[SUCCESS] Booked for {date} at {time}"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Full example: &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/examples/snippets/servers/elicitation.py"&gt;examples/snippets/servers/elicitation.py&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;!-- /snippet-source --&gt; 
&lt;p&gt;The &lt;code&gt;elicit()&lt;/code&gt; method returns an &lt;code&gt;ElicitationResult&lt;/code&gt; with:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;action&lt;/code&gt;: "accept", "decline", or "cancel"&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;data&lt;/code&gt;: The validated response (only when accepted)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;validation_error&lt;/code&gt;: Any validation error message&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Sampling&lt;/h3&gt; 
&lt;p&gt;Tools can interact with LLMs through sampling (generating text):&lt;/p&gt; 
&lt;!-- snippet-source examples/snippets/servers/sampling.py --&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mcp.server.fastmcp import Context, FastMCP
from mcp.server.session import ServerSession
from mcp.types import SamplingMessage, TextContent

mcp = FastMCP(name="Sampling Example")


@mcp.tool()
async def generate_poem(topic: str, ctx: Context[ServerSession, None]) -&amp;gt; str:
    """Generate a poem using LLM sampling."""
    prompt = f"Write a short poem about {topic}"

    result = await ctx.session.create_message(
        messages=[
            SamplingMessage(
                role="user",
                content=TextContent(type="text", text=prompt),
            )
        ],
        max_tokens=100,
    )

    if result.content.type == "text":
        return result.content.text
    return str(result.content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Full example: &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/examples/snippets/servers/sampling.py"&gt;examples/snippets/servers/sampling.py&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;!-- /snippet-source --&gt; 
&lt;h3&gt;Logging and Notifications&lt;/h3&gt; 
&lt;p&gt;Tools can send logs and notifications through the context:&lt;/p&gt; 
&lt;!-- snippet-source examples/snippets/servers/notifications.py --&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mcp.server.fastmcp import Context, FastMCP
from mcp.server.session import ServerSession

mcp = FastMCP(name="Notifications Example")


@mcp.tool()
async def process_data(data: str, ctx: Context[ServerSession, None]) -&amp;gt; str:
    """Process data with logging."""
    # Different log levels
    await ctx.debug(f"Debug: Processing '{data}'")
    await ctx.info("Info: Starting processing")
    await ctx.warning("Warning: This is experimental")
    await ctx.error("Error: (This is just a demo)")

    # Notify about resource changes
    await ctx.session.send_resource_list_changed()

    return f"Processed: {data}"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Full example: &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/examples/snippets/servers/notifications.py"&gt;examples/snippets/servers/notifications.py&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;!-- /snippet-source --&gt; 
&lt;h3&gt;Authentication&lt;/h3&gt; 
&lt;p&gt;Authentication can be used by servers that want to expose tools accessing protected resources.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;mcp.server.auth&lt;/code&gt; implements OAuth 2.1 resource server functionality, where MCP servers act as Resource Servers (RS) that validate tokens issued by separate Authorization Servers (AS). This follows the &lt;a href="https://modelcontextprotocol.io/specification/2025-06-18/basic/authorization"&gt;MCP authorization specification&lt;/a&gt; and implements RFC 9728 (Protected Resource Metadata) for AS discovery.&lt;/p&gt; 
&lt;p&gt;MCP servers can use authentication by providing an implementation of the &lt;code&gt;TokenVerifier&lt;/code&gt; protocol:&lt;/p&gt; 
&lt;!-- snippet-source examples/snippets/servers/oauth_server.py --&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;"""
Run from the repository root:
    uv run examples/snippets/servers/oauth_server.py
"""

from pydantic import AnyHttpUrl

from mcp.server.auth.provider import AccessToken, TokenVerifier
from mcp.server.auth.settings import AuthSettings
from mcp.server.fastmcp import FastMCP


class SimpleTokenVerifier(TokenVerifier):
    """Simple token verifier for demonstration."""

    async def verify_token(self, token: str) -&amp;gt; AccessToken | None:
        pass  # This is where you would implement actual token validation


# Create FastMCP instance as a Resource Server
mcp = FastMCP(
    "Weather Service",
    # Token verifier for authentication
    token_verifier=SimpleTokenVerifier(),
    # Auth settings for RFC 9728 Protected Resource Metadata
    auth=AuthSettings(
        issuer_url=AnyHttpUrl("https://auth.example.com"),  # Authorization Server URL
        resource_server_url=AnyHttpUrl("http://localhost:3001"),  # This server's URL
        required_scopes=["user"],
    ),
)


@mcp.tool()
async def get_weather(city: str = "London") -&amp;gt; dict[str, str]:
    """Get weather data for a city"""
    return {
        "city": city,
        "temperature": "22",
        "condition": "Partly cloudy",
        "humidity": "65%",
    }


if __name__ == "__main__":
    mcp.run(transport="streamable-http")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Full example: &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/examples/snippets/servers/oauth_server.py"&gt;examples/snippets/servers/oauth_server.py&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;!-- /snippet-source --&gt; 
&lt;p&gt;For a complete example with separate Authorization Server and Resource Server implementations, see &lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/examples/servers/simple-auth/"&gt;&lt;code&gt;examples/servers/simple-auth/&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Architecture:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Authorization Server (AS)&lt;/strong&gt;: Handles OAuth flows, user authentication, and token issuance&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Resource Server (RS)&lt;/strong&gt;: Your MCP server that validates tokens and serves protected resources&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Client&lt;/strong&gt;: Discovers AS through RFC 9728, obtains tokens, and uses them with the MCP server&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/src/mcp/server/auth/provider.py"&gt;TokenVerifier&lt;/a&gt; for more details on implementing token validation.&lt;/p&gt; 
&lt;h3&gt;FastMCP Properties&lt;/h3&gt; 
&lt;p&gt;The FastMCP server instance accessible via &lt;code&gt;ctx.fastmcp&lt;/code&gt; provides access to server configuration and metadata:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;ctx.fastmcp.name&lt;/code&gt; - The server's name as defined during initialization&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ctx.fastmcp.instructions&lt;/code&gt; - Server instructions/description provided to clients&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ctx.fastmcp.settings&lt;/code&gt; - Complete server configuration object containing: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;debug&lt;/code&gt; - Debug mode flag&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;log_level&lt;/code&gt; - Current logging level&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;host&lt;/code&gt; and &lt;code&gt;port&lt;/code&gt; - Server network configuration&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;mount_path&lt;/code&gt;, &lt;code&gt;sse_path&lt;/code&gt;, &lt;code&gt;streamable_http_path&lt;/code&gt; - Transport paths&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;stateless_http&lt;/code&gt; - Whether the server operates in stateless mode&lt;/li&gt; 
   &lt;li&gt;And other configuration options&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;@mcp.tool()
def server_info(ctx: Context) -&amp;gt; dict:
    """Get information about the current server."""
    return {
        "name": ctx.fastmcp.name,
        "instructions": ctx.fastmcp.instructions,
        "debug_mode": ctx.fastmcp.settings.debug,
        "log_level": ctx.fastmcp.settings.log_level,
        "host": ctx.fastmcp.settings.host,
        "port": ctx.fastmcp.settings.port,
    }
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Session Properties and Methods&lt;/h3&gt; 
&lt;p&gt;The session object accessible via &lt;code&gt;ctx.session&lt;/code&gt; provides advanced control over client communication:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;ctx.session.client_params&lt;/code&gt; - Client initialization parameters and declared capabilities&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;await ctx.session.send_log_message(level, data, logger)&lt;/code&gt; - Send log messages with full control&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;await ctx.session.create_message(messages, max_tokens)&lt;/code&gt; - Request LLM sampling/completion&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;await ctx.session.send_progress_notification(token, progress, total, message)&lt;/code&gt; - Direct progress updates&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;await ctx.session.send_resource_updated(uri)&lt;/code&gt; - Notify clients that a specific resource changed&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;await ctx.session.send_resource_list_changed()&lt;/code&gt; - Notify clients that the resource list changed&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;await ctx.session.send_tool_list_changed()&lt;/code&gt; - Notify clients that the tool list changed&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;await ctx.session.send_prompt_list_changed()&lt;/code&gt; - Notify clients that the prompt list changed&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;@mcp.tool()
async def notify_data_update(resource_uri: str, ctx: Context) -&amp;gt; str:
    """Update data and notify clients of the change."""
    # Perform data update logic here
    
    # Notify clients that this specific resource changed
    await ctx.session.send_resource_updated(AnyUrl(resource_uri))
    
    # If this affects the overall resource list, notify about that too
    await ctx.session.send_resource_list_changed()
    
    return f"Updated {resource_uri} and notified clients"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Request Context Properties&lt;/h3&gt; 
&lt;p&gt;The request context accessible via &lt;code&gt;ctx.request_context&lt;/code&gt; contains request-specific information and resources:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;ctx.request_context.lifespan_context&lt;/code&gt; - Access to resources initialized during server startup 
  &lt;ul&gt; 
   &lt;li&gt;Database connections, configuration objects, shared services&lt;/li&gt; 
   &lt;li&gt;Type-safe access to resources defined in your server's lifespan function&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ctx.request_context.meta&lt;/code&gt; - Request metadata from the client including: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;progressToken&lt;/code&gt; - Token for progress notifications&lt;/li&gt; 
   &lt;li&gt;Other client-provided metadata&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ctx.request_context.request&lt;/code&gt; - The original MCP request object for advanced processing&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ctx.request_context.request_id&lt;/code&gt; - Unique identifier for this request&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Example with typed lifespan context
@dataclass
class AppContext:
    db: Database
    config: AppConfig

@mcp.tool()
def query_with_config(query: str, ctx: Context) -&amp;gt; str:
    """Execute a query using shared database and configuration."""
    # Access typed lifespan context
    app_ctx: AppContext = ctx.request_context.lifespan_context
    
    # Use shared resources
    connection = app_ctx.db
    settings = app_ctx.config
    
    # Execute query with configuration
    result = connection.execute(query, timeout=settings.query_timeout)
    return str(result)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Full lifespan example: &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/examples/snippets/servers/lifespan_example.py"&gt;examples/snippets/servers/lifespan_example.py&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;Running Your Server&lt;/h2&gt; 
&lt;h3&gt;Development Mode&lt;/h3&gt; 
&lt;p&gt;The fastest way to test and debug your server is with the MCP Inspector:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run mcp dev server.py

# Add dependencies
uv run mcp dev server.py --with pandas --with numpy

# Mount local code
uv run mcp dev server.py --with-editable .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Claude Desktop Integration&lt;/h3&gt; 
&lt;p&gt;Once your server is ready, install it in Claude Desktop:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run mcp install server.py

# Custom name
uv run mcp install server.py --name "My Analytics Server"

# Environment variables
uv run mcp install server.py -v API_KEY=abc123 -v DB_URL=postgres://...
uv run mcp install server.py -f .env
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Direct Execution&lt;/h3&gt; 
&lt;p&gt;For advanced scenarios like custom deployments:&lt;/p&gt; 
&lt;!-- snippet-source examples/snippets/servers/direct_execution.py --&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;"""Example showing direct execution of an MCP server.

This is the simplest way to run an MCP server directly.
cd to the `examples/snippets` directory and run:
    uv run direct-execution-server
    or
    python servers/direct_execution.py
"""

from mcp.server.fastmcp import FastMCP

mcp = FastMCP("My App")


@mcp.tool()
def hello(name: str = "World") -&amp;gt; str:
    """Say hello to someone."""
    return f"Hello, {name}!"


def main():
    """Entry point for the direct execution server."""
    mcp.run()


if __name__ == "__main__":
    main()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Full example: &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/examples/snippets/servers/direct_execution.py"&gt;examples/snippets/servers/direct_execution.py&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;!-- /snippet-source --&gt; 
&lt;p&gt;Run it with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python servers/direct_execution.py
# or
uv run mcp run servers/direct_execution.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note that &lt;code&gt;uv run mcp run&lt;/code&gt; or &lt;code&gt;uv run mcp dev&lt;/code&gt; only supports server using FastMCP and not the low-level server variant.&lt;/p&gt; 
&lt;h3&gt;Streamable HTTP Transport&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Streamable HTTP transport is superseding SSE transport for production deployments.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;!-- snippet-source examples/snippets/servers/streamable_config.py --&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;"""
Run from the repository root:
    uv run examples/snippets/servers/streamable_config.py
"""

from mcp.server.fastmcp import FastMCP

# Stateful server (maintains session state)
mcp = FastMCP("StatefulServer")

# Other configuration options:
# Stateless server (no session persistence)
# mcp = FastMCP("StatelessServer", stateless_http=True)

# Stateless server (no session persistence, no sse stream with supported client)
# mcp = FastMCP("StatelessServer", stateless_http=True, json_response=True)


# Add a simple tool to demonstrate the server
@mcp.tool()
def greet(name: str = "World") -&amp;gt; str:
    """Greet someone by name."""
    return f"Hello, {name}!"


# Run server with streamable_http transport
if __name__ == "__main__":
    mcp.run(transport="streamable-http")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Full example: &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/examples/snippets/servers/streamable_config.py"&gt;examples/snippets/servers/streamable_config.py&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;!-- /snippet-source --&gt; 
&lt;p&gt;You can mount multiple FastMCP servers in a Starlette application:&lt;/p&gt; 
&lt;!-- snippet-source examples/snippets/servers/streamable_starlette_mount.py --&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;"""
Run from the repository root:
    uvicorn examples.snippets.servers.streamable_starlette_mount:app --reload
"""

import contextlib

from starlette.applications import Starlette
from starlette.routing import Mount

from mcp.server.fastmcp import FastMCP

# Create the Echo server
echo_mcp = FastMCP(name="EchoServer", stateless_http=True)


@echo_mcp.tool()
def echo(message: str) -&amp;gt; str:
    """A simple echo tool"""
    return f"Echo: {message}"


# Create the Math server
math_mcp = FastMCP(name="MathServer", stateless_http=True)


@math_mcp.tool()
def add_two(n: int) -&amp;gt; int:
    """Tool to add two to the input"""
    return n + 2


# Create a combined lifespan to manage both session managers
@contextlib.asynccontextmanager
async def lifespan(app: Starlette):
    async with contextlib.AsyncExitStack() as stack:
        await stack.enter_async_context(echo_mcp.session_manager.run())
        await stack.enter_async_context(math_mcp.session_manager.run())
        yield


# Create the Starlette app and mount the MCP servers
app = Starlette(
    routes=[
        Mount("/echo", echo_mcp.streamable_http_app()),
        Mount("/math", math_mcp.streamable_http_app()),
    ],
    lifespan=lifespan,
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Full example: &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/examples/snippets/servers/streamable_starlette_mount.py"&gt;examples/snippets/servers/streamable_starlette_mount.py&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;!-- /snippet-source --&gt; 
&lt;p&gt;For low level server with Streamable HTTP implementations, see:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Stateful server: &lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/examples/servers/simple-streamablehttp/"&gt;&lt;code&gt;examples/servers/simple-streamablehttp/&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Stateless server: &lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/examples/servers/simple-streamablehttp-stateless/"&gt;&lt;code&gt;examples/servers/simple-streamablehttp-stateless/&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The streamable HTTP transport supports:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Stateful and stateless operation modes&lt;/li&gt; 
 &lt;li&gt;Resumability with event stores&lt;/li&gt; 
 &lt;li&gt;JSON or SSE response formats&lt;/li&gt; 
 &lt;li&gt;Better scalability for multi-node deployments&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Mounting to an Existing ASGI Server&lt;/h3&gt; 
&lt;p&gt;By default, SSE servers are mounted at &lt;code&gt;/sse&lt;/code&gt; and Streamable HTTP servers are mounted at &lt;code&gt;/mcp&lt;/code&gt;. You can customize these paths using the methods described below.&lt;/p&gt; 
&lt;p&gt;For more information on mounting applications in Starlette, see the &lt;a href="https://www.starlette.io/routing/#submounting-routes"&gt;Starlette documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;SSE servers&lt;/h4&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: SSE transport is being superseded by &lt;a href="https://modelcontextprotocol.io/specification/2025-03-26/basic/transports#streamable-http"&gt;Streamable HTTP transport&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;You can mount the SSE server to an existing ASGI server using the &lt;code&gt;sse_app&lt;/code&gt; method. This allows you to integrate the SSE server with other ASGI applications.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from starlette.applications import Starlette
from starlette.routing import Mount, Host
from mcp.server.fastmcp import FastMCP


mcp = FastMCP("My App")

# Mount the SSE server to the existing ASGI server
app = Starlette(
    routes=[
        Mount('/', app=mcp.sse_app()),
    ]
)

# or dynamically mount as host
app.router.routes.append(Host('mcp.acme.corp', app=mcp.sse_app()))
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;When mounting multiple MCP servers under different paths, you can configure the mount path in several ways:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from starlette.applications import Starlette
from starlette.routing import Mount
from mcp.server.fastmcp import FastMCP

# Create multiple MCP servers
github_mcp = FastMCP("GitHub API")
browser_mcp = FastMCP("Browser")
curl_mcp = FastMCP("Curl")
search_mcp = FastMCP("Search")

# Method 1: Configure mount paths via settings (recommended for persistent configuration)
github_mcp.settings.mount_path = "/github"
browser_mcp.settings.mount_path = "/browser"

# Method 2: Pass mount path directly to sse_app (preferred for ad-hoc mounting)
# This approach doesn't modify the server's settings permanently

# Create Starlette app with multiple mounted servers
app = Starlette(
    routes=[
        # Using settings-based configuration
        Mount("/github", app=github_mcp.sse_app()),
        Mount("/browser", app=browser_mcp.sse_app()),
        # Using direct mount path parameter
        Mount("/curl", app=curl_mcp.sse_app("/curl")),
        Mount("/search", app=search_mcp.sse_app("/search")),
    ]
)

# Method 3: For direct execution, you can also pass the mount path to run()
if __name__ == "__main__":
    search_mcp.run(transport="sse", mount_path="/search")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more information on mounting applications in Starlette, see the &lt;a href="https://www.starlette.io/routing/#submounting-routes"&gt;Starlette documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Advanced Usage&lt;/h2&gt; 
&lt;h3&gt;Low-Level Server&lt;/h3&gt; 
&lt;p&gt;For more control, you can use the low-level server implementation directly. This gives you full access to the protocol and allows you to customize every aspect of your server, including lifecycle management through the lifespan API:&lt;/p&gt; 
&lt;!-- snippet-source examples/snippets/servers/lowlevel/lifespan.py --&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;"""
Run from the repository root:
    uv run examples/snippets/servers/lowlevel/lifespan.py
"""

from collections.abc import AsyncIterator
from contextlib import asynccontextmanager
from typing import Any

import mcp.server.stdio
import mcp.types as types
from mcp.server.lowlevel import NotificationOptions, Server
from mcp.server.models import InitializationOptions


# Mock database class for example
class Database:
    """Mock database class for example."""

    @classmethod
    async def connect(cls) -&amp;gt; "Database":
        """Connect to database."""
        print("Database connected")
        return cls()

    async def disconnect(self) -&amp;gt; None:
        """Disconnect from database."""
        print("Database disconnected")

    async def query(self, query_str: str) -&amp;gt; list[dict[str, str]]:
        """Execute a query."""
        # Simulate database query
        return [{"id": "1", "name": "Example", "query": query_str}]


@asynccontextmanager
async def server_lifespan(_server: Server) -&amp;gt; AsyncIterator[dict[str, Any]]:
    """Manage server startup and shutdown lifecycle."""
    # Initialize resources on startup
    db = await Database.connect()
    try:
        yield {"db": db}
    finally:
        # Clean up on shutdown
        await db.disconnect()


# Pass lifespan to server
server = Server("example-server", lifespan=server_lifespan)


@server.list_tools()
async def handle_list_tools() -&amp;gt; list[types.Tool]:
    """List available tools."""
    return [
        types.Tool(
            name="query_db",
            description="Query the database",
            inputSchema={
                "type": "object",
                "properties": {"query": {"type": "string", "description": "SQL query to execute"}},
                "required": ["query"],
            },
        )
    ]


@server.call_tool()
async def query_db(name: str, arguments: dict[str, Any]) -&amp;gt; list[types.TextContent]:
    """Handle database query tool call."""
    if name != "query_db":
        raise ValueError(f"Unknown tool: {name}")

    # Access lifespan context
    ctx = server.request_context
    db = ctx.lifespan_context["db"]

    # Execute query
    results = await db.query(arguments["query"])

    return [types.TextContent(type="text", text=f"Query results: {results}")]


async def run():
    """Run the server with lifespan management."""
    async with mcp.server.stdio.stdio_server() as (read_stream, write_stream):
        await server.run(
            read_stream,
            write_stream,
            InitializationOptions(
                server_name="example-server",
                server_version="0.1.0",
                capabilities=server.get_capabilities(
                    notification_options=NotificationOptions(),
                    experimental_capabilities={},
                ),
            ),
        )


if __name__ == "__main__":
    import asyncio

    asyncio.run(run())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Full example: &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/examples/snippets/servers/lowlevel/lifespan.py"&gt;examples/snippets/servers/lowlevel/lifespan.py&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;!-- /snippet-source --&gt; 
&lt;p&gt;The lifespan API provides:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;A way to initialize resources when the server starts and clean them up when it stops&lt;/li&gt; 
 &lt;li&gt;Access to initialized resources through the request context in handlers&lt;/li&gt; 
 &lt;li&gt;Type-safe context passing between lifespan and request handlers&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- snippet-source examples/snippets/servers/lowlevel/basic.py --&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;"""
Run from the repository root:
uv run examples/snippets/servers/lowlevel/basic.py
"""

import asyncio

import mcp.server.stdio
import mcp.types as types
from mcp.server.lowlevel import NotificationOptions, Server
from mcp.server.models import InitializationOptions

# Create a server instance
server = Server("example-server")


@server.list_prompts()
async def handle_list_prompts() -&amp;gt; list[types.Prompt]:
    """List available prompts."""
    return [
        types.Prompt(
            name="example-prompt",
            description="An example prompt template",
            arguments=[types.PromptArgument(name="arg1", description="Example argument", required=True)],
        )
    ]


@server.get_prompt()
async def handle_get_prompt(name: str, arguments: dict[str, str] | None) -&amp;gt; types.GetPromptResult:
    """Get a specific prompt by name."""
    if name != "example-prompt":
        raise ValueError(f"Unknown prompt: {name}")

    arg1_value = (arguments or {}).get("arg1", "default")

    return types.GetPromptResult(
        description="Example prompt",
        messages=[
            types.PromptMessage(
                role="user",
                content=types.TextContent(type="text", text=f"Example prompt text with argument: {arg1_value}"),
            )
        ],
    )


async def run():
    """Run the basic low-level server."""
    async with mcp.server.stdio.stdio_server() as (read_stream, write_stream):
        await server.run(
            read_stream,
            write_stream,
            InitializationOptions(
                server_name="example",
                server_version="0.1.0",
                capabilities=server.get_capabilities(
                    notification_options=NotificationOptions(),
                    experimental_capabilities={},
                ),
            ),
        )


if __name__ == "__main__":
    asyncio.run(run())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Full example: &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/examples/snippets/servers/lowlevel/basic.py"&gt;examples/snippets/servers/lowlevel/basic.py&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;!-- /snippet-source --&gt; 
&lt;p&gt;Caution: The &lt;code&gt;uv run mcp run&lt;/code&gt; and &lt;code&gt;uv run mcp dev&lt;/code&gt; tool doesn't support low-level server.&lt;/p&gt; 
&lt;h4&gt;Structured Output Support&lt;/h4&gt; 
&lt;p&gt;The low-level server supports structured output for tools, allowing you to return both human-readable content and machine-readable structured data. Tools can define an &lt;code&gt;outputSchema&lt;/code&gt; to validate their structured output:&lt;/p&gt; 
&lt;!-- snippet-source examples/snippets/servers/lowlevel/structured_output.py --&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;"""
Run from the repository root:
    uv run examples/snippets/servers/lowlevel/structured_output.py
"""

import asyncio
from typing import Any

import mcp.server.stdio
import mcp.types as types
from mcp.server.lowlevel import NotificationOptions, Server
from mcp.server.models import InitializationOptions

server = Server("example-server")


@server.list_tools()
async def list_tools() -&amp;gt; list[types.Tool]:
    """List available tools with structured output schemas."""
    return [
        types.Tool(
            name="get_weather",
            description="Get current weather for a city",
            inputSchema={
                "type": "object",
                "properties": {"city": {"type": "string", "description": "City name"}},
                "required": ["city"],
            },
            outputSchema={
                "type": "object",
                "properties": {
                    "temperature": {"type": "number", "description": "Temperature in Celsius"},
                    "condition": {"type": "string", "description": "Weather condition"},
                    "humidity": {"type": "number", "description": "Humidity percentage"},
                    "city": {"type": "string", "description": "City name"},
                },
                "required": ["temperature", "condition", "humidity", "city"],
            },
        )
    ]


@server.call_tool()
async def call_tool(name: str, arguments: dict[str, Any]) -&amp;gt; dict[str, Any]:
    """Handle tool calls with structured output."""
    if name == "get_weather":
        city = arguments["city"]

        # Simulated weather data - in production, call a weather API
        weather_data = {
            "temperature": 22.5,
            "condition": "partly cloudy",
            "humidity": 65,
            "city": city,  # Include the requested city
        }

        # low-level server will validate structured output against the tool's
        # output schema, and additionally serialize it into a TextContent block
        # for backwards compatibility with pre-2025-06-18 clients.
        return weather_data
    else:
        raise ValueError(f"Unknown tool: {name}")


async def run():
    """Run the structured output server."""
    async with mcp.server.stdio.stdio_server() as (read_stream, write_stream):
        await server.run(
            read_stream,
            write_stream,
            InitializationOptions(
                server_name="structured-output-example",
                server_version="0.1.0",
                capabilities=server.get_capabilities(
                    notification_options=NotificationOptions(),
                    experimental_capabilities={},
                ),
            ),
        )


if __name__ == "__main__":
    asyncio.run(run())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Full example: &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/examples/snippets/servers/lowlevel/structured_output.py"&gt;examples/snippets/servers/lowlevel/structured_output.py&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;!-- /snippet-source --&gt; 
&lt;p&gt;Tools can return data in three ways:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Content only&lt;/strong&gt;: Return a list of content blocks (default behavior before spec revision 2025-06-18)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Structured data only&lt;/strong&gt;: Return a dictionary that will be serialized to JSON (Introduced in spec revision 2025-06-18)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Both&lt;/strong&gt;: Return a tuple of (content, structured_data) preferred option to use for backwards compatibility&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;When an &lt;code&gt;outputSchema&lt;/code&gt; is defined, the server automatically validates the structured output against the schema. This ensures type safety and helps catch errors early.&lt;/p&gt; 
&lt;h3&gt;Writing MCP Clients&lt;/h3&gt; 
&lt;p&gt;The SDK provides a high-level client interface for connecting to MCP servers using various &lt;a href="https://modelcontextprotocol.io/specification/2025-03-26/basic/transports"&gt;transports&lt;/a&gt;:&lt;/p&gt; 
&lt;!-- snippet-source examples/snippets/clients/stdio_client.py --&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;"""
cd to the `examples/snippets/clients` directory and run:
    uv run client
"""

import asyncio
import os

from pydantic import AnyUrl

from mcp import ClientSession, StdioServerParameters, types
from mcp.client.stdio import stdio_client
from mcp.shared.context import RequestContext

# Create server parameters for stdio connection
server_params = StdioServerParameters(
    command="uv",  # Using uv to run the server
    args=["run", "server", "fastmcp_quickstart", "stdio"],  # We're already in snippets dir
    env={"UV_INDEX": os.environ.get("UV_INDEX", "")},
)


# Optional: create a sampling callback
async def handle_sampling_message(
    context: RequestContext[ClientSession, None], params: types.CreateMessageRequestParams
) -&amp;gt; types.CreateMessageResult:
    print(f"Sampling request: {params.messages}")
    return types.CreateMessageResult(
        role="assistant",
        content=types.TextContent(
            type="text",
            text="Hello, world! from model",
        ),
        model="gpt-3.5-turbo",
        stopReason="endTurn",
    )


async def run():
    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write, sampling_callback=handle_sampling_message) as session:
            # Initialize the connection
            await session.initialize()

            # List available prompts
            prompts = await session.list_prompts()
            print(f"Available prompts: {[p.name for p in prompts.prompts]}")

            # Get a prompt (greet_user prompt from fastmcp_quickstart)
            if prompts.prompts:
                prompt = await session.get_prompt("greet_user", arguments={"name": "Alice", "style": "friendly"})
                print(f"Prompt result: {prompt.messages[0].content}")

            # List available resources
            resources = await session.list_resources()
            print(f"Available resources: {[r.uri for r in resources.resources]}")

            # List available tools
            tools = await session.list_tools()
            print(f"Available tools: {[t.name for t in tools.tools]}")

            # Read a resource (greeting resource from fastmcp_quickstart)
            resource_content = await session.read_resource(AnyUrl("greeting://World"))
            content_block = resource_content.contents[0]
            if isinstance(content_block, types.TextContent):
                print(f"Resource content: {content_block.text}")

            # Call a tool (add tool from fastmcp_quickstart)
            result = await session.call_tool("add", arguments={"a": 5, "b": 3})
            result_unstructured = result.content[0]
            if isinstance(result_unstructured, types.TextContent):
                print(f"Tool result: {result_unstructured.text}")
            result_structured = result.structuredContent
            print(f"Structured tool result: {result_structured}")


def main():
    """Entry point for the client script."""
    asyncio.run(run())


if __name__ == "__main__":
    main()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Full example: &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/examples/snippets/clients/stdio_client.py"&gt;examples/snippets/clients/stdio_client.py&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;!-- /snippet-source --&gt; 
&lt;p&gt;Clients can also connect using &lt;a href="https://modelcontextprotocol.io/specification/2025-03-26/basic/transports#streamable-http"&gt;Streamable HTTP transport&lt;/a&gt;:&lt;/p&gt; 
&lt;!-- snippet-source examples/snippets/clients/streamable_basic.py --&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;"""
Run from the repository root:
    uv run examples/snippets/clients/streamable_basic.py
"""

import asyncio

from mcp import ClientSession
from mcp.client.streamable_http import streamablehttp_client


async def main():
    # Connect to a streamable HTTP server
    async with streamablehttp_client("http://localhost:8000/mcp") as (
        read_stream,
        write_stream,
        _,
    ):
        # Create a session using the client streams
        async with ClientSession(read_stream, write_stream) as session:
            # Initialize the connection
            await session.initialize()
            # List available tools
            tools = await session.list_tools()
            print(f"Available tools: {[tool.name for tool in tools.tools]}")


if __name__ == "__main__":
    asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Full example: &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/examples/snippets/clients/streamable_basic.py"&gt;examples/snippets/clients/streamable_basic.py&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;!-- /snippet-source --&gt; 
&lt;h3&gt;Client Display Utilities&lt;/h3&gt; 
&lt;p&gt;When building MCP clients, the SDK provides utilities to help display human-readable names for tools, resources, and prompts:&lt;/p&gt; 
&lt;!-- snippet-source examples/snippets/clients/display_utilities.py --&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;"""
cd to the `examples/snippets` directory and run:
    uv run display-utilities-client
"""

import asyncio
import os

from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
from mcp.shared.metadata_utils import get_display_name

# Create server parameters for stdio connection
server_params = StdioServerParameters(
    command="uv",  # Using uv to run the server
    args=["run", "server", "fastmcp_quickstart", "stdio"],
    env={"UV_INDEX": os.environ.get("UV_INDEX", "")},
)


async def display_tools(session: ClientSession):
    """Display available tools with human-readable names"""
    tools_response = await session.list_tools()

    for tool in tools_response.tools:
        # get_display_name() returns the title if available, otherwise the name
        display_name = get_display_name(tool)
        print(f"Tool: {display_name}")
        if tool.description:
            print(f"   {tool.description}")


async def display_resources(session: ClientSession):
    """Display available resources with human-readable names"""
    resources_response = await session.list_resources()

    for resource in resources_response.resources:
        display_name = get_display_name(resource)
        print(f"Resource: {display_name} ({resource.uri})")

    templates_response = await session.list_resource_templates()
    for template in templates_response.resourceTemplates:
        display_name = get_display_name(template)
        print(f"Resource Template: {display_name}")


async def run():
    """Run the display utilities example."""
    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write) as session:
            # Initialize the connection
            await session.initialize()

            print("=== Available Tools ===")
            await display_tools(session)

            print("\n=== Available Resources ===")
            await display_resources(session)


def main():
    """Entry point for the display utilities client."""
    asyncio.run(run())


if __name__ == "__main__":
    main()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Full example: &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/examples/snippets/clients/display_utilities.py"&gt;examples/snippets/clients/display_utilities.py&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;!-- /snippet-source --&gt; 
&lt;p&gt;The &lt;code&gt;get_display_name()&lt;/code&gt; function implements the proper precedence rules for displaying names:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;For tools: &lt;code&gt;title&lt;/code&gt; &amp;gt; &lt;code&gt;annotations.title&lt;/code&gt; &amp;gt; &lt;code&gt;name&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;For other objects: &lt;code&gt;title&lt;/code&gt; &amp;gt; &lt;code&gt;name&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This ensures your client UI shows the most user-friendly names that servers provide.&lt;/p&gt; 
&lt;h3&gt;OAuth Authentication for Clients&lt;/h3&gt; 
&lt;p&gt;The SDK includes &lt;a href="https://modelcontextprotocol.io/specification/2025-03-26/basic/authorization"&gt;authorization support&lt;/a&gt; for connecting to protected MCP servers:&lt;/p&gt; 
&lt;!-- snippet-source examples/snippets/clients/oauth_client.py --&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;"""
Before running, specify running MCP RS server URL.
To spin up RS server locally, see
    examples/servers/simple-auth/README.md

cd to the `examples/snippets` directory and run:
    uv run oauth-client
"""

import asyncio
from urllib.parse import parse_qs, urlparse

from pydantic import AnyUrl

from mcp import ClientSession
from mcp.client.auth import OAuthClientProvider, TokenStorage
from mcp.client.streamable_http import streamablehttp_client
from mcp.shared.auth import OAuthClientInformationFull, OAuthClientMetadata, OAuthToken


class InMemoryTokenStorage(TokenStorage):
    """Demo In-memory token storage implementation."""

    def __init__(self):
        self.tokens: OAuthToken | None = None
        self.client_info: OAuthClientInformationFull | None = None

    async def get_tokens(self) -&amp;gt; OAuthToken | None:
        """Get stored tokens."""
        return self.tokens

    async def set_tokens(self, tokens: OAuthToken) -&amp;gt; None:
        """Store tokens."""
        self.tokens = tokens

    async def get_client_info(self) -&amp;gt; OAuthClientInformationFull | None:
        """Get stored client information."""
        return self.client_info

    async def set_client_info(self, client_info: OAuthClientInformationFull) -&amp;gt; None:
        """Store client information."""
        self.client_info = client_info


async def handle_redirect(auth_url: str) -&amp;gt; None:
    print(f"Visit: {auth_url}")


async def handle_callback() -&amp;gt; tuple[str, str | None]:
    callback_url = input("Paste callback URL: ")
    params = parse_qs(urlparse(callback_url).query)
    return params["code"][0], params.get("state", [None])[0]


async def main():
    """Run the OAuth client example."""
    oauth_auth = OAuthClientProvider(
        server_url="http://localhost:8001",
        client_metadata=OAuthClientMetadata(
            client_name="Example MCP Client",
            redirect_uris=[AnyUrl("http://localhost:3000/callback")],
            grant_types=["authorization_code", "refresh_token"],
            response_types=["code"],
            scope="user",
        ),
        storage=InMemoryTokenStorage(),
        redirect_handler=handle_redirect,
        callback_handler=handle_callback,
    )

    async with streamablehttp_client("http://localhost:8001/mcp", auth=oauth_auth) as (read, write, _):
        async with ClientSession(read, write) as session:
            await session.initialize()

            tools = await session.list_tools()
            print(f"Available tools: {[tool.name for tool in tools.tools]}")

            resources = await session.list_resources()
            print(f"Available resources: {[r.uri for r in resources.resources]}")


def run():
    asyncio.run(main())


if __name__ == "__main__":
    run()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Full example: &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/examples/snippets/clients/oauth_client.py"&gt;examples/snippets/clients/oauth_client.py&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;!-- /snippet-source --&gt; 
&lt;p&gt;For a complete working example, see &lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/examples/clients/simple-auth-client/"&gt;&lt;code&gt;examples/clients/simple-auth-client/&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Parsing Tool Results&lt;/h3&gt; 
&lt;p&gt;When calling tools through MCP, the &lt;code&gt;CallToolResult&lt;/code&gt; object contains the tool's response in a structured format. Understanding how to parse this result is essential for properly handling tool outputs.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;"""examples/snippets/clients/parsing_tool_results.py"""

import asyncio

from mcp import ClientSession, StdioServerParameters, types
from mcp.client.stdio import stdio_client


async def parse_tool_results():
    """Demonstrates how to parse different types of content in CallToolResult."""
    server_params = StdioServerParameters(
        command="python", args=["path/to/mcp_server.py"]
    )

    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write) as session:
            await session.initialize()

            # Example 1: Parsing text content
            result = await session.call_tool("get_data", {"format": "text"})
            for content in result.content:
                if isinstance(content, types.TextContent):
                    print(f"Text: {content.text}")

            # Example 2: Parsing structured content from JSON tools
            result = await session.call_tool("get_user", {"id": "123"})
            if hasattr(result, "structuredContent") and result.structuredContent:
                # Access structured data directly
                user_data = result.structuredContent
                print(f"User: {user_data.get('name')}, Age: {user_data.get('age')}")

            # Example 3: Parsing embedded resources
            result = await session.call_tool("read_config", {})
            for content in result.content:
                if isinstance(content, types.EmbeddedResource):
                    resource = content.resource
                    if isinstance(resource, types.TextResourceContents):
                        print(f"Config from {resource.uri}: {resource.text}")
                    elif isinstance(resource, types.BlobResourceContents):
                        print(f"Binary data from {resource.uri}")

            # Example 4: Parsing image content
            result = await session.call_tool("generate_chart", {"data": [1, 2, 3]})
            for content in result.content:
                if isinstance(content, types.ImageContent):
                    print(f"Image ({content.mimeType}): {len(content.data)} bytes")

            # Example 5: Handling errors
            result = await session.call_tool("failing_tool", {})
            if result.isError:
                print("Tool execution failed!")
                for content in result.content:
                    if isinstance(content, types.TextContent):
                        print(f"Error: {content.text}")


async def main():
    await parse_tool_results()


if __name__ == "__main__":
    asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;MCP Primitives&lt;/h3&gt; 
&lt;p&gt;The MCP protocol defines three core primitives that servers can implement:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Primitive&lt;/th&gt; 
   &lt;th&gt;Control&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Example Use&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Prompts&lt;/td&gt; 
   &lt;td&gt;User-controlled&lt;/td&gt; 
   &lt;td&gt;Interactive templates invoked by user choice&lt;/td&gt; 
   &lt;td&gt;Slash commands, menu options&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Resources&lt;/td&gt; 
   &lt;td&gt;Application-controlled&lt;/td&gt; 
   &lt;td&gt;Contextual data managed by the client application&lt;/td&gt; 
   &lt;td&gt;File contents, API responses&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Tools&lt;/td&gt; 
   &lt;td&gt;Model-controlled&lt;/td&gt; 
   &lt;td&gt;Functions exposed to the LLM to take actions&lt;/td&gt; 
   &lt;td&gt;API calls, data updates&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Server Capabilities&lt;/h3&gt; 
&lt;p&gt;MCP servers declare capabilities during initialization:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Capability&lt;/th&gt; 
   &lt;th&gt;Feature Flag&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;prompts&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;listChanged&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Prompt template management&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;resources&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;subscribe&lt;/code&gt;&lt;br /&gt;&lt;code&gt;listChanged&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Resource exposure and updates&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;tools&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;listChanged&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Tool discovery and execution&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;logging&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;Server logging configuration&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;completions&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;Argument completion suggestions&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://modelcontextprotocol.io"&gt;Model Context Protocol documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://spec.modelcontextprotocol.io"&gt;Model Context Protocol specification&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/modelcontextprotocol/servers"&gt;Officially supported servers&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We are passionate about supporting contributors of all levels of experience and would love to see you get involved in the project. See the &lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/CONTRIBUTING.md"&gt;contributing guide&lt;/a&gt; to get started.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the MIT License - see the LICENSE file for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>pathwaycom/pathway</title>
      <link>https://github.com/pathwaycom/pathway</link>
      <description>&lt;p&gt;Python ETL framework for stream processing, real-time analytics, LLM pipelines, and RAG.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;a href="https://pathway.com/"&gt; &lt;img src="https://pathway.com/logo-light.svg?sanitize=true" /&gt; &lt;/a&gt; 
 &lt;br /&gt;
 &lt;br /&gt; 
 &lt;a href="https://trendshift.io/repositories/10388" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/10388" alt="pathwaycom%2Fpathway | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; 
 &lt;br /&gt;
 &lt;br /&gt; 
&lt;/div&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/pathwaycom/pathway/actions/workflows/ubuntu_test.yml"&gt; &lt;img src="https://github.com/pathwaycom/pathway/actions/workflows/ubuntu_test.yml/badge.svg?sanitize=true" alt="ubuntu" /&gt; &lt;br /&gt; &lt;/a&gt;&lt;a href="https://github.com/pathwaycom/pathway/actions/workflows/release.yml"&gt; &lt;img src="https://github.com/pathwaycom/pathway/actions/workflows/release.yml/badge.svg?sanitize=true" alt="Last release" /&gt;&lt;/a&gt; &lt;a href="https://badge.fury.io/py/pathway"&gt;&lt;img src="https://badge.fury.io/py/pathway.svg?sanitize=true" alt="PyPI version" height="18" /&gt;&lt;/a&gt; &lt;a href="https://badge.fury.io/py/pathway"&gt;&lt;img src="https://static.pepy.tech/badge/pathway" alt="PyPI downloads" height="18" /&gt;&lt;/a&gt; &lt;a href="https://github.com/pathwaycom/pathway/raw/main/LICENSE.txt"&gt; &lt;img src="https://img.shields.io/badge/license-BSL-green" alt="License: BSL" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;a href="https://discord.gg/pathway"&gt; &lt;img src="https://img.shields.io/discord/1042405378304004156?logo=discord" alt="chat on Discord" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/intent/follow?screen_name=pathway_com"&gt; &lt;img src="https://img.shields.io/twitter/follow/pathwaycom" alt="follow on Twitter" /&gt;&lt;/a&gt; &lt;a href="https://linkedin.com/company/pathway"&gt; &lt;img src="https://img.shields.io/badge/pathway-0077B5?style=social&amp;amp;logo=linkedin" alt="follow on LinkedIn" /&gt;&lt;/a&gt; &lt;a href="https://github.com/dylanhogg/awesome-python/raw/main/README.md"&gt; &lt;img src="https://awesome.re/badge.svg?sanitize=true" alt="Awesome Python" /&gt;&lt;/a&gt; &lt;a href="https://gurubase.io/g/pathway"&gt; &lt;img src="https://img.shields.io/badge/Gurubase-Ask%20Pathway%20Guru-006BFF" alt="Pathway Guru" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;a href="https://raw.githubusercontent.com/pathwaycom/pathway/main/#getting-started"&gt;Getting Started&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/pathwaycom/pathway/main/#deployment"&gt;Deployment&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/pathwaycom/pathway/main/#resources"&gt;Documentation and Support&lt;/a&gt; | &lt;a href="https://pathway.com/blog/"&gt;Blog&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/pathwaycom/pathway/main/#license"&gt;License&lt;/a&gt; &lt;/p&gt; 
&lt;h1&gt;Pathway&lt;a id="pathway"&gt; Live Data Framework&lt;/a&gt;&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://pathway.com"&gt;Pathway&lt;/a&gt; is a Python ETL framework for stream processing, real-time analytics, LLM pipelines, and RAG.&lt;/p&gt; 
&lt;p&gt;Pathway comes with an &lt;strong&gt;easy-to-use Python API&lt;/strong&gt;, allowing you to seamlessly integrate your favorite Python ML libraries. Pathway code is versatile and robust: &lt;strong&gt;you can use it in both development and production environments, handling both batch and streaming data effectively&lt;/strong&gt;. The same code can be used for local development, CI/CD tests, running batch jobs, handling stream replays, and processing data streams.&lt;/p&gt; 
&lt;p&gt;Pathway is powered by a &lt;strong&gt;scalable Rust engine&lt;/strong&gt; based on Differential Dataflow and performs incremental computation. Your Pathway code, despite being written in Python, is run by the Rust engine, enabling multithreading, multiprocessing, and distributed computations. All the pipeline is kept in memory and can be easily deployed with &lt;strong&gt;Docker and Kubernetes&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;You can install Pathway with pip:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install -U pathway
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For any questions, you will find the community and team behind the project &lt;a href="https://discord.com/invite/pathway"&gt;on Discord&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Use-cases and templates&lt;/h2&gt; 
&lt;p&gt;Ready to see what Pathway can do?&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://pathway.com/developers/templates"&gt;Try one of our easy-to-run examples&lt;/a&gt;!&lt;/p&gt; 
&lt;p&gt;Available in both notebook and docker formats, these ready-to-launch examples can be launched in just a few clicks. Pick one and start your hands-on experience with Pathway today!&lt;/p&gt; 
&lt;h3&gt;Event processing and real-time analytics pipelines&lt;/h3&gt; 
&lt;p&gt;With its unified engine for batch and streaming and its full Python compatibility, Pathway makes data processing as easy as possible. It's the ideal solution for a wide range of data processing pipelines, including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://pathway.com/developers/templates/kafka-etl"&gt;Showcase: Real-time ETL.&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pathway.com/developers/templates/realtime-log-monitoring"&gt;Showcase: Event-driven pipelines with alerting.&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pathway.com/developers/templates/linear_regression_with_kafka/"&gt;Showcase: Realtime analytics.&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pathway.com/developers/user-guide/connecting-to-data/switch-from-batch-to-streaming"&gt;Docs: Switch from batch to streaming.&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;AI Pipelines&lt;/h3&gt; 
&lt;p&gt;Pathway provides dedicated LLM tooling to build live LLM and RAG pipelines. Wrappers for most common LLM services and utilities are included, making working with LLMs and RAGs pipelines incredibly easy. Check out our &lt;a href="https://pathway.com/developers/user-guide/llm-xpack/overview"&gt;LLM xpack documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Don't hesitate to try one of our runnable examples featuring LLM tooling. You can find such examples &lt;a href="https://pathway.com/developers/user-guide/llm-xpack/llm-examples"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://pathway.com/developers/templates/unstructured-to-structured/"&gt;Template: Unstructured data to SQL on-the-fly.&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pathway.com/developers/templates/private-rag-ollama-mistral"&gt;Template: Private RAG with Ollama and Mistral AI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pathway.com/developers/templates/adaptive-rag"&gt;Template: Adaptive RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pathway.com/developers/templates/multimodal-rag"&gt;Template: Multimodal RAG with gpt-4o&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;A wide range of connectors&lt;/strong&gt;: Pathway comes with connectors that connect to external data sources such as Kafka, GDrive, PostgreSQL, or SharePoint. Its Airbyte connector allows you to connect to more than 300 different data sources. If the connector you want is not available, you can build your own custom connector using Pathway Python connector.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Stateless and stateful transformations&lt;/strong&gt;: Pathway supports stateful transformations such as joins, windowing, and sorting. It provides many transformations directly implemented in Rust. In addition to the provided transformation, you can use any Python function. You can implement your own or you can use any Python library to process your data.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Persistence&lt;/strong&gt;: Pathway provides persistence to save the state of the computation. This allows you to restart your pipeline after an update or a crash. Your pipelines are in good hands with Pathway!&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Consistency&lt;/strong&gt;: Pathway handles the time for you, making sure all your computations are consistent. In particular, Pathway manages late and out-of-order points by updating its results whenever new (or late, in this case) data points come into the system. The free version of Pathway gives the "at least once" consistency while the enterprise version provides the "exactly once" consistency.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Scalable Rust engine&lt;/strong&gt;: with Pathway Rust engine, you are free from the usual limits imposed by Python. You can easily do multithreading, multiprocessing, and distributed computations.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;LLM helpers&lt;/strong&gt;: Pathway provides an LLM extension with all the utilities to integrate LLMs with your data pipelines (LLM wrappers, parsers, embedders, splitters), including an in-memory real-time Vector Index, and integrations with LLamaIndex and LangChain. You can quickly build and deploy RAG applications with your live documents.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Getting started&lt;a id="getting-started"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;h3&gt;Installation&lt;a id="installation"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Pathway requires Python 3.10 or above.&lt;/p&gt; 
&lt;p&gt;You can install the current release of Pathway using &lt;code&gt;pip&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ pip install -U pathway
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;⚠️ Pathway is available on MacOS and Linux. Users of other systems should run Pathway on a Virtual Machine.&lt;/p&gt; 
&lt;h3&gt;Example: computing the sum of positive values in real time.&lt;a id="example"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import pathway as pw

# Define the schema of your data (Optional)
class InputSchema(pw.Schema):
  value: int

# Connect to your data using connectors
input_table = pw.io.csv.read(
  "./input/",
  schema=InputSchema
)

#Define your operations on the data
filtered_table = input_table.filter(input_table.value&amp;gt;=0)
result_table = filtered_table.reduce(
  sum_value = pw.reducers.sum(filtered_table.value)
)

# Load your results to external systems
pw.io.jsonlines.write(result_table, "output.jsonl")

# Run the computation
pw.run()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run Pathway &lt;a href="https://colab.research.google.com/drive/1aBIJ2HCng-YEUOMrr0qtj0NeZMEyRz55?usp=sharing"&gt;in Google Colab&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You can find more examples &lt;a href="https://github.com/pathwaycom/pathway/tree/main/examples"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Deployment&lt;a id="deployment"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;h3&gt;Locally&lt;a id="running-pathway-locally"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;To use Pathway, you only need to import it:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import pathway as pw
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Now, you can easily create your processing pipeline, and let Pathway handle the updates. Once your pipeline is created, you can launch the computation on streaming data with a one-line command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;pw.run()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can then run your Pathway project (say, &lt;code&gt;main.py&lt;/code&gt;) just like a normal Python script: &lt;code&gt;$ python main.py&lt;/code&gt;. Pathway comes with a monitoring dashboard that allows you to keep track of the number of messages sent by each connector and the latency of the system. The dashboard also includes log messages.&lt;/p&gt; 
&lt;img src="https://d14l3brkh44201.cloudfront.net/pathway-dashboard.png" width="1326" alt="Pathway dashboard" /&gt; 
&lt;p&gt;Alternatively, you can use the pathway'ish version:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ pathway spawn python main.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Pathway natively supports multithreading. To launch your application with 3 threads, you can do as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ pathway spawn --threads 3 python main.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To jumpstart a Pathway project, you can use our &lt;a href="https://github.com/pathwaycom/cookiecutter-pathway"&gt;cookiecutter template&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Docker&lt;a id="docker"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;You can easily run Pathway using docker.&lt;/p&gt; 
&lt;h4&gt;Pathway image&lt;/h4&gt; 
&lt;p&gt;You can use the &lt;a href="https://hub.docker.com/r/pathwaycom/pathway"&gt;Pathway docker image&lt;/a&gt;, using a Dockerfile:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-dockerfile"&gt;FROM pathwaycom/pathway:latest

WORKDIR /app

COPY requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD [ "python", "./your-script.py" ]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can then build and run the Docker image:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;docker build -t my-pathway-app .
docker run -it --rm --name my-pathway-app my-pathway-app
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Run a single Python script&lt;/h4&gt; 
&lt;p&gt;When dealing with single-file projects, creating a full-fledged &lt;code&gt;Dockerfile&lt;/code&gt; might seem unnecessary. In such scenarios, you can execute a Python script directly using the Pathway Docker image. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;docker run -it --rm --name my-pathway-app -v "$PWD":/app pathwaycom/pathway:latest python my-pathway-app.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Python docker image&lt;/h4&gt; 
&lt;p&gt;You can also use a standard Python image and install Pathway using pip with a Dockerfile:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-dockerfile"&gt;FROM --platform=linux/x86_64 python:3.10

RUN pip install -U pathway
COPY ./pathway-script.py pathway-script.py

CMD ["python", "-u", "pathway-script.py"]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Kubernetes and cloud&lt;a id="k8s"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Docker containers are ideally suited for deployment on the cloud with Kubernetes. If you want to scale your Pathway application, you may be interested in our Pathway for Enterprise. Pathway for Enterprise is specially tailored towards end-to-end data processing and real time intelligent analytics. It scales using distributed computing on the cloud and supports distributed Kubernetes deployment, with external persistence setup.&lt;/p&gt; 
&lt;p&gt;You can easily deploy Pathway using services like Render: see &lt;a href="https://pathway.com/developers/user-guide/deployment/render-deploy/"&gt;how to deploy Pathway in a few clicks&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you are interested, don't hesitate to &lt;a href="mailto:contact@pathway.com"&gt;contact us&lt;/a&gt; to learn more.&lt;/p&gt; 
&lt;h2&gt;Performance&lt;a id="performance"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;Pathway is made to outperform state-of-the-art technologies designed for streaming and batch data processing tasks, including: Flink, Spark, and Kafka Streaming. It also makes it possible to implement a lot of algorithms/UDF's in streaming mode which are not readily supported by other streaming frameworks (especially: temporal joins, iterative graph algorithms, machine learning routines).&lt;/p&gt; 
&lt;p&gt;If you are curious, here are &lt;a href="https://github.com/pathwaycom/pathway-benchmarks"&gt;some benchmarks to play with&lt;/a&gt;.&lt;/p&gt; 
&lt;img src="https://github.com/pathwaycom/pathway-benchmarks/raw/main/images/bm-wordcount-lineplot.png" width="1326" alt="WordCount Graph" /&gt; 
&lt;h2&gt;Documentation and Support&lt;a id="resources"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;The entire documentation of Pathway is available at &lt;a href="https://pathway.com/developers/user-guide/introduction/welcome"&gt;pathway.com/developers/&lt;/a&gt;, including the &lt;a href="https://pathway.com/developers/api-docs/pathway"&gt;API Docs&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you have any question, don't hesitate to &lt;a href="https://github.com/pathwaycom/pathway/issues"&gt;open an issue on GitHub&lt;/a&gt;, join us on &lt;a href="https://discord.com/invite/pathway"&gt;Discord&lt;/a&gt;, or send us an email at &lt;a href="mailto:contact@pathway.com"&gt;contact@pathway.com&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;a id="license"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;Pathway is distributed on a &lt;a href="https://github.com/pathwaycom/pathway/raw/main/LICENSE.txt"&gt;BSL 1.1 License&lt;/a&gt; which allows for unlimited non-commercial use, as well as use of the Pathway package &lt;a href="https://pathway.com/license/"&gt;for most commercial purposes&lt;/a&gt;, free of charge. Code in this repository automatically converts to Open Source (Apache 2.0 License) after 4 years. Some &lt;a href="https://github.com/pathwaycom"&gt;public repos&lt;/a&gt; which are complementary to this one (examples, libraries, connectors, etc.) are licensed as Open Source, under the MIT license.&lt;/p&gt; 
&lt;h2&gt;Contribution guidelines&lt;a id="contribution-guidelines"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;If you develop a library or connector which you would like to integrate with this repo, we suggest releasing it first as a separate repo on a MIT/Apache 2.0 license.&lt;/p&gt; 
&lt;p&gt;For all concerns regarding core Pathway functionalities, Issues are encouraged. For further information, don't hesitate to engage with Pathway's &lt;a href="https://discord.gg/pathway"&gt;Discord community&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>oop7/YTSage</title>
      <link>https://github.com/oop7/YTSage</link>
      <description>&lt;p&gt;Modern YouTube downloader with a clean PySide6 interface. Download videos in any quality, extract audio, fetch subtitles, sponserBlock, and view video metadata. Built with yt-dlp for reliable performance.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h1&gt;🎥 YTSage&lt;/h1&gt; 
 &lt;img src="https://github.com/user-attachments/assets/f95f7bfb-8591-4d32-b795-68e61efd670c" width="800" alt="YTSage Interface" /&gt; 
 &lt;p&gt;&lt;a href="https://badge.fury.io/py/ytsage"&gt;&lt;img src="https://img.shields.io/pypi/v/ytsage?color=dc2626&amp;amp;style=for-the-badge&amp;amp;logo=pypi&amp;amp;logoColor=white" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://opensource.org/licenses/MIT"&gt;&lt;img src="https://img.shields.io/badge/License-MIT-374151?style=for-the-badge&amp;amp;logo=opensource&amp;amp;logoColor=white" alt="License: MIT" /&gt;&lt;/a&gt; &lt;a href="https://www.python.org/downloads/"&gt;&lt;img src="https://img.shields.io/badge/python-3.7+-1f2937?style=for-the-badge&amp;amp;logo=python&amp;amp;logoColor=white" alt="Python 3.7+" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/ytsage"&gt;&lt;img src="https://img.shields.io/pypi/dm/ytsage?color=4b5563&amp;amp;style=for-the-badge&amp;amp;logo=download&amp;amp;logoColor=white" alt="Downloads" /&gt;&lt;/a&gt; &lt;a href="https://github.com/oop7/YTSage/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/oop7/YTSage?color=dc2626&amp;amp;style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white" alt="GitHub Stars" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;A modern YouTube downloader with a clean PySide6 interface.&lt;/strong&gt;&lt;br /&gt; Download videos in any quality, extract audio, fetch subtitles, and more.&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/oop7/YTSage/main/#installation"&gt;Installation&lt;/a&gt; • &lt;a href="https://raw.githubusercontent.com/oop7/YTSage/main/#features"&gt;Features&lt;/a&gt; • &lt;a href="https://raw.githubusercontent.com/oop7/YTSage/main/#usage"&gt;Usage&lt;/a&gt; • &lt;a href="https://raw.githubusercontent.com/oop7/YTSage/main/#screenshots"&gt;Screenshots&lt;/a&gt; • &lt;a href="https://raw.githubusercontent.com/oop7/YTSage/main/#contributing"&gt;Contributing&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;a id="features"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;✨ Features&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Core Features&lt;/th&gt; 
    &lt;th&gt;Advanced Features&lt;/th&gt; 
    &lt;th&gt;Extra Features&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;🎥 Format Table&lt;/td&gt; 
    &lt;td&gt;🚫 SponsorBlock Integration&lt;/td&gt; 
    &lt;td&gt;💾 Save Download Path&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;🎵 Audio Extraction&lt;/td&gt; 
    &lt;td&gt;📝 Multi-Subtitle Select &amp;amp; Merge&lt;/td&gt; 
    &lt;td&gt;🔄 Auto-Update yt-dlp&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;✨ Simple UI&lt;/td&gt; 
    &lt;td&gt;💾 Save Description&lt;/td&gt; 
    &lt;td&gt;🛠️ FFmpeg/yt-dlp Detection&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;📋 Playlist Support&lt;/td&gt; 
    &lt;td&gt;🖼️ Save thumbnail&lt;/td&gt; 
    &lt;td&gt;⚙️ Custom Commands&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;🖼️ Playlist Selector&lt;/td&gt; 
    &lt;td&gt;🚀 Speed Limiter&lt;/td&gt; 
    &lt;td&gt;🍪 Login with Cookies&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;✂️ Trim Video Sections&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;a id="installation"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;🚀 Installation&lt;/h2&gt; 
&lt;h3&gt;Quick Install (Recommended)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install ytsage
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run the application
ytsage
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;📦 Other Installation Methods&lt;/h3&gt; 
&lt;h3&gt;Pre-built Executables&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;🪟 Windows: &lt;code&gt;YTSage.exe&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;🪟 Windows: &lt;code&gt;YTSage-ffmpeg.exe&lt;/code&gt; (Includes FFmpeg)&lt;/li&gt; 
 &lt;li&gt;🐧 Linux: &lt;code&gt;YTSage_{version}_amd64.deb&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;🐧 Linux: &lt;code&gt;YTSage-x86_64.AppImage&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;🍎 macOS: &lt;code&gt;YTSage-macOS-app.zip&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;🍎 macOS: &lt;code&gt;YTSage-{version}.dmg&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;🛠️ Manual Installation from Source&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Clone repository
git clone https://github.com/oop7/YTSage.git

# Navigate to directory
cd YTSage

# Install dependencies
pip install -r requirements.txt

# Run application
python main.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p&gt;&lt;a id="screenshots"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;📸 Screenshots&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/f95f7bfb-8591-4d32-b795-68e61efd670c" alt="Main Interface" width="400" /&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/f7b3ebab-3054-4c77-8109-c899a8b10047" alt="Playlist Download" width="400" /&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;&lt;em&gt;Main Interface&lt;/em&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;em&gt;Playlist Download&lt;/em&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/a80d2ae2-0031-4ed0-bee4-93293634c62a" alt="Audio Format Selection with Save Thumbnail" width="400" /&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/5236e3cc-8a8d-4d85-a660-782a740ef9af" alt="Subtitle Options merged with Remove Sponsor Segments" width="400" /&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;&lt;em&gt;Audio Format&lt;/em&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;em&gt;Subtitle Options&lt;/em&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;a id="usage"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;📖 Usage&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;🎯 Basic Usage&lt;/summary&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;strong&gt;Launch YTSage&lt;/strong&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Paste YouTube URL&lt;/strong&gt; (or use "Paste URL" button)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Click "Analyze"&lt;/strong&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Select Format:&lt;/strong&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;code&gt;Video&lt;/code&gt; for video downloads&lt;/li&gt; 
    &lt;li&gt;&lt;code&gt;Audio Only&lt;/code&gt; for audio extraction&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Choose Options:&lt;/strong&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Enable subtitles &amp;amp; select language&lt;/li&gt; 
    &lt;li&gt;Enable subtitle merge&lt;/li&gt; 
    &lt;li&gt;Save thumbnail&lt;/li&gt; 
    &lt;li&gt;Remove sponsor segments&lt;/li&gt; 
    &lt;li&gt;Save description&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Select Output Directory&lt;/strong&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Click "Download"&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;📋 Playlist Download&lt;/summary&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;strong&gt;Paste Playlist URL&lt;/strong&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Click "Analyze"&lt;/strong&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Select videos from the playlist selector (optional, defaults to all)&lt;/strong&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Choose desired format/quality&lt;/strong&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Click "Download"&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;💡 The application automatically handles the download queue&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;🧰 Advanced Options&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Quality Selection:&lt;/strong&gt; Choose the highest resolution for best quality&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Subtitle Options:&lt;/strong&gt; Filter languages and embed into video&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Custom Commands:&lt;/strong&gt; Access advanced yt-dlp features&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Save Description:&lt;/strong&gt; Save the description of the video&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Save Thumbnail:&lt;/strong&gt; Save the thumbnail of the video&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Remove Sponsor Segments:&lt;/strong&gt; Remove sponsor segments from the video&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Speed Limiter:&lt;/strong&gt; Limit the download speed&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Login with Cookies:&lt;/strong&gt; Login to YouTube using cookies to access private content&lt;br /&gt; How to use it: 
   &lt;ol&gt; 
    &lt;li&gt;Extract cookies from your browser using an extension like &lt;a href="https://github.com/moustachauve/cookie-editor?tab=readme-ov-file"&gt;cookie-editor&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;Copy the cookies in Netscape format&lt;/li&gt; 
    &lt;li&gt;Create a file named &lt;code&gt;cookies.txt&lt;/code&gt; and paste the cookies into it&lt;/li&gt; 
    &lt;li&gt;Select the &lt;code&gt;cookies.txt&lt;/code&gt; file in the app&lt;/li&gt; 
   &lt;/ol&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Save Download Path:&lt;/strong&gt; Save the download path&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Update yt-dlp:&lt;/strong&gt; Update yt-dlp&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;FFmpeg/yt-dlp Detection:&lt;/strong&gt; Automatically detect FFmpeg/yt-dlp&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Custom Commands:&lt;/strong&gt; Access advanced yt-dlp features&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Trim Video:&lt;/strong&gt; Download only specific parts of a video by specifying time ranges (HH:MM:SS format)&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;🛠️ Troubleshooting&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Format table not displaying:&lt;/strong&gt; Update yt-dlp to the latest version&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Download fails:&lt;/strong&gt; Check your internet connection and ensure the video is available&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Audio extraction issues:&lt;/strong&gt; Verify FFmpeg is properly installed&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;🧩 Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Python:&lt;/strong&gt; 3.7 or higher&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;GUI Framework:&lt;/strong&gt; PySide6&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Download Engine:&lt;/strong&gt; yt-dlp&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Media Processing:&lt;/strong&gt; FFmpeg&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Additional Libraries:&lt;/strong&gt; Pillow, requests, packaging, markdown, pygame&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a id="contributing"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;👥 Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions! Here's how you can help:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;🍴 Fork the repository&lt;/li&gt; 
 &lt;li&gt;🌿 Create your feature branch: &lt;pre&gt;&lt;code class="language-bash"&gt;git checkout -b feature/AmazingFeature
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;💾 Commit your changes: &lt;pre&gt;&lt;code class="language-bash"&gt;git commit -m 'Add some AmazingFeature'
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;📤 Push to the branch: &lt;pre&gt;&lt;code class="language-bash"&gt;git push origin feature/AmazingFeature
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;🔄 Open a Pull Request&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;📊 Star History&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://star-history.com/#oop7/YTSage&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=oop7/YTSage&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;📜 License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the MIT License - see the &lt;a href="https://raw.githubusercontent.com/oop7/YTSage/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h2&gt;🙏 Acknowledgments&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Technology&lt;/th&gt; 
    &lt;th&gt;Purpose&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp"&gt;yt-dlp&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Download Engine&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://wiki.qt.io/Qt_for_Python"&gt;PySide6&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;GUI Framework&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://ffmpeg.org/"&gt;FFmpeg&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Media Processing&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://python-pillow.org/"&gt;Pillow&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Image Processing&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://requests.readthedocs.io/"&gt;requests&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;HTTP Requests&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://packaging.python.org/"&gt;packaging&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Packaging&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://python-markdown.github.io/"&gt;markdown&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Markdown Processing&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.pygame.org/"&gt;pygame&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Audio Playback&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://pixabay.com/sound-effects/new-notification-09-352705/"&gt;New Notification 09 by Universfield&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Notification Sound&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;⚠️ Disclaimer&lt;/h2&gt; 
&lt;p&gt;This tool is for personal use only. Please respect YouTube's terms of service and content creators' rights.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;Made with ❤️ by &lt;a href="https://github.com/oop7"&gt;oop7&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>tadata-org/fastapi_mcp</title>
      <link>https://github.com/tadata-org/fastapi_mcp</link>
      <description>&lt;p&gt;Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt;&lt;a href="https://github.com/tadata-org/fastapi_mcp"&gt;&lt;img src="https://github.com/user-attachments/assets/7e44e98b-a0ba-4aff-a68a-4ffee3a6189c" alt="fastapi-to-mcp" height="100/" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;span style="font-size: 0.85em; font-weight: normal;"&gt;Built by &lt;a href="https://tadata.com"&gt;Tadata&lt;/a&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;h1 align="center"&gt; FastAPI-MCP &lt;/h1&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://trendshift.io/repositories/14064" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/14064" alt="tadata-org%2Ffastapi_mcp | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;p align="center"&gt;Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://pypi.org/project/fastapi-mcp/"&gt;&lt;img src="https://img.shields.io/pypi/v/fastapi-mcp?color=%2334D058&amp;amp;label=pypi%20package" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/fastapi-mcp/"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/fastapi-mcp.svg?sanitize=true" alt="Python Versions" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/tadata-org/fastapi_mcp/main/#"&gt;&lt;img src="https://img.shields.io/badge/FastAPI-009485.svg?logo=fastapi&amp;amp;logoColor=white" alt="FastAPI" /&gt;&lt;/a&gt; &lt;a href="https://github.com/tadata-org/fastapi_mcp/actions/workflows/ci.yml"&gt;&lt;img src="https://github.com/tadata-org/fastapi_mcp/actions/workflows/ci.yml/badge.svg?sanitize=true" alt="CI" /&gt;&lt;/a&gt; &lt;a href="https://codecov.io/gh/tadata-org/fastapi_mcp"&gt;&lt;img src="https://codecov.io/gh/tadata-org/fastapi_mcp/branch/main/graph/badge.svg?sanitize=true" alt="Coverage" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p align="center"&gt;&lt;a href="https://github.com/tadata-org/fastapi_mcp"&gt;&lt;img src="https://github.com/user-attachments/assets/b205adc6-28c0-4e3c-a68b-9c1a80eb7d0c" alt="fastapi-mcp-usage" height="400" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Authentication&lt;/strong&gt; built in, using your existing FastAPI dependencies!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;FastAPI-native:&lt;/strong&gt; Not just another OpenAPI -&amp;gt; MCP converter&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Zero/Minimal configuration&lt;/strong&gt; required - just point it at your FastAPI app and it works&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Preserving schemas&lt;/strong&gt; of your request models and response models&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Preserve documentation&lt;/strong&gt; of all your endpoints, just as it is in Swagger&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Flexible deployment&lt;/strong&gt; - Mount your MCP server to the same app, or deploy separately&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ASGI transport&lt;/strong&gt; - Uses FastAPI's ASGI interface directly for efficient communication&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Hosted Solution&lt;/h2&gt; 
&lt;p&gt;If you prefer a managed hosted solution check out &lt;a href="https://tadata.com"&gt;tadata.com&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;We recommend using &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt;, a fast Python package installer:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv add fastapi-mcp
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, you can install with pip:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install fastapi-mcp
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Basic Usage&lt;/h2&gt; 
&lt;p&gt;The simplest way to use FastAPI-MCP is to add an MCP server directly to your FastAPI application:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from fastapi import FastAPI
from fastapi_mcp import FastApiMCP

app = FastAPI()

mcp = FastApiMCP(app)

# Mount the MCP server directly to your FastAPI app
mcp.mount()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;That's it! Your auto-generated MCP server is now available at &lt;code&gt;https://app.base.url/mcp&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Documentation, Examples and Advanced Usage&lt;/h2&gt; 
&lt;p&gt;FastAPI-MCP provides &lt;a href="https://fastapi-mcp.tadata.com/"&gt;comprehensive documentation&lt;/a&gt;. Additionaly, check out the &lt;a href="https://raw.githubusercontent.com/tadata-org/fastapi_mcp/main/examples"&gt;examples directory&lt;/a&gt; for code samples demonstrating these features in action.&lt;/p&gt; 
&lt;h2&gt;FastAPI-first Approach&lt;/h2&gt; 
&lt;p&gt;FastAPI-MCP is designed as a native extension of FastAPI, not just a converter that generates MCP tools from your API. This approach offers several key advantages:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Native dependencies&lt;/strong&gt;: Secure your MCP endpoints using familiar FastAPI &lt;code&gt;Depends()&lt;/code&gt; for authentication and authorization&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ASGI transport&lt;/strong&gt;: Communicates directly with your FastAPI app using its ASGI interface, eliminating the need for HTTP calls from the MCP to your API&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Unified infrastructure&lt;/strong&gt;: Your FastAPI app doesn't need to run separately from the MCP server (though &lt;a href="https://fastapi-mcp.tadata.com/advanced/deploy#deploying-separately-from-original-fastapi-app"&gt;separate deployment&lt;/a&gt; is also supported)&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This design philosophy ensures minimum friction when adding MCP capabilities to your existing FastAPI services.&lt;/p&gt; 
&lt;h2&gt;Development and Contributing&lt;/h2&gt; 
&lt;p&gt;Thank you for considering contributing to FastAPI-MCP! We encourage the community to post Issues and create Pull Requests.&lt;/p&gt; 
&lt;p&gt;Before you get started, please see our &lt;a href="https://raw.githubusercontent.com/tadata-org/fastapi_mcp/main/CONTRIBUTING.md"&gt;Contribution Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;p&gt;Join &lt;a href="https://join.slack.com/t/themcparty/shared_invite/zt-30yxr1zdi-2FG~XjBA0xIgYSYuKe7~Xg"&gt;MCParty Slack community&lt;/a&gt; to connect with other MCP enthusiasts, ask questions, and share your experiences with FastAPI-MCP.&lt;/p&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.10+ (Recommended 3.12)&lt;/li&gt; 
 &lt;li&gt;uv&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;MIT License. Copyright (c) 2025 Tadata Inc.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>LeCAR-Lab/ASAP</title>
      <link>https://github.com/LeCAR-Lab/ASAP</link>
      <description>&lt;p&gt;Official implementation of [RSS 2025] "ASAP: Aligning Simulation and Real-World Physics for Learning Agile Humanoid Whole-Body Skills"&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt; ASAP: Aligning Simulation and Real-World Physics for &lt;p&gt;Learning Agile Humanoid Whole-Body Skills &lt;/p&gt;&lt;/h1&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;Robotics: Science and Systems (RSS) 2025&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://agile.human2humanoid.com/"&gt;[Website]&lt;/a&gt; &lt;a href="https://arxiv.org/pdf/2502.01143"&gt;[Arxiv]&lt;/a&gt; &lt;a href="https://www.youtube.com/watch?v=tu7LSNYWDTs&amp;amp;ab_channel=LeCARLabatCMU"&gt;[Video]&lt;/a&gt;&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/imgs/CMU-NV-logo-crop-png.png" height="50&amp;quot;" /&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://developer.nvidia.com/isaac-gym"&gt;&lt;img src="https://img.shields.io/badge/IsaacGym-Preview4-b.svg?sanitize=true" alt="IsaacGym" /&gt;&lt;/a&gt; &lt;a href="https://docs.isaacsim.omniverse.nvidia.com/4.2.0/index.html"&gt;&lt;img src="https://img.shields.io/badge/IsaacSim-4.2.0-b.svg?sanitize=true" alt="IsaacSim" /&gt;&lt;/a&gt; &lt;a href="https://docs.isaacsim.omniverse.nvidia.com/4.2.0/index.html"&gt;&lt;img src="https://img.shields.io/badge/Genesis-0.2.1-b.svg?sanitize=true" alt="IsaacSim" /&gt;&lt;/a&gt; &lt;a href="https://ubuntu.com/blog/tag/22-04-lts"&gt;&lt;img src="https://img.shields.io/badge/Platform-linux--64-orange.svg?sanitize=true" alt="Linux platform" /&gt;&lt;/a&gt; &lt;a href=""&gt;&lt;img src="https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true" alt="License: MIT" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;img src="https://agile.human2humanoid.com/static/images/asap-preview-gif-480P.gif" width="400px" /&gt; 
&lt;/div&gt; 
&lt;!-- # Table of Contents --&gt; 
&lt;h2&gt;📚 Table of Contents&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#overview"&gt;Overview&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Links: &lt;a href="https://agile.human2humanoid.com/"&gt;Website&lt;/a&gt; • &lt;a href="https://arxiv.org/pdf/2502.01143"&gt;Arxiv&lt;/a&gt; • &lt;a href="https://www.youtube.com/watch?v=tu7LSNYWDTs&amp;amp;ab_channel=LeCARLabatCMU"&gt;Video&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#installation"&gt;Installation &amp;amp; Setup&lt;/a&gt;&lt;/strong&gt;&lt;br /&gt; 2.1 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#isaacgym-conda-env"&gt;Base Frameworks&lt;/a&gt;&lt;br /&gt; 2.2 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#install-isaacgym"&gt;IsaacGym Setup&lt;/a&gt;&lt;br /&gt; 2.3 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#install-humanoidverse"&gt;HumanoidVerse Setup&lt;/a&gt;&lt;br /&gt; 2.4 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#isaaclab-environment"&gt;IsaacSim + IsaacLab Setup&lt;/a&gt;&lt;br /&gt; 2.5 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#genesis-environment"&gt;Genesis Environment Setup&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#motion-tracking-training"&gt;Training Pipelines&lt;/a&gt;&lt;/strong&gt;&lt;br /&gt; 3.1 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#motion-tracking-training"&gt;Phase-Based Motion Tracking&lt;/a&gt;&lt;br /&gt; 3.2 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#asap-delta-action-model-training"&gt;ASAP Delta Action Model&lt;/a&gt;&lt;br /&gt; - &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#train-delta-action-model"&gt;Train Delta Action Model&lt;/a&gt;&lt;br /&gt; - &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#use-delta-action-model-for-policy-finetuning"&gt;Finetune Policy with Delta Action Model&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#motion-retargeting-to-any-humanoid"&gt;Motion Retargeting to Any Humanoid&lt;/a&gt;&lt;/strong&gt;&lt;br /&gt; 4.1 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#1-smpl-shape-preparation"&gt;Step 1: SMPL Shape Preparation&lt;/a&gt;&lt;br /&gt; 4.2 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#2-smpl-motion-preparation-amass"&gt;Step 2: SMPL Motion Preparation (AMASS)&lt;/a&gt;&lt;br /&gt; 4.3 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#3-robot-xml-and-motion-config-preparation"&gt;Step 3: Robot XML &amp;amp; Motion Config&lt;/a&gt;&lt;br /&gt; 4.4 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#4-humanoid-smpl-shape-fitting"&gt;Step 4: Humanoid-SMPL Shape Fitting&lt;/a&gt;&lt;br /&gt; 4.5 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#5-humanoid-smpl-motion-retargeting"&gt;Step 5: Humanoid-SMPL Motion Retargeting&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#sim2simsim2real"&gt;Deployment: Sim2Sim &amp;amp; Sim2Real&lt;/a&gt;&lt;/strong&gt;&lt;br /&gt; 5.1 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#environment-setup"&gt;Environment Setup&lt;/a&gt;&lt;br /&gt; 5.2 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#sim2sim"&gt;Sim2Sim Deployment&lt;/a&gt;&lt;br /&gt; 5.3 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#sim2real"&gt;Sim2Real Deployment&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#citation"&gt;Citation&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#license"&gt;License&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;TODO&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Release code backbone&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Release phase-based motion tracking training pipeline&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Release ASAP motion datasets&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Release motion retargeting pipeline&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Release sim2sim in MuJoCo&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Release sim2real with UnitreeSDK&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Release ASAP delta action model training pipeline&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Installation&lt;/h1&gt; 
&lt;p&gt;ASAP codebase is built on top of &lt;a href="https://github.com/LeCAR-Lab/HumanoidVerse"&gt;HumanoidVerse&lt;/a&gt; (a multi-simulator framework for humanoid learning) and &lt;a href="https://github.com/LeCAR-Lab/human2humanoid"&gt;Human2Humanoid&lt;/a&gt; (our prior work on humanoid whole-body tracking).&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/LeCAR-Lab/HumanoidVerse"&gt;HumanoidVerse&lt;/a&gt; allows you to train humanoid skills in multiple simulators, including IsaacGym, IsaacSim, and Genesis. Its key design logic is the separation and modularization of simulators, tasks, and algorithms, which enables smooth transfers between different simulators and the real world with minimum effort (just one line of code change). We leverage this framework to develop &lt;a href="https://agile.human2humanoid.com/"&gt;ASAP&lt;/a&gt; and study how to best transfer policies across simulators and the real world.&lt;/p&gt; 
&lt;h2&gt;IsaacGym Conda Env&lt;/h2&gt; 
&lt;p&gt;Create mamba/conda environment, in the following we use conda for example, but you can use mamba as well.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;conda create -n hvgym python=3.8
conda activate hvgym
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Install IsaacGym&lt;/h3&gt; 
&lt;p&gt;Download &lt;a href="https://developer.nvidia.com/isaac-gym/download"&gt;IsaacGym&lt;/a&gt; and extract:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;wget https://developer.nvidia.com/isaac-gym-preview-4
tar -xvzf isaac-gym-preview-4
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Install IsaacGym Python API:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -e isaacgym/python
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Test installation:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python 1080_balls_of_solitude.py  # or
python joint_monkey.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For libpython error:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Check conda path: &lt;pre&gt;&lt;code class="language-bash"&gt;conda info -e
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Set LD_LIBRARY_PATH: &lt;pre&gt;&lt;code class="language-bash"&gt;export LD_LIBRARY_PATH=&amp;lt;/path/to/conda/envs/your_env/lib&amp;gt;:$LD_LIBRARY_PATH
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Install HumanoidVerse&lt;/h3&gt; 
&lt;p&gt;Install dependencies:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -e .
pip install -e isaac_utils
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Test with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;HYDRA_FULL_ERROR=1 python humanoidverse/train_agent.py \
+simulator=isaacgym \
+exp=locomotion \
+domain_rand=NO_domain_rand \
+rewards=loco/reward_g1_locomotion \
+robot=g1/g1_29dof_anneal_23dof \
+terrain=terrain_locomotion_plane \
+obs=loco/leggedloco_obs_singlestep_withlinvel \
num_envs=1 \
project_name=TestIsaacGymInstallation \
experiment_name=G123dof_loco \
headless=False
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;Note:&lt;/summary&gt; This is ONLY for testing, NOT how we train the locomotion policy in the ASAP paper. But still, you can train a locomotion policy by: 
 &lt;pre&gt;&lt;code class="language-bash"&gt;HYDRA_FULL_ERROR=1 python humanoidverse/train_agent.py \
+simulator=isaacgym \
+exp=locomotion \
+domain_rand=NO_domain_rand \
+rewards=loco/reward_g1_locomotion \
+robot=g1/g1_29dof_anneal_23dof \
+terrain=terrain_locomotion_plane \
+obs=loco/leggedloco_obs_singlestep_withlinvel \
num_envs=4096 \
project_name=TestIsaacGymInstallation \
experiment_name=G123dof_loco \
headless=True \
rewards.reward_penalty_curriculum=True \
rewards.reward_initial_penalty_scale=0.1 \
rewards.reward_penalty_degree=0.00003 
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;IsaacLab Environment&lt;/h2&gt; 
&lt;h3&gt;Install IsaacSim&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Download Omniverse Launcher&lt;/li&gt; 
 &lt;li&gt;Install Isaac Sim through launcher&lt;/li&gt; 
 &lt;li&gt;Set environment variables:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export ISAACSIM_PATH="${HOME}/.local/share/ov/pkg/isaac-sim-4.2.0"
export ISAACSIM_PYTHON_EXE="${ISAACSIM_PATH}/python.sh"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Install IsaacLab&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/isaac-sim/IsaacLab.git
cd IsaacLab &amp;amp;&amp;amp; ./isaaclab.sh --conda hvlab
mamba activate hvlab
sudo apt install cmake build-essential
./isaaclab.sh --install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Setup HumanoidVerse&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -e .
pip install -e isaac_utils
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Genesis Environment&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mamba create -n hvgen python=3.10
mamba activate hvgen
pip install genesis-world torch
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Install dependencies:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -e .
pip install -e isaac_utils
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Motion Tracking Training&lt;/h1&gt; 
&lt;p&gt;Train a phase-based motion tracking policy to imitate Cristiano Ronaldo's signature Siuuu move&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python humanoidverse/train_agent.py \
+simulator=isaacgym \
+exp=motion_tracking \
+domain_rand=NO_domain_rand \
+rewards=motion_tracking/reward_motion_tracking_dm_2real \
+robot=g1/g1_29dof_anneal_23dof \
+terrain=terrain_locomotion_plane \
+obs=motion_tracking/deepmimic_a2c_nolinvel_LARGEnoise_history \
num_envs=4096 \
project_name=MotionTracking \
experiment_name=MotionTracking_CR7 \
robot.motion.motion_file="humanoidverse/data/motions/g1_29dof_anneal_23dof/TairanTestbed/singles/0-TairanTestbed_TairanTestbed_CR7_video_CR7_level1_filter_amass.pkl" \
rewards.reward_penalty_curriculum=True \
rewards.reward_penalty_degree=0.00001 \
env.config.resample_motion_when_training=False \
env.config.termination.terminate_when_motion_far=True \
env.config.termination_curriculum.terminate_when_motion_far_curriculum=True \
env.config.termination_curriculum.terminate_when_motion_far_threshold_min=0.3 \
env.config.termination_curriculum.terminate_when_motion_far_curriculum_degree=0.000025 \
robot.asset.self_collisions=0
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After training, you can visualize the policy by:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python humanoidverse/eval_agent.py \
+checkpoint=logs/MotionTracking/xxxxxxxx_xxxxxxx-MotionTracking_CR7-motion_tracking-g1_29dof_anneal_23dof/model_5800.pt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This is the visualization of the policy after traning 5800 iters. The policy is able to imitate the motion of Cristiano Ronaldo's Siuuu move. With more training, the policy will be more accurate and smooth (see the video in the &lt;a href="https://arxiv.org/pdf/2502.01143"&gt;paper&lt;/a&gt;).&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/imgs/motion_tracking_5800.gif" width="400px" /&gt; 
&lt;h1&gt;ASAP delta action model training&lt;/h1&gt; 
&lt;p&gt;Note that the only difference between the delta action model training and naive motion tracking training is that delta action model needs a motion file with extra keyname &lt;code&gt;"action"&lt;/code&gt; in the motion file, so that the resulting RL policy we are training is able to use the delta action model to &lt;code&gt;"control the robot"&lt;/code&gt; to match the real-world/sim2sim motions.&lt;/p&gt; 
&lt;h2&gt;Train delta action model&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;python roboverse/train_agent.py \                                                                                   
  +simulator=isaacgym \
  +exp=train_delta_a_open_loop \
  +domain_rand=NO_domain_rand \
  +rewards=motion_tracking/delta_a/reward_delta_a_openloop \
  +robot=g1/g1_29dof_anneal_23dof \
  +terrain=terrain_locomotion_plane \
  +obs=delta_a/open_loop \
  num_envs=5000 \
  project_name=DeltaA_Training \
  experiment_name=openloopDeltaA_training \
  robot.motion.motion_file="&amp;lt;PATH_TO_YOUR_MOTION_FILE_WITH_ACTION_KEYNAME&amp;gt;" \
  env.config.max_episode_length_s=1.0 \
  rewards.reward_scales.penalty_minimal_action_norm=-0.1 \
  +device=cuda:0 \
  env.config.resample_motion_when_training=True \
  env.config.resample_time_interval_s=10000
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Use delta action model for policy finetuning&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;HYDRA_FULL_ERROR=1 \
python roboverse/train_agent.py \
+simulator=isaacgym \
+exp=train_delta_a_closed_loop \
algo.config.policy_checkpoint='&amp;lt;PATH_TO_YOUR_DELTA_A_MODEL&amp;gt;' \
+domain_rand=NO_domain_rand_finetune_with_deltaA \
+rewards=motion_tracking/reward_motion_tracking_dm_simfinetuning \
+robot=g1/g1_29dof_anneal_23dof \
+terrain=terrain_locomotion_plane \
+obs=delta_a/train_policy_with_delta_a \
num_envs=4096 \
project_name=DeltaA_Finetune \
experiment_name=finetune_with_deltaA \
robot.motion.motion_file="&amp;lt;PATH_TO_YOUR_MOTION_FILE&amp;gt;" \
+opt=wandb \
env.config.add_extra_action=True \
+checkpoint="&amp;lt;PATH_TO_YOUR_POLICY_TO_BE_FINETUNED&amp;gt;" \
domain_rand.push_robots=False \
env.config.noise_to_initial_level=1 \
rewards.reward_penalty_curriculum=True \
+device=cuda:0 \
algo.config.save_interval=5 \
algo.config.num_learning_iterations=1000 

&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Motion Retargeting to Any Humanoid&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] Here we share a generic humanoid motion retargeting pipeline to any humanoid from &lt;a href="https://github.com/ZhengyiLuo/PHC"&gt;PHC&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] We have provided all the SMPL motions (&lt;code&gt;ASAP/humanoidverse/data/motions/raw_tairantestbed_smpl&lt;/code&gt;) and retargtted G1 motions (&lt;code&gt;ASAP/humanoidverse/data/motions/g1_29dof_anneal_23dof/TairanTestbed/singles&lt;/code&gt;) used in the ASAP paper in this codebase. If you are interested in using these motions G1, you can ignore this section. If you are interested in retargeting other humanoids or other motions, you can follow the steps below to prepare the SMPL shapes and motions.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;It has three steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;SMPL Shape preparation&lt;/li&gt; 
 &lt;li&gt;SMPL Motion preparation&lt;/li&gt; 
 &lt;li&gt;Robot XML and Motion Config preparation&lt;/li&gt; 
 &lt;li&gt;Humanoid-SMPL shape fitting&lt;/li&gt; 
 &lt;li&gt;Humanoid-SMPL motion retargeting&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;1. SMPL Shape preparation&lt;/h2&gt; 
&lt;p&gt;Download &lt;a href="https://download.is.tue.mpg.de/download.php?domain=smpl&amp;amp;sfile=SMPL_python_v.1.1.0.zip"&gt;v1.1.0 SMPL files with pkl format&lt;/a&gt; and put it under &lt;code&gt;humanoidverse/data/smpl/&lt;/code&gt;, and you should have:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;|-- ASAP
    |-- humanoidverse
        |-- data
            |-- smpl
                |-- SMPL_python_v.1.1.0.zip
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then &lt;code&gt;cd ASAP/humanoidverse/data/smpl/&lt;/code&gt; and &lt;code&gt;unzip SMPL_python_v.1.1.0.zip&lt;/code&gt;, after some copying and moving, you should have:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;|-- ASAP
    |-- humanoidverse
        |-- data
            |-- smpl
                |-- SMPL_python_v.1.1.0
                |-- models
                    |-- basicmodel_f_lbs_10_207_0_v1.1.0.pkl
                    |-- basicmodel_m_lbs_10_207_0_v1.1.0.pkl
                    |-- basicmodel_neutral_lbs_10_207_0_v1.1.0.pkl
                |-- smpl_webuser
                |-- ...

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Rename these three pkl files and move it under smpl like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;|-- ASAP
    |-- humanoidverse
        |-- data
            |-- smpl
                |-- SMPL_FEMALE.pkl
                |-- SMPL_MALE.pkl
                |-- SMPL_NEUTRAL.pkl
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;2. SMPL Motion preparation (AMASS)&lt;/h2&gt; 
&lt;p&gt;Download &lt;a href="https://amass.is.tue.mpg.de/index.html"&gt;AMASS Dataset&lt;/a&gt; with &lt;code&gt;SMPL + H G format&lt;/code&gt; and put it under &lt;code&gt;humanoidverse/data/motions/AMASS/AMASS_Complete/&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;|-- ASAP
    |-- humanoidverse
        |-- data
            |-- AMASS
                |-- AMASS_Complete
                    |-- ACCAD.tar.bz2
                    |-- BMLhandball.tar.bz2
                    |-- BMLmovi.tar.bz2
                    |-- BMLrub.tar
                    |-- CMU.tar.bz2
                    |-- ...
                    |-- Transitions.tar.bz2
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;And then cd ASAP/humanoidverse/data/motions/AMASS/AMASS_Complete/ and extract all the motion files by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;for file in *.tar.bz2; do
    tar -xvjf "$file"
done
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then you should have:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;|-- ASAP
    |-- humanoidverse
        |-- data
            |-- AMASS
                |-- AMASS_Complete
                    |-- ACCAD
                    |-- BioMotionLab_NTroje
                    |-- BMLhandball
                    |-- BMLmovi
                    |-- CMU
                    |-- ...
                    |-- Transitions
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;3. Robot XML and Motion Config preparation&lt;/h2&gt; 
&lt;p&gt;Make sure you have robot xml and meshes ready at (G1 as example) &lt;code&gt;humanoidverse/data/robots/g1/g1_29dof_anneal_23dof_fitmotionONLY.xml&lt;/code&gt; And add your config for the robot motion in &lt;code&gt;humanoidverse/config/robot/g1_29dof_anneal_23dof.yaml&lt;/code&gt; with like the following. Remember to link the xml path in the config.&lt;/p&gt; 
&lt;h2&gt;4. Humanoid-SMPL shape fitting&lt;/h2&gt; 
&lt;p&gt;Run the following command to fit the SMPL shape to the humanoid.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python scripts/data_process/fit_smpl_shape.py +robot=g1/g1_29dof_anneal_23dof
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;And you should have you shape file located at &lt;code&gt;humanoidverse/data/shape/g1_29dof_anneal_23dof/shape_optimized_v1.pkl&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;If you want to visualize the shape, you can run with flag &lt;code&gt;+vis=True&lt;/code&gt;, then you can have visualization of the fitted SMPL body shape and the humanoid body keypoints like this shape. The blue is the humanoid body keypoints and the orange is the fitted SMPL body keypoint. You can tune the &lt;code&gt;robot motion&lt;/code&gt; in &lt;code&gt;humanoidverse/config/robot/g1_29dof_anneal_23dof.yaml&lt;/code&gt; to adjust the correspondence, extend links lengths to get better fitted SMPL shape.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/imgs/g1_29dof_anneal_23dof_shape.png" width="400px" /&gt; 
&lt;h2&gt;5. Humanoid-SMPL motion retargeting&lt;/h2&gt; 
&lt;p&gt;Run the following command to retarget the motion to the humanoid.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python scripts/data_process/fit_smpl_motion.py +robot=g1/g1_29dof_anneal_23dof
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Visualize motion&lt;/h3&gt; 
&lt;p&gt;Run&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python scripts/vis/vis_q_mj.py +robot=g1/g1_29dof_anneal_23dof +visualize_motion_file="humanoidverse/data/motions/g1_29dof_anneal_23dof/TairanTestbed/singles/0-motions_raw_tairantestbed_smpl_video_side_jump_level4_filter_amass.pkl"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To test, and you should have you one single motion file located at &lt;code&gt;humanoidverse/data/motions/g1_29dof_anneal_23dof/TairanTestbed/singles/0-motions_raw_tairantestbed_smpl_video_side_jump_level4_filter_amass.pkl&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;If you want to visualize the motion, you can run&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python scripts/vis/vis_q_mj.py +robot=g1/g1_29dof_anneal_23dof +visualize_motion_file="humanoidverse/data/motions/g1_29dof_anneal_23dof/TairanTestbed/singles/0-motions_raw_tairantestbed_smpl_video_side_jump_level4_filter_amass.pkl"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You should have&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/imgs/g1_29dof_anneal_23dof_motion.gif" width="400px" /&gt; 
&lt;h1&gt;Sim2Sim/Sim2Real&lt;/h1&gt; 
&lt;h2&gt;Environment Setup&lt;/h2&gt; 
&lt;p&gt;Env Installation:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;mamba create -n asap_deploy python=3.10
mamba activate asap_deploy
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Install ros2-python&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# this adds the conda-forge channel to the new created environment configuration 
conda config --env --add channels conda-forge
# and the robostack channel
conda config --env --add channels robostack-staging
# remove the defaults channel just in case, this might return an error if it is not in the list which is ok
conda config --env --remove channels defaults
# install the ros2-python package
conda install ros-humble-desktop
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Test Ros2Installation:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;rviz2
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You should see the UI like this:&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/imgs/rviz.png" width="400px" /&gt; 
&lt;p&gt;Install Unitree SDK&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone git@github.com:unitreerobotics/unitree_sdk2_python.git
cd unitree_sdk2_python
pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;minor issue to fix:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install --upgrade numpy scipy
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Sim2Sim&lt;/h2&gt; 
&lt;p&gt;start the simulation in the sim2real folder:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python sim_env/base_sim.py --config=config/g1_29dof_hist.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;in another terminal, start the policy:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python rl_policy/deepmimic_dec_loco_height.py --config=config/g1_29dof_hist.yaml --loco_model_path=./models/dec_loco/20250109_231507-noDR_rand_history_loco_stand_height_noise-decoupled_locomotion-g1_29dof/model_6600.onnx --mimic_model_paths=./models/mimic
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;click to the policy terminal and press &lt;code&gt;]&lt;/code&gt; to activate the locomotion policy&lt;/li&gt; 
 &lt;li&gt;click to the policy terminal and press &lt;code&gt;[&lt;/code&gt; to activate the asap policy (phase-based motion tracking policy)&lt;/li&gt; 
 &lt;li&gt;click to the policy terminal and press &lt;code&gt;;&lt;/code&gt; to switch to the asap policy&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;i&lt;/code&gt; to make the robot the initial position&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;o&lt;/code&gt; to emergence stop the robot&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;9&lt;/code&gt; in mujoco viewer to release the robostack&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;=&lt;/code&gt; to switch between tapping and walking for the locomotion policy&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;w/a/s/d&lt;/code&gt; to control the linear velocity&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;q/e&lt;/code&gt; to control the angular velocity&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;z&lt;/code&gt; to set all commands to zero&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;And you should be able to play around with some checkpoints from the ASAP paper. Have fun!&lt;/p&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/imgs/asap-sim2sim-clip0-ezgif.com-video-to-gif-converter.gif" width="300px" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/imgs/asap-sim2sim-clip1-ezgif.com-video-to-gif-converter.gif" width="300px" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/imgs/asap-sim2sim-clip3-ezgif.com-video-to-gif-converter.gif" width="300px" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/imgs/asap-sim2sim-clip4-ezgif.com-video-to-gif-converter.gif" width="300px" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/imgs/asap-sim2sim-clip5-ezgif.com-video-to-gif-converter.gif" width="300px" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/imgs/asap-sim2sim-clip6-ezgif.com-video-to-gif-converter.gif" width="300px" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;!-- Replace gif1.gif ... gif6.gif with your actual gif filenames and optionally add captions below each if desired --&gt; 
&lt;h2&gt;Sim2Real&lt;/h2&gt; 
&lt;p&gt;&lt;code&gt;Note from Tairan&lt;/code&gt;: make sure to make the G1 robot to 29dof following this &lt;a href="https://support.unitree.com/home/en/G1_developer/waist_fastener"&gt;doc&lt;/a&gt; and restart the robot after waist unlocking. If you don't know how to log into the Unitree Explore APP, contact unitree support.&lt;/p&gt; 
&lt;p&gt;Enter Low-Level for g1&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Open humanoid and wait until the head blue light is constantly on&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;L2+R2&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;L2+A&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;L2+B&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Connect PC to the G1 by ethernet cable and configure the network following &lt;a href="https://support.unitree.com/home/en/G1_developer/quick_development"&gt;this document&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Before starting the policy, modify the &lt;code&gt;config/g1_29dof_hist.yaml&lt;/code&gt; to set &lt;code&gt;INTERFACE&lt;/code&gt; to &lt;code&gt;eth0&lt;/code&gt; (if you are using linux), basically the network interface that you are using to connect to the robot with your PC's IP shown as &lt;code&gt;192.168.123.xxx&lt;/code&gt; in &lt;code&gt;ifconfig&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;start the policy:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python rl_policy/deepmimic_dec_loco_height.py --config=config/g1_29dof_hist.yaml --loco_model_path=./models/dec_loco/20250109_231507-noDR_rand_history_loco_stand_height_noise-decoupled_locomotion-g1_29dof/model_6600.onnx --mimic_model_paths=./models/mimic
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;click to the policy terminal and press &lt;code&gt;]&lt;/code&gt; to activate the locomotion policy&lt;/li&gt; 
 &lt;li&gt;click to the policy terminal and press &lt;code&gt;[&lt;/code&gt; to activate the asap policy (phase-based motion tracking policy)&lt;/li&gt; 
 &lt;li&gt;click to the policy terminal and press &lt;code&gt;;&lt;/code&gt; to switch to the asap policy&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;i&lt;/code&gt; to make the robot the initial position&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;o&lt;/code&gt; to emergence stop the robot&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;9&lt;/code&gt; in mujoco viewer to release the robostack&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;=&lt;/code&gt; to switch between tapping and walking for the locomotion policy&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;w/a/s/d&lt;/code&gt; to control the linear velocity&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;q/e&lt;/code&gt; to control the angular velocity&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;z&lt;/code&gt; to set all commands to zero&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;o&lt;/code&gt; to emergence stop the robot&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;‼️Alert &amp;amp; Disclaimer&lt;/h3&gt; 
&lt;p&gt;Deploying these models on physical hardware can be hazardous. Unless you have deep sim‑to‑real expertise and robust safety protocols, we strongly advise against running the model on real robots. These models are supplied for research use only, and we disclaim all responsibility for any harm, loss, or malfunction arising from their deployment.&lt;/p&gt; 
&lt;h3&gt;Demo code to collect real-world data&lt;/h3&gt; 
&lt;p&gt;We provide a demo code to collect real-world data in the &lt;code&gt;sim2real/rl_policy/listener_deltaa.py&lt;/code&gt; file. Since MoCap setup is hard to transfer across different robots/labs, we hope this code can help you to collect data for your own experiments. Contact us (&lt;a href="mailto:tairanh@andrew.cmu.edu"&gt;tairanh@andrew.cmu.edu&lt;/a&gt;) if you have any questions.&lt;/p&gt; 
&lt;h1&gt;Citation&lt;/h1&gt; 
&lt;p&gt;If you find our work useful, please consider citing us!&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@article{he2025asap,
  title={ASAP: Aligning Simulation and Real-World Physics for Learning Agile Humanoid Whole-Body Skills},
  author={He, Tairan and Gao, Jiawei and Xiao, Wenli and Zhang, Yuanhang and Wang, Zi and Wang, Jiashun and Luo, Zhengyi and He, Guanqi and Sobanbabu, Nikhil and Pan, Chaoyi and Yi, Zeji and Qu, Guannan and Kitani, Kris and Hodgins, Jessica and Fan, Linxi "Jim" and Zhu, Yuke and Liu, Changliu and Shi, Guanya},
  journal={arXiv preprint arXiv:2502.01143},
  year={2025}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;p&gt;This project is licensed under the MIT License - see the &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ostris/ai-toolkit</title>
      <link>https://github.com/ostris/ai-toolkit</link>
      <description>&lt;p&gt;The ultimate training toolkit for finetuning diffusion models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AI Toolkit by Ostris&lt;/h1&gt; 
&lt;p&gt;AI Toolkit is an all in one training suite for diffusion models. I try to support all the latest models on consumer grade hardware. Image and video models. It can be run as a GUI or CLI. It is designed to be easy to use but still have every feature imaginable.&lt;/p&gt; 
&lt;h2&gt;Support My Work&lt;/h2&gt; 
&lt;p&gt;If you enjoy my projects or use them commercially, please consider sponsoring me. Every bit helps! 💖&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/orgs/ostris"&gt;Sponsor on GitHub&lt;/a&gt; | &lt;a href="https://www.patreon.com/ostris"&gt;Support on Patreon&lt;/a&gt; | &lt;a href="https://www.paypal.com/donate/?hosted_button_id=9GEFUKC8T9R9W"&gt;Donate on PayPal&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Current Sponsors&lt;/h3&gt; 
&lt;p&gt;All of these people / organizations are the ones who selflessly make this project possible. Thank you!!&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Last updated: 2025-08-08 17:01 UTC&lt;/em&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://x.com/NuxZoe" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://pbs.twimg.com/profile_images/1919488160125616128/QAZXTMEj_400x400.png" alt="a16z" width="200" height="200" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;a href="https://github.com/replicate" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/60410876?v=4" alt="Replicate" width="200" height="200" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;a href="https://github.com/huggingface" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/25720743?v=4" alt="Hugging Face" width="200" height="200" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;a href="https://github.com/josephrocca" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/1167575?u=92d92921b4cb5c8c7e225663fed53c4b41897736&amp;amp;v=4" alt="josephrocca" width="200" height="200" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/162524101/81a72689c3754ac5b9e38612ce5ce914/eyJ3IjoyMDB9/1.png?token-hash=JHRjAxd2XxV1aXIUijj-l65pfTnLoefYSvgNPAsw2lI%3D" alt="Prasanth Veerina" width="200" height="200" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;a href="https://github.com/weights-ai" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/185568492?v=4" alt="Weights" width="200" height="200" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;hr style="width:100%;border:none;height:2px;background:#ddd;margin:30px 0;" /&gt; 
&lt;p align="center"&gt; &lt;img src="https://c8.patreon.com/4/200/93304/J" alt="Joseph Rocca" width="150" height="150" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/161471720/dd330b4036d44a5985ed5985c12a5def/eyJ3IjoyMDB9/1.jpeg?token-hash=k1f4Vv7TevzYa9tqlzAjsogYmkZs8nrXQohPCDGJGkc%3D" alt="Vladimir Sotnikov" width="150" height="150" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/33158543/C" alt="clement Delangue" width="150" height="150" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/8654302/b0f5ebedc62a47c4b56222693e1254e9/eyJ3IjoyMDB9/2.jpeg?token-hash=suI7_QjKUgWpdPuJPaIkElkTrXfItHlL8ZHLPT-w_d4%3D" alt="Misch Strotz" width="150" height="150" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/120239481/49b1ce70d3d24704b8ec34de24ec8f55/eyJ3IjoyMDB9/1.jpeg?token-hash=o0y1JqSXqtGvVXnxb06HMXjQXs6OII9yMMx5WyyUqT4%3D" alt="nitish PNR" width="150" height="150" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;/p&gt; 
&lt;hr style="width:100%;border:none;height:2px;background:#ddd;margin:30px 0;" /&gt; 
&lt;p align="center"&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/2298192/1228b69bd7d7481baf3103315183250d/eyJ3IjoyMDB9/1.jpg?token-hash=opN1e4r4Nnvqbtr8R9HI8eyf9m5F50CiHDOdHzb4UcA%3D" alt="Mohamed Oumoumad" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/548524/S" alt="Steve Hanff" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/152118848/3b15a43d71714552b5ed1c9f84e66adf/eyJ3IjoyMDB9/1.png?token-hash=MKf3sWHz0MFPm_OAFjdsNvxoBfN5B5l54mn1ORdlRy8%3D" alt="Kristjan Retter" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/83319230/M" alt="Miguel Lara" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/8449560/P" alt="Patron" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;a href="https://x.com/NuxZoe" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://pbs.twimg.com/profile_images/1916482710069014528/RDLnPRSg_400x400.jpg" alt="tungsten" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/169502989/220069e79ce745b29237e94c22a729df/eyJ3IjoyMDB9/1.png?token-hash=E8E2JOqx66k2zMtYUw8Gy57dw-gVqA6OPpdCmWFFSFw%3D" alt="Timothy Bielec" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/34200989/58ae95ebda0640c8b7a91b4fa31357aa/eyJ3IjoyMDB9/1.jpeg?token-hash=4mVDM1kCYGauYa33zLG14_g0oj9_UjDK_-Qp4zk42GE%3D" alt="Noah Miller" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/27288932/6c35d2d961ee4e14a7a368c990791315/eyJ3IjoyMDB9/1.jpeg?token-hash=TGIto_PGEG2NEKNyqwzEnRStOkhrjb3QlMhHA3raKJY%3D" alt="David Garrido" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;a href="https://x.com/RalFingerLP" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://pbs.twimg.com/profile_images/919595465041162241/ZU7X3T5k_400x400.jpg" alt="RalFinger" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;hr style="width:100%;border:none;height:2px;background:#ddd;margin:30px 0;" /&gt; 
&lt;p align="center"&gt; &lt;a href="http://www.ir-ltd.net" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://pbs.twimg.com/profile_images/1602579392198283264/6Tm2GYus_400x400.jpg" alt="IR-Entertainment Ltd" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/9547341/bb35d9a222fd460e862e960ba3eacbaf/eyJ3IjoyMDB9/1.jpeg?token-hash=Q2XGDvkCbiONeWNxBCTeTMOcuwTjOaJ8Z-CAf5xq3Hs%3D" alt="Travis Harrington" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/98811435/3a3632d1795b4c2b9f8f0270f2f6a650/eyJ3IjoyMDB9/1.jpeg?token-hash=657rzuJ0bZavMRZW3XZ-xQGqm3Vk6FkMZgFJVMCOPdk%3D" alt="EmmanuelMr18" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/81275465/1e4148fe9c47452b838949d02dd9a70f/eyJ3IjoyMDB9/1.jpeg?token-hash=YAX1ucxybpCIujUCXfdwzUQkttIn3c7pfi59uaFPSwM%3D" alt="Aaron Amortegui" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/155963250/6f8fd7075c3b4247bfeb054ba49172d6/eyJ3IjoyMDB9/1.png?token-hash=z81EHmdU2cqSrwa9vJmZTV3h0LG-z9Qakhxq34FrYT4%3D" alt="Un Defined" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/45562978/0de33cf52ec642ae8a2f612cddec4ca6/eyJ3IjoyMDB9/1.jpeg?token-hash=aD4debMD5ZQjqTII6s4zYSgVK2-bdQt9p3eipi0bENs%3D" alt="Jack English" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/27791680/J" alt="Jean-Tristan Marin" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/570742/4ceb33453a5a4745b430a216aba9280f/eyJ3IjoyMDB9/1.jpg?token-hash=nPcJ2zj3sloND9jvbnbYnob2vMXRnXdRuujthqDLWlU%3D" alt="Al H" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/82763/f99cc484361d4b9d94fe4f0814ada303/eyJ3IjoyMDB9/1.jpeg?token-hash=A3JWlBNL0b24FFWb-FCRDAyhs-OAxg-zrhfBXP_axuU%3D" alt="Doron Adler" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/103077711/bb215761cc004e80bd9cec7d4bcd636d/eyJ3IjoyMDB9/2.jpeg?token-hash=3U8kdZSUpnmeYIDVK4zK9TTXFpnAud_zOwBRXx18018%3D" alt="John Dopamine" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/99036356/7ae9c4d80e604e739b68cca12ee2ed01/eyJ3IjoyMDB9/3.png?token-hash=ZhsBMoTOZjJ-Y6h5NOmU5MT-vDb2fjK46JDlpEehkVQ%3D" alt="Noctre" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/141098579/1a9f0a1249d447a7a0df718a57343912/eyJ3IjoyMDB9/2.png?token-hash=_n-AQmPgY0FP9zCGTIEsr5ka4Y7YuaMkt3qL26ZqGg8%3D" alt="The Local Lab" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/93348210/5c650f32a0bc481d80900d2674528777/eyJ3IjoyMDB9/1.jpeg?token-hash=0jiknRw3jXqYWW6En8bNfuHgVDj4LI_rL7lSS4-_xlo%3D" alt="Armin Behjati" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/134129880/680c7e14cd1a4d1a9face921fb010f88/eyJ3IjoyMDB9/1.png?token-hash=5fqqHE6DCTbt7gDQL7VRcWkV71jF7FvWcLhpYl5aMXA%3D" alt="Bharat Prabhakar" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/70218846/C" alt="Cosmosis" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/30931983/54ab4e4ceab946e79a6418d205f9ed51/eyJ3IjoyMDB9/1.png?token-hash=j2phDrgd6IWuqKqNIDbq9fR2B3fMF-GUCQSdETS1w5Y%3D" alt="HestoySeghuro ." width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/4105384/J" alt="Jack Blakely" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/4541423/S" alt="Sören " width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;a href="https://www.youtube.com/@happyme7055" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://yt3.googleusercontent.com/ytc/AIdro_mFqhIRk99SoEWY2gvSvVp6u1SkCGMkRqYQ1OlBBeoOVp8=s160-c-k-c0x00ffffff-no-rj" alt="Marcus Rass" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;img src="https://c8.patreon.com/4/200/53077895/M" alt="Marc" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/157407541/bb9d80cffdab4334ad78366060561520/eyJ3IjoyMDB9/2.png?token-hash=WYz-U_9zabhHstOT5UIa5jBaoFwrwwqyWxWEzIR2m_c%3D" alt="Tokio Studio srl IT10640050968" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/44568304/a9d83a0e786b41b4bdada150f7c9271c/eyJ3IjoyMDB9/1.jpeg?token-hash=FtxnwrSrknQUQKvDRv2rqPceX2EF23eLq4pNQYM_fmw%3D" alt="Albert Bukoski" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/5048649/B" alt="Ben Ward" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/111904990/08b1cf65be6a4de091c9b73b693b3468/eyJ3IjoyMDB9/1.png?token-hash=_Odz6RD3CxtubEHbUxYujcjw6zAajbo3w8TRz249VBA%3D" alt="Brian Smith" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/494309/J" alt="Julian Tsependa" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/5602036/K" alt="Kelevra" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/159203973/36c817f941ac4fa18103a4b8c0cb9cae/eyJ3IjoyMDB9/1.png?token-hash=zkt72HW3EoiIEAn3LSk9gJPBsXfuTVcc4rRBS3CeR8w%3D" alt="Marko jak" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/24653779/R" alt="RayHell" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/76566911/6485eaf5ec6249a7b524ee0b979372f0/eyJ3IjoyMDB9/1.jpeg?token-hash=mwCSkTelDBaengG32NkN0lVl5mRjB-cwo6-a47wnOsU%3D" alt="the biitz" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/32633822/1ab5612efe80417cbebfe91e871fc052/eyJ3IjoyMDB9/1.png?token-hash=pOS_IU3b3RL5-iL96A3Xqoj2bQ-dDo4RUkBylcMED_s%3D" alt="Zack Abrams" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/97985240/3d1d0e6905d045aba713e8132cab4a30/eyJ3IjoyMDB9/1.png?token-hash=fRavvbO_yqWKA_OsJb5DzjfKZ1Yt-TG-ihMoeVBvlcM%3D" alt="עומר מכלוף" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;a href="https://github.com/julien-blanchon" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/11278197?v=4" alt="Blanchon" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/11198131/e696d9647feb4318bcf16243c2425805/eyJ3IjoyMDB9/1.jpeg?token-hash=c2c2p1SaiX86iXAigvGRvzm4jDHvIFCg298A49nIfUM%3D" alt="Nicholas Agranoff" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/785333/bdb9ede5765d42e5a2021a86eebf0d8f/eyJ3IjoyMDB9/2.jpg?token-hash=l_rajMhxTm6wFFPn7YdoKBxeUqhdRXKdy6_8SGCuNsE%3D" alt="Sapjes " width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/2446176/S" alt="Scott VanKirk" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/83034/W" alt="william tatum" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/138787189/2b5662dcb638466282ac758e3ac651b4/eyJ3IjoyMDB9/1.png?token-hash=zwj7MScO18vhDxhKt6s5q4gdeNJM3xCLuhSt8zlqlZs%3D" alt="Антон Антонио" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/30530914/T" alt="Techer " width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/25209707/36ae876d662d4d85aaf162b6d67d31e7/eyJ3IjoyMDB9/1.png?token-hash=Zows_A6uqlY5jClhfr4Y3QfMnDKVkS3mbxNHUDkVejo%3D" alt="fjioq8" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/46680573/ee3d99c04a674dd5a8e1ecfb926db6a2/eyJ3IjoyMDB9/1.jpeg?token-hash=cgD4EXyfZMPnXIrcqWQ5jGqzRUfqjPafb9yWfZUPB4Q%3D" alt="Neil Murray" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="Joakim Sällström" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/63510241/A" alt="Andrew Park" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;a href="https://github.com/Spikhalskiy" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/532108?u=2464983638afea8caf4cd9f0e4a7bc3e6a63bb0a&amp;amp;v=4" alt="Dmitry Spikhalsky" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;img src="https://c8.patreon.com/4/200/88567307/E" alt="el Chavo" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/117569999/55f75c57f95343e58402529cec852b26/eyJ3IjoyMDB9/1.jpeg?token-hash=squblHZH4-eMs3gI46Uqu1oTOK9sQ-0gcsFdZcB9xQg%3D" alt="James Thompson" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/66157709/6fe70df085e24464995a1a9293a53760/eyJ3IjoyMDB9/1.jpeg?token-hash=eqe0wvg6JfbRUGMKpL_x3YPI5Ppf18aUUJe2EzADU-g%3D" alt="Joey Santana" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="Heikki Rinkinen" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/6175608/B" alt="Bobbie " width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;a href="https://github.com/Slartibart23" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/133593860?u=31217adb2522fb295805824ffa7e14e8f0fca6fa&amp;amp;v=4" alt="Slarti" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="Tommy Falkowski" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/28533016/e8f6044ccfa7483f87eeaa01c894a773/eyJ3IjoyMDB9/2.png?token-hash=ak-h3JWB50hyenCavcs32AAPw6nNhmH2nBFKpdk5hvM%3D" alt="William Tatum" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="Karol Stępień" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/156564939/17dbfd45c59d4cf29853d710cb0c5d6f/eyJ3IjoyMDB9/1.png?token-hash=e6wXA_S8cgJeEDI9eJK934eB0TiM8mxJm9zW_VH0gDU%3D" alt="Hans Untch" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/59408413/B" alt="ByteC" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/3712451/432e22a355494ec0a1ea1927ff8d452e/eyJ3IjoyMDB9/7.jpeg?token-hash=OpQ9SAfVQ4Un9dSYlGTHuApZo5GlJ797Mo0DtVtMOSc%3D" alt="David Shorey" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/53634141/c1441f6c605344bbaef885d4272977bb/eyJ3IjoyMDB9/1.JPG?token-hash=Aizd6AxQhY3n6TBE5AwCVeSwEBbjALxQmu6xqc08qBo%3D" alt="Jana Spacelight" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/11180426/J" alt="jarrett towe" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/21828017/J" alt="Jim" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/63232055/2300b4ab370341b5b476902c9b8218ee/eyJ3IjoyMDB9/1.png?token-hash=R9Nb4O0aLBRwxT1cGHUMThlvf6A2MD5SO88lpZBdH7M%3D" alt="Marek P" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/9944625/P" alt="Pomoe " width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/25047900/423e4cb73aba457f8f9c6e5582eddaeb/eyJ3IjoyMDB9/1.jpeg?token-hash=81RvQXBbT66usxqtyWum9Ul4oBn3qHK1cM71IvthC-U%3D" alt="Ruairi Robinson" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/178476551/0b9e83efcd234df5a6bea30d59e6c1cd/eyJ3IjoyMDB9/1.png?token-hash=3XoYMrMxk-K6GelM22mE-FwkjFulX9hpIL7QI3wO2jI%3D" alt="Timmy" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/10876902/T" alt="Tyssel" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="Juan Franco" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;Requirements:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;python &amp;gt;3.10&lt;/li&gt; 
 &lt;li&gt;Nvidia GPU with enough ram to do what you need&lt;/li&gt; 
 &lt;li&gt;python venv&lt;/li&gt; 
 &lt;li&gt;git&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Linux:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/ostris/ai-toolkit.git
cd ai-toolkit
python3 -m venv venv
source venv/bin/activate
# install torch first
pip3 install --no-cache-dir torch==2.7.0 torchvision==0.22.0 torchaudio==2.7.0 --index-url https://download.pytorch.org/whl/cu126
pip3 install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Windows:&lt;/p&gt; 
&lt;p&gt;If you are having issues with Windows. I recommend using the easy install script at &lt;a href="https://github.com/Tavris1/AI-Toolkit-Easy-Install"&gt;https://github.com/Tavris1/AI-Toolkit-Easy-Install&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/ostris/ai-toolkit.git
cd ai-toolkit
python -m venv venv
.\venv\Scripts\activate
pip install --no-cache-dir torch==2.7.0 torchvision==0.22.0 torchaudio==2.7.0 --index-url https://download.pytorch.org/whl/cu126
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;AI Toolkit UI&lt;/h1&gt; 
&lt;img src="https://ostris.com/wp-content/uploads/2025/02/toolkit-ui.jpg" alt="AI Toolkit UI" width="100%" /&gt; 
&lt;p&gt;The AI Toolkit UI is a web interface for the AI Toolkit. It allows you to easily start, stop, and monitor jobs. It also allows you to easily train models with a few clicks. It also allows you to set a token for the UI to prevent unauthorized access so it is mostly safe to run on an exposed server.&lt;/p&gt; 
&lt;h2&gt;Running the UI&lt;/h2&gt; 
&lt;p&gt;Requirements:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Node.js &amp;gt; 18&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The UI does not need to be kept running for the jobs to run. It is only needed to start/stop/monitor jobs. The commands below will install / update the UI and it's dependencies and start the UI.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd ui
npm run build_and_start
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can now access the UI at &lt;code&gt;http://localhost:8675&lt;/code&gt; or &lt;code&gt;http://&amp;lt;your-ip&amp;gt;:8675&lt;/code&gt; if you are running it on a server.&lt;/p&gt; 
&lt;h2&gt;Securing the UI&lt;/h2&gt; 
&lt;p&gt;If you are hosting the UI on a cloud provider or any network that is not secure, I highly recommend securing it with an auth token. You can do this by setting the environment variable &lt;code&gt;AI_TOOLKIT_AUTH&lt;/code&gt; to super secure password. This token will be required to access the UI. You can set this when starting the UI like so:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Linux
AI_TOOLKIT_AUTH=super_secure_password npm run build_and_start

# Windows
set AI_TOOLKIT_AUTH=super_secure_password &amp;amp;&amp;amp; npm run build_and_start

# Windows Powershell
$env:AI_TOOLKIT_AUTH="super_secure_password"; npm run build_and_start
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;FLUX.1 Training&lt;/h2&gt; 
&lt;h3&gt;Tutorial&lt;/h3&gt; 
&lt;p&gt;To get started quickly, check out &lt;a href="https://x.com/araminta_k"&gt;@araminta_k&lt;/a&gt; tutorial on &lt;a href="https://www.youtube.com/watch?v=HzGW_Kyermg"&gt;Finetuning Flux Dev on a 3090&lt;/a&gt; with 24GB VRAM.&lt;/p&gt; 
&lt;h3&gt;Requirements&lt;/h3&gt; 
&lt;p&gt;You currently need a GPU with &lt;strong&gt;at least 24GB of VRAM&lt;/strong&gt; to train FLUX.1. If you are using it as your GPU to control your monitors, you probably need to set the flag &lt;code&gt;low_vram: true&lt;/code&gt; in the config file under &lt;code&gt;model:&lt;/code&gt;. This will quantize the model on CPU and should allow it to train with monitors attached. Users have gotten it to work on Windows with WSL, but there are some reports of a bug when running on windows natively. I have only tested on linux for now. This is still extremely experimental and a lot of quantizing and tricks had to happen to get it to fit on 24GB at all.&lt;/p&gt; 
&lt;h3&gt;FLUX.1-dev&lt;/h3&gt; 
&lt;p&gt;FLUX.1-dev has a non-commercial license. Which means anything you train will inherit the non-commercial license. It is also a gated model, so you need to accept the license on HF before using it. Otherwise, this will fail. Here are the required steps to setup a license.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Sign into HF and accept the model access here &lt;a href="https://huggingface.co/black-forest-labs/FLUX.1-dev"&gt;black-forest-labs/FLUX.1-dev&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Make a file named &lt;code&gt;.env&lt;/code&gt; in the root on this folder&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/settings/tokens/new?"&gt;Get a READ key from huggingface&lt;/a&gt; and add it to the &lt;code&gt;.env&lt;/code&gt; file like so &lt;code&gt;HF_TOKEN=your_key_here&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;FLUX.1-schnell&lt;/h3&gt; 
&lt;p&gt;FLUX.1-schnell is Apache 2.0. Anything trained on it can be licensed however you want and it does not require a HF_TOKEN to train. However, it does require a special adapter to train with it, &lt;a href="https://huggingface.co/ostris/FLUX.1-schnell-training-adapter"&gt;ostris/FLUX.1-schnell-training-adapter&lt;/a&gt;. It is also highly experimental. For best overall quality, training on FLUX.1-dev is recommended.&lt;/p&gt; 
&lt;p&gt;To use it, You just need to add the assistant to the &lt;code&gt;model&lt;/code&gt; section of your config file like so:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;      model:
        name_or_path: "black-forest-labs/FLUX.1-schnell"
        assistant_lora_path: "ostris/FLUX.1-schnell-training-adapter"
        is_flux: true
        quantize: true
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You also need to adjust your sample steps since schnell does not require as many&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;      sample:
        guidance_scale: 1  # schnell does not do guidance
        sample_steps: 4  # 1 - 4 works well
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Training&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Copy the example config file located at &lt;code&gt;config/examples/train_lora_flux_24gb.yaml&lt;/code&gt; (&lt;code&gt;config/examples/train_lora_flux_schnell_24gb.yaml&lt;/code&gt; for schnell) to the &lt;code&gt;config&lt;/code&gt; folder and rename it to &lt;code&gt;whatever_you_want.yml&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Edit the file following the comments in the file&lt;/li&gt; 
 &lt;li&gt;Run the file like so &lt;code&gt;python run.py config/whatever_you_want.yml&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;A folder with the name and the training folder from the config file will be created when you start. It will have all checkpoints and images in it. You can stop the training at any time using ctrl+c and when you resume, it will pick back up from the last checkpoint.&lt;/p&gt; 
&lt;p&gt;IMPORTANT. If you press crtl+c while it is saving, it will likely corrupt that checkpoint. So wait until it is done saving&lt;/p&gt; 
&lt;h3&gt;Need help?&lt;/h3&gt; 
&lt;p&gt;Please do not open a bug report unless it is a bug in the code. You are welcome to &lt;a href="https://discord.gg/VXmU2f5WEU"&gt;Join my Discord&lt;/a&gt; and ask for help there. However, please refrain from PMing me directly with general question or support. Ask in the discord and I will answer when I can.&lt;/p&gt; 
&lt;h2&gt;Gradio UI&lt;/h2&gt; 
&lt;p&gt;To get started training locally with a with a custom UI, once you followed the steps above and &lt;code&gt;ai-toolkit&lt;/code&gt; is installed:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd ai-toolkit #in case you are not yet in the ai-toolkit folder
huggingface-cli login #provide a `write` token to publish your LoRA at the end
python flux_train_ui.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You will instantiate a UI that will let you upload your images, caption them, train and publish your LoRA &lt;img src="https://raw.githubusercontent.com/ostris/ai-toolkit/main/assets/lora_ease_ui.png" alt="image" /&gt;&lt;/p&gt; 
&lt;h2&gt;Training in RunPod&lt;/h2&gt; 
&lt;p&gt;Example RunPod template: &lt;strong&gt;runpod/pytorch:2.2.0-py3.10-cuda12.1.1-devel-ubuntu22.04&lt;/strong&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;You need a minimum of 24GB VRAM, pick a GPU by your preference.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Example config ($0.5/hr):&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;1x A40 (48 GB VRAM)&lt;/li&gt; 
 &lt;li&gt;19 vCPU 100 GB RAM&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Custom overrides (you need some storage to clone FLUX.1, store datasets, store trained models and samples):&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;~120 GB Disk&lt;/li&gt; 
 &lt;li&gt;~120 GB Pod Volume&lt;/li&gt; 
 &lt;li&gt;Start Jupyter Notebook&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;1. Setup&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;git clone https://github.com/ostris/ai-toolkit.git
cd ai-toolkit
git submodule update --init --recursive
python -m venv venv
source venv/bin/activate
pip install torch
pip install -r requirements.txt
pip install --upgrade accelerate transformers diffusers huggingface_hub #Optional, run it if you run into issues
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Upload your dataset&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Create a new folder in the root, name it &lt;code&gt;dataset&lt;/code&gt; or whatever you like.&lt;/li&gt; 
 &lt;li&gt;Drag and drop your .jpg, .jpeg, or .png images and .txt files inside the newly created dataset folder.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;3. Login into Hugging Face with an Access Token&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Get a READ token from &lt;a href="https://huggingface.co/settings/tokens"&gt;here&lt;/a&gt; and request access to Flux.1-dev model from &lt;a href="https://huggingface.co/black-forest-labs/FLUX.1-dev"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;huggingface-cli login&lt;/code&gt; and paste your token.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;4. Training&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Copy an example config file located at &lt;code&gt;config/examples&lt;/code&gt; to the config folder and rename it to &lt;code&gt;whatever_you_want.yml&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Edit the config following the comments in the file.&lt;/li&gt; 
 &lt;li&gt;Change &lt;code&gt;folder_path: "/path/to/images/folder"&lt;/code&gt; to your dataset path like &lt;code&gt;folder_path: "/workspace/ai-toolkit/your-dataset"&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Run the file: &lt;code&gt;python run.py config/whatever_you_want.yml&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Screenshot from RunPod&lt;/h3&gt; 
&lt;img width="1728" alt="RunPod Training Screenshot" src="https://github.com/user-attachments/assets/53a1b8ef-92fa-4481-81a7-bde45a14a7b5" /&gt; 
&lt;h2&gt;Training in Modal&lt;/h2&gt; 
&lt;h3&gt;1. Setup&lt;/h3&gt; 
&lt;h4&gt;ai-toolkit:&lt;/h4&gt; 
&lt;pre&gt;&lt;code&gt;git clone https://github.com/ostris/ai-toolkit.git
cd ai-toolkit
git submodule update --init --recursive
python -m venv venv
source venv/bin/activate
pip install torch
pip install -r requirements.txt
pip install --upgrade accelerate transformers diffusers huggingface_hub #Optional, run it if you run into issues
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Modal:&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Run &lt;code&gt;pip install modal&lt;/code&gt; to install the modal Python package.&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;modal setup&lt;/code&gt; to authenticate (if this doesn’t work, try &lt;code&gt;python -m modal setup&lt;/code&gt;).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Hugging Face:&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Get a READ token from &lt;a href="https://huggingface.co/settings/tokens"&gt;here&lt;/a&gt; and request access to Flux.1-dev model from &lt;a href="https://huggingface.co/black-forest-labs/FLUX.1-dev"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;huggingface-cli login&lt;/code&gt; and paste your token.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;2. Upload your dataset&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Drag and drop your dataset folder containing the .jpg, .jpeg, or .png images and .txt files in &lt;code&gt;ai-toolkit&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;3. Configs&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Copy an example config file located at &lt;code&gt;config/examples/modal&lt;/code&gt; to the &lt;code&gt;config&lt;/code&gt; folder and rename it to &lt;code&gt;whatever_you_want.yml&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Edit the config following the comments in the file, &lt;strong&gt;&lt;ins&gt;be careful and follow the example &lt;code&gt;/root/ai-toolkit&lt;/code&gt; paths&lt;/ins&gt;&lt;/strong&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;4. Edit run_modal.py&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Set your entire local &lt;code&gt;ai-toolkit&lt;/code&gt; path at &lt;code&gt;code_mount = modal.Mount.from_local_dir&lt;/code&gt; like:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;code_mount = modal.Mount.from_local_dir("/Users/username/ai-toolkit", remote_path="/root/ai-toolkit")
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Choose a &lt;code&gt;GPU&lt;/code&gt; and &lt;code&gt;Timeout&lt;/code&gt; in &lt;code&gt;@app.function&lt;/code&gt; &lt;em&gt;(default is A100 40GB and 2 hour timeout)&lt;/em&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;5. Training&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Run the config file in your terminal: &lt;code&gt;modal run run_modal.py --config-file-list-str=/root/ai-toolkit/config/whatever_you_want.yml&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;You can monitor your training in your local terminal, or on &lt;a href="https://modal.com/"&gt;modal.com&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Models, samples and optimizer will be stored in &lt;code&gt;Storage &amp;gt; flux-lora-models&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;6. Saving the model&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Check contents of the volume by running &lt;code&gt;modal volume ls flux-lora-models&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Download the content by running &lt;code&gt;modal volume get flux-lora-models your-model-name&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Example: &lt;code&gt;modal volume get flux-lora-models my_first_flux_lora_v1&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Screenshot from Modal&lt;/h3&gt; 
&lt;img width="1728" alt="Modal Traning Screenshot" src="https://github.com/user-attachments/assets/7497eb38-0090-49d6-8ad9-9c8ea7b5388b" /&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Dataset Preparation&lt;/h2&gt; 
&lt;p&gt;Datasets generally need to be a folder containing images and associated text files. Currently, the only supported formats are jpg, jpeg, and png. Webp currently has issues. The text files should be named the same as the images but with a &lt;code&gt;.txt&lt;/code&gt; extension. For example &lt;code&gt;image2.jpg&lt;/code&gt; and &lt;code&gt;image2.txt&lt;/code&gt;. The text file should contain only the caption. You can add the word &lt;code&gt;[trigger]&lt;/code&gt; in the caption file and if you have &lt;code&gt;trigger_word&lt;/code&gt; in your config, it will be automatically replaced.&lt;/p&gt; 
&lt;p&gt;Images are never upscaled but they are downscaled and placed in buckets for batching. &lt;strong&gt;You do not need to crop/resize your images&lt;/strong&gt;. The loader will automatically resize them and can handle varying aspect ratios.&lt;/p&gt; 
&lt;h2&gt;Training Specific Layers&lt;/h2&gt; 
&lt;p&gt;To train specific layers with LoRA, you can use the &lt;code&gt;only_if_contains&lt;/code&gt; network kwargs. For instance, if you want to train only the 2 layers used by The Last Ben, &lt;a href="https://x.com/__TheBen/status/1829554120270987740"&gt;mentioned in this post&lt;/a&gt;, you can adjust your network kwargs like so:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;      network:
        type: "lora"
        linear: 128
        linear_alpha: 128
        network_kwargs:
          only_if_contains:
            - "transformer.single_transformer_blocks.7.proj_out"
            - "transformer.single_transformer_blocks.20.proj_out"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The naming conventions of the layers are in diffusers format, so checking the state dict of a model will reveal the suffix of the name of the layers you want to train. You can also use this method to only train specific groups of weights. For instance to only train the &lt;code&gt;single_transformer&lt;/code&gt; for FLUX.1, you can use the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;      network:
        type: "lora"
        linear: 128
        linear_alpha: 128
        network_kwargs:
          only_if_contains:
            - "transformer.single_transformer_blocks."
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also exclude layers by their names by using &lt;code&gt;ignore_if_contains&lt;/code&gt; network kwarg. So to exclude all the single transformer blocks,&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;      network:
        type: "lora"
        linear: 128
        linear_alpha: 128
        network_kwargs:
          ignore_if_contains:
            - "transformer.single_transformer_blocks."
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;code&gt;ignore_if_contains&lt;/code&gt; takes priority over &lt;code&gt;only_if_contains&lt;/code&gt;. So if a weight is covered by both, if will be ignored.&lt;/p&gt; 
&lt;h2&gt;LoKr Training&lt;/h2&gt; 
&lt;p&gt;To learn more about LoKr, read more about it at &lt;a href="https://github.com/KohakuBlueleaf/LyCORIS/raw/main/docs/Guidelines.md"&gt;KohakuBlueleaf/LyCORIS&lt;/a&gt;. To train a LoKr model, you can adjust the network type in the config file like so:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;      network:
        type: "lokr"
        lokr_full_rank: true
        lokr_factor: 8
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Everything else should work the same including layer targeting.&lt;/p&gt; 
&lt;h2&gt;Updates&lt;/h2&gt; 
&lt;p&gt;Only larger updates are listed here. There are usually smaller daily updated that are omitted.&lt;/p&gt; 
&lt;h3&gt;Jul 17, 2025&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Make it easy to add control images to the samples in the ui&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Jul 11, 2025&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Added better video config settings to the UI for video models.&lt;/li&gt; 
 &lt;li&gt;Added Wan I2V training to the UI&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;June 29, 2025&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Fixed issue where Kontext forced sizes on sampling&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;June 26, 2025&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Added support for FLUX.1 Kontext training&lt;/li&gt; 
 &lt;li&gt;added support for instruction dataset training&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;June 25, 2025&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Added support for OmniGen2 training&lt;/li&gt; 
 &lt;li&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;June 17, 2025&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Performance optimizations for batch preparation&lt;/li&gt; 
 &lt;li&gt;Added some docs via a popup for items in the simple ui explaining what settings do. Still a WIP&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;June 16, 2025&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Hide control images in the UI when viewing datasets&lt;/li&gt; 
 &lt;li&gt;WIP on mean flow loss&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;June 12, 2025&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Fixed issue that resulted in blank captions in the dataloader&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;June 10, 2025&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Decided to keep track up updates in the readme&lt;/li&gt; 
 &lt;li&gt;Added support for SDXL in the UI&lt;/li&gt; 
 &lt;li&gt;Added support for SD 1.5 in the UI&lt;/li&gt; 
 &lt;li&gt;Fixed UI Wan 2.1 14b name bug&lt;/li&gt; 
 &lt;li&gt;Added support for for conv training in the UI for models that support it&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>dagster-io/dagster</title>
      <link>https://github.com/dagster-io/dagster</link>
      <description>&lt;p&gt;An orchestration platform for the development, production, and observation of data assets.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;!-- Note: Do not try adding the dark mode version here with the `picture` element, it will break formatting in PyPI --&gt; 
 &lt;a target="_blank" href="https://dagster.io" style="background:none"&gt; &lt;img alt="dagster logo" src="https://raw.githubusercontent.com/dagster-io/dagster/master/.github/dagster-readme-header.svg?sanitize=true" width="auto" height="100%" /&gt; &lt;/a&gt; 
 &lt;a target="_blank" href="https://github.com/dagster-io/dagster" style="background:none"&gt; &lt;img src="https://img.shields.io/github/stars/dagster-io/dagster?labelColor=4F43DD&amp;amp;color=163B36&amp;amp;logo=github" /&gt; &lt;/a&gt; 
 &lt;a target="_blank" href="https://github.com/dagster-io/dagster/raw/master/LICENSE" style="background:none"&gt; &lt;img src="https://img.shields.io/badge/License-Apache_2.0-blue.svg?label=license&amp;amp;labelColor=4F43DD&amp;amp;color=163B36" /&gt; &lt;/a&gt; 
 &lt;a target="_blank" href="https://pypi.org/project/dagster/" style="background:none"&gt; &lt;img src="https://img.shields.io/pypi/v/dagster?labelColor=4F43DD&amp;amp;color=163B36" /&gt; &lt;/a&gt; 
 &lt;a target="_blank" href="https://pypi.org/project/dagster/" style="background:none"&gt; &lt;img src="https://img.shields.io/pypi/pyversions/dagster?labelColor=4F43DD&amp;amp;color=163B36" /&gt; &lt;/a&gt; 
 &lt;a target="_blank" href="https://twitter.com/dagster" style="background:none"&gt; &lt;img src="https://img.shields.io/badge/twitter-dagster-blue.svg?labelColor=4F43DD&amp;amp;color=163B36&amp;amp;logo=twitter" /&gt; &lt;/a&gt; 
 &lt;a target="_blank" href="https://dagster.io/slack" style="background:none"&gt; &lt;img src="https://img.shields.io/badge/slack-dagster-blue.svg?labelColor=4F43DD&amp;amp;color=163B36&amp;amp;logo=slack" /&gt; &lt;/a&gt; 
 &lt;a target="_blank" href="https://linkedin.com/showcase/dagster" style="background:none"&gt; &lt;img src="https://img.shields.io/badge/linkedin-dagster-blue.svg?labelColor=4F43DD&amp;amp;color=163B36&amp;amp;logo=linkedin" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Dagster is a cloud-native data pipeline orchestrator for the whole development lifecycle, with integrated lineage and observability, a declarative programming model, and best-in-class testability.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;It is designed for &lt;strong&gt;developing and maintaining data assets&lt;/strong&gt;, such as tables, data sets, machine learning models, and reports.&lt;/p&gt; 
&lt;p&gt;With Dagster, you declare—as Python functions—the data assets that you want to build. Dagster then helps you run your functions at the right time and keep your assets up-to-date.&lt;/p&gt; 
&lt;p&gt;Here is an example of a graph of three assets defined in Python:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from dagster import asset
from pandas import DataFrame, read_html, get_dummies
from sklearn.linear_model import LinearRegression

@asset
def country_populations() -&amp;gt; DataFrame:
    df = read_html("https://tinyurl.com/mry64ebh")[0]
    df.columns = ["country", "pop2022", "pop2023", "change", "continent", "region"]
    df["change"] = df["change"].str.rstrip("%").str.replace("−", "-").astype("float")
    return df

@asset
def continent_change_model(country_populations: DataFrame) -&amp;gt; LinearRegression:
    data = country_populations.dropna(subset=["change"])
    return LinearRegression().fit(get_dummies(data[["continent"]]), data["change"])

@asset
def continent_stats(country_populations: DataFrame, continent_change_model: LinearRegression) -&amp;gt; DataFrame:
    result = country_populations.groupby("continent").sum()
    result["pop_change_factor"] = continent_change_model.coef_
    return result
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The graph loaded into Dagster's web UI:&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img width="100%" alt="An example asset graph as rendered in the Dagster UI" src="https://raw.githubusercontent.com/dagster-io/dagster/master/.github/example-lineage.png" /&gt; &lt;/p&gt; 
&lt;p&gt;Dagster is built to be used at every stage of the data development lifecycle - local development, unit tests, integration tests, staging environments, all the way up to production.&lt;/p&gt; 
&lt;h2&gt;Quick Start:&lt;/h2&gt; 
&lt;p&gt;If you're new to Dagster, we recommend checking out the &lt;a href="https://docs.dagster.io"&gt;docs&lt;/a&gt; or following the hands-on &lt;a href="https://docs.dagster.io/etl-pipeline-tutorial/"&gt;tutorial&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Dagster is available on PyPI and officially supports Python 3.9 through Python 3.13.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install dagster dagster-webserver
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This installs two packages:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;dagster&lt;/code&gt;: The core programming model.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;dagster-webserver&lt;/code&gt;: The server that hosts Dagster's web UI for developing and operating Dagster jobs and assets.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;You can find the full Dagster documentation &lt;a href="https://docs.dagster.io"&gt;here&lt;/a&gt;, including the &lt;a href="https://docs.dagster.io/getting-started/quickstart"&gt;Quickstart guide&lt;/a&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Key Features:&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;img width="100%" alt="image" src="https://raw.githubusercontent.com/dagster-io/dagster/master/.github/key-features-cards.svg?sanitize=true" /&gt; &lt;/p&gt; 
&lt;h3&gt;Dagster as a productivity platform&lt;/h3&gt; 
&lt;p&gt;Identify the key assets you need to create using a declarative approach, or you can focus on running basic tasks. Embrace CI/CD best practices from the get-go: build reusable components, spot data quality issues, and flag bugs early.&lt;/p&gt; 
&lt;h3&gt;Dagster as a robust orchestration engine&lt;/h3&gt; 
&lt;p&gt;Put your pipelines into production with a robust multi-tenant, multi-tool engine that scales technically and organizationally.&lt;/p&gt; 
&lt;h3&gt;Dagster as a unified control plane&lt;/h3&gt; 
&lt;p&gt;Maintain control over your data as the complexity scales. Centralize your metadata in one tool with built-in observability, diagnostics, cataloging, and lineage. Spot any issues and identify performance improvement opportunities.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Master the Modern Data Stack with integrations&lt;/h2&gt; 
&lt;p&gt;Dagster provides a growing library of integrations for today’s most popular data tools. Integrate with the tools you already use, and deploy to your infrastructure.&lt;/p&gt; 
&lt;br /&gt; 
&lt;p align="center"&gt; &lt;a target="_blank" href="https://dagster.io/integrations" style="background:none"&gt; &lt;img width="100%" alt="image" src="https://raw.githubusercontent.com/dagster-io/dagster/master/.github/integrations-bar-for-readme.png" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;p&gt;Connect with thousands of other data practitioners building with Dagster. Share knowledge, get help, and contribute to the open-source project. To see featured material and upcoming events, check out our &lt;a href="https://dagster.io/community"&gt;Dagster Community&lt;/a&gt; page.&lt;/p&gt; 
&lt;p&gt;Join our community here:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;🌟 &lt;a href="https://github.com/dagster-io/dagster"&gt;Star us on GitHub&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;📥 &lt;a href="https://dagster.io/newsletter-signup"&gt;Subscribe to our Newsletter&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;🐦 &lt;a href="https://twitter.com/dagster"&gt;Follow us on Twitter&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;🕴️ &lt;a href="https://linkedin.com/showcase/dagster"&gt;Follow us on LinkedIn&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;📺 &lt;a href="https://www.youtube.com/@dagsterio"&gt;Subscribe to our YouTube channel&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;📚 &lt;a href="https://dagster.io/blog"&gt;Read our blog posts&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;👋 &lt;a href="https://dagster.io/slack"&gt;Join us on Slack&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;🗃 &lt;a href="https://discuss.dagster.io"&gt;Browse Slack archives&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;✏️ &lt;a href="https://github.com/dagster-io/dagster/discussions"&gt;Start a GitHub Discussion&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;For details on contributing or running the project for development, check out our &lt;a href="https://docs.dagster.io/about/contributing"&gt;contributing guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Dagster is &lt;a href="https://github.com/dagster-io/dagster/raw/master/LICENSE"&gt;Apache 2.0 licensed&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>datalab-to/marker</title>
      <link>https://github.com/datalab-to/marker</link>
      <description>&lt;p&gt;Convert PDF to markdown + JSON quickly with high accuracy&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Marker&lt;/h1&gt; 
&lt;p&gt;Marker converts documents to markdown, JSON, chunks, and HTML quickly and accurately.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Converts PDF, image, PPTX, DOCX, XLSX, HTML, EPUB files in all languages&lt;/li&gt; 
 &lt;li&gt;Formats tables, forms, equations, inline math, links, references, and code blocks&lt;/li&gt; 
 &lt;li&gt;Extracts and saves images&lt;/li&gt; 
 &lt;li&gt;Removes headers/footers/other artifacts&lt;/li&gt; 
 &lt;li&gt;Extensible with your own formatting and logic&lt;/li&gt; 
 &lt;li&gt;Does structured extraction, given a JSON schema (beta)&lt;/li&gt; 
 &lt;li&gt;Optionally boost accuracy with LLMs (and your own prompt)&lt;/li&gt; 
 &lt;li&gt;Works on GPU, CPU, or MPS&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Performance&lt;/h2&gt; 
&lt;img src="https://raw.githubusercontent.com/datalab-to/marker/master/data/images/overall.png" width="800px" /&gt; 
&lt;p&gt;Marker benchmarks favorably compared to cloud services like Llamaparse and Mathpix, as well as other open source tools.&lt;/p&gt; 
&lt;p&gt;The above results are running single PDF pages serially. Marker is significantly faster when running in batch mode, with a projected throughput of 25 pages/second on an H100.&lt;/p&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/datalab-to/marker/master/#benchmarks"&gt;below&lt;/a&gt; for detailed speed and accuracy benchmarks, and instructions on how to run your own benchmarks.&lt;/p&gt; 
&lt;h2&gt;Hybrid Mode&lt;/h2&gt; 
&lt;p&gt;For the highest accuracy, pass the &lt;code&gt;--use_llm&lt;/code&gt; flag to use an LLM alongside marker. This will do things like merge tables across pages, handle inline math, format tables properly, and extract values from forms. It can use any gemini or ollama model. By default, it uses &lt;code&gt;gemini-2.0-flash&lt;/code&gt;. See &lt;a href="https://raw.githubusercontent.com/datalab-to/marker/master/#llm-services"&gt;below&lt;/a&gt; for details.&lt;/p&gt; 
&lt;p&gt;Here is a table benchmark comparing marker, gemini flash alone, and marker with use_llm:&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/datalab-to/marker/master/data/images/table.png" width="400px" /&gt; 
&lt;p&gt;As you can see, the use_llm mode offers higher accuracy than marker or gemini alone.&lt;/p&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;PDF&lt;/th&gt; 
   &lt;th&gt;File type&lt;/th&gt; 
   &lt;th&gt;Markdown&lt;/th&gt; 
   &lt;th&gt;JSON&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://greenteapress.com/thinkpython/thinkpython.pdf"&gt;Think Python&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Textbook&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/VikParuchuri/marker/raw/master/data/examples/markdown/thinkpython/thinkpython.md"&gt;View&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/VikParuchuri/marker/raw/master/data/examples/json/thinkpython.json"&gt;View&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/2101.03961.pdf"&gt;Switch Transformers&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;arXiv paper&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/VikParuchuri/marker/raw/master/data/examples/markdown/switch_transformers/switch_trans.md"&gt;View&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/VikParuchuri/marker/raw/master/data/examples/json/switch_trans.json"&gt;View&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/1804.07821.pdf"&gt;Multi-column CNN&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;arXiv paper&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/VikParuchuri/marker/raw/master/data/examples/markdown/multicolcnn/multicolcnn.md"&gt;View&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/VikParuchuri/marker/raw/master/data/examples/json/multicolcnn.json"&gt;View&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;Commercial usage&lt;/h1&gt; 
&lt;p&gt;I want marker to be as widely accessible as possible, while still funding my development/training costs. Research and personal usage is always okay, but there are some restrictions on commercial usage.&lt;/p&gt; 
&lt;p&gt;The weights for the models are licensed &lt;code&gt;cc-by-nc-sa-4.0&lt;/code&gt;, but I will waive that for any organization under $2M USD in gross revenue in the most recent 12-month period AND under $2M in lifetime VC/angel funding raised. You also must not be competitive with the &lt;a href="https://www.datalab.to/"&gt;Datalab API&lt;/a&gt;. If you want to remove the GPL license requirements (dual-license) and/or use the weights commercially over the revenue limit, check out the options &lt;a href="https://www.datalab.to"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Hosted API&lt;/h1&gt; 
&lt;p&gt;There's a hosted API for marker available &lt;a href="https://www.datalab.to/"&gt;here&lt;/a&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Supports PDF, image, PPT, PPTX, DOC, DOCX, XLS, XLSX, HTML, EPUB files&lt;/li&gt; 
 &lt;li&gt;1/4th the price of leading cloud-based competitors&lt;/li&gt; 
 &lt;li&gt;Fast - ~15s for a 250 page PDF&lt;/li&gt; 
 &lt;li&gt;Supports LLM mode&lt;/li&gt; 
 &lt;li&gt;High uptime (99.99%)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Community&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://discord.gg//KuZwXNGnfH"&gt;Discord&lt;/a&gt; is where we discuss future development.&lt;/p&gt; 
&lt;h1&gt;Installation&lt;/h1&gt; 
&lt;p&gt;You'll need python 3.10+ and &lt;a href="https://pytorch.org/get-started/locally/"&gt;PyTorch&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Install with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install marker-pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you want to use marker on documents other than PDFs, you will need to install additional dependencies with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install marker-pdf[full]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Usage&lt;/h1&gt; 
&lt;p&gt;First, some configuration:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Your torch device will be automatically detected, but you can override this. For example, &lt;code&gt;TORCH_DEVICE=cuda&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Some PDFs, even digital ones, have bad text in them. Set &lt;code&gt;--force_ocr&lt;/code&gt; to force OCR on all lines, or the &lt;code&gt;strip_existing_ocr&lt;/code&gt; to keep all digital text, and strip out any existing OCR text.&lt;/li&gt; 
 &lt;li&gt;If you care about inline math, set &lt;code&gt;force_ocr&lt;/code&gt; to convert inline math to LaTeX.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Interactive App&lt;/h2&gt; 
&lt;p&gt;I've included a streamlit app that lets you interactively try marker with some basic options. Run it with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install streamlit streamlit-ace
marker_gui
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Convert a single file&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;marker_single /path/to/file.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can pass in PDFs or images.&lt;/p&gt; 
&lt;p&gt;Options:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--page_range TEXT&lt;/code&gt;: Specify which pages to process. Accepts comma-separated page numbers and ranges. Example: &lt;code&gt;--page_range "0,5-10,20"&lt;/code&gt; will process pages 0, 5 through 10, and page 20.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--output_format [markdown|json|html|chunks]&lt;/code&gt;: Specify the format for the output results.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--output_dir PATH&lt;/code&gt;: Directory where output files will be saved. Defaults to the value specified in settings.OUTPUT_DIR.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--paginate_output&lt;/code&gt;: Paginates the output, using &lt;code&gt;\n\n{PAGE_NUMBER}&lt;/code&gt; followed by &lt;code&gt;-&lt;/code&gt; * 48, then &lt;code&gt;\n\n&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--use_llm&lt;/code&gt;: Uses an LLM to improve accuracy. You will need to configure the LLM backend - see &lt;a href="https://raw.githubusercontent.com/datalab-to/marker/master/#llm-services"&gt;below&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--force_ocr&lt;/code&gt;: Force OCR processing on the entire document, even for pages that might contain extractable text. This will also format inline math properly.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--block_correction_prompt&lt;/code&gt;: if LLM mode is active, an optional prompt that will be used to correct the output of marker. This is useful for custom formatting or logic that you want to apply to the output.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--strip_existing_ocr&lt;/code&gt;: Remove all existing OCR text in the document and re-OCR with surya.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--redo_inline_math&lt;/code&gt;: If you want the absolute highest quality inline math conversion, use this along with &lt;code&gt;--use_llm&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--disable_image_extraction&lt;/code&gt;: Don't extract images from the PDF. If you also specify &lt;code&gt;--use_llm&lt;/code&gt;, then images will be replaced with a description.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--debug&lt;/code&gt;: Enable debug mode for additional logging and diagnostic information.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--processors TEXT&lt;/code&gt;: Override the default processors by providing their full module paths, separated by commas. Example: &lt;code&gt;--processors "module1.processor1,module2.processor2"&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--config_json PATH&lt;/code&gt;: Path to a JSON configuration file containing additional settings.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;config --help&lt;/code&gt;: List all available builders, processors, and converters, and their associated configuration. These values can be used to build a JSON configuration file for additional tweaking of marker defaults.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--converter_cls&lt;/code&gt;: One of &lt;code&gt;marker.converters.pdf.PdfConverter&lt;/code&gt; (default) or &lt;code&gt;marker.converters.table.TableConverter&lt;/code&gt;. The &lt;code&gt;PdfConverter&lt;/code&gt; will convert the whole PDF, the &lt;code&gt;TableConverter&lt;/code&gt; will only extract and convert tables.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--llm_service&lt;/code&gt;: Which llm service to use if &lt;code&gt;--use_llm&lt;/code&gt; is passed. This defaults to &lt;code&gt;marker.services.gemini.GoogleGeminiService&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--help&lt;/code&gt;: see all of the flags that can be passed into marker. (it supports many more options then are listed above)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The list of supported languages for surya OCR is &lt;a href="https://github.com/VikParuchuri/surya/raw/master/surya/recognition/languages.py"&gt;here&lt;/a&gt;. If you don't need OCR, marker can work with any language.&lt;/p&gt; 
&lt;h2&gt;Convert multiple files&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;marker /path/to/input/folder
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;marker&lt;/code&gt; supports all the same options from &lt;code&gt;marker_single&lt;/code&gt; above.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--workers&lt;/code&gt; is the number of conversion workers to run simultaneously. This is automatically set by default, but you can increase it to increase throughput, at the cost of more CPU/GPU usage. Marker will use 5GB of VRAM per worker at the peak, and 3.5GB average.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Convert multiple files on multiple GPUs&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;NUM_DEVICES=4 NUM_WORKERS=15 marker_chunk_convert ../pdf_in ../md_out
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;NUM_DEVICES&lt;/code&gt; is the number of GPUs to use. Should be &lt;code&gt;2&lt;/code&gt; or greater.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;NUM_WORKERS&lt;/code&gt; is the number of parallel processes to run on each GPU.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Use from python&lt;/h2&gt; 
&lt;p&gt;See the &lt;code&gt;PdfConverter&lt;/code&gt; class at &lt;code&gt;marker/converters/pdf.py&lt;/code&gt; function for additional arguments that can be passed.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from marker.converters.pdf import PdfConverter
from marker.models import create_model_dict
from marker.output import text_from_rendered

converter = PdfConverter(
    artifact_dict=create_model_dict(),
)
rendered = converter("FILEPATH")
text, _, images = text_from_rendered(rendered)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;code&gt;rendered&lt;/code&gt; will be a pydantic basemodel with different properties depending on the output type requested. With markdown output (default), you'll have the properties &lt;code&gt;markdown&lt;/code&gt;, &lt;code&gt;metadata&lt;/code&gt;, and &lt;code&gt;images&lt;/code&gt;. For json output, you'll have &lt;code&gt;children&lt;/code&gt;, &lt;code&gt;block_type&lt;/code&gt;, and &lt;code&gt;metadata&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Custom configuration&lt;/h3&gt; 
&lt;p&gt;You can pass configuration using the &lt;code&gt;ConfigParser&lt;/code&gt;. To see all available options, do &lt;code&gt;marker_single --help&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from marker.converters.pdf import PdfConverter
from marker.models import create_model_dict
from marker.config.parser import ConfigParser

config = {
    "output_format": "json",
    "ADDITIONAL_KEY": "VALUE"
}
config_parser = ConfigParser(config)

converter = PdfConverter(
    config=config_parser.generate_config_dict(),
    artifact_dict=create_model_dict(),
    processor_list=config_parser.get_processors(),
    renderer=config_parser.get_renderer(),
    llm_service=config_parser.get_llm_service()
)
rendered = converter("FILEPATH")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Extract blocks&lt;/h3&gt; 
&lt;p&gt;Each document consists of one or more pages. Pages contain blocks, which can themselves contain other blocks. It's possible to programmatically manipulate these blocks.&lt;/p&gt; 
&lt;p&gt;Here's an example of extracting all forms from a document:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from marker.converters.pdf import PdfConverter
from marker.models import create_model_dict
from marker.schema import BlockTypes

converter = PdfConverter(
    artifact_dict=create_model_dict(),
)
document = converter.build_document("FILEPATH")
forms = document.contained_blocks((BlockTypes.Form,))
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Look at the processors for more examples of extracting and manipulating blocks.&lt;/p&gt; 
&lt;h2&gt;Other converters&lt;/h2&gt; 
&lt;p&gt;You can also use other converters that define different conversion pipelines:&lt;/p&gt; 
&lt;h3&gt;Extract tables&lt;/h3&gt; 
&lt;p&gt;The &lt;code&gt;TableConverter&lt;/code&gt; will only convert and extract tables:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from marker.converters.table import TableConverter
from marker.models import create_model_dict
from marker.output import text_from_rendered

converter = TableConverter(
    artifact_dict=create_model_dict(),
)
rendered = converter("FILEPATH")
text, _, images = text_from_rendered(rendered)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This takes all the same configuration as the PdfConverter. You can specify the configuration &lt;code&gt;force_layout_block=Table&lt;/code&gt; to avoid layout detection and instead assume every page is a table. Set &lt;code&gt;output_format=json&lt;/code&gt; to also get cell bounding boxes.&lt;/p&gt; 
&lt;p&gt;You can also run this via the CLI with&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;marker_single FILENAME --use_llm --force_layout_block Table --converter_cls marker.converters.table.TableConverter --output_format json
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;OCR Only&lt;/h3&gt; 
&lt;p&gt;If you only want to run OCR, you can also do that through the &lt;code&gt;OCRConverter&lt;/code&gt;. Set &lt;code&gt;--keep_chars&lt;/code&gt; to keep individual characters and bounding boxes.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from marker.converters.ocr import OCRConverter
from marker.models import create_model_dict

converter = OCRConverter(
    artifact_dict=create_model_dict(),
)
rendered = converter("FILEPATH")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This takes all the same configuration as the PdfConverter.&lt;/p&gt; 
&lt;p&gt;You can also run this via the CLI with&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;marker_single FILENAME --converter_cls marker.converters.ocr.OCRConverter
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Structured Extraction (beta)&lt;/h3&gt; 
&lt;p&gt;You can run structured extraction via the &lt;code&gt;ExtractionConverter&lt;/code&gt;. This requires an llm service to be setup first (see &lt;a href="https://raw.githubusercontent.com/datalab-to/marker/master/#llm-services"&gt;here&lt;/a&gt; for details). You'll get a JSON output with the extracted values.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from marker.converters.extraction import ExtractionConverter
from marker.models import create_model_dict
from marker.config.parser import ConfigParser
from pydantic import BaseModel

class Links(BaseModel):
    links: list[str]
    
schema = Links.model_json_schema()
config_parser = ConfigParser({
    "page_schema": schema
})

converter = ExtractionConverter(
    artifact_dict=create_model_dict(),
    config=config_parser.generate_config_dict(),
    llm_service=config_parser.get_llm_service(),
)
rendered = converter("FILEPATH")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Rendered will have an &lt;code&gt;original_markdown&lt;/code&gt; field. If you pass this back in next time you run the converter, as the &lt;code&gt;existing_markdown&lt;/code&gt; config key, you can skip re-parsing the document.&lt;/p&gt; 
&lt;h1&gt;Output Formats&lt;/h1&gt; 
&lt;h2&gt;Markdown&lt;/h2&gt; 
&lt;p&gt;Markdown output will include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;image links (images will be saved in the same folder)&lt;/li&gt; 
 &lt;li&gt;formatted tables&lt;/li&gt; 
 &lt;li&gt;embedded LaTeX equations (fenced with &lt;code&gt;$$&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Code is fenced with triple backticks&lt;/li&gt; 
 &lt;li&gt;Superscripts for footnotes&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;HTML&lt;/h2&gt; 
&lt;p&gt;HTML output is similar to markdown output:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Images are included via &lt;code&gt;img&lt;/code&gt; tags&lt;/li&gt; 
 &lt;li&gt;equations are fenced with &lt;code&gt;&amp;lt;math&amp;gt;&lt;/code&gt; tags&lt;/li&gt; 
 &lt;li&gt;code is in &lt;code&gt;pre&lt;/code&gt; tags&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;JSON&lt;/h2&gt; 
&lt;p&gt;JSON output will be organized in a tree-like structure, with the leaf nodes being blocks. Examples of leaf nodes are a single list item, a paragraph of text, or an image.&lt;/p&gt; 
&lt;p&gt;The output will be a list, with each list item representing a page. Each page is considered a block in the internal marker schema. There are different types of blocks to represent different elements.&lt;/p&gt; 
&lt;p&gt;Pages have the keys:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;id&lt;/code&gt; - unique id for the block.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;block_type&lt;/code&gt; - the type of block. The possible block types can be seen in &lt;code&gt;marker/schema/__init__.py&lt;/code&gt;. As of this writing, they are ["Line", "Span", "FigureGroup", "TableGroup", "ListGroup", "PictureGroup", "Page", "Caption", "Code", "Figure", "Footnote", "Form", "Equation", "Handwriting", "TextInlineMath", "ListItem", "PageFooter", "PageHeader", "Picture", "SectionHeader", "Table", "Text", "TableOfContents", "Document"]&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;html&lt;/code&gt; - the HTML for the page. Note that this will have recursive references to children. The &lt;code&gt;content-ref&lt;/code&gt; tags must be replaced with the child content if you want the full html. You can see an example of this at &lt;code&gt;marker/output.py:json_to_html&lt;/code&gt;. That function will take in a single block from the json output, and turn it into HTML.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;polygon&lt;/code&gt; - the 4-corner polygon of the page, in (x1,y1), (x2,y2), (x3, y3), (x4, y4) format. (x1,y1) is the top left, and coordinates go clockwise.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;children&lt;/code&gt; - the child blocks.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The child blocks have two additional keys:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;section_hierarchy&lt;/code&gt; - indicates the sections that the block is part of. &lt;code&gt;1&lt;/code&gt; indicates an h1 tag, &lt;code&gt;2&lt;/code&gt; an h2, and so on.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;images&lt;/code&gt; - base64 encoded images. The key will be the block id, and the data will be the encoded image.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Note that child blocks of pages can have their own children as well (a tree structure).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
      "id": "/page/10/Page/366",
      "block_type": "Page",
      "html": "&amp;lt;content-ref src='/page/10/SectionHeader/0'&amp;gt;&amp;lt;/content-ref&amp;gt;&amp;lt;content-ref src='/page/10/SectionHeader/1'&amp;gt;&amp;lt;/content-ref&amp;gt;&amp;lt;content-ref src='/page/10/Text/2'&amp;gt;&amp;lt;/content-ref&amp;gt;&amp;lt;content-ref src='/page/10/Text/3'&amp;gt;&amp;lt;/content-ref&amp;gt;&amp;lt;content-ref src='/page/10/Figure/4'&amp;gt;&amp;lt;/content-ref&amp;gt;&amp;lt;content-ref src='/page/10/SectionHeader/5'&amp;gt;&amp;lt;/content-ref&amp;gt;&amp;lt;content-ref src='/page/10/SectionHeader/6'&amp;gt;&amp;lt;/content-ref&amp;gt;&amp;lt;content-ref src='/page/10/TextInlineMath/7'&amp;gt;&amp;lt;/content-ref&amp;gt;&amp;lt;content-ref src='/page/10/TextInlineMath/8'&amp;gt;&amp;lt;/content-ref&amp;gt;&amp;lt;content-ref src='/page/10/Table/9'&amp;gt;&amp;lt;/content-ref&amp;gt;&amp;lt;content-ref src='/page/10/SectionHeader/10'&amp;gt;&amp;lt;/content-ref&amp;gt;&amp;lt;content-ref src='/page/10/Text/11'&amp;gt;&amp;lt;/content-ref&amp;gt;",
      "polygon": [[0.0, 0.0], [612.0, 0.0], [612.0, 792.0], [0.0, 792.0]],
      "children": [
        {
          "id": "/page/10/SectionHeader/0",
          "block_type": "SectionHeader",
          "html": "&amp;lt;h1&amp;gt;Supplementary Material for &amp;lt;i&amp;gt;Subspace Adversarial Training&amp;lt;/i&amp;gt; &amp;lt;/h1&amp;gt;",
          "polygon": [
            [217.845703125, 80.630859375], [374.73046875, 80.630859375],
            [374.73046875, 107.0],
            [217.845703125, 107.0]
          ],
          "children": null,
          "section_hierarchy": {
            "1": "/page/10/SectionHeader/1"
          },
          "images": {}
        },
        ...
        ]
    }


&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Chunks&lt;/h2&gt; 
&lt;p&gt;Chunks format is similar to JSON, but flattens everything into a single list instead of a tree. Only the top level blocks from each page show up. It also has the full HTML of each block inside, so you don't need to crawl the tree to reconstruct it. This enable flexible and easy chunking for RAG.&lt;/p&gt; 
&lt;h2&gt;Metadata&lt;/h2&gt; 
&lt;p&gt;All output formats will return a metadata dictionary, with the following fields:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
    "table_of_contents": [
      {
        "title": "Introduction",
        "heading_level": 1,
        "page_id": 0,
        "polygon": [...]
      }
    ], // computed PDF table of contents
    "page_stats": [
      {
        "page_id":  0, 
        "text_extraction_method": "pdftext",
        "block_counts": [("Span", 200), ...]
      },
      ...
    ]
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;LLM Services&lt;/h1&gt; 
&lt;p&gt;When running with the &lt;code&gt;--use_llm&lt;/code&gt; flag, you have a choice of services you can use:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;Gemini&lt;/code&gt; - this will use the Gemini developer API by default. You'll need to pass &lt;code&gt;--gemini_api_key&lt;/code&gt; to configuration.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Google Vertex&lt;/code&gt; - this will use vertex, which can be more reliable. You'll need to pass &lt;code&gt;--vertex_project_id&lt;/code&gt;. To use it, set &lt;code&gt;--llm_service=marker.services.vertex.GoogleVertexService&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Ollama&lt;/code&gt; - this will use local models. You can configure &lt;code&gt;--ollama_base_url&lt;/code&gt; and &lt;code&gt;--ollama_model&lt;/code&gt;. To use it, set &lt;code&gt;--llm_service=marker.services.ollama.OllamaService&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Claude&lt;/code&gt; - this will use the anthropic API. You can configure &lt;code&gt;--claude_api_key&lt;/code&gt;, and &lt;code&gt;--claude_model_name&lt;/code&gt;. To use it, set &lt;code&gt;--llm_service=marker.services.claude.ClaudeService&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;OpenAI&lt;/code&gt; - this supports any openai-like endpoint. You can configure &lt;code&gt;--openai_api_key&lt;/code&gt;, &lt;code&gt;--openai_model&lt;/code&gt;, and &lt;code&gt;--openai_base_url&lt;/code&gt;. To use it, set &lt;code&gt;--llm_service=marker.services.openai.OpenAIService&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Azure OpenAI&lt;/code&gt; - this uses the Azure OpenAI service. You can configure &lt;code&gt;--azure_endpoint&lt;/code&gt;, &lt;code&gt;--azure_api_key&lt;/code&gt;, and &lt;code&gt;--deployment_name&lt;/code&gt;. To use it, set &lt;code&gt;--llm_service=marker.services.azure_openai.AzureOpenAIService&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;These services may have additional optional configuration as well - you can see it by viewing the classes.&lt;/p&gt; 
&lt;h1&gt;Internals&lt;/h1&gt; 
&lt;p&gt;Marker is easy to extend. The core units of marker are:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;Providers&lt;/code&gt;, at &lt;code&gt;marker/providers&lt;/code&gt;. These provide information from a source file, like a PDF.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Builders&lt;/code&gt;, at &lt;code&gt;marker/builders&lt;/code&gt;. These generate the initial document blocks and fill in text, using info from the providers.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Processors&lt;/code&gt;, at &lt;code&gt;marker/processors&lt;/code&gt;. These process specific blocks, for example the table formatter is a processor.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Renderers&lt;/code&gt;, at &lt;code&gt;marker/renderers&lt;/code&gt;. These use the blocks to render output.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Schema&lt;/code&gt;, at &lt;code&gt;marker/schema&lt;/code&gt;. The classes for all the block types.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Converters&lt;/code&gt;, at &lt;code&gt;marker/converters&lt;/code&gt;. They run the whole end to end pipeline.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To customize processing behavior, override the &lt;code&gt;processors&lt;/code&gt;. To add new output formats, write a new &lt;code&gt;renderer&lt;/code&gt;. For additional input formats, write a new &lt;code&gt;provider.&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Processors and renderers can be directly passed into the base &lt;code&gt;PDFConverter&lt;/code&gt;, so you can specify your own custom processing easily.&lt;/p&gt; 
&lt;h2&gt;API server&lt;/h2&gt; 
&lt;p&gt;There is a very simple API server you can run like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install -U uvicorn fastapi python-multipart
marker_server --port 8001
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will start a fastapi server that you can access at &lt;code&gt;localhost:8001&lt;/code&gt;. You can go to &lt;code&gt;localhost:8001/docs&lt;/code&gt; to see the endpoint options.&lt;/p&gt; 
&lt;p&gt;You can send requests like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;import requests
import json

post_data = {
    'filepath': 'FILEPATH',
    # Add other params here
}

requests.post("http://localhost:8001/marker", data=json.dumps(post_data)).json()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note that this is not a very robust API, and is only intended for small-scale use. If you want to use this server, but want a more robust conversion option, you can use the hosted &lt;a href="https://www.datalab.to/plans"&gt;Datalab API&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Troubleshooting&lt;/h1&gt; 
&lt;p&gt;There are some settings that you may find useful if things aren't working the way you expect:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;If you have issues with accuracy, try setting &lt;code&gt;--use_llm&lt;/code&gt; to use an LLM to improve quality. You must set &lt;code&gt;GOOGLE_API_KEY&lt;/code&gt; to a Gemini API key for this to work.&lt;/li&gt; 
 &lt;li&gt;Make sure to set &lt;code&gt;force_ocr&lt;/code&gt; if you see garbled text - this will re-OCR the document.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;TORCH_DEVICE&lt;/code&gt; - set this to force marker to use a given torch device for inference.&lt;/li&gt; 
 &lt;li&gt;If you're getting out of memory errors, decrease worker count. You can also try splitting up long PDFs into multiple files.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Debugging&lt;/h2&gt; 
&lt;p&gt;Pass the &lt;code&gt;debug&lt;/code&gt; option to activate debug mode. This will save images of each page with detected layout and text, as well as output a json file with additional bounding box information.&lt;/p&gt; 
&lt;h1&gt;Benchmarks&lt;/h1&gt; 
&lt;h2&gt;Overall PDF Conversion&lt;/h2&gt; 
&lt;p&gt;We created a &lt;a href="https://huggingface.co/datasets/datalab-to/marker_benchmark"&gt;benchmark set&lt;/a&gt; by extracting single PDF pages from common crawl. We scored based on a heuristic that aligns text with ground truth text segments, and an LLM as a judge scoring method.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Method&lt;/th&gt; 
   &lt;th&gt;Avg Time&lt;/th&gt; 
   &lt;th&gt;Heuristic Score&lt;/th&gt; 
   &lt;th&gt;LLM Score&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;marker&lt;/td&gt; 
   &lt;td&gt;2.83837&lt;/td&gt; 
   &lt;td&gt;95.6709&lt;/td&gt; 
   &lt;td&gt;4.23916&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;llamaparse&lt;/td&gt; 
   &lt;td&gt;23.348&lt;/td&gt; 
   &lt;td&gt;84.2442&lt;/td&gt; 
   &lt;td&gt;3.97619&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;mathpix&lt;/td&gt; 
   &lt;td&gt;6.36223&lt;/td&gt; 
   &lt;td&gt;86.4281&lt;/td&gt; 
   &lt;td&gt;4.15626&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;docling&lt;/td&gt; 
   &lt;td&gt;3.69949&lt;/td&gt; 
   &lt;td&gt;86.7073&lt;/td&gt; 
   &lt;td&gt;3.70429&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Benchmarks were run on an H100 for markjer and docling - llamaparse and mathpix used their cloud services. We can also look at it by document type:&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/datalab-to/marker/master/data/images/per_doc.png" width="1000px" /&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Document Type&lt;/th&gt; 
   &lt;th&gt;Marker heuristic&lt;/th&gt; 
   &lt;th&gt;Marker LLM&lt;/th&gt; 
   &lt;th&gt;Llamaparse Heuristic&lt;/th&gt; 
   &lt;th&gt;Llamaparse LLM&lt;/th&gt; 
   &lt;th&gt;Mathpix Heuristic&lt;/th&gt; 
   &lt;th&gt;Mathpix LLM&lt;/th&gt; 
   &lt;th&gt;Docling Heuristic&lt;/th&gt; 
   &lt;th&gt;Docling LLM&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Scientific paper&lt;/td&gt; 
   &lt;td&gt;96.6737&lt;/td&gt; 
   &lt;td&gt;4.34899&lt;/td&gt; 
   &lt;td&gt;87.1651&lt;/td&gt; 
   &lt;td&gt;3.96421&lt;/td&gt; 
   &lt;td&gt;91.2267&lt;/td&gt; 
   &lt;td&gt;4.46861&lt;/td&gt; 
   &lt;td&gt;92.135&lt;/td&gt; 
   &lt;td&gt;3.72422&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Book page&lt;/td&gt; 
   &lt;td&gt;97.1846&lt;/td&gt; 
   &lt;td&gt;4.16168&lt;/td&gt; 
   &lt;td&gt;90.9532&lt;/td&gt; 
   &lt;td&gt;4.07186&lt;/td&gt; 
   &lt;td&gt;93.8886&lt;/td&gt; 
   &lt;td&gt;4.35329&lt;/td&gt; 
   &lt;td&gt;90.0556&lt;/td&gt; 
   &lt;td&gt;3.64671&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Other&lt;/td&gt; 
   &lt;td&gt;95.1632&lt;/td&gt; 
   &lt;td&gt;4.25076&lt;/td&gt; 
   &lt;td&gt;81.1385&lt;/td&gt; 
   &lt;td&gt;4.01835&lt;/td&gt; 
   &lt;td&gt;79.6231&lt;/td&gt; 
   &lt;td&gt;4.00306&lt;/td&gt; 
   &lt;td&gt;83.8223&lt;/td&gt; 
   &lt;td&gt;3.76147&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Form&lt;/td&gt; 
   &lt;td&gt;88.0147&lt;/td&gt; 
   &lt;td&gt;3.84663&lt;/td&gt; 
   &lt;td&gt;66.3081&lt;/td&gt; 
   &lt;td&gt;3.68712&lt;/td&gt; 
   &lt;td&gt;64.7512&lt;/td&gt; 
   &lt;td&gt;3.33129&lt;/td&gt; 
   &lt;td&gt;68.3857&lt;/td&gt; 
   &lt;td&gt;3.40491&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Presentation&lt;/td&gt; 
   &lt;td&gt;95.1562&lt;/td&gt; 
   &lt;td&gt;4.13669&lt;/td&gt; 
   &lt;td&gt;81.2261&lt;/td&gt; 
   &lt;td&gt;4&lt;/td&gt; 
   &lt;td&gt;83.6737&lt;/td&gt; 
   &lt;td&gt;3.95683&lt;/td&gt; 
   &lt;td&gt;84.8405&lt;/td&gt; 
   &lt;td&gt;3.86331&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Financial document&lt;/td&gt; 
   &lt;td&gt;95.3697&lt;/td&gt; 
   &lt;td&gt;4.39106&lt;/td&gt; 
   &lt;td&gt;82.5812&lt;/td&gt; 
   &lt;td&gt;4.16111&lt;/td&gt; 
   &lt;td&gt;81.3115&lt;/td&gt; 
   &lt;td&gt;4.05556&lt;/td&gt; 
   &lt;td&gt;86.3882&lt;/td&gt; 
   &lt;td&gt;3.8&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Letter&lt;/td&gt; 
   &lt;td&gt;98.4021&lt;/td&gt; 
   &lt;td&gt;4.5&lt;/td&gt; 
   &lt;td&gt;93.4477&lt;/td&gt; 
   &lt;td&gt;4.28125&lt;/td&gt; 
   &lt;td&gt;96.0383&lt;/td&gt; 
   &lt;td&gt;4.45312&lt;/td&gt; 
   &lt;td&gt;92.0952&lt;/td&gt; 
   &lt;td&gt;4.09375&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Engineering document&lt;/td&gt; 
   &lt;td&gt;93.9244&lt;/td&gt; 
   &lt;td&gt;4.04412&lt;/td&gt; 
   &lt;td&gt;77.4854&lt;/td&gt; 
   &lt;td&gt;3.72059&lt;/td&gt; 
   &lt;td&gt;80.3319&lt;/td&gt; 
   &lt;td&gt;3.88235&lt;/td&gt; 
   &lt;td&gt;79.6807&lt;/td&gt; 
   &lt;td&gt;3.42647&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Legal document&lt;/td&gt; 
   &lt;td&gt;96.689&lt;/td&gt; 
   &lt;td&gt;4.27759&lt;/td&gt; 
   &lt;td&gt;86.9769&lt;/td&gt; 
   &lt;td&gt;3.87584&lt;/td&gt; 
   &lt;td&gt;91.601&lt;/td&gt; 
   &lt;td&gt;4.20805&lt;/td&gt; 
   &lt;td&gt;87.8383&lt;/td&gt; 
   &lt;td&gt;3.65552&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Newspaper page&lt;/td&gt; 
   &lt;td&gt;98.8733&lt;/td&gt; 
   &lt;td&gt;4.25806&lt;/td&gt; 
   &lt;td&gt;84.7492&lt;/td&gt; 
   &lt;td&gt;3.90323&lt;/td&gt; 
   &lt;td&gt;96.9963&lt;/td&gt; 
   &lt;td&gt;4.45161&lt;/td&gt; 
   &lt;td&gt;92.6496&lt;/td&gt; 
   &lt;td&gt;3.51613&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Magazine page&lt;/td&gt; 
   &lt;td&gt;98.2145&lt;/td&gt; 
   &lt;td&gt;4.38776&lt;/td&gt; 
   &lt;td&gt;87.2902&lt;/td&gt; 
   &lt;td&gt;3.97959&lt;/td&gt; 
   &lt;td&gt;93.5934&lt;/td&gt; 
   &lt;td&gt;4.16327&lt;/td&gt; 
   &lt;td&gt;93.0892&lt;/td&gt; 
   &lt;td&gt;4.02041&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Throughput&lt;/h2&gt; 
&lt;p&gt;We benchmarked throughput using a &lt;a href="https://www.greenteapress.com/thinkpython/thinkpython.pdf"&gt;single long PDF&lt;/a&gt;.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Method&lt;/th&gt; 
   &lt;th&gt;Time per page&lt;/th&gt; 
   &lt;th&gt;Time per document&lt;/th&gt; 
   &lt;th&gt;VRAM used&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;marker&lt;/td&gt; 
   &lt;td&gt;0.18&lt;/td&gt; 
   &lt;td&gt;43.42&lt;/td&gt; 
   &lt;td&gt;3.17GB&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;The projected throughput is 122 pages per second on an H100 - we can run 22 individual processes given the VRAM used.&lt;/p&gt; 
&lt;h2&gt;Table Conversion&lt;/h2&gt; 
&lt;p&gt;Marker can extract tables from PDFs using &lt;code&gt;marker.converters.table.TableConverter&lt;/code&gt;. The table extraction performance is measured by comparing the extracted HTML representation of tables against the original HTML representations using the test split of &lt;a href="https://developer.ibm.com/exchanges/data/all/fintabnet/"&gt;FinTabNet&lt;/a&gt;. The HTML representations are compared using a tree edit distance based metric to judge both structure and content. Marker detects and identifies the structure of all tables in a PDF page and achieves these scores:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Method&lt;/th&gt; 
   &lt;th&gt;Avg score&lt;/th&gt; 
   &lt;th&gt;Total tables&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;marker&lt;/td&gt; 
   &lt;td&gt;0.816&lt;/td&gt; 
   &lt;td&gt;99&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;marker w/use_llm&lt;/td&gt; 
   &lt;td&gt;0.907&lt;/td&gt; 
   &lt;td&gt;99&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;gemini&lt;/td&gt; 
   &lt;td&gt;0.829&lt;/td&gt; 
   &lt;td&gt;99&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;The &lt;code&gt;--use_llm&lt;/code&gt; flag can significantly improve table recognition performance, as you can see.&lt;/p&gt; 
&lt;p&gt;We filter out tables that we cannot align with the ground truth, since fintabnet and our layout model have slightly different detection methods (this results in some tables being split/merged).&lt;/p&gt; 
&lt;h2&gt;Running your own benchmarks&lt;/h2&gt; 
&lt;p&gt;You can benchmark the performance of marker on your machine. Install marker manually with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;git clone https://github.com/VikParuchuri/marker.git
poetry install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Overall PDF Conversion&lt;/h3&gt; 
&lt;p&gt;Download the benchmark data &lt;a href="https://drive.google.com/file/d/1ZSeWDo2g1y0BRLT7KnbmytV2bjWARWba/view?usp=sharing"&gt;here&lt;/a&gt; and unzip. Then run the overall benchmark like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;python benchmarks/overall.py --methods marker --scores heuristic,llm
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Options:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--use_llm&lt;/code&gt; use an llm to improve the marker results.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--max_rows&lt;/code&gt; how many rows to process for the benchmark.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--methods&lt;/code&gt; can be &lt;code&gt;llamaparse&lt;/code&gt;, &lt;code&gt;mathpix&lt;/code&gt;, &lt;code&gt;docling&lt;/code&gt;, &lt;code&gt;marker&lt;/code&gt;. Comma separated.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--scores&lt;/code&gt; which scoring functions to use, can be &lt;code&gt;llm&lt;/code&gt;, &lt;code&gt;heuristic&lt;/code&gt;. Comma separated.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Table Conversion&lt;/h3&gt; 
&lt;p&gt;The processed FinTabNet dataset is hosted &lt;a href="https://huggingface.co/datasets/datalab-to/fintabnet-test"&gt;here&lt;/a&gt; and is automatically downloaded. Run the benchmark with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;python benchmarks/table/table.py --max_rows 100
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Options:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--use_llm&lt;/code&gt; uses an llm with marker to improve accuracy.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--use_gemini&lt;/code&gt; also benchmarks gemini 2.0 flash.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;How it works&lt;/h1&gt; 
&lt;p&gt;Marker is a pipeline of deep learning models:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Extract text, OCR if necessary (heuristics, &lt;a href="https://github.com/VikParuchuri/surya"&gt;surya&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;Detect page layout and find reading order (&lt;a href="https://github.com/VikParuchuri/surya"&gt;surya&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;Clean and format each block (heuristics, &lt;a href="https://github.com/VikParuchuri/texify"&gt;texify&lt;/a&gt;, &lt;a href="https://github.com/VikParuchuri/surya"&gt;surya&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;Optionally use an LLM to improve quality&lt;/li&gt; 
 &lt;li&gt;Combine blocks and postprocess complete text&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;It only uses models where necessary, which improves speed and accuracy.&lt;/p&gt; 
&lt;h1&gt;Limitations&lt;/h1&gt; 
&lt;p&gt;PDF is a tricky format, so marker will not always work perfectly. Here are some known limitations that are on the roadmap to address:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Very complex layouts, with nested tables and forms, may not work&lt;/li&gt; 
 &lt;li&gt;Forms may not be rendered well&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Note: Passing the &lt;code&gt;--use_llm&lt;/code&gt; and &lt;code&gt;--force_ocr&lt;/code&gt; flags will mostly solve these issues.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>