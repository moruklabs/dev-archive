<rss version="2.0">
  <channel>
    <title>GitHub All Languages Weekly Trending</title>
    <description>Weekly Trending of All Languages in GitHub</description>
    <pubDate>Sat, 27 Dec 2025 01:39:56 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>sgl-project/mini-sglang</title>
      <link>https://github.com/sgl-project/mini-sglang</link>
      <description>&lt;p&gt;A compact implementation of SGLang, designed to demystify the complexities of modern LLM serving systems.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img width="400" src="https://raw.githubusercontent.com/sgl-project/mini-sglang/main/assets/logo.png" /&gt; &lt;/p&gt; 
&lt;h1&gt;Mini-SGLang&lt;/h1&gt; 
&lt;p&gt;A &lt;strong&gt;lightweight yet high-performance&lt;/strong&gt; inference framework for Large Language Models.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;Mini-SGLang is a compact implementation of &lt;a href="https://github.com/sgl-project/sglang"&gt;SGLang&lt;/a&gt;, designed to demystify the complexities of modern LLM serving systems. With a compact codebase of &lt;strong&gt;~5,000 lines of Python&lt;/strong&gt;, it serves as both a capable inference engine and a transparent reference for researchers and developers.&lt;/p&gt; 
&lt;h2&gt;‚ú® Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;High Performance&lt;/strong&gt;: Achieves state-of-the-art throughput and latency with advanced optimizations.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Lightweight &amp;amp; Readable&lt;/strong&gt;: A clean, modular, and fully type-annotated codebase that is easy to understand and modify.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Advanced Optimizations&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Radix Cache&lt;/strong&gt;: Reuses KV cache for shared prefixes across requests.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Chunked Prefill&lt;/strong&gt;: Reduces peak memory usage for long-context serving.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Overlap Scheduling&lt;/strong&gt;: Hides CPU scheduling overhead with GPU computation.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Tensor Parallelism&lt;/strong&gt;: Scales inference across multiple GPUs.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Optimized Kernels&lt;/strong&gt;: Integrates &lt;strong&gt;FlashAttention&lt;/strong&gt; and &lt;strong&gt;FlashInfer&lt;/strong&gt; for maximum efficiency.&lt;/li&gt; 
   &lt;li&gt;...&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üöÄ Quick Start&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;‚ö†Ô∏è Platform Support&lt;/strong&gt;: Mini-SGLang currently supports &lt;strong&gt;Linux only&lt;/strong&gt; (x86_64 and aarch64). Windows and macOS are not supported due to dependencies on Linux-specific CUDA kernels (&lt;code&gt;sgl-kernel&lt;/code&gt;, &lt;code&gt;flashinfer&lt;/code&gt;). We recommend using &lt;a href="https://learn.microsoft.com/en-us/windows/wsl/install"&gt;WSL2&lt;/a&gt; on Windows or Docker for cross-platform compatibility.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;1. Environment Setup&lt;/h3&gt; 
&lt;p&gt;We recommend using &lt;code&gt;uv&lt;/code&gt; for a fast and reliable installation (note that &lt;code&gt;uv&lt;/code&gt; does not conflict with &lt;code&gt;conda&lt;/code&gt;).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Create a virtual environment (Python 3.10+ recommended)
uv venv --python=3.12
source .venv/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Prerequisites&lt;/strong&gt;: Mini-SGLang relies on CUDA kernels that are JIT-compiled. Ensure you have the &lt;strong&gt;NVIDIA CUDA Toolkit&lt;/strong&gt; installed and that its version matches your driver's version. You can check your driver's CUDA capability with &lt;code&gt;nvidia-smi&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;2. Installation&lt;/h3&gt; 
&lt;p&gt;Install Mini-SGLang directly from the source:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/sgl-project/mini-sglang.git
cd mini-sglang &amp;amp;&amp;amp; uv venv --python=3.12 &amp;amp;&amp;amp; source .venv/bin/activate
uv pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;üí° Installing on Windows (WSL2)&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;Since Mini-SGLang requires Linux-specific dependencies, Windows users should use WSL2:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install WSL2&lt;/strong&gt; (if not already installed):&lt;/p&gt; &lt;pre&gt;&lt;code class="language-powershell"&gt;# In PowerShell (as Administrator)
wsl --install
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install CUDA on WSL2&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Follow &lt;a href="https://docs.nvidia.com/cuda/wsl-user-guide/index.html"&gt;NVIDIA's WSL2 CUDA guide&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;Ensure your Windows GPU drivers support WSL2&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install Mini-SGLang in WSL2&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Inside WSL2 terminal
git clone https://github.com/sgl-project/mini-sglang.git
cd mini-sglang &amp;amp;&amp;amp; uv venv --python=3.12 &amp;amp;&amp;amp; source .venv/bin/activate
uv pip install -e .
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Access from Windows&lt;/strong&gt;: The server will be accessible at &lt;code&gt;http://localhost:8000&lt;/code&gt; from Windows browsers and applications.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;h3&gt;3. Online Serving&lt;/h3&gt; 
&lt;p&gt;Launch an OpenAI-compatible API server with a single command.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Deploy Qwen/Qwen3-0.6B on a single GPU
python -m minisgl --model "Qwen/Qwen3-0.6B"

# Deploy meta-llama/Llama-3.1-70B-Instruct on 4 GPUs with Tensor Parallelism, on port 30000
python -m minisgl --model "meta-llama/Llama-3.1-70B-Instruct" --tp 4 --port 30000
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Once the server is running, you can send requests using standard tools like &lt;code&gt;curl&lt;/code&gt; or any OpenAI-compatible client.&lt;/p&gt; 
&lt;h3&gt;4. Interactive Shell&lt;/h3&gt; 
&lt;p&gt;Chat with your model directly in the terminal by adding the &lt;code&gt;--shell&lt;/code&gt; flag.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m minisgl --model "Qwen/Qwen3-0.6B" --shell
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://lmsys.org/images/blog/minisgl/shell.png" alt="shell-example" /&gt;&lt;/p&gt; 
&lt;p&gt;You can also use &lt;code&gt;/reset&lt;/code&gt; to clear the chat history.&lt;/p&gt; 
&lt;h2&gt;Benchmark&lt;/h2&gt; 
&lt;h3&gt;Offline inference&lt;/h3&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/sgl-project/mini-sglang/main/benchmark/offline/bench.py"&gt;bench.py&lt;/a&gt; for more details. Set &lt;code&gt;MINISGL_DISABLE_OVERLAP_SCHEDULING=1&lt;/code&gt; for ablation study on overlap scheduling.&lt;/p&gt; 
&lt;p&gt;Test Configuration:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Hardware: 1xH200 GPU.&lt;/li&gt; 
 &lt;li&gt;Model: Qwen3-0.6B, Qwen3-14B&lt;/li&gt; 
 &lt;li&gt;Total Requests: 256 sequences&lt;/li&gt; 
 &lt;li&gt;Input Length: Randomly sampled between 100-1024 tokens&lt;/li&gt; 
 &lt;li&gt;Output Length: Randomly sampled between 100-1024 tokens&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://lmsys.org/images/blog/minisgl/offline.png" alt="offline" /&gt;&lt;/p&gt; 
&lt;h3&gt;Online inference&lt;/h3&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/sgl-project/mini-sglang/main/benchmark/online/bench_qwen.py"&gt;benchmark_qwen.py&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;p&gt;Test Configuration:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Hardware: 4xH200 GPU, connected by NVLink.&lt;/li&gt; 
 &lt;li&gt;Model: Qwen3-32B&lt;/li&gt; 
 &lt;li&gt;Dataset: &lt;a href="https://github.com/alibaba-edu/qwen-bailian-usagetraces-anon/raw/main/qwen_traceA_blksz_16.jsonl"&gt;Qwen trace&lt;/a&gt;, replaying first 1000 requests.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Launch command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Mini-SGLang
python -m minisgl --model "Qwen/Qwen3-32B" --tp 4 --cache naive

# SGLang
python3 -m sglang.launch_server --model "Qwen/Qwen3-32B" --tp 4 \
    --disable-radix --port 1919 --decode-attention flashinfer
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://lmsys.org/images/blog/minisgl/online.png" alt="online" /&gt;&lt;/p&gt; 
&lt;h2&gt;üìö Learn More&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/sgl-project/mini-sglang/main/docs/features.md"&gt;Detailed Features&lt;/a&gt;&lt;/strong&gt;: Explore all available features and command-line arguments.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/sgl-project/mini-sglang/main/docs/structures.md"&gt;System Architecture&lt;/a&gt;&lt;/strong&gt;: Dive deep into the design and data flow of Mini-SGLang.&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>google/A2UI</title>
      <link>https://github.com/google/A2UI</link>
      <description>&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;A2UI: Agent-to-User Interface&lt;/h1&gt; 
&lt;p&gt;A2UI is an open-source project, complete with a format optimized for representing updateable agent-generated UIs and an initial set of renderers, that allows agents to generate or populate rich user interfaces.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/google/A2UI/main/docs/assets/a2ui_gallery_examples.png" alt="Gallery of A2UI components" height="400" /&gt; 
&lt;p&gt;&lt;em&gt;A gallery of A2UI rendered cards, showing a variety of UI compositions that A2UI can achieve.&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;‚ö†Ô∏è Status: Early Stage Public Preview&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; A2UI is currently in &lt;strong&gt;v0.8 (Public Preview)&lt;/strong&gt;. The specification and implementations are functional but are still evolving. We are opening the project to foster collaboration, gather feedback, and solicit contributions (e.g., on client renderers). Expect changes.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Summary&lt;/h2&gt; 
&lt;p&gt;Generative AI excels at creating text and code, but agents can struggle to present rich, interactive interfaces to users, especially when those agents are remote or running across trust boundaries.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;A2UI&lt;/strong&gt; is an open standard and set of libraries that allows agents to "speak UI." Agents send a declarative JSON format describing the &lt;em&gt;intent&lt;/em&gt; of the UI. The client application then renders this using its own native component library (Flutter, Angular, Lit, etc.).&lt;/p&gt; 
&lt;p&gt;This approach ensures that agent-generated UIs are &lt;strong&gt;safe like data, but expressive like code&lt;/strong&gt;.&lt;/p&gt; 
&lt;h2&gt;High-Level Philosophy&lt;/h2&gt; 
&lt;p&gt;A2UI was designed to address the specific challenges of interoperable, cross-platform, generative or template-based UI responses from agents.&lt;/p&gt; 
&lt;p&gt;The project's core philosophies:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Security first&lt;/strong&gt;: Running arbitrary code generated by an LLM may present a security risk. A2UI is a declarative data format, not executable code. Your client application maintains a "catalog" of trusted, pre-approved UI components (e.g., Card, Button, TextField), and the agent can only request to render components from that catalog.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;LLM-friendly and incrementally updateable&lt;/strong&gt;: The UI is represented as a flat list of components with ID references which is easy for LLMs to generate incrementally, allowing for progressive rendering and a responsive user experience. An agent can efficiently make incremental changes to the UI based on new user requests as the conversation progresses.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Framework-agnostic and portable&lt;/strong&gt;: A2UI separates the UI structure from the UI implementation. The agent sends a description of the component tree and its associated data model. Your client application is responsible for mapping these abstract descriptions to its native widgets‚Äîbe it web components, Flutter widgets, React components, SwiftUI views or something else entirely. The same A2UI JSON payload from an agent can be rendered on multiple different clients built on top of different frameworks.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexibility&lt;/strong&gt;: A2UI also features an open registry pattern that allows developers to map server-side types to custom client implementations, from native mobile widgets to React components. By registering a "Smart Wrapper," you can connect any existing UI component‚Äîincluding secure iframe containers for legacy content‚Äîto A2UI's data binding and event system. Crucially, this places security firmly in the developer's hands, enabling them to enforce strict sandboxing policies and "trust ladders" directly within their custom component logic rather than relying solely on the core system.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Use Cases&lt;/h2&gt; 
&lt;p&gt;Some of the use cases include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Dynamic Data Collection:&lt;/strong&gt; An agent generates a bespoke form (date pickers, sliders, inputs) based on the specific context of a conversation (e.g., booking a specialized reservation).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Remote Sub-Agents:&lt;/strong&gt; An orchestrator agent delegates a task to a remote specialized agent (e.g., a travel booking agent) which returns a UI payload to be rendered inside the main chat window.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Adaptive Workflows:&lt;/strong&gt; Enterprise agents that generate approval dashboards or data visualizations on the fly based on the user's query.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Architecture&lt;/h2&gt; 
&lt;p&gt;The A2UI flow disconnects the generation of UI from the execution of UI:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Generation:&lt;/strong&gt; An Agent (using Gemini or another LLM) generates or uses a pre-generated &lt;code&gt;A2UI Response&lt;/code&gt;, a JSON payload describing the composition of UI components and their properties.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Transport:&lt;/strong&gt; This message is sent to the client application (via A2A, AG UI, etc.).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Resolution:&lt;/strong&gt; The Client's &lt;strong&gt;A2UI Renderer&lt;/strong&gt; parses the JSON.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Rendering:&lt;/strong&gt; The Renderer maps the abstract components (e.g., &lt;code&gt;type: 'text-field'&lt;/code&gt;) to the concrete implementation in the client's codebase.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Dependencies&lt;/h2&gt; 
&lt;p&gt;A2UI is designed to be a lightweight format, but it fits into a larger ecosystem:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Transports:&lt;/strong&gt; Compatible with &lt;strong&gt;A2A Protocol&lt;/strong&gt; and &lt;strong&gt;AG UI&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;LLMs:&lt;/strong&gt; Can be generated by any model capable of generating JSON output.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Host Frameworks:&lt;/strong&gt; Requires a host application built in a supported framework (currently: Web or Flutter).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;The best way to understand A2UI is to run the samples.&lt;/p&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Node.js (for web clients)&lt;/li&gt; 
 &lt;li&gt;Python (for agent samples)&lt;/li&gt; 
 &lt;li&gt;A valid &lt;a href="https://aistudio.google.com/"&gt;Gemini API Key&lt;/a&gt; is required for the samples.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Running the Restaurant Finder Demo&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Clone the repository:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/google/A2UI.git
cd A2UI
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Set your API Key:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;export GEMINI_API_KEY="your_gemini_api_key"
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Run the Agent (Backend):&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cd samples/agent/adk/restaurant_finder
uv run .
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Run the Client (Frontend):&lt;/strong&gt; Open a new terminal window:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Install and build the Lit renderer
cd renderers/lit
npm install
npm run build

# Install and run the shell client
cd ../../samples/client/lit/shell
npm install
npm run dev
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;For Flutter developers, check out the &lt;a href="https://github.com/flutter/genui"&gt;GenUI SDK&lt;/a&gt;, which uses A2UI under the hood.&lt;/p&gt; 
&lt;p&gt;CopilotKit has a public &lt;a href="https://go.copilotkit.ai/A2UI-widget-builder"&gt;A2UI Widget Builder&lt;/a&gt; to try out as well.&lt;/p&gt; 
&lt;h2&gt;Roadmap&lt;/h2&gt; 
&lt;p&gt;We hope to work with the community on the following:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Spec Stabilization:&lt;/strong&gt; Moving towards a v1.0 specification.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;More Renderers:&lt;/strong&gt; Adding official support for React, Jetpack Compose, iOS (SwiftUI), and more.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Additional Transports:&lt;/strong&gt; Support for REST and more.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Additional Agent Frameworks:&lt;/strong&gt; Genkit, LangGraph, and more.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contribute&lt;/h2&gt; 
&lt;p&gt;A2UI is an &lt;strong&gt;Apache 2.0&lt;/strong&gt; licensed project. We believe the future of UI is agentic, and we want to work with you to help build it.&lt;/p&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/google/A2UI/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for details on how to get started.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>exo-explore/exo</title>
      <link>https://github.com/exo-explore/exo</link>
      <description>&lt;p&gt;Run your own AI cluster at home with everyday devices üì±üíª üñ•Ô∏è‚åö&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="/docs/imgs/exo-logo-black-bg.jpg" /&gt; 
  &lt;img alt="exo logo" src="https://raw.githubusercontent.com/exo-explore/exo/main/docs/imgs/exo-logo-transparent.png" width="50%" height="50%" /&gt; 
 &lt;/picture&gt; 
 &lt;p&gt;exo: Run your own AI cluster at home with everyday devices. Maintained by &lt;a href="https://x.com/exolabs"&gt;exo labs&lt;/a&gt;.&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;a href="https://discord.gg/72NsF6ux" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://img.shields.io/badge/Discord-Join%20Server-5865F2?logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://x.com/exolabs" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://img.shields.io/twitter/follow/exolabs?style=social" alt="X" /&gt;&lt;/a&gt; &lt;a href="https://www.apache.org/licenses/LICENSE-2.0.html" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://img.shields.io/badge/License-Apache2.0-blue.svg?sanitize=true" alt="License: Apache-2.0" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;p&gt;exo connects all your devices into an AI cluster. Not only does exo enable running models larger than would fit on a single device, but with &lt;a href="https://x.com/exolabs/status/2001817749744476256?s=20"&gt;day-0 support for RDMA over Thunderbolt&lt;/a&gt;, makes models run faster as you add more devices.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Automatic Device Discovery&lt;/strong&gt;: Devices running exo automatically discover each other - no manual configuration.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;RDMA over Thunderbolt&lt;/strong&gt;: exo ships with &lt;a href="https://x.com/exolabs/status/2001817749744476256?s=20"&gt;day-0 support for RDMA over Thunderbolt 5&lt;/a&gt;, enabling 99% reduction in latency between devices.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Topology-Aware Auto Parallel&lt;/strong&gt;: exo figures out the best way to split your model across all available devices based on a realtime view of your device topology. It takes into account device resources and network latency/bandwidth between each link.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Tensor Parallelism&lt;/strong&gt;: exo supports sharding models, for up to 1.8x speedup on 2 devices and 3.2x speedup on 4 devices.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MLX Support&lt;/strong&gt;: exo uses &lt;a href="https://github.com/ml-explore/mlx"&gt;MLX&lt;/a&gt; as an inference backend and &lt;a href="https://ml-explore.github.io/mlx/build/html/usage/distributed.html"&gt;MLX distributed&lt;/a&gt; for distributed communication.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Benchmarks&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;Qwen3-235B (8-bit) on 4 √ó M3 Ultra Mac Studio with Tensor Parallel RDMA&lt;/summary&gt; 
 &lt;img src="https://raw.githubusercontent.com/exo-explore/exo/main/docs/benchmarks/jeffgeerling/mac-studio-cluster-ai-full-1-qwen3-235b.jpeg" alt="Benchmark - Qwen3-235B (8-bit) on 4 √ó M3 Ultra Mac Studio with Tensor Parallel RDMA" width="80%" /&gt; 
 &lt;p&gt; &lt;strong&gt;Source:&lt;/strong&gt; &lt;a href="https://www.jeffgeerling.com/blog/2025/15-tb-vram-on-mac-studio-rdma-over-thunderbolt-5"&gt;Jeff Geerling: 15 TB VRAM on Mac Studio ‚Äì RDMA over Thunderbolt‚ÄØ5&lt;/a&gt; &lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;DeepSeek v3.1 671B (8-bit) on 4 √ó M3 Ultra Mac Studio with Tensor Parallel RDMA&lt;/summary&gt; 
 &lt;img src="https://raw.githubusercontent.com/exo-explore/exo/main/docs/benchmarks/jeffgeerling/mac-studio-cluster-ai-full-2-deepseek-3.1-671b.jpeg" alt="Benchmark - DeepSeek v3.1 671B (8-bit) on 4 √ó M3 Ultra Mac Studio with Tensor Parallel RDMA" width="80%" /&gt; 
 &lt;p&gt; &lt;strong&gt;Source:&lt;/strong&gt; &lt;a href="https://www.jeffgeerling.com/blog/2025/15-tb-vram-on-mac-studio-rdma-over-thunderbolt-5"&gt;Jeff Geerling: 15 TB VRAM on Mac Studio ‚Äì RDMA over Thunderbolt‚ÄØ5&lt;/a&gt; &lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Kimi K2 Thinking (native 4-bit) on 4 √ó M3 Ultra Mac Studio with Tensor Parallel RDMA&lt;/summary&gt; 
 &lt;img src="https://raw.githubusercontent.com/exo-explore/exo/main/docs/benchmarks/jeffgeerling/mac-studio-cluster-ai-full-3-kimi-k2-thinking.jpeg" alt="Benchmark - Kimi K2 Thinking (native 4-bit) on 4 √ó M3 Ultra Mac Studio with Tensor Parallel RDMA" width="80%" /&gt; 
 &lt;p&gt; &lt;strong&gt;Source:&lt;/strong&gt; &lt;a href="https://www.jeffgeerling.com/blog/2025/15-tb-vram-on-mac-studio-rdma-over-thunderbolt-5"&gt;Jeff Geerling: 15 TB VRAM on Mac Studio ‚Äì RDMA over Thunderbolt‚ÄØ5&lt;/a&gt; &lt;/p&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;Devices running exo automatically discover each other, without needing any manual configuration. Each device provides an API and a dashboard for interacting with your cluster (runs at &lt;code&gt;http://localhost:52415&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;There are two ways to run exo:&lt;/p&gt; 
&lt;h3&gt;Run from Source (Mac &amp;amp; Linux)&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Prerequisites:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/Homebrew/brew"&gt;brew&lt;/a&gt; (for simple package management on MacOS)&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/astral-sh/uv"&gt;uv&lt;/a&gt; (for Python dependency management)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/vladkens/macmon"&gt;macmon&lt;/a&gt; (for hardware monitoring on Apple Silicon)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/nodejs/node"&gt;node&lt;/a&gt; (for building the dashboard)&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;brew install uv macmon node
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/rust-lang/rustup"&gt;rust&lt;/a&gt; (to build Rust bindings, nightly for now)&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
rustup toolchain install nightly
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Clone the repo, build the dashboard, and run exo:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone exo
git clone https://github.com/exo-explore/exo

# Build dashboard
cd exo/dashboard &amp;amp;&amp;amp; npm install &amp;amp;&amp;amp; npm run build &amp;amp;&amp;amp; cd ..

# Run exo
uv run exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This starts the exo dashboard and API at &lt;a href="http://localhost:52415/"&gt;http://localhost:52415/&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;macOS App&lt;/h3&gt; 
&lt;p&gt;exo ships a macOS app that runs in the background on your Mac.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/exo-explore/exo/main/docs/imgs/macos-app-one-macbook.png" alt="exo macOS App - running on a MacBook" width="35%" /&gt; 
&lt;p&gt;The macOS app requires macOS Tahoe 26.2 or later.&lt;/p&gt; 
&lt;p&gt;Download the latest build here: &lt;a href="https://assets.exolabs.net/EXO-latest.dmg"&gt;EXO-latest.dmg&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The app will ask for permission to modify system settings and install a new Network profile. Improvements to this are being worked on.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Using the API&lt;/h3&gt; 
&lt;p&gt;If you prefer to interact with exo via the API, here is an example creating an instance of a small model (&lt;code&gt;mlx-community/Llama-3.2-1B-Instruct-4bit&lt;/code&gt;), sending a chat completions request and deleting the instance.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;1. Preview instance placements&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;The &lt;code&gt;/instance/previews&lt;/code&gt; endpoint will preview all valid placements for your model.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl "http://localhost:52415/instance/previews?model_id=llama-3.2-1b"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Sample response:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "previews": [
    {
      "model_id": "mlx-community/Llama-3.2-1B-Instruct-4bit",
      "sharding": "Pipeline",
      "instance_meta": "MlxRing",
      "instance": {...},
      "memory_delta_by_node": {"local": 729808896},
      "error": null
    }
    // ...possibly more placements...
  ]
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will return all valid placements for this model. Pick a placement that you like. To pick the first one, pipe into &lt;code&gt;jq&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl "http://localhost:52415/instance/previews?model_id=llama-3.2-1b" | jq -c '.previews[] | select(.error == null) | .instance' | head -n1
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;2. Create a model instance&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Send a POST to &lt;code&gt;/instance&lt;/code&gt; with your desired placement in the &lt;code&gt;instance&lt;/code&gt; field (the full payload must match types as in &lt;code&gt;CreateInstanceParams&lt;/code&gt;), which you can copy from step 1:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -X POST http://localhost:52415/instance \
  -H 'Content-Type: application/json' \
  -d '{
    "instance": {...}
  }'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Sample response:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "message": "Command received.",
  "command_id": "e9d1a8ab-...."
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;3. Send a chat completion&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Now, make a POST to &lt;code&gt;/v1/chat/completions&lt;/code&gt; (the same format as OpenAI's API):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -N -X POST http://localhost:52415/v1/chat/completions \
  -H 'Content-Type: application/json' \
  -d '{
    "model": "mlx-community/Llama-3.2-1B-Instruct-4bit",
    "messages": [
      {"role": "user", "content": "What is Llama 3.2 1B?"}
    ],
    "stream": true
  }'
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;4. Delete the instance&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;When you're done, delete the instance by its ID (find it via &lt;code&gt;/state&lt;/code&gt; or &lt;code&gt;/instance&lt;/code&gt; endpoints):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -X DELETE http://localhost:52415/instance/YOUR_INSTANCE_ID
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;&lt;em&gt;Other useful API endpoints&lt;/em&gt;:&lt;/em&gt;*&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;List all models: &lt;code&gt;curl http://localhost:52415/models&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Inspect instance IDs and deployment state: &lt;code&gt;curl http://localhost:52415/state&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For further details, see API types and endpoints in &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/src/exo/master/api.py"&gt;src/exo/master/api.py&lt;/a&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Hardware Accelerator Support&lt;/h2&gt; 
&lt;p&gt;On macOS, exo uses the GPU. On Linux, exo currently runs on CPU. We are working on extending hardware accelerator support. If you'd like support for a new hardware platform, please &lt;a href="https://github.com/exo-explore/exo/issues"&gt;search for an existing feature request&lt;/a&gt; and add a thumbs up so we know what hardware is important to the community.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for guidelines on how to contribute to exo.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>davila7/claude-code-templates</title>
      <link>https://github.com/davila7/claude-code-templates</link>
      <description>&lt;p&gt;CLI tool for configuring and monitoring Claude Code&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="https://www.npmjs.com/package/claude-code-templates"&gt;&lt;img src="https://img.shields.io/npm/v/claude-code-templates.svg?sanitize=true" alt="npm version" /&gt;&lt;/a&gt; &lt;a href="https://www.npmjs.com/package/claude-code-templates"&gt;&lt;img src="https://img.shields.io/npm/dt/claude-code-templates.svg?sanitize=true" alt="npm downloads" /&gt;&lt;/a&gt; &lt;a href="https://opensource.org/licenses/MIT"&gt;&lt;img src="https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true" alt="License: MIT" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/davila7/claude-code-templates/main/CONTRIBUTING.md"&gt;&lt;img src="https://img.shields.io/badge/PRs-welcome-brightgreen.svg?sanitize=true" alt="PRs Welcome" /&gt;&lt;/a&gt; &lt;a href="https://z.ai/subscribe?ic=8JVLJQFSKB&amp;amp;utm_source=github&amp;amp;utm_medium=badge&amp;amp;utm_campaign=readme"&gt;&lt;img src="https://img.shields.io/badge/Sponsored%20by-Z.AI-2563eb?style=flat&amp;amp;logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggZD0iTTEyIDJMMiAyMkgyMkwxMiAyWiIgZmlsbD0id2hpdGUiLz4KPC9zdmc+" alt="Sponsored by Z.AI" /&gt;&lt;/a&gt; &lt;a href="https://buymeacoffee.com/daniavila"&gt;&lt;img src="https://img.shields.io/badge/Buy%20Me%20A%20Coffee-support-yellow?style=flat&amp;amp;logo=buy-me-a-coffee" alt="Buy Me A Coffee" /&gt;&lt;/a&gt; &lt;a href="https://github.com/davila7/claude-code-templates"&gt;&lt;img src="https://img.shields.io/github/stars/davila7/claude-code-templates.svg?style=social&amp;amp;label=Star" alt="GitHub stars" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/15113" target="_blank"&gt; &lt;img src="https://trendshift.io/api/badge/repositories/15113" alt="davila7%2Fclaude-code-templates | Trendshift" style="width: 200px; height: 40px;" width="125" height="40" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;br /&gt; &lt;a href="https://vercel.com/oss"&gt; &lt;img alt="Vercel OSS Program" src="https://vercel.com/oss/program-badge.svg?sanitize=true" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;h3&gt;ü§ù Partnership&lt;/h3&gt; 
 &lt;p&gt; &lt;strong&gt;This project is sponsored by &lt;a href="https://z.ai" target="_blank"&gt;Z.AI&lt;/a&gt;&lt;/strong&gt;&lt;br /&gt; Supporting Claude Code Templates with the &lt;strong&gt;GLM CODING PLAN&lt;/strong&gt; &lt;/p&gt; 
 &lt;p&gt; &lt;a href="https://z.ai/subscribe?ic=8JVLJQFSKB&amp;amp;utm_source=github&amp;amp;utm_medium=readme&amp;amp;utm_campaign=partnership" target="_blank"&gt; &lt;img src="https://img.shields.io/badge/Get%2010%25%20OFF-GLM%20Coding%20Plan-2563eb?style=for-the-badge" alt="GLM Coding Plan" /&gt; &lt;/a&gt; &lt;/p&gt; 
 &lt;p&gt; &lt;em&gt;Top-tier coding performance powered by GLM-4.6 ‚Ä¢ Starting at $3/month&lt;/em&gt;&lt;br /&gt; &lt;em&gt;Seamlessly integrates with Claude Code, Cursor, Cline &amp;amp; 10+ AI coding tools&lt;/em&gt; &lt;/p&gt; 
 &lt;p&gt; &lt;code&gt;npx claude-code-templates@latest --setting partnerships/glm-coding-plan --yes&lt;/code&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h1&gt;Claude Code Templates (&lt;a href="https://aitmpl.com"&gt;aitmpl.com&lt;/a&gt;)&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;Ready-to-use configurations for Anthropic's Claude Code.&lt;/strong&gt; A comprehensive collection of AI agents, custom commands, settings, hooks, external integrations (MCPs), and project templates to enhance your development workflow.&lt;/p&gt; 
&lt;h2&gt;Browse &amp;amp; Install Components and Templates&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://aitmpl.com"&gt;Browse All Templates&lt;/a&gt;&lt;/strong&gt; - Interactive web interface to explore and install 100+ agents, commands, settings, hooks, and MCPs.&lt;/p&gt; 
&lt;img width="1049" height="855" alt="Screenshot 2025-08-19 at 08 09 24" src="https://github.com/user-attachments/assets/e3617410-9b1c-4731-87b7-a3858800b737" /&gt; 
&lt;h2&gt;üöÄ Quick Installation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install a complete development stack
npx claude-code-templates@latest --agent development-team/frontend-developer --command testing/generate-tests --mcp development/github-integration --yes

# Browse and install interactively
npx claude-code-templates@latest

# Install specific components
npx claude-code-templates@latest --agent development-tools/code-reviewer --yes
npx claude-code-templates@latest --command performance/optimize-bundle --yes
npx claude-code-templates@latest --setting performance/mcp-timeouts --yes
npx claude-code-templates@latest --hook git/pre-commit-validation --yes
npx claude-code-templates@latest --mcp database/postgresql-integration --yes
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;What You Get&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Component&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Examples&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ü§ñ Agents&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;AI specialists for specific domains&lt;/td&gt; 
   &lt;td&gt;Security auditor, React performance optimizer, database architect&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;‚ö° Commands&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Custom slash commands&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;/generate-tests&lt;/code&gt;, &lt;code&gt;/optimize-bundle&lt;/code&gt;, &lt;code&gt;/check-security&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üîå MCPs&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;External service integrations&lt;/td&gt; 
   &lt;td&gt;GitHub, PostgreSQL, Stripe, AWS, OpenAI&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;‚öôÔ∏è Settings&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Claude Code configurations&lt;/td&gt; 
   &lt;td&gt;Timeouts, memory settings, output styles&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ü™ù Hooks&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Automation triggers&lt;/td&gt; 
   &lt;td&gt;Pre-commit validation, post-completion actions&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üé® Skills&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Reusable capabilities with progressive disclosure&lt;/td&gt; 
   &lt;td&gt;PDF processing, Excel automation, custom workflows&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;üõ†Ô∏è Additional Tools&lt;/h2&gt; 
&lt;p&gt;Beyond the template catalog, Claude Code Templates includes powerful development tools:&lt;/p&gt; 
&lt;h3&gt;üìä Claude Code Analytics&lt;/h3&gt; 
&lt;p&gt;Monitor your AI-powered development sessions in real-time with live state detection and performance metrics.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npx claude-code-templates@latest --analytics
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;üí¨ Conversation Monitor&lt;/h3&gt; 
&lt;p&gt;Mobile-optimized interface to view Claude responses in real-time with secure remote access.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Local access
npx claude-code-templates@latest --chats

# Secure remote access via Cloudflare Tunnel
npx claude-code-templates@latest --chats --tunnel
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;üîç Health Check&lt;/h3&gt; 
&lt;p&gt;Comprehensive diagnostics to ensure your Claude Code installation is optimized.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npx claude-code-templates@latest --health-check
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;üîå Plugin Dashboard&lt;/h3&gt; 
&lt;p&gt;View marketplaces, installed plugins, and manage permissions from a unified interface.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npx claude-code-templates@latest --plugins
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;üìñ Documentation&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://docs.aitmpl.com/"&gt;üìö docs.aitmpl.com&lt;/a&gt;&lt;/strong&gt; - Complete guides, examples, and API reference for all components and tools.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions! &lt;strong&gt;&lt;a href="https://aitmpl.com"&gt;Browse existing templates&lt;/a&gt;&lt;/strong&gt; to see what's available, then check our &lt;a href="https://raw.githubusercontent.com/davila7/claude-code-templates/main/CONTRIBUTING.md"&gt;contributing guidelines&lt;/a&gt; to add your own agents, commands, MCPs, settings, or hooks.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Please read our &lt;a href="https://raw.githubusercontent.com/davila7/claude-code-templates/main/CODE_OF_CONDUCT.md"&gt;Code of Conduct&lt;/a&gt; before contributing.&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Attribution&lt;/h2&gt; 
&lt;p&gt;This collection includes components from multiple sources:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Scientific Skills:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/K-Dense-AI/claude-scientific-skills"&gt;K-Dense-AI/claude-scientific-skills&lt;/a&gt;&lt;/strong&gt; by K-Dense Inc. - MIT License (139 scientific skills for biology, chemistry, medicine, and computational research)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Official Anthropic:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/anthropics/skills"&gt;anthropics/skills&lt;/a&gt;&lt;/strong&gt; - Official Anthropic skills (21 skills)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/anthropics/claude-code"&gt;anthropics/claude-code&lt;/a&gt;&lt;/strong&gt; - Development guides and examples (10 skills)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Community Skills &amp;amp; Agents:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/obra/superpowers"&gt;obra/superpowers&lt;/a&gt;&lt;/strong&gt; by Jesse Obra - MIT License (14 workflow skills)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/alirezarezvani/claude-skills"&gt;alirezarezvani/claude-skills&lt;/a&gt;&lt;/strong&gt; by Alireza Rezvani - MIT License (36 professional role skills)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/wshobson/agents"&gt;wshobson/agents&lt;/a&gt;&lt;/strong&gt; by wshobson - MIT License (48 agents)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;NerdyChefsAI Skills&lt;/strong&gt; - Community contribution - MIT License (specialized enterprise skills)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Commands &amp;amp; Tools:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/hesreallyhim/awesome-claude-code"&gt;awesome-claude-code&lt;/a&gt;&lt;/strong&gt; by hesreallyhim - CC0 1.0 Universal (21 commands)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/mehdi-lamrani/awesome-claude-skills"&gt;awesome-claude-skills&lt;/a&gt;&lt;/strong&gt; - Apache 2.0 (community skills)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;move-code-quality-skill&lt;/strong&gt; - MIT License&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;cocoindex-claude&lt;/strong&gt; - Apache 2.0&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Each of these resources retains its &lt;strong&gt;original license and attribution&lt;/strong&gt;, as defined by their respective authors. We respect and credit all original creators for their work and contributions to the Claude ecosystem.&lt;/p&gt; 
&lt;h2&gt;üìÑ License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the MIT License - see the &lt;a href="https://raw.githubusercontent.com/davila7/claude-code-templates/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h2&gt;üîó Links&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;üåê Browse Templates&lt;/strong&gt;: &lt;a href="https://aitmpl.com"&gt;aitmpl.com&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üìö Documentation&lt;/strong&gt;: &lt;a href="https://docs.aitmpl.com"&gt;docs.aitmpl.com&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üí¨ Community&lt;/strong&gt;: &lt;a href="https://github.com/davila7/claude-code-templates/discussions"&gt;GitHub Discussions&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üêõ Issues&lt;/strong&gt;: &lt;a href="https://github.com/davila7/claude-code-templates/issues"&gt;GitHub Issues&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Stargazers over time&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://starchart.cc/davila7/claude-code-templates"&gt;&lt;img src="https://starchart.cc/davila7/claude-code-templates.svg?variant=adaptive" alt="Stargazers over time" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;‚≠ê Found this useful? Give us a star to support the project!&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://buymeacoffee.com/daniavila"&gt;&lt;img src="https://img.buymeacoffee.com/button-api/?text=Buy%20me%20a%20coffee&amp;amp;slug=daniavila&amp;amp;button_colour=FFDD00&amp;amp;font_colour=000000&amp;amp;font_family=Cookie&amp;amp;outline_colour=000000&amp;amp;coffee_colour=ffffff" alt="Buy Me A Coffee" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>anthropics/claude-code</title>
      <link>https://github.com/anthropics/claude-code</link>
      <description>&lt;p&gt;Claude Code is an agentic coding tool that lives in your terminal, understands your codebase, and helps you code faster by executing routine tasks, explaining complex code, and handling git workflows - all through natural language commands.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Claude Code&lt;/h1&gt; 
&lt;p&gt;&lt;img src="https://img.shields.io/badge/Node.js-18%2B-brightgreen?style=flat-square" alt="" /&gt; &lt;a href="https://www.npmjs.com/package/@anthropic-ai/claude-code"&gt;&lt;img src="https://img.shields.io/npm/v/@anthropic-ai/claude-code.svg?style=flat-square" alt="npm" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Claude Code is an agentic coding tool that lives in your terminal, understands your codebase, and helps you code faster by executing routine tasks, explaining complex code, and handling git workflows -- all through natural language commands. Use it in your terminal, IDE, or tag @claude on Github.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Learn more in the &lt;a href="https://docs.anthropic.com/en/docs/claude-code/overview"&gt;official documentation&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/anthropics/claude-code/main/demo.gif" /&gt; 
&lt;h2&gt;Get started&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install Claude Code:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;MacOS/Linux:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -fsSL https://claude.ai/install.sh | bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Homebrew (MacOS):&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;brew install --cask claude-code
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Windows:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;irm https://claude.ai/install.ps1 | iex
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;NPM:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npm install -g @anthropic-ai/claude-code
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;NOTE: If installing with NPM, you also need to install &lt;a href="https://nodejs.org/en/download/"&gt;Node.js 18+&lt;/a&gt;&lt;/p&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Navigate to your project directory and run &lt;code&gt;claude&lt;/code&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Plugins&lt;/h2&gt; 
&lt;p&gt;This repository includes several Claude Code plugins that extend functionality with custom commands and agents. See the &lt;a href="https://raw.githubusercontent.com/anthropics/claude-code/main/plugins/README.md"&gt;plugins directory&lt;/a&gt; for detailed documentation on available plugins.&lt;/p&gt; 
&lt;h2&gt;Reporting Bugs&lt;/h2&gt; 
&lt;p&gt;We welcome your feedback. Use the &lt;code&gt;/bug&lt;/code&gt; command to report issues directly within Claude Code, or file a &lt;a href="https://github.com/anthropics/claude-code/issues"&gt;GitHub issue&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Connect on Discord&lt;/h2&gt; 
&lt;p&gt;Join the &lt;a href="https://anthropic.com/discord"&gt;Claude Developers Discord&lt;/a&gt; to connect with other developers using Claude Code. Get help, share feedback, and discuss your projects with the community.&lt;/p&gt; 
&lt;h2&gt;Data collection, usage, and retention&lt;/h2&gt; 
&lt;p&gt;When you use Claude Code, we collect feedback, which includes usage data (such as code acceptance or rejections), associated conversation data, and user feedback submitted via the &lt;code&gt;/bug&lt;/code&gt; command.&lt;/p&gt; 
&lt;h3&gt;How we use your data&lt;/h3&gt; 
&lt;p&gt;See our &lt;a href="https://docs.anthropic.com/en/docs/claude-code/data-usage"&gt;data usage policies&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Privacy safeguards&lt;/h3&gt; 
&lt;p&gt;We have implemented several safeguards to protect your data, including limited retention periods for sensitive information, restricted access to user session data, and clear policies against using feedback for model training.&lt;/p&gt; 
&lt;p&gt;For full details, please review our &lt;a href="https://www.anthropic.com/legal/commercial-terms"&gt;Commercial Terms of Service&lt;/a&gt; and &lt;a href="https://www.anthropic.com/legal/privacy"&gt;Privacy Policy&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>cocoindex-io/cocoindex</title>
      <link>https://github.com/cocoindex-io/cocoindex</link>
      <description>&lt;p&gt;Data transformation framework for AI. Ultra performant, with incremental processing. üåü Star if you like it!&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://cocoindex.io/images/github.svg?sanitize=true" alt="CocoIndex" /&gt; &lt;/p&gt; 
&lt;h1 align="center"&gt;Data transformation for AI&lt;/h1&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/cocoindex-io/cocoindex"&gt;&lt;img src="https://img.shields.io/github/stars/cocoindex-io/cocoindex?color=5B5BD6" alt="GitHub" /&gt;&lt;/a&gt; &lt;a href="https://cocoindex.io/docs/getting_started/quickstart"&gt;&lt;img src="https://img.shields.io/badge/Documentation-394e79?logo=readthedocs&amp;amp;logoColor=00B9FF" alt="Documentation" /&gt;&lt;/a&gt; &lt;a href="https://opensource.org/licenses/Apache-2.0"&gt;&lt;img src="https://img.shields.io/badge/license-Apache%202.0-5B5BD6?logoColor=white" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/cocoindex/"&gt;&lt;img src="https://img.shields.io/pypi/v/cocoindex?color=5B5BD6" alt="PyPI version" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;!--[![PyPI - Downloads](https://img.shields.io/pypi/dm/cocoindex)](https://pypistats.org/packages/cocoindex) --&gt; 
 &lt;p&gt;&lt;a href="https://pepy.tech/projects/cocoindex"&gt;&lt;img src="https://static.pepy.tech/badge/cocoindex/month" alt="PyPI Downloads" /&gt;&lt;/a&gt; &lt;a href="https://github.com/cocoindex-io/cocoindex/actions/workflows/CI.yml"&gt;&lt;img src="https://github.com/cocoindex-io/cocoindex/actions/workflows/CI.yml/badge.svg?event=push&amp;amp;color=5B5BD6" alt="CI" /&gt;&lt;/a&gt; &lt;a href="https://github.com/cocoindex-io/cocoindex/actions/workflows/release.yml"&gt;&lt;img src="https://github.com/cocoindex-io/cocoindex/actions/workflows/release.yml/badge.svg?event=push&amp;amp;color=5B5BD6" alt="release" /&gt;&lt;/a&gt; &lt;a href="https://discord.com/invite/zpA9S2DR7s"&gt;&lt;img src="https://img.shields.io/discord/1314801574169673738?logo=discord&amp;amp;color=5B5BD6&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://trendshift.io/repositories/13939" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/13939" alt="cocoindex-io%2Fcocoindex | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;p&gt;Ultra performant data transformation framework for AI, with core engine written in Rust. Support incremental processing and data lineage out-of-box. Exceptional developer velocity. Production-ready at day 0.&lt;/p&gt; 
&lt;p&gt;‚≠ê Drop a star to help us grow!&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;!-- Keep these links. Translations will automatically update with the README. --&gt; 
 &lt;p&gt;&lt;a href="https://readme-i18n.com/cocoindex-io/cocoindex?lang=de"&gt;Deutsch&lt;/a&gt; | &lt;a href="https://readme-i18n.com/cocoindex-io/cocoindex?lang=en"&gt;English&lt;/a&gt; | &lt;a href="https://readme-i18n.com/cocoindex-io/cocoindex?lang=es"&gt;Espa√±ol&lt;/a&gt; | &lt;a href="https://readme-i18n.com/cocoindex-io/cocoindex?lang=fr"&gt;fran√ßais&lt;/a&gt; | &lt;a href="https://readme-i18n.com/cocoindex-io/cocoindex?lang=ja"&gt;Êó•Êú¨Ë™û&lt;/a&gt; | &lt;a href="https://readme-i18n.com/cocoindex-io/cocoindex?lang=ko"&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | &lt;a href="https://readme-i18n.com/cocoindex-io/cocoindex?lang=pt"&gt;Portugu√™s&lt;/a&gt; | &lt;a href="https://readme-i18n.com/cocoindex-io/cocoindex?lang=ru"&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; | &lt;a href="https://readme-i18n.com/cocoindex-io/cocoindex?lang=zh"&gt;‰∏≠Êñá&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;p align="center"&gt; &lt;img src="https://cocoindex.io/images/transformation.svg?sanitize=true" alt="CocoIndex Transformation" /&gt; &lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;CocoIndex makes it effortless to transform data with AI, and keep source data and target in sync. Whether you‚Äôre building a vector index, creating knowledge graphs for context engineering or performing any custom data transformations ‚Äî goes beyond SQL.&lt;/p&gt; 
&lt;br /&gt; 
&lt;p align="center"&gt; &lt;img alt="CocoIndex Features" src="https://cocoindex.io/images/venn2.svg?sanitize=true" /&gt; &lt;/p&gt; 
&lt;br /&gt; 
&lt;h2&gt;Exceptional velocity&lt;/h2&gt; 
&lt;p&gt;Just declare transformation in dataflow with ~100 lines of python&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# import
data['content'] = flow_builder.add_source(...)

# transform
data['out'] = data['content']
    .transform(...)
    .transform(...)

# collect data
collector.collect(...)

# export to db, vector db, graph db ...
collector.export(...)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;CocoIndex follows the idea of &lt;a href="https://en.wikipedia.org/wiki/Dataflow_programming"&gt;Dataflow&lt;/a&gt; programming model. Each transformation creates a new field solely based on input fields, without hidden states and value mutation. All data before/after each transformation is observable, with lineage out of the box.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Particularly&lt;/strong&gt;, developers don't explicitly mutate data by creating, updating and deleting. They just need to define transformation/formula for a set of source data.&lt;/p&gt; 
&lt;h2&gt;Plug-and-Play Building Blocks&lt;/h2&gt; 
&lt;p&gt;Native builtins for different source, targets and transformations. Standardize interface, make it 1-line code switch between different components - as easy as assembling building blocks.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://cocoindex.io/images/components.svg?sanitize=true" alt="CocoIndex Features" /&gt; &lt;/p&gt; 
&lt;h2&gt;Data Freshness&lt;/h2&gt; 
&lt;p&gt;CocoIndex keep source data and target in sync effortlessly.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://github.com/user-attachments/assets/f4eb29b3-84ee-4fa0-a1e2-80eedeeabde6" alt="Incremental Processing" width="700" /&gt; &lt;/p&gt; 
&lt;p&gt;It has out-of-box support for incremental indexing:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;minimal recomputation on source or logic change.&lt;/li&gt; 
 &lt;li&gt;(re-)processing necessary portions; reuse cache when possible&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;If you're new to CocoIndex, we recommend checking out&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìñ &lt;a href="https://cocoindex.io/docs"&gt;Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;‚ö° &lt;a href="https://cocoindex.io/docs/getting_started/quickstart"&gt;Quick Start Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üé¨ &lt;a href="https://youtu.be/gv5R8nOXsWU?si=9ioeKYkMEnYevTXT"&gt;Quick Start Video Tutorial&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Setup&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install CocoIndex Python library&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip install -U cocoindex
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://cocoindex.io/docs/getting_started/installation#-install-postgres"&gt;Install Postgres&lt;/a&gt; if you don't have one. CocoIndex uses it for incremental processing.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;(Optional) Install Claude Code skill for enhanced development experience. Run these commands in &lt;a href="https://claude.com/claude-code"&gt;Claude Code&lt;/a&gt;:&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code&gt;/plugin marketplace add cocoindex-io/cocoindex-claude
/plugin install cocoindex-skills@cocoindex
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Define data flow&lt;/h2&gt; 
&lt;p&gt;Follow &lt;a href="https://cocoindex.io/docs/getting_started/quickstart"&gt;Quick Start Guide&lt;/a&gt; to define your first indexing flow. An example flow looks like:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;@cocoindex.flow_def(name="TextEmbedding")
def text_embedding_flow(flow_builder: cocoindex.FlowBuilder, data_scope: cocoindex.DataScope):
    # Add a data source to read files from a directory
    data_scope["documents"] = flow_builder.add_source(cocoindex.sources.LocalFile(path="markdown_files"))

    # Add a collector for data to be exported to the vector index
    doc_embeddings = data_scope.add_collector()

    # Transform data of each document
    with data_scope["documents"].row() as doc:
        # Split the document into chunks, put into `chunks` field
        doc["chunks"] = doc["content"].transform(
            cocoindex.functions.SplitRecursively(),
            language="markdown", chunk_size=2000, chunk_overlap=500)

        # Transform data of each chunk
        with doc["chunks"].row() as chunk:
            # Embed the chunk, put into `embedding` field
            chunk["embedding"] = chunk["text"].transform(
                cocoindex.functions.SentenceTransformerEmbed(
                    model="sentence-transformers/all-MiniLM-L6-v2"))

            # Collect the chunk into the collector.
            doc_embeddings.collect(filename=doc["filename"], location=chunk["location"],
                                   text=chunk["text"], embedding=chunk["embedding"])

    # Export collected data to a vector index.
    doc_embeddings.export(
        "doc_embeddings",
        cocoindex.targets.Postgres(),
        primary_key_fields=["filename", "location"],
        vector_indexes=[
            cocoindex.VectorIndexDef(
                field_name="embedding",
                metric=cocoindex.VectorSimilarityMetric.COSINE_SIMILARITY)])
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It defines an index flow like this:&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img width="400" alt="Data Flow" src="https://github.com/user-attachments/assets/2ea7be6d-3d94-42b1-b2bd-22515577e463" /&gt; &lt;/p&gt; 
&lt;h2&gt;üöÄ Examples and demo&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Example&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/cocoindex-io/cocoindex/main/examples/text_embedding"&gt;Text Embedding&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Index text documents with embeddings for semantic search&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/cocoindex-io/cocoindex/main/examples/code_embedding"&gt;Code Embedding&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Index code embeddings for semantic search&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/cocoindex-io/cocoindex/main/examples/pdf_embedding"&gt;PDF Embedding&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Parse PDF and index text embeddings for semantic search&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/cocoindex-io/cocoindex/main/examples/pdf_elements_embedding"&gt;PDF Elements Embedding&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Extract text and images from PDFs; embed text with SentenceTransformers and images with CLIP; store in Qdrant for multimodal search&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/cocoindex-io/cocoindex/main/examples/manuals_llm_extraction"&gt;Manuals LLM Extraction&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Extract structured information from a manual using LLM&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/cocoindex-io/cocoindex/main/examples/amazon_s3_embedding"&gt;Amazon S3 Embedding&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Index text documents from Amazon S3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/cocoindex-io/cocoindex/main/examples/azure_blob_embedding"&gt;Azure Blob Storage Embedding&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Index text documents from Azure Blob Storage&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/cocoindex-io/cocoindex/main/examples/gdrive_text_embedding"&gt;Google Drive Text Embedding&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Index text documents from Google Drive&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/cocoindex-io/cocoindex/main/examples/meeting_notes_graph"&gt;Meeting Notes to Knowledge Graph&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Extract structured meeting info from Google Drive and build a knowledge graph&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/cocoindex-io/cocoindex/main/examples/docs_to_knowledge_graph"&gt;Docs to Knowledge Graph&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Extract relationships from Markdown documents and build a knowledge graph&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/cocoindex-io/cocoindex/main/examples/text_embedding_qdrant"&gt;Embeddings to Qdrant&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Index documents in a Qdrant collection for semantic search&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/cocoindex-io/cocoindex/main/examples/text_embedding_lancedb"&gt;Embeddings to LanceDB&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Index documents in a LanceDB collection for semantic search&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/cocoindex-io/cocoindex/main/examples/fastapi_server_docker"&gt;FastAPI Server with Docker&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Run the semantic search server in a Dockerized FastAPI setup&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/cocoindex-io/cocoindex/main/examples/product_recommendation"&gt;Product Recommendation&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Build real-time product recommendations with LLM and graph database&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/cocoindex-io/cocoindex/main/examples/image_search"&gt;Image Search with Vision API&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Generates detailed captions for images using a vision model, embeds them, enables live-updating semantic search via FastAPI and served on a React frontend&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/cocoindex-io/cocoindex/main/examples/face_recognition"&gt;Face Recognition&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Recognize faces in images and build embedding index&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/cocoindex-io/cocoindex/main/examples/paper_metadata"&gt;Paper Metadata&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Index papers in PDF files, and build metadata tables for each paper&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/cocoindex-io/cocoindex/main/examples/multi_format_indexing"&gt;Multi Format Indexing&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Build visual document index from PDFs and images with ColPali for semantic search&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/cocoindex-io/cocoindex/main/examples/custom_source_hn"&gt;Custom Source HackerNews&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Index HackerNews threads and comments, using &lt;em&gt;CocoIndex Custom Source&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/cocoindex-io/cocoindex/main/examples/custom_output_files"&gt;Custom Output Files&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Convert markdown files to HTML files and save them to a local directory, using &lt;em&gt;CocoIndex Custom Targets&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/cocoindex-io/cocoindex/main/examples/patient_intake_extraction"&gt;Patient intake form extraction&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Use LLM to extract structured data from patient intake forms with different formats&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/cocoindex-io/cocoindex/main/examples/hn_trending_topics"&gt;HackerNews Trending Topics&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Extract trending topics from HackerNews threads and comments, using &lt;em&gt;CocoIndex Custom Source&lt;/em&gt; and LLM&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/cocoindex-io/cocoindex/main/examples/patient_intake_extraction_baml"&gt;Patient Intake Form Extraction with BAML&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Extract structured data from patient intake forms using BAML&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/cocoindex-io/cocoindex/main/examples/patient_intake_extraction_dspy"&gt;Patient Intake Form Extraction with DSPy&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Extract structured data from patient intake forms using DSPy&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;More coming and stay tuned üëÄ!&lt;/p&gt; 
&lt;h2&gt;üìñ Documentation&lt;/h2&gt; 
&lt;p&gt;For detailed documentation, visit &lt;a href="https://cocoindex.io/docs"&gt;CocoIndex Documentation&lt;/a&gt;, including a &lt;a href="https://cocoindex.io/docs/getting_started/quickstart"&gt;Quickstart guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; 
&lt;p&gt;We love contributions from our community ‚ù§Ô∏è. For details on contributing or running the project for development, check out our &lt;a href="https://cocoindex.io/docs/about/contributing"&gt;contributing guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;üë• Community&lt;/h2&gt; 
&lt;p&gt;Welcome with a huge coconut hug ü••‚ãÜÔΩ°Àöü§ó. We are super excited for community contributions of all kinds - whether it's code improvements, documentation updates, issue reports, feature requests, and discussions in our Discord.&lt;/p&gt; 
&lt;p&gt;Join our community here:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üåü &lt;a href="https://github.com/cocoindex-io/cocoindex"&gt;Star us on GitHub&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üëã &lt;a href="https://discord.com/invite/zpA9S2DR7s"&gt;Join our Discord community&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;‚ñ∂Ô∏è &lt;a href="https://www.youtube.com/@cocoindex-io"&gt;Subscribe to our YouTube channel&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üìú &lt;a href="https://cocoindex.io/blogs/"&gt;Read our blog posts&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Support us&lt;/h2&gt; 
&lt;p&gt;We are constantly improving, and more features and examples are coming soon. If you love this project, please drop us a star ‚≠ê at GitHub repo &lt;a href="https://github.com/cocoindex-io/cocoindex"&gt;&lt;img src="https://img.shields.io/github/stars/cocoindex-io/cocoindex?color=5B5BD6" alt="GitHub" /&gt;&lt;/a&gt; to stay tuned and help us grow.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;CocoIndex is Apache 2.0 licensed.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>AUTOMATIC1111/stable-diffusion-webui</title>
      <link>https://github.com/AUTOMATIC1111/stable-diffusion-webui</link>
      <description>&lt;p&gt;Stable Diffusion web UI&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Stable Diffusion web UI&lt;/h1&gt; 
&lt;p&gt;A web interface for Stable Diffusion, implemented using Gradio library.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/screenshot.png" alt="" /&gt;&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features"&gt;Detailed feature showcase with images&lt;/a&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Original txt2img and img2img modes&lt;/li&gt; 
 &lt;li&gt;One click install and run script (but you still must install python and git)&lt;/li&gt; 
 &lt;li&gt;Outpainting&lt;/li&gt; 
 &lt;li&gt;Inpainting&lt;/li&gt; 
 &lt;li&gt;Color Sketch&lt;/li&gt; 
 &lt;li&gt;Prompt Matrix&lt;/li&gt; 
 &lt;li&gt;Stable Diffusion Upscale&lt;/li&gt; 
 &lt;li&gt;Attention, specify parts of text that the model should pay more attention to 
  &lt;ul&gt; 
   &lt;li&gt;a man in a &lt;code&gt;((tuxedo))&lt;/code&gt; - will pay more attention to tuxedo&lt;/li&gt; 
   &lt;li&gt;a man in a &lt;code&gt;(tuxedo:1.21)&lt;/code&gt; - alternative syntax&lt;/li&gt; 
   &lt;li&gt;select text and press &lt;code&gt;Ctrl+Up&lt;/code&gt; or &lt;code&gt;Ctrl+Down&lt;/code&gt; (or &lt;code&gt;Command+Up&lt;/code&gt; or &lt;code&gt;Command+Down&lt;/code&gt; if you're on a MacOS) to automatically adjust attention to selected text (code contributed by anonymous user)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Loopback, run img2img processing multiple times&lt;/li&gt; 
 &lt;li&gt;X/Y/Z plot, a way to draw a 3 dimensional plot of images with different parameters&lt;/li&gt; 
 &lt;li&gt;Textual Inversion 
  &lt;ul&gt; 
   &lt;li&gt;have as many embeddings as you want and use any names you like for them&lt;/li&gt; 
   &lt;li&gt;use multiple embeddings with different numbers of vectors per token&lt;/li&gt; 
   &lt;li&gt;works with half precision floating point numbers&lt;/li&gt; 
   &lt;li&gt;train embeddings on 8GB (also reports of 6GB working)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Extras tab with: 
  &lt;ul&gt; 
   &lt;li&gt;GFPGAN, neural network that fixes faces&lt;/li&gt; 
   &lt;li&gt;CodeFormer, face restoration tool as an alternative to GFPGAN&lt;/li&gt; 
   &lt;li&gt;RealESRGAN, neural network upscaler&lt;/li&gt; 
   &lt;li&gt;ESRGAN, neural network upscaler with a lot of third party models&lt;/li&gt; 
   &lt;li&gt;SwinIR and Swin2SR (&lt;a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/2092"&gt;see here&lt;/a&gt;), neural network upscalers&lt;/li&gt; 
   &lt;li&gt;LDSR, Latent diffusion super resolution upscaling&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Resizing aspect ratio options&lt;/li&gt; 
 &lt;li&gt;Sampling method selection 
  &lt;ul&gt; 
   &lt;li&gt;Adjust sampler eta values (noise multiplier)&lt;/li&gt; 
   &lt;li&gt;More advanced noise setting options&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Interrupt processing at any time&lt;/li&gt; 
 &lt;li&gt;4GB video card support (also reports of 2GB working)&lt;/li&gt; 
 &lt;li&gt;Correct seeds for batches&lt;/li&gt; 
 &lt;li&gt;Live prompt token length validation&lt;/li&gt; 
 &lt;li&gt;Generation parameters 
  &lt;ul&gt; 
   &lt;li&gt;parameters you used to generate images are saved with that image&lt;/li&gt; 
   &lt;li&gt;in PNG chunks for PNG, in EXIF for JPEG&lt;/li&gt; 
   &lt;li&gt;can drag the image to PNG info tab to restore generation parameters and automatically copy them into UI&lt;/li&gt; 
   &lt;li&gt;can be disabled in settings&lt;/li&gt; 
   &lt;li&gt;drag and drop an image/text-parameters to promptbox&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Read Generation Parameters Button, loads parameters in promptbox to UI&lt;/li&gt; 
 &lt;li&gt;Settings page&lt;/li&gt; 
 &lt;li&gt;Running arbitrary python code from UI (must run with &lt;code&gt;--allow-code&lt;/code&gt; to enable)&lt;/li&gt; 
 &lt;li&gt;Mouseover hints for most UI elements&lt;/li&gt; 
 &lt;li&gt;Possible to change defaults/mix/max/step values for UI elements via text config&lt;/li&gt; 
 &lt;li&gt;Tiling support, a checkbox to create images that can be tiled like textures&lt;/li&gt; 
 &lt;li&gt;Progress bar and live image generation preview 
  &lt;ul&gt; 
   &lt;li&gt;Can use a separate neural network to produce previews with almost none VRAM or compute requirement&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Negative prompt, an extra text field that allows you to list what you don't want to see in generated image&lt;/li&gt; 
 &lt;li&gt;Styles, a way to save part of prompt and easily apply them via dropdown later&lt;/li&gt; 
 &lt;li&gt;Variations, a way to generate same image but with tiny differences&lt;/li&gt; 
 &lt;li&gt;Seed resizing, a way to generate same image but at slightly different resolution&lt;/li&gt; 
 &lt;li&gt;CLIP interrogator, a button that tries to guess prompt from an image&lt;/li&gt; 
 &lt;li&gt;Prompt Editing, a way to change prompt mid-generation, say to start making a watermelon and switch to anime girl midway&lt;/li&gt; 
 &lt;li&gt;Batch Processing, process a group of files using img2img&lt;/li&gt; 
 &lt;li&gt;Img2img Alternative, reverse Euler method of cross attention control&lt;/li&gt; 
 &lt;li&gt;Highres Fix, a convenience option to produce high resolution pictures in one click without usual distortions&lt;/li&gt; 
 &lt;li&gt;Reloading checkpoints on the fly&lt;/li&gt; 
 &lt;li&gt;Checkpoint Merger, a tab that allows you to merge up to 3 checkpoints into one&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Custom-Scripts"&gt;Custom scripts&lt;/a&gt; with many extensions from community&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/"&gt;Composable-Diffusion&lt;/a&gt;, a way to use multiple prompts at once 
  &lt;ul&gt; 
   &lt;li&gt;separate prompts using uppercase &lt;code&gt;AND&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;also supports weights for prompts: &lt;code&gt;a cat :1.2 AND a dog AND a penguin :2.2&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;No token limit for prompts (original stable diffusion lets you use up to 75 tokens)&lt;/li&gt; 
 &lt;li&gt;DeepDanbooru integration, creates danbooru style tags for anime prompts&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Xformers"&gt;xformers&lt;/a&gt;, major speed increase for select cards: (add &lt;code&gt;--xformers&lt;/code&gt; to commandline args)&lt;/li&gt; 
 &lt;li&gt;via extension: &lt;a href="https://github.com/yfszzx/stable-diffusion-webui-images-browser"&gt;History tab&lt;/a&gt;: view, direct and delete images conveniently within the UI&lt;/li&gt; 
 &lt;li&gt;Generate forever option&lt;/li&gt; 
 &lt;li&gt;Training tab 
  &lt;ul&gt; 
   &lt;li&gt;hypernetworks and embeddings options&lt;/li&gt; 
   &lt;li&gt;Preprocessing images: cropping, mirroring, autotagging using BLIP or deepdanbooru (for anime)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Clip skip&lt;/li&gt; 
 &lt;li&gt;Hypernetworks&lt;/li&gt; 
 &lt;li&gt;Loras (same as Hypernetworks but more pretty)&lt;/li&gt; 
 &lt;li&gt;A separate UI where you can choose, with preview, which embeddings, hypernetworks or Loras to add to your prompt&lt;/li&gt; 
 &lt;li&gt;Can select to load a different VAE from settings screen&lt;/li&gt; 
 &lt;li&gt;Estimated completion time in progress bar&lt;/li&gt; 
 &lt;li&gt;API&lt;/li&gt; 
 &lt;li&gt;Support for dedicated &lt;a href="https://github.com/runwayml/stable-diffusion#inpainting-with-stable-diffusion"&gt;inpainting model&lt;/a&gt; by RunwayML&lt;/li&gt; 
 &lt;li&gt;via extension: &lt;a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui-aesthetic-gradients"&gt;Aesthetic Gradients&lt;/a&gt;, a way to generate images with a specific aesthetic by using clip images embeds (implementation of &lt;a href="https://github.com/vicgalle/stable-diffusion-aesthetic-gradients"&gt;https://github.com/vicgalle/stable-diffusion-aesthetic-gradients&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Stability-AI/stablediffusion"&gt;Stable Diffusion 2.0&lt;/a&gt; support - see &lt;a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features#stable-diffusion-20"&gt;wiki&lt;/a&gt; for instructions&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2211.06679"&gt;Alt-Diffusion&lt;/a&gt; support - see &lt;a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features#alt-diffusion"&gt;wiki&lt;/a&gt; for instructions&lt;/li&gt; 
 &lt;li&gt;Now without any bad letters!&lt;/li&gt; 
 &lt;li&gt;Load checkpoints in safetensors format&lt;/li&gt; 
 &lt;li&gt;Eased resolution restriction: generated image's dimensions must be a multiple of 8 rather than 64&lt;/li&gt; 
 &lt;li&gt;Now with a license!&lt;/li&gt; 
 &lt;li&gt;Reorder elements in the UI from settings screen&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/segmind/SSD-1B"&gt;Segmind Stable Diffusion&lt;/a&gt; support&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation and Running&lt;/h2&gt; 
&lt;p&gt;Make sure the required &lt;a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Dependencies"&gt;dependencies&lt;/a&gt; are met and follow the instructions available for:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Install-and-Run-on-NVidia-GPUs"&gt;NVidia&lt;/a&gt; (recommended)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Install-and-Run-on-AMD-GPUs"&gt;AMD&lt;/a&gt; GPUs.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/openvinotoolkit/stable-diffusion-webui/wiki/Installation-on-Intel-Silicon"&gt;Intel CPUs, Intel GPUs (both integrated and discrete)&lt;/a&gt; (external wiki page)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/wangshuai09/stable-diffusion-webui/wiki/Install-and-run-on-Ascend-NPUs"&gt;Ascend NPUs&lt;/a&gt; (external wiki page)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Alternatively, use online services (like Google Colab):&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Online-Services"&gt;List of Online Services&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Installation on Windows 10/11 with NVidia-GPUs using release package&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Download &lt;code&gt;sd.webui.zip&lt;/code&gt; from &lt;a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/releases/tag/v1.0.0-pre"&gt;v1.0.0-pre&lt;/a&gt; and extract its contents.&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;update.bat&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;run.bat&lt;/code&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;For more details see &lt;a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Install-and-Run-on-NVidia-GPUs"&gt;Install-and-Run-on-NVidia-GPUs&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Automatic Installation on Windows&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install &lt;a href="https://www.python.org/downloads/release/python-3106/"&gt;Python 3.10.6&lt;/a&gt; (Newer version of Python does not support torch), checking "Add Python to PATH".&lt;/li&gt; 
 &lt;li&gt;Install &lt;a href="https://git-scm.com/download/win"&gt;git&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Download the stable-diffusion-webui repository, for example by running &lt;code&gt;git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;webui-user.bat&lt;/code&gt; from Windows Explorer as normal, non-administrator, user.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Automatic Installation on Linux&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install the dependencies:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Debian-based:
sudo apt install wget git python3 python3-venv libgl1 libglib2.0-0
# Red Hat-based:
sudo dnf install wget git python3 gperftools-libs libglvnd-glx
# openSUSE-based:
sudo zypper install wget git python3 libtcmalloc4 libglvnd
# Arch-based:
sudo pacman -S wget git python3
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If your system is very new, you need to install python3.11 or python3.10:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Ubuntu 24.04
sudo add-apt-repository ppa:deadsnakes/ppa
sudo apt update
sudo apt install python3.11

# Manjaro/Arch
sudo pacman -S yay
yay -S python311 # do not confuse with python3.11 package

# Only for 3.11
# Then set up env variable in launch script
export python_cmd="python3.11"
# or in webui-user.sh
python_cmd="python3.11"
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Navigate to the directory you would like the webui to be installed and execute the following command:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;wget -q https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/webui.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or just clone the repo wherever you want:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Run &lt;code&gt;webui.sh&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Check &lt;code&gt;webui-user.sh&lt;/code&gt; for options.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Installation on Apple Silicon&lt;/h3&gt; 
&lt;p&gt;Find the instructions &lt;a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Installation-on-Apple-Silicon"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Here's how to add code to this repo: &lt;a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Contributing"&gt;Contributing&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;The documentation was moved from this README over to the project's &lt;a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki"&gt;wiki&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For the purposes of getting Google and other search engines to crawl the wiki, here's a link to the (not for humans) &lt;a href="https://github-wiki-see.page/m/AUTOMATIC1111/stable-diffusion-webui/wiki"&gt;crawlable wiki&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Credits&lt;/h2&gt; 
&lt;p&gt;Licenses for borrowed code can be found in &lt;code&gt;Settings -&amp;gt; Licenses&lt;/code&gt; screen, and also in &lt;code&gt;html/licenses.html&lt;/code&gt; file.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Stable Diffusion - &lt;a href="https://github.com/Stability-AI/stablediffusion"&gt;https://github.com/Stability-AI/stablediffusion&lt;/a&gt;, &lt;a href="https://github.com/CompVis/taming-transformers"&gt;https://github.com/CompVis/taming-transformers&lt;/a&gt;, &lt;a href="https://github.com/mcmonkey4eva/sd3-ref"&gt;https://github.com/mcmonkey4eva/sd3-ref&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;k-diffusion - &lt;a href="https://github.com/crowsonkb/k-diffusion.git"&gt;https://github.com/crowsonkb/k-diffusion.git&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Spandrel - &lt;a href="https://github.com/chaiNNer-org/spandrel"&gt;https://github.com/chaiNNer-org/spandrel&lt;/a&gt; implementing 
  &lt;ul&gt; 
   &lt;li&gt;GFPGAN - &lt;a href="https://github.com/TencentARC/GFPGAN.git"&gt;https://github.com/TencentARC/GFPGAN.git&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;CodeFormer - &lt;a href="https://github.com/sczhou/CodeFormer"&gt;https://github.com/sczhou/CodeFormer&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;ESRGAN - &lt;a href="https://github.com/xinntao/ESRGAN"&gt;https://github.com/xinntao/ESRGAN&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;SwinIR - &lt;a href="https://github.com/JingyunLiang/SwinIR"&gt;https://github.com/JingyunLiang/SwinIR&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Swin2SR - &lt;a href="https://github.com/mv-lab/swin2sr"&gt;https://github.com/mv-lab/swin2sr&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;LDSR - &lt;a href="https://github.com/Hafiidz/latent-diffusion"&gt;https://github.com/Hafiidz/latent-diffusion&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;MiDaS - &lt;a href="https://github.com/isl-org/MiDaS"&gt;https://github.com/isl-org/MiDaS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Ideas for optimizations - &lt;a href="https://github.com/basujindal/stable-diffusion"&gt;https://github.com/basujindal/stable-diffusion&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Cross Attention layer optimization - Doggettx - &lt;a href="https://github.com/Doggettx/stable-diffusion"&gt;https://github.com/Doggettx/stable-diffusion&lt;/a&gt;, original idea for prompt editing.&lt;/li&gt; 
 &lt;li&gt;Cross Attention layer optimization - InvokeAI, lstein - &lt;a href="https://github.com/invoke-ai/InvokeAI"&gt;https://github.com/invoke-ai/InvokeAI&lt;/a&gt; (originally &lt;a href="http://github.com/lstein/stable-diffusion"&gt;http://github.com/lstein/stable-diffusion&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;Sub-quadratic Cross Attention layer optimization - Alex Birch (&lt;a href="https://github.com/Birch-san/diffusers/pull/1"&gt;https://github.com/Birch-san/diffusers/pull/1&lt;/a&gt;), Amin Rezaei (&lt;a href="https://github.com/AminRezaei0x443/memory-efficient-attention"&gt;https://github.com/AminRezaei0x443/memory-efficient-attention&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;Textual Inversion - Rinon Gal - &lt;a href="https://github.com/rinongal/textual_inversion"&gt;https://github.com/rinongal/textual_inversion&lt;/a&gt; (we're not using his code, but we are using his ideas).&lt;/li&gt; 
 &lt;li&gt;Idea for SD upscale - &lt;a href="https://github.com/jquesnelle/txt2imghd"&gt;https://github.com/jquesnelle/txt2imghd&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Noise generation for outpainting mk2 - &lt;a href="https://github.com/parlance-zz/g-diffuser-bot"&gt;https://github.com/parlance-zz/g-diffuser-bot&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;CLIP interrogator idea and borrowing some code - &lt;a href="https://github.com/pharmapsychotic/clip-interrogator"&gt;https://github.com/pharmapsychotic/clip-interrogator&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Idea for Composable Diffusion - &lt;a href="https://github.com/energy-based-model/Compositional-Visual-Generation-with-Composable-Diffusion-Models-PyTorch"&gt;https://github.com/energy-based-model/Compositional-Visual-Generation-with-Composable-Diffusion-Models-PyTorch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;xformers - &lt;a href="https://github.com/facebookresearch/xformers"&gt;https://github.com/facebookresearch/xformers&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;DeepDanbooru - interrogator for anime diffusers &lt;a href="https://github.com/KichangKim/DeepDanbooru"&gt;https://github.com/KichangKim/DeepDanbooru&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Sampling in float32 precision from a float16 UNet - marunine for the idea, Birch-san for the example Diffusers implementation (&lt;a href="https://github.com/Birch-san/diffusers-play/tree/92feee6"&gt;https://github.com/Birch-san/diffusers-play/tree/92feee6&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;Instruct pix2pix - Tim Brooks (star), Aleksander Holynski (star), Alexei A. Efros (no star) - &lt;a href="https://github.com/timothybrooks/instruct-pix2pix"&gt;https://github.com/timothybrooks/instruct-pix2pix&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Security advice - RyotaK&lt;/li&gt; 
 &lt;li&gt;UniPC sampler - Wenliang Zhao - &lt;a href="https://github.com/wl-zhao/UniPC"&gt;https://github.com/wl-zhao/UniPC&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;TAESD - Ollin Boer Bohan - &lt;a href="https://github.com/madebyollin/taesd"&gt;https://github.com/madebyollin/taesd&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;LyCORIS - KohakuBlueleaf&lt;/li&gt; 
 &lt;li&gt;Restart sampling - lambertae - &lt;a href="https://github.com/Newbeeer/diffusion_restart_sampling"&gt;https://github.com/Newbeeer/diffusion_restart_sampling&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Hypertile - tfernd - &lt;a href="https://github.com/tfernd/HyperTile"&gt;https://github.com/tfernd/HyperTile&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Initial Gradio script - posted on 4chan by an Anonymous user. Thank you Anonymous user.&lt;/li&gt; 
 &lt;li&gt;(You)&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>anthropics/skills</title>
      <link>https://github.com/anthropics/skills</link>
      <description>&lt;p&gt;Public repository for Agent Skills&lt;/p&gt;&lt;hr&gt;&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; This repository contains Anthropic's implementation of skills for Claude. For information about the Agent Skills standard, see &lt;a href="http://agentskills.io"&gt;agentskills.io&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h1&gt;Skills&lt;/h1&gt; 
&lt;p&gt;Skills are folders of instructions, scripts, and resources that Claude loads dynamically to improve performance on specialized tasks. Skills teach Claude how to complete specific tasks in a repeatable way, whether that's creating documents with your company's brand guidelines, analyzing data using your organization's specific workflows, or automating personal tasks.&lt;/p&gt; 
&lt;p&gt;For more information, check out:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://support.claude.com/en/articles/12512176-what-are-skills"&gt;What are skills?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://support.claude.com/en/articles/12512180-using-skills-in-claude"&gt;Using skills in Claude&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://support.claude.com/en/articles/12512198-creating-custom-skills"&gt;How to create custom skills&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://anthropic.com/engineering/equipping-agents-for-the-real-world-with-agent-skills"&gt;Equipping agents for the real world with Agent Skills&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;About This Repository&lt;/h1&gt; 
&lt;p&gt;This repository contains skills that demonstrate what's possible with Claude's skills system. These skills range from creative applications (art, music, design) to technical tasks (testing web apps, MCP server generation) to enterprise workflows (communications, branding, etc.).&lt;/p&gt; 
&lt;p&gt;Each skill is self-contained in its own folder with a &lt;code&gt;SKILL.md&lt;/code&gt; file containing the instructions and metadata that Claude uses. Browse through these skills to get inspiration for your own skills or to understand different patterns and approaches.&lt;/p&gt; 
&lt;p&gt;Many skills in this repo are open source (Apache 2.0). We've also included the document creation &amp;amp; editing skills that power &lt;a href="https://www.anthropic.com/news/create-files"&gt;Claude's document capabilities&lt;/a&gt; under the hood in the &lt;a href="https://raw.githubusercontent.com/anthropics/skills/main/skills/docx"&gt;&lt;code&gt;skills/docx&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/anthropics/skills/main/skills/pdf"&gt;&lt;code&gt;skills/pdf&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/anthropics/skills/main/skills/pptx"&gt;&lt;code&gt;skills/pptx&lt;/code&gt;&lt;/a&gt;, and &lt;a href="https://raw.githubusercontent.com/anthropics/skills/main/skills/xlsx"&gt;&lt;code&gt;skills/xlsx&lt;/code&gt;&lt;/a&gt; subfolders. These are source-available, not open source, but we wanted to share these with developers as a reference for more complex skills that are actively used in a production AI application.&lt;/p&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;These skills are provided for demonstration and educational purposes only.&lt;/strong&gt; While some of these capabilities may be available in Claude, the implementations and behaviors you receive from Claude may differ from what is shown in these skills. These skills are meant to illustrate patterns and possibilities. Always test skills thoroughly in your own environment before relying on them for critical tasks.&lt;/p&gt; 
&lt;h1&gt;Skill Sets&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/anthropics/skills/main/skills"&gt;./skills&lt;/a&gt;: Skill examples for Creative &amp;amp; Design, Development &amp;amp; Technical, Enterprise &amp;amp; Communication, and Document Skills&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/anthropics/skills/main/spec"&gt;./spec&lt;/a&gt;: The Agent Skills specification&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/anthropics/skills/main/template"&gt;./template&lt;/a&gt;: Skill template&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Try in Claude Code, Claude.ai, and the API&lt;/h1&gt; 
&lt;h2&gt;Claude Code&lt;/h2&gt; 
&lt;p&gt;You can register this repository as a Claude Code Plugin marketplace by running the following command in Claude Code:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;/plugin marketplace add anthropics/skills
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then, to install a specific set of skills:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Select &lt;code&gt;Browse and install plugins&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Select &lt;code&gt;anthropic-agent-skills&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Select &lt;code&gt;document-skills&lt;/code&gt; or &lt;code&gt;example-skills&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Select &lt;code&gt;Install now&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Alternatively, directly install either Plugin via:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;/plugin install document-skills@anthropic-agent-skills
/plugin install example-skills@anthropic-agent-skills
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After installing the plugin, you can use the skill by just mentioning it. For instance, if you install the &lt;code&gt;document-skills&lt;/code&gt; plugin from the marketplace, you can ask Claude Code to do something like: "Use the PDF skill to extract the form fields from &lt;code&gt;path/to/some-file.pdf&lt;/code&gt;"&lt;/p&gt; 
&lt;h2&gt;Claude.ai&lt;/h2&gt; 
&lt;p&gt;These example skills are all already available to paid plans in Claude.ai.&lt;/p&gt; 
&lt;p&gt;To use any skill from this repository or upload custom skills, follow the instructions in &lt;a href="https://support.claude.com/en/articles/12512180-using-skills-in-claude#h_a4222fa77b"&gt;Using skills in Claude&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Claude API&lt;/h2&gt; 
&lt;p&gt;You can use Anthropic's pre-built skills, and upload custom skills, via the Claude API. See the &lt;a href="https://docs.claude.com/en/api/skills-guide#creating-a-skill"&gt;Skills API Quickstart&lt;/a&gt; for more.&lt;/p&gt; 
&lt;h1&gt;Creating a Basic Skill&lt;/h1&gt; 
&lt;p&gt;Skills are simple to create - just a folder with a &lt;code&gt;SKILL.md&lt;/code&gt; file containing YAML frontmatter and instructions. You can use the &lt;strong&gt;template-skill&lt;/strong&gt; in this repository as a starting point:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-markdown"&gt;---
name: my-skill-name
description: A clear description of what this skill does and when to use it
---

# My Skill Name

[Add your instructions here that Claude will follow when this skill is active]

## Examples
- Example usage 1
- Example usage 2

## Guidelines
- Guideline 1
- Guideline 2
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The frontmatter requires only two fields:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;name&lt;/code&gt; - A unique identifier for your skill (lowercase, hyphens for spaces)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;description&lt;/code&gt; - A complete description of what the skill does and when to use it&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The markdown content below contains the instructions, examples, and guidelines that Claude will follow. For more details, see &lt;a href="https://support.claude.com/en/articles/12512198-creating-custom-skills"&gt;How to create custom skills&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Partner Skills&lt;/h1&gt; 
&lt;p&gt;Skills are a great way to teach Claude how to get better at using specific pieces of software. As we see awesome example skills from partners, we may highlight some of them here:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Notion&lt;/strong&gt; - &lt;a href="https://www.notion.so/notiondevs/Notion-Skills-for-Claude-28da4445d27180c7af1df7d8615723d0"&gt;Notion Skills for Claude&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>astral-sh/ty</title>
      <link>https://github.com/astral-sh/ty</link>
      <description>&lt;p&gt;An extremely fast Python type checker and language server, written in Rust.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ty&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/astral-sh/ty"&gt;&lt;img src="https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ty/main/assets/badge/v0.json" alt="ty" /&gt;&lt;/a&gt; &lt;a href="https://pypi.python.org/pypi/ty"&gt;&lt;img src="https://img.shields.io/pypi/v/ty.svg?sanitize=true" alt="PyPI" /&gt;&lt;/a&gt; &lt;a href="https://discord.com/invite/astral-sh"&gt;&lt;img src="https://img.shields.io/badge/Discord-%235865F2.svg?logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;An extremely fast Python type checker and language server, written in Rust.&lt;/p&gt; 
&lt;br /&gt; 
&lt;p align="center"&gt; &lt;img alt="Shows a bar chart with benchmark results." width="500px" src="https://raw.githubusercontent.com/astral-sh/ty/main/docs/assets/ty-benchmark-cli.svg?sanitize=true" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;i&gt;Type checking the &lt;a href="https://github.com/home-assistant/core"&gt;home-assistant&lt;/a&gt; project without caching.&lt;/i&gt; &lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;ty is backed by &lt;a href="https://astral.sh"&gt;Astral&lt;/a&gt;, the creators of &lt;a href="https://github.com/astral-sh/uv"&gt;uv&lt;/a&gt; and &lt;a href="https://github.com/astral-sh/ruff"&gt;Ruff&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Highlights&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;10x - 100x faster than mypy and Pyright&lt;/li&gt; 
 &lt;li&gt;Comprehensive &lt;a href="https://docs.astral.sh/ty/features/diagnostics/"&gt;diagnostics&lt;/a&gt; with rich contextual information&lt;/li&gt; 
 &lt;li&gt;Configurable &lt;a href="https://docs.astral.sh/ty/rules/"&gt;rule levels&lt;/a&gt;, &lt;a href="https://docs.astral.sh/ty/reference/configuration/#overrides"&gt;per-file overrides&lt;/a&gt;, &lt;a href="https://docs.astral.sh/ty/suppression/"&gt;suppression comments&lt;/a&gt;, and first-class project support&lt;/li&gt; 
 &lt;li&gt;Designed for adoption, with support for &lt;a href="https://docs.astral.sh/ty/features/type-system/#redeclarations"&gt;redeclarations&lt;/a&gt; and &lt;a href="https://docs.astral.sh/ty/features/type-system/#gradual-guarantee"&gt;partially typed code&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.astral.sh/ty/features/language-server/"&gt;Language server&lt;/a&gt; with code navigation, completions, code actions, auto-import, inlay hints, on-hover help, etc.&lt;/li&gt; 
 &lt;li&gt;Fine-grained &lt;a href="https://docs.astral.sh/ty/features/language-server/#fine-grained-incrementality"&gt;incremental analysis&lt;/a&gt; designed for fast updates when editing files in an IDE&lt;/li&gt; 
 &lt;li&gt;Editor integrations for &lt;a href="https://docs.astral.sh/ty/editors/#vs-code"&gt;VS Code&lt;/a&gt;, &lt;a href="https://docs.astral.sh/ty/editors/#pycharm"&gt;PyCharm&lt;/a&gt;, &lt;a href="https://docs.astral.sh/ty/editors/#neovim"&gt;Neovim&lt;/a&gt; and more&lt;/li&gt; 
 &lt;li&gt;Advanced typing features like first-class &lt;a href="https://docs.astral.sh/ty/features/type-system/#intersection-types"&gt;intersection types&lt;/a&gt;, advanced &lt;a href="https://docs.astral.sh/ty/features/type-system/#top-and-bottom-materializations"&gt;type narrowing&lt;/a&gt;, and &lt;a href="https://docs.astral.sh/ty/features/type-system/#reachability-based-on-types"&gt;sophisticated reachability analysis&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Getting started&lt;/h2&gt; 
&lt;p&gt;Run ty with &lt;a href="https://docs.astral.sh/uv/guides/tools/#running-tools"&gt;uvx&lt;/a&gt; to get started quickly:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;uvx ty check
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or, check out the &lt;a href="https://play.ty.dev"&gt;ty playground&lt;/a&gt; to try it out in your browser.&lt;/p&gt; 
&lt;p&gt;To learn more about using ty, see the &lt;a href="https://docs.astral.sh/ty/"&gt;documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;To install ty, see the &lt;a href="https://docs.astral.sh/ty/installation/"&gt;installation&lt;/a&gt; documentation.&lt;/p&gt; 
&lt;p&gt;To add the ty language server to your editor, see the &lt;a href="https://docs.astral.sh/ty/editors/"&gt;editor integration&lt;/a&gt; guide.&lt;/p&gt; 
&lt;h2&gt;Getting help&lt;/h2&gt; 
&lt;p&gt;If you have questions or want to report a bug, please open an &lt;a href="https://github.com/astral-sh/ty/issues"&gt;issue&lt;/a&gt; in this repository.&lt;/p&gt; 
&lt;p&gt;You may also join our &lt;a href="https://discord.com/invite/astral-sh"&gt;Discord server&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Development of this project takes place in the &lt;a href="https://github.com/astral-sh/ruff"&gt;Ruff&lt;/a&gt; repository at this time. Please &lt;a href="https://github.com/astral-sh/ruff/pulls"&gt;open pull requests&lt;/a&gt; there for changes to anything in the &lt;code&gt;ruff&lt;/code&gt; submodule (which includes all of the Rust source code).&lt;/p&gt; 
&lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/astral-sh/ty/main/CONTRIBUTING.md"&gt;contributing guide&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;FAQ&lt;/h2&gt; 
&lt;!-- We intentionally use smaller headings for the FAQ items --&gt; 
&lt;!-- markdownlint-disable MD001 --&gt; 
&lt;h4&gt;Why is ty doing _____?&lt;/h4&gt; 
&lt;p&gt;See our &lt;a href="https://docs.astral.sh/ty/reference/typing-faq"&gt;typing FAQ&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;How do you pronounce ty?&lt;/h4&gt; 
&lt;p&gt;It's pronounced as "tee - why" (&lt;a href="https://en.wikipedia.org/wiki/Help:IPA/English#Key"&gt;&lt;code&gt;/tiÀê wa…™/&lt;/code&gt;&lt;/a&gt;)&lt;/p&gt; 
&lt;h4&gt;How should I stylize ty?&lt;/h4&gt; 
&lt;p&gt;Just "ty", please.&lt;/p&gt; 
&lt;!-- markdownlint-enable MD001 --&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;ty is licensed under the MIT license (&lt;a href="https://raw.githubusercontent.com/astral-sh/ty/main/LICENSE"&gt;LICENSE&lt;/a&gt; or &lt;a href="https://opensource.org/licenses/MIT"&gt;https://opensource.org/licenses/MIT&lt;/a&gt;).&lt;/p&gt; 
&lt;p&gt;Unless you explicitly state otherwise, any contribution intentionally submitted for inclusion in ty by you, as defined in the MIT license, shall be licensed as above, without any additional terms or conditions.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a target="_blank" href="https://astral.sh" style="background:none"&gt; &lt;img src="https://raw.githubusercontent.com/astral-sh/uv/main/assets/svg/Astral.svg?sanitize=true" alt="Made by Astral" /&gt; &lt;/a&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>swisskyrepo/PayloadsAllTheThings</title>
      <link>https://github.com/swisskyrepo/PayloadsAllTheThings</link>
      <description>&lt;p&gt;A list of useful payloads and bypass for Web Application Security and Pentest/CTF&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Payloads All The Things&lt;/h1&gt; 
&lt;p&gt;A list of useful payloads and bypasses for Web Application Security. Feel free to improve with your payloads and techniques!&lt;/p&gt; 
&lt;p&gt;You can also contribute with a &lt;span&gt;üçª&lt;/span&gt; IRL, or using the sponsor button.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/sponsors/swisskyrepo"&gt;&lt;img src="https://img.shields.io/static/v1?label=Sponsor&amp;amp;message=%E2%9D%A4&amp;amp;logo=GitHub&amp;amp;link=https://github.com/sponsors/swisskyrepo" alt="Sponsor" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/intent/tweet?text=Payloads%20All%20The%20Things,%20a%20list%20of%20useful%20payloads%20and%20bypasses%20for%20Web%20Application%20Security%20-%20by%20@pentest_swissky&amp;amp;url=https://github.com/swisskyrepo/PayloadsAllTheThings/"&gt;&lt;img src="https://img.shields.io/twitter/url/http/shields.io.svg?style=social" alt="Tweet" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;An alternative display version is available at &lt;a href="https://swisskyrepo.github.io/PayloadsAllTheThings/"&gt;PayloadsAllTheThingsWeb&lt;/a&gt;.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/swisskyrepo/PayloadsAllTheThings/master/.github/banner.png" alt="banner" /&gt; &lt;/p&gt; 
&lt;h2&gt;&lt;span&gt;üìñ&lt;/span&gt; Documentation&lt;/h2&gt; 
&lt;p&gt;Every section contains the following files, you can use the &lt;code&gt;_template_vuln&lt;/code&gt; folder to create a new chapter:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;README.md - vulnerability description and how to exploit it, including several payloads&lt;/li&gt; 
 &lt;li&gt;Intruder - a set of files to give to Burp Intruder&lt;/li&gt; 
 &lt;li&gt;Images - pictures for the README.md&lt;/li&gt; 
 &lt;li&gt;Files - some files referenced in the README.md&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You might also like the other projects from the AllTheThings family :&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://swisskyrepo.github.io/InternalAllTheThings/"&gt;InternalAllTheThings&lt;/a&gt; - Active Directory and Internal Pentest Cheatsheets&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://swisskyrepo.github.io/HardwareAllTheThings/"&gt;HardwareAllTheThings&lt;/a&gt; - Hardware/IOT Pentesting Wiki&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You want more? Check the &lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/raw/master/_LEARNING_AND_SOCIALS/BOOKS.md"&gt;Books&lt;/a&gt; and &lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/raw/master/_LEARNING_AND_SOCIALS/YOUTUBE.md"&gt;YouTube channel&lt;/a&gt; selections.&lt;/p&gt; 
&lt;h2&gt;&lt;span&gt;üßëüíª&lt;/span&gt; Contributions&lt;/h2&gt; 
&lt;p&gt;Be sure to read &lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/raw/master/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=swisskyrepo/PayloadsAllTheThings&amp;amp;max=36" alt="sponsors-list" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;Thanks again for your contribution! &lt;span&gt;‚ù§Ô∏è&lt;/span&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;span&gt;üçª&lt;/span&gt; Sponsors&lt;/h2&gt; 
&lt;p&gt;This project is proudly sponsored by these companies.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Logo&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://serpapi.com"&gt;&lt;img src="https://avatars.githubusercontent.com/u/34724717?s=40&amp;amp;v=4" alt="sponsor-serpapi" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;SerpApi&lt;/strong&gt; is a real time API to access Google search results. It solves the issues of having to rent proxies, solving captchas, and JSON parsing.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://projectdiscovery.io/"&gt;&lt;img src="https://avatars.githubusercontent.com/u/50994705?s=40&amp;amp;v=4" alt="sponsor-projectdiscovery" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;ProjectDiscovery&lt;/strong&gt; - Detect real, exploitable vulnerabilities. Harness the power of Nuclei for fast and accurate findings without false positives.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.vaadata.com/"&gt;&lt;img src="https://avatars.githubusercontent.com/u/48131541?s=40&amp;amp;v=4" alt="sponsor-vaadata" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;VAADATA&lt;/strong&gt; - Ethical Hacking Services&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt;</description>
    </item>
    
    <item>
      <title>danielmiessler/Personal_AI_Infrastructure</title>
      <link>https://github.com/danielmiessler/Personal_AI_Infrastructure</link>
      <description>&lt;p&gt;Personal AI Infrastructure for upgrading humans.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="./pai-logo.png" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="./pai-logo.png" /&gt; 
  &lt;img alt="PAI Logo" src="https://raw.githubusercontent.com/danielmiessler/Personal_AI_Infrastructure/main/pai-logo.png" width="600" /&gt; 
 &lt;/picture&gt; 
 &lt;br /&gt; 
 &lt;br /&gt; 
 &lt;h1&gt;Personal AI Infrastructure&lt;/h1&gt; 
 &lt;h3&gt;Open-source scaffolding for building your own AI-powered operating system&lt;/h3&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/danielmiessler/Personal_AI_Infrastructure/releases"&gt;&lt;img src="https://img.shields.io/badge/version-0.9.1-blue?style=for-the-badge" alt="Version" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/danielmiessler/Personal_AI_Infrastructure/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-MIT-green?style=for-the-badge" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://claude.ai/code"&gt;&lt;img src="https://img.shields.io/badge/Claude_Code-Powered-8B5CF6?style=for-the-badge" alt="Claude Code" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Personal_AI_Infrastructure/main/#-quick-start"&gt;&lt;strong&gt;Quick Start&lt;/strong&gt;&lt;/a&gt; ¬∑ &lt;a href="https://raw.githubusercontent.com/danielmiessler/Personal_AI_Infrastructure/main/#-documentation"&gt;&lt;strong&gt;Documentation&lt;/strong&gt;&lt;/a&gt; ¬∑ &lt;a href="https://raw.githubusercontent.com/danielmiessler/Personal_AI_Infrastructure/main/#-examples"&gt;&lt;strong&gt;Examples&lt;/strong&gt;&lt;/a&gt; ¬∑ &lt;a href="https://raw.githubusercontent.com/danielmiessler/Personal_AI_Infrastructure/main/#-updating-pai"&gt;&lt;strong&gt;Updating&lt;/strong&gt;&lt;/a&gt; ¬∑ &lt;a href="https://raw.githubusercontent.com/danielmiessler/Personal_AI_Infrastructure/main/#-community"&gt;&lt;strong&gt;Community&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://youtu.be/iKwRWwabkEc"&gt;&lt;img src="https://img.youtube.com/vi/iKwRWwabkEc/maxresdefault.jpg" alt="PAI Overview Video" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;a href="https://youtu.be/iKwRWwabkEc"&gt;Watch the full PAI walkthrough&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href="https://danielmiessler.com/blog/real-internet-of-things"&gt;Read: The Real Internet of Things&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
 &lt;hr /&gt; 
 &lt;h1&gt;The best AI in the world should be available to everyone&lt;/h1&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="docs/images/pai-overview.png" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="docs/images/pai-overview.png" /&gt; 
  &lt;img alt="PAI Architecture Overview" src="https://raw.githubusercontent.com/danielmiessler/Personal_AI_Infrastructure/main/docs/images/pai-overview.png" width="800" /&gt; 
 &lt;/picture&gt; 
&lt;/div&gt; 
&lt;p&gt;Right now the most powerful AI setups are being built inside companies for efficiency and profits.&lt;/p&gt; 
&lt;p&gt;That's all good, but I think the purpose of technology is to serve humans‚Äînot the other way around. These new AI frameworks should be available to everyone, including people not in technology, so that regular people can use it to help them flourish.&lt;/p&gt; 
&lt;p&gt;That's what PAI is. It's the foundation for building a Personal AI System that understands your larger goals and context, gets better over time, and that works for &lt;em&gt;you&lt;/em&gt; because it's &lt;em&gt;yours&lt;/em&gt;. Not some generic chatbot. Not some common assistant. A full platform for magnifying yourself and your impact on the world.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Related reading:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://danielmiessler.com/blog/real-internet-of-things"&gt;The Real Internet of Things&lt;/a&gt; ‚Äî The vision behind PAI (full book)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://danielmiessler.com/blog/ai-predictable-path-7-components-2024"&gt;AI's Predictable Path: 7 Components&lt;/a&gt; ‚Äî Visual walkthrough of where AI is heading&lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;h2&gt;What is PAI?&lt;/h2&gt; 
&lt;p&gt;PAI (Personal AI Infrastructure) is an open-source template for building your own AI-powered operating system. It's currently built on &lt;a href="https://claude.ai/code"&gt;Claude Code&lt;/a&gt;, but designed to be platform-independent ‚Äî the architecture, skills, and workflows are structured so future migrations to other AI platforms are straightforward.&lt;/p&gt; 
&lt;picture&gt; 
 &lt;source media="(prefers-color-scheme: dark)" srcset="docs/images/pai-infrastructure.png" /&gt; 
 &lt;source media="(prefers-color-scheme: light)" srcset="docs/images/pai-infrastructure.png" /&gt; 
 &lt;img alt="PAI Infrastructure Architecture" src="https://raw.githubusercontent.com/danielmiessler/Personal_AI_Infrastructure/main/docs/images/pai-infrastructure.png" width="800" /&gt; 
&lt;/picture&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Component&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Skills&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Self-contained AI capabilities with routing, workflows, and documentation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Agents&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Specialized AI personalities for different tasks (engineer, researcher, designer)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Hooks&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Event-driven automation that captures work and manages state&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;History&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Automatic documentation system (UOCS) that captures everything&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] &lt;strong&gt;Start clean, small, and simple.&lt;/strong&gt; Build the scaffolding that makes AI reliable.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;br /&gt; 
&lt;h2&gt;What's New in v0.9.0&lt;/h2&gt; 
&lt;p&gt;Big updates! PAI is now fully &lt;strong&gt;platform-agnostic&lt;/strong&gt; ‚Äî your AI identity, your system.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Feature&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üìä &lt;strong&gt;Observability Dashboard&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Real-time agent monitoring with live charts&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üé≠ &lt;strong&gt;Genericized Identity&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Configure your DA name, it flows everywhere&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;‚öôÔ∏è &lt;strong&gt;Better Configuration&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Clear docs for all environment variables&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;üëâ &lt;a href="https://raw.githubusercontent.com/danielmiessler/Personal_AI_Infrastructure/main/#-updates"&gt;&lt;strong&gt;See full changelog&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;h2&gt;üöÄ Quick Start&lt;/h2&gt; 
&lt;p&gt;Choose your platform:&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üçé macOS&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;1. Clone PAI&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/danielmiessler/PAI.git ~/PAI
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;2. Create Symlink&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Remove existing ~/.claude if present (backup first if needed)
[ -d ~/.claude ] &amp;amp;&amp;amp; mv ~/.claude ~/.claude.backup
ln -s ~/PAI/.claude ~/.claude
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;3. Run the Setup Wizard&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;~/.claude/Tools/setup/bootstrap.sh
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;4. Add Your API Keys&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;cp ~/.claude/.env.example ~/.claude/.env
nano ~/.claude/.env
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;5. Start Claude Code&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;source ~/.zshrc  # Load PAI environment
claude
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üêß Linux&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;1. Clone PAI&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/danielmiessler/PAI.git ~/PAI
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;2. Create Symlink&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Remove existing ~/.claude if present (backup first if needed)
[ -d ~/.claude ] &amp;amp;&amp;amp; mv ~/.claude ~/.claude.backup
ln -s ~/PAI/.claude ~/.claude
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;3. Run the Setup Wizard&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;~/.claude/Tools/setup/bootstrap.sh
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;4. Add Your API Keys&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;cp ~/.claude/.env.example ~/.claude/.env
nano ~/.claude/.env
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;5. Start Claude Code&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;source ~/.bashrc  # Load PAI environment
claude
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ü™ü Windows&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;1. Clone PAI&lt;/strong&gt; (PowerShell)&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-powershell"&gt;git clone https://github.com/danielmiessler/PAI.git $env:USERPROFILE\PAI
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;2. Create Symlink&lt;/strong&gt; (Run PowerShell as Administrator)&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-powershell"&gt;# Remove existing .claude if present (backup first if needed)
if (Test-Path "$env:USERPROFILE\.claude") { Rename-Item "$env:USERPROFILE\.claude" "$env:USERPROFILE\.claude.backup" }
New-Item -ItemType SymbolicLink -Path "$env:USERPROFILE\.claude" -Target "$env:USERPROFILE\PAI\.claude"
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;3. Run the Setup Wizard&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-powershell"&gt;&amp;amp; "$env:USERPROFILE\.claude\tools\setup\bootstrap.ps1"
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;4. Add Your API Keys&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-powershell"&gt;Copy-Item "$env:USERPROFILE\.claude\.env.example" "$env:USERPROFILE\.claude\.env"
notepad "$env:USERPROFILE\.claude\.env"
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;5. Start Claude Code&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-powershell"&gt;# Restart PowerShell to load environment, then:
claude
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;br /&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] The setup wizard will configure your name, email, AI assistant name, and environment variables to customize to your environment.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;üìö For detailed setup, see &lt;a href="https://raw.githubusercontent.com/danielmiessler/Personal_AI_Infrastructure/main/docs/QUICKSTART.md"&gt;&lt;code&gt;docs/QUICKSTART.md&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;h2&gt;üìö Documentation&lt;/h2&gt; 
&lt;p&gt;All documentation lives in the CORE skill (&lt;code&gt;.claude/Skills/CORE/&lt;/code&gt;):&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Document&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Personal_AI_Infrastructure/main/.claude/Skills/CORE/CONSTITUTION.md"&gt;&lt;strong&gt;CONSTITUTION.md&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;System philosophy, architecture, operating principles&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Personal_AI_Infrastructure/main/.claude/Skills/CORE/SkillSystem.md"&gt;&lt;strong&gt;SkillSystem.md&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;How to create your own skills&lt;/strong&gt; ‚Äî the canonical skill structure guide&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Personal_AI_Infrastructure/main/.claude/Skills/CORE/SKILL.md"&gt;&lt;strong&gt;SKILL.md&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Main PAI skill with identity, preferences, quick reference&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Personal_AI_Infrastructure/main/.claude/Skills/CORE/HookSystem.md"&gt;HookSystem.md&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Event-driven automation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Personal_AI_Infrastructure/main/.claude/Skills/CORE/HistorySystem.md"&gt;HistorySystem.md&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Automatic work documentation (UOCS)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Additional Reference&lt;/strong&gt;&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Document&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Personal_AI_Infrastructure/main/.claude/Skills/CORE/Prompting.md"&gt;Prompting.md&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Prompt engineering patterns&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Personal_AI_Infrastructure/main/.claude/Skills/CORE/Aesthetic.md"&gt;Aesthetic.md&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Visual design system&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Personal_AI_Infrastructure/main/.claude/voice-server/README.md"&gt;voice-server/README.md&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Text-to-speech feedback&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;br /&gt; 
&lt;h2&gt;üé® Examples&lt;/h2&gt; 
&lt;p&gt;Explore example skills in &lt;code&gt;.claude/Skills/&lt;/code&gt;:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Skill&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Observability/&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Real-time agent monitoring dashboard with WebSocket streaming&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;BrightData/&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Four-tier progressive web scraping with automatic fallback&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Fabric/&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Native Fabric patterns&lt;/strong&gt; ‚Äî 248 patterns run directly in Claude's context (no CLI needed)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Research/&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Multi-source research workflows&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Createskill/&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Templates for creating new skills&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Each skill demonstrates the skills-as-containers pattern with routing, workflows, and self-contained documentation.&lt;/p&gt; 
&lt;h3&gt;Native Fabric Patterns&lt;/h3&gt; 
&lt;p&gt;The Fabric skill now executes patterns &lt;strong&gt;natively&lt;/strong&gt; within Claude Code ‚Äî no CLI spawning required:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Your subscription's power&lt;/strong&gt; ‚Äî Patterns run with your Opus/Sonnet model, not Fabric's configured model&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Full context&lt;/strong&gt; ‚Äî Patterns have access to your entire conversation history&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Faster execution&lt;/strong&gt; ‚Äî No process spawning overhead&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;248 patterns included&lt;/strong&gt; ‚Äî extract_wisdom, summarize, threat modeling, and more&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Update patterns from upstream
.claude/Skills/Fabric/tools/update-patterns.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Only use &lt;code&gt;fabric&lt;/code&gt; CLI for YouTube transcripts (&lt;code&gt;-y&lt;/code&gt;) or pattern updates (&lt;code&gt;-U&lt;/code&gt;).&lt;/p&gt; 
&lt;br /&gt; 
&lt;h2&gt;üèóÔ∏è The Thirteen Founding Principles&lt;/h2&gt; 
&lt;p&gt;PAI is built on 13 foundational principles that define how to build reliable AI infrastructure.&lt;/p&gt; 
&lt;p&gt;Complete architecture documentation: &lt;a href="https://raw.githubusercontent.com/danielmiessler/Personal_AI_Infrastructure/main/.claude/Skills/CORE/Architecture.md"&gt;&lt;code&gt;.claude/Skills/CORE/Architecture.md&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;1. Clear Thinking + Prompting is King&lt;/h3&gt; 
&lt;p&gt;The quality of outcomes depends on the quality of thinking and prompts. Before any code, before any architecture‚Äîthere must be clear thinking.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/danielmiessler/Personal_AI_Infrastructure/main/docs/images/principle-01-clear-thinking.png" alt="Clear Thinking + Prompting" width="100%" /&gt; 
&lt;hr /&gt; 
&lt;h3&gt;2. Scaffolding &amp;gt; Model&lt;/h3&gt; 
&lt;p&gt;The system architecture matters more than the underlying AI model. A well-structured system with good scaffolding will outperform a more powerful model with poor structure.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/danielmiessler/Personal_AI_Infrastructure/main/docs/images/principle-02-scaffolding.png" alt="Scaffolding &gt; Model" width="100%" /&gt; 
&lt;hr /&gt; 
&lt;h3&gt;3. As Deterministic as Possible&lt;/h3&gt; 
&lt;p&gt;Favor predictable, repeatable outcomes over flexibility. Same input ‚Üí Same output. Always.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/danielmiessler/Personal_AI_Infrastructure/main/docs/images/principle-03-deterministic.png" alt="Deterministic Systems" width="100%" /&gt; 
&lt;hr /&gt; 
&lt;h3&gt;4. Code Before Prompts&lt;/h3&gt; 
&lt;p&gt;Write code to solve problems, use prompts to orchestrate code. Prompts should never replicate functionality that code can provide.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/danielmiessler/Personal_AI_Infrastructure/main/docs/images/principle-04-code-before-prompts.png" alt="Code Before Prompts" width="100%" /&gt; 
&lt;hr /&gt; 
&lt;h3&gt;5. Spec / Test / Evals First&lt;/h3&gt; 
&lt;p&gt;Define expected behavior before writing implementation. If you can't specify it, you can't test it. If you can't test it, you can't trust it.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/danielmiessler/Personal_AI_Infrastructure/main/docs/images/principle-05-spec-test-evals.png" alt="Spec / Test / Evals First" width="100%" /&gt; 
&lt;hr /&gt; 
&lt;h3&gt;6. UNIX Philosophy&lt;/h3&gt; 
&lt;p&gt;Do one thing well. Compose tools through standard interfaces. Build small, focused tools‚Äîcompose them for complex operations.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/danielmiessler/Personal_AI_Infrastructure/main/docs/images/principle-06-unix-philosophy.png" alt="UNIX Philosophy" width="100%" /&gt; 
&lt;hr /&gt; 
&lt;h3&gt;7. ENG / SRE Principles&lt;/h3&gt; 
&lt;p&gt;Apply software engineering and site reliability practices to AI systems. AI infrastructure is infrastructure‚Äîtreat it with the same rigor.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/danielmiessler/Personal_AI_Infrastructure/main/docs/images/principle-07-eng-sre.png" alt="ENG / SRE Principles" width="100%" /&gt; 
&lt;hr /&gt; 
&lt;h3&gt;8. CLI as Interface&lt;/h3&gt; 
&lt;p&gt;Every operation should be accessible via command line. If there's no CLI command for it, you can't script it or test it reliably.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/danielmiessler/Personal_AI_Infrastructure/main/docs/images/principle-08-cli-interface.png" alt="CLI as Interface" width="100%" /&gt; 
&lt;hr /&gt; 
&lt;h3&gt;9. Goal ‚Üí Code ‚Üí CLI ‚Üí Prompts ‚Üí Agents&lt;/h3&gt; 
&lt;p&gt;The proper development pipeline for any new feature. Each layer builds on the previous‚Äîskip a layer, get a shaky system.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/danielmiessler/Personal_AI_Infrastructure/main/docs/images/principle-09-implementation-pipeline.png" alt="Implementation Pipeline" width="100%" /&gt; 
&lt;hr /&gt; 
&lt;h3&gt;10. Meta / Self Update System&lt;/h3&gt; 
&lt;p&gt;The system should be able to improve itself. A system that can't update itself will stagnate.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/danielmiessler/Personal_AI_Infrastructure/main/docs/images/principle-10-self-update.png" alt="Self-Improving System" width="100%" /&gt; 
&lt;hr /&gt; 
&lt;h3&gt;11. Custom Skill Management&lt;/h3&gt; 
&lt;p&gt;Skills are the organizational unit for all domain expertise. Skills are how PAI scales‚Äîeach new domain gets its own skill, maintaining organization as the system grows.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/danielmiessler/Personal_AI_Infrastructure/main/docs/images/principle-11-skill-management.png" alt="Skill Architecture" width="100%" /&gt; 
&lt;hr /&gt; 
&lt;h3&gt;12. Custom History System&lt;/h3&gt; 
&lt;p&gt;Automatic capture and preservation of valuable work. Memory makes intelligence compound. Without history, every session starts from zero.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/danielmiessler/Personal_AI_Infrastructure/main/docs/images/principle-12-history-system.png" alt="History System" width="100%" /&gt; 
&lt;hr /&gt; 
&lt;h3&gt;13. Custom Agent Personalities / Voices&lt;/h3&gt; 
&lt;p&gt;Specialized agents with distinct personalities for different tasks. Personality isn't decoration‚Äîit's functional.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/danielmiessler/Personal_AI_Infrastructure/main/docs/images/principle-13-agent-personalities.png" alt="Agent Personalities" width="100%" /&gt; 
&lt;br /&gt; 
&lt;h2&gt;üõ†Ô∏è Technology Stack&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Category&lt;/th&gt; 
   &lt;th&gt;Choice&lt;/th&gt; 
   &lt;th&gt;Note&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Runtime&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Bun&lt;/td&gt; 
   &lt;td&gt;NOT Node.js&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Language&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;TypeScript&lt;/td&gt; 
   &lt;td&gt;NOT Python&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Package Manager&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Bun&lt;/td&gt; 
   &lt;td&gt;NOT npm/yarn/pnpm&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Format&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Markdown&lt;/td&gt; 
   &lt;td&gt;NOT HTML for basic content&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Testing&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Vitest&lt;/td&gt; 
   &lt;td&gt;When needed&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Voice&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;ElevenLabs&lt;/td&gt; 
   &lt;td&gt;TTS integration&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;br /&gt; 
&lt;h2&gt;üîÑ Updating PAI&lt;/h2&gt; 
&lt;p&gt;PAI includes an intelligent sideloading system that helps you update while preserving your customizations.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# In Claude Code, run:
/paiupdate    # or just /pa
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;What happens:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Your DA fetches the latest PAI to a staging area (doesn't touch your files)&lt;/li&gt; 
 &lt;li&gt;Analyzes differences between upstream and your customizations&lt;/li&gt; 
 &lt;li&gt;Generates a personalized report showing conflicts vs. safe updates&lt;/li&gt; 
 &lt;li&gt;You choose what to adopt ‚Äî your DA handles the merge&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Your custom skills, modified hooks, and personalized settings are &lt;strong&gt;never blindly overwritten&lt;/strong&gt;. The system understands that your &lt;code&gt;env.DA&lt;/code&gt;, custom environment variables, and personal tweaks are intentional.&lt;/p&gt; 
&lt;br /&gt; 
&lt;h2&gt;üí¨ Community&lt;/h2&gt; 
&lt;p&gt;Kai and I work hard to address issues and PRs throughout the week ‚Äî we try not to get too far behind!&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Channel&lt;/th&gt; 
   &lt;th&gt;Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üêõ &lt;strong&gt;Issues&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/danielmiessler/Personal_AI_Infrastructure/issues"&gt;Report bugs or request features&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üí¨ &lt;strong&gt;Discussions&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/danielmiessler/Personal_AI_Infrastructure/discussions"&gt;Ask questions and share ideas&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üé• &lt;strong&gt;Video&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://youtu.be/iKwRWwabkEc"&gt;Watch the full PAI walkthrough&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üìù &lt;strong&gt;Blog&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://danielmiessler.com/blog/real-internet-of-things"&gt;The Real Internet of Things&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;br /&gt; 
&lt;h2&gt;üìù Updates&lt;/h2&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;strong&gt;v0.9.1 (2025-12-04) ‚Äî Setup Script Fix&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;PAI_DIR Auto-Configuration&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;setup.sh&lt;/code&gt; now automatically configures &lt;code&gt;PAI_DIR&lt;/code&gt; in &lt;code&gt;settings.json&lt;/code&gt; with your actual home directory path&lt;/li&gt; 
  &lt;li&gt;No more manual editing of &lt;code&gt;__HOME__/.claude&lt;/code&gt; placeholder&lt;/li&gt; 
  &lt;li&gt;Clear error messaging if hooks fail due to misconfigured paths&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;Improved Documentation&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Added &lt;code&gt;_setupNote&lt;/code&gt; in &lt;code&gt;settings.json&lt;/code&gt; explaining the fix&lt;/li&gt; 
  &lt;li&gt;Updated &lt;code&gt;_envDocs&lt;/code&gt; with troubleshooting guidance&lt;/li&gt; 
  &lt;li&gt;QUICKSTART.md troubleshooting section for PAI_DIR issues&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;Bug Fix&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Fixes #110 ‚Äî Hook failures caused by unexpanded PAI_DIR placeholder&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;v0.9.0 (2025-12-01) ‚Äî Platform Agnostic Release&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;This release focuses on making PAI fully portable and fork-friendly. Your AI, your identity, your system.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Observability Dashboard&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Complete real-time agent monitoring at &lt;code&gt;.claude/Skills/Observability/&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;WebSocket streaming of all agent activity&lt;/li&gt; 
  &lt;li&gt;Live pulse charts, event timelines, and swim lanes&lt;/li&gt; 
  &lt;li&gt;Multiple themes (Tokyo Night, Nord, Catppuccin, etc.)&lt;/li&gt; 
  &lt;li&gt;Security obfuscation for sensitive data&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;Genericized Agent Identity&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;All agent references now use &lt;code&gt;process.env.DA || 'main'&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;No more hardcoded names ‚Äî your DA name flows through the entire system&lt;/li&gt; 
  &lt;li&gt;Observability dashboard shows your configured identity&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;Platform-Agnostic Configuration&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Clear separation: &lt;code&gt;settings.json&lt;/code&gt; for identity/paths, &lt;code&gt;.env&lt;/code&gt; for API keys&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;DA&lt;/code&gt; (Digital Assistant name) ‚Äî your AI's identity&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;PAI_DIR&lt;/code&gt; ‚Äî root directory for all configuration&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;TIME_ZONE&lt;/code&gt; ‚Äî configurable timezone for timestamps&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;Skill System Improvements&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Canonical TitleCase file naming throughout&lt;/li&gt; 
  &lt;li&gt;Standardized skill-workflow-notification script for dashboard detection&lt;/li&gt; 
  &lt;li&gt;All paths use &lt;code&gt;${PAI_DIR}/&lt;/code&gt; for location-agnostic installation&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;v0.8.0 (2025-11-25) ‚Äî Research &amp;amp; Documentation&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;Research Skill&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Comprehensive research skill with 10 specialized workflows&lt;/li&gt; 
  &lt;li&gt;Multi-source research with parallel agent execution&lt;/li&gt; 
  &lt;li&gt;Fabric pattern integration (242+ AI patterns)&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;Infrastructure&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Path standardization using &lt;code&gt;${PAI_DIR}/&lt;/code&gt; throughout&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;PAI_CONTRACT.md&lt;/code&gt; defining core guarantees&lt;/li&gt; 
  &lt;li&gt;Self-test validation system for health checks&lt;/li&gt; 
  &lt;li&gt;Protection system for PAI-specific files&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;v0.7.0 (2025-11-20) ‚Äî Protection &amp;amp; Clarity&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;PAI Path Resolution System&lt;/strong&gt; (#112)&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Centralized &lt;code&gt;pai-paths.ts&lt;/code&gt; library ‚Äî single source of truth&lt;/li&gt; 
  &lt;li&gt;Smart detection with fallback to &lt;code&gt;~/.claude&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;Updated 7 hooks to use centralized paths&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;PAI vs Kai Clarity&lt;/strong&gt; (#113)&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;PAI_CONTRACT.md&lt;/code&gt; ‚Äî official contract defining boundaries&lt;/li&gt; 
  &lt;li&gt;Self-test system (&lt;code&gt;bun ${PAI_DIR}/hooks/self-test.ts&lt;/code&gt;)&lt;/li&gt; 
  &lt;li&gt;Clear README section distinguishing PAI from Kai&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;Protection System&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;.pai-protected.json&lt;/code&gt; manifest of protected files&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;validate-protected.ts&lt;/code&gt; script for pre-commit validation&lt;/li&gt; 
  &lt;li&gt;Pre-commit hook template for automated checks&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;v0.6.5 (2025-11-18) ‚Äî BrightData Integration&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;Four-Tier Progressive Web Scraping&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Tier 1: WebFetch (free, built-in)&lt;/li&gt; 
  &lt;li&gt;Tier 2: cURL with headers (free, more reliable)&lt;/li&gt; 
  &lt;li&gt;Tier 3: Playwright (free, JavaScript rendering)&lt;/li&gt; 
  &lt;li&gt;Tier 4: Bright Data MCP (paid, anti-bot bypass)&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;v0.6.0 (2025-11-15) ‚Äî Major Architecture Update&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;Repository Restructure&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Moved all configuration to &lt;code&gt;.claude/&lt;/code&gt; directory&lt;/li&gt; 
  &lt;li&gt;Skills-as-containers architecture&lt;/li&gt; 
  &lt;li&gt;Three-tier progressive disclosure&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;Skills System&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Art skill with visual content generation&lt;/li&gt; 
  &lt;li&gt;Story-explanation skill for narrative summaries&lt;/li&gt; 
  &lt;li&gt;Create-skill and create-cli meta-skills&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;Hook System&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Comprehensive event capture system&lt;/li&gt; 
  &lt;li&gt;Session summary and tool output capture&lt;/li&gt; 
  &lt;li&gt;Tab title updates&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;Voice Integration&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Voice server with ElevenLabs TTS&lt;/li&gt; 
  &lt;li&gt;Session start notifications&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;v0.5.0 and Earlier&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;v0.5.0 ‚Äî Foundation&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;CORE skill as central context loader&lt;/li&gt; 
  &lt;li&gt;Constitution defining system principles&lt;/li&gt; 
  &lt;li&gt;CLI-First Architecture pattern&lt;/li&gt; 
  &lt;li&gt;Initial skills: Fabric, FFUF, Alex Hormozi pitch&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;Pre-v0.5.0 ‚Äî Early Development&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Initial repository setup&lt;/li&gt; 
  &lt;li&gt;Basic settings.json structure&lt;/li&gt; 
  &lt;li&gt;Agent personality definitions&lt;/li&gt; 
  &lt;li&gt;Foundational hook experiments&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;br /&gt; 
&lt;h2&gt;üìú License&lt;/h2&gt; 
&lt;p&gt;MIT License ‚Äî see &lt;a href="https://raw.githubusercontent.com/danielmiessler/Personal_AI_Infrastructure/main/LICENSE"&gt;&lt;code&gt;LICENSE&lt;/code&gt;&lt;/a&gt; for details.&lt;/p&gt; 
&lt;br /&gt; 
&lt;h2&gt;üôè Acknowledgments&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Built on &lt;a href="https://code.claude.com"&gt;Claude Code&lt;/a&gt; by Anthropic.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;PAI is the technical foundation for &lt;a href="https://human3.unsupervised-learning.com"&gt;Human 3.0&lt;/a&gt; ‚Äî a program I created to help people transform into a version of themselves that can thrive in the post-corporate world that's coming. Human 3.0 means AI-augmented humans who build and control their own AI systems.&lt;/p&gt; 
&lt;p&gt;Right now, the most sophisticated AI infrastructure exists inside corporations with massive engineering teams. PAI exists to change that. To give individuals the same scaffolding that companies spend millions building.&lt;/p&gt; 
&lt;p&gt;Your AI, knowing how you work, learning from your patterns, serving your goals ‚Äî not some corporation's engagement metrics. That's what this enables.&lt;/p&gt; 
&lt;br /&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;strong&gt;Start clean. Start small. Build the AI infrastructure you need.&lt;/strong&gt;&lt;/p&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Personal_AI_Infrastructure/main/#personal-ai-infrastructure"&gt;‚¨Ü Back to Top&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>danielmiessler/Fabric</title>
      <link>https://github.com/danielmiessler/Fabric</link>
      <description>&lt;p&gt;Fabric is an open-source framework for augmenting humans using AI. It provides a modular system for solving specific problems using a crowdsourced set of AI prompts that can be used anywhere.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;a href="https://go.warp.dev/fabric" target="_blank"&gt; &lt;sup&gt;Special thanks to:&lt;/sup&gt; &lt;br /&gt; &lt;img alt="Warp sponsorship" width="400" src="https://raw.githubusercontent.com/warpdotdev/brand-assets/refs/heads/main/Github/Sponsor/Warp-Github-LG-02.png" /&gt; &lt;br /&gt; 
  &lt;h&gt;
   Warp, built for coding with multiple AI agents 
   &lt;br /&gt; 
   &lt;sup&gt;Available for macOS, Linux and Windows&lt;/sup&gt; 
  &lt;/h&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/danielmiessler/Fabric/main/docs/images/fabric-logo-gif.gif" alt="fabriclogo" width="400" height="400" /&gt; 
 &lt;h1&gt;&lt;code&gt;fabric&lt;/code&gt;&lt;/h1&gt; 
 &lt;p&gt;&lt;img src="https://img.shields.io/badge/mission-human_flourishing_via_AI_augmentation-purple" alt="Static Badge" /&gt; &lt;br /&gt; &lt;img src="https://img.shields.io/github/languages/top/danielmiessler/fabric" alt="GitHub top language" /&gt; &lt;img src="https://img.shields.io/github/last-commit/danielmiessler/fabric" alt="GitHub last commit" /&gt; &lt;a href="https://opensource.org/licenses/MIT"&gt;&lt;img src="https://img.shields.io/badge/License-MIT-green.svg?sanitize=true" alt="License: MIT" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/danielmiessler/fabric"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;h4&gt;&lt;code&gt;fabric&lt;/code&gt; is an open-source framework for augmenting humans using AI.&lt;/h4&gt; 
 &lt;/div&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/danielmiessler/Fabric/main/docs/images/fabric-summarize.png" alt="Screenshot of fabric" /&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#updates"&gt;Updates&lt;/a&gt; ‚Ä¢ &lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#what-and-why"&gt;What and Why&lt;/a&gt; ‚Ä¢ &lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#philosophy"&gt;Philosophy&lt;/a&gt; ‚Ä¢ &lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#installation"&gt;Installation&lt;/a&gt; ‚Ä¢ &lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#usage"&gt;Usage&lt;/a&gt; ‚Ä¢ &lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#rest-api-server"&gt;REST API&lt;/a&gt; ‚Ä¢ &lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#examples"&gt;Examples&lt;/a&gt; ‚Ä¢ &lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#just-use-the-patterns"&gt;Just Use the Patterns&lt;/a&gt; ‚Ä¢ &lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#custom-patterns"&gt;Custom Patterns&lt;/a&gt; ‚Ä¢ &lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#helper-apps"&gt;Helper Apps&lt;/a&gt; ‚Ä¢ &lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#meta"&gt;Meta&lt;/a&gt;&lt;/p&gt;  
&lt;h2&gt;What and why&lt;/h2&gt; 
&lt;p&gt;Since the start of modern AI in late 2022 we've seen an &lt;strong&gt;&lt;em&gt;extraordinary&lt;/em&gt;&lt;/strong&gt; number of AI applications for accomplishing tasks. There are thousands of websites, chat-bots, mobile apps, and other interfaces for using all the different AI out there.&lt;/p&gt; 
&lt;p&gt;It's all really exciting and powerful, but &lt;em&gt;it's not easy to integrate this functionality into our lives.&lt;/em&gt;&lt;/p&gt; 
&lt;div class="align center"&gt; 
 &lt;h4&gt;In other words, AI doesn't have a capabilities problem‚Äîit has an &lt;em&gt;integration&lt;/em&gt; problem.&lt;/h4&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Fabric was created to address this by creating and organizing the fundamental units of AI‚Äîthe prompts themselves!&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Fabric organizes prompts by real-world task, allowing people to create, collect, and organize their most important AI solutions in a single place for use in their favorite tools. And if you're command-line focused, you can use Fabric itself as the interface!&lt;/p&gt; 
&lt;h2&gt;Updates&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click to view recent updates&lt;/summary&gt; 
 &lt;p&gt;Dear Users,&lt;/p&gt; 
 &lt;p&gt;We've been doing so many exciting things here at Fabric, I wanted to give a quick summary here to give you a sense of our development velocity!&lt;/p&gt; 
 &lt;p&gt;Below are the &lt;strong&gt;new features and capabilities&lt;/strong&gt; we've added (newest first):&lt;/p&gt; 
 &lt;h3&gt;Recent Major Features&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://github.com/danielmiessler/fabric/releases/tag/v1.4.356"&gt;v1.4.356&lt;/a&gt; (Dec 22, 2025) ‚Äî &lt;strong&gt;Complete Internationalization&lt;/strong&gt;: Full i18n support for setup prompts across all 10 languages with intelligent environment variable handling‚Äîmaking Fabric truly accessible worldwide while maintaining configuration consistency.&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/danielmiessler/fabric/releases/tag/v1.4.350"&gt;v1.4.350&lt;/a&gt; (Dec 18, 2025) ‚Äî &lt;strong&gt;Interactive API Documentation&lt;/strong&gt;: Adds Swagger/OpenAPI UI at &lt;code&gt;/swagger/index.html&lt;/code&gt; with comprehensive REST API documentation, enhanced developer guides, and improved endpoint discoverability for easier integration.&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/danielmiessler/fabric/releases/tag/v1.4.338"&gt;v1.4.338&lt;/a&gt; (Dec 4, 2025) ‚Äî Add Abacus vendor support for Chat-LLM models (see &lt;a href="https://abacus.ai/app/route-llm-apis"&gt;RouteLLM APIs&lt;/a&gt;).&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/danielmiessler/fabric/releases/tag/v1.4.337"&gt;v1.4.337&lt;/a&gt; (Dec 4, 2025) ‚Äî Add "Z AI" vendor support. See the &lt;a href="https://docs.z.ai/guides/overview/overview"&gt;Z AI overview&lt;/a&gt; page for more details.&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/danielmiessler/fabric/releases/tag/v1.4.334"&gt;v1.4.334&lt;/a&gt; (Nov 26, 2025) ‚Äî &lt;strong&gt;Claude Opus 4.5&lt;/strong&gt;: Updates the Anthropic SDK to the latest and adds the new &lt;a href="https://www.anthropic.com/news/claude-opus-4-5"&gt;Claude Opus 4.5&lt;/a&gt; to the available models.&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/danielmiessler/fabric/releases/tag/v1.4.331"&gt;v1.4.331&lt;/a&gt; (Nov 23, 2025) ‚Äî &lt;strong&gt;Support for GitHub Models&lt;/strong&gt;: Adds support for using GitHub Models.&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/danielmiessler/fabric/releases/tag/v1.4.322"&gt;v1.4.322&lt;/a&gt; (Nov 5, 2025) ‚Äî &lt;strong&gt;Interactive HTML Concept Maps and Claude Sonnet 4.5&lt;/strong&gt;: Adds &lt;code&gt;create_conceptmap&lt;/code&gt; pattern for visual knowledge representation using Vis.js, introduces WELLNESS category with psychological analysis patterns, and upgrades to Claude Sonnet 4.5&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/danielmiessler/fabric/releases/tag/v1.4.317"&gt;v1.4.317&lt;/a&gt; (Sep 21, 2025) ‚Äî &lt;strong&gt;Portuguese Language Variants&lt;/strong&gt;: Adds BCP 47 locale normalization with support for Brazilian Portuguese (pt-BR) and European Portuguese (pt-PT) with intelligent fallback chains&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/danielmiessler/fabric/releases/tag/v1.4.314"&gt;v1.4.314&lt;/a&gt; (Sep 17, 2025) ‚Äî &lt;strong&gt;Azure OpenAI Migration&lt;/strong&gt;: Migrates to official &lt;code&gt;openai-go/azure&lt;/code&gt; SDK with improved authentication and default API version support&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/danielmiessler/fabric/releases/tag/v1.4.311"&gt;v1.4.311&lt;/a&gt; (Sep 13, 2025) ‚Äî &lt;strong&gt;More internationalization support&lt;/strong&gt;: Adds de (German), fa (Persian / Farsi), fr (French), it (Italian), ja (Japanese), pt (Portuguese), zh (Chinese)&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/danielmiessler/fabric/releases/tag/v1.4.309"&gt;v1.4.309&lt;/a&gt; (Sep 9, 2025) ‚Äî &lt;strong&gt;Comprehensive internationalization support&lt;/strong&gt;: Includes English and Spanish locale files.&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/danielmiessler/fabric/releases/tag/v1.4.303"&gt;v1.4.303&lt;/a&gt; (Aug 29, 2025) ‚Äî &lt;strong&gt;New Binary Releases&lt;/strong&gt;: Linux ARM and Windows ARM targets. You can run Fabric on the Raspberry PI and on your Windows Surface!&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/danielmiessler/fabric/releases/tag/v1.4.294"&gt;v1.4.294&lt;/a&gt; (Aug 20, 2025) ‚Äî &lt;strong&gt;Venice AI Support&lt;/strong&gt;: Added the Venice AI provider. Venice is a Privacy-First, Open-Source AI provider. See their &lt;a href="https://docs.venice.ai/overview/about-venice"&gt;"About Venice"&lt;/a&gt; page for details.&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/danielmiessler/fabric/releases/tag/v1.4.291"&gt;v1.4.291&lt;/a&gt; (Aug 18, 2025) ‚Äî &lt;strong&gt;Speech To Text&lt;/strong&gt;: Add OpenAI speech-to-text support with &lt;code&gt;--transcribe-file&lt;/code&gt;, &lt;code&gt;--transcribe-model&lt;/code&gt;, and &lt;code&gt;--split-media-file&lt;/code&gt; flags.&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/danielmiessler/fabric/releases/tag/v1.4.287"&gt;v1.4.287&lt;/a&gt; (Aug 16, 2025) ‚Äî &lt;strong&gt;AI Reasoning&lt;/strong&gt;: Add Thinking to Gemini models and introduce &lt;code&gt;readme_updates&lt;/code&gt; python script&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/danielmiessler/fabric/releases/tag/v1.4.286"&gt;v1.4.286&lt;/a&gt; (Aug 14, 2025) ‚Äî &lt;strong&gt;AI Reasoning&lt;/strong&gt;: Introduce Thinking Config Across Anthropic and OpenAI Providers&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/danielmiessler/fabric/releases/tag/v1.4.285"&gt;v1.4.285&lt;/a&gt; (Aug 13, 2025) ‚Äî &lt;strong&gt;Extended Context&lt;/strong&gt;: Enable One Million Token Context Beta Feature for Sonnet-4&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/danielmiessler/fabric/releases/tag/v1.4.284"&gt;v1.4.284&lt;/a&gt; (Aug 12, 2025) ‚Äî &lt;strong&gt;Easy Shell Completions Setup&lt;/strong&gt;: Introduce One-Liner Curl Install for Completions&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/danielmiessler/fabric/releases/tag/v1.4.283"&gt;v1.4.283&lt;/a&gt; (Aug 12, 2025) ‚Äî &lt;strong&gt;Model Management&lt;/strong&gt;: Add Vendor Selection Support for Models&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/danielmiessler/fabric/releases/tag/v1.4.282"&gt;v1.4.282&lt;/a&gt; (Aug 11, 2025) ‚Äî &lt;strong&gt;Enhanced Shell Completions&lt;/strong&gt;: Enhanced Shell Completions for Fabric CLI Binaries&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/danielmiessler/fabric/releases/tag/v1.4.281"&gt;v1.4.281&lt;/a&gt; (Aug 11, 2025) ‚Äî &lt;strong&gt;Gemini Search Tool&lt;/strong&gt;: Add Web Search Tool Support for Gemini Models&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/danielmiessler/fabric/releases/tag/v1.4.278"&gt;v1.4.278&lt;/a&gt; (Aug 9, 2025) ‚Äî &lt;strong&gt;Enhance YouTube Transcripts&lt;/strong&gt;: Enhance YouTube Support with Custom yt-dlp Arguments&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/danielmiessler/fabric/releases/tag/v1.4.277"&gt;v1.4.277&lt;/a&gt; (Aug 8, 2025) ‚Äî &lt;strong&gt;Desktop Notifications&lt;/strong&gt;: Add cross-platform desktop notifications to Fabric CLI&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/danielmiessler/fabric/releases/tag/v1.4.274"&gt;v1.4.274&lt;/a&gt; (Aug 7, 2025) ‚Äî &lt;strong&gt;Claude 4.1 Added&lt;/strong&gt;: Add Support for Claude Opus 4.1 Model&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/danielmiessler/fabric/releases/tag/v1.4.271"&gt;v1.4.271&lt;/a&gt; (Jul 28, 2025) ‚Äî &lt;strong&gt;AI Summarized Release Notes&lt;/strong&gt;: Enable AI summary updates for GitHub releases&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/danielmiessler/fabric/releases/tag/v1.4.268"&gt;v1.4.268&lt;/a&gt; (Jul 26, 2025) ‚Äî &lt;strong&gt;Gemini TTS Voice Selection&lt;/strong&gt;: add Gemini TTS voice selection and listing functionality&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/danielmiessler/fabric/releases/tag/v1.4.267"&gt;v1.4.267&lt;/a&gt; (Jul 26, 2025) ‚Äî &lt;strong&gt;Text-to-Speech&lt;/strong&gt;: Update Gemini Plugin to New SDK with TTS Support&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/danielmiessler/fabric/releases/tag/v1.4.258"&gt;v1.4.258&lt;/a&gt; (Jul 17, 2025) ‚Äî &lt;strong&gt;Onboarding Improved&lt;/strong&gt;: Add startup check to initialize config and .env file automatically&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/danielmiessler/fabric/releases/tag/v1.4.257"&gt;v1.4.257&lt;/a&gt; (Jul 17, 2025) ‚Äî &lt;strong&gt;OpenAI Routing Control&lt;/strong&gt;: Introduce CLI Flag to Disable OpenAI Responses API&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/danielmiessler/fabric/releases/tag/v1.4.252"&gt;v1.4.252&lt;/a&gt; (Jul 16, 2025) ‚Äî &lt;strong&gt;Hide Thinking Block&lt;/strong&gt;: Optional Hiding of Model Thinking Process with Configurable Tags&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/danielmiessler/fabric/releases/tag/v1.4.246"&gt;v1.4.246&lt;/a&gt; (Jul 14, 2025) ‚Äî &lt;strong&gt;Automatic ChangeLog Updates&lt;/strong&gt;: Add AI-powered changelog generation with high-performance Go tool and comprehensive caching&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/danielmiessler/fabric/releases/tag/v1.4.245"&gt;v1.4.245&lt;/a&gt; (Jul 11, 2025) ‚Äî &lt;strong&gt;Together AI&lt;/strong&gt;: Together AI Support with OpenAI Fallback Mechanism Added&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/danielmiessler/fabric/releases/tag/v1.4.232"&gt;v1.4.232&lt;/a&gt; (Jul 6, 2025) ‚Äî &lt;strong&gt;Add Custom&lt;/strong&gt;: Add Custom Patterns Directory Support&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/danielmiessler/fabric/releases/tag/v1.4.231"&gt;v1.4.231&lt;/a&gt; (Jul 5, 2025) ‚Äî &lt;strong&gt;OAuth Auto-Auth&lt;/strong&gt;: OAuth Authentication Support for Anthropic (Use your Max Subscription)&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/danielmiessler/fabric/releases/tag/v1.4.230"&gt;v1.4.230&lt;/a&gt; (Jul 5, 2025) ‚Äî &lt;strong&gt;Model Management&lt;/strong&gt;: Add advanced image generation parameters for OpenAI models with four new CLI flags&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/danielmiessler/fabric/releases/tag/v1.4.227"&gt;v1.4.227&lt;/a&gt; (Jul 4, 2025) ‚Äî &lt;strong&gt;Add Image&lt;/strong&gt;: Add Image Generation Support to Fabric&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/danielmiessler/fabric/releases/tag/v1.4.226"&gt;v1.4.226&lt;/a&gt; (Jul 4, 2025) ‚Äî &lt;strong&gt;Web Search&lt;/strong&gt;: OpenAI Plugin Now Supports Web Search Functionality&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/danielmiessler/fabric/releases/tag/v1.4.225"&gt;v1.4.225&lt;/a&gt; (Jul 4, 2025) ‚Äî &lt;strong&gt;Web Search&lt;/strong&gt;: Runtime Web Search Control via Command-Line &lt;code&gt;--search&lt;/code&gt; Flag&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/danielmiessler/fabric/releases/tag/v1.4.224"&gt;v1.4.224&lt;/a&gt; (Jul 1, 2025) ‚Äî &lt;strong&gt;Add code_review&lt;/strong&gt;: Add code_review pattern and updates in Pattern_Descriptions&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/danielmiessler/fabric/releases/tag/v1.4.222"&gt;v1.4.222&lt;/a&gt; (Jul 1, 2025) ‚Äî &lt;strong&gt;OpenAI Plugin&lt;/strong&gt;: OpenAI Plugin Migrates to New Responses API&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/danielmiessler/fabric/releases/tag/v1.4.218"&gt;v1.4.218&lt;/a&gt; (Jun 27, 2025) ‚Äî &lt;strong&gt;Model Management&lt;/strong&gt;: Add Support for OpenAI Search and Research Model Variants&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/danielmiessler/fabric/releases/tag/v1.4.217"&gt;v1.4.217&lt;/a&gt; (Jun 26, 2025) ‚Äî &lt;strong&gt;New YouTube&lt;/strong&gt;: New YouTube Transcript Endpoint Added to REST API&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/danielmiessler/fabric/releases/tag/v1.4.212"&gt;v1.4.212&lt;/a&gt; (Jun 23, 2025) ‚Äî &lt;strong&gt;Add Langdock&lt;/strong&gt;: Add Langdock AI and enhance generic OpenAI compatible support&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/danielmiessler/fabric/releases/tag/v1.4.211"&gt;v1.4.211&lt;/a&gt; (Jun 19, 2025) ‚Äî &lt;strong&gt;REST API&lt;/strong&gt;: REST API and Web UI Now Support Dynamic Pattern Variables&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/danielmiessler/fabric/releases/tag/v1.4.210"&gt;v1.4.210&lt;/a&gt; (Jun 18, 2025) ‚Äî &lt;strong&gt;Add Citations&lt;/strong&gt;: Add Citation Support to Perplexity Response&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/danielmiessler/fabric/releases/tag/v1.4.208"&gt;v1.4.208&lt;/a&gt; (Jun 17, 2025) ‚Äî &lt;strong&gt;Add Perplexity&lt;/strong&gt;: Add Perplexity AI Provider with Token Limits Support&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/danielmiessler/fabric/releases/tag/v1.4.203"&gt;v1.4.203&lt;/a&gt; (Jun 14, 2025) ‚Äî &lt;strong&gt;Add Amazon Bedrock&lt;/strong&gt;: Add support for Amazon Bedrock&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;These features represent our commitment to making Fabric the most powerful and flexible AI augmentation framework available!&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;Intro videos&lt;/h2&gt; 
&lt;p&gt;Keep in mind that many of these were recorded when Fabric was Python-based, so remember to use the current &lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#installation"&gt;install instructions&lt;/a&gt; below.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=UbDyjIIGaxQ"&gt;Network Chuck&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=vF-MQmVxnCs"&gt;David Bombal&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=wPEyyigh10g"&gt;My Own Intro to the Tool&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/results?search_query=fabric+ai"&gt;More Fabric YouTube Videos&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Navigation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#fabric"&gt;&lt;code&gt;fabric&lt;/code&gt;&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#what-and-why"&gt;What and why&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#updates"&gt;Updates&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#recent-major-features"&gt;Recent Major Features&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#intro-videos"&gt;Intro videos&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#navigation"&gt;Navigation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#changelog"&gt;Changelog&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#philosophy"&gt;Philosophy&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#breaking-problems-into-components"&gt;Breaking problems into components&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#too-many-prompts"&gt;Too many prompts&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#installation"&gt;Installation&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#one-line-install-recommended"&gt;One-Line Install (Recommended)&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#manual-binary-downloads"&gt;Manual Binary Downloads&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#using-package-managers"&gt;Using package managers&lt;/a&gt; 
      &lt;ul&gt; 
       &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#macos-homebrew"&gt;macOS (Homebrew)&lt;/a&gt;&lt;/li&gt; 
       &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#arch-linux-aur"&gt;Arch Linux (AUR)&lt;/a&gt;&lt;/li&gt; 
       &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#windows"&gt;Windows&lt;/a&gt;&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#from-source"&gt;From Source&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#docker"&gt;Docker&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#environment-variables"&gt;Environment Variables&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#setup"&gt;Setup&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#per-pattern-model-mapping"&gt;Per-Pattern Model Mapping&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#add-aliases-for-all-patterns"&gt;Add aliases for all patterns&lt;/a&gt; 
      &lt;ul&gt; 
       &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#save-your-files-in-markdown-using-aliases"&gt;Save your files in markdown using aliases&lt;/a&gt;&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#migration"&gt;Migration&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#upgrading"&gt;Upgrading&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#shell-completions"&gt;Shell Completions&lt;/a&gt; 
      &lt;ul&gt; 
       &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#quick-install-no-clone-required"&gt;Quick install (no clone required)&lt;/a&gt;&lt;/li&gt; 
       &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#zsh-completion"&gt;Zsh Completion&lt;/a&gt;&lt;/li&gt; 
       &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#bash-completion"&gt;Bash Completion&lt;/a&gt;&lt;/li&gt; 
       &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#fish-completion"&gt;Fish Completion&lt;/a&gt;&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#usage"&gt;Usage&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#debug-levels"&gt;Debug Levels&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#extensions"&gt;Extensions&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#rest-api-server"&gt;REST API Server&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#our-approach-to-prompting"&gt;Our approach to prompting&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#examples"&gt;Examples&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#just-use-the-patterns"&gt;Just use the Patterns&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#prompt-strategies"&gt;Prompt Strategies&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#custom-patterns"&gt;Custom Patterns&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#setting-up-custom-patterns"&gt;Setting Up Custom Patterns&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#using-custom-patterns"&gt;Using Custom Patterns&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#how-it-works"&gt;How It Works&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#helper-apps"&gt;Helper Apps&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#to_pdf"&gt;&lt;code&gt;to_pdf&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#to_pdf-installation"&gt;&lt;code&gt;to_pdf&lt;/code&gt; Installation&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#code_helper"&gt;&lt;code&gt;code_helper&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#pbpaste"&gt;pbpaste&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#web-interface-fabric-web-app"&gt;Web Interface (Fabric Web App)&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#meta"&gt;Meta&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#primary-contributors"&gt;Primary contributors&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#contributors"&gt;Contributors&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;h2&gt;Changelog&lt;/h2&gt; 
&lt;p&gt;Fabric is evolving rapidly.&lt;/p&gt; 
&lt;p&gt;Stay current with the latest features by reviewing the &lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/CHANGELOG.md"&gt;CHANGELOG&lt;/a&gt; for all recent changes.&lt;/p&gt; 
&lt;h2&gt;Philosophy&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;AI isn't a thing; it's a &lt;em&gt;magnifier&lt;/em&gt; of a thing. And that thing is &lt;strong&gt;human creativity&lt;/strong&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;We believe the purpose of technology is to help humans flourish, so when we talk about AI we start with the &lt;strong&gt;human&lt;/strong&gt; problems we want to solve.&lt;/p&gt; 
&lt;h3&gt;Breaking problems into components&lt;/h3&gt; 
&lt;p&gt;Our approach is to break problems into individual pieces (see below) and then apply AI to them one at a time. See below for some examples.&lt;/p&gt; 
&lt;img width="2078" alt="augmented_challenges" src="https://github.com/danielmiessler/fabric/assets/50654/31997394-85a9-40c2-879b-b347e4701f06" /&gt; 
&lt;h3&gt;Too many prompts&lt;/h3&gt; 
&lt;p&gt;Prompts are good for this, but the biggest challenge I faced in 2023‚Äî‚Äîwhich still exists today‚Äîis &lt;strong&gt;the sheer number of AI prompts out there&lt;/strong&gt;. We all have prompts that are useful, but it's hard to discover new ones, know if they are good or not, &lt;em&gt;and manage different versions of the ones we like&lt;/em&gt;.&lt;/p&gt; 
&lt;p&gt;One of &lt;code&gt;fabric&lt;/code&gt;'s primary features is helping people collect and integrate prompts, which we call &lt;em&gt;Patterns&lt;/em&gt;, into various parts of their lives.&lt;/p&gt; 
&lt;p&gt;Fabric has Patterns for all sorts of life and work activities, including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Extracting the most interesting parts of YouTube videos and podcasts&lt;/li&gt; 
 &lt;li&gt;Writing an essay in your own voice with just an idea as an input&lt;/li&gt; 
 &lt;li&gt;Summarizing opaque academic papers&lt;/li&gt; 
 &lt;li&gt;Creating perfectly matched AI art prompts for a piece of writing&lt;/li&gt; 
 &lt;li&gt;Rating the quality of content to see if you want to read/watch the whole thing&lt;/li&gt; 
 &lt;li&gt;Getting summaries of long, boring content&lt;/li&gt; 
 &lt;li&gt;Explaining code to you&lt;/li&gt; 
 &lt;li&gt;Turning bad documentation into usable documentation&lt;/li&gt; 
 &lt;li&gt;Creating social media posts from any content input&lt;/li&gt; 
 &lt;li&gt;And a million more‚Ä¶&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;One-Line Install (Recommended)&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Unix/Linux/macOS:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -fsSL https://raw.githubusercontent.com/danielmiessler/fabric/main/scripts/installer/install.sh | bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Windows PowerShell:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;iwr -useb https://raw.githubusercontent.com/danielmiessler/fabric/main/scripts/installer/install.ps1 | iex
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/scripts/installer/README.md"&gt;scripts/installer/README.md&lt;/a&gt; for custom installation options and troubleshooting.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Manual Binary Downloads&lt;/h3&gt; 
&lt;p&gt;The latest release binary archives and their expected SHA256 hashes can be found at &lt;a href="https://github.com/danielmiessler/fabric/releases/latest"&gt;https://github.com/danielmiessler/fabric/releases/latest&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Using package managers&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; using Homebrew or the Arch Linux package managers makes &lt;code&gt;fabric&lt;/code&gt; available as &lt;code&gt;fabric-ai&lt;/code&gt;, so add the following alias to your shell startup files to account for this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;alias fabric='fabric-ai'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;macOS (Homebrew)&lt;/h4&gt; 
&lt;p&gt;&lt;code&gt;brew install fabric-ai&lt;/code&gt;&lt;/p&gt; 
&lt;h4&gt;Arch Linux (AUR)&lt;/h4&gt; 
&lt;p&gt;&lt;code&gt;yay -S fabric-ai&lt;/code&gt;&lt;/p&gt; 
&lt;h4&gt;Windows&lt;/h4&gt; 
&lt;p&gt;Use the official Microsoft supported &lt;code&gt;Winget&lt;/code&gt; tool:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;winget install danielmiessler.Fabric&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;From Source&lt;/h3&gt; 
&lt;p&gt;To install Fabric, &lt;a href="https://go.dev/doc/install"&gt;make sure Go is installed&lt;/a&gt;, and then run the following command.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install Fabric directly from the repo
go install github.com/danielmiessler/fabric/cmd/fabric@latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Docker&lt;/h3&gt; 
&lt;p&gt;Run Fabric using pre-built Docker images:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Use latest image from Docker Hub
docker run --rm -it kayvan/fabric:latest --version

# Use specific version from GHCR
docker run --rm -it ghcr.io/ksylvan/fabric:v1.4.305 --version

# Run setup (first time)
mkdir -p $HOME/.fabric-config
docker run --rm -it -v $HOME/.fabric-config:/root/.config/fabric kayvan/fabric:latest --setup

# Use Fabric with your patterns
docker run --rm -it -v $HOME/.fabric-config:/root/.config/fabric kayvan/fabric:latest -p summarize

# Run the REST API server (see REST API Server section)
docker run --rm -it -p 8080:8080 -v $HOME/.fabric-config:/root/.config/fabric kayvan/fabric:latest --serve
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Images available at:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Docker Hub: &lt;a href="https://hub.docker.com/repository/docker/kayvan/fabric/general"&gt;kayvan/fabric&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;GHCR: &lt;a href="https://github.com/ksylvan/fabric/pkgs/container/fabric"&gt;ksylvan/fabric&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/scripts/docker/README.md"&gt;scripts/docker/README.md&lt;/a&gt; for building custom images and advanced configuration.&lt;/p&gt; 
&lt;h3&gt;Environment Variables&lt;/h3&gt; 
&lt;p&gt;You may need to set some environment variables in your &lt;code&gt;~/.bashrc&lt;/code&gt; on linux or &lt;code&gt;~/.zshrc&lt;/code&gt; file on mac to be able to run the &lt;code&gt;fabric&lt;/code&gt; command. Here is an example of what you can add:&lt;/p&gt; 
&lt;p&gt;For Intel based macs or linux&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Golang environment variables
export GOROOT=/usr/local/go
export GOPATH=$HOME/go

# Update PATH to include GOPATH and GOROOT binaries
export PATH=$GOPATH/bin:$GOROOT/bin:$HOME/.local/bin:$PATH
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;for Apple Silicon based macs&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Golang environment variables
export GOROOT=$(brew --prefix go)/libexec
export GOPATH=$HOME/go
export PATH=$GOPATH/bin:$GOROOT/bin:$HOME/.local/bin:$PATH
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Setup&lt;/h3&gt; 
&lt;p&gt;Now run the following command&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run the setup to set up your directories and keys
fabric --setup
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If everything works you are good to go.&lt;/p&gt; 
&lt;h3&gt;Per-Pattern Model Mapping&lt;/h3&gt; 
&lt;p&gt;You can configure specific models for individual patterns using environment variables like &lt;code&gt;FABRIC_MODEL_PATTERN_NAME=vendor|model&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;This makes it easy to maintain these per-pattern model mappings in your shell startup files.&lt;/p&gt; 
&lt;h3&gt;Add aliases for all patterns&lt;/h3&gt; 
&lt;p&gt;In order to add aliases for all your patterns and use them directly as commands, for example, &lt;code&gt;summarize&lt;/code&gt; instead of &lt;code&gt;fabric --pattern summarize&lt;/code&gt; You can add the following to your &lt;code&gt;.zshrc&lt;/code&gt; or &lt;code&gt;.bashrc&lt;/code&gt; file. You can also optionally set the &lt;code&gt;FABRIC_ALIAS_PREFIX&lt;/code&gt; environment variable before, if you'd prefer all the fabric aliases to start with the same prefix.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Loop through all files in the ~/.config/fabric/patterns directory
for pattern_file in $HOME/.config/fabric/patterns/*; do
    # Get the base name of the file (i.e., remove the directory path)
    pattern_name="$(basename "$pattern_file")"
    alias_name="${FABRIC_ALIAS_PREFIX:-}${pattern_name}"

    # Create an alias in the form: alias pattern_name="fabric --pattern pattern_name"
    alias_command="alias $alias_name='fabric --pattern $pattern_name'"

    # Evaluate the alias command to add it to the current shell
    eval "$alias_command"
done

yt() {
    if [ "$#" -eq 0 ] || [ "$#" -gt 2 ]; then
        echo "Usage: yt [-t | --timestamps] youtube-link"
        echo "Use the '-t' flag to get the transcript with timestamps."
        return 1
    fi

    transcript_flag="--transcript"
    if [ "$1" = "-t" ] || [ "$1" = "--timestamps" ]; then
        transcript_flag="--transcript-with-timestamps"
        shift
    fi
    local video_link="$1"
    fabric -y "$video_link" $transcript_flag
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can add the below code for the equivalent aliases inside PowerShell by running &lt;code&gt;notepad $PROFILE&lt;/code&gt; inside a PowerShell window:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;# Path to the patterns directory
$patternsPath = Join-Path $HOME ".config/fabric/patterns"
foreach ($patternDir in Get-ChildItem -Path $patternsPath -Directory) {
    # Prepend FABRIC_ALIAS_PREFIX if set; otherwise use empty string
    $prefix = $env:FABRIC_ALIAS_PREFIX ?? ''
    $patternName = "$($patternDir.Name)"
    $aliasName = "$prefix$patternName"
    # Dynamically define a function for each pattern
    $functionDefinition = @"
function $aliasName {
    [CmdletBinding()]
    param(
        [Parameter(ValueFromPipeline = `$true)]
        [string] `$InputObject,

        [Parameter(ValueFromRemainingArguments = `$true)]
        [String[]] `$patternArgs
    )

    begin {
        # Initialize an array to collect pipeline input
        `$collector = @()
    }

    process {
        # Collect pipeline input objects
        if (`$InputObject) {
            `$collector += `$InputObject
        }
    }

    end {
        # Join all pipeline input into a single string, separated by newlines
        `$pipelineContent = `$collector -join "`n"

        # If there's pipeline input, include it in the call to fabric
        if (`$pipelineContent) {
            `$pipelineContent | fabric --pattern $patternName `$patternArgs
        } else {
            # No pipeline input; just call fabric with the additional args
            fabric --pattern $patternName `$patternArgs
        }
    }
}
"@
    # Add the function to the current session
    Invoke-Expression $functionDefinition
}

# Define the 'yt' function as well
function yt {
    [CmdletBinding()]
    param(
        [Parameter()]
        [Alias("timestamps")]
        [switch]$t,

        [Parameter(Position = 0, ValueFromPipeline = $true)]
        [string]$videoLink
    )

    begin {
        $transcriptFlag = "--transcript"
        if ($t) {
            $transcriptFlag = "--transcript-with-timestamps"
        }
    }

    process {
        if (-not $videoLink) {
            Write-Error "Usage: yt [-t | --timestamps] youtube-link"
            return
        }
    }

    end {
        if ($videoLink) {
            # Execute and allow output to flow through the pipeline
            fabric -y $videoLink $transcriptFlag
        }
    }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This also creates a &lt;code&gt;yt&lt;/code&gt; alias that allows you to use &lt;code&gt;yt https://www.youtube.com/watch?v=4b0iet22VIk&lt;/code&gt; to get transcripts, comments, and metadata.&lt;/p&gt; 
&lt;h4&gt;Save your files in markdown using aliases&lt;/h4&gt; 
&lt;p&gt;If in addition to the above aliases you would like to have the option to save the output to your favorite markdown note vault like Obsidian then instead of the above add the following to your &lt;code&gt;.zshrc&lt;/code&gt; or &lt;code&gt;.bashrc&lt;/code&gt; file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Define the base directory for Obsidian notes
obsidian_base="/path/to/obsidian"

# Loop through all files in the ~/.config/fabric/patterns directory
for pattern_file in ~/.config/fabric/patterns/*; do
    # Get the base name of the file (i.e., remove the directory path)
    pattern_name=$(basename "$pattern_file")

    # Remove any existing alias with the same name
    unalias "$pattern_name" 2&amp;gt;/dev/null

    # Define a function dynamically for each pattern
    eval "
    $pattern_name() {
        local title=\$1
        local date_stamp=\$(date +'%Y-%m-%d')
        local output_path=\"\$obsidian_base/\${date_stamp}-\${title}.md\"

        # Check if a title was provided
        if [ -n \"\$title\" ]; then
            # If a title is provided, use the output path
            fabric --pattern \"$pattern_name\" -o \"\$output_path\"
        else
            # If no title is provided, use --stream
            fabric --pattern \"$pattern_name\" --stream
        fi
    }
    "
done
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will allow you to use the patterns as aliases like in the above for example &lt;code&gt;summarize&lt;/code&gt; instead of &lt;code&gt;fabric --pattern summarize --stream&lt;/code&gt;, however if you pass in an extra argument like this &lt;code&gt;summarize "my_article_title"&lt;/code&gt; your output will be saved in the destination that you set in &lt;code&gt;obsidian_base="/path/to/obsidian"&lt;/code&gt; in the following format &lt;code&gt;YYYY-MM-DD-my_article_title.md&lt;/code&gt; where the date gets autogenerated for you. You can tweak the date format by tweaking the &lt;code&gt;date_stamp&lt;/code&gt; format.&lt;/p&gt; 
&lt;h3&gt;Migration&lt;/h3&gt; 
&lt;p&gt;If you have the Legacy (Python) version installed and want to migrate to the Go version, here's how you do it. It's basically two steps: 1) uninstall the Python version, and 2) install the Go version.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Uninstall Legacy Fabric
pipx uninstall fabric

# Clear any old Fabric aliases
(check your .bashrc, .zshrc, etc.)
# Install the Go version
go install github.com/danielmiessler/fabric/cmd/fabric@latest
# Run setup for the new version. Important because things have changed
fabric --setup
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then &lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#environment-variables"&gt;set your environmental variables&lt;/a&gt; as shown above.&lt;/p&gt; 
&lt;h3&gt;Upgrading&lt;/h3&gt; 
&lt;p&gt;The great thing about Go is that it's super easy to upgrade. Just run the same command you used to install it in the first place and you'll always get the latest version.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;go install github.com/danielmiessler/fabric/cmd/fabric@latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Shell Completions&lt;/h3&gt; 
&lt;p&gt;Fabric provides shell completion scripts for Zsh, Bash, and Fish shells, making it easier to use the CLI by providing tab completion for commands and options.&lt;/p&gt; 
&lt;h4&gt;Quick install (no clone required)&lt;/h4&gt; 
&lt;p&gt;You can install completions directly via a one-liner:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -fsSL https://raw.githubusercontent.com/danielmiessler/Fabric/refs/heads/main/completions/setup-completions.sh | sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Optional variants:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Dry-run (see actions without changing your system)
curl -fsSL https://raw.githubusercontent.com/danielmiessler/Fabric/refs/heads/main/completions/setup-completions.sh | sh -s -- --dry-run

# Override the download source (advanced)
FABRIC_COMPLETIONS_BASE_URL="https://raw.githubusercontent.com/danielmiessler/Fabric/refs/heads/main/completions" \
    sh -c "$(curl -fsSL https://raw.githubusercontent.com/danielmiessler/Fabric/refs/heads/main/completions/setup-completions.sh)"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Zsh Completion&lt;/h4&gt; 
&lt;p&gt;To enable Zsh completion:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Copy the completion file to a directory in your $fpath
mkdir -p ~/.zsh/completions
cp completions/_fabric ~/.zsh/completions/

# Add the directory to fpath in your .zshrc before compinit
echo 'fpath=(~/.zsh/completions $fpath)' &amp;gt;&amp;gt; ~/.zshrc
echo 'autoload -Uz compinit &amp;amp;&amp;amp; compinit' &amp;gt;&amp;gt; ~/.zshrc
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Bash Completion&lt;/h4&gt; 
&lt;p&gt;To enable Bash completion:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Source the completion script in your .bashrc
echo 'source /path/to/fabric/completions/fabric.bash' &amp;gt;&amp;gt; ~/.bashrc

# Or copy to the system-wide bash completion directory
sudo cp completions/fabric.bash /etc/bash_completion.d/
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Fish Completion&lt;/h4&gt; 
&lt;p&gt;To enable Fish completion:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Copy the completion file to the fish completions directory
mkdir -p ~/.config/fish/completions
cp completions/fabric.fish ~/.config/fish/completions/
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;p&gt;Once you have it all set up, here's how to use it.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;fabric -h
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-plaintext"&gt;Usage:
  fabric [OPTIONS]

Application Options:
  -p, --pattern=                    Choose a pattern from the available patterns
  -v, --variable=                   Values for pattern variables, e.g. -v=#role:expert -v=#points:30
  -C, --context=                    Choose a context from the available contexts
      --session=                    Choose a session from the available sessions
  -a, --attachment=                 Attachment path or URL (e.g. for OpenAI image recognition messages)
  -S, --setup                       Run setup for all reconfigurable parts of fabric
  -t, --temperature=                Set temperature (default: 0.7)
  -T, --topp=                       Set top P (default: 0.9)
  -s, --stream                      Stream
  -P, --presencepenalty=            Set presence penalty (default: 0.0)
  -r, --raw                         Use the defaults of the model without sending chat options
                                    (temperature, top_p, etc.). Only affects OpenAI-compatible providers.
                                    Anthropic models always use smart parameter selection to comply with
                                    model-specific requirements.
  -F, --frequencypenalty=           Set frequency penalty (default: 0.0)
  -l, --listpatterns                List all patterns
  -L, --listmodels                  List all available models
  -x, --listcontexts                List all contexts
  -X, --listsessions                List all sessions
  -U, --updatepatterns              Update patterns
  -c, --copy                        Copy to clipboard
  -m, --model=                      Choose model
  -V, --vendor=                     Specify vendor for chosen model (e.g., -V "LM Studio" -m openai/gpt-oss-20b)
      --modelContextLength=         Model context length (only affects ollama)
  -o, --output=                     Output to file
      --output-session              Output the entire session (also a temporary one) to the output file
  -n, --latest=                     Number of latest patterns to list (default: 0)
  -d, --changeDefaultModel          Change default model
  -y, --youtube=                    YouTube video or play list "URL" to grab transcript, comments from it
                                    and send to chat or print it put to the console and store it in the
                                    output file
      --playlist                    Prefer playlist over video if both ids are present in the URL
      --transcript                  Grab transcript from YouTube video and send to chat (it is used per
                                    default).
      --transcript-with-timestamps  Grab transcript from YouTube video with timestamps and send to chat
      --comments                    Grab comments from YouTube video and send to chat
      --metadata                    Output video metadata
  -g, --language=                   Specify the Language Code for the chat, e.g. -g=en -g=zh
  -u, --scrape_url=                 Scrape website URL to markdown using Jina AI
  -q, --scrape_question=            Search question using Jina AI
  -e, --seed=                       Seed to be used for LMM generation
  -w, --wipecontext=                Wipe context
  -W, --wipesession=                Wipe session
      --printcontext=               Print context
      --printsession=               Print session
      --readability                 Convert HTML input into a clean, readable view
      --input-has-vars              Apply variables to user input
      --no-variable-replacement     Disable pattern variable replacement
      --dry-run                     Show what would be sent to the model without actually sending it
      --serve                       Serve the Fabric Rest API
      --serveOllama                 Serve the Fabric Rest API with ollama endpoints
      --address=                    The address to bind the REST API (default: :8080)
      --api-key=                    API key used to secure server routes
      --config=                     Path to YAML config file
      --version                     Print current version
      --listextensions              List all registered extensions
      --addextension=               Register a new extension from config file path
      --rmextension=                Remove a registered extension by name
      --strategy=                   Choose a strategy from the available strategies
      --liststrategies              List all strategies
      --listvendors                 List all vendors
      --shell-complete-list         Output raw list without headers/formatting (for shell completion)
      --search                      Enable web search tool for supported models (Anthropic, OpenAI, Gemini)
      --search-location=            Set location for web search results (e.g., 'America/Los_Angeles')
      --image-file=                 Save generated image to specified file path (e.g., 'output.png')
      --image-size=                 Image dimensions: 1024x1024, 1536x1024, 1024x1536, auto (default: auto)
      --image-quality=              Image quality: low, medium, high, auto (default: auto)
      --image-compression=          Compression level 0-100 for JPEG/WebP formats (default: not set)
      --image-background=           Background type: opaque, transparent (default: opaque, only for
                                    PNG/WebP)
      --suppress-think              Suppress text enclosed in thinking tags
      --think-start-tag=            Start tag for thinking sections (default: &amp;lt;think&amp;gt;)
      --think-end-tag=              End tag for thinking sections (default: &amp;lt;/think&amp;gt;)
      --disable-responses-api       Disable OpenAI Responses API (default: false)
      --voice=                      TTS voice name for supported models (e.g., Kore, Charon, Puck)
                                    (default: Kore)
      --list-gemini-voices          List all available Gemini TTS voices
      --notification                Send desktop notification when command completes
      --notification-command=       Custom command to run for notifications (overrides built-in
                                    notifications)
      --yt-dlp-args=                Additional arguments to pass to yt-dlp (e.g. '--cookies-from-browser brave')
      --thinking=                   Set reasoning/thinking level (e.g., off, low, medium, high, or
                                    numeric tokens for Anthropic or Google Gemini)
      --debug=                     Set debug level (0: off, 1: basic, 2: detailed, 3: trace)
Help Options:
  -h, --help                        Show this help message
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Debug Levels&lt;/h3&gt; 
&lt;p&gt;Use the &lt;code&gt;--debug&lt;/code&gt; flag to control runtime logging:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;0&lt;/code&gt;: off (default)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;1&lt;/code&gt;: basic debug info&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;2&lt;/code&gt;: detailed debugging&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;3&lt;/code&gt;: trace level&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Extensions&lt;/h3&gt; 
&lt;p&gt;Fabric supports extensions that can be called within patterns. See the &lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/internal/plugins/template/Examples/README.md"&gt;Extension Guide&lt;/a&gt; for complete documentation.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Important:&lt;/strong&gt; Extensions only work within pattern files, not via direct stdin. See the guide for details and examples.&lt;/p&gt; 
&lt;h2&gt;REST API Server&lt;/h2&gt; 
&lt;p&gt;Fabric includes a built-in REST API server that exposes all core functionality over HTTP. Start the server with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;fabric --serve
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The server provides endpoints for:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Chat completions with streaming responses&lt;/li&gt; 
 &lt;li&gt;Pattern management (create, read, update, delete)&lt;/li&gt; 
 &lt;li&gt;Context and session management&lt;/li&gt; 
 &lt;li&gt;Model and vendor listing&lt;/li&gt; 
 &lt;li&gt;YouTube transcript extraction&lt;/li&gt; 
 &lt;li&gt;Configuration management&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For complete endpoint documentation, authentication setup, and usage examples, see &lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/docs/rest-api.md"&gt;REST API Documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Our approach to prompting&lt;/h2&gt; 
&lt;p&gt;Fabric &lt;em&gt;Patterns&lt;/em&gt; are different than most prompts you'll see.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;First, we use &lt;code&gt;Markdown&lt;/code&gt; to help ensure maximum readability and editability&lt;/strong&gt;. This not only helps the creator make a good one, but also anyone who wants to deeply understand what it does. &lt;em&gt;Importantly, this also includes the AI you're sending it to!&lt;/em&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Here's an example of a Fabric Pattern.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;https://github.com/danielmiessler/Fabric/blob/main/data/patterns/extract_wisdom/system.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;img width="1461" alt="pattern-example" src="https://github.com/danielmiessler/fabric/assets/50654/b910c551-9263-405f-9735-71ca69bbab6d" /&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Next, we are extremely clear in our instructions&lt;/strong&gt;, and we use the Markdown structure to emphasize what we want the AI to do, and in what order.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;And finally, we tend to use the System section of the prompt almost exclusively&lt;/strong&gt;. In over a year of being heads-down with this stuff, we've just seen more efficacy from doing that. If that changes, or we're shown data that says otherwise, we will adjust.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;The following examples use the macOS &lt;code&gt;pbpaste&lt;/code&gt; to paste from the clipboard. See the &lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#pbpaste"&gt;pbpaste&lt;/a&gt; section below for Windows and Linux alternatives.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Now let's look at some things you can do with Fabric.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Run the &lt;code&gt;summarize&lt;/code&gt; Pattern based on input from &lt;code&gt;stdin&lt;/code&gt;. In this case, the body of an article.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pbpaste | fabric --pattern summarize
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run the &lt;code&gt;analyze_claims&lt;/code&gt; Pattern with the &lt;code&gt;--stream&lt;/code&gt; option to get immediate and streaming results.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pbpaste | fabric --stream --pattern analyze_claims
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run the &lt;code&gt;extract_wisdom&lt;/code&gt; Pattern with the &lt;code&gt;--stream&lt;/code&gt; option to get immediate and streaming results from any Youtube video (much like in the original introduction video).&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;fabric -y "https://youtube.com/watch?v=uXs-zPc63kM" --stream --pattern extract_wisdom
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Create patterns- you must create a .md file with the pattern and save it to &lt;code&gt;~/.config/fabric/patterns/[yourpatternname]&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run a &lt;code&gt;analyze_claims&lt;/code&gt; pattern on a website. Fabric uses Jina AI to scrape the URL into markdown format before sending it to the model.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;fabric -u https://github.com/danielmiessler/fabric/ -p analyze_claims
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Just use the Patterns&lt;/h2&gt; 
&lt;img width="1173" alt="fabric-patterns-screenshot" src="https://github.com/danielmiessler/fabric/assets/50654/9186a044-652b-4673-89f7-71cf066f32d8" /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;p&gt;If you're not looking to do anything fancy, and you just want a lot of great prompts, you can navigate to the &lt;a href="https://github.com/danielmiessler/fabric/tree/main/data/patterns"&gt;&lt;code&gt;/patterns&lt;/code&gt;&lt;/a&gt; directory and start exploring!&lt;/p&gt; 
&lt;p&gt;We hope that if you used nothing else from Fabric, the Patterns by themselves will make the project useful.&lt;/p&gt; 
&lt;p&gt;You can use any of the Patterns you see there in any AI application that you have, whether that's ChatGPT or some other app or website. Our plan and prediction is that people will soon be sharing many more than those we've published, and they will be way better than ours.&lt;/p&gt; 
&lt;p&gt;The wisdom of crowds for the win.&lt;/p&gt; 
&lt;h3&gt;Prompt Strategies&lt;/h3&gt; 
&lt;p&gt;Fabric also implements prompt strategies like "Chain of Thought" or "Chain of Draft" which can be used in addition to the basic patterns.&lt;/p&gt; 
&lt;p&gt;See the &lt;a href="https://arxiv.org/pdf/2502.18600"&gt;Thinking Faster by Writing Less&lt;/a&gt; paper and the &lt;a href="https://learnprompting.org/docs/advanced/thought_generation/introduction"&gt;Thought Generation section of Learn Prompting&lt;/a&gt; for examples of prompt strategies.&lt;/p&gt; 
&lt;p&gt;Each strategy is available as a small &lt;code&gt;json&lt;/code&gt; file in the &lt;a href="https://github.com/danielmiessler/fabric/tree/main/data/strategies"&gt;&lt;code&gt;/strategies&lt;/code&gt;&lt;/a&gt; directory.&lt;/p&gt; 
&lt;p&gt;The prompt modification of the strategy is applied to the system prompt and passed on to the LLM in the chat session.&lt;/p&gt; 
&lt;p&gt;Use &lt;code&gt;fabric -S&lt;/code&gt; and select the option to install the strategies in your &lt;code&gt;~/.config/fabric&lt;/code&gt; directory.&lt;/p&gt; 
&lt;h2&gt;Custom Patterns&lt;/h2&gt; 
&lt;p&gt;You may want to use Fabric to create your own custom Patterns‚Äîbut not share them with others. No problem!&lt;/p&gt; 
&lt;p&gt;Fabric now supports a dedicated custom patterns directory that keeps your personal patterns separate from the built-in ones. This means your custom patterns won't be overwritten when you update Fabric's built-in patterns.&lt;/p&gt; 
&lt;h3&gt;Setting Up Custom Patterns&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Run the Fabric setup:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;fabric --setup
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Select the "Custom Patterns" option from the Tools menu and enter your desired directory path (e.g., &lt;code&gt;~/my-custom-patterns&lt;/code&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Fabric will automatically create the directory if it does not exist.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Using Custom Patterns&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Create your custom pattern directory structure:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;mkdir -p ~/my-custom-patterns/my-analyzer
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Create your pattern file&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;echo "You are an expert analyzer of ..." &amp;gt; ~/my-custom-patterns/my-analyzer/system.md
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Use your custom pattern:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;fabric --pattern my-analyzer "analyze this text"
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;How It Works&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Priority System&lt;/strong&gt;: Custom patterns take precedence over built-in patterns with the same name&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Seamless Integration&lt;/strong&gt;: Custom patterns appear in &lt;code&gt;fabric --listpatterns&lt;/code&gt; alongside built-in ones&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Update Safe&lt;/strong&gt;: Your custom patterns are never affected by &lt;code&gt;fabric --updatepatterns&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Private by Default&lt;/strong&gt;: Custom patterns remain private unless you explicitly share them&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Your custom patterns are completely private and won't be affected by Fabric updates!&lt;/p&gt; 
&lt;h2&gt;Helper Apps&lt;/h2&gt; 
&lt;p&gt;Fabric also makes use of some core helper apps (tools) to make it easier to integrate with your various workflows. Here are some examples:&lt;/p&gt; 
&lt;h3&gt;&lt;code&gt;to_pdf&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;to_pdf&lt;/code&gt; is a helper command that converts LaTeX files to PDF format. You can use it like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;to_pdf input.tex
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will create a PDF file from the input LaTeX file in the same directory.&lt;/p&gt; 
&lt;p&gt;You can also use it with stdin which works perfectly with the &lt;code&gt;write_latex&lt;/code&gt; pattern:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;echo "ai security primer" | fabric --pattern write_latex | to_pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will create a PDF file named &lt;code&gt;output.pdf&lt;/code&gt; in the current directory.&lt;/p&gt; 
&lt;h3&gt;&lt;code&gt;to_pdf&lt;/code&gt; Installation&lt;/h3&gt; 
&lt;p&gt;To install &lt;code&gt;to_pdf&lt;/code&gt;, install it the same way as you install Fabric, just with a different repo name.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;go install github.com/danielmiessler/fabric/cmd/to_pdf@latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Make sure you have a LaTeX distribution (like TeX Live or MiKTeX) installed on your system, as &lt;code&gt;to_pdf&lt;/code&gt; requires &lt;code&gt;pdflatex&lt;/code&gt; to be available in your system's PATH.&lt;/p&gt; 
&lt;h3&gt;&lt;code&gt;code_helper&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;code_helper&lt;/code&gt; is used in conjunction with the &lt;code&gt;create_coding_feature&lt;/code&gt; pattern. It generates a &lt;code&gt;json&lt;/code&gt; representation of a directory of code that can be fed into an AI model with instructions to create a new feature or edit the code in a specified way.&lt;/p&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/data/patterns/create_coding_feature/README.md"&gt;the Create Coding Feature Pattern README&lt;/a&gt; for details.&lt;/p&gt; 
&lt;p&gt;Install it first using:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;go install github.com/danielmiessler/fabric/cmd/code_helper@latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;pbpaste&lt;/h2&gt; 
&lt;p&gt;The &lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/#examples"&gt;examples&lt;/a&gt; use the macOS program &lt;code&gt;pbpaste&lt;/code&gt; to paste content from the clipboard to pipe into &lt;code&gt;fabric&lt;/code&gt; as the input. &lt;code&gt;pbpaste&lt;/code&gt; is not available on Windows or Linux, but there are alternatives.&lt;/p&gt; 
&lt;p&gt;On Windows, you can use the PowerShell command &lt;code&gt;Get-Clipboard&lt;/code&gt; from a PowerShell command prompt. If you like, you can also alias it to &lt;code&gt;pbpaste&lt;/code&gt;. If you are using classic PowerShell, edit the file &lt;code&gt;~\Documents\WindowsPowerShell\.profile.ps1&lt;/code&gt;, or if you are using PowerShell Core, edit &lt;code&gt;~\Documents\PowerShell\.profile.ps1&lt;/code&gt; and add the alias,&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;Set-Alias pbpaste Get-Clipboard
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;On Linux, you can use &lt;code&gt;xclip -selection clipboard -o&lt;/code&gt; to paste from the clipboard. You will likely need to install &lt;code&gt;xclip&lt;/code&gt; with your package manager. For Debian based systems including Ubuntu,&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;sudo apt update
sudo apt install xclip -y
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also create an alias by editing &lt;code&gt;~/.bashrc&lt;/code&gt; or &lt;code&gt;~/.zshrc&lt;/code&gt; and adding the alias,&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;alias pbpaste='xclip -selection clipboard -o'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Web Interface (Fabric Web App)&lt;/h2&gt; 
&lt;p&gt;Fabric now includes a built-in web interface that provides a GUI alternative to the command-line interface. Refer to &lt;a href="https://raw.githubusercontent.com/danielmiessler/Fabric/main/web/README.md"&gt;Web App README&lt;/a&gt; for installation instructions and an overview of features.&lt;/p&gt; 
&lt;h2&gt;Meta&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Special thanks to the following people for their inspiration and contributions!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;em&gt;Jonathan Dunn&lt;/em&gt; for being the absolute MVP dev on the project, including spearheading the new Go version, as well as the GUI! All this while also being a full-time medical doctor!&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;Caleb Sima&lt;/em&gt; for pushing me over the edge of whether to make this a public project or not.&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;Eugen Eisler&lt;/em&gt; and &lt;em&gt;Frederick Ros&lt;/em&gt; for their invaluable contributions to the Go version&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;David Peters&lt;/em&gt; for his work on the web interface.&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;Joel Parish&lt;/em&gt; for super useful input on the project's Github directory structure..&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;Joseph Thacker&lt;/em&gt; for the idea of a &lt;code&gt;-c&lt;/code&gt; context flag that adds pre-created context in the &lt;code&gt;./config/fabric/&lt;/code&gt; directory to all Pattern queries.&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;Jason Haddix&lt;/em&gt; for the idea of a stitch (chained Pattern) to filter content using a local model before sending on to a cloud model, i.e., cleaning customer data using &lt;code&gt;llama2&lt;/code&gt; before sending on to &lt;code&gt;gpt-4&lt;/code&gt; for analysis.&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;Andre Guerra&lt;/em&gt; for assisting with numerous components to make things simpler and more maintainable.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Primary contributors&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/danielmiessler"&gt;&lt;img src="https://avatars.githubusercontent.com/u/50654?v=4" title="Daniel Miessler" width="50" height="50" alt="Daniel Miessler" /&gt;&lt;/a&gt; &lt;a href="https://github.com/xssdoctor"&gt;&lt;img src="https://avatars.githubusercontent.com/u/9218431?v=4" title="Jonathan Dunn" width="50" height="50" alt="Jonathan Dunn" /&gt;&lt;/a&gt; &lt;a href="https://github.com/sbehrens"&gt;&lt;img src="https://avatars.githubusercontent.com/u/688589?v=4" title="Scott Behrens" width="50" height="50" alt="Scott Behrens" /&gt;&lt;/a&gt; &lt;a href="https://github.com/agu3rra"&gt;&lt;img src="https://avatars.githubusercontent.com/u/10410523?v=4" title="Andre Guerra" width="50" height="50" alt="Andre Guerra" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Contributors&lt;/h3&gt; 
&lt;a href="https://github.com/danielmiessler/fabric/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=danielmiessler/fabric" alt="contrib.rocks" /&gt; &lt;/a&gt; 
&lt;p&gt;Made with &lt;a href="https://contrib.rocks"&gt;contrib.rocks&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;fabric&lt;/code&gt; was created by &lt;a href="https://danielmiessler.com/subscribe" target="_blank"&gt;Daniel Miessler&lt;/a&gt; in January of 2024. &lt;br /&gt;&lt;br /&gt; &lt;a href="https://twitter.com/intent/user?screen_name=danielmiessler"&gt;&lt;img src="https://img.shields.io/twitter/follow/danielmiessler" alt="X (formerly Twitter) Follow" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>google/langextract</title>
      <link>https://github.com/google/langextract</link>
      <description>&lt;p&gt;A Python library for extracting structured information from unstructured text using LLMs with precise source grounding and interactive visualization.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://github.com/google/langextract"&gt; &lt;img src="https://raw.githubusercontent.com/google/langextract/main/docs/_static/logo.svg?sanitize=true" alt="LangExtract Logo" width="128" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h1&gt;LangExtract&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://pypi.org/project/langextract/"&gt;&lt;img src="https://img.shields.io/pypi/v/langextract.svg?sanitize=true" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://github.com/google/langextract"&gt;&lt;img src="https://img.shields.io/github/stars/google/langextract.svg?style=social&amp;amp;label=Star" alt="GitHub stars" /&gt;&lt;/a&gt; &lt;img src="https://github.com/google/langextract/actions/workflows/ci.yaml/badge.svg?sanitize=true" alt="Tests" /&gt; &lt;a href="https://doi.org/10.5281/zenodo.17015089"&gt;&lt;img src="https://zenodo.org/badge/DOI/10.5281/zenodo.17015089.svg?sanitize=true" alt="DOI" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#why-langextract"&gt;Why LangExtract?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#quick-start"&gt;Quick Start&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#api-key-setup-for-cloud-models"&gt;API Key Setup for Cloud Models&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#adding-custom-model-providers"&gt;Adding Custom Model Providers&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#using-openai-models"&gt;Using OpenAI Models&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#using-local-llms-with-ollama"&gt;Using Local LLMs with Ollama&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#more-examples"&gt;More Examples&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#romeo-and-juliet-full-text-extraction"&gt;&lt;em&gt;Romeo and Juliet&lt;/em&gt; Full Text Extraction&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#medication-extraction"&gt;Medication Extraction&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#radiology-report-structuring-radextract"&gt;Radiology Report Structuring: RadExtract&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#community-providers"&gt;Community Providers&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#testing"&gt;Testing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#disclaimer"&gt;Disclaimer&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;LangExtract is a Python library that uses LLMs to extract structured information from unstructured text documents based on user-defined instructions. It processes materials such as clinical notes or reports, identifying and organizing key details while ensuring the extracted data corresponds to the source text.&lt;/p&gt; 
&lt;h2&gt;Why LangExtract?&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Precise Source Grounding:&lt;/strong&gt; Maps every extraction to its exact location in the source text, enabling visual highlighting for easy traceability and verification.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Reliable Structured Outputs:&lt;/strong&gt; Enforces a consistent output schema based on your few-shot examples, leveraging controlled generation in supported models like Gemini to guarantee robust, structured results.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Optimized for Long Documents:&lt;/strong&gt; Overcomes the "needle-in-a-haystack" challenge of large document extraction by using an optimized strategy of text chunking, parallel processing, and multiple passes for higher recall.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Interactive Visualization:&lt;/strong&gt; Instantly generates a self-contained, interactive HTML file to visualize and review thousands of extracted entities in their original context.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible LLM Support:&lt;/strong&gt; Supports your preferred models, from cloud-based LLMs like the Google Gemini family to local open-source models via the built-in Ollama interface.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Adaptable to Any Domain:&lt;/strong&gt; Define extraction tasks for any domain using just a few examples. LangExtract adapts to your needs without requiring any model fine-tuning.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Leverages LLM World Knowledge:&lt;/strong&gt; Utilize precise prompt wording and few-shot examples to influence how the extraction task may utilize LLM knowledge. The accuracy of any inferred information and its adherence to the task specification are contingent upon the selected LLM, the complexity of the task, the clarity of the prompt instructions, and the nature of the prompt examples.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Using cloud-hosted models like Gemini requires an API key. See the &lt;a href="https://raw.githubusercontent.com/google/langextract/main/#api-key-setup-for-cloud-models"&gt;API Key Setup&lt;/a&gt; section for instructions on how to get and configure your key.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Extract structured information with just a few lines of code.&lt;/p&gt; 
&lt;h3&gt;1. Define Your Extraction Task&lt;/h3&gt; 
&lt;p&gt;First, create a prompt that clearly describes what you want to extract. Then, provide a high-quality example to guide the model.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import langextract as lx
import textwrap

# 1. Define the prompt and extraction rules
prompt = textwrap.dedent("""\
    Extract characters, emotions, and relationships in order of appearance.
    Use exact text for extractions. Do not paraphrase or overlap entities.
    Provide meaningful attributes for each entity to add context.""")

# 2. Provide a high-quality example to guide the model
examples = [
    lx.data.ExampleData(
        text="ROMEO. But soft! What light through yonder window breaks? It is the east, and Juliet is the sun.",
        extractions=[
            lx.data.Extraction(
                extraction_class="character",
                extraction_text="ROMEO",
                attributes={"emotional_state": "wonder"}
            ),
            lx.data.Extraction(
                extraction_class="emotion",
                extraction_text="But soft!",
                attributes={"feeling": "gentle awe"}
            ),
            lx.data.Extraction(
                extraction_class="relationship",
                extraction_text="Juliet is the sun",
                attributes={"type": "metaphor"}
            ),
        ]
    )
]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Run the Extraction&lt;/h3&gt; 
&lt;p&gt;Provide your input text and the prompt materials to the &lt;code&gt;lx.extract&lt;/code&gt; function.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# The input text to be processed
input_text = "Lady Juliet gazed longingly at the stars, her heart aching for Romeo"

# Run the extraction
result = lx.extract(
    text_or_documents=input_text,
    prompt_description=prompt,
    examples=examples,
    model_id="gemini-2.5-flash",
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Model Selection&lt;/strong&gt;: &lt;code&gt;gemini-2.5-flash&lt;/code&gt; is the recommended default, offering an excellent balance of speed, cost, and quality. For highly complex tasks requiring deeper reasoning, &lt;code&gt;gemini-2.5-pro&lt;/code&gt; may provide superior results. For large-scale or production use, a Tier 2 Gemini quota is suggested to increase throughput and avoid rate limits. See the &lt;a href="https://ai.google.dev/gemini-api/docs/rate-limits#tier-2"&gt;rate-limit documentation&lt;/a&gt; for details.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Model Lifecycle&lt;/strong&gt;: Note that Gemini models have a lifecycle with defined retirement dates. Users should consult the &lt;a href="https://cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versions"&gt;official model version documentation&lt;/a&gt; to stay informed about the latest stable and legacy versions.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;3. Visualize the Results&lt;/h3&gt; 
&lt;p&gt;The extractions can be saved to a &lt;code&gt;.jsonl&lt;/code&gt; file, a popular format for working with language model data. LangExtract can then generate an interactive HTML visualization from this file to review the entities in context.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Save the results to a JSONL file
lx.io.save_annotated_documents([result], output_name="extraction_results.jsonl", output_dir=".")

# Generate the visualization from the file
html_content = lx.visualize("extraction_results.jsonl")
with open("visualization.html", "w") as f:
    if hasattr(html_content, 'data'):
        f.write(html_content.data)  # For Jupyter/Colab
    else:
        f.write(html_content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This creates an animated and interactive HTML file:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/google/langextract/main/docs/_static/romeo_juliet_basic.gif" alt="Romeo and Juliet Basic Visualization " /&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note on LLM Knowledge Utilization:&lt;/strong&gt; This example demonstrates extractions that stay close to the text evidence - extracting "longing" for Lady Juliet's emotional state and identifying "yearning" from "gazed longingly at the stars." The task could be modified to generate attributes that draw more heavily from the LLM's world knowledge (e.g., adding &lt;code&gt;"identity": "Capulet family daughter"&lt;/code&gt; or &lt;code&gt;"literary_context": "tragic heroine"&lt;/code&gt;). The balance between text-evidence and knowledge-inference is controlled by your prompt instructions and example attributes.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Scaling to Longer Documents&lt;/h3&gt; 
&lt;p&gt;For larger texts, you can process entire documents directly from URLs with parallel processing and enhanced sensitivity:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Process Romeo &amp;amp; Juliet directly from Project Gutenberg
result = lx.extract(
    text_or_documents="https://www.gutenberg.org/files/1513/1513-0.txt",
    prompt_description=prompt,
    examples=examples,
    model_id="gemini-2.5-flash",
    extraction_passes=3,    # Improves recall through multiple passes
    max_workers=20,         # Parallel processing for speed
    max_char_buffer=1000    # Smaller contexts for better accuracy
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This approach can extract hundreds of entities from full novels while maintaining high accuracy. The interactive visualization seamlessly handles large result sets, making it easy to explore hundreds of entities from the output JSONL file. &lt;strong&gt;&lt;a href="https://github.com/google/langextract/raw/main/docs/examples/longer_text_example.md"&gt;See the full &lt;em&gt;Romeo and Juliet&lt;/em&gt; extraction example ‚Üí&lt;/a&gt;&lt;/strong&gt; for detailed results and performance insights.&lt;/p&gt; 
&lt;h3&gt;Vertex AI Batch Processing&lt;/h3&gt; 
&lt;p&gt;Save costs on large-scale tasks by enabling Vertex AI Batch API: &lt;code&gt;language_model_params={"vertexai": True, "batch": {"enabled": True}}&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;See an example of the Vertex AI Batch API usage in &lt;a href="https://raw.githubusercontent.com/google/langextract/main/docs/examples/batch_api_example.md"&gt;this example&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;From PyPI&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install langextract
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Recommended for most users. For isolated environments, consider using a virtual environment:&lt;/em&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m venv langextract_env
source langextract_env/bin/activate  # On Windows: langextract_env\Scripts\activate
pip install langextract
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;From Source&lt;/h3&gt; 
&lt;p&gt;LangExtract uses modern Python packaging with &lt;code&gt;pyproject.toml&lt;/code&gt; for dependency management:&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Installing with &lt;code&gt;-e&lt;/code&gt; puts the package in development mode, allowing you to modify the code without reinstalling.&lt;/em&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/google/langextract.git
cd langextract

# For basic installation:
pip install -e .

# For development (includes linting tools):
pip install -e ".[dev]"

# For testing (includes pytest):
pip install -e ".[test]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Docker&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker build -t langextract .
docker run --rm -e LANGEXTRACT_API_KEY="your-api-key" langextract python your_script.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;API Key Setup for Cloud Models&lt;/h2&gt; 
&lt;p&gt;When using LangExtract with cloud-hosted models (like Gemini or OpenAI), you'll need to set up an API key. On-device models don't require an API key. For developers using local LLMs, LangExtract offers built-in support for Ollama and can be extended to other third-party APIs by updating the inference endpoints.&lt;/p&gt; 
&lt;h3&gt;API Key Sources&lt;/h3&gt; 
&lt;p&gt;Get API keys from:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://aistudio.google.com/app/apikey"&gt;AI Studio&lt;/a&gt; for Gemini models&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://cloud.google.com/vertex-ai/generative-ai/docs/sdks/overview"&gt;Vertex AI&lt;/a&gt; for enterprise use&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://platform.openai.com/api-keys"&gt;OpenAI Platform&lt;/a&gt; for OpenAI models&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Setting up API key in your environment&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Option 1: Environment Variable&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export LANGEXTRACT_API_KEY="your-api-key-here"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Option 2: .env File (Recommended)&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Add your API key to a &lt;code&gt;.env&lt;/code&gt; file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Add API key to .env file
cat &amp;gt;&amp;gt; .env &amp;lt;&amp;lt; 'EOF'
LANGEXTRACT_API_KEY=your-api-key-here
EOF

# Keep your API key secure
echo '.env' &amp;gt;&amp;gt; .gitignore
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In your Python code:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import langextract as lx

result = lx.extract(
    text_or_documents=input_text,
    prompt_description="Extract information...",
    examples=[...],
    model_id="gemini-2.5-flash"
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Option 3: Direct API Key (Not Recommended for Production)&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;You can also provide the API key directly in your code, though this is not recommended for production use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;result = lx.extract(
    text_or_documents=input_text,
    prompt_description="Extract information...",
    examples=[...],
    model_id="gemini-2.5-flash",
    api_key="your-api-key-here"  # Only use this for testing/development
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Option 4: Vertex AI (Service Accounts)&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Use &lt;a href="https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform"&gt;Vertex AI&lt;/a&gt; for authentication with service accounts:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;result = lx.extract(
    text_or_documents=input_text,
    prompt_description="Extract information...",
    examples=[...],
    model_id="gemini-2.5-flash",
    language_model_params={
        "vertexai": True,
        "project": "your-project-id",
        "location": "global"  # or regional endpoint
    }
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Adding Custom Model Providers&lt;/h2&gt; 
&lt;p&gt;LangExtract supports custom LLM providers via a lightweight plugin system. You can add support for new models without changing core code.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Add new model support independently of the core library&lt;/li&gt; 
 &lt;li&gt;Distribute your provider as a separate Python package&lt;/li&gt; 
 &lt;li&gt;Keep custom dependencies isolated&lt;/li&gt; 
 &lt;li&gt;Override or extend built-in providers via priority-based resolution&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See the detailed guide in &lt;a href="https://raw.githubusercontent.com/google/langextract/main/langextract/providers/README.md"&gt;Provider System Documentation&lt;/a&gt; to learn how to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Register a provider with &lt;code&gt;@registry.register(...)&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Publish an entry point for discovery&lt;/li&gt; 
 &lt;li&gt;Optionally provide a schema with &lt;code&gt;get_schema_class()&lt;/code&gt; for structured output&lt;/li&gt; 
 &lt;li&gt;Integrate with the factory via &lt;code&gt;create_model(...)&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Using OpenAI Models&lt;/h2&gt; 
&lt;p&gt;LangExtract supports OpenAI models (requires optional dependency: &lt;code&gt;pip install langextract[openai]&lt;/code&gt;):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import langextract as lx

result = lx.extract(
    text_or_documents=input_text,
    prompt_description=prompt,
    examples=examples,
    model_id="gpt-4o",  # Automatically selects OpenAI provider
    api_key=os.environ.get('OPENAI_API_KEY'),
    fence_output=True,
    use_schema_constraints=False
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note: OpenAI models require &lt;code&gt;fence_output=True&lt;/code&gt; and &lt;code&gt;use_schema_constraints=False&lt;/code&gt; because LangExtract doesn't implement schema constraints for OpenAI yet.&lt;/p&gt; 
&lt;h2&gt;Using Local LLMs with Ollama&lt;/h2&gt; 
&lt;p&gt;LangExtract supports local inference using Ollama, allowing you to run models without API keys:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import langextract as lx

result = lx.extract(
    text_or_documents=input_text,
    prompt_description=prompt,
    examples=examples,
    model_id="gemma2:2b",  # Automatically selects Ollama provider
    model_url="http://localhost:11434",
    fence_output=False,
    use_schema_constraints=False
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Quick setup:&lt;/strong&gt; Install Ollama from &lt;a href="https://ollama.com/"&gt;ollama.com&lt;/a&gt;, run &lt;code&gt;ollama pull gemma2:2b&lt;/code&gt;, then &lt;code&gt;ollama serve&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;For detailed installation, Docker setup, and examples, see &lt;a href="https://raw.githubusercontent.com/google/langextract/main/examples/ollama/"&gt;&lt;code&gt;examples/ollama/&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;More Examples&lt;/h2&gt; 
&lt;p&gt;Additional examples of LangExtract in action:&lt;/p&gt; 
&lt;h3&gt;&lt;em&gt;Romeo and Juliet&lt;/em&gt; Full Text Extraction&lt;/h3&gt; 
&lt;p&gt;LangExtract can process complete documents directly from URLs. This example demonstrates extraction from the full text of &lt;em&gt;Romeo and Juliet&lt;/em&gt; from Project Gutenberg (147,843 characters), showing parallel processing, sequential extraction passes, and performance optimization for long document processing.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/google/langextract/raw/main/docs/examples/longer_text_example.md"&gt;View &lt;em&gt;Romeo and Juliet&lt;/em&gt; Full Text Example ‚Üí&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h3&gt;Medication Extraction&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Disclaimer:&lt;/strong&gt; This demonstration is for illustrative purposes of LangExtract's baseline capability only. It does not represent a finished or approved product, is not intended to diagnose or suggest treatment of any disease or condition, and should not be used for medical advice.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;LangExtract excels at extracting structured medical information from clinical text. These examples demonstrate both basic entity recognition (medication names, dosages, routes) and relationship extraction (connecting medications to their attributes), showing LangExtract's effectiveness for healthcare applications.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/google/langextract/raw/main/docs/examples/medication_examples.md"&gt;View Medication Examples ‚Üí&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h3&gt;Radiology Report Structuring: RadExtract&lt;/h3&gt; 
&lt;p&gt;Explore RadExtract, a live interactive demo on HuggingFace Spaces that shows how LangExtract can automatically structure radiology reports. Try it directly in your browser with no setup required.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://huggingface.co/spaces/google/radextract"&gt;View RadExtract Demo ‚Üí&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Community Providers&lt;/h2&gt; 
&lt;p&gt;Extend LangExtract with custom model providers! Check out our &lt;a href="https://raw.githubusercontent.com/google/langextract/main/COMMUNITY_PROVIDERS.md"&gt;Community Provider Plugins&lt;/a&gt; registry to discover providers created by the community or add your own.&lt;/p&gt; 
&lt;p&gt;For detailed instructions on creating a provider plugin, see the &lt;a href="https://raw.githubusercontent.com/google/langextract/main/examples/custom_provider_plugin/"&gt;Custom Provider Plugin Example&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions are welcome! See &lt;a href="https://github.com/google/langextract/raw/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; to get started with development, testing, and pull requests. You must sign a &lt;a href="https://cla.developers.google.com/about"&gt;Contributor License Agreement&lt;/a&gt; before submitting patches.&lt;/p&gt; 
&lt;h2&gt;Testing&lt;/h2&gt; 
&lt;p&gt;To run tests locally from the source:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone the repository
git clone https://github.com/google/langextract.git
cd langextract

# Install with test dependencies
pip install -e ".[test]"

# Run all tests
pytest tests
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or reproduce the full CI matrix locally with tox:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;tox  # runs pylint + pytest on Python 3.10 and 3.11
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Ollama Integration Testing&lt;/h3&gt; 
&lt;p&gt;If you have Ollama installed locally, you can run integration tests:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Test Ollama integration (requires Ollama running with gemma2:2b model)
tox -e ollama-integration
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This test will automatically detect if Ollama is available and run real inference tests.&lt;/p&gt; 
&lt;h2&gt;Development&lt;/h2&gt; 
&lt;h3&gt;Code Formatting&lt;/h3&gt; 
&lt;p&gt;This project uses automated formatting tools to maintain consistent code style:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Auto-format all code
./autoformat.sh

# Or run formatters separately
isort langextract tests --profile google --line-length 80
pyink langextract tests --config pyproject.toml
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Pre-commit Hooks&lt;/h3&gt; 
&lt;p&gt;For automatic formatting checks:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pre-commit install  # One-time setup
pre-commit run --all-files  # Manual run
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Linting&lt;/h3&gt; 
&lt;p&gt;Run linting before submitting PRs:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pylint --rcfile=.pylintrc langextract tests
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/google/langextract/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for full development guidelines.&lt;/p&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;This is not an officially supported Google product. If you use LangExtract in production or publications, please cite accordingly and acknowledge usage. Use is subject to the &lt;a href="https://github.com/google/langextract/raw/main/LICENSE"&gt;Apache 2.0 License&lt;/a&gt;. For health-related applications, use of LangExtract is also subject to the &lt;a href="https://developers.google.com/health-ai-developer-foundations/terms"&gt;Health AI Developer Foundations Terms of Use&lt;/a&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;Happy Extracting!&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>resemble-ai/chatterbox</title>
      <link>https://github.com/resemble-ai/chatterbox</link>
      <description>&lt;p&gt;SoTA open-source TTS&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/resemble-ai/chatterbox/master/Chatterbox-Turbo.jpg" alt="Chatterbox Turbo Image" /&gt;&lt;/p&gt; 
&lt;h1&gt;Chatterbox TTS&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://resemble-ai.github.io/chatterbox_turbo_demopage/"&gt;&lt;img src="https://img.shields.io/badge/listen-demo_samples-blue" alt="Alt Text" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/spaces/ResembleAI/chatterbox-turbo-demo"&gt;&lt;img src="https://huggingface.co/datasets/huggingface/badges/resolve/main/open-in-hf-spaces-sm.svg?sanitize=true" alt="Alt Text" /&gt;&lt;/a&gt; &lt;a href="https://podonos.com/resembleai/chatterbox"&gt;&lt;img src="https://static-public.podonos.com/badges/insight-on-pdns-sm-dark.svg?sanitize=true" alt="Alt Text" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/rJq9cRJBJ6"&gt;&lt;img src="https://img.shields.io/discord/1377773249798344776?label=join%20discord&amp;amp;logo=discord&amp;amp;style=flat" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;_Made with ‚ô•Ô∏è by &lt;a href="https://resemble.ai" target="_blank"&gt;&lt;img width="100" alt="resemble-logo-horizontal" src="https://github.com/user-attachments/assets/35cf756b-3506-4943-9c72-c05ddfa4e525" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Chatterbox&lt;/strong&gt; is a family of three state-of-the-art, open-source text-to-speech models by Resemble AI.&lt;/p&gt; 
&lt;p&gt;We are excited to introduce &lt;strong&gt;Chatterbox-Turbo&lt;/strong&gt;, our most efficient model yet. Built on a streamlined 350M parameter architecture, &lt;strong&gt;Turbo&lt;/strong&gt; delivers high-quality speech with less compute and VRAM than our previous models. We have also distilled the speech-token-to-mel decoder, previously a bottleneck, reducing generation from 10 steps to just &lt;strong&gt;one&lt;/strong&gt;, while retaining high-fidelity audio output.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Paralinguistic tags&lt;/strong&gt; are now native to the Turbo model, allowing you to use &lt;code&gt;[cough]&lt;/code&gt;, &lt;code&gt;[laugh]&lt;/code&gt;, &lt;code&gt;[chuckle]&lt;/code&gt;, and more to add distinct realism. While Turbo was built primarily for low-latency voice agents, it excels at narration and creative workflows.&lt;/p&gt; 
&lt;p&gt;If you like the model but need to scale or tune it for higher accuracy, check out our competitively priced TTS service (&lt;a href="https://resemble.ai"&gt;link&lt;/a&gt;). It delivers reliable performance with ultra-low latency of sub 200ms‚Äîideal for production use in agents, applications, or interactive media.&lt;/p&gt; 
&lt;img width="1200" height="600" alt="Podonos Turbo Eval" src="https://storage.googleapis.com/chatterbox-demo-samples/turbo/podonos_turbo.png" /&gt; 
&lt;h3&gt;‚ö° Model Zoo&lt;/h3&gt; 
&lt;p&gt;Choose the right model for your application.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Model&lt;/th&gt; 
   &lt;th align="left"&gt;Size&lt;/th&gt; 
   &lt;th align="left"&gt;Languages&lt;/th&gt; 
   &lt;th align="left"&gt;Key Features&lt;/th&gt; 
   &lt;th align="left"&gt;Best For&lt;/th&gt; 
   &lt;th align="left"&gt;ü§ó&lt;/th&gt; 
   &lt;th align="left"&gt;Examples&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;Chatterbox-Turbo&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;350M&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;English&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Paralinguistic Tags (&lt;code&gt;[laugh]&lt;/code&gt;), Lower Compute and VRAM&lt;/td&gt; 
   &lt;td align="left"&gt;Zero-shot voice agents, Production&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://huggingface.co/spaces/ResembleAI/chatterbox-turbo-demo"&gt;Demo&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://resemble-ai.github.io/chatterbox_turbo_demopage/"&gt;Listen&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;Chatterbox-Multilingual &lt;a href="https://raw.githubusercontent.com/resemble-ai/chatterbox/master/#supported-languages"&gt;(Language list)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;500M&lt;/td&gt; 
   &lt;td align="left"&gt;23+&lt;/td&gt; 
   &lt;td align="left"&gt;Zero-shot cloning, Multiple Languages&lt;/td&gt; 
   &lt;td align="left"&gt;Global applications, Localization&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://huggingface.co/spaces/ResembleAI/Chatterbox-Multilingual-TTS"&gt;Demo&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://resemble-ai.github.io/chatterbox_demopage/"&gt;Listen&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;Chatterbox &lt;a href="https://raw.githubusercontent.com/resemble-ai/chatterbox/master/#original-chatterbox-tips"&gt;(Tips and Tricks)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;500M&lt;/td&gt; 
   &lt;td align="left"&gt;English&lt;/td&gt; 
   &lt;td align="left"&gt;CFG &amp;amp; Exaggeration tuning&lt;/td&gt; 
   &lt;td align="left"&gt;General zero-shot TTS with creative controls&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://huggingface.co/spaces/ResembleAI/Chatterbox"&gt;Demo&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://resemble-ai.github.io/chatterbox_demopage/"&gt;Listen&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install chatterbox-tts
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, you can install from source:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# conda create -yn chatterbox python=3.11
# conda activate chatterbox

git clone https://github.com/resemble-ai/chatterbox.git
cd chatterbox
pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We developed and tested Chatterbox on Python 3.11 on Debian 11 OS; the versions of the dependencies are pinned in &lt;code&gt;pyproject.toml&lt;/code&gt; to ensure consistency. You can modify the code or dependencies in this installation mode.&lt;/p&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h5&gt;Chatterbox-Turbo&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import torchaudio as ta
import torch
from chatterbox.tts_turbo import ChatterboxTurboTTS

# Load the Turbo model
model = ChatterboxTurboTTS.from_pretrained(device="cuda")

# Generate with Paralinguistic Tags
text = "Hi there, Sarah here from MochaFone calling you back [chuckle], have you got one minute to chat about the billing issue?"

# Generate audio (requires a reference clip for voice cloning)
wav = model.generate(text, audio_prompt_path="your_10s_ref_clip.wav")

ta.save("test-turbo.wav", wav, model.sr)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;Chatterbox and Chatterbox-Multilingual&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;
import torchaudio as ta
from chatterbox.tts import ChatterboxTTS
from chatterbox.mtl_tts import ChatterboxMultilingualTTS

# English example
model = ChatterboxTTS.from_pretrained(device="cuda")

text = "Ezreal and Jinx teamed up with Ahri, Yasuo, and Teemo to take down the enemy's Nexus in an epic late-game pentakill."
wav = model.generate(text)
ta.save("test-english.wav", wav, model.sr)

# Multilingual examples
multilingual_model = ChatterboxMultilingualTTS.from_pretrained(device=device)

french_text = "Bonjour, comment √ßa va? Ceci est le mod√®le de synth√®se vocale multilingue Chatterbox, il prend en charge 23 langues."
wav_french = multilingual_model.generate(spanish_text, language_id="fr")
ta.save("test-french.wav", wav_french, model.sr)

chinese_text = "‰Ω†Â•ΩÔºå‰ªäÂ§©Â§©Ê∞îÁúü‰∏çÈîôÔºåÂ∏åÊúõ‰Ω†Êúâ‰∏Ä‰∏™ÊÑâÂø´ÁöÑÂë®Êú´„ÄÇ"
wav_chinese = multilingual_model.generate(chinese_text, language_id="zh")
ta.save("test-chinese.wav", wav_chinese, model.sr)

# If you want to synthesize with a different voice, specify the audio prompt
AUDIO_PROMPT_PATH = "YOUR_FILE.wav"
wav = model.generate(text, audio_prompt_path=AUDIO_PROMPT_PATH)
ta.save("test-2.wav", wav, model.sr)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;code&gt;example_tts.py&lt;/code&gt; and &lt;code&gt;example_vc.py&lt;/code&gt; for more examples.&lt;/p&gt; 
&lt;h2&gt;Supported Languages&lt;/h2&gt; 
&lt;p&gt;Arabic (ar) ‚Ä¢ Danish (da) ‚Ä¢ German (de) ‚Ä¢ Greek (el) ‚Ä¢ English (en) ‚Ä¢ Spanish (es) ‚Ä¢ Finnish (fi) ‚Ä¢ French (fr) ‚Ä¢ Hebrew (he) ‚Ä¢ Hindi (hi) ‚Ä¢ Italian (it) ‚Ä¢ Japanese (ja) ‚Ä¢ Korean (ko) ‚Ä¢ Malay (ms) ‚Ä¢ Dutch (nl) ‚Ä¢ Norwegian (no) ‚Ä¢ Polish (pl) ‚Ä¢ Portuguese (pt) ‚Ä¢ Russian (ru) ‚Ä¢ Swedish (sv) ‚Ä¢ Swahili (sw) ‚Ä¢ Turkish (tr) ‚Ä¢ Chinese (zh)&lt;/p&gt; 
&lt;h2&gt;Original Chatterbox Tips&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;General Use (TTS and Voice Agents):&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Ensure that the reference clip matches the specified language tag. Otherwise, language transfer outputs may inherit the accent of the reference clip‚Äôs language. To mitigate this, set &lt;code&gt;cfg_weight&lt;/code&gt; to &lt;code&gt;0&lt;/code&gt;.&lt;/li&gt; 
   &lt;li&gt;The default settings (&lt;code&gt;exaggeration=0.5&lt;/code&gt;, &lt;code&gt;cfg_weight=0.5&lt;/code&gt;) work well for most prompts across all languages.&lt;/li&gt; 
   &lt;li&gt;If the reference speaker has a fast speaking style, lowering &lt;code&gt;cfg_weight&lt;/code&gt; to around &lt;code&gt;0.3&lt;/code&gt; can improve pacing.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Expressive or Dramatic Speech:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Try lower &lt;code&gt;cfg_weight&lt;/code&gt; values (e.g. &lt;code&gt;~0.3&lt;/code&gt;) and increase &lt;code&gt;exaggeration&lt;/code&gt; to around &lt;code&gt;0.7&lt;/code&gt; or higher.&lt;/li&gt; 
   &lt;li&gt;Higher &lt;code&gt;exaggeration&lt;/code&gt; tends to speed up speech; reducing &lt;code&gt;cfg_weight&lt;/code&gt; helps compensate with slower, more deliberate pacing.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Built-in PerTh Watermarking for Responsible AI&lt;/h2&gt; 
&lt;p&gt;Every audio file generated by Chatterbox includes &lt;a href="https://github.com/resemble-ai/perth"&gt;Resemble AI's Perth (Perceptual Threshold) Watermarker&lt;/a&gt; - imperceptible neural watermarks that survive MP3 compression, audio editing, and common manipulations while maintaining nearly 100% detection accuracy.&lt;/p&gt; 
&lt;h2&gt;Watermark extraction&lt;/h2&gt; 
&lt;p&gt;You can look for the watermark using the following script.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import perth
import librosa

AUDIO_PATH = "YOUR_FILE.wav"

# Load the watermarked audio
watermarked_audio, sr = librosa.load(AUDIO_PATH, sr=None)

# Initialize watermarker (same as used for embedding)
watermarker = perth.PerthImplicitWatermarker()

# Extract watermark
watermark = watermarker.get_watermark(watermarked_audio, sample_rate=sr)
print(f"Extracted watermark: {watermark}")
# Output: 0.0 (no watermark) or 1.0 (watermarked)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Official Discord&lt;/h2&gt; 
&lt;p&gt;üëã Join us on &lt;a href="https://discord.gg/rJq9cRJBJ6"&gt;Discord&lt;/a&gt; and let's build something awesome together!&lt;/p&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/FunAudioLLM/CosyVoice"&gt;Cosyvoice&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning"&gt;Real-Time-Voice-Cloning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/yl4579/HiFTNet"&gt;HiFT-GAN&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/meta-llama/llama3"&gt;Llama 3&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/xingchensong/S3Tokenizer"&gt;S3Tokenizer&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you find this model useful, please consider citing.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@misc{chatterboxtts2025,
  author       = {{Resemble AI}},
  title        = {{Chatterbox-TTS}},
  year         = {2025},
  howpublished = {\url{https://github.com/resemble-ai/chatterbox}},
  note         = {GitHub repository}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;Don't use this model to do bad things. Prompts are sourced from freely available data on the internet.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>