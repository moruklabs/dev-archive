<rss version="2.0">
  <channel>
    <title>GitHub All Languages Daily Trending</title>
    <description>Daily Trending of All Languages in GitHub</description>
    <pubDate>Sat, 30 Aug 2025 01:30:00 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>twbs/bootstrap</title>
      <link>https://github.com/twbs/bootstrap</link>
      <description>&lt;p&gt;The most popular HTML, CSS, and JavaScript framework for developing responsive, mobile first projects on the web.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://getbootstrap.com/"&gt; &lt;img src="https://getbootstrap.com/docs/5.3/assets/brand/bootstrap-logo-shadow.png" alt="Bootstrap logo" width="200" height="165" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h3 align="center"&gt;Bootstrap&lt;/h3&gt; 
&lt;p align="center"&gt; Sleek, intuitive, and powerful front-end framework for faster and easier web development. &lt;br /&gt; &lt;a href="https://getbootstrap.com/docs/5.3/"&gt;&lt;strong&gt;Explore Bootstrap docs ¬ª&lt;/strong&gt;&lt;/a&gt; &lt;br /&gt; &lt;br /&gt; &lt;a href="https://github.com/twbs/bootstrap/issues/new?assignees=-&amp;amp;labels=bug&amp;amp;template=bug_report.yml"&gt;Report bug&lt;/a&gt; ¬∑ &lt;a href="https://github.com/twbs/bootstrap/issues/new?assignees=&amp;amp;labels=feature&amp;amp;template=feature_request.yml"&gt;Request feature&lt;/a&gt; ¬∑ &lt;a href="https://blog.getbootstrap.com/"&gt;Blog&lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;Bootstrap 5&lt;/h2&gt; 
&lt;p&gt;Our default branch is for development of our Bootstrap 5 release. Head to the &lt;a href="https://github.com/twbs/bootstrap/tree/v4-dev"&gt;&lt;code&gt;v4-dev&lt;/code&gt; branch&lt;/a&gt; to view the readme, documentation, and source code for Bootstrap 4.&lt;/p&gt; 
&lt;h2&gt;Table of contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/twbs/bootstrap/main/#quick-start"&gt;Quick start&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/twbs/bootstrap/main/#status"&gt;Status&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/twbs/bootstrap/main/#whats-included"&gt;What‚Äôs included&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/twbs/bootstrap/main/#bugs-and-feature-requests"&gt;Bugs and feature requests&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/twbs/bootstrap/main/#documentation"&gt;Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/twbs/bootstrap/main/#contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/twbs/bootstrap/main/#community"&gt;Community&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/twbs/bootstrap/main/#versioning"&gt;Versioning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/twbs/bootstrap/main/#creators"&gt;Creators&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/twbs/bootstrap/main/#thanks"&gt;Thanks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/twbs/bootstrap/main/#copyright-and-license"&gt;Copyright and license&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quick start&lt;/h2&gt; 
&lt;p&gt;Several quick start options are available:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/twbs/bootstrap/archive/v5.3.8.zip"&gt;Download the latest release&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Clone the repo: &lt;code&gt;git clone https://github.com/twbs/bootstrap.git&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Install with &lt;a href="https://www.npmjs.com/"&gt;npm&lt;/a&gt;: &lt;code&gt;npm install bootstrap@v5.3.8&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Install with &lt;a href="https://yarnpkg.com/"&gt;yarn&lt;/a&gt;: &lt;code&gt;yarn add bootstrap@v5.3.8&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Install with &lt;a href="https://bun.sh/"&gt;Bun&lt;/a&gt;: &lt;code&gt;bun add bootstrap@v5.3.8&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Install with &lt;a href="https://getcomposer.org/"&gt;Composer&lt;/a&gt;: &lt;code&gt;composer require twbs/bootstrap:5.3.8&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Install with &lt;a href="https://www.nuget.org/"&gt;NuGet&lt;/a&gt;: CSS: &lt;code&gt;Install-Package bootstrap&lt;/code&gt; Sass: &lt;code&gt;Install-Package bootstrap.sass&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Read the &lt;a href="https://getbootstrap.com/docs/5.3/getting-started/introduction/"&gt;Getting started page&lt;/a&gt; for information on the framework contents, templates, examples, and more.&lt;/p&gt; 
&lt;h2&gt;Status&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/twbs/bootstrap/actions/workflows/js.yml?query=workflow%3AJS+branch%3Amain"&gt;&lt;img src="https://img.shields.io/github/actions/workflow/status/twbs/bootstrap/js.yml?branch=main&amp;amp;label=JS%20Tests&amp;amp;logo=github" alt="Build Status" /&gt;&lt;/a&gt; &lt;a href="https://www.npmjs.com/package/bootstrap"&gt;&lt;img src="https://img.shields.io/npm/v/bootstrap?logo=npm&amp;amp;logoColor=fff" alt="npm version" /&gt;&lt;/a&gt; &lt;a href="https://rubygems.org/gems/bootstrap"&gt;&lt;img src="https://img.shields.io/gem/v/bootstrap?logo=rubygems&amp;amp;logoColor=fff" alt="Gem version" /&gt;&lt;/a&gt; &lt;a href="https://atmospherejs.com/twbs/bootstrap"&gt;&lt;img src="https://img.shields.io/badge/meteor-twbs%3Abootstrap-blue?logo=meteor&amp;amp;logoColor=fff" alt="Meteor Atmosphere" /&gt;&lt;/a&gt; &lt;a href="https://packagist.org/packages/twbs/bootstrap"&gt;&lt;img src="https://img.shields.io/packagist/vpre/twbs/bootstrap?logo=packagist&amp;amp;logoColor=fff" alt="Packagist Prerelease" /&gt;&lt;/a&gt; &lt;a href="https://www.nuget.org/packages/bootstrap/absoluteLatest"&gt;&lt;img src="https://img.shields.io/nuget/vpre/bootstrap?logo=nuget&amp;amp;logoColor=fff" alt="NuGet" /&gt;&lt;/a&gt; &lt;a href="https://coveralls.io/github/twbs/bootstrap?branch=main"&gt;&lt;img src="https://img.shields.io/coveralls/github/twbs/bootstrap/main?logo=coveralls&amp;amp;logoColor=fff" alt="Coverage Status" /&gt;&lt;/a&gt; &lt;a href="https://github.com/twbs/bootstrap/raw/main/dist/css/bootstrap.min.css"&gt;&lt;img src="https://img.badgesize.io/twbs/bootstrap/main/dist/css/bootstrap.min.css?compression=gzip&amp;amp;label=CSS%20gzip%20size" alt="CSS gzip size" /&gt;&lt;/a&gt; &lt;a href="https://github.com/twbs/bootstrap/raw/main/dist/css/bootstrap.min.css"&gt;&lt;img src="https://img.badgesize.io/twbs/bootstrap/main/dist/css/bootstrap.min.css?compression=brotli&amp;amp;label=CSS%20Brotli%20size" alt="CSS Brotli size" /&gt;&lt;/a&gt; &lt;a href="https://github.com/twbs/bootstrap/raw/main/dist/js/bootstrap.min.js"&gt;&lt;img src="https://img.badgesize.io/twbs/bootstrap/main/dist/js/bootstrap.min.js?compression=gzip&amp;amp;label=JS%20gzip%20size" alt="JS gzip size" /&gt;&lt;/a&gt; &lt;a href="https://github.com/twbs/bootstrap/raw/main/dist/js/bootstrap.min.js"&gt;&lt;img src="https://img.badgesize.io/twbs/bootstrap/main/dist/js/bootstrap.min.js?compression=brotli&amp;amp;label=JS%20Brotli%20size" alt="JS Brotli size" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/ossf-scorecard/github.com/twbs/bootstrap" alt="Open Source Security Foundation Scorecard" /&gt; &lt;a href="https://raw.githubusercontent.com/twbs/bootstrap/main/#backers"&gt;&lt;img src="https://img.shields.io/opencollective/backers/bootstrap?logo=opencollective&amp;amp;logoColor=fff" alt="Backers on Open Collective" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/twbs/bootstrap/main/#sponsors"&gt;&lt;img src="https://img.shields.io/opencollective/sponsors/bootstrap?logo=opencollective&amp;amp;logoColor=fff" alt="Sponsors on Open Collective" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;What‚Äôs included&lt;/h2&gt; 
&lt;p&gt;Within the download you‚Äôll find the following directories and files, logically grouping common assets and providing both compiled and minified variations.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Download contents&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-text"&gt;bootstrap/
‚îú‚îÄ‚îÄ css/
‚îÇ   ‚îú‚îÄ‚îÄ bootstrap-grid.css
‚îÇ   ‚îú‚îÄ‚îÄ bootstrap-grid.css.map
‚îÇ   ‚îú‚îÄ‚îÄ bootstrap-grid.min.css
‚îÇ   ‚îú‚îÄ‚îÄ bootstrap-grid.min.css.map
‚îÇ   ‚îú‚îÄ‚îÄ bootstrap-grid.rtl.css
‚îÇ   ‚îú‚îÄ‚îÄ bootstrap-grid.rtl.css.map
‚îÇ   ‚îú‚îÄ‚îÄ bootstrap-grid.rtl.min.css
‚îÇ   ‚îú‚îÄ‚îÄ bootstrap-grid.rtl.min.css.map
‚îÇ   ‚îú‚îÄ‚îÄ bootstrap-reboot.css
‚îÇ   ‚îú‚îÄ‚îÄ bootstrap-reboot.css.map
‚îÇ   ‚îú‚îÄ‚îÄ bootstrap-reboot.min.css
‚îÇ   ‚îú‚îÄ‚îÄ bootstrap-reboot.min.css.map
‚îÇ   ‚îú‚îÄ‚îÄ bootstrap-reboot.rtl.css
‚îÇ   ‚îú‚îÄ‚îÄ bootstrap-reboot.rtl.css.map
‚îÇ   ‚îú‚îÄ‚îÄ bootstrap-reboot.rtl.min.css
‚îÇ   ‚îú‚îÄ‚îÄ bootstrap-reboot.rtl.min.css.map
‚îÇ   ‚îú‚îÄ‚îÄ bootstrap-utilities.css
‚îÇ   ‚îú‚îÄ‚îÄ bootstrap-utilities.css.map
‚îÇ   ‚îú‚îÄ‚îÄ bootstrap-utilities.min.css
‚îÇ   ‚îú‚îÄ‚îÄ bootstrap-utilities.min.css.map
‚îÇ   ‚îú‚îÄ‚îÄ bootstrap-utilities.rtl.css
‚îÇ   ‚îú‚îÄ‚îÄ bootstrap-utilities.rtl.css.map
‚îÇ   ‚îú‚îÄ‚îÄ bootstrap-utilities.rtl.min.css
‚îÇ   ‚îú‚îÄ‚îÄ bootstrap-utilities.rtl.min.css.map
‚îÇ   ‚îú‚îÄ‚îÄ bootstrap.css
‚îÇ   ‚îú‚îÄ‚îÄ bootstrap.css.map
‚îÇ   ‚îú‚îÄ‚îÄ bootstrap.min.css
‚îÇ   ‚îú‚îÄ‚îÄ bootstrap.min.css.map
‚îÇ   ‚îú‚îÄ‚îÄ bootstrap.rtl.css
‚îÇ   ‚îú‚îÄ‚îÄ bootstrap.rtl.css.map
‚îÇ   ‚îú‚îÄ‚îÄ bootstrap.rtl.min.css
‚îÇ   ‚îî‚îÄ‚îÄ bootstrap.rtl.min.css.map
‚îî‚îÄ‚îÄ js/
    ‚îú‚îÄ‚îÄ bootstrap.bundle.js
    ‚îú‚îÄ‚îÄ bootstrap.bundle.js.map
    ‚îú‚îÄ‚îÄ bootstrap.bundle.min.js
    ‚îú‚îÄ‚îÄ bootstrap.bundle.min.js.map
    ‚îú‚îÄ‚îÄ bootstrap.esm.js
    ‚îú‚îÄ‚îÄ bootstrap.esm.js.map
    ‚îú‚îÄ‚îÄ bootstrap.esm.min.js
    ‚îú‚îÄ‚îÄ bootstrap.esm.min.js.map
    ‚îú‚îÄ‚îÄ bootstrap.js
    ‚îú‚îÄ‚îÄ bootstrap.js.map
    ‚îú‚îÄ‚îÄ bootstrap.min.js
    ‚îî‚îÄ‚îÄ bootstrap.min.js.map
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p&gt;We provide compiled CSS and JS (&lt;code&gt;bootstrap.*&lt;/code&gt;), as well as compiled and minified CSS and JS (&lt;code&gt;bootstrap.min.*&lt;/code&gt;). &lt;a href="https://web.dev/articles/source-maps"&gt;Source maps&lt;/a&gt; (&lt;code&gt;bootstrap.*.map&lt;/code&gt;) are available for use with certain browsers‚Äô developer tools. Bundled JS files (&lt;code&gt;bootstrap.bundle.js&lt;/code&gt; and minified &lt;code&gt;bootstrap.bundle.min.js&lt;/code&gt;) include &lt;a href="https://popper.js.org/docs/v2/"&gt;Popper&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Bugs and feature requests&lt;/h2&gt; 
&lt;p&gt;Have a bug or a feature request? Please first read the &lt;a href="https://github.com/twbs/bootstrap/raw/main/.github/CONTRIBUTING.md#using-the-issue-tracker"&gt;issue guidelines&lt;/a&gt; and search for existing and closed issues. If your problem or idea is not addressed yet, &lt;a href="https://github.com/twbs/bootstrap/issues/new/choose"&gt;please open a new issue&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;Bootstrap‚Äôs documentation, included in this repo in the root directory, is built with &lt;a href="https://astro.build/"&gt;Astro&lt;/a&gt; and publicly hosted on GitHub Pages at &lt;a href="https://getbootstrap.com/"&gt;https://getbootstrap.com/&lt;/a&gt;. The docs may also be run locally.&lt;/p&gt; 
&lt;p&gt;Documentation search is powered by &lt;a href="https://docsearch.algolia.com/"&gt;Algolia's DocSearch&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Running documentation locally&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Run &lt;code&gt;npm install&lt;/code&gt; to install the Node.js dependencies, including Astro (the site builder).&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;npm run test&lt;/code&gt; (or a specific npm script) to rebuild distributed CSS and JavaScript files, as well as our docs assets.&lt;/li&gt; 
 &lt;li&gt;From the root &lt;code&gt;/bootstrap&lt;/code&gt; directory, run &lt;code&gt;npm run docs-serve&lt;/code&gt; in the command line.&lt;/li&gt; 
 &lt;li&gt;Open &lt;a href="http://localhost:9001"&gt;http://localhost:9001&lt;/a&gt; in your browser, and voil√†.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Learn more about using Astro by reading its &lt;a href="https://docs.astro.build/en/getting-started/"&gt;documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Documentation for previous releases&lt;/h3&gt; 
&lt;p&gt;You can find all our previous releases docs on &lt;a href="https://getbootstrap.com/docs/versions/"&gt;https://getbootstrap.com/docs/versions/&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/twbs/bootstrap/releases"&gt;Previous releases&lt;/a&gt; and their documentation are also available for download.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Please read through our &lt;a href="https://github.com/twbs/bootstrap/raw/main/.github/CONTRIBUTING.md"&gt;contributing guidelines&lt;/a&gt;. Included are directions for opening issues, coding standards, and notes on development.&lt;/p&gt; 
&lt;p&gt;Moreover, if your pull request contains JavaScript patches or features, you must include &lt;a href="https://github.com/twbs/bootstrap/tree/main/js/tests"&gt;relevant unit tests&lt;/a&gt;. All HTML and CSS should conform to the &lt;a href="https://github.com/mdo/code-guide"&gt;Code Guide&lt;/a&gt;, maintained by &lt;a href="https://github.com/mdo"&gt;Mark Otto&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Editor preferences are available in the &lt;a href="https://github.com/twbs/bootstrap/raw/main/.editorconfig"&gt;editor config&lt;/a&gt; for easy use in common text editors. Read more and download plugins at &lt;a href="https://editorconfig.org/"&gt;https://editorconfig.org/&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;p&gt;Get updates on Bootstrap‚Äôs development and chat with the project maintainers and community members.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Follow &lt;a href="https://x.com/getbootstrap"&gt;@getbootstrap on X&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Read and subscribe to &lt;a href="https://blog.getbootstrap.com/"&gt;The Official Bootstrap Blog&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Ask questions and explore &lt;a href="https://github.com/twbs/bootstrap/discussions"&gt;our GitHub Discussions&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Discuss, ask questions, and more on &lt;a href="https://discord.gg/bZUvakRU3M"&gt;the community Discord&lt;/a&gt; or &lt;a href="https://www.reddit.com/r/bootstrap/"&gt;Bootstrap subreddit&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Chat with fellow Bootstrappers in IRC. On the &lt;code&gt;irc.libera.chat&lt;/code&gt; server, in the &lt;code&gt;#bootstrap&lt;/code&gt; channel.&lt;/li&gt; 
 &lt;li&gt;Implementation help may be found at Stack Overflow (tagged &lt;a href="https://stackoverflow.com/questions/tagged/bootstrap-5"&gt;&lt;code&gt;bootstrap-5&lt;/code&gt;&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;Developers should use the keyword &lt;code&gt;bootstrap&lt;/code&gt; on packages which modify or add to the functionality of Bootstrap when distributing through &lt;a href="https://www.npmjs.com/browse/keyword/bootstrap"&gt;npm&lt;/a&gt; or similar delivery mechanisms for maximum discoverability.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Versioning&lt;/h2&gt; 
&lt;p&gt;For transparency into our release cycle and in striving to maintain backward compatibility, Bootstrap is maintained under &lt;a href="https://semver.org/"&gt;the Semantic Versioning guidelines&lt;/a&gt;. Sometimes we screw up, but we adhere to those rules whenever possible.&lt;/p&gt; 
&lt;p&gt;See &lt;a href="https://github.com/twbs/bootstrap/releases"&gt;the Releases section of our GitHub project&lt;/a&gt; for changelogs for each release version of Bootstrap. Release announcement posts on &lt;a href="https://blog.getbootstrap.com/"&gt;the official Bootstrap blog&lt;/a&gt; contain summaries of the most noteworthy changes made in each release.&lt;/p&gt; 
&lt;h2&gt;Creators&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Mark Otto&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://x.com/mdo"&gt;https://x.com/mdo&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mdo"&gt;https://github.com/mdo&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Jacob Thornton&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://x.com/fat"&gt;https://x.com/fat&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/fat"&gt;https://github.com/fat&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Thanks&lt;/h2&gt; 
&lt;a href="https://www.browserstack.com/"&gt; &lt;img src="https://live.browserstack.com/images/opensource/browserstack-logo.svg?sanitize=true" alt="BrowserStack" width="192" height="42" /&gt; &lt;/a&gt; 
&lt;p&gt;Thanks to &lt;a href="https://www.browserstack.com/"&gt;BrowserStack&lt;/a&gt; for providing the infrastructure that allows us to test in real browsers!&lt;/p&gt; 
&lt;a href="https://www.netlify.com/"&gt; &lt;img src="https://www.netlify.com/v3/img/components/full-logo-light.svg?sanitize=true" alt="Netlify" width="147" height="40" /&gt; &lt;/a&gt; 
&lt;p&gt;Thanks to &lt;a href="https://www.netlify.com/"&gt;Netlify&lt;/a&gt; for providing us with Deploy Previews!&lt;/p&gt; 
&lt;h2&gt;Sponsors&lt;/h2&gt; 
&lt;p&gt;Support this project by becoming a sponsor. Your logo will show up here with a link to your website. [&lt;a href="https://opencollective.com/bootstrap#sponsor"&gt;Become a sponsor&lt;/a&gt;]&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://opencollective.com/bootstrap/sponsor/0/website"&gt;&lt;img src="https://opencollective.com/bootstrap/sponsor/0/avatar.svg?sanitize=true" alt="OC sponsor 0" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/bootstrap/sponsor/1/website"&gt;&lt;img src="https://opencollective.com/bootstrap/sponsor/1/avatar.svg?sanitize=true" alt="OC sponsor 1" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/bootstrap/sponsor/2/website"&gt;&lt;img src="https://opencollective.com/bootstrap/sponsor/2/avatar.svg?sanitize=true" alt="OC sponsor 2" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/bootstrap/sponsor/3/website"&gt;&lt;img src="https://opencollective.com/bootstrap/sponsor/3/avatar.svg?sanitize=true" alt="OC sponsor 3" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/bootstrap/sponsor/4/website"&gt;&lt;img src="https://opencollective.com/bootstrap/sponsor/4/avatar.svg?sanitize=true" alt="OC sponsor 4" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/bootstrap/sponsor/5/website"&gt;&lt;img src="https://opencollective.com/bootstrap/sponsor/5/avatar.svg?sanitize=true" alt="OC sponsor 5" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/bootstrap/sponsor/6/website"&gt;&lt;img src="https://opencollective.com/bootstrap/sponsor/6/avatar.svg?sanitize=true" alt="OC sponsor 6" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/bootstrap/sponsor/7/website"&gt;&lt;img src="https://opencollective.com/bootstrap/sponsor/7/avatar.svg?sanitize=true" alt="OC sponsor 7" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/bootstrap/sponsor/8/website"&gt;&lt;img src="https://opencollective.com/bootstrap/sponsor/8/avatar.svg?sanitize=true" alt="OC sponsor 8" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/bootstrap/sponsor/9/website"&gt;&lt;img src="https://opencollective.com/bootstrap/sponsor/9/avatar.svg?sanitize=true" alt="OC sponsor 9" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Backers&lt;/h2&gt; 
&lt;p&gt;Thank you to all our backers! üôè [&lt;a href="https://opencollective.com/bootstrap#backer"&gt;Become a backer&lt;/a&gt;]&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://opencollective.com/bootstrap#backers"&gt;&lt;img src="https://opencollective.com/bootstrap/backers.svg?width=890" alt="Backers" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Copyright and license&lt;/h2&gt; 
&lt;p&gt;Code and documentation copyright 2011-2025 the &lt;a href="https://github.com/twbs/bootstrap/graphs/contributors"&gt;Bootstrap Authors&lt;/a&gt;. Code released under the &lt;a href="https://github.com/twbs/bootstrap/raw/main/LICENSE"&gt;MIT License&lt;/a&gt;. Docs released under &lt;a href="https://creativecommons.org/licenses/by/3.0/"&gt;Creative Commons&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>rustdesk/rustdesk</title>
      <link>https://github.com/rustdesk/rustdesk</link>
      <description>&lt;p&gt;An open-source remote desktop application designed for self-hosting, as an alternative to TeamViewer.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/rustdesk/rustdesk/master/res/logo-header.svg?sanitize=true" alt="RustDesk - Your remote desktop" /&gt;&lt;br /&gt; &lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/#raw-steps-to-build"&gt;Build&lt;/a&gt; ‚Ä¢ &lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/#how-to-build-with-docker"&gt;Docker&lt;/a&gt; ‚Ä¢ &lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/#file-structure"&gt;Structure&lt;/a&gt; ‚Ä¢ &lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/#snapshot"&gt;Snapshot&lt;/a&gt;&lt;br /&gt; [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-UA.md"&gt;–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-CS.md"&gt;ƒçesky&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-ZH.md"&gt;‰∏≠Êñá&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-HU.md"&gt;Magyar&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-ES.md"&gt;Espa√±ol&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-FA.md"&gt;ŸÅÿßÿ±ÿ≥€å&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-FR.md"&gt;Fran√ßais&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-DE.md"&gt;Deutsch&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-PL.md"&gt;Polski&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-ID.md"&gt;Indonesian&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-FI.md"&gt;Suomi&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-ML.md"&gt;‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡¥Ç&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-JP.md"&gt;Êó•Êú¨Ë™û&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-NL.md"&gt;Nederlands&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-IT.md"&gt;Italiano&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-RU.md"&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-PTBR.md"&gt;Portugu√™s (Brasil)&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-EO.md"&gt;Esperanto&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-KR.md"&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-AR.md"&gt;ÿßŸÑÿπÿ±ÿ®Ÿä&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-VN.md"&gt;Ti·∫øng Vi·ªát&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-DA.md"&gt;Dansk&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-GR.md"&gt;ŒïŒªŒªŒ∑ŒΩŒπŒ∫Œ¨&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-TR.md"&gt;T√ºrk√ße&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-NO.md"&gt;Norsk&lt;/a&gt;]&lt;br /&gt; &lt;b&gt;We need your help to translate this README, &lt;a href="https://github.com/rustdesk/rustdesk/tree/master/src/lang"&gt;RustDesk UI&lt;/a&gt; and &lt;a href="https://github.com/rustdesk/doc.rustdesk.com"&gt;RustDesk Doc&lt;/a&gt; to your native language&lt;/b&gt; &lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Caution] &lt;strong&gt;Misuse Disclaimer:&lt;/strong&gt; &lt;br /&gt; The developers of RustDesk do not condone or support any unethical or illegal use of this software. Misuse, such as unauthorized access, control or invasion of privacy, is strictly against our guidelines. The authors are not responsible for any misuse of the application.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Chat with us: &lt;a href="https://discord.gg/nDceKgxnkV"&gt;Discord&lt;/a&gt; | &lt;a href="https://twitter.com/rustdesk"&gt;Twitter&lt;/a&gt; | &lt;a href="https://www.reddit.com/r/rustdesk"&gt;Reddit&lt;/a&gt; | &lt;a href="https://www.youtube.com/@rustdesk"&gt;YouTube&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://rustdesk.com/pricing.html"&gt;&lt;img src="https://img.shields.io/badge/RustDesk%20Server%20Pro-Advanced%20Features-blue" alt="RustDesk Server Pro" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Yet another remote desktop solution, written in Rust. Works out of the box with no configuration required. You have full control of your data, with no concerns about security. You can use our rendezvous/relay server, &lt;a href="https://rustdesk.com/server"&gt;set up your own&lt;/a&gt;, or &lt;a href="https://github.com/rustdesk/rustdesk-server-demo"&gt;write your own rendezvous/relay server&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/71636191/171661982-430285f0-2e12-4b1d-9957-4a58e375304d.png" alt="image" /&gt;&lt;/p&gt; 
&lt;p&gt;RustDesk welcomes contribution from everyone. See &lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for help getting started.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/rustdesk/rustdesk/wiki/FAQ"&gt;&lt;strong&gt;FAQ&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/rustdesk/rustdesk/releases"&gt;&lt;strong&gt;BINARY DOWNLOAD&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/rustdesk/rustdesk/releases/tag/nightly"&gt;&lt;strong&gt;NIGHTLY BUILD&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://f-droid.org/en/packages/com.carriez.flutter_hbb"&gt;&lt;img src="https://f-droid.org/badge/get-it-on.png" alt="Get it on F-Droid" height="80" /&gt;&lt;/a&gt; &lt;a href="https://flathub.org/apps/com.rustdesk.RustDesk"&gt;&lt;img src="https://flathub.org/api/badge?svg&amp;amp;locale=en" alt="Get it on Flathub" height="80" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Dependencies&lt;/h2&gt; 
&lt;p&gt;Desktop versions use Flutter or Sciter (deprecated) for GUI, this tutorial is for Sciter only, since it is easier and more friendly to start. Check out our &lt;a href="https://github.com/rustdesk/rustdesk/raw/master/.github/workflows/flutter-build.yml"&gt;CI&lt;/a&gt; for building Flutter version.&lt;/p&gt; 
&lt;p&gt;Please download Sciter dynamic library yourself.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/c-smile/sciter-sdk/master/bin.win/x64/sciter.dll"&gt;Windows&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/c-smile/sciter-sdk/master/bin.lnx/x64/libsciter-gtk.so"&gt;Linux&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/c-smile/sciter-sdk/master/bin.osx/libsciter.dylib"&gt;macOS&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Raw Steps to build&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Prepare your Rust development env and C++ build env&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install &lt;a href="https://github.com/microsoft/vcpkg"&gt;vcpkg&lt;/a&gt;, and set &lt;code&gt;VCPKG_ROOT&lt;/code&gt; env variable correctly&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Windows: vcpkg install libvpx:x64-windows-static libyuv:x64-windows-static opus:x64-windows-static aom:x64-windows-static&lt;/li&gt; 
   &lt;li&gt;Linux/macOS: vcpkg install libvpx libyuv opus aom&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;run &lt;code&gt;cargo run&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;a href="https://rustdesk.com/docs/en/dev/build/"&gt;Build&lt;/a&gt;&lt;/h2&gt; 
&lt;h2&gt;How to Build on Linux&lt;/h2&gt; 
&lt;h3&gt;Ubuntu 18 (Debian 10)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;sudo apt install -y zip g++ gcc git curl wget nasm yasm libgtk-3-dev clang libxcb-randr0-dev libxdo-dev \
        libxfixes-dev libxcb-shape0-dev libxcb-xfixes0-dev libasound2-dev libpulse-dev cmake make \
        libclang-dev ninja-build libgstreamer1.0-dev libgstreamer-plugins-base1.0-dev libpam0g-dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;openSUSE Tumbleweed&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;sudo zypper install gcc-c++ git curl wget nasm yasm gcc gtk3-devel clang libxcb-devel libXfixes-devel cmake alsa-lib-devel gstreamer-devel gstreamer-plugins-base-devel xdotool-devel pam-devel
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Fedora 28 (CentOS 8)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;sudo yum -y install gcc-c++ git curl wget nasm yasm gcc gtk3-devel clang libxcb-devel libxdo-devel libXfixes-devel pulseaudio-libs-devel cmake alsa-lib-devel gstreamer1-devel gstreamer1-plugins-base-devel pam-devel
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Arch (Manjaro)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;sudo pacman -Syu --needed unzip git cmake gcc curl wget yasm nasm zip make pkg-config clang gtk3 xdotool libxcb libxfixes alsa-lib pipewire
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Install vcpkg&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;git clone https://github.com/microsoft/vcpkg
cd vcpkg
git checkout 2023.04.15
cd ..
vcpkg/bootstrap-vcpkg.sh
export VCPKG_ROOT=$HOME/vcpkg
vcpkg/vcpkg install libvpx libyuv opus aom
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Fix libvpx (For Fedora)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;cd vcpkg/buildtrees/libvpx/src
cd *
./configure
sed -i 's/CFLAGS+=-I/CFLAGS+=-fPIC -I/g' Makefile
sed -i 's/CXXFLAGS+=-I/CXXFLAGS+=-fPIC -I/g' Makefile
make
cp libvpx.a $HOME/vcpkg/installed/x64-linux/lib/
cd
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Build&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
source $HOME/.cargo/env
git clone --recurse-submodules https://github.com/rustdesk/rustdesk
cd rustdesk
mkdir -p target/debug
wget https://raw.githubusercontent.com/c-smile/sciter-sdk/master/bin.lnx/x64/libsciter-gtk.so
mv libsciter-gtk.so target/debug
VCPKG_ROOT=$HOME/vcpkg cargo run
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;How to build with Docker&lt;/h2&gt; 
&lt;p&gt;Begin by cloning the repository and building the Docker container:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;git clone https://github.com/rustdesk/rustdesk
cd rustdesk
git submodule update --init --recursive
docker build -t "rustdesk-builder" .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then, each time you need to build the application, run the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;docker run --rm -it -v $PWD:/home/user/rustdesk -v rustdesk-git-cache:/home/user/.cargo/git -v rustdesk-registry-cache:/home/user/.cargo/registry -e PUID="$(id -u)" -e PGID="$(id -g)" rustdesk-builder
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note that the first build may take longer before dependencies are cached, subsequent builds will be faster. Additionally, if you need to specify different arguments to the build command, you may do so at the end of the command in the &lt;code&gt;&amp;lt;OPTIONAL-ARGS&amp;gt;&lt;/code&gt; position. For instance, if you wanted to build an optimized release version, you would run the command above followed by &lt;code&gt;--release&lt;/code&gt;. The resulting executable will be available in the target folder on your system, and can be run with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;target/debug/rustdesk
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or, if you're running a release executable:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;target/release/rustdesk
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Please ensure that you run these commands from the root of the RustDesk repository, or the application may not find the required resources. Also note that other cargo subcommands such as &lt;code&gt;install&lt;/code&gt; or &lt;code&gt;run&lt;/code&gt; are not currently supported via this method as they would install or run the program inside the container instead of the host.&lt;/p&gt; 
&lt;h2&gt;File Structure&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rustdesk/rustdesk/tree/master/libs/hbb_common"&gt;libs/hbb_common&lt;/a&gt;&lt;/strong&gt;: video codec, config, tcp/udp wrapper, protobuf, fs functions for file transfer, and some other utility functions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rustdesk/rustdesk/tree/master/libs/scrap"&gt;libs/scrap&lt;/a&gt;&lt;/strong&gt;: screen capture&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rustdesk/rustdesk/tree/master/libs/enigo"&gt;libs/enigo&lt;/a&gt;&lt;/strong&gt;: platform specific keyboard/mouse control&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rustdesk/rustdesk/tree/master/libs/clipboard"&gt;libs/clipboard&lt;/a&gt;&lt;/strong&gt;: file copy and paste implementation for Windows, Linux, macOS.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rustdesk/rustdesk/tree/master/src/ui"&gt;src/ui&lt;/a&gt;&lt;/strong&gt;: obsolete Sciter UI (deprecated)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rustdesk/rustdesk/tree/master/src/server"&gt;src/server&lt;/a&gt;&lt;/strong&gt;: audio/clipboard/input/video services, and network connections&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rustdesk/rustdesk/tree/master/src/client.rs"&gt;src/client.rs&lt;/a&gt;&lt;/strong&gt;: start a peer connection&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rustdesk/rustdesk/tree/master/src/rendezvous_mediator.rs"&gt;src/rendezvous_mediator.rs&lt;/a&gt;&lt;/strong&gt;: Communicate with &lt;a href="https://github.com/rustdesk/rustdesk-server"&gt;rustdesk-server&lt;/a&gt;, wait for remote direct (TCP hole punching) or relayed connection&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rustdesk/rustdesk/tree/master/src/platform"&gt;src/platform&lt;/a&gt;&lt;/strong&gt;: platform specific code&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rustdesk/rustdesk/tree/master/flutter"&gt;flutter&lt;/a&gt;&lt;/strong&gt;: Flutter code for desktop and mobile&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rustdesk/rustdesk/tree/master/flutter/web/v1/js"&gt;flutter/web/js&lt;/a&gt;&lt;/strong&gt;: JavaScript for Flutter web client&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Screenshots&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://github.com/rustdesk/rustdesk/assets/28412477/db82d4e7-c4bc-4823-8e6f-6af7eadf7651" alt="Connection Manager" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/rustdesk/rustdesk/assets/28412477/9baa91e9-3362-4d06-aa1a-7518edcbd7ea" alt="Connected to a Windows PC" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/rustdesk/rustdesk/assets/28412477/39511ad3-aa9a-4f8c-8947-1cce286a46ad" alt="File Transfer" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/rustdesk/rustdesk/assets/28412477/78e8708f-e87e-4570-8373-1360033ea6c5" alt="TCP Tunneling" /&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>humanlayer/humanlayer</title>
      <link>https://github.com/humanlayer/humanlayer</link>
      <description>&lt;p&gt;HumanLayer enables AI agents to communicate with humans in tool-based and async workflows. Guarantee human oversight of high-stakes function calls with approval workflows across slack, email and more. Bring your LLM and Framework of choice and start giving your AI agents safe access to the world. Agentic Workflows, human in the loop, tool calling&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/humanlayer/humanlayer/main/docs/images/wordmark-light.svg?sanitize=true" alt="Wordmark Logo of HumanLayer" /&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;üöß &lt;strong&gt;HumanLayer&lt;/strong&gt; is undergoing some changes...stay tuned! üöß&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;h3&gt; &lt;p&gt;&lt;a href="https://humanlayer.dev/code"&gt;HumanLayer Code&lt;/a&gt; | &lt;a href="https://humanlayer.dev/discord"&gt;Discord&lt;/a&gt; | &lt;a href="https://github.com/humanlayer/humanlayer/releases"&gt;Release&lt;/a&gt;&lt;/p&gt; &lt;/h3&gt; 
 &lt;p&gt;&lt;a href="https://github.com/humanlayer/humanlayer"&gt;&lt;img src="https://img.shields.io/github/stars/humanlayer/humanlayer" alt="GitHub Repo stars" /&gt;&lt;/a&gt; &lt;a href="https://opensource.org/licenses/Apache-2"&gt;&lt;img src="https://img.shields.io/badge/License-Apache-green.svg?sanitize=true" alt="License: Apache-2" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;img referrerpolicy="no-referrer-when-downgrade" src="https://static.scarf.sh/a.png?x-pxid=fcfc0926-d841-47fb-b8a6-6aba3a6c3228" /&gt; 
&lt;/div&gt; 
&lt;h2&gt;Table of contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/humanlayer/humanlayer/main/#getting-started"&gt;Getting Started&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/humanlayer/humanlayer/main/#why-humanlayer"&gt;Why HumanLayer?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/humanlayer/humanlayer/main/#key-features"&gt;Key Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/humanlayer/humanlayer/main/#examples"&gt;Examples&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/humanlayer/humanlayer/main/#roadmap"&gt;Roadmap&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/humanlayer/humanlayer/main/#contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/humanlayer/humanlayer/main/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Why HumanLayer?&lt;/h2&gt; 
&lt;p&gt;Functions and tools are a key part of &lt;a href="https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance"&gt;Agentic Workflows&lt;/a&gt;. They enable LLMs to interact meaningfully with the outside world and automate broad scopes of impactful work. Correct and accurate function calling is essential for AI agents that do meaningful things like book appointments, interact with customers, manage billing information, write+execute code, and more.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://louis-dupont.medium.com/transforming-software-interactions-with-tool-calling-and-llms-dc39185247e9"&gt;&lt;img src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r8rEqjGZs_e6dibWeaqaQg.png" alt="Tool Calling Loop from Louis Dupont" /&gt;&lt;/a&gt; &lt;em&gt;From &lt;a href="https://louis-dupont.medium.com/transforming-software-interactions-with-tool-calling-and-llms-dc39185247e9"&gt;https://louis-dupont.medium.com/transforming-software-interactions-with-tool-calling-and-llms-dc39185247e9&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;However&lt;/strong&gt;, the most useful functions we can give to an LLM are also the most risky. We can all imagine the value of an AI Database Administrator that constantly tunes and refactors our SQL database, but most teams wouldn't give an LLM access to run arbitrary SQL statements against a production database (heck, we mostly don't even let humans do that). That is:&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;h3&gt;
  &lt;blockquote&gt;
   Even with state-of-the-art agentic reasoning and prompt routing, LLMs are not sufficiently reliable to be given access to high-stakes functions without human oversight
  &lt;/blockquote&gt;&lt;/h3&gt; 
&lt;/div&gt; 
&lt;p&gt;To better define what is meant by "high stakes", some examples:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Low Stakes&lt;/strong&gt;: Read Access to public data (e.g. search wikipedia, access public APIs and DataSets)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Low Stakes&lt;/strong&gt;: Communicate with agent author (e.g. an engineer might empower an agent to send them a private Slack message with updates on progress)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Medium Stakes&lt;/strong&gt;: Read Access to Private Data (e.g. read emails, access calendars, query a CRM)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Medium Stakes&lt;/strong&gt;: Communicate with strict rules (e.g. sending based on a specific sequence of hard-coded email templates)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;High Stakes&lt;/strong&gt;: Communicate on my Behalf or on behalf of my Company (e.g. send emails, post to slack, publish social/blog content)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;High Stakes&lt;/strong&gt;: Write Access to Private Data (e.g. update CRM records, modify feature toggles, update billing information)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center"&gt;
 &lt;img style="width: 600px" alt="Image showing the levels of function stakes stacked on top of one another" src="https://raw.githubusercontent.com/humanlayer/humanlayer/main/docs/images/function_stakes.png" /&gt;
&lt;/div&gt; 
&lt;p&gt;The high stakes functions are the ones that are the most valuable and promise the most impact in automating away human workflows. But they are also the ones where "90% accuracy" is not acceptable. Reliability is further impacted by today's LLMs' tendency to hallucinate or craft low-quality text that is clearly AI generated. The sooner teams can get Agents reliably and safely calling these tools with high-quality inputs, the sooner they can reap massive benefits.&lt;/p&gt; 
&lt;p&gt;HumanLayer provides a set of tools to &lt;em&gt;deterministically&lt;/em&gt; guarantee human oversight of high stakes function calls. Even if the LLM makes a mistake or hallucinates, HumanLayer is baked into the tool/function itself, guaranteeing a human in the loop.&lt;/p&gt; 
&lt;div align="center"&gt;
 &lt;img style="width: 400px" alt="HumanLayer @require_approval decorator wrapping the Commnicate on my behalf function" src="https://raw.githubusercontent.com/humanlayer/humanlayer/main/docs/images/humanlayer_require_approval.png" /&gt;
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;h3&gt;
  &lt;blockquote&gt;
    HumanLayer provides a set of tools to *deterministically* guarantee human oversight of high stakes function calls 
  &lt;/blockquote&gt;&lt;/h3&gt; 
&lt;/div&gt; 
&lt;h3&gt;The Future: Autonomous Agents and the "Outer Loop"&lt;/h3&gt; 
&lt;p&gt;&lt;em&gt;Read More: &lt;a href="https://theouterloop.substack.com/p/openais-realtime-api-is-a-step-towards"&gt;OpenAI's RealTime API is a step towards outer-loop agents&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;Between &lt;code&gt;require_approval&lt;/code&gt; and &lt;code&gt;human_as_tool&lt;/code&gt;, HumanLayer is built to empower the next generation of AI agents - Autonomous Agents, but it's just a piece of the puzzle. To clarify "next generation", we can summarize briefly the history of LLM applications.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Gen 1&lt;/strong&gt;: Chat - human-initiated question / response interface&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Gen 2&lt;/strong&gt;: Agentic Assistants - frameworks drive prompt routing, tool calling, chain of thought, and context window management to get much more reliability and functionality. Most workflows are initiated by humans in single-shot "here's a task, go do it" or rolling chat interfaces.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Gen 3&lt;/strong&gt;: Autonomous Agents - no longer human initiated, agents will live in the "outer loop" driving toward their goals using various tools and functions. Human/Agent communication is Agent-initiated rather than human-initiated.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/humanlayer/humanlayer/main/docs/images/gen-2-gen-3-agents.png" alt="gen2 vs gen 3 agents" /&gt;&lt;/p&gt; 
&lt;p&gt;Gen 3 autonomous agents will need ways to consult humans for input on various tasks. In order for these agents to perform actual useful work, they'll need human oversight for sensitive operations.&lt;/p&gt; 
&lt;p&gt;These agents will require ways to contact one or more humans across various channels including chat, email, sms, and more.&lt;/p&gt; 
&lt;p&gt;While early versions of these agents may technically be "human initiated" in that they get kicked off on a regular schedule by e.g. a cron or similar, the best ones will be managing their own scheduling and costs. This will require toolkits for inspecting costs and something akin to &lt;code&gt;sleep_until&lt;/code&gt;. They'll need to run in orchestration frameworks that can durably serialize and resume agent workflows across tool calls that might not return for hours or days. These frameworks will need to support context window management by a "manager LLM" and enable agents to fork sub-chains to handle specialized tasks and roles.&lt;/p&gt; 
&lt;p&gt;Example use cases for these outer loop agents include &lt;a href="https://raw.githubusercontent.com/humanlayer/humanlayer/main/examples/langchain/04-human_as_tool_linkedin.py"&gt;the linkedin inbox assistant&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/humanlayer/humanlayer/main/examples/langchain/05-approvals_and_humans_composite.py"&gt;the customer onboarding assistant&lt;/a&gt;, but that's really just scratching the surface.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;The HumanLayer SDK and docs are open-source and we welcome contributions in the form of issues, documentation, pull requests, and more. See &lt;a href="https://raw.githubusercontent.com/humanlayer/humanlayer/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Fun Stuff&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#humanlayer/humanlayer&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=humanlayer/humanlayer&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Development Conventions&lt;/h2&gt; 
&lt;h3&gt;TODO Annotations&lt;/h3&gt; 
&lt;p&gt;We use a priority-based TODO annotation system throughout the codebase:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;TODO(0)&lt;/code&gt;: Critical - never merge&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;TODO(1)&lt;/code&gt;: High - architectural flaws, major bugs&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;TODO(2)&lt;/code&gt;: Medium - minor bugs, missing features&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;TODO(3)&lt;/code&gt;: Low - polish, tests, documentation&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;TODO(4)&lt;/code&gt;: Questions/investigations needed&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;PERF&lt;/code&gt;: Performance optimization opportunities&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;The HumanLayer SDK and CodeLayer sources in this repo are licensed under the Apache 2 License.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>QuentinFuxa/WhisperLiveKit</title>
      <link>https://github.com/QuentinFuxa/WhisperLiveKit</link>
      <description>&lt;p&gt;Real-time &amp; local speech-to-text, translation, and speaker diarization. With server &amp; web UI.&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt;WhisperLiveKit&lt;/h1&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/QuentinFuxa/WhisperLiveKit/refs/heads/main/demo.png" alt="WhisperLiveKit Demo" width="730" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt;&lt;b&gt;Real-time, Fully Local Speech-to-Text with Speaker Identification&lt;/b&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://pypi.org/project/whisperlivekit/"&gt;&lt;img alt="PyPI Version" src="https://img.shields.io/pypi/v/whisperlivekit?color=g" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/whisperlivekit"&gt;&lt;img alt="PyPI Downloads" src="https://static.pepy.tech/personalized-badge/whisperlivekit?period=total&amp;amp;units=international_system&amp;amp;left_color=grey&amp;amp;right_color=brightgreen&amp;amp;left_text=installations" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/whisperlivekit/"&gt;&lt;img alt="Python Versions" src="https://img.shields.io/badge/python-3.9--3.13-dark_green" /&gt;&lt;/a&gt; &lt;a href="https://github.com/QuentinFuxa/WhisperLiveKit/raw/main/LICENSE"&gt;&lt;img alt="License" src="https://img.shields.io/badge/License-MIT/Dual Licensed-dark_green" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;Real-time speech transcription directly to your browser, with a ready-to-use backend+server and a simple frontend. ‚ú®&lt;/p&gt; 
&lt;h4&gt;Powered by Leading Research:&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ufal/SimulStreaming"&gt;SimulStreaming&lt;/a&gt; (SOTA 2025) - Ultra-low latency transcription with AlignAtt policy&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ufal/whisper_streaming"&gt;WhisperStreaming&lt;/a&gt; (SOTA 2023) - Low latency transcription with LocalAgreement policy&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2507.18446"&gt;Streaming Sortformer&lt;/a&gt; (SOTA 2025) - Advanced real-time speaker diarization&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/juanmc2005/diart"&gt;Diart&lt;/a&gt; (SOTA 2021) - Real-time speaker diarization&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/snakers4/silero-vad"&gt;Silero VAD&lt;/a&gt; (2024) - Enterprise-grade Voice Activity Detection&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Why not just run a simple Whisper model on every audio batch?&lt;/strong&gt; Whisper is designed for complete utterances, not real-time chunks. Processing small segments loses context, cuts off words mid-syllable, and produces poor transcription. WhisperLiveKit uses state-of-the-art simultaneous speech research for intelligent buffering and incremental processing.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Architecture&lt;/h3&gt; 
&lt;img alt="Architecture" src="https://raw.githubusercontent.com/QuentinFuxa/WhisperLiveKit/refs/heads/main/architecture.png" /&gt; 
&lt;p&gt;&lt;em&gt;The backend supports multiple concurrent users. Voice Activity Detection reduces overhead when no voice is detected.&lt;/em&gt;&lt;/p&gt; 
&lt;h3&gt;Installation &amp;amp; Quick Start&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install whisperlivekit
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;FFmpeg is required&lt;/strong&gt; and must be installed before using WhisperLiveKit&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;OS&lt;/th&gt; 
    &lt;th&gt;How to install&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Ubuntu/Debian&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;sudo apt install ffmpeg&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MacOS&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;brew install ffmpeg&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Windows&lt;/td&gt; 
    &lt;td&gt;Download .exe from &lt;a href="https://ffmpeg.org/download.html"&gt;https://ffmpeg.org/download.html&lt;/a&gt; and add to PATH&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Quick Start&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Start the transcription server:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;whisperlivekit-server --model base --language en
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Open your browser&lt;/strong&gt; and navigate to &lt;code&gt;http://localhost:8000&lt;/code&gt;. Start speaking and watch your words appear in real-time!&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;ul&gt; 
  &lt;li&gt;See &lt;a href="https://github.com/QuentinFuxa/WhisperLiveKit/raw/main/whisperlivekit/simul_whisper/whisper/tokenizer.py"&gt;tokenizer.py&lt;/a&gt; for the list of all available languages.&lt;/li&gt; 
  &lt;li&gt;For HTTPS requirements, see the &lt;strong&gt;Parameters&lt;/strong&gt; section for SSL configuration options.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Optional Dependencies&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Optional&lt;/th&gt; 
   &lt;th&gt;&lt;code&gt;pip install&lt;/code&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Speaker diarization with Sortformer&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;git+https://github.com/NVIDIA/NeMo.git@main#egg=nemo_toolkit[asr]&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Speaker diarization with Diart&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;diart&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Original Whisper backend&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;whisper&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Improved timestamps backend&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;whisper-timestamped&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Apple Silicon optimization backend&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;mlx-whisper&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenAI API backend&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;openai&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;See &lt;strong&gt;Parameters &amp;amp; Configuration&lt;/strong&gt; below on how to use them.&lt;/p&gt; 
&lt;h3&gt;Usage Examples&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Command-line Interface&lt;/strong&gt;: Start the transcription server with various options:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Use better model than default (small)
whisperlivekit-server --model large-v3

# Advanced configuration with diarization and language
whisperlivekit-server --host 0.0.0.0 --port 8000 --model medium --diarization --language fr
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Python API Integration&lt;/strong&gt;: Check &lt;a href="https://github.com/QuentinFuxa/WhisperLiveKit/raw/main/whisperlivekit/basic_server.py"&gt;basic_server&lt;/a&gt; for a more complete example of how to use the functions and classes.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from whisperlivekit import TranscriptionEngine, AudioProcessor, parse_args
from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.responses import HTMLResponse
from contextlib import asynccontextmanager
import asyncio

transcription_engine = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    global transcription_engine
    transcription_engine = TranscriptionEngine(model="medium", diarization=True, lan="en")
    yield

app = FastAPI(lifespan=lifespan)

async def handle_websocket_results(websocket: WebSocket, results_generator):
    async for response in results_generator:
        await websocket.send_json(response)
    await websocket.send_json({"type": "ready_to_stop"})

@app.websocket("/asr")
async def websocket_endpoint(websocket: WebSocket):
    global transcription_engine

    # Create a new AudioProcessor for each connection, passing the shared engine
    audio_processor = AudioProcessor(transcription_engine=transcription_engine)    
    results_generator = await audio_processor.create_tasks()
    results_task = asyncio.create_task(handle_websocket_results(websocket, results_generator))
    await websocket.accept()
    while True:
        message = await websocket.receive_bytes()
        await audio_processor.process_audio(message)        
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Frontend Implementation&lt;/strong&gt;: The package includes an HTML/JavaScript implementation &lt;a href="https://github.com/QuentinFuxa/WhisperLiveKit/raw/main/whisperlivekit/web/live_transcription.html"&gt;here&lt;/a&gt;. You can also import it using &lt;code&gt;from whisperlivekit import get_web_interface_html&lt;/code&gt; &amp;amp; &lt;code&gt;page = get_web_interface_html()&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;Parameters &amp;amp; Configuration&lt;/h2&gt; 
&lt;p&gt;An important list of parameters can be changed. But what &lt;em&gt;should&lt;/em&gt; you change?&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;the &lt;code&gt;--model&lt;/code&gt; size. List and recommandations &lt;a href="https://github.com/QuentinFuxa/WhisperLiveKit/raw/main/available_models.md"&gt;here&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;the &lt;code&gt;--language&lt;/code&gt;. List &lt;a href="https://github.com/QuentinFuxa/WhisperLiveKit/raw/main/whisperlivekit/simul_whisper/whisper/tokenizer.py"&gt;here&lt;/a&gt;. If you use &lt;code&gt;auto&lt;/code&gt;, the model attempts to detect the language automatically, but it tends to bias towards English.&lt;/li&gt; 
 &lt;li&gt;the &lt;code&gt;--backend&lt;/code&gt; ? you can switch to &lt;code&gt;--backend faster-whisper&lt;/code&gt; if &lt;code&gt;simulstreaming&lt;/code&gt; does not work correctly or if you prefer to avoid the dual-license requirements.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--warmup-file&lt;/code&gt;, if you have one&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--host&lt;/code&gt;, &lt;code&gt;--port&lt;/code&gt;, &lt;code&gt;--ssl-certfile&lt;/code&gt;, &lt;code&gt;--ssl-keyfile&lt;/code&gt;, if you set up a server&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--diarization&lt;/code&gt;, if you want to use it.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The rest I don't recommend. But below are your options.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Parameter&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Default&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--model&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Whisper model size.&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;small&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--language&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Source language code or &lt;code&gt;auto&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;auto&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--task&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;transcribe&lt;/code&gt; or &lt;code&gt;translate&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;transcribe&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--backend&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Processing backend&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;simulstreaming&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--min-chunk-size&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Minimum audio chunk size (seconds)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;1.0&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--no-vac&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Disable Voice Activity Controller&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--no-vad&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Disable Voice Activity Detection&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--warmup-file&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Audio file path for model warmup&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;jfk.wav&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--host&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Server host address&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;localhost&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--port&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Server port&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;8000&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--ssl-certfile&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Path to the SSL certificate file (for HTTPS support)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--ssl-keyfile&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Path to the SSL private key file (for HTTPS support)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;WhisperStreaming backend options&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Default&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--confidence-validation&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Use confidence scores for faster validation&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--buffer_trimming&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Buffer trimming strategy (&lt;code&gt;sentence&lt;/code&gt; or &lt;code&gt;segment&lt;/code&gt;)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;segment&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;SimulStreaming backend options&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Default&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--frame-threshold&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;AlignAtt frame threshold (lower = faster, higher = more accurate)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;25&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--beams&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Number of beams for beam search (1 = greedy decoding)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;1&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--decoder&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Force decoder type (&lt;code&gt;beam&lt;/code&gt; or &lt;code&gt;greedy&lt;/code&gt;)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;auto&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--audio-max-len&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Maximum audio buffer length (seconds)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;30.0&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--audio-min-len&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Minimum audio length to process (seconds)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;0.0&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--cif-ckpt-path&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Path to CIF model for word boundary detection&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--never-fire&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Never truncate incomplete words&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--init-prompt&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Initial prompt for the model&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--static-init-prompt&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Static prompt that doesn't scroll&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--max-context-tokens&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Maximum context tokens&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--model-path&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Direct path to .pt model file. Download it if not found&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;./base.pt&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--preloaded-model-count&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Optional. Number of models to preload in memory to speed up loading (set up to the expected number of concurrent users)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;1&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Diarization options&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Default&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--diarization&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Enable speaker identification&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--diarization-backend&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;diart&lt;/code&gt; or &lt;code&gt;sortformer&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;sortformer&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--segmentation-model&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Hugging Face model ID for Diart segmentation model. &lt;a href="https://github.com/juanmc2005/diart/tree/main?tab=readme-ov-file#pre-trained-models"&gt;Available models&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pyannote/segmentation-3.0&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--embedding-model&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Hugging Face model ID for Diart embedding model. &lt;a href="https://github.com/juanmc2005/diart/tree/main?tab=readme-ov-file#pre-trained-models"&gt;Available models&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;speechbrain/spkrec-ecapa-voxceleb&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;For diarization using Diart, you need access to pyannote.audio models:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/pyannote/segmentation"&gt;Accept user conditions&lt;/a&gt; for the &lt;code&gt;pyannote/segmentation&lt;/code&gt; model&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/pyannote/segmentation-3.0"&gt;Accept user conditions&lt;/a&gt; for the &lt;code&gt;pyannote/segmentation-3.0&lt;/code&gt; model&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/pyannote/embedding"&gt;Accept user conditions&lt;/a&gt; for the &lt;code&gt;pyannote/embedding&lt;/code&gt; model&lt;/li&gt; 
  &lt;li&gt;Login with HuggingFace: &lt;code&gt;huggingface-cli login&lt;/code&gt;&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;üöÄ Deployment Guide&lt;/h3&gt; 
&lt;p&gt;To deploy WhisperLiveKit in production:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Server Setup&lt;/strong&gt;: Install production ASGI server &amp;amp; launch with multiple workers&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install uvicorn gunicorn
gunicorn -k uvicorn.workers.UvicornWorker -w 4 your_app:app
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Frontend&lt;/strong&gt;: Host your customized version of the &lt;code&gt;html&lt;/code&gt; example &amp;amp; ensure WebSocket connection points correctly&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Nginx Configuration&lt;/strong&gt; (recommended for production):&lt;/p&gt; &lt;pre&gt;&lt;code class="language-nginx"&gt;server {
   listen 80;
   server_name your-domain.com;
    location / {
        proxy_pass http://localhost:8000;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        proxy_set_header Host $host;
}}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;HTTPS Support&lt;/strong&gt;: For secure deployments, use "wss://" instead of "ws://" in WebSocket URL&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;üêã Docker&lt;/h2&gt; 
&lt;p&gt;Deploy the application easily using Docker with GPU or CPU support.&lt;/p&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Docker installed on your system&lt;/li&gt; 
 &lt;li&gt;For GPU support: NVIDIA Docker runtime installed&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Quick Start&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;With GPU acceleration (recommended):&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker build -t wlk .
docker run --gpus all -p 8000:8000 --name wlk wlk
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;CPU only:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker build -f Dockerfile.cpu -t wlk .
docker run -p 8000:8000 --name wlk wlk
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Advanced Usage&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Custom configuration:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Example with custom model and language
docker run --gpus all -p 8000:8000 --name wlk wlk --model large-v3 --language fr
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Memory Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Large models&lt;/strong&gt;: Ensure your Docker runtime has sufficient memory allocated&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Customization&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--build-arg&lt;/code&gt; Options: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;EXTRAS="whisper-timestamped"&lt;/code&gt; - Add extras to the image's installation (no spaces). Remember to set necessary container options!&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;HF_PRECACHE_DIR="./.cache/"&lt;/code&gt; - Pre-load a model cache for faster first-time start&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;HF_TKN_FILE="./token"&lt;/code&gt; - Add your Hugging Face Hub access token to download gated models&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üîÆ Use Cases&lt;/h2&gt; 
&lt;p&gt;Capture discussions in real-time for meeting transcription, help hearing-impaired users follow conversations through accessibility tools, transcribe podcasts or videos automatically for content creation, transcribe support calls with speaker identification for customer service...&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>TheAlgorithms/Python</title>
      <link>https://github.com/TheAlgorithms/Python</link>
      <description>&lt;p&gt;All Algorithms implemented in Python&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;!-- Title: --&gt; 
 &lt;a href="https://github.com/TheAlgorithms/"&gt; &lt;img src="https://raw.githubusercontent.com/TheAlgorithms/website/1cd824df116b27029f17c2d1b42d81731f28a920/public/logo.svg?sanitize=true" height="100" /&gt; &lt;/a&gt; 
 &lt;h1&gt;&lt;a href="https://github.com/TheAlgorithms/"&gt;The Algorithms&lt;/a&gt; - Python&lt;/h1&gt; 
 &lt;!-- Labels: --&gt; 
 &lt;!-- First row: --&gt; 
 &lt;a href="https://gitpod.io/#https://github.com/TheAlgorithms/Python"&gt; &lt;img src="https://img.shields.io/badge/Gitpod-Ready--to--Code-blue?logo=gitpod&amp;amp;style=flat-square" height="20" alt="Gitpod Ready-to-Code" /&gt; &lt;/a&gt; 
 &lt;a href="https://github.com/TheAlgorithms/Python/raw/master/CONTRIBUTING.md"&gt; &lt;img src="https://img.shields.io/static/v1.svg?label=Contributions&amp;amp;message=Welcome&amp;amp;color=0059b3&amp;amp;style=flat-square" height="20" alt="Contributions Welcome" /&gt; &lt;/a&gt; 
 &lt;img src="https://img.shields.io/github/repo-size/TheAlgorithms/Python.svg?label=Repo%20size&amp;amp;style=flat-square" height="20" /&gt; 
 &lt;a href="https://the-algorithms.com/discord"&gt; &lt;img src="https://img.shields.io/discord/808045925556682782.svg?logo=discord&amp;amp;colorB=7289DA&amp;amp;style=flat-square" height="20" alt="Discord chat" /&gt; &lt;/a&gt; 
 &lt;a href="https://gitter.im/TheAlgorithms/community"&gt; &lt;img src="https://img.shields.io/badge/Chat-Gitter-ff69b4.svg?label=Chat&amp;amp;logo=gitter&amp;amp;style=flat-square" height="20" alt="Gitter chat" /&gt; &lt;/a&gt; 
 &lt;!-- Second row: --&gt; 
 &lt;br /&gt; 
 &lt;a href="https://github.com/TheAlgorithms/Python/actions"&gt; &lt;img src="https://img.shields.io/github/actions/workflow/status/TheAlgorithms/Python/build.yml?branch=master&amp;amp;label=CI&amp;amp;logo=github&amp;amp;style=flat-square" height="20" alt="GitHub Workflow Status" /&gt; &lt;/a&gt; 
 &lt;a href="https://github.com/pre-commit/pre-commit"&gt; &lt;img src="https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&amp;amp;logoColor=white&amp;amp;style=flat-square" height="20" alt="pre-commit" /&gt; &lt;/a&gt; 
 &lt;a href="https://docs.astral.sh/ruff/formatter/"&gt; &lt;img src="https://img.shields.io/static/v1?label=code%20style&amp;amp;message=ruff&amp;amp;color=black&amp;amp;style=flat-square" height="20" alt="code style: black" /&gt; &lt;/a&gt; 
 &lt;!-- Short description: --&gt; 
 &lt;h3&gt;All algorithms implemented in Python - for education&lt;/h3&gt; 
&lt;/div&gt; 
&lt;p&gt;Implementations are for learning purposes only. They may be less efficient than the implementations in the Python standard library. Use them at your discretion.&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;Read through our &lt;a href="https://raw.githubusercontent.com/TheAlgorithms/Python/master/CONTRIBUTING.md"&gt;Contribution Guidelines&lt;/a&gt; before you contribute.&lt;/p&gt; 
&lt;h2&gt;Community Channels&lt;/h2&gt; 
&lt;p&gt;We are on &lt;a href="https://the-algorithms.com/discord"&gt;Discord&lt;/a&gt; and &lt;a href="https://gitter.im/TheAlgorithms/community"&gt;Gitter&lt;/a&gt;! Community channels are a great way for you to ask questions and get help. Please join us!&lt;/p&gt; 
&lt;h2&gt;List of Algorithms&lt;/h2&gt; 
&lt;p&gt;See our &lt;a href="https://raw.githubusercontent.com/TheAlgorithms/Python/master/DIRECTORY.md"&gt;directory&lt;/a&gt; for easier navigation and a better overview of the project.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>nats-io/nats-server</title>
      <link>https://github.com/nats-io/nats-server</link>
      <description>&lt;p&gt;High-Performance server for NATS.io, the cloud and edge native messaging system.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/nats-io/nats-server/main/logos/nats-horizontal-color.png" width="300" alt="NATS Logo" /&gt; &lt;/p&gt; 
&lt;p&gt;&lt;a href="https://nats.io"&gt;NATS&lt;/a&gt; is a simple, secure and performant communications system for digital systems, services and devices. NATS is part of the Cloud Native Computing Foundation (&lt;a href="https://cncf.io"&gt;CNCF&lt;/a&gt;). NATS has over &lt;a href="https://nats.io/download/"&gt;40 client language implementations&lt;/a&gt;, and its server can run on-premise, in the cloud, at the edge, and even on a Raspberry Pi. NATS can secure and simplify design and operation of modern distributed systems.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.apache.org/licenses/LICENSE-2.0"&gt;&lt;img src="https://img.shields.io/badge/License-Apache2-blue.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://github.com/nats-io/nats-server/actions/workflows/tests.yaml"&gt;&lt;img src="https://github.com/nats-io/nats-server/actions/workflows/tests.yaml/badge.svg?branch=main" alt="Build" /&gt;&lt;/a&gt; &lt;a href="https://github.com/nats-io/nats-server/releases/latest"&gt;&lt;img src="https://img.shields.io/github/v/release/nats-io/nats-server" alt="Release" /&gt;&lt;/a&gt; &lt;a href="https://slack.nats.io"&gt;&lt;img src="https://img.shields.io/badge/chat-on%20slack-green" alt="Slack" /&gt;&lt;/a&gt; &lt;a href="https://coveralls.io/r/nats-io/nats-server?branch=main"&gt;&lt;img src="https://coveralls.io/repos/github/nats-io/nats-server/badge.svg?branch=main" alt="Coverage" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/_/nats"&gt;&lt;img src="https://img.shields.io/docker/pulls/_/nats.svg?sanitize=true" alt="Docker Downloads" /&gt;&lt;/a&gt; &lt;a href="https://somsubhra.github.io/github-release-stats/?username=nats-io&amp;amp;repository=nats-server"&gt;&lt;img src="https://img.shields.io/github/downloads/nats-io/nats-server/total.svg?logo=github" alt="GitHub Downloads" /&gt;&lt;/a&gt; &lt;a href="https://bestpractices.coreinfrastructure.org/projects/1895"&gt;&lt;img src="https://bestpractices.coreinfrastructure.org/projects/1895/badge" alt="CII Best Practices" /&gt;&lt;/a&gt; &lt;a href="https://artifacthub.io/packages/helm/nats/nats"&gt;&lt;img src="https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/nats" alt="Artifact Hub" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://nats.io"&gt;Official Website&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.nats.io"&gt;Official Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.nats.io/reference/faq"&gt;FAQ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Watch &lt;a href="https://rethink.synadia.com/episodes/1/"&gt;a video overview&lt;/a&gt; of NATS.&lt;/li&gt; 
 &lt;li&gt;Watch &lt;a href="https://www.youtube.com/watch?v=sm63oAVPqAM"&gt;this video from SCALE 13x&lt;/a&gt; to learn more about its origin story and design philosophy.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contact&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://twitter.com/nats_io"&gt;Twitter&lt;/a&gt;: Follow us on Twitter!&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://groups.google.com/forum/#!forum/natsio"&gt;Google Groups&lt;/a&gt;: Where you can ask questions&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://natsio.slack.com"&gt;Slack&lt;/a&gt;: Click &lt;a href="https://slack.nats.io"&gt;here&lt;/a&gt; to join. You can ask questions to our maintainers and to the rich and active community.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;If you are interested in contributing to NATS, read about our...&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nats-io/nats-server/main/CONTRIBUTING.md"&gt;Contributing guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/nats-io"&gt;Report issues or propose Pull Requests&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Roadmap&lt;/h2&gt; 
&lt;p&gt;The NATS product roadmap can be found &lt;a href="https://nats.io/about/#roadmap"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Adopters&lt;/h2&gt; 
&lt;p&gt;Who uses NATS? See our &lt;a href="https://nats.io/#who-uses-nats"&gt;list of users&lt;/a&gt; on &lt;a href="https://nats.io"&gt;https://nats.io&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Security&lt;/h2&gt; 
&lt;h3&gt;Security Audit&lt;/h3&gt; 
&lt;p&gt;A third party security audit was performed by Trail of Bits following engagement by the Open Source Technology Improvement Fund (OSTIF). You can see the &lt;a href="https://github.com/trailofbits/publications/raw/master/reviews/2025-04-ostif-nats-securityreview.pdf"&gt;full report from April 2025 here&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Reporting Security Vulnerabilities&lt;/h3&gt; 
&lt;p&gt;If you've found a vulnerability or a potential vulnerability in the NATS server, please let us know at &lt;a href="mailto:security@nats.io"&gt;nats-security&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Unless otherwise noted, the NATS source files are distributed under the Apache Version 2.0 license found in the LICENSE file.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Canner/WrenAI</title>
      <link>https://github.com/Canner/WrenAI</link>
      <description>&lt;p&gt;‚ö°Ô∏è GenBI (Generative BI) queries any database in natural language, generates accurate SQL (Text-to-SQL), charts (Text-to-Chart), and AI-powered insights in seconds.&lt;/p&gt;&lt;hr&gt;&lt;p align="center" id="top"&gt; &lt;a href="https://getwren.ai/?utm_source=github&amp;amp;utm_medium=title&amp;amp;utm_campaign=readme"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: light)" srcset="./misc/wrenai_logo.png" /&gt; 
   &lt;img src="https://raw.githubusercontent.com/Canner/WrenAI/main/misc/wrenai_logo_white.png" width="300px" /&gt; 
  &lt;/picture&gt; &lt;/a&gt;&lt;/p&gt;
&lt;h1 align="center"&gt;&lt;a href="https://getwren.ai/?utm_source=github&amp;amp;utm_medium=title&amp;amp;utm_campaign=readme"&gt;Wren AI - Open-Source GenBI Agent&lt;/a&gt;&lt;/h1&gt;
&lt;a href="https://getwren.ai/?utm_source=github&amp;amp;utm_medium=title&amp;amp;utm_campaign=readme"&gt; &lt;/a&gt; 
&lt;p&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a aria-label="Follow us on X" href="https://x.com/getwrenai"&gt; &lt;img alt="" src="https://img.shields.io/badge/-@getwrenai-blue?style=for-the-badge&amp;amp;logo=x&amp;amp;logoColor=white&amp;amp;labelColor=gray&amp;amp;logoWidth=20" /&gt; &lt;/a&gt; &lt;a aria-label="Releases" href="https://github.com/canner/WrenAI/releases"&gt; &lt;img alt="" src="https://img.shields.io/github/v/release/canner/WrenAI?logo=github&amp;amp;label=GitHub%20Release&amp;amp;color=blue&amp;amp;style=for-the-badge" /&gt; &lt;/a&gt; &lt;a aria-label="License" href="https://github.com/Canner/WrenAI/raw/main/LICENSE"&gt; &lt;img alt="" src="https://img.shields.io/github/license/canner/WrenAI?color=blue&amp;amp;style=for-the-badge" /&gt; &lt;/a&gt; &lt;a href="https://docs.getwren.ai"&gt; &lt;img src="https://img.shields.io/badge/docs-online-brightgreen?style=for-the-badge" alt="Docs" /&gt; &lt;/a&gt; &lt;a aria-label="Join the community on GitHub" href="https://discord.gg/5DvshJqG8Z"&gt; &lt;img alt="" src="https://img.shields.io/badge/-JOIN%20THE%20COMMUNITY-blue?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white&amp;amp;labelColor=grey&amp;amp;logoWidth=20" /&gt; &lt;/a&gt; &lt;a aria-label="Canner" href="https://cannerdata.com/?utm_source=github&amp;amp;utm_medium=badge&amp;amp;utm_campaign=readme"&gt; &lt;img src="https://img.shields.io/badge/%F0%9F%A7%A1-Made%20by%20Canner-blue?style=for-the-badge" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/9263" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/9263" alt="Canner%2FWrenAI | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Wren AI is your GenBI Agent, that you can query any database with natural language ‚Üí get accurate SQL(Text-to-SQL), charts(Text-to-Charts) &amp;amp; AI-generated insights in seconds. ‚ö°Ô∏è&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Canner/WrenAI/main/misc/workflow.png" /&gt; &lt;/p&gt; 
&lt;h2&gt;üòç Demos&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/f9c1cb34-5a95-4580-8890-ec9644da4160"&gt;https://github.com/user-attachments/assets/f9c1cb34-5a95-4580-8890-ec9644da4160&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/90ad1d35-bb1e-490b-9676-b29863ff090b"&gt;Watch GenBI Demo&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ü§ñ Features&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;What you get&lt;/th&gt; 
   &lt;th&gt;Why it matters&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Talk to Your Data&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Ask in any language ‚Üí precise SQL &amp;amp; answers&lt;/td&gt; 
   &lt;td&gt;Slash the SQL learning curveÔªø&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;GenBI Insights&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;AI-written summaries, charts &amp;amp; reports&lt;/td&gt; 
   &lt;td&gt;Decision-ready context in one clickÔªø&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Semantic Layer&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;MDL models encode schema, metrics, joins&lt;/td&gt; 
   &lt;td&gt;Keeps LLM outputs accurate &amp;amp; governedÔªø&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Embed via API&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Generate queries &amp;amp; charts inside your apps (&lt;a href="https://wrenai.readme.io/reference/cloud-getting-started"&gt;API Docs&lt;/a&gt;)&lt;/td&gt; 
   &lt;td&gt;Build custom agents, SaaS features, chatbotsÔªø (&lt;a href="https://huggingface.co/spaces/getWrenAI/wrenai-cloud-api-demo"&gt;Streamlit Live Demo&lt;/a&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;ü§© &lt;a href="https://getwren.ai/genbi?utm_source=github&amp;amp;utm_medium=content&amp;amp;utm_campaign=readme"&gt;Learn more about GenBI&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üöÄ Getting Started&lt;/h2&gt; 
&lt;p&gt;Using Wren AI is super simple, you can set it up within 3 minutes, and start to interact with your data!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Visit our &lt;a href="http://docs.getwren.ai/oss/installation?utm_source=github&amp;amp;utm_medium=content&amp;amp;utm_campaign=readme"&gt;Install in your local environment&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Visit the &lt;a href="https://docs.getwren.ai/oss/guide/connect/overview?utm_source=github&amp;amp;utm_medium=content&amp;amp;utm_campaign=readme"&gt;Usage Guides&lt;/a&gt; to learn more about how to use Wren AI.&lt;/li&gt; 
 &lt;li&gt;Or just start with &lt;a href="https://getwren.ai/?utm_source=github&amp;amp;utm_medium=content&amp;amp;utm_campaign=readme"&gt;Wren AI Cloud&lt;/a&gt; our Managed Cloud Service. (&lt;a href="https://docs.getwren.ai/oss/overview/cloud_vs_self_host"&gt;OSS vs. Commercial Plans&lt;/a&gt;).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üèóÔ∏è Architecture&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Canner/WrenAI/main/misc/how_wrenai_works.png" /&gt; &lt;/p&gt; 
&lt;p&gt;üëâ &lt;a href="https://getwren.ai/post/how-we-design-our-semantic-engine-for-llms-the-backbone-of-the-semantic-layer-for-llm-architecture?utm_source=github&amp;amp;utm_medium=content&amp;amp;utm_campaign=readme"&gt;Learn more about our Design&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üîå Data Sources&lt;/h2&gt; 
&lt;p&gt;If your data source is not listed here, vote for it in our &lt;a href="https://github.com/Canner/WrenAI/discussions/327"&gt;GitHub discussion thread&lt;/a&gt;. It will be a valuable input for us to decide on the next supported data sources.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Athena (Trino)&lt;/li&gt; 
 &lt;li&gt;Redshift&lt;/li&gt; 
 &lt;li&gt;BigQuery&lt;/li&gt; 
 &lt;li&gt;DuckDB&lt;/li&gt; 
 &lt;li&gt;PostgreSQL&lt;/li&gt; 
 &lt;li&gt;MySQL&lt;/li&gt; 
 &lt;li&gt;Microsoft SQL Server&lt;/li&gt; 
 &lt;li&gt;ClickHouse&lt;/li&gt; 
 &lt;li&gt;Oracle&lt;/li&gt; 
 &lt;li&gt;Trino&lt;/li&gt; 
 &lt;li&gt;Snowflake&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ü§ñ LLM Models&lt;/h2&gt; 
&lt;p&gt;Wren AI supports integration with various Large Language Models (LLMs), including but not limited to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;OpenAI Models&lt;/li&gt; 
 &lt;li&gt;Azure OpenAI Models&lt;/li&gt; 
 &lt;li&gt;DeepSeek Models&lt;/li&gt; 
 &lt;li&gt;Google AI Studio ‚Äì Gemini Models&lt;/li&gt; 
 &lt;li&gt;Vertex AI Models (Gemini + Anthropic)&lt;/li&gt; 
 &lt;li&gt;Bedrock Models&lt;/li&gt; 
 &lt;li&gt;Anthropic API Models&lt;/li&gt; 
 &lt;li&gt;Groq Models&lt;/li&gt; 
 &lt;li&gt;Ollama Models&lt;/li&gt; 
 &lt;li&gt;Databricks Models&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Check &lt;a href="https://github.com/Canner/WrenAI/tree/main/wren-ai-service/docs/config_examples"&gt;configuration examples here&lt;/a&gt;!&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!CAUTION] The performance of Wren AI depends significantly on the capabilities of the LLM you choose. We strongly recommend using the most powerful model available for optimal results. Using less capable models may lead to reduced performance, slower response times, or inaccurate outputs.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;üìö Documentation&lt;/h2&gt; 
&lt;p&gt;Visit &lt;a href="https://docs.getwren.ai/oss/overview/introduction?utm_source=github&amp;amp;utm_medium=content&amp;amp;utm_campaign=readme"&gt;Wren AI documentation&lt;/a&gt; to view the full documentation.&lt;/p&gt; 
&lt;h2&gt;üì™ Keep Posted?&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.getwren.ai/blog/?utm_source=github&amp;amp;utm_medium=content&amp;amp;utm_campaign=readme"&gt;Subscribe our blog&lt;/a&gt; and &lt;a href="https://www.linkedin.com/company/wrenai"&gt;Follow our LinkedIn&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üõ†Ô∏è Contribution&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Star ‚≠ê the repo to show support (it really helps).&lt;/li&gt; 
 &lt;li&gt;Open an issue for bugs, ideas, or discussions.&lt;/li&gt; 
 &lt;li&gt;Read &lt;a href="https://github.com/Canner/WrenAI/raw/main/CONTRIBUTING.md"&gt;Contribution Guidelines&lt;/a&gt; for setup &amp;amp; PR guidelines.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;‚≠êÔ∏è Community&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Join 1.3k+ developers in our &lt;a href="https://discord.gg/5DvshJqG8Z"&gt;Discord&lt;/a&gt; for real-time help and roadmap previews.&lt;/li&gt; 
 &lt;li&gt;If there are any issues, please visit &lt;a href="https://github.com/Canner/WrenAI/issues"&gt;GitHub Issues&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Explore our &lt;a href="https://wrenai.notion.site/"&gt;public roadmap&lt;/a&gt; to stay updated on upcoming features and improvements!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Please note that our &lt;a href="https://raw.githubusercontent.com/Canner/WrenAI/main/CODE_OF_CONDUCT.md"&gt;Code of Conduct&lt;/a&gt; applies to all Wren AI community channels. Users are &lt;strong&gt;highly encouraged&lt;/strong&gt; to read and adhere to them to avoid repercussions.&lt;/p&gt; 
&lt;h2&gt;üéâ Our Contributors&lt;/h2&gt; 
&lt;a href="https://github.com/canner/wrenAI/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=Canner/WrenAI" /&gt; &lt;/a&gt; 
&lt;p align="right"&gt; &lt;a href="https://raw.githubusercontent.com/Canner/WrenAI/main/#top"&gt;‚¨ÜÔ∏è Back to Top&lt;/a&gt; &lt;/p&gt;</description>
    </item>
    
    <item>
      <title>asgeirtj/system_prompts_leaks</title>
      <link>https://github.com/asgeirtj/system_prompts_leaks</link>
      <description>&lt;p&gt;Collection of extracted System Prompts from popular chatbots like ChatGPT, Claude &amp; Gemini&lt;/p&gt;&lt;hr&gt;&lt;p&gt;NEW: 23 Aug 2025 &lt;a href="https://github.com/asgeirtj/system_prompts_leaks/raw/main/OpenAI/gpt-5-thinking.md"&gt;OpenAI/gpt-5-thinking.md &lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;System Prompts Leaks&lt;/h1&gt; 
&lt;p&gt;Collection of system message instructions for various publicly deployed chatbots.&lt;/p&gt; 
&lt;p&gt;Feel free to do PR's.&lt;/p&gt; 
&lt;p&gt;Please use discussions tabs for discussions not the Issues tab.&lt;/p&gt; 
&lt;p&gt;Discord username: asgeirtj&lt;br /&gt; X profile: &lt;a href="https://x.com/asgeirtj"&gt;https://x.com/asgeirtj&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#asgeirtj/system_prompts_leaks&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=asgeirtj/system_prompts_leaks&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>spf13/cobra</title>
      <link>https://github.com/spf13/cobra</link>
      <description>&lt;p&gt;A Commander for modern Go CLI interactions&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;a href="https://cobra.dev"&gt; &lt;img width="512" height="535" alt="cobra-logo" src="https://github.com/user-attachments/assets/c8bf9aad-b5ae-41d3-8899-d83baec10af8" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;p&gt;Cobra is a library for creating powerful modern CLI applications.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://cobra.dev"&gt;Visit Cobra.dev for extensive documentation&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Cobra is used in many Go projects such as &lt;a href="https://kubernetes.io/"&gt;Kubernetes&lt;/a&gt;, &lt;a href="https://gohugo.io"&gt;Hugo&lt;/a&gt;, and &lt;a href="https://github.com/cli/cli"&gt;GitHub CLI&lt;/a&gt; to name a few. &lt;a href="https://raw.githubusercontent.com/spf13/cobra/main/site/content/projects_using_cobra.md"&gt;This list&lt;/a&gt; contains a more extensive list of projects using Cobra.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/spf13/cobra/actions?query=workflow%3ATest"&gt;&lt;img src="https://img.shields.io/github/actions/workflow/status/spf13/cobra/test.yml?branch=main&amp;amp;longCache=true&amp;amp;label=Test&amp;amp;logo=github%20actions&amp;amp;logoColor=fff" alt="" /&gt;&lt;/a&gt; &lt;a href="https://pkg.go.dev/github.com/spf13/cobra"&gt;&lt;img src="https://pkg.go.dev/badge/github.com/spf13/cobra.svg?sanitize=true" alt="Go Reference" /&gt;&lt;/a&gt; &lt;a href="https://goreportcard.com/report/github.com/spf13/cobra"&gt;&lt;img src="https://goreportcard.com/badge/github.com/spf13/cobra" alt="Go Report Card" /&gt;&lt;/a&gt; &lt;a href="https://gophers.slack.com/archives/CD3LP1199"&gt;&lt;img src="https://img.shields.io/badge/Slack-cobra-brightgreen" alt="Slack" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;div align="center" markdown="1"&gt; 
 &lt;sup&gt;Supported by:&lt;/sup&gt; 
 &lt;br /&gt; 
 &lt;br /&gt; 
 &lt;a href="https://www.warp.dev/cobra"&gt; &lt;img alt="Warp sponsorship" width="400" src="https://github.com/user-attachments/assets/ab8dd143-b0fd-4904-bdc5-dd7ecac94eae" /&gt; &lt;/a&gt; 
 &lt;h3&gt;&lt;a href="https://www.warp.dev/cobra"&gt;Warp, the AI terminal for devs&lt;/a&gt;&lt;/h3&gt; 
 &lt;p&gt;&lt;a href="https://www.warp.dev/cobra"&gt;Try Cobra in Warp today&lt;/a&gt;&lt;br /&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h1&gt;Overview&lt;/h1&gt; 
&lt;p&gt;Cobra is a library providing a simple interface to create powerful modern CLI interfaces similar to git &amp;amp; go tools.&lt;/p&gt; 
&lt;p&gt;Cobra provides:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Easy subcommand-based CLIs: &lt;code&gt;app server&lt;/code&gt;, &lt;code&gt;app fetch&lt;/code&gt;, etc.&lt;/li&gt; 
 &lt;li&gt;Fully POSIX-compliant flags (including short &amp;amp; long versions)&lt;/li&gt; 
 &lt;li&gt;Nested subcommands&lt;/li&gt; 
 &lt;li&gt;Global, local and cascading flags&lt;/li&gt; 
 &lt;li&gt;Intelligent suggestions (&lt;code&gt;app srver&lt;/code&gt;... did you mean &lt;code&gt;app server&lt;/code&gt;?)&lt;/li&gt; 
 &lt;li&gt;Automatic help generation for commands and flags&lt;/li&gt; 
 &lt;li&gt;Grouping help for subcommands&lt;/li&gt; 
 &lt;li&gt;Automatic help flag recognition of &lt;code&gt;-h&lt;/code&gt;, &lt;code&gt;--help&lt;/code&gt;, etc.&lt;/li&gt; 
 &lt;li&gt;Automatically generated shell autocomplete for your application (bash, zsh, fish, powershell)&lt;/li&gt; 
 &lt;li&gt;Automatically generated man pages for your application&lt;/li&gt; 
 &lt;li&gt;Command aliases so you can change things without breaking them&lt;/li&gt; 
 &lt;li&gt;The flexibility to define your own help, usage, etc.&lt;/li&gt; 
 &lt;li&gt;Optional seamless integration with &lt;a href="https://github.com/spf13/viper"&gt;viper&lt;/a&gt; for 12-factor apps&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Concepts&lt;/h1&gt; 
&lt;p&gt;Cobra is built on a structure of commands, arguments &amp;amp; flags.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Commands&lt;/strong&gt; represent actions, &lt;strong&gt;Args&lt;/strong&gt; are things and &lt;strong&gt;Flags&lt;/strong&gt; are modifiers for those actions.&lt;/p&gt; 
&lt;p&gt;The best applications read like sentences when used, and as a result, users intuitively know how to interact with them.&lt;/p&gt; 
&lt;p&gt;The pattern to follow is &lt;code&gt;APPNAME VERB NOUN --ADJECTIVE&lt;/code&gt; or &lt;code&gt;APPNAME COMMAND ARG --FLAG&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;A few good real world examples may better illustrate this point.&lt;/p&gt; 
&lt;p&gt;In the following example, 'server' is a command, and 'port' is a flag:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;hugo server --port=1313
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In this command we are telling Git to clone the url bare.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;git clone URL --bare
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Commands&lt;/h2&gt; 
&lt;p&gt;Command is the central point of the application. Each interaction that the application supports will be contained in a Command. A command can have children commands and optionally run an action.&lt;/p&gt; 
&lt;p&gt;In the example above, 'server' is the command.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://pkg.go.dev/github.com/spf13/cobra#Command"&gt;More about cobra.Command&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Flags&lt;/h2&gt; 
&lt;p&gt;A flag is a way to modify the behavior of a command. Cobra supports fully POSIX-compliant flags as well as the Go &lt;a href="https://golang.org/pkg/flag/"&gt;flag package&lt;/a&gt;. A Cobra command can define flags that persist through to children commands and flags that are only available to that command.&lt;/p&gt; 
&lt;p&gt;In the example above, 'port' is the flag.&lt;/p&gt; 
&lt;p&gt;Flag functionality is provided by the &lt;a href="https://github.com/spf13/pflag"&gt;pflag library&lt;/a&gt;, a fork of the flag standard library which maintains the same interface while adding POSIX compliance.&lt;/p&gt; 
&lt;h1&gt;Installing&lt;/h1&gt; 
&lt;p&gt;Using Cobra is easy. First, use &lt;code&gt;go get&lt;/code&gt; to install the latest version of the library.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;go get -u github.com/spf13/cobra@latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Next, include Cobra in your application:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-go"&gt;import "github.com/spf13/cobra"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Usage&lt;/h1&gt; 
&lt;p&gt;&lt;code&gt;cobra-cli&lt;/code&gt; is a command line program to generate cobra applications and command files. It will bootstrap your application scaffolding to rapidly develop a Cobra-based application. It is the easiest way to incorporate Cobra into your application.&lt;/p&gt; 
&lt;p&gt;It can be installed by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;go install github.com/spf13/cobra-cli@latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For complete details on using the Cobra-CLI generator, please read &lt;a href="https://github.com/spf13/cobra-cli/raw/main/README.md"&gt;The Cobra Generator README&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;For complete details on using the Cobra library, please read &lt;a href="https://raw.githubusercontent.com/spf13/cobra/main/site/content/user_guide.md"&gt;The Cobra User Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;p&gt;Cobra is released under the Apache 2.0 license. See &lt;a href="https://raw.githubusercontent.com/spf13/cobra/main/LICENSE.txt"&gt;LICENSE.txt&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>transformerlab/transformerlab-app</title>
      <link>https://github.com/transformerlab/transformerlab-app</link>
      <description>&lt;p&gt;Open Source Application for Advanced LLM + Diffusion Engineering: interact, train, fine-tune, and evaluate large language models on your own computer.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;a href="https://transformerlab.ai"&gt;
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/transformerlab/transformerlab-app/refs/heads/main/assets/Transformer-Lab_Logo_Reverse.svg" /&gt; 
   &lt;source media="(prefers-color-scheme: light)" srcset="https://raw.githubusercontent.com/transformerlab/transformerlab-app/refs/heads/main/assets/Transformer-Lab_Logo.svg" /&gt; 
   &lt;img alt="transformer lab logo" src="https://raw.githubusercontent.com/transformerlab/transformerlab-app/refs/heads/main/assets/Transformer-Lab_Logo.svg?sanitize=true" style="max-width: 650px" /&gt; 
  &lt;/picture&gt;&lt;/a&gt; 
 &lt;p align="center"&gt; 100% Open Source Toolkit for Large Language Models: Train, Tune, Chat on your own Machine &lt;br /&gt; &lt;a href="https://transformerlab.ai/docs/download/"&gt;&lt;strong&gt;Download&lt;/strong&gt;&lt;/a&gt; ¬∑ &lt;a href="https://transformerlab.ai/docs/intro"&gt;&lt;strong&gt;Explore the docs ¬ª&lt;/strong&gt;&lt;/a&gt; &lt;br /&gt; &lt;br /&gt; &lt;a href="https://youtu.be/tY5TAvKviLo"&gt;View Demo&lt;/a&gt; ¬∑ &lt;a href="https://github.com/transformerlab/transformerlab-app/issues"&gt;Report Bugs&lt;/a&gt; ¬∑ &lt;a href="https://github.com/transformerlab/transformerlab-app/issues/new"&gt;Suggest Features&lt;/a&gt; ¬∑ &lt;a href="https://discord.gg/transformerlab"&gt;Join Discord&lt;/a&gt; ¬∑ &lt;a href="https://twitter.com/transformerlab"&gt;Follow on Twitter&lt;/a&gt; &lt;/p&gt; 
 &lt;p align="center"&gt; Note: Transformer Lab is actively being developed. Please join our Discord or follow us on Twitter for updates. Questions, feedback and contributions are highly valued!&lt;/p&gt; 
&lt;/div&gt; 
&lt;!-- ABOUT THE PROJECT --&gt; 
&lt;h2&gt;Download Now&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://transformerlab.ai/docs/download"&gt;&lt;img src="https://img.shields.io/badge/Download-EF2D5E?style=for-the-badge&amp;amp;logoColor=white&amp;amp;logo=DocuSign" alt="Download Icon" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;About The Project&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/transformerlab/transformerlab-app/main/assets/transformerlab-demo-jan2025.gif" alt="Product Screen Shot" /&gt;&lt;/p&gt; 
&lt;p&gt;Transformer Lab is an app that allows anyone to experiment with Large Language Models.&lt;/p&gt; 
&lt;h2&gt;Backed by Mozilla&lt;/h2&gt; 
&lt;p&gt;Transformer Lab is proud to be supported by Mozilla through the &lt;a href="https://future.mozilla.org/builders/"&gt;Mozilla Builders Program&lt;/a&gt;&lt;/p&gt; 
&lt;a href="https://future.mozilla.org/builders/"&gt; &lt;img src="https://transformerlab.ai/img/mozilla-builders-2024.png" alt="Mozilla Builders Logo" width="300" /&gt; &lt;/a&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;p&gt;Transformer Lab allows you to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üíï &lt;strong&gt;One-click Download Hundreds of Popular Models&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;DeepSeek, Qwen, Gemma, Phi4, Llama, Mistral, Mixtral, Stable Diffusion, Flux, Command-R, and dozens more&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;‚¨á &lt;strong&gt;Download any LLM, VLM, or Diffusion model from Huggingface&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;üé∂ &lt;strong&gt;Finetune / Train Across Different Hardware&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Finetune using MLX on Apple Silicon&lt;/li&gt; 
   &lt;li&gt;Finetune using Huggingface on GPU&lt;/li&gt; 
   &lt;li&gt;Finetune Diffusion LoRAs on GPU&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;‚öñÔ∏è &lt;strong&gt;RLHF and Preference Optimization&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;DPO&lt;/li&gt; 
   &lt;li&gt;ORPO&lt;/li&gt; 
   &lt;li&gt;SIMPO&lt;/li&gt; 
   &lt;li&gt;Reward Modeling&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;üíª &lt;strong&gt;Work with Models Across Operating Systems&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;Windows App&lt;/li&gt; 
   &lt;li&gt;MacOS App&lt;/li&gt; 
   &lt;li&gt;Linux&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;üí¨ &lt;strong&gt;Chat with Models&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Chat&lt;/li&gt; 
   &lt;li&gt;Completions&lt;/li&gt; 
   &lt;li&gt;Visualize Model Architecture&lt;/li&gt; 
   &lt;li&gt;Inspect activations &amp;amp; attention for each generated token&lt;/li&gt; 
   &lt;li&gt;Preset (Templated) Prompts&lt;/li&gt; 
   &lt;li&gt;Chat History&lt;/li&gt; 
   &lt;li&gt;Tweak generation parameters&lt;/li&gt; 
   &lt;li&gt;Batched Inference&lt;/li&gt; 
   &lt;li&gt;Tool Use / Function Calling (in alpha)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;üöÇ &lt;strong&gt;Use Different Inference Engines&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;MLX on Apple Silicon&lt;/li&gt; 
   &lt;li&gt;FastChat&lt;/li&gt; 
   &lt;li&gt;vLLM&lt;/li&gt; 
   &lt;li&gt;Llama CPP&lt;/li&gt; 
   &lt;li&gt;SGLang&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;üñºÔ∏è &lt;strong&gt;Support for Image Diffusion Models&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Run and experiment with image generation models (e.g., Stable Diffusion, Flux, etc.)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;üßë‚Äçüéì &lt;strong&gt;Evaluate models&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;üìñ &lt;strong&gt;RAG (Retreival Augmented Generation)&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Drag and Drop File UI&lt;/li&gt; 
   &lt;li&gt;Works on Apple MLX, FastChat, and other engines&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;üìì &lt;strong&gt;Build Datasets for Training&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Pull from hundreds of common datasets available on HuggingFace&lt;/li&gt; 
   &lt;li&gt;Provide your own dataset using drag and drop&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;üî¢ &lt;strong&gt;Calculate Embeddings&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;üíÅ &lt;strong&gt;Full REST API&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;üå© &lt;strong&gt;Run in the Cloud&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;You can run the user interface on your desktop/laptop while the engine runs on a remote or cloud machine&lt;/li&gt; 
   &lt;li&gt;Or you can run everything locally on a single machine&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;üîÄ &lt;strong&gt;Convert Models Across Platforms&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Convert from/to Huggingface, MLX, GGUF&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;üîå &lt;strong&gt;Plugin Support&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Easily install from a gallery of existing plugins&lt;/li&gt; 
   &lt;li&gt;Write your own plugins to extend functionality&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;üßë‚Äçüíª &lt;strong&gt;Embedded Monaco Code Editor&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Edit plugins and view what's happening behind the scenes&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;üìù &lt;strong&gt;Prompt Editing&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Easily edit System Messages or Prompt Templates&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;üìú &lt;strong&gt;Inference Logs&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;While doing inference or RAG, view a log of the raw queries sent to the model&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;And you can do the above, all through a simple cross-platform GUI.&lt;/p&gt; 
&lt;!-- GETTING STARTED --&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://transformerlab.ai/docs/download"&gt;Click here&lt;/a&gt; to download Transformer Lab.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://transformerlab.ai/docs/intro"&gt;Read this page&lt;/a&gt; to learn how to install and use.&lt;/p&gt; 
&lt;h3&gt;Built With&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.electronjs.org/"&gt;&lt;img src="https://img.shields.io/badge/Electron-20232A?style=for-the-badge&amp;amp;logo=electron&amp;amp;logoColor=61DAFB" alt="Electron" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://reactjs.org/"&gt;&lt;img src="https://img.shields.io/badge/React-20232A?style=for-the-badge&amp;amp;logo=react&amp;amp;logoColor=61DAFB" alt="React" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%A4%97_HuggingFace-20232A?style=for-the-badge" alt="HuggingFace" /&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Developers&lt;/h2&gt; 
&lt;h3&gt;Building from Scratch&lt;/h3&gt; 
&lt;p&gt;To build the app yourself, pull this repo, and follow the steps below:&lt;/p&gt; 
&lt;p&gt;(Please note that the current build doesn't work on Node v23 but it works on v22)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npm install
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npm start
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Packaging for Production&lt;/h2&gt; 
&lt;p&gt;To package apps for the local platform:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npm run package
&lt;/code&gt;&lt;/pre&gt; 
&lt;!-- LICENSE --&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Distributed under the AGPL V3 License. See &lt;code&gt;LICENSE.txt&lt;/code&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;Reference&lt;/h2&gt; 
&lt;p&gt;If you found Transformer Lab useful in your research or applications, please cite using the following BibTeX:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@software{transformerlab,
  author = {Asaria, Ali},
  title = {Transformer Lab: Experiment with Large Language Models},
  month = December,
  year = 2023,
  url = {https://github.com/transformerlab/transformerlab-app}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;!-- CONTACT --&gt; 
&lt;h2&gt;Contact&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://twitter.com/aliasaria"&gt;@aliasaria&lt;/a&gt; - Ali Asasria&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/dadmobile"&gt;@dadmobile&lt;/a&gt; - Tony Salomone&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- MARKDOWN LINKS &amp; IMAGES --&gt;</description>
    </item>
    
    <item>
      <title>mercurjs/mercur</title>
      <link>https://github.com/mercurjs/mercur</link>
      <description>&lt;p&gt;Open-source multi-vendor marketplace platform for B2B &amp; B2C. Built on top of MedusaJS. Create your own custom marketplace. üõçÔ∏è&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://cdn.prod.website-files.com/6790aeffc4b432ccaf1b56e5/67a225dc6fa298afc1cc4ae6_Mercur%20Cover.png" alt="Mercur Main Cover" /&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;h1&gt;Mercur &lt;br /&gt; Open Source Marketplace Platform&lt;/h1&gt; 
 &lt;!-- Shields.io Badges --&gt; 
 &lt;a href="https://github.com/mercurjs/mercur/tree/main?tab=MIT-1-ov-file"&gt; &lt;img alt="License" src="https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true" /&gt; &lt;/a&gt; 
 &lt;a href="https://raw.githubusercontent.com/mercurjs/mercur/main/#"&gt; &lt;img alt="PRs Welcome" src="https://img.shields.io/badge/PRs-welcome-brightgreen.svg?sanitize=true" /&gt; &lt;/a&gt; 
 &lt;a href="https://rigbyjs.com/#contact"&gt; &lt;img alt="Support" src="https://img.shields.io/badge/support-contact%20author-blueviolet.svg?sanitize=true" /&gt; &lt;/a&gt; 
 &lt;!-- Website Links --&gt; 
 &lt;p&gt; &lt;a href="https://mercurjs.com/"&gt;Mercur&lt;/a&gt; | &lt;a href="https://docs.mercurjs.com/"&gt;Docs&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h1&gt;What is Mercur?&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://www.mercurjs.com/"&gt;Mercur&lt;/a&gt; is the first truly limitless open source marketplace platform that combines the simplicity of SaaS with the freedom of open source. Built on &lt;a href="https://github.com/medusajs/medusa"&gt;MedusaJS&lt;/a&gt;, it empowers businesses to create custom marketplaces without choosing between ownership and ease of use.&lt;/p&gt; 
&lt;p&gt;Mercur is a platform to start, customize, manage, and scale your marketplace for every business model with a modern technology stack.&lt;/p&gt; 
&lt;h2&gt;Announcing Mercur 1.0&lt;/h2&gt; 
&lt;p&gt;After months of development, testing, and close collaboration with early adopters, we‚Äôre excited to announce the official release of &lt;strong&gt;Mercur 1.0&lt;/strong&gt; - the first truly limitless marketplace platform. Version 1.0 is fully open source and ready to be self-hosted, giving you &lt;strong&gt;full control over infrastructure, customizations, and data&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;With this version, &lt;strong&gt;Mercur is production-ready for B2C marketplaces&lt;/strong&gt;. The first complete version includes a vendor system, admin panel, and a fully built B2C Storefront. Read more in &lt;strong&gt;&lt;a href="https://www.mercurjs.com/updates/mercur-1-0-release"&gt;official release announcement&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Why Choose Mercur?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Full Ownership: Unlike SaaS platforms, you own your marketplace with no transaction fees or vendor lock-in&lt;/li&gt; 
 &lt;li&gt;Modern Foundation: Built on MedusaJS, offering a modern tech stack that developers love&lt;/li&gt; 
 &lt;li&gt;Beautiful by Default: Create stunning storefronts without sacrificing customization&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Power Any Marketplace Model&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Custom B2B Marketplace: Build enterprise-grade platforms with specialized workflows&lt;/li&gt; 
 &lt;li&gt;Custom B2C Marketplace: Create engaging consumer marketplaces with modern UX&lt;/li&gt; 
 &lt;li&gt;eCommerce Extension: Transform your store into a marketplace (coming soon)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://cdn.prod.website-files.com/6790aeffc4b432ccaf1b56e5/67b46aa08180d5b8499c6a15_Use-cases.jpg" alt="Mercur Use Cases" /&gt; &amp;nbsp;&lt;/p&gt; 
&lt;h1&gt;Ready-to-go marketplace features&lt;/h1&gt; 
&lt;p&gt;&lt;b&gt;Storefronts for Marketplace &lt;/b&gt; &lt;br /&gt; Customizable storefronts designed for B2B and B2C with all elements including browsing and buying products across multiple vendors at once.&lt;/p&gt; 
&lt;p&gt;Discover &lt;a href="https://github.com/mercurjs/b2c-marketplace-storefront"&gt;B2C Storefront Repository&lt;/a&gt; - &lt;a href="https://b2c.mercurjs.com/"&gt;üõçÔ∏è Check demo &lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;b&gt;Admin Panel&lt;/b&gt; &lt;br /&gt; Control over whole marketplace: setting product categories, vendors, commissions and rules&lt;/p&gt; 
&lt;p&gt;&lt;b&gt;Vendor Panel&lt;/b&gt; &lt;br /&gt; A powerful dashboard giving sellers complete control over their products, orders, and store management in one intuitive interface.&lt;/p&gt; 
&lt;p&gt;Discover &lt;a href="https://github.com/mercurjs/vendor-panel"&gt;Vendor Panel&lt;/a&gt; - &lt;a href="https://www.mercurjs.com/contact"&gt; Contact us to get demo &lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;b&gt;Integrations&lt;/b&gt; &lt;br /&gt; Built-in integration with Stripe for payments and Resend for communication needs. More integrations coming soon.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://cdn.prod.website-files.com/6790aeffc4b432ccaf1b56e5/67a1020f202572832c954ead_6b96703adfe74613f85133f83a19b1f0_Fleek%20Tilt%20-%20Readme.png" alt="Mercur" /&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;h4&gt;Setup Medusa project&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone the repository
git clone https://github.com/mercurjs/mercur.git

# Change directory
cd mercur

# Install dependencies
yarn install

# Build packages
yarn build

# Go to backend folder
cd apps/backend

# Clone .env.template
cp .env.template .env

# In the .env file replace user, password, address and port parameters in the DATABASE_URL variable with your values
DATABASE_URL=postgres://[user]:[password]@[address]:[port]/$DB_NAME
# For example:
DATABASE_URL=postgres://postgres:postgres@localhost:5432/$DB_NAME

# Setup database and run migrations
yarn medusa db:create &amp;amp;&amp;amp; yarn medusa db:migrate &amp;amp;&amp;amp; yarn run seed

# Create admin user
npx medusa user --email &amp;lt;email&amp;gt; --password &amp;lt;password&amp;gt;

# Go to root folder
cd ../..

# Start Mercur
yarn dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h2&gt;Prerequisites&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Node.js v20+&lt;/li&gt; 
 &lt;li&gt;PostgreSQL&lt;/li&gt; 
 &lt;li&gt;Git CLI&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Resources&lt;/h1&gt; 
&lt;h4&gt;Learn more about Mercur&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.mercurjs.com/"&gt;Mercur Website&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.mercurjs.com/introduction"&gt;Mercur Docs&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Learn more about Medusa&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.medusajs.com/"&gt;Medusa Website&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.medusajs.com/v2"&gt;Medusa Docs&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>OpenBMB/MiniCPM-V</title>
      <link>https://github.com/OpenBMB/MiniCPM-V</link>
      <description>&lt;p&gt;MiniCPM-V 4.5: A GPT-4o Level MLLM for Single Image, Multi Image and Video Understanding on Your Phone&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpm_v_and_minicpm_o_title.png" width="500em" /&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;A GPT-4o Level MLLM for Single Image, Multi Image and Video Understanding on Your Phone&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/README_zh.md"&gt;‰∏≠Êñá&lt;/a&gt; | English&lt;/strong&gt;&lt;/p&gt; 
 &lt;span style="display: inline-flex; align-items: center; margin-right: 2px;"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/wechat.png" alt="WeChat" style="margin-right: 4px;" /&gt; &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/wechat.md" target="_blank"&gt; WeChat&lt;/a&gt; &amp;nbsp;| &lt;/span&gt; &amp;nbsp; 
 &lt;span style="display: inline-flex; align-items: center; margin-left: -8px;"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/discord.png" alt="Discord" style="margin-right: 4px;" /&gt; &lt;a href="https://discord.gg/rftuRMbqzf" target="_blank"&gt; Discord&lt;/a&gt; &amp;nbsp; &lt;/span&gt; 
 &lt;p align="center"&gt; MiniCPM-V 4.5 &lt;a href="https://huggingface.co/openbmb/MiniCPM-V-4_5"&gt;ü§ó&lt;/a&gt; &lt;a href="http://101.126.42.235:30910/"&gt;ü§ñ&lt;/a&gt; | MiniCPM-o 2.6 &lt;a href="https://huggingface.co/openbmb/MiniCPM-o-2_6"&gt;ü§ó&lt;/a&gt; &lt;a href="https://minicpm-omni-webdemo-us.modelbest.cn/"&gt; ü§ñ&lt;/a&gt; | &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-Cookbook"&gt;üç≥ Cookbook&lt;/a&gt; | üìÑ Technical Report (Coming Soon) &lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;MiniCPM-V&lt;/strong&gt; is a series of efficient end-side multimodal LLMs (MLLMs), which accept images, videos and text as inputs and deliver high-quality text outputs. &lt;strong&gt;MiniCPM-o&lt;/strong&gt; additionally takes audio as inputs and provide high-quality speech outputs in an end-to-end fashion. Since February 2024, we have released 7 versions of the model, aiming to achieve &lt;strong&gt;strong performance and efficient deployment&lt;/strong&gt;. The most notable models in the series currently include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;MiniCPM-V 4.5&lt;/strong&gt;: üî•üî•üî• The latest and most capable model in the MiniCPM-V series. With a total of 8B parameters, this model &lt;strong&gt;outperforms GPT-4o-latest, Gemini-2.0 Pro, and Qwen2.5-VL 72B&lt;/strong&gt; in vision-language capabilities, making it the most performant on-device multimodal model in the open-source community. This version brings &lt;strong&gt;new features including efficient high refresh rate and long video understanding (up to 96x compression rate for video tokens), controllable hybrid fast/deep thinking, strong handwritten OCR and complex table/document parsing&lt;/strong&gt;. It also advances MiniCPM-V's popular features such as trustworthy behavior, multilingual support and end-side deployability.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;MiniCPM-o 2.6&lt;/strong&gt;: ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è The most capable model in the MiniCPM-o series. With a total of 8B parameters, this end-to-end model &lt;strong&gt;achieves comparable performance to GPT-4o-202405 in vision, speech, and multimodal live streaming&lt;/strong&gt;, making it one of the most versatile and performant models in the open-source community. For the new voice mode, MiniCPM-o 2.6 &lt;strong&gt;supports bilingual real-time speech conversation with configurable voices&lt;/strong&gt;, and also allows for fun capabilities such as emotion/speed/style control, end-to-end voice cloning, role play, etc. Due to its superior token density, MiniCPM-o 2.6 can for the first time &lt;strong&gt;support multimodal live streaming on end-side devices&lt;/strong&gt; such as iPad.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;News 
 &lt;!-- omit in toc --&gt;&lt;/h2&gt; 
&lt;h4&gt;üìå Pinned&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;[2025.08.26] üî•üî•üî• We open-source MiniCPM-V 4.5, which outperforms GPT-4o-latest, Gemini-2.0 Pro, and Qwen2.5-VL 72B. It advances popular capabilities of MiniCPM-V, and brings useful new features. Try it now!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2025.08.01] ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è We open-sourced the &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook"&gt;MiniCPM-V &amp;amp; o Cookbook&lt;/a&gt;! It provides comprehensive guides for diverse user scenarios, paired with our new &lt;a href="https://minicpm-o.readthedocs.io/en/latest/index.html"&gt;Docs Site&lt;/a&gt; for smoother onboarding.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2025.06.20] ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è Our official &lt;a href="https://ollama.com/openbmb"&gt;Ollama repository&lt;/a&gt; is released. Try our latest models with &lt;a href="https://ollama.com/openbmb/minicpm-o2.6"&gt;one click&lt;/a&gt;ÔºÅ&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2025.03.01] üöÄüöÄüöÄ RLAIF-V, the alignment technique of MiniCPM-o, is accepted by CVPR 2025 HighlightsÔºÅThe &lt;a href="https://github.com/RLHF-V/RLAIF-V"&gt;code&lt;/a&gt;, &lt;a href="https://huggingface.co/datasets/openbmb/RLAIF-V-Dataset"&gt;dataset&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/2405.17220"&gt;paper&lt;/a&gt; are open-sourced!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2025.01.24] üì¢üì¢üì¢ MiniCPM-o 2.6 technical report is released! See &lt;a href="https://openbmb.notion.site/MiniCPM-o-2-6-A-GPT-4o-Level-MLLM-for-Vision-Speech-and-Multimodal-Live-Streaming-on-Your-Phone-185ede1b7a558042b5d5e45e6b237da9"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2025.01.19] üì¢ &lt;strong&gt;ATTENTION!&lt;/strong&gt; We are currently working on merging MiniCPM-o 2.6 into the official repositories of llama.cpp, Ollama, and vllm. Until the merge is complete, please USE OUR LOCAL FORKS of &lt;a href="https://github.com/OpenBMB/llama.cpp/raw/minicpm-omni/examples/llava/README-minicpmo2.6.md"&gt;llama.cpp&lt;/a&gt;, &lt;a href="https://github.com/OpenBMB/ollama/raw/minicpm-v2.6/examples/minicpm-v2.6/README.md"&gt;Ollama&lt;/a&gt;, and &lt;a href="https://github.com/OpenBMB/MiniCPM-o?tab=readme-ov-file#efficient-inference-with-llamacpp-ollama-vllm"&gt;vllm&lt;/a&gt;. &lt;strong&gt;Using the official repositories before the merge may lead to unexpected issues&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2025.01.19] ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è MiniCPM-o tops GitHub Trending and reaches top-2 on Hugging Face Trending!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2025.01.17] We have updated the usage of MiniCPM-o 2.6 int4 quantization version and resolved the model initialization error. Click &lt;a href="https://huggingface.co/openbmb/MiniCPM-o-2_6-int4"&gt;here&lt;/a&gt; and try it now!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2025.01.13] üî•üî•üî• We open-source MiniCPM-o 2.6, which matches GPT-4o-202405 on vision, speech and multimodal live streaming. It advances popular capabilities of MiniCPM-V 2.6, and supports various new fun features. Try it now!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2024.08.17] üöÄüöÄüöÄ MiniCPM-V 2.6 is now fully supported by &lt;a href="https://github.com/ggerganov/llama.cpp"&gt;official&lt;/a&gt; llama.cpp! GGUF models of various sizes are available &lt;a href="https://huggingface.co/openbmb/MiniCPM-V-2_6-gguf"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2024.08.06] üî•üî•üî• We open-source MiniCPM-V 2.6, which outperforms GPT-4V on single image, multi-image and video understanding. It advances popular features of MiniCPM-Llama3-V 2.5, and can support real-time video understanding on iPad. Try it now!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2024.08.03] MiniCPM-Llama3-V 2.5 technical report is released! See &lt;a href="https://arxiv.org/abs/2408.01800"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2024.05.23] üî•üî•üî• MiniCPM-V tops GitHub Trending and Hugging Face Trending! Our demo, recommended by Hugging Face Gradio‚Äôs official account, is available &lt;a href="https://huggingface.co/spaces/openbmb/MiniCPM-Llama3-V-2_5"&gt;here&lt;/a&gt;. Come and try it out!&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click to view more news.&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;[2025.08.02] üöÄüöÄüöÄ We open-source MiniCPM-V 4.0, which outperforms GPT-4.1-mini-20250414 in image understanding. It advances popular features of MiniCPM-V 2.6, and largely improves the efficiency. We also open-source the iOS App on iPhone and iPad. Try it now!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2025.01.23] üí°üí°üí° MiniCPM-o 2.6 is now supported by &lt;a href="https://github.com/PKU-Alignment/align-anything"&gt;Align-Anything&lt;/a&gt;, a framework by PKU-Alignment Team for aligning any-to-any modality large models with human intentions. It supports DPO and SFT fine-tuning on both vision and audio. Try it now!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.08.15] We now also support multi-image SFT. For more details, please refer to the &lt;a href="https://github.com/OpenBMB/MiniCPM-V/tree/main/finetune"&gt;document&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.08.14] MiniCPM-V 2.6 now also supports &lt;a href="https://github.com/modelscope/ms-swift/issues/1613"&gt;fine-tuning&lt;/a&gt; with the SWIFT framework!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.08.10] üöÄüöÄüöÄ MiniCPM-Llama3-V 2.5 is now fully supported by &lt;a href="https://github.com/ggerganov/llama.cpp"&gt;official&lt;/a&gt; llama.cpp! GGUF models of various sizes are available &lt;a href="https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-gguf"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.07.19] MiniCPM-Llama3-V 2.5 supports vLLM now! See &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#inference-with-vllm"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.06.03] Now, you can run MiniCPM-Llama3-V 2.5 on multiple low VRAM GPUs(12 GB or 16 GB) by distributing the model's layers across multiple GPUs. For more details, Check this &lt;a href="https://github.com/OpenBMB/MiniCPM-V/raw/main/docs/inference_on_multiple_gpus.md"&gt;link&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.05.28] üöÄüöÄüöÄ MiniCPM-Llama3-V 2.5 now fully supports its feature in llama.cpp and Ollama! Please pull the latest code &lt;strong&gt;of our provided forks&lt;/strong&gt; (&lt;a href="https://github.com/OpenBMB/llama.cpp/raw/minicpm-v2.5/examples/minicpmv/README.md"&gt;llama.cpp&lt;/a&gt;, &lt;a href="https://github.com/OpenBMB/ollama/tree/minicpm-v2.5/examples/minicpm-v2.5"&gt;Ollama&lt;/a&gt;). GGUF models in various sizes are available &lt;a href="https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-gguf/tree/main"&gt;here&lt;/a&gt;. MiniCPM-Llama3-V 2.5 series is &lt;strong&gt;not supported by the official repositories yet&lt;/strong&gt;, and we are working hard to merge PRs. Please stay tuned!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.05.28] üí´ We now support LoRA fine-tuning for MiniCPM-Llama3-V 2.5, using only 2 V100 GPUs! See more statistics &lt;a href="https://github.com/OpenBMB/MiniCPM-V/tree/main/finetune#model-fine-tuning-memory-usage-statistics"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.05.25] MiniCPM-Llama3-V 2.5 now supports streaming outputs and customized system prompts. Try it &lt;a href="https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5#usage"&gt;here&lt;/a&gt;!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.05.24] We release the MiniCPM-Llama3-V 2.5 &lt;a href="https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-gguf"&gt;gguf&lt;/a&gt;, which supports &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#inference-with-llamacpp"&gt;llama.cpp&lt;/a&gt; inference and provides a 6~8 token/s smooth decoding on mobile phones. Try it now!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.05.23] üîç We've released a comprehensive comparison between Phi-3-vision-128k-instruct and MiniCPM-Llama3-V 2.5, including benchmarks evaluations, multilingual capabilities, and inference efficiency üåüüìäüåçüöÄ. Click &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/compare_with_phi-3_vision.md"&gt;here&lt;/a&gt; to view more details.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.05.20] We open-soure MiniCPM-Llama3-V 2.5, it has improved OCR capability and supports 30+ languages, representing the first end-side MLLM achieving GPT-4V level performance! We provide &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#deployment-on-mobile-phone"&gt;efficient inference&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/finetune/readme.md"&gt;simple fine-tuning&lt;/a&gt;. Try it now!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.04.23] MiniCPM-V-2.0 supports vLLM now! Click &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#inference-with-vllm"&gt;here&lt;/a&gt; to view more details.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.04.18] We create a HuggingFace Space to host the demo of MiniCPM-V 2.0 at &lt;a href="https://huggingface.co/spaces/openbmb/MiniCPM-V-2"&gt;here&lt;/a&gt;!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.04.17] MiniCPM-V-2.0 supports deploying &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#webui-demo"&gt;WebUI Demo&lt;/a&gt; now!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.04.15] MiniCPM-V-2.0 now also supports &lt;a href="https://github.com/modelscope/swift/raw/main/docs/source/Multi-Modal/minicpm-v-2%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5.md"&gt;fine-tuning&lt;/a&gt; with the SWIFT framework!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.04.12] We open-source MiniCPM-V 2.0, which achieves comparable performance with Gemini Pro in understanding scene text and outperforms strong Qwen-VL-Chat 9.6B and Yi-VL 34B on &lt;a href="https://rank.opencompass.org.cn/leaderboard-multimodal"&gt;OpenCompass&lt;/a&gt;, a comprehensive evaluation over 11 popular benchmarks. Click &lt;a href="https://openbmb.vercel.app/minicpm-v-2"&gt;here&lt;/a&gt; to view the MiniCPM-V 2.0 technical blog.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.03.14] MiniCPM-V now supports &lt;a href="https://github.com/modelscope/swift/raw/main/docs/source/Multi-Modal/minicpm-v%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5.md"&gt;fine-tuning&lt;/a&gt; with the SWIFT framework. Thanks to &lt;a href="https://github.com/Jintao-Huang"&gt;Jintao&lt;/a&gt; for the contributionÔºÅ&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.03.01] MiniCPM-V now can be deployed on Mac!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.02.01] We open-source MiniCPM-V and OmniLMM-12B, which support efficient end-side deployment and powerful multimodal capabilities correspondingly.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;Contents 
 &lt;!-- omit in toc --&gt;&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#minicpm-v-45"&gt;MiniCPM-V 4.5&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#minicpm-o-26"&gt;MiniCPM-o 2.6&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#minicpm-v--o-cookbook"&gt;MiniCPM-V &amp;amp; o Cookbook&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#chat-with-our-demo-on-gradio-"&gt;Chat with Our Demo on Gradio ü§ó&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#inference"&gt;Inference&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#model-zoo"&gt;Model Zoo&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#multi-turn-conversation"&gt;Multi-turn Conversation&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#chat-with-multiple-images"&gt;Chat with Multiple Images&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#in-context-few-shot-learning"&gt;In-context Few-shot Learning&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#chat-with-video"&gt;Chat with Video&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#speech-and-audio-mode"&gt;Speech and Audio Mode&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#multimodal-live-streaming"&gt;Multimodal Live Streaming&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#inference-on-multiple-gpus"&gt;Inference on Multiple GPUs&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#inference-on-mac"&gt;Inference on Mac&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#efficient-inference-with-llamacpp-ollama-vllm"&gt;Efficient Inference with llama.cpp, Ollama, vLLM&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#fine-tuning"&gt;Fine-tuning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#awesome-work-using-minicpm-v--minicpm-o"&gt;Awesome work using MiniCPM-V &amp;amp; MiniCPM-o&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#faqs"&gt;FAQs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#limitations"&gt;Limitations&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;MiniCPM-V 4.5&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;MiniCPM-V 4.5&lt;/strong&gt; is the latest and most capable model in the MiniCPM-V series. The model is built on Qwen3-8B and SigLIP2-400M with a total of 8B parameters. It exhibits a significant performance improvement over previous MiniCPM-V and MiniCPM-o models, and introduces new useful features. Notable features of MiniCPM-V 4.5 include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;üî• &lt;strong&gt;State-of-the-art Vision-Language Capability.&lt;/strong&gt; MiniCPM-V 4.5 achieves an average score of 77.0 on OpenCompass, a comprehensive evaluation of 8 popular benchmarks. &lt;strong&gt;With only 8B parameters, it surpasses widely used proprietary models like GPT-4o-latest, Gemini-2.0 Pro, and strong open-source models like Qwen2.5-VL 72B&lt;/strong&gt; for vision-language capabilities, making it the most performant MLLM under 30B parameters.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üé¨ &lt;strong&gt;Efficient High Refresh Rate and Long Video Understanding.&lt;/strong&gt; Powered by a new unified 3D-Resampler over images and videos, MiniCPM-V 4.5 can now achieve 96x compression rate for video tokens, where 6 448x448 video frames can be jointly compressed into 64 video tokens (normally 1,536 tokens for most MLLMs). This means that the model can percieve significantly more video frames without increasing the LLM inference cost. This brings state-of-the-art high refresh rate (up to 10FPS) video understanding and long video understanding capabilities on Video-MME, LVBench, MLVU, MotionBench, FavorBench, etc., efficiently.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;‚öôÔ∏è &lt;strong&gt;Controllable Hybrid Fast/Deep Thinking.&lt;/strong&gt; MiniCPM-V 4.5 supports both fast thinking for efficient frequent usage with competitive performance, and deep thinking for more complex problem solving. To cover efficiency and performance trade-offs in different user scenarios, this fast/deep thinking mode can be switched in a highly controlled fashion.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üí™ &lt;strong&gt;Strong OCR, Document Parsing and Others.&lt;/strong&gt; Based on &lt;a href="https://arxiv.org/pdf/2403.11703"&gt;LLaVA-UHD&lt;/a&gt; architecture, MiniCPM-V 4.5 can process high-resolution images with any aspect ratio and up to 1.8 million pixels (e.g., 1344x1344), using 4x less visual tokens than most MLLMs. The model achieves &lt;strong&gt;leading performance on OCRBench, surpassing proprietary models such as GPT-4o-latest and Gemini 2.5&lt;/strong&gt;. It also achieves state-of-the-art performance for PDF document parsing capability on OmniDocBench among general MLLMs. Based on the the latest &lt;a href="https://github.com/RLHF-V/RLAIF-V/"&gt;RLAIF-V&lt;/a&gt; and &lt;a href="https://github.com/OpenBMB/VisCPM"&gt;VisCPM&lt;/a&gt; techniques, it features &lt;strong&gt;trustworthy behaviors&lt;/strong&gt;, outperforming GPT-4o-latest on MMHal-Bench, and supports &lt;strong&gt;multilingual capabilities&lt;/strong&gt; in more than 30 languages.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üí´ &lt;strong&gt;Easy Usage.&lt;/strong&gt; MiniCPM-V 4.5 can be easily used in various ways: (1) &lt;a href="https://github.com/tc-mb/llama.cpp/raw/Support-MiniCPM-V-4.5/docs/multimodal/minicpmv4.5.md"&gt;llama.cpp&lt;/a&gt; and &lt;a href="https://github.com/tc-mb/ollama/tree/MIniCPM-V"&gt;ollama&lt;/a&gt; support for efficient CPU inference on local devices, (2) &lt;a href="https://huggingface.co/openbmb/MiniCPM-V-4_5-int4"&gt;int4&lt;/a&gt;, &lt;a href="https://huggingface.co/openbmb/MiniCPM-V-4_5-gguf"&gt;GGUF&lt;/a&gt; and &lt;a href="https://github.com/tc-mb/AutoAWQ"&gt;AWQ&lt;/a&gt; format quantized models in 16 sizes, (3) &lt;a href="https://github.com/tc-mb/sglang/tree/main"&gt;SGLang&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#efficient-inference-with-llamacpp-ollama-vllm"&gt;vLLM&lt;/a&gt; support for high-throughput and memory-efficient inference, (4) fine-tuning on new domains and tasks with &lt;a href="https://github.com/tc-mb/transformers/tree/main"&gt;Transformers&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/llamafactory_train_and_infer.md"&gt;LLaMA-Factory&lt;/a&gt;, (5) quick &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#chat-with-our-demo-on-gradio"&gt;local WebUI demo&lt;/a&gt;, (6) optimized &lt;a href="https://github.com/tc-mb/MiniCPM-o-demo-iOS"&gt;local iOS app&lt;/a&gt; on iPhone and iPad, and (7) online web demo on &lt;a href="http://101.126.42.235:30910/"&gt;server&lt;/a&gt;. See our &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook"&gt;Cookbook&lt;/a&gt; for full usages!&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Key Techniques 
 &lt;!-- omit in toc --&gt;&lt;/h3&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpm-v-4dot5-framework.png" , width="100%" /&gt; 
&lt;/div&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Architechture: Unified 3D-Resampler for High-density Video Compression.&lt;/strong&gt; MiniCPM-V 4.5 introduces a 3D-Resampler that overcomes the performance-efficiency trade-off in video understanding. By grouping and jointly compressing up to 6 consecutive video frames into just 64 tokens (the same token count used for a single image in MiniCPM-V series), MiniCPM-V 4.5 achieves a 96√ó compression rate for video tokens. This allows the model to process more video frames without additional LLM computational cost, enabling high refresh rate video and long video understanding. The architecture supports unified encoding for images, multi-image inputs, and videos, ensuring seamless capability and knowledge transfer.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Pre-training: Unified Learning for OCR and Knowledge from Documents.&lt;/strong&gt; Existing MLLMs learn OCR capability and knowledge from documents in isolated training approaches. We observe the essential difference between these two training approaches is the visibility of the text in images. By dynamically corrupting text regions in documents with varying noise levels and asking the model to reconstruct the text, the model learns to adaptively and properly switch between accurate text recognition (when text is visible) and multimodal context-based knowledge reasoning (when text is heavily obscured). This eliminates reliance on error-prone document parsers in knowledge learning from documents, and prevents hallucinations from over-augmented OCR data, resulting in top-tier OCR and multimodal knowledge performance with minimal engineering overhead.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Post-training: Hybrid Fast/Deep Thinking with Multimodal RL.&lt;/strong&gt; MiniCPM-V 4.5 offers a balanced reasoning experience through two switchable modes: fast thinking for efficient daily use and deep thinking for complex tasks. Using a new hybrid reinforcement learning method, the model jointly optimizes both modes, significantly enhancing fast-mode performance without compromising deep-mode capability. Incorporated with &lt;a href="https://github.com/OpenBMB/RLPR"&gt;RLPR&lt;/a&gt; and &lt;a href="https://github.com/RLHF-V/RLAIF-V"&gt;RLAIF-V&lt;/a&gt;, it generalizes robust reasoning skills from broad multimodal data while effectively reducing hallucinations.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Evaluation 
 &lt;!-- omit in toc --&gt;&lt;/h3&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/radar_minicpm_v45.png" , width="60%" /&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv_4_5_evaluation_result.png" , width="80%" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;Inference Efficiency&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;OpenCompass&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="left"&gt; 
 &lt;table style="margin: 0px auto;"&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="left"&gt;Model&lt;/th&gt; 
    &lt;th&gt;Size&lt;/th&gt; 
    &lt;th&gt;Avg Score ‚Üë&lt;/th&gt; 
    &lt;th&gt;Total Inference Time ‚Üì&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody align="center"&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;GLM-4.1V-9B-Thinking&lt;/td&gt; 
    &lt;td&gt;10.3B&lt;/td&gt; 
    &lt;td&gt;76.6&lt;/td&gt; 
    &lt;td&gt;17.5h&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;MiMo-VL-7B-RL&lt;/td&gt; 
    &lt;td&gt;8.3B&lt;/td&gt; 
    &lt;td&gt;76.4&lt;/td&gt; 
    &lt;td&gt;11h&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;MiniCPM-V 4.5&lt;/td&gt; 
    &lt;td&gt;8.7B&lt;/td&gt; 
    &lt;td&gt;&lt;b&gt;77.0&lt;/b&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;b&gt;7.5h&lt;/b&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Video-MME&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="left"&gt; 
 &lt;table style="margin: 0px auto;"&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="left"&gt;Model&lt;/th&gt; 
    &lt;th&gt;Size&lt;/th&gt; 
    &lt;th&gt;Avg Score ‚Üë&lt;/th&gt; 
    &lt;th&gt;Total Inference Time ‚Üì&lt;/th&gt; 
    &lt;th&gt;GPU Mem ‚Üì&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody align="center"&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;Qwen2.5-VL-7B-Instruct&lt;/td&gt; 
    &lt;td&gt;8.3B&lt;/td&gt; 
    &lt;td&gt;71.6&lt;/td&gt; 
    &lt;td&gt;3h&lt;/td&gt; 
    &lt;td&gt;60G&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;GLM-4.1V-9B-Thinking&lt;/td&gt; 
    &lt;td&gt;10.3B&lt;/td&gt; 
    &lt;td&gt;&lt;b&gt;73.6&lt;/b&gt;&lt;/td&gt; 
    &lt;td&gt;2.63h&lt;/td&gt; 
    &lt;td&gt;32G&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;MiniCPM-V 4.5&lt;/td&gt; 
    &lt;td&gt;8.7B&lt;/td&gt; 
    &lt;td&gt;73.5&lt;/td&gt; 
    &lt;td&gt;&lt;b&gt;0.26h&lt;/b&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;b&gt;28G&lt;/b&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;p&gt;Both Video-MME and OpenCompass were evaluated using 8√óA100 GPUs for inference. The reported inference time of Video-MME excludes the cost of video frame extraction.&lt;/p&gt; 
&lt;h3&gt;Examples 
 &lt;!-- omit in toc --&gt;&lt;/h3&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://www.youtube.com/watch?v=Cn23FujYMMU"&gt;&lt;img src="./assets/minicpmv4_5/MiniCPM-V 4.5-8.26_img.jpeg" , width="70%" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;div style="display: flex; flex-direction: column; align-items: center;"&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv4_5/en_case1.png" alt="en_case1" style="margin-bottom: 5px;" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv4_5/en_case2.png" alt="en_case2" style="margin-bottom: 5px;" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv4_5/en_case3.jpeg" alt="en_case3" style="margin-bottom: 5px;" /&gt; 
&lt;/div&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click to view more cases.&lt;/summary&gt; 
 &lt;div style="display: flex; flex-direction: column; align-items: center;"&gt; 
  &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv4_5/zh_extra.jpeg" alt="zh_extra" style="margin-bottom: 5px;" /&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;p&gt;We deploy MiniCPM-V 4.5 on iPad M4 with &lt;a href="https://github.com/tc-mb/MiniCPM-o-demo-iOS"&gt;iOS demo&lt;/a&gt;. The demo video is the raw screen recording without edition.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv4_5/v45_en_handwriting.gif" width="45%/" /&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv4_5/v45_en_cot.gif" width="45%/" /&gt; &lt;/p&gt;
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv4_5/v45_cn_handwriting.gif" width="45%/" /&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv4_5/v45_cn_travel.gif" width="45%/" /&gt; &lt;/p&gt;
&lt;table align="center"&gt;   
&lt;/table&gt; 
&lt;h2&gt;MiniCPM-o 2.6&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;MiniCPM-o 2.6&lt;/strong&gt; is the latest and most capable model in the MiniCPM-o series. The model is built in an end-to-end fashion based on SigLip-400M, Whisper-medium-300M, ChatTTS-200M, and Qwen2.5-7B with a total of 8B parameters. It exhibits a significant performance improvement over MiniCPM-V 2.6, and introduces new features for real-time speech conversation and multimodal live streaming. Notable features of MiniCPM-o 2.6 include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;üî• &lt;strong&gt;Leading Visual Capability.&lt;/strong&gt; MiniCPM-o 2.6 achieves an average score of 70.2 on OpenCompass, a comprehensive evaluation of 8 popular benchmarks. &lt;strong&gt;With only 8B parameters, it surpasses widely used proprietary models like GPT-4o-202405, Gemini 1.5 Pro, and Claude 3.5 Sonnet&lt;/strong&gt; for single image understanding. It also &lt;strong&gt;outperforms GPT-4V and Claude 3.5 Sonnet&lt;/strong&gt; in multi-image and video understanding, and shows promising in-context learning capability.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üéô &lt;strong&gt;State-of-the-art Speech Capability.&lt;/strong&gt; MiniCPM-o 2.6 supports &lt;strong&gt;bilingual real-time speech conversation with configurable voices&lt;/strong&gt; in English and Chinese. It &lt;strong&gt;outperforms GPT-4o-realtime on audio understanding tasks&lt;/strong&gt; such as ASR and STT translation, and shows &lt;strong&gt;state-of-the-art performance on speech conversation in both semantic and acoustic evaluations in the open-source community&lt;/strong&gt;. It also allows for fun features such as emotion/speed/style control, end-to-end voice cloning, role play, etc.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üé¨ &lt;strong&gt;Strong Multimodal Live Streaming Capability.&lt;/strong&gt; As a new feature, MiniCPM-o 2.6 can &lt;strong&gt;accept continuous video and audio streams independent of user queries, and support real-time speech interaction&lt;/strong&gt;. It &lt;strong&gt;outperforms GPT-4o-202408 and Claude 3.5 Sonnet and shows state-of-art performance in the open-source community on StreamingBench&lt;/strong&gt;, a comprehensive benchmark for real-time video understanding, omni-source (video &amp;amp; audio) understanding, and multimodal contextual understanding.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üí™ &lt;strong&gt;Strong OCR Capability and Others.&lt;/strong&gt; Advancing popular visual capabilities from MiniCPM-V series, MiniCPM-o 2.6 can process images with any aspect ratio and up to 1.8 million pixels (e.g., 1344x1344). It achieves &lt;strong&gt;state-of-the-art performance on OCRBench for models under 25B, surpassing proprietary models such as GPT-4o-202405&lt;/strong&gt;. Based on the the latest &lt;a href="https://github.com/RLHF-V/RLAIF-V/"&gt;RLAIF-V&lt;/a&gt; and &lt;a href="https://github.com/OpenBMB/VisCPM"&gt;VisCPM&lt;/a&gt; techniques, it features &lt;strong&gt;trustworthy behaviors&lt;/strong&gt;, outperforming GPT-4o and Claude 3.5 Sonnet on MMHal-Bench, and supports &lt;strong&gt;multilingual capabilities&lt;/strong&gt; on more than 30 languages.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üöÄ &lt;strong&gt;Superior Efficiency.&lt;/strong&gt; In addition to its friendly size, MiniCPM-o 2.6 also shows &lt;strong&gt;state-of-the-art token density&lt;/strong&gt; (i.e., the number of pixels encoded into each visual token). &lt;strong&gt;It produces only 640 tokens when processing a 1.8M pixel image, which is 75% fewer than most models&lt;/strong&gt;. This directly improves the inference speed, first-token latency, memory usage, and power consumption. As a result, MiniCPM-o 2.6 can efficiently support &lt;strong&gt;multimodal live streaming&lt;/strong&gt; on end-side devices such as iPads.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üí´ &lt;strong&gt;Easy Usage.&lt;/strong&gt; MiniCPM-o 2.6 can be easily used in various ways: (1) &lt;a href="https://github.com/OpenBMB/llama.cpp/raw/minicpm-omni/examples/llava/README-minicpmo2.6.md"&gt;llama.cpp&lt;/a&gt; support for efficient CPU inference on local devices, (2) &lt;a href="https://huggingface.co/openbmb/MiniCPM-o-2_6-int4"&gt;int4&lt;/a&gt; and &lt;a href="https://huggingface.co/openbmb/MiniCPM-o-2_6-gguf"&gt;GGUF&lt;/a&gt; format quantized models in 16 sizes, (3) &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#efficient-inference-with-llamacpp-ollama-vllm"&gt;vLLM&lt;/a&gt; support for high-throughput and memory-efficient inference, (4) fine-tuning on new domains and tasks with &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/llamafactory_train_and_infer.md"&gt;LLaMA-Factory&lt;/a&gt;, (5) quick &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#chat-with-our-demo-on-gradio"&gt;local WebUI demo&lt;/a&gt;, and (6) online web demo on &lt;a href="https://minicpm-omni-webdemo-us.modelbest.cn/"&gt;server&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Model Architecture.&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;End-to-end Omni-modal Architecture.&lt;/strong&gt; Different modality encoders/decoders are connected and trained in an &lt;strong&gt;end-to-end&lt;/strong&gt; fashion to fully exploit rich multimodal knowledge. The model is trained in a fully end-to-end manner with only CE loss.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Omni-modal Live Streaming Mechanism.&lt;/strong&gt; (1) We change the offline modality encoder/decoders into online ones for &lt;strong&gt;streaming inputs/outputs.&lt;/strong&gt; (2) We devise a &lt;strong&gt;time-division multiplexing (TDM) mechanism&lt;/strong&gt; for omni-modality streaming processing in the LLM backbone. It divides parallel omni-modality streams into sequential info within small periodic time slices.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Configurable Speech Modeling Design.&lt;/strong&gt; We devise a multimodal system prompt, including traditional text system prompt, and &lt;strong&gt;a new audio system prompt to determine the assistant voice&lt;/strong&gt;. This enables flexible voice configurations in inference time, and also facilitates end-to-end voice cloning and description-based voice creation.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpm-o-26-framework-v2.png" , width="80%" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;Evaluation 
 &lt;!-- omit in toc --&gt;&lt;/h3&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/radar.jpg" , width="80%" /&gt; 
&lt;/div&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click to view visual understanding results.&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;Image Understanding&lt;/strong&gt;&lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;table style="margin: 0px auto;"&gt; 
   &lt;thead&gt; 
    &lt;tr&gt; 
     &lt;th align="left"&gt;Model&lt;/th&gt; 
     &lt;th&gt;Size&lt;/th&gt; 
     &lt;th&gt;Token Density&lt;sup&gt;+&lt;/sup&gt;&lt;/th&gt; 
     &lt;th&gt;OpenCompass&lt;/th&gt; 
     &lt;th&gt;OCRBench&lt;/th&gt; 
     &lt;th&gt;MathVista mini&lt;/th&gt; 
     &lt;th&gt;ChartQA&lt;/th&gt; 
     &lt;th&gt;MMVet&lt;/th&gt; 
     &lt;th&gt;MMStar&lt;/th&gt; 
     &lt;th&gt;MME&lt;/th&gt; 
     &lt;th&gt;MMB1.1 test&lt;/th&gt; 
     &lt;th&gt;AI2D&lt;/th&gt; 
     &lt;th&gt;MMMU val&lt;/th&gt; 
     &lt;th&gt;HallusionBench&lt;/th&gt; 
     &lt;th&gt;TextVQA val&lt;/th&gt; 
     &lt;th&gt;DocVQA test&lt;/th&gt; 
     &lt;th&gt;MathVerse mini&lt;/th&gt; 
     &lt;th&gt;MathVision&lt;/th&gt; 
     &lt;th&gt;MMHal Score&lt;/th&gt; 
    &lt;/tr&gt; 
   &lt;/thead&gt; 
   &lt;tbody align="center"&gt; 
    &lt;tr&gt; 
     &lt;td colspan="19" align="left"&gt;&lt;strong&gt;Proprietary&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;GPT-4o-20240513&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;1088&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;69.9&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;736&lt;/td&gt; 
     &lt;td&gt;61.3&lt;/td&gt; 
     &lt;td&gt;85.7&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;69.1&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;63.9&lt;/td&gt; 
     &lt;td&gt;2328.7&lt;/td&gt; 
     &lt;td&gt;82.2&lt;/td&gt; 
     &lt;td&gt;84.6&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;69.2&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;55.0&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;92.8&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;50.2&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;30.4&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;3.6&lt;/u&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Claude3.5-Sonnet&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;750&lt;/td&gt; 
     &lt;td&gt;67.9&lt;/td&gt; 
     &lt;td&gt;788&lt;/td&gt; 
     &lt;td&gt;61.6&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;90.8&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;66.0&lt;/td&gt; 
     &lt;td&gt;62.2&lt;/td&gt; 
     &lt;td&gt;1920.0&lt;/td&gt; 
     &lt;td&gt;78.5&lt;/td&gt; 
     &lt;td&gt;80.2&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;65.9&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;49.9&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;95.2&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;3.4&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Gemini 1.5 Pro&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;64.4&lt;/td&gt; 
     &lt;td&gt;754&lt;/td&gt; 
     &lt;td&gt;57.7&lt;/td&gt; 
     &lt;td&gt;81.3&lt;/td&gt; 
     &lt;td&gt;64.0&lt;/td&gt; 
     &lt;td&gt;59.1&lt;/td&gt; 
     &lt;td&gt;2110.6&lt;/td&gt; 
     &lt;td&gt;73.9&lt;/td&gt; 
     &lt;td&gt;79.1&lt;/td&gt; 
     &lt;td&gt;60.6&lt;/td&gt; 
     &lt;td&gt;45.6&lt;/td&gt; 
     &lt;td&gt;73.5&lt;/td&gt; 
     &lt;td&gt;86.5&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;19.2&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;GPT-4o-mini-20240718&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;1088&lt;/td&gt; 
     &lt;td&gt;64.1&lt;/td&gt; 
     &lt;td&gt;785&lt;/td&gt; 
     &lt;td&gt;52.4&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;66.9&lt;/td&gt; 
     &lt;td&gt;54.8&lt;/td&gt; 
     &lt;td&gt;2003.4&lt;/td&gt; 
     &lt;td&gt;76.0&lt;/td&gt; 
     &lt;td&gt;77.8&lt;/td&gt; 
     &lt;td&gt;60.0&lt;/td&gt; 
     &lt;td&gt;46.1&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;3.3&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td colspan="19" align="left"&gt;&lt;strong&gt;Open Source&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Cambrian-34B&lt;/td&gt; 
     &lt;td&gt;34B&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;1820&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;58.3&lt;/td&gt; 
     &lt;td&gt;591&lt;/td&gt; 
     &lt;td&gt;50.3&lt;/td&gt; 
     &lt;td&gt;75.6&lt;/td&gt; 
     &lt;td&gt;53.2&lt;/td&gt; 
     &lt;td&gt;54.2&lt;/td&gt; 
     &lt;td&gt;2049.9&lt;/td&gt; 
     &lt;td&gt;77.8&lt;/td&gt; 
     &lt;td&gt;79.5&lt;/td&gt; 
     &lt;td&gt;50.4&lt;/td&gt; 
     &lt;td&gt;41.6&lt;/td&gt; 
     &lt;td&gt;76.7&lt;/td&gt; 
     &lt;td&gt;75.5&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;GLM-4V-9B&lt;/td&gt; 
     &lt;td&gt;13B&lt;/td&gt; 
     &lt;td&gt;784&lt;/td&gt; 
     &lt;td&gt;59.1&lt;/td&gt; 
     &lt;td&gt;776&lt;/td&gt; 
     &lt;td&gt;51.1&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;58.0&lt;/td&gt; 
     &lt;td&gt;54.8&lt;/td&gt; 
     &lt;td&gt;2018.8&lt;/td&gt; 
     &lt;td&gt;67.9&lt;/td&gt; 
     &lt;td&gt;71.2&lt;/td&gt; 
     &lt;td&gt;46.9&lt;/td&gt; 
     &lt;td&gt;45.0&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Pixtral-12B&lt;/td&gt; 
     &lt;td&gt;12B&lt;/td&gt; 
     &lt;td&gt;256&lt;/td&gt; 
     &lt;td&gt;61.0&lt;/td&gt; 
     &lt;td&gt;685&lt;/td&gt; 
     &lt;td&gt;56.9&lt;/td&gt; 
     &lt;td&gt;81.8&lt;/td&gt; 
     &lt;td&gt;58.5&lt;/td&gt; 
     &lt;td&gt;54.5&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;72.7&lt;/td&gt; 
     &lt;td&gt;79.0&lt;/td&gt; 
     &lt;td&gt;51.1&lt;/td&gt; 
     &lt;td&gt;47.0&lt;/td&gt; 
     &lt;td&gt;75.7&lt;/td&gt; 
     &lt;td&gt;90.7&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;VITA-1.5&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;784&lt;/td&gt; 
     &lt;td&gt;63.3&lt;/td&gt; 
     &lt;td&gt;741&lt;/td&gt; 
     &lt;td&gt;66.2&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;52.7&lt;/td&gt; 
     &lt;td&gt;60.2&lt;/td&gt; 
     &lt;td&gt;2328.1&lt;/td&gt; 
     &lt;td&gt;76.8&lt;/td&gt; 
     &lt;td&gt;79.2&lt;/td&gt; 
     &lt;td&gt;52.6&lt;/td&gt; 
     &lt;td&gt;44.6&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;DeepSeek-VL2-27B (4B)&lt;/td&gt; 
     &lt;td&gt;27B&lt;/td&gt; 
     &lt;td&gt;672&lt;/td&gt; 
     &lt;td&gt;66.4&lt;/td&gt; 
     &lt;td&gt;809&lt;/td&gt; 
     &lt;td&gt;63.9&lt;/td&gt; 
     &lt;td&gt;86.0&lt;/td&gt; 
     &lt;td&gt;60.0&lt;/td&gt; 
     &lt;td&gt;61.9&lt;/td&gt; 
     &lt;td&gt;2253.0&lt;/td&gt; 
     &lt;td&gt;81.2&lt;/td&gt; 
     &lt;td&gt;83.8&lt;/td&gt; 
     &lt;td&gt;54.0&lt;/td&gt; 
     &lt;td&gt;45.3&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;84.2&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;93.3&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;3.0&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Qwen2-VL-7B&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;784&lt;/td&gt; 
     &lt;td&gt;67.1&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;866&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;58.2&lt;/td&gt; 
     &lt;td&gt;83.0&lt;/td&gt; 
     &lt;td&gt;62.0&lt;/td&gt; 
     &lt;td&gt;60.7&lt;/td&gt; 
     &lt;td&gt;2326.0&lt;/td&gt; 
     &lt;td&gt;81.8&lt;/td&gt; 
     &lt;td&gt;83.0&lt;/td&gt; 
     &lt;td&gt;54.1&lt;/td&gt; 
     &lt;td&gt;50.6&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;84.3&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;94.5&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;31.9&lt;/td&gt; 
     &lt;td&gt;16.3&lt;/td&gt; 
     &lt;td&gt;3.2&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;LLaVA-OneVision-72B&lt;/td&gt; 
     &lt;td&gt;72B&lt;/td&gt; 
     &lt;td&gt;182&lt;/td&gt; 
     &lt;td&gt;68.1&lt;/td&gt; 
     &lt;td&gt;741&lt;/td&gt; 
     &lt;td&gt;67.5&lt;/td&gt; 
     &lt;td&gt;83.7&lt;/td&gt; 
     &lt;td&gt;60.6&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;65.8&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;2261.0&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;85.0&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;85.6&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;56.8&lt;/td&gt; 
     &lt;td&gt;49.0&lt;/td&gt; 
     &lt;td&gt;80.5&lt;/td&gt; 
     &lt;td&gt;91.3&lt;/td&gt; 
     &lt;td&gt;39.1&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;3.5&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;InternVL2.5-8B&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;706&lt;/td&gt; 
     &lt;td&gt;68.3&lt;/td&gt; 
     &lt;td&gt;822&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;64.4&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;84.8&lt;/td&gt; 
     &lt;td&gt;62.8&lt;/td&gt; 
     &lt;td&gt;62.8&lt;/td&gt; 
     &lt;td&gt;2344.0&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;83.6&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;84.5&lt;/td&gt; 
     &lt;td&gt;56.0&lt;/td&gt; 
     &lt;td&gt;50.1&lt;/td&gt; 
     &lt;td&gt;79.1&lt;/td&gt; 
     &lt;td&gt;93.0&lt;/td&gt; 
     &lt;td&gt;39.5&lt;/td&gt; 
     &lt;td&gt;19.7&lt;/td&gt; 
     &lt;td&gt;3.4&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;MiniCPM-V 2.6&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;2822&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;65.2&lt;/td&gt; 
     &lt;td&gt;852*&lt;/td&gt; 
     &lt;td&gt;60.6&lt;/td&gt; 
     &lt;td&gt;79.4&lt;/td&gt; 
     &lt;td&gt;60.0&lt;/td&gt; 
     &lt;td&gt;57.5&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;2348.4*&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;78.0&lt;/td&gt; 
     &lt;td&gt;82.1&lt;/td&gt; 
     &lt;td&gt;49.8*&lt;/td&gt; 
     &lt;td&gt;48.1*&lt;/td&gt; 
     &lt;td&gt;80.1&lt;/td&gt; 
     &lt;td&gt;90.8&lt;/td&gt; 
     &lt;td&gt;25.7&lt;/td&gt; 
     &lt;td&gt;18.3&lt;/td&gt; 
     &lt;td&gt;3.6&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;MiniCPM-o 2.6&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;2822&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;70.2&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;897*&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;71.9*&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;86.9*&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;67.5&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;64.0&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;2372.0*&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;80.5&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;85.8&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;50.4*&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;51.9&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;82.0&lt;/td&gt; 
     &lt;td&gt;93.5&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;41.4*&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;23.1*&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;3.8&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt; 
  &lt;/table&gt; 
 &lt;/div&gt; * We evaluate this benchmark using chain-of-thought prompting. Specifically, for MME, we used this technique only for the Cognition set. 
 &lt;p&gt;&lt;sup&gt;+&lt;/sup&gt; Token Density: number of pixels encoded into each visual token at maximum resolution, i.e., # pixels at maximum resolution / # visual tokens.&lt;/p&gt; 
 &lt;p&gt;Note: For proprietary models, we calculate token density based on the image encoding charging strategy defined in the official API documentation, which provides an upper-bound estimation.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Multi-image and Video Understanding&lt;/strong&gt;&lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;table style="margin: 0px auto;"&gt; 
   &lt;thead&gt; 
    &lt;tr&gt; 
     &lt;th align="left"&gt;Model&lt;/th&gt; 
     &lt;th&gt;Size&lt;/th&gt; 
     &lt;th&gt;BLINK val&lt;/th&gt; 
     &lt;th&gt;Mantis Eval&lt;/th&gt; 
     &lt;th&gt;MIRB&lt;/th&gt; 
     &lt;th&gt;Video-MME (wo / w subs)&lt;/th&gt; 
    &lt;/tr&gt; 
   &lt;/thead&gt; 
   &lt;tbody align="center"&gt; 
    &lt;tr&gt; 
     &lt;td colspan="6" align="left"&gt;&lt;strong&gt;Proprietary&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;GPT-4o-20240513&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;68.0&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;71.9/77.2&lt;strong&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;GPT4V&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;54.6&lt;/td&gt; 
     &lt;td&gt;62.7&lt;/td&gt; 
     &lt;td&gt;53.1&lt;/td&gt; 
     &lt;td&gt;59.9/63.3&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td colspan="6" align="left"&gt;&lt;strong&gt;Open-source&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;VITA-1.5&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;45.0&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;56.1/58.7&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;LLaVA-NeXT-Interleave 14B&lt;/td&gt; 
     &lt;td&gt;14B&lt;/td&gt; 
     &lt;td&gt;52.6&lt;/td&gt; 
     &lt;td&gt;66.4&lt;/td&gt; 
     &lt;td&gt;30.2&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;LLaVA-OneVision-72B&lt;/td&gt; 
     &lt;td&gt;72B&lt;/td&gt; 
     &lt;td&gt;55.4&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;77.6&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;66.2/69.5&lt;/u&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;MANTIS 8B&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;49.1&lt;/td&gt; 
     &lt;td&gt;59.5&lt;/td&gt; 
     &lt;td&gt;34.8&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Qwen2-VL-7B&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;53.2&lt;/td&gt; 
     &lt;td&gt;69.6*&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;67.6*&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;63.3/69.0&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;InternVL2.5-8B&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;54.8&lt;/td&gt; 
     &lt;td&gt;67.7&lt;/td&gt; 
     &lt;td&gt;52.5&lt;/td&gt; 
     &lt;td&gt;64.2/66.9&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;MiniCPM-V 2.6&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;53.0&lt;/td&gt; 
     &lt;td&gt;69.1&lt;/td&gt; 
     &lt;td&gt;53.8&lt;/td&gt; 
     &lt;td&gt;60.9/63.6&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;MiniCPM-o 2.6&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;56.7&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;71.9&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;58.6&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;63.9/67.9&lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt; 
  &lt;/table&gt; 
 &lt;/div&gt; * We evaluate officially released checkpoints by ourselves. 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click to view audio understanding and speech conversation results.&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;Audio Understanding&lt;/strong&gt;&lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;table style="margin: 0px auto;"&gt; 
   &lt;thead&gt; 
    &lt;tr&gt; 
     &lt;th align="left"&gt;Task&lt;/th&gt; 
     &lt;th&gt;Size&lt;/th&gt; 
     &lt;th colspan="3"&gt;ASR (zh)&lt;/th&gt; 
     &lt;th colspan="3"&gt;ASR (en)&lt;/th&gt; 
     &lt;th colspan="2"&gt;AST&lt;/th&gt; 
     &lt;th&gt;Emotion&lt;/th&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;th align="left"&gt;Metric&lt;/th&gt; 
     &lt;td&gt;&lt;/td&gt; 
     &lt;th colspan="3"&gt;CER‚Üì&lt;/th&gt; 
     &lt;th colspan="3"&gt;WER‚Üì&lt;/th&gt; 
     &lt;th colspan="2"&gt;BLEU‚Üë&lt;/th&gt; 
     &lt;th&gt;ACC‚Üë&lt;/th&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;th align="left"&gt;Dataset&lt;/th&gt; 
     &lt;td&gt;&lt;/td&gt; 
     &lt;th&gt;AISHELL-1&lt;/th&gt; 
     &lt;th&gt;Fleurs zh&lt;/th&gt; 
     &lt;th&gt;WenetSpeech test-net&lt;/th&gt; 
     &lt;th&gt;LibriSpeech test-clean&lt;/th&gt; 
     &lt;th&gt;GigaSpeech&lt;/th&gt; 
     &lt;th&gt;TED-LIUM&lt;/th&gt; 
     &lt;th&gt;CoVoST en2zh&lt;/th&gt; 
     &lt;th&gt;CoVoST zh2en&lt;/th&gt; 
     &lt;th&gt;MELD emotion&lt;/th&gt; 
    &lt;/tr&gt; 
   &lt;/thead&gt; 
   &lt;tbody align="center"&gt; 
    &lt;tr&gt; 
     &lt;td colspan="11" align="left"&gt;&lt;strong&gt;Proprietary&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;GPT-4o-Realtime&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;7.3*&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;5.4*&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;28.9*&lt;/td&gt; 
     &lt;td&gt;2.6*&lt;/td&gt; 
     &lt;td&gt;12.9*&lt;/td&gt; 
     &lt;td&gt;4.8*&lt;/td&gt; 
     &lt;td&gt;37.1*&lt;/td&gt; 
     &lt;td&gt;15.7*&lt;/td&gt; 
     &lt;td&gt;33.2*&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Gemini 1.5 Pro&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;4.5*&lt;/td&gt; 
     &lt;td&gt;5.9*&lt;/td&gt; 
     &lt;td&gt;14.3*&lt;/td&gt; 
     &lt;td&gt;2.9*&lt;/td&gt; 
     &lt;td&gt;10.6*&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;3.0*&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;47.3*&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;22.6*&lt;/td&gt; 
     &lt;td&gt;48.4*&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td colspan="11" align="left"&gt;&lt;strong&gt;Open-Source&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Qwen2-Audio-7B&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;7.5&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;1.6&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;45.2&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;24.4&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;55.3&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Qwen2-Audio-7B-Instruct&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;2.6*&lt;/td&gt; 
     &lt;td&gt;6.9*&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;10.3*&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;3.1*&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;9.7&lt;/u&gt;*&lt;/td&gt; 
     &lt;td&gt;5.9*&lt;/td&gt; 
     &lt;td&gt;39.5*&lt;/td&gt; 
     &lt;td&gt;22.9*&lt;/td&gt; 
     &lt;td&gt;17.4*&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;VITA-1.5&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;2.16&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;8.4&lt;/td&gt; 
     &lt;td&gt;3.4&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;GLM-4-Voice-Base&lt;/td&gt; 
     &lt;td&gt;9B&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;2.5&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;2.8&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;MiniCPM-o 2.6&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;1.6&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;4.4&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;6.9&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;1.7&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;8.7&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;3.0&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;48.2&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;27.2&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;52.4&lt;/u&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt; 
  &lt;/table&gt; 
 &lt;/div&gt; * We evaluate officially released checkpoints by ourselves.
 &lt;br /&gt;
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;Speech Generation&lt;/strong&gt;&lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;table style="margin: 0px auto;"&gt; 
   &lt;thead&gt; 
    &lt;tr&gt; 
     &lt;th align="left"&gt;Task&lt;/th&gt; 
     &lt;th&gt;Size&lt;/th&gt; 
     &lt;th colspan="9"&gt;SpeechQA&lt;/th&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;th align="left"&gt;Metric&lt;/th&gt; 
     &lt;th&gt;&lt;/th&gt; 
     &lt;th colspan="3"&gt;ACC‚Üë&lt;/th&gt; 
     &lt;th&gt;G-Eval (10 point)‚Üë&lt;/th&gt; 
     &lt;th&gt;Semantic ELO score‚Üë&lt;/th&gt; 
     &lt;th&gt;Acoustic ELO score‚Üë&lt;/th&gt; 
     &lt;th&gt;Overall ELO score‚Üë&lt;/th&gt; 
     &lt;th&gt;UTMOS‚Üë&lt;/th&gt; 
     &lt;th&gt;ASR-WER‚Üì&lt;/th&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;th align="left"&gt;Dataset&lt;/th&gt; 
     &lt;th&gt;&lt;/th&gt; 
     &lt;th&gt;Speech Llama Q.&lt;/th&gt; 
     &lt;th&gt;Speech Web Q.&lt;/th&gt; 
     &lt;th&gt;Speech Trivia QA&lt;/th&gt; 
     &lt;th&gt;Speech AlpacaEval&lt;/th&gt; 
     &lt;th colspan="5"&gt;AudioArena&lt;/th&gt; 
    &lt;/tr&gt; 
   &lt;/thead&gt; 
   &lt;tbody align="center"&gt; 
    &lt;tr&gt; 
     &lt;td colspan="11" align="left"&gt;&lt;strong&gt;Proprietary&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;GPT-4o-Realtime&lt;/td&gt; 
     &lt;td&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;71.7&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;51.6&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;69.7&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;7.4&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;1157&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;1203&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;1200&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;4.2&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;2.3&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td colspan="11" align="left"&gt;&lt;strong&gt;Open-Source&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;GLM-4-Voice&lt;/td&gt; 
     &lt;td&gt;9B&lt;/td&gt; 
     &lt;td&gt;50.0&lt;/td&gt; 
     &lt;td&gt;32.0&lt;/td&gt; 
     &lt;td&gt;36.4&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;5.1&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;999&lt;/td&gt; 
     &lt;td&gt;1147&lt;/td&gt; 
     &lt;td&gt;1035&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;4.1&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;11.7&lt;/u&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Llama-Omni&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;45.3&lt;/td&gt; 
     &lt;td&gt;22.9&lt;/td&gt; 
     &lt;td&gt;10.7&lt;/td&gt; 
     &lt;td&gt;3.9&lt;/td&gt; 
     &lt;td&gt;960&lt;/td&gt; 
     &lt;td&gt;878&lt;/td&gt; 
     &lt;td&gt;897&lt;/td&gt; 
     &lt;td&gt;3.2&lt;/td&gt; 
     &lt;td&gt;24.3&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;VITA-1.5&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;46.7&lt;/td&gt; 
     &lt;td&gt;28.1&lt;/td&gt; 
     &lt;td&gt;23.3&lt;/td&gt; 
     &lt;td&gt;2.0&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Moshi&lt;/td&gt; 
     &lt;td&gt;7B&lt;/td&gt; 
     &lt;td&gt;43.7&lt;/td&gt; 
     &lt;td&gt;23.8&lt;/td&gt; 
     &lt;td&gt;16.7&lt;/td&gt; 
     &lt;td&gt;2.4&lt;/td&gt; 
     &lt;td&gt;871&lt;/td&gt; 
     &lt;td&gt;808&lt;/td&gt; 
     &lt;td&gt;875&lt;/td&gt; 
     &lt;td&gt;2.8&lt;/td&gt; 
     &lt;td&gt;8.2&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Mini-Omni&lt;/td&gt; 
     &lt;td&gt;1B&lt;/td&gt; 
     &lt;td&gt;22.0&lt;/td&gt; 
     &lt;td&gt;12.8&lt;/td&gt; 
     &lt;td&gt;6.9&lt;/td&gt; 
     &lt;td&gt;2.5&lt;/td&gt; 
     &lt;td&gt;926&lt;/td&gt; 
     &lt;td&gt;803&lt;/td&gt; 
     &lt;td&gt;865&lt;/td&gt; 
     &lt;td&gt;3.4&lt;/td&gt; 
     &lt;td&gt;10.0&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;MiniCPM-o 2.6&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;61.0&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;40.0&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;40.2&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;5.1&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;1088&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;1163&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;1131&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;4.2&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;9.8&lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt; 
  &lt;/table&gt; 
 &lt;/div&gt; All results are from AudioEvals, and the evaluation methods along with further details can be found in 
 &lt;a href="https://github.com/OpenBMB/UltraEval-Audio" target="_blank"&gt;AudioEvals&lt;/a&gt;.
 &lt;br /&gt;
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;End-to-end Voice Cloning&lt;/strong&gt;&lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;table style="margin: 0px auto;"&gt; 
   &lt;thead&gt; 
    &lt;tr&gt; 
     &lt;th align="left"&gt;Task&lt;/th&gt; 
     &lt;th colspan="2"&gt;Voice cloning&lt;/th&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;th align="left"&gt;Metric&lt;/th&gt; 
     &lt;th&gt;SIMO‚Üë&lt;/th&gt; 
     &lt;th&gt;SIMO‚Üë&lt;/th&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;th align="left"&gt;Dataset&lt;/th&gt; 
     &lt;th&gt;Seed-TTS test-zh&lt;/th&gt; 
     &lt;th&gt;Seed-TTS test-en&lt;/th&gt; 
    &lt;/tr&gt; 
   &lt;/thead&gt; 
   &lt;tbody align="center"&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;F5-TTS&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;76&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;67&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;CosyVoice&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;75&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;64&lt;/u&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;FireRedTTS&lt;/td&gt; 
     &lt;td&gt;63&lt;/td&gt; 
     &lt;td&gt;46&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;MiniCPM-o 2.6&lt;/td&gt; 
     &lt;td&gt;57&lt;/td&gt; 
     &lt;td&gt;47&lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt; 
  &lt;/table&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click to view multimodal live streaming results.&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;Multimodal Live Streaming&lt;/strong&gt;: results on StreamingBench&lt;/p&gt; 
 &lt;table style="margin: 0px auto;"&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="left"&gt;Model&lt;/th&gt; 
    &lt;th&gt;Size&lt;/th&gt; 
    &lt;th&gt;Real-Time Video Understanding&lt;/th&gt; 
    &lt;th&gt;Omni-Source Understanding&lt;/th&gt; 
    &lt;th&gt;Contextual Understanding&lt;/th&gt; 
    &lt;th&gt;Overall&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody align="center"&gt; 
   &lt;tr&gt; 
    &lt;td colspan="7" align="left"&gt;&lt;strong&gt;Proprietary&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;Gemini 1.5 Pro&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;&lt;u&gt;77.4&lt;/u&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;67.8&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;51.1&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;70.3&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;GPT-4o-202408&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;74.5&lt;/td&gt; 
    &lt;td&gt;51.0&lt;/td&gt; 
    &lt;td&gt;&lt;u&gt;48.0&lt;/u&gt;&lt;/td&gt; 
    &lt;td&gt;64.1&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;Claude-3.5-Sonnet&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;74.0&lt;/td&gt; 
    &lt;td&gt;41.4&lt;/td&gt; 
    &lt;td&gt;37.8&lt;/td&gt; 
    &lt;td&gt;59.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td colspan="9" align="left"&gt;&lt;strong&gt;Open-source&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;VILA-1.5&lt;/td&gt; 
    &lt;td&gt;8B&lt;/td&gt; 
    &lt;td&gt;61.5&lt;/td&gt; 
    &lt;td&gt;37.5&lt;/td&gt; 
    &lt;td&gt;26.7&lt;/td&gt; 
    &lt;td&gt;49.5&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;LongVA&lt;/td&gt; 
    &lt;td&gt;7B&lt;/td&gt; 
    &lt;td&gt;63.1&lt;/td&gt; 
    &lt;td&gt;35.9&lt;/td&gt; 
    &lt;td&gt;30.2&lt;/td&gt; 
    &lt;td&gt;50.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;LLaVA-Next-Video-34B&lt;/td&gt; 
    &lt;td&gt;34B&lt;/td&gt; 
    &lt;td&gt;69.8&lt;/td&gt; 
    &lt;td&gt;41.7&lt;/td&gt; 
    &lt;td&gt;34.3&lt;/td&gt; 
    &lt;td&gt;56.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;Qwen2-VL-7B&lt;/td&gt; 
    &lt;td&gt;8B&lt;/td&gt; 
    &lt;td&gt;71.2&lt;/td&gt; 
    &lt;td&gt;40.7&lt;/td&gt; 
    &lt;td&gt;33.1&lt;/td&gt; 
    &lt;td&gt;57.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;InternVL2-8B&lt;/td&gt; 
    &lt;td&gt;8B&lt;/td&gt; 
    &lt;td&gt;70.1&lt;/td&gt; 
    &lt;td&gt;42.7&lt;/td&gt; 
    &lt;td&gt;34.1&lt;/td&gt; 
    &lt;td&gt;57.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;VITA-1.5&lt;/td&gt; 
    &lt;td&gt;8B&lt;/td&gt; 
    &lt;td&gt;70.9&lt;/td&gt; 
    &lt;td&gt;40.8&lt;/td&gt; 
    &lt;td&gt;35.8&lt;/td&gt; 
    &lt;td&gt;57.4&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;LLaVA-OneVision-7B&lt;/td&gt; 
    &lt;td&gt;8B&lt;/td&gt; 
    &lt;td&gt;74.3&lt;/td&gt; 
    &lt;td&gt;40.8&lt;/td&gt; 
    &lt;td&gt;31.0&lt;/td&gt; 
    &lt;td&gt;58.4&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;InternLM-XC2.5-OL-7B&lt;/td&gt; 
    &lt;td&gt;8B&lt;/td&gt; 
    &lt;td&gt;75.4&lt;/td&gt; 
    &lt;td&gt;46.2&lt;/td&gt; 
    &lt;td&gt;33.6&lt;/td&gt; 
    &lt;td&gt;60.8&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;MiniCPM-V 2.6&lt;/td&gt; 
    &lt;td&gt;8B&lt;/td&gt; 
    &lt;td&gt;72.4&lt;/td&gt; 
    &lt;td&gt;40.2&lt;/td&gt; 
    &lt;td&gt;33.4&lt;/td&gt; 
    &lt;td&gt;57.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;MiniCPM-o 2.6&lt;/td&gt; 
    &lt;td&gt;8B&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;79.9&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;u&gt;53.4&lt;/u&gt;&lt;/td&gt; 
    &lt;td&gt;38.5&lt;/td&gt; 
    &lt;td&gt;&lt;u&gt;66.0&lt;/u&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h3&gt;Examples 
 &lt;!-- omit in toc --&gt;&lt;/h3&gt; 
&lt;p&gt;We deploy MiniCPM-o 2.6 on end devices. The demo video is the raw-speed recording on an iPad Pro and a Web demo.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://www.youtube.com/watch?v=vRIMbxJzStY&amp;amp;t=2s"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmo2_6/2dot6_o_demo_video_img.png" , width="70%" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;div style="display: flex; flex-direction: column; align-items: center;"&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmo2_6/minicpmo2_6_math_intersect.png" alt="math" style="margin-bottom: 5px;" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmo2_6/minicpmo2_6_diagram_train_NN.png" alt="diagram" style="margin-bottom: 5px;" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmo2_6/minicpmo2_6_multi-image_bike.png" alt="bike" style="margin-bottom: 5px;" /&gt; 
&lt;/div&gt; 
&lt;h2&gt;Legacy Models 
 &lt;!-- omit in toc --&gt;&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Model&lt;/th&gt; 
   &lt;th align="center"&gt;Introduction and Guidance&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-V 4.0&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/minicpm_v4_en.md"&gt;Document&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-V 2.6&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/minicpm_v2dot6_en.md"&gt;Document&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-Llama3-V 2.5&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/minicpm_llama3_v2dot5.md"&gt;Document&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-V 2.0&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/minicpm_v2.md"&gt;Document&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-V 1.0&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/minicpm_v1.md"&gt;Document&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;OmniLMM-12B&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/omnilmm_en.md"&gt;Document&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;MiniCPM-V &amp;amp; o Cookbook&lt;/h2&gt; 
&lt;p&gt;Discover comprehensive, ready-to-deploy solutions for the MiniCPM-V and MiniCPM-o model series in our structured &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook"&gt;cookbook&lt;/a&gt;, which empowers developers to rapidly implement multimodal AI applications with integrated vision, speech, and live-streaming capabilities. Key features include:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Easy Usage Documentation&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Our comprehensive &lt;a href="https://minicpm-o.readthedocs.io/en/latest/index.html"&gt;documentation website&lt;/a&gt; presents every recipe in a clear, well-organized manner. All features are displayed at a glance, making it easy for you to quickly find exactly what you need.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Broad User Spectrum&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;We support a wide range of users, from individuals to enterprises and researchers.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Individuals&lt;/strong&gt;: Enjoy effortless inference using &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/deployment/ollama/minicpm-v4_ollama.md"&gt;Ollama&lt;/a&gt; and &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/deployment/llama.cpp/minicpm-v4_llamacpp.md"&gt;Llama.cpp&lt;/a&gt; with minimal setup.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Enterprises&lt;/strong&gt;: Achieve high-throughput, scalable performance with &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/deployment/vllm/minicpm-v4_vllm.md"&gt;vLLM&lt;/a&gt; and &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/deployment/sglang/MiniCPM-v4_sglang.md"&gt;SGLang&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Researchers&lt;/strong&gt;: Leverage advanced frameworks including &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/finetune/finetune_full.md"&gt;Transformers&lt;/a&gt;, &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/finetune/finetune_llamafactory.md"&gt;LLaMA-Factory&lt;/a&gt;, &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/finetune/swift.md"&gt;SWIFT&lt;/a&gt;, and &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/finetune/align_anything.md"&gt;Align-anything&lt;/a&gt; to enable flexible model development and cutting-edge experimentation.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Versatile Deployment Scenarios&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Our ecosystem delivers optimal solution for a variety of hardware environments and deployment demands.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Web demo&lt;/strong&gt;: Launch interactive multimodal AI web demo with &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/demo/README.md"&gt;FastAPI&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Quantized deployment&lt;/strong&gt;: Maximize efficiency and minimize resource consumption using &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/quantization/gguf/minicpm-v4_gguf_quantize.md"&gt;GGUF&lt;/a&gt; and &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/quantization/bnb/minicpm-v4_bnb_quantize.md"&gt;BNB&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;End devices&lt;/strong&gt;: Bring powerful AI experiences to &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/demo/ios_demo/ios.md"&gt;iPhone and iPad&lt;/a&gt;, supporting offline and privacy-sensitive applications.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Chat with Our Demo on Gradio ü§ó&lt;/h2&gt; 
&lt;p&gt;We provide online and local demos powered by Hugging Face Gradio &lt;a href="https://github.com/gradio-app/gradio"&gt;&lt;img src="https://img.shields.io/github/stars/gradio-app/gradio" /&gt;&lt;/a&gt;, the most popular model deployment framework nowadays. It supports streaming outputs, progress bars, queuing, alerts, and other useful features.&lt;/p&gt; 
&lt;h3&gt;Online Demo 
 &lt;!-- omit in toc --&gt;&lt;/h3&gt; 
&lt;p&gt;Click here to try out the online demo of &lt;a href="https://minicpm-omni-webdemo-us.modelbest.cn/"&gt;MiniCPM-o 2.6&lt;/a&gt; | &lt;a href="http://120.92.209.146:8887/"&gt;MiniCPM-V 2.6&lt;/a&gt; | &lt;a href="https://huggingface.co/spaces/openbmb/MiniCPM-Llama3-V-2_5"&gt;MiniCPM-Llama3-V 2.5&lt;/a&gt; | &lt;a href="https://huggingface.co/spaces/openbmb/MiniCPM-V-2"&gt;MiniCPM-V 2.0&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Local WebUI Demo 
 &lt;!-- omit in toc --&gt;&lt;/h3&gt; 
&lt;p&gt;You can easily build your own local WebUI demo using the following commands.&lt;/p&gt; 
&lt;p&gt;Please ensure that &lt;code&gt;transformers==4.44.2&lt;/code&gt; is installed, as other versions may have compatibility issues.&lt;/p&gt; 
&lt;p&gt;If you are using an older version of PyTorch, you might encounter this issue &lt;code&gt;"weight_norm_fwd_first_dim_kernel" not implemented for 'BFloat16'&lt;/code&gt;, Please add &lt;code&gt;self.minicpmo_model.tts.float()&lt;/code&gt; during the model initialization.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;For real-time voice/video call demo:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;launch model server:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install -r requirements_o2.6.txt

python web_demos/minicpm-o_2.6/model_server.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;launch web server:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# Make sure Node and PNPM is installed.
sudo apt-get update
sudo apt-get install nodejs npm
npm install -g pnpm


cd web_demos/minicpm-o_2.6/web_server
# create ssl cert for https, https is required to request camera and microphone permissions.
bash ./make_ssl_cert.sh  # output key.pem and cert.pem

pnpm install  # install requirements
pnpm run dev  # start server
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Open &lt;code&gt;https://localhost:8088/&lt;/code&gt; in browser and enjoy the real-time voice/video call.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;For chatbot demo:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install -r requirements_o2.6.txt

python web_demos/minicpm-o_2.6/chatbot_web_demo_o2.6.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Open &lt;code&gt;http://localhost:8000/&lt;/code&gt; in browser and enjoy the vision mode chatbot.&lt;/p&gt; 
&lt;h2&gt;Inference&lt;/h2&gt; 
&lt;h3&gt;Model Zoo&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Model&lt;/th&gt; 
   &lt;th align="center"&gt;Device&lt;/th&gt; 
   &lt;th align="center"&gt;Memory&lt;/th&gt; 
   &lt;th align="left"&gt;‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ Description&lt;/th&gt; 
   &lt;th align="center"&gt;Download&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-V 4.5&lt;/td&gt; 
   &lt;td align="center"&gt;GPU&lt;/td&gt; 
   &lt;td align="center"&gt;18 GB&lt;/td&gt; 
   &lt;td align="left"&gt;The latest version, strong end-side multimodal performance for single image, multi-image and video understanding.&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/openbmb/MiniCPM-V-4_5"&gt;ü§ó&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href="https://modelscope.cn/models/OpenBMB/MiniCPM-V-4_5"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelscope_logo.png" width="20px" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-V 4.5 gguf&lt;/td&gt; 
   &lt;td align="center"&gt;CPU&lt;/td&gt; 
   &lt;td align="center"&gt;8 GB&lt;/td&gt; 
   &lt;td align="left"&gt;The gguf version, lower memory usage and faster inference.&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/openbmb/MiniCPM-V-4_5-gguf"&gt;ü§ó&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href="https://modelscope.cn/models/OpenBMB/MiniCPM-V-4_5-gguf"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelscope_logo.png" width="20px" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-V 4.5 int4&lt;/td&gt; 
   &lt;td align="center"&gt;GPU&lt;/td&gt; 
   &lt;td align="center"&gt;9 GB&lt;/td&gt; 
   &lt;td align="left"&gt;The int4 quantized version, lower GPU memory usage.&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/openbmb/MiniCPM-V-4_5-int4"&gt;ü§ó&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href="https://modelscope.cn/models/OpenBMB/MiniCPM-V-4_5-int4"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelscope_logo.png" width="20px" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-V 4.5 AWQ&lt;/td&gt; 
   &lt;td align="center"&gt;GPU&lt;/td&gt; 
   &lt;td align="center"&gt;9 GB&lt;/td&gt; 
   &lt;td align="left"&gt;The int4 quantized version, lower GPU memory usage.&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/openbmb/MiniCPM-V-4_5-AWQ"&gt;ü§ó&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href="https://modelscope.cn/models/OpenBMB/MiniCPM-V-4_5-AWQ"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelscope_logo.png" width="20px" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-o 2.6&lt;/td&gt; 
   &lt;td align="center"&gt;GPU&lt;/td&gt; 
   &lt;td align="center"&gt;18 GB&lt;/td&gt; 
   &lt;td align="left"&gt;The latest version, achieving GPT-4o level performance for vision, speech and multimodal live streaming on end-side devices.&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/openbmb/MiniCPM-o-2_6"&gt;ü§ó&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href="https://modelscope.cn/models/OpenBMB/MiniCPM-o-2_6"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelscope_logo.png" width="20px" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-o 2.6 gguf&lt;/td&gt; 
   &lt;td align="center"&gt;CPU&lt;/td&gt; 
   &lt;td align="center"&gt;8 GB&lt;/td&gt; 
   &lt;td align="left"&gt;The gguf version, lower memory usage and faster inference.&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/openbmb/MiniCPM-o-2_6-gguf"&gt;ü§ó&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href="https://modelscope.cn/models/OpenBMB/MiniCPM-o-2_6-gguf"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelscope_logo.png" width="20px" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-o 2.6 int4&lt;/td&gt; 
   &lt;td align="center"&gt;GPU&lt;/td&gt; 
   &lt;td align="center"&gt;9 GB&lt;/td&gt; 
   &lt;td align="left"&gt;The int4 quantized version, lower GPU memory usage.&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/openbmb/MiniCPM-o-2_6-int4"&gt;ü§ó&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href="https://modelscope.cn/models/OpenBMB/MiniCPM-o-2_6-int4"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelscope_logo.png" width="20px" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Multi-turn Conversation&lt;/h3&gt; 
&lt;p&gt;If you wish to enable long-thinking mode, provide the argument &lt;code&gt;enable_thinking=True&lt;/code&gt; to the chat function.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install -r requirements_o2.6.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Please refer to the following codes to run.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmo2_6/show_demo.jpg" width="500px" /&gt; 
&lt;/div&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import torch
from PIL import Image
from transformers import AutoModel, AutoTokenizer

torch.manual_seed(100)

model = AutoModel.from_pretrained('openbmb/MiniCPM-V-4_5', trust_remote_code=True, # or openbmb/MiniCPM-o-2_6
    attn_implementation='sdpa', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager
model = model.eval().cuda()
tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-4_5', trust_remote_code=True) # or openbmb/MiniCPM-o-2_6

image = Image.open('./assets/minicpmo2_6/show_demo.jpg').convert('RGB')

enable_thinking=False # If `enable_thinking=True`, the long-thinking mode is enabled.

# First round chat 
question = "What is the landform in the picture?"
msgs = [{'role': 'user', 'content': [image, question]}]

answer = model.chat(
    msgs=msgs,
    tokenizer=tokenizer,
    enable_thinking=enable_thinking
)
print(answer)

# Second round chat, pass history context of multi-turn conversation
msgs.append({"role": "assistant", "content": [answer]})
msgs.append({"role": "user", "content": ["What should I pay attention to when traveling here?"]})

answer = model.chat(
    msgs=msgs,
    tokenizer=tokenizer
)
print(answer)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You will get the following output:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# round1
The landform in the picture is karst topography. Karst landscapes are characterized by distinctive, jagged limestone hills or mountains with steep, irregular peaks and deep valleys‚Äîexactly what you see here These unique formations result from the dissolution of soluble rocks like limestone over millions of years through water erosion.

This scene closely resembles the famous karst landscape of Guilin and Yangshuo in China‚Äôs Guangxi Province. The area features dramatic, pointed limestone peaks rising dramatically above serene rivers and lush green forests, creating a breathtaking and iconic natural beauty that attracts millions of visitors each year for its picturesque views.

# round2
When traveling to a karst landscape like this, here are some important tips:

1. Wear comfortable shoes: The terrain can be uneven and hilly.
2. Bring water and snacks for energy during hikes or boat rides.
3. Protect yourself from the sun with sunscreen, hats, and sunglasses‚Äîespecially since you‚Äôll likely spend time outdoors exploring scenic spots.
4. Respect local customs and nature regulations by not littering or disturbing wildlife.

By following these guidelines, you'll have a safe and enjoyable trip while appreciating the stunning natural beauty of places such as Guilin‚Äôs karst mountains.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Chat with Multiple Images&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt; Click to view Python code running MiniCPM-V-4_5 with multiple images input. &lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import torch
from PIL import Image
from transformers import AutoModel, AutoTokenizer

model = AutoModel.from_pretrained('openbmb/MiniCPM-V-4_5', trust_remote_code=True,  # or openbmb/MiniCPM-o-2_6
    attn_implementation='sdpa', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager
model = model.eval().cuda()
tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-4_5', trust_remote_code=True)  # or openbmb/MiniCPM-o-2_6

image1 = Image.open('image1.jpg').convert('RGB')
image2 = Image.open('image2.jpg').convert('RGB')
question = 'Compare image 1 and image 2, tell me about the differences between image 1 and image 2.'

msgs = [{'role': 'user', 'content': [image1, image2, question]}]

answer = model.chat(
    msgs=msgs,
    tokenizer=tokenizer
)
print(answer)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h4&gt;In-context Few-shot Learning&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt; Click to view Python code running MiniCPM-V-4_5 with few-shot input. &lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import torch
from PIL import Image
from transformers import AutoModel, AutoTokenizer

model = AutoModel.from_pretrained('openbmb/MiniCPM-V-4_5', trust_remote_code=True,  # or openbmb/MiniCPM-o-2_6
    attn_implementation='sdpa', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager
model = model.eval().cuda()
tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-4_5', trust_remote_code=True)  # or openbmb/MiniCPM-o-2_6

question = "production date" 
image1 = Image.open('example1.jpg').convert('RGB')
answer1 = "2023.08.04"
image2 = Image.open('example2.jpg').convert('RGB')
answer2 = "2007.04.24"
image_test = Image.open('test.jpg').convert('RGB')

msgs = [
    {'role': 'user', 'content': [image1, question]}, {'role': 'assistant', 'content': [answer1]},
    {'role': 'user', 'content': [image2, question]}, {'role': 'assistant', 'content': [answer2]},
    {'role': 'user', 'content': [image_test, question]}
]

answer = model.chat(
    msgs=msgs,
    tokenizer=tokenizer
)
print(answer)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h4&gt;Chat with Video&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt; Click to view Python code running MiniCPM-V-4_5 by with video input and 3D-Resampler. &lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;## The 3d-resampler compresses multiple frames into 64 tokens by introducing temporal_ids. 
# To achieve this, you need to organize your video data into two corresponding sequences: 
#   frames: List[Image]
#   temporal_ids: List[List[Int]].

import torch
from PIL import Image
from transformers import AutoModel, AutoTokenizer
from decord import VideoReader, cpu    # pip install decord
from scipy.spatial import cKDTree
import numpy as np
import math

model = AutoModel.from_pretrained('openbmb/MiniCPM-V-4_5', trust_remote_code=True,  # or openbmb/MiniCPM-o-2_6
    attn_implementation='sdpa', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager
model = model.eval().cuda()
tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-4_5', trust_remote_code=True)  # or openbmb/MiniCPM-o-2_6

MAX_NUM_FRAMES=180 # Indicates the maximum number of frames received after the videos are packed. The actual maximum number of valid frames is MAX_NUM_FRAMES * MAX_NUM_PACKING.
MAX_NUM_PACKING=3  # indicates the maximum packing number of video frames. valid range: 1-6
TIME_SCALE = 0.1 

def map_to_nearest_scale(values, scale):
    tree = cKDTree(np.asarray(scale)[:, None])
    _, indices = tree.query(np.asarray(values)[:, None])
    return np.asarray(scale)[indices]


def group_array(arr, size):
    return [arr[i:i+size] for i in range(0, len(arr), size)]

def encode_video(video_path, choose_fps=3, force_packing=None):
    def uniform_sample(l, n):
        gap = len(l) / n
        idxs = [int(i * gap + gap / 2) for i in range(n)]
        return [l[i] for i in idxs]
    vr = VideoReader(video_path, ctx=cpu(0))
    fps = vr.get_avg_fps()
    video_duration = len(vr) / fps
        
    if choose_fps * int(video_duration) &amp;lt;= MAX_NUM_FRAMES:
        packing_nums = 1
        choose_frames = round(min(choose_fps, round(fps)) * min(MAX_NUM_FRAMES, video_duration))
        
    else:
        packing_nums = math.ceil(video_duration * choose_fps / MAX_NUM_FRAMES)
        if packing_nums &amp;lt;= MAX_NUM_PACKING:
            choose_frames = round(video_duration * choose_fps)
        else:
            choose_frames = round(MAX_NUM_FRAMES * MAX_NUM_PACKING)
            packing_nums = MAX_NUM_PACKING

    frame_idx = [i for i in range(0, len(vr))]      
    frame_idx =  np.array(uniform_sample(frame_idx, choose_frames))

    if force_packing:
        packing_nums = min(force_packing, MAX_NUM_PACKING)
    
    print(video_path, ' duration:', video_duration)
    print(f'get video frames={len(frame_idx)}, packing_nums={packing_nums}')
    
    frames = vr.get_batch(frame_idx).asnumpy()

    frame_idx_ts = frame_idx / fps
    scale = np.arange(0, video_duration, TIME_SCALE)

    frame_ts_id = map_to_nearest_scale(frame_idx_ts, scale) / TIME_SCALE
    frame_ts_id = frame_ts_id.astype(np.int32)

    assert len(frames) == len(frame_ts_id)

    frames = [Image.fromarray(v.astype('uint8')).convert('RGB') for v in frames]
    frame_ts_id_group = group_array(frame_ts_id, packing_nums)
    
    return frames, frame_ts_id_group


video_path="video_test.mp4"
fps = 5 # fps for video
force_packing = None # You can set force_packing to ensure that 3D packing is forcibly enabled; otherwise, encode_video will dynamically set the packing quantity based on the duration.
frames, frame_ts_id_group = encode_video(video_path, fps, force_packing=force_packing)

question = "Describe the video"
msgs = [
    {'role': 'user', 'content': frames + [question]}, 
]


answer = model.chat(
    msgs=msgs,
    tokenizer=tokenizer,
    use_image_id=False,
    max_slice_nums=1,
    temporal_ids=frame_ts_id_group
)
print(answer)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h4&gt;Speech and Audio Mode&lt;/h4&gt; 
&lt;p&gt;Model initialization&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import torch
import librosa
from transformers import AutoModel, AutoTokenizer

model = AutoModel.from_pretrained('openbmb/MiniCPM-o-2_6', trust_remote_code=True,
    attn_implementation='sdpa', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager
model = model.eval().cuda()
tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-o-2_6', trust_remote_code=True)

model.init_tts()
model.tts.float()
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h5&gt;Mimick 
 &lt;!-- omit in toc --&gt;&lt;/h5&gt; 
&lt;p&gt;&lt;code&gt;Mimick&lt;/code&gt; task reflects a model's end-to-end speech modeling capability. The model takes audio input, and outputs an ASR transcription and subsequently reconstructs the original audio with high similarity. The higher the similarity between the reconstructed audio and the original audio, the stronger the model's foundational capability in end-to-end speech modeling.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;mimick_prompt = "Please repeat each user's speech, including voice style and speech content."
audio_input, _ = librosa.load('./assets/input_examples/Trump_WEF_2018_10s.mp3', sr=16000, mono=True) # load the audio to be mimicked

# `./assets/input_examples/fast-pace.wav`, 
# `./assets/input_examples/chi-english-1.wav` 
# `./assets/input_examples/exciting-emotion.wav` 
# for different aspects of speech-centric features.

msgs = [{'role': 'user', 'content': [mimick_prompt, audio_input]}]
res = model.chat(
    msgs=msgs,
    tokenizer=tokenizer,
    sampling=True,
    max_new_tokens=128,
    use_tts_template=True,
    temperature=0.3,
    generate_audio=True,
    output_audio_path='output_mimick.wav', # save the tts result to output_audio_path
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h5&gt;General Speech Conversation with Configurable Voices 
 &lt;!-- omit in toc --&gt;&lt;/h5&gt; 
&lt;p&gt;A general usage scenario of &lt;code&gt;MiniCPM-o-2.6&lt;/code&gt; is role-playing a specific character based on the audio prompt. It will mimic the voice of the character to some extent and act like the character in text, including language style. In this mode, &lt;code&gt;MiniCPM-o-2.6&lt;/code&gt; sounds &lt;strong&gt;more natural and human-like&lt;/strong&gt;. Self-defined audio prompts can be used to customize the voice of the character in an end-to-end manner.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;ref_audio, _ = librosa.load('./assets/input_examples/icl_20.wav', sr=16000, mono=True) # load the reference audio
sys_prompt = model.get_sys_prompt(ref_audio=ref_audio, mode='audio_roleplay', language='en')

# round one
user_question = {'role': 'user', 'content': [librosa.load('xxx.wav', sr=16000, mono=True)[0]]}
msgs = [sys_prompt, user_question]
res = model.chat(
    msgs=msgs,
    tokenizer=tokenizer,
    sampling=True,
    max_new_tokens=128,
    use_tts_template=True,
    generate_audio=True,
    temperature=0.3,
    output_audio_path='result_roleplay_round_1.wav',
)

# round two
history = msgs.append({'role': 'assistant', 'content': res})
user_question = {'role': 'user', 'content': [librosa.load('xxx.wav', sr=16000, mono=True)[0]]}
msgs = history.append(user_question)
res = model.chat(
    msgs=msgs,
    tokenizer=tokenizer,
    sampling=True,
    max_new_tokens=128,
    use_tts_template=True,
    generate_audio=True,
    temperature=0.3,
    output_audio_path='result_roleplay_round_2.wav',
)
print(res)
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h5&gt;Speech Conversation as an AI Assistant 
 &lt;!-- omit in toc --&gt;&lt;/h5&gt; 
&lt;p&gt;An enhanced feature of &lt;code&gt;MiniCPM-o-2.6&lt;/code&gt; is to act as an AI assistant, but only with limited choice of voices. In this mode, &lt;code&gt;MiniCPM-o-2.6&lt;/code&gt; is &lt;strong&gt;less human-like and more like a voice assistant&lt;/strong&gt;. In this mode, the model is more instruction-following. For demo, you are suggested to use &lt;code&gt;assistant_female_voice&lt;/code&gt;, &lt;code&gt;assistant_male_voice&lt;/code&gt;, and &lt;code&gt;assistant_default_female_voice&lt;/code&gt;. Other voices may work but not as stable as the default voices.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Please note that, &lt;code&gt;assistant_female_voice&lt;/code&gt; and &lt;code&gt;assistant_male_voice&lt;/code&gt; are more stable but sounds like robots, while &lt;code&gt;assistant_default_female_voice&lt;/code&gt; is more human-alike but not stable, its voice often changes in multiple turns. We suggest you to try stable voices &lt;code&gt;assistant_female_voice&lt;/code&gt; and &lt;code&gt;assistant_male_voice&lt;/code&gt;.&lt;/em&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;ref_audio, _ = librosa.load('./assets/input_examples/assistant_female_voice.wav', sr=16000, mono=True) # or use `./assets/input_examples/assistant_male_voice.wav`
sys_prompt = model.get_sys_prompt(ref_audio=ref_audio, mode='audio_assistant', language='en') 
user_question = {'role': 'user', 'content': [librosa.load('xxx.wav', sr=16000, mono=True)[0]]} # load the user's audio question

# round one
msgs = [sys_prompt, user_question]
res = model.chat(
    msgs=msgs,
    tokenizer=tokenizer,
    sampling=True,
    max_new_tokens=128,
    use_tts_template=True,
    generate_audio=True,
    temperature=0.3,
    output_audio_path='result_assistant_round_1.wav',
)

# round two
history = msgs.append({'role': 'assistant', 'content': res})
user_question = {'role': 'user', 'content': [librosa.load('xxx.wav', sr=16000, mono=True)[0]]}
msgs = history.append(user_question)
res = model.chat(
    msgs=msgs,
    tokenizer=tokenizer,
    sampling=True,
    max_new_tokens=128,
    use_tts_template=True,
    generate_audio=True,
    temperature=0.3,
    output_audio_path='result_assistant_round_2.wav',
)
print(res)
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h5&gt;Instruction-to-Speech 
 &lt;!-- omit in toc --&gt;&lt;/h5&gt; 
&lt;p&gt;&lt;code&gt;MiniCPM-o-2.6&lt;/code&gt; can also do Instruction-to-Speech, aka &lt;strong&gt;Voice Creation&lt;/strong&gt;. You can describe a voice in detail, and the model will generate a voice that matches the description. For more Instruction-to-Speech sample instructions, you can refer to &lt;a href="https://voxinstruct.github.io/VoxInstruct/"&gt;https://voxinstruct.github.io/VoxInstruct/&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;instruction = 'Speak like a male charming superstar, radiating confidence and style in every word.'

msgs = [{'role': 'user', 'content': [instruction]}]

res = model.chat(
    msgs=msgs,
    tokenizer=tokenizer,
    sampling=True,
    max_new_tokens=128,
    use_tts_template=True,
    generate_audio=True,
    temperature=0.3,
    output_audio_path='result_voice_creation.wav',
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h5&gt;Voice Cloning 
 &lt;!-- omit in toc --&gt;&lt;/h5&gt; 
&lt;p&gt;&lt;code&gt;MiniCPM-o-2.6&lt;/code&gt; can also do zero-shot text-to-speech, aka &lt;strong&gt;Voice Cloning&lt;/strong&gt;. With this mode, model will act like a TTS model.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;ref_audio, _ = librosa.load('./assets/input_examples/icl_20.wav', sr=16000, mono=True) # load the reference audio
sys_prompt = model.get_sys_prompt(ref_audio=ref_audio, mode='voice_cloning', language='en')
text_prompt = f"Please read the text below."
user_question = {'role': 'user', 'content': [text_prompt, "content that you want to read"]}

msgs = [sys_prompt, user_question]
res = model.chat(
    msgs=msgs,
    tokenizer=tokenizer,
    sampling=True,
    max_new_tokens=128,
    use_tts_template=True,
    generate_audio=True,
    temperature=0.3,
    output_audio_path='result_voice_cloning.wav',
)

&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h5&gt;Addressing Various Audio Understanding Tasks 
 &lt;!-- omit in toc --&gt;&lt;/h5&gt; 
&lt;p&gt;&lt;code&gt;MiniCPM-o-2.6&lt;/code&gt; can also be used to address various audio understanding tasks, such as ASR, speaker analysis, general audio captioning, and sound scene tagging.&lt;/p&gt; 
&lt;p&gt;For audio-to-text tasks, you can use the following prompts:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ASR with ZH(same as AST en2zh): &lt;code&gt;ËØ∑‰ªîÁªÜÂê¨ËøôÊÆµÈü≥È¢ëÁâáÊÆµÔºåÂπ∂Â∞ÜÂÖ∂ÂÜÖÂÆπÈÄêÂ≠óËÆ∞ÂΩï„ÄÇ&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;ASR with EN(same as AST zh2en): &lt;code&gt;Please listen to the audio snippet carefully and transcribe the content.&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Speaker Analysis: &lt;code&gt;Based on the speaker's content, speculate on their gender, condition, age range, and health status.&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;General Audio Caption: &lt;code&gt;Summarize the main content of the audio.&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;General Sound Scene Tagging: &lt;code&gt;Utilize one keyword to convey the audio's content or the associated scene.&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;task_prompt = "Please listen to the audio snippet carefully and transcribe the content." + "\n" # can change to other prompts.
audio_input, _ = librosa.load('./assets/input_examples/audio_understanding.mp3', sr=16000, mono=True) # load the audio to be captioned

msgs = [{'role': 'user', 'content': [task_prompt, audio_input]}]

res = model.chat(
    msgs=msgs,
    tokenizer=tokenizer,
    sampling=True,
    max_new_tokens=128,
    use_tts_template=True,
    generate_audio=True,
    temperature=0.3,
    output_audio_path='result_audio_understanding.wav',
)
print(res)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Multimodal Live Streaming&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt; Click to view Python code running MiniCPM-o 2.6 with chat inference. &lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import math
import numpy as np
from PIL import Image
from moviepy.editor import VideoFileClip
import tempfile
import librosa
import soundfile as sf
import torch
from transformers import AutoModel, AutoTokenizer

def get_video_chunk_content(video_path, flatten=True):
    video = VideoFileClip(video_path)
    print('video_duration:', video.duration)
    
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=True) as temp_audio_file:
        temp_audio_file_path = temp_audio_file.name
        video.audio.write_audiofile(temp_audio_file_path, codec="pcm_s16le", fps=16000)
        audio_np, sr = librosa.load(temp_audio_file_path, sr=16000, mono=True)
    num_units = math.ceil(video.duration)
    
    # 1 frame + 1s audio chunk
    contents= []
    for i in range(num_units):
        frame = video.get_frame(i+1)
        image = Image.fromarray((frame).astype(np.uint8))
        audio = audio_np[sr*i:sr*(i+1)]
        if flatten:
            contents.extend(["&amp;lt;unit&amp;gt;", image, audio])
        else:
            contents.append(["&amp;lt;unit&amp;gt;", image, audio])
    
    return contents


model = AutoModel.from_pretrained('openbmb/MiniCPM-o-2_6', trust_remote_code=True,
    attn_implementation='sdpa', torch_dtype=torch.bfloat16)
model = model.eval().cuda()
tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-o-2_6', trust_remote_code=True)

model.init_tts()

# If you are using an older version of PyTorch, you might encounter this issue "weight_norm_fwd_first_dim_kernel" not implemented for 'BFloat16', Please convert the TTS to float32 type.
# model.tts.float()

# https://huggingface.co/openbmb/MiniCPM-o-2_6/blob/main/assets/Skiing.mp4
video_path="assets/Skiing.mp4"
sys_msg = model.get_sys_prompt(mode='omni', language='en')
# if use voice clone prompt, please set ref_audio
# ref_audio_path = '/path/to/ref_audio'
# ref_audio, _ = librosa.load(ref_audio_path, sr=16000, mono=True)
# sys_msg = model.get_sys_prompt(ref_audio=ref_audio, mode='omni', language='en')

contents = get_video_chunk_content(video_path)
msg = {"role":"user", "content": contents}
msgs = [sys_msg, msg]

# please set generate_audio=True and output_audio_path to save the tts result
generate_audio = True
output_audio_path = 'output.wav'

res = model.chat(
    msgs=msgs,
    tokenizer=tokenizer,
    sampling=True,
    temperature=0.5,
    max_new_tokens=4096,
    omni_input=True, # please set omni_input=True when omni inference
    use_tts_template=True,
    generate_audio=generate_audio,
    output_audio_path=output_audio_path,
    max_slice_nums=1,
    use_image_id=False,
    return_dict=True
)
print(res)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; Click to view Python code running MiniCPM-o 2.6 with streaming inference. &lt;/summary&gt; 
 &lt;p&gt;Note: The streaming inference has a slight performance degradation because the audio encoding is not global.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# a new conversation need reset session first, it will reset the kv-cache
model.reset_session()

contents = get_video_chunk_content(video_path, flatten=False)
session_id = '123'
generate_audio = True

# 1. prefill system prompt
res = model.streaming_prefill(
    session_id=session_id,
    msgs=[sys_msg], 
    tokenizer=tokenizer
)

# 2. prefill video/audio chunks
for content in contents:
    msgs = [{"role":"user", "content": content}]
    res = model.streaming_prefill(
        session_id=session_id,
        msgs=msgs, 
        tokenizer=tokenizer
    )

# 3. generate
res = model.streaming_generate(
    session_id=session_id,
    tokenizer=tokenizer,
    temperature=0.5,
    generate_audio=generate_audio
)

audios = []
text = ""

if generate_audio:
    for r in res:
        audio_wav = r.audio_wav
        sampling_rate = r.sampling_rate
        txt = r.text

        audios.append(audio_wav)
        text += txt
        
    res = np.concatenate(audios)
    sf.write("output.wav", res, samplerate=sampling_rate)
    print("text:", text)
    print("audio saved to output.wav")
else:
    for r in res:
        text += r['text']
    print("text:", text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Inference on Multiple GPUs&lt;/h3&gt; 
&lt;p&gt;You can run MiniCPM-Llama3-V 2.5 on multiple low VRAM GPUs (12 GB or 16 GB) by distributing the model's layers across multiple GPUs. Please refer to this &lt;a href="https://github.com/OpenBMB/MiniCPM-V/raw/main/docs/inference_on_multiple_gpus.md"&gt;tutorial&lt;/a&gt; for detailed instructions on how to load the model and inference using multiple low VRAM GPUs.&lt;/p&gt; 
&lt;h3&gt;Inference on Mac&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click to view an example, to run MiniCPM-Llama3-V 2.5 on üíª Mac with MPS (Apple silicon or AMD GPUs). &lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# test.py  Need more than 16GB memory.
import torch
from PIL import Image
from transformers import AutoModel, AutoTokenizer

model = AutoModel.from_pretrained('openbmb/MiniCPM-Llama3-V-2_5', trust_remote_code=True, low_cpu_mem_usage=True)
model = model.to(device='mps')

tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-Llama3-V-2_5', trust_remote_code=True)
model.eval()

image = Image.open('./assets/hk_OCR.jpg').convert('RGB')
question = 'Where is this photo taken?'
msgs = [{'role': 'user', 'content': question}]

answer, context, _ = model.chat(
    image=image,
    msgs=msgs,
    context=None,
    tokenizer=tokenizer,
    sampling=True
)
print(answer)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Run with command:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;PYTORCH_ENABLE_MPS_FALLBACK=1 python test.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Efficient Inference with llama.cpp, Ollama, vLLM&lt;/h3&gt; 
&lt;p&gt;See &lt;a href="https://github.com/OpenBMB/llama.cpp/tree/minicpmv-main/examples/llava/README-minicpmv2.6.md"&gt;our fork of llama.cpp&lt;/a&gt; for more detail. This implementation supports smooth inference of 16~18 token/s on iPad (test environmentÔºöiPad Pro + M4).&lt;/p&gt; 
&lt;p&gt;See &lt;a href="https://github.com/OpenBMB/ollama/raw/minicpm-v2.6/examples/minicpm-v2.6/README.md"&gt;our fork of Ollama&lt;/a&gt; for more detail. This implementation supports smooth inference of 16~18 token/s on iPad (test environmentÔºöiPad Pro + M4).&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt; vLLM now officially supports MiniCPM-V 2.6, MiniCPM-Llama3-V 2.5 and MiniCPM-V 2.0. And you can use our fork to run MiniCPM-o 2.6 for now. Click to see. &lt;/summary&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Install vLLM(&amp;gt;=0.7.1):&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;pip install vllm
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="2"&gt; 
  &lt;li&gt;Run Example:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://docs.vllm.ai/en/latest/getting_started/examples/vision_language.html"&gt;Vision Language&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://docs.vllm.ai/en/latest/getting_started/examples/audio_language.html"&gt;Audio Language&lt;/a&gt; &lt;/li&gt;
 &lt;/ul&gt;
&lt;/details&gt;   
&lt;h2&gt;Fine-tuning&lt;/h2&gt; 
&lt;h3&gt;Simple Fine-tuning 
 &lt;!-- omit in toc --&gt;&lt;/h3&gt; 
&lt;p&gt;We support simple fine-tuning with Hugging Face for MiniCPM-o 2.6, MiniCPM-V 2.6, MiniCPM-Llama3-V 2.5 and MiniCPM-V 2.0.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/finetune/readme.md"&gt;Reference Document&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;With Align-Anything 
 &lt;!-- omit in toc --&gt;&lt;/h3&gt; 
&lt;p&gt;We support fine-tuning MiniCPM-o 2.6 by PKU-Alignment Team (both vision and audio, SFT and DPO) with the &lt;a href="https://github.com/PKU-Alignment/align-anything"&gt;Align-Anything framework&lt;/a&gt;. Align-Anything is a scalable framework that aims to align any-modality large models with human intentions, open-sourcing the &lt;a href="https://huggingface.co/datasets/PKU-Alignment/align-anything"&gt;datasets, models and benchmarks&lt;/a&gt;. Benefiting from its concise and modular design, it supports 30+ open-source benchmarks, 40+ models and algorithms including SFT, SimPO, RLHF, &lt;em&gt;etc&lt;/em&gt;. It also provides 30+ directly runnable scripts, making it suitable for beginners to quickly get started.&lt;/p&gt; 
&lt;p&gt;Best Practices: &lt;a href="https://github.com/PKU-Alignment/align-anything/tree/main/scripts"&gt;MiniCPM-o 2.6&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;With LLaMA-Factory 
 &lt;!-- omit in toc --&gt;&lt;/h3&gt; 
&lt;p&gt;We support fine-tuning MiniCPM-o 2.6 and MiniCPM-V 2.6 with the LLaMA-Factory framework. LLaMA-Factory provides a solution for flexibly customizing the fine-tuning (Lora/Full/Qlora) of 200+ LLMs without the need for coding through the built-in web UI LLaMABoard. It supports various training methods like sft/ppo/dpo/kto and advanced algorithms like Galore/BAdam/LLaMA-Pro/Pissa/LongLoRA.&lt;/p&gt; 
&lt;p&gt;Best Practices: &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/llamafactory_train_and_infer.md"&gt;MiniCPM-o 2.6 | MiniCPM-V 2.6&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;With the SWIFT Framework 
 &lt;!-- omit in toc --&gt;&lt;/h3&gt; 
&lt;p&gt;We now support MiniCPM-V series fine-tuning with the SWIFT framework. SWIFT supports training, inference, evaluation and deployment of nearly 200 LLMs and MLLMs . It supports the lightweight training solutions provided by PEFT and a complete Adapters Library including techniques such as NEFTune, LoRA+ and LLaMA-PRO.&lt;/p&gt; 
&lt;p&gt;Best PracticesÔºö&lt;a href="https://github.com/modelscope/swift/raw/main/docs/source/Multi-Modal/minicpm-v%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5.md"&gt;MiniCPM-V 1.0&lt;/a&gt;, &lt;a href="https://github.com/modelscope/swift/raw/main/docs/source/Multi-Modal/minicpm-v-2%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5.md"&gt;MiniCPM-V 2.0&lt;/a&gt;, &lt;a href="https://github.com/modelscope/ms-swift/issues/1613"&gt;MiniCPM-V 2.6&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Awesome work using MiniCPM-V &amp;amp; MiniCPM-o&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/CatchTheTornado/text-extract-api"&gt;text-extract-api&lt;/a&gt;: Document extraction API using OCRs and Ollama supported models &lt;img src="https://img.shields.io/github/stars/CatchTheTornado/text-extract-api" alt="GitHub Repo stars" /&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/heshengtao/comfyui_LLM_party"&gt;comfyui_LLM_party&lt;/a&gt;: Build LLM workflows and integrate into existing image workflows &lt;img src="https://img.shields.io/github/stars/heshengtao/comfyui_LLM_party" alt="GitHub Repo stars" /&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/imanoop7/Ollama-OCR"&gt;Ollama-OCR&lt;/a&gt;: OCR package uses vlms through Ollama to extract text from images and PDF &lt;img src="https://img.shields.io/github/stars/imanoop7/Ollama-OCR" alt="GitHub Repo stars" /&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/MixLabPro/comfyui-mixlab-nodes"&gt;comfyui-mixlab-nodes&lt;/a&gt;: ComfyUI node suite supports Workflow-to-APP„ÄÅGPT&amp;amp;3D and more &lt;img src="https://img.shields.io/github/stars/MixLabPro/comfyui-mixlab-nodes" alt="GitHub Repo stars" /&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/HumanAIGC-Engineering/OpenAvatarChat"&gt;OpenAvatarChat&lt;/a&gt;: Interactive digital human conversation implementation on single PC &lt;img src="https://img.shields.io/github/stars/HumanAIGC-Engineering/OpenAvatarChat" alt="GitHub Repo stars" /&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/arkohut/pensieve"&gt;pensieve&lt;/a&gt;: A privacy-focused passive recording project by recording screen content &lt;img src="https://img.shields.io/github/stars/arkohut/pensieve" alt="GitHub Repo stars" /&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/icereed/paperless-gpt"&gt;paperless-gpt&lt;/a&gt;: Use LLMs to handle paperless-ngx, AI-powered titles, tags and OCR &lt;img src="https://img.shields.io/github/stars/icereed/paperless-gpt" alt="GitHub Repo stars" /&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/kimjammer/Neuro"&gt;Neuro&lt;/a&gt;: A recreation of Neuro-Sama, but running on local models on consumer hardware &lt;img src="https://img.shields.io/github/stars/kimjammer/Neuro" alt="GitHub Repo stars" /&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;FAQs&lt;/h2&gt; 
&lt;p&gt;Click here to view the &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/faqs.md"&gt;FAQs&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Limitations&lt;/h2&gt; 
&lt;p&gt;As an experimental trial, we find MiniCPM-o 2.6 has notable limitations worth further investigation and improvement.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Unstable speech output.&lt;/strong&gt; The speech generation can be flawed with noisy backgrounds and unmeaningful sounds.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Repeated response.&lt;/strong&gt; The model tends to repeat its response when encountering similar consecutive user queries.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;High-latency on Web Demo.&lt;/strong&gt; Users may experience unusual high-latency when using web demo hosted on overseas servers. We recommend deploying the demo locally or with good network connections.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Model License 
 &lt;!-- omit in toc --&gt;&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;This repository is released under the &lt;a href="https://github.com/OpenBMB/MiniCPM/raw/main/LICENSE"&gt;Apache-2.0&lt;/a&gt; License.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;The usage of MiniCPM-o/V model weights must strictly follow &lt;a href="https://github.com/OpenBMB/MiniCPM/raw/main/MiniCPM%20Model%20License.md"&gt;MiniCPM Model License.md&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;The models and weights of MiniCPM are completely free for academic research. after filling out a &lt;a href="https://modelbest.feishu.cn/share/base/form/shrcnpV5ZT9EJ6xYjh3Kx0J6v8g"&gt;"questionnaire"&lt;/a&gt; for registration, are also available for free commercial use.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Statement 
 &lt;!-- omit in toc --&gt;&lt;/h2&gt; 
&lt;p&gt;As MLLMs, MiniCPM-o/V models generate content by learning a large number of multimodal corpora, but they cannot comprehend, express personal opinions, or make value judgements. Anything generated by MiniCPM-o/V models does not represent the views and positions of the model developers&lt;/p&gt; 
&lt;p&gt;We will not be liable for any problems arising from the use of MiniCPM-o/V models, including but not limited to data security issues, risk of public opinion, or any risks and problems arising from the misdirection, misuse, dissemination, or misuse of the model.&lt;/p&gt; 
&lt;h2&gt;Institutions 
 &lt;!-- omit in toc --&gt;&lt;/h2&gt; 
&lt;p&gt;This project is developed by the following institutions:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/thunlp.png" width="28px" /&gt; &lt;a href="https://nlp.csai.tsinghua.edu.cn/"&gt;THUNLP&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelbest.png" width="28px" /&gt; &lt;a href="https://modelbest.cn/"&gt;ModelBest&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üåü Star History 
 &lt;!-- omit in toc --&gt;&lt;/h2&gt; 
&lt;!-- &lt;table align="center"&gt;
    &lt;p align="center"&gt;
      &lt;img src="assets/star_history.svg"/&gt;
    &lt;/p&gt;
&lt;/table&gt; --&gt; 
&lt;picture&gt; 
 &lt;source media="(prefers-color-scheme: dark)" srcset="
      https://api.star-history.com/svg?repos=OpenBMB/MiniCPM-o&amp;amp;type=Date&amp;amp;theme=dark
    " /&gt; 
 &lt;source media="(prefers-color-scheme: light)" srcset="
      https://api.star-history.com/svg?repos=OpenBMB/MiniCPM-o&amp;amp;type=Date
    " /&gt; 
 &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=OpenBMB/MiniCPM-o&amp;amp;type=Date" /&gt; 
&lt;/picture&gt; 
&lt;h2&gt;Key Techniques and Other Multimodal Projects 
 &lt;!-- omit in toc --&gt;&lt;/h2&gt; 
&lt;p&gt;üëè Welcome to explore key techniques of MiniCPM-o/V and other multimodal projects of our team:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/OpenBMB/VisCPM/tree/main"&gt;VisCPM&lt;/a&gt; | &lt;a href="https://github.com/OpenBMB/RLPR"&gt;RLPR&lt;/a&gt; | &lt;a href="https://github.com/RLHF-V/RLHF-V"&gt;RLHF-V&lt;/a&gt; | &lt;a href="https://github.com/thunlp/LLaVA-UHD"&gt;LLaVA-UHD&lt;/a&gt; | &lt;a href="https://github.com/RLHF-V/RLAIF-V"&gt;RLAIF-V&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Citation 
 &lt;!-- omit in toc --&gt;&lt;/h2&gt; 
&lt;p&gt;If you find our model/code/paper helpful, please consider citing our papers üìù and staring us ‚≠êÔ∏èÔºÅ&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bib"&gt;@article{yao2024minicpm,
  title={MiniCPM-V: A GPT-4V Level MLLM on Your Phone},
  author={Yao, Yuan and Yu, Tianyu and Zhang, Ao and Wang, Chongyi and Cui, Junbo and Zhu, Hongji and Cai, Tianchi and Li, Haoyu and Zhao, Weilin and He, Zhihui and others},
  journal={arXiv preprint arXiv:2408.01800},
  year={2024}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>opf/openproject</title>
      <link>https://github.com/opf/openproject</link>
      <description>&lt;p&gt;OpenProject is the leading open source project management software.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OpenProject&lt;/h1&gt; 
&lt;p&gt;&lt;img src="https://img.shields.io/github/v/release/opf/openproject" alt="GitHub release (latest by date)" /&gt; &lt;img src="https://img.shields.io/github/commit-activity/m/opf/openproject" alt="GitHub commit activity" /&gt; &lt;img src="https://img.shields.io/github/checks-status/opf/openproject/dev" alt="GitHub branch checks state" /&gt; &lt;a href="https://github.com/opf/openproject/actions/workflows/test-core.yml"&gt;&lt;img src="https://github.com/opf/openproject/actions/workflows/test-core.yml/badge.svg?branch=dev" alt="Github Tests" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;We empower teams to achieve great things together for the good of society.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;OpenProject is a web-based project management software. Use OpenProject to manage your projects, tasks and goals. Collaborate via work packages and link them to your pull requests on Github. &lt;a href="https://www.openproject.org/docs/system-admin-guide/integrations/github-integration/"&gt;Read more about the OpenProject GitHub integration&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/opf/openproject/dev/GitHub-tab-new.png" alt="Screenshot of OpenProject, showing the GitHub tab on a work package" /&gt;&lt;/p&gt; 
&lt;p&gt;OpenProject's key features are:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.openproject.org/collaboration-software-features/#project-planning"&gt;Project planning and scheduling&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.openproject.org/collaboration-software-features/#product-management"&gt;Product roadmap and release planning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.openproject.org/collaboration-software-features/#task-management"&gt;Task management and team collaboration&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.openproject.org/collaboration-software-features/#agile-scrum"&gt;Agile and Scrum&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.openproject.org/collaboration-software-features/#time-tracking"&gt;Time tracking, cost reporting, and budgeting&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.openproject.org/collaboration-software-features/#bug-tracking"&gt;Bug tracking&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.openproject.org/docs/user-guide/wysiwyg/"&gt;Wikis&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.openproject.org/docs/user-guide/forums/"&gt;Forums&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.openproject.org/docs/user-guide/meetings/"&gt;Meeting agendas and meeting minutes&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;More information and screenshots can be found on our &lt;a href="https://www.openproject.org"&gt;website&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Start now with OpenProject&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Free Trial&lt;/strong&gt;: &lt;a href="https://start.openproject.com/"&gt;Start a 14-days free trial of OpenProject&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Community Edition&lt;/strong&gt;, free of charge: Download OpenProject and get started with the self-hosted Community edition. If you want to run an instance of OpenProject in production (or for evaluation), refer to our in-depth &lt;a href="https://www.openproject.org/download-and-installation/"&gt;installation guides&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Enterprise Edition&lt;/strong&gt;: Sign up for the Enterprise version, choose between cloud or on-premises and benefit from comprehensive support and Enterprise add-ons.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Documentation&lt;/strong&gt;: Explore our &lt;a href="https://www.openproject.org/docs/"&gt;comprehensive documentation&lt;/a&gt; to help you get up and running quickly.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Training&lt;/strong&gt;: &lt;a href="https://www.openproject.org/training-and-consulting/#training-signup"&gt;Book one of our training or consulting offers&lt;/a&gt; to get your team on board in no time.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Report bugs&lt;/h2&gt; 
&lt;p&gt;You found a bug? Please &lt;a href="https://www.openproject.org/docs/development/report-a-bug/"&gt;report it&lt;/a&gt; to our &lt;a href="https://community.openproject.org/projects/openproject"&gt;OpenProject Community&lt;/a&gt;. Thank you!&lt;/p&gt; 
&lt;h2&gt;Contribute&lt;/h2&gt; 
&lt;p&gt;OpenProject is supported by its Community members, both companies and individuals.&lt;/p&gt; 
&lt;p&gt;We are always looking for new members to our Community, so if you are interested in improving OpenProject we would be glad to welcome and support you getting into the code. There are guides as well, e.g. a &lt;a href="https://www.openproject.org/docs/development/development-environment/"&gt;Quick Start for Developers&lt;/a&gt;, but don't hesitate to simply &lt;a href="https://www.openproject.org/contact"&gt;contact us&lt;/a&gt; if you have questions.&lt;/p&gt; 
&lt;p&gt;Working on OpenProject comes with the satisfaction of working on a widely used open source application.&lt;/p&gt; 
&lt;p&gt;Also, if you do not want to be limited to working on open source in your free time, OpenProject GmbH, the company contributing to the OpenProject development, &lt;a href="https://www.openproject.org/career/"&gt;is hiring&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Stay in contact&lt;/h2&gt; 
&lt;p&gt;Here you can find our &lt;a href="https://www.openproject.org/contact/"&gt;contact information&lt;/a&gt;. As we regularly update OpenProject, we recommend staying in touch ‚Äì here is where you can find us:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.openproject.org/blog/community-instance/"&gt;OpenProject Community&lt;/a&gt; with &lt;a href="https://community.openproject.org/projects/openproject/forums"&gt;forum discussions&lt;/a&gt;: The open instance where we develop our features ‚Äì transparent and open for discussions, bug reports or feature requests.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.linkedin.com/company/18706985"&gt;LinkedIn&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.reddit.com/r/openproject/"&gt;Reddit&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://fosstodon.org/@openproject"&gt;Fosstodon&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://twitter.com/openproject"&gt;Twitter/X&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Security / responsible disclosure&lt;/h2&gt; 
&lt;p&gt;We take security very seriously at OpenProject. We value any kind of feedback that will keep our Community secure. If you happen to come across a security issue we urge you to disclose it to us privately to allow our users and Community enough time to upgrade. Security issues will always take precedence over anything else in the pipeline.&lt;/p&gt; 
&lt;p&gt;For more information on how to disclose a security vulnerability, &lt;a href="https://raw.githubusercontent.com/opf/openproject/dev/docs/security-and-privacy/statement-on-security/README.md"&gt;please see this page&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;OpenProject is licensed under the terms of the GNU General Public License version 3. See &lt;a href="https://raw.githubusercontent.com/opf/openproject/dev/COPYRIGHT"&gt;COPYRIGHT&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/opf/openproject/dev/LICENSE"&gt;LICENSE&lt;/a&gt; files for details.&lt;/p&gt; 
&lt;h2&gt;Credits&lt;/h2&gt; 
&lt;h3&gt;Icons&lt;/h3&gt; 
&lt;p&gt;Thanks to Vincent Le Moign and his fabulous Minicons icons on &lt;a href="http://www.webalys.com/minicons/icons-free-pack.php"&gt;webalys.com&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;OpenProject icon font&lt;/h3&gt; 
&lt;p&gt;Published and created by the OpenProject Foundation (OPF) under &lt;a href="http://creativecommons.org/licenses/by/3.0/"&gt;Creative Commons Attribution 3.0 Unported License&lt;/a&gt; with icons from the following sources &lt;a href="http://www.webalys.com/minicons"&gt;Minicons Free Vector Icons Pack&lt;/a&gt; and &lt;a href="http://www.webalys.com/design-interface-application-framework.php"&gt;User Interface Design framework&lt;/a&gt; both by webalys&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Creative Commons License&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;OpenProject Icon Font by the OpenProject Foundation (OPF) is licensed under Creative Commons Attribution 3.0 Unported License and Free for both personal and commercial use. You can copy, adapt, remix, distribute or transmit it.&lt;/p&gt; 
&lt;p&gt;Under this condition: provide a mention of the "OpenProject Foundation" and a link back to OpenProject &lt;a href="http://www.openproject.org"&gt;www.openproject.org&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>microsoft/mcp</title>
      <link>https://github.com/microsoft/mcp</link>
      <description>&lt;p&gt;Catalog of official Microsoft MCP (Model Context Protocol) server implementations for AI-powered data access and tool integration&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Microsoft Model Context Protocol (MCP) Servers&lt;/h1&gt; 
&lt;p&gt;This repository catalogs various Microsoft implementations of the Model Context Protocol (MCP), an open standard that facilitates seamless integration between AI applications and external data sources and tools. MCP enables AI models to access the context they need to perform tasks effectively.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìò What is MCP?&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Model Context Protocol (MCP)&lt;/strong&gt; is an open protocol that standardizes how applications provide context to large language models (LLMs). It allows AI applications to connect with various data sources and tools in a consistent manner, enhancing their capabilities and flexibility. MCP follows a client-server architecture where:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;MCP Hosts&lt;/strong&gt;: Applications like AI assistants or integrated development environments (IDEs) that initiate connections.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MCP Clients&lt;/strong&gt;: Connectors within the host application that maintain 1:1 connections with servers.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MCP Servers&lt;/strong&gt;: Services that provide context and capabilities through the standardized MCP.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For more details, visit the &lt;a href="https://modelcontextprotocol.io"&gt;official MCP website&lt;/a&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìÇ Microsoft MCP Servers&lt;/h2&gt; 
&lt;p&gt;Below are Microsoft's official MCP server implementations:&lt;/p&gt; 
&lt;h3&gt;üìÖ Azure DevOps MCP Server&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Repository&lt;/strong&gt;: &lt;a href="https://github.com/microsoft/azure-devops-mcp"&gt;Azure DevOps MCP Server - Public Preview&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Description&lt;/strong&gt;: The MCP Server for Azure DevOps enables you to bring context into AI workflows and interact with Azure DevOps artifacts such as work items, test plans, builds, releases, and pull requests.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;üî∑ Azure MCP Server&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Repository&lt;/strong&gt;: &lt;a href="https://github.com/microsoft/mcp/tree/main/servers/Azure.Mcp.Server#readme"&gt;microsoft/mcp&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Description&lt;/strong&gt;: Implements the MCP standard to manage Azure resources, enabling declarative provisioning and integration with AI workflows.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;‚ú® Azure AI Foundry MCP Server&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Repository&lt;/strong&gt;: &lt;a href="https://github.com/azure-ai-foundry/mcp-foundry"&gt;azure-ai-foundry/mcp-foundry&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Description&lt;/strong&gt;: An experimental MCP server implementation for Azure AI Foundry that exposes unified tools for models, knowledge, evaluation and deployment.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;üìä Clarity MCP Server&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Repository&lt;/strong&gt;: &lt;a href="https://www.npmjs.com/package/@microsoft/clarity-mcp-server"&gt;@microsoft/clarity-mcp-server&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Description&lt;/strong&gt;: An MCP server for Microsoft Clarity analytics integration. Enables AI models to access web analytics data, heatmaps, and session recordings to understand user behavior and site performance.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;üóÉÔ∏è Dataverse MCP Server&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Documentation&lt;/strong&gt;: &lt;a href="https://go.microsoft.com/fwlink/?linkid=2320176"&gt;Microsoft Dataverse&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Description&lt;/strong&gt;: Chat over your business data using NL - Discover tables, run queries, retrieve data, insert or update records, and execute custom prompts grounded in business knowledge and context.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;üìÅ Files MCP Server&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Repository&lt;/strong&gt;: &lt;a href="https://github.com/microsoft/files-mcp-server"&gt;microsoft/files-mcp-server&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Description&lt;/strong&gt;: Provides a declarative control plane for managing file-based resources, supporting AI workflows that involve static files and documentation synchronization.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;üìù Markitdown MCP Server&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Repository&lt;/strong&gt;: &lt;a href="https://github.com/microsoft/markitdown"&gt;microsoft/markitdown&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Description&lt;/strong&gt;: A specialized MCP server for Markdown processing and manipulation. Enables AI models to read, write, and transform Markdown content with robust parsing and formatting capabilities.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;üíª Microsoft Dev Box MCP&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Package&lt;/strong&gt;: &lt;a href="https://www.npmjs.com/package/@microsoft/devbox-mcp"&gt;@microsoft/devbox-mcp&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Description&lt;/strong&gt;: An MCP server for &lt;a href="https://azure.microsoft.com/products/dev-box"&gt;Microsoft Dev Box&lt;/a&gt;. Enables natural language interactions for developer-focused operations like managing Dev Boxes, configuring environments, and handling pools. Check out the &lt;a href="https://www.npmjs.com/package/@microsoft/devbox-mcp?activeTab=readme"&gt;README&lt;/a&gt; for more details. Use this &lt;a href="https://insiders.vscode.dev/redirect/mcp/install?name=Dev%20Box&amp;amp;config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22%40microsoft%2Fdevbox-mcp%40latest%22%5D%7D"&gt;one-click link&lt;/a&gt; to install Dev Box MCP in your VS Code.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;üõ¢Ô∏èMicrosoft Fabric Real-Time Intelligence MCP Server&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Repository&lt;/strong&gt;: &lt;a href="https://aka.ms/rti.mcp.repo"&gt;RTI MCP Server&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Description&lt;/strong&gt;: Chat with your real-time data the new agentic way using natural language and AI. MCP server for &lt;a href="https://aka.ms/fabricrti"&gt;Fabric Real-Time Intelligence&lt;/a&gt; supporting tools for &lt;a href="https://aka.ms/eventhouse"&gt;Eventhouse&lt;/a&gt;, &lt;a href="https://aka.ms/adx"&gt;Azure Data Explorer&lt;/a&gt;, and other RTI services (coming soon)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;üìö Microsoft Learn Docs MCP Server&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Repository&lt;/strong&gt;: &lt;a href="https://github.com/microsoftdocs/mcp"&gt;microsoftdocs/mcp&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Description&lt;/strong&gt;: A remote MCP server that provides structured access to official Microsoft Learn documentation. Enables AI models to retrieve accurate, authoritative, and context-aware technical content for code generation, question answering, and workflow grounding.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;üõ¢Ô∏èMicrosoft SQL MCP Server&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Repository&lt;/strong&gt;: &lt;a href="https://aka.ms/MssqlMcp"&gt;MSSQL MCP Server&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Description&lt;/strong&gt;: Chat with your business data the new agentic way using natural language and AI. Connect to any SQL database‚Äîfrom ground (on-premises) to Azure cloud to Microsoft Fabric via a simple connection string. Discover and define table schemas, manage tables, and perform CRUD operations through conversational prompts.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;üé≠ Playwright MCP&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Repository&lt;/strong&gt;: &lt;a href="https://github.com/microsoft/playwright-mcp"&gt;microsoft/playwright-mcp&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Description&lt;/strong&gt;: An MCP server for browsing the internet. Enables LLMs to interact with web pages through structured accessibility snapshots. Useful for web navigation and form-filling, data extraction from structured content, automated testing driven by LLMs, and general-purpose browser interaction for agents.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;‚ò∏Ô∏è Azure Kubernetes Service (AKS) MCP Server&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Repository&lt;/strong&gt;: &lt;a href="https://github.com/Azure/aks-mcp"&gt;Azure/aks-mcp&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Description&lt;/strong&gt;: An MCP server that enables AI assistants to interact with Azure Kubernetes Service (AKS) clusters. It serves as a bridge between AI tools and AKS, translating natural language requests into AKS operations and returning the results in a format the AI tools can understand.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;üìö Microsoft 365 Agents Toolkit MCP&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Repository&lt;/strong&gt;: &lt;a href="https://aka.ms/m365agentstoolkit-mcp"&gt;M365 Agents Toolkit MCP&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Description&lt;/strong&gt;: An MCP server that provides a seamless connection between AI agents and developers for building apps and agents for Microsoft 365 and Microsoft 365 Copilot.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìé Related Resources&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/microsoft/mcp/tree/main/Resources"&gt;Microsoft MCP Resources&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://modelcontextprotocol.io/introduction"&gt;MCP Pattern Overview&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://modelcontextprotocol.io/sdk"&gt;MCP SDKs and Building Blocks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://spec.modelcontextprotocol.io/specification/2025-03-26/"&gt;MCP Specification&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üèóÔ∏è Templates&lt;/h2&gt; 
&lt;p&gt;Looking for starter templates that use MCP? Check out the &lt;a href="https://azure.github.io/awesome-azd/?tags=mcp"&gt;Azure Developer CLI (azd) templates&lt;/a&gt; tagged with MCP.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href="https://cla.opensource.microsoft.com"&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; 
&lt;p&gt;This project has adopted the &lt;a href="https://opensource.microsoft.com/codeofconduct/"&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href="https://opensource.microsoft.com/codeofconduct/faq/"&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href="mailto:opencode@microsoft.com"&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; 
&lt;h2&gt;Trademarks&lt;/h2&gt; 
&lt;p&gt;This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow &lt;a href="https://www.microsoft.com/legal/intellectualproperty/trademarks/usage/general"&gt;Microsoft's Trademark &amp;amp; Brand Guidelines&lt;/a&gt;. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>microsoft/terminal</title>
      <link>https://github.com/microsoft/terminal</link>
      <description>&lt;p&gt;The new Windows Terminal and the original Windows console host, all in the same place!&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://github.com/microsoft/terminal/assets/91625426/333ddc76-8ab2-4eb4-a8c0-4d7b953b1179" alt="terminal-logos" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://dev.azure.com/shine-oss/terminal/_build/latest?definitionId=1&amp;amp;branchName=main"&gt;&lt;img src="https://dev.azure.com/shine-oss/terminal/_apis/build/status%2FTerminal%20CI?branchName=main" alt="Terminal Build Status" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Welcome to the Windows Terminal, Console and Command-Line repo&lt;/h1&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/#installing-and-running-windows-terminal"&gt;Installing and running Windows Terminal&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/#microsoft-store-recommended"&gt;Microsoft Store [Recommended]&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/#other-install-methods"&gt;Other install methods&lt;/a&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/#via-github"&gt;Via GitHub&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/#via-windows-package-manager-cli-aka-winget"&gt;Via Windows Package Manager CLI (aka winget)&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/#via-chocolatey-unofficial"&gt;Via Chocolatey (unofficial)&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/#via-scoop-unofficial"&gt;Via Scoop (unofficial)&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/#installing-windows-terminal-canary"&gt;Installing Windows Terminal Canary&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/#windows-terminal-roadmap"&gt;Windows Terminal Roadmap&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/#terminal--console-overview"&gt;Terminal &amp;amp; Console Overview&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/#windows-terminal"&gt;Windows Terminal&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/#the-windows-console-host"&gt;The Windows Console Host&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/#shared-components"&gt;Shared Components&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/#creating-the-new-windows-terminal"&gt;Creating the new Windows Terminal&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/#resources"&gt;Resources&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/#faq"&gt;FAQ&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/#i-built-and-ran-the-new-terminal-but-it-looks-just-like-the-old-console"&gt;I built and ran the new Terminal, but it looks just like the old console&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/#documentation"&gt;Documentation&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/#contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/#communicating-with-the-team"&gt;Communicating with the Team&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/#developer-guidance"&gt;Developer Guidance&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/#prerequisites"&gt;Prerequisites&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/#building-the-code"&gt;Building the Code&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/#building-in-powershell"&gt;Building in PowerShell&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/#building-in-cmd"&gt;Building in Cmd&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/#running--debugging"&gt;Running &amp;amp; Debugging&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/#coding-guidance"&gt;Coding Guidance&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/#code-of-conduct"&gt;Code of Conduct&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;br /&gt; 
&lt;p&gt;This repository contains the source code for:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://aka.ms/terminal"&gt;Windows Terminal&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aka.ms/terminal-preview"&gt;Windows Terminal Preview&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;The Windows console host (&lt;code&gt;conhost.exe&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Components shared between the two projects&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/src/tools/ColorTool"&gt;ColorTool&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/samples"&gt;Sample projects&lt;/a&gt; that show how to consume the Windows Console APIs&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Related repositories include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.microsoft.com/windows/terminal"&gt;Windows Terminal Documentation&lt;/a&gt; (&lt;a href="https://github.com/MicrosoftDocs/terminal"&gt;Repo: Contribute to the docs&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/MicrosoftDocs/Console-Docs"&gt;Console API Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Microsoft/Cascadia-Code"&gt;Cascadia Code Font&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installing and running Windows Terminal&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Windows Terminal requires Windows 10 2004 (build 19041) or later&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Microsoft Store [Recommended]&lt;/h3&gt; 
&lt;p&gt;Install the &lt;a href="https://aka.ms/terminal"&gt;Windows Terminal from the Microsoft Store&lt;/a&gt;. This allows you to always be on the latest version when we release new builds with automatic upgrades.&lt;/p&gt; 
&lt;p&gt;This is our preferred method.&lt;/p&gt; 
&lt;h3&gt;Other install methods&lt;/h3&gt; 
&lt;h4&gt;Via GitHub&lt;/h4&gt; 
&lt;p&gt;For users who are unable to install Windows Terminal from the Microsoft Store, released builds can be manually downloaded from this repository's &lt;a href="https://github.com/microsoft/terminal/releases"&gt;Releases page&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Download the &lt;code&gt;Microsoft.WindowsTerminal_&amp;lt;versionNumber&amp;gt;.msixbundle&lt;/code&gt; file from the &lt;strong&gt;Assets&lt;/strong&gt; section. To install the app, you can simply double-click on the &lt;code&gt;.msixbundle&lt;/code&gt; file, and the app installer should automatically run. If that fails for any reason, you can try the following command at a PowerShell prompt:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;# NOTE: If you are using PowerShell 7+, please run
# Import-Module Appx -UseWindowsPowerShell
# before using Add-AppxPackage.

Add-AppxPackage Microsoft.WindowsTerminal_&amp;lt;versionNumber&amp;gt;.msixbundle
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] If you install Terminal manually:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;You may need to install the &lt;a href="https://docs.microsoft.com/troubleshoot/cpp/c-runtime-packages-desktop-bridge#how-to-install-and-update-desktop-framework-packages"&gt;VC++ v14 Desktop Framework Package&lt;/a&gt;. This should only be necessary on older builds of Windows 10 and only if you get an error about missing framework packages.&lt;/li&gt; 
  &lt;li&gt;Terminal will not auto-update when new builds are released so you will need to regularly install the latest Terminal release to receive all the latest fixes and improvements!&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Via Windows Package Manager CLI (aka winget)&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://github.com/microsoft/winget-cli"&gt;winget&lt;/a&gt; users can download and install the latest Terminal release by installing the &lt;code&gt;Microsoft.WindowsTerminal&lt;/code&gt; package:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;winget install --id Microsoft.WindowsTerminal -e
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Dependency support is available in WinGet version &lt;a href="https://github.com/microsoft/winget-cli/releases"&gt;1.6.2631 or later&lt;/a&gt;. To install the Terminal stable release 1.18 or later, please make sure you have the updated version of the WinGet client.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Via Chocolatey (unofficial)&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://chocolatey.org"&gt;Chocolatey&lt;/a&gt; users can download and install the latest Terminal release by installing the &lt;code&gt;microsoft-windows-terminal&lt;/code&gt; package:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;choco install microsoft-windows-terminal
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To upgrade Windows Terminal using Chocolatey, run the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;choco upgrade microsoft-windows-terminal
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you have any issues when installing/upgrading the package please go to the &lt;a href="https://chocolatey.org/packages/microsoft-windows-terminal"&gt;Windows Terminal package page&lt;/a&gt; and follow the &lt;a href="https://chocolatey.org/docs/package-triage-process"&gt;Chocolatey triage process&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;Via Scoop (unofficial)&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://scoop.sh"&gt;Scoop&lt;/a&gt; users can download and install the latest Terminal release by installing the &lt;code&gt;windows-terminal&lt;/code&gt; package:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;scoop bucket add extras
scoop install windows-terminal
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To update Windows Terminal using Scoop, run the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;scoop update windows-terminal
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you have any issues when installing/updating the package, please search for or report the same on the &lt;a href="https://github.com/lukesampson/scoop-extras/issues"&gt;issues page&lt;/a&gt; of Scoop Extras bucket repository.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Installing Windows Terminal Canary&lt;/h2&gt; 
&lt;p&gt;Windows Terminal Canary is a nightly build of Windows Terminal. This build has the latest code from our &lt;code&gt;main&lt;/code&gt; branch, giving you an opportunity to try features before they make it to Windows Terminal Preview.&lt;/p&gt; 
&lt;p&gt;Windows Terminal Canary is our least stable offering, so you may discover bugs before we have had a chance to find them.&lt;/p&gt; 
&lt;p&gt;Windows Terminal Canary is available as an App Installer distribution and a Portable ZIP distribution.&lt;/p&gt; 
&lt;p&gt;The App Installer distribution supports automatic updates. Due to platform limitations, this installer only works on Windows 11.&lt;/p&gt; 
&lt;p&gt;The Portable ZIP distribution is a portable application. It will not automatically update and will not automatically check for updates. This portable ZIP distribution works on Windows 10 (19041+) and Windows 11.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Distribution&lt;/th&gt; 
   &lt;th align="center"&gt;Architecture&lt;/th&gt; 
   &lt;th&gt;Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;App Installer&lt;/td&gt; 
   &lt;td align="center"&gt;x64, arm64, x86&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://aka.ms/terminal-canary-installer"&gt;download&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Portable ZIP&lt;/td&gt; 
   &lt;td align="center"&gt;x64&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://aka.ms/terminal-canary-zip-x64"&gt;download&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Portable ZIP&lt;/td&gt; 
   &lt;td align="center"&gt;ARM64&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://aka.ms/terminal-canary-zip-arm64"&gt;download&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Portable ZIP&lt;/td&gt; 
   &lt;td align="center"&gt;x86&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://aka.ms/terminal-canary-zip-x86"&gt;download&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;em&gt;Learn more about the &lt;a href="https://learn.microsoft.com/windows/terminal/distributions"&gt;types of Windows Terminal distributions&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Windows Terminal Roadmap&lt;/h2&gt; 
&lt;p&gt;The plan for the Windows Terminal &lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/doc/roadmap-2023.md"&gt;is described here&lt;/a&gt; and will be updated as the project proceeds.&lt;/p&gt; 
&lt;h2&gt;Terminal &amp;amp; Console Overview&lt;/h2&gt; 
&lt;p&gt;Please take a few minutes to review the overview below before diving into the code:&lt;/p&gt; 
&lt;h3&gt;Windows Terminal&lt;/h3&gt; 
&lt;p&gt;Windows Terminal is a new, modern, feature-rich, productive terminal application for command-line users. It includes many of the features most frequently requested by the Windows command-line community including support for tabs, rich text, globalization, configurability, theming &amp;amp; styling, and more.&lt;/p&gt; 
&lt;p&gt;The Terminal will also need to meet our goals and measures to ensure it remains fast and efficient, and doesn't consume vast amounts of memory or power.&lt;/p&gt; 
&lt;h3&gt;The Windows Console Host&lt;/h3&gt; 
&lt;p&gt;The Windows Console host, &lt;code&gt;conhost.exe&lt;/code&gt;, is Windows' original command-line user experience. It also hosts Windows' command-line infrastructure and the Windows Console API server, input engine, rendering engine, user preferences, etc. The console host code in this repository is the actual source from which the &lt;code&gt;conhost.exe&lt;/code&gt; in Windows itself is built.&lt;/p&gt; 
&lt;p&gt;Since taking ownership of the Windows command-line in 2014, the team added several new features to the Console, including background transparency, line-based selection, support for &lt;a href="https://en.wikipedia.org/wiki/ANSI_escape_code"&gt;ANSI / Virtual Terminal sequences&lt;/a&gt;, &lt;a href="https://devblogs.microsoft.com/commandline/24-bit-color-in-the-windows-console/"&gt;24-bit color&lt;/a&gt;, a &lt;a href="https://devblogs.microsoft.com/commandline/windows-command-line-introducing-the-windows-pseudo-console-conpty/"&gt;Pseudoconsole ("ConPTY")&lt;/a&gt;, and more.&lt;/p&gt; 
&lt;p&gt;However, because Windows Console's primary goal is to maintain backward compatibility, we have been unable to add many of the features the community (and the team) have been wanting for the last several years including tabs, unicode text, and emoji.&lt;/p&gt; 
&lt;p&gt;These limitations led us to create the new Windows Terminal.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;You can read more about the evolution of the command-line in general, and the Windows command-line specifically in &lt;a href="https://devblogs.microsoft.com/commandline/windows-command-line-backgrounder/"&gt;this accompanying series of blog posts&lt;/a&gt; on the Command-Line team's blog.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Shared Components&lt;/h3&gt; 
&lt;p&gt;While overhauling Windows Console, we modernized its codebase considerably, cleanly separating logical entities into modules and classes, introduced some key extensibility points, replaced several old, home-grown collections and containers with safer, more efficient &lt;a href="https://docs.microsoft.com/en-us/cpp/standard-library/stl-containers?view=vs-2022"&gt;STL containers&lt;/a&gt;, and made the code simpler and safer by using Microsoft's &lt;a href="https://github.com/Microsoft/wil"&gt;Windows Implementation Libraries - WIL&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;This overhaul resulted in several of Console's key components being available for re-use in any terminal implementation on Windows. These components include a new DirectWrite-based text layout and rendering engine, a text buffer capable of storing both UTF-16 and UTF-8, a VT parser/emitter, and more.&lt;/p&gt; 
&lt;h3&gt;Creating the new Windows Terminal&lt;/h3&gt; 
&lt;p&gt;When we started planning the new Windows Terminal application, we explored and evaluated several approaches and technology stacks. We ultimately decided that our goals would be best met by continuing our investment in our C++ codebase, which would allow us to reuse several of the aforementioned modernized components in both the existing Console and the new Terminal. Further, we realized that this would allow us to build much of the Terminal's core itself as a reusable UI control that others can incorporate into their own applications.&lt;/p&gt; 
&lt;p&gt;The result of this work is contained within this repo and delivered as the Windows Terminal application you can download from the Microsoft Store, or &lt;a href="https://github.com/microsoft/terminal/releases"&gt;directly from this repo's releases&lt;/a&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Resources&lt;/h2&gt; 
&lt;p&gt;For more information about Windows Terminal, you may find some of these resources useful and interesting:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://devblogs.microsoft.com/commandline"&gt;Command-Line Blog&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://devblogs.microsoft.com/commandline/windows-command-line-backgrounder/"&gt;Command-Line Backgrounder Blog Series&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Windows Terminal Launch: &lt;a href="https://www.youtube.com/watch?v=8gw0rXPMMPE&amp;amp;list=PLEHMQNlPj-Jzh9DkNpqipDGCZZuOwrQwR&amp;amp;index=2&amp;amp;t=0s"&gt;Terminal "Sizzle Video"&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Windows Terminal Launch: &lt;a href="https://www.youtube.com/watch?v=KMudkRcwjCw"&gt;Build 2019 Session&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Run As Radio: &lt;a href="https://www.runasradio.com/Shows/Show/645"&gt;Show 645 - Windows Terminal with Richard Turner&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Azure Devops Podcast: &lt;a href="http://azuredevopspodcast.clear-measure.com/kayla-cinnamon-and-rich-turner-on-devops-on-the-windows-terminal-team-episode-54"&gt;Episode 54 - Kayla Cinnamon and Rich Turner on DevOps on the Windows Terminal&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Microsoft Ignite 2019 Session: &lt;a href="https://myignite.techcommunity.microsoft.com/sessions/81329?source=sessions"&gt;The Modern Windows Command Line: Windows Terminal - BRK3321&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;FAQ&lt;/h2&gt; 
&lt;h3&gt;I built and ran the new Terminal, but it looks just like the old console&lt;/h3&gt; 
&lt;p&gt;Cause: You're launching the incorrect solution in Visual Studio.&lt;/p&gt; 
&lt;p&gt;Solution: Make sure you're building &amp;amp; deploying the &lt;code&gt;CascadiaPackage&lt;/code&gt; project in Visual Studio.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] &lt;code&gt;OpenConsole.exe&lt;/code&gt; is just a locally-built &lt;code&gt;conhost.exe&lt;/code&gt;, the classic Windows Console that hosts Windows' command-line infrastructure. OpenConsole is used by Windows Terminal to connect to and communicate with command-line applications (via &lt;a href="https://devblogs.microsoft.com/commandline/windows-command-line-introducing-the-windows-pseudo-console-conpty/"&gt;ConPty&lt;/a&gt;).&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;All project documentation is located at &lt;a href="https://aka.ms/terminal-docs"&gt;aka.ms/terminal-docs&lt;/a&gt;. If you would like to contribute to the documentation, please submit a pull request on the &lt;a href="https://github.com/MicrosoftDocs/terminal"&gt;Windows Terminal Documentation repo&lt;/a&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We are excited to work alongside you, our amazing community, to build and enhance Windows Terminal!&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;&lt;strong&gt;BEFORE you start work on a feature/fix&lt;/strong&gt;&lt;/em&gt;, please read &amp;amp; follow our &lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/CONTRIBUTING.md"&gt;Contributor's Guide&lt;/a&gt; to help avoid any wasted or duplicate effort.&lt;/p&gt; 
&lt;h2&gt;Communicating with the Team&lt;/h2&gt; 
&lt;p&gt;The easiest way to communicate with the team is via GitHub issues.&lt;/p&gt; 
&lt;p&gt;Please file new issues, feature requests and suggestions, but &lt;strong&gt;DO search for similar open/closed preexisting issues before creating a new issue.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;If you would like to ask a question that you feel doesn't warrant an issue (yet), please reach out to us via Twitter:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Christopher Nguyen, Product Manager: &lt;a href="https://twitter.com/nguyen_dows"&gt;@nguyen_dows&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Dustin Howett, Engineering Lead: &lt;a href="https://twitter.com/DHowett"&gt;@dhowett&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Mike Griese, Senior Developer: &lt;a href="https://mastodon.social/@zadjii"&gt;@zadjii@mastodon.social&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Carlos Zamora, Developer: &lt;a href="https://twitter.com/cazamor_msft"&gt;@cazamor_msft&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Pankaj Bhojwani, Developer&lt;/li&gt; 
 &lt;li&gt;Leonard Hecker, Developer: &lt;a href="https://twitter.com/LeonardHecker"&gt;@LeonardHecker&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Developer Guidance&lt;/h2&gt; 
&lt;h2&gt;Prerequisites&lt;/h2&gt; 
&lt;p&gt;You can configure your environment to build Terminal in one of two ways:&lt;/p&gt; 
&lt;h3&gt;Using WinGet configuration file&lt;/h3&gt; 
&lt;p&gt;After cloning the repository, you can use a &lt;a href="https://learn.microsoft.com/en-us/windows/package-manager/configuration/#use-a-winget-configuration-file-to-configure-your-machine"&gt;WinGet configuration file&lt;/a&gt; to set up your environment. The &lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/.config/configuration.winget"&gt;default configuration file&lt;/a&gt; installs Visual Studio 2022 Community &amp;amp; rest of the required tools. There are two other variants of the configuration file available in the &lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/.config"&gt;.config&lt;/a&gt; directory for Enterprise &amp;amp; Professional editions of Visual Studio 2022. To run the default configuration file, you can either double-click the file from explorer or run the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;winget configure .config\configuration.winget
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Manual configuration&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;You must be running Windows 10 2004 (build &amp;gt;= 10.0.19041.0) or later to run Windows Terminal&lt;/li&gt; 
 &lt;li&gt;You must &lt;a href="https://docs.microsoft.com/en-us/windows/uwp/get-started/enable-your-device-for-development"&gt;enable Developer Mode in the Windows Settings app&lt;/a&gt; to locally install and run Windows Terminal&lt;/li&gt; 
 &lt;li&gt;You must have &lt;a href="https://github.com/PowerShell/PowerShell/releases/latest"&gt;PowerShell 7 or later&lt;/a&gt; installed&lt;/li&gt; 
 &lt;li&gt;You must have the &lt;a href="https://developer.microsoft.com/en-us/windows/downloads/windows-sdk/"&gt;Windows 11 (10.0.22621.0) SDK&lt;/a&gt; installed&lt;/li&gt; 
 &lt;li&gt;You must have at least &lt;a href="https://visualstudio.microsoft.com/downloads/"&gt;VS 2022&lt;/a&gt; installed&lt;/li&gt; 
 &lt;li&gt;You must install the following Workloads via the VS Installer. Note: Opening the solution in VS 2022 will &lt;a href="https://devblogs.microsoft.com/setup/configure-visual-studio-across-your-organization-with-vsconfig/"&gt;prompt you to install missing components automatically&lt;/a&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;Desktop Development with C++&lt;/li&gt; 
   &lt;li&gt;Universal Windows Platform Development&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;The following Individual Components&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;C++ (v143) Universal Windows Platform Tools&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;You must install the &lt;a href="https://docs.microsoft.com/dotnet/framework/install/guide-for-developers#to-install-the-net-framework-developer-pack-or-targeting-pack"&gt;.NET Framework Targeting Pack&lt;/a&gt; to build test projects&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Building the Code&lt;/h2&gt; 
&lt;p&gt;OpenConsole.sln may be built from within Visual Studio or from the command-line using a set of convenience scripts &amp;amp; tools in the &lt;strong&gt;/tools&lt;/strong&gt; directory:&lt;/p&gt; 
&lt;h3&gt;Building in PowerShell&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;Import-Module .\tools\OpenConsole.psm1
Set-MsBuildDevEnvironment
Invoke-OpenConsoleBuild
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Building in Cmd&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;.\tools\razzle.cmd
bcz
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Running &amp;amp; Debugging&lt;/h2&gt; 
&lt;p&gt;To debug the Windows Terminal in VS, right click on &lt;code&gt;CascadiaPackage&lt;/code&gt; (in the Solution Explorer) and go to properties. In the Debug menu, change "Application process" and "Background task process" to "Native Only".&lt;/p&gt; 
&lt;p&gt;You should then be able to build &amp;amp; debug the Terminal project by hitting &lt;kbd&gt;F5&lt;/kbd&gt;. Make sure to select either the "x64" or the "x86" platform - the Terminal doesn't build for "Any Cpu" (because the Terminal is a C++ application, not a C# one).&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;üëâ You will &lt;em&gt;not&lt;/em&gt; be able to launch the Terminal directly by running the WindowsTerminal.exe. For more details on why, see &lt;a href="https://github.com/microsoft/terminal/issues/926"&gt;#926&lt;/a&gt;, &lt;a href="https://github.com/microsoft/terminal/issues/4043"&gt;#4043&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Coding Guidance&lt;/h3&gt; 
&lt;p&gt;Please review these brief docs below about our coding practices.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;üëâ If you find something missing from these docs, feel free to contribute to any of our documentation files anywhere in the repository (or write some new ones!)&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;This is a work in progress as we learn what we'll need to provide people in order to be effective contributors to our project.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/doc/STYLE.md"&gt;Coding Style&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/doc/ORGANIZATION.md"&gt;Code Organization&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/doc/EXCEPTIONS.md"&gt;Exceptions in our legacy codebase&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/doc/WIL.md"&gt;Helpful smart pointers and macros for interfacing with Windows in WIL&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Code of Conduct&lt;/h2&gt; 
&lt;p&gt;This project has adopted the &lt;a href="https://opensource.microsoft.com/codeofconduct/"&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href="https://opensource.microsoft.com/codeofconduct/faq/"&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href="mailto:opencode@microsoft.com"&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>juspay/hyperswitch</title>
      <link>https://github.com/juspay/hyperswitch</link>
      <description>&lt;p&gt;An open source payments switch written in Rust to make payments fast, reliable and affordable&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/juspay/hyperswitch/main/docs/imgs/hyperswitch-logo-dark.svg#gh-dark-mode-only" alt="Hyperswitch-Logo" width="40%" /&gt; &lt;img src="https://raw.githubusercontent.com/juspay/hyperswitch/main/docs/imgs/hyperswitch-logo-light.svg#gh-light-mode-only" alt="Hyperswitch-Logo" width="40%" /&gt; &lt;/p&gt; 
&lt;h1 align="center"&gt;Composable Open-Source Payments Infrastructure&lt;/h1&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/juspay/hyperswitch/main/docs/gifs/quickstart.gif" alt="Quickstart demo" /&gt; &lt;/p&gt; 
&lt;!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} --&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/juspay/hyperswitch/actions?query=workflow%3ACI+branch%3Amain"&gt; &lt;img src="https://github.com/juspay/hyperswitch/workflows/CI-push/badge.svg?sanitize=true" /&gt; &lt;/a&gt; &lt;a href="https://github.com/juspay/hyperswitch/raw/main/LICENSE"&gt; &lt;img src="https://img.shields.io/github/license/juspay/hyperswitch" /&gt; &lt;/a&gt; &lt;a href="https://github.com/juspay/hyperswitch/raw/main/LICENSE"&gt; &lt;img src="https://img.shields.io/badge/Made_in-Rust-orange" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://www.linkedin.com/company/hyperswitch/"&gt; &lt;img src="https://img.shields.io/badge/follow-hyperswitch-blue?logo=linkedin&amp;amp;labelColor=grey" /&gt; &lt;/a&gt; &lt;a href="https://x.com/hyperswitchio"&gt; &lt;img src="https://img.shields.io/badge/follow-%40hyperswitchio-white?logo=x&amp;amp;labelColor=grey" /&gt; &lt;/a&gt; &lt;a href="https://inviter.co/hyperswitch-slack"&gt; &lt;img src="https://img.shields.io/badge/chat-on_slack-blue?logo=slack&amp;amp;labelColor=grey&amp;amp;color=%233f0e40" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìÅ Table of Contents&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/juspay/hyperswitch/main/#-what-can-i-do-with-hyperswitch"&gt;What Can I Do with Hyperswitch?&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/juspay/hyperswitch/main/#-quickstart-local-setup"&gt;Quickstart (Local Setup)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/juspay/hyperswitch/main/#cloud-deployment"&gt;Cloud Deployment&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/juspay/hyperswitch/main/#hosted-sandbox-no-setup-required"&gt;Hosted Sandbox (No Setup Required)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/juspay/hyperswitch/main/#-why-hyperswitch"&gt;Why Hyperswitch?&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/juspay/hyperswitch/main/#architectural-overview"&gt;Architectural Overview&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/juspay/hyperswitch/main/#our-vision"&gt;Our Vision&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/juspay/hyperswitch/main/#community--contributions"&gt;Community &amp;amp; Contributions&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/juspay/hyperswitch/main/#feature-requests--bugs"&gt;Feature Requests &amp;amp; Bugs&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/juspay/hyperswitch/main/#versioning"&gt;Versioning&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/juspay/hyperswitch/main/#copyright-and-license"&gt;License&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/juspay/hyperswitch/main/#team-behind-hyperswitch"&gt;Team Behind Hyperswitch&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;summary&gt;&lt;h2&gt; What Can I Do with Hyperswitch?&lt;/h2&gt;&lt;/summary&gt; 
&lt;p&gt;Hyperswitch offers a modular, open-source payments infrastructure designed for flexibility and control. Apart from our Payment Suite offering, this solution allows businesses to pick and integrate only the modules they need on top of their existing payment stack ‚Äî without unnecessary complexity or vendor lock-in.&lt;/p&gt; 
&lt;p&gt;Each module is independent and purpose-built to optimize different aspects of payment processing.&lt;/p&gt; 
&lt;h3&gt; Learn More About The Payment Modules &lt;/h3&gt; 
&lt;details&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Cost Observability&lt;/strong&gt;&lt;br /&gt; Advanced observability tools to audit, monitor, and optimize your payment costs. Detect hidden fees, downgrades, and penalties with self-serve dashboards and actionable insights.&lt;br /&gt; &lt;em&gt;&lt;a href="https://docs.hyperswitch.io/about-hyperswitch/payments-modules/ai-powered-cost-observability"&gt;Read more&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Revenue Recovery&lt;/strong&gt;&lt;br /&gt; Combat passive churn with intelligent retry strategies tuned by card bin, region, method, and more. Offers fine-grained control over retry algorithms, penalty budgets, and recovery transparency.&lt;br /&gt; &lt;em&gt;&lt;a href="https://docs.hyperswitch.io/about-hyperswitch/payments-modules/revenue-recovery"&gt;Read more&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Vault&lt;/strong&gt;&lt;br /&gt; A PCI-compliant vault service to store cards, tokens, wallets, and bank credentials. Provides a unified, secure, and reusable store of customer-linked payment methods.&lt;br /&gt; &lt;em&gt;&lt;a href="https://docs.hyperswitch.io/about-hyperswitch/payments-modules/vault"&gt;Read more&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Intelligent Routing&lt;/strong&gt;&lt;br /&gt; Route each transaction to the PSP with the highest predicted auth rate. Reduce retries, avoid downtime, and minimize latency while maximizing first attempt success.&lt;br /&gt; &lt;em&gt;&lt;a href="https://docs.hyperswitch.io/about-hyperswitch/payments-modules/intelligent-routing"&gt;Read more&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Reconciliation&lt;/strong&gt;&lt;br /&gt; Automate 2-way and 3-way reconciliation with backdated support, staggered scheduling, and customizable outputs. Reduces manual ops effort and increases audit confidence.&lt;br /&gt; &lt;em&gt;&lt;a href="https://docs.hyperswitch.io/about-hyperswitch/payments-modules/reconciliation"&gt;Read more&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Alternate Payment Methods&lt;/strong&gt;&lt;br /&gt; Drop-in widgets for PayPal, Apple Pay, Google Pay, Samsung Pay, Pay by Bank, and BNPL providers like Klarna. Maximizes conversions with seamless one-click checkout.&lt;br /&gt; &lt;em&gt;&lt;a href="https://docs.hyperswitch.io/about-hyperswitch/payments-modules/enable-alternate-payment-method-widgets"&gt;Read more&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;h3&gt; Local Setup via Docker &lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# One-click local setup

git clone --depth 1 --branch latest https://github.com/juspay/hyperswitch

cd hyperswitch

scripts/setup.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;This script: &lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Detects Docker/Podman&lt;/li&gt; 
  &lt;li&gt;Offers multiple deployment profiles: 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Standard&lt;/strong&gt;: App server + Control Center&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Full&lt;/strong&gt;: Includes monitoring + schedulers&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Minimal&lt;/strong&gt;: Standalone App server&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;Provides access links when done&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;If you need further help, check out our &lt;a href="https://docs.hyperswitch.io/hyperswitch-open-source/overview/unified-local-setup-using-docker"&gt;video tutorial&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;üëâ After setup, &lt;a href="https://docs.hyperswitch.io/hyperswitch-open-source/account-setup/using-hyperswitch-control-center#add-a-payment-processor"&gt;configure a connector&lt;/a&gt; and &lt;a href="https://docs.hyperswitch.io/hyperswitch-open-source/account-setup/test-a-payment"&gt;test a payment&lt;/a&gt;.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h3&gt;Hosted Sandbox (No Setup Required)&lt;/h3&gt; 
&lt;p&gt;Hyperswitch offers a fully hosted sandbox environment that requires no setup. You can explore the Control Center, configure payment connectors, and test payments directly from the UI.&lt;/p&gt; 
&lt;a href="https://app.hyperswitch.io"&gt; &lt;img src="https://github.com/juspay/hyperswitch/raw/main/docs/imgs/try-the-sandbox.png?raw=true" height="35" /&gt; &lt;/a&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt; What you can do in the Hosted Sandbox&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Access the full Control Center&lt;/li&gt; 
  &lt;li&gt;Configure payment connectors&lt;/li&gt; 
  &lt;li&gt;View logs, routing rules, and retry strategies&lt;/li&gt; 
  &lt;li&gt;Try payments directly from the UI&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;&lt;strong&gt;Cloud Deployment&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;You can deploy to AWS, GCP, or Azure using Helm or CDK scripts. Fastest path:&lt;/p&gt; 
&lt;p&gt;Click to deploy via AWS:&lt;/p&gt; 
&lt;a href="https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/new?stackName=HyperswitchBootstarp&amp;amp;templateURL=https://hyperswitch-synth.s3.eu-central-1.amazonaws.com/hs-starter-config.yaml"&gt; &lt;img src="https://github.com/juspay/hyperswitch/raw/main/docs/imgs/aws_button.png?raw=true" height="35" /&gt; &lt;/a&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Cloud Deployment Instructions&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Click the AWS deployment button above to launch the stack.&lt;/li&gt; 
  &lt;li&gt;Follow the guided steps in the AWS Console (approx. 30‚Äì45 mins).&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;‚úÖ This setup provisions Hyperswitch on your cloud account using CloudFormation.&lt;/p&gt; 
 &lt;p&gt;üìò For full instructions and Helm-based deployments, check out the&lt;br /&gt; &lt;a href="https://docs.hyperswitch.io/hyperswitch-open-source/deploy-on-kubernetes-using-helm"&gt;Cloud Install Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;/details&gt; 
&lt;a href="https://raw.githubusercontent.com/juspay/hyperswitch/main/#architectural-overview"&gt; &lt;h2 id="architectural-overview"&gt;Architectural Overview&lt;/h2&gt; &lt;/a&gt; 
&lt;img src="https://raw.githubusercontent.com/juspay/hyperswitch/main/docs/imgs/features.png" /&gt; 
&lt;img src="https://raw.githubusercontent.com/juspay/hyperswitch/main/docs/imgs/non-functional-features.png" /&gt; 
&lt;img src="https://raw.githubusercontent.com/juspay/hyperswitch/main/docs/imgs/hyperswitch-architecture-v1.png" /&gt; 
&lt;h2&gt;Why Hyperswitch?&lt;/h2&gt; 
&lt;p&gt;Hyperswitch is a commercial open-source payments stack purpose-built for scale, flexibility, and developer experience. Designed with a modular architecture, Hyperswitch lets you pick only the components you need‚Äîwhether it‚Äôs routing, retries, vaulting, or observability‚Äîwithout vendor lock-in or bloated integrations.&lt;/p&gt; 
&lt;p&gt;Built in Rust for performance and reliability, Hyperswitch supports global payment methods (cards, wallets, BNPL, UPI, Pay by Bank), exposes smart routing and retry logic, and provides a visual workflow builder in the Control Center. Whether you're integrating a full payment suite or augmenting an existing stack with a single module, Hyperswitch meets you where you are.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;‚ÄúLinux for Payments‚Äù&lt;/strong&gt; ‚Äî Hyperswitch is a well-architected reference for teams who want to own their payments stack.&lt;/p&gt; 
&lt;p&gt;We believe in:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt; Embracing Payment Diversity:&lt;/strong&gt; Innovation comes from enabling choice‚Äîacross payment methods, processors, and flows.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt; Open Source by Default:&lt;/strong&gt; Transparency drives trust and builds better, reusable software.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt; Community-Driven Development:&lt;/strong&gt; Our roadmap is shaped by real-world use cases and contributors.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt; Systems-Level Engineering:&lt;/strong&gt; We hold ourselves to a high bar for reliability, security, and performance.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt; Maximizing Value Creation:&lt;/strong&gt; For developers, customers, and partners alike.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt; Community-Driven, Enterprise-Tested:&lt;/strong&gt; Hyperswitch is built in the open with real-world feedback from developers and contributors, and maintained by Juspay, the team powering payment infrastructure for 400+ leading enterprises worldwide.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributors from around the world to help build Hyperswitch. Whether you're fixing bugs, improving documentation, or adding new features, your help is appreciated.&lt;/p&gt; 
&lt;p&gt;Please read our &lt;a href="https://github.com/juspay/hyperswitch/raw/main/docs/CONTRIBUTING.md"&gt;contributing guidelines&lt;/a&gt; to get started.&lt;/p&gt; 
&lt;p&gt;Join the conversation on &lt;a href="https://inviter.co/hyperswitch-slack"&gt;Slack&lt;/a&gt; or explore open issues on &lt;a href="https://github.com/juspay/hyperswitch/issues"&gt;GitHub&lt;/a&gt;.&lt;/p&gt; 
&lt;a href="https://raw.githubusercontent.com/juspay/hyperswitch/main/#feature-requests"&gt; &lt;h2 id="feature-requests"&gt;Feature requests &amp;amp; Bugs&lt;/h2&gt; &lt;/a&gt; 
&lt;p&gt;For new product features, enhancements, roadmap discussions, or to share queries and ideas, visit our &lt;a href="https://github.com/juspay/hyperswitch/discussions"&gt;GitHub Discussions&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;For reporting a bug, please read the issue guidelines and search for &lt;a href="https://github.com/juspay/hyperswitch/issues"&gt;existing and closed issues&lt;/a&gt;. If your problem or idea is not addressed yet, please &lt;a href="https://github.com/juspay/hyperswitch/issues/new/choose"&gt;open a new issue&lt;/a&gt;.&lt;/p&gt; 
&lt;a href="https://raw.githubusercontent.com/juspay/hyperswitch/main/#versioning"&gt; &lt;h2 id="versioning"&gt;Versioning&lt;/h2&gt; &lt;/a&gt; 
&lt;p&gt;Check the &lt;a href="https://raw.githubusercontent.com/juspay/hyperswitch/main/CHANGELOG.md"&gt;CHANGELOG.md&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;a href="https://raw.githubusercontent.com/juspay/hyperswitch/main/#copyright-and-license"&gt; &lt;h2 id="copyright-and-license"&gt;Copyright and License&lt;/h2&gt; &lt;/a&gt; 
&lt;p&gt;This product is licensed under the &lt;a href="https://raw.githubusercontent.com/juspay/hyperswitch/main/LICENSE"&gt;Apache 2.0 License&lt;/a&gt;.&lt;/p&gt; 
&lt;a href="https://raw.githubusercontent.com/juspay/hyperswitch/main/#team-behind-hyperswitch"&gt; &lt;h2 id="team-behind-hyperswitch"&gt;Team behind Hyperswitch&lt;/h2&gt; &lt;/a&gt; 
&lt;p&gt;The core team of 150+ engineers building Hyperswitch. Keep up the great work! ü•Ç&lt;/p&gt; 
&lt;a href="https://github.com/juspay/hyperswitch/graphs/contributors"&gt; &lt;img src="https://contributors-img.web.app/image?repo=juspay/hyperswitch" alt="Contributors" /&gt; &lt;/a&gt;</description>
    </item>
    
    <item>
      <title>mlabonne/llm-course</title>
      <link>https://github.com/mlabonne/llm-course</link>
      <description>&lt;p&gt;Course to get into Large Language Models (LLMs) with roadmaps and Colab notebooks.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/mlabonne/llm-course/main/img/banner.png" alt="LLM Course" /&gt; 
 &lt;p align="center"&gt; ùïè &lt;a href="https://twitter.com/maximelabonne"&gt;Follow me on X&lt;/a&gt; ‚Ä¢ ü§ó &lt;a href="https://huggingface.co/mlabonne"&gt;Hugging Face&lt;/a&gt; ‚Ä¢ üíª &lt;a href="https://mlabonne.github.io/blog"&gt;Blog&lt;/a&gt; ‚Ä¢ üìô &lt;a href="https://packt.link/a/9781836200079"&gt;LLM Engineer's Handbook&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;a href="https://a.co/d/a2M67rE"&gt;&lt;img align="right" width="25%" src="https://i.imgur.com/7iNjEq2.png" alt="LLM Engineer's Handbook Cover" /&gt;&lt;/a&gt;The LLM course is divided into three parts:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;üß© &lt;strong&gt;LLM Fundamentals&lt;/strong&gt; is optional and covers fundamental knowledge about mathematics, Python, and neural networks.&lt;/li&gt; 
 &lt;li&gt;üßë‚Äçüî¨ &lt;strong&gt;The LLM Scientist&lt;/strong&gt; focuses on building the best possible LLMs using the latest techniques.&lt;/li&gt; 
 &lt;li&gt;üë∑ &lt;strong&gt;The LLM Engineer&lt;/strong&gt; focuses on creating LLM-based applications and deploying them.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Based on this course, I wrote the &lt;a href="https://packt.link/a/9781836200079"&gt;LLM Engineer's Handbook&lt;/a&gt; with Paul Iuzstin. It's a hands-on and detailed book that covers an end-to-end LLM application from design to deployment. The LLM course will always stay free but feel free to support my work by purchasing the book.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;For an interactive version of this course, I created an LLM assistant that will answer questions and test your knowledge in a personalized way on &lt;a href="https://hf.co/chat/assistant/66029d2e5f4a884f7aabc9d1"&gt;&lt;strong&gt;HuggingChat&lt;/strong&gt;&lt;/a&gt; or &lt;a href="https://chat.openai.com/g/g-yviLuLqvI-llm-course"&gt;&lt;strong&gt;ChatGPT&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;üìù Notebooks&lt;/h2&gt; 
&lt;p&gt;A list of notebooks and articles I wrote about LLMs.&lt;/p&gt; 
&lt;h3&gt;Tools&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Notebook&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Notebook&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üßê &lt;a href="https://github.com/mlabonne/llm-autoeval"&gt;LLM AutoEval&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Automatically evaluate your LLMs using RunPod&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/drive/1Igs3WZuXAIv9X0vwqiE90QlEPys8e8Oa?usp=sharing"&gt;&lt;img src="https://raw.githubusercontent.com/mlabonne/llm-course/main/img/colab.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ü•± LazyMergekit&lt;/td&gt; 
   &lt;td&gt;Easily merge models using MergeKit in one click.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb?usp=sharing"&gt;&lt;img src="https://raw.githubusercontent.com/mlabonne/llm-course/main/img/colab.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ü¶é LazyAxolotl&lt;/td&gt; 
   &lt;td&gt;Fine-tune models in the cloud using Axolotl in one click.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/drive/1TsDKNo2riwVmU55gjuBgB1AXVtRRfRHW?usp=sharing"&gt;&lt;img src="https://raw.githubusercontent.com/mlabonne/llm-course/main/img/colab.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;‚ö° AutoQuant&lt;/td&gt; 
   &lt;td&gt;Quantize LLMs in GGUF, GPTQ, EXL2, AWQ, and HQQ formats in one click.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/drive/1b6nqC7UZVt8bx4MksX7s656GXPM-eWw4?usp=sharing"&gt;&lt;img src="https://raw.githubusercontent.com/mlabonne/llm-course/main/img/colab.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üå≥ Model Family Tree&lt;/td&gt; 
   &lt;td&gt;Visualize the family tree of merged models.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/drive/1s2eQlolcI1VGgDhqWIANfkfKvcKrMyNr?usp=sharing"&gt;&lt;img src="https://raw.githubusercontent.com/mlabonne/llm-course/main/img/colab.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üöÄ ZeroSpace&lt;/td&gt; 
   &lt;td&gt;Automatically create a Gradio chat interface using a free ZeroGPU.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/drive/1LcVUW5wsJTO2NGmozjji5CkC--646LgC"&gt;&lt;img src="https://raw.githubusercontent.com/mlabonne/llm-course/main/img/colab.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;‚úÇÔ∏è AutoAbliteration&lt;/td&gt; 
   &lt;td&gt;Automatically abliteration models with custom datasets.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/drive/1RmLv-pCMBBsQGXQIM8yF-OdCNyoylUR1?usp=sharing"&gt;&lt;img src="https://raw.githubusercontent.com/mlabonne/llm-course/main/img/colab.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üßº AutoDedup&lt;/td&gt; 
   &lt;td&gt;Automatically deduplicate datasets using the Rensa library.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/drive/1o1nzwXWAa8kdkEJljbJFW1VuI-3VZLUn?usp=sharing"&gt;&lt;img src="https://raw.githubusercontent.com/mlabonne/llm-course/main/img/colab.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Fine-tuning&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Notebook&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Article&lt;/th&gt; 
   &lt;th&gt;Notebook&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Fine-tune Llama 3.1 with Unsloth&lt;/td&gt; 
   &lt;td&gt;Ultra-efficient supervised fine-tuning in Google Colab.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://mlabonne.github.io/blog/posts/2024-07-29_Finetune_Llama31.html"&gt;Article&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/drive/164cg_O7SV7G8kZr_JXqLd6VC7pd86-1Z?usp=sharing"&gt;&lt;img src="https://raw.githubusercontent.com/mlabonne/llm-course/main/img/colab.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Fine-tune Llama 3 with ORPO&lt;/td&gt; 
   &lt;td&gt;Cheaper and faster fine-tuning in a single stage with ORPO.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://mlabonne.github.io/blog/posts/2024-04-19_Fine_tune_Llama_3_with_ORPO.html"&gt;Article&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/drive/1eHNWg9gnaXErdAa8_mcvjMupbSS6rDvi"&gt;&lt;img src="https://raw.githubusercontent.com/mlabonne/llm-course/main/img/colab.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Fine-tune Mistral-7b with DPO&lt;/td&gt; 
   &lt;td&gt;Boost the performance of supervised fine-tuned models with DPO.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://mlabonne.github.io/blog/posts/Fine_tune_Mistral_7b_with_DPO.html"&gt;Article&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/drive/15iFBr1xWgztXvhrj5I9fBv20c7CFOPBE?usp=sharing"&gt;&lt;img src="https://raw.githubusercontent.com/mlabonne/llm-course/main/img/colab.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Fine-tune Mistral-7b with QLoRA&lt;/td&gt; 
   &lt;td&gt;Supervised fine-tune Mistral-7b in a free-tier Google Colab with TRL.&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/drive/1o_w0KastmEJNVwT5GoqMCciH-18ca5WS?usp=sharing"&gt;&lt;img src="https://raw.githubusercontent.com/mlabonne/llm-course/main/img/colab.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Fine-tune CodeLlama using Axolotl&lt;/td&gt; 
   &lt;td&gt;End-to-end guide to the state-of-the-art tool for fine-tuning.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://mlabonne.github.io/blog/posts/A_Beginners_Guide_to_LLM_Finetuning.html"&gt;Article&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/drive/1Xu0BrCB7IShwSWKVcfAfhehwjDrDMH5m?usp=sharing"&gt;&lt;img src="https://raw.githubusercontent.com/mlabonne/llm-course/main/img/colab.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Fine-tune Llama 2 with QLoRA&lt;/td&gt; 
   &lt;td&gt;Step-by-step guide to supervised fine-tune Llama 2 in Google Colab.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://mlabonne.github.io/blog/posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html"&gt;Article&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/drive/1PEQyJO1-f6j0S_XJ8DV50NkpzasXkrzd?usp=sharing"&gt;&lt;img src="https://raw.githubusercontent.com/mlabonne/llm-course/main/img/colab.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Quantization&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Notebook&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Article&lt;/th&gt; 
   &lt;th&gt;Notebook&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Introduction to Quantization&lt;/td&gt; 
   &lt;td&gt;Large language model optimization using 8-bit quantization.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://mlabonne.github.io/blog/posts/Introduction_to_Weight_Quantization.html"&gt;Article&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/drive/1DPr4mUQ92Cc-xf4GgAaB6dFcFnWIvqYi?usp=sharing"&gt;&lt;img src="https://raw.githubusercontent.com/mlabonne/llm-course/main/img/colab.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;4-bit Quantization using GPTQ&lt;/td&gt; 
   &lt;td&gt;Quantize your own open-source LLMs to run them on consumer hardware.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://mlabonne.github.io/blog/4bit_quantization/"&gt;Article&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/drive/1lSvVDaRgqQp_mWK_jC9gydz6_-y6Aq4A?usp=sharing"&gt;&lt;img src="https://raw.githubusercontent.com/mlabonne/llm-course/main/img/colab.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Quantization with GGUF and llama.cpp&lt;/td&gt; 
   &lt;td&gt;Quantize Llama 2 models with llama.cpp and upload GGUF versions to the HF Hub.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html"&gt;Article&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/drive/1pL8k7m04mgE5jo2NrjGi8atB0j_37aDD?usp=sharing"&gt;&lt;img src="https://raw.githubusercontent.com/mlabonne/llm-course/main/img/colab.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ExLlamaV2: The Fastest Library to Run&amp;nbsp;LLMs&lt;/td&gt; 
   &lt;td&gt;Quantize and run EXL2&amp;nbsp;models and upload them to the HF Hub.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://mlabonne.github.io/blog/posts/ExLlamaV2_The_Fastest_Library_to_Run%C2%A0LLMs.html"&gt;Article&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/drive/1yrq4XBlxiA0fALtMoT2dwiACVc77PHou?usp=sharing"&gt;&lt;img src="https://raw.githubusercontent.com/mlabonne/llm-course/main/img/colab.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Other&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Notebook&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Article&lt;/th&gt; 
   &lt;th&gt;Notebook&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Merge LLMs with MergeKit&lt;/td&gt; 
   &lt;td&gt;Create your own models easily, no GPU required!&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://mlabonne.github.io/blog/posts/2024-01-08_Merge_LLMs_with_mergekit%20copy.html"&gt;Article&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/drive/1_JS7JKJAQozD48-LhYdegcuuZ2ddgXfr?usp=sharing"&gt;&lt;img src="https://raw.githubusercontent.com/mlabonne/llm-course/main/img/colab.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Create MoEs with MergeKit&lt;/td&gt; 
   &lt;td&gt;Combine multiple experts into a single frankenMoE&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://mlabonne.github.io/blog/posts/2024-03-28_Create_Mixture_of_Experts_with_MergeKit.html"&gt;Article&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb?usp=sharing"&gt;&lt;img src="https://raw.githubusercontent.com/mlabonne/llm-course/main/img/colab.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Uncensor any LLM with abliteration&lt;/td&gt; 
   &lt;td&gt;Fine-tuning without retraining&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://mlabonne.github.io/blog/posts/2024-06-04_Uncensor_any_LLM_with_abliteration.html"&gt;Article&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/drive/1VYm3hOcvCpbGiqKZb141gJwjdmmCcVpR?usp=sharing"&gt;&lt;img src="https://raw.githubusercontent.com/mlabonne/llm-course/main/img/colab.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Improve ChatGPT with Knowledge Graphs&lt;/td&gt; 
   &lt;td&gt;Augment ChatGPT's answers with knowledge graphs.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://mlabonne.github.io/blog/posts/Article_Improve_ChatGPT_with_Knowledge_Graphs.html"&gt;Article&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/drive/1mwhOSw9Y9bgEaIFKT4CLi0n18pXRM4cj?usp=sharing"&gt;&lt;img src="https://raw.githubusercontent.com/mlabonne/llm-course/main/img/colab.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Decoding Strategies in Large Language Models&lt;/td&gt; 
   &lt;td&gt;A guide to text generation from beam search to nucleus sampling&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://mlabonne.github.io/blog/posts/2022-06-07-Decoding_strategies.html"&gt;Article&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/drive/19CJlOS5lI29g-B3dziNn93Enez1yiHk2?usp=sharing"&gt;&lt;img src="https://raw.githubusercontent.com/mlabonne/llm-course/main/img/colab.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;üß© LLM Fundamentals&lt;/h2&gt; 
&lt;p&gt;This section introduces essential knowledge about mathematics, Python, and neural networks. You might not want to start here but refer to it as needed.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Toggle section (optional)&lt;/summary&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/mlabonne/llm-course/main/img/roadmap_fundamentals.png" alt="" /&gt;&lt;/p&gt; 
 &lt;h3&gt;1. Mathematics for Machine Learning&lt;/h3&gt; 
 &lt;p&gt;Before mastering machine learning, it is important to understand the fundamental mathematical concepts that power these algorithms.&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Linear Algebra&lt;/strong&gt;: This is crucial for understanding many algorithms, especially those used in deep learning. Key concepts include vectors, matrices, determinants, eigenvalues and eigenvectors, vector spaces, and linear transformations.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Calculus&lt;/strong&gt;: Many machine learning algorithms involve the optimization of continuous functions, which requires an understanding of derivatives, integrals, limits, and series. Multivariable calculus and the concept of gradients are also important.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Probability and Statistics&lt;/strong&gt;: These are crucial for understanding how models learn from data and make predictions. Key concepts include probability theory, random variables, probability distributions, expectations, variance, covariance, correlation, hypothesis testing, confidence intervals, maximum likelihood estimation, and Bayesian inference.&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;üìö Resources:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=fNk_zzaMoSs&amp;amp;list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab"&gt;3Blue1Brown - The Essence of Linear Algebra&lt;/a&gt;: Series of videos that give a geometric intuition to these concepts.&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=qBigTkBLU6g&amp;amp;list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9"&gt;StatQuest with Josh Starmer - Statistics Fundamentals&lt;/a&gt;: Offers simple and clear explanations for many statistical concepts.&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://automata88.medium.com/list/cacc224d5e7d"&gt;AP Statistics Intuition by Ms Aerin&lt;/a&gt;: List of Medium articles that provide the intuition behind every probability distribution.&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://immersivemath.com/ila/learnmore.html"&gt;Immersive Linear Algebra&lt;/a&gt;: Another visual interpretation of linear algebra.&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://www.khanacademy.org/math/linear-algebra"&gt;Khan Academy - Linear Algebra&lt;/a&gt;: Great for beginners as it explains the concepts in a very intuitive way.&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://www.khanacademy.org/math/calculus-1"&gt;Khan Academy - Calculus&lt;/a&gt;: An interactive course that covers all the basics of calculus.&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://www.khanacademy.org/math/statistics-probability"&gt;Khan Academy - Probability and Statistics&lt;/a&gt;: Delivers the material in an easy-to-understand format.&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;hr /&gt; 
 &lt;h3&gt;2. Python for Machine Learning&lt;/h3&gt; 
 &lt;p&gt;Python is a powerful and flexible programming language that's particularly good for machine learning, thanks to its readability, consistency, and robust ecosystem of data science libraries.&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Python Basics&lt;/strong&gt;: Python programming requires a good understanding of the basic syntax, data types, error handling, and object-oriented programming.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Data Science Libraries&lt;/strong&gt;: It includes familiarity with NumPy for numerical operations, Pandas for data manipulation and analysis, Matplotlib and Seaborn for data visualization.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Data Preprocessing&lt;/strong&gt;: This involves feature scaling and normalization, handling missing data, outlier detection, categorical data encoding, and splitting data into training, validation, and test sets.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Machine Learning Libraries&lt;/strong&gt;: Proficiency with Scikit-learn, a library providing a wide selection of supervised and unsupervised learning algorithms, is vital. Understanding how to implement algorithms like linear regression, logistic regression, decision trees, random forests, k-nearest neighbors (K-NN), and K-means clustering is important. Dimensionality reduction techniques like PCA and t-SNE are also helpful for visualizing high-dimensional data.&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;üìö Resources:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://realpython.com/"&gt;Real Python&lt;/a&gt;: A comprehensive resource with articles and tutorials for both beginner and advanced Python concepts.&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=rfscVS0vtbw"&gt;freeCodeCamp - Learn Python&lt;/a&gt;: Long video that provides a full introduction into all of the core concepts in Python.&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://jakevdp.github.io/PythonDataScienceHandbook/"&gt;Python Data Science Handbook&lt;/a&gt;: Free digital book that is a great resource for learning pandas, NumPy, Matplotlib, and Seaborn.&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://youtu.be/i_LwzRVP7bg"&gt;freeCodeCamp - Machine Learning for Everybody&lt;/a&gt;: Practical introduction to different machine learning algorithms for beginners.&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://www.udacity.com/course/intro-to-machine-learning--ud120"&gt;Udacity - Intro to Machine Learning&lt;/a&gt;: Free course that covers PCA and several other machine learning concepts.&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;hr /&gt; 
 &lt;h3&gt;3. Neural Networks&lt;/h3&gt; 
 &lt;p&gt;Neural networks are a fundamental part of many machine learning models, particularly in the realm of deep learning. To utilize them effectively, a comprehensive understanding of their design and mechanics is essential.&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Fundamentals&lt;/strong&gt;: This includes understanding the structure of a neural network, such as layers, weights, biases, and activation functions (sigmoid, tanh, ReLU, etc.)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Training and Optimization&lt;/strong&gt;: Familiarize yourself with backpropagation and different types of loss functions, like Mean Squared Error (MSE) and Cross-Entropy. Understand various optimization algorithms like Gradient Descent, Stochastic Gradient Descent, RMSprop, and Adam.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Overfitting&lt;/strong&gt;: Understand the concept of overfitting (where a model performs well on training data but poorly on unseen data) and learn various regularization techniques (dropout, L1/L2 regularization, early stopping, data augmentation) to prevent it.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Implement a Multilayer Perceptron (MLP)&lt;/strong&gt;: Build an MLP, also known as a fully connected network, using PyTorch.&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;üìö Resources:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=aircAruvnKk"&gt;3Blue1Brown - But what is a Neural Network?&lt;/a&gt;: This video gives an intuitive explanation of neural networks and their inner workings.&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=VyWAvY2CF9c"&gt;freeCodeCamp - Deep Learning Crash Course&lt;/a&gt;: This video efficiently introduces all the most important concepts in deep learning.&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://course.fast.ai/"&gt;Fast.ai - Practical Deep Learning&lt;/a&gt;: Free course designed for people with coding experience who want to learn about deep learning.&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://www.youtube.com/playlist?list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4"&gt;Patrick Loeber - PyTorch Tutorials&lt;/a&gt;: Series of videos for complete beginners to learn about PyTorch.&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;hr /&gt; 
 &lt;h3&gt;4. Natural Language Processing (NLP)&lt;/h3&gt; 
 &lt;p&gt;NLP is a fascinating branch of artificial intelligence that bridges the gap between human language and machine understanding. From simple text processing to understanding linguistic nuances, NLP plays a crucial role in many applications like translation, sentiment analysis, chatbots, and much more.&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Text Preprocessing&lt;/strong&gt;: Learn various text preprocessing steps like tokenization (splitting text into words or sentences), stemming (reducing words to their root form), lemmatization (similar to stemming but considers the context), stop word removal, etc.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Feature Extraction Techniques&lt;/strong&gt;: Become familiar with techniques to convert text data into a format that can be understood by machine learning algorithms. Key methods include Bag-of-words (BoW), Term Frequency-Inverse Document Frequency (TF-IDF), and n-grams.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Word Embeddings&lt;/strong&gt;: Word embeddings are a type of word representation that allows words with similar meanings to have similar representations. Key methods include Word2Vec, GloVe, and FastText.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Recurrent Neural Networks (RNNs)&lt;/strong&gt;: Understand the working of RNNs, a type of neural network designed to work with sequence data. Explore LSTMs and GRUs, two RNN variants that are capable of learning long-term dependencies.&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;üìö Resources:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://lena-voita.github.io/nlp_course/word_embeddings.html"&gt;Lena Voita - Word Embeddings&lt;/a&gt;: Beginner-friendly course about concepts related to word embeddings.&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://realpython.com/natural-language-processing-spacy-python/"&gt;RealPython - NLP with spaCy in Python&lt;/a&gt;: Exhaustive guide about the spaCy library for NLP tasks in Python.&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://www.kaggle.com/learn-guide/natural-language-processing"&gt;Kaggle - NLP Guide&lt;/a&gt;: A few notebooks and resources for a hands-on explanation of NLP in Python.&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://jalammar.github.io/illustrated-word2vec/"&gt;Jay Alammar - The Illustration Word2Vec&lt;/a&gt;: A good reference to understand the famous Word2Vec architecture.&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://jaketae.github.io/study/pytorch-rnn/"&gt;Jake Tae - PyTorch RNN from Scratch&lt;/a&gt;: Practical and simple implementation of RNN, LSTM, and GRU models in PyTorch.&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/"&gt;colah's blog - Understanding LSTM Networks&lt;/a&gt;: A more theoretical article about the LSTM network.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;üßë‚Äçüî¨ The LLM Scientist&lt;/h2&gt; 
&lt;p&gt;This section of the course focuses on learning how to build the best possible LLMs using the latest techniques.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/mlabonne/llm-course/main/img/roadmap_scientist.png" alt="" /&gt;&lt;/p&gt; 
&lt;h3&gt;1. The LLM architecture&lt;/h3&gt; 
&lt;p&gt;An in-depth knowledge of the Transformer architecture is not required, but it's important to understand the main steps of modern LLMs: converting text into numbers through tokenization, processing these tokens through layers including attention mechanisms, and finally generating new text through various sampling strategies.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Architectural Overview&lt;/strong&gt;: Understand the evolution from encoder-decoder Transformers to decoder-only architectures like GPT, which form the basis of modern LLMs. Focus on how these models process and generate text at a high level.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Tokenization&lt;/strong&gt;: Learn the principles of tokenization - how text is converted into numerical representations that LLMs can process. Explore different tokenization strategies and their impact on model performance and output quality.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Attention mechanisms&lt;/strong&gt;: Master the core concepts of attention mechanisms, particularly self-attention and its variants. Understand how these mechanisms enable LLMs to process long-range dependencies and maintain context throughout sequences.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Sampling techniques&lt;/strong&gt;: Explore various text generation approaches and their tradeoffs. Compare deterministic methods like greedy search and beam search with probabilistic approaches like temperature sampling and nucleus sampling.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;üìö &lt;strong&gt;References&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=wjZofJX0v4M"&gt;Visual intro to Transformers&lt;/a&gt; by 3Blue1Brown: Visual introduction to Transformers for complete beginners.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://bbycroft.net/llm"&gt;LLM Visualization&lt;/a&gt; by Brendan Bycroft: Interactive 3D visualization of LLM internals.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=kCc8FmEb1nY"&gt;nanoGPT&lt;/a&gt; by Andrej Karpathy: A 2h-long YouTube video to reimplement GPT from scratch (for programmers). He also made a video about &lt;a href="https://www.youtube.com/watch?v=zduSFxRajkE"&gt;tokenization&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://lilianweng.github.io/posts/2018-06-24-attention/"&gt;Attention? Attention!&lt;/a&gt; by Lilian Weng: Historical overview to introduce the need for attention mechanisms.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mlabonne.github.io/blog/posts/2023-06-07-Decoding_strategies.html"&gt;Decoding Strategies in LLMs&lt;/a&gt; by Maxime Labonne: Provide code and a visual introduction to the different decoding strategies to generate text.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;2. Pre-training models&lt;/h3&gt; 
&lt;p&gt;Pre-training is a computationally intensive and expensive process. While it's not the focus of this course, it's important to have a solid understanding of how models are pre-trained, especially in terms of data and parameters. Pre-training can also be performed by hobbyists at a small scale with &amp;lt;1B models.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Data preparation&lt;/strong&gt;: Pre-training requires massive datasets (e.g., &lt;a href="https://arxiv.org/abs/2307.09288"&gt;Llama 3.1&lt;/a&gt; was trained on 15 trillion tokens) that need careful curation, cleaning, deduplication, and tokenization. Modern pre-training pipelines implement sophisticated filtering to remove low-quality or problematic content.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Distributed training&lt;/strong&gt;: Combine different parallelization strategies: data parallel (batch distribution), pipeline parallel (layer distribution), and tensor parallel (operation splitting). These strategies require optimized network communication and memory management across GPU clusters.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Training optimization&lt;/strong&gt;: Use adaptive learning rates with warm-up, gradient clipping, and normalization to prevent explosions, mixed-precision training for memory efficiency, and modern optimizers (AdamW, Lion) with tuned hyperparameters.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Monitoring&lt;/strong&gt;: Track key metrics (loss, gradients, GPU stats) using dashboards, implement targeted logging for distributed training issues, and set up performance profiling to identify bottlenecks in computation and communication across devices.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;üìö &lt;strong&gt;References&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1"&gt;FineWeb&lt;/a&gt; by Penedo et al.: Article to recreate a large-scale dataset for LLM pretraining (15T), including FineWeb-Edu, a high-quality subset.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.together.ai/blog/redpajama-data-v2"&gt;RedPajama v2&lt;/a&gt; by Weber et al.: Another article and paper about a large-scale pre-training dataset with a lot of interesting quality filters.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/huggingface/nanotron"&gt;nanotron&lt;/a&gt; by Hugging Face: Minimalistic LLM training codebase used to make &lt;a href="https://github.com/huggingface/smollm"&gt;SmolLM2&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.andrew.cmu.edu/course/11-667/lectures/W10L2%20Scaling%20Up%20Parallel%20Training.pdf"&gt;Parallel training&lt;/a&gt; by Chenyan Xiong: Overview of optimization and parallelism techniques.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2407.20018"&gt;Distributed training&lt;/a&gt; by Duan et al.: A survey about efficient training of LLM on distributed architectures.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://allenai.org/olmo"&gt;OLMo 2&lt;/a&gt; by AI2: Open-source language model with model, data, training, and evaluation code.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.llm360.ai/"&gt;LLM360&lt;/a&gt; by LLM360: A framework for open-source LLMs with training and data preparation code, data, metrics, and models.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;3. Post-training datasets&lt;/h3&gt; 
&lt;p&gt;Post-training datasets have a precise structure with instructions and answers (supervised fine-tuning) or instructions and chosen/rejected answers (preference alignment). Conversational structures are a lot rarer than the raw text used for pre-training, which is why we often need to process seed data and refine it to improve the accuracy, diversity, and complexity of the samples. More information and examples are available in my repo &lt;a href="https://github.com/mlabonne/llm-datasets"&gt;üíæ LLM Datasets&lt;/a&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Storage &amp;amp; chat templates&lt;/strong&gt;: Because of the conversational structure, post-training datasets are stored in a specific format like ShareGPT or OpenAI/HF. Then, these formats are mapped to a chat template like ChatML or Alpaca to produce the final samples the model is trained on.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Synthetic data generation&lt;/strong&gt;: Create instruction-response pairs based on seed data using frontier models like GPT-4o. This approach allows for flexible and scalable dataset creation with high-quality answers. Key considerations include designing diverse seed tasks and effective system prompts.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Data enhancement&lt;/strong&gt;: Enhance existing samples using techniques like verified outputs (using unit tests or solvers), multiple answers with rejection sampling, &lt;a href="https://arxiv.org/abs/2406.00770"&gt;Auto-Evol&lt;/a&gt;, Chain-of-Thought, Branch-Solve-Merge, personas, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Quality filtering&lt;/strong&gt;: Traditional techniques involve rule-based filtering, removing duplicates or near-duplicates (with MinHash or embeddings), and n-gram decontamination. Reward models and judge LLMs complement this step with fine-grained and customizable quality control.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;üìö &lt;strong&gt;References&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/spaces/argilla/synthetic-data-generator"&gt;Synthetic Data Generator&lt;/a&gt; by Argilla: Beginner-friendly way of building datasets using natural language in a Hugging Face space.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mlabonne/llm-datasets"&gt;LLM Datasets&lt;/a&gt; by Maxime Labonne: Curated list of datasets and tools for post-training.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/NVIDIA/NeMo-Curator"&gt;NeMo-Curator&lt;/a&gt; by Nvidia: Dataset preparation and curation framework for pre- and post-training data.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://distilabel.argilla.io/dev/sections/pipeline_samples/"&gt;Distilabel&lt;/a&gt; by Argilla: Framework to generate synthetic data. It also includes interesting reproductions of papers like UltraFeedback.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/MinishLab/semhash"&gt;Semhash&lt;/a&gt; by MinishLab: Minimalistic library for near-deduplication and decontamination with a distilled embedding model.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/docs/transformers/main/en/chat_templating"&gt;Chat Template&lt;/a&gt; by Hugging Face: Hugging Face's documentation about chat templates.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;4. Supervised Fine-Tuning&lt;/h3&gt; 
&lt;p&gt;SFT turns base models into helpful assistants, capable of answering questions and following instructions. During this process, they learn how to structure answers and reactivate a subset of knowledge learned during pre-training. Instilling new knowledge is possible but superficial: it cannot be used to learn a completely new language. Always prioritize data quality over parameter optimization.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Training techniques&lt;/strong&gt;: Full fine-tuning updates all model parameters but requires significant compute. Parameter-efficient fine-tuning techniques like LoRA and QLoRA reduce memory requirements by training a small number of adapter parameters while keeping base weights frozen. QLoRA combines 4-bit quantization with LoRA to reduce VRAM usage. These techniques are all implemented in the most popular fine-tuning frameworks: &lt;a href="https://huggingface.co/docs/trl/en/index"&gt;TRL&lt;/a&gt;, &lt;a href="https://docs.unsloth.ai/"&gt;Unsloth&lt;/a&gt;, and &lt;a href="https://axolotl.ai/"&gt;Axolotl&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Training parameters&lt;/strong&gt;: Key parameters include learning rate with schedulers, batch size, gradient accumulation, number of epochs, optimizer (like 8-bit AdamW), weight decay for regularization, and warmup steps for training stability. LoRA also adds three parameters: rank (typically 16-128), alpha (1-2x rank), and target modules.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Distributed training&lt;/strong&gt;: Scale training across multiple GPUs using DeepSpeed or FSDP. DeepSpeed provides three ZeRO optimization stages with increasing levels of memory efficiency through state partitioning. Both methods support gradient checkpointing for memory efficiency.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Monitoring&lt;/strong&gt;: Track training metrics including loss curves, learning rate schedules, and gradient norms. Monitor for common issues like loss spikes, gradient explosions, or performance degradation.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;üìö &lt;strong&gt;References&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/blog/mlabonne/sft-llama3"&gt;Fine-tune Llama 3.1 Ultra-Efficiently with Unsloth&lt;/a&gt; by Maxime Labonne: Hands-on tutorial on how to fine-tune a Llama 3.1 model using Unsloth.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://axolotl-ai-cloud.github.io/axolotl/"&gt;Axolotl - Documentation&lt;/a&gt; by Wing Lian: Lots of interesting information related to distributed training and dataset formats.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://parlance-labs.com/education/"&gt;Mastering LLMs&lt;/a&gt; by Hamel Husain: Collection of educational resources about fine-tuning (but also RAG, evaluation, applications, and prompt engineering).&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://lightning.ai/pages/community/lora-insights/"&gt;LoRA insights&lt;/a&gt; by Sebastian Raschka: Practical insights about LoRA and how to select the best parameters.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;5. Preference Alignment&lt;/h3&gt; 
&lt;p&gt;Preference alignment is a second stage in the post-training pipeline, focused on aligning generated answers with human preferences. This stage was designed to tune the tone of LLMs and reduce toxicity and hallucinations. However, it has become increasingly important to also boost their performance and improve usefulness. Unlike SFT, there are many preference alignment algorithms. Here, we'll focus on the three most important ones: DPO, GRPO, and PPO.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Rejection sampling&lt;/strong&gt;: For each prompt, use the trained model to generate multiple responses, and score them to infer chosen/rejected answers. This creates on-policy data, where both responses come from the model being trained, improving alignment stability.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://arxiv.org/abs/2305.18290"&gt;Direct Preference Optimization&lt;/a&gt;&lt;/strong&gt; Directly optimizes the policy to maximize the likelihood of chosen responses over rejected ones. It doesn't require reward modeling, which makes it more computationally efficient than RL techniques but slightly worse in terms of quality. Great for creating chat models.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Reward model&lt;/strong&gt;: Train a reward model with human feedback to predict metrics like human preferences. It can leverage frameworks like &lt;a href="https://huggingface.co/docs/trl/en/index"&gt;TRL&lt;/a&gt;, &lt;a href="https://github.com/volcengine/verl"&gt;verl&lt;/a&gt;, and &lt;a href="https://github.com/OpenRLHF/OpenRLHF"&gt;OpenRLHF&lt;/a&gt; for scalable training.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Reinforcement Learning&lt;/strong&gt;: RL techniques like &lt;a href="https://arxiv.org/abs/2402.03300"&gt;GRPO&lt;/a&gt; and &lt;a href="https://arxiv.org/abs/1707.06347"&gt;PPO&lt;/a&gt; iteratively update a policy to maximize rewards while staying close to the initial behavior. They can use a reward model or reward functions to score responses. They tend to be computationally expensive and require careful tuning of hyperparameters, including learning rate, batch size, and clip range. Ideal for creating reasoning models.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;üìö &lt;strong&gt;References&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/blog/rlhf"&gt;Illustrating RLHF&lt;/a&gt; by Hugging Face: Introduction to RLHF with reward model training and fine-tuning with reinforcement learning.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives"&gt;LLM Training: RLHF and Its Alternatives&lt;/a&gt; by Sebastian Raschka: Overview of the RLHF process and alternatives like RLAIF.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/blog/pref-tuning"&gt;Preference Tuning LLMs&lt;/a&gt; by Hugging Face: Comparison of the DPO, IPO, and KTO algorithms to perform preference alignment.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mlabonne.github.io/blog/posts/Fine_tune_Mistral_7b_with_DPO.html"&gt;Fine-tune with DPO&lt;/a&gt; by Maxime Labonne: Tutorial to fine-tune a Mistral-7b model with DPO and reproduce &lt;a href="https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B"&gt;NeuralHermes-2.5&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/learn/llm-course/en/chapter12/5"&gt;Fine-tune with GRPO&lt;/a&gt; by Maxime Labonne: Practical exercise to fine-tune a small model with GRPO.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://wandb.ai/alexander-vishnevskiy/dpo/reports/TRL-Original-DPO--Vmlldzo1NjI4MTc4"&gt;DPO Wandb logs&lt;/a&gt; by Alexander Vishnevskiy: It shows you the main DPO metrics to track and the trends you should expect.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;6. Evaluation&lt;/h3&gt; 
&lt;p&gt;Reliably evaluating LLMs is a complex but essential task guiding data generation and training. It provides invaluable feedback about areas of improvement, which can be leveraged to modify the data mixture, quality, and training parameters. However, it's always good to remember Goodhart's law: "When a measure becomes a target, it ceases to be a good measure."&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Automated benchmarks&lt;/strong&gt;: Evaluate models on specific tasks using curated datasets and metrics, like MMLU. It works well for concrete tasks but struggles with abstract and creative capabilities. It is also prone to data contamination.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Human evaluation&lt;/strong&gt;: It involves humans prompting models and grading responses. Methods range from vibe checks to systematic annotations with specific guidelines and large-scale community voting (arena). It is more suited for subjective tasks and less reliable for factual accuracy.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Model-based evaluation&lt;/strong&gt;: Use judge and reward models to evaluate model outputs. It highly correlates with human preferences but suffers from bias toward their own outputs and inconsistent scoring.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Feedback signal&lt;/strong&gt;: Analyze error patterns to identify specific weaknesses, such as limitations in following complex instructions, lack of specific knowledge, or susceptibility to adversarial prompts. This can be improved with better data generation and training parameters.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;üìö &lt;strong&gt;References&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/huggingface/evaluation-guidebook"&gt;Evaluation guidebook&lt;/a&gt; by Cl√©mentine Fourrier: Practical insights and theoretical knowledge about LLM evaluation.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard"&gt;Open LLM Leaderboard&lt;/a&gt; by Hugging Face: Main leaderboard to compare LLMs in an open and reproducible way (automated benchmarks).&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/EleutherAI/lm-evaluation-harness"&gt;Language Model Evaluation Harness&lt;/a&gt; by EleutherAI: A popular framework for evaluating LLMs using automated benchmarks.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/huggingface/lighteval"&gt;Lighteval&lt;/a&gt; by Hugging Face: Alternative evaluation framework that also includes model-based evaluations.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://lmarena.ai/"&gt;Chatbot Arena&lt;/a&gt; by LMSYS: Elo rating of general-purpose LLMs, based on comparisons made by humans (human evaluation).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;7. Quantization&lt;/h3&gt; 
&lt;p&gt;Quantization is the process of converting the parameters and activations of a model using a lower precision. For example, weights stored using 16 bits can be converted into a 4-bit representation. This technique has become increasingly important to reduce the computational and memory costs associated with LLMs.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Base techniques&lt;/strong&gt;: Learn the different levels of precision (FP32, FP16, INT8, etc.) and how to perform na√Øve quantization with absmax and zero-point techniques.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;GGUF &amp;amp; llama.cpp&lt;/strong&gt;: Originally designed to run on CPUs, &lt;a href="https://github.com/ggerganov/llama.cpp"&gt;llama.cpp&lt;/a&gt; and the GGUF format have become the most popular tools to run LLMs on consumer-grade hardware. It supports storing special tokens, vocabulary, and metadata in a single file.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;GPTQ &amp;amp; AWQ&lt;/strong&gt;: Techniques like &lt;a href="https://arxiv.org/abs/2210.17323"&gt;GPTQ&lt;/a&gt;/&lt;a href="https://github.com/turboderp/exllamav2"&gt;EXL2&lt;/a&gt; and &lt;a href="https://arxiv.org/abs/2306.00978"&gt;AWQ&lt;/a&gt; introduce layer-by-layer calibration that retains performance at extremely low bitwidths. They reduce catastrophic outliers using dynamic scaling, selectively skipping or re-centering the heaviest parameters.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;SmoothQuant &amp;amp; ZeroQuant&lt;/strong&gt;: New quantization-friendly transformations (SmoothQuant) and compiler-based optimizations (ZeroQuant) help mitigate outliers before quantization. They also reduce hardware overhead by fusing certain ops and optimizing dataflow.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;üìö &lt;strong&gt;References&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://mlabonne.github.io/blog/posts/Introduction_to_Weight_Quantization.html"&gt;Introduction to quantization&lt;/a&gt; by Maxime Labonne: Overview of quantization, absmax and zero-point quantization, and LLM.int8() with code.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html"&gt;Quantize Llama models with llama.cpp&lt;/a&gt; by Maxime Labonne: Tutorial on how to quantize a Llama 2 model using llama.cpp and the GGUF format.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mlabonne.github.io/blog/posts/4_bit_Quantization_with_GPTQ.html"&gt;4-bit LLM Quantization with GPTQ&lt;/a&gt; by Maxime Labonne: Tutorial on how to quantize an LLM using the GPTQ algorithm with AutoGPTQ.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://medium.com/friendliai/understanding-activation-aware-weight-quantization-awq-boosting-inference-serving-efficiency-in-10bb0faf63a8"&gt;Understanding Activation-Aware Weight Quantization&lt;/a&gt; by FriendliAI: Overview of the AWQ technique and its benefits.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mit-han-lab/smoothquant/raw/main/examples/smoothquant_llama_demo.ipynb"&gt;SmoothQuant on Llama 2 7B&lt;/a&gt; by MIT HAN Lab: Tutorial on how to use SmoothQuant with a Llama 2 model in 8-bit precision.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.deepspeed.ai/tutorials/model-compression/"&gt;DeepSpeed Model Compression&lt;/a&gt; by DeepSpeed: Tutorial on how to use ZeroQuant and extreme compression (XTC) with DeepSpeed Compression.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;8. New Trends&lt;/h3&gt; 
&lt;p&gt;Here are notable topics that didn't fit into other categories. Some are established (model merging, multimodal) techniques, but others are more experimental (interpretability, test-time compute scaling) and the focus of numerous research papers.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Model merging&lt;/strong&gt;: Merging trained models has become a popular way of creating performant models without any fine-tuning. The popular &lt;a href="https://github.com/cg123/mergekit"&gt;mergekit&lt;/a&gt; library implements the most popular merging methods, like SLERP, &lt;a href="https://arxiv.org/abs/2311.03099"&gt;DARE&lt;/a&gt;, and &lt;a href="https://arxiv.org/abs/2311.03099"&gt;TIES&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multimodal models&lt;/strong&gt;: These models (like &lt;a href="https://openai.com/research/clip"&gt;CLIP&lt;/a&gt;, &lt;a href="https://stability.ai/stable-image"&gt;Stable Diffusion&lt;/a&gt;, or &lt;a href="https://llava-vl.github.io/"&gt;LLaVA&lt;/a&gt;) process multiple types of inputs (text, images, audio, etc.) with a unified embedding space, which unlocks powerful applications like text-to-image.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Interpretability&lt;/strong&gt;: Mechanistic interpretability techniques like Sparse Autoencoders (SAEs) have made remarkable progress to provide insights about the inner workings of LLMs. This has also been applied with techniques such as abliteration, which allow you to modify the behavior of models without training.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Test-time compute&lt;/strong&gt;: Reasoning models trained with RL techniques can be further improved by scaling the compute budget during test time. It can involve multiple calls, MCTS, or specialized models like a Process Reward Model (PRM). Iterative steps with precise scoring significantly improve performance for complex reasoning tasks.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;üìö &lt;strong&gt;References&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://mlabonne.github.io/blog/posts/2024-01-08_Merge_LLMs_with_mergekit.html"&gt;Merge LLMs with mergekit&lt;/a&gt; by Maxime Labonne: Tutorial about model merging using mergekit.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/merveenoyan/smol-vision"&gt;Smol Vision&lt;/a&gt; by Merve Noyan: Collection of notebooks and scripts dedicated to small multimodal models.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huyenchip.com/2023/10/10/multimodal.html"&gt;Large Multimodal Models&lt;/a&gt; by Chip Huyen: Overview of multimodal systems and the recent history of this field.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/blog/mlabonne/abliteration"&gt;Unsensor any LLM with abliteration&lt;/a&gt; by Maxime Labonne: Direct application of interpretability techniques to modify the style of a model.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://adamkarvonen.github.io/machine_learning/2024/06/11/sae-intuitions.html"&gt;Intuitive Explanation of SAEs&lt;/a&gt; by Adam Karvonen: Article about how SAEs work and why they make sense for interpretability.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute"&gt;Scaling test-time compute&lt;/a&gt; by Beeching et al.: Tutorial and experiments to outperform Llama 3.1 70B on MATH-500 with a 3B model.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üë∑ The LLM Engineer&lt;/h2&gt; 
&lt;p&gt;This section of the course focuses on learning how to build LLM-powered applications that can be used in production, with a focus on augmenting models and deploying them.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/mlabonne/llm-course/main/img/roadmap_engineer.png" alt="" /&gt;&lt;/p&gt; 
&lt;h3&gt;1. Running LLMs&lt;/h3&gt; 
&lt;p&gt;Running LLMs can be difficult due to high hardware requirements. Depending on your use case, you might want to simply consume a model through an API (like GPT-4) or run it locally. In any case, additional prompting and guidance techniques can improve and constrain the output for your applications.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;LLM APIs&lt;/strong&gt;: APIs are a convenient way to deploy LLMs. This space is divided between private LLMs (&lt;a href="https://platform.openai.com/"&gt;OpenAI&lt;/a&gt;, &lt;a href="https://cloud.google.com/vertex-ai/docs/generative-ai/learn/overview"&gt;Google&lt;/a&gt;, &lt;a href="https://docs.anthropic.com/claude/reference/getting-started-with-the-api"&gt;Anthropic&lt;/a&gt;, etc.) and open-source LLMs (&lt;a href="https://openrouter.ai/"&gt;OpenRouter&lt;/a&gt;, &lt;a href="https://huggingface.co/inference-api"&gt;Hugging Face&lt;/a&gt;, &lt;a href="https://www.together.ai/"&gt;Together AI&lt;/a&gt;, etc.).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Open-source LLMs&lt;/strong&gt;: The &lt;a href="https://huggingface.co/models"&gt;Hugging Face Hub&lt;/a&gt; is a great place to find LLMs. You can directly run some of them in &lt;a href="https://huggingface.co/spaces"&gt;Hugging Face Spaces&lt;/a&gt;, or download and run them locally in apps like &lt;a href="https://lmstudio.ai/"&gt;LM Studio&lt;/a&gt; or through the CLI with &lt;a href="https://github.com/ggerganov/llama.cpp"&gt;llama.cpp&lt;/a&gt; or &lt;a href="https://ollama.ai/"&gt;ollama&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Prompt engineering&lt;/strong&gt;: Common techniques include zero-shot prompting, few-shot prompting, chain of thought, and ReAct. They work better with bigger models, but can be adapted to smaller ones.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Structuring outputs&lt;/strong&gt;: Many tasks require a structured output, like a strict template or a JSON format. Libraries like &lt;a href="https://github.com/outlines-dev/outlines"&gt;Outlines&lt;/a&gt; can be used to guide the generation and respect a given structure. Some APIs also support structured output generation natively using JSON schemas.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;üìö &lt;strong&gt;References&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.kdnuggets.com/run-an-llm-locally-with-lm-studio"&gt;Run an LLM locally with LM Studio&lt;/a&gt; by Nisha Arya: Short guide on how to use LM Studio.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.promptingguide.ai/"&gt;Prompt engineering guide&lt;/a&gt; by DAIR.AI: Exhaustive list of prompt techniques with examples&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://dottxt-ai.github.io/outlines/latest/quickstart/"&gt;Outlines - Quickstart&lt;/a&gt;: List of guided generation techniques enabled by Outlines.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://lmql.ai/docs/language/overview.html"&gt;LMQL - Overview&lt;/a&gt;: Introduction to the LMQL language.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;2. Building a Vector Storage&lt;/h3&gt; 
&lt;p&gt;Creating a vector storage is the first step to building a Retrieval Augmented Generation (RAG) pipeline. Documents are loaded, split, and relevant chunks are used to produce vector representations (embeddings) that are stored for future use during inference.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Ingesting documents&lt;/strong&gt;: Document loaders are convenient wrappers that can handle many formats: PDF, JSON, HTML, Markdown, etc. They can also directly retrieve data from some databases and APIs (GitHub, Reddit, Google Drive, etc.).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Splitting documents&lt;/strong&gt;: Text splitters break down documents into smaller, semantically meaningful chunks. Instead of splitting text after &lt;em&gt;n&lt;/em&gt; characters, it's often better to split by header or recursively, with some additional metadata.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Embedding models&lt;/strong&gt;: Embedding models convert text into vector representations. Picking task-specific models significantly improves performance for semantic search and RAG.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Vector databases&lt;/strong&gt;: Vector databases (like &lt;a href="https://www.trychroma.com/"&gt;Chroma&lt;/a&gt;, &lt;a href="https://www.pinecone.io/"&gt;Pinecone&lt;/a&gt;, &lt;a href="https://milvus.io/"&gt;Milvus&lt;/a&gt;, &lt;a href="https://faiss.ai/"&gt;FAISS&lt;/a&gt;, &lt;a href="https://github.com/spotify/annoy"&gt;Annoy&lt;/a&gt;, etc.) are designed to store embedding vectors. They enable efficient retrieval of data that is 'most similar' to a query based on vector similarity.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;üìö &lt;strong&gt;References&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://python.langchain.com/docs/how_to/#text-splitters"&gt;LangChain - Text splitters&lt;/a&gt;: List of different text splitters implemented in LangChain.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.sbert.net/"&gt;Sentence Transformers library&lt;/a&gt;: Popular library for embedding models.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/spaces/mteb/leaderboard"&gt;MTEB Leaderboard&lt;/a&gt;: Leaderboard for embedding models.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.datacamp.com/blog/the-top-5-vector-databases"&gt;The Top 7 Vector Databases&lt;/a&gt; by Moez Ali: A comparison of the best and most popular vector databases.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;3. Retrieval Augmented Generation&lt;/h3&gt; 
&lt;p&gt;With RAG, LLMs retrieve contextual documents from a database to improve the accuracy of their answers. RAG is a popular way of augmenting the model's knowledge without any fine-tuning.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Orchestrators&lt;/strong&gt;: Orchestrators like &lt;a href="https://python.langchain.com/docs/get_started/introduction"&gt;LangChain&lt;/a&gt; and &lt;a href="https://docs.llamaindex.ai/en/stable/"&gt;LlamaIndex&lt;/a&gt; are popular frameworks to connect your LLMs with tools and databases. The Model Context Protocol (MCP) introduces a new standard to pass data and context to models across providers.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Retrievers&lt;/strong&gt;: Query rewriters and generative retrievers like CoRAG and HyDE enhance search by transforming user queries. Multi-vector and hybrid retrieval methods combine embeddings with keyword signals to improve recall and precision.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Memory&lt;/strong&gt;: To remember previous instructions and answers, LLMs and chatbots like ChatGPT add this history to their context window. This buffer can be improved with summarization (e.g., using a smaller LLM), a vector store + RAG, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Evaluation&lt;/strong&gt;: We need to evaluate both the document retrieval (context precision and recall) and generation stages (faithfulness and answer relevancy). It can be simplified with tools &lt;a href="https://github.com/explodinggradients/ragas/tree/main"&gt;Ragas&lt;/a&gt; and &lt;a href="https://github.com/confident-ai/deepeval"&gt;DeepEval&lt;/a&gt; (assessing quality).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;üìö &lt;strong&gt;References&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.llamaindex.ai/en/stable/getting_started/concepts.html"&gt;Llamaindex - High-level concepts&lt;/a&gt;: Main concepts to know when building RAG pipelines.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://modelcontextprotocol.io/introduction"&gt;Model Context Protocol&lt;/a&gt;: Introduction to MCP with motivate, architecture, and quick starts.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.pinecone.io/learn/series/langchain/langchain-retrieval-augmentation/"&gt;Pinecone - Retrieval Augmentation&lt;/a&gt;: Overview of the retrieval augmentation process.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://python.langchain.com/docs/tutorials/rag/"&gt;LangChain - Q&amp;amp;A with RAG&lt;/a&gt;: Step-by-step tutorial to build a typical RAG pipeline.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://python.langchain.com/docs/how_to/chatbots_memory/"&gt;LangChain - Memory types&lt;/a&gt;: List of different types of memories with relevant usage.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.ragas.io/en/stable/concepts/metrics/index.html"&gt;RAG pipeline - Metrics&lt;/a&gt;: Overview of the main metrics used to evaluate RAG pipelines.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;4. Advanced RAG&lt;/h3&gt; 
&lt;p&gt;Real-life applications can require complex pipelines, including SQL or graph databases, as well as automatically selecting relevant tools and APIs. These advanced techniques can improve a baseline solution and provide additional features.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Query construction&lt;/strong&gt;: Structured data stored in traditional databases requires a specific query language like SQL, Cypher, metadata, etc. We can directly translate the user instruction into a query to access the data with query construction.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Agents augment LLMs by automatically selecting the most relevant tools to provide an answer. These tools can be as simple as using Google or Wikipedia, or more complex like a Python interpreter or Jira.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Post-processing&lt;/strong&gt;: Final step that processes the inputs that are fed to the LLM. It enhances the relevance and diversity of documents retrieved with re-ranking, &lt;a href="https://github.com/Raudaschl/rag-fusion"&gt;RAG-fusion&lt;/a&gt;, and classification.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Program LLMs&lt;/strong&gt;: Frameworks like &lt;a href="https://github.com/stanfordnlp/dspy"&gt;DSPy&lt;/a&gt; allow you to optimize prompts and weights based on automated evaluations in a programmatic way.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;üìö &lt;strong&gt;References&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://blog.langchain.dev/query-construction/"&gt;LangChain - Query Construction&lt;/a&gt;: Blog post about different types of query construction.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://python.langchain.com/docs/tutorials/sql_qa/"&gt;LangChain - SQL&lt;/a&gt;: Tutorial on how to interact with SQL databases with LLMs, involving Text-to-SQL and an optional SQL agent.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.pinecone.io/learn/series/langchain/langchain-agents/"&gt;Pinecone - LLM agents&lt;/a&gt;: Introduction to agents and tools with different types.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://lilianweng.github.io/posts/2023-06-23-agent/"&gt;LLM Powered Autonomous Agents&lt;/a&gt; by Lilian Weng: A more theoretical article about LLM agents.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://blog.langchain.dev/applying-openai-rag/"&gt;LangChain - OpenAI's RAG&lt;/a&gt;: Overview of the RAG strategies employed by OpenAI, including post-processing.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://dspy-docs.vercel.app/docs/building-blocks/solving_your_task"&gt;DSPy in 8 Steps&lt;/a&gt;: General-purpose guide to DSPy introducing modules, signatures, and optimizers.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;5. Agents&lt;/h3&gt; 
&lt;p&gt;An LLM agent can autonomously perform tasks by taking actions based on reasoning about its environment, typically through the use of tools or functions to interact with external systems.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Agent fundamentals&lt;/strong&gt;: Agents operate using thoughts (internal reasoning to decide what to do next), action (executing tasks, often by interacting with external tools), and observation (analyzing feedback or results to refine the next step).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Agent frameworks&lt;/strong&gt;: Agent development can be streamlined using different frameworks like &lt;a href="https://www.langchain.com/langgraph"&gt;LangGraph&lt;/a&gt; (design and visualization of workflows), &lt;a href="https://docs.llamaindex.ai/en/stable/use_cases/agents/"&gt;LlamaIndex&lt;/a&gt; (data-augmented agents with RAG), or &lt;a href="https://github.com/huggingface/smolagents"&gt;smolagents&lt;/a&gt; (beginner-friendly, lightweight option).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-agents&lt;/strong&gt;: More experimental frameworks include collaboration between different agents, such as &lt;a href="https://docs.crewai.com/introduction"&gt;CrewAI&lt;/a&gt; (role-based team orchestration), &lt;a href="https://github.com/microsoft/autogen"&gt;AutoGen&lt;/a&gt; (conversation-driven multi-agent systems), and &lt;a href="https://github.com/openai/openai-agents-python"&gt;OpenAI Agents SDK&lt;/a&gt; (production-ready with strong OpenAI model integration).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;üìö &lt;strong&gt;References&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/learn/agents-course/unit0/introduction"&gt;Agents Course&lt;/a&gt;: Popular course about AI agents made by Hugging Face.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://langfuse.com/blog/2025-03-19-ai-agent-comparison"&gt;AI Agents Comparison&lt;/a&gt; by Jannik Maierh√∂fer: Comparison of features across different open-source AI agent frameworks.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://langchain-ai.github.io/langgraph/concepts/why-langgraph/"&gt;LangGraph&lt;/a&gt;: Overview of how to build AI agents with LangGraph.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.llamaindex.ai/en/stable/use_cases/agents/"&gt;LlamaIndex Agents&lt;/a&gt;: Uses cases and resources to build agents with LlamaIndex.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/docs/smolagents/index"&gt;smolagents&lt;/a&gt;: Documentation with a guided tour, how-to guides, and more conceptual articles.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;6. Inference optimization&lt;/h3&gt; 
&lt;p&gt;Text generation is a costly process that requires expensive hardware. In addition to quantization, various techniques have been proposed to maximize throughput and reduce inference costs.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Flash Attention&lt;/strong&gt;: Optimization of the attention mechanism to transform its complexity from quadratic to linear, speeding up both training and inference.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Key-value cache&lt;/strong&gt;: Understand the key-value cache and the improvements introduced in &lt;a href="https://arxiv.org/abs/1911.02150"&gt;Multi-Query Attention&lt;/a&gt; (MQA) and &lt;a href="https://arxiv.org/abs/2305.13245"&gt;Grouped-Query Attention&lt;/a&gt; (GQA).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Speculative decoding&lt;/strong&gt;: Use a small model to produce drafts that are then reviewed by a larger model to speed up text generation.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;üìö &lt;strong&gt;References&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one"&gt;GPU Inference&lt;/a&gt; by Hugging Face: Explain how to optimize inference on GPUs.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices"&gt;LLM Inference&lt;/a&gt; by Databricks: Best practices for how to optimize LLM inference in production.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/docs/transformers/main/en/llm_tutorial_optimization"&gt;Optimizing LLMs for Speed and Memory&lt;/a&gt; by Hugging Face: Explain three main techniques to optimize speed and memory, namely quantization, Flash Attention, and architectural innovations.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/blog/assisted-generation"&gt;Assisted Generation&lt;/a&gt; by Hugging Face: HF's version of speculative decoding, it's an interesting blog post about how it works with code to implement it.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;7. Deploying LLMs&lt;/h3&gt; 
&lt;p&gt;Deploying LLMs at scale is an engineering feat that can require multiple clusters of GPUs. In other scenarios, demos and local apps can be achieved with a much lower complexity.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Local deployment&lt;/strong&gt;: Privacy is an important advantage that open-source LLMs have over private ones. Local LLM servers (&lt;a href="https://lmstudio.ai/"&gt;LM Studio&lt;/a&gt;, &lt;a href="https://ollama.ai/"&gt;Ollama&lt;/a&gt;, &lt;a href="https://github.com/oobabooga/text-generation-webui"&gt;oobabooga&lt;/a&gt;, &lt;a href="https://github.com/LostRuins/koboldcpp"&gt;kobold.cpp&lt;/a&gt;, etc.) capitalize on this advantage to power local apps.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Demo deployment&lt;/strong&gt;: Frameworks like &lt;a href="https://www.gradio.app/"&gt;Gradio&lt;/a&gt; and &lt;a href="https://docs.streamlit.io/"&gt;Streamlit&lt;/a&gt; are helpful to prototype applications and share demos. You can also easily host them online, for example using &lt;a href="https://huggingface.co/spaces"&gt;Hugging Face Spaces&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Server deployment&lt;/strong&gt;: Deploy LLMs at scale requires cloud (see also &lt;a href="https://skypilot.readthedocs.io/en/latest/"&gt;SkyPilot&lt;/a&gt;) or on-prem infrastructure and often leverage optimized text generation frameworks like &lt;a href="https://github.com/huggingface/text-generation-inference"&gt;TGI&lt;/a&gt;, &lt;a href="https://github.com/vllm-project/vllm/tree/main"&gt;vLLM&lt;/a&gt;, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Edge deployment&lt;/strong&gt;: In constrained environments, high-performance frameworks like &lt;a href="https://github.com/mlc-ai/mlc-llm"&gt;MLC LLM&lt;/a&gt; and &lt;a href="https://github.com/wangzhaode/mnn-llm/raw/master/README_en.md"&gt;mnn-llm&lt;/a&gt; can deploy LLM in web browsers, Android, and iOS.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;üìö &lt;strong&gt;References&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.streamlit.io/knowledge-base/tutorials/build-conversational-apps"&gt;Streamlit - Build a basic LLM app&lt;/a&gt;: Tutorial to make a basic ChatGPT-like app using Streamlit.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/blog/sagemaker-huggingface-llm"&gt;HF LLM Inference Container&lt;/a&gt;: Deploy LLMs on Amazon SageMaker using Hugging Face's inference container.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.philschmid.de/"&gt;Philschmid&amp;nbsp;blog&lt;/a&gt; by Philipp Schmid: Collection of high-quality articles about LLM deployment using Amazon SageMaker.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://hamel.dev/notes/llm/inference/03_inference.html"&gt;Optimizing latence&lt;/a&gt; by Hamel Husain: Comparison of TGI, vLLM, CTranslate2, and mlc in terms of throughput and latency.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;8. Securing LLMs&lt;/h3&gt; 
&lt;p&gt;In addition to traditional security problems associated with software, LLMs have unique weaknesses due to the way they are trained and prompted.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Prompt hacking&lt;/strong&gt;: Different techniques related to prompt engineering, including prompt injection (additional instruction to hijack the model's answer), data/prompt leaking (retrieve its original data/prompt), and jailbreaking (craft prompts to bypass safety features).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Backdoors&lt;/strong&gt;: Attack vectors can target the training data itself, by poisoning the training data (e.g., with false information) or creating backdoors (secret triggers to change the model's behavior during inference).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Defensive measures&lt;/strong&gt;: The best way to protect your LLM applications is to test them against these vulnerabilities (e.g., using red teaming and checks like &lt;a href="https://github.com/leondz/garak/"&gt;garak&lt;/a&gt;) and observe them in production (with a framework like &lt;a href="https://github.com/langfuse/langfuse"&gt;langfuse&lt;/a&gt;).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;üìö &lt;strong&gt;References&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://owasp.org/www-project-top-10-for-large-language-model-applications/"&gt;OWASP LLM Top 10&lt;/a&gt; by HEGO Wiki: List of the 10 most critical vulnerabilities seen in LLM applications.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/jthack/PIPE"&gt;Prompt Injection Primer&lt;/a&gt; by Joseph Thacker: Short guide dedicated to prompt injection for engineers.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://llmsecurity.net/"&gt;LLM Security&lt;/a&gt; by &lt;a href="https://twitter.com/llm_sec"&gt;@llm_sec&lt;/a&gt;: Extensive list of resources related to LLM security.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/red-teaming"&gt;Red teaming LLMs&lt;/a&gt; by Microsoft: Guide on how to perform red teaming with LLMs.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;p&gt;This roadmap was inspired by the excellent &lt;a href="https://github.com/milanm/DevOps-Roadmap"&gt;DevOps Roadmap&lt;/a&gt; from Milan Milanoviƒá and Romano Roth.&lt;/p&gt; 
&lt;p&gt;Special thanks to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Thomas Thelen for motivating me to create a roadmap&lt;/li&gt; 
 &lt;li&gt;Andr√© Frade for his input and review of the first draft&lt;/li&gt; 
 &lt;li&gt;Dino Dunn for providing resources about LLM security&lt;/li&gt; 
 &lt;li&gt;Magdalena Kuhn for improving the "human evaluation" part&lt;/li&gt; 
 &lt;li&gt;Odoverdose for suggesting 3Blue1Brown's video about Transformers&lt;/li&gt; 
 &lt;li&gt;Everyone who contributed to the educational references in this course :)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;em&gt;Disclaimer: I am not affiliated with any sources listed here.&lt;/em&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#mlabonne/llm-course&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=mlabonne/llm-course&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>