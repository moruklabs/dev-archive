<rss version="2.0">
  <channel>
    <title>GitHub Python Daily Trending</title>
    <description>Daily Trending of Python in GitHub</description>
    <pubDate>Thu, 23 Oct 2025 01:36:29 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>DrewThomasson/ebook2audiobook</title>
      <link>https://github.com/DrewThomasson/ebook2audiobook</link>
      <description>&lt;p&gt;Generate audiobooks from e-books, voice cloning &amp; 1107+ languages!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;📚 ebook2audiobook&lt;/h1&gt; 
&lt;p&gt;CPU/GPU Converter from eBooks to audiobooks with chapters and metadata&lt;br /&gt; using XTTSv2, Bark, Vits, Fairseq, YourTTS, Tacotron and more. Supports voice cloning and +1110 languages!&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] &lt;strong&gt;This tool is intended for use with non-DRM, legally acquired eBooks only.&lt;/strong&gt; &lt;br /&gt; The authors are not responsible for any misuse of this software or any resulting legal consequences. &lt;br /&gt; Use this tool responsibly and in accordance with all applicable laws.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;a href="https://discord.gg/63Tv3F65k6"&gt;&lt;img src="https://dcbadge.limes.pink/api/server/https://discord.gg/63Tv3F65k6" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Thanks to support ebook2audiobook developers!&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://ko-fi.com/athomasson2"&gt;&lt;img src="https://img.shields.io/badge/Ko--fi-F16061?style=for-the-badge&amp;amp;logo=ko-fi&amp;amp;logoColor=white" alt="Ko-Fi" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Run locally&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#launching-gradio-web-interface"&gt;&lt;img src="https://img.shields.io/badge/Quick%20Start-blue?style=for-the-badge" alt="Quick Start" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/DrewThomasson/ebook2audiobook/actions/workflows/Docker-Build.yml"&gt;&lt;img src="https://github.com/DrewThomasson/ebook2audiobook/actions/workflows/Docker-Build.yml/badge.svg?sanitize=true" alt="Docker Build" /&gt;&lt;/a&gt; &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/releases/latest"&gt;&lt;img src="https://img.shields.io/badge/Download-Now-blue.svg?sanitize=true" alt="Download" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;a href="https://github.com/DrewThomasson/ebook2audiobook"&gt; &lt;img src="https://img.shields.io/badge/Platform-mac%20|%20linux%20|%20windows-lightgrey" alt="Platform" /&gt; &lt;/a&gt;
&lt;a href="https://hub.docker.com/r/athomasson2/ebook2audiobook"&gt; &lt;img alt="Docker Pull Count" src="https://img.shields.io/docker/pulls/athomasson2/ebook2audiobook.svg?sanitize=true" /&gt; &lt;/a&gt; 
&lt;h3&gt;Run Remotely&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/spaces/drewThomasson/ebook2audiobook"&gt;&lt;img src="https://img.shields.io/badge/Hugging%20Face-Spaces-yellow?style=flat&amp;amp;logo=huggingface" alt="Hugging Face" /&gt;&lt;/a&gt; &lt;a href="https://colab.research.google.com/github/DrewThomasson/ebook2audiobook/blob/main/Notebooks/colab_ebook2audiobook.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Free Google Colab" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Rihcus/ebook2audiobookXTTS/raw/main/Notebooks/kaggle-ebook2audiobook.ipynb"&gt;&lt;img src="https://img.shields.io/badge/Kaggle-035a7d?style=flat&amp;amp;logo=kaggle&amp;amp;logoColor=white" alt="Kaggle" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;GUI Interface&lt;/h4&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/assets/demo_web_gui.gif" alt="demo_web_gui" /&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click to see images of Web GUI&lt;/summary&gt; 
 &lt;img width="1728" alt="GUI Screen 1" src="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/assets/gui_1.png" /&gt; 
 &lt;img width="1728" alt="GUI Screen 2" src="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/assets/gui_2.png" /&gt; 
 &lt;img width="1728" alt="GUI Screen 3" src="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/assets/gui_3.png" /&gt; 
&lt;/details&gt; 
&lt;h2&gt;Demos&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;New Default Voice Demo&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/750035dc-e355-46f1-9286-05c1d9e88cea"&gt;https://github.com/user-attachments/assets/750035dc-e355-46f1-9286-05c1d9e88cea&lt;/a&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;More Demos&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;ASMR Voice&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/68eee9a1-6f71-4903-aacd-47397e47e422"&gt;https://github.com/user-attachments/assets/68eee9a1-6f71-4903-aacd-47397e47e422&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Rainy Day Voice&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/d25034d9-c77f-43a9-8f14-0d167172b080"&gt;https://github.com/user-attachments/assets/d25034d9-c77f-43a9-8f14-0d167172b080&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Scarlett Voice&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/b12009ee-ec0d-45ce-a1ef-b3a52b9f8693"&gt;https://github.com/user-attachments/assets/b12009ee-ec0d-45ce-a1ef-b3a52b9f8693&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;David Attenborough Voice&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/81c4baad-117e-4db5-ac86-efc2b7fea921"&gt;https://github.com/user-attachments/assets/81c4baad-117e-4db5-ac86-efc2b7fea921&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://github.com/DrewThomasson/VoxNovel/raw/dc5197dff97252fa44c391dc0596902d71278a88/readme_files/example_in_app.jpeg" alt="Example" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;README.md&lt;/h2&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#-ebook2audiobook"&gt;ebook2audiobook&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#features"&gt;Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#gui-interface"&gt;GUI Interface&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#demos"&gt;Demos&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#supported-languages"&gt;Supported Languages&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#hardware-requirements"&gt;Minimum Requirements&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#launching-gradio-web-interface"&gt;Usage&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#launching-gradio-web-interface"&gt;Run Locally&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#launching-gradio-web-interface"&gt;Launching Gradio Web Interface&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#basic--usage"&gt;Basic Headless Usage&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#example-of-custom-model-zip-upload"&gt;Headless Custom XTTS Model Usage&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#help-command-output"&gt;Help command output&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#run-remotely"&gt;Run Remotely&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#fine-tuned-tts-models"&gt;Fine Tuned TTS models&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#fine-tuned-tts-collection"&gt;Collection of Fine-Tuned TTS Models&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#fine-tune-your-own-xttsv2-model"&gt;Train XTTSv2&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#docker-gpu-options"&gt;Docker&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#docker-gpu-options"&gt;GPU options&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#running-the-pre-built-docker-container"&gt;Docker Run&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#building-the-docker-container"&gt;Docker Build&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#docker-compose"&gt;Docker Compose&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#docker-headless-guide"&gt;Docker headless guide&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#docker-container-file-locations"&gt;Docker container file locations&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#common-docker-issues"&gt;Common Docker issues&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#supported-ebook-formats"&gt;Supported eBook Formats&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#output-formats"&gt;Output Formats&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#updating-to-latest-version"&gt;Updating to Latest Version&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#reverting-to-older-versions"&gt;Revert to older Version&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#common-issues"&gt;Common Issues&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#special-thanks"&gt;Special Thanks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#table-of-contents"&gt;Table of Contents&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;📚 Splits eBook into chapters for organized audio.&lt;/li&gt; 
 &lt;li&gt;🎙️ High-quality text-to-speech with &lt;a href="https://huggingface.co/coqui/XTTS-v2"&gt;Coqui XTTSv2&lt;/a&gt; and &lt;a href="https://github.com/facebookresearch/fairseq/tree/main/examples/mms"&gt;Fairseq&lt;/a&gt; (and more).&lt;/li&gt; 
 &lt;li&gt;🗣️ Optional voice cloning with your own voice file.&lt;/li&gt; 
 &lt;li&gt;🌍 Supports +1110 languages (English by default). &lt;a href="https://dl.fbaipublicfiles.com/mms/tts/all-tts-languages.html"&gt;List of Supported languages&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;🖥️ Designed to run on 4GB RAM.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Supported Languages&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;&lt;strong&gt;Arabic (ar)&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;strong&gt;Chinese (zh)&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;strong&gt;English (en)&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;strong&gt;Spanish (es)&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;French (fr)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;German (de)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Italian (it)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Portuguese (pt)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Polish (pl)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Turkish (tr)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Russian (ru)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Dutch (nl)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Czech (cs)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Japanese (ja)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Hindi (hi)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Bengali (bn)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Hungarian (hu)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Korean (ko)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Vietnamese (vi)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Swedish (sv)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Persian (fa)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Yoruba (yo)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Swahili (sw)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Indonesian (id)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Slovak (sk)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Croatian (hr)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Tamil (ta)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Danish (da)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://dl.fbaipublicfiles.com/mms/tts/all-tts-languages.html"&gt;&lt;strong&gt;+1100 languages and dialects here&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Hardware Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;4gb RAM minimum, 8GB recommended&lt;/li&gt; 
 &lt;li&gt;Virtualization enabled if running on windows (Docker only)&lt;/li&gt; 
 &lt;li&gt;CPU (intel, AMD, ARM), GPU (Nvidia, AMD*, Intel*) (Recommended), MPS (Apple Silicon CPU) *available very soon&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] &lt;strong&gt;Before to post an install or bug issue search carefully to the opened and closed issues TAB&lt;br /&gt; to be sure your issue does not exist already.&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] &lt;strong&gt;Lacking of any standards structure like what is a chapter, paragraph, preface etc.&lt;br /&gt; you should first remove manually any text you don't want to be converted in audio.&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Installation Instructions&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Clone repo&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/DrewThomasson/ebook2audiobook.git
cd ebook2audiobook
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Launching Gradio Web Interface&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Run ebook2audiobook&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Linux/MacOS&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;./ebook2audiobook.sh  # Run launch script
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Mac Launcher&lt;/strong&gt;&lt;br /&gt; Double click &lt;code&gt;Mac Ebook2Audiobook Launcher.command&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Windows&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;ebook2audiobook.cmd  # Run launch script or double click on it
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Windows Launcher&lt;/strong&gt;&lt;br /&gt; Double click &lt;code&gt;ebook2audiobook.cmd&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Manual Python Install&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# (for experts only!)
REQUIRED_PROGRAMS=("calibre" "ffmpeg" "nodejs" "mecab" "espeak-ng" "rust" "sox")
REQUIRED_PYTHON_VERSION="3.12"
pip install -r requirements.txt  # Install Python Requirements
python app.py  # Run Ebook2Audiobook
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Open the Web App&lt;/strong&gt;: Click the URL provided in the terminal to access the web app and convert eBooks. &lt;code&gt;http://localhost:7860/&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;For Public Link&lt;/strong&gt;: &lt;code&gt;python app.py --share&lt;/code&gt; (all OS) &lt;code&gt;./ebook2audiobook.sh --share&lt;/code&gt; (Linux/MacOS) &lt;code&gt;ebook2audiobook.cmd --share&lt;/code&gt; (Windows)&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] &lt;strong&gt;If the script is stopped and run again, you need to refresh your gradio GUI interface&lt;br /&gt; to let the web page reconnect to the new connection socket.&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Basic Usage&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Linux/MacOS&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;./ebook2audiobook.sh --headless --ebook &amp;lt;path_to_ebook_file&amp;gt; \
    --voice [path_to_voice_file] --language [language_code]
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Windows&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;ebook2audiobook.cmd --headless --ebook &amp;lt;path_to_ebook_file&amp;gt;
    --voice [path_to_voice_file] --language [language_code]
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;[--ebook]&lt;/strong&gt;: Path to your eBook file&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;[--voice]&lt;/strong&gt;: Voice cloning file path (optional)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;[--language]&lt;/strong&gt;: Language code in ISO-639-3 (i.e.: ita for italian, eng for english, deu for german...).&lt;br /&gt; Default language is eng and --language is optional for default language set in ./lib/lang.py.&lt;br /&gt; The ISO-639-1 2 letters codes are also supported.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Example of Custom Model Zip Upload&lt;/h3&gt; 
&lt;p&gt;(must be a .zip file containing the mandatory model files. Example for XTTSv2: config.json, model.pth, vocab.json and ref.wav)&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Linux/MacOS&lt;/strong&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;./ebook2audiobook.sh --headless --ebook &amp;lt;ebook_file_path&amp;gt; \
    --voice &amp;lt;target_voice_file_path&amp;gt; --language &amp;lt;language&amp;gt; --custom_model &amp;lt;custom_model_path&amp;gt;
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;ebook2audiobook.cmd --headless --ebook &amp;lt;ebook_file_path&amp;gt; \
    --voice &amp;lt;target_voice_file_path&amp;gt; --language &amp;lt;language&amp;gt; --custom_model &amp;lt;custom_model_path&amp;gt;
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&amp;lt;custom_model_path&amp;gt;&lt;/strong&gt;: Path to &lt;code&gt;model_name.zip&lt;/code&gt; file, which must contain (according to the tts engine) all the mandatory files&lt;br /&gt; (see ./lib/models.py).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;For Detailed Guide with list of all Parameters to use&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Linux/MacOS&lt;/strong&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;./ebook2audiobook.sh --help
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;ebook2audiobook.cmd --help
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Or for all OS&lt;/strong&gt; &lt;code&gt;python app.py --help &lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a id="help-command-output"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;usage: app.py [-h] [--session SESSION] [--share] [--headless] [--ebook EBOOK]
              [--ebooks_dir EBOOKS_DIR] [--language LANGUAGE] [--voice VOICE]
              [--device {cpu,gpu,mps}]
              [--tts_engine {XTTSv2,BARK,VITS,FAIRSEQ,TACOTRON2,YOURTTS,xtts,bark,vits,fairseq,tacotron,yourtts}]
              [--custom_model CUSTOM_MODEL] [--fine_tuned FINE_TUNED]
              [--output_format OUTPUT_FORMAT] [--temperature TEMPERATURE]
              [--length_penalty LENGTH_PENALTY] [--num_beams NUM_BEAMS]
              [--repetition_penalty REPETITION_PENALTY] [--top_k TOP_K]
              [--top_p TOP_P] [--speed SPEED] [--enable_text_splitting]
              [--text_temp TEXT_TEMP] [--waveform_temp WAVEFORM_TEMP]
              [--output_dir OUTPUT_DIR] [--version]

Convert eBooks to Audiobooks using a Text-to-Speech model. You can either launch the Gradio interface or run the script in headless mode for direct conversion.

options:
  -h, --help            show this help message and exit
  --session SESSION     Session to resume the conversion in case of interruption, crash, 
                            or reuse of custom models and custom cloning voices.

**** The following options are for all modes:
  Optional

**** The following option are for gradio/gui mode only:
  Optional

  --share               Enable a public shareable Gradio link.

**** The following options are for --headless mode only:
  --headless            Run the script in headless mode
  --ebook EBOOK         Path to the ebook file for conversion. Cannot be used when --ebooks_dir is present.
  --ebooks_dir EBOOKS_DIR
                        Relative or absolute path of the directory containing the files to convert. 
                            Cannot be used when --ebook is present.
  --language LANGUAGE   Language of the e-book. Default language is set 
                            in ./lib/lang.py sed as default if not present. All compatible language codes are in ./lib/lang.py

optional parameters:
  --voice VOICE         (Optional) Path to the voice cloning file for TTS engine. 
                            Uses the default voice if not present.
  --device {cpu,gpu,mps}
                        (Optional) Pprocessor unit type for the conversion. 
                            Default is set in ./lib/conf.py if not present. Fall back to CPU if GPU not available.
  --tts_engine {XTTSv2,BARK,VITS,FAIRSEQ,TACOTRON2,YOURTTS,xtts,bark,vits,fairseq,tacotron,yourtts}
                        (Optional) Preferred TTS engine (available are: ['XTTSv2', 'BARK', 'VITS', 'FAIRSEQ', 'TACOTRON2', 'YOURTTS', 'xtts', 'bark', 'vits', 'fairseq', 'tacotron', 'yourtts'].
                            Default depends on the selected language. The tts engine should be compatible with the chosen language
  --custom_model CUSTOM_MODEL
                        (Optional) Path to the custom model zip file cntaining mandatory model files. 
                            Please refer to ./lib/models.py
  --fine_tuned FINE_TUNED
                        (Optional) Fine tuned model path. Default is builtin model.
  --output_format OUTPUT_FORMAT
                        (Optional) Output audio format. Default is set in ./lib/conf.py
  --temperature TEMPERATURE
                        (xtts only, optional) Temperature for the model. 
                            Default to config.json model. Higher temperatures lead to more creative outputs.
  --length_penalty LENGTH_PENALTY
                        (xtts only, optional) A length penalty applied to the autoregressive decoder. 
                            Default to config.json model. Not applied to custom models.
  --num_beams NUM_BEAMS
                        (xtts only, optional) Controls how many alternative sequences the model explores. Must be equal or greater than length penalty. 
                            Default to config.json model.
  --repetition_penalty REPETITION_PENALTY
                        (xtts only, optional) A penalty that prevents the autoregressive decoder from repeating itself. 
                            Default to config.json model.
  --top_k TOP_K         (xtts only, optional) Top-k sampling. 
                            Lower values mean more likely outputs and increased audio generation speed. 
                            Default to config.json model.
  --top_p TOP_P         (xtts only, optional) Top-p sampling. 
                            Lower values mean more likely outputs and increased audio generation speed. Default to config.json model.
  --speed SPEED         (xtts only, optional) Speed factor for the speech generation. 
                            Default to config.json model.
  --enable_text_splitting
                        (xtts only, optional) Enable TTS text splitting. This option is known to not be very efficient. 
                            Default to config.json model.
  --text_temp TEXT_TEMP
                        (bark only, optional) Text Temperature for the model. 
                            Default to 0.85. Higher temperatures lead to more creative outputs.
  --waveform_temp WAVEFORM_TEMP
                        (bark only, optional) Waveform Temperature for the model. 
                            Default to 0.5. Higher temperatures lead to more creative outputs.
  --output_dir OUTPUT_DIR
                        (Optional) Path to the output directory. Default is set in ./lib/conf.py
  --version             Show the version of the script and exit

Example usage:    
Windows:
    Gradio/GUI:
    ebook2audiobook.cmd
    Headless mode:
    ebook2audiobook.cmd --headless --ebook '/path/to/file'
Linux/Mac:
    Gradio/GUI:
    ./ebook2audiobook.sh
    Headless mode:
    ./ebook2audiobook.sh --headless --ebook '/path/to/file'
    
Tip: to add of silence (1.4 seconds) into your text just use "###" or "[pause]".

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;NOTE: in gradio/gui mode, to cancel a running conversion, just click on the [X] from the ebook upload component.&lt;/p&gt; 
&lt;p&gt;TIP: if it needs some more pauses, just add '###' or '[pause]' between the words you wish more pause. one [pause] equals to 1.4 seconds&lt;/p&gt; 
&lt;h4&gt;Docker GPU Options&lt;/h4&gt; 
&lt;p&gt;Available pre-build tags: &lt;code&gt;latest&lt;/code&gt; (CUDA 11.8)&lt;/p&gt; 
&lt;h4&gt;Edit: IF GPU isn't detected then you'll have to build the image -&amp;gt; &lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#building-the-docker-container"&gt;Building the Docker Container&lt;/a&gt;&lt;/h4&gt; 
&lt;h4&gt;Running the pre-built Docker Container&lt;/h4&gt; 
&lt;p&gt;-Run with CPU only&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;docker run --pull always --rm -p 7860:7860 athomasson2/ebook2audiobook
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;-Run with GPU Speedup (NVIDIA compatible only)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;docker run --pull always --rm --gpus all -p 7860:7860 athomasson2/ebook2audiobook
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command will start the Gradio interface on port 7860.(localhost:7860)&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;For more options add the parameter &lt;code&gt;--help&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Building the Docker Container&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;You can build the docker image with the command:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;docker build -t athomasson2/ebook2audiobook .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Avalible Docker Build Arguments&lt;/h4&gt; 
&lt;p&gt;&lt;code&gt;--build-arg TORCH_VERSION=cuda118&lt;/code&gt; Available tags: [cuda121, cuda118, cuda128, rocm, xpu, cpu]&lt;/p&gt; 
&lt;p&gt;All CUDA version numbers should work, Ex: CUDA 11.6-&amp;gt; cuda116&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;--build-arg SKIP_XTTS_TEST=true&lt;/code&gt; (Saves space by not baking XTTSv2 model into docker image)&lt;/p&gt; 
&lt;h2&gt;Docker container file locations&lt;/h2&gt; 
&lt;p&gt;All ebook2audiobooks will have the base dir of &lt;code&gt;/app/&lt;/code&gt; For example: &lt;code&gt;tmp&lt;/code&gt; = &lt;code&gt;/app/tmp&lt;/code&gt; &lt;code&gt;audiobooks&lt;/code&gt; = &lt;code&gt;/app/audiobooks&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;Docker headless guide&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Before you do run this you need to create a dir named "input-folder" in your current dir which will be linked, This is where you can put your input files for the docker image to see&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mkdir input-folder &amp;amp;&amp;amp; mkdir Audiobooks
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;In the command below swap out &lt;strong&gt;YOUR_INPUT_FILE.TXT&lt;/strong&gt; with the name of your input file&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run --pull always --rm \
    -v $(pwd)/input-folder:/app/input_folder \
    -v $(pwd)/audiobooks:/app/audiobooks \
    athomasson2/ebook2audiobook \
    --headless --ebook /input_folder/YOUR_EBOOK_FILE
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;The output Audiobooks will be found in the Audiobook folder which will also be located in your local dir you ran this docker command in&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;To get the help command for the other parameters this program has you can run this&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run --pull always --rm athomasson2/ebook2audiobook --help

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;That will output this &lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#help-command-output"&gt;Help command output&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Docker Compose&lt;/h3&gt; 
&lt;p&gt;This project uses Docker Compose to run locally. You can enable or disable GPU support by setting either &lt;code&gt;*gpu-enabled&lt;/code&gt; or &lt;code&gt;*gpu-disabled&lt;/code&gt; in &lt;code&gt;docker-compose.yml&lt;/code&gt;&lt;/p&gt; 
&lt;h4&gt;Steps to Run&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Clone the Repository&lt;/strong&gt; (if you haven't already): &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/DrewThomasson/ebook2audiobook.git
cd ebook2audiobook
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Set GPU Support (disabled by default)&lt;/strong&gt; To enable GPU support, modify &lt;code&gt;docker-compose.yml&lt;/code&gt; and change &lt;code&gt;*gpu-disabled&lt;/code&gt; to &lt;code&gt;*gpu-enabled&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Start the service:&lt;/strong&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Docker
docker-compose up -d # To update add --build

# Podman
podman compose -f podman-compose.yml up -d # To update add --build
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Access the service:&lt;/strong&gt; The service will be available at &lt;a href="http://localhost:7860"&gt;http://localhost:7860&lt;/a&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Common Docker Issues&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;My NVIDIA GPU isnt being detected?? -&amp;gt; &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/wiki/GPU-ISSUES"&gt;GPU ISSUES Wiki Page&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;python: can't open file '/home/user/app/app.py': [Errno 2] No such file or directory&lt;/code&gt; (Just remove all post arguments as I replaced the &lt;code&gt;CMD&lt;/code&gt; with &lt;code&gt;ENTRYPOINT&lt;/code&gt; in the &lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/Dockerfile"&gt;Dockerfile&lt;/a&gt;)&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Example: &lt;code&gt;docker run --pull always athomasson2/ebook2audiobook app.py --script_mode full_docker&lt;/code&gt; - &amp;gt; corrected - &amp;gt; &lt;code&gt;docker run --pull always athomasson2/ebook2audiobook&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Arguments can be easily added like this now &lt;code&gt;docker run --pull always athomasson2/ebook2audiobook --share&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Docker gets stuck downloading Fine-Tuned models. (This does not happen for every computer but some appear to run into this issue) Disabling the progress bar appears to fix the issue, as discussed &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/issues/191"&gt;here in #191&lt;/a&gt; Example of adding this fix in the &lt;code&gt;docker run&lt;/code&gt; command&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-Dockerfile"&gt;docker run --pull always --rm --gpus all -e HF_HUB_DISABLE_PROGRESS_BARS=1 -e HF_HUB_ENABLE_HF_TRANSFER=0 \
    -p 7860:7860 athomasson2/ebook2audiobook
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Fine Tuned TTS models&lt;/h2&gt; 
&lt;h4&gt;Fine Tune your own XTTSv2 model&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/spaces/drewThomasson/xtts-finetune-webui-gpu"&gt;&lt;img src="https://img.shields.io/badge/Hugging%20Face-Spaces-yellow?style=flat&amp;amp;logo=huggingface" alt="Hugging Face" /&gt;&lt;/a&gt; &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/raw/v25/Notebooks/finetune/xtts/kaggle-xtts-finetune-webui-gradio-gui.ipynb"&gt;&lt;img src="https://img.shields.io/badge/Kaggle-035a7d?style=flat&amp;amp;logo=kaggle&amp;amp;logoColor=white" alt="Kaggle" /&gt;&lt;/a&gt; &lt;a href="https://colab.research.google.com/github/DrewThomasson/ebook2audiobook/blob/v25/Notebooks/finetune/xtts/colab_xtts_finetune_webui.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;De-noise training data&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/spaces/drewThomasson/DeepFilterNet2_no_limit"&gt;&lt;img src="https://img.shields.io/badge/Hugging%20Face-Spaces-yellow?style=flat&amp;amp;logo=huggingface" alt="Hugging Face" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Rikorose/DeepFilterNet"&gt;&lt;img src="https://img.shields.io/badge/DeepFilterNet-181717?logo=github" alt="GitHub Repo" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Fine Tuned TTS Collection&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/drewThomasson/fineTunedTTSModels/tree/main"&gt;&lt;img src="https://img.shields.io/badge/Hugging%20Face-Models-yellow?style=flat&amp;amp;logo=huggingface" alt="Hugging Face" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;For an XTTSv2 custom model a ref audio clip of the voice reference is mandatory:&lt;/p&gt; 
&lt;h2&gt;Supported eBook Formats&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;.epub&lt;/code&gt;, &lt;code&gt;.pdf&lt;/code&gt;, &lt;code&gt;.mobi&lt;/code&gt;, &lt;code&gt;.txt&lt;/code&gt;, &lt;code&gt;.html&lt;/code&gt;, &lt;code&gt;.rtf&lt;/code&gt;, &lt;code&gt;.chm&lt;/code&gt;, &lt;code&gt;.lit&lt;/code&gt;, &lt;code&gt;.pdb&lt;/code&gt;, &lt;code&gt;.fb2&lt;/code&gt;, &lt;code&gt;.odt&lt;/code&gt;, &lt;code&gt;.cbr&lt;/code&gt;, &lt;code&gt;.cbz&lt;/code&gt;, &lt;code&gt;.prc&lt;/code&gt;, &lt;code&gt;.lrf&lt;/code&gt;, &lt;code&gt;.pml&lt;/code&gt;, &lt;code&gt;.snb&lt;/code&gt;, &lt;code&gt;.cbc&lt;/code&gt;, &lt;code&gt;.rb&lt;/code&gt;, &lt;code&gt;.tcr&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Best results&lt;/strong&gt;: &lt;code&gt;.epub&lt;/code&gt; or &lt;code&gt;.mobi&lt;/code&gt; for automatic chapter detection&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Output Formats&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Creates a &lt;code&gt;['m4b', 'm4a', 'mp4', 'webm', 'mov', 'mp3', 'flac', 'wav', 'ogg', 'aac']&lt;/code&gt; (set in ./lib/conf.py) file with metadata and chapters.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Updating to Latest Version&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git pull # Locally/Compose

docker pull athomasson2/ebook2audiobook:latest # For Pre-build docker images
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Reverting to older Versions&lt;/h2&gt; 
&lt;p&gt;Releases can be found -&amp;gt; &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/releases"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git checkout tags/VERSION_NUM # Locally/Compose -&amp;gt; Example: git checkout tags/v25.7.7

athomasson2/ebook2audiobook:VERSION_NUM # For Pre-build docker images -&amp;gt; Example: athomasson2/ebook2audiobook:v25.7.7
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Common Issues:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;My NVIDIA GPU isnt being detected?? -&amp;gt; &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/wiki/GPU-ISSUES"&gt;GPU ISSUES Wiki Page&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;CPU is slow (better on server smp CPU) while NVIDIA GPU can have almost real time conversion. &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/discussions/19#discussioncomment-10879846"&gt;Discussion about this&lt;/a&gt; For faster multilingual generation I would suggest my other &lt;a href="https://github.com/DrewThomasson/ebook2audiobookpiper-tts"&gt;project that uses piper-tts&lt;/a&gt; instead (It doesn't have zero-shot voice cloning though, and is Siri quality voices, but it is much faster on cpu).&lt;/li&gt; 
 &lt;li&gt;"I'm having dependency issues" - Just use the docker, its fully self contained and has a headless mode, add &lt;code&gt;--help&lt;/code&gt; parameter at the end of the docker run command for more information.&lt;/li&gt; 
 &lt;li&gt;"Im getting a truncated audio issue!" - PLEASE MAKE AN ISSUE OF THIS, we don't speak every language and need advise from users to fine tune the sentence splitting logic.😊&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;What we need help with! 🙌&lt;/h2&gt; 
&lt;h2&gt;&lt;a href="https://github.com/DrewThomasson/ebook2audiobook/issues/32"&gt;Full list of things can be found here&lt;/a&gt;&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Any help from people speaking any of the supported languages to help us improve the models&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Do you need to rent a GPU to boost service from us?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;A poll is open here &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/discussions/889"&gt;https://github.com/DrewThomasson/ebook2audiobook/discussions/889&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Special Thanks&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Coqui TTS&lt;/strong&gt;: &lt;a href="https://github.com/idiap/coqui-ai-TTS"&gt;Coqui TTS GitHub&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Calibre&lt;/strong&gt;: &lt;a href="https://calibre-ebook.com"&gt;Calibre Website&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;FFmpeg&lt;/strong&gt;: &lt;a href="https://ffmpeg.org"&gt;FFmpeg Website&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/DrewThomasson/ebook2audiobook/issues/8"&gt;@shakenbake15 for better chapter saving method&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>EbookFoundation/free-programming-books</title>
      <link>https://github.com/EbookFoundation/free-programming-books</link>
      <description>&lt;p&gt;📚 Freely available programming books&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;List of Free Learning Resources In Many Languages&lt;/h1&gt; 
&lt;div align="center" markdown="1"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/sindresorhus/awesome"&gt;&lt;img src="https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg?sanitize=true" alt="Awesome" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://creativecommons.org/licenses/by/4.0/"&gt;&lt;img src="https://img.shields.io/github/license/EbookFoundation/free-programming-books" alt="License: CC BY 4.0" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/pulls?q=is%3Apr+is%3Amerged+created%3A2025-10-01..2025-10-31"&gt;&lt;img src="https://img.shields.io/github/hacktoberfest/2025/EbookFoundation/free-programming-books?label=Hacktoberfest+2025" alt="Hacktoberfest 2025 stats" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;Search the list at &lt;a href="https://ebookfoundation.github.io/free-programming-books-search/"&gt;https://ebookfoundation.github.io/free-programming-books-search/&lt;/a&gt; &lt;a href="https://ebookfoundation.github.io/free-programming-books-search/"&gt;&lt;img src="https://img.shields.io/website?style=flat&amp;amp;logo=www&amp;amp;logoColor=whitesmoke&amp;amp;label=Dynamic%20search%20site&amp;amp;down_color=red&amp;amp;down_message=down&amp;amp;up_color=green&amp;amp;up_message=up&amp;amp;url=https%3A%2F%2Febookfoundation.github.io%2Ffree-programming-books-search%2F" alt="https://ebookfoundation.github.io/free-programming-books-search/" /&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;This page is available as an easy-to-read website. Access it by clicking on &lt;a href="https://ebookfoundation.github.io/free-programming-books/"&gt;&lt;img src="https://img.shields.io/website?style=flat&amp;amp;logo=www&amp;amp;logoColor=whitesmoke&amp;amp;label=Static%20site&amp;amp;down_color=red&amp;amp;down_message=down&amp;amp;up_color=green&amp;amp;up_message=up&amp;amp;url=https%3A%2F%2Febookfoundation.github.io%2Ffree-programming-books%2F" alt="https://ebookfoundation.github.io/free-programming-books/" /&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;form action="https://ebookfoundation.github.io/free-programming-books-search"&gt; 
  &lt;input type="text" id="fpbSearch" name="search" required placeholder="Search Book or Author" /&gt; 
  &lt;label for="submit"&gt; &lt;/label&gt; 
  &lt;input type="submit" id="submit" name="submit" value="Search" /&gt; 
 &lt;/form&gt; 
&lt;/div&gt; 
&lt;h2&gt;Intro&lt;/h2&gt; 
&lt;p&gt;This list was originally a clone of &lt;a href="https://web.archive.org/web/20140606191453/http://stackoverflow.com/questions/194812/list-of-freely-available-programming-books/392926"&gt;StackOverflow - List of Freely Available Programming Books&lt;/a&gt; with contributions from Karan Bhangui and George Stocker.&lt;/p&gt; 
&lt;p&gt;The list was moved to GitHub by Victor Felder for collaborative updating and maintenance. It has grown to become one of &lt;a href="https://octoverse.github.com/"&gt;GitHub's most popular repositories&lt;/a&gt;.&lt;/p&gt; 
&lt;div align="center" markdown="1"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/EbookFoundation/free-programming-books/network"&gt;&lt;img src="https://img.shields.io/github/forks/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=whitesmoke&amp;amp;label=Forks" alt="GitHub repo forks" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=whitesmoke&amp;amp;label=Stars" alt="GitHub repo stars" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/graphs/contributors"&gt;&lt;img src="https://img.shields.io/github/contributors-anon/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=whitesmoke&amp;amp;label=Contributors" alt="GitHub repo contributors" /&gt;&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/sponsors/EbookFoundation"&gt;&lt;img src="https://img.shields.io/github/sponsors/EbookFoundation?style=flat&amp;amp;logo=github&amp;amp;logoColor=whitesmoke&amp;amp;label=Sponsors" alt="GitHub org sponsors" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/watchers"&gt;&lt;img src="https://img.shields.io/github/watchers/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=whitesmoke&amp;amp;label=Watchers" alt="GitHub repo watchers" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/archive/refs/heads/main.zip"&gt;&lt;img src="https://img.shields.io/github/repo-size/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=whitesmoke&amp;amp;label=Repo%20Size" alt="GitHub repo size" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;The &lt;a href="https://ebookfoundation.org"&gt;Free Ebook Foundation&lt;/a&gt; now administers the repo, a not-for-profit organization devoted to promoting the creation, distribution, archiving, and sustainability of free ebooks. &lt;a href="https://ebookfoundation.org/contributions.html"&gt;Donations&lt;/a&gt; to the Free Ebook Foundation are tax-deductible in the US.&lt;/p&gt; 
&lt;h2&gt;How To Contribute&lt;/h2&gt; 
&lt;p&gt;Please read &lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/CONTRIBUTING.md"&gt;CONTRIBUTING&lt;/a&gt;. If you're new to GitHub, &lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/HOWTO.md"&gt;welcome&lt;/a&gt;! Remember to abide by our adapted from &lt;img src="https://img.shields.io/badge/Contributor%20Covenant-1.3-4baaaa.svg?sanitize=true" alt="Contributor Covenant 1.3" /&gt; &lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/CODE_OF_CONDUCT.md"&gt;Code of Conduct&lt;/a&gt; too (&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/#translations"&gt;translations&lt;/a&gt; also available).&lt;/p&gt; 
&lt;p&gt;Click on these badges to see how you might be able to help:&lt;/p&gt; 
&lt;div align="center" markdown="1"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/EbookFoundation/free-programming-books/issues"&gt;&lt;img src="https://img.shields.io/github/issues/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=red&amp;amp;label=Issues" alt="GitHub repo Issues" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22"&gt;&lt;img src="https://img.shields.io/github/issues/EbookFoundation/free-programming-books/good%20first%20issue?style=flat&amp;amp;logo=github&amp;amp;logoColor=green&amp;amp;label=Good%20First%20issues" alt="GitHub repo Good Issues for newbies" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/issues?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22"&gt;&lt;img src="https://img.shields.io/github/issues/EbookFoundation/free-programming-books/help%20wanted?style=flat&amp;amp;logo=github&amp;amp;logoColor=b545d1&amp;amp;label=%22Help%20Wanted%22%20issues" alt="GitHub Help Wanted issues" /&gt;&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/EbookFoundation/free-programming-books/pulls"&gt;&lt;img src="https://img.shields.io/github/issues-pr/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=orange&amp;amp;label=PRs" alt="GitHub repo PRs" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/pulls?q=is%3Apr+is%3Amerged"&gt;&lt;img src="https://img.shields.io/github/issues-search/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=green&amp;amp;label=Merged%20PRs&amp;amp;query=is%3Amerged" alt="GitHub repo Merged PRs" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/pulls?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22"&gt;&lt;img src="https://img.shields.io/github/issues-pr/EbookFoundation/free-programming-books/help%20wanted?style=flat&amp;amp;logo=github&amp;amp;logoColor=b545d1&amp;amp;label=%22Help%20Wanted%22%20PRs" alt="GitHub Help Wanted PRs" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;How To Share&lt;/h2&gt; 
&lt;div align="left" markdown="1"&gt; 
 &lt;a href="https://www.facebook.com/share.php?u=https%3A%2F%2Fgithub.com%2FEbookFoundation%2Ffree-programming-books&amp;amp;p%5Bimages%5D%5B0%5D=&amp;amp;p%5Btitle%5D=Free%20Programming%20Books&amp;amp;p%5Bsummary%5D="&gt;Share on Facebook&lt;/a&gt;
 &lt;br /&gt; 
 &lt;a href="http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://github.com/EbookFoundation/free-programming-books&amp;amp;title=Free%20Programming%20Books&amp;amp;summary=&amp;amp;source="&gt;Share on LinkedIn&lt;/a&gt;
 &lt;br /&gt; 
 &lt;a href="https://toot.kytta.dev/?text=https://github.com/EbookFoundation/free-programming-books"&gt;Share on Mastodon/Fediverse&lt;/a&gt;
 &lt;br /&gt; 
 &lt;a href="https://t.me/share/url?url=https://github.com/EbookFoundation/free-programming-books"&gt;Share on Telegram&lt;/a&gt;
 &lt;br /&gt; 
 &lt;a href="https://twitter.com/intent/tweet?text=https://github.com/EbookFoundation/free-programming-books%0AFree%20Programming%20Books"&gt;Share on 𝕏 (Twitter)&lt;/a&gt;
 &lt;br /&gt; 
&lt;/div&gt; 
&lt;h2&gt;Resources&lt;/h2&gt; 
&lt;p&gt;This project lists books and other resources grouped by genres:&lt;/p&gt; 
&lt;h3&gt;Books&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-langs.md"&gt;English, By Programming Language&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-subjects.md"&gt;English, By Subject&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;Other Languages&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ar.md"&gt;Arabic / al arabiya / العربية&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-hy.md"&gt;Armenian / Հայերեն&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-az.md"&gt;Azerbaijani / Азәрбајҹан дили / آذربايجانجا ديلي&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-bn.md"&gt;Bengali / বাংলা&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-bg.md"&gt;Bulgarian / български&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-my.md"&gt;Burmese / မြန်မာဘာသာ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-zh.md"&gt;Chinese / 中文&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-cs.md"&gt;Czech / čeština / český jazyk&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ca.md"&gt;Catalan / catalan / català&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-da.md"&gt;Danish / dansk&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-nl.md"&gt;Dutch / Nederlands&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-et.md"&gt;Estonian / eesti keel&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-fi.md"&gt;Finnish / suomi / suomen kieli&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-fr.md"&gt;French / français&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-de.md"&gt;German / Deutsch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-el.md"&gt;Greek / ελληνικά&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-he.md"&gt;Hebrew / עברית&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-hi.md"&gt;Hindi / हिन्दी&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-hu.md"&gt;Hungarian / magyar / magyar nyelv&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-id.md"&gt;Indonesian / Bahasa Indonesia&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-it.md"&gt;Italian / italiano&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ja.md"&gt;Japanese / 日本語&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ko.md"&gt;Korean / 한국어&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-lv.md"&gt;Latvian / Latviešu&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ml.md"&gt;Malayalam / മലയാളം&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-no.md"&gt;Norwegian / Norsk&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-fa_IR.md"&gt;Persian / Farsi (Iran) / فارسى&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-pl.md"&gt;Polish / polski / język polski / polszczyzna&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-pt_BR.md"&gt;Portuguese (Brazil)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-pt_PT.md"&gt;Portuguese (Portugal)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ro.md"&gt;Romanian (Romania) / limba română / român&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ru.md"&gt;Russian / Русский язык&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-sr.md"&gt;Serbian / српски језик / srpski jezik&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-sk.md"&gt;Slovak / slovenčina&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-es.md"&gt;Spanish / español / castellano&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-sv.md"&gt;Swedish / Svenska&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ta.md"&gt;Tamil / தமிழ்&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-te.md"&gt;Telugu / తెలుగు&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-th.md"&gt;Thai / ไทย&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-tr.md"&gt;Turkish / Türkçe&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-uk.md"&gt;Ukrainian / Українська&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ur.md"&gt;Urdu / اردو&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-vi.md"&gt;Vietnamese / Tiếng Việt&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Cheat Sheets&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-cheatsheets.md"&gt;All Languages&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Free Online Courses&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ar.md"&gt;Arabic / al arabiya / العربية&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-bn.md"&gt;Bengali / বাংলা&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-bg.md"&gt;Bulgarian / български&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-my.md"&gt;Burmese / မြန်မာဘာသာ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-zh.md"&gt;Chinese / 中文&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-en.md"&gt;English&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-fi.md"&gt;Finnish / suomi / suomen kieli&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-fr.md"&gt;French / français&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-de.md"&gt;German / Deutsch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-el.md"&gt;Greek / ελληνικά&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-he.md"&gt;Hebrew / עברית&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-hi.md"&gt;Hindi / हिंदी&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-id.md"&gt;Indonesian / Bahasa Indonesia&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-it.md"&gt;Italian / italiano&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ja.md"&gt;Japanese / 日本語&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-kn.md"&gt;Kannada / ಕನ್ನಡ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-kk.md"&gt;Kazakh / қазақша&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-km.md"&gt;Khmer / ភាសាខ្មែរ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ko.md"&gt;Korean / 한국어&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ml.md"&gt;Malayalam / മലയാളം&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-mr.md"&gt;Marathi / मराठी&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ne.md"&gt;Nepali / नेपाली&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-no.md"&gt;Norwegian / Norsk&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-fa_IR.md"&gt;Persian / Farsi (Iran) / فارسى&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-pl.md"&gt;Polish / polski / język polski / polszczyzna&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-pt_BR.md"&gt;Portuguese (Brazil)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-pt_PT.md"&gt;Portuguese (Portugal)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-pa.md"&gt;Punjabi / ਪੰਜਾਬੀ / پنجابی&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ro.md"&gt;Romanian (Romania) / limba română / român&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ru.md"&gt;Russian / Русский язык&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-si.md"&gt;Sinhala / සිංහල&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-es.md"&gt;Spanish / español / castellano&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-sv.md"&gt;Swedish / svenska&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ta.md"&gt;Tamil / தமிழ்&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-te.md"&gt;Telugu / తెలుగు&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-th.md"&gt;Thai / ภาษาไทย&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-tr.md"&gt;Turkish / Türkçe&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-uk.md"&gt;Ukrainian / Українська&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ur.md"&gt;Urdu / اردو&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-vi.md"&gt;Vietnamese / Tiếng Việt&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Interactive Programming Resources&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-interactive-tutorials-zh.md"&gt;Chinese / 中文&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-interactive-tutorials-en.md"&gt;English&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-interactive-tutorials-de.md"&gt;German / Deutsch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-interactive-tutorials-ja.md"&gt;Japanese / 日本語&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-interactive-tutorials-ru.md"&gt;Russian / Русский язык&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Problem Sets and Competitive Programming&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/problem-sets-competitive-programming.md"&gt;Problem Sets&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Podcast - Screencast&lt;/h3&gt; 
&lt;p&gt;Free Podcasts and Screencasts:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-ar.md"&gt;Arabic / al Arabiya / العربية&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-my.md"&gt;Burmese / မြန်မာဘာသာ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-zh.md"&gt;Chinese / 中文&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-cs.md"&gt;Czech / čeština / český jazyk&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-nl.md"&gt;Dutch / Nederlands&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-en.md"&gt;English&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-fi.md"&gt;Finnish / Suomi&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-fr.md"&gt;French / français&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-de.md"&gt;German / Deutsch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-he.md"&gt;Hebrew / עברית&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-id.md"&gt;Indonesian / Bahasa Indonesia&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-fa_IR.md"&gt;Persian / Farsi (Iran) / فارسى&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-pl.md"&gt;Polish / polski / język polski / polszczyzna&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-pt_BR.md"&gt;Portuguese (Brazil)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-pt_PT.md"&gt;Portuguese (Portugal)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-ru.md"&gt;Russian / Русский язык&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-si.md"&gt;Sinhala / සිංහල&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-es.md"&gt;Spanish / español / castellano&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-sv.md"&gt;Swedish / Svenska&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-tr.md"&gt;Turkish / Türkçe&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-uk.md"&gt;Ukrainian / Українська&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Programming Playgrounds&lt;/h3&gt; 
&lt;p&gt;Write, compile, and run your code within a browser. Try it out!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-playgrounds-zh.md"&gt;Chinese / 中文&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-playgrounds.md"&gt;English&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-playgrounds-de.md"&gt;German / Deutsch&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Translations&lt;/h2&gt; 
&lt;p&gt;Volunteers have translated many of our Contributing, How-to, and Code of Conduct documents into languages covered by our lists.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;English 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/CODE_OF_CONDUCT.md"&gt;Code of Conduct&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/CONTRIBUTING.md"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/HOWTO.md"&gt;How-to&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;... &lt;em&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/README.md#translations"&gt;More languages&lt;/a&gt;&lt;/em&gt; ...&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You might notice that there are &lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/README.md#translations"&gt;some missing translations here&lt;/a&gt; - perhaps you would like to help out by &lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/CONTRIBUTING.md#help-out-by-contributing-a-translation"&gt;contributing a translation&lt;/a&gt;?&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Each file included in this repository is licensed under the &lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/LICENSE"&gt;CC BY License&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>harvard-edge/cs249r_book</title>
      <link>https://github.com/harvard-edge/cs249r_book</link>
      <description>&lt;p&gt;Introduction to Machine Learning Systems&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Machine Learning Systems&lt;/h1&gt; 
&lt;p&gt;&lt;em&gt;Principles and Practices of Engineering Artificially Intelligent Systems&lt;/em&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p align="center"&gt; 
  &lt;!-- Row 1: Project Health --&gt; &lt;/p&gt;
 &lt;p&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/actions/workflows/validate-dev.yml"&gt;&lt;img src="https://img.shields.io/github/actions/workflow/status/harvard-edge/cs249r_book/validate-dev.yml?branch=dev&amp;amp;label=Build&amp;amp;logo=githubactions&amp;amp;cacheSeconds=300" alt="Build" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/github/last-commit/harvard-edge/cs249r_book/dev?label=Last%20Commit&amp;amp;logo=git&amp;amp;cacheSeconds=300" alt="Last Commit" /&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;/p&gt; 
 &lt;p align="center"&gt; 
  &lt;!-- Row 2: Access &amp; Ecosystem --&gt; &lt;/p&gt;
 &lt;p&gt;&lt;a href="https://mlsysbook.ai"&gt;&lt;img src="https://img.shields.io/website?url=https%3A%2F%2Fmlsysbook.ai&amp;amp;label=Website&amp;amp;logo=readthedocs" alt="Website" /&gt;&lt;/a&gt; &lt;a href="https://mlsysbook.org"&gt;&lt;img src="https://img.shields.io/website?url=https%3A%2F%2Fmlsysbook.org&amp;amp;label=Ecosystem&amp;amp;logo=internet-explorer" alt="Ecosystem" /&gt;&lt;/a&gt; &lt;a href="https://mlsysbook.org"&gt;&lt;img src="https://img.shields.io/badge/Cite-IEEE%20CODES%2B%20ISSS%202024-blue?logo=academia" alt="Citation" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;/p&gt; 
 &lt;p align="center"&gt; 
  &lt;!-- Row 3: Support --&gt; &lt;/p&gt;
 &lt;p&gt;&lt;a href="https://opencollective.com/mlsysbook"&gt;&lt;img src="https://img.shields.io/badge/Fund%20Us-Open%20Collective-blue.svg?logo=open-collective" alt="Funding" /&gt;&lt;/a&gt; &lt;a href="https://github.com/harvard-edge/cs249r_book/raw/dev/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/License-CC--BY--NC--SA%204.0-blue.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://www.netlify.com"&gt;&lt;img src="https://img.shields.io/badge/Powered%20by-Netlify-00C7B7?logo=netlify&amp;amp;logoColor=white" alt="Powered by Netlify" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;/p&gt; 
 &lt;p align="center"&gt; 
  &lt;!-- Reader Navigation --&gt; &lt;/p&gt;
 &lt;p&gt;&lt;strong&gt;&lt;a href="https://mlsysbook.ai"&gt;📖 Read Online&lt;/a&gt;&lt;/strong&gt; • &lt;strong&gt;&lt;a href="https://mlsysbook.ai/pdf"&gt;💾 Download PDF&lt;/a&gt;&lt;/strong&gt; • &lt;strong&gt;&lt;a href="https://mlsysbook.ai/epub"&gt;💾 Download ePub&lt;/a&gt;&lt;/strong&gt; • &lt;strong&gt;&lt;a href="https://mlsysbook.org"&gt;🌐 Explore Ecosystem&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;/p&gt; 
 &lt;p&gt;📚 &lt;strong&gt;Hardcopy edition coming 2026 via MIT Press!&lt;/strong&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;About This Book&lt;/h2&gt; 
&lt;p&gt;The &lt;strong&gt;open-source textbook&lt;/strong&gt; that teaches you to build real-world AI systems — from edge devices to cloud deployment. Originally developed as Harvard University's CS249r course by &lt;a href="https://github.com/profvjreddi/homepage"&gt;Prof. Vijay Janapa Reddi&lt;/a&gt;, now used by universities and students worldwide.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Our mission:&lt;/strong&gt; Expand access to AI systems education worldwide — empowering learners, one chapter and one lab at a time.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Why This Book Exists&lt;/h3&gt; 
&lt;p&gt;&lt;em&gt;"This grew out of a concern that while students could train AI models, few understood how to build the systems that actually make them work. As AI becomes more capable and autonomous, the critical bottleneck won't be the algorithms - it will be the engineers who can build efficient, scalable, and sustainable systems that safely harness that intelligence."&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;— Vijay Janapa Reddi&lt;/strong&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;📚 What You'll Learn&lt;/h2&gt; 
&lt;p&gt;Go beyond training models — master the &lt;strong&gt;full stack&lt;/strong&gt; of real-world ML systems.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Topic&lt;/th&gt; 
   &lt;th&gt;What You'll Build&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;System Design&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Scalable, maintainable ML architectures&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Data Engineering&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Robust pipelines for collection, labeling, and processing&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Model Deployment&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Production-ready systems from prototypes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;MLOps &amp;amp; Monitoring&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Reliable, continuously operating systems&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Edge AI&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Resource-efficient deployment on mobile, embedded, and IoT&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h2&gt;⭐ Support This Work&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;h3&gt;Show Your Support&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;Star this repository&lt;/strong&gt; to help us demonstrate the value of open AI education to funders and institutions.&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/harvard-edge/cs249r_book?style=for-the-badge&amp;amp;logo=github&amp;amp;color=gold" alt="Stars" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Goal:&lt;/strong&gt; 10,000 stars = $100,000 in additional education funding&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book"&gt;&lt;strong&gt;⭐ Star Now&lt;/strong&gt;&lt;/a&gt; — &lt;em&gt;takes 2 seconds!&lt;/em&gt;&lt;/p&gt; 
 &lt;h3&gt;Fund the Mission (New!)&lt;/h3&gt; 
 &lt;p&gt;We've graduated this project from Harvard to enable global access and expand AI systems education worldwide. Please help us support educators globally, especially in the Global South, by providing TinyML kits for students, funding workshops, and sustaining our open-source infrastructure.&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://opencollective.com/mlsysbook"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%92%9D%20Support%20AI%20Education-Open%20Collective-blue.svg?style=for-the-badge" alt="Open Collective" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;From $15/month to sponsor a learner to $250 for workshops — every contribution democratizes AI education.&lt;/em&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🌐 Community &amp;amp; Resources&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Resource&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://mlsysbook.org"&gt;📚 &lt;strong&gt;Main Site&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Complete learning platform&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://mlsysbook.org/tinytorch"&gt;🔥 &lt;strong&gt;TinyTorch&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Educational ML framework&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/discussions"&gt;💬 &lt;strong&gt;Discussions&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Ask questions, share insights&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://mlsysbook.org/community"&gt;👥 &lt;strong&gt;Community&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Join our global learning community&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🎯 For Different Audiences&lt;/h2&gt; 
&lt;h3&gt;🎓 Students&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://mlsysbook.ai"&gt;📖 Read online&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mlsysbook.ai/Machine-Learning-Systems.pdf"&gt;📄 Download PDF&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mlsysbook.org"&gt;🧪 Try hands-on labs&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;👩‍🏫 Educators&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://mlsysbook.org"&gt;📋 Course materials&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mlsysbook.org"&gt;🎯 Instructor resources&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mlsysbook.org"&gt;💡 Teaching guides&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;🛠️ Contributors&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/harvard-edge/cs249r_book/dev/docs/contribute.md"&gt;🤝 Contribution guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/harvard-edge/cs249r_book/dev/#development"&gt;⚡ Development setup&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/discussions"&gt;💬 Join discussions&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🚀 Quick Start&lt;/h2&gt; 
&lt;h3&gt;For Readers&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Read online (continuously updated)
open https://mlsysbook.ai

# Or download PDF for offline access
curl -O https://mlsysbook.ai/Machine-Learning-Systems.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;For Contributors&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/harvard-edge/cs249r_book.git
cd cs249r_book

# Quick setup (recommended)
./binder setup      # Setup environment and dependencies
./binder doctor     # Check system health

# Fast development workflow
./binder preview intro    # Fast chapter development
./binder build intro      # Build specific chapter
./binder build            # Build complete book (HTML)
./binder help            # See all commands
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🤝 Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions from the global community! Here's how you can help:&lt;/p&gt; 
&lt;h3&gt;Ways to Contribute&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;📝 Content&lt;/strong&gt; — Suggest edits, improvements, or new examples&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🛠️ Tools&lt;/strong&gt; — Enhance development scripts and automation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🎨 Design&lt;/strong&gt; — Improve figures, diagrams, and visual elements&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🌍 Localization&lt;/strong&gt; — Translate content for global accessibility&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🔧 Infrastructure&lt;/strong&gt; — Help with build systems and deployment&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Quality Standards&lt;/h3&gt; 
&lt;p&gt;All contributions benefit from automated quality assurance:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;✅ &lt;strong&gt;Pre-commit validation&lt;/strong&gt; — Automatic cleanup and checks&lt;/li&gt; 
 &lt;li&gt;📋 &lt;strong&gt;Content review&lt;/strong&gt; — Formatting and style validation&lt;/li&gt; 
 &lt;li&gt;🧪 &lt;strong&gt;Testing&lt;/strong&gt; — Build and link verification&lt;/li&gt; 
 &lt;li&gt;👥 &lt;strong&gt;Peer review&lt;/strong&gt; — Community feedback&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/harvard-edge/cs249r_book/dev/docs/contribute.md"&gt;&lt;strong&gt;Start Contributing →&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🛠️ Development&lt;/h2&gt; 
&lt;h3&gt;Book Binder CLI (Recommended)&lt;/h3&gt; 
&lt;p&gt;The &lt;strong&gt;Book Binder&lt;/strong&gt; is our lightning-fast development CLI for streamlined building and iteration:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Chapter development (fast iteration)
./binder preview intro                # Build and preview single chapter
./binder preview intro,ml_systems     # Build and preview multiple chapters

# Complete book building
./binder build                        # Build complete website (HTML)
./binder pdf                          # Build complete PDF
./binder epub                         # Build complete EPUB

# Management
./binder clean                        # Clean artifacts
./binder status                       # Show current status
./binder doctor                       # Run health check
./binder help                         # Show all commands
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Development Commands&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Book Binder CLI (Recommended)
./binder setup            # First-time setup
./binder build            # Build complete HTML book
./binder pdf              # Build complete PDF book  
./binder epub             # Build complete EPUB book
./binder preview intro    # Preview chapter development

# Traditional setup (if needed)
python3 -m venv .venv
source .venv/bin/activate
pip install -r tools/dependencies/requirements.txt
pre-commit install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Project Structure&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;MLSysBook/
├── binder                   # ⚡ Fast development CLI (recommended)
├── quarto/                  # Main book content (Quarto)
│   ├── contents/            # Chapter content
│   │   ├── core/            # Core chapters
│   │   ├── labs/            # Hands-on labs
│   │   ├── frontmatter/     # Preface, acknowledgments
│   │   ├── backmatter/      # References and resources
│   │   └── parts/           # Book parts and sections
│   ├── _extensions/         # Quarto extensions
│   ├── config/              # Build configurations
│   │   ├── _quarto-html.yml # Website build configuration
│   │   └── _quarto-pdf.yml  # PDF build configuration
│   ├── data/                # Cross-reference and metadata files
│   ├── assets/              # Images, styles, media
│   ├── filters/             # Lua filters
│   ├── scripts/             # Build scripts
│   └── _quarto.yml          # Active config (symlink)
├── tools/                   # Development automation
│   ├── scripts/             # Organized development scripts
│   │   ├── content/         # Content management tools
│   │   ├── cross_refs/      # Cross-reference management
│   │   ├── genai/           # AI-assisted content tools
│   │   ├── maintenance/     # System maintenance scripts
│   │   ├── testing/         # Test and validation scripts
│   │   └── utilities/       # General utility scripts
│   ├── dependencies/        # Package requirements  
│   └── setup/               # Setup and configuration
├── config/                  # Project configuration
│   ├── dev/                 # Development configurations
│   ├── linting/             # Code quality configurations
│   └── quarto/              # Quarto publishing settings
├── docs/                    # Documentation
│   ├── BINDER.md            # Binder CLI guide
│   ├── BUILD.md             # Build instructions
│   ├── DEVELOPMENT.md       # Development guide
│   └── contribute.md        # Contribution guidelines
├── CHANGELOG.md             # Project changelog
├── CITATION.bib             # Citation information
├── pyproject.toml           # Python project configuration
└── README.md                # This file
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Documentation&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/harvard-edge/cs249r_book/dev/docs/BINDER.md"&gt;⚡ Binder CLI Guide&lt;/a&gt; — Fast development with the Book Binder&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/harvard-edge/cs249r_book/dev/docs/DEVELOPMENT.md"&gt;📋 Development Guide&lt;/a&gt; — Comprehensive setup and workflow&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/harvard-edge/cs249r_book/dev/docs/MAINTENANCE_GUIDE.md"&gt;🛠️ Maintenance Guide&lt;/a&gt; — Daily tasks and troubleshooting&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/harvard-edge/cs249r_book/dev/docs/BUILD.md"&gt;🔨 Build Instructions&lt;/a&gt; — Detailed build process&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/harvard-edge/cs249r_book/dev/docs/contribute.md"&gt;🤝 Contribution Guidelines&lt;/a&gt; — How to contribute effectively&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Publishing&lt;/h3&gt; 
&lt;p&gt;Publishing is handled through GitHub Actions workflows for consistent, automated deployment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Build locally to test before publishing
./binder build        # Build HTML
./binder pdf          # Build PDF  
./binder epub         # Build EPUB

# Publishing happens via GitHub Actions
# See docs/PUBLISH_LIVE_WORKFLOW.md for details
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Publishing Workflow:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Automated Deployment&lt;/strong&gt; — GitHub Actions workflows handle all publishing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Quality Checks&lt;/strong&gt; — Automated validation before deployment&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multiple Formats&lt;/strong&gt; — HTML, PDF, and EPUB published simultaneously&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Preview Deployments&lt;/strong&gt; — Pull requests get automatic preview deployments&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/harvard-edge/cs249r_book/dev/docs/PUBLISH_LIVE_WORKFLOW.md"&gt;Publishing Documentation&lt;/a&gt; for detailed instructions.&lt;/p&gt; 
&lt;h3&gt;Getting Started&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# First time setup
./binder setup

# Check system health
./binder doctor

# Quick preview
./binder preview intro
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;📋 Citation &amp;amp; License&lt;/h2&gt; 
&lt;h3&gt;Citation&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@inproceedings{reddi2024mlsysbook,
  title        = {MLSysBook.AI: Principles and Practices of Machine Learning Systems Engineering},
  author       = {Reddi, Vijay Janapa},
  booktitle    = {2024 International Conference on Hardware/Software Codesign and System Synthesis (CODES+ ISSS)},
  pages        = {41--42},
  year         = {2024},
  organization = {IEEE},
  url          = {https://mlsysbook.org}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;License&lt;/h3&gt; 
&lt;p&gt;This work is licensed under &lt;strong&gt;Creative Commons Attribution–NonCommercial–ShareAlike 4.0 International&lt;/strong&gt; (CC BY-NC-SA 4.0). You may share and adapt the material for non-commercial purposes with appropriate credit.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🙏 Contributors&lt;/h2&gt; 
&lt;p&gt;Thanks goes to these wonderful people who have contributed to making this resource better for everyone:&lt;/p&gt; 
&lt;!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section --&gt; 
&lt;!-- prettier-ignore-start --&gt; 
&lt;!-- markdownlint-disable --&gt; 
&lt;table&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/profvjreddi"&gt;&lt;img src="https://avatars.githubusercontent.com/profvjreddi?s=100" width="100px;" alt="Vijay Janapa Reddi" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Vijay Janapa Reddi&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/hzeljko"&gt;&lt;img src="https://avatars.githubusercontent.com/hzeljko?s=100" width="100px;" alt="Zeljko Hrcek" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Zeljko Hrcek&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/Mjrovai"&gt;&lt;img src="https://avatars.githubusercontent.com/Mjrovai?s=100" width="100px;" alt="Marcelo Rovai" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Marcelo Rovai&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/jasonjabbour"&gt;&lt;img src="https://avatars.githubusercontent.com/jasonjabbour?s=100" width="100px;" alt="Jason Jabbour" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Jason Jabbour&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/uchendui"&gt;&lt;img src="https://avatars.githubusercontent.com/uchendui?s=100" width="100px;" alt="Ikechukwu Uchendu" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Ikechukwu Uchendu&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/kai4avaya"&gt;&lt;img src="https://avatars.githubusercontent.com/kai4avaya?s=100" width="100px;" alt="Kai Kleinbard" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Kai Kleinbard&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/Naeemkh"&gt;&lt;img src="https://avatars.githubusercontent.com/Naeemkh?s=100" width="100px;" alt="Naeem Khoshnevis" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Naeem Khoshnevis&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/Sara-Khosravi"&gt;&lt;img src="https://avatars.githubusercontent.com/Sara-Khosravi?s=100" width="100px;" alt="Sara Khosravi" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Sara Khosravi&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/V0XNIHILI"&gt;&lt;img src="https://avatars.githubusercontent.com/V0XNIHILI?s=100" width="100px;" alt="Douwe den Blanken" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Douwe den Blanken&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/18jeffreyma"&gt;&lt;img src="https://avatars.githubusercontent.com/18jeffreyma?s=100" width="100px;" alt="Jeffrey Ma" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Jeffrey Ma&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/shanzehbatool"&gt;&lt;img src="https://avatars.githubusercontent.com/shanzehbatool?s=100" width="100px;" alt="shanzehbatool" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;shanzehbatool&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/eliasab16"&gt;&lt;img src="https://avatars.githubusercontent.com/eliasab16?s=100" width="100px;" alt="Elias" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Elias&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/JaredP94"&gt;&lt;img src="https://avatars.githubusercontent.com/JaredP94?s=100" width="100px;" alt="Jared Ping" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Jared Ping&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/ishapira1"&gt;&lt;img src="https://avatars.githubusercontent.com/ishapira1?s=100" width="100px;" alt="Itai Shapira" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Itai Shapira&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"&gt;&lt;img src="https://www.gravatar.com/avatar/8863743b4f26c1a20e730fcf7ebc3bc0?d=identicon&amp;amp;s=100?s=100" width="100px;" alt="Maximilian Lam" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Maximilian Lam&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/jaysonzlin"&gt;&lt;img src="https://avatars.githubusercontent.com/jaysonzlin?s=100" width="100px;" alt="Jayson Lin" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Jayson Lin&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/sophiacho1"&gt;&lt;img src="https://avatars.githubusercontent.com/sophiacho1?s=100" width="100px;" alt="Sophia Cho" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Sophia Cho&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/andreamurillomtz"&gt;&lt;img src="https://avatars.githubusercontent.com/andreamurillomtz?s=100" width="100px;" alt="Andrea" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Andrea&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/alxrod"&gt;&lt;img src="https://avatars.githubusercontent.com/alxrod?s=100" width="100px;" alt="Alex Rodriguez" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Alex Rodriguez&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/korneelf1"&gt;&lt;img src="https://avatars.githubusercontent.com/korneelf1?s=100" width="100px;" alt="Korneel Van den Berghe" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Korneel Van den Berghe&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/colbybanbury"&gt;&lt;img src="https://avatars.githubusercontent.com/colbybanbury?s=100" width="100px;" alt="Colby Banbury" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Colby Banbury&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/zishenwan"&gt;&lt;img src="https://avatars.githubusercontent.com/zishenwan?s=100" width="100px;" alt="Zishen Wan" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Zishen Wan&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/mmaz"&gt;&lt;img src="https://avatars.githubusercontent.com/mmaz?s=100" width="100px;" alt="Mark Mazumder" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Mark Mazumder&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/DivyaAmirtharaj"&gt;&lt;img src="https://avatars.githubusercontent.com/DivyaAmirtharaj?s=100" width="100px;" alt="Divya Amirtharaj" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Divya Amirtharaj&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/ma3mool"&gt;&lt;img src="https://avatars.githubusercontent.com/ma3mool?s=100" width="100px;" alt="Abdulrahman Mahmoud" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Abdulrahman Mahmoud&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/srivatsankrishnan"&gt;&lt;img src="https://avatars.githubusercontent.com/srivatsankrishnan?s=100" width="100px;" alt="Srivatsan Krishnan" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Srivatsan Krishnan&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/James-QiuHaoran"&gt;&lt;img src="https://avatars.githubusercontent.com/James-QiuHaoran?s=100" width="100px;" alt="Haoran Qiu" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Haoran Qiu&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/aptl26"&gt;&lt;img src="https://avatars.githubusercontent.com/aptl26?s=100" width="100px;" alt="Aghyad Deeb" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Aghyad Deeb&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/arnaumarin"&gt;&lt;img src="https://avatars.githubusercontent.com/arnaumarin?s=100" width="100px;" alt="marin-llobet" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;marin-llobet&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/jared-ni"&gt;&lt;img src="https://avatars.githubusercontent.com/jared-ni?s=100" width="100px;" alt="Jared Ni" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Jared Ni&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/oishib"&gt;&lt;img src="https://avatars.githubusercontent.com/oishib?s=100" width="100px;" alt="oishib" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;oishib&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/MichaelSchnebly"&gt;&lt;img src="https://avatars.githubusercontent.com/MichaelSchnebly?s=100" width="100px;" alt="Michael Schnebly" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Michael Schnebly&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/ELSuitorHarvard"&gt;&lt;img src="https://avatars.githubusercontent.com/ELSuitorHarvard?s=100" width="100px;" alt="ELSuitorHarvard" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;ELSuitorHarvard&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/Ekhao"&gt;&lt;img src="https://avatars.githubusercontent.com/Ekhao?s=100" width="100px;" alt="Emil Njor" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Emil Njor&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/VThuong99"&gt;&lt;img src="https://avatars.githubusercontent.com/VThuong99?s=100" width="100px;" alt="Thuong Duong" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Thuong Duong&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/AditiR-42"&gt;&lt;img src="https://avatars.githubusercontent.com/AditiR-42?s=100" width="100px;" alt="Aditi Raju" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Aditi Raju&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/jaywonchung"&gt;&lt;img src="https://avatars.githubusercontent.com/jaywonchung?s=100" width="100px;" alt="Jae-Won Chung" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Jae-Won Chung&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/leo47007"&gt;&lt;img src="https://avatars.githubusercontent.com/leo47007?s=100" width="100px;" alt="Yu-Shun Hsiao" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Yu-Shun Hsiao&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/BaeHenryS"&gt;&lt;img src="https://avatars.githubusercontent.com/BaeHenryS?s=100" width="100px;" alt="Henry Bae" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Henry Bae&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/ShvetankPrakash"&gt;&lt;img src="https://avatars.githubusercontent.com/ShvetankPrakash?s=100" width="100px;" alt="Shvetank Prakash" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Shvetank Prakash&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"&gt;&lt;img src="https://www.gravatar.com/avatar/af39c27c6090c50a1921a9b6366e81cc?d=identicon&amp;amp;s=100?s=100" width="100px;" alt="Emeka Ezike" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Emeka Ezike&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/arbass22"&gt;&lt;img src="https://avatars.githubusercontent.com/arbass22?s=100" width="100px;" alt="Andrew Bass" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Andrew Bass&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/jzhou1318"&gt;&lt;img src="https://avatars.githubusercontent.com/jzhou1318?s=100" width="100px;" alt="Jennifer Zhou" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Jennifer Zhou&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/aryatschand"&gt;&lt;img src="https://avatars.githubusercontent.com/aryatschand?s=100" width="100px;" alt="Arya Tschand" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Arya Tschand&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/pongtr"&gt;&lt;img src="https://avatars.githubusercontent.com/pongtr?s=100" width="100px;" alt="Pong Trairatvorakul" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Pong Trairatvorakul&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"&gt;&lt;img src="https://www.gravatar.com/avatar/0c931fcfd03cd548d44c90602dd773ba?d=identicon&amp;amp;s=100?s=100" width="100px;" alt="Matthew Stewart" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Matthew Stewart&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/marcozennaro"&gt;&lt;img src="https://avatars.githubusercontent.com/marcozennaro?s=100" width="100px;" alt="Marco Zennaro" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Marco Zennaro&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/euranofshin"&gt;&lt;img src="https://avatars.githubusercontent.com/euranofshin?s=100" width="100px;" alt="Eura Nofshin" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Eura Nofshin&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/BrunoScaglione"&gt;&lt;img src="https://avatars.githubusercontent.com/BrunoScaglione?s=100" width="100px;" alt="Bruno Scaglione" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Bruno Scaglione&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/taunoe"&gt;&lt;img src="https://avatars.githubusercontent.com/taunoe?s=100" width="100px;" alt="Tauno Erik" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Tauno Erik&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/alex-oesterling"&gt;&lt;img src="https://avatars.githubusercontent.com/alex-oesterling?s=100" width="100px;" alt="Alex Oesterling" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Alex Oesterling&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/gnodipac886"&gt;&lt;img src="https://avatars.githubusercontent.com/gnodipac886?s=100" width="100px;" alt="gnodipac886" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;gnodipac886&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/Gjain234"&gt;&lt;img src="https://avatars.githubusercontent.com/Gjain234?s=100" width="100px;" alt="Gauri Jain" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Gauri Jain&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/Allen-Kuang"&gt;&lt;img src="https://avatars.githubusercontent.com/Allen-Kuang?s=100" width="100px;" alt="Allen-Kuang" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Allen-Kuang&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/TheHiddenLayer"&gt;&lt;img src="https://avatars.githubusercontent.com/TheHiddenLayer?s=100" width="100px;" alt="TheHiddenLayer" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;TheHiddenLayer&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/FinAminToastCrunch"&gt;&lt;img src="https://avatars.githubusercontent.com/FinAminToastCrunch?s=100" width="100px;" alt="Fin Amin" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Fin Amin&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"&gt;&lt;img src="https://www.gravatar.com/avatar/468ef35acc69f3266efd700992daa369?d=identicon&amp;amp;s=100?s=100" width="100px;" alt="Fatima Shah" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Fatima Shah&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/serco425"&gt;&lt;img src="https://avatars.githubusercontent.com/serco425?s=100" width="100px;" alt="Sercan Aygün" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Sercan Aygün&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/vitasam"&gt;&lt;img src="https://avatars.githubusercontent.com/vitasam?s=100" width="100px;" alt="The Random DIY" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;The Random DIY&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/BravoBaldo"&gt;&lt;img src="https://avatars.githubusercontent.com/BravoBaldo?s=100" width="100px;" alt="Baldassarre Cesarano" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Baldassarre Cesarano&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/YangZhou1997"&gt;&lt;img src="https://avatars.githubusercontent.com/YangZhou1997?s=100" width="100px;" alt="Yang Zhou" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Yang Zhou&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/YLab-UChicago"&gt;&lt;img src="https://avatars.githubusercontent.com/YLab-UChicago?s=100" width="100px;" alt="yanjingl" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;yanjingl&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/AbenezerKb"&gt;&lt;img src="https://avatars.githubusercontent.com/AbenezerKb?s=100" width="100px;" alt="Abenezer Angamo" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Abenezer Angamo&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/jasonlyik"&gt;&lt;img src="https://avatars.githubusercontent.com/jasonlyik?s=100" width="100px;" alt="Jason Yik" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Jason Yik&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/aethernavshulkraven-allain"&gt;&lt;img src="https://avatars.githubusercontent.com/aethernavshulkraven-allain?s=100" width="100px;" alt="अरनव शुक्ला | Arnav Shukla" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;अरनव शुक्ला | Arnav Shukla&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/arighosh05"&gt;&lt;img src="https://avatars.githubusercontent.com/arighosh05?s=100" width="100px;" alt="Aritra Ghosh" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Aritra Ghosh&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/happyappledog"&gt;&lt;img src="https://avatars.githubusercontent.com/happyappledog?s=100" width="100px;" alt="happyappledog" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;happyappledog&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/abigailswallow"&gt;&lt;img src="https://avatars.githubusercontent.com/abigailswallow?s=100" width="100px;" alt="abigailswallow" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;abigailswallow&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/bilgeacun"&gt;&lt;img src="https://avatars.githubusercontent.com/bilgeacun?s=100" width="100px;" alt="Bilge Acun" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Bilge Acun&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/atcheng2"&gt;&lt;img src="https://avatars.githubusercontent.com/atcheng2?s=100" width="100px;" alt="Andy Cheng" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Andy Cheng&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/cursoragent"&gt;&lt;img src="https://avatars.githubusercontent.com/cursoragent?s=100" width="100px;" alt="Cursor Agent" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Cursor Agent&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/emmanuel2406"&gt;&lt;img src="https://avatars.githubusercontent.com/emmanuel2406?s=100" width="100px;" alt="Emmanuel Rassou" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Emmanuel Rassou&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/jessicaquaye"&gt;&lt;img src="https://avatars.githubusercontent.com/jessicaquaye?s=100" width="100px;" alt="Jessica Quaye" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Jessica Quaye&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/vijay-edu"&gt;&lt;img src="https://avatars.githubusercontent.com/vijay-edu?s=100" width="100px;" alt="Vijay Edupuganti" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Vijay Edupuganti&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/sjohri20"&gt;&lt;img src="https://avatars.githubusercontent.com/sjohri20?s=100" width="100px;" alt="Shreya Johri" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Shreya Johri&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/skmur"&gt;&lt;img src="https://avatars.githubusercontent.com/skmur?s=100" width="100px;" alt="Sonia Murthy" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Sonia Murthy&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"&gt;&lt;img src="https://www.gravatar.com/avatar/fc4f3460cdfb9365ab59bdeafb06413e?d=identicon&amp;amp;s=100?s=100" width="100px;" alt="Costin-Andrei Oncescu" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Costin-Andrei Oncescu&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"&gt;&lt;img src="https://www.gravatar.com/avatar/0d6b8616427d8b19d425c9808692e347?d=identicon&amp;amp;s=100?s=100" width="100px;" alt="formlsysbookissue" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;formlsysbookissue&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"&gt;&lt;img src="https://www.gravatar.com/avatar/7cd8d5dfd83071f23979019d97655dc5?d=identicon&amp;amp;s=100?s=100" width="100px;" alt="Annie Laurie Cook" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Annie Laurie Cook&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"&gt;&lt;img src="https://www.gravatar.com/avatar/f88052cca4f401d9b0f43aed0a53434a?d=identicon&amp;amp;s=100?s=100" width="100px;" alt="Jothi Ramaswamy" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Jothi Ramaswamy&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"&gt;&lt;img src="https://www.gravatar.com/avatar/35a8d9ffd03f05e79a2c6ce6206a56f2?d=identicon&amp;amp;s=100?s=100" width="100px;" alt="Batur Arslan" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Batur Arslan&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"&gt;&lt;img src="https://www.gravatar.com/avatar/bd53d146aa888548c8db4da02bf81e7a?d=identicon&amp;amp;s=100?s=100" width="100px;" alt="Curren Iyer" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Curren Iyer&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"&gt;&lt;img src="https://www.gravatar.com/avatar/468ef35acc69f3266efd700992daa369?d=identicon&amp;amp;s=100?s=100" width="100px;" alt="Fatima Shah" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Fatima Shah&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"&gt;&lt;img src="https://www.gravatar.com/avatar/8d8410338458e08bd5e4b96f58e1c217?d=identicon&amp;amp;s=100?s=100" width="100px;" alt="Edward Jin" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Edward Jin&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"&gt;&lt;img src="https://www.gravatar.com/avatar/a5a47df988ab1720dd706062e523ca32?d=identicon&amp;amp;s=100?s=100" width="100px;" alt="a-saraf" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;a-saraf&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"&gt;&lt;img src="https://www.gravatar.com/avatar/c2dc311aa8122d5f5f061e1db14682b1?d=identicon&amp;amp;s=100?s=100" width="100px;" alt="songhan" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;songhan&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"&gt;&lt;img src="https://www.gravatar.com/avatar/4814aad67982ab07a69006a1ce9d2a72?d=identicon&amp;amp;s=100?s=100" width="100px;" alt="jvijay" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;jvijay&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"&gt;&lt;img src="https://www.gravatar.com/avatar/43b1feff77c8a95fd581774fb8ec891f?d=identicon&amp;amp;s=100?s=100" width="100px;" alt="Zishen" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Zishen&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;!-- markdownlint-restore --&gt; 
&lt;!-- prettier-ignore-end --&gt; 
&lt;!-- ALL-CONTRIBUTORS-LIST:END --&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;strong&gt;Made with ❤️ for AI learners worldwide&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Our goal is to educate 1 million AI systems engineers for the future at the edge of AI.&lt;/p&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>PaddlePaddle/PaddleOCR</title>
      <link>https://github.com/PaddlePaddle/PaddleOCR</link>
      <description>&lt;p&gt;Turn any PDF or image document into structured data for your AI. A powerful, lightweight OCR toolkit that bridges the gap between images/PDFs and LLMs. Supports 100+ languages.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/docs/images/Banner.png" alt="PaddleOCR Banner" /&gt; &lt;/p&gt; 
 &lt;p&gt;English | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_cn.md"&gt;简体中文&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_tcn.md"&gt;繁體中文&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_ja.md"&gt;日本語&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_ko.md"&gt;한국어&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_fr.md"&gt;Français&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_ru.md"&gt;Русский&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_es.md"&gt;Español&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_ar.md"&gt;العربية&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/PaddlePaddle/PaddleOCR"&gt;&lt;img src="https://img.shields.io/github/stars/PaddlePaddle/PaddleOCR?color=ccf" alt="stars" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2507.05595"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2507.05595-b31b1b.svg?logo=arXiv" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/paddleocr"&gt;&lt;img src="https://static.pepy.tech/badge/paddleocr/month" alt="PyPI Downloads" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/paddleocr"&gt;&lt;img src="https://static.pepy.tech/badge/paddleocr" alt="PyPI Downloads" /&gt;&lt;/a&gt; &lt;a href="https://github.com/PaddlePaddle/PaddleOCR/network/dependents"&gt;&lt;img src="https://img.shields.io/badge/Used%20by-5.9k%2B%20repositories-blue" alt="Used by" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://img.shields.io/badge/python-3.8~3.12-aff.svg?sanitize=true" alt="python" /&gt; &lt;img src="https://img.shields.io/badge/os-linux%2C%20win%2C%20mac-pink.svg?sanitize=true" alt="os" /&gt; &lt;img src="https://img.shields.io/badge/hardware-cpu%2C%20gpu%2C%20xpu%2C%20npu-yellow.svg?sanitize=true" alt="hardware" /&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-Apache_2.0-green" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/PaddlePaddle/PaddleOCR"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;PaddleOCR is an industry-leading, production-ready OCR and document AI engine, offering end-to-end solutions from text extraction to intelligent document understanding&lt;/strong&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h1&gt;PaddleOCR&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://www.paddlepaddle.org.cn/en"&gt;&lt;img src="https://img.shields.io/badge/PaddlePaddle-3.0-orange" alt="Framework" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/#"&gt;&lt;img src="https://img.shields.io/badge/Recognition%20Accuracy-%F0%9F%8F%86-green" alt="Accuracy" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/#"&gt;&lt;img src="https://img.shields.io/badge/Support_Languages-100+-brightgreen" alt="Multi-Language" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/#"&gt;&lt;img src="https://img.shields.io/badge/Handwriting-%E2%9C%93-success" alt="Handwriting" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/#"&gt;&lt;img src="https://img.shields.io/badge/Heterogeneous%20Hardware-Kunlunxin%20%7C%20Ascend_NPU-red" alt="Hardware" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] PaddleOCR now provides an MCP server that supports integration with Agent applications like Claude Desktop. For details, please refer to &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/mcp_server.html"&gt;PaddleOCR MCP Server&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;The PaddleOCR 3.0 Technical Report is now available. See details at: &lt;a href="https://arxiv.org/abs/2507.05595"&gt;PaddleOCR 3.0 Technical Report&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;The PaddleOCR-VL Technical Report is now available. See details at &lt;a href="https://arxiv.org/abs/2510.14528"&gt;PaddleOCR-VL Technical Report&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;PaddleOCR&lt;/strong&gt; converts documents and images into &lt;strong&gt;structured, AI-friendly data&lt;/strong&gt; (like JSON and Markdown) with &lt;strong&gt;industry-leading accuracy&lt;/strong&gt;—powering AI applications for everyone from indie developers and startups to large enterprises worldwide. With over &lt;strong&gt;50,000 stars&lt;/strong&gt; and deep integration into leading projects like &lt;strong&gt;MinerU, RAGFlow, and OmniParser&lt;/strong&gt;, PaddleOCR has become the &lt;strong&gt;premier solution&lt;/strong&gt; for developers building intelligent document applications in the &lt;strong&gt;AI era&lt;/strong&gt;.&lt;/p&gt; 
&lt;h3&gt;PaddleOCR 3.0 Core Features&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/spaces/PaddlePaddle/PaddleOCR-VL_Online_Demo"&gt;&lt;img src="https://img.shields.io/badge/PaddleOCR--VL-_Demo_on_HuggingFace-yellow?logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAF8AAABYCAMAAACkl9t/AAAAk1BMVEVHcEz/nQv/nQv/nQr/nQv/nQr/nQv/nQv/nQr/wRf/txT/pg7/yRr/rBD/zRz/ngv/oAz/zhz/nwv/txT/ngv/0B3+zBz/nQv/0h7/wxn/vRb/thXkuiT/rxH/pxD/ogzcqyf/nQvTlSz/czCxky7/SjifdjT/Mj3+Mj3wMj15aTnDNz+DSD9RTUBsP0FRO0Q6O0WyIxEIAAAAGHRSTlMADB8zSWF3krDDw8TJ1NbX5efv8ff9/fxKDJ9uAAAGKklEQVR42u2Z63qjOAyGC4RwCOfB2JAGqrSb2WnTw/1f3UaWcSGYNKTdf/P+mOkTrE+yJBulvfvLT2A5ruenaVHyIks33npl/6C4s/ZLAM45SOi/1FtZPyFur1OYofBX3w7d54Bxm+E8db+nDr12ttmESZ4zludJEG5S7TO72YPlKZFyE+YCYUJTBZsMiNS5Sd7NlDmKM2Eg2JQg8awbglfqgbhArjxkS7dgp2RH6hc9AMLdZYUtZN5DJr4molC8BfKrEkPKEnEVjLbgW1fLy77ZVOJagoIcLIl+IxaQZGjiX597HopF5CkaXVMDO9Pyix3AFV3kw4lQLCbHuMovz8FallbcQIJ5Ta0vks9RnolbCK84BtjKRS5uA43hYoZcOBGIG2Epbv6CvFVQ8m8loh66WNySsnN7htL58LNp+NXT8/PhXiBXPMjLSxtwp8W9f/1AngRierBkA+kk/IpUSOeKByzn8y3kAAAfh//0oXgV4roHm/kz4E2z//zRc3/lgwBzbM2mJxQEa5pqgX7d1L0htrhx7LKxOZlKbwcAWyEOWqYSI8YPtgDQVjpB5nvaHaSnBaQSD6hweDi8PosxD6/PT09YY3xQA7LTCTKfYX+QHpA0GCcqmEHvr/cyfKQTEuwgbs2kPxJEB0iNjfJcCTPyocx+A0griHSmADiC91oNGVwJ69RudYe65vJmoqfpul0lrqXadW0jFKH5BKwAeCq+Den7s+3zfRJzA61/Uj/9H/VzLKTx9jFPPdXeeP+L7WEvDLAKAIoF8bPTKT0+TM7W8ePj3Rz/Yn3kOAp2f1Kf0Weony7pn/cPydvhQYV+eFOfmOu7VB/ViPe34/EN3RFHY/yRuT8ddCtMPH/McBAT5s+vRde/gf2c/sPsjLK+m5IBQF5tO+h2tTlBGnP6693JdsvofjOPnnEHkh2TnV/X1fBl9S5zrwuwF8NFrAVJVwCAPTe8gaJlomqlp0pv4Pjn98tJ/t/fL++6unpR1YGC2n/KCoa0tTLoKiEeUPDl94nj+5/Tv3/eT5vBQ60X1S0oZr+IWRR8Ldhu7AlLjPISlJcO9vrFotky9SpzDequlwEir5beYAc0R7D9KS1DXva0jhYRDXoExPdc6yw5GShkZXe9QdO/uOvHofxjrV/TNS6iMJS+4TcSTgk9n5agJdBQbB//IfF/HpvPt3Tbi7b6I6K0R72p6ajryEJrENW2bbeVUGjfgoals4L443c7BEE4mJO2SpbRngxQrAKRudRzGQ8jVOL2qDVjjI8K1gc3TIJ5KiFZ1q+gdsARPB4NQS4AjwVSt72DSoXNyOWUrU5mQ9nRYyjp89Xo7oRI6Bga9QNT1mQ/ptaJq5T/7WcgAZywR/XlPGAUDdet3LE+qS0TI+g+aJU8MIqjo0Kx8Ly+maxLjJmjQ18rA0YCkxLQbUZP1WqdmyQGJLUm7VnQFqodmXSqmRrdVpqdzk5LvmvgtEcW8PMGdaS23EOWyDVbACZzUJPaqMbjDxpA3Qrgl0AikimGDbqmyT8P8NOYiqrldF8rX+YN7TopX4UoHuSCYY7cgX4gHwclQKl1zhx0THf+tCAUValzjI7Wg9EhptrkIcfIJjA94evOn8B2eHaVzvBrnl2ig0So6hvPaz0IGcOvTHvUIlE2+prqAxLSQxZlU2stql1NqCCLdIiIN/i1DBEHUoElM9dBravbiAnKqgpi4IBkw+utSPIoBijDXJipSVV7MpOEJUAc5Qmm3BnUN+w3hteEieYKfRZSIUcXKMVf0u5wD4EwsUNVvZOtUT7A2GkffHjByWpHqvRBYrTV72a6j8zZ6W0DTE86Hn04bmyWX3Ri9WH7ZU6Q7h+ZHo0nHUAcsQvVhXRDZHChwiyi/hnPuOsSEF6Exk3o6Y9DT1eZ+6cASXk2Y9k+6EOQMDGm6WBK10wOQJCBwren86cPPWUcRAnTVjGcU1LBgs9FURiX/e6479yZcLwCBmTxiawEwrOcleuu12t3tbLv/N4RLYIBhYexm7Fcn4OJcn0+zc+s8/VfPeddZHAGN6TT8eGczHdR/Gts1/MzDkThr23zqrVfAMFT33Nx1RJsx1k5zuWILLnG/vsH+Fv5D4NTVcp1Gzo8AAAAAElFTkSuQmCC&amp;amp;labelColor=white" alt="HuggingFace" /&gt;&lt;/a&gt; &lt;a href="https://aistudio.baidu.com/application/detail/98365"&gt;&lt;img src="https://img.shields.io/badge/PaddleOCR--VL-_Demo_on_AI_Studio-1927BA?logo=data:image/png;base64,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&amp;amp;labelColor=white" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://www.modelscope.cn/studios/PaddlePaddle/PaddleOCR-VL_Online_Demo"&gt;&lt;img src="https://img.shields.io/badge/PaddleOCR--VL-_Demo_on_ModelScope-purple?logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIzIiBoZWlnaHQ9IjIwMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCiA8Zz4KICA8dGl0bGU+TGF5ZXIgMTwvdGl0bGU+CiAgPHBhdGggaWQ9InN2Z18xNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTAsODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTUiIGZpbGw9IiM2MjRhZmYiIGQ9Im05OS4xNCwxMTUuNDlsMjUuNjUsMGwwLDI1LjY1bC0yNS42NSwwbDAsLTI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTYiIGZpbGw9IiM2MjRhZmYiIGQ9Im0xNzYuMDksMTQxLjE0bC0yNS42NDk5OSwwbDAsMjIuMTlsNDcuODQsMGwwLC00Ny44NGwtMjIuMTksMGwwLDI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTciIGZpbGw9IiMzNmNmZDEiIGQ9Im0xMjQuNzksODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTgiIGZpbGw9IiMzNmNmZDEiIGQ9Im0wLDY0LjE5bDI1LjY1LDBsMCwyNS42NWwtMjUuNjUsMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzE5IiBmaWxsPSIjNjI0YWZmIiBkPSJtMTk4LjI4LDg5Ljg0bDI1LjY0OTk5LDBsMCwyNS42NDk5OWwtMjUuNjQ5OTksMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIwIiBmaWxsPSIjMzZjZmQxIiBkPSJtMTk4LjI4LDY0LjE5bDI1LjY0OTk5LDBsMCwyNS42NWwtMjUuNjQ5OTksMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIxIiBmaWxsPSIjNjI0YWZmIiBkPSJtMTUwLjQ0LDQybDAsMjIuMTlsMjUuNjQ5OTksMGwwLDI1LjY1bDIyLjE5LDBsMCwtNDcuODRsLTQ3Ljg0LDB6Ii8+CiAgPHBhdGggaWQ9InN2Z18yMiIgZmlsbD0iIzM2Y2ZkMSIgZD0ibTczLjQ5LDg5Ljg0bDI1LjY1LDBsMCwyNS42NDk5OWwtMjUuNjUsMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIzIiBmaWxsPSIjNjI0YWZmIiBkPSJtNDcuODQsNjQuMTlsMjUuNjUsMGwwLC0yMi4xOWwtNDcuODQsMGwwLDQ3Ljg0bDIyLjE5LDBsMCwtMjUuNjV6Ii8+CiAgPHBhdGggaWQ9InN2Z18yNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTQ3Ljg0LDExNS40OWwtMjIuMTksMGwwLDQ3Ljg0bDQ3Ljg0LDBsMCwtMjIuMTlsLTI1LjY1LDBsMCwtMjUuNjV6Ii8+CiA8L2c+Cjwvc3ZnPg==&amp;amp;labelColor=white" alt="ModelScope" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://aistudio.baidu.com/community/app/91660/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP--OCRv5-Demo_on_AI_Studio-1927BA?logo=data:image/png;base64,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&amp;amp;labelColor=white" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://aistudio.baidu.com/community/app/518494/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP--StructureV3-Demo_on_AI_Studio-1927BA?logo=data:image/png;base64,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&amp;amp;labelColor=white" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://aistudio.baidu.com/community/app/518493/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP--ChatOCRv4-Demo_on_AI_Studio-1927BA?logo=data:image/png;base64,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&amp;amp;labelColor=white" alt="AI Studio" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;PaddleOCR-VL - Multilingual Document Parsing via a 0.9B VLM&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;The SOTA and resource-efficient model tailored for document parsing&lt;/strong&gt;, that supports 109 languages and excels in recognizing complex elements (e.g., text, tables, formulas, and charts), while maintaining minimal resource consumption.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-OCRv5 — Universal Scene Text Recognition&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;Single model supports five text types&lt;/strong&gt; (Simplified Chinese, Traditional Chinese, English, Japanese, and Pinyin) with &lt;strong&gt;13% accuracy improvement&lt;/strong&gt;. Solves multilingual mixed document recognition challenges.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-StructureV3 — Complex Document Parsing&lt;/strong&gt;&lt;br /&gt; Intelligently converts complex PDFs and document images into &lt;strong&gt;Markdown and JSON files that preserve original structure&lt;/strong&gt;. &lt;strong&gt;Outperforms&lt;/strong&gt; numerous commercial solutions in public benchmarks. &lt;strong&gt;Perfectly maintains document layout and hierarchical structure&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-ChatOCRv4 — Intelligent Information Extraction&lt;/strong&gt;&lt;br /&gt; Natively integrates ERNIE 4.5 to &lt;strong&gt;precisely extract key information&lt;/strong&gt; from massive documents, with 15% accuracy improvement over previous generation. Makes documents "&lt;strong&gt;understand&lt;/strong&gt;" your questions and provide accurate answers.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;In addition to providing an outstanding model library, PaddleOCR 3.0 also offers user-friendly tools covering model training, inference, and service deployment, so developers can rapidly bring AI applications to production.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/main/images/paddleocr/README/Arch.jpg" alt="PaddleOCR Architecture" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Special Note&lt;/strong&gt;: PaddleOCR 3.x introduces several significant interface changes. &lt;strong&gt;Old code written based on PaddleOCR 2.x is likely incompatible with PaddleOCR 3.x&lt;/strong&gt;. Please ensure that the documentation you are reading matches the version of PaddleOCR you are using. &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/update/upgrade_notes.html"&gt;This document&lt;/a&gt; explains the reasons for the upgrade and the major changes from PaddleOCR 2.x to 3.x.&lt;/p&gt; 
&lt;h2&gt;📣 Recent updates&lt;/h2&gt; 
&lt;h3&gt;🔥🔥 2025.10.16: PaddleOCR 3.3.0 released, includes:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Released PaddleOCR-VL:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Model Introduction&lt;/strong&gt;:&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;PaddleOCR-VL&lt;/strong&gt; is a SOTA and resource-efficient model tailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a compact yet powerful vision-language model (VLM) that integrates a NaViT-style dynamic resolution visual encoder with the ERNIE-4.5-0.3B language model to enable accurate element recognition. &lt;strong&gt;This innovative model efficiently supports 109 languages and excels in recognizing complex elements (e.g., text, tables, formulas, and charts), while maintaining minimal resource consumption&lt;/strong&gt;. Through comprehensive evaluations on widely used public benchmarks and in-house benchmarks, PaddleOCR-VL achieves SOTA performance in both page-level document parsing and element-level recognition. It significantly outperforms existing solutions, exhibits strong competitiveness against top-tier VLMs, and delivers fast inference speeds. These strengths make it highly suitable for practical deployment in real-world scenarios. The model has been released on &lt;a href="https://huggingface.co/PaddlePaddle/PaddleOCR-VL"&gt;HuggingFace&lt;/a&gt;. Everyone is welcome to download and use it! More introduction infomation can be found in &lt;a href="https://www.paddleocr.ai/latest/version3.x/algorithm/PaddleOCR-VL/PaddleOCR-VL.html"&gt;PaddleOCR-VL&lt;/a&gt;.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Core Features&lt;/strong&gt;:&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Compact yet Powerful VLM Architecture&lt;/strong&gt;: We present a novel vision-language model that is specifically designed for resource-efficient inference, achieving outstanding performance in element recognition. By integrating a NaViT-style dynamic high-resolution visual encoder with the lightweight ERNIE-4.5-0.3B language model, we significantly enhance the model’s recognition capabilities and decoding efficiency. This integration maintains high accuracy while reducing computational demands, making it well-suited for efficient and practical document processing applications.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;SOTA Performance on Document Parsing&lt;/strong&gt;: PaddleOCR-VL achieves state-of-the-art performance in both page-level document parsing and element-level recognition. It significantly outperforms existing pipeline-based solutions and exhibiting strong competitiveness against leading vision-language models (VLMs) in document parsing. Moreover, it excels in recognizing complex document elements, such as text, tables, formulas, and charts, making it suitable for a wide range of challenging content types, including handwritten text and historical documents. This makes it highly versatile and suitable for a wide range of document types and scenarios.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Multilingual Support&lt;/strong&gt;: PaddleOCR-VL Supports 109 languages, covering major global languages, including but not limited to Chinese, English, Japanese, Latin, and Korean, as well as languages with different scripts and structures, such as Russian (Cyrillic script), Arabic, Hindi (Devanagari script), and Thai. This broad language coverage substantially enhances the applicability of our system to multilingual and globalized document processing scenarios.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Released PP-OCRv5 Multilingual Recognition Model:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Improved the accuracy and coverage of Latin script recognition; added support for Cyrillic, Arabic, Devanagari, Telugu, Tamil, and other language systems, covering recognition of 109 languages. The model has only 2M parameters, and the accuracy of some models has increased by over 40% compared to the previous generation.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;2025.08.21: Release of PaddleOCR 3.2.0&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Significant Model Additions:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Introduced training, inference, and deployment for PP-OCRv5 recognition models in English, Thai, and Greek. &lt;strong&gt;The PP-OCRv5 English model delivers an 11% improvement in English scenarios compared to the main PP-OCRv5 model, with the Thai and Greek recognition models achieving accuracies of 82.68% and 89.28%, respectively.&lt;/strong&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Deployment Capability Upgrades:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Full support for PaddlePaddle framework versions 3.1.0 and 3.1.1.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Comprehensive upgrade of the PP-OCRv5 C++ local deployment solution, now supporting both Linux and Windows, with feature parity and identical accuracy to the Python implementation.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;High-performance inference now supports CUDA 12, and inference can be performed using either the Paddle Inference or ONNX Runtime backends.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;The high-stability service-oriented deployment solution is now fully open-sourced, allowing users to customize Docker images and SDKs as required.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;The high-stability service-oriented deployment solution also supports invocation via manually constructed HTTP requests, enabling client-side code development in any programming language.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Benchmark Support:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;All production lines now support fine-grained benchmarking, enabling measurement of end-to-end inference time as well as per-layer and per-module latency data to assist with performance analysis. &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/docs/version3.x/pipeline_usage/instructions/benchmark.en.md"&gt;Here's&lt;/a&gt; how to set up and use the benchmark feature.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Documentation has been updated to include key metrics for commonly used configurations on mainstream hardware, such as inference latency and memory usage, providing deployment references for users.&lt;/strong&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Bug Fixes:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Resolved the issue of failed log saving during model training.&lt;/li&gt; 
    &lt;li&gt;Upgraded the data augmentation component for formula models for compatibility with newer versions of the albumentations dependency, and fixed deadlock warnings when using the tokenizers package in multi-process scenarios.&lt;/li&gt; 
    &lt;li&gt;Fixed inconsistencies in switch behaviors (e.g., &lt;code&gt;use_chart_parsing&lt;/code&gt;) in the PP-StructureV3 configuration files compared to other pipelines.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Other Enhancements:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Separated core and optional dependencies. Only minimal core dependencies are required for basic text recognition; additional dependencies for document parsing and information extraction can be installed as needed.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Enabled support for NVIDIA RTX 50 series graphics cards on Windows; users can refer to the &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/docs/version3.x/installation.en.md"&gt;installation guide&lt;/a&gt; for the corresponding PaddlePaddle framework versions.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;PP-OCR series models now support returning single-character coordinates.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;Added AIStudio, ModelScope, and other model download sources, allowing users to specify the source for model downloads.&lt;/li&gt; 
    &lt;li&gt;Added support for chart-to-table conversion via the PP-Chart2Table module.&lt;/li&gt; 
    &lt;li&gt;Optimized documentation descriptions to improve usability.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;2025.08.15: PaddleOCR 3.1.1 Released&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Bug Fixes:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Added the missing methods &lt;code&gt;save_vector&lt;/code&gt;, &lt;code&gt;save_visual_info_list&lt;/code&gt;, &lt;code&gt;load_vector&lt;/code&gt;, and &lt;code&gt;load_visual_info_list&lt;/code&gt; in the &lt;code&gt;PP-ChatOCRv4&lt;/code&gt; class.&lt;/li&gt; 
    &lt;li&gt;Added the missing parameters &lt;code&gt;glossary&lt;/code&gt; and &lt;code&gt;llm_request_interval&lt;/code&gt; to the &lt;code&gt;translate&lt;/code&gt; method in the &lt;code&gt;PPDocTranslation&lt;/code&gt; class.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Documentation Improvements:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Added a demo to the MCP documentation.&lt;/li&gt; 
    &lt;li&gt;Added information about the PaddlePaddle and PaddleOCR version used for performance metrics testing in the documentation.&lt;/li&gt; 
    &lt;li&gt;Fixed errors and omissions in the production line document translation.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Others:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Changed the MCP server dependency to use the pure Python library &lt;code&gt;puremagic&lt;/code&gt; instead of &lt;code&gt;python-magic&lt;/code&gt; to reduce installation issues.&lt;/li&gt; 
    &lt;li&gt;Retested PP-OCRv5 performance metrics with PaddleOCR version 3.1.0 and updated the documentation.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;2025.06.29: PaddleOCR 3.1.0 Released&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Key Models and Pipelines:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Added PP-OCRv5 Multilingual Text Recognition Model&lt;/strong&gt;, which supports the training and inference process for text recognition models in 37 languages, including French, Spanish, Portuguese, Russian, Korean, etc. &lt;strong&gt;Average accuracy improved by over 30%.&lt;/strong&gt; &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/algorithm/PP-OCRv5/PP-OCRv5_multi_languages.html"&gt;Details&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;Upgraded the &lt;strong&gt;PP-Chart2Table model&lt;/strong&gt; in PP-StructureV3, further enhancing the capability of converting charts to tables. On internal custom evaluation sets, the metric (RMS-F1) &lt;strong&gt;increased by 9.36 percentage points (71.24% -&amp;gt; 80.60%).&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;Newly launched &lt;strong&gt;document translation pipeline, PP-DocTranslation, based on PP-StructureV3 and ERNIE 4.5&lt;/strong&gt;, which supports the translation of Markdown format documents, various complex-layout PDF documents, and document images, with the results saved as Markdown format documents. &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/pipeline_usage/PP-DocTranslation.html"&gt;Details&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;New MCP server:&lt;/strong&gt; &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/mcp_server.html"&gt;Details&lt;/a&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Supports both OCR and PP-StructureV3 pipelines.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;Supports three working modes: local Python library, AIStudio Community Cloud Service, and self-hosted service.&lt;/li&gt; 
    &lt;li&gt;Supports invoking local services via stdio and remote services via Streamable HTTP.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Documentation Optimization:&lt;/strong&gt; Improved the descriptions in some user guides for a smoother reading experience.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;2025.06.26: PaddleOCR 3.0.3 Released&lt;/strong&gt;&lt;/summary&gt; - Bug Fix: Resolved the issue where the `enable_mkldnn` parameter was not effective, restoring the default behavior of using MKL-DNN for CPU inference. 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;2025.06.19: PaddleOCR 3.0.2 Released&lt;/strong&gt;&lt;/summary&gt; - **New Features:** 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;The default download source has been changed from &lt;code&gt;BOS&lt;/code&gt; to &lt;code&gt;HuggingFace&lt;/code&gt;. Users can also change the environment variable &lt;code&gt;PADDLE_PDX_MODEL_SOURCE&lt;/code&gt; to &lt;code&gt;BOS&lt;/code&gt; to set the model download source back to Baidu Object Storage (BOS).&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Added service invocation examples for six languages—C++, Java, Go, C#, Node.js, and PHP—for pipelines like PP-OCRv5, PP-StructureV3, and PP-ChatOCRv4.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Improved the layout partition sorting algorithm in the PP-StructureV3 pipeline, enhancing the sorting logic for complex vertical layouts to deliver better results.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Enhanced model selection logic: when a language is specified but a model version is not, the system will automatically select the latest model version supporting that language.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Set a default upper limit for MKL-DNN cache size to prevent unlimited growth, while also allowing users to configure cache capacity.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Updated default configurations for high-performance inference to support Paddle MKL-DNN acceleration and optimized the logic for automatic configuration selection for smarter choices.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Adjusted the logic for obtaining the default device to consider the actual support for computing devices by the installed Paddle framework, making program behavior more intuitive.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Added Android example for PP-OCRv5. &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/on_device_deployment.html"&gt;Details&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Bug Fixes:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Fixed an issue with some CLI parameters in PP-StructureV3 not taking effect.&lt;/li&gt; 
    &lt;li&gt;Resolved an issue where &lt;code&gt;export_paddlex_config_to_yaml&lt;/code&gt; would not function correctly in certain cases.&lt;/li&gt; 
    &lt;li&gt;Corrected the discrepancy between the actual behavior of &lt;code&gt;save_path&lt;/code&gt; and its documentation description.&lt;/li&gt; 
    &lt;li&gt;Fixed potential multithreading errors when using MKL-DNN in basic service deployment.&lt;/li&gt; 
    &lt;li&gt;Corrected channel order errors in image preprocessing for the Latex-OCR model.&lt;/li&gt; 
    &lt;li&gt;Fixed channel order errors in saving visualized images within the text recognition module.&lt;/li&gt; 
    &lt;li&gt;Resolved channel order errors in visualized table results within PP-StructureV3 pipeline.&lt;/li&gt; 
    &lt;li&gt;Fixed an overflow issue in the calculation of &lt;code&gt;overlap_ratio&lt;/code&gt; under extremely special circumstances in the PP-StructureV3 pipeline.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Documentation Improvements:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Updated the description of the &lt;code&gt;enable_mkldnn&lt;/code&gt; parameter in the documentation to accurately reflect the program's actual behavior.&lt;/li&gt; 
    &lt;li&gt;Fixed errors in the documentation regarding the &lt;code&gt;lang&lt;/code&gt; and &lt;code&gt;ocr_version&lt;/code&gt; parameters.&lt;/li&gt; 
    &lt;li&gt;Added instructions for exporting pipeline configuration files via CLI.&lt;/li&gt; 
    &lt;li&gt;Fixed missing columns in the performance data table for PP-OCRv5.&lt;/li&gt; 
    &lt;li&gt;Refined benchmark metrics for PP-StructureV3 across different configurations.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Others:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Relaxed version restrictions on dependencies like numpy and pandas, restoring support for Python 3.12.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;History Log&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;2025.06.05: &lt;strong&gt;PaddleOCR 3.0.1 Released&lt;/strong&gt;, includes:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Optimisation of certain models and model configurations:&lt;/strong&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Updated the default model configuration for PP-OCRv5, changing both detection and recognition from mobile to server models. To improve default performance in most scenarios, the parameter &lt;code&gt;limit_side_len&lt;/code&gt; in the configuration has been changed from 736 to 64.&lt;/li&gt; 
    &lt;li&gt;Added a new text line orientation classification model &lt;code&gt;PP-LCNet_x1_0_textline_ori&lt;/code&gt; with an accuracy of 99.42%. The default text line orientation classifier for OCR, PP-StructureV3, and PP-ChatOCRv4 pipelines has been updated to this model.&lt;/li&gt; 
    &lt;li&gt;Optimized the text line orientation classification model &lt;code&gt;PP-LCNet_x0_25_textline_ori&lt;/code&gt;, improving accuracy by 3.3 percentage points to a current accuracy of 98.85%.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Optimizations and fixes for some issues in version 3.0.0, &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/update/update.html"&gt;details&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;🔥🔥2025.05.20: Official Release of &lt;strong&gt;PaddleOCR v3.0&lt;/strong&gt;, including:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-OCRv5&lt;/strong&gt;: High-Accuracy Text Recognition Model for All Scenarios - Instant Text from Images/PDFs.&lt;/p&gt; 
   &lt;ol&gt; 
    &lt;li&gt;🌐 Single-model support for &lt;strong&gt;five&lt;/strong&gt; text types - Seamlessly process &lt;strong&gt;Simplified Chinese, Traditional Chinese, Simplified Chinese Pinyin, English&lt;/strong&gt; and &lt;strong&gt;Japanese&lt;/strong&gt; within a single model.&lt;/li&gt; 
    &lt;li&gt;✍️ Improved &lt;strong&gt;handwriting recognition&lt;/strong&gt;: Significantly better at complex cursive scripts and non-standard handwriting.&lt;/li&gt; 
    &lt;li&gt;🎯 &lt;strong&gt;13-point accuracy gain&lt;/strong&gt; over PP-OCRv4, achieving state-of-the-art performance across a variety of real-world scenarios.&lt;/li&gt; 
   &lt;/ol&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-StructureV3&lt;/strong&gt;: General-Purpose Document Parsing – Unleash SOTA Images/PDFs Parsing for Real-World Scenarios!&lt;/p&gt; 
   &lt;ol&gt; 
    &lt;li&gt;🧮 &lt;strong&gt;High-Accuracy multi-scene PDF parsing&lt;/strong&gt;, leading both open- and closed-source solutions on the OmniDocBench benchmark.&lt;/li&gt; 
    &lt;li&gt;🧠 Specialized capabilities include &lt;strong&gt;seal recognition&lt;/strong&gt;, &lt;strong&gt;chart-to-table conversion&lt;/strong&gt;, &lt;strong&gt;table recognition with nested formulas/images&lt;/strong&gt;, &lt;strong&gt;vertical text document parsing&lt;/strong&gt;, and &lt;strong&gt;complex table structure analysis&lt;/strong&gt;.&lt;/li&gt; 
   &lt;/ol&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-ChatOCRv4&lt;/strong&gt;: Intelligent Document Understanding – Extract Key Information, not just text from Images/PDFs.&lt;/p&gt; 
   &lt;ol&gt; 
    &lt;li&gt;🔥 &lt;strong&gt;15-point accuracy gain&lt;/strong&gt; in key-information extraction on PDF/PNG/JPG files over the previous generation.&lt;/li&gt; 
    &lt;li&gt;💻 Native support for &lt;strong&gt;ERNIE 4.5&lt;/strong&gt;, with compatibility for large-model deployments via PaddleNLP, Ollama, vLLM, and more.&lt;/li&gt; 
    &lt;li&gt;🤝 Integrated &lt;a href="https://github.com/PaddlePaddle/PaddleMIX/tree/develop/paddlemix/examples/ppdocbee2"&gt;PP-DocBee2&lt;/a&gt;, enabling extraction and understanding of printed text, handwriting, seals, tables, charts, and other common elements in complex documents.&lt;/li&gt; 
   &lt;/ol&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/update/update.html"&gt;History Log&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;⚡ Quick Start&lt;/h2&gt; 
&lt;h3&gt;1. Run online demo&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aistudio.baidu.com/community/app/91660/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP_OCRv5-AI_Studio-green" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://aistudio.baidu.com/community/app/518494/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP_StructureV3-AI_Studio-green" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://aistudio.baidu.com/community/app/518493/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP_ChatOCRv4-AI_Studio-green" alt="AI Studio" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;2. Installation&lt;/h3&gt; 
&lt;p&gt;Install PaddlePaddle refer to &lt;a href="https://www.paddlepaddle.org.cn/en/install/quick?docurl=/documentation/docs/en/develop/install/pip/linux-pip_en.html"&gt;Installation Guide&lt;/a&gt;, after then, install the PaddleOCR toolkit.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# If you only want to use the basic text recognition feature (returns text position coordinates and content), including the PP-OCR series
python -m pip install paddleocr
# If you want to use all features such as document parsing, document understanding, document translation, key information extraction, etc.
# python -m pip install "paddleocr[all]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Starting from version 3.2.0, in addition to the &lt;code&gt;all&lt;/code&gt; dependency group demonstrated above, PaddleOCR also supports installing partial optional features by specifying other dependency groups. All dependency groups provided by PaddleOCR are as follows:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Dependency Group Name&lt;/th&gt; 
   &lt;th&gt;Corresponding Functionality&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;doc-parser&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Document parsing: can be used to extract layout elements such as tables, formulas, stamps, images, etc. from documents; includes models like PP-StructureV3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ie&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Information extraction: can be used to extract key information from documents, such as names, dates, addresses, amounts, etc.; includes models like PP-ChatOCRv4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;trans&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Document translation: can be used to translate documents from one language to another; includes models like PP-DocTranslation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;all&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Complete functionality&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;3. Run inference by CLI&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run PP-OCRv5 inference
paddleocr ocr -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_ocr_002.png --use_doc_orientation_classify False --use_doc_unwarping False --use_textline_orientation False  

# Run PP-StructureV3 inference
paddleocr pp_structurev3 -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/pp_structure_v3_demo.png --use_doc_orientation_classify False --use_doc_unwarping False

# Get the Qianfan API Key at first, and then run PP-ChatOCRv4 inference
paddleocr pp_chatocrv4_doc -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/vehicle_certificate-1.png -k 驾驶室准乘人数 --qianfan_api_key your_api_key --use_doc_orientation_classify False --use_doc_unwarping False 

# Run PaddleOCR-VL inference
paddleocr doc_parser -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/paddleocr_vl_demo.png

# Get more information about "paddleocr ocr"
paddleocr ocr --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;4. Run inference by API&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;4.1 PP-OCRv5 Example&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Initialize PaddleOCR instance
from paddleocr import PaddleOCR
ocr = PaddleOCR(
    use_doc_orientation_classify=False,
    use_doc_unwarping=False,
    use_textline_orientation=False)

# Run OCR inference on a sample image 
result = ocr.predict(
    input="https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_ocr_002.png")

# Visualize the results and save the JSON results
for res in result:
    res.print()
    res.save_to_img("output")
    res.save_to_json("output")
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;4.2 PP-StructureV3 Example&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from pathlib import Path
from paddleocr import PPStructureV3

pipeline = PPStructureV3(
    use_doc_orientation_classify=False,
    use_doc_unwarping=False
)

# For Image
output = pipeline.predict(
    input="https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/pp_structure_v3_demo.png",
)

# Visualize the results and save the JSON results
for res in output:
    res.print() 
    res.save_to_json(save_path="output") 
    res.save_to_markdown(save_path="output")           
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;4.3 PP-ChatOCRv4 Example&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from paddleocr import PPChatOCRv4Doc

chat_bot_config = {
    "module_name": "chat_bot",
    "model_name": "ernie-3.5-8k",
    "base_url": "https://qianfan.baidubce.com/v2",
    "api_type": "openai",
    "api_key": "api_key",  # your api_key
}

retriever_config = {
    "module_name": "retriever",
    "model_name": "embedding-v1",
    "base_url": "https://qianfan.baidubce.com/v2",
    "api_type": "qianfan",
    "api_key": "api_key",  # your api_key
}

pipeline = PPChatOCRv4Doc(
    use_doc_orientation_classify=False,
    use_doc_unwarping=False
)

visual_predict_res = pipeline.visual_predict(
    input="https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/vehicle_certificate-1.png",
    use_common_ocr=True,
    use_seal_recognition=True,
    use_table_recognition=True,
)

mllm_predict_info = None
use_mllm = False
# If a multimodal large model is used, the local mllm service needs to be started. You can refer to the documentation: https://github.com/PaddlePaddle/PaddleX/blob/release/3.0/docs/pipeline_usage/tutorials/vlm_pipelines/doc_understanding.en.md performs deployment and updates the mllm_chat_bot_config configuration.
if use_mllm:
    mllm_chat_bot_config = {
        "module_name": "chat_bot",
        "model_name": "PP-DocBee",
        "base_url": "http://127.0.0.1:8080/",  # your local mllm service url
        "api_type": "openai",
        "api_key": "api_key",  # your api_key
    }

    mllm_predict_res = pipeline.mllm_pred(
        input="https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/vehicle_certificate-1.png",
        key_list=["驾驶室准乘人数"],
        mllm_chat_bot_config=mllm_chat_bot_config,
    )
    mllm_predict_info = mllm_predict_res["mllm_res"]

visual_info_list = []
for res in visual_predict_res:
    visual_info_list.append(res["visual_info"])
    layout_parsing_result = res["layout_parsing_result"]

vector_info = pipeline.build_vector(
    visual_info_list, flag_save_bytes_vector=True, retriever_config=retriever_config
)
chat_result = pipeline.chat(
    key_list=["驾驶室准乘人数"],
    visual_info=visual_info_list,
    vector_info=vector_info,
    mllm_predict_info=mllm_predict_info,
    chat_bot_config=chat_bot_config,
    retriever_config=retriever_config,
)
print(chat_result)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;4.4 PaddleOCR-VL Example&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from paddleocr import PaddleOCRVL

pipeline = PaddleOCRVL()
output = pipeline.predict("https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/paddleocr_vl_demo.png")
for res in output:
    res.print()
    res.save_to_json(save_path="output")
    res.save_to_markdown(save_path="output")
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;5. Chinese Heterogeneous AI Accelerators&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/other_devices_support/paddlepaddle_install_NPU.html"&gt;Huawei Ascend&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/other_devices_support/paddlepaddle_install_XPU.html"&gt;KUNLUNXIN&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🧩 More Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Convert models to ONNX format: &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/obtaining_onnx_models.html"&gt;Obtaining ONNX Models&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Accelerate inference using engines like OpenVINO, ONNX Runtime, TensorRT, or perform inference using ONNX format models: &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/high_performance_inference.html"&gt;High-Performance Inference&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Accelerate inference using multi-GPU and multi-process: &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/pipeline_usage/instructions/parallel_inference.html"&gt;Parallel Inference for Pipelines&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Integrate PaddleOCR into applications written in C++, C#, Java, etc.: &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/serving.html"&gt;Serving&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;⛰️ Advanced Tutorials&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/pipeline_usage/OCR.html"&gt;PP-OCRv5 Tutorial&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/pipeline_usage/PP-StructureV3.html"&gt;PP-StructureV3 Tutorial&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/pipeline_usage/PP-ChatOCRv4.html"&gt;PP-ChatOCRv4 Tutorial&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/pipeline_usage/PaddleOCR-VL.html"&gt;PaddleOCR-VL Tutorial&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🔄 Quick Overview of Execution Results&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/docs/images/demo.gif" alt="PP-OCRv5 Demo" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/docs/images/blue_v3.gif" alt="PP-StructureV3 Demo" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;✨ Stay Tuned&lt;/h2&gt; 
&lt;p&gt;⭐ &lt;strong&gt;Star this repository to keep up with exciting updates and new releases, including powerful OCR and document parsing capabilities!&lt;/strong&gt; ⭐&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="1200" src="https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/main/images/paddleocr/README/star_paddleocr.en.gif" alt="Star-Project" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;👩‍👩‍👧‍👦 Community&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;PaddlePaddle WeChat official account&lt;/th&gt; 
    &lt;th align="center"&gt;Join the tech discussion group&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/refs/heads/main/images/paddleocr/README/qrcode_for_paddlepaddle_official_account.jpg" width="150" /&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/refs/heads/main/images/paddleocr/README/qr_code_for_the_questionnaire.jpg" width="150" /&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;😃 Awesome Projects Leveraging PaddleOCR&lt;/h2&gt; 
&lt;p&gt;PaddleOCR wouldn't be where it is today without its incredible community! 💗 A massive thank you to all our longtime partners, new collaborators, and everyone who's poured their passion into PaddleOCR — whether we've named you or not. Your support fuels our fire!&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Project Name&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/infiniflow/ragflow"&gt;RAGFlow&lt;/a&gt; &lt;a href="https://github.com/infiniflow/ragflow"&gt;&lt;img src="https://img.shields.io/github/stars/infiniflow/ragflow" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;RAG engine based on deep document understanding.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/opendatalab/MinerU"&gt;MinerU&lt;/a&gt; &lt;a href="https://github.com/opendatalab/MinerU"&gt;&lt;img src="https://img.shields.io/github/stars/opendatalab/MinerU" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Multi-type Document to Markdown Conversion Tool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/hiroi-sora/Umi-OCR"&gt;Umi-OCR&lt;/a&gt; &lt;a href="https://github.com/hiroi-sora/Umi-OCR"&gt;&lt;img src="https://img.shields.io/github/stars/hiroi-sora/Umi-OCR" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Free, Open-source, Batch Offline OCR Software.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/OmniParser"&gt;OmniParser&lt;/a&gt;&lt;a href="https://github.com/microsoft/OmniParser"&gt;&lt;img src="https://img.shields.io/github/stars/microsoft/OmniParser" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;OmniParser: Screen Parsing tool for Pure Vision Based GUI Agent.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/netease-youdao/QAnything"&gt;QAnything&lt;/a&gt;&lt;a href="https://github.com/netease-youdao/QAnything"&gt;&lt;img src="https://img.shields.io/github/stars/netease-youdao/QAnything" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Question and Answer based on Anything.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/opendatalab/PDF-Extract-Kit"&gt;PDF-Extract-Kit&lt;/a&gt; &lt;a href="https://github.com/opendatalab/PDF-Extract-Kit"&gt;&lt;img src="https://img.shields.io/github/stars/opendatalab/PDF-Extract-Kit" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;A powerful open-source toolkit designed to efficiently extract high-quality content from complex and diverse PDF documents.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/PantsuDango/Dango-Translator"&gt;Dango-Translator&lt;/a&gt;&lt;a href="https://github.com/PantsuDango/Dango-Translator"&gt;&lt;img src="https://img.shields.io/github/stars/PantsuDango/Dango-Translator" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Recognize text on the screen, translate it and show the translation results in real time.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/awesome_projects.md"&gt;Learn more projects&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/awesome_projects.md"&gt;More projects based on PaddleOCR&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;👩‍👩‍👧‍👦 Contributors&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/PaddlePaddle/PaddleOCR/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=PaddlePaddle/PaddleOCR&amp;amp;max=400&amp;amp;columns=20" width="800" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;🌟 Star&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="800" src="https://api.star-history.com/svg?repos=PaddlePaddle/PaddleOCR&amp;amp;type=Date" alt="Star-history" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;📄 License&lt;/h2&gt; 
&lt;p&gt;This project is released under the &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/LICENSE"&gt;Apache 2.0 license&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;🎓 Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;@misc{cui2025paddleocr30technicalreport,
      title={PaddleOCR 3.0 Technical Report}, 
      author={Cheng Cui and Ting Sun and Manhui Lin and Tingquan Gao and Yubo Zhang and Jiaxuan Liu and Xueqing Wang and Zelun Zhang and Changda Zhou and Hongen Liu and Yue Zhang and Wenyu Lv and Kui Huang and Yichao Zhang and Jing Zhang and Jun Zhang and Yi Liu and Dianhai Yu and Yanjun Ma},
      year={2025},
      eprint={2507.05595},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2507.05595}, 
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>guofei9987/blind_watermark</title>
      <link>https://github.com/guofei9987/blind_watermark</link>
      <description>&lt;p&gt;Blind&amp;Invisible Watermark ，图片盲水印，提取水印无须原图！&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;blind-watermark&lt;/h1&gt; 
&lt;p&gt;Blind watermark based on DWT-DCT-SVD.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://pypi.org/project/blind_watermark/"&gt;&lt;img src="https://img.shields.io/pypi/v/blind_watermark" alt="PyPI" /&gt;&lt;/a&gt; &lt;a href="https://travis-ci.com/guofei9987/blind_watermark"&gt;&lt;img src="https://travis-ci.com/guofei9987/blind_watermark.svg?branch=master" alt="Build Status" /&gt;&lt;/a&gt; &lt;a href="https://codecov.io/gh/guofei9987/blind_watermark"&gt;&lt;img src="https://codecov.io/gh/guofei9987/blind_watermark/branch/master/graph/badge.svg?sanitize=true" alt="codecov" /&gt;&lt;/a&gt; &lt;a href="https://github.com/guofei9987/blind_watermark/raw/master/LICENSE"&gt;&lt;img src="https://img.shields.io/pypi/l/blind_watermark.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/badge/python-%3E=3.5-green.svg?sanitize=true" alt="Python" /&gt; &lt;img src="https://img.shields.io/badge/platform-windows%20%7C%20linux%20%7C%20macos-green.svg?sanitize=true" alt="Platform" /&gt; &lt;a href="https://github.com/guofei9987/blind_watermark/"&gt;&lt;img src="https://img.shields.io/github/stars/guofei9987/blind_watermark.svg?style=social" alt="stars" /&gt;&lt;/a&gt; &lt;a href="https://github.com/guofei9987/blind_watermark/fork"&gt;&lt;img src="https://img.shields.io/github/forks/guofei9987/blind_watermark?style=social" alt="fork" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/blind-watermark"&gt;&lt;img src="https://pepy.tech/badge/blind-watermark" alt="Downloads" /&gt;&lt;/a&gt; &lt;a href="https://github.com/guofei9987/blind_watermark/discussions"&gt;&lt;img src="https://img.shields.io/badge/discussions-green.svg?sanitize=true" alt="Discussions" /&gt;&lt;/a&gt; &lt;a href="https://hellogithub.com/repository/guofei9987/blind_watermark" target="_blank"&gt;&lt;img src="https://abroad.hellogithub.com/v1/widgets/recommend.svg?rid=3834302ff46a40f188a651ef8bd26ff5&amp;amp;claim_uid=se0WHo8cbiLv2w1&amp;amp;theme=small" alt="Featured｜HelloGitHub" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Documentation:&lt;/strong&gt; &lt;a href="https://BlindWatermark.github.io/blind_watermark/#/en/"&gt;https://BlindWatermark.github.io/blind_watermark/#/en/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;文档：&lt;/strong&gt; &lt;a href="https://BlindWatermark.github.io/blind_watermark/#/zh/"&gt;https://BlindWatermark.github.io/blind_watermark/#/zh/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;中文 readme&lt;/strong&gt; &lt;a href="https://raw.githubusercontent.com/guofei9987/blind_watermark/master/README_cn.md"&gt;README_cn.md&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Source code:&lt;/strong&gt; &lt;a href="https://github.com/guofei9987/blind_watermark"&gt;https://github.com/guofei9987/blind_watermark&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;install&lt;/h1&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install blind-watermark
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For the current developer version:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bach"&gt;git clone git@github.com:guofei9987/blind_watermark.git
cd blind_watermark
pip install .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;How to use&lt;/h1&gt; 
&lt;h2&gt;Use in bash&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# embed watermark into image:
blind_watermark --embed --pwd 1234 examples/pic/ori_img.jpeg "watermark text" examples/output/embedded.png
# extract watermark from image:
blind_watermark --extract --pwd 1234 --wm_shape 111 examples/output/embedded.png
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Use in Python&lt;/h2&gt; 
&lt;p&gt;Original Image + Watermark = Watermarked Image&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/guofei9987/blind_watermark/master/docs/%E5%8E%9F%E5%9B%BE.jpeg" alt="origin_image" /&gt; + '@guofei9987 开源万岁！' = &lt;img src="https://raw.githubusercontent.com/guofei9987/blind_watermark/master/docs/%E6%89%93%E4%B8%8A%E6%B0%B4%E5%8D%B0%E7%9A%84%E5%9B%BE.jpg" alt="打上水印的图" /&gt;&lt;/p&gt; 
&lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/guofei9987/blind_watermark/master/examples/example_str.py"&gt;codes&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Embed watermark:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from blind_watermark import WaterMark

bwm1 = WaterMark(password_img=1, password_wm=1)
bwm1.read_img('pic/ori_img.jpg')
wm = '@guofei9987 开源万岁！'
bwm1.read_wm(wm, mode='str')
bwm1.embed('output/embedded.png')
len_wm = len(bwm1.wm_bit)
print('Put down the length of wm_bit {len_wm}'.format(len_wm=len_wm))
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Extract watermark:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;bwm1 = WaterMark(password_img=1, password_wm=1)
wm_extract = bwm1.extract('output/embedded.png', wm_shape=len_wm, mode='str')
print(wm_extract)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Output:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;@guofei9987 开源万岁！&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;attacks on Watermarked Image&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;attack method&lt;/th&gt; 
   &lt;th&gt;image after attack&lt;/th&gt; 
   &lt;th&gt;extracted watermark&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Rotate 45 Degrees&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/guofei9987/blind_watermark/master/docs/%E6%97%8B%E8%BD%AC%E6%94%BB%E5%87%BB.jpg" alt="旋转攻击" /&gt;&lt;/td&gt; 
   &lt;td&gt;'@guofei9987 开源万岁！'&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Random crop&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/guofei9987/blind_watermark/master/docs/%E6%88%AA%E5%B1%8F%E6%94%BB%E5%87%BB2_%E8%BF%98%E5%8E%9F.jpg" alt="截屏攻击" /&gt;&lt;/td&gt; 
   &lt;td&gt;'@guofei9987 开源万岁！'&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Masks&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/guofei9987/blind_watermark/master/docs/%E5%A4%9A%E9%81%AE%E6%8C%A1%E6%94%BB%E5%87%BB.jpg" alt="多遮挡攻击" /&gt;&lt;/td&gt; 
   &lt;td&gt;'@guofei9987 开源万岁！'&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Vertical cut&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/guofei9987/blind_watermark/master/docs/%E6%A8%AA%E5%90%91%E8%A3%81%E5%89%AA%E6%94%BB%E5%87%BB_%E5%A1%AB%E8%A1%A5.jpg" alt="横向裁剪攻击" /&gt;&lt;/td&gt; 
   &lt;td&gt;'@guofei9987 开源万岁！'&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Horizontal cut&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/guofei9987/blind_watermark/master/docs/%E7%BA%B5%E5%90%91%E8%A3%81%E5%89%AA%E6%94%BB%E5%87%BB_%E5%A1%AB%E8%A1%A5.jpg" alt="纵向裁剪攻击" /&gt;&lt;/td&gt; 
   &lt;td&gt;'@guofei9987 开源万岁！'&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Resize&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/guofei9987/blind_watermark/master/docs/%E7%BC%A9%E6%94%BE%E6%94%BB%E5%87%BB.jpg" alt="缩放攻击" /&gt;&lt;/td&gt; 
   &lt;td&gt;'@guofei9987 开源万岁！'&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Pepper Noise&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/guofei9987/blind_watermark/master/docs/%E6%A4%92%E7%9B%90%E6%94%BB%E5%87%BB.jpg" alt="椒盐攻击" /&gt;&lt;/td&gt; 
   &lt;td&gt;'@guofei9987 开源万岁！'&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Brightness 10% Down&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/guofei9987/blind_watermark/master/docs/%E4%BA%AE%E5%BA%A6%E6%94%BB%E5%87%BB.jpg" alt="亮度攻击" /&gt;&lt;/td&gt; 
   &lt;td&gt;'@guofei9987 开源万岁！'&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;embed images&lt;/h3&gt; 
&lt;p&gt;embed watermark:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from blind_watermark import WaterMark

bwm1 = WaterMark(password_wm=1, password_img=1)
# read original image
bwm1.read_img('pic/ori_img.jpg')
# read watermark
bwm1.read_wm('pic/watermark.png')
# embed
bwm1.embed('output/embedded.png')
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Extract watermark:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;bwm1 = WaterMark(password_wm=1, password_img=1)
# notice that wm_shape is necessary
bwm1.extract(filename='output/embedded.png', wm_shape=(128, 128), out_wm_name='output/extracted.png', )
&lt;/code&gt;&lt;/pre&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;attack method&lt;/th&gt; 
   &lt;th&gt;image after attack&lt;/th&gt; 
   &lt;th&gt;extracted watermark&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Rotate 45 Degrees&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/guofei9987/blind_watermark/master/docs/%E6%97%8B%E8%BD%AC%E6%94%BB%E5%87%BB.jpg" alt="旋转攻击" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/guofei9987/blind_watermark/master/docs/%E6%97%8B%E8%BD%AC%E6%94%BB%E5%87%BB_%E6%8F%90%E5%8F%96%E6%B0%B4%E5%8D%B0.png" alt="" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Random crop&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/guofei9987/blind_watermark/master/docs/%E6%88%AA%E5%B1%8F%E6%94%BB%E5%87%BB2_%E8%BF%98%E5%8E%9F.jpg" alt="截屏攻击" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/guofei9987/blind_watermark/master/docs/%E5%A4%9A%E9%81%AE%E6%8C%A1%E6%94%BB%E5%87%BB_%E6%8F%90%E5%8F%96%E6%B0%B4%E5%8D%B0.png" alt="多遮挡_提取水印" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Mask&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/guofei9987/blind_watermark/master/docs/%E5%A4%9A%E9%81%AE%E6%8C%A1%E6%94%BB%E5%87%BB.jpg" alt="多遮挡攻击" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/guofei9987/blind_watermark/master/docs/%E5%A4%9A%E9%81%AE%E6%8C%A1%E6%94%BB%E5%87%BB_%E6%8F%90%E5%8F%96%E6%B0%B4%E5%8D%B0.png" alt="多遮挡_提取水印" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;embed array of bits&lt;/h3&gt; 
&lt;p&gt;See it &lt;a href="https://raw.githubusercontent.com/guofei9987/blind_watermark/master/examples/example_bit.py"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;As demo, we embed 6 bytes data:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;wm = [True, False, True, True, True, False]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Embed:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from blind_watermark import WaterMark

bwm1 = WaterMark(password_img=1, password_wm=1)
bwm1.read_ori_img('pic/ori_img.jpg')
bwm1.read_wm([True, False, True, True, True, False], mode='bit')
bwm1.embed('output/embedded.png')
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Extract:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;bwm1 = WaterMark(password_img=1, password_wm=1, wm_shape=6)
wm_extract = bwm1.extract('output/打上水印的图.png', mode='bit')
print(wm_extract)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Notice that &lt;code&gt;wm_shape&lt;/code&gt; (shape of watermark) is necessary&lt;/p&gt; 
&lt;p&gt;The output &lt;code&gt;wm_extract&lt;/code&gt; is an array of float. set a threshold such as 0.5.&lt;/p&gt; 
&lt;h1&gt;Concurrency&lt;/h1&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;WaterMark(..., processes=None)
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;processes&lt;/code&gt; number of processes, can be integer. Default &lt;code&gt;None&lt;/code&gt;, which means using all processes.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Related Project&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;text_blind_watermark (Embed message into text): &lt;a href="https://github.com/guofei9987/text_blind_watermark"&gt;https://github.com/guofei9987/text_blind_watermark&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;HideInfo（hide as image, hide as sounds, hide as text）：&lt;a href="https://github.com/guofei9987/HideInfo"&gt;https://github.com/guofei9987/HideInfo&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>m-bain/whisperX</title>
      <link>https://github.com/m-bain/whisperX</link>
      <description>&lt;p&gt;WhisperX: Automatic Speech Recognition with Word-level Timestamps (&amp; Diarization)&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt;WhisperX&lt;/h1&gt; 
&lt;h2&gt;Recall.ai - Meeting Transcription API&lt;/h2&gt; 
&lt;p&gt;If you’re looking for a transcription API for meetings, consider checking out &lt;a href="https://www.recall.ai/product/meeting-transcription-api?utm_source=github&amp;amp;utm_medium=sponsorship&amp;amp;utm_campaign=mbain-whisperx"&gt;Recall.ai's Meeting Transcription API&lt;/a&gt;, an API that works with Zoom, Google Meet, Microsoft Teams, and more. Recall.ai diarizes by pulling the speaker data and separate audio streams from the meeting platforms, which means 100% accurate speaker diarization with actual speaker names.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/m-bain/whisperX/stargazers"&gt; &lt;img src="https://img.shields.io/github/stars/m-bain/whisperX.svg?colorA=orange&amp;amp;colorB=orange&amp;amp;logo=github" alt="GitHub stars" /&gt; &lt;/a&gt; &lt;a href="https://github.com/m-bain/whisperX/issues"&gt; &lt;img src="https://img.shields.io/github/issues/m-bain/whisperx.svg?sanitize=true" alt="GitHub issues" /&gt; &lt;/a&gt; &lt;a href="https://github.com/m-bain/whisperX/raw/master/LICENSE"&gt; &lt;img src="https://img.shields.io/github/license/m-bain/whisperX.svg?sanitize=true" alt="GitHub license" /&gt; &lt;/a&gt; &lt;a href="https://arxiv.org/abs/2303.00747"&gt; &lt;img src="http://img.shields.io/badge/Arxiv-2303.00747-B31B1B.svg?sanitize=true" alt="ArXiv paper" /&gt; &lt;/a&gt; &lt;a href="https://twitter.com/intent/tweet?text=&amp;amp;url=https%3A%2F%2Fgithub.com%2Fm-bain%2FwhisperX"&gt; &lt;img src="https://img.shields.io/twitter/url/https/github.com/m-bain/whisperX.svg?style=social" alt="Twitter" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;img width="1216" align="center" alt="whisperx-arch" src="https://raw.githubusercontent.com/m-bain/whisperX/refs/heads/main/figures/pipeline.png" /&gt; 
&lt;!-- &lt;p align="left"&gt;Whisper-Based Automatic Speech Recognition (ASR) with improved timestamp accuracy + quality via forced phoneme alignment and voice-activity based batching for fast inference.&lt;/p&gt; --&gt; 
&lt;!-- &lt;h2 align="left", id="what-is-it"&gt;What is it 🔎&lt;/h2&gt; --&gt; 
&lt;p&gt;This repository provides fast automatic speech recognition (70x realtime with large-v2) with word-level timestamps and speaker diarization.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;⚡️ Batched inference for 70x realtime transcription using whisper large-v2&lt;/li&gt; 
 &lt;li&gt;🪶 &lt;a href="https://github.com/guillaumekln/faster-whisper"&gt;faster-whisper&lt;/a&gt; backend, requires &amp;lt;8GB gpu memory for large-v2 with beam_size=5&lt;/li&gt; 
 &lt;li&gt;🎯 Accurate word-level timestamps using wav2vec2 alignment&lt;/li&gt; 
 &lt;li&gt;👯‍♂️ Multispeaker ASR using speaker diarization from &lt;a href="https://github.com/pyannote/pyannote-audio"&gt;pyannote-audio&lt;/a&gt; (speaker ID labels)&lt;/li&gt; 
 &lt;li&gt;🗣️ VAD preprocessing, reduces hallucination &amp;amp; batching with no WER degradation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Whisper&lt;/strong&gt; is an ASR model &lt;a href="https://github.com/openai/whisper"&gt;developed by OpenAI&lt;/a&gt;, trained on a large dataset of diverse audio. Whilst it does produces highly accurate transcriptions, the corresponding timestamps are at the utterance-level, not per word, and can be inaccurate by several seconds. OpenAI's whisper does not natively support batching.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Phoneme-Based ASR&lt;/strong&gt; A suite of models finetuned to recognise the smallest unit of speech distinguishing one word from another, e.g. the element p in "tap". A popular example model is &lt;a href="https://huggingface.co/facebook/wav2vec2-large-960h-lv60-self"&gt;wav2vec2.0&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Forced Alignment&lt;/strong&gt; refers to the process by which orthographic transcriptions are aligned to audio recordings to automatically generate phone level segmentation.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Voice Activity Detection (VAD)&lt;/strong&gt; is the detection of the presence or absence of human speech.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Speaker Diarization&lt;/strong&gt; is the process of partitioning an audio stream containing human speech into homogeneous segments according to the identity of each speaker.&lt;/p&gt; 
&lt;h2 align="left" , id="highlights"&gt;New🚨&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;1st place at &lt;a href="https://eval.ai/web/challenges/challenge-page/1637/leaderboard/3931/WER"&gt;Ego4d transcription challenge&lt;/a&gt; 🏆&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;WhisperX&lt;/em&gt; accepted at INTERSPEECH 2023&lt;/li&gt; 
 &lt;li&gt;v3 transcript segment-per-sentence: using nltk sent_tokenize for better subtitlting &amp;amp; better diarization&lt;/li&gt; 
 &lt;li&gt;v3 released, 70x speed-up open-sourced. Using batched whisper with &lt;a href="https://github.com/guillaumekln/faster-whisper"&gt;faster-whisper&lt;/a&gt; backend!&lt;/li&gt; 
 &lt;li&gt;v2 released, code cleanup, imports whisper library VAD filtering is now turned on by default, as in the paper.&lt;/li&gt; 
 &lt;li&gt;Paper drop🎓👨‍🏫! Please see our &lt;a href="https://arxiv.org/abs/2303.00747"&gt;ArxiV preprint&lt;/a&gt; for benchmarking and details of WhisperX. We also introduce more efficient batch inference resulting in large-v2 with *60-70x REAL TIME speed.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2 align="left" id="setup"&gt;Setup ⚙️&lt;/h2&gt; 
&lt;h3&gt;0. CUDA Installation&lt;/h3&gt; 
&lt;p&gt;To use WhisperX with GPU acceleration, install the CUDA toolkit 12.8 before WhisperX. Skip this step if using only the CPU.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;For &lt;strong&gt;Linux&lt;/strong&gt; users, install the CUDA toolkit 12.8 following this guide: &lt;a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/"&gt;CUDA Installation Guide for Linux&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;For &lt;strong&gt;Windows&lt;/strong&gt; users, download and install the CUDA toolkit 12.8: &lt;a href="https://developer.nvidia.com/cuda-12-8-1-download-archive"&gt;CUDA Downloads&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;1. Simple Installation (Recommended)&lt;/h3&gt; 
&lt;p&gt;The easiest way to install WhisperX is through PyPi:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install whisperx
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or if using &lt;a href="https://docs.astral.sh/uv/guides/tools/#running-tools"&gt;uvx&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uvx whisperx
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Advanced Installation Options&lt;/h3&gt; 
&lt;p&gt;These installation methods are for developers or users with specific needs. If you're not sure, stick with the simple installation above.&lt;/p&gt; 
&lt;h4&gt;Option A: Install from GitHub&lt;/h4&gt; 
&lt;p&gt;To install directly from the GitHub repository:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uvx git+https://github.com/m-bain/whisperX.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Option B: Developer Installation&lt;/h4&gt; 
&lt;p&gt;If you want to modify the code or contribute to the project:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/m-bain/whisperX.git
cd whisperX
uv sync --all-extras --dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The development version may contain experimental features and bugs. Use the stable PyPI release for production environments.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;You may also need to install ffmpeg, rust etc. Follow openAI instructions here &lt;a href="https://github.com/openai/whisper#setup"&gt;https://github.com/openai/whisper#setup&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Speaker Diarization&lt;/h3&gt; 
&lt;p&gt;To &lt;strong&gt;enable Speaker Diarization&lt;/strong&gt;, include your Hugging Face access token (read) that you can generate from &lt;a href="https://huggingface.co/settings/tokens"&gt;Here&lt;/a&gt; after the &lt;code&gt;--hf_token&lt;/code&gt; argument and accept the user agreement for the following models: &lt;a href="https://huggingface.co/pyannote/segmentation-3.0"&gt;Segmentation&lt;/a&gt; and &lt;a href="https://huggingface.co/pyannote/speaker-diarization-3.1"&gt;Speaker-Diarization-3.1&lt;/a&gt; (if you choose to use Speaker-Diarization 2.x, follow requirements &lt;a href="https://huggingface.co/pyannote/speaker-diarization"&gt;here&lt;/a&gt; instead.)&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;&lt;br /&gt; As of Oct 11, 2023, there is a known issue regarding slow performance with pyannote/Speaker-Diarization-3.0 in whisperX. It is due to dependency conflicts between faster-whisper and pyannote-audio 3.0.0. Please see &lt;a href="https://github.com/m-bain/whisperX/issues/499"&gt;this issue&lt;/a&gt; for more details and potential workarounds.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2 align="left" id="example"&gt;Usage 💬 (command line)&lt;/h2&gt; 
&lt;h3&gt;English&lt;/h3&gt; 
&lt;p&gt;Run whisper on example segment (using default params, whisper small) add &lt;code&gt;--highlight_words True&lt;/code&gt; to visualise word timings in the .srt file.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;whisperx path/to/audio.wav
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Result using &lt;em&gt;WhisperX&lt;/em&gt; with forced alignment to wav2vec2.0 large:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://user-images.githubusercontent.com/36994049/208253969-7e35fe2a-7541-434a-ae91-8e919540555d.mp4"&gt;https://user-images.githubusercontent.com/36994049/208253969-7e35fe2a-7541-434a-ae91-8e919540555d.mp4&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Compare this to original whisper out the box, where many transcriptions are out of sync:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://user-images.githubusercontent.com/36994049/207743923-b4f0d537-29ae-4be2-b404-bb941db73652.mov"&gt;https://user-images.githubusercontent.com/36994049/207743923-b4f0d537-29ae-4be2-b404-bb941db73652.mov&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;For increased timestamp accuracy, at the cost of higher gpu mem, use bigger models (bigger alignment model not found to be that helpful, see paper) e.g.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;whisperx path/to/audio.wav --model large-v2 --align_model WAV2VEC2_ASR_LARGE_LV60K_960H --batch_size 4
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To label the transcript with speaker ID's (set number of speakers if known e.g. &lt;code&gt;--min_speakers 2&lt;/code&gt; &lt;code&gt;--max_speakers 2&lt;/code&gt;):&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;whisperx path/to/audio.wav --model large-v2 --diarize --highlight_words True
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To run on CPU instead of GPU (and for running on Mac OS X):&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;whisperx path/to/audio.wav --compute_type int8 --device cpu
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Other languages&lt;/h3&gt; 
&lt;p&gt;The phoneme ASR alignment model is &lt;em&gt;language-specific&lt;/em&gt;, for tested languages these models are &lt;a href="https://github.com/m-bain/whisperX/raw/f2da2f858e99e4211fe4f64b5f2938b007827e17/whisperx/alignment.py#L24-L58"&gt;automatically picked from torchaudio pipelines or huggingface&lt;/a&gt;. Just pass in the &lt;code&gt;--language&lt;/code&gt; code, and use the whisper &lt;code&gt;--model large&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Currently default models provided for &lt;code&gt;{en, fr, de, es, it}&lt;/code&gt; via torchaudio pipelines and many other languages via Hugging Face. Please find the list of currently supported languages under &lt;code&gt;DEFAULT_ALIGN_MODELS_HF&lt;/code&gt; on &lt;a href="https://github.com/m-bain/whisperX/raw/main/whisperx/alignment.py"&gt;alignment.py&lt;/a&gt;. If the detected language is not in this list, you need to find a phoneme-based ASR model from &lt;a href="https://huggingface.co/models"&gt;huggingface model hub&lt;/a&gt; and test it on your data.&lt;/p&gt; 
&lt;h4&gt;E.g. German&lt;/h4&gt; 
&lt;pre&gt;&lt;code&gt;whisperx --model large-v2 --language de path/to/audio.wav
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://user-images.githubusercontent.com/36994049/208298811-e36002ba-3698-4731-97d4-0aebd07e0eb3.mov"&gt;https://user-images.githubusercontent.com/36994049/208298811-e36002ba-3698-4731-97d4-0aebd07e0eb3.mov&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;See more examples in other languages &lt;a href="https://raw.githubusercontent.com/m-bain/whisperX/main/EXAMPLES.md"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Python usage 🐍&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import whisperx
import gc
from whisperx.diarize import DiarizationPipeline

device = "cuda"
audio_file = "audio.mp3"
batch_size = 16 # reduce if low on GPU mem
compute_type = "float16" # change to "int8" if low on GPU mem (may reduce accuracy)

# 1. Transcribe with original whisper (batched)
model = whisperx.load_model("large-v2", device, compute_type=compute_type)

# save model to local path (optional)
# model_dir = "/path/"
# model = whisperx.load_model("large-v2", device, compute_type=compute_type, download_root=model_dir)

audio = whisperx.load_audio(audio_file)
result = model.transcribe(audio, batch_size=batch_size)
print(result["segments"]) # before alignment

# delete model if low on GPU resources
# import gc; import torch; gc.collect(); torch.cuda.empty_cache(); del model

# 2. Align whisper output
model_a, metadata = whisperx.load_align_model(language_code=result["language"], device=device)
result = whisperx.align(result["segments"], model_a, metadata, audio, device, return_char_alignments=False)

print(result["segments"]) # after alignment

# delete model if low on GPU resources
# import gc; import torch; gc.collect(); torch.cuda.empty_cache(); del model_a

# 3. Assign speaker labels
diarize_model = DiarizationPipeline(use_auth_token=YOUR_HF_TOKEN, device=device)

# add min/max number of speakers if known
diarize_segments = diarize_model(audio)
# diarize_model(audio, min_speakers=min_speakers, max_speakers=max_speakers)

result = whisperx.assign_word_speakers(diarize_segments, result)
print(diarize_segments)
print(result["segments"]) # segments are now assigned speaker IDs
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Demos 🚀&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://replicate.com/victor-upmeet/whisperx"&gt;&lt;img src="https://img.shields.io/static/v1?label=Replicate+WhisperX+large-v3&amp;amp;message=Demo+%26+Cloud+API&amp;amp;color=blue" alt="Replicate (large-v3" /&gt;&lt;/a&gt; &lt;a href="https://replicate.com/daanelson/whisperx"&gt;&lt;img src="https://img.shields.io/static/v1?label=Replicate+WhisperX+large-v2&amp;amp;message=Demo+%26+Cloud+API&amp;amp;color=blue" alt="Replicate (large-v2" /&gt;&lt;/a&gt; &lt;a href="https://replicate.com/carnifexer/whisperx"&gt;&lt;img src="https://img.shields.io/static/v1?label=Replicate+WhisperX+medium&amp;amp;message=Demo+%26+Cloud+API&amp;amp;color=blue" alt="Replicate (medium)" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;If you don't have access to your own GPUs, use the links above to try out WhisperX.&lt;/p&gt; 
&lt;h2 align="left" id="whisper-mod"&gt;Technical Details 👷‍♂️&lt;/h2&gt; 
&lt;p&gt;For specific details on the batching and alignment, the effect of VAD, as well as the chosen alignment model, see the preprint &lt;a href="https://www.robots.ox.ac.uk/~vgg/publications/2023/Bain23/bain23.pdf"&gt;paper&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To reduce GPU memory requirements, try any of the following (2. &amp;amp; 3. can affect quality):&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;reduce batch size, e.g. &lt;code&gt;--batch_size 4&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;use a smaller ASR model &lt;code&gt;--model base&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Use lighter compute type &lt;code&gt;--compute_type int8&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Transcription differences from openai's whisper:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Transcription without timestamps. To enable single pass batching, whisper inference is performed &lt;code&gt;--without_timestamps True&lt;/code&gt;, this ensures 1 forward pass per sample in the batch. However, this can cause discrepancies the default whisper output.&lt;/li&gt; 
 &lt;li&gt;VAD-based segment transcription, unlike the buffered transcription of openai's. In the WhisperX paper we show this reduces WER, and enables accurate batched inference&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--condition_on_prev_text&lt;/code&gt; is set to &lt;code&gt;False&lt;/code&gt; by default (reduces hallucination)&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2 align="left" id="limitations"&gt;Limitations ⚠️&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Transcript words which do not contain characters in the alignment models dictionary e.g. "2014." or "£13.60" cannot be aligned and therefore are not given a timing.&lt;/li&gt; 
 &lt;li&gt;Overlapping speech is not handled particularly well by whisper nor whisperx&lt;/li&gt; 
 &lt;li&gt;Diarization is far from perfect&lt;/li&gt; 
 &lt;li&gt;Language specific wav2vec2 model is needed&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2 align="left" id="contribute"&gt;Contribute 🧑‍🏫&lt;/h2&gt; 
&lt;p&gt;If you are multilingual, a major way you can contribute to this project is to find phoneme models on huggingface (or train your own) and test them on speech for the target language. If the results look good send a pull request and some examples showing its success.&lt;/p&gt; 
&lt;p&gt;Bug finding and pull requests are also highly appreciated to keep this project going, since it's already diverging from the original research scope.&lt;/p&gt; 
&lt;h2 align="left" id="coming-soon"&gt;TODO 🗓&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;p&gt;Multilingual init&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;p&gt;Automatic align model selection based on language detection&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;p&gt;Python usage&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;p&gt;Incorporating speaker diarization&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;p&gt;Model flush, for low gpu mem resources&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;p&gt;Faster-whisper backend&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;p&gt;Add max-line etc. see (openai's whisper utils.py)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;p&gt;Sentence-level segments (nltk toolbox)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;p&gt;Improve alignment logic&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;p&gt;update examples with diarization and word highlighting&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;p&gt;Subtitle .ass output &amp;lt;- bring this back (removed in v3)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;p&gt;Add benchmarking code (TEDLIUM for spd/WER &amp;amp; word segmentation)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;p&gt;Allow silero-vad as alternative VAD option&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;p&gt;Improve diarization (word level). &lt;em&gt;Harder than first thought...&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2 align="left" id="contact"&gt;Contact/Support 📇&lt;/h2&gt; 
&lt;p&gt;Contact &lt;a href="mailto:maxhbain@gmail.com"&gt;maxhbain@gmail.com&lt;/a&gt; for queries.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.buymeacoffee.com/maxhbain" target="_blank"&gt;&lt;img src="https://cdn.buymeacoffee.com/buttons/default-orange.png" alt="Buy Me A Coffee" height="41" width="174" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2 align="left" id="acks"&gt;Acknowledgements 🙏&lt;/h2&gt; 
&lt;p&gt;This work, and my PhD, is supported by the &lt;a href="https://www.robots.ox.ac.uk/~vgg/"&gt;VGG (Visual Geometry Group)&lt;/a&gt; and the University of Oxford.&lt;/p&gt; 
&lt;p&gt;Of course, this is builds on &lt;a href="https://github.com/openai/whisper"&gt;openAI's whisper&lt;/a&gt;. Borrows important alignment code from &lt;a href="https://pytorch.org/tutorials/intermediate/forced_alignment_with_torchaudio_tutorial.html"&gt;PyTorch tutorial on forced alignment&lt;/a&gt; And uses the wonderful pyannote VAD / Diarization &lt;a href="https://github.com/pyannote/pyannote-audio"&gt;https://github.com/pyannote/pyannote-audio&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Valuable VAD &amp;amp; Diarization Models from:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;[pyannote audio][https://github.com/pyannote/pyannote-audio]&lt;/li&gt; 
 &lt;li&gt;[silero vad][https://github.com/snakers4/silero-vad]&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Great backend from &lt;a href="https://github.com/guillaumekln/faster-whisper"&gt;faster-whisper&lt;/a&gt; and &lt;a href="https://github.com/OpenNMT/CTranslate2"&gt;CTranslate2&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Those who have &lt;a href="https://www.buymeacoffee.com/maxhbain"&gt;supported this work financially&lt;/a&gt; 🙏&lt;/p&gt; 
&lt;p&gt;Finally, thanks to the OS &lt;a href="https://github.com/m-bain/whisperX/graphs/contributors"&gt;contributors&lt;/a&gt; of this project, keeping it going and identifying bugs.&lt;/p&gt; 
&lt;h2 align="left" id="cite"&gt;Citation&lt;/h2&gt; If you use this in your research, please cite the paper: 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@article{bain2022whisperx,
  title={WhisperX: Time-Accurate Speech Transcription of Long-Form Audio},
  author={Bain, Max and Huh, Jaesung and Han, Tengda and Zisserman, Andrew},
  journal={INTERSPEECH 2023},
  year={2023}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Skyvern-AI/skyvern</title>
      <link>https://github.com/Skyvern-AI/skyvern</link>
      <description>&lt;p&gt;Automate browser-based workflows with LLMs and Computer Vision&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt; &lt;a href="https://www.skyvern.com"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="fern/images/skyvern_logo.png" /&gt; 
   &lt;img height="120" src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/skyvern_logo_blackbg.png" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; &lt;br /&gt; &lt;/h1&gt; 
&lt;p align="center"&gt; 🐉 Automate Browser-based workflows using LLMs and Computer Vision 🐉 &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://www.skyvern.com/"&gt;&lt;img src="https://img.shields.io/badge/Website-blue?logo=googlechrome&amp;amp;logoColor=black" /&gt;&lt;/a&gt; &lt;a href="https://www.skyvern.com/docs/"&gt;&lt;img src="https://img.shields.io/badge/Docs-yellow?logo=gitbook&amp;amp;logoColor=black" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/fG2XXEuQX3"&gt;&lt;img src="https://img.shields.io/discord/1212486326352617534?logo=discord&amp;amp;label=discord" /&gt;&lt;/a&gt; 
 &lt;!-- &lt;a href="https://pepy.tech/project/skyvern" target="_blank"&gt;&lt;img src="https://static.pepy.tech/badge/skyvern" alt="Total Downloads"/&gt;&lt;/a&gt; --&gt; &lt;a href="https://github.com/skyvern-ai/skyvern"&gt;&lt;img src="https://img.shields.io/github/stars/skyvern-ai/skyvern" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Skyvern-AI/skyvern/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/skyvern-ai/skyvern" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/skyvernai"&gt;&lt;img src="https://img.shields.io/twitter/follow/skyvernai?style=social" /&gt;&lt;/a&gt; &lt;a href="https://www.linkedin.com/company/95726232"&gt;&lt;img src="https://img.shields.io/badge/Follow%20 on%20LinkedIn-8A2BE2?logo=linkedin" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.skyvern.com"&gt;Skyvern&lt;/a&gt; automates browser-based workflows using LLMs and computer vision. It provides a simple API endpoint to fully automate manual workflows on a large number of websites, replacing brittle or unreliable automation solutions.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/geico_shu_recording_cropped.gif" /&gt; &lt;/p&gt; 
&lt;p&gt;Traditional approaches to browser automations required writing custom scripts for websites, often relying on DOM parsing and XPath-based interactions which would break whenever the website layouts changed.&lt;/p&gt; 
&lt;p&gt;Instead of only relying on code-defined XPath interactions, Skyvern relies on Vision LLMs to learn and interact with the websites.&lt;/p&gt; 
&lt;h1&gt;How it works&lt;/h1&gt; 
&lt;p&gt;Skyvern was inspired by the Task-Driven autonomous agent design popularized by &lt;a href="https://github.com/yoheinakajima/babyagi"&gt;BabyAGI&lt;/a&gt; and &lt;a href="https://github.com/Significant-Gravitas/AutoGPT"&gt;AutoGPT&lt;/a&gt; -- with one major bonus: we give Skyvern the ability to interact with websites using browser automation libraries like &lt;a href="https://playwright.dev/"&gt;Playwright&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Skyvern uses a swarm of agents to comprehend a website, and plan and execute its actions:&lt;/p&gt; 
&lt;picture&gt; 
 &lt;source media="(prefers-color-scheme: dark)" srcset="fern/images/skyvern_2_0_system_diagram.png" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/skyvern_2_0_system_diagram.png" /&gt; 
&lt;/picture&gt; 
&lt;p&gt;This approach has a few advantages:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Skyvern can operate on websites it's never seen before, as it's able to map visual elements to actions necessary to complete a workflow, without any customized code&lt;/li&gt; 
 &lt;li&gt;Skyvern is resistant to website layout changes, as there are no pre-determined XPaths or other selectors our system is looking for while trying to navigate&lt;/li&gt; 
 &lt;li&gt;Skyvern is able to take a single workflow and apply it to a large number of websites, as it's able to reason through the interactions necessary to complete the workflow&lt;/li&gt; 
 &lt;li&gt;Skyvern leverages LLMs to reason through interactions to ensure we can cover complex situations. Examples include: 
  &lt;ol&gt; 
   &lt;li&gt;If you wanted to get an auto insurance quote from Geico, the answer to a common question "Were you eligible to drive at 18?" could be inferred from the driver receiving their license at age 16&lt;/li&gt; 
   &lt;li&gt;If you were doing competitor analysis, it's understanding that an Arnold Palmer 22 oz can at 7/11 is almost definitely the same product as a 23 oz can at Gopuff (even though the sizes are slightly different, which could be a rounding error!)&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;A detailed technical report can be found &lt;a href="https://www.skyvern.com/blog/skyvern-2-0-state-of-the-art-web-navigation-with-85-8-on-webvoyager-eval/"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Demo&lt;/h1&gt; 
&lt;!-- Redo demo --&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/5cab4668-e8e2-4982-8551-aab05ff73a7f"&gt;https://github.com/user-attachments/assets/5cab4668-e8e2-4982-8551-aab05ff73a7f&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Performance &amp;amp; Evaluation&lt;/h1&gt; 
&lt;p&gt;Skyvern has SOTA performance on the &lt;a href="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/webbench.ai"&gt;WebBench benchmark&lt;/a&gt; with a 64.4% accuracy. The technical report + evaluation can be found &lt;a href="https://www.skyvern.com/blog/web-bench-a-new-way-to-compare-ai-browser-agents/"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/performance/webbench_overall.png" /&gt; &lt;/p&gt; 
&lt;h2&gt;Performance on WRITE tasks (eg filling out forms, logging in, downloading files, etc)&lt;/h2&gt; 
&lt;p&gt;Skyvern is the best performing agent on WRITE tasks (eg filling out forms, logging in, downloading files, etc), which is primarily used for RPA (Robotic Process Automation) adjacent tasks.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/performance/webbench_write.png" /&gt; &lt;/p&gt; 
&lt;h1&gt;Quickstart&lt;/h1&gt; 
&lt;h2&gt;Skyvern Cloud&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://app.skyvern.com"&gt;Skyvern Cloud&lt;/a&gt; is a managed cloud version of Skyvern that allows you to run Skyvern without worrying about the infrastructure. It allows you to run multiple Skyvern instances in parallel and comes bundled with anti-bot detection mechanisms, proxy network, and CAPTCHA solvers.&lt;/p&gt; 
&lt;p&gt;If you'd like to try it out, navigate to &lt;a href="https://app.skyvern.com"&gt;app.skyvern.com&lt;/a&gt; and create an account.&lt;/p&gt; 
&lt;h2&gt;Install &amp;amp; Run&lt;/h2&gt; 
&lt;p&gt;Dependencies needed:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.python.org/downloads/"&gt;Python 3.11.x&lt;/a&gt;, works with 3.12, not ready yet for 3.13&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nodejs.org/en/download/"&gt;NodeJS &amp;amp; NPM&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Additionally, for Windows:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://rustup.rs/"&gt;Rust&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;VS Code with C++ dev tools and Windows SDK&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;1. Install Skyvern&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install skyvern
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Run Skyvern&lt;/h3&gt; 
&lt;p&gt;This is most helpful for first time run (db setup, db migrations etc).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;skyvern quickstart
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. Run task&lt;/h3&gt; 
&lt;h4&gt;UI (Recommended)&lt;/h4&gt; 
&lt;p&gt;Start the Skyvern service and UI (when DB is up and running)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;skyvern run all
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Go to &lt;a href="http://localhost:8080"&gt;http://localhost:8080&lt;/a&gt; and use the UI to run a task&lt;/p&gt; 
&lt;h4&gt;Code&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from skyvern import Skyvern

skyvern = Skyvern()
task = await skyvern.run_task(prompt="Find the top post on hackernews today")
print(task)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Skyvern starts running the task in a browser that pops up and closes it when the task is done. You will be able to view the task from &lt;a href="http://localhost:8080/history"&gt;http://localhost:8080/history&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;You can also run a task on different targets:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from skyvern import Skyvern

# Run on Skyvern Cloud
skyvern = Skyvern(api_key="SKYVERN API KEY")

# Local Skyvern service
skyvern = Skyvern(base_url="http://localhost:8000", api_key="LOCAL SKYVERN API KEY")

task = await skyvern.run_task(prompt="Find the top post on hackernews today")
print(task)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Advanced Usage&lt;/h2&gt; 
&lt;h3&gt;Control your own browser (Chrome)&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;⚠️ WARNING: Since &lt;a href="https://developer.chrome.com/blog/remote-debugging-port"&gt;Chrome 136&lt;/a&gt;, Chrome refuses any CDP connect to the browser using the default user_data_dir. In order to use your browser data, Skyvern copies your default user_data_dir to &lt;code&gt;./tmp/user_data_dir&lt;/code&gt; the first time connecting to your local browser. ⚠️&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ol&gt; 
 &lt;li&gt;Just With Python Code&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from skyvern import Skyvern

# The path to your Chrome browser. This example path is for Mac.
browser_path = "/Applications/Google Chrome.app/Contents/MacOS/Google Chrome"
skyvern = Skyvern(
    base_url="http://localhost:8000",
    api_key="YOUR_API_KEY",
    browser_path=browser_path,
)
task = await skyvern.run_task(
    prompt="Find the top post on hackernews today",
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;With Skyvern Service&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Add two variables to your .env file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# The path to your Chrome browser. This example path is for Mac.
CHROME_EXECUTABLE_PATH="/Applications/Google Chrome.app/Contents/MacOS/Google Chrome"
BROWSER_TYPE=cdp-connect
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Restart Skyvern service &lt;code&gt;skyvern run all&lt;/code&gt; and run the task through UI or code&lt;/p&gt; 
&lt;h3&gt;Run Skyvern with any remote browser&lt;/h3&gt; 
&lt;p&gt;Grab the cdp connection url and pass it to Skyvern&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from skyvern import Skyvern

skyvern = Skyvern(cdp_url="your cdp connection url")
task = await skyvern.run_task(
    prompt="Find the top post on hackernews today",
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Get consistent output schema from your run&lt;/h3&gt; 
&lt;p&gt;You can do this by adding the &lt;code&gt;data_extraction_schema&lt;/code&gt; parameter:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from skyvern import Skyvern

skyvern = Skyvern()
task = await skyvern.run_task(
    prompt="Find the top post on hackernews today",
    data_extraction_schema={
        "type": "object",
        "properties": {
            "title": {
                "type": "string",
                "description": "The title of the top post"
            },
            "url": {
                "type": "string",
                "description": "The URL of the top post"
            },
            "points": {
                "type": "integer",
                "description": "Number of points the post has received"
            }
        }
    }
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Helpful commands to debug issues&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Launch the Skyvern Server Separately*
skyvern run server

# Launch the Skyvern UI
skyvern run ui

# Check status of the Skyvern service
skyvern status

# Stop the Skyvern service
skyvern stop all

# Stop the Skyvern UI
skyvern stop ui

# Stop the Skyvern Server Separately
skyvern stop server
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Docker Compose setup&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Make sure you have &lt;a href="https://www.docker.com/products/docker-desktop/"&gt;Docker Desktop&lt;/a&gt; installed and running on your machine&lt;/li&gt; 
 &lt;li&gt;Make sure you don't have postgres running locally (Run &lt;code&gt;docker ps&lt;/code&gt; to check)&lt;/li&gt; 
 &lt;li&gt;Clone the repository and navigate to the root directory&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;skyvern init llm&lt;/code&gt; to generate a &lt;code&gt;.env&lt;/code&gt; file. This will be copied into the Docker image.&lt;/li&gt; 
 &lt;li&gt;Fill in the LLM provider key on the &lt;a href="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/docker-compose.yml"&gt;docker-compose.yml&lt;/a&gt;. &lt;em&gt;If you want to run Skyvern on a remote server, make sure you set the correct server ip for the UI container in &lt;a href="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/docker-compose.yml"&gt;docker-compose.yml&lt;/a&gt;.&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;Run the following command via the commandline: &lt;pre&gt;&lt;code class="language-bash"&gt; docker compose up -d
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Navigate to &lt;code&gt;http://localhost:8080&lt;/code&gt; in your browser to start using the UI&lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Important:&lt;/strong&gt; Only one Postgres container can run on port 5432 at a time. If you switch from the CLI-managed Postgres to Docker Compose, you must first remove the original container:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;docker rm -f postgresql-container
&lt;/code&gt;&lt;/pre&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;If you encounter any database related errors while using Docker to run Skyvern, check which Postgres container is running with &lt;code&gt;docker ps&lt;/code&gt;.&lt;/p&gt; 
&lt;h1&gt;Skyvern Features&lt;/h1&gt; 
&lt;h2&gt;Skyvern Tasks&lt;/h2&gt; 
&lt;p&gt;Tasks are the fundamental building block inside Skyvern. Each task is a single request to Skyvern, instructing it to navigate through a website and accomplish a specific goal.&lt;/p&gt; 
&lt;p&gt;Tasks require you to specify a &lt;code&gt;url&lt;/code&gt;, &lt;code&gt;prompt&lt;/code&gt;, and can optionally include a &lt;code&gt;data schema&lt;/code&gt; (if you want the output to conform to a specific schema) and &lt;code&gt;error codes&lt;/code&gt; (if you want Skyvern to stop running in specific situations).&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/skyvern_2_0_screenshot.png" /&gt; &lt;/p&gt; 
&lt;h2&gt;Skyvern Workflows&lt;/h2&gt; 
&lt;p&gt;Workflows are a way to chain multiple tasks together to form a cohesive unit of work.&lt;/p&gt; 
&lt;p&gt;For example, if you wanted to download all invoices newer than January 1st, you could create a workflow that first navigated to the invoices page, then filtered down to only show invoices newer than January 1st, extracted a list of all eligible invoices, and iterated through each invoice to download it.&lt;/p&gt; 
&lt;p&gt;Another example is if you wanted to automate purchasing products from an e-commerce store, you could create a workflow that first navigated to the desired product, then added it to a cart. Second, it would navigate to the cart and validate the cart state. Finally, it would go through the checkout process to purchase the items.&lt;/p&gt; 
&lt;p&gt;Supported workflow features include:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Browser Task&lt;/li&gt; 
 &lt;li&gt;Browser Action&lt;/li&gt; 
 &lt;li&gt;Data Extraction&lt;/li&gt; 
 &lt;li&gt;Validation&lt;/li&gt; 
 &lt;li&gt;For Loops&lt;/li&gt; 
 &lt;li&gt;File parsing&lt;/li&gt; 
 &lt;li&gt;Sending emails&lt;/li&gt; 
 &lt;li&gt;Text Prompts&lt;/li&gt; 
 &lt;li&gt;HTTP Request Block&lt;/li&gt; 
 &lt;li&gt;Custom Code Block&lt;/li&gt; 
 &lt;li&gt;Uploading files to block storage&lt;/li&gt; 
 &lt;li&gt;(Coming soon) Conditionals&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/block_example_v2.png" /&gt; &lt;/p&gt; 
&lt;h2&gt;Livestreaming&lt;/h2&gt; 
&lt;p&gt;Skyvern allows you to livestream the viewport of the browser to your local machine so that you can see exactly what Skyvern is doing on the web. This is useful for debugging and understanding how Skyvern is interacting with a website, and intervening when necessary&lt;/p&gt; 
&lt;h2&gt;Form Filling&lt;/h2&gt; 
&lt;p&gt;Skyvern is natively capable of filling out form inputs on websites. Passing in information via the &lt;code&gt;navigation_goal&lt;/code&gt; will allow Skyvern to comprehend the information and fill out the form accordingly.&lt;/p&gt; 
&lt;h2&gt;Data Extraction&lt;/h2&gt; 
&lt;p&gt;Skyvern is also capable of extracting data from a website.&lt;/p&gt; 
&lt;p&gt;You can also specify a &lt;code&gt;data_extraction_schema&lt;/code&gt; directly within the main prompt to tell Skyvern exactly what data you'd like to extract from the website, in jsonc format. Skyvern's output will be structured in accordance to the supplied schema.&lt;/p&gt; 
&lt;h2&gt;File Downloading&lt;/h2&gt; 
&lt;p&gt;Skyvern is also capable of downloading files from a website. All downloaded files are automatically uploaded to block storage (if configured), and you can access them via the UI.&lt;/p&gt; 
&lt;h2&gt;Authentication&lt;/h2&gt; 
&lt;p&gt;Skyvern supports a number of different authentication methods to make it easier to automate tasks behind a login. If you'd like to try it out, please reach out to us &lt;a href="mailto:founders@skyvern.com"&gt;via email&lt;/a&gt; or &lt;a href="https://discord.gg/fG2XXEuQX3"&gt;discord&lt;/a&gt;.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/secure_password_task_example.png" /&gt; &lt;/p&gt; 
&lt;h3&gt;🔐 2FA Support (TOTP)&lt;/h3&gt; 
&lt;p&gt;Skyvern supports a number of different 2FA methods to allow you to automate workflows that require 2FA.&lt;/p&gt; 
&lt;p&gt;Examples include:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;QR-based 2FA (e.g. Google Authenticator, Authy)&lt;/li&gt; 
 &lt;li&gt;Email based 2FA&lt;/li&gt; 
 &lt;li&gt;SMS based 2FA&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;🔐 Learn more about 2FA support &lt;a href="https://www.skyvern.com/docs/credentials/totp"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Password Manager Integrations&lt;/h3&gt; 
&lt;p&gt;Skyvern currently supports the following password manager integrations:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Bitwarden&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; 1Password&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; LastPass&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Model Context Protocol (MCP)&lt;/h2&gt; 
&lt;p&gt;Skyvern supports the Model Context Protocol (MCP) to allow you to use any LLM that supports MCP.&lt;/p&gt; 
&lt;p&gt;See the MCP documentation &lt;a href="https://github.com/Skyvern-AI/skyvern/raw/main/integrations/mcp/README.md"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Zapier / Make.com / N8N Integration&lt;/h2&gt; 
&lt;p&gt;Skyvern supports Zapier, Make.com, and N8N to allow you to connect your Skyvern workflows to other apps.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.skyvern.com/docs/integrations/zapier"&gt;Zapier&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.skyvern.com/docs/integrations/make.com"&gt;Make.com&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.skyvern.com/docs/integrations/n8n"&gt;N8N&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;🔐 Learn more about 2FA support &lt;a href="https://www.skyvern.com/docs/credentials/totp"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Real-world examples of Skyvern&lt;/h1&gt; 
&lt;p&gt;We love to see how Skyvern is being used in the wild. Here are some examples of how Skyvern is being used to automate workflows in the real world. Please open PRs to add your own examples!&lt;/p&gt; 
&lt;h2&gt;Invoice Downloading on many different websites&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://meetings.hubspot.com/skyvern/demo"&gt;Book a demo to see it live&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/invoice_downloading.gif" /&gt; &lt;/p&gt; 
&lt;h2&gt;Automate the job application process&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://app.skyvern.com/tasks/create/job_application"&gt;💡 See it in action&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/job_application_demo.gif" /&gt; &lt;/p&gt; 
&lt;h2&gt;Automate materials procurement for a manufacturing company&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://app.skyvern.com/tasks/create/finditparts"&gt;💡 See it in action&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/finditparts_recording_crop.gif" /&gt; &lt;/p&gt; 
&lt;h2&gt;Navigating to government websites to register accounts or fill out forms&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://app.skyvern.com/tasks/create/california_edd"&gt;💡 See it in action&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/edd_services.gif" /&gt; &lt;/p&gt; 
&lt;!-- Add example of delaware entity lookups x2 --&gt; 
&lt;h2&gt;Filling out random contact us forms&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://app.skyvern.com/tasks/create/contact_us_forms"&gt;💡 See it in action&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/contact_forms.gif" /&gt; &lt;/p&gt; 
&lt;h2&gt;Retrieving insurance quotes from insurance providers in any language&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://app.skyvern.com/tasks/create/bci_seguros"&gt;💡 See it in action&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/bci_seguros_recording.gif" /&gt; &lt;/p&gt; 
&lt;p&gt;&lt;a href="https://app.skyvern.com/tasks/create/geico"&gt;💡 See it in action&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/geico_shu_recording_cropped.gif" /&gt; &lt;/p&gt; 
&lt;h1&gt;Contributor Setup&lt;/h1&gt; 
&lt;p&gt;Make sure to have &lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;uv&lt;/a&gt; installed.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Run this to create your virtual environment (&lt;code&gt;.venv&lt;/code&gt;) &lt;pre&gt;&lt;code class="language-bash"&gt;uv sync --group dev
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Perform initial server configuration &lt;pre&gt;&lt;code class="language-bash"&gt;uv run skyvern quickstart
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Navigate to &lt;code&gt;http://localhost:8080&lt;/code&gt; in your browser to start using the UI &lt;em&gt;The Skyvern CLI supports Windows, WSL, macOS, and Linux environments.&lt;/em&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h1&gt;Documentation&lt;/h1&gt; 
&lt;p&gt;More extensive documentation can be found on our &lt;a href="https://www.skyvern.com/docs"&gt;📕 docs page&lt;/a&gt;. Please let us know if something is unclear or missing by opening an issue or reaching out to us &lt;a href="mailto:founders@skyvern.com"&gt;via email&lt;/a&gt; or &lt;a href="https://discord.gg/fG2XXEuQX3"&gt;discord&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Supported LLMs&lt;/h1&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Provider&lt;/th&gt; 
   &lt;th&gt;Supported Models&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenAI&lt;/td&gt; 
   &lt;td&gt;gpt4-turbo, gpt-4o, gpt-4o-mini&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Anthropic&lt;/td&gt; 
   &lt;td&gt;Claude 3 (Haiku, Sonnet, Opus), Claude 3.5 (Sonnet)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Azure OpenAI&lt;/td&gt; 
   &lt;td&gt;Any GPT models. Better performance with a multimodal llm (azure/gpt4-o)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;AWS Bedrock&lt;/td&gt; 
   &lt;td&gt;Anthropic Claude 3 (Haiku, Sonnet, Opus), Claude 3.5 (Sonnet)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Gemini&lt;/td&gt; 
   &lt;td&gt;Gemini 2.5 Pro and flash, Gemini 2.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ollama&lt;/td&gt; 
   &lt;td&gt;Run any locally hosted model via &lt;a href="https://github.com/ollama/ollama"&gt;Ollama&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenRouter&lt;/td&gt; 
   &lt;td&gt;Access models through &lt;a href="https://openrouter.ai"&gt;OpenRouter&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenAI-compatible&lt;/td&gt; 
   &lt;td&gt;Any custom API endpoint that follows OpenAI's API format (via &lt;a href="https://docs.litellm.ai/docs/providers/openai_compatible"&gt;liteLLM&lt;/a&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h4&gt;Environment Variables&lt;/h4&gt; 
&lt;h5&gt;OpenAI&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ENABLE_OPENAI&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register OpenAI models&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;OpenAI API Key&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;sk-1234567890&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_API_BASE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;OpenAI API Base, optional&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;https://openai.api.base&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_ORGANIZATION&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;OpenAI Organization ID, optional&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;your-org-id&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Recommended &lt;code&gt;LLM_KEY&lt;/code&gt;: &lt;code&gt;OPENAI_GPT4O&lt;/code&gt;, &lt;code&gt;OPENAI_GPT4O_MINI&lt;/code&gt;, &lt;code&gt;OPENAI_GPT4_1&lt;/code&gt;, &lt;code&gt;OPENAI_O4_MINI&lt;/code&gt;, &lt;code&gt;OPENAI_O3&lt;/code&gt;&lt;/p&gt; 
&lt;h5&gt;Anthropic&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ENABLE_ANTHROPIC&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register Anthropic models&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ANTHROPIC_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Anthropic API key&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;sk-1234567890&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Recommended&lt;code&gt;LLM_KEY&lt;/code&gt;: &lt;code&gt;ANTHROPIC_CLAUDE3.5_SONNET&lt;/code&gt;, &lt;code&gt;ANTHROPIC_CLAUDE3.7_SONNET&lt;/code&gt;, &lt;code&gt;ANTHROPIC_CLAUDE4_OPUS&lt;/code&gt;, &lt;code&gt;ANTHROPIC_CLAUDE4_SONNET&lt;/code&gt;&lt;/p&gt; 
&lt;h5&gt;Azure OpenAI&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ENABLE_AZURE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register Azure OpenAI models&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;AZURE_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Azure deployment API key&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;sk-1234567890&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;AZURE_DEPLOYMENT&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Azure OpenAI Deployment Name&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;skyvern-deployment&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;AZURE_API_BASE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Azure deployment api base url&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;https://skyvern-deployment.openai.azure.com/&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;AZURE_API_VERSION&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Azure API Version&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;2024-02-01&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Recommended &lt;code&gt;LLM_KEY&lt;/code&gt;: &lt;code&gt;AZURE_OPENAI&lt;/code&gt;&lt;/p&gt; 
&lt;h5&gt;AWS Bedrock&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ENABLE_BEDROCK&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register AWS Bedrock models. To use AWS Bedrock, you need to make sure your &lt;a href="https://github.com/boto/boto3?tab=readme-ov-file#using-boto3"&gt;AWS configurations&lt;/a&gt; are set up correctly first.&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Recommended &lt;code&gt;LLM_KEY&lt;/code&gt;: &lt;code&gt;BEDROCK_ANTHROPIC_CLAUDE3.7_SONNET_INFERENCE_PROFILE&lt;/code&gt;, &lt;code&gt;BEDROCK_ANTHROPIC_CLAUDE4_OPUS_INFERENCE_PROFILE&lt;/code&gt;, &lt;code&gt;BEDROCK_ANTHROPIC_CLAUDE4_SONNET_INFERENCE_PROFILE&lt;/code&gt;&lt;/p&gt; 
&lt;h5&gt;Gemini&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ENABLE_GEMINI&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register Gemini models&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;GEMINI_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Gemini API Key&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;your_google_gemini_api_key&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Recommended &lt;code&gt;LLM_KEY&lt;/code&gt;: &lt;code&gt;GEMINI_2.5_PRO_PREVIEW&lt;/code&gt;, &lt;code&gt;GEMINI_2.5_FLASH_PREVIEW&lt;/code&gt;&lt;/p&gt; 
&lt;h5&gt;Ollama&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ENABLE_OLLAMA&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register local models via Ollama&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OLLAMA_SERVER_URL&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;URL for your Ollama server&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;http://host.docker.internal:11434&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OLLAMA_MODEL&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Ollama model name to load&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;qwen2.5:7b-instruct&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Recommended &lt;code&gt;LLM_KEY&lt;/code&gt;: &lt;code&gt;OLLAMA&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Note: Ollama does not support vision yet.&lt;/p&gt; 
&lt;h5&gt;OpenRouter&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ENABLE_OPENROUTER&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register OpenRouter models&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENROUTER_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;OpenRouter API key&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;sk-1234567890&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENROUTER_MODEL&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;OpenRouter model name&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;mistralai/mistral-small-3.1-24b-instruct&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENROUTER_API_BASE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;OpenRouter API base URL&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;https://api.openrouter.ai/v1&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Recommended &lt;code&gt;LLM_KEY&lt;/code&gt;: &lt;code&gt;OPENROUTER&lt;/code&gt;&lt;/p&gt; 
&lt;h5&gt;OpenAI-Compatible&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ENABLE_OPENAI_COMPATIBLE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register a custom OpenAI-compatible API endpoint&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_COMPATIBLE_MODEL_NAME&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Model name for OpenAI-compatible endpoint&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;yi-34b&lt;/code&gt;, &lt;code&gt;gpt-3.5-turbo&lt;/code&gt;, &lt;code&gt;mistral-large&lt;/code&gt;, etc.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_COMPATIBLE_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;API key for OpenAI-compatible endpoint&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;sk-1234567890&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_COMPATIBLE_API_BASE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Base URL for OpenAI-compatible endpoint&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;https://api.together.xyz/v1&lt;/code&gt;, &lt;code&gt;http://localhost:8000/v1&lt;/code&gt;, etc.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_COMPATIBLE_API_VERSION&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;API version for OpenAI-compatible endpoint, optional&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;2023-05-15&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_COMPATIBLE_MAX_TOKENS&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Maximum tokens for completion, optional&lt;/td&gt; 
   &lt;td&gt;Integer&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;4096&lt;/code&gt;, &lt;code&gt;8192&lt;/code&gt;, etc.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_COMPATIBLE_TEMPERATURE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Temperature setting, optional&lt;/td&gt; 
   &lt;td&gt;Float&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;0.0&lt;/code&gt;, &lt;code&gt;0.5&lt;/code&gt;, &lt;code&gt;0.7&lt;/code&gt;, etc.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_COMPATIBLE_SUPPORTS_VISION&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Whether model supports vision, optional&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Supported LLM Key: &lt;code&gt;OPENAI_COMPATIBLE&lt;/code&gt;&lt;/p&gt; 
&lt;h5&gt;General LLM Configuration&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;LLM_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The name of the model you want to use&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;See supported LLM keys above&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;SECONDARY_LLM_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The name of the model for mini agents skyvern runs with&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;See supported LLM keys above&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;LLM_CONFIG_MAX_TOKENS&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Override the max tokens used by the LLM&lt;/td&gt; 
   &lt;td&gt;Integer&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;128000&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;Feature Roadmap&lt;/h1&gt; 
&lt;p&gt;This is our planned roadmap for the next few months. If you have any suggestions or would like to see a feature added, please don't hesitate to reach out to us &lt;a href="mailto:founders@skyvern.com"&gt;via email&lt;/a&gt; or &lt;a href="https://discord.gg/fG2XXEuQX3"&gt;discord&lt;/a&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Open Source&lt;/strong&gt; - Open Source Skyvern's core codebase&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Workflow support&lt;/strong&gt; - Allow support to chain multiple Skyvern calls together&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Improved context&lt;/strong&gt; - Improve Skyvern's ability to understand content around interactable elements by introducing feeding relevant label context through the text prompt&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Cost Savings&lt;/strong&gt; - Improve Skyvern's stability and reduce the cost of running Skyvern by optimizing the context tree passed into Skyvern&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Self-serve UI&lt;/strong&gt; - Deprecate the Streamlit UI in favour of a React-based UI component that allows users to kick off new jobs in Skyvern&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Workflow UI Builder&lt;/strong&gt; - Introduce a UI to allow users to build and analyze workflows visually&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Chrome Viewport streaming&lt;/strong&gt; - Introduce a way to live-stream the Chrome viewport to the user's browser (as a part of the self-serve UI)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Past Runs UI&lt;/strong&gt; - Deprecate the Streamlit UI in favour of a React-based UI that allows you to visualize past runs and their results&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Auto workflow builder ("Observer") mode&lt;/strong&gt; - Allow Skyvern to auto-generate workflows as it's navigating the web to make it easier to build new workflows&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Prompt Caching&lt;/strong&gt; - Introduce a caching layer to the LLM calls to dramatically reduce the cost of running Skyvern (memorize past actions and repeat them!)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Web Evaluation Dataset&lt;/strong&gt; - Integrate Skyvern with public benchmark tests to track the quality of our models over time&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;strong&gt;Improved Debug mode&lt;/strong&gt; - Allow Skyvern to plan its actions and get "approval" before running them, allowing you to debug what it's doing and more easily iterate on the prompt&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;strong&gt;Chrome Extension&lt;/strong&gt; - Allow users to interact with Skyvern through a Chrome extension (incl voice mode, saving tasks, etc.)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;strong&gt;Skyvern Action Recorder&lt;/strong&gt; - Allow Skyvern to watch a user complete a task and then automatically generate a workflow for it&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;strong&gt;Interactable Livestream&lt;/strong&gt; - Allow users to interact with the livestream in real-time to intervene when necessary (such as manually submitting sensitive forms)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;strong&gt;Integrate LLM Observability tools&lt;/strong&gt; - Integrate LLM Observability tools to allow back-testing prompt changes with specific data sets + visualize the performance of Skyvern over time&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Langchain Integration&lt;/strong&gt; - Create langchain integration in langchain_community to use Skyvern as a "tool".&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Contributing&lt;/h1&gt; 
&lt;p&gt;We welcome PRs and suggestions! Don't hesitate to open a PR/issue or to reach out to us &lt;a href="mailto:founders@skyvern.com"&gt;via email&lt;/a&gt; or &lt;a href="https://discord.gg/fG2XXEuQX3"&gt;discord&lt;/a&gt;. Please have a look at our &lt;a href="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/CONTRIBUTING.md"&gt;contribution guide&lt;/a&gt; and &lt;a href="https://github.com/skyvern-ai/skyvern/issues?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22"&gt;"Help Wanted" issues&lt;/a&gt; to get started!&lt;/p&gt; 
&lt;p&gt;If you want to chat with the skyvern repository to get a high level overview of how it is structured, how to build off it, and how to resolve usage questions, check out &lt;a href="https://sage.storia.ai?utm_source=github&amp;amp;utm_medium=referral&amp;amp;utm_campaign=skyvern-readme"&gt;Code Sage&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Telemetry&lt;/h1&gt; 
&lt;p&gt;By Default, Skyvern collects basic usage statistics to help us understand how Skyvern is being used. If you would like to opt-out of telemetry, please set the &lt;code&gt;SKYVERN_TELEMETRY&lt;/code&gt; environment variable to &lt;code&gt;false&lt;/code&gt;.&lt;/p&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;p&gt;Skyvern's open source repository is supported via a managed cloud. All of the core logic powering Skyvern is available in this open source repository licensed under the &lt;a href="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/LICENSE"&gt;AGPL-3.0 License&lt;/a&gt;, with the exception of anti-bot measures available in our managed cloud offering.&lt;/p&gt; 
&lt;p&gt;If you have any questions or concerns around licensing, please &lt;a href="mailto:support@skyvern.com"&gt;contact us&lt;/a&gt; and we would be happy to help.&lt;/p&gt; 
&lt;h1&gt;Star History&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#Skyvern-AI/skyvern&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=Skyvern-AI/skyvern&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>huggingface/lerobot</title>
      <link>https://github.com/huggingface/lerobot</link>
      <description>&lt;p&gt;🤗 LeRobot: Making AI for Robotics more accessible with end-to-end learning&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img alt="LeRobot, Hugging Face Robotics Library" src="https://raw.githubusercontent.com/huggingface/lerobot/main/media/lerobot-logo-thumbnail.png" width="100%" /&gt; &lt;br /&gt; &lt;br /&gt; &lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/huggingface/lerobot/actions/workflows/nightly.yml?query=branch%3Amain"&gt;&lt;img src="https://github.com/huggingface/lerobot/actions/workflows/nightly.yml/badge.svg?branch=main" alt="Tests" /&gt;&lt;/a&gt; &lt;a href="https://www.python.org/downloads/"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/lerobot" alt="Python versions" /&gt;&lt;/a&gt; &lt;a href="https://github.com/huggingface/lerobot/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/lerobot/"&gt;&lt;img src="https://img.shields.io/pypi/status/lerobot" alt="Status" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/lerobot/"&gt;&lt;img src="https://img.shields.io/pypi/v/lerobot" alt="Version" /&gt;&lt;/a&gt; &lt;a href="https://github.com/huggingface/lerobot/raw/main/CODE_OF_CONDUCT.md"&gt;&lt;img src="https://img.shields.io/badge/Contributor%20Covenant-v2.1-ff69b4.svg?sanitize=true" alt="Contributor Covenant" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/s3KuuzsPFb"&gt;&lt;img src="https://dcbadge.vercel.app/api/server/C5P34WJ68S?style=flat" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;!-- [![Coverage](https://codecov.io/gh/huggingface/lerobot/branch/main/graph/badge.svg?token=TODO)](https://codecov.io/gh/huggingface/lerobot) --&gt; 
&lt;/div&gt; 
&lt;h2 align="center"&gt; &lt;p&gt;&lt;a href="https://huggingface.co/docs/lerobot/hope_jr"&gt; Build Your Own HopeJR Robot!&lt;/a&gt;&lt;/p&gt; &lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/huggingface/lerobot/main/media/hope_jr/hopejr.png" alt="HopeJR robot" title="HopeJR robot" width="60%" /&gt; 
 &lt;p&gt;&lt;strong&gt;Meet HopeJR – A humanoid robot arm and hand for dexterous manipulation!&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Control it with exoskeletons and gloves for precise hand movements.&lt;/p&gt; 
 &lt;p&gt;Perfect for advanced manipulation tasks! 🤖&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://huggingface.co/docs/lerobot/hope_jr"&gt; See the full HopeJR tutorial here.&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;h2 align="center"&gt; &lt;p&gt;&lt;a href="https://huggingface.co/docs/lerobot/so101"&gt; Build Your Own SO-101 Robot!&lt;/a&gt;&lt;/p&gt; &lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/huggingface/lerobot/main/media/so101/so101.webp" alt="SO-101 follower arm" title="SO-101 follower arm" width="90%" /&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/huggingface/lerobot/main/media/so101/so101-leader.webp" alt="SO-101 leader arm" title="SO-101 leader arm" width="90%" /&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
 &lt;p&gt;&lt;strong&gt;Meet the updated SO100, the SO-101 – Just €114 per arm!&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Train it in minutes with a few simple moves on your laptop.&lt;/p&gt; 
 &lt;p&gt;Then sit back and watch your creation act autonomously! 🤯&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://huggingface.co/docs/lerobot/so101"&gt; See the full SO-101 tutorial here.&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;Want to take it to the next level? Make your SO-101 mobile by building LeKiwi!&lt;/p&gt; 
 &lt;p&gt;Check out the &lt;a href="https://huggingface.co/docs/lerobot/lekiwi"&gt;LeKiwi tutorial&lt;/a&gt; and bring your robot to life on wheels.&lt;/p&gt; 
 &lt;img src="https://raw.githubusercontent.com/huggingface/lerobot/main/media/lekiwi/kiwi.webp" alt="LeKiwi mobile robot" title="LeKiwi mobile robot" width="50%" /&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;h3 align="center"&gt; &lt;p&gt;LeRobot: State-of-the-art AI for real-world robotics&lt;/p&gt; &lt;/h3&gt; 
&lt;hr /&gt; 
&lt;p&gt;🤗 LeRobot aims to provide models, datasets, and tools for real-world robotics in PyTorch. The goal is to lower the barrier to entry to robotics so that everyone can contribute and benefit from sharing datasets and pretrained models.&lt;/p&gt; 
&lt;p&gt;🤗 LeRobot contains state-of-the-art approaches that have been shown to transfer to the real-world with a focus on imitation learning and reinforcement learning.&lt;/p&gt; 
&lt;p&gt;🤗 LeRobot already provides a set of pretrained models, datasets with human collected demonstrations, and simulation environments to get started without assembling a robot. In the coming weeks, the plan is to add more and more support for real-world robotics on the most affordable and capable robots out there.&lt;/p&gt; 
&lt;p&gt;🤗 LeRobot hosts pretrained models and datasets on this Hugging Face community page: &lt;a href="https://huggingface.co/lerobot"&gt;huggingface.co/lerobot&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;Examples of pretrained models on simulation environments&lt;/h4&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/huggingface/lerobot/main/media/gym/aloha_act.gif" width="100%" alt="ACT policy on ALOHA env" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/huggingface/lerobot/main/media/gym/simxarm_tdmpc.gif" width="100%" alt="TDMPC policy on SimXArm env" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/huggingface/lerobot/main/media/gym/pusht_diffusion.gif" width="100%" alt="Diffusion policy on PushT env" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;ACT policy on ALOHA env&lt;/td&gt; 
   &lt;td align="center"&gt;TDMPC policy on SimXArm env&lt;/td&gt; 
   &lt;td align="center"&gt;Diffusion policy on PushT env&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;LeRobot works with Python 3.10+ and PyTorch 2.2+.&lt;/p&gt; 
&lt;h3&gt;Environment Setup&lt;/h3&gt; 
&lt;p&gt;Create a virtual environment with Python 3.10 and activate it, e.g. with &lt;a href="https://conda-forge.org/download/"&gt;&lt;code&gt;miniforge&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;conda create -y -n lerobot python=3.10
conda activate lerobot
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;When using &lt;code&gt;conda&lt;/code&gt;, install &lt;code&gt;ffmpeg&lt;/code&gt; in your environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;conda install ffmpeg -c conda-forge
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; This usually installs &lt;code&gt;ffmpeg 7.X&lt;/code&gt; for your platform compiled with the &lt;code&gt;libsvtav1&lt;/code&gt; encoder. If &lt;code&gt;libsvtav1&lt;/code&gt; is not supported (check supported encoders with &lt;code&gt;ffmpeg -encoders&lt;/code&gt;), you can:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;em&gt;[On any platform]&lt;/em&gt; Explicitly install &lt;code&gt;ffmpeg 7.X&lt;/code&gt; using:&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;conda install ffmpeg=7.1.1 -c conda-forge
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;em&gt;[On Linux only]&lt;/em&gt; Install &lt;a href="https://trac.ffmpeg.org/wiki/CompilationGuide/Ubuntu#GettheDependencies"&gt;ffmpeg build dependencies&lt;/a&gt; and &lt;a href="https://trac.ffmpeg.org/wiki/CompilationGuide/Ubuntu#libsvtav1"&gt;compile ffmpeg from source with libsvtav1&lt;/a&gt;, and make sure you use the corresponding ffmpeg binary to your install with &lt;code&gt;which ffmpeg&lt;/code&gt;.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Install LeRobot 🤗&lt;/h3&gt; 
&lt;h4&gt;From Source&lt;/h4&gt; 
&lt;p&gt;First, clone the repository and navigate into the directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/huggingface/lerobot.git
cd lerobot
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then, install the library in editable mode. This is useful if you plan to contribute to the code.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; If you encounter build errors, you may need to install additional dependencies (&lt;code&gt;cmake&lt;/code&gt;, &lt;code&gt;build-essential&lt;/code&gt;, and &lt;code&gt;ffmpeg libs&lt;/code&gt;). On Linux, run: &lt;code&gt;sudo apt-get install cmake build-essential python3-dev pkg-config libavformat-dev libavcodec-dev libavdevice-dev libavutil-dev libswscale-dev libswresample-dev libavfilter-dev&lt;/code&gt;. For other systems, see: &lt;a href="https://pyav.org/docs/develop/overview/installation.html#bring-your-own-ffmpeg"&gt;Compiling PyAV&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;For simulations, 🤗 LeRobot comes with gymnasium environments that can be installed as extras:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/huggingface/gym-aloha"&gt;aloha&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/huggingface/gym-xarm"&gt;xarm&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/huggingface/gym-pusht"&gt;pusht&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For instance, to install 🤗 LeRobot with aloha and pusht, use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -e ".[aloha, pusht]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Installation from PyPI&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Core Library:&lt;/strong&gt; Install the base package with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install lerobot
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;This installs only the default dependencies.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Extra Features:&lt;/strong&gt; To install additional functionality, use one of the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install 'lerobot[all]'          # All available features
pip install 'lerobot[aloha,pusht]'  # Specific features (Aloha &amp;amp; Pusht)
pip install 'lerobot[feetech]'      # Feetech motor support
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Replace &lt;code&gt;[...]&lt;/code&gt; with your desired features.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Available Tags:&lt;/strong&gt; For a full list of optional dependencies, see: &lt;a href="https://pypi.org/project/lerobot/"&gt;https://pypi.org/project/lerobot/&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Weights &amp;amp; Biases&lt;/h3&gt; 
&lt;p&gt;To use &lt;a href="https://docs.wandb.ai/quickstart"&gt;Weights and Biases&lt;/a&gt; for experiment tracking, log in with&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;wandb login
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;(note: you will also need to enable WandB in the configuration. See below.)&lt;/p&gt; 
&lt;h3&gt;Visualize datasets&lt;/h3&gt; 
&lt;p&gt;Check out &lt;a href="https://github.com/huggingface/lerobot/raw/main/examples/dataset/load_lerobot_dataset.py"&gt;example 1&lt;/a&gt; that illustrates how to use our dataset class which automatically downloads data from the Hugging Face hub.&lt;/p&gt; 
&lt;p&gt;You can also locally visualize episodes from a dataset on the hub by executing our script from the command line:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;lerobot-dataset-viz \
    --repo-id lerobot/pusht \
    --episode-index 0
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or from a dataset in a local folder with the &lt;code&gt;root&lt;/code&gt; option and the &lt;code&gt;--mode local&lt;/code&gt; (in the following case the dataset will be searched for in &lt;code&gt;./my_local_data_dir/lerobot/pusht&lt;/code&gt;)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;lerobot-dataset-viz \
    --repo-id lerobot/pusht \
    --root ./my_local_data_dir \
    --mode local \
    --episode-index 0
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It will open &lt;code&gt;rerun.io&lt;/code&gt; and display the camera streams, robot states and actions, like this:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github-production-user-asset-6210df.s3.amazonaws.com/4681518/328035972-fd46b787-b532-47e2-bb6f-fd536a55a7ed.mov?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;amp;X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240505%2Fus-east-1%2Fs3%2Faws4_request&amp;amp;X-Amz-Date=20240505T172924Z&amp;amp;X-Amz-Expires=300&amp;amp;X-Amz-Signature=d680b26c532eeaf80740f08af3320d22ad0b8a4e4da1bcc4f33142c15b509eda&amp;amp;X-Amz-SignedHeaders=host&amp;amp;actor_id=24889239&amp;amp;key_id=0&amp;amp;repo_id=748713144"&gt;https://github-production-user-asset-6210df.s3.amazonaws.com/4681518/328035972-fd46b787-b532-47e2-bb6f-fd536a55a7ed.mov?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;amp;X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240505%2Fus-east-1%2Fs3%2Faws4_request&amp;amp;X-Amz-Date=20240505T172924Z&amp;amp;X-Amz-Expires=300&amp;amp;X-Amz-Signature=d680b26c532eeaf80740f08af3320d22ad0b8a4e4da1bcc4f33142c15b509eda&amp;amp;X-Amz-SignedHeaders=host&amp;amp;actor_id=24889239&amp;amp;key_id=0&amp;amp;repo_id=748713144&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Our script can also visualize datasets stored on a distant server. See &lt;code&gt;lerobot-dataset-viz --help&lt;/code&gt; for more instructions.&lt;/p&gt; 
&lt;h3&gt;The &lt;code&gt;LeRobotDataset&lt;/code&gt; format&lt;/h3&gt; 
&lt;p&gt;A dataset in &lt;code&gt;LeRobotDataset&lt;/code&gt; format is very simple to use. It can be loaded from a repository on the Hugging Face hub or a local folder simply with e.g. &lt;code&gt;dataset = LeRobotDataset("lerobot/aloha_static_coffee")&lt;/code&gt; and can be indexed into like any Hugging Face and PyTorch dataset. For instance &lt;code&gt;dataset[0]&lt;/code&gt; will retrieve a single temporal frame from the dataset containing observation(s) and an action as PyTorch tensors ready to be fed to a model.&lt;/p&gt; 
&lt;p&gt;A specificity of &lt;code&gt;LeRobotDataset&lt;/code&gt; is that, rather than retrieving a single frame by its index, we can retrieve several frames based on their temporal relationship with the indexed frame, by setting &lt;code&gt;delta_timestamps&lt;/code&gt; to a list of relative times with respect to the indexed frame. For example, with &lt;code&gt;delta_timestamps = {"observation.image": [-1, -0.5, -0.2, 0]}&lt;/code&gt; one can retrieve, for a given index, 4 frames: 3 "previous" frames 1 second, 0.5 seconds, and 0.2 seconds before the indexed frame, and the indexed frame itself (corresponding to the 0 entry). See example &lt;a href="https://github.com/huggingface/lerobot/raw/main/examples/dataset/load_lerobot_dataset.py"&gt;1_load_lerobot_dataset.py&lt;/a&gt; for more details on &lt;code&gt;delta_timestamps&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Under the hood, the &lt;code&gt;LeRobotDataset&lt;/code&gt; format makes use of several ways to serialize data which can be useful to understand if you plan to work more closely with this format. We tried to make a flexible yet simple dataset format that would cover most type of features and specificities present in reinforcement learning and robotics, in simulation and in real-world, with a focus on cameras and robot states but easily extended to other types of sensory inputs as long as they can be represented by a tensor.&lt;/p&gt; 
&lt;p&gt;Here are the important details and internal structure organization of a typical &lt;code&gt;LeRobotDataset&lt;/code&gt; instantiated with &lt;code&gt;dataset = LeRobotDataset("lerobot/aloha_static_coffee")&lt;/code&gt;. The exact features will change from dataset to dataset but not the main aspects:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;dataset attributes:
  ├ hf_dataset: a Hugging Face dataset (backed by Arrow/parquet). Typical features example:
  │  ├ observation.images.cam_high (VideoFrame):
  │  │   VideoFrame = {'path': path to a mp4 video, 'timestamp' (float32): timestamp in the video}
  │  ├ observation.state (list of float32): position of an arm joints (for instance)
  │  ... (more observations)
  │  ├ action (list of float32): goal position of an arm joints (for instance)
  │  ├ episode_index (int64): index of the episode for this sample
  │  ├ frame_index (int64): index of the frame for this sample in the episode ; starts at 0 for each episode
  │  ├ timestamp (float32): timestamp in the episode
  │  ├ next.done (bool): indicates the end of an episode ; True for the last frame in each episode
  │  └ index (int64): general index in the whole dataset
  ├ meta: a LeRobotDatasetMetadata object containing:
  │  ├ info: a dictionary of metadata on the dataset
  │  │  ├ codebase_version (str): this is to keep track of the codebase version the dataset was created with
  │  │  ├ fps (int): frame per second the dataset is recorded/synchronized to
  │  │  ├ features (dict): all features contained in the dataset with their shapes and types
  │  │  ├ total_episodes (int): total number of episodes in the dataset
  │  │  ├ total_frames (int): total number of frames in the dataset
  │  │  ├ robot_type (str): robot type used for recording
  │  │  ├ data_path (str): formattable string for the parquet files
  │  │  └ video_path (str): formattable string for the video files (if using videos)
  │  ├ episodes: a DataFrame containing episode metadata with columns:
  │  │  ├ episode_index (int): index of the episode
  │  │  ├ tasks (list): list of tasks for this episode
  │  │  ├ length (int): number of frames in this episode
  │  │  ├ dataset_from_index (int): start index of this episode in the dataset
  │  │  └ dataset_to_index (int): end index of this episode in the dataset
  │  ├ stats: a dictionary of statistics (max, mean, min, std) for each feature in the dataset, for instance
  │  │  ├ observation.images.front_cam: {'max': tensor with same number of dimensions (e.g. `(c, 1, 1)` for images, `(c,)` for states), etc.}
  │  │  └ ...
  │  └ tasks: a DataFrame containing task information with task names as index and task_index as values
  ├ root (Path): local directory where the dataset is stored
  ├ image_transforms (Callable): optional image transformations to apply to visual modalities
  └ delta_timestamps (dict): optional delta timestamps for temporal queries
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;A &lt;code&gt;LeRobotDataset&lt;/code&gt; is serialised using several widespread file formats for each of its parts, namely:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;hf_dataset stored using Hugging Face datasets library serialization to parquet&lt;/li&gt; 
 &lt;li&gt;videos are stored in mp4 format to save space&lt;/li&gt; 
 &lt;li&gt;metadata are stored in plain json/jsonl files&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Dataset can be uploaded/downloaded from the HuggingFace hub seamlessly. To work on a local dataset, you can specify its location with the &lt;code&gt;root&lt;/code&gt; argument if it's not in the default &lt;code&gt;~/.cache/huggingface/lerobot&lt;/code&gt; location.&lt;/p&gt; 
&lt;h4&gt;Reproduce state-of-the-art (SOTA)&lt;/h4&gt; 
&lt;p&gt;We provide some pretrained policies on our &lt;a href="https://huggingface.co/lerobot"&gt;hub page&lt;/a&gt; that can achieve state-of-the-art performances. You can reproduce their training by loading the config from their run. Simply running:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;lerobot-train --config_path=lerobot/diffusion_pusht
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;reproduces SOTA results for Diffusion Policy on the PushT task.&lt;/p&gt; 
&lt;h2&gt;Contribute&lt;/h2&gt; 
&lt;p&gt;If you would like to contribute to 🤗 LeRobot, please check out our &lt;a href="https://github.com/huggingface/lerobot/raw/main/CONTRIBUTING.md"&gt;contribution guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Add a pretrained policy&lt;/h3&gt; 
&lt;p&gt;Once you have trained a policy you may upload it to the Hugging Face hub using a hub id that looks like &lt;code&gt;${hf_user}/${repo_name}&lt;/code&gt; (e.g. &lt;a href="https://huggingface.co/lerobot/diffusion_pusht"&gt;lerobot/diffusion_pusht&lt;/a&gt;).&lt;/p&gt; 
&lt;p&gt;You first need to find the checkpoint folder located inside your experiment directory (e.g. &lt;code&gt;outputs/train/2024-05-05/20-21-12_aloha_act_default/checkpoints/002500&lt;/code&gt;). Within that there is a &lt;code&gt;pretrained_model&lt;/code&gt; directory which should contain:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;config.json&lt;/code&gt;: A serialized version of the policy configuration (following the policy's dataclass config).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;model.safetensors&lt;/code&gt;: A set of &lt;code&gt;torch.nn.Module&lt;/code&gt; parameters, saved in &lt;a href="https://huggingface.co/docs/safetensors/index"&gt;Hugging Face Safetensors&lt;/a&gt; format.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;train_config.json&lt;/code&gt;: A consolidated configuration containing all parameters used for training. The policy configuration should match &lt;code&gt;config.json&lt;/code&gt; exactly. This is useful for anyone who wants to evaluate your policy or for reproducibility.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To upload these to the hub, run the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;huggingface-cli upload ${hf_user}/${repo_name} path/to/pretrained_model
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href="https://github.com/huggingface/lerobot/raw/main/src/lerobot/scripts/lerobot_eval.py"&gt;lerobot_eval.py&lt;/a&gt; for an example of how other people may use your policy.&lt;/p&gt; 
&lt;h3&gt;Acknowledgment&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;The LeRobot team 🤗 for building SmolVLA &lt;a href="https://arxiv.org/abs/2506.01844"&gt;Paper&lt;/a&gt;, &lt;a href="https://huggingface.co/blog/smolvla"&gt;Blog&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Thanks to Tony Zhao, Zipeng Fu and colleagues for open sourcing ACT policy, ALOHA environments and datasets. Ours are adapted from &lt;a href="https://tonyzhaozh.github.io/aloha"&gt;ALOHA&lt;/a&gt; and &lt;a href="https://mobile-aloha.github.io"&gt;Mobile ALOHA&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Thanks to Cheng Chi, Zhenjia Xu and colleagues for open sourcing Diffusion policy, Pusht environment and datasets, as well as UMI datasets. Ours are adapted from &lt;a href="https://diffusion-policy.cs.columbia.edu"&gt;Diffusion Policy&lt;/a&gt; and &lt;a href="https://umi-gripper.github.io"&gt;UMI Gripper&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Thanks to Nicklas Hansen, Yunhai Feng and colleagues for open sourcing TDMPC policy, Simxarm environments and datasets. Ours are adapted from &lt;a href="https://github.com/nicklashansen/tdmpc"&gt;TDMPC&lt;/a&gt; and &lt;a href="https://www.yunhaifeng.com/FOWM"&gt;FOWM&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Thanks to Antonio Loquercio and Ashish Kumar for their early support.&lt;/li&gt; 
 &lt;li&gt;Thanks to &lt;a href="https://sjlee.cc/"&gt;Seungjae (Jay) Lee&lt;/a&gt;, &lt;a href="https://mahis.life/"&gt;Mahi Shafiullah&lt;/a&gt; and colleagues for open sourcing &lt;a href="https://sjlee.cc/vq-bet/"&gt;VQ-BeT&lt;/a&gt; policy and helping us adapt the codebase to our repository. The policy is adapted from &lt;a href="https://github.com/jayLEE0301/vq_bet_official"&gt;VQ-BeT repo&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you want, you can cite this work with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{cadene2024lerobot,
    author = {Cadene, Remi and Alibert, Simon and Soare, Alexander and Gallouedec, Quentin and Zouitine, Adil and Palma, Steven and Kooijmans, Pepijn and Aractingi, Michel and Shukor, Mustafa and Aubakirova, Dana and Russi, Martino and Capuano, Francesco and Pascal, Caroline and Choghari, Jade and Moss, Jess and Wolf, Thomas},
    title = {LeRobot: State-of-the-art Machine Learning for Real-World Robotics in Pytorch},
    howpublished = "\url{https://github.com/huggingface/lerobot}",
    year = {2024}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#huggingface/lerobot&amp;amp;Timeline"&gt;&lt;img src="https://api.star-history.com/svg?repos=huggingface/lerobot&amp;amp;type=Timeline" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>emcie-co/parlant</title>
      <link>https://github.com/emcie-co/parlant</link>
      <description>&lt;p&gt;LLM agents built for control. Designed for real-world use. Deployed in minutes.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://github.com/emcie-co/parlant/blob/develop/docs/LogoTransparentLight.png?raw=true" /&gt; 
  &lt;img alt="Parlant - AI Agent Framework" src="https://github.com/emcie-co/parlant/raw/develop/docs/LogoTransparentDark.png?raw=true" width="400" /&gt; 
 &lt;/picture&gt; 
 &lt;h3&gt;Finally, LLM agents that actually follow instructions&lt;/h3&gt; 
 &lt;p&gt; &lt;a href="https://www.parlant.io/" target="_blank"&gt;🌐 Website&lt;/a&gt; • &lt;a href="https://www.parlant.io/docs/quickstart/installation" target="_blank"&gt;⚡ Quick Start&lt;/a&gt; • &lt;a href="https://discord.gg/duxWqxKk6J" target="_blank"&gt;💬 Discord&lt;/a&gt; • &lt;a href="https://www.parlant.io/docs/quickstart/examples" target="_blank"&gt;📖 Examples&lt;/a&gt; &lt;/p&gt; 
 &lt;p&gt; 
  &lt;!-- Keep these links. Translations will automatically update with the README. --&gt; &lt;a href="https://zdoc.app/de/emcie-co/parlant"&gt;Deutsch&lt;/a&gt; | &lt;a href="https://zdoc.app/es/emcie-co/parlant"&gt;Español&lt;/a&gt; | &lt;a href="https://zdoc.app/fr/emcie-co/parlant"&gt;français&lt;/a&gt; | &lt;a href="https://zdoc.app/ja/emcie-co/parlant"&gt;日本語&lt;/a&gt; | &lt;a href="https://zdoc.app/ko/emcie-co/parlant"&gt;한국어&lt;/a&gt; | &lt;a href="https://zdoc.app/pt/emcie-co/parlant"&gt;Português&lt;/a&gt; | &lt;a href="https://zdoc.app/ru/emcie-co/parlant"&gt;Русский&lt;/a&gt; | &lt;a href="https://zdoc.app/zh/emcie-co/parlant"&gt;中文&lt;/a&gt; &lt;/p&gt; 
 &lt;p&gt; &lt;a href="https://pypi.org/project/parlant/"&gt;&lt;img alt="PyPI" src="https://img.shields.io/pypi/v/parlant?color=blue" /&gt;&lt;/a&gt; &lt;img alt="Python 3.10+" src="https://img.shields.io/badge/python-3.10+-blue" /&gt; &lt;a href="https://opensource.org/licenses/Apache-2.0"&gt;&lt;img alt="License" src="https://img.shields.io/badge/license-Apache%202.0-green" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/duxWqxKk6J"&gt;&lt;img alt="Discord" src="https://img.shields.io/discord/1312378700993663007?color=7289da&amp;amp;logo=discord&amp;amp;logoColor=white" /&gt;&lt;/a&gt; &lt;img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/emcie-co/parlant?style=social" /&gt; &lt;/p&gt; 
 &lt;a href="https://trendshift.io/repositories/12768" target="_blank"&gt; &lt;img src="https://trendshift.io/api/badge/repositories/12768" alt="Trending on TrendShift" style="width: 250px; height: 55px;" width="250" height="55" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;🎯 The Problem Every AI Developer Faces&lt;/h2&gt; 
&lt;p&gt;You build an AI agent. It works great in testing. Then real users start talking to it and...&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;❌ It ignores your carefully crafted system prompts&lt;/li&gt; 
 &lt;li&gt;❌ It hallucinates responses in critical moments&lt;/li&gt; 
 &lt;li&gt;❌ It can't handle edge cases consistently&lt;/li&gt; 
 &lt;li&gt;❌ Each conversation feels like a roll of the dice&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Sound familiar?&lt;/strong&gt; You're not alone. This is the #1 pain point for developers building production AI agents.&lt;/p&gt; 
&lt;h2&gt;⚡ The Solution: Stop Fighting Prompts, Teach Principles&lt;/h2&gt; 
&lt;p&gt;Parlant flips the script on AI agent development. Instead of hoping your LLM will follow instructions, &lt;strong&gt;Parlant ensures it&lt;/strong&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Traditional approach: Cross your fingers 🤞
system_prompt = "You are a helpful assistant. Please follow these 47 rules..."

# Parlant approach: Ensured compliance ✅
await agent.create_guideline(
    condition="Customer asks about refunds",
    action="Check order status first to see if eligible",
    tools=[check_order_status],
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;✅ &lt;a href="https://www.parlant.io/blog/how-parlant-guarantees-compliance"&gt;Blog: How Parlant Ensures Agent Compliance&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;🆚 &lt;a href="https://www.parlant.io/blog/parlant-vs-dspy"&gt;Blog: Parlant vs DSPy&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Parlant gives you all the structure you need to build customer-facing agents that behave exactly as your business requires:&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://parlant.io/docs/concepts/customization/journeys"&gt;Journeys&lt;/a&gt;&lt;/strong&gt;: Define clear customer journeys and how your agent should respond at each step.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://parlant.io/docs/concepts/customization/guidelines"&gt;Behavioral Guidelines&lt;/a&gt;&lt;/strong&gt;: Easily craft agent behavior; Parlant will match the relevant elements contextually.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://parlant.io/docs/concepts/customization/tools"&gt;Tool Use&lt;/a&gt;&lt;/strong&gt;: Attach external APIs, data fetchers, or backend services to specific interaction events.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://parlant.io/docs/concepts/customization/glossary"&gt;Domain Adaptation&lt;/a&gt;&lt;/strong&gt;: Teach your agent domain-specific terminology and craft personalized responses.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://parlant.io/docs/concepts/customization/canned-responses"&gt;Canned Responses&lt;/a&gt;&lt;/strong&gt;: Use response templates to eliminate hallucinations and guarantee style consistency.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://parlant.io/docs/advanced/explainability"&gt;Explainability&lt;/a&gt;&lt;/strong&gt;: Understand why and when each guideline was matched and followed.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center"&gt; 
 &lt;h2&gt;🚀 Get Your Agent Running in 60 Seconds&lt;/h2&gt; 
&lt;/div&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install parlant
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import parlant.sdk as p

@p.tool
async def get_weather(context: p.ToolContext, city: str) -&amp;gt; p.ToolResult:
    # Your weather API logic here
    return p.ToolResult(f"Sunny, 72°F in {city}")

@p.tool
async def get_datetime(context: p.ToolContext) -&amp;gt; p.ToolResult:
    from datetime import datetime
    return p.ToolResult(datetime.now())

async def main():
    async with p.Server() as server:
        agent = await server.create_agent(
            name="WeatherBot",
            description="Helpful weather assistant"
        )

        # Have the agent's context be updated on every response (though
        # update interval is customizable) using a context variable.
        await agent.create_variable(name="current-datetime", tool=get_datetime)

        # Control and guide agent behavior with natural language
        await agent.create_guideline(
            condition="User asks about weather",
            action="Get current weather and provide a friendly response with suggestions",
            tools=[get_weather]
        )

        # Add other (reliably enforced) behavioral modeling elements
        # ...

        # 🎉 Test playground ready at http://localhost:8800
        # Integrate the official React widget into your app,
        # or follow the tutorial to build your own frontend!

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;That's it!&lt;/strong&gt; Your agent is running with ensured rule-following behavior.&lt;/p&gt; 
&lt;h2&gt;🎬 See It In Action&lt;/h2&gt; 
&lt;img alt="Parlant Demo" src="https://github.com/emcie-co/parlant/raw/develop/docs/demo.gif?raw=true" width="100%" /&gt; 
&lt;h2&gt;🔥 Why Developers Are Switching to Parlant&lt;/h2&gt; 
&lt;table width="100%"&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;🏗️ &lt;strong&gt;Traditional AI Frameworks&lt;/strong&gt;&lt;/h3&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;⚡ &lt;strong&gt;Parlant&lt;/strong&gt;&lt;/h3&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%"&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Write complex system prompts&lt;/li&gt; 
     &lt;li&gt;Hope the LLM follows them&lt;/li&gt; 
     &lt;li&gt;Debug unpredictable behaviors&lt;/li&gt; 
     &lt;li&gt;Scale by prompt engineering&lt;/li&gt; 
     &lt;li&gt;Cross fingers for reliability&lt;/li&gt; 
    &lt;/ul&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Define rules in natural language&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Ensured&lt;/strong&gt; rule compliance&lt;/li&gt; 
     &lt;li&gt;Predictable, consistent behavior&lt;/li&gt; 
     &lt;li&gt;Scale by adding guidelines&lt;/li&gt; 
     &lt;li&gt;Production-ready from day one&lt;/li&gt; 
    &lt;/ul&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;🎯 Perfect For Your Use Case&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;Financial Services&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;Healthcare&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;E-commerce&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;Legal Tech&lt;/strong&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;Compliance-first design&lt;/td&gt; 
    &lt;td align="center"&gt;HIPAA-ready agents&lt;/td&gt; 
    &lt;td align="center"&gt;Customer service at scale&lt;/td&gt; 
    &lt;td align="center"&gt;Precise legal guidance&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;Built-in risk management&lt;/td&gt; 
    &lt;td align="center"&gt;Patient data protection&lt;/td&gt; 
    &lt;td align="center"&gt;Order processing automation&lt;/td&gt; 
    &lt;td align="center"&gt;Document review assistance&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;🛠️ Enterprise-Grade Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;🧭 Conversational Journeys&lt;/strong&gt; - Lead the customer step-by-step to a goal&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🎯 Dynamic Guideline Matching&lt;/strong&gt; - Context-aware rule application&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🔧 Reliable Tool Integration&lt;/strong&gt; - APIs, databases, external services&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;📊 Conversation Analytics&lt;/strong&gt; - Deep insights into agent behavior&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🔄 Iterative Refinement&lt;/strong&gt; - Continuously improve agent responses&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🛡️ Built-in Guardrails&lt;/strong&gt; - Prevent hallucination and off-topic responses&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;📱 React Widget&lt;/strong&gt; - &lt;a href="https://github.com/emcie-co/parlant-chat-react"&gt;Drop-in chat UI for any web app&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🔍 Full Explainability&lt;/strong&gt; - Understand every decision your agent makes&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;📈 Join 10,000+ Developers Building Better AI&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;strong&gt;Companies using Parlant:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Financial institutions • Healthcare providers • Legal firms • E-commerce platforms&lt;/em&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://star-history.com/#emcie-co/parlant&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=emcie-co/parlant&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;🌟 What Developers Are Saying&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;"By far the most elegant conversational AI framework that I've come across! Developing with Parlant is pure joy."&lt;/em&gt; &lt;strong&gt;— Vishal Ahuja, Senior Lead, Customer-Facing Conversational AI @ JPMorgan Chase&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;🏃‍♂️ Quick Start Paths&lt;/h2&gt; 
&lt;table border="0"&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;🎯 I want to test it myself&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.parlant.io/docs/quickstart/installation"&gt;→ 5-minute quickstart&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;🛠️ I want to see an example&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.parlant.io/docs/quickstart/examples"&gt;→ Healthcare agent example&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;🚀 I want to get involved&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://discord.gg/duxWqxKk6J"&gt;→ Join our Discord community&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;🤝 Community &amp;amp; Support&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;💬 &lt;strong&gt;&lt;a href="https://discord.gg/duxWqxKk6J"&gt;Discord Community&lt;/a&gt;&lt;/strong&gt; - Get help from the team and community&lt;/li&gt; 
 &lt;li&gt;📖 &lt;strong&gt;&lt;a href="https://parlant.io/docs/quickstart/installation"&gt;Documentation&lt;/a&gt;&lt;/strong&gt; - Comprehensive guides and examples&lt;/li&gt; 
 &lt;li&gt;🐛 &lt;strong&gt;&lt;a href="https://github.com/emcie-co/parlant/issues"&gt;GitHub Issues&lt;/a&gt;&lt;/strong&gt; - Bug reports and feature requests&lt;/li&gt; 
 &lt;li&gt;📧 &lt;strong&gt;&lt;a href="https://parlant.io/contact"&gt;Direct Support&lt;/a&gt;&lt;/strong&gt; - Direct line to our engineering team&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;📄 License&lt;/h2&gt; 
&lt;p&gt;Apache 2.0 - Use it anywhere, including commercial projects.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;strong&gt;Ready to build AI agents that actually work?&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;⭐ &lt;strong&gt;Star this repo&lt;/strong&gt; • 🚀 &lt;strong&gt;&lt;a href="https://parlant.io/"&gt;Try Parlant now&lt;/a&gt;&lt;/strong&gt; • 💬 &lt;strong&gt;&lt;a href="https://discord.gg/duxWqxKk6J"&gt;Join Discord&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Built with ❤️ by the team at &lt;a href="https://emcie.co"&gt;Emcie&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>deepseek-ai/DeepSeek-V3</title>
      <link>https://github.com/deepseek-ai/DeepSeek-V3</link>
      <description>&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://github.com/deepseek-ai/DeepSeek-V2/raw/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" /&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;div align="center" style="line-height: 1;"&gt; 
 &lt;a href="https://www.deepseek.com/"&gt;&lt;img alt="Homepage" src="https://github.com/deepseek-ai/DeepSeek-V2/raw/main/figures/badge.svg?raw=true" /&gt;&lt;/a&gt; 
 &lt;a href="https://chat.deepseek.com/"&gt;&lt;img alt="Chat" src="https://img.shields.io/badge/🤖%20Chat-DeepSeek%20V3-536af5?color=536af5&amp;amp;logoColor=white" /&gt;&lt;/a&gt; 
 &lt;a href="https://huggingface.co/deepseek-ai"&gt;&lt;img alt="Hugging Face" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&amp;amp;logoColor=white" /&gt;&lt;/a&gt; 
 &lt;br /&gt; 
 &lt;a href="https://discord.gg/Tc7c45Zzu5"&gt;&lt;img alt="Discord" src="https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&amp;amp;logoColor=white&amp;amp;color=7289da" /&gt;&lt;/a&gt; 
 &lt;a href="https://github.com/deepseek-ai/DeepSeek-V2/raw/main/figures/qr.jpeg?raw=true"&gt;&lt;img alt="Wechat" src="https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&amp;amp;logoColor=white" /&gt;&lt;/a&gt; 
 &lt;a href="https://twitter.com/deepseek_ai"&gt;&lt;img alt="Twitter Follow" src="https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&amp;amp;logoColor=white" /&gt;&lt;/a&gt; 
 &lt;br /&gt; 
 &lt;a href="https://github.com/deepseek-ai/DeepSeek-V3/raw/main/LICENSE-CODE"&gt;&lt;img alt="Code License" src="https://img.shields.io/badge/Code_License-MIT-f5de53?&amp;amp;color=f5de53" /&gt;&lt;/a&gt; 
 &lt;a href="https://github.com/deepseek-ai/DeepSeek-V3/raw/main/LICENSE-MODEL"&gt;&lt;img alt="Model License" src="https://img.shields.io/badge/Model_License-Model_Agreement-f5de53?&amp;amp;color=f5de53" /&gt;&lt;/a&gt; 
 &lt;br /&gt; 
 &lt;a href="https://arxiv.org/pdf/2412.19437"&gt;&lt;b&gt;Paper Link&lt;/b&gt;👁️&lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/#1-introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/#2-model-summary"&gt;Model Summary&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/#3-model-downloads"&gt;Model Downloads&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/#4-evaluation-results"&gt;Evaluation Results&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/#5-chat-website--api-platform"&gt;Chat Website &amp;amp; API Platform&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/#6-how-to-run-locally"&gt;How to Run Locally&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/#7-license"&gt;License&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/#8-citation"&gt;Citation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/#9-contact"&gt;Contact&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;1. Introduction&lt;/h2&gt; 
&lt;p&gt;We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img width="80%" src="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/figures/benchmark.png" /&gt; &lt;/p&gt; 
&lt;h2&gt;2. Model Summary&lt;/h2&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;Architecture: Innovative Load Balancing Strategy and Training Objective&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;On top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free strategy for load balancing, which minimizes the performance degradation that arises from encouraging load balancing.&lt;/li&gt; 
 &lt;li&gt;We investigate a Multi-Token Prediction (MTP) objective and prove it beneficial to model performance. It can also be used for speculative decoding for inference acceleration.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;Pre-Training: Towards Ultimate Training Efficiency&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;We design an FP8 mixed precision training framework and, for the first time, validate the feasibility and effectiveness of FP8 training on an extremely large-scale model.&lt;/li&gt; 
 &lt;li&gt;Through co-design of algorithms, frameworks, and hardware, we overcome the communication bottleneck in cross-node MoE training, nearly achieving full computation-communication overlap.&lt;br /&gt; This significantly enhances our training efficiency and reduces the training costs, enabling us to further scale up the model size without additional overhead.&lt;/li&gt; 
 &lt;li&gt;At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;Post-Training: Knowledge Distillation from DeepSeek-R1&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;We introduce an innovative methodology to distill reasoning capabilities from the long-Chain-of-Thought (CoT) model, specifically from one of the DeepSeek R1 series models, into standard LLMs, particularly DeepSeek-V3. Our pipeline elegantly incorporates the verification and reflection patterns of R1 into DeepSeek-V3 and notably improves its reasoning performance. Meanwhile, we also maintain a control over the output style and length of DeepSeek-V3.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;3. Model Downloads&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;#Total Params&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;#Activated Params&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;Context Length&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;Download&lt;/strong&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;DeepSeek-V3-Base&lt;/td&gt; 
    &lt;td align="center"&gt;671B&lt;/td&gt; 
    &lt;td align="center"&gt;37B&lt;/td&gt; 
    &lt;td align="center"&gt;128K&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3-Base"&gt;🤗 Hugging Face&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;DeepSeek-V3&lt;/td&gt; 
    &lt;td align="center"&gt;671B&lt;/td&gt; 
    &lt;td align="center"&gt;37B&lt;/td&gt; 
    &lt;td align="center"&gt;128K&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3"&gt;🤗 Hugging Face&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] The total size of DeepSeek-V3 models on Hugging Face is 685B, which includes 671B of the Main Model weights and 14B of the Multi-Token Prediction (MTP) Module weights.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;To ensure optimal performance and flexibility, we have partnered with open-source communities and hardware vendors to provide multiple ways to run the model locally. For step-by-step guidance, check out Section 6: &lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/#6-how-to-run-locally"&gt;How_to Run_Locally&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For developers looking to dive deeper, we recommend exploring &lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/README_WEIGHTS.md"&gt;README_WEIGHTS.md&lt;/a&gt; for details on the Main Model weights and the Multi-Token Prediction (MTP) Modules. Please note that MTP support is currently under active development within the community, and we welcome your contributions and feedback.&lt;/p&gt; 
&lt;h2&gt;4. Evaluation Results&lt;/h2&gt; 
&lt;h3&gt;Base Model&lt;/h3&gt; 
&lt;h4&gt;Standard Benchmarks&lt;/h4&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;&lt;/th&gt; 
    &lt;th&gt;Benchmark (Metric)&lt;/th&gt; 
    &lt;th&gt;# Shots&lt;/th&gt; 
    &lt;th&gt;DeepSeek-V2&lt;/th&gt; 
    &lt;th&gt;Qwen2.5 72B&lt;/th&gt; 
    &lt;th&gt;LLaMA3.1 405B&lt;/th&gt; 
    &lt;th&gt;DeepSeek-V3&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;Architecture&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;MoE&lt;/td&gt; 
    &lt;td&gt;Dense&lt;/td&gt; 
    &lt;td&gt;Dense&lt;/td&gt; 
    &lt;td&gt;MoE&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;# Activated Params&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;21B&lt;/td&gt; 
    &lt;td&gt;72B&lt;/td&gt; 
    &lt;td&gt;405B&lt;/td&gt; 
    &lt;td&gt;37B&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;# Total Params&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;236B&lt;/td&gt; 
    &lt;td&gt;72B&lt;/td&gt; 
    &lt;td&gt;405B&lt;/td&gt; 
    &lt;td&gt;671B&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;English&lt;/td&gt; 
    &lt;td&gt;Pile-test (BPB)&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;0.606&lt;/td&gt; 
    &lt;td&gt;0.638&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;0.542&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;0.548&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;BBH (EM)&lt;/td&gt; 
    &lt;td&gt;3-shot&lt;/td&gt; 
    &lt;td&gt;78.8&lt;/td&gt; 
    &lt;td&gt;79.8&lt;/td&gt; 
    &lt;td&gt;82.9&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;87.5&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;MMLU (Acc.)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;78.4&lt;/td&gt; 
    &lt;td&gt;85.0&lt;/td&gt; 
    &lt;td&gt;84.4&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;87.1&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;MMLU-Redux (Acc.)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;75.6&lt;/td&gt; 
    &lt;td&gt;83.2&lt;/td&gt; 
    &lt;td&gt;81.3&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;86.2&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;MMLU-Pro (Acc.)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;51.4&lt;/td&gt; 
    &lt;td&gt;58.3&lt;/td&gt; 
    &lt;td&gt;52.8&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;64.4&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;DROP (F1)&lt;/td&gt; 
    &lt;td&gt;3-shot&lt;/td&gt; 
    &lt;td&gt;80.4&lt;/td&gt; 
    &lt;td&gt;80.6&lt;/td&gt; 
    &lt;td&gt;86.0&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;89.0&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;ARC-Easy (Acc.)&lt;/td&gt; 
    &lt;td&gt;25-shot&lt;/td&gt; 
    &lt;td&gt;97.6&lt;/td&gt; 
    &lt;td&gt;98.4&lt;/td&gt; 
    &lt;td&gt;98.4&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;98.9&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;ARC-Challenge (Acc.)&lt;/td&gt; 
    &lt;td&gt;25-shot&lt;/td&gt; 
    &lt;td&gt;92.2&lt;/td&gt; 
    &lt;td&gt;94.5&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;95.3&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;95.3&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;HellaSwag (Acc.)&lt;/td&gt; 
    &lt;td&gt;10-shot&lt;/td&gt; 
    &lt;td&gt;87.1&lt;/td&gt; 
    &lt;td&gt;84.8&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;89.2&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;88.9&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;PIQA (Acc.)&lt;/td&gt; 
    &lt;td&gt;0-shot&lt;/td&gt; 
    &lt;td&gt;83.9&lt;/td&gt; 
    &lt;td&gt;82.6&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;85.9&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;84.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;WinoGrande (Acc.)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;86.3&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;82.3&lt;/td&gt; 
    &lt;td&gt;85.2&lt;/td&gt; 
    &lt;td&gt;84.9&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;RACE-Middle (Acc.)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;73.1&lt;/td&gt; 
    &lt;td&gt;68.1&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;74.2&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;67.1&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;RACE-High (Acc.)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;52.6&lt;/td&gt; 
    &lt;td&gt;50.3&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;56.8&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;51.3&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;TriviaQA (EM)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;80.0&lt;/td&gt; 
    &lt;td&gt;71.9&lt;/td&gt; 
    &lt;td&gt;82.7&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;82.9&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;NaturalQuestions (EM)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;38.6&lt;/td&gt; 
    &lt;td&gt;33.2&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;41.5&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;40.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;AGIEval (Acc.)&lt;/td&gt; 
    &lt;td&gt;0-shot&lt;/td&gt; 
    &lt;td&gt;57.5&lt;/td&gt; 
    &lt;td&gt;75.8&lt;/td&gt; 
    &lt;td&gt;60.6&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;79.6&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Code&lt;/td&gt; 
    &lt;td&gt;HumanEval (Pass@1)&lt;/td&gt; 
    &lt;td&gt;0-shot&lt;/td&gt; 
    &lt;td&gt;43.3&lt;/td&gt; 
    &lt;td&gt;53.0&lt;/td&gt; 
    &lt;td&gt;54.9&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;65.2&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;MBPP (Pass@1)&lt;/td&gt; 
    &lt;td&gt;3-shot&lt;/td&gt; 
    &lt;td&gt;65.0&lt;/td&gt; 
    &lt;td&gt;72.6&lt;/td&gt; 
    &lt;td&gt;68.4&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;75.4&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;LiveCodeBench-Base (Pass@1)&lt;/td&gt; 
    &lt;td&gt;3-shot&lt;/td&gt; 
    &lt;td&gt;11.6&lt;/td&gt; 
    &lt;td&gt;12.9&lt;/td&gt; 
    &lt;td&gt;15.5&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;19.4&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;CRUXEval-I (Acc.)&lt;/td&gt; 
    &lt;td&gt;2-shot&lt;/td&gt; 
    &lt;td&gt;52.5&lt;/td&gt; 
    &lt;td&gt;59.1&lt;/td&gt; 
    &lt;td&gt;58.5&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;67.3&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;CRUXEval-O (Acc.)&lt;/td&gt; 
    &lt;td&gt;2-shot&lt;/td&gt; 
    &lt;td&gt;49.8&lt;/td&gt; 
    &lt;td&gt;59.9&lt;/td&gt; 
    &lt;td&gt;59.9&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;69.8&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Math&lt;/td&gt; 
    &lt;td&gt;GSM8K (EM)&lt;/td&gt; 
    &lt;td&gt;8-shot&lt;/td&gt; 
    &lt;td&gt;81.6&lt;/td&gt; 
    &lt;td&gt;88.3&lt;/td&gt; 
    &lt;td&gt;83.5&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;89.3&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;MATH (EM)&lt;/td&gt; 
    &lt;td&gt;4-shot&lt;/td&gt; 
    &lt;td&gt;43.4&lt;/td&gt; 
    &lt;td&gt;54.4&lt;/td&gt; 
    &lt;td&gt;49.0&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;61.6&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;MGSM (EM)&lt;/td&gt; 
    &lt;td&gt;8-shot&lt;/td&gt; 
    &lt;td&gt;63.6&lt;/td&gt; 
    &lt;td&gt;76.2&lt;/td&gt; 
    &lt;td&gt;69.9&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;79.8&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;CMath (EM)&lt;/td&gt; 
    &lt;td&gt;3-shot&lt;/td&gt; 
    &lt;td&gt;78.7&lt;/td&gt; 
    &lt;td&gt;84.5&lt;/td&gt; 
    &lt;td&gt;77.3&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;90.7&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Chinese&lt;/td&gt; 
    &lt;td&gt;CLUEWSC (EM)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;82.0&lt;/td&gt; 
    &lt;td&gt;82.5&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;83.0&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;82.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;C-Eval (Acc.)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;81.4&lt;/td&gt; 
    &lt;td&gt;89.2&lt;/td&gt; 
    &lt;td&gt;72.5&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;90.1&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;CMMLU (Acc.)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;84.0&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;89.5&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;73.7&lt;/td&gt; 
    &lt;td&gt;88.8&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;CMRC (EM)&lt;/td&gt; 
    &lt;td&gt;1-shot&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;77.4&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;75.8&lt;/td&gt; 
    &lt;td&gt;76.0&lt;/td&gt; 
    &lt;td&gt;76.3&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;C3 (Acc.)&lt;/td&gt; 
    &lt;td&gt;0-shot&lt;/td&gt; 
    &lt;td&gt;77.4&lt;/td&gt; 
    &lt;td&gt;76.7&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;79.7&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;78.6&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;CCPM (Acc.)&lt;/td&gt; 
    &lt;td&gt;0-shot&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;93.0&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;88.5&lt;/td&gt; 
    &lt;td&gt;78.6&lt;/td&gt; 
    &lt;td&gt;92.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Multilingual&lt;/td&gt; 
    &lt;td&gt;MMMLU-non-English (Acc.)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;64.0&lt;/td&gt; 
    &lt;td&gt;74.8&lt;/td&gt; 
    &lt;td&gt;73.8&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;79.4&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Best results are shown in bold. Scores with a gap not exceeding 0.3 are considered to be at the same level. DeepSeek-V3 achieves the best performance on most benchmarks, especially on math and code tasks. For more evaluation details, please check our paper.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Context Window&lt;/h4&gt; 
&lt;p align="center"&gt; &lt;img width="80%" src="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/figures/niah.png" /&gt; &lt;/p&gt; 
&lt;p&gt;Evaluation results on the &lt;code&gt;Needle In A Haystack&lt;/code&gt; (NIAH) tests. DeepSeek-V3 performs well across all context window lengths up to &lt;strong&gt;128K&lt;/strong&gt;.&lt;/p&gt; 
&lt;h3&gt;Chat Model&lt;/h3&gt; 
&lt;h4&gt;Standard Benchmarks (Models larger than 67B)&lt;/h4&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;Benchmark (Metric)&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;DeepSeek V2-0506&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;DeepSeek V2.5-0905&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;Qwen2.5 72B-Inst.&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;Llama3.1 405B-Inst.&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;Claude-3.5-Sonnet-1022&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;GPT-4o 0513&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;DeepSeek V3&lt;/strong&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;Architecture&lt;/td&gt; 
    &lt;td&gt;MoE&lt;/td&gt; 
    &lt;td&gt;MoE&lt;/td&gt; 
    &lt;td&gt;Dense&lt;/td&gt; 
    &lt;td&gt;Dense&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;MoE&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;# Activated Params&lt;/td&gt; 
    &lt;td&gt;21B&lt;/td&gt; 
    &lt;td&gt;21B&lt;/td&gt; 
    &lt;td&gt;72B&lt;/td&gt; 
    &lt;td&gt;405B&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;37B&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;# Total Params&lt;/td&gt; 
    &lt;td&gt;236B&lt;/td&gt; 
    &lt;td&gt;236B&lt;/td&gt; 
    &lt;td&gt;72B&lt;/td&gt; 
    &lt;td&gt;405B&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;671B&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;English&lt;/td&gt; 
    &lt;td&gt;MMLU (EM)&lt;/td&gt; 
    &lt;td&gt;78.2&lt;/td&gt; 
    &lt;td&gt;80.6&lt;/td&gt; 
    &lt;td&gt;85.3&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;88.6&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;88.3&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;87.2&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;88.5&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;MMLU-Redux (EM)&lt;/td&gt; 
    &lt;td&gt;77.9&lt;/td&gt; 
    &lt;td&gt;80.3&lt;/td&gt; 
    &lt;td&gt;85.6&lt;/td&gt; 
    &lt;td&gt;86.2&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;88.9&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;88.0&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;89.1&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;MMLU-Pro (EM)&lt;/td&gt; 
    &lt;td&gt;58.5&lt;/td&gt; 
    &lt;td&gt;66.2&lt;/td&gt; 
    &lt;td&gt;71.6&lt;/td&gt; 
    &lt;td&gt;73.3&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;78.0&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;72.6&lt;/td&gt; 
    &lt;td&gt;75.9&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;DROP (3-shot F1)&lt;/td&gt; 
    &lt;td&gt;83.0&lt;/td&gt; 
    &lt;td&gt;87.8&lt;/td&gt; 
    &lt;td&gt;76.7&lt;/td&gt; 
    &lt;td&gt;88.7&lt;/td&gt; 
    &lt;td&gt;88.3&lt;/td&gt; 
    &lt;td&gt;83.7&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;91.6&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;IF-Eval (Prompt Strict)&lt;/td&gt; 
    &lt;td&gt;57.7&lt;/td&gt; 
    &lt;td&gt;80.6&lt;/td&gt; 
    &lt;td&gt;84.1&lt;/td&gt; 
    &lt;td&gt;86.0&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;86.5&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;84.3&lt;/td&gt; 
    &lt;td&gt;86.1&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;GPQA-Diamond (Pass@1)&lt;/td&gt; 
    &lt;td&gt;35.3&lt;/td&gt; 
    &lt;td&gt;41.3&lt;/td&gt; 
    &lt;td&gt;49.0&lt;/td&gt; 
    &lt;td&gt;51.1&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;65.0&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;49.9&lt;/td&gt; 
    &lt;td&gt;59.1&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;SimpleQA (Correct)&lt;/td&gt; 
    &lt;td&gt;9.0&lt;/td&gt; 
    &lt;td&gt;10.2&lt;/td&gt; 
    &lt;td&gt;9.1&lt;/td&gt; 
    &lt;td&gt;17.1&lt;/td&gt; 
    &lt;td&gt;28.4&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;38.2&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;24.9&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;FRAMES (Acc.)&lt;/td&gt; 
    &lt;td&gt;66.9&lt;/td&gt; 
    &lt;td&gt;65.4&lt;/td&gt; 
    &lt;td&gt;69.8&lt;/td&gt; 
    &lt;td&gt;70.0&lt;/td&gt; 
    &lt;td&gt;72.5&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;80.5&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;73.3&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;LongBench v2 (Acc.)&lt;/td&gt; 
    &lt;td&gt;31.6&lt;/td&gt; 
    &lt;td&gt;35.4&lt;/td&gt; 
    &lt;td&gt;39.4&lt;/td&gt; 
    &lt;td&gt;36.1&lt;/td&gt; 
    &lt;td&gt;41.0&lt;/td&gt; 
    &lt;td&gt;48.1&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;48.7&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Code&lt;/td&gt; 
    &lt;td&gt;HumanEval-Mul (Pass@1)&lt;/td&gt; 
    &lt;td&gt;69.3&lt;/td&gt; 
    &lt;td&gt;77.4&lt;/td&gt; 
    &lt;td&gt;77.3&lt;/td&gt; 
    &lt;td&gt;77.2&lt;/td&gt; 
    &lt;td&gt;81.7&lt;/td&gt; 
    &lt;td&gt;80.5&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;82.6&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;LiveCodeBench (Pass@1-COT)&lt;/td&gt; 
    &lt;td&gt;18.8&lt;/td&gt; 
    &lt;td&gt;29.2&lt;/td&gt; 
    &lt;td&gt;31.1&lt;/td&gt; 
    &lt;td&gt;28.4&lt;/td&gt; 
    &lt;td&gt;36.3&lt;/td&gt; 
    &lt;td&gt;33.4&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;40.5&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;LiveCodeBench (Pass@1)&lt;/td&gt; 
    &lt;td&gt;20.3&lt;/td&gt; 
    &lt;td&gt;28.4&lt;/td&gt; 
    &lt;td&gt;28.7&lt;/td&gt; 
    &lt;td&gt;30.1&lt;/td&gt; 
    &lt;td&gt;32.8&lt;/td&gt; 
    &lt;td&gt;34.2&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;37.6&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;Codeforces (Percentile)&lt;/td&gt; 
    &lt;td&gt;17.5&lt;/td&gt; 
    &lt;td&gt;35.6&lt;/td&gt; 
    &lt;td&gt;24.8&lt;/td&gt; 
    &lt;td&gt;25.3&lt;/td&gt; 
    &lt;td&gt;20.3&lt;/td&gt; 
    &lt;td&gt;23.6&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;51.6&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;SWE Verified (Resolved)&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;22.6&lt;/td&gt; 
    &lt;td&gt;23.8&lt;/td&gt; 
    &lt;td&gt;24.5&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;50.8&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;38.8&lt;/td&gt; 
    &lt;td&gt;42.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;Aider-Edit (Acc.)&lt;/td&gt; 
    &lt;td&gt;60.3&lt;/td&gt; 
    &lt;td&gt;71.6&lt;/td&gt; 
    &lt;td&gt;65.4&lt;/td&gt; 
    &lt;td&gt;63.9&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;84.2&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;72.9&lt;/td&gt; 
    &lt;td&gt;79.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;Aider-Polyglot (Acc.)&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;18.2&lt;/td&gt; 
    &lt;td&gt;7.6&lt;/td&gt; 
    &lt;td&gt;5.8&lt;/td&gt; 
    &lt;td&gt;45.3&lt;/td&gt; 
    &lt;td&gt;16.0&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;49.6&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Math&lt;/td&gt; 
    &lt;td&gt;AIME 2024 (Pass@1)&lt;/td&gt; 
    &lt;td&gt;4.6&lt;/td&gt; 
    &lt;td&gt;16.7&lt;/td&gt; 
    &lt;td&gt;23.3&lt;/td&gt; 
    &lt;td&gt;23.3&lt;/td&gt; 
    &lt;td&gt;16.0&lt;/td&gt; 
    &lt;td&gt;9.3&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;39.2&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;MATH-500 (EM)&lt;/td&gt; 
    &lt;td&gt;56.3&lt;/td&gt; 
    &lt;td&gt;74.7&lt;/td&gt; 
    &lt;td&gt;80.0&lt;/td&gt; 
    &lt;td&gt;73.8&lt;/td&gt; 
    &lt;td&gt;78.3&lt;/td&gt; 
    &lt;td&gt;74.6&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;90.2&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;CNMO 2024 (Pass@1)&lt;/td&gt; 
    &lt;td&gt;2.8&lt;/td&gt; 
    &lt;td&gt;10.8&lt;/td&gt; 
    &lt;td&gt;15.9&lt;/td&gt; 
    &lt;td&gt;6.8&lt;/td&gt; 
    &lt;td&gt;13.1&lt;/td&gt; 
    &lt;td&gt;10.8&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;43.2&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Chinese&lt;/td&gt; 
    &lt;td&gt;CLUEWSC (EM)&lt;/td&gt; 
    &lt;td&gt;89.9&lt;/td&gt; 
    &lt;td&gt;90.4&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;91.4&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;84.7&lt;/td&gt; 
    &lt;td&gt;85.4&lt;/td&gt; 
    &lt;td&gt;87.9&lt;/td&gt; 
    &lt;td&gt;90.9&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;C-Eval (EM)&lt;/td&gt; 
    &lt;td&gt;78.6&lt;/td&gt; 
    &lt;td&gt;79.5&lt;/td&gt; 
    &lt;td&gt;86.1&lt;/td&gt; 
    &lt;td&gt;61.5&lt;/td&gt; 
    &lt;td&gt;76.7&lt;/td&gt; 
    &lt;td&gt;76.0&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;86.5&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;C-SimpleQA (Correct)&lt;/td&gt; 
    &lt;td&gt;48.5&lt;/td&gt; 
    &lt;td&gt;54.1&lt;/td&gt; 
    &lt;td&gt;48.4&lt;/td&gt; 
    &lt;td&gt;50.4&lt;/td&gt; 
    &lt;td&gt;51.3&lt;/td&gt; 
    &lt;td&gt;59.3&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;64.8&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] All models are evaluated in a configuration that limits the output length to 8K. Benchmarks containing fewer than 1000 samples are tested multiple times using varying temperature settings to derive robust final results. DeepSeek-V3 stands as the best-performing open-source model, and also exhibits competitive performance against frontier closed-source models.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Open Ended Generation Evaluation&lt;/h4&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;Arena-Hard&lt;/th&gt; 
    &lt;th&gt;AlpacaEval 2.0&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;DeepSeek-V2.5-0905&lt;/td&gt; 
    &lt;td&gt;76.2&lt;/td&gt; 
    &lt;td&gt;50.5&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Qwen2.5-72B-Instruct&lt;/td&gt; 
    &lt;td&gt;81.2&lt;/td&gt; 
    &lt;td&gt;49.1&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;LLaMA-3.1 405B&lt;/td&gt; 
    &lt;td&gt;69.3&lt;/td&gt; 
    &lt;td&gt;40.5&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;GPT-4o-0513&lt;/td&gt; 
    &lt;td&gt;80.4&lt;/td&gt; 
    &lt;td&gt;51.1&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Claude-Sonnet-3.5-1022&lt;/td&gt; 
    &lt;td&gt;85.2&lt;/td&gt; 
    &lt;td&gt;52.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;DeepSeek-V3&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;85.5&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;70.0&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] English open-ended conversation evaluations. For AlpacaEval 2.0, we use the length-controlled win rate as the metric.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;5. Chat Website &amp;amp; API Platform&lt;/h2&gt; 
&lt;p&gt;You can chat with DeepSeek-V3 on DeepSeek's official website: &lt;a href="https://chat.deepseek.com/sign_in"&gt;chat.deepseek.com&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;We also provide OpenAI-Compatible API at DeepSeek Platform: &lt;a href="https://platform.deepseek.com/"&gt;platform.deepseek.com&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;6. How to Run Locally&lt;/h2&gt; 
&lt;p&gt;DeepSeek-V3 can be deployed locally using the following hardware and open-source community software:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;DeepSeek-Infer Demo&lt;/strong&gt;: We provide a simple and lightweight demo for FP8 and BF16 inference.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;SGLang&lt;/strong&gt;: Fully support the DeepSeek-V3 model in both BF16 and FP8 inference modes, with Multi-Token Prediction &lt;a href="https://github.com/sgl-project/sglang/issues/2591"&gt;coming soon&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;LMDeploy&lt;/strong&gt;: Enables efficient FP8 and BF16 inference for local and cloud deployment.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;TensorRT-LLM&lt;/strong&gt;: Currently supports BF16 inference and INT4/8 quantization, with FP8 support coming soon.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;vLLM&lt;/strong&gt;: Support DeepSeek-V3 model with FP8 and BF16 modes for tensor parallelism and pipeline parallelism.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;LightLLM&lt;/strong&gt;: Supports efficient single-node or multi-node deployment for FP8 and BF16.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;AMD GPU&lt;/strong&gt;: Enables running the DeepSeek-V3 model on AMD GPUs via SGLang in both BF16 and FP8 modes.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Huawei Ascend NPU&lt;/strong&gt;: Supports running DeepSeek-V3 on Huawei Ascend devices in both INT8 and BF16.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Since FP8 training is natively adopted in our framework, we only provide FP8 weights. If you require BF16 weights for experimentation, you can use the provided conversion script to perform the transformation.&lt;/p&gt; 
&lt;p&gt;Here is an example of converting FP8 weights to BF16:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;cd inference
python fp8_cast_bf16.py --input-fp8-hf-path /path/to/fp8_weights --output-bf16-hf-path /path/to/bf16_weights
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Hugging Face's Transformers has not been directly supported yet.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;6.1 Inference with DeepSeek-Infer Demo (example only)&lt;/h3&gt; 
&lt;h4&gt;System Requirements&lt;/h4&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Linux with Python 3.10 only. Mac and Windows are not supported.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Dependencies:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-pip-requirements"&gt;torch==2.4.1
triton==3.0.0
transformers==4.46.3
safetensors==0.4.5
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Model Weights &amp;amp; Demo Code Preparation&lt;/h4&gt; 
&lt;p&gt;First, clone our DeepSeek-V3 GitHub repository:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;git clone https://github.com/deepseek-ai/DeepSeek-V3.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Navigate to the &lt;code&gt;inference&lt;/code&gt; folder and install dependencies listed in &lt;code&gt;requirements.txt&lt;/code&gt;. Easiest way is to use a package manager like &lt;code&gt;conda&lt;/code&gt; or &lt;code&gt;uv&lt;/code&gt; to create a new virtual environment and install the dependencies.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;cd DeepSeek-V3/inference
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Download the model weights from Hugging Face, and put them into &lt;code&gt;/path/to/DeepSeek-V3&lt;/code&gt; folder.&lt;/p&gt; 
&lt;h4&gt;Model Weights Conversion&lt;/h4&gt; 
&lt;p&gt;Convert Hugging Face model weights to a specific format:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;python convert.py --hf-ckpt-path /path/to/DeepSeek-V3 --save-path /path/to/DeepSeek-V3-Demo --n-experts 256 --model-parallel 16
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Run&lt;/h4&gt; 
&lt;p&gt;Then you can chat with DeepSeek-V3:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;torchrun --nnodes 2 --nproc-per-node 8 --node-rank $RANK --master-addr $ADDR generate.py --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --interactive --temperature 0.7 --max-new-tokens 200
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or batch inference on a given file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;torchrun --nnodes 2 --nproc-per-node 8 --node-rank $RANK --master-addr $ADDR generate.py --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --input-file $FILE
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;6.2 Inference with SGLang (recommended)&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/sgl-project/sglang"&gt;SGLang&lt;/a&gt; currently supports &lt;a href="https://lmsys.org/blog/2024-09-04-sglang-v0-3/#deepseek-multi-head-latent-attention-mla-throughput-optimizations"&gt;MLA optimizations&lt;/a&gt;, &lt;a href="https://lmsys.org/blog/2024-12-04-sglang-v0-4/#data-parallelism-attention-for-deepseek-models"&gt;DP Attention&lt;/a&gt;, FP8 (W8A8), FP8 KV Cache, and Torch Compile, delivering state-of-the-art latency and throughput performance among open-source frameworks.&lt;/p&gt; 
&lt;p&gt;Notably, &lt;a href="https://github.com/sgl-project/sglang/releases/tag/v0.4.1"&gt;SGLang v0.4.1&lt;/a&gt; fully supports running DeepSeek-V3 on both &lt;strong&gt;NVIDIA and AMD GPUs&lt;/strong&gt;, making it a highly versatile and robust solution.&lt;/p&gt; 
&lt;p&gt;SGLang also supports &lt;a href="https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3#example-serving-with-2-h208"&gt;multi-node tensor parallelism&lt;/a&gt;, enabling you to run this model on multiple network-connected machines.&lt;/p&gt; 
&lt;p&gt;Multi-Token Prediction (MTP) is in development, and progress can be tracked in the &lt;a href="https://github.com/sgl-project/sglang/issues/2591"&gt;optimization plan&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Here are the launch instructions from the SGLang team: &lt;a href="https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3"&gt;https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;6.3 Inference with LMDeploy (recommended)&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/InternLM/lmdeploy"&gt;LMDeploy&lt;/a&gt;, a flexible and high-performance inference and serving framework tailored for large language models, now supports DeepSeek-V3. It offers both offline pipeline processing and online deployment capabilities, seamlessly integrating with PyTorch-based workflows.&lt;/p&gt; 
&lt;p&gt;For comprehensive step-by-step instructions on running DeepSeek-V3 with LMDeploy, please refer to here: &lt;a href="https://github.com/InternLM/lmdeploy/issues/2960"&gt;https://github.com/InternLM/lmdeploy/issues/2960&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;6.4 Inference with TRT-LLM (recommended)&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/NVIDIA/TensorRT-LLM"&gt;TensorRT-LLM&lt;/a&gt; now supports the DeepSeek-V3 model, offering precision options such as BF16 and INT4/INT8 weight-only. Support for FP8 is currently in progress and will be released soon. You can access the custom branch of TRTLLM specifically for DeepSeek-V3 support through the following link to experience the new features directly: &lt;a href="https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/deepseek_v3"&gt;https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/deepseek_v3&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;6.5 Inference with vLLM (recommended)&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/vllm-project/vllm"&gt;vLLM&lt;/a&gt; v0.6.6 supports DeepSeek-V3 inference for FP8 and BF16 modes on both NVIDIA and AMD GPUs. Aside from standard techniques, vLLM offers &lt;em&gt;pipeline parallelism&lt;/em&gt; allowing you to run this model on multiple machines connected by networks. For detailed guidance, please refer to the &lt;a href="https://docs.vllm.ai/en/latest/serving/distributed_serving.html"&gt;vLLM instructions&lt;/a&gt;. Please feel free to follow &lt;a href="https://github.com/vllm-project/vllm/issues/11539"&gt;the enhancement plan&lt;/a&gt; as well.&lt;/p&gt; 
&lt;h3&gt;6.6 Inference with LightLLM (recommended)&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/ModelTC/lightllm/tree/main"&gt;LightLLM&lt;/a&gt; v1.0.1 supports single-machine and multi-machine tensor parallel deployment for DeepSeek-R1 (FP8/BF16) and provides mixed-precision deployment, with more quantization modes continuously integrated. For more details, please refer to &lt;a href="https://lightllm-en.readthedocs.io/en/latest/getting_started/quickstart.html"&gt;LightLLM instructions&lt;/a&gt;. Additionally, LightLLM offers PD-disaggregation deployment for DeepSeek-V2, and the implementation of PD-disaggregation for DeepSeek-V3 is in development.&lt;/p&gt; 
&lt;h3&gt;6.7 Recommended Inference Functionality with AMD GPUs&lt;/h3&gt; 
&lt;p&gt;In collaboration with the AMD team, we have achieved Day-One support for AMD GPUs using SGLang, with full compatibility for both FP8 and BF16 precision. For detailed guidance, please refer to the &lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/#63-inference-with-lmdeploy-recommended"&gt;SGLang instructions&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;6.8 Recommended Inference Functionality with Huawei Ascend NPUs&lt;/h3&gt; 
&lt;p&gt;The &lt;a href="https://www.hiascend.com/en/software/mindie"&gt;MindIE&lt;/a&gt; framework from the Huawei Ascend community has successfully adapted the BF16 version of DeepSeek-V3. For step-by-step guidance on Ascend NPUs, please follow the &lt;a href="https://modelers.cn/models/MindIE/deepseekv3"&gt;instructions here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;7. License&lt;/h2&gt; 
&lt;p&gt;This code repository is licensed under &lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/LICENSE-CODE"&gt;the MIT License&lt;/a&gt;. The use of DeepSeek-V3 Base/Chat models is subject to &lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/LICENSE-MODEL"&gt;the Model License&lt;/a&gt;. DeepSeek-V3 series (including Base and Chat) supports commercial use.&lt;/p&gt; 
&lt;h2&gt;8. Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;@misc{deepseekai2024deepseekv3technicalreport,
      title={DeepSeek-V3 Technical Report}, 
      author={DeepSeek-AI},
      year={2024},
      eprint={2412.19437},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.19437}, 
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;9. Contact&lt;/h2&gt; 
&lt;p&gt;If you have any questions, please raise an issue or contact us at &lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/service@deepseek.com"&gt;service@deepseek.com&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>hiroi-sora/Umi-OCR</title>
      <link>https://github.com/hiroi-sora/Umi-OCR</link>
      <description>&lt;p&gt;OCR software, free and offline. 开源、免费的离线OCR软件。支持截屏/批量导入图片，PDF文档识别，排除水印/页眉页脚，扫描/生成二维码。内置多国语言库。&lt;/p&gt;&lt;hr&gt;&lt;p align="left"&gt; &lt;span&gt; &lt;b&gt;中文&lt;/b&gt; &lt;/span&gt; &lt;span&gt; • &lt;/span&gt; &lt;a href="https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/README_en.md"&gt; English &lt;/a&gt; &lt;span&gt; • &lt;/span&gt; &lt;a href="https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/README_ja.md"&gt; 日本語 &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/hiroi-sora/Umi-OCR"&gt; &lt;img width="200" height="128" src="https://tupian.li/images/2022/10/27/icon---256.png" alt="Umi-OCR" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h1 align="center"&gt;Umi-OCR 文字识别工具&lt;/h1&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/hiroi-sora/Umi-OCR/releases/latest"&gt; &lt;img src="https://img.shields.io/github/v/release/hiroi-sora/Umi-OCR?style=flat-square" alt="Umi-OCR" /&gt; &lt;/a&gt; &lt;a href="https://github.com/hiroi-sora/Umi-OCR/raw/main/LICENSE"&gt; &lt;img src="https://img.shields.io/github/license/hiroi-sora/Umi-OCR?style=flat-square" alt="LICENSE" /&gt; &lt;/a&gt; &lt;a href="#下载发行版"&gt; &lt;img src="https://img.shields.io/github/downloads/hiroi-sora/Umi-OCR/total?style=flat-square" alt="forks" /&gt; &lt;/a&gt; &lt;a href="https://star-history.com/#hiroi-sora/Umi-OCR"&gt; &lt;img src="https://img.shields.io/github/stars/hiroi-sora/Umi-OCR?style=flat-square" alt="stars" /&gt; &lt;/a&gt; &lt;a href="https://github.com/hiroi-sora/Umi-OCR/forks"&gt; &lt;img src="https://img.shields.io/github/forks/hiroi-sora/Umi-OCR?style=flat-square" alt="forks" /&gt; &lt;/a&gt; &lt;a href="https://hosted.weblate.org/engage/umi-ocr/"&gt; &lt;img src="https://hosted.weblate.org/widget/umi-ocr/svg-badge.svg?sanitize=true" alt="翻译状态" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;h3&gt; &lt;a href="#目录"&gt; 使用说明 &lt;/a&gt; &lt;span&gt; • &lt;/span&gt; &lt;a href="#下载发行版"&gt; 下载地址 &lt;/a&gt; &lt;span&gt; • &lt;/span&gt; &lt;a href="https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/CHANGE_LOG.md"&gt; 更新日志 &lt;/a&gt; &lt;span&gt; • &lt;/span&gt; &lt;a href="https://github.com/hiroi-sora/Umi-OCR/issues"&gt; 提交Bug &lt;/a&gt; &lt;/h3&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;strong&gt;免费，开源，可批量的离线OCR软件&lt;/strong&gt;
 &lt;br /&gt; 
 &lt;sub&gt;适用于 Windows7 x64 、Linux x64 &lt;/sub&gt;
&lt;/div&gt;
&lt;br /&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;免费&lt;/strong&gt;：本项目所有代码开源，完全免费。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;方便&lt;/strong&gt;：解压即用，离线运行，无需网络。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;高效&lt;/strong&gt;：自带高效率的离线OCR引擎，内置多种语言识别库。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;灵活&lt;/strong&gt;：支持命令行、HTTP接口等外部调用方式。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;功能&lt;/strong&gt;：截图OCR / 批量OCR / PDF识别 / 二维码 / 公式识别&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="center"&gt;&lt;img src="https://tupian.li/images/2023/11/19/65599097ab5f4.png" alt="1-标题-1.png" style="width: 80%;" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://tupian.li/images/2023/11/19/6559909fdeeba.png" alt="1-标题-2.png" /&gt;&lt;/p&gt; 
&lt;h2&gt;目录&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/#%E6%88%AA%E5%9B%BEOCR"&gt;截图识别&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/#%E6%96%87%E6%9C%AC%E5%90%8E%E5%A4%84%E7%90%86"&gt;排版解析&lt;/a&gt; - 识别不同排版，按正确顺序输出文字&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/#%E6%89%B9%E9%87%8FOCR"&gt;批量识别&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/#%E5%BF%BD%E7%95%A5%E5%8C%BA%E5%9F%9F"&gt;忽略区域&lt;/a&gt; - 排除截图水印处的文字&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/#%E4%BA%8C%E7%BB%B4%E7%A0%81"&gt;二维码&lt;/a&gt; 支持扫码或生成二维码图片&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/#%E6%96%87%E6%A1%A3%E8%AF%86%E5%88%AB"&gt;文档识别&lt;/a&gt; 从PDF扫描件中提取文本，或转为双层可搜索PDF&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/#%E5%85%A8%E5%B1%80%E8%AE%BE%E7%BD%AE"&gt;全局设置&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/docs/README_CLI.md"&gt;命令行调用&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/docs/http/README.md"&gt;HTTP接口&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/#%E6%9E%84%E5%BB%BA%E9%A1%B9%E7%9B%AE"&gt;构建项目（Windows、Linux）&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;使用源码&lt;/h2&gt; 
&lt;p&gt;开发者请务必阅读 &lt;a href="https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/#%E6%9E%84%E5%BB%BA%E9%A1%B9%E7%9B%AE"&gt;构建项目&lt;/a&gt; 。&lt;/p&gt; 
&lt;h2&gt;下载发行版&lt;/h2&gt; 
&lt;p&gt;以下发布链接均长期维护，提供稳定版本的下载。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;蓝奏云&lt;/strong&gt; &lt;a href="https://hiroi-sora.lanzoul.com/s/umi-ocr"&gt;https://hiroi-sora.lanzoul.com/s/umi-ocr&lt;/a&gt; （国内推荐，免注册/无限速）&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;GitHub&lt;/strong&gt; &lt;a href="https://github.com/hiroi-sora/Umi-OCR/releases/latest"&gt;https://github.com/hiroi-sora/Umi-OCR/releases/latest&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Source Forge&lt;/strong&gt; &lt;a href="https://sourceforge.net/projects/umi-ocr"&gt;https://sourceforge.net/projects/umi-ocr&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;•&amp;nbsp;&amp;nbsp;Scoop Installer&lt;/b&gt;（点击展开）&lt;/summary&gt; 
 &lt;p&gt;&lt;a href="https://scoop.sh/"&gt;Scoop&lt;/a&gt; 是一款Windows下的命令行安装程序，可方便地管理多个应用。您可以先安装 Scoop ，再使用以下指令安装 &lt;code&gt;Umi-OCR&lt;/code&gt; ：&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;添加 &lt;code&gt;extras&lt;/code&gt; 桶：&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code&gt;scoop bucket add extras
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;（可选1）安装 Umi-OCR（自带 &lt;code&gt;Rapid-OCR&lt;/code&gt; 引擎，兼容性好）：&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code&gt;scoop install extras/umi-ocr
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;（可选2）安装 Umi-OCR（自带 &lt;code&gt;Paddle-OCR&lt;/code&gt; 引擎，速度稍快）：&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code&gt;scoop install extras/umi-ocr-paddle
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;不要同时安装二者，快捷方式可能会被覆盖。但您可以额外导入 &lt;a href="https://github.com/hiroi-sora/Umi-OCR_plugins"&gt;插件&lt;/a&gt; ，随时切换不同OCR引擎。&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;br /&gt; 
&lt;h2&gt;开始使用&lt;/h2&gt; 
&lt;p&gt;软件发布包下载为 &lt;code&gt;.7z&lt;/code&gt; 压缩包或 &lt;code&gt;.7z.exe&lt;/code&gt; 自解压包。自解压包可在没有安装压缩软件的电脑上，解压文件。&lt;/p&gt; 
&lt;p&gt;本软件无需安装。解压后，点击 &lt;code&gt;Umi-OCR.exe&lt;/code&gt; 即可启动程序。&lt;/p&gt; 
&lt;p&gt;遇到任何问题，请提 &lt;a href="https://github.com/hiroi-sora/Umi-OCR/issues"&gt;Issue&lt;/a&gt; ，我会尽可能帮助你。&lt;/p&gt; 
&lt;h2&gt;界面语言&lt;/h2&gt; 
&lt;p&gt;Umi-OCR 支持的界面多国语言。在第一次打开软件时，将会按照你的电脑的系统设置，自动切换语言。&lt;/p&gt; 
&lt;p&gt;如果需要手动切换语言，请参考下图，&lt;code&gt;全局设置&lt;/code&gt;→&lt;code&gt;语言/Language&lt;/code&gt; 。&lt;/p&gt; 
&lt;p align="center"&gt;&lt;img src="https://tupian.li/images/2023/11/19/65599c3f9e600.png" alt="1-标题-1.png" style="width: 80%;" /&gt;&lt;/p&gt; 
&lt;h2&gt;标签页&lt;/h2&gt; 
&lt;p&gt;Umi-OCR v2 由一系列灵活好用的&lt;strong&gt;标签页&lt;/strong&gt;组成。您可按照自己的喜好，打开需要的标签页。&lt;/p&gt; 
&lt;p&gt;标签栏左上角可以切换&lt;strong&gt;窗口置顶&lt;/strong&gt;。右上角能够&lt;strong&gt;锁定标签页&lt;/strong&gt;，以防止日常使用中误触关闭标签页。&lt;/p&gt; 
&lt;h3&gt;截图OCR&lt;/h3&gt; 
&lt;p align="center"&gt;&lt;img src="https://tupian.li/images/2023/11/19/65599097aba8e.png" alt="2-截图-1.png" style="width: 80%;" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;截图OCR&lt;/strong&gt;：打开这一页后，就可以用快捷键唤起截图，识别图中的文字。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;左侧的图片预览栏，可直接用鼠标划选复制。&lt;/li&gt; 
 &lt;li&gt;右侧的识别记录栏，可以编辑文字，允许划选多个记录复制。&lt;/li&gt; 
 &lt;li&gt;也支持在别处复制图片，粘贴到Umi-OCR进行识别。&lt;/li&gt; 
 &lt;li&gt;关于 &lt;a href="https://github.com/hiroi-sora/Umi-OCR/issues/254"&gt;公式识别&lt;/a&gt; 功能&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;文本后处理&lt;/h4&gt; 
&lt;p align="center"&gt;&lt;img src="https://tupian.li/images/2023/11/19/6559909f3e378.png" alt="2-截图-2.png" style="width: 80%;" /&gt;&lt;/p&gt; 
&lt;p&gt;关于 &lt;strong&gt;OCR文本后处理 - 排版解析方案&lt;/strong&gt;： 可以整理OCR结果的排版和顺序，使文本更适合阅读和使用。预设方案：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;多栏-按自然段换行&lt;/code&gt;：适合大部分情景，自动识别多栏布局，按自然段规则进行换行。&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;多栏-总是换行&lt;/code&gt;：每段语句都进行换行。&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;多栏-无换行&lt;/code&gt;：强制将所有语句合并到同一行。&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;单栏-按自然段换行&lt;/code&gt;/&lt;code&gt;总是换行&lt;/code&gt;/&lt;code&gt;无换行&lt;/code&gt;：与上述类似，不过 不区分多栏布局。&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;单栏-保留缩进&lt;/code&gt;：适用于解析代码截图，保留行首缩进和行中空格。&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;不做处理&lt;/code&gt;：OCR引擎的原始输出，默认每段语句都进行换行。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;上述方案，均能自动处理横排和竖排（从右到左）的排版。（竖排文字还需要OCR引擎本身支持）&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;批量OCR&lt;/h3&gt; 
&lt;p align="center"&gt;&lt;img src="https://tupian.li/images/2023/11/19/655990a2511e0.png" alt="3-批量-1.png" style="width: 80%;" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;批量OCR&lt;/strong&gt;：这一页用于批量导入本地图片进行识别。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;支持格式：&lt;code&gt;jpg, jpe, jpeg, jfif, png, webp, bmp, tif, tiff&lt;/code&gt;。&lt;/li&gt; 
 &lt;li&gt;保存识别结果的支持格式：&lt;code&gt;txt, jsonl, md, csv(Excel)&lt;/code&gt;。&lt;/li&gt; 
 &lt;li&gt;与截图OCR一样，支持&lt;code&gt;文本后处理&lt;/code&gt;功能，整理OCR文本的排版和顺序。&lt;/li&gt; 
 &lt;li&gt;没有数量上限，可一次性导入几百张图片进行任务。&lt;/li&gt; 
 &lt;li&gt;支持任务完成后自动关机/待机。&lt;/li&gt; 
 &lt;li&gt;如果要识别像素超大的长图或大图，请调整：&lt;strong&gt;页面的设置→文字识别→限制图像边长→【调高数值】&lt;/strong&gt;。&lt;/li&gt; 
 &lt;li&gt;拥有特殊功能 &lt;code&gt;忽略区域&lt;/code&gt; 。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;忽略区域&lt;/h4&gt; 
&lt;p align="center"&gt;&lt;img src="https://tupian.li/images/2023/11/19/6559911d28be7.png" alt="3-批量-2.png" style="width: 80%;" /&gt;&lt;/p&gt; 
&lt;p&gt;关于 &lt;strong&gt;OCR文本后处理 - 忽略区域&lt;/strong&gt;： 批量OCR中的一种特殊功能，适用于排除图片中的不想要的文字。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;在批量识别页的右栏设置中可进入忽略区域编辑器。&lt;/li&gt; 
 &lt;li&gt;如上方样例，图片顶部和右下角存在多个水印 / LOGO。如果批量识别这类图片，水印会对识别结果造成干扰。&lt;/li&gt; 
 &lt;li&gt;按住右键，绘制多个矩形框。这些区域内的文字将在任务中被忽略。&lt;/li&gt; 
 &lt;li&gt;请尽量将矩形框画得大一些，完全包裹住水印所有可能出现的位置。&lt;/li&gt; 
 &lt;li&gt;注意，只有处于忽略区域框内部的整个文本块（而不是单个字符）会被忽略。如下图所示，黄色边框的深色矩形是一个忽略区域。那么只有&lt;code&gt;key_mouse&lt;/code&gt;才会被忽略。&lt;code&gt;pubsub_connector.py&lt;/code&gt;、&lt;code&gt;pubsub_service.py&lt;/code&gt; 这两个文本块得以保留。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="center"&gt;&lt;img src="https://tupian.li/images/2024/05/30/66587bf03ae15.png" alt="忽略区域范围示例.png" style="width: 80%;" /&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;文档识别&lt;/h3&gt; 
&lt;p align="center"&gt;&lt;img src="https://github.com/hiroi-sora/Umi-OCR/assets/56373419/fc2266ee-b9b7-4079-8b10-6610e6da6cf5" alt="" style="width: 80%;" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;文档识别&lt;/strong&gt;：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;支持格式：&lt;code&gt;pdf, xps, epub, mobi, fb2, cbz&lt;/code&gt;。&lt;/li&gt; 
 &lt;li&gt;对扫描件进行OCR，或提取原有文本。可输出为 &lt;strong&gt;双层可搜索PDF&lt;/strong&gt; 。&lt;/li&gt; 
 &lt;li&gt;支持设定 &lt;strong&gt;忽略区域&lt;/strong&gt; ，可用于排除页眉页脚的文字。&lt;/li&gt; 
 &lt;li&gt;可设置任务完成后 &lt;strong&gt;自动关机/休眠&lt;/strong&gt; 。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;二维码&lt;/h3&gt; 
&lt;p align="center"&gt;&lt;img src="https://tupian.li/images/2023/11/19/655991268d6b1.png" alt="4-二维码-1.png" style="width: 80%;" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;扫码&lt;/strong&gt;：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;截图/粘贴/拖入本地图片，读取其中的二维码、条形码。&lt;/li&gt; 
 &lt;li&gt;支持一图多码。&lt;/li&gt; 
 &lt;li&gt;支持19种协议，如下：&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;code&gt;Aztec&lt;/code&gt;,&lt;code&gt;Codabar&lt;/code&gt;,&lt;code&gt;Code128&lt;/code&gt;,&lt;code&gt;Code39&lt;/code&gt;,&lt;code&gt;Code93&lt;/code&gt;,&lt;code&gt;DataBar&lt;/code&gt;,&lt;code&gt;DataBarExpanded&lt;/code&gt;,&lt;code&gt;DataMatrix&lt;/code&gt;,&lt;code&gt;EAN13&lt;/code&gt;,&lt;code&gt;EAN8&lt;/code&gt;,&lt;code&gt;ITF&lt;/code&gt;,&lt;code&gt;LinearCodes&lt;/code&gt;,&lt;code&gt;MatrixCodes&lt;/code&gt;,&lt;code&gt;MaxiCode&lt;/code&gt;,&lt;code&gt;MicroQRCode&lt;/code&gt;,&lt;code&gt;PDF417&lt;/code&gt;,&lt;code&gt;QRCode&lt;/code&gt;,&lt;code&gt;UPCA&lt;/code&gt;,&lt;code&gt;UPCE&lt;/code&gt;&lt;/p&gt; 
&lt;p align="center"&gt;&lt;img src="https://tupian.li/images/2023/11/19/6559911cda737.png" alt="4-二维码-2.png" style="width: 80%;" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;生成码&lt;/strong&gt;：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;输入文本，生成二维码图片。&lt;/li&gt; 
 &lt;li&gt;支持19种协议和&lt;strong&gt;纠错等级&lt;/strong&gt;等参数。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;全局设置&lt;/h3&gt; 
&lt;p align="center"&gt;&lt;img src="https://tupian.li/images/2023/11/19/655991252e780.png" alt="5-全局设置-1.png" style="width: 80%;" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;全局设置&lt;/strong&gt;：在这里可以调整软件的全局参数。常用功能如下：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;一键添加快捷方式或设置开机自启。&lt;/li&gt; 
 &lt;li&gt;更改界面&lt;strong&gt;语言&lt;/strong&gt;。Umi支持繁中、英语、日语等语言。&lt;/li&gt; 
 &lt;li&gt;切换界面&lt;strong&gt;主题&lt;/strong&gt;。Umi拥有多个亮/暗主题。&lt;/li&gt; 
 &lt;li&gt;调整界面&lt;strong&gt;文字的大小&lt;/strong&gt;和&lt;strong&gt;字体&lt;/strong&gt;。&lt;/li&gt; 
 &lt;li&gt;切换OCR插件。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;渲染器&lt;/strong&gt;：软件界面默认支持显卡加速渲染。如果在你的机器上出现截屏闪烁、UI错位的情况，请调整&lt;code&gt;界面和外观&lt;/code&gt; → &lt;code&gt;渲染器&lt;/code&gt; ，尝试切换到不同渲染方案，或关闭硬件加速。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;调用接口：&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/docs/README_CLI.md"&gt;命令行手册&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/docs/http/README.md"&gt;HTTP接口手册&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;关于项目结构&lt;/h2&gt; 
&lt;h3&gt;各仓库：&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hiroi-sora/Umi-OCR"&gt;主仓库&lt;/a&gt; 👈&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hiroi-sora/Umi-OCR_plugins"&gt;插件库&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hiroi-sora/Umi-OCR_runtime_windows"&gt;Windows 运行库&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hiroi-sora/Umi-OCR_runtime_linux"&gt;Linux 运行库&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;工程结构：&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;**&lt;/code&gt; 后缀表示本仓库(&lt;code&gt;主仓库&lt;/code&gt;)包含的内容。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Umi-OCR
├─ Umi-OCR.exe
├─ umi-ocr.sh
└─ UmiOCR-data
   ├─ main.py **
   ├─ version.py **
   ├─ qt_res **
   │  └─ 项目qt资源，包括图标和qml源码
   ├─ py_src **
   │  └─ 项目python源码
   ├─ plugins
   │  └─ 插件
   └─ i18n **
      └─ 翻译文件
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;支持的离线OCR引擎：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hiroi-sora/PaddleOCR-json"&gt;PaddleOCR-json&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hiroi-sora/RapidOCR-json"&gt;RapidOCR-json&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;运行环境框架：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/skywind3000/PyStand"&gt;PyStand&lt;/a&gt; 定制版&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;构建项目&lt;/h2&gt; 
&lt;p&gt;请跳转下述仓库，完成对应平台的开发/运行环境部署。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hiroi-sora/Umi-OCR_runtime_windows"&gt;Windows&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hiroi-sora/Umi-OCR_runtime_linux"&gt;Linux&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;软件本地化翻译：&lt;/h2&gt; 
&lt;p&gt;本项目使用 Weblate 平台进行UI界面的本地化翻译协作。我们欢迎任何译者参与翻译工作，您可进入此链接 &lt;a href="https://hosted.weblate.org/engage/umi-ocr/"&gt;Weblate: Umi-OCR&lt;/a&gt; ，在线校对、补充现有语言，或添加新语言。&lt;/p&gt; 
&lt;p&gt;感谢以下译者，为 Umi-OCR 贡献了本地化翻译工作：&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;译者&lt;/th&gt; 
   &lt;th&gt;贡献语言&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://hosted.weblate.org/user/q021"&gt;bob&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;English, 繁體中文, 日本語&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/QZGao"&gt;Qingzheng Gao&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;English, 繁體中文&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://hosted.weblate.org/user/ChiaLingWeng"&gt;Weng, Chia-Ling&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;English, 繁體中文&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://hosted.weblate.org/user/linzow"&gt;linzow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;English, 繁體中文&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://hosted.weblate.org/user/ultramarkorj9"&gt;Marcos i&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;English, Português&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://hosted.weblate.org/user/qwedc001"&gt;Eric Guo&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;English&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://hosted.weblate.org/user/steven0081"&gt;steven0081&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;English&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://hosted.weblate.org/user/random4t4x14"&gt;Brandon Cagle&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;English&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://hosted.weblate.org/user/plum7x"&gt;plum7x&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;繁體中文&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://hosted.weblate.org/user/hugoalh"&gt;hugoalh&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;繁體中文&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://hosted.weblate.org/user/Anarkiisto"&gt;Anarkiisto&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;繁體中文&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://hosted.weblate.org/user/umren190402"&gt;ドコモ光&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;日本語&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://hosted.weblate.org/user/ypf"&gt;杨鹏&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Português&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://hosted.weblate.org/user/1969"&gt;Вячеслав Анатольевич Малышев&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Русский&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://hosted.weblate.org/user/muhammadyusuf.kurbonov2002"&gt;Muhammadyusuf Kurbonov&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Русский&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://hosted.weblate.org/user/TamilNeram/"&gt;தமிழ்நேரம்&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;தமிழ்&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;如果有信息错误或人员缺漏，请在 &lt;a href="https://github.com/hiroi-sora/Umi-OCR/discussions/449"&gt;这个讨论&lt;/a&gt; 中回复。&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;赞助&lt;/h2&gt; 
&lt;p&gt;Umi-OCR 项目主要由作者 &lt;a href="https://github.com/hiroi-sora"&gt;hiroi-sora&lt;/a&gt; 用业余时间在开发和维护。如果您喜欢这款软件，欢迎赞助。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;国内用户可通过 &lt;a href="https://afdian.com/a/hiroi-sora"&gt;爱发电&lt;/a&gt; 赞助作者。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#hiroi-sora/Umi-OCR&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=hiroi-sora/Umi-OCR&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;a href="https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/CHANGE_LOG.md"&gt;更新日志&lt;/a&gt;&lt;/h2&gt; 
&lt;h2&gt;开发计划&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;已完成的工作&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;标签页框架。&lt;/li&gt; 
  &lt;li&gt;OCR API控制器。&lt;/li&gt; 
  &lt;li&gt;OCR 任务控制器。&lt;/li&gt; 
  &lt;li&gt;主题管理器，支持切换浅色/深色主题主题。&lt;/li&gt; 
  &lt;li&gt;实现 &lt;strong&gt;批量OCR&lt;/strong&gt;。&lt;/li&gt; 
  &lt;li&gt;实现 &lt;strong&gt;截图OCR&lt;/strong&gt;。&lt;/li&gt; 
  &lt;li&gt;快捷键机制。&lt;/li&gt; 
  &lt;li&gt;系统托盘菜单。&lt;/li&gt; 
  &lt;li&gt;文本块后处理（排版优化）。&lt;/li&gt; 
  &lt;li&gt;引擎内存清理。&lt;/li&gt; 
  &lt;li&gt;软件界面多国语言。&lt;/li&gt; 
  &lt;li&gt;命令行模式。&lt;/li&gt; 
  &lt;li&gt;Win7兼容。&lt;/li&gt; 
  &lt;li&gt;Excel（csv）输出格式。&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;Esc&lt;/code&gt;中断截图操作&lt;/li&gt; 
  &lt;li&gt;外置主题文件&lt;/li&gt; 
  &lt;li&gt;字体切换&lt;/li&gt; 
  &lt;li&gt;加载动画&lt;/li&gt; 
  &lt;li&gt;忽略区域。&lt;/li&gt; 
  &lt;li&gt;二维码识别。&lt;/li&gt; 
  &lt;li&gt;批量识别页面的图片预览窗口。&lt;/li&gt; 
  &lt;li&gt;PDF识别。&lt;/li&gt; 
  &lt;li&gt;调用本地图片浏览器打开图片。 &lt;a href="https://github.com/hiroi-sora/Umi-OCR/issues/335"&gt;#335&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;重复上一次截图。 &lt;a href="https://github.com/hiroi-sora/Umi-OCR/issues/357"&gt;#357&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;修Bug：文档识别在Windows7系统的兼容性问题。&lt;/li&gt; 
  &lt;li&gt;HTTP/命令行接口添加二维码识别/生成功能。 (#423)&lt;/li&gt; 
  &lt;li&gt;二维码接口的文档。&lt;/li&gt; 
  &lt;li&gt;Linux 平台移植。&lt;/li&gt; 
  &lt;li&gt;HTTP 文档识别接口。&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;!-- ##### 正在进行的工作 --&gt; 
&lt;h5&gt;远期计划&lt;/h5&gt; 
&lt;details&gt; 
 &lt;summary&gt;展开&lt;/summary&gt; 
 &lt;p&gt;这些是预想中的功能，在开发初期已预留好接口，将在远期慢慢实现。&lt;/p&gt; 
 &lt;p&gt;但开发途中受限于实际情况，可能更改功能设计、新增及取消功能。&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;p&gt;重构底层插件机制。&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;p&gt;在线 OCR API 插件。&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;p&gt;独立的数学公式识别插件。&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;p&gt;“数学公式”标签页，提供独立的数学公式识别/Latex渲染。&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;p&gt;检查更新机制。&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;p&gt;排版解析之外的文本后处理模块（如保留数字、半全角字符转换、文本纠错）。&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;p&gt;关键接口函数添加事件触发方式。&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;基于GPU的离线OCR。&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;图片翻译&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;离线翻译。&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;固定区域识别。&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;识别表格图片，输出为Excel。&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;历史记录系统。&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;兼容 MacOS / Ubuntu 等平台。&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt;</description>
    </item>
    
    <item>
      <title>pollen-robotics/reachy_mini</title>
      <link>https://github.com/pollen-robotics/reachy_mini</link>
      <description>&lt;p&gt;Reachy Mini's SDK&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Reachy Mini&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;⚠️ Reachy Mini is still in beta. Expect bugs, some of them we won't fix right away if they are not a priority.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;a href="https://www.pollen-robotics.com/reachy-mini/"&gt;Reachy Mini&lt;/a&gt; is an expressive, open-source robot designed for human-robot interaction, creative coding, and AI experimentation. We made it to be affordable, easy to use, hackable and cute, so that you can focus on building cool AI applications!&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.pollen-robotics.com/reachy-mini/"&gt;&lt;img src="https://raw.githubusercontent.com/pollen-robotics/reachy_mini/develop/docs/assets/reachy_mini_hello.gif" alt="Reachy Mini Hello" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Versions Lite &amp;amp; Wireless&lt;/h3&gt; 
&lt;p&gt;Reachy Mini's hardware comes in two flavors:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Reachy Mini lite&lt;/strong&gt;: where the robot is directly connected to your computer via USB. And the code that controls the robot (the daemon) runs on your computer.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Reachy Mini wireless&lt;/strong&gt;: where an Raspberry Pi is embedded in the robot, and the code that controls the robot (the daemon) runs on the Raspberry Pi. You can connect to it via Wi-Fi from your computer. (TODO: add link to section on how to set it up)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;There is also a simulated version of Reachy Mini in &lt;a href="https://mujoco.org"&gt;MuJoCo&lt;/a&gt; that you can use to prototype your applications before deploying them on the real robot. It behaves like the lite version where the daemon runs on your computer.&lt;/p&gt; 
&lt;h2&gt;Assembly guide&lt;/h2&gt; 
&lt;p&gt;Follow our step-by-step &lt;a href="https://www.pollen-robotics.com/wp-content/uploads/2025/10/Reachy_Mini_Assembly_BETA_v2_LOW-compresse.pdf"&gt;Assembly Guide&lt;/a&gt;. Most builders finish in about 3 hours, our current speed record is 43 minutes. The guide walks you through every step with clear visuals so you can assemble Reachy Mini confidently from start to finish. Enjoy the build!&lt;/p&gt; 
&lt;h2&gt;Software overview&lt;/h2&gt; 
&lt;p&gt;This repository provides everything you need to control Reachy Mini, both in simulation and on the real robot. It consists of two main parts:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;The 😈 Daemon 😈&lt;/strong&gt;: A background service that manages communication with the robot's motors and sensors, or with the simulation environment. It should be running before you can control the robot. It can run either for the simulation (MuJoCo) or for the real robot.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🐍 SDK &amp;amp; 🕸️ API&lt;/strong&gt; to control the robot's main features (head, antennas, camera, speakers, microphone, etc.) and connect with your AI experimentation. Depending on your preferences and needs, there is a &lt;a href="https://raw.githubusercontent.com/pollen-robotics/reachy_mini/develop/#using-the-python-sdk"&gt;Python SDK&lt;/a&gt; and a &lt;a href="https://raw.githubusercontent.com/pollen-robotics/reachy_mini/develop/#using-the-rest-api"&gt;HTTP REST API&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Using the &lt;a href="https://raw.githubusercontent.com/pollen-robotics/reachy_mini/develop/#using-the-python-sdk"&gt;Python SDK&lt;/a&gt;, making your robot move only require a few lines of code, as illustrated in the example below:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from reachy_mini import ReachyMini
from reachy_mini.utils import create_head_pose

with ReachyMini() as reachy_mini:
    # Move the head up (10mm on z-axis) and roll it 15 degrees
    pose = create_head_pose(z=10, roll=15, degrees=True, mm=True)
    reachy_mini.goto_target(head=pose, duration=2.0)

    # Reset to default pose
    pose = create_head_pose() 
    reachy_mini.goto_target(head=pose, duration=2.0)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;and using the &lt;a href="https://raw.githubusercontent.com/pollen-robotics/reachy_mini/develop/#using-the-rest-api"&gt;REST API&lt;/a&gt;, reading the current state of the robot:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl 'http://localhost:8000/api/state/full'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Those two examples above assume that the daemon is already running (either in simulation or on the real robot) locally.&lt;/p&gt; 
&lt;h2&gt;Installation of the daemon and Python SDK&lt;/h2&gt; 
&lt;p&gt;As mentioned above, before being able to use the robot, you need to run the daemon that will handle the communication with the motors.&lt;/p&gt; 
&lt;p&gt;We support and test on Linux and macOS. It's also working on Windows, but it is less tested at the moment. Do not hesitate to open an issue if you encounter any problem.&lt;/p&gt; 
&lt;p&gt;The daemon is built in Python, so you need to have Python installed on your computer (versions from 3.10 to 3.13 are supported). We recommend using a virtual environment to avoid dependency conflicts with your other Python projects.&lt;/p&gt; 
&lt;p&gt;You can install Reachy Mini from the source code or from PyPI.&lt;/p&gt; 
&lt;p&gt;From PyPI, you can install the package with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install reachy-mini
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;From the source code, you can install the package with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/pollen-robotics/reachy_mini
pip install -e ./reachy_mini
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The same package provides both the daemon and the Python SDK.&lt;/p&gt; 
&lt;h2&gt;Run the reachy mini daemon&lt;/h2&gt; 
&lt;p&gt;Before being able to use the robot, you need to run the daemon that will handle the communication with the motors. This daemon can run either in simulation (MuJoCo) or on the real robot.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;reachy-mini-daemon
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or run it via the Python module:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m reachy_mini.daemon.app.main
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Additional argument for both simulation and real robot:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;--localhost-only: (default behavior). The server will only accept connections from localhost.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;--no-localhost-only: If set, the server will accept connections from any connection on the local network.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;In simulation (&lt;a href="https://mujoco.org"&gt;MuJoCo&lt;/a&gt;)&lt;/h3&gt; 
&lt;p&gt;You first have to install the optional dependency &lt;code&gt;mujoco&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install reachy-mini[mujoco]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then run the daemon with the &lt;code&gt;--sim&lt;/code&gt;&amp;nbsp;argument.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;reachy-mini-daemon --sim
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Additional arguments:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;--scene &amp;lt;empty|minimal&amp;gt; : (Default empty). Choose between a basic empty scene, or a scene with a table and some objects.
&lt;/code&gt;&lt;/pre&gt; 
&lt;img src="https://www.pollen-robotics.com/wp-content/uploads/2025/06/Reachy_mini_simulation.gif" width="250" alt="Reachy Mini in MuJoCo" /&gt; 
&lt;p&gt;&lt;em&gt;Note: On OSX in order to run mujoco, you need to use mjpython (see &lt;a href="https://mujoco.readthedocs.io/en/stable/python.html#passive-viewer"&gt;here&lt;/a&gt;). So, you should run the daemon with:&lt;/em&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt; mjpython -m reachy_mini.daemon.app.main --sim
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;For the lite version (connected via USB)&lt;/h3&gt; 
&lt;p&gt;It should automatically detect the serial port of the robot. If it does not, you can specify it manually with the &lt;code&gt;-p&lt;/code&gt; option:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;reachy-mini-daemon -p &amp;lt;serial_port&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Usage&lt;/h3&gt; 
&lt;p&gt;For more information about the daemon and its options, you can run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;reachy-mini-daemon --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Run the demo&lt;/h2&gt; 
&lt;p&gt;Conversational demo for the Reachy Mini robot combining LLM realtime APIs, vision pipelines, and choreographed motion libraries: &lt;a href="https://github.com/pollen-robotics/reachy_mini_conversation_demo"&gt;reachy_mini_conversation_demo&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Using the Python SDK&lt;/h2&gt; 
&lt;p&gt;The API is designed to be simple and intuitive. You can control the robot's features such as the head, antennas, camera, speakers, and microphone. For instance, to move the head of the robot, you can use the &lt;code&gt;goto_target&lt;/code&gt; method as shown in the example below:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from reachy_mini import ReachyMini
from reachy_mini.utils import create_head_pose

with ReachyMini() as reachy_mini:
    # Move the head up (10mm on z-axis) and roll it 15 degrees
    pose = create_head_pose(z=10, roll=15, degrees=True, mm=True)
    reachy_mini.goto_target(head=pose, duration=2.0)

    # Reset to default pose
    pose = create_head_pose() 
    reachy_mini.goto_target(head=pose, duration=2.0)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For a full description of the SDK, please refer to the &lt;a href="https://raw.githubusercontent.com/pollen-robotics/reachy_mini/develop/docs/python-sdk.md"&gt;Python SDK documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Using the REST API&lt;/h2&gt; 
&lt;p&gt;The daemon also provides a REST API via &lt;a href="https://fastapi.tiangolo.com/"&gt;fastapi&lt;/a&gt; that you can use to control the robot and get its state. The API is accessible via HTTP and WebSocket.&lt;/p&gt; 
&lt;p&gt;By default, the API server runs on &lt;code&gt;http://localhost:8000&lt;/code&gt;. The API is documented using OpenAPI, and you can access the documentation at &lt;code&gt;http://localhost:8000/docs&lt;/code&gt; when the daemon is running.&lt;/p&gt; 
&lt;p&gt;More information about the API can be found in the &lt;a href="https://raw.githubusercontent.com/pollen-robotics/reachy_mini/develop/docs/rest-api.md"&gt;HTTP API documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Open source &amp;amp; contribution&lt;/h2&gt; 
&lt;p&gt;This project is actively developed and maintained by the &lt;a href="https://www.pollen-robotics.com"&gt;Pollen Robotics team&lt;/a&gt; and the &lt;a href="https://huggingface.co/"&gt;Hugging Face team&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We welcome contributions from the community! If you want to report a bug or request a feature, please open an issue on GitHub. If you want to contribute code, please fork the repository and submit a pull request.&lt;/p&gt; 
&lt;h3&gt;3D models&lt;/h3&gt; 
&lt;p&gt;TODO&lt;/p&gt; 
&lt;h3&gt;Contributing&lt;/h3&gt; 
&lt;p&gt;Development tools are available in the optional dependencies.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -e .[dev]
pre-commit install
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Your files will be checked before any commit. Checks may also be manually run with&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pre-commit run --all-files
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Checks are performed by Ruff. You may want to &lt;a href="https://docs.astral.sh/ruff/editors/setup/"&gt;configure your IDE to support it&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the Apache 2.0 License - see the &lt;a href="https://raw.githubusercontent.com/pollen-robotics/reachy_mini/develop/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;p&gt;The robot design files are licensed under the &lt;a href="https://raw.githubusercontent.com/pollen-robotics/reachy_mini/develop/TODO"&gt;TODO&lt;/a&gt; license.&lt;/p&gt; 
&lt;h3&gt;Simulation model used&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://polyhaven.com/a/food_apple_01"&gt;https://polyhaven.com/a/food_apple_01&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://polyhaven.com/a/croissant"&gt;https://polyhaven.com/a/croissant&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://polyhaven.com/a/wooden_table_02"&gt;https://polyhaven.com/a/wooden_table_02&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://polyhaven.com/a/rubber_duck_toy"&gt;https://polyhaven.com/a/rubber_duck_toy&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>rossant/awesome-math</title>
      <link>https://github.com/rossant/awesome-math</link>
      <description>&lt;p&gt;A curated list of awesome mathematics resources&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Awesome Math &lt;a href="https://github.com/sindresorhus/awesome"&gt;&lt;img src="https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg?sanitize=true" alt="Awesome" /&gt;&lt;/a&gt;&lt;/h1&gt; 
&lt;p&gt;A curated list of awesome mathematics resources.&lt;/p&gt; 
&lt;p&gt;All resources are freely available except those with a 💲 icon.&lt;/p&gt; 
&lt;h1&gt;Contents&lt;/h1&gt; 
&lt;!-- START_TOC --&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#contents"&gt;Contents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#general-resources"&gt;General Resources&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#learning-platforms"&gt;Learning Platforms&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#learn-to-learn"&gt;Learn to Learn&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#youtube-series"&gt;Youtube Series&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#tools"&gt;Tools&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#questions-and-answers"&gt;Questions and Answers&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#encyclopedia"&gt;Encyclopedia&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#books"&gt;Books&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#magazines"&gt;Magazines&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#blogs"&gt;Blogs&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#meetings-and-conferences"&gt;Meetings and Conferences&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#misc"&gt;Misc&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#branches-of-mathematics"&gt;Branches of Mathematics&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#foundations-of-mathematics"&gt;Foundations of Mathematics&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#transition-to-pure-rigour-math"&gt;Transition To Pure Rigour Math&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#set-theory"&gt;Set Theory&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#logic"&gt;Logic&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#category-theory"&gt;Category Theory&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#type-theory"&gt;Type Theory&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#homotopy-type-theory"&gt;Homotopy Type Theory&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#surreal-numbers"&gt;Surreal Numbers&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#number-theory"&gt;Number Theory&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#algebraic-number-theory"&gt;Algebraic Number Theory&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#analytic-number-theory"&gt;Analytic Number Theory&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#algebra"&gt;Algebra&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#abstract-algebra"&gt;Abstract Algebra&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#group-theory"&gt;Group Theory&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#linear-algebra"&gt;Linear Algebra&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#ring-theory"&gt;Ring Theory&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#galois-theory"&gt;Galois Theory&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#lie-algebras"&gt;Lie Algebras&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#combinatorics"&gt;Combinatorics&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#graph-theory"&gt;Graph Theory&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#geometry-and-topology"&gt;Geometry and Topology&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#differential-geometry"&gt;Differential Geometry&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#algebraic-geometry"&gt;Algebraic Geometry&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#algebraic-statistics"&gt;Algebraic Statistics&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#topology"&gt;Topology&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#algebraic-topology"&gt;Algebraic Topology&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#analysis"&gt;Analysis&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#real-analysis"&gt;Real Analysis&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#harmonic-analysis"&gt;Harmonic Analysis&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#complex-analysis"&gt;Complex Analysis&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#functional-analysis"&gt;Functional Analysis&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#measure-theory"&gt;Measure Theory&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#ordinary-differential-equations"&gt;Ordinary Differential Equations&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#partial-differential-equations"&gt;Partial Differential Equations&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#chaos-theory"&gt;Chaos Theory&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#probability-and-statistics"&gt;Probability and Statistics&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#probability-theory"&gt;Probability Theory&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#statistics"&gt;Statistics&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#statistical-learning"&gt;Statistical Learning&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#stochastic-processes"&gt;Stochastic processes&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#numerical-analysis"&gt;Numerical Analysis&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#signal-processing"&gt;Signal processing&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#mathematics-for-computer-science"&gt;Mathematics for Computer Science&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#mathematical-biology"&gt;Mathematical Biology&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#mathematical-physics"&gt;Mathematical Physics&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#students-lecture-notes"&gt;Students Lecture Notes&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#related-awesome-lists"&gt;Related Awesome Lists&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- END_TOC --&gt; 
&lt;h1&gt;General Resources&lt;/h1&gt; 
&lt;h2&gt;Learning Platforms&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.khanacademy.org/math"&gt;Khan Academy&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.coursera.org/courses?query=mathematics&amp;amp;languages=en"&gt;Coursera&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://ocw.mit.edu/courses/mathematics/"&gt;MIT OpenCourseWare&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.edx.org/course/subject/math"&gt;edX&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://brilliant.org/courses/#math-foundational"&gt;Brilliant&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://misterwootube.com/"&gt;WooTube&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mathigon.org/"&gt;Mathigon&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://calculus.org/"&gt;Calculus.org&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ximera.osu.edu/"&gt;Ximera&lt;/a&gt; : free interactive mathematics textbooks (Ohio State University)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.almostfun.org/lessons/"&gt;Almost Fun&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/c/OxfordMathematics"&gt;Oxford Mathematics&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mathacademy.com/"&gt;Math Academy&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Learn to Learn&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/nelson-brochado/understanding-math"&gt;Understanding Mathematics&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Youtube Series&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/@BrandonFoltz"&gt;Brandon Foltz&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw"&gt;StatQuest&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/@3blue1brown"&gt;3Blue1Brown&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/@iit"&gt;NPTEL&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/@patrickjmt"&gt;PatrickJMT&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/@ProfessorLeonard"&gt;Professor Leonard&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://www.youtube.com/playlist?list=PLDesaqWTN6ESsmwELdrzhcGiRhk5DjwLP"&gt;Precalculus - College Algebra/Trigonometry&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.youtube.com/playlist?list=PLF797E961509B4EB5"&gt;Calculus 1&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.youtube.com/playlist?list=PLDesaqWTN6EQ2J4vgsN1HyBeRADEh4Cw-"&gt;Calculus 2&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.youtube.com/playlist?list=PLDesaqWTN6ESk16YRmzuJ8f6-rnuy0Ry7"&gt;Calculus 3&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.youtube.com/playlist?list=PLDesaqWTN6ESPaHy2QUKVaXNZuQNxkYQ_"&gt;Differential Equations&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.youtube.com/playlist?list=PLDesaqWTN6ETc1ZwHWijCBcZ2gOvS2tTN"&gt;To The Point Math&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/@crashcourse"&gt;Crash Course&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/@harvard"&gt;Harvard&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/@mitocw"&gt;MIT OpenCourseWare&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/@Mathologer"&gt;Mathologer&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/@TheMathDistrict"&gt;The Math District&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/@mathematicalmonk"&gt;Mathematical Monk&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/@TheMathSorcerer"&gt;The Math Sorcerer&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Tools&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.symbolab.com/"&gt;Symbolab&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.desmos.com/calculator"&gt;Desmos&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://www.mathwords.com/"&gt;Math Words&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://www.wolframalpha.com/"&gt;Wolfram Alpha&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://maxima.sourceforge.io/"&gt;Maxima&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.sympy.org/"&gt;Sympy&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://www.sagemath.org/"&gt;Sagemath&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Nonanti/MathFlow"&gt;MathFlow&lt;/a&gt; - C# math expression library with symbolic computation (differentiation, simplification, equation solving)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://unitconverters.net"&gt;Unit Converter&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.geogebra.org/?lang=en"&gt;GeoGebra&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://www2.macaulay2.com/Macaulay2/"&gt;Macaulay2&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.singular.uni-kl.de/"&gt;Singular&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.gnu.org/software/octave/"&gt;GNU Octave&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://magma.maths.usyd.edu.au/magma/"&gt;Magma&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.maplesoft.com/products/Maple/"&gt;Maple&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.mathworks.com/products/matlab.html"&gt;Matlab&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.wolfram.com/mathematica/"&gt;Wolfram Mathematica&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://freemathapp.org"&gt;Free Math&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://chrome.google.com/webstore/detail/xhub/anidddebgkllnnnnjfkmjcaallemhjee"&gt;xhub&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.copypastemathjax.com/"&gt;CopyPasteMathjax&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.financecharts.com/pages/5724-retirement-calculators-and-stock-market-tips"&gt;Finance calculators&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mathcheap.xyz"&gt;Mathcheap&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://midpointcalculator.co"&gt;Midpoint Calculator&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://quartilecalculator.net"&gt;Quartiles Calculator&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://corca.io/"&gt;Corca Editor&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Questions and Answers&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="http://math.stackexchange.com/"&gt;Mathematics Stack Exchange&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://mathoverflow.net/"&gt;MathOverflow&lt;/a&gt; - for professional mathematicians&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Encyclopedia&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.encyclopediaofmath.org"&gt;Encyclopedia of Mathematics&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://planetmath.org/"&gt;Planetmath&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://proofwiki.org/wiki/Main_Page"&gt;ProofWiki&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://mathworld.wolfram.com/"&gt;Wolfram Mathworld&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://oeis.org"&gt;The On-Line Encyclopedia of Integer Sequences&lt;/a&gt; - Great compendium of many different integer sequences. Founded 1964 by N. J. A. Sloane.&lt;/li&gt; 
 &lt;li&gt;💲 &lt;a href="https://press.princeton.edu/books/hardcover/9780691118802/the-princeton-companion-to-mathematics"&gt;The Princeton Companion to Mathematics&lt;/a&gt; - Timothy Gowers (Professor, Fields medallist), June Barrow-Green (Professor), and Imre Leader (Professor).&lt;/li&gt; 
 &lt;li&gt;💲 &lt;a href="https://link.springer.com/book/10.1007/978-3-662-52844-0"&gt;Encyclopedia of Distances (4th Edition)&lt;/a&gt; - Michel Marie Deza, Elena Deza.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Books&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://archive.org/details/TarasovCalculus"&gt;Calculus: Basic Concepts for High Schools&lt;/a&gt; - L.V. Tarasov&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://www.cis.upenn.edu/~jean/math-basics.pdf"&gt;Basics of Algebra, Topology, and Differential Calculus&lt;/a&gt; - Jean Gallier (University of Pennsylvania)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://people.math.gatech.edu/%7Ecain/notes/calculus.html"&gt;Multivariable Calculus&lt;/a&gt; - G. Cain, J. Herod (Georgia Tech)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://en.wikibooks.org/wiki/Wikibooks:Mathematics_bookshelf"&gt;Wikibooks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://people.math.gatech.edu/~cain/textbooks/onlinebooks.html"&gt;Online Mathematics Textbooks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://www.wallace.ccfaculty.org/book/Beginning_and_Intermediate_Algebra.pdf"&gt;Beginning and Intermediate Algebra&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/EbookFoundation/free-programming-books/raw/master/books/free-programming-books-subjects.md#mathematics"&gt;Free Mathematics Books&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://www.mecmath.net/trig/trigbook.pdf"&gt;Trigonometry&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.manning.com/books/math-for-frontend-web-dev"&gt;Math for Frontend Web Dev&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.manning.com/books/grokking-statistics"&gt;Grokking Statistics&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Magazines&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.quantamagazine.org/mathematics/"&gt;Quanta Magazine&lt;/a&gt; - Features latest research breakthroughs in an accessible style for non-experts.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.ams.org/journals/bull/all_issues.html"&gt;Bulletin of the American Mathematical Society&lt;/a&gt; - Expository articles on contemporary mathematical research, written in a way that gives insight to mathematicians who may not be experts in the particular topic.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://www.ams.org/cgi-bin/notices/amsnotices.pl?article_id=fullissue&amp;amp;article_type=gallery&amp;amp;gallery_type=fullissue"&gt;Notices of the American Mathematical Society&lt;/a&gt; - Publicizes activities of the Society and features surveys, reports, news, announcements, and opinions on industry trends, academia, and research.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://euromathsoc.org/magazine"&gt;European Mathematical Society Magazine&lt;/a&gt; - The Magazine features announcements about meetings and conferences, articles outlining current trends in scientific development, reports on member societies, and many other informational items.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ima.org.uk/publications/mathematics-today/"&gt;Mathematics Today by Institute of Mathematics and its Applications&lt;/a&gt; - News, opinions, and articles related to mathematics, so the reader stays updated.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://cms.math.ca/publications/crux/"&gt;Crux Mathematicorum by Canadian Mathematical Society&lt;/a&gt; - source of unique and challenging mathematical problems designed for the secondary and undergraduate levels. It includes an Olympiad Corner which is helpful for math competitions.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Blogs&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://betterexplained.com/"&gt;BetterExplained&lt;/a&gt; - Maintained by Kalid Azad&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://ilovemaths.com/"&gt;ILoveMaths&lt;/a&gt; - For grades 6 thru 12 in K-12 system&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.3blue1brown.com/"&gt;3blue1brown&lt;/a&gt; - Animated Maths&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.mathsisfun.com"&gt;Mathsisfun&lt;/a&gt; simple text lightweight site for students up to highschool&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://calculus123.com/wiki/Peter_Saveliev"&gt;MathematicsIsAScience&lt;/a&gt; - Peter Saveliev (Professor of mathematics at Marshall University, Huntington WV, USA)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Meetings and Conferences&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://mathsjam.com/"&gt;MathsJam&lt;/a&gt; - monthly local recreational maths/puzzle meetups and an annual gathering in Staffordshire, England&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://talkingmathsinpublic.uk/"&gt;Talking Maths in Public&lt;/a&gt; - a conference for maths communicators, running every two years, usually in the UK&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.bridgesmathart.org/"&gt;Bridges&lt;/a&gt; - an annual conference on mathematical connections in art, music, architecture, and culture. The 2025 meeting is in Eindhoven, Netherlands.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Misc&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Areas_of_mathematics"&gt;Areas of mathematics on Wikipedia&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://tutorial.math.lamar.edu/"&gt;Paul's Online Math Notes&lt;/a&gt; - Paul Dawkins (Lamar University)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://faculty.atu.edu/mfinan/nnotes.html"&gt;List of electronic textbooks&lt;/a&gt; - Marcel B. Finan (Arkansas Tech University)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://at.yorku.ca/topology/"&gt;Topology Atlas&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://djm.cc/library/Recreations_in_Mathematics_Licks_edited.pdf"&gt;Recreations in Math&lt;/a&gt; - H. E. Licks (1917)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://djm.cc/library/Magic_Squares_Cubes_Andrews_edited.pdf"&gt;Magic Squares and Cubes&lt;/a&gt; - W. S. Andrews (1917)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://web.stanford.edu/~boyd/cvxbook/"&gt;Convex Optimization&lt;/a&gt; - Stephen Boyd and Lieven Vandenberghe&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://fabricebaudoin.wordpress.com/"&gt;Fabrice Baudoin's Notes&lt;/a&gt; - Both research and lecture notes on many topics, Including Diffusions on foliated manifold, Stochastic Calculus, Global analysis in Dirichlet spaces, and more.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Branches of Mathematics&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;Content Format&lt;/strong&gt; &lt;br /&gt; 📖 Books &lt;br /&gt; 🎥 Videos &lt;br /&gt; 📝 Lecture notes, slides, articles, papers&lt;/p&gt; 
&lt;h2&gt;Foundations of Mathematics&lt;/h2&gt; 
&lt;h3&gt;Transition To Pure Rigour Math&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.trillia.com/zakon1.html"&gt;Basic Concepts of Mathematics&lt;/a&gt; - Elias Zakon&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://richardhammack.github.io/BookOfProof/"&gt;Book of Proof&lt;/a&gt; - Richard Hammak (Virginia Commonwealth University)&lt;/li&gt; 
 &lt;li&gt;📖 &lt;a href="https://ia800501.us.archive.org/7/items/how-to-prove-it-a-structured-approach-daniel-j.-velleman/How%20to%20Prove%20It%20A%20Structured%20Approach%20%28Daniel%20J.%20Velleman%29.pdf"&gt;How to Prove It: A Structured Approach (3rd Edition)&lt;/a&gt; - Daniel J. Velleman (Professor).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Set Theory&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.cosc.brocku.ca/~duentsch/papers/methprimer1.html"&gt;Sets, Relations, Functions&lt;/a&gt; - Ivo Düntsch, Günther Gediga&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.math.toronto.edu/weiss/set_theory.pdf"&gt;An Introduction to Set Theory&lt;/a&gt; - William A. R. Weiss&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.settheory.net/"&gt;Set Theory and Foundations of Mathematics&lt;/a&gt; - Sylvain Poirier&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://plato.stanford.edu/entries/set-theory/"&gt;Set Theory on the Stanford Encyclopedia of Philosophy&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Logic&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;📝 &lt;a href="https://pdfs.semanticscholar.org/6967/f52773d9c2ccfc94658657a5761e0f00e95a.pdf"&gt;Introduction to Logic&lt;/a&gt; - Michael Genesereth, Eric Kao (Stanford University)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://www.fecundity.com/codex/forallx.pdf"&gt;An Introduction to Formal Logic&lt;/a&gt; - P.D. Magnus (University at Albany)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://euclid.trentu.ca/math/sb/pcml/pcml-16.pdf"&gt;A Problem Course in Mathematical Logic&lt;/a&gt; - Stefan Bilaniuk (Trent University)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://poincare.matf.bg.ac.rs/~zarkom/Book_Math__Cutland_Computability.pdf"&gt;Computability - An introduction to recursive function theory&lt;/a&gt; - Nigel Cutland (University of Hull)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://homepages.uc.edu/~martinj/Symbolic_Logic/341%20Syllabus,%20Textbook,%20Handouts,%20Notes/LPL%20textbook.pdf"&gt;Language, Proof, and Logic&lt;/a&gt; - Jon Barwise, John Etchemendy&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.mathematik.uni-muenchen.de/~schwicht/lectures/logic/ws03/ml.pdf"&gt;Mathematical Logic&lt;/a&gt; - Helmut Schwichtenberg&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.personal.psu.edu/t20/notes/logic.pdf"&gt;Mathematical Logic&lt;/a&gt; - Stephen G. Simpson (Pennsylvania State University)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://maude.sip.ucm.es/~miguelpt/papers/flogic.pdf"&gt;Formal Logic&lt;/a&gt; - Miguel Palomino&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://web.math.princeton.edu/~nelson/books/pa.pdf"&gt;Predictive Arithmetic&lt;/a&gt; - Edward Nelson&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://people.uleth.ca/~dave.morris/books/proofs+concepts.html"&gt;Proofs and Concepts: the fundamentals of abstract mathematics&lt;/a&gt; - Joy Morris, Dave Morris&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://www.tedsundstrom.com/mathreasoning"&gt;Mathematical Reasoning: Writing and Proof&lt;/a&gt; - Ted Sundstrom&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://leanprover.github.io/logic_and_proof/"&gt;Logic and Proof&lt;/a&gt; - Jeremy Avigad, Robert Y. Lewis, and Floris van Doorn&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://teorth.github.io/QED"&gt;QED - an interactive textbook&lt;/a&gt; - Terence Tao&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://builds.openlogicproject.org/"&gt;Open Logic Textbook&lt;/a&gt; - collaborative effort, main contributors listed &lt;a href="https://openlogicproject.org/people/"&gt;here&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Category Theory&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.mathematik.tu-darmstadt.de/~streicher/CTCL.pdf"&gt;Introduction to Category Theory and Categorical Logic&lt;/a&gt; - Thomas Streicher&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.cs.man.ac.uk/~hsimmons/zCATS.pdf"&gt;An Introduction to Category Theory&lt;/a&gt; - Harold Simmons&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.211.4754&amp;amp;rep=rep1&amp;amp;type=pdf"&gt;Category Theory&lt;/a&gt; - Steve Awodey (Carnegie Mellon University)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.mathematik.uni-muenchen.de/~pareigis/Vorlesungen/04SS/Cats1.pdf"&gt;Category Theory&lt;/a&gt; - B. Pareigis&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://web.archive.org/web/20181221233252/http://www.math.mcgill.ca/triples/Barr-Wells-ctcs.pdf"&gt;Category Theory for Computing Science&lt;/a&gt; - Michael Barr, Charles Wells&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.tac.mta.ca/tac/reprints/articles/12/tr12.pdf"&gt;Toposes, Triples and Theories&lt;/a&gt; - Michael Barr, Charles Wells&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.tac.mta.ca/tac/reprints/articles/3/tr3abs.html"&gt;Abelian Categories&lt;/a&gt; - Peter Freyd&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.tac.mta.ca/tac/reprints/articles/7/tr7abs.html"&gt;Categories and Groupoids&lt;/a&gt; - P. J. Higgins&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.tac.mta.ca/tac/reprints/articles/10/tr10abs.html"&gt;Basic Concepts of Enriched Category Theory&lt;/a&gt; - G. M. Kelley&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.tac.mta.ca/tac/reprints/articles/17/tr17abs.html"&gt;Abstract and Concrete Categories: The Joy of Cats&lt;/a&gt; - Jiri Adamek, Horst Herrlich, George Strecker&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://math.mit.edu/~dspivak/teaching/sp18/7Sketches.pdf"&gt;Seven Sketches in Compositionality: An Invitation to Applied Category Theory&lt;/a&gt; - Brendan Fong and David I. Spivak (MIT)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.math.jhu.edu/~eriehl/context/"&gt;Category Theory in Context&lt;/a&gt; - Emily Riehl (John Hopkins University)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Type Theory&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.paultaylor.eu/stable/prot.pdf"&gt;Proofs and Types&lt;/a&gt; - Jean-Yves Girard&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://archive-pml.github.io/martin-lof/pdfs/Bibliopolis-Book-retypeset-1984.pdf"&gt;Intuitionistic Type Theory&lt;/a&gt; - Per Martin-Lof&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://www.cs.kent.ac.uk/people/staff/sjt/TTFP/"&gt;Type Theory and Functional Programming&lt;/a&gt; - Simon Thompson&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.cse.chalmers.se/research/group/logic/book/book.pdf"&gt;Programming in Martin-Lof’s Type Theory&lt;/a&gt; - Bengt Nordstrom, Kent Petersson, Jan M. Smith&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Homotopy Type Theory&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;📝 &lt;a href="https://hottheory.files.wordpress.com/2013/03/hott-online-611-ga1a258c.pdf"&gt;Homotopy Type Theory&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Surreal Numbers&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.math.harvard.edu/~knill/teaching/mathe320_2015_fall/blog15/surreal1.pdf"&gt;Surreal Numbers - How two ex-students turned on to pure mathematics and found total happiness&lt;/a&gt; - D. E. Knuth&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://web.mit.edu/sp.268/www/2010/surreal.pdf"&gt;Surreal Numbers and Games&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.ohio.edu/people/ehrlich/ConwayNames.pdf"&gt;Conway names, the simplicity hierarchy and the surreal number tree&lt;/a&gt; - Philip Ehrlich&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Number Theory&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;📝 &lt;a href="http://wstein.org/ent/ent.pdf"&gt;Elementary Number Theory: Primes, Congruences, and Secrets&lt;/a&gt; - William Stein&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://math.utoledo.edu/~codenth/Spring_13/3200/ENT-books/Elementary_Number_Theory-Clark.pdf"&gt;Elementary Number Theory&lt;/a&gt; - W. Edwin Clark (University of South Florida)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.maths.qmul.ac.uk/~pjc/notes/nt.pdf"&gt;A Course on Number Theory&lt;/a&gt; - Peter J. Cameron&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://shoup.net/ntb/ntb-v2.pdf"&gt;A Computational Introduction to Number Theory and Algebra&lt;/a&gt; - Victor Shoup&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://alpha.math.uga.edu/~pete/4400FULL.pdf"&gt;Number Theory: A Contemporary Introduction&lt;/a&gt; - Pete L. Clark&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.trillia.com/moser-number.html"&gt;An Introduction to the Theory of Numbers&lt;/a&gt; - Leo Moser&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://www.poritz.net/jonathan/share/yaintt/"&gt;Yet Another Introductory Number Theory Textbook&lt;/a&gt; - Jonathan A. Poritz&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Algebraic Number Theory&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;📝 &lt;a href="https://feog.github.io/ANT10.pdf"&gt;Introduction to Algebraic Number Theory&lt;/a&gt; - F. Oggier&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.jmilne.org/math/CourseNotes/ANT.pdf"&gt;Algebraic Number Theory&lt;/a&gt; - J.S. Milne&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://people.math.gatech.edu/~mbaker/pdf/ANTBook.pdf"&gt;Algebraic Number Theory Course Notes&lt;/a&gt; - Matthew Baker (Georgia Tech)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.math.uiuc.edu/~r-ash/ANT.html"&gt;A Course In Algebraic Number Theory&lt;/a&gt; - Robert Ash&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Analytic Number Theory&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.math.uiuc.edu/~hildebr/ant/main.pdf"&gt;Introduction to Analytic Number Theory&lt;/a&gt; - A.J. Hildebrand (University of Illinois)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://math.nsc.ru/~vdovin/lectures/numth_eng.pdf"&gt;Elements of Analytic Number Theory&lt;/a&gt; - P. S. Kolesnikov, E. P. Vdovin (Novosibirsk)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.mathematik.uni-muenchen.de/~forster/v/ann/annth_all.pdf"&gt;Analytic Number Theory&lt;/a&gt; - Otto Forster (LMU Munich)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www2.math.uu.se/~astrombe/analtalt08/www_notes.pdf"&gt;Analytic Number Theory - Lecture Notes based on Davenport’s book&lt;/a&gt; - Andreas Strömbergsson&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Algebra&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.math.uwaterloo.ca/~snburris/htdocs/ualg.html"&gt;A Course in Universal Algebra&lt;/a&gt; - S. Burris, H.P. Sankappanavar&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://faculty.math.illinois.edu/~r-ash/ComAlg.html"&gt;A Course in Commutative Algebra&lt;/a&gt; - Robert Ash&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://djm.cc/library/First_Algebra_Hawkes_Luby_Touton_edited.pdf"&gt;First Course in Algebra&lt;/a&gt; - Herbert E. Hawkes, William A. Luby, Frank C. Touton (1910)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://djm.cc/library/Second_Algebra_Hawkes_Luby_Touton_edited.pdf"&gt;Second Course in Algebra&lt;/a&gt; - Herbert E. Hawkes, William A. Luby, Frank C. Touton (1911)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://djm.cc/library/Algebra_Elementary_Text-Book_Part_I_Chrystal_edited.pdf"&gt;Algebra: An Elementary Text-Book, Part I&lt;/a&gt; - G. Chrystal (1904)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://djm.cc/library/Algebra_Elementary_Text-Book_Part_II_Chrystal_edited02.pdf"&gt;Algebra: An Elementary Text-Book, Part II&lt;/a&gt; - G. Chrystal (1900)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://jamesbrennan.org/algebra"&gt;Understanding Algebra&lt;/a&gt; - James W. Brennan&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Abstract Algebra&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;📝 &lt;a href="https://zodml.org/sites/default/files/Introduction_to_Abstract_Algebra_0.pdf"&gt;Introduction to Abstract Algebra&lt;/a&gt; - D. S. Malik, John N. Mordeson, M.K. Sen (Creighton University)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://aleph0.clarku.edu/~djoyce/ma225/algebra.pdf"&gt;Introduction to Modern Algebra&lt;/a&gt; - David Joyce (Clark University)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://feog.github.io/AA11.pdf"&gt;Algebraic Methods&lt;/a&gt; - F. Oggier&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://abstract.ups.edu/download/aata-20150812.pdf"&gt;Abstract Algebra : Theory and Applications&lt;/a&gt; - Thomas W. Judson, Robert A. Beezer (Austin State University)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.maths.usyd.edu.au/u/bobh/UoS/rfwhole.pdf"&gt;An Undergraduate Course in Abstract Algebra&lt;/a&gt; - Robert Howlett&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.math.miami.edu/~ec/book"&gt;Elements of Abstract and Linear Algebra&lt;/a&gt; - E.H. Connell (University of Miami)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.math.uiuc.edu/~r-ash/Algebra.html"&gt;Abstract Algebra: The Basic Graduate Year&lt;/a&gt; - Robert Ash&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://web.archive.org/web/20150528171650/extension.harvard.edu/open-learning-initiative/abstract-algebra"&gt;Abstract Algebra: Harvard Extension (Archived)&lt;/a&gt; - Benedict Gross&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://www.youtube.com/playlist?list=PLA58AC5CABC1321A3"&gt;Abstract Algebra: Harvard Extension Videos&lt;/a&gt; - Benedict Gross&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Group Theory&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;📝 &lt;a href="https://www2.bc.edu/mark-reeder/Groups.pdf"&gt;Notes on Group Theory&lt;/a&gt; - Mark Reeder&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.jmilne.org/math/CourseNotes/GT.pdf"&gt;Group Theory&lt;/a&gt; - J.S. Milne&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.maths.qmul.ac.uk/~pjc/notes/gt.pdf"&gt;Notes on Finite Group Theory&lt;/a&gt; - Peter J. Cameron&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.cns.gatech.edu/GroupTheory/index.html"&gt;Group Theory&lt;/a&gt; - Pedrag Civitanovic&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Linear Algebra&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.math.ubc.ca/~carrell/NB.pdf"&gt;Fundamentals of Linear Algebra&lt;/a&gt; - James B. Carrell&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://web.archive.org/web/20140824074655/http://mathstat.helsinki.fi/~fluch/linear_algebra_1-sp07/la1.pdf"&gt;Linear Algebra and Matrices&lt;/a&gt; - Martin Fluch&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.maths.usyd.edu.au/u/bobh/UoS/MATH2902/vswhole.pdf"&gt;Vector Space Theory&lt;/a&gt; - Robert Howlett&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://joshua.smcvt.edu/linearalgebra"&gt;Linear Algebra&lt;/a&gt; - Jim Hefferon&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://github.com/juanklopper/MIT_OCW_Linear_Algebra_18_06"&gt;MIT OpenCourseWare Lectures on Linear Algebra (18.06) as Jupyter Notebooks&lt;/a&gt; - Juan Klopper&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.numbertheory.org/book/"&gt;Elementary Linear Algebra&lt;/a&gt; - Keith Matthews&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://linear.ups.edu/"&gt;A First Courses in Linear Algebra&lt;/a&gt; - Rob Breezer&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://www.math.ucdavis.edu/~linear/"&gt;Linear Algebra&lt;/a&gt; - David Cherney, Tom Denton, Andrew Waldron&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://oaktrust.library.tamu.edu/handle/1969.1/2502"&gt;Introduction to vectors and tensors, Vol 1: linear and multilinear algebra&lt;/a&gt; - Ray M Bowen, C. C. Wang&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://oaktrust.library.tamu.edu/handle/1969.1/3609"&gt;Introduction to vectors and tensors, Vol 2: vector and tensor analysis&lt;/a&gt; - Ray M Bowen, C. C. Wang&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://web.stanford.edu/~boyd/vmls/vmls.pdf"&gt;Introduction to Applied Linear Algebra&lt;/a&gt; - Stephen Boyd (Stanford University), Lieven Vandenberghe (UCLA)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://www.math.brown.edu/~treil/papers/LADW/LADW_2017-09-04.pdf"&gt;Linear Algebra Done Wrong&lt;/a&gt; - Sergei Treil&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://immersivemath.com/ila/index.html"&gt;Immersive Linear Algebra&lt;/a&gt; - J. Ström, K. Åström, and T. Akenine-Möller&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://textbooks.math.gatech.edu/ila/"&gt;Interactive Linear Algebra&lt;/a&gt; - Dan Margalit and Joseph Rabinoff&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://people.math.gatech.edu/~herod/Hspace/Hspace.html"&gt;Linear Algebra, Infinite Dimensions, and Maple&lt;/a&gt; - James Herod&lt;/li&gt; 
 &lt;li&gt;📖 &lt;a href="https://linear.axler.net/"&gt;Linear Algebra Done Right&lt;/a&gt; - Sheldon Axler&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Ring Theory&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.math.uni-duesseldorf.de/~wisbauer/book.pdf"&gt;Foundations of Module and Ring Theory&lt;/a&gt; - Robert Wisbauer (University of Düsseldorf)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Galois Theory&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.maths.gla.ac.uk/~ajb/dvi-ps/Galois.pdf"&gt;An Introduction to Galois Theory&lt;/a&gt; - Andrew Baker (University of Glasgow)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.jmilne.org/math/CourseNotes/FT.pdf"&gt;Fields and Galois Theory&lt;/a&gt; - J.S. Milne&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://homepages.warwick.ac.uk/~masda/MA3D5/Galois.pdf"&gt;Galois theory&lt;/a&gt; - Miles Reid&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://eclass.uoa.gr/modules/document/file.php/MATH594/Stewart%20Galois%204th%20edition.pdf"&gt;Galois Theory&lt;/a&gt; - Ian Stewart&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://arxiv.org/pdf/2408.07499"&gt;Galois Theory&lt;/a&gt; — Tom Leinster (University of Edinburgh)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Lie Algebras&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.math.harvard.edu/~shlomo/docs/lie_algebras.pdf"&gt;Lie Algebras&lt;/a&gt; - Shlomo Sternberg&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Combinatorics&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.math.utk.edu/~wagner/papers/comb.pdf"&gt;Basic Combinatorics&lt;/a&gt; - Carl G. Wagner (University of Tennessee)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://people.math.gatech.edu/~trotter/book.pdf"&gt;Applied Combinatorics&lt;/a&gt; - Mitchel T. Keller, William T. Trotter&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.maths.qmul.ac.uk/~pjc/notes/comb.pdf"&gt;Notes on Combinatorics&lt;/a&gt; - Peter J. Cameron&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://algo.inria.fr/flajolet/Publications/book.pdf"&gt;Analytic Combinatorics&lt;/a&gt; - Philippe Flajolet, Robert Sedgewick&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.math.upenn.edu/~wilf/DownldGF.html"&gt;generatingfunctionology&lt;/a&gt; - Herbert Wilf&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Graph Theory&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.personal.psu.edu/cxg286/Math485.pdf"&gt;Graph Theory: Lecture Notes&lt;/a&gt; - Christopher Griffin&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.cs.unibo.it/babaoglu/courses/cas00-01/tutorials/GraphTheory.pdf"&gt;Graph Theory&lt;/a&gt; - Reinhard Diestel&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://learngraphtheory.org/"&gt;Graph Theory : Interactive Algorithm Visualizer | Graph Theory Learning Platform&lt;/a&gt; - Hadjoudj Mohammed Islam&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Geometry and Topology&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;📝 &lt;a href="http://polly.phys.msu.ru/~belyaev/geometry.pdf"&gt;Fundamentals of Geometry&lt;/a&gt; - Oleg A. Belyaev&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://www.math.upenn.edu/~wilf/AeqB.html"&gt;A=B&lt;/a&gt; - M. Petkovsek, H. Wilf, D. Zeilberger&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://aleph0.clarku.edu/~djoyce/java/elements/toc.html"&gt;Elements&lt;/a&gt; - Euclid&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://starrhorse.com/euclid/"&gt;Euclid's Elements Redux&lt;/a&gt; - Daniel Callahan&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.math.ubc.ca/~cass/graphics/manual/"&gt;Mathematical Illustrations&lt;/a&gt; - Bill Casselman&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://www.c82.net/euclid/"&gt;Byrne's Euclid&lt;/a&gt; - Oliver Byrne&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://djm.cc/library/Plane_Geometry_Wentworth_Smith_edited.pdf"&gt;Plane Geometry&lt;/a&gt; - George Wentworth and David Eugene Smith (1913)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://djm.cc/library/Plane_Spherical_Trigonometry_Wentworth_Smith_edited_2.pdf"&gt;Planes and Spherical Trigonometry&lt;/a&gt; - George Wentworth and David Eugene Smith (1915)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://djm.cc/library/Coordinate_Geometry_Fine_Thompson_edited03.pdf"&gt;Coordinate Geometry&lt;/a&gt; - Henry Buchard Fine and Henry Dallas Thompson (1911)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://djm.cc/library/Analytic_Geometry_Siceloff_Wentworth_Smith_edited.pdf"&gt;Analytic Geometry&lt;/a&gt; - Lewis Parker Siceloff, George Wentworth, David Eugene Smith (1922)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Differential Geometry&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;📝 &lt;a href="https://people.math.ethz.ch/~salamon/PREPRINTS/diffgeo.pdf"&gt;Introduction to Differential Geometry&lt;/a&gt; - Joel W. Robbin, Dietmar A. Salamon&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://www.cis.upenn.edu/~jean/gbooks/manif.html"&gt;Notes on Differential Geometry and Lie Groups&lt;/a&gt; - Jean Gallier (University of Pennsylvania)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.mat.univie.ac.at/~michor/dgbook.pdf"&gt;Topics in Differential Geometry&lt;/a&gt; - Peter W. Michor&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://mysite.science.uottawa.ca/rossmann/Differential%20Geometry%20book_files/Diffgeo.pdf"&gt;Lectures on Differential Geometry&lt;/a&gt; - Wulf Rossmann&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.matematik.lu.se/matematiklu/personal/sigma/Riemann.pdf"&gt;An Introduction to Riemannian Geometry&lt;/a&gt; - Sigmundur Gudmundsson (Lund University)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://msri.org/publications/books/gt3m/"&gt;The Geometry and Topology of Three-Manifolds&lt;/a&gt; - W. Thurston&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.math.harvard.edu/~shlomo/docs/semi_riemannian_geometry.pdf"&gt;Semi-Riemann Geometry and General Relativity&lt;/a&gt; - Shlomo Sternberg&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.cs.cmu.edu/~kmcrane/Projects/DDG/paper.pdf"&gt;Discrete Differential Geometry&lt;/a&gt; - Keenan Crane&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Algebraic Geometry&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;📝 &lt;a href="https://ksda.ccny.cuny.edu/PostedPapers/rickksda1107.pdf"&gt;A Brief Introduction to Algebraic Geometry&lt;/a&gt; - R.C. Churchill&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.math.lsa.umich.edu/~idolga/631.pdf"&gt;Introduction to Algebraic Geometry&lt;/a&gt; - Igor V. Dolgachev&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://math.stanford.edu/~vakil/216blog/FOAGjun1113public.pdf"&gt;Foundations of Algebraic Geometry&lt;/a&gt; - Ravi Vakil&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.cis.upenn.edu/~jean/algeoms.pdf"&gt;Algebraic Geometry&lt;/a&gt; - Jean Gallier, Stephen S. Shatz (University of Pennsylvania)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.jmilne.org/math/CourseNotes/AG.pdf"&gt;Algebraic Geometry&lt;/a&gt; - J.S. Milne&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.mathematik.uni-kl.de/~gathmann/class/alggeom-2002/main.pdf"&gt;Algebraic Geometry&lt;/a&gt; - Andreas Gathmann (University of Kaiserslautern)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://stacks.math.columbia.edu/"&gt;The Stacks Project&lt;/a&gt; - Maintained by Aise Johan de Jong (Columbia)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Algebraic Statistics&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;📝 &lt;a href="https://math.berkeley.edu/~bernd/owl.pdf"&gt;Lectures on Algebraic Statistics&lt;/a&gt; - Mathias Drton, Bernd Sturmfels, Seth Sullivant&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://www3.diism.unisi.it/~chiantini/did/00Book.pdf"&gt;An Introduction to Algebraic Statistics&lt;/a&gt; - Cristiano Bocci, Luca Chiantini and Anthony V. Geramita&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://tore.tuhh.de/dspace-cris-server/api/core/bitstreams/a0c378d5-ce8e-442a-8891-9e7f763b4279/content"&gt;Algebraic Statistics&lt;/a&gt; - Karl-Heinz Zimmermann&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://yaroslavvb.com/papers/pachter-algebraic.pdf"&gt;Algebraic Statistics for Computational Biology&lt;/a&gt; - Pachter, and Sturmfels.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Topology&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;📝 &lt;a href="https://www.math.upenn.edu/~ghrist/notes.html"&gt;Elementary Applied Topology&lt;/a&gt; - Robert Ghrist (UPenn)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.math.colostate.edu/~renzo/teaching/Topology10/Notes.pdf"&gt;Introduction to Topology&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.math.bme.hu/~kalex/Teaching/Spring10/Topology/TopNotes_Spring10.pdf"&gt;Introduction to Topology&lt;/a&gt; - Alex Küronya&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.math.clemson.edu/~jimlb/Teaching/2009-10/Math986/Topology.pdf"&gt;Introductory Topology&lt;/a&gt; - Jim L. Brown&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://webusers.imj-prg.fr/~pierre.schapira/lectnotes/Topo.pdf"&gt;General Topology&lt;/a&gt; - Pierre Schapira (Paris VI University)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.pdmi.ras.ru/~olegviro/topoman/eng-book-nopfs.pdf"&gt;Elementary Topology Problem Textbook&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.math.ku.dk/~moller/e03/3gt/notes/gtnotes.pdf"&gt;General Topology&lt;/a&gt; - Jesper M. Møller&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://mathonline.wikidot.com/topology"&gt;Topology Topics&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Algebraic Topology&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.math.cornell.edu/~hatcher/AT/AT.pdf"&gt;Algebraic Topology&lt;/a&gt; - Allen Hatcher&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.math.uchicago.edu/~may/CONCISE/ConciseRevised.pdf"&gt;A Concise Course in Algebraic Topology&lt;/a&gt; - J. P. May&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.math.muni.cz/~cadek/at/at.pdf"&gt;Introduction to Algebraic Topology&lt;/a&gt; - Martin Cadek&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://webusers.imj-prg.fr/~pierre.schapira/lectnotes/AlTo.pdf"&gt;Algebra and Topology&lt;/a&gt; - Pierre Schapira (Paris VI University)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.indiana.edu/~jfdavis/teaching/m623/book.pdf"&gt;Lecture Notes in Algebraic Topology&lt;/a&gt; - James F. Davis, Paul Kirk (Indiana University)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://www.ma.utexas.edu/ibl1/courses/resources/12_15_07_grad_alg_top_mooremethod.pdf"&gt;Algebraic Topology&lt;/a&gt; - Michael Starbird&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.math.nus.edu.sg/~matwujie/ma5209.pdf"&gt;Lecture Notes on Algebraic Topology&lt;/a&gt; - Jie Wu&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Analysis&lt;/h2&gt; 
&lt;h3&gt;Real Analysis&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;📝 &lt;a href="https://ocw.mit.edu/resources/res-18-001-calculus-online-textbook-spring-2005/textbook/"&gt;MIT OpenCourseWare Lectures on Calculus&lt;/a&gt; - G. Strang&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.math.wisc.edu/~keisler/calc.html"&gt;Elementary Calculus: An Approach Using Infinitesimals&lt;/a&gt; - Professor H. Jerome Keisler&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://www.math.ucdavis.edu/~hunter/intro_analysis_pdf/intro_analysis.pdf"&gt;An Introduction to Real Analysis&lt;/a&gt; - John K. Hunter (University of California at Davis)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://ramanujan.math.trinity.edu/wtrench/texts/TRENCH_REAL_ANALYSIS.PDF"&gt;Introduction to Real Analysis&lt;/a&gt; - William F. Trench (Trinity University, Texas)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.jirka.org/ra/realanal.pdf"&gt;Basic Analysis: Introduction to Real Analysis&lt;/a&gt; - Jiří Lebl&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://prac.im.pwr.wroc.pl/~kwasnicki/pl/stuff/tbb-hyper.pdf"&gt;Elementary Real Analysis&lt;/a&gt; - Thomson, Bruckner&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://ms.mcmaster.ca/~sawyer/Publications/Real_Analysis.pdf"&gt;Lecture Notes in Real Analysis&lt;/a&gt; - Eric T. Sawyer (McMaster University)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://math.harvard.edu/~ctm/papers/home/text/class/harvard/212a/course/course.pdf"&gt;Real Analysis&lt;/a&gt; - C. McMullen&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://bass.math.uconn.edu/3rd.pdf"&gt;Real Analysis for Graduate Students&lt;/a&gt; - Richard F. Bass&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.math.purdue.edu/~torres/pubs/Modern-real-analysis.pdf"&gt;Modern Real Analysis&lt;/a&gt; - William P. Ziemer (Indiana University)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.trillia.com/zakon-analysisI.html"&gt;Mathematical Analysis Vol I&lt;/a&gt; - Elias Zakon&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.trillia.com/zakon-analysisII.html"&gt;Mathematical Analysis Vol II&lt;/a&gt; - Elias Zakon&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.math.harvard.edu/~shlomo/docs/Advanced_Calculus.pdf"&gt;Advanced Calculus&lt;/a&gt; - Lynn Loomis, Schlomo Sternberg&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://spot.colorado.edu/~baggett/analysis.html"&gt; Analysis of Functions of a Single Variable&lt;/a&gt; - Lawerence Baggett&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.synechism.org/wp/the-calculus-of-functions-of-several-variables/"&gt;The Calculus of Functions of Several Variables&lt;/a&gt; - Dan Sloughter&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://web.pdx.edu/~erdman/PTAC/problemtext_pdf.pdf"&gt;A ProblemText in Advanced Calculus&lt;/a&gt; - John M. Erdman&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://hdl.handle.net/2027/spo.5597602.0001.001"&gt;Calculus and Linear Algebra. Vol. 1&lt;/a&gt; - Wilfred Kaplan, Donald J. Lewis&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://quod.lib.umich.edu/s/spobooks/5597602.0002.001"&gt;Calculus and Linear Algebra. Vol. 2&lt;/a&gt; - Wilfred Kaplan, Donald J. Lewis&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.math.odu.edu/~jhh/counter10.html"&gt;Introduction to Calculus I and II&lt;/a&gt; - J.H. Heinbockel&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://faculty.gvsu.edu/boelkinm/Home/Active_Calculus.html"&gt;Active Calculus&lt;/a&gt; - Matt Boelkins&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://math.berkeley.edu/~gbergman/ug.hndts/#Rudin"&gt;Supplements to the Exercises in Chapters 1-7 of Walter Rudin's "Principles of Mathematical Analysis"&lt;/a&gt; - George M. Bergman&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://calculusmadeeasy.org/"&gt;Calculus Made Easy&lt;/a&gt; - Silvanus P. Thompson (1910)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://djm.cc/library/Elements_Differential_Integral_Calculus_Granville_edited_2.pdf"&gt;Elements of Differential and Integral Calculus&lt;/a&gt; - William Anthony Granville (1911)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://stitz-zeager.com/szprecalculus07042013.pdf"&gt;Precalculus&lt;/a&gt; - Carl Stitz, Jeff Zeager&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Harmonic Analysis&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.math.uiuc.edu/~laugesen/545/545Lectures.pdf"&gt;Harmonic Analysis Lecture Notes&lt;/a&gt; - Richard S. Laugesen (University of Illinois at Urbana–Champaign)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.math.uchicago.edu/~schlag/harmonicnotes.pdf"&gt;Harmonic Analysis&lt;/a&gt; - W. Schlag&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://see.stanford.edu/materials/lsoftaee261/book-fall-07.pdf"&gt;Lecture Notes: Fourier Transform and its Applications&lt;/a&gt; - Brad Osgood&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.reed.edu/physics/courses/Physics331.f08/pdf/Fourier.pdf"&gt;Fourier Analysis&lt;/a&gt; - Lucas Illing&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://ccrma.stanford.edu/~jos/mdft"&gt;Mathematics of the Discrete Fourier Transform (DFT) with Audio Applications&lt;/a&gt; - Julius O. Smith III (Stanford University)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Complex Analysis&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;📝 &lt;a href="https://mtaylor.web.unc.edu/wp-content/uploads/sites/16915/2018/04/complex.pdf"&gt;Introduction to Complex Analysis&lt;/a&gt; - Michael Taylor&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.math.uiuc.edu/~jpda/jpd-complex-geometry-book-5-refs-bip.pdf"&gt;An Introduction to Complex Analysis and Geometry&lt;/a&gt; - John P. D'Angelo (University of Illinois)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://math.sfsu.edu/beck/papers/complex.pdf"&gt;A First Course in Complex Analysis&lt;/a&gt; - Matthias Beck, Gerald Marchesi, Dennis Pixton, Lucas Sabalka&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.math.wustl.edu/~sk/books/guide.pdf"&gt;A Guide to Complex Variables&lt;/a&gt; - Steven G. Krantz&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.maths.manchester.ac.uk/~cwalkden/complex-analysis/complex_analysis.pdf"&gt;Complex Analysis&lt;/a&gt; - Charles Walkden&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.math.ku.dk/noter/filer/koman-12.pdf"&gt;Complex Analysis&lt;/a&gt; - Christian Berg&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://people.math.sc.edu/girardi/m7034/book/AshComplexVariablesWithHyperlinks.pdf"&gt;Complex Variables&lt;/a&gt; - R. B. Ash, W.P. Novinger&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.maths.lth.se/matematiklu/personal/olofsson/CompHT06.pdf"&gt;Complex Analysis&lt;/a&gt; - Christer Bennewitz&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://web.archive.org/web/20150620124453/https://www.math.washington.edu/~marshall/math_536/Notes.pdf"&gt;Complex Analysis&lt;/a&gt; - Donald E. Marshall&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://gauss.math.yale.edu/~ws442/complex.pdf"&gt;A Concise Course in Complex Analysis and Riemann Surfaces&lt;/a&gt; - Wilhelm Schlag&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://people.math.gatech.edu/%7Ecain/winter99/complex.html"&gt;Complex Analysis&lt;/a&gt; - G. Cain (Georgia Tech)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://complex-analysis.com/"&gt;Complex Analysis&lt;/a&gt; - Juan Carlos Ponce Campuzano&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Functional Analysis&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;📝 &lt;a href="https://www.math.uwaterloo.ca/~lwmarcou/notes/pmath453.pdf"&gt;An Introduction to Functional Analysis&lt;/a&gt; - Laurent W. Marcoux (University of Waterloo)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://users.math.msu.edu/users/jeffrey/920/920notes.pdf"&gt;Functional Analysis: Lecture Notes&lt;/a&gt; - Jeff Schenker (Michigan State University)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://archive.org/details/TB_Ward___Functional_analysis_lecture_notes"&gt;Functional Analysis Lecture Notes&lt;/a&gt; - T.B. Ward (University of East Anglia)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.maths.lancs.ac.uk/~belton/www/notes/fa_notes.pdf"&gt;Functional Analysis&lt;/a&gt; - Alexander C. R. Belton&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://www.mat.univie.ac.at/~gerald/ftp/book-fa/fa.pdf"&gt;Topics in Real and Functional Analysis&lt;/a&gt; - Gerald Teschl&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www2.math.ou.edu/~cremling/teaching/lecturenotes/fa-new/LN-I.pdf"&gt;Functional Analysis&lt;/a&gt; - Christian Remling&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.math.harvard.edu/~shlomo/docs/Real_Variables.pdf"&gt;Theory of Functions of a Real Variable&lt;/a&gt; - Shlomo Sternberg&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://spot.colorado.edu/~baggett/functional.html"&gt;Functional Analysis&lt;/a&gt; - Lawerence Baggett&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Measure Theory&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;📝 &lt;a href="https://terrytao.files.wordpress.com/2012/12/gsm-126-tao5-measure-book.pdf"&gt;An Introduction to Measure Theory&lt;/a&gt; - Terence Tao (UCLA)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.mat.uniroma2.it/~cannarsa/cam_0607.pdf"&gt;Lecture Notes on Measure Theory and Functional Analysis&lt;/a&gt; - P. Cannarsa, T. D’Aprile&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.math.chalmers.se/~borell/MeasureTheory.pdf"&gt;Lecture Notes in Measure Theory&lt;/a&gt; - Christer Borell&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.gold-saucer.org/math/lebesgue/lebesgue.pdf"&gt;A Crash Course on the Lebesgue Integral and Measure Theory&lt;/a&gt; - Steve Cheng&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://www.math.ucdavis.edu/~hunter/measure_theory/measure_notes.pdf"&gt;Measure Theory&lt;/a&gt; - John K. Hunter (University of California at Davis)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://people.math.ethz.ch/~salamon/PREPRINTS/measure.pdf"&gt;Measure and Integration&lt;/a&gt; - Dietmar A. Salamon (ETH Zürich)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.math.ucsd.edu/~bdriver/240-00-01/Lecture_Notes/measurep.pdf"&gt;Lecture notes: Measure Theory&lt;/a&gt; - Bruce K. Driver&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Ordinary Differential Equations&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.synechism.org/wp/difference-equations-to-differential-equations/"&gt;Difference Equations To Differential Equations&lt;/a&gt; - Dan Sloughter&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://www.math.uni-bielefeld.de/~grigor/odelec2008.pdf"&gt;Ordinary Differential Equation&lt;/a&gt; - Alexander Grigorian (University of Bielefeld)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.cs.bgu.ac.il/~leonid/ode_bio_files/Ionascu_LectNotes.pdf"&gt;Ordinary Differential Equations: Lecture Notes&lt;/a&gt; - Eugen J. Ionascu&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.math.lmu.de/~philip/publications/lectureNotes/ODE.pdf"&gt;Ordinary Differential Equations&lt;/a&gt; - Peter Philip&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://users.math.msu.edu/users/gnagy/teaching/ode.pdf"&gt;Ordinary Differential Equations&lt;/a&gt; - Gabriel Nagy&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.mat.univie.ac.at/~gerald/ftp/book-ode/ode.pdf"&gt;Ordinary Differential Equations and Dynamical Systems&lt;/a&gt; - Gerald Teschl&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://leipper.org/manuals/zip-fill/dn-difeq-notes.pdf"&gt;Notes on Differential Equations&lt;/a&gt; - Bob Terrell&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://digitalcommons.trinity.edu/mono/8/"&gt;Elementary Differential Equations&lt;/a&gt; - William F. Trench&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://digitalcommons.trinity.edu/mono/9/"&gt;Elementary Differential Equations With Boundary Value Problems&lt;/a&gt; - William F. Trench&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.jirka.org/diffyqs/"&gt;Notes on Diffy Qs: Differential Equations for Engineers&lt;/a&gt; - Jiří Lebl&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://djm.cc/library/Differential_Equations_Phillips_edited.pdf"&gt;Differential Equations&lt;/a&gt; - H. B. Phillips (1922)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Partial Differential Equations&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;📝 &lt;a href="https://www.math.ucdavis.edu/~hunter/pdes/pde_notes.pdf"&gt;Notes on Partial Differential Equations&lt;/a&gt; - John K. Hunter (University of California at Davis)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.math.uni-leipzig.de/~miersemann/pdebook.pdf"&gt;Partial Differential Equations: Lecture Notes&lt;/a&gt; - Erich Miersemann (Leipzig University)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.mathphysics.com/pde/"&gt;Linear Methods of Applied Mathematics&lt;/a&gt; - E. Harrell, J. Herod (Georgia Tech)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Chaos Theory&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;📝 &lt;a href="https://archive.org/details/chaosmakingnewsc0000unse"&gt;Chaos: Making a New Science&lt;/a&gt; - James Gleick&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://archive.org/details/complexityguided0000mitc?utm_source=chatgpt.com"&gt;Complexity: A Guided Tour&lt;/a&gt; - Melanie Mitchell (Oxford University)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Probability and Statistics&lt;/h2&gt; 
&lt;h3&gt;Probability Theory&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;📝 &lt;a href="https://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/amsbook.mac.pdf"&gt;Introduction to Probability&lt;/a&gt; - Charles M. Grinstead, J. Laurie Snell&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://vfu.bg/en/e-Learning/Math--Bertsekas_Tsitsiklis_Introduction_to_probability.pdf"&gt;Introduction to Probability&lt;/a&gt; - Dimitri P. Bertsekas, John N. Tsitsiklis (MIT)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.maths.uq.edu.au/~kroese/asitp.pdf"&gt;A Short Introduction to Probability&lt;/a&gt; - Dirk P. Kroese (University of Queensland)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://www.math.duke.edu/~rtd/PTE/PTE4_1.pdf"&gt;Probability: Theory and Examples&lt;/a&gt; - Rick Durrett&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://github.com/mavam/stat-cookbook/releases/download/0.2.3/stat-cookbook.pdf"&gt;Probability and Statistics Cookbook&lt;/a&gt; - Matthias Vallentin (UC Berkeley)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.wzchen.com/probability-cheatsheet/"&gt;The Only Probability Cheatsheet You'll Ever Need&lt;/a&gt; - William Chen&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.ellerman.org/Davids-Stuff/Maths/Rota-Baclawski-Prob-Theory-79.pdf"&gt;An Introduction to Probability and Random Processes&lt;/a&gt; - Gian-Carlo Rota, Kenneth Baclawski&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://arxiv.org/pdf/1906.01803.pdf"&gt;Foundations of Constructive Probability Theory&lt;/a&gt; - Yuen-Kwok Chan&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Statistics&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;📝 &lt;a href="http://homepages.math.uic.edu/~rgmartin/Teaching/Stat411/Notes/411notes.pdf"&gt;Lecture Notes on Statistical Theory&lt;/a&gt; - Ryan Martin (University of Illinois)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www-library.desy.de/preparch/books/vstatmp_engl.pdf"&gt;Introduction to Statistics and Data Analysis for Physicists&lt;/a&gt; - Gerhard Bohm, Günter Zech&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.iiserpune.ac.in/~ayan/MTH201/Sahoo_textbook.pdf"&gt;Probability and Mathematical Statistics&lt;/a&gt; - Prasanna Sahoo (University of Louisville)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://math.arizona.edu/~faris/stat.pdf"&gt;Lectures on Statistics&lt;/a&gt; - William G. Faris&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://pages.pomona.edu/~ajr04747/Fall2009/Math152/Notes/Math152NotesFall09.pdf"&gt;Statistical Theory&lt;/a&gt; - Adolfo J. Rumbos&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://mason.gmu.edu/~jgentle/books/MathStat.pdf"&gt;Theory of Statistics&lt;/a&gt; - James E. Gentle (George Mason University)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://math.arizona.edu/~jwatkins/notests.pdf"&gt;Theory of Statistics&lt;/a&gt; - Joseph C. Watkins (University of Arizona)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://web.archive.org/web/20130523134625/http://www.aiaccess.net/e_gm.htm"&gt;Glossary of Data Modeling&lt;/a&gt; - AI Access&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.ats.ucla.edu/stat/papers/"&gt;Statistics Papers&lt;/a&gt; - List of statistics papers curated by the Institute for Digital Research and Education (IDRE) at UCLA on methods such as bootstrap and factor invariance.&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://itl.nist.gov/div898/handbook/index.htm"&gt;NIST Handbook of Statistical Methods&lt;/a&gt; - Resource on practical statistics directed towards scientists and engineers.&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://vassarstats.net/textbook/"&gt;Concepts and Applications of Inferential Statistics&lt;/a&gt; - Richard Lowry&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.cosc.brocku.ca/~duentsch/papers/methprimer2.html"&gt;Rough set data analysis: A road to non-invasive knowledge discovery&lt;/a&gt; - Ivo Düntsch, Günther Gediga&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://statsthinking21.org/"&gt;Statistical Thinking for the 21st Century&lt;/a&gt; - Russell A. Poldrack&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://jonathanweisberg.org/vip/"&gt;Odds and Ends: Introducing Probability &amp;amp; Decision with a Visual Emphasis&lt;/a&gt; - Jonathan Weisberg&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://seeing-theory.brown.edu/"&gt;Seeing Theory&lt;/a&gt; - Daniel Kunin, Jingru Guo, Tyler Dae Devlin, and Daniel Xiang&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://www.statisticsdonewrong.com/"&gt;Statistics Done Wrong&lt;/a&gt; - Alex Reinhart&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://link.springer.com/book/10.1007/978-0-387-21736-9"&gt;All of Statistics: A Concise Course in Statistical Inference&lt;/a&gt; - Larry Wasserman&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Statistical Learning&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;📝 &lt;a href="http://www-bcf.usc.edu/~gareth/ISL/ISLR%20First%20Printing.pdf"&gt;An Introduction to Statistical Learning with Applications in R&lt;/a&gt; - Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://web.stanford.edu/~hastie/Papers/ESLII.pdf"&gt;The Elements of Statistical Learning&lt;/a&gt; - Trevor Hastie, Robert Tibshirani, Jerome Friedman&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://web.stanford.edu/class/cs229t/notes.pdf"&gt;Statistical Learning Theory&lt;/a&gt; - Percy Liang&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://webdocs.cs.ualberta.ca/~sutton/book/bookdraft2016sep.pdf"&gt;Reinforcement Learning: An Introduction&lt;/a&gt; - Richard S. Sutton, Andrew G. Barto&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Stochastic processes&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.math.tifr.res.in/~publ/ln/tifr24.pdf"&gt;Lectures on Stochastic Processes&lt;/a&gt; - K. Ito (Tata Institute of Fundamental Research, Bombay)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.math.harvard.edu/~knill/teaching/math144_1994/probability.pdf"&gt;Probability and Stochastic Processes with Applications&lt;/a&gt; - Oliver Knill (Harvard University)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://statweb.stanford.edu/~adembo/math-136/nnotes.pdf"&gt;Stochastic Processes&lt;/a&gt; - Amir Dembo (Stanford University)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.mi.fu-berlin.de/wiki/pub/CompMolBio/MarkovKetten15/stochastic_processes_2011.pdf"&gt;Lecture Notes on Stochastic Processes&lt;/a&gt; - Frank Noé, Bettina Keller and Jan-Hendrik Prinz (Freie Universität Berlin)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://www.ma.utexas.edu/users/gordanz/notes/introduction_to_stochastic_processes.pdf"&gt;Introduction to Stochastic Processes - Lecture Notes&lt;/a&gt; - Gordan Žitković (University of Texas)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://www.math.uwaterloo.ca/~mscott/Little_Notes.pdf"&gt;Applied Stochastic Processes in science and engineering&lt;/a&gt; - Matt Scott (University of Waterloo)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.math.leidenuniv.nl/~spieksma/colleges/sp-master/sp-hvz1.pdf"&gt;An Introduction to Stochastic Processes in Continuous Time&lt;/a&gt; - Flora Spieksma (Leiden University)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://pages.uoregon.edu/dlevin/MARKOV/markovmixing.pdf"&gt;Markov Chains and Mixing Times&lt;/a&gt; - David A. Levin, Yuval Peres, Elizabeth L. Wilmer&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.stat.yale.edu/~pollard/Books/1984book/pollard1984.pdf"&gt;Convergence of Stochastic Processes&lt;/a&gt; - David Pollard&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Numerical Analysis&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.math.umd.edu/~dlevy/resources/notes.pdf"&gt;Introduction to Numerical Analysis&lt;/a&gt; - Doron Levy (University of Maryland)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.ima.umn.edu/~arnold/597.00-01/nabook.pdf"&gt;A Concise Introduction to Numerical Analysis&lt;/a&gt; - Douglas N. Arnold (University of Minnesota)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://people.cs.uchicago.edu/~ridg/newna/nalrs.pdf"&gt;Numerical Analysis&lt;/a&gt; - L. Ridgway Scott&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://uknowledge.uky.edu/cgi/viewcontent.cgi?article=1000&amp;amp;context=math_textbooks"&gt;Lectures In Basic Computational Numerical Analysis&lt;/a&gt; - J. M. McDonough (University of Kentucky)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://user.math.uni-bremen.de/schmi/SS04/YSU_Notes.pdf"&gt;Advanced Numerical Methods and Their Applications to Industrial Problems: Adaptive Finite Element Methods&lt;/a&gt; - Alfred Schmidt, Arsen Narimanyan&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://ece.uwaterloo.ca/~dwharder/nm/"&gt;Numerical Analysis for Engineers&lt;/a&gt; - Douglas Wilhelm Harder&lt;/li&gt; 
 &lt;li&gt;📝🎥 &lt;a href="https://www.cs.utexas.edu/users/flame/laff/alaff/frontmatter.html"&gt;Advanced Linear Algebra: Foundations to Frontiers&lt;/a&gt; - Robert van de Geijn, Margaret Myers (University of Texas at Austin)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Signal processing&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.ece.rutgers.edu/~orfanidi/intro2sp/orfanidis-i2sp.pdf"&gt;Introduction to Signal Processing&lt;/a&gt; - Sophocles J. Orfanidis (Rutgers University)&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.fourierandwavelets.org/FSP_v1.1_2014.pdf"&gt;Foundations of Signal Processing&lt;/a&gt; - Martin Vetterli, Jelena Kovacevic, Vivek K Goyal&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://ee.stanford.edu/~gray/sp.pdf"&gt;An Introduction to Statistical Signal Processing&lt;/a&gt; - Robert M. Gray, Lee D. Davisson&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://greenteapress.com/wp/think-dsp/"&gt;Think DSP&lt;/a&gt; - Allen B. Downey&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://www.uio.no/studier/emner/matnat/math/MAT-INF2360/v15/kompendium/applinalgpython.pdf"&gt;Linear algebra, signal processing, and wavelets. A unified approach.&lt;/a&gt; - Øyvind Ryan (University of Oslo)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Mathematics for Computer Science&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;📝 &lt;a href="https://people.csail.mit.edu/meyer/mcs.pdf"&gt;Mathematics for Computer Science&lt;/a&gt; - Eric Lehman, F. Thomson Leighton, Albert R. Meyer&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.math.upenn.edu/%7Ewilf/AlgComp3.html"&gt;Algorithms and Complexity&lt;/a&gt; - H. Wilf&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://people.eecs.berkeley.edu/~varaiya/papers_ps.dir/NOO.pdf"&gt;Lecture Notes on Optimization&lt;/a&gt; - Pravin Varaiya&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.inference.org.uk/mackay/itila/book.html"&gt;Information Theory, Inference, and Learning Algorithms&lt;/a&gt; - David J. C. MacKay&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="https://hypertextbook.com/chaos/"&gt;The Chaos Textbook: Mathematics in the age of the computer&lt;/a&gt; - Glenn Elert&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Mathematical Biology&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.math.ust.hk/~machas/mathematical-biology.pdf"&gt;Mathematical Biology&lt;/a&gt; - Jeffrey Chasnov&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Mathematical Physics&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;📝 &lt;a href="http://oaktrust.library.tamu.edu/handle/1969.1/2501"&gt;Introduction to Continuum Mechanics&lt;/a&gt; - Ray. M. Bowen&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.physics.miami.edu/nearing/mathmethods/"&gt;Mathematical Tools for Physics&lt;/a&gt; - James Nearing&lt;/li&gt; 
 &lt;li&gt;📝 &lt;a href="http://www.malaspina.com/etext/heavens.htm"&gt;Mechanism of the Heavens (1831)&lt;/a&gt; - Mary Somerville&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Students Lecture Notes&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://web.evanchen.cc/coursework.html"&gt;Evan Chen&lt;/a&gt; - MIT. 2012 ~ 2018. Covers Combinatorics, Number Theory, Honors Algebra, Set Theory, Real Analysis, Graph Theory, and more.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://dec41.user.srcf.net/notes/"&gt;Dexter Chua&lt;/a&gt; - Harvard. 2013 ~ 2018. Covers Analysis, Probability, Linear Algebra, Complex Analysis, Numerical Analysis, Statistics, Optimization, Algebraic Topology, Quantum Field Theory, and more.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Related Awesome Lists&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mostafatouny/awesome-theoretical-computer-science"&gt;Theoretical Computer Science&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;p&gt;&lt;a href="http://creativecommons.org/publicdomain/zero/1.0/"&gt;&lt;img src="http://i.creativecommons.org/p/zero/1.0/88x31.png" alt="CC0" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;To the extent possible under law, &lt;a href="http://cyrille.rossant.net"&gt;Cyrille Rossant&lt;/a&gt; has waived all copyright and related or neighboring rights to this work.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>fishaudio/fish-speech</title>
      <link>https://github.com/fishaudio/fish-speech</link>
      <description>&lt;p&gt;SOTA Open Source TTS&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h1&gt;Fish Speech&lt;/h1&gt; 
 &lt;p&gt;&lt;strong&gt;English&lt;/strong&gt; | &lt;a href="https://raw.githubusercontent.com/fishaudio/fish-speech/main/docs/README.zh.md"&gt;简体中文&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/fishaudio/fish-speech/main/docs/README.pt-BR.md"&gt;Portuguese&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/fishaudio/fish-speech/main/docs/README.ja.md"&gt;日本語&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/fishaudio/fish-speech/main/docs/README.ko.md"&gt;한국어&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/fishaudio/fish-speech/main/docs/README.ar.md"&gt;العربية&lt;/a&gt; &lt;br /&gt;&lt;/p&gt; 
 &lt;a href="https://www.producthunt.com/posts/fish-speech-1-4?embed=true&amp;amp;utm_source=badge-featured&amp;amp;utm_medium=badge&amp;amp;utm_souce=badge-fish-speech-1-4" target="_blank"&gt; &lt;img src="https://api.producthunt.com/widgets/embed-image/v1/featured.svg?post_id=488440&amp;amp;theme=light" alt="Fish Speech 1.4 - Open-Source Multilingual Text-to-Speech with Voice Cloning | Product Hunt" style="width: 250px; height: 54px;" width="250" height="54" /&gt; &lt;/a&gt; 
 &lt;a href="https://trendshift.io/repositories/7014" target="_blank"&gt; &lt;img src="https://trendshift.io/api/badge/repositories/7014" alt="fishaudio%2Ffish-speech | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt; &lt;/a&gt; 
 &lt;br /&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://count.getloli.com/get/@fish-speech?theme=asoul" /&gt;
 &lt;br /&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;a target="_blank" href="https://discord.gg/Es5qTB9BcN"&gt; &lt;img alt="Discord" src="https://img.shields.io/discord/1214047546020728892?color=%23738ADB&amp;amp;label=Discord&amp;amp;logo=discord&amp;amp;logoColor=white&amp;amp;style=flat-square" /&gt; &lt;/a&gt; 
 &lt;a target="_blank" href="https://hub.docker.com/r/fishaudio/fish-speech"&gt; &lt;img alt="Docker" src="https://img.shields.io/docker/pulls/fishaudio/fish-speech?style=flat-square&amp;amp;logo=docker" /&gt; &lt;/a&gt; 
 &lt;a target="_blank" href="https://pd.qq.com/s/bwxia254o"&gt; &lt;img alt="QQ Channel" src="https://img.shields.io/badge/QQ-blue?logo=tencentqq" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a target="_blank" href="https://huggingface.co/spaces/TTS-AGI/TTS-Arena-V2"&gt; &lt;img alt="TTS-Arena2 Score" src="https://img.shields.io/badge/TTS_Arena2-Rank_%231-gold?style=flat-square&amp;amp;logo=trophy&amp;amp;logoColor=white" /&gt; &lt;/a&gt; 
 &lt;a target="_blank" href="https://huggingface.co/spaces/fishaudio/fish-speech-1"&gt; &lt;img alt="Huggingface" src="https://img.shields.io/badge/🤗%20-space%20demo-yellow" /&gt; &lt;/a&gt; 
 &lt;a target="_blank" href="https://huggingface.co/fishaudio/openaudio-s1-mini"&gt; &lt;img alt="HuggingFace Model" src="https://img.shields.io/badge/🤗%20-models-orange" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] &lt;strong&gt;License Notice&lt;/strong&gt;&lt;br /&gt; This codebase is released under &lt;strong&gt;Apache License&lt;/strong&gt; and all model weights are released under &lt;strong&gt;CC-BY-NC-SA-4.0 License&lt;/strong&gt;. Please refer to &lt;a href="https://raw.githubusercontent.com/fishaudio/fish-speech/main/LICENSE"&gt;LICENSE&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING] &lt;strong&gt;Legal Disclaimer&lt;/strong&gt;&lt;br /&gt; We do not hold any responsibility for any illegal usage of the codebase. Please refer to your local laws about DMCA and other related laws.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Start Here&lt;/h2&gt; 
&lt;p&gt;Here are the official documents for Fish Speech, follow the instructions to get started easily.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://speech.fish.audio/install/"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://speech.fish.audio/finetune/"&gt;Finetune&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://speech.fish.audio/inference/"&gt;Inference&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://speech.fish.audio/examples"&gt;Samples&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🎉 Announcement&lt;/h2&gt; 
&lt;p&gt;We are excited to announce that we have rebranded to &lt;strong&gt;OpenAudio&lt;/strong&gt; — introducing a revolutionary new series of advanced Text-to-Speech models that builds upon the foundation of Fish-Speech.&lt;/p&gt; 
&lt;p&gt;We are proud to release &lt;strong&gt;OpenAudio-S1&lt;/strong&gt; as the first model in this series, delivering significant improvements in quality, performance, and capabilities.&lt;/p&gt; 
&lt;p&gt;OpenAudio-S1 comes in two versions: &lt;strong&gt;OpenAudio-S1&lt;/strong&gt; and &lt;strong&gt;OpenAudio-S1-mini&lt;/strong&gt;. Both models are now available on &lt;a href="https://fish.audio"&gt;Fish Audio Playground&lt;/a&gt; (for &lt;strong&gt;OpenAudio-S1&lt;/strong&gt;) and &lt;a href="https://huggingface.co/fishaudio/openaudio-s1-mini"&gt;Hugging Face&lt;/a&gt; (for &lt;strong&gt;OpenAudio-S1-mini&lt;/strong&gt;).&lt;/p&gt; 
&lt;p&gt;Visit the &lt;a href="https://openaudio.com/blogs/s1"&gt;OpenAudio website&lt;/a&gt; for blog &amp;amp; tech report.&lt;/p&gt; 
&lt;h2&gt;Highlights ✨&lt;/h2&gt; 
&lt;h3&gt;&lt;strong&gt;Excellent TTS quality&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;We use Seed TTS Eval Metrics to evaluate the model performance, and the results show that OpenAudio S1 achieves &lt;strong&gt;0.008 WER&lt;/strong&gt; and &lt;strong&gt;0.004 CER&lt;/strong&gt; on English text, which is significantly better than previous models. (English, auto eval, based on OpenAI gpt-4o-transcribe, speaker distance using Revai/pyannote-wespeaker-voxceleb-resnet34-LM)&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Word Error Rate (WER)&lt;/th&gt; 
   &lt;th&gt;Character Error Rate (CER)&lt;/th&gt; 
   &lt;th&gt;Speaker Distance&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;S1&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;0.008&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;0.004&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;0.332&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;S1-mini&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;0.011&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;0.005&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;0.380&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;&lt;strong&gt;Best Model in TTS-Arena2&lt;/strong&gt; 🏆&lt;/h3&gt; 
&lt;p&gt;OpenAudio S1 has achieved the &lt;strong&gt;#1 ranking&lt;/strong&gt; on &lt;a href="https://arena.speechcolab.org/"&gt;TTS-Arena2&lt;/a&gt;, the benchmark for text-to-speech evaluation:&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/fishaudio/fish-speech/main/docs/assets/Elo.jpg" alt="TTS-Arena2 Ranking" style="width: 75%;" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;&lt;strong&gt;Speech Control&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;OpenAudio S1 &lt;strong&gt;supports a variety of emotional, tone, and special markers&lt;/strong&gt; to enhance speech synthesis:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Basic emotions&lt;/strong&gt;:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;(angry) (sad) (excited) (surprised) (satisfied) (delighted) 
(scared) (worried) (upset) (nervous) (frustrated) (depressed)
(empathetic) (embarrassed) (disgusted) (moved) (proud) (relaxed)
(grateful) (confident) (interested) (curious) (confused) (joyful)
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Advanced emotions&lt;/strong&gt;:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;(disdainful) (unhappy) (anxious) (hysterical) (indifferent) 
(impatient) (guilty) (scornful) (panicked) (furious) (reluctant)
(keen) (disapproving) (negative) (denying) (astonished) (serious)
(sarcastic) (conciliative) (comforting) (sincere) (sneering)
(hesitating) (yielding) (painful) (awkward) (amused)
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Tone markers&lt;/strong&gt;:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;(in a hurry tone) (shouting) (screaming) (whispering) (soft tone)
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Special audio effects&lt;/strong&gt;:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;(laughing) (chuckling) (sobbing) (crying loudly) (sighing) (panting)
(groaning) (crowd laughing) (background laughter) (audience laughing)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also use Ha,ha,ha to control, there's many other cases waiting to be explored by yourself.&lt;/p&gt; 
&lt;p&gt;(Support for English, Chinese and Japanese now, and more languages is coming soon!)&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;Two Type of Models&lt;/strong&gt;&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Size&lt;/th&gt; 
   &lt;th&gt;Availability&lt;/th&gt; 
   &lt;th&gt;Features&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;S1&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;4B parameters&lt;/td&gt; 
   &lt;td&gt;Avaliable on &lt;a href="https://fish.audio/"&gt;fish.audio&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Full-featured flagship model&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;S1-mini&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;0.5B parameters&lt;/td&gt; 
   &lt;td&gt;Avaliable on huggingface &lt;a href="https://huggingface.co/spaces/fishaudio/openaudio-s1-mini"&gt;hf space&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Distilled version with core capabilities&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Both S1 and S1-mini incorporate online Reinforcement Learning from Human Feedback (RLHF).&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Zero-shot &amp;amp; Few-shot TTS:&lt;/strong&gt; Input a 10 to 30-second vocal sample to generate high-quality TTS output. &lt;strong&gt;For detailed guidelines, see &lt;a href="https://docs.fish.audio/resources/best-practices/voice-cloning"&gt;Voice Cloning Best Practices&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Multilingual &amp;amp; Cross-lingual Support:&lt;/strong&gt; Simply copy and paste multilingual text into the input box—no need to worry about the language. Currently supports English, Japanese, Korean, Chinese, French, German, Arabic, and Spanish.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;No Phoneme Dependency:&lt;/strong&gt; The model has strong generalization capabilities and does not rely on phonemes for TTS. It can handle text in any language script.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Highly Accurate:&lt;/strong&gt; Achieves a low CER (Character Error Rate) of around 0.4% and WER (Word Error Rate) of around 0.8% for Seed-TTS Eval.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Fast:&lt;/strong&gt; Accelerated by torch compile, the real-time factor is approximately 1:7 on an Nvidia RTX 4090 GPU.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;WebUI Inference:&lt;/strong&gt; Features an easy-to-use, Gradio-based web UI compatible with Chrome, Firefox, Edge, and other browsers.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Deploy-Friendly:&lt;/strong&gt; Easily set up an inference server with native support for Linux and Windows (macOS support coming soon), minimizing performance loss.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;&lt;strong&gt;Media &amp;amp; Demos&lt;/strong&gt;&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;h3&gt;&lt;strong&gt;Social Media&lt;/strong&gt;&lt;/h3&gt; 
 &lt;a href="https://x.com/FishAudio/status/1929915992299450398" target="_blank"&gt; &lt;img src="https://img.shields.io/badge/𝕏-Latest_Demo-black?style=for-the-badge&amp;amp;logo=x&amp;amp;logoColor=white" alt="Latest Demo on X" /&gt; &lt;/a&gt; 
 &lt;h3&gt;&lt;strong&gt;Interactive Demos&lt;/strong&gt;&lt;/h3&gt; 
 &lt;a href="https://fish.audio" target="_blank"&gt; &lt;img src="https://img.shields.io/badge/Fish_Audio-Try_OpenAudio_S1-blue?style=for-the-badge" alt="Try OpenAudio S1" /&gt; &lt;/a&gt; 
 &lt;a href="https://huggingface.co/spaces/fishaudio/openaudio-s1-mini" target="_blank"&gt; &lt;img src="https://img.shields.io/badge/Hugging_Face-Try_S1_Mini-yellow?style=for-the-badge" alt="Try S1 Mini" /&gt; &lt;/a&gt; 
 &lt;h3&gt;&lt;strong&gt;Video Showcases&lt;/strong&gt;&lt;/h3&gt; 
 &lt;a href="https://www.youtube.com/watch?v=SYuPvd7m06A" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/fishaudio/fish-speech/main/docs/assets/Thumbnail.jpg" alt="OpenAudio S1 Video" style="width: 50%;" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Credits&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/daniilrobnikov/vits2"&gt;VITS2 (daniilrobnikov)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/fishaudio/Bert-VITS2"&gt;Bert-VITS2&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/innnky/gpt-vits"&gt;GPT VITS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/b04901014/MQTTS"&gt;MQTTS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pytorch-labs/gpt-fast"&gt;GPT Fast&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/RVC-Boss/GPT-SoVITS"&gt;GPT-SoVITS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/QwenLM/Qwen3"&gt;Qwen3&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Tech Report (V1.4)&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{fish-speech-v1.4,
      title={Fish-Speech: Leveraging Large Language Models for Advanced Multilingual Text-to-Speech Synthesis},
      author={Shijia Liao and Yuxuan Wang and Tianyu Li and Yifan Cheng and Ruoyi Zhang and Rongzhi Zhou and Yijin Xing},
      year={2024},
      eprint={2411.01156},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      url={https://arxiv.org/abs/2411.01156},
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
  </channel>
</rss>