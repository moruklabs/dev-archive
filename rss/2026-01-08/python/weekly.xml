<rss version="2.0">
  <channel>
    <title>GitHub Python Weekly Trending</title>
    <description>Weekly Trending of Python in GitHub</description>
    <pubDate>Wed, 07 Jan 2026 01:55:30 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>OpenBB-finance/OpenBB</title>
      <link>https://github.com/OpenBB-finance/OpenBB</link>
      <description>&lt;p&gt;Financial data platform for analysts, quants and AI agents.&lt;/p&gt;&lt;hr&gt;&lt;br /&gt; 
&lt;img src="https://github.com/OpenBB-finance/OpenBB/raw/develop/images/odp-light.svg?raw=true#gh-light-mode-only" alt="Open Data Platform by OpenBB logo" width="600" /&gt; 
&lt;img src="https://github.com/OpenBB-finance/OpenBB/raw/develop/images/odp-dark.svg?raw=true#gh-dark-mode-only" alt="Open Data Platform by OpenBB logo" width="600" /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;a href="https://x.com/openbb_finance"&gt;&lt;img src="https://img.shields.io/twitter/url/https/twitter.com/openbb_finance.svg?style=social&amp;amp;label=Follow%20%40openbb_finance" alt="Twitter" /&gt;&lt;/a&gt; &lt;a href="https://discord.com/invite/xPHTuHCmuV"&gt;&lt;img src="https://img.shields.io/discord/831165782750789672" alt="Discord Shield" /&gt;&lt;/a&gt; &lt;a href="https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/OpenBB-finance/OpenBB"&gt;&lt;img src="https://img.shields.io/static/v1?label=Dev%20Containers&amp;amp;message=Open&amp;amp;color=blue&amp;amp;logo=visualstudiocode" alt="Open in Dev Containers" /&gt;&lt;/a&gt; &lt;a href="https://codespaces.new/OpenBB-finance/OpenBB"&gt; &lt;img src="https://github.com/codespaces/badge.svg?sanitize=true" height="20" /&gt; &lt;/a&gt; &lt;a target="_blank" href="https://colab.research.google.com/github/OpenBB-finance/OpenBB/blob/develop/examples/googleColab.ipynb"&gt; &lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt; &lt;/a&gt; &lt;a href="https://pypi.org/project/openbb/"&gt;&lt;img src="https://img.shields.io/pypi/v/openbb?color=blue&amp;amp;label=PyPI%20Package" alt="PyPI" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Open Data Platform by OpenBB (ODP) is the open-source toolset that helps data engineers integrate proprietary, licensed, and public data sources into downstream applications like AI copilots and research dashboards.&lt;/p&gt; 
&lt;p&gt;ODP operates as the "connect once, consume everywhere" infrastructure layer that consolidates and exposes data to multiple surfaces at once: Python environments for quants, OpenBB Workspace and Excel for analysts, MCP servers for AI agents, and REST APIs for other applications.&lt;/p&gt; 
&lt;a href="https://pro.openbb.co"&gt; 
 &lt;div align="center"&gt; 
  &lt;img src="https://openbb-cms.directus.app/assets/70b971ef-7a7e-486e-b5ae-1cc602f2162c.png" alt="Logo" width="1000" /&gt; 
 &lt;/div&gt; &lt;/a&gt; 
&lt;p&gt;Get started with: &lt;code&gt;pip install openbb&lt;/code&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openbb import obb
output = obb.equity.price.historical("AAPL")
df = output.to_dataframe()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Data integrations available can be found here: &lt;a href="https://docs.openbb.co/python/reference"&gt;https://docs.openbb.co/python/reference&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;OpenBB Workspace&lt;/h2&gt; 
&lt;p&gt;While the Open Data Platform provides the open-source data integration foundation, &lt;strong&gt;OpenBB Workspace&lt;/strong&gt; offers the enterprise UI for analysts to visualize datasets and leverage AI agents. The platform's "connect once, consume everywhere" architecture enables seamless integration between the two.&lt;/p&gt; 
&lt;p&gt;You can find OpenBB Workspace at &lt;a href="https://pro.openbb.co"&gt;https://pro.openbb.co&lt;/a&gt;. &lt;a href="https://pro.openbb.co"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;a href="https://pro.openbb.co"&gt; 
 &lt;div align="center"&gt; 
  &lt;img src="https://openbb-cms.directus.app/assets/f69b6aaf-0821-4bc8-a43c-715e03a924ef.png" alt="Logo" width="1000" /&gt; 
 &lt;/div&gt; &lt;/a&gt; 
&lt;p&gt;Data integration:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;You can learn more about adding data to the OpenBB workspace from the &lt;a href="https://docs.openbb.co/workspace"&gt;docs&lt;/a&gt; or &lt;a href="https://github.com/OpenBB-finance/backends-for-openbb"&gt;this open source repository&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;AI Agents integration:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;You can learn more about adding AI agents to the OpenBB workspace from &lt;a href="https://github.com/OpenBB-finance/agents-for-openbb"&gt;this open source repository&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Integrating Open Data Platform to the OpenBB Workspace&lt;/h3&gt; 
&lt;p&gt;Connect this library to the OpenBB Workspace with a few simple commands, in a Python (3.9.21 - 3.12) environment.&lt;/p&gt; 
&lt;h4&gt;Run an ODP backend&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Install the packages.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip install "openbb[all]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Start the API server over localhost.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;openbb-api
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will launch a FastAPI server, via Uvicorn, at &lt;code&gt;127.0.0.1:6900&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;You can check that it works by going to &lt;a href="http://127.0.0.1:6900"&gt;http://127.0.0.1:6900&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;Integrate the ODP Backend to OpenBB Workspace&lt;/h4&gt; 
&lt;p&gt;Sign-in to the &lt;a href="https://pro.openbb.co/"&gt;OpenBB Workspace&lt;/a&gt;, and follow the following steps:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/75cffb4a-5e95-470a-b9d0-6ffd4067e069" alt="CleanShot 2025-05-17 at 09 51 56@2x" /&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Go to the "Apps" tab&lt;/li&gt; 
 &lt;li&gt;Click on "Connect backend"&lt;/li&gt; 
 &lt;li&gt;Fill in the form with: Name: Open Data Platform URL: &lt;a href="http://127.0.0.1:6900"&gt;http://127.0.0.1:6900&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Click on "Test". You should get a "Test successful" with the number of apps found.&lt;/li&gt; 
 &lt;li&gt;Click on "Add".&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;That's it.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;!-- TABLE OF CONTENTS --&gt; 
&lt;details closed="closed"&gt; 
 &lt;summary&gt;&lt;h2 style="display: inline-block"&gt;Table of Contents&lt;/h2&gt;&lt;/summary&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#1-installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#2-contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#3-license"&gt;License&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#4-disclaimer"&gt;Disclaimer&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#5-contacts"&gt;Contacts&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#6-star-history"&gt;Star History&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#7-contributors"&gt;Contributors&lt;/a&gt;&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;h2&gt;1. Installation&lt;/h2&gt; 
&lt;p&gt;The ODP Python Package can be installed from &lt;a href="https://pypi.org/project/openbb/"&gt;PyPI package&lt;/a&gt; by running &lt;code&gt;pip install openbb&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;or by cloning the repository directly with &lt;code&gt;git clone https://github.com/OpenBB-finance/OpenBB.git&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Please find more about the installation process, in the &lt;a href="https://docs.openbb.co/python/installation"&gt;OpenBB Documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;ODP CLI installation&lt;/h3&gt; 
&lt;p&gt;The ODP CLI is a command-line interface that allows you to access the ODP directly from your command line.&lt;/p&gt; 
&lt;p&gt;It can be installed by running &lt;code&gt;pip install openbb-cli&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;or by cloning the repository directly with &lt;code&gt;git clone https://github.com/OpenBB-finance/OpenBB.git&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Please find more about the installation process in the &lt;a href="https://docs.openbb.co/cli/installation"&gt;OpenBB Documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;2. Contributing&lt;/h2&gt; 
&lt;p&gt;There are three main ways of contributing to this project. (Hopefully you have starred the project by now ‚≠êÔ∏è)&lt;/p&gt; 
&lt;h3&gt;Become a Contributor&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;More information on our &lt;a href="https://docs.openbb.co/python/developer"&gt;Developer Documentation&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Create a GitHub ticket&lt;/h3&gt; 
&lt;p&gt;Before creating a ticket make sure the one you are creating doesn't exist already &lt;a href="https://github.com/OpenBB-finance/OpenBB/issues"&gt;among the existing issues&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&amp;amp;labels=bug&amp;amp;template=bug_report.md&amp;amp;title=%5BBug%5D"&gt;Report bug&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&amp;amp;labels=enhancement&amp;amp;template=enhancement.md&amp;amp;title=%5BIMPROVE%5D"&gt;Suggest improvement&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&amp;amp;labels=new+feature&amp;amp;template=feature_request.md&amp;amp;title=%5BFR%5D"&gt;Request a feature&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Provide feedback&lt;/h3&gt; 
&lt;p&gt;We are most active on &lt;a href="https://openbb.co/discord"&gt;our Discord&lt;/a&gt;, but feel free to reach out to us in any of &lt;a href="https://openbb.co/links"&gt;our social media&lt;/a&gt; for feedback.&lt;/p&gt; 
&lt;h2&gt;3. License&lt;/h2&gt; 
&lt;p&gt;Distributed under the AGPLv3 License. See &lt;a href="https://github.com/OpenBB-finance/OpenBB/raw/main/LICENSE"&gt;LICENSE&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;4. Disclaimer&lt;/h2&gt; 
&lt;p&gt;Trading in financial instruments involves high risks including the risk of losing some, or all, of your investment amount, and may not be suitable for all investors.&lt;/p&gt; 
&lt;p&gt;Before deciding to trade in a financial instrument you should be fully informed of the risks and costs associated with trading the financial markets, carefully consider your investment objectives, level of experience, and risk appetite, and seek professional advice where needed.&lt;/p&gt; 
&lt;p&gt;The data contained in the Open Data Platform is not necessarily accurate.&lt;/p&gt; 
&lt;p&gt;OpenBB and any provider of the data contained in this website will not accept liability for any loss or damage as a result of your trading, or your reliance on the information displayed.&lt;/p&gt; 
&lt;p&gt;All names, logos, and brands of third parties that may be referenced in our sites, products or documentation are trademarks of their respective owners. Unless otherwise specified, OpenBB and its products and services are not endorsed by, sponsored by, or affiliated with these third parties.&lt;/p&gt; 
&lt;p&gt;Our use of these names, logos, and brands is for identification purposes only, and does not imply any such endorsement, sponsorship, or affiliation.&lt;/p&gt; 
&lt;h2&gt;5. Contacts&lt;/h2&gt; 
&lt;p&gt;If you have any questions about the platform or anything OpenBB, feel free to email us at &lt;code&gt;support@openbb.co&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;If you want to say hi, or are interested in partnering with us, feel free to reach us at &lt;code&gt;hello@openbb.co&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Any of our social media platforms: &lt;a href="https://openbb.co/links"&gt;openbb.co/links&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;6. Star History&lt;/h2&gt; 
&lt;p&gt;This is a proxy of our growth and that we are just getting started.&lt;/p&gt; 
&lt;p&gt;But for more metrics important to us check &lt;a href="https://openbb.co/open"&gt;openbb.co/open&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://api.star-history.com/svg?repos=openbb-finance/OpenBB&amp;amp;type=Date&amp;amp;theme=dark"&gt;&lt;img src="https://api.star-history.com/svg?repos=openbb-finance/OpenBB&amp;amp;type=Date&amp;amp;theme=dark" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;7. Contributors&lt;/h2&gt; 
&lt;p&gt;OpenBB wouldn't be OpenBB without you. If we are going to disrupt financial industry, every contribution counts. Thank you for being part of this journey.&lt;/p&gt; 
&lt;a href="https://github.com/OpenBB-finance/OpenBB/graphs/contributors"&gt; &lt;img src="https://contributors-img.web.app/image?repo=OpenBB-finance/OpenBB" width="800" /&gt; &lt;/a&gt; 
&lt;!-- MARKDOWN LINKS &amp; IMAGES --&gt; 
&lt;!-- https://www.markdownguide.org/basic-syntax/#reference-style-links --&gt;</description>
    </item>
    
    <item>
      <title>microsoft/BitNet</title>
      <link>https://github.com/microsoft/BitNet</link>
      <description>&lt;p&gt;Official inference framework for 1-bit LLMs&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;bitnet.cpp&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://opensource.org/licenses/MIT"&gt;&lt;img src="https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true" alt="License: MIT" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/badge/version-1.0-blue" alt="version" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/microsoft/BitNet-b1.58-2B-4T"&gt;&lt;img src="https://raw.githubusercontent.com/microsoft/BitNet/main/assets/header_model_release.png" alt="BitNet Model on Hugging Face" width="800" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Try it out via this &lt;a href="https://bitnet-demo.azurewebsites.net/"&gt;demo&lt;/a&gt;, or build and run it on your own &lt;a href="https://github.com/microsoft/BitNet?tab=readme-ov-file#build-from-source"&gt;CPU&lt;/a&gt; or &lt;a href="https://github.com/microsoft/BitNet/raw/main/gpu/README.md"&gt;GPU&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;bitnet.cpp is the official inference framework for 1-bit LLMs (e.g., BitNet b1.58). It offers a suite of optimized kernels, that support &lt;strong&gt;fast&lt;/strong&gt; and &lt;strong&gt;lossless&lt;/strong&gt; inference of 1.58-bit models on CPU and GPU (NPU support will coming next).&lt;/p&gt; 
&lt;p&gt;The first release of bitnet.cpp is to support inference on CPUs. bitnet.cpp achieves speedups of &lt;strong&gt;1.37x&lt;/strong&gt; to &lt;strong&gt;5.07x&lt;/strong&gt; on ARM CPUs, with larger models experiencing greater performance gains. Additionally, it reduces energy consumption by &lt;strong&gt;55.4%&lt;/strong&gt; to &lt;strong&gt;70.0%&lt;/strong&gt;, further boosting overall efficiency. On x86 CPUs, speedups range from &lt;strong&gt;2.37x&lt;/strong&gt; to &lt;strong&gt;6.17x&lt;/strong&gt; with energy reductions between &lt;strong&gt;71.9%&lt;/strong&gt; to &lt;strong&gt;82.2%&lt;/strong&gt;. Furthermore, bitnet.cpp can run a 100B BitNet b1.58 model on a single CPU, achieving speeds comparable to human reading (5-7 tokens per second), significantly enhancing the potential for running LLMs on local devices. Please refer to the &lt;a href="https://arxiv.org/abs/2410.16144"&gt;technical report&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/microsoft/BitNet/main/assets/m2_performance.jpg" alt="m2_performance" width="800" /&gt; 
&lt;img src="https://raw.githubusercontent.com/microsoft/BitNet/main/assets/intel_performance.jpg" alt="m2_performance" width="800" /&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;The tested models are dummy setups used in a research context to demonstrate the inference performance of bitnet.cpp.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Demo&lt;/h2&gt; 
&lt;p&gt;A demo of bitnet.cpp running a BitNet b1.58 3B model on Apple M2:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/7f46b736-edec-4828-b809-4be780a3e5b1"&gt;https://github.com/user-attachments/assets/7f46b736-edec-4828-b809-4be780a3e5b1&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;What's New:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;05/20/2025 &lt;a href="https://github.com/microsoft/BitNet/raw/main/gpu/README.md"&gt;BitNet Official GPU inference kernel&lt;/a&gt; &lt;img src="https://img.shields.io/badge/NEW-red" alt="NEW" /&gt;&lt;/li&gt; 
 &lt;li&gt;04/14/2025 &lt;a href="https://huggingface.co/microsoft/BitNet-b1.58-2B-4T"&gt;BitNet Official 2B Parameter Model on Hugging Face&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;02/18/2025 &lt;a href="https://arxiv.org/abs/2502.11880"&gt;Bitnet.cpp: Efficient Edge Inference for Ternary LLMs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;11/08/2024 &lt;a href="https://arxiv.org/abs/2411.04965"&gt;BitNet a4.8: 4-bit Activations for 1-bit LLMs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;10/21/2024 &lt;a href="https://arxiv.org/abs/2410.16144"&gt;1-bit AI Infra: Part 1.1, Fast and Lossless BitNet b1.58 Inference on CPUs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;10/17/2024 bitnet.cpp 1.0 released.&lt;/li&gt; 
 &lt;li&gt;03/21/2024 &lt;a href="https://github.com/microsoft/unilm/raw/master/bitnet/The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ.pdf"&gt;The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;02/27/2024 &lt;a href="https://arxiv.org/abs/2402.17764"&gt;The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;10/17/2023 &lt;a href="https://arxiv.org/abs/2310.11453"&gt;BitNet: Scaling 1-bit Transformers for Large Language Models&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;p&gt;This project is based on the &lt;a href="https://github.com/ggerganov/llama.cpp"&gt;llama.cpp&lt;/a&gt; framework. We would like to thank all the authors for their contributions to the open-source community. Also, bitnet.cpp's kernels are built on top of the Lookup Table methodologies pioneered in &lt;a href="https://github.com/microsoft/T-MAC/"&gt;T-MAC&lt;/a&gt;. For inference of general low-bit LLMs beyond ternary models, we recommend using T-MAC.&lt;/p&gt; 
&lt;h2&gt;Official Models&lt;/h2&gt; 
&lt;table&gt;  
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;th rowspan="2"&gt;Model&lt;/th&gt; 
   &lt;th rowspan="2"&gt;Parameters&lt;/th&gt; 
   &lt;th rowspan="2"&gt;CPU&lt;/th&gt; 
   &lt;th colspan="3"&gt;Kernel&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th&gt;I2_S&lt;/th&gt; 
   &lt;th&gt;TL1&lt;/th&gt; 
   &lt;th&gt;TL2&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/microsoft/BitNet-b1.58-2B-4T"&gt;BitNet-b1.58-2B-4T&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;2.4B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Supported Models&lt;/h2&gt; 
&lt;p&gt;‚ùóÔ∏è&lt;strong&gt;We use existing 1-bit LLMs available on &lt;a href="https://huggingface.co/"&gt;Hugging Face&lt;/a&gt; to demonstrate the inference capabilities of bitnet.cpp. We hope the release of bitnet.cpp will inspire the development of 1-bit LLMs in large-scale settings in terms of model size and training tokens.&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt;  
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;th rowspan="2"&gt;Model&lt;/th&gt; 
   &lt;th rowspan="2"&gt;Parameters&lt;/th&gt; 
   &lt;th rowspan="2"&gt;CPU&lt;/th&gt; 
   &lt;th colspan="3"&gt;Kernel&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th&gt;I2_S&lt;/th&gt; 
   &lt;th&gt;TL1&lt;/th&gt; 
   &lt;th&gt;TL2&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/1bitLLM/bitnet_b1_58-large"&gt;bitnet_b1_58-large&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;0.7B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/1bitLLM/bitnet_b1_58-3B"&gt;bitnet_b1_58-3B&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;3.3B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/HF1BitLLM/Llama3-8B-1.58-100B-tokens"&gt;Llama3-8B-1.58-100B-tokens&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;8.0B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/collections/tiiuae/falcon3-67605ae03578be86e4e87026"&gt;Falcon3 Family&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;1B-10B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/collections/tiiuae/falcon-edge-series-6804fd13344d6d8a8fa71130"&gt;Falcon-E Family&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;1B-3B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;python&amp;gt;=3.9&lt;/li&gt; 
 &lt;li&gt;cmake&amp;gt;=3.22&lt;/li&gt; 
 &lt;li&gt;clang&amp;gt;=18 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;For Windows users, install &lt;a href="https://visualstudio.microsoft.com/downloads/"&gt;Visual Studio 2022&lt;/a&gt;. In the installer, toggle on at least the following options(this also automatically installs the required additional tools like CMake):&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Desktop-development with C++&lt;/li&gt; 
     &lt;li&gt;C++-CMake Tools for Windows&lt;/li&gt; 
     &lt;li&gt;Git for Windows&lt;/li&gt; 
     &lt;li&gt;C++-Clang Compiler for Windows&lt;/li&gt; 
     &lt;li&gt;MS-Build Support for LLVM-Toolset (clang)&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;For Debian/Ubuntu users, you can download with &lt;a href="https://apt.llvm.org/"&gt;Automatic installation script&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash -c "$(wget -O - https://apt.llvm.org/llvm.sh)"&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;conda (highly recommend)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Build from source&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] If you are using Windows, please remember to always use a Developer Command Prompt / PowerShell for VS2022 for the following commands. Please refer to the FAQs below if you see any issues.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ol&gt; 
 &lt;li&gt;Clone the repo&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone --recursive https://github.com/microsoft/BitNet.git
cd BitNet
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Install the dependencies&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# (Recommended) Create a new conda environment
conda create -n bitnet-cpp python=3.9
conda activate bitnet-cpp

pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Build the project&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Manually download the model and run with local path
huggingface-cli download microsoft/BitNet-b1.58-2B-4T-gguf --local-dir models/BitNet-b1.58-2B-4T
python setup_env.py -md models/BitNet-b1.58-2B-4T -q i2_s

&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;
usage: setup_env.py [-h] [--hf-repo {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}] [--model-dir MODEL_DIR] [--log-dir LOG_DIR] [--quant-type {i2_s,tl1}] [--quant-embd]
                    [--use-pretuned]

Setup the environment for running inference

optional arguments:
  -h, --help            show this help message and exit
  --hf-repo {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}, -hr {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}
                        Model used for inference
  --model-dir MODEL_DIR, -md MODEL_DIR
                        Directory to save/load the model
  --log-dir LOG_DIR, -ld LOG_DIR
                        Directory to save the logging info
  --quant-type {i2_s,tl1}, -q {i2_s,tl1}
                        Quantization type
  --quant-embd          Quantize the embeddings to f16
  --use-pretuned, -p    Use the pretuned kernel parameters
&lt;/pre&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Basic usage&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run inference with the quantized model
python run_inference.py -m models/BitNet-b1.58-2B-4T/ggml-model-i2_s.gguf -p "You are a helpful assistant" -cnv
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;
usage: run_inference.py [-h] [-m MODEL] [-n N_PREDICT] -p PROMPT [-t THREADS] [-c CTX_SIZE] [-temp TEMPERATURE] [-cnv]

Run inference

optional arguments:
  -h, --help            show this help message and exit
  -m MODEL, --model MODEL
                        Path to model file
  -n N_PREDICT, --n-predict N_PREDICT
                        Number of tokens to predict when generating text
  -p PROMPT, --prompt PROMPT
                        Prompt to generate text from
  -t THREADS, --threads THREADS
                        Number of threads to use
  -c CTX_SIZE, --ctx-size CTX_SIZE
                        Size of the prompt context
  -temp TEMPERATURE, --temperature TEMPERATURE
                        Temperature, a hyperparameter that controls the randomness of the generated text
  -cnv, --conversation  Whether to enable chat mode or not (for instruct models.)
                        (When this option is turned on, the prompt specified by -p will be used as the system prompt.)
&lt;/pre&gt; 
&lt;h3&gt;Benchmark&lt;/h3&gt; 
&lt;p&gt;We provide scripts to run the inference benchmark providing a model.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;usage: e2e_benchmark.py -m MODEL [-n N_TOKEN] [-p N_PROMPT] [-t THREADS]  
   
Setup the environment for running the inference  
   
required arguments:  
  -m MODEL, --model MODEL  
                        Path to the model file. 
   
optional arguments:  
  -h, --help  
                        Show this help message and exit. 
  -n N_TOKEN, --n-token N_TOKEN  
                        Number of generated tokens. 
  -p N_PROMPT, --n-prompt N_PROMPT  
                        Prompt to generate text from. 
  -t THREADS, --threads THREADS  
                        Number of threads to use. 
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Here's a brief explanation of each argument:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;-m&lt;/code&gt;, &lt;code&gt;--model&lt;/code&gt;: The path to the model file. This is a required argument that must be provided when running the script.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-n&lt;/code&gt;, &lt;code&gt;--n-token&lt;/code&gt;: The number of tokens to generate during the inference. It is an optional argument with a default value of 128.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-p&lt;/code&gt;, &lt;code&gt;--n-prompt&lt;/code&gt;: The number of prompt tokens to use for generating text. This is an optional argument with a default value of 512.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-t&lt;/code&gt;, &lt;code&gt;--threads&lt;/code&gt;: The number of threads to use for running the inference. It is an optional argument with a default value of 2.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-h&lt;/code&gt;, &lt;code&gt;--help&lt;/code&gt;: Show the help message and exit. Use this argument to display usage information.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python utils/e2e_benchmark.py -m /path/to/model -n 200 -p 256 -t 4  
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command would run the inference benchmark using the model located at &lt;code&gt;/path/to/model&lt;/code&gt;, generating 200 tokens from a 256 token prompt, utilizing 4 threads.&lt;/p&gt; 
&lt;p&gt;For the model layout that do not supported by any public model, we provide scripts to generate a dummy model with the given model layout, and run the benchmark on your machine:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python utils/generate-dummy-bitnet-model.py models/bitnet_b1_58-large --outfile models/dummy-bitnet-125m.tl1.gguf --outtype tl1 --model-size 125M

# Run benchmark with the generated model, use -m to specify the model path, -p to specify the prompt processed, -n to specify the number of token to generate
python utils/e2e_benchmark.py -m models/dummy-bitnet-125m.tl1.gguf -p 512 -n 128
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Convert from &lt;code&gt;.safetensors&lt;/code&gt; Checkpoints&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Prepare the .safetensors model file
huggingface-cli download microsoft/bitnet-b1.58-2B-4T-bf16 --local-dir ./models/bitnet-b1.58-2B-4T-bf16

# Convert to gguf model
python ./utils/convert-helper-bitnet.py ./models/bitnet-b1.58-2B-4T-bf16
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;FAQ (Frequently Asked Questions)üìå&lt;/h3&gt; 
&lt;h4&gt;Q1: The build dies with errors building llama.cpp due to issues with std::chrono in log.cpp?&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; This is an issue introduced in recent version of llama.cpp. Please refer to this &lt;a href="https://github.com/tinglou/llama.cpp/commit/4e3db1e3d78cc1bcd22bcb3af54bd2a4628dd323"&gt;commit&lt;/a&gt; in the &lt;a href="https://github.com/abetlen/llama-cpp-python/issues/1942"&gt;discussion&lt;/a&gt; to fix this issue.&lt;/p&gt; 
&lt;h4&gt;Q2: How to build with clang in conda environment on windows?&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Before building the project, verify your clang installation and access to Visual Studio tools by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;clang -v
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command checks that you are using the correct version of clang and that the Visual Studio tools are available. If you see an error message such as:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;'clang' is not recognized as an internal or external command, operable program or batch file.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It indicates that your command line window is not properly initialized for Visual Studio tools.&lt;/p&gt; 
&lt;p&gt;‚Ä¢ If you are using Command Prompt, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;"C:\Program Files\Microsoft Visual Studio\2022\Professional\Common7\Tools\VsDevCmd.bat" -startdir=none -arch=x64 -host_arch=x64
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;‚Ä¢ If you are using Windows PowerShell, run the following commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Import-Module "C:\Program Files\Microsoft Visual Studio\2022\Professional\Common7\Tools\Microsoft.VisualStudio.DevShell.dll" Enter-VsDevShell 3f0e31ad -SkipAutomaticLocation -DevCmdArguments "-arch=x64 -host_arch=x64"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;These steps will initialize your environment and allow you to use the correct Visual Studio tools.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>timescale/pg-aiguide</title>
      <link>https://github.com/timescale/pg-aiguide</link>
      <description>&lt;p&gt;MCP server and Claude plugin for Postgres skills and documentation. Helps AI coding tools generate better PostgreSQL code.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;pg-aiguide&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;AI-optimized PostgreSQL expertise for coding assistants&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;pg-aiguide helps AI coding tools write dramatically better PostgreSQL code. It provides:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Semantic search&lt;/strong&gt; across the official PostgreSQL manual (version-aware)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;AI-optimized ‚Äúskills‚Äù&lt;/strong&gt; ‚Äî curated, opinionated Postgres best practices used automatically by AI agents&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Extension ecosystem docs&lt;/strong&gt;, starting with TimescaleDB, with more coming soon&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Use it either as:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;a &lt;strong&gt;public MCP server&lt;/strong&gt; that can be used with any AI coding agent, or&lt;/li&gt; 
 &lt;li&gt;a &lt;strong&gt;Claude Code plugin&lt;/strong&gt; optimized for use with Claude's native skill support.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;‚≠ê Why pg-aiguide?&lt;/h2&gt; 
&lt;p&gt;AI coding tools often generate Postgres code that is:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;outdated&lt;/li&gt; 
 &lt;li&gt;missing constraints and indexes&lt;/li&gt; 
 &lt;li&gt;unaware of modern PG features&lt;/li&gt; 
 &lt;li&gt;inconsistent with real-world best practices&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;pg-aiguide fixes that by giving AI agents deep, versioned PostgreSQL knowledge and proven patterns.&lt;/p&gt; 
&lt;h3&gt;See the difference&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/5a426381-09b5-4635-9050-f55422253a3d"&gt;https://github.com/user-attachments/assets/5a426381-09b5-4635-9050-f55422253a3d&lt;/a&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Video Transcript &lt;/summary&gt; 
 &lt;p&gt;Prompt given to Claude Code:&lt;/p&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;Please describe the schema you would create for an e-commerce website two times, first with the tiger mcp server disabled, then with the tiger mcp server enabled. For each time, write the schema to its own file in the current working directory. Then compare the two files and let me know which approach generated the better schema, using both qualitative and quantitative reasons. For this example, only use standard Postgres.&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;p&gt;Result (summarized):&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;4√ó more constraints&lt;/strong&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;55% more indexes&lt;/strong&gt; (including partial/expression indexes)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;PG17-recommended patterns&lt;/strong&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Modern features&lt;/strong&gt; (&lt;code&gt;GENERATED ALWAYS AS IDENTITY&lt;/code&gt;, &lt;code&gt;NULLS NOT DISTINCT&lt;/code&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Cleaner naming &amp;amp; documentation&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;Conclusion: &lt;em&gt;pg-aiguide produces more robust, performant, maintainable schemas.&lt;/em&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;üöÄ Quickstart&lt;/h2&gt; 
&lt;p&gt;pg-aiguide is available as a &lt;strong&gt;public MCP server&lt;/strong&gt;:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://mcp.tigerdata.com/docs"&gt;https://mcp.tigerdata.com/docs&lt;/a&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Manual MCP configuration using JSON&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "pg-aiguide": {
      "url": "https://mcp.tigerdata.com/docs"
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p&gt;Or it can be used as a &lt;strong&gt;Claude Code Plugin&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;claude plugin marketplace add timescale/pg-aiguide
claude plugin install pg@aiguide
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Install by environment&lt;/h3&gt; 
&lt;h4&gt;One-click installs&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://cursor.com/en/install-mcp?name=pg-aiguide&amp;amp;config=eyJuYW1lIjoicGctYWlndWlkZSIsInR5cGUiOiJodHRwIiwidXJsIjoiaHR0cHM6Ly9tY3AudGlnZXJkYXRhLmNvbS9kb2NzIn0="&gt;&lt;img src="https://img.shields.io/badge/Install_in-Cursor-000000?style=flat-square&amp;amp;logoColor=white" alt="Install in Cursor" /&gt;&lt;/a&gt; &lt;a href="https://vscode.dev/redirect/mcp/install?name=pg-aiguide&amp;amp;config=%7B%22type%22%3A%22http%22%2C%22url%22%3A%22https%3A%2F%2Fmcp.tigerdata.com%2Fdocs%22%7D"&gt;&lt;img src="https://img.shields.io/badge/Install_in-VS_Code-0098FF?style=flat-square&amp;amp;logo=visualstudiocode&amp;amp;logoColor=white" alt="Install in VS Code" /&gt;&lt;/a&gt; &lt;a href="https://insiders.vscode.dev/redirect/mcp/install?name=pg-aiguide&amp;amp;config=%7B%22type%22%3A%22http%22%2C%22url%22%3A%22https%3A%2F%2Fmcp.tigerdata.com%2Fdocs%22%7D&amp;amp;quality=insiders"&gt;&lt;img src="https://img.shields.io/badge/Install_in-VS_Code_Insiders-24bfa5?style=flat-square&amp;amp;logo=visualstudiocode&amp;amp;logoColor=white" alt="Install in VS Code Insiders" /&gt;&lt;/a&gt; &lt;a href="https://vs-open.link/mcp-install?%7B%22type%22%3A%22http%22%2C%22url%22%3A%22https%3A%2F%2Fmcp.tigerdata.com%2Fdocs%22%7D"&gt;&lt;img src="https://img.shields.io/badge/Install_in-Visual_Studio-C16FDE?style=flat-square&amp;amp;logo=visualstudio&amp;amp;logoColor=white" alt="Install in Visual Studio" /&gt;&lt;/a&gt; &lt;a href="https://block.github.io/goose/extension?cmd=&amp;amp;arg=&amp;amp;id=pg-aiguide&amp;amp;name=pg-aiguide&amp;amp;description=MCP%20Server%20for%20pg-aiguide"&gt;&lt;img src="https://block.github.io/goose/img/extension-install-dark.svg?sanitize=true" alt="Install in Goose" /&gt;&lt;/a&gt; &lt;a href="https://lmstudio.ai/install-mcp?name=pg-aiguide&amp;amp;config=eyJuYW1lIjoicGctYWlndWlkZSIsInR5cGUiOiJodHRwIiwidXJsIjoiaHR0cHM6Ly9tY3AudGlnZXJkYXRhLmNvbS9kb2NzIn0="&gt;&lt;img src="https://files.lmstudio.ai/deeplink/mcp-install-light.svg?sanitize=true" alt="Add MCP Server pg-aiguide to LM Studio" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Claude Code&lt;/summary&gt; 
 &lt;p&gt;This repo serves as a claude code marketplace plugin. To install, run:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;claude plugin marketplace add timescale/pg-aiguide
claude plugin install pg@aiguide
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;This plugin uses the skills available in the &lt;code&gt;skills&lt;/code&gt; directory as well as our publicly available MCP server endpoint hosted by TigerData for searching PostgreSQL documentation.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; Codex &lt;/summary&gt; 
 &lt;p&gt;Run the following to add the MCP server to codex:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;codex mcp add --url "https://mcp.tigerdata.com/docs" pg-aiguide
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; Cursor &lt;/summary&gt; 
 &lt;p&gt;One-click install:&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://cursor.com/en-US/install-mcp?name=pg-aiguide&amp;amp;config=eyJ1cmwiOiJodHRwczovL21jcC50aWdlcmRhdGEuY29tL2RvY3MifQ%3D%3D"&gt;&lt;img src="https://cursor.com/deeplink/mcp-install-dark.svg?sanitize=true" alt="Install MCP Server" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;Or add the following to &lt;code&gt;.cursor/mcp.json&lt;/code&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "pg-aiguide": {
      "url": "https://mcp.tigerdata.com/docs"
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; Gemini CLI &lt;/summary&gt; 
 &lt;p&gt;Run the following to add the MCP server to Gemini CLI:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;gemini mcp add -s user pg-aiguide "https://mcp.tigerdata.com/docs" -t http
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; Visual Studio &lt;/summary&gt; 
 &lt;p&gt;Click the button to install:&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://vs-open.link/mcp-install?%7B%22type%22%3A%22http%22%2C%22url%22%3A%22https%3A%2F%2Fmcp.tigerdata.com%2Fdocs%22%7D"&gt;&lt;img src="https://img.shields.io/badge/Install_in-Visual_Studio-C16FDE?style=flat-square&amp;amp;logo=visualstudio&amp;amp;logoColor=white" alt="Install in Visual Studio" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; VS Code &lt;/summary&gt; 
 &lt;p&gt;Click the button to install:&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://vscode.dev/redirect/mcp/install?name=pg-aiguide&amp;amp;config=%7B%22type%22%3A%22http%22%2C%22url%22%3A%22https%3A%2F%2Fmcp.tigerdata.com%2Fdocs%22%7D"&gt;&lt;img src="https://img.shields.io/badge/Install_in-VS_Code-0098FF?style=flat-square&amp;amp;logo=visualstudiocode&amp;amp;logoColor=white" alt="Install in VS Code" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;Alternatively, run the following to add the MCP server to VS Code:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;code --add-mcp '{"name":"pg-aiguide","type":"http","url":"https://mcp.tigerdata.com/docs"}'
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; VS Code Insiders &lt;/summary&gt; 
 &lt;p&gt;Click the button to install:&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://insiders.vscode.dev/redirect/mcp/install?name=pg-aiguide&amp;amp;config=%7B%22type%22%3A%22http%22%2C%22url%22%3A%22https%3A%2F%2Fmcp.tigerdata.com%2Fdocs%22%7D&amp;amp;quality=insiders"&gt;&lt;img src="https://img.shields.io/badge/Install_in-VS_Code_Insiders-24bfa5?style=flat-square&amp;amp;logo=visualstudiocode&amp;amp;logoColor=white" alt="Install in VS Code Insiders" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;Alternatively, run the following to add the MCP server to VS Code Insiders:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;code-insiders --add-mcp '{"name":"pg-aiguide","type":"http","url":"https://mcp.tigerdata.com/docs"}'
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; Windsurf &lt;/summary&gt; 
 &lt;p&gt;Add the following to &lt;code&gt;~/.codeium/windsurf/mcp_config.json&lt;/code&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "pg-aiguide": {
      "serverUrl": "https://mcp.tigerdata.com/docs"
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;üí° Your First Prompt&lt;/h3&gt; 
&lt;p&gt;Once installed, pg-aiguide can answer Postgres questions or design schemas.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Simple schema example prompt&lt;/strong&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Create a Postgres table schema for storing usernames and unique email addresses.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Complex schema example prompt&lt;/strong&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;You are a senior software engineer. You are given a task to generate a Postgres schema for an IoT device company. The devices collect environmental data on a factory floor. The data includes temperature, humidity, pressure, as the main data points as well as other measurements that vary from device to device. Each device has a unique id and a human-readable name. We want to record the time the data was collected as well. Analysis for recent data includes finding outliers and anomalies based on measurements, as well as analyzing the data of particular devices for ad-hoc analysis. Historical data analysis includes analyzing the history of data for one device or getting statistics for all devices over long periods of time.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;h3&gt;Semantic Search (MCP Tools)&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/timescale/pg-aiguide/main/API.md#semantic_search_postgres_docs"&gt;&lt;strong&gt;&lt;code&gt;semantic_search_postgres_docs&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt; Performs semantic search over the official PostgreSQL manual, with results scoped to a specific Postgres version.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/timescale/pg-aiguide/main/API.md#semantic_search_tiger_docs"&gt;&lt;strong&gt;&lt;code&gt;semantic_search_tiger_docs&lt;/code&gt;&lt;/strong&gt; &lt;/a&gt; Searches Tiger Data‚Äôs documentation corpus, including TimescaleDB and future ecosystem extensions.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Skills (AI-Optimized Best Practices)&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/timescale/pg-aiguide/main/API.md#view_skill"&gt;&lt;code&gt;view_skill&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;&lt;br /&gt; Exposes curated, opinionated PostgreSQL best-practice skills used automatically by AI coding assistants.&lt;/p&gt; &lt;p&gt;These skills provide guidance on:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Schema design&lt;/li&gt; 
   &lt;li&gt;Indexing strategies&lt;/li&gt; 
   &lt;li&gt;Data types&lt;/li&gt; 
   &lt;li&gt;Data integrity and constraints&lt;/li&gt; 
   &lt;li&gt;Naming conventions&lt;/li&gt; 
   &lt;li&gt;Performance tuning&lt;/li&gt; 
   &lt;li&gt;Modern PostgreSQL features&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üîå Ecosystem Documentation&lt;/h2&gt; 
&lt;p&gt;Supported today:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;TimescaleDB&lt;/strong&gt; (docs + skills)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Coming soon:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;pgvector&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;PostGIS&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We welcome contributions for additional extensions and tools.&lt;/p&gt; 
&lt;h2&gt;üõ† Development&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/timescale/pg-aiguide/main/DEVELOPMENT.md"&gt;DEVELOPMENT.md&lt;/a&gt; for:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;running the MCP server locally&lt;/li&gt; 
 &lt;li&gt;adding new skills&lt;/li&gt; 
 &lt;li&gt;adding new docs&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;new Postgres best-practice skills&lt;/li&gt; 
 &lt;li&gt;additional documentation corpora&lt;/li&gt; 
 &lt;li&gt;search quality improvements&lt;/li&gt; 
 &lt;li&gt;bug reports and feature ideas&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìÑ License&lt;/h2&gt; 
&lt;p&gt;Apache 2.0&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>sherlock-project/sherlock</title>
      <link>https://github.com/sherlock-project/sherlock</link>
      <description>&lt;p&gt;Hunt down social media accounts by username across social networks&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;br /&gt; &lt;a href="https://sherlock-project.github.io/" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/sherlock-project/sherlock/master/images/sherlock-logo.png" alt="sherlock" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;span&gt;Hunt down social media accounts by username across &lt;a href="https://sherlockproject.xyz/sites"&gt;400+ social networks&lt;/a&gt;&lt;/span&gt; &lt;br /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://sherlockproject.xyz/installation"&gt;Installation&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;‚Ä¢&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href="https://sherlockproject.xyz/usage"&gt;Usage&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;‚Ä¢&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href="https://sherlockproject.xyz/contribute"&gt;Contributing&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;img width="70%" height="70%" src="https://raw.githubusercontent.com/sherlock-project/sherlock/master/images/demo.png" alt="demo" /&gt; &lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING]&lt;br /&gt; Packages for ParrotOS and Ubuntu 24.04, maintained by a third party, appear to be &lt;strong&gt;broken&lt;/strong&gt;.&lt;br /&gt; Users of these systems should defer to pipx/pip or Docker.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Method&lt;/th&gt; 
   &lt;th&gt;Notes&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;pipx install sherlock-project&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pip&lt;/code&gt; may be used in place of &lt;code&gt;pipx&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;docker run -it --rm sherlock/sherlock&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;dnf install sherlock-project&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Community-maintained packages are available for Debian (&amp;gt;= 13), Ubuntu (&amp;gt;= 22.10), Homebrew, Kali, and BlackArch. These packages are not directly supported or maintained by the Sherlock Project.&lt;/p&gt; 
&lt;p&gt;See all alternative installation methods &lt;a href="https://sherlockproject.xyz/installation"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;General usage&lt;/h2&gt; 
&lt;p&gt;To search for only one user:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sherlock user123
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To search for more than one user:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sherlock user1 user2 user3
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Accounts found will be stored in an individual text file with the corresponding username (e.g &lt;code&gt;user123.txt&lt;/code&gt;).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;$ sherlock --help
usage: sherlock [-h] [--version] [--verbose] [--folderoutput FOLDEROUTPUT]
                [--output OUTPUT] [--tor] [--unique-tor] [--csv] [--xlsx]
                [--site SITE_NAME] [--proxy PROXY_URL] [--json JSON_FILE]
                [--timeout TIMEOUT] [--print-all] [--print-found] [--no-color]
                [--browse] [--local] [--nsfw]
                USERNAMES [USERNAMES ...]

Sherlock: Find Usernames Across Social Networks (Version 0.14.3)

positional arguments:
  USERNAMES             One or more usernames to check with social networks.
                        Check similar usernames using {?} (replace to '_', '-', '.').

optional arguments:
  -h, --help            show this help message and exit
  --version             Display version information and dependencies.
  --verbose, -v, -d, --debug
                        Display extra debugging information and metrics.
  --folderoutput FOLDEROUTPUT, -fo FOLDEROUTPUT
                        If using multiple usernames, the output of the results will be
                        saved to this folder.
  --output OUTPUT, -o OUTPUT
                        If using single username, the output of the result will be saved
                        to this file.
  --tor, -t             Make requests over Tor; increases runtime; requires Tor to be
                        installed and in system path.
  --unique-tor, -u      Make requests over Tor with new Tor circuit after each request;
                        increases runtime; requires Tor to be installed and in system
                        path.
  --csv                 Create Comma-Separated Values (CSV) File.
  --xlsx                Create the standard file for the modern Microsoft Excel
                        spreadsheet (xlsx).
  --site SITE_NAME      Limit analysis to just the listed sites. Add multiple options to
                        specify more than one site.
  --proxy PROXY_URL, -p PROXY_URL
                        Make requests over a proxy. e.g. socks5://127.0.0.1:1080
  --json JSON_FILE, -j JSON_FILE
                        Load data from a JSON file or an online, valid, JSON file.
  --timeout TIMEOUT     Time (in seconds) to wait for response to requests (Default: 60)
  --print-all           Output sites where the username was not found.
  --print-found         Output sites where the username was found.
  --no-color            Don't color terminal output
  --browse, -b          Browse to all results on default browser.
  --local, -l           Force the use of the local data.json file.
  --nsfw                Include checking of NSFW sites from default list.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Apify Actor Usage &lt;a href="https://apify.com/netmilk/sherlock?fpr=sherlock"&gt;&lt;img src="https://apify.com/actor-badge?actor=netmilk/sherlock" alt="Sherlock Actor" /&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://apify.com/netmilk/sherlock?fpr=sherlock"&gt;&lt;img src="https://apify.com/ext/run-on-apify.png" alt="Run Sherlock Actor on Apify" width="176" height="39" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;You can run Sherlock in the cloud without installation using the &lt;a href="https://apify.com/netmilk/sherlock?fpr=sherlock"&gt;Sherlock Actor&lt;/a&gt; on &lt;a href="https://apify.com?fpr=sherlock"&gt;Apify&lt;/a&gt; free of charge.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;$ echo '{"usernames":["user123"]}' | apify call -so netmilk/sherlock
[{
  "username": "user123",
  "links": [
    "https://www.1337x.to/user/user123/",
    ...
  ]
}]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Read more about the &lt;a href="https://raw.githubusercontent.com/sherlock-project/sherlock/.actor/README.md"&gt;Sherlock Actor&lt;/a&gt;, including how to use it programmatically via the Apify &lt;a href="https://apify.com/netmilk/sherlock/api?fpr=sherlock"&gt;API&lt;/a&gt;, &lt;a href="https://docs.apify.com/cli/?fpr=sherlock"&gt;CLI&lt;/a&gt; and &lt;a href="https://docs.apify.com/sdk?fpr=sherlock"&gt;JS/TS and Python SDKs&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Credits&lt;/h2&gt; 
&lt;p&gt;Thank you to everyone who has contributed to Sherlock! ‚ù§Ô∏è&lt;/p&gt; 
&lt;a href="https://github.com/sherlock-project/sherlock/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?&amp;amp;columns=25&amp;amp;max=10000&amp;amp;&amp;amp;repo=sherlock-project/sherlock" alt="contributors" /&gt; &lt;/a&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;picture&gt; 
 &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=sherlock-project/sherlock&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
 &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=sherlock-project/sherlock&amp;amp;type=Date" /&gt; 
 &lt;img alt="Sherlock Project Star History Chart" src="https://api.star-history.com/svg?repos=sherlock-project/sherlock&amp;amp;type=Date" /&gt; 
&lt;/picture&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;MIT ¬© Sherlock Project&lt;br /&gt; Original Creator - &lt;a href="https://github.com/sdushantha"&gt;Siddharth Dushantha&lt;/a&gt;&lt;/p&gt; 
&lt;!-- Reference Links --&gt;</description>
    </item>
    
    <item>
      <title>google-gemini/computer-use-preview</title>
      <link>https://github.com/google-gemini/computer-use-preview</link>
      <description>&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Computer Use Preview&lt;/h1&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;This section will guide you through setting up and running the Computer Use Preview model, either the Gemini Developer API or Vertex AI. Follow these steps to get started.&lt;/p&gt; 
&lt;h3&gt;1. Installation&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Clone the Repository&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/google/computer-use-preview.git
cd computer-use-preview
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Set up Python Virtual Environment and Install Dependencies&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Install Playwright and Browser Dependencies&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install system dependencies required by Playwright for Chrome
playwright install-deps chrome

# Install the Chrome browser for Playwright
playwright install chrome
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Configuration&lt;/h3&gt; 
&lt;p&gt;You can get started using either the Gemini Developer API or Vertex AI.&lt;/p&gt; 
&lt;h4&gt;A. If using the Gemini Developer API:&lt;/h4&gt; 
&lt;p&gt;You need a Gemini API key to use the agent:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export GEMINI_API_KEY="YOUR_GEMINI_API_KEY"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or to add this to your virtual environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;echo 'export GEMINI_API_KEY="YOUR_GEMINI_API_KEY"' &amp;gt;&amp;gt; .venv/bin/activate
# After editing, you'll need to deactivate and reactivate your virtual
# environment if it's already active:
deactivate
source .venv/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Replace &lt;code&gt;YOUR_GEMINI_API_KEY&lt;/code&gt; with your actual key.&lt;/p&gt; 
&lt;h4&gt;B. If using the Vertex AI Client:&lt;/h4&gt; 
&lt;p&gt;You need to explicitly use Vertex AI, then provide project and location to use the agent:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export USE_VERTEXAI=true
export VERTEXAI_PROJECT="YOUR_PROJECT_ID"
export VERTEXAI_LOCATION="YOUR_LOCATION"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or to add this to your virtual environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;echo 'export USE_VERTEXAI=true' &amp;gt;&amp;gt; .venv/bin/activate
echo 'export VERTEXAI_PROJECT="your-project-id"' &amp;gt;&amp;gt; .venv/bin/activate
echo 'export VERTEXAI_LOCATION="your-location"' &amp;gt;&amp;gt; .venv/bin/activate
# After editing, you'll need to deactivate and reactivate your virtual
# environment if it's already active:
deactivate
source .venv/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Replace &lt;code&gt;YOUR_PROJECT_ID&lt;/code&gt; and &lt;code&gt;YOUR_LOCATION&lt;/code&gt; with your actual project and location.&lt;/p&gt; 
&lt;h3&gt;3. Running the Tool&lt;/h3&gt; 
&lt;p&gt;The primary way to use the tool is via the &lt;code&gt;main.py&lt;/code&gt; script.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;General Command Structure:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python main.py --query "Go to Google and type 'Hello World' into the search bar"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Available Environments:&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;You can specify a particular environment with the &lt;code&gt;--env &amp;lt;environment&amp;gt;&lt;/code&gt; flag. Available options:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;playwright&lt;/code&gt;: Runs the browser locally using Playwright.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;browserbase&lt;/code&gt;: Connects to a Browserbase instance.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Local Playwright&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Runs the agent using a Chrome browser instance controlled locally by Playwright.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python main.py --query="Go to Google and type 'Hello World' into the search bar" --env="playwright"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also specify an initial URL for the Playwright environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python main.py --query="Go to Google and type 'Hello World' into the search bar" --env="playwright" --initial_url="https://www.google.com/search?q=latest+AI+news"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Browserbase&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Runs the agent using Browserbase as the browser backend. Ensure the proper Browserbase environment variables are set:&lt;code&gt;BROWSERBASE_API_KEY&lt;/code&gt; and &lt;code&gt;BROWSERBASE_PROJECT_ID&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python main.py --query="Go to Google and type 'Hello World' into the search bar" --env="browserbase"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Agent CLI&lt;/h2&gt; 
&lt;p&gt;The &lt;code&gt;main.py&lt;/code&gt; script is the command-line interface (CLI) for running the browser agent.&lt;/p&gt; 
&lt;h3&gt;Command-Line Arguments&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Argument&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Required&lt;/th&gt; 
   &lt;th&gt;Default&lt;/th&gt; 
   &lt;th&gt;Supported Environment(s)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--query&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The natural language query for the browser agent to execute.&lt;/td&gt; 
   &lt;td&gt;Yes&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
   &lt;td&gt;All&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--env&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The computer use environment to use. Must be one of the following: &lt;code&gt;playwright&lt;/code&gt;, or &lt;code&gt;browserbase&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;No&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
   &lt;td&gt;All&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--initial_url&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The initial URL to load when the browser starts.&lt;/td&gt; 
   &lt;td&gt;No&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.google.com"&gt;https://www.google.com&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;All&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--highlight_mouse&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;If specified, the agent will attempt to highlight the mouse cursor's position in the screenshots. This is useful for visual debugging.&lt;/td&gt; 
   &lt;td&gt;No&lt;/td&gt; 
   &lt;td&gt;False (not highlighted)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;playwright&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Environment Variables&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Required&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;GEMINI_API_KEY&lt;/td&gt; 
   &lt;td&gt;Your API key for the Gemini model.&lt;/td&gt; 
   &lt;td&gt;Yes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BROWSERBASE_API_KEY&lt;/td&gt; 
   &lt;td&gt;Your API key for Browserbase.&lt;/td&gt; 
   &lt;td&gt;Yes (when using the browserbase environment)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BROWSERBASE_PROJECT_ID&lt;/td&gt; 
   &lt;td&gt;Your Project ID for Browserbase.&lt;/td&gt; 
   &lt;td&gt;Yes (when using the browserbase environment)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Known Issues&lt;/h2&gt; 
&lt;h3&gt;Playwright Dropdown Menu&lt;/h3&gt; 
&lt;p&gt;On certain operating systems, the Playwright browser is unable to capture &lt;code&gt;&amp;lt;select&amp;gt;&lt;/code&gt; elements because they are rendered by the operating system. As a result, the agent is unable to send the correct screenshot to the model.&lt;/p&gt; 
&lt;p&gt;There are several ways to mitigate this.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Use the Browserbase option instead of Playwright.&lt;/li&gt; 
 &lt;li&gt;Inject a script like &lt;a href="https://github.com/amitamb/proxy-select"&gt;proxy-select&lt;/a&gt; to render a custom &lt;code&gt;&amp;lt;select&amp;gt;&lt;/code&gt; element. You must inject &lt;code&gt;proxy-select.css&lt;/code&gt; and &lt;code&gt;proxy-select.js&lt;/code&gt; into each page that has a non-custom &lt;code&gt;&amp;lt;select&amp;gt;&lt;/code&gt; element. You can do this in the &lt;a href="https://github.com/google-gemini/computer-use-preview/raw/main/computers/playwright/playwright.py#L100"&gt;&lt;code&gt;Playwright.__enter__&lt;/code&gt;&lt;/a&gt; method by adding a few lines of code, like the following (replacing &lt;code&gt;PROXY_SELECT_JS&lt;/code&gt; and &lt;code&gt;PROXY_SELECT_CSS&lt;/code&gt; with the appropriate variables):&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;self._page.add_init_script(PROXY_SELECT_JS)
def inject_style(page):
    try:
        page.add_style_tag(content=PROXY_SELECT_CSS)
    except Exception as e:
        print(f"Error injecting style: {e}")

self._page.on('domcontentloaded', inject_style)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note, option 2 does not work 100% of the time, but is a temporary workaround for certain websites. The better option is to use Browserbase.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>OpenHands/OpenHands</title>
      <link>https://github.com/OpenHands/OpenHands</link>
      <description>&lt;p&gt;üôå OpenHands: AI-Driven Development&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a name="readme-top"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenHands/docs/main/openhands/static/img/logo.png" alt="Logo" width="200" /&gt; 
 &lt;h1 align="center" style="border-bottom: none"&gt;OpenHands: AI-Driven Development&lt;/h1&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/OpenHands/OpenHands/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/LICENSE-MIT-20B2AA?style=for-the-badge" alt="MIT License" /&gt;&lt;/a&gt; 
 &lt;a href="https://docs.google.com/spreadsheets/d/1wOUdFCMyY6Nt0AIqF705KN4JKOWgeI4wUGUP60krXXs/edit?gid=811504672#gid=811504672"&gt;&lt;img src="https://img.shields.io/badge/SWEBench-77.6-00cc00?logoColor=FFE165&amp;amp;style=for-the-badge" alt="Benchmark Score" /&gt;&lt;/a&gt; 
 &lt;br /&gt; 
 &lt;a href="https://docs.openhands.dev/sdk"&gt;&lt;img src="https://img.shields.io/badge/Documentation-000?logo=googledocs&amp;amp;logoColor=FFE165&amp;amp;style=for-the-badge" alt="Check out the documentation" /&gt;&lt;/a&gt; 
 &lt;a href="https://arxiv.org/abs/2511.03690"&gt;&lt;img src="https://img.shields.io/badge/Paper-000?logoColor=FFE165&amp;amp;logo=arxiv&amp;amp;style=for-the-badge" alt="Tech Report" /&gt;&lt;/a&gt; 
 &lt;!-- Keep these links. Translations will automatically update with the README. --&gt; 
 &lt;p&gt;&lt;a href="https://www.readme-i18n.com/OpenHands/OpenHands?lang=de"&gt;Deutsch&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/OpenHands/OpenHands?lang=es"&gt;Espa√±ol&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/OpenHands/OpenHands?lang=fr"&gt;fran√ßais&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/OpenHands/OpenHands?lang=ja"&gt;Êó•Êú¨Ë™û&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/OpenHands/OpenHands?lang=ko"&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/OpenHands/OpenHands?lang=pt"&gt;Portugu√™s&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/OpenHands/OpenHands?lang=ru"&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/OpenHands/OpenHands?lang=zh"&gt;‰∏≠Êñá&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;p&gt;üôå&amp;nbsp;Welcome to OpenHands, a &lt;a href="https://raw.githubusercontent.com/OpenHands/OpenHands/main/COMMUNITY.md"&gt;community&lt;/a&gt; focused on AI-driven development. We‚Äôd love for you to &lt;a href="https://dub.sh/openhands"&gt;join us on Slack&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;There are a few ways to work with OpenHands:&lt;/p&gt; 
&lt;h3&gt;OpenHands Software Agent SDK&lt;/h3&gt; 
&lt;p&gt;The SDK is a composable Python library that contains all of our agentic tech. It's the engine that powers everything else below.&lt;/p&gt; 
&lt;p&gt;Define agents in code, then run them locally, or scale to 1000s of agents in the cloud.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://docs.openhands.dev/sdk"&gt;Check out the docs&lt;/a&gt; or &lt;a href="https://github.com/OpenHands/software-agent-sdk/"&gt;view the source&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;OpenHands CLI&lt;/h3&gt; 
&lt;p&gt;The CLI is the easiest way to start using OpenHands. The experience will be familiar to anyone who has worked with e.g. Claude Code or Codex. You can power it with Claude, GPT, or any other LLM.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://docs.openhands.dev/openhands/usage/run-openhands/cli-mode"&gt;Check out the docs&lt;/a&gt; or &lt;a href="https://github.com/OpenHands/OpenHands-CLI"&gt;view the source&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;OpenHands Local GUI&lt;/h3&gt; 
&lt;p&gt;Use the Local GUI for running agents on your laptop. It comes with a REST API and a single-page React application. The experience will be familiar to anyone who has used Devin or Jules.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://docs.openhands.dev/openhands/usage/run-openhands/local-setup"&gt;Check out the docs&lt;/a&gt; or view the source in this repo.&lt;/p&gt; 
&lt;h3&gt;OpenHands Cloud&lt;/h3&gt; 
&lt;p&gt;This is a deployment of OpenHands GUI, running on hosted infrastructure.&lt;/p&gt; 
&lt;p&gt;You can try it with a free $10 credit by &lt;a href="https://app.all-hands.dev"&gt;signing in with your GitHub account&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;OpenHands Cloud comes with source-available features and integrations:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Integrations with Slack, Jira, and Linear&lt;/li&gt; 
 &lt;li&gt;Multi-user support&lt;/li&gt; 
 &lt;li&gt;RBAC and permissions&lt;/li&gt; 
 &lt;li&gt;Collaboration features (e.g., conversation sharing)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;OpenHands Enterprise&lt;/h3&gt; 
&lt;p&gt;Large enterprises can work with us to self-host OpenHands Cloud in their own VPC, via Kubernetes. OpenHands Enterprise can also work with the CLI and SDK above.&lt;/p&gt; 
&lt;p&gt;OpenHands Enterprise is source-available--you can see all the source code here in the enterprise/ directory, but you'll need to purchase a license if you want to run it for more than one month.&lt;/p&gt; 
&lt;p&gt;Enterprise contracts also come with extended support and access to our research team.&lt;/p&gt; 
&lt;p&gt;Learn more at &lt;a href="https://openhands.dev/enterprise"&gt;openhands.dev/enterprise&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Everything Else&lt;/h3&gt; 
&lt;p&gt;Check out our &lt;a href="https://github.com/orgs/openhands/projects/1"&gt;Product Roadmap&lt;/a&gt;, and feel free to &lt;a href="https://github.com/OpenHands/OpenHands/issues"&gt;open up an issue&lt;/a&gt; if there's something you'd like to see!&lt;/p&gt; 
&lt;p&gt;You might also be interested in our &lt;a href="https://github.com/OpenHands/benchmarks"&gt;evaluation infrastructure&lt;/a&gt;, our &lt;a href="https://github.com/OpenHands/openhands-chrome-extension/"&gt;chrome extension&lt;/a&gt;, or our &lt;a href="https://github.com/OpenHands/ToM-SWE"&gt;Theory-of-Mind module&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;All our work is available under the MIT license, except for the &lt;code&gt;enterprise/&lt;/code&gt; directory in this repository (see the &lt;a href="https://raw.githubusercontent.com/OpenHands/OpenHands/main/enterprise/LICENSE"&gt;enterprise license&lt;/a&gt; for details). The core &lt;code&gt;openhands&lt;/code&gt; and &lt;code&gt;agent-server&lt;/code&gt; Docker images are fully MIT-licensed as well.&lt;/p&gt; 
&lt;p&gt;If you need help with anything, or just want to chat, &lt;a href="https://dub.sh/openhands"&gt;come find us on Slack&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>resemble-ai/chatterbox</title>
      <link>https://github.com/resemble-ai/chatterbox</link>
      <description>&lt;p&gt;SoTA open-source TTS&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/resemble-ai/chatterbox/master/Chatterbox-Turbo.jpg" alt="Chatterbox Turbo Image" /&gt;&lt;/p&gt; 
&lt;h1&gt;Chatterbox TTS&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://resemble-ai.github.io/chatterbox_turbo_demopage/"&gt;&lt;img src="https://img.shields.io/badge/listen-demo_samples-blue" alt="Alt Text" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/spaces/ResembleAI/chatterbox-turbo-demo"&gt;&lt;img src="https://huggingface.co/datasets/huggingface/badges/resolve/main/open-in-hf-spaces-sm.svg?sanitize=true" alt="Alt Text" /&gt;&lt;/a&gt; &lt;a href="https://podonos.com/resembleai/chatterbox"&gt;&lt;img src="https://static-public.podonos.com/badges/insight-on-pdns-sm-dark.svg?sanitize=true" alt="Alt Text" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/rJq9cRJBJ6"&gt;&lt;img src="https://img.shields.io/discord/1377773249798344776?label=join%20discord&amp;amp;logo=discord&amp;amp;style=flat" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;_Made with ‚ô•Ô∏è by &lt;a href="https://resemble.ai" target="_blank"&gt;&lt;img width="100" alt="resemble-logo-horizontal" src="https://github.com/user-attachments/assets/35cf756b-3506-4943-9c72-c05ddfa4e525" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Chatterbox&lt;/strong&gt; is a family of three state-of-the-art, open-source text-to-speech models by Resemble AI.&lt;/p&gt; 
&lt;p&gt;We are excited to introduce &lt;strong&gt;Chatterbox-Turbo&lt;/strong&gt;, our most efficient model yet. Built on a streamlined 350M parameter architecture, &lt;strong&gt;Turbo&lt;/strong&gt; delivers high-quality speech with less compute and VRAM than our previous models. We have also distilled the speech-token-to-mel decoder, previously a bottleneck, reducing generation from 10 steps to just &lt;strong&gt;one&lt;/strong&gt;, while retaining high-fidelity audio output.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Paralinguistic tags&lt;/strong&gt; are now native to the Turbo model, allowing you to use &lt;code&gt;[cough]&lt;/code&gt;, &lt;code&gt;[laugh]&lt;/code&gt;, &lt;code&gt;[chuckle]&lt;/code&gt;, and more to add distinct realism. While Turbo was built primarily for low-latency voice agents, it excels at narration and creative workflows.&lt;/p&gt; 
&lt;p&gt;If you like the model but need to scale or tune it for higher accuracy, check out our competitively priced TTS service (&lt;a href="https://resemble.ai"&gt;link&lt;/a&gt;). It delivers reliable performance with ultra-low latency of sub 200ms‚Äîideal for production use in agents, applications, or interactive media.&lt;/p&gt; 
&lt;img width="1200" height="600" alt="Podonos Turbo Eval" src="https://storage.googleapis.com/chatterbox-demo-samples/turbo/podonos_turbo.png" /&gt; 
&lt;h3&gt;‚ö° Model Zoo&lt;/h3&gt; 
&lt;p&gt;Choose the right model for your application.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Model&lt;/th&gt; 
   &lt;th align="left"&gt;Size&lt;/th&gt; 
   &lt;th align="left"&gt;Languages&lt;/th&gt; 
   &lt;th align="left"&gt;Key Features&lt;/th&gt; 
   &lt;th align="left"&gt;Best For&lt;/th&gt; 
   &lt;th align="left"&gt;ü§ó&lt;/th&gt; 
   &lt;th align="left"&gt;Examples&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;Chatterbox-Turbo&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;350M&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;English&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Paralinguistic Tags (&lt;code&gt;[laugh]&lt;/code&gt;), Lower Compute and VRAM&lt;/td&gt; 
   &lt;td align="left"&gt;Zero-shot voice agents, Production&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://huggingface.co/spaces/ResembleAI/chatterbox-turbo-demo"&gt;Demo&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://resemble-ai.github.io/chatterbox_turbo_demopage/"&gt;Listen&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;Chatterbox-Multilingual &lt;a href="https://raw.githubusercontent.com/resemble-ai/chatterbox/master/#supported-languages"&gt;(Language list)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;500M&lt;/td&gt; 
   &lt;td align="left"&gt;23+&lt;/td&gt; 
   &lt;td align="left"&gt;Zero-shot cloning, Multiple Languages&lt;/td&gt; 
   &lt;td align="left"&gt;Global applications, Localization&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://huggingface.co/spaces/ResembleAI/Chatterbox-Multilingual-TTS"&gt;Demo&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://resemble-ai.github.io/chatterbox_demopage/"&gt;Listen&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;Chatterbox &lt;a href="https://raw.githubusercontent.com/resemble-ai/chatterbox/master/#original-chatterbox-tips"&gt;(Tips and Tricks)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;500M&lt;/td&gt; 
   &lt;td align="left"&gt;English&lt;/td&gt; 
   &lt;td align="left"&gt;CFG &amp;amp; Exaggeration tuning&lt;/td&gt; 
   &lt;td align="left"&gt;General zero-shot TTS with creative controls&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://huggingface.co/spaces/ResembleAI/Chatterbox"&gt;Demo&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://resemble-ai.github.io/chatterbox_demopage/"&gt;Listen&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install chatterbox-tts
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, you can install from source:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# conda create -yn chatterbox python=3.11
# conda activate chatterbox

git clone https://github.com/resemble-ai/chatterbox.git
cd chatterbox
pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We developed and tested Chatterbox on Python 3.11 on Debian 11 OS; the versions of the dependencies are pinned in &lt;code&gt;pyproject.toml&lt;/code&gt; to ensure consistency. You can modify the code or dependencies in this installation mode.&lt;/p&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h5&gt;Chatterbox-Turbo&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import torchaudio as ta
import torch
from chatterbox.tts_turbo import ChatterboxTurboTTS

# Load the Turbo model
model = ChatterboxTurboTTS.from_pretrained(device="cuda")

# Generate with Paralinguistic Tags
text = "Hi there, Sarah here from MochaFone calling you back [chuckle], have you got one minute to chat about the billing issue?"

# Generate audio (requires a reference clip for voice cloning)
wav = model.generate(text, audio_prompt_path="your_10s_ref_clip.wav")

ta.save("test-turbo.wav", wav, model.sr)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;Chatterbox and Chatterbox-Multilingual&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;
import torchaudio as ta
from chatterbox.tts import ChatterboxTTS
from chatterbox.mtl_tts import ChatterboxMultilingualTTS

# English example
model = ChatterboxTTS.from_pretrained(device="cuda")

text = "Ezreal and Jinx teamed up with Ahri, Yasuo, and Teemo to take down the enemy's Nexus in an epic late-game pentakill."
wav = model.generate(text)
ta.save("test-english.wav", wav, model.sr)

# Multilingual examples
multilingual_model = ChatterboxMultilingualTTS.from_pretrained(device=device)

french_text = "Bonjour, comment √ßa va? Ceci est le mod√®le de synth√®se vocale multilingue Chatterbox, il prend en charge 23 langues."
wav_french = multilingual_model.generate(spanish_text, language_id="fr")
ta.save("test-french.wav", wav_french, model.sr)

chinese_text = "‰Ω†Â•ΩÔºå‰ªäÂ§©Â§©Ê∞îÁúü‰∏çÈîôÔºåÂ∏åÊúõ‰Ω†Êúâ‰∏Ä‰∏™ÊÑâÂø´ÁöÑÂë®Êú´„ÄÇ"
wav_chinese = multilingual_model.generate(chinese_text, language_id="zh")
ta.save("test-chinese.wav", wav_chinese, model.sr)

# If you want to synthesize with a different voice, specify the audio prompt
AUDIO_PROMPT_PATH = "YOUR_FILE.wav"
wav = model.generate(text, audio_prompt_path=AUDIO_PROMPT_PATH)
ta.save("test-2.wav", wav, model.sr)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;code&gt;example_tts.py&lt;/code&gt; and &lt;code&gt;example_vc.py&lt;/code&gt; for more examples.&lt;/p&gt; 
&lt;h2&gt;Supported Languages&lt;/h2&gt; 
&lt;p&gt;Arabic (ar) ‚Ä¢ Danish (da) ‚Ä¢ German (de) ‚Ä¢ Greek (el) ‚Ä¢ English (en) ‚Ä¢ Spanish (es) ‚Ä¢ Finnish (fi) ‚Ä¢ French (fr) ‚Ä¢ Hebrew (he) ‚Ä¢ Hindi (hi) ‚Ä¢ Italian (it) ‚Ä¢ Japanese (ja) ‚Ä¢ Korean (ko) ‚Ä¢ Malay (ms) ‚Ä¢ Dutch (nl) ‚Ä¢ Norwegian (no) ‚Ä¢ Polish (pl) ‚Ä¢ Portuguese (pt) ‚Ä¢ Russian (ru) ‚Ä¢ Swedish (sv) ‚Ä¢ Swahili (sw) ‚Ä¢ Turkish (tr) ‚Ä¢ Chinese (zh)&lt;/p&gt; 
&lt;h2&gt;Original Chatterbox Tips&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;General Use (TTS and Voice Agents):&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Ensure that the reference clip matches the specified language tag. Otherwise, language transfer outputs may inherit the accent of the reference clip‚Äôs language. To mitigate this, set &lt;code&gt;cfg_weight&lt;/code&gt; to &lt;code&gt;0&lt;/code&gt;.&lt;/li&gt; 
   &lt;li&gt;The default settings (&lt;code&gt;exaggeration=0.5&lt;/code&gt;, &lt;code&gt;cfg_weight=0.5&lt;/code&gt;) work well for most prompts across all languages.&lt;/li&gt; 
   &lt;li&gt;If the reference speaker has a fast speaking style, lowering &lt;code&gt;cfg_weight&lt;/code&gt; to around &lt;code&gt;0.3&lt;/code&gt; can improve pacing.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Expressive or Dramatic Speech:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Try lower &lt;code&gt;cfg_weight&lt;/code&gt; values (e.g. &lt;code&gt;~0.3&lt;/code&gt;) and increase &lt;code&gt;exaggeration&lt;/code&gt; to around &lt;code&gt;0.7&lt;/code&gt; or higher.&lt;/li&gt; 
   &lt;li&gt;Higher &lt;code&gt;exaggeration&lt;/code&gt; tends to speed up speech; reducing &lt;code&gt;cfg_weight&lt;/code&gt; helps compensate with slower, more deliberate pacing.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Built-in PerTh Watermarking for Responsible AI&lt;/h2&gt; 
&lt;p&gt;Every audio file generated by Chatterbox includes &lt;a href="https://github.com/resemble-ai/perth"&gt;Resemble AI's Perth (Perceptual Threshold) Watermarker&lt;/a&gt; - imperceptible neural watermarks that survive MP3 compression, audio editing, and common manipulations while maintaining nearly 100% detection accuracy.&lt;/p&gt; 
&lt;h2&gt;Watermark extraction&lt;/h2&gt; 
&lt;p&gt;You can look for the watermark using the following script.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import perth
import librosa

AUDIO_PATH = "YOUR_FILE.wav"

# Load the watermarked audio
watermarked_audio, sr = librosa.load(AUDIO_PATH, sr=None)

# Initialize watermarker (same as used for embedding)
watermarker = perth.PerthImplicitWatermarker()

# Extract watermark
watermark = watermarker.get_watermark(watermarked_audio, sample_rate=sr)
print(f"Extracted watermark: {watermark}")
# Output: 0.0 (no watermark) or 1.0 (watermarked)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Official Discord&lt;/h2&gt; 
&lt;p&gt;üëã Join us on &lt;a href="https://discord.gg/rJq9cRJBJ6"&gt;Discord&lt;/a&gt; and let's build something awesome together!&lt;/p&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/FunAudioLLM/CosyVoice"&gt;Cosyvoice&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning"&gt;Real-Time-Voice-Cloning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/yl4579/HiFTNet"&gt;HiFT-GAN&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/meta-llama/llama3"&gt;Llama 3&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/xingchensong/S3Tokenizer"&gt;S3Tokenizer&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you find this model useful, please consider citing.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@misc{chatterboxtts2025,
  author       = {{Resemble AI}},
  title        = {{Chatterbox-TTS}},
  year         = {2025},
  howpublished = {\url{https://github.com/resemble-ai/chatterbox}},
  note         = {GitHub repository}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;Don't use this model to do bad things. Prompts are sourced from freely available data on the internet.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>davila7/claude-code-templates</title>
      <link>https://github.com/davila7/claude-code-templates</link>
      <description>&lt;p&gt;CLI tool for configuring and monitoring Claude Code&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="https://www.npmjs.com/package/claude-code-templates"&gt;&lt;img src="https://img.shields.io/npm/v/claude-code-templates.svg?sanitize=true" alt="npm version" /&gt;&lt;/a&gt; &lt;a href="https://www.npmjs.com/package/claude-code-templates"&gt;&lt;img src="https://img.shields.io/npm/dt/claude-code-templates.svg?sanitize=true" alt="npm downloads" /&gt;&lt;/a&gt; &lt;a href="https://opensource.org/licenses/MIT"&gt;&lt;img src="https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true" alt="License: MIT" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/davila7/claude-code-templates/main/CONTRIBUTING.md"&gt;&lt;img src="https://img.shields.io/badge/PRs-welcome-brightgreen.svg?sanitize=true" alt="PRs Welcome" /&gt;&lt;/a&gt; &lt;a href="https://z.ai/subscribe?ic=8JVLJQFSKB&amp;amp;utm_source=github&amp;amp;utm_medium=badge&amp;amp;utm_campaign=readme"&gt;&lt;img src="https://img.shields.io/badge/Sponsored%20by-Z.AI-2563eb?style=flat&amp;amp;logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggZD0iTTEyIDJMMiAyMkgyMkwxMiAyWiIgZmlsbD0id2hpdGUiLz4KPC9zdmc+" alt="Sponsored by Z.AI" /&gt;&lt;/a&gt; &lt;a href="https://buymeacoffee.com/daniavila"&gt;&lt;img src="https://img.shields.io/badge/Buy%20Me%20A%20Coffee-support-yellow?style=flat&amp;amp;logo=buy-me-a-coffee" alt="Buy Me A Coffee" /&gt;&lt;/a&gt; &lt;a href="https://github.com/davila7/claude-code-templates"&gt;&lt;img src="https://img.shields.io/github/stars/davila7/claude-code-templates.svg?style=social&amp;amp;label=Star" alt="GitHub stars" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/15113" target="_blank"&gt; &lt;img src="https://trendshift.io/api/badge/repositories/15113" alt="davila7%2Fclaude-code-templates | Trendshift" style="width: 200px; height: 40px;" width="125" height="40" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;br /&gt; &lt;a href="https://vercel.com/oss"&gt; &lt;img alt="Vercel OSS Program" src="https://vercel.com/oss/program-badge.svg?sanitize=true" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;h3&gt;ü§ù Partnership&lt;/h3&gt; 
 &lt;p&gt; &lt;strong&gt;This project is sponsored by &lt;a href="https://z.ai" target="_blank"&gt;Z.AI&lt;/a&gt;&lt;/strong&gt;&lt;br /&gt; Supporting Claude Code Templates with the &lt;strong&gt;GLM CODING PLAN&lt;/strong&gt; &lt;/p&gt; 
 &lt;p&gt; &lt;a href="https://z.ai/subscribe?ic=8JVLJQFSKB&amp;amp;utm_source=github&amp;amp;utm_medium=readme&amp;amp;utm_campaign=partnership" target="_blank"&gt; &lt;img src="https://img.shields.io/badge/Get%2010%25%20OFF-GLM%20Coding%20Plan-2563eb?style=for-the-badge" alt="GLM Coding Plan" /&gt; &lt;/a&gt; &lt;/p&gt; 
 &lt;p&gt; &lt;em&gt;Top-tier coding performance powered by GLM-4.6 ‚Ä¢ Starting at $3/month&lt;/em&gt;&lt;br /&gt; &lt;em&gt;Seamlessly integrates with Claude Code, Cursor, Cline &amp;amp; 10+ AI coding tools&lt;/em&gt; &lt;/p&gt; 
 &lt;p&gt; &lt;code&gt;npx claude-code-templates@latest --setting partnerships/glm-coding-plan --yes&lt;/code&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h1&gt;Claude Code Templates (&lt;a href="https://aitmpl.com"&gt;aitmpl.com&lt;/a&gt;)&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;Ready-to-use configurations for Anthropic's Claude Code.&lt;/strong&gt; A comprehensive collection of AI agents, custom commands, settings, hooks, external integrations (MCPs), and project templates to enhance your development workflow.&lt;/p&gt; 
&lt;h2&gt;Browse &amp;amp; Install Components and Templates&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://aitmpl.com"&gt;Browse All Templates&lt;/a&gt;&lt;/strong&gt; - Interactive web interface to explore and install 100+ agents, commands, settings, hooks, and MCPs.&lt;/p&gt; 
&lt;img width="1049" height="855" alt="Screenshot 2025-08-19 at 08 09 24" src="https://github.com/user-attachments/assets/e3617410-9b1c-4731-87b7-a3858800b737" /&gt; 
&lt;h2&gt;üöÄ Quick Installation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install a complete development stack
npx claude-code-templates@latest --agent development-team/frontend-developer --command testing/generate-tests --mcp development/github-integration --yes

# Browse and install interactively
npx claude-code-templates@latest

# Install specific components
npx claude-code-templates@latest --agent development-tools/code-reviewer --yes
npx claude-code-templates@latest --command performance/optimize-bundle --yes
npx claude-code-templates@latest --setting performance/mcp-timeouts --yes
npx claude-code-templates@latest --hook git/pre-commit-validation --yes
npx claude-code-templates@latest --mcp database/postgresql-integration --yes
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;What You Get&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Component&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Examples&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ü§ñ Agents&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;AI specialists for specific domains&lt;/td&gt; 
   &lt;td&gt;Security auditor, React performance optimizer, database architect&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;‚ö° Commands&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Custom slash commands&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;/generate-tests&lt;/code&gt;, &lt;code&gt;/optimize-bundle&lt;/code&gt;, &lt;code&gt;/check-security&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üîå MCPs&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;External service integrations&lt;/td&gt; 
   &lt;td&gt;GitHub, PostgreSQL, Stripe, AWS, OpenAI&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;‚öôÔ∏è Settings&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Claude Code configurations&lt;/td&gt; 
   &lt;td&gt;Timeouts, memory settings, output styles&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ü™ù Hooks&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Automation triggers&lt;/td&gt; 
   &lt;td&gt;Pre-commit validation, post-completion actions&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üé® Skills&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Reusable capabilities with progressive disclosure&lt;/td&gt; 
   &lt;td&gt;PDF processing, Excel automation, custom workflows&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;üõ†Ô∏è Additional Tools&lt;/h2&gt; 
&lt;p&gt;Beyond the template catalog, Claude Code Templates includes powerful development tools:&lt;/p&gt; 
&lt;h3&gt;üìä Claude Code Analytics&lt;/h3&gt; 
&lt;p&gt;Monitor your AI-powered development sessions in real-time with live state detection and performance metrics.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npx claude-code-templates@latest --analytics
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;üí¨ Conversation Monitor&lt;/h3&gt; 
&lt;p&gt;Mobile-optimized interface to view Claude responses in real-time with secure remote access.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Local access
npx claude-code-templates@latest --chats

# Secure remote access via Cloudflare Tunnel
npx claude-code-templates@latest --chats --tunnel
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;üîç Health Check&lt;/h3&gt; 
&lt;p&gt;Comprehensive diagnostics to ensure your Claude Code installation is optimized.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npx claude-code-templates@latest --health-check
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;üîå Plugin Dashboard&lt;/h3&gt; 
&lt;p&gt;View marketplaces, installed plugins, and manage permissions from a unified interface.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npx claude-code-templates@latest --plugins
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;üìñ Documentation&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://docs.aitmpl.com/"&gt;üìö docs.aitmpl.com&lt;/a&gt;&lt;/strong&gt; - Complete guides, examples, and API reference for all components and tools.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions! &lt;strong&gt;&lt;a href="https://aitmpl.com"&gt;Browse existing templates&lt;/a&gt;&lt;/strong&gt; to see what's available, then check our &lt;a href="https://raw.githubusercontent.com/davila7/claude-code-templates/main/CONTRIBUTING.md"&gt;contributing guidelines&lt;/a&gt; to add your own agents, commands, MCPs, settings, or hooks.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Please read our &lt;a href="https://raw.githubusercontent.com/davila7/claude-code-templates/main/CODE_OF_CONDUCT.md"&gt;Code of Conduct&lt;/a&gt; before contributing.&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Attribution&lt;/h2&gt; 
&lt;p&gt;This collection includes components from multiple sources:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Scientific Skills:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/K-Dense-AI/claude-scientific-skills"&gt;K-Dense-AI/claude-scientific-skills&lt;/a&gt;&lt;/strong&gt; by K-Dense Inc. - MIT License (139 scientific skills for biology, chemistry, medicine, and computational research)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Official Anthropic:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/anthropics/skills"&gt;anthropics/skills&lt;/a&gt;&lt;/strong&gt; - Official Anthropic skills (21 skills)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/anthropics/claude-code"&gt;anthropics/claude-code&lt;/a&gt;&lt;/strong&gt; - Development guides and examples (10 skills)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Community Skills &amp;amp; Agents:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/obra/superpowers"&gt;obra/superpowers&lt;/a&gt;&lt;/strong&gt; by Jesse Obra - MIT License (14 workflow skills)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/alirezarezvani/claude-skills"&gt;alirezarezvani/claude-skills&lt;/a&gt;&lt;/strong&gt; by Alireza Rezvani - MIT License (36 professional role skills)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/wshobson/agents"&gt;wshobson/agents&lt;/a&gt;&lt;/strong&gt; by wshobson - MIT License (48 agents)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;NerdyChefsAI Skills&lt;/strong&gt; - Community contribution - MIT License (specialized enterprise skills)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Commands &amp;amp; Tools:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/hesreallyhim/awesome-claude-code"&gt;awesome-claude-code&lt;/a&gt;&lt;/strong&gt; by hesreallyhim - CC0 1.0 Universal (21 commands)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/mehdi-lamrani/awesome-claude-skills"&gt;awesome-claude-skills&lt;/a&gt;&lt;/strong&gt; - Apache 2.0 (community skills)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;move-code-quality-skill&lt;/strong&gt; - MIT License&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;cocoindex-claude&lt;/strong&gt; - Apache 2.0&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Each of these resources retains its &lt;strong&gt;original license and attribution&lt;/strong&gt;, as defined by their respective authors. We respect and credit all original creators for their work and contributions to the Claude ecosystem.&lt;/p&gt; 
&lt;h2&gt;üìÑ License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the MIT License - see the &lt;a href="https://raw.githubusercontent.com/davila7/claude-code-templates/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h2&gt;üîó Links&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;üåê Browse Templates&lt;/strong&gt;: &lt;a href="https://aitmpl.com"&gt;aitmpl.com&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üìö Documentation&lt;/strong&gt;: &lt;a href="https://docs.aitmpl.com"&gt;docs.aitmpl.com&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üí¨ Community&lt;/strong&gt;: &lt;a href="https://github.com/davila7/claude-code-templates/discussions"&gt;GitHub Discussions&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üêõ Issues&lt;/strong&gt;: &lt;a href="https://github.com/davila7/claude-code-templates/issues"&gt;GitHub Issues&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Stargazers over time&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://starchart.cc/davila7/claude-code-templates"&gt;&lt;img src="https://starchart.cc/davila7/claude-code-templates.svg?variant=adaptive" alt="Stargazers over time" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;‚≠ê Found this useful? Give us a star to support the project!&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://buymeacoffee.com/daniavila"&gt;&lt;img src="https://img.buymeacoffee.com/button-api/?text=Buy%20me%20a%20coffee&amp;amp;slug=daniavila&amp;amp;button_colour=FFDD00&amp;amp;font_colour=000000&amp;amp;font_family=Cookie&amp;amp;outline_colour=000000&amp;amp;coffee_colour=ffffff" alt="Buy Me A Coffee" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>SYSTRAN/faster-whisper</title>
      <link>https://github.com/SYSTRAN/faster-whisper</link>
      <description>&lt;p&gt;Faster Whisper transcription with CTranslate2&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="https://github.com/SYSTRAN/faster-whisper/actions?query=workflow%3ACI"&gt;&lt;img src="https://github.com/SYSTRAN/faster-whisper/workflows/CI/badge.svg?sanitize=true" alt="CI" /&gt;&lt;/a&gt; &lt;a href="https://badge.fury.io/py/faster-whisper"&gt;&lt;img src="https://badge.fury.io/py/faster-whisper.svg?sanitize=true" alt="PyPI version" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Faster Whisper transcription with CTranslate2&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;faster-whisper&lt;/strong&gt; is a reimplementation of OpenAI's Whisper model using &lt;a href="https://github.com/OpenNMT/CTranslate2/"&gt;CTranslate2&lt;/a&gt;, which is a fast inference engine for Transformer models.&lt;/p&gt; 
&lt;p&gt;This implementation is up to 4 times faster than &lt;a href="https://github.com/openai/whisper"&gt;openai/whisper&lt;/a&gt; for the same accuracy while using less memory. The efficiency can be further improved with 8-bit quantization on both CPU and GPU.&lt;/p&gt; 
&lt;h2&gt;Benchmark&lt;/h2&gt; 
&lt;h3&gt;Whisper&lt;/h3&gt; 
&lt;p&gt;For reference, here's the time and memory usage that are required to transcribe &lt;a href="https://www.youtube.com/watch?v=0u7tTptBo9I"&gt;&lt;strong&gt;13 minutes&lt;/strong&gt;&lt;/a&gt; of audio using different implementations:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/openai/whisper"&gt;openai/whisper&lt;/a&gt;@&lt;a href="https://github.com/openai/whisper/tree/v20240930"&gt;v20240930&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ggerganov/whisper.cpp"&gt;whisper.cpp&lt;/a&gt;@&lt;a href="https://github.com/ggerganov/whisper.cpp/tree/v1.7.2"&gt;v1.7.2&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/huggingface/transformers"&gt;transformers&lt;/a&gt;@&lt;a href="https://github.com/huggingface/transformers/tree/v4.46.3"&gt;v4.46.3&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/SYSTRAN/faster-whisper"&gt;faster-whisper&lt;/a&gt;@&lt;a href="https://github.com/SYSTRAN/faster-whisper/tree/v1.1.0"&gt;v1.1.0&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Large-v2 model on GPU&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Implementation&lt;/th&gt; 
   &lt;th&gt;Precision&lt;/th&gt; 
   &lt;th&gt;Beam size&lt;/th&gt; 
   &lt;th&gt;Time&lt;/th&gt; 
   &lt;th&gt;VRAM Usage&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;openai/whisper&lt;/td&gt; 
   &lt;td&gt;fp16&lt;/td&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;2m23s&lt;/td&gt; 
   &lt;td&gt;4708MB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;whisper.cpp (Flash Attention)&lt;/td&gt; 
   &lt;td&gt;fp16&lt;/td&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;1m05s&lt;/td&gt; 
   &lt;td&gt;4127MB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;transformers (SDPA)[^1]&lt;/td&gt; 
   &lt;td&gt;fp16&lt;/td&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;1m52s&lt;/td&gt; 
   &lt;td&gt;4960MB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;faster-whisper&lt;/td&gt; 
   &lt;td&gt;fp16&lt;/td&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;1m03s&lt;/td&gt; 
   &lt;td&gt;4525MB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;faster-whisper (&lt;code&gt;batch_size=8&lt;/code&gt;)&lt;/td&gt; 
   &lt;td&gt;fp16&lt;/td&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;17s&lt;/td&gt; 
   &lt;td&gt;6090MB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;faster-whisper&lt;/td&gt; 
   &lt;td&gt;int8&lt;/td&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;59s&lt;/td&gt; 
   &lt;td&gt;2926MB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;faster-whisper (&lt;code&gt;batch_size=8&lt;/code&gt;)&lt;/td&gt; 
   &lt;td&gt;int8&lt;/td&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;16s&lt;/td&gt; 
   &lt;td&gt;4500MB&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;distil-whisper-large-v3 model on GPU&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Implementation&lt;/th&gt; 
   &lt;th&gt;Precision&lt;/th&gt; 
   &lt;th&gt;Beam size&lt;/th&gt; 
   &lt;th&gt;Time&lt;/th&gt; 
   &lt;th&gt;YT Commons WER&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;transformers (SDPA) (&lt;code&gt;batch_size=16&lt;/code&gt;)&lt;/td&gt; 
   &lt;td&gt;fp16&lt;/td&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;46m12s&lt;/td&gt; 
   &lt;td&gt;14.801&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;faster-whisper (&lt;code&gt;batch_size=16&lt;/code&gt;)&lt;/td&gt; 
   &lt;td&gt;fp16&lt;/td&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;25m50s&lt;/td&gt; 
   &lt;td&gt;13.527&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;em&gt;GPU Benchmarks are Executed with CUDA 12.4 on a NVIDIA RTX 3070 Ti 8GB.&lt;/em&gt; [^1]: transformers OOM for any batch size &amp;gt; 1&lt;/p&gt; 
&lt;h3&gt;Small model on CPU&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Implementation&lt;/th&gt; 
   &lt;th&gt;Precision&lt;/th&gt; 
   &lt;th&gt;Beam size&lt;/th&gt; 
   &lt;th&gt;Time&lt;/th&gt; 
   &lt;th&gt;RAM Usage&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;openai/whisper&lt;/td&gt; 
   &lt;td&gt;fp32&lt;/td&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;6m58s&lt;/td&gt; 
   &lt;td&gt;2335MB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;whisper.cpp&lt;/td&gt; 
   &lt;td&gt;fp32&lt;/td&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;2m05s&lt;/td&gt; 
   &lt;td&gt;1049MB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;whisper.cpp (OpenVINO)&lt;/td&gt; 
   &lt;td&gt;fp32&lt;/td&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;1m45s&lt;/td&gt; 
   &lt;td&gt;1642MB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;faster-whisper&lt;/td&gt; 
   &lt;td&gt;fp32&lt;/td&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;2m37s&lt;/td&gt; 
   &lt;td&gt;2257MB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;faster-whisper (&lt;code&gt;batch_size=8&lt;/code&gt;)&lt;/td&gt; 
   &lt;td&gt;fp32&lt;/td&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;1m06s&lt;/td&gt; 
   &lt;td&gt;4230MB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;faster-whisper&lt;/td&gt; 
   &lt;td&gt;int8&lt;/td&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;1m42s&lt;/td&gt; 
   &lt;td&gt;1477MB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;faster-whisper (&lt;code&gt;batch_size=8&lt;/code&gt;)&lt;/td&gt; 
   &lt;td&gt;int8&lt;/td&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;51s&lt;/td&gt; 
   &lt;td&gt;3608MB&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;em&gt;Executed with 8 threads on an Intel Core i7-12700K.&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.9 or greater&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Unlike openai-whisper, FFmpeg does &lt;strong&gt;not&lt;/strong&gt; need to be installed on the system. The audio is decoded with the Python library &lt;a href="https://github.com/PyAV-Org/PyAV"&gt;PyAV&lt;/a&gt; which bundles the FFmpeg libraries in its package.&lt;/p&gt; 
&lt;h3&gt;GPU&lt;/h3&gt; 
&lt;p&gt;GPU execution requires the following NVIDIA libraries to be installed:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://developer.nvidia.com/cublas"&gt;cuBLAS for CUDA 12&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://developer.nvidia.com/cudnn"&gt;cuDNN 9 for CUDA 12&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The latest versions of &lt;code&gt;ctranslate2&lt;/code&gt; only support CUDA 12 and cuDNN 9. For CUDA 11 and cuDNN 8, the current workaround is downgrading to the &lt;code&gt;3.24.0&lt;/code&gt; version of &lt;code&gt;ctranslate2&lt;/code&gt;, for CUDA 12 and cuDNN 8, downgrade to the &lt;code&gt;4.4.0&lt;/code&gt; version of &lt;code&gt;ctranslate2&lt;/code&gt;, (This can be done with &lt;code&gt;pip install --force-reinstall ctranslate2==4.4.0&lt;/code&gt; or specifying the version in a &lt;code&gt;requirements.txt&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;There are multiple ways to install the NVIDIA libraries mentioned above. The recommended way is described in the official NVIDIA documentation, but we also suggest other installation methods below.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Other installation methods (click to expand)&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; For all these methods below, keep in mind the above note regarding CUDA versions. Depending on your setup, you may need to install the &lt;em&gt;CUDA 11&lt;/em&gt; versions of libraries that correspond to the CUDA 12 libraries listed in the instructions below.&lt;/p&gt; 
 &lt;h4&gt;Use Docker&lt;/h4&gt; 
 &lt;p&gt;The libraries (cuBLAS, cuDNN) are installed in this official NVIDIA CUDA Docker images: &lt;code&gt;nvidia/cuda:12.3.2-cudnn9-runtime-ubuntu22.04&lt;/code&gt;.&lt;/p&gt; 
 &lt;h4&gt;Install with &lt;code&gt;pip&lt;/code&gt; (Linux only)&lt;/h4&gt; 
 &lt;p&gt;On Linux these libraries can be installed with &lt;code&gt;pip&lt;/code&gt;. Note that &lt;code&gt;LD_LIBRARY_PATH&lt;/code&gt; must be set before launching Python.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip install nvidia-cublas-cu12 nvidia-cudnn-cu12==9.*

export LD_LIBRARY_PATH=`python3 -c 'import os; import nvidia.cublas.lib; import nvidia.cudnn.lib; print(os.path.dirname(nvidia.cublas.lib.__file__) + ":" + os.path.dirname(nvidia.cudnn.lib.__file__))'`
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Download the libraries from Purfview's repository (Windows &amp;amp; Linux)&lt;/h4&gt; 
 &lt;p&gt;Purfview's &lt;a href="https://github.com/Purfview/whisper-standalone-win"&gt;whisper-standalone-win&lt;/a&gt; provides the required NVIDIA libraries for Windows &amp;amp; Linux in a &lt;a href="https://github.com/Purfview/whisper-standalone-win/releases/tag/libs"&gt;single archive&lt;/a&gt;. Decompress the archive and place the libraries in a directory included in the &lt;code&gt;PATH&lt;/code&gt;.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;The module can be installed from &lt;a href="https://pypi.org/project/faster-whisper/"&gt;PyPI&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install faster-whisper
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;Other installation methods (click to expand)&lt;/summary&gt; 
 &lt;h3&gt;Install the master branch&lt;/h3&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip install --force-reinstall "faster-whisper @ https://github.com/SYSTRAN/faster-whisper/archive/refs/heads/master.tar.gz"
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;Install a specific commit&lt;/h3&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip install --force-reinstall "faster-whisper @ https://github.com/SYSTRAN/faster-whisper/archive/a4f1cc8f11433e454c3934442b5e1a4ed5e865c3.tar.gz"
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Faster-whisper&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from faster_whisper import WhisperModel

model_size = "large-v3"

# Run on GPU with FP16
model = WhisperModel(model_size, device="cuda", compute_type="float16")

# or run on GPU with INT8
# model = WhisperModel(model_size, device="cuda", compute_type="int8_float16")
# or run on CPU with INT8
# model = WhisperModel(model_size, device="cpu", compute_type="int8")

segments, info = model.transcribe("audio.mp3", beam_size=5)

print("Detected language '%s' with probability %f" % (info.language, info.language_probability))

for segment in segments:
    print("[%.2fs -&amp;gt; %.2fs] %s" % (segment.start, segment.end, segment.text))
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Warning:&lt;/strong&gt; &lt;code&gt;segments&lt;/code&gt; is a &lt;em&gt;generator&lt;/em&gt; so the transcription only starts when you iterate over it. The transcription can be run to completion by gathering the segments in a list or a &lt;code&gt;for&lt;/code&gt; loop:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;segments, _ = model.transcribe("audio.mp3")
segments = list(segments)  # The transcription will actually run here.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Batched Transcription&lt;/h3&gt; 
&lt;p&gt;The following code snippet illustrates how to run batched transcription on an example audio file. &lt;code&gt;BatchedInferencePipeline.transcribe&lt;/code&gt; is a drop-in replacement for &lt;code&gt;WhisperModel.transcribe&lt;/code&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from faster_whisper import WhisperModel, BatchedInferencePipeline

model = WhisperModel("turbo", device="cuda", compute_type="float16")
batched_model = BatchedInferencePipeline(model=model)
segments, info = batched_model.transcribe("audio.mp3", batch_size=16)

for segment in segments:
    print("[%.2fs -&amp;gt; %.2fs] %s" % (segment.start, segment.end, segment.text))
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Faster Distil-Whisper&lt;/h3&gt; 
&lt;p&gt;The Distil-Whisper checkpoints are compatible with the Faster-Whisper package. In particular, the latest &lt;a href="https://huggingface.co/distil-whisper/distil-large-v3"&gt;distil-large-v3&lt;/a&gt; checkpoint is intrinsically designed to work with the Faster-Whisper transcription algorithm. The following code snippet demonstrates how to run inference with distil-large-v3 on a specified audio file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from faster_whisper import WhisperModel

model_size = "distil-large-v3"

model = WhisperModel(model_size, device="cuda", compute_type="float16")
segments, info = model.transcribe("audio.mp3", beam_size=5, language="en", condition_on_previous_text=False)

for segment in segments:
    print("[%.2fs -&amp;gt; %.2fs] %s" % (segment.start, segment.end, segment.text))
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more information about the distil-large-v3 model, refer to the original &lt;a href="https://huggingface.co/distil-whisper/distil-large-v3"&gt;model card&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Word-level timestamps&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;segments, _ = model.transcribe("audio.mp3", word_timestamps=True)

for segment in segments:
    for word in segment.words:
        print("[%.2fs -&amp;gt; %.2fs] %s" % (word.start, word.end, word.word))
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;VAD filter&lt;/h3&gt; 
&lt;p&gt;The library integrates the &lt;a href="https://github.com/snakers4/silero-vad"&gt;Silero VAD&lt;/a&gt; model to filter out parts of the audio without speech:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;segments, _ = model.transcribe("audio.mp3", vad_filter=True)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The default behavior is conservative and only removes silence longer than 2 seconds. See the available VAD parameters and default values in the &lt;a href="https://github.com/SYSTRAN/faster-whisper/raw/master/faster_whisper/vad.py"&gt;source code&lt;/a&gt;. They can be customized with the dictionary argument &lt;code&gt;vad_parameters&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;segments, _ = model.transcribe(
    "audio.mp3",
    vad_filter=True,
    vad_parameters=dict(min_silence_duration_ms=500),
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Vad filter is enabled by default for batched transcription.&lt;/p&gt; 
&lt;h3&gt;Logging&lt;/h3&gt; 
&lt;p&gt;The library logging level can be configured like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import logging

logging.basicConfig()
logging.getLogger("faster_whisper").setLevel(logging.DEBUG)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Going further&lt;/h3&gt; 
&lt;p&gt;See more model and transcription options in the &lt;a href="https://github.com/SYSTRAN/faster-whisper/raw/master/faster_whisper/transcribe.py"&gt;&lt;code&gt;WhisperModel&lt;/code&gt;&lt;/a&gt; class implementation.&lt;/p&gt; 
&lt;h2&gt;Community integrations&lt;/h2&gt; 
&lt;p&gt;Here is a non exhaustive list of open-source projects using faster-whisper. Feel free to add your project to the list!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/speaches-ai/speaches"&gt;speaches&lt;/a&gt; is an OpenAI compatible server using &lt;code&gt;faster-whisper&lt;/code&gt;. It's easily deployable with Docker, works with OpenAI SDKs/CLI, supports streaming, and live transcription.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/m-bain/whisperX"&gt;WhisperX&lt;/a&gt; is an award-winning Python library that offers speaker diarization and accurate word-level timestamps using wav2vec2 alignment&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Softcatala/whisper-ctranslate2"&gt;whisper-ctranslate2&lt;/a&gt; is a command line client based on faster-whisper and compatible with the original client from openai/whisper.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/MahmoudAshraf97/whisper-diarization"&gt;whisper-diarize&lt;/a&gt; is a speaker diarization tool that is based on faster-whisper and NVIDIA NeMo.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Purfview/whisper-standalone-win"&gt;whisper-standalone-win&lt;/a&gt; Standalone CLI executables of faster-whisper for Windows, Linux &amp;amp; macOS.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hedrergudene/asr-sd-pipeline"&gt;asr-sd-pipeline&lt;/a&gt; provides a scalable, modular, end to end multi-speaker speech to text solution implemented using AzureML pipelines.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/zh-plus/Open-Lyrics"&gt;Open-Lyrics&lt;/a&gt; is a Python library that transcribes voice files using faster-whisper, and translates/polishes the resulting text into &lt;code&gt;.lrc&lt;/code&gt; files in the desired language using OpenAI-GPT.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/geekodour/wscribe"&gt;wscribe&lt;/a&gt; is a flexible transcript generation tool supporting faster-whisper, it can export word level transcript and the exported transcript then can be edited with &lt;a href="https://github.com/geekodour/wscribe-editor"&gt;wscribe-editor&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/BANDAS-Center/aTrain"&gt;aTrain&lt;/a&gt; is a graphical user interface implementation of faster-whisper developed at the BANDAS-Center at the University of Graz for transcription and diarization in Windows (&lt;a href="https://apps.microsoft.com/detail/atrain/9N15Q44SZNS2"&gt;Windows Store App&lt;/a&gt;) and Linux.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ufal/whisper_streaming"&gt;Whisper-Streaming&lt;/a&gt; implements real-time mode for offline Whisper-like speech-to-text models with faster-whisper as the most recommended back-end. It implements a streaming policy with self-adaptive latency based on the actual source complexity, and demonstrates the state of the art.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/collabora/WhisperLive"&gt;WhisperLive&lt;/a&gt; is a nearly-live implementation of OpenAI's Whisper which uses faster-whisper as the backend to transcribe audio in real-time.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/BBC-Esq/ctranslate2-faster-whisper-transcriber"&gt;Faster-Whisper-Transcriber&lt;/a&gt; is a simple but reliable voice transcriber that provides a user-friendly interface.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/softcatala/open-dubbing"&gt;Open-dubbing&lt;/a&gt; is open dubbing is an AI dubbing system which uses machine learning models to automatically translate and synchronize audio dialogue into different languages.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/heimoshuiyu/whisper-fastapi"&gt;Whisper-FastAPI&lt;/a&gt; whisper-fastapi is a very simple script that provides an API backend compatible with OpenAI, HomeAssistant, and Konele (Android voice typing) formats.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Model conversion&lt;/h2&gt; 
&lt;p&gt;When loading a model from its size such as &lt;code&gt;WhisperModel("large-v3")&lt;/code&gt;, the corresponding CTranslate2 model is automatically downloaded from the &lt;a href="https://huggingface.co/Systran"&gt;Hugging Face Hub&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We also provide a script to convert any Whisper models compatible with the Transformers library. They could be the original OpenAI models or user fine-tuned models.&lt;/p&gt; 
&lt;p&gt;For example the command below converts the &lt;a href="https://huggingface.co/openai/whisper-large-v3"&gt;original "large-v3" Whisper model&lt;/a&gt; and saves the weights in FP16:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install transformers[torch]&amp;gt;=4.23

ct2-transformers-converter --model openai/whisper-large-v3 --output_dir whisper-large-v3-ct2
--copy_files tokenizer.json preprocessor_config.json --quantization float16
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;The option &lt;code&gt;--model&lt;/code&gt; accepts a model name on the Hub or a path to a model directory.&lt;/li&gt; 
 &lt;li&gt;If the option &lt;code&gt;--copy_files tokenizer.json&lt;/code&gt; is not used, the tokenizer configuration is automatically downloaded when the model is loaded later.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Models can also be converted from the code. See the &lt;a href="https://opennmt.net/CTranslate2/python/ctranslate2.converters.TransformersConverter.html"&gt;conversion API&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Load a converted model&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Directly load the model from a local directory:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;model = faster_whisper.WhisperModel("whisper-large-v3-ct2")
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/docs/transformers/model_sharing#upload-with-the-web-interface"&gt;Upload your model to the Hugging Face Hub&lt;/a&gt; and load it from its name:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;model = faster_whisper.WhisperModel("username/whisper-large-v3-ct2")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Comparing performance against other implementations&lt;/h2&gt; 
&lt;p&gt;If you are comparing the performance against other Whisper implementations, you should make sure to run the comparison with similar settings. In particular:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Verify that the same transcription options are used, especially the same beam size. For example in openai/whisper, &lt;code&gt;model.transcribe&lt;/code&gt; uses a default beam size of 1 but here we use a default beam size of 5.&lt;/li&gt; 
 &lt;li&gt;Transcription speed is closely affected by the number of words in the transcript, so ensure that other implementations have a similar WER (Word Error Rate) to this one.&lt;/li&gt; 
 &lt;li&gt;When running on CPU, make sure to set the same number of threads. Many frameworks will read the environment variable &lt;code&gt;OMP_NUM_THREADS&lt;/code&gt;, which can be set when running your script:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;OMP_NUM_THREADS=4 python3 my_script.py
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Shubhamsaboo/awesome-llm-apps</title>
      <link>https://github.com/Shubhamsaboo/awesome-llm-apps</link>
      <description>&lt;p&gt;Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="http://www.theunwindai.com"&gt; &lt;img src="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/unwind_black.png" width="900px" alt="Unwind AI" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://www.linkedin.com/in/shubhamsaboo/"&gt; &lt;img src="https://img.shields.io/badge/-Follow%20Shubham%20Saboo-blue?logo=linkedin&amp;amp;style=flat-square" alt="LinkedIn" /&gt; &lt;/a&gt; &lt;a href="https://twitter.com/Saboo_Shubham_"&gt; &lt;img src="https://img.shields.io/twitter/follow/Shubham_Saboo" alt="Twitter" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; 
 &lt;!-- Keep these links. Translations will automatically update with the README. --&gt; &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=de"&gt;Deutsch&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=es"&gt;Espa√±ol&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=fr"&gt;fran√ßais&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ja"&gt;Êó•Êú¨Ë™û&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ko"&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=pt"&gt;Portugu√™s&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ru"&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=zh"&gt;‰∏≠Êñá&lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h1&gt;üåü Awesome LLM Apps&lt;/h1&gt; 
&lt;p&gt;A curated collection of &lt;strong&gt;Awesome LLM apps built with RAG, AI Agents, Multi-agent Teams, MCP, Voice Agents, and more.&lt;/strong&gt; This repository features LLM apps that use models from &lt;img src="https://cdn.simpleicons.org/openai" alt="openai logo" width="25" height="15" /&gt;&lt;strong&gt;OpenAI&lt;/strong&gt; , &lt;img src="https://cdn.simpleicons.org/anthropic" alt="anthropic logo" width="25" height="15" /&gt;&lt;strong&gt;Anthropic&lt;/strong&gt;, &lt;img src="https://cdn.simpleicons.org/googlegemini" alt="google logo" width="25" height="18" /&gt;&lt;strong&gt;Google&lt;/strong&gt;, &lt;img src="https://cdn.simpleicons.org/x" alt="X logo" width="25" height="15" /&gt;&lt;strong&gt;xAI&lt;/strong&gt; and open-source models like &lt;img src="https://cdn.simpleicons.org/alibabacloud" alt="alibaba logo" width="25" height="15" /&gt;&lt;strong&gt;Qwen&lt;/strong&gt; or &lt;img src="https://cdn.simpleicons.org/meta" alt="meta logo" width="25" height="15" /&gt;&lt;strong&gt;Llama&lt;/strong&gt; that you can run locally on your computer.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/9876" target="_blank"&gt; &lt;img src="https://trendshift.io/api/badge/repositories/9876" alt="Shubhamsaboo%2Fawesome-llm-apps | Trendshift" style="width: 250px; height: 55px;" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;ü§î Why Awesome LLM Apps?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;üí° Discover practical and creative ways LLMs can be applied across different domains, from code repositories to email inboxes and more.&lt;/li&gt; 
 &lt;li&gt;üî• Explore apps that combine LLMs from OpenAI, Anthropic, Gemini, and open-source alternatives with AI Agents, Agent Teams, MCP &amp;amp; RAG.&lt;/li&gt; 
 &lt;li&gt;üéì Learn from well-documented projects and contribute to the growing open-source ecosystem of LLM-powered applications.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üôè Thanks to our sponsors&lt;/h2&gt; 
&lt;table align="center" cellpadding="16" cellspacing="12"&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td align="center"&gt; &lt;a href="https://tsdb.co/shubham-gh" target="_blank" rel="noopener" title="Tiger Data"&gt; &lt;img src="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/sponsors/tigerdata.png" alt="Tiger Data" width="500" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;a href="https://tsdb.co/shubham-gh" target="_blank" rel="noopener" style="text-decoration: none; color: #333; font-weight: bold; font-size: 18px;"&gt; Tiger Data MCP &lt;/a&gt; &lt;/td&gt; 
   &lt;td align="center"&gt; &lt;a href="https://github.com/speechmatics/speechmatics-academy" target="_blank" rel="noopener" title="Speechmatics"&gt; &lt;img src="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/sponsors/speechmatics.png" alt="Speechmatics" width="500" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;a href="https://github.com/speechmatics/speechmatics-academy" target="_blank" rel="noopener" style="text-decoration: none; color: #333; font-weight: bold; font-size: 18px;"&gt; Speechmatics &lt;/a&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt; &lt;a href="https://okara.ai/?utm_source=oss&amp;amp;utm_medium=sponsorship&amp;amp;utm_campaign=awesome-llm-apps" title="Okara"&gt; &lt;img src="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/sponsors/okara.png" alt="Okara" width="500" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;a href="https://okara.ai/?utm_source=oss&amp;amp;utm_medium=sponsorship&amp;amp;utm_campaign=awesome-llm-apps" style="text-decoration: none; color: #333; font-weight: bold; font-size: 18px;"&gt; Okara AI &lt;/a&gt; &lt;/td&gt; 
   &lt;td align="center"&gt; &lt;a href="https://sponsorunwindai.com/" title="Become a Sponsor"&gt; &lt;img src="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/sponsor_awesome_llm_apps.png" alt="Become a Sponsor" width="500" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;a href="https://sponsorunwindai.com/" style="text-decoration: none; color: #333; font-weight: bold; font-size: 18px;"&gt; Become a Sponsor &lt;/a&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;üìÇ Featured AI Projects&lt;/h2&gt; 
&lt;h3&gt;AI Agents&lt;/h3&gt; 
&lt;h3&gt;üå± Starter AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_blog_to_podcast_agent/"&gt;üéôÔ∏è AI Blog to Podcast Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_breakup_recovery_agent/"&gt;‚ù§Ô∏è‚Äçü©π AI Breakup Recovery Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_data_analysis_agent/"&gt;üìä AI Data Analysis Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_medical_imaging_agent/"&gt;ü©ª AI Medical Imaging Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_meme_generator_agent_browseruse/"&gt;üòÇ AI Meme Generator Agent (Browser)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_music_generator_agent/"&gt;üéµ AI Music Generator Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_travel_agent/"&gt;üõ´ AI Travel Agent (Local &amp;amp; Cloud)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/gemini_multimodal_agent_demo/"&gt;‚ú® Gemini Multimodal Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/mixture_of_agents/"&gt;üîÑ Mixture of Agents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/xai_finance_agent/"&gt;üìä xAI Finance Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/opeani_research_agent/"&gt;üîç OpenAI Research Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/web_scrapping_ai_agent/"&gt;üï∏Ô∏è Web Scraping AI Agent (Local &amp;amp; Cloud SDK)&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üöÄ Advanced AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_home_renovation_agent"&gt;üèöÔ∏è üçå AI Home Renovation Agent with Nano Banana Pro&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_deep_research_agent/"&gt;üîç AI Deep Research Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_due_diligence_agent"&gt;üìä AI Due Diligence Agent with Gemini 3 and Nano Banana Pro&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/research_agent_gemini_interaction_api"&gt;üî¨ AI Research Planner &amp;amp; Executor (Google Interactions API)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_consultant_agent"&gt;ü§ù AI Consultant Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_system_architect_r1/"&gt;üèóÔ∏è AI System Architect Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_financial_coach_agent/"&gt;üí∞ AI Financial Coach Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_movie_production_agent/"&gt;üé¨ AI Movie Production Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_investment_agent/"&gt;üìà AI Investment Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_health_fitness_agent/"&gt;üèãÔ∏è‚Äç‚ôÇÔ∏è AI Health &amp;amp; Fitness Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/product_launch_intelligence_agent"&gt;üöÄ AI Product Launch Intelligence Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_journalist_agent/"&gt;üóûÔ∏è AI Journalist Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_mental_wellbeing_agent/"&gt;üß† AI Mental Wellbeing Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_meeting_agent/"&gt;üìë AI Meeting Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_Self-Evolving_agent/"&gt;üß¨ AI Self-Evolving Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_news_and_podcast_agents/"&gt;üéß AI Social Media News and Podcast Agent&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üéÆ Autonomous Game Playing Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/autonomous_game_playing_agent_apps/ai_3dpygame_r1/"&gt;üéÆ AI 3D Pygame Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/autonomous_game_playing_agent_apps/ai_chess_agent/"&gt;‚ôú AI Chess Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/autonomous_game_playing_agent_apps/ai_tic_tac_toe_agent/"&gt;üé≤ AI Tic-Tac-Toe Agent&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ü§ù Multi-agent Teams&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_competitor_intelligence_agent_team/"&gt;üß≤ AI Competitor Intelligence Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_finance_agent_team/"&gt;üí≤ AI Finance Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_game_design_agent_team/"&gt;üé® AI Game Design Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_legal_agent_team/"&gt;üë®‚Äç‚öñÔ∏è AI Legal Agent Team (Cloud &amp;amp; Local)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_recruitment_agent_team/"&gt;üíº AI Recruitment Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_real_estate_agent_team"&gt;üè† AI Real Estate Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_services_agency/"&gt;üë®‚Äçüíº AI Services Agency (CrewAI)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_teaching_agent_team/"&gt;üë®‚Äçüè´ AI Teaching Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_coding_agent_team/"&gt;üíª Multimodal Coding Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_design_agent_team/"&gt;‚ú® Multimodal Design Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_uiux_feedback_agent_team/"&gt;üé® üçå Multimodal UI/UX Feedback Agent Team with Nano Banana&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_travel_planner_agent_team/"&gt;üåè AI Travel Planner Agent Team&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üó£Ô∏è Voice AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/voice_ai_agents/ai_audio_tour_agent/"&gt;üó£Ô∏è AI Audio Tour Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/voice_ai_agents/customer_support_voice_agent/"&gt;üìû Customer Support Voice Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/voice_ai_agents/voice_rag_openaisdk/"&gt;üîä Voice RAG Agent (OpenAI SDK)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/akshayaggarwal99/jarvis-ai-assistant"&gt;üéôÔ∏è OpenSource Voice Dictation Agent (like Wispr Flow&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;img src="https://cdn.simpleicons.org/modelcontextprotocol" alt="mcp logo" width="25" height="20" /&gt; MCP AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/mcp_ai_agents/browser_mcp_agent/"&gt;‚ôæÔ∏è Browser MCP Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/mcp_ai_agents/github_mcp_agent/"&gt;üêô GitHub MCP Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/mcp_ai_agents/notion_mcp_agent"&gt;üìë Notion MCP Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/mcp_ai_agents/ai_travel_planner_mcp_agent_team"&gt;üåç AI Travel Planner MCP Agent&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üìÄ RAG (Retrieval Augmented Generation)&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/agentic_rag_embedding_gemma"&gt;üî• Agentic RAG with Embedding Gemma&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/agentic_rag_with_reasoning/"&gt;üßê Agentic RAG with Reasoning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/ai_blog_search/"&gt;üì∞ AI Blog Search (RAG)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/autonomous_rag/"&gt;üîç Autonomous RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/contextualai_rag_agent/"&gt;üîÑ Contextual AI RAG Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/corrective_rag/"&gt;üîÑ Corrective RAG (CRAG)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/deepseek_local_rag_agent/"&gt;üêã Deepseek Local RAG Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/gemini_agentic_rag/"&gt;ü§î Gemini Agentic RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/hybrid_search_rag/"&gt;üëÄ Hybrid Search RAG (Cloud)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/llama3.1_local_rag/"&gt;üîÑ Llama 3.1 Local RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/local_hybrid_search_rag/"&gt;üñ•Ô∏è Local Hybrid Search RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/local_rag_agent/"&gt;ü¶ô Local RAG Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/rag-as-a-service/"&gt;üß© RAG-as-a-Service&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/rag_agent_cohere/"&gt;‚ú® RAG Agent with Cohere&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/rag_chain/"&gt;‚õìÔ∏è Basic RAG Chain&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/rag_database_routing/"&gt;üì† RAG with Database Routing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/vision_rag/"&gt;üñºÔ∏è Vision RAG&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üíæ LLM Apps with Memory Tutorials&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/ai_arxiv_agent_memory/"&gt;üíæ AI ArXiv Agent with Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/ai_travel_agent_memory/"&gt;üõ©Ô∏è AI Travel Agent with Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/llama3_stateful_chat/"&gt;üí¨ Llama3 Stateful Chat&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/llm_app_personalized_memory/"&gt;üìù LLM App with Personalized Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/local_chatgpt_with_memory/"&gt;üóÑÔ∏è Local ChatGPT Clone with Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/multi_llm_memory/"&gt;üß† Multi-LLM Application with Shared Memory&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üí¨ Chat with X Tutorials&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_github/"&gt;üí¨ Chat with GitHub (GPT &amp;amp; Llama3)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_gmail/"&gt;üì® Chat with Gmail&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_pdf/"&gt;üìÑ Chat with PDF (GPT &amp;amp; Llama3)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_research_papers/"&gt;üìö Chat with Research Papers (ArXiv) (GPT &amp;amp; Llama3)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_substack/"&gt;üìù Chat with Substack&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_youtube_videos/"&gt;üìΩÔ∏è Chat with YouTube Videos&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üéØ LLM Optimization Tools&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_optimization_tools/toonify_token_optimization/"&gt;üéØ Toonify Token Optimization&lt;/a&gt; - Reduce LLM API costs by 30-60% using TOON format&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üîß LLM Fine-tuning Tutorials&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;img src="https://cdn.simpleicons.org/google" alt="google logo" width="20" height="15" /&gt; &lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_finetuning_tutorials/gemma3_finetuning/"&gt;Gemma 3 Fine-tuning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;img src="https://cdn.simpleicons.org/meta" alt="meta logo" width="25" height="15" /&gt; &lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_finetuning_tutorials/llama3.2_finetuning/"&gt;Llama 3.2 Fine-tuning&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üßë‚Äçüè´ AI Agent Framework Crash Course&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://cdn.simpleicons.org/google" alt="google logo" width="25" height="15" /&gt; &lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/ai_agent_framework_crash_course/google_adk_crash_course/"&gt;Google ADK Crash Course&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Starter agent; model‚Äëagnostic (OpenAI, Claude)&lt;/li&gt; 
 &lt;li&gt;Structured outputs (Pydantic)&lt;/li&gt; 
 &lt;li&gt;Tools: built‚Äëin, function, third‚Äëparty, MCP tools&lt;/li&gt; 
 &lt;li&gt;Memory; callbacks; Plugins&lt;/li&gt; 
 &lt;li&gt;Simple multi‚Äëagent; Multi‚Äëagent patterns&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://cdn.simpleicons.org/openai" alt="openai logo" width="25" height="15" /&gt; &lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/ai_agent_framework_crash_course/openai_sdk_crash_course/"&gt;OpenAI Agents SDK Crash Course&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Starter agent; function calling; structured outputs&lt;/li&gt; 
 &lt;li&gt;Tools: built‚Äëin, function, third‚Äëparty integrations&lt;/li&gt; 
 &lt;li&gt;Memory; callbacks; evaluation&lt;/li&gt; 
 &lt;li&gt;Multi‚Äëagent patterns; agent handoffs&lt;/li&gt; 
 &lt;li&gt;Swarm orchestration; routing logic&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üöÄ Getting Started&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Clone the repository&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/Shubhamsaboo/awesome-llm-apps.git 
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Navigate to the desired project directory&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cd awesome-llm-apps/starter_ai_agents/ai_travel_agent
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install the required dependencies&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Follow the project-specific instructions&lt;/strong&gt; in each project's &lt;code&gt;README.md&lt;/code&gt; file to set up and run the app.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;&lt;img src="https://cdn.simpleicons.org/github" alt="github logo" width="25" height="20" /&gt; Thank You, Community, for the Support! üôè&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#Shubhamsaboo/awesome-llm-apps&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=Shubhamsaboo/awesome-llm-apps&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;üåü &lt;strong&gt;Don‚Äôt miss out on future updates! Star the repo now and be the first to know about new and exciting LLM apps with RAG and AI Agents.&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>yichuan-w/LEANN</title>
      <link>https://github.com/yichuan-w/LEANN</link>
      <description>&lt;p&gt;RAG on Everything with LEANN. Enjoy 97% storage savings while running a fast, accurate, and 100% private RAG application on your personal device.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/yichuan-w/LEANN/main/assets/logo-text.png" alt="LEANN Logo" width="400" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://img.shields.io/badge/Python-3.9%20%7C%203.10%20%7C%203.11%20%7C%203.12%20%7C%203.13-blue.svg?sanitize=true" alt="Python Versions" /&gt; &lt;img src="https://github.com/yichuan-w/LEANN/actions/workflows/build-and-publish.yml/badge.svg?sanitize=true" alt="CI Status" /&gt; &lt;img src="https://img.shields.io/badge/Platform-Ubuntu%20%26%20Arch%20%26%20WSL%20%7C%20macOS%20(ARM64%2FIntel)-lightgrey" alt="Platform" /&gt; &lt;img src="https://img.shields.io/badge/License-MIT-green.svg?sanitize=true" alt="MIT License" /&gt; &lt;img src="https://img.shields.io/badge/MCP-Native%20Integration-blue" alt="MCP Integration" /&gt; &lt;a href="https://join.slack.com/t/leann-e2u9779/shared_invite/zt-3ckd2f6w1-OX08~NN4gkWhh10PRVBj1Q"&gt; &lt;img src="https://img.shields.io/badge/Slack-Join-4A154B?logo=slack&amp;amp;logoColor=white" alt="Join Slack" /&gt; &lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/assets/wechat_user_group.JPG" title="Join WeChat group"&gt; &lt;img src="https://img.shields.io/badge/WeChat-Join-2DC100?logo=wechat&amp;amp;logoColor=white" alt="Join WeChat group" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://forms.gle/rDbZf864gMNxhpTq8"&gt; &lt;img src="https://img.shields.io/badge/üì£_Community_Survey-Help_Shape_v0.4-007ec6?style=for-the-badge&amp;amp;logo=google-forms&amp;amp;logoColor=white" alt="Take Survey" /&gt; &lt;/a&gt; 
 &lt;p&gt; We track &lt;b&gt;zero telemetry&lt;/b&gt;. This survey is the ONLY way to tell us if you want &lt;br /&gt; &lt;b&gt;GPU Acceleration&lt;/b&gt; or &lt;b&gt;More Integrations&lt;/b&gt; next.&lt;br /&gt; üëâ &lt;a href="https://forms.gle/rDbZf864gMNxhpTq8"&gt;&lt;b&gt;Click here to cast your vote (2 mins)&lt;/b&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2 align="center" tabindex="-1" class="heading-element" dir="auto"&gt; The smallest vector index in the world. RAG Everything with LEANN! &lt;/h2&gt; 
&lt;p&gt;LEANN is an innovative vector database that democratizes personal AI. Transform your laptop into a powerful RAG system that can index and search through millions of documents while using &lt;strong&gt;97% less storage&lt;/strong&gt; than traditional solutions &lt;strong&gt;without accuracy loss&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;LEANN achieves this through &lt;em&gt;graph-based selective recomputation&lt;/em&gt; with &lt;em&gt;high-degree preserving pruning&lt;/em&gt;, computing embeddings on-demand instead of storing them all. &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#%EF%B8%8F-architecture--how-it-works"&gt;Illustration Fig ‚Üí&lt;/a&gt; | &lt;a href="https://arxiv.org/abs/2506.08276"&gt;Paper ‚Üí&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Ready to RAG Everything?&lt;/strong&gt; Transform your laptop into a personal AI assistant that can semantic search your &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-personal-data-manager-process-any-documents-pdf-txt-md"&gt;file system&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-your-personal-email-secretary-rag-on-apple-mail"&gt;emails&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-time-machine-for-the-web-rag-your-entire-browser-history"&gt;browser history&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-wechat-detective-unlock-your-golden-memories"&gt;chat history&lt;/a&gt;&lt;/strong&gt; (&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-wechat-detective-unlock-your-golden-memories"&gt;WeChat&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-imessage-history-your-personal-conversation-archive"&gt;iMessage&lt;/a&gt;), &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-chatgpt-chat-history-your-personal-ai-conversation-archive"&gt;agent memory&lt;/a&gt;&lt;/strong&gt; (&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-chatgpt-chat-history-your-personal-ai-conversation-archive"&gt;ChatGPT&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-claude-chat-history-your-personal-ai-conversation-archive"&gt;Claude&lt;/a&gt;), &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#mcp-integration-rag-on-live-data-from-any-platform"&gt;live data&lt;/a&gt;&lt;/strong&gt; (&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#slack-messages-search-your-team-conversations"&gt;Slack&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-twitter-bookmarks-your-personal-tweet-library"&gt;Twitter&lt;/a&gt;), &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-claude-code-integration-transform-your-development-workflow"&gt;codebase&lt;/a&gt;&lt;/strong&gt;* , or external knowledge bases (i.e., 60M documents) - all on your laptop, with zero cloud costs and complete privacy.&lt;/p&gt; 
&lt;p&gt;* Claude Code only supports basic &lt;code&gt;grep&lt;/code&gt;-style keyword search. &lt;strong&gt;LEANN&lt;/strong&gt; is a drop-in &lt;strong&gt;semantic search MCP service fully compatible with Claude Code&lt;/strong&gt;, unlocking intelligent retrieval without changing your workflow. üî• Check out &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/packages/leann-mcp/README.md"&gt;the easy setup ‚Üí&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Why LEANN?&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/yichuan-w/LEANN/main/assets/effects.png" alt="LEANN vs Traditional Vector DB Storage Comparison" width="70%" /&gt; &lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;The numbers speak for themselves:&lt;/strong&gt; Index 60 million text chunks in just 6GB instead of 201GB. From emails to browser history, everything fits on your laptop. &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-storage-comparison"&gt;See detailed benchmarks for different applications below ‚Üì&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;üîí &lt;strong&gt;Privacy:&lt;/strong&gt; Your data never leaves your laptop. No OpenAI, no cloud, no "terms of service".&lt;/p&gt; 
&lt;p&gt;ü™∂ &lt;strong&gt;Lightweight:&lt;/strong&gt; Graph-based recomputation eliminates heavy embedding storage, while smart graph pruning and CSR format minimize graph storage overhead. Always less storage, less memory usage!&lt;/p&gt; 
&lt;p&gt;üì¶ &lt;strong&gt;Portable:&lt;/strong&gt; Transfer your entire knowledge base between devices (even with others) with minimal cost - your personal AI memory travels with you.&lt;/p&gt; 
&lt;p&gt;üìà &lt;strong&gt;Scalability:&lt;/strong&gt; Handle messy personal data that would crash traditional vector DBs, easily managing your growing personalized data and agent generated memory!&lt;/p&gt; 
&lt;p&gt;‚ú® &lt;strong&gt;No Accuracy Loss:&lt;/strong&gt; Maintain the same search quality as heavyweight solutions while using 97% less storage.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;üì¶ Prerequisites: Install uv&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://docs.astral.sh/uv/getting-started/installation/#installation-methods"&gt;Install uv&lt;/a&gt; first if you don't have it. Typically, you can install it with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -LsSf https://astral.sh/uv/install.sh | sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;üöÄ Quick Install&lt;/h3&gt; 
&lt;p&gt;Clone the repository to access all examples and try amazing applications,&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/yichuan-w/LEANN.git leann
cd leann
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;and install LEANN from &lt;a href="https://pypi.org/project/leann/"&gt;PyPI&lt;/a&gt; to run them immediately:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv venv
source .venv/bin/activate
uv pip install leann
&lt;/code&gt;&lt;/pre&gt; 
&lt;!--
&gt; Low-resource? See "Low-resource setups" in the [Configuration Guide](docs/configuration-guide.md#low-resource-setups). --&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;strong&gt;üîß Build from Source (Recommended for development)&lt;/strong&gt; &lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/yichuan-w/LEANN.git leann
cd leann
git submodule update --init --recursive
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;macOS:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Note: DiskANN requires MacOS 13.3 or later.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;brew install libomp boost protobuf zeromq pkgconf
uv sync --extra diskann
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Linux (Ubuntu/Debian):&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Note: On Ubuntu 20.04, you may need to build a newer Abseil and pin Protobuf (e.g., v3.20.x) for building DiskANN. See &lt;a href="https://github.com/yichuan-w/LEANN/issues/30"&gt;Issue #30&lt;/a&gt; for a step-by-step note.&lt;/p&gt; 
 &lt;p&gt;You can manually install &lt;a href="https://www.intel.com/content/www/us/en/developer/tools/oneapi/onemkl.html"&gt;Intel oneAPI MKL&lt;/a&gt; instead of &lt;code&gt;libmkl-full-dev&lt;/code&gt; for DiskANN. You can also use &lt;code&gt;libopenblas-dev&lt;/code&gt; for building HNSW only, by removing &lt;code&gt;--extra diskann&lt;/code&gt; in the command below.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt-get update &amp;amp;&amp;amp; sudo apt-get install -y \
  libomp-dev libboost-all-dev protobuf-compiler libzmq3-dev \
  pkg-config libabsl-dev libaio-dev libprotobuf-dev \
  libmkl-full-dev

uv sync --extra diskann
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Linux (Arch Linux):&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;sudo pacman -Syu &amp;amp;&amp;amp; sudo pacman -S --needed base-devel cmake pkgconf git gcc \
  boost boost-libs protobuf abseil-cpp libaio zeromq

# For MKL in DiskANN
sudo pacman -S --needed base-devel git
git clone https://aur.archlinux.org/paru-bin.git
cd paru-bin &amp;amp;&amp;amp; makepkg -si
paru -S intel-oneapi-mkl intel-oneapi-compiler
source /opt/intel/oneapi/setvars.sh

uv sync --extra diskann
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Linux (RHEL / CentOS Stream / Oracle / Rocky / AlmaLinux):&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;See &lt;a href="https://github.com/yichuan-w/LEANN/issues/50"&gt;Issue #50&lt;/a&gt; for more details.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;sudo dnf groupinstall -y "Development Tools"
sudo dnf install -y libomp-devel boost-devel protobuf-compiler protobuf-devel \
  abseil-cpp-devel libaio-devel zeromq-devel pkgconf-pkg-config

# For MKL in DiskANN
sudo dnf install -y intel-oneapi-mkl intel-oneapi-mkl-devel \
  intel-oneapi-openmp || sudo dnf install -y intel-oneapi-compiler
source /opt/intel/oneapi/setvars.sh

uv sync --extra diskann
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;Our declarative API makes RAG as easy as writing a config file.&lt;/p&gt; 
&lt;p&gt;Check out &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/demo.ipynb"&gt;demo.ipynb&lt;/a&gt; or &lt;a href="https://colab.research.google.com/github/yichuan-w/LEANN/blob/main/demo.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from leann import LeannBuilder, LeannSearcher, LeannChat
from pathlib import Path
INDEX_PATH = str(Path("./").resolve() / "demo.leann")

# Build an index
builder = LeannBuilder(backend_name="hnsw")
builder.add_text("LEANN saves 97% storage compared to traditional vector databases.")
builder.add_text("Tung Tung Tung Sahur called‚Äîthey need their banana‚Äëcrocodile hybrid back")
builder.build_index(INDEX_PATH)

# Search
searcher = LeannSearcher(INDEX_PATH)
results = searcher.search("fantastical AI-generated creatures", top_k=1)

# Chat with your data
chat = LeannChat(INDEX_PATH, llm_config={"type": "hf", "model": "Qwen/Qwen3-0.6B"})
response = chat.ask("How much storage does LEANN save?", top_k=1)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Performance Optimization: Task-Specific Prompt Templates&lt;/h2&gt; 
&lt;p&gt;LEANN now supports prompt templates for task-specific embedding models like Google's EmbeddingGemma. This feature enables &lt;strong&gt;significant performance gains&lt;/strong&gt; by using smaller, faster models without sacrificing search quality.&lt;/p&gt; 
&lt;h3&gt;Real-World Performance&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Benchmark (MacBook M1 Pro, LM Studio):&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;EmbeddingGemma 300M (QAT)&lt;/strong&gt; with templates: &lt;strong&gt;4-5x faster&lt;/strong&gt; than Qwen 600M&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Search quality:&lt;/strong&gt; Identical ranking to larger models&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Use case:&lt;/strong&gt; Ideal for real-time workflows (e.g., pre-commit hooks in Claude Code; ~7min for whole LEANN's code + doc files on MacBook M1 Pro)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Quick Example&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Build index with task-specific templates
leann build my-index ./docs \
  --embedding-mode ollama \
  --embedding-model embeddinggemma \
  --embedding-prompt-template "title: none | text: " \
  --query-prompt-template "task: search result | query: "

# Search automatically applies query template
leann search my-index "How does LEANN optimize vector search?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Templates are automatically persisted and applied during searches (CLI, MCP, API). No manual configuration needed after indexing.&lt;/p&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/docs/configuration-guide.md#task-specific-prompt-templates"&gt;Configuration Guide&lt;/a&gt; for detailed usage and model recommendations.&lt;/p&gt; 
&lt;h2&gt;RAG on Everything!&lt;/h2&gt; 
&lt;p&gt;LEANN supports RAG on various data sources including documents (&lt;code&gt;.pdf&lt;/code&gt;, &lt;code&gt;.txt&lt;/code&gt;, &lt;code&gt;.md&lt;/code&gt;), Apple Mail, Google Search History, WeChat, ChatGPT conversations, Claude conversations, iMessage conversations, and &lt;strong&gt;live data from any platform through MCP (Model Context Protocol) servers&lt;/strong&gt; - including Slack, Twitter, and more.&lt;/p&gt; 
&lt;h3&gt;Generation Model Setup&lt;/h3&gt; 
&lt;h4&gt;LLM Backend&lt;/h4&gt; 
&lt;p&gt;LEANN supports many LLM providers for text generation (HuggingFace, Ollama, Anthropic, and Any OpenAI compatible API).&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üîë OpenAI API Setup (Default)&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Set your OpenAI API key as an environment variable:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;export OPENAI_API_KEY="your-api-key-here"
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Make sure to use &lt;code&gt;--llm openai&lt;/code&gt; flag when using the CLI. You can also specify the model name with &lt;code&gt;--llm-model &amp;lt;model-name&amp;gt;&lt;/code&gt; flag.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üõ†Ô∏è Supported LLM &amp;amp; Embedding Providers (via OpenAI Compatibility)&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Thanks to the widespread adoption of the OpenAI API format, LEANN is compatible out-of-the-box with a vast array of LLM and embedding providers. Simply set the &lt;code&gt;OPENAI_BASE_URL&lt;/code&gt; and &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; environment variables to connect to your preferred service.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-sh"&gt;export OPENAI_API_KEY="xxx"
export OPENAI_BASE_URL="http://localhost:1234/v1" # base url of the provider
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;To use OpenAI compatible endpoint with the CLI interface:&lt;/p&gt; 
 &lt;p&gt;If you are using it for text generation, make sure to use &lt;code&gt;--llm openai&lt;/code&gt; flag and specify the model name with &lt;code&gt;--llm-model &amp;lt;model-name&amp;gt;&lt;/code&gt; flag.&lt;/p&gt; 
 &lt;p&gt;If you are using it for embedding, set the &lt;code&gt;--embedding-mode openai&lt;/code&gt; flag and specify the model name with &lt;code&gt;--embedding-model &amp;lt;MODEL&amp;gt;&lt;/code&gt;.&lt;/p&gt; 
 &lt;hr /&gt; 
 &lt;p&gt;Below is a list of base URLs for common providers to get you started.&lt;/p&gt; 
 &lt;h3&gt;üñ•Ô∏è Local Inference Engines (Recommended for full privacy)&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Provider&lt;/th&gt; 
    &lt;th&gt;Sample Base URL&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Ollama&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;http://localhost:11434/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;LM Studio&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;http://localhost:1234/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;vLLM&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;http://localhost:8000/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;llama.cpp&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;http://localhost:8080/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;SGLang&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;http://localhost:30000/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;LiteLLM&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;http://localhost:4000&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;hr /&gt; 
 &lt;h3&gt;‚òÅÔ∏è Cloud Providers&lt;/h3&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;strong&gt;üö® A Note on Privacy:&lt;/strong&gt; Before choosing a cloud provider, carefully review their privacy and data retention policies. Depending on their terms, your data may be used for their own purposes, including but not limited to human reviews and model training, which can lead to serious consequences if not handled properly.&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Provider&lt;/th&gt; 
    &lt;th&gt;Base URL&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;OpenAI&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;https://api.openai.com/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;OpenRouter&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;https://openrouter.ai/api/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Gemini&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;https://generativelanguage.googleapis.com/v1beta/openai/&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;x.AI (Grok)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;https://api.x.ai/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Groq AI&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;https://api.groq.com/openai/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;DeepSeek&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;https://api.deepseek.com/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;SiliconFlow&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;https://api.siliconflow.cn/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Zhipu (BigModel)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;https://open.bigmodel.cn/api/paas/v4/&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Mistral AI&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;https://api.mistral.ai/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Anthropic&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;https://api.anthropic.com/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Jina AI&lt;/strong&gt; (Embeddings)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;https://api.jina.ai/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;strong&gt;üí° Tip: Separate Embedding Provider&lt;/strong&gt;&lt;/p&gt; 
  &lt;p&gt;To use a different provider for embeddings (e.g., Jina AI) while using another for LLM, use &lt;code&gt;--embedding-api-base&lt;/code&gt; and &lt;code&gt;--embedding-api-key&lt;/code&gt;:&lt;/p&gt; 
  &lt;pre&gt;&lt;code class="language-bash"&gt;leann build my-index --docs ./docs \
  --embedding-mode openai \
  --embedding-model jina-embeddings-v3 \
  --embedding-api-base https://api.jina.ai/v1 \
  --embedding-api-key $JINA_API_KEY
&lt;/code&gt;&lt;/pre&gt; 
 &lt;/blockquote&gt; 
 &lt;p&gt;If your provider isn't on this list, don't worry! Check their documentation for an OpenAI-compatible endpoint‚Äîchances are, it's OpenAI Compatible too!&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üîß Ollama Setup (Recommended for full privacy)&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;macOS:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;First, &lt;a href="https://ollama.com/download/mac"&gt;download Ollama for macOS&lt;/a&gt;.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Pull a lightweight model (recommended for consumer hardware)
ollama pull llama3.2:1b
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Linux:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Install Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Start Ollama service manually
ollama serve &amp;amp;

# Pull a lightweight model (recommended for consumer hardware)
ollama pull llama3.2:1b
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;‚≠ê Flexible Configuration&lt;/h2&gt; 
&lt;p&gt;LEANN provides flexible parameters for embedding models, search strategies, and data processing to fit your specific needs.&lt;/p&gt; 
&lt;p&gt;üìö &lt;strong&gt;Need configuration best practices?&lt;/strong&gt; Check our &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/docs/configuration-guide.md"&gt;Configuration Guide&lt;/a&gt; for detailed optimization tips, model selection advice, and solutions to common issues like slow embeddings or poor search quality.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: Common Parameters (Available in All Examples)&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;All RAG examples share these common parameters. &lt;strong&gt;Interactive mode&lt;/strong&gt; is available in all examples - simply run without &lt;code&gt;--query&lt;/code&gt; to start a continuous Q&amp;amp;A session where you can ask multiple questions. Type 'quit' to exit.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Environment Variables (GPU Device Selection)
LEANN_EMBEDDING_DEVICE       # GPU for embedding model (e.g., cuda:0, cuda:1, cpu)
LEANN_LLM_DEVICE             # GPU for HFChat LLM (e.g., cuda:1, or "cuda" for multi-GPU auto)

# Core Parameters (General preprocessing for all examples)
--index-dir DIR              # Directory to store the index (default: current directory)
--query "YOUR QUESTION"      # Single query mode. Omit for interactive chat (type 'quit' to exit), and now you can play with your index interactively
--max-items N                # Limit data preprocessing (default: -1, process all data)
--force-rebuild              # Force rebuild index even if it exists

# Embedding Parameters
--embedding-model MODEL      # e.g., facebook/contriever, text-embedding-3-small, mlx-community/Qwen3-Embedding-0.6B-8bit or nomic-embed-text
--embedding-mode MODE        # sentence-transformers, openai, mlx, or ollama

# LLM Parameters (Text generation models)
--llm TYPE                   # LLM backend: openai, ollama, hf, or anthropic (default: openai)
--llm-model MODEL            # Model name (default: gpt-4o) e.g., gpt-4o-mini, llama3.2:1b, Qwen/Qwen2.5-1.5B-Instruct
--thinking-budget LEVEL      # Thinking budget for reasoning models: low/medium/high (supported by o3, o3-mini, GPT-Oss:20b, and other reasoning models)

# Search Parameters
--top-k N                    # Number of results to retrieve (default: 20)
--search-complexity N        # Search complexity for graph traversal (default: 32)

# Chunking Parameters
--chunk-size N               # Size of text chunks (default varies by source: 256 for most, 192 for WeChat)
--chunk-overlap N            # Overlap between chunks (default varies: 25-128 depending on source)

# Index Building Parameters
--backend-name NAME          # Backend to use: hnsw or diskann (default: hnsw)
--graph-degree N             # Graph degree for index construction (default: 32)
--build-complexity N         # Build complexity for index construction (default: 64)
--compact / --no-compact     # Use compact storage (default: true). Must be `no-compact` for `no-recompute` build.
--recompute / --no-recompute # Enable/disable embedding recomputation (default: enabled). Should not do a `no-recompute` search in a `recompute` build.
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;üìÑ Personal Data Manager: Process Any Documents (&lt;code&gt;.pdf&lt;/code&gt;, &lt;code&gt;.txt&lt;/code&gt;, &lt;code&gt;.md&lt;/code&gt;)!&lt;/h3&gt; 
&lt;p&gt;Ask questions directly about your personal PDFs, documents, and any directory containing your files!&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/yichuan-w/LEANN/main/videos/paper_clear.gif" alt="LEANN Document Search Demo" width="600" /&gt; &lt;/p&gt; 
&lt;p&gt;The example below asks a question about summarizing our paper (uses default data in &lt;code&gt;data/&lt;/code&gt;, which is a directory with diverse data sources: two papers, Pride and Prejudice, and a Technical report about LLM in Huawei in Chinese), and this is the &lt;strong&gt;easiest example&lt;/strong&gt; to run here:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;source .venv/bin/activate # Don't forget to activate the virtual environment
python -m apps.document_rag --query "What are the main techniques LEANN explores?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: Document-Specific Arguments&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h4&gt;Parameters&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;--data-dir DIR           # Directory containing documents to process (default: data)
--file-types .ext .ext   # Filter by specific file types (optional - all LlamaIndex supported types if omitted)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Example Commands&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Process all documents with larger chunks for academic papers
python -m apps.document_rag --data-dir "~/Documents/Papers" --chunk-size 1024

# Filter only markdown and Python files with smaller chunks
python -m apps.document_rag --data-dir "./docs" --chunk-size 256 --file-types .md .py

# Enable AST-aware chunking for code files
python -m apps.document_rag --enable-code-chunking --data-dir "./my_project"

# Or use the specialized code RAG for better code understanding
python -m apps.code_rag --repo-dir "./my_codebase" --query "How does authentication work?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;üé® ColQwen: Multimodal PDF Retrieval with Vision-Language Models&lt;/h3&gt; 
&lt;p&gt;Search through PDFs using both text and visual understanding with ColQwen2/ColPali models. Perfect for research papers, technical documents, and any PDFs with complex layouts, figures, or diagrams.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;üçé Mac Users&lt;/strong&gt;: ColQwen is optimized for Apple Silicon with MPS acceleration for faster inference!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Build index from PDFs
python -m apps.colqwen_rag build --pdfs ./my_papers/ --index research_papers

# Search with text queries
python -m apps.colqwen_rag search research_papers "How does attention mechanism work?"

# Interactive Q&amp;amp;A
python -m apps.colqwen_rag ask research_papers --interactive
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: ColQwen Setup &amp;amp; Usage&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h4&gt;Prerequisites&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Install dependencies
uv pip install colpali_engine pdf2image pillow matplotlib qwen_vl_utils einops seaborn
brew install poppler  # macOS only, for PDF processing
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Build Index&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python -m apps.colqwen_rag build \
  --pdfs ./pdf_directory/ \
  --index my_index \
  --model colqwen2  # or colpali
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Search&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python -m apps.colqwen_rag search my_index "your question here" --top-k 5
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Models&lt;/h4&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;ColQwen2&lt;/strong&gt; (&lt;code&gt;colqwen2&lt;/code&gt;): Latest vision-language model with improved performance&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;ColPali&lt;/strong&gt; (&lt;code&gt;colpali&lt;/code&gt;): Proven multimodal retriever&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;For detailed usage, see the &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/docs/COLQWEN_GUIDE.md"&gt;ColQwen Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h3&gt;üìß Your Personal Email Secretary: RAG on Apple Mail!&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The examples below currently support macOS only. Windows support coming soon.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/yichuan-w/LEANN/main/videos/mail_clear.gif" alt="LEANN Email Search Demo" width="600" /&gt; &lt;/p&gt; 
&lt;p&gt;Before running the example below, you need to grant full disk access to your terminal/VS Code in System Preferences ‚Üí Privacy &amp;amp; Security ‚Üí Full Disk Access.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m apps.email_rag --query "What's the food I ordered by DoorDash or Uber Eats mostly?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;780K email chunks ‚Üí 78MB storage.&lt;/strong&gt; Finally, search your email like you search Google.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: Email-Specific Arguments&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h4&gt;Parameters&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;--mail-path PATH         # Path to specific mail directory (auto-detects if omitted)
--include-html          # Include HTML content in processing (useful for newsletters)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Example Commands&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Search work emails from a specific account
python -m apps.email_rag --mail-path "~/Library/Mail/V10/WORK_ACCOUNT"

# Find all receipts and order confirmations (includes HTML)
python -m apps.email_rag --query "receipt order confirmation invoice" --include-html
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: Example queries you can try&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Once the index is built, you can ask questions like:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;"Find emails from my boss about deadlines"&lt;/li&gt; 
  &lt;li&gt;"What did John say about the project timeline?"&lt;/li&gt; 
  &lt;li&gt;"Show me emails about travel expenses"&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;üîç Time Machine for the Web: RAG Your Entire Chrome Browser History!&lt;/h3&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/yichuan-w/LEANN/main/videos/google_clear.gif" alt="LEANN Browser History Search Demo" width="600" /&gt; &lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m apps.browser_rag --query "Tell me my browser history about machine learning?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;38K browser entries ‚Üí 6MB storage.&lt;/strong&gt; Your browser history becomes your personal search engine.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: Browser-Specific Arguments&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h4&gt;Parameters&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;--chrome-profile PATH    # Path to Chrome profile directory (auto-detects if omitted)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Example Commands&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Search academic research from your browsing history
python -m apps.browser_rag --query "arxiv papers machine learning transformer architecture"

# Track competitor analysis across work profile
python -m apps.browser_rag --chrome-profile "~/Library/Application Support/Google/Chrome/Work Profile" --max-items 5000
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: How to find your Chrome profile&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;The default Chrome profile path is configured for a typical macOS setup. If you need to find your specific Chrome profile:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Open Terminal&lt;/li&gt; 
  &lt;li&gt;Run: &lt;code&gt;ls ~/Library/Application\ Support/Google/Chrome/&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;Look for folders like "Default", "Profile 1", "Profile 2", etc.&lt;/li&gt; 
  &lt;li&gt;Use the full path as your &lt;code&gt;--chrome-profile&lt;/code&gt; argument&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;&lt;strong&gt;Common Chrome profile locations:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;macOS: &lt;code&gt;~/Library/Application Support/Google/Chrome/Default&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;Linux: &lt;code&gt;~/.config/google-chrome/Default&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üí¨ Click to expand: Example queries you can try&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Once the index is built, you can ask questions like:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;"What websites did I visit about machine learning?"&lt;/li&gt; 
  &lt;li&gt;"Find my search history about programming"&lt;/li&gt; 
  &lt;li&gt;"What YouTube videos did I watch recently?"&lt;/li&gt; 
  &lt;li&gt;"Show me websites I visited about travel planning"&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;üí¨ WeChat Detective: Unlock Your Golden Memories!&lt;/h3&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/yichuan-w/LEANN/main/videos/wechat_clear.gif" alt="LEANN WeChat Search Demo" width="600" /&gt; &lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m apps.wechat_rag --query "Show me all group chats about weekend plans"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;400K messages ‚Üí 64MB storage&lt;/strong&gt; Search years of chat history in any language.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üîß Click to expand: Installation Requirements&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;First, you need to install the &lt;a href="https://github.com/sunnyyoung/WeChatTweak-CLI"&gt;WeChat exporter&lt;/a&gt;,&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;brew install sunnyyoung/repo/wechattweak-cli
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;or install it manually (if you have issues with Homebrew):&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;sudo packages/wechat-exporter/wechattweak-cli install
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Troubleshooting:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Installation issues&lt;/strong&gt;: Check the &lt;a href="https://github.com/sunnyyoung/WeChatTweak-CLI/issues/41"&gt;WeChatTweak-CLI issues page&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Export errors&lt;/strong&gt;: If you encounter the error below, try restarting WeChat &lt;pre&gt;&lt;code class="language-bash"&gt;Failed to export WeChat data. Please ensure WeChat is running and WeChatTweak is installed.
Failed to find or export WeChat data. Exiting.
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: WeChat-Specific Arguments&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h4&gt;Parameters&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;--export-dir DIR         # Directory to store exported WeChat data (default: wechat_export_direct)
--force-export          # Force re-export even if data exists
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Example Commands&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Search for travel plans discussed in group chats
python -m apps.wechat_rag --query "travel plans" --max-items 10000

# Re-export and search recent chats (useful after new messages)
python -m apps.wechat_rag --force-export --query "work schedule"
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üí¨ Click to expand: Example queries you can try&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Once the index is built, you can ask questions like:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;"ÊàëÊÉ≥‰π∞È≠îÊúØÂ∏àÁ∫¶Áø∞ÈÄäÁöÑÁêÉË°£ÔºåÁªôÊàë‰∏Ä‰∫õÂØπÂ∫îËÅäÂ§©ËÆ∞ÂΩï?" (Chinese: Show me chat records about buying Magic Johnson's jersey)&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;ü§ñ ChatGPT Chat History: Your Personal AI Conversation Archive!&lt;/h3&gt; 
&lt;p&gt;Transform your ChatGPT conversations into a searchable knowledge base! Search through all your ChatGPT discussions about coding, research, brainstorming, and more.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m apps.chatgpt_rag --export-path chatgpt_export.html --query "How do I create a list in Python?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Unlock your AI conversation history.&lt;/strong&gt; Never lose track of valuable insights from your ChatGPT discussions again.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: How to Export ChatGPT Data&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;Step-by-step export process:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;strong&gt;Sign in to ChatGPT&lt;/strong&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Click your profile icon&lt;/strong&gt; in the top right corner&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Navigate to Settings&lt;/strong&gt; ‚Üí &lt;strong&gt;Data Controls&lt;/strong&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Click "Export"&lt;/strong&gt; under Export Data&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Confirm the export&lt;/strong&gt; request&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Download the ZIP file&lt;/strong&gt; from the email link (expires in 24 hours)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Extract or use directly&lt;/strong&gt; with LEANN&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;&lt;strong&gt;Supported formats:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;.html&lt;/code&gt; files from ChatGPT exports&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;.zip&lt;/code&gt; archives from ChatGPT&lt;/li&gt; 
  &lt;li&gt;Directories with multiple export files&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: ChatGPT-Specific Arguments&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h4&gt;Parameters&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;--export-path PATH           # Path to ChatGPT export file (.html/.zip) or directory (default: ./chatgpt_export)
--separate-messages         # Process each message separately instead of concatenated conversations
--chunk-size N              # Text chunk size (default: 512)
--chunk-overlap N           # Overlap between chunks (default: 128)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Example Commands&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Basic usage with HTML export
python -m apps.chatgpt_rag --export-path conversations.html

# Process ZIP archive from ChatGPT
python -m apps.chatgpt_rag --export-path chatgpt_export.zip

# Search with specific query
python -m apps.chatgpt_rag --export-path chatgpt_data.html --query "Python programming help"

# Process individual messages for fine-grained search
python -m apps.chatgpt_rag --separate-messages --export-path chatgpt_export.html

# Process directory containing multiple exports
python -m apps.chatgpt_rag --export-path ./chatgpt_exports/ --max-items 1000
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üí° Click to expand: Example queries you can try&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Once your ChatGPT conversations are indexed, you can search with queries like:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;"What did I ask ChatGPT about Python programming?"&lt;/li&gt; 
  &lt;li&gt;"Show me conversations about machine learning algorithms"&lt;/li&gt; 
  &lt;li&gt;"Find discussions about web development frameworks"&lt;/li&gt; 
  &lt;li&gt;"What coding advice did ChatGPT give me?"&lt;/li&gt; 
  &lt;li&gt;"Search for conversations about debugging techniques"&lt;/li&gt; 
  &lt;li&gt;"Find ChatGPT's recommendations for learning resources"&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;ü§ñ Claude Chat History: Your Personal AI Conversation Archive!&lt;/h3&gt; 
&lt;p&gt;Transform your Claude conversations into a searchable knowledge base! Search through all your Claude discussions about coding, research, brainstorming, and more.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m apps.claude_rag --export-path claude_export.json --query "What did I ask about Python dictionaries?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Unlock your AI conversation history.&lt;/strong&gt; Never lose track of valuable insights from your Claude discussions again.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: How to Export Claude Data&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;Step-by-step export process:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;strong&gt;Open Claude&lt;/strong&gt; in your browser&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Navigate to Settings&lt;/strong&gt; (look for gear icon or settings menu)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Find Export/Download&lt;/strong&gt; options in your account settings&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Download conversation data&lt;/strong&gt; (usually in JSON format)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Place the file&lt;/strong&gt; in your project directory&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;&lt;em&gt;Note: Claude export methods may vary depending on the interface you're using. Check Claude's help documentation for the most current export instructions.&lt;/em&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Supported formats:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;.json&lt;/code&gt; files (recommended)&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;.zip&lt;/code&gt; archives containing JSON data&lt;/li&gt; 
  &lt;li&gt;Directories with multiple export files&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: Claude-Specific Arguments&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h4&gt;Parameters&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;--export-path PATH           # Path to Claude export file (.json/.zip) or directory (default: ./claude_export)
--separate-messages         # Process each message separately instead of concatenated conversations
--chunk-size N              # Text chunk size (default: 512)
--chunk-overlap N           # Overlap between chunks (default: 128)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Example Commands&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Basic usage with JSON export
python -m apps.claude_rag --export-path my_claude_conversations.json

# Process ZIP archive from Claude
python -m apps.claude_rag --export-path claude_export.zip

# Search with specific query
python -m apps.claude_rag --export-path claude_data.json --query "machine learning advice"

# Process individual messages for fine-grained search
python -m apps.claude_rag --separate-messages --export-path claude_export.json

# Process directory containing multiple exports
python -m apps.claude_rag --export-path ./claude_exports/ --max-items 1000
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üí° Click to expand: Example queries you can try&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Once your Claude conversations are indexed, you can search with queries like:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;"What did I ask Claude about Python programming?"&lt;/li&gt; 
  &lt;li&gt;"Show me conversations about machine learning algorithms"&lt;/li&gt; 
  &lt;li&gt;"Find discussions about software architecture patterns"&lt;/li&gt; 
  &lt;li&gt;"What debugging advice did Claude give me?"&lt;/li&gt; 
  &lt;li&gt;"Search for conversations about data structures"&lt;/li&gt; 
  &lt;li&gt;"Find Claude's recommendations for learning resources"&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;üí¨ iMessage History: Your Personal Conversation Archive!&lt;/h3&gt; 
&lt;p&gt;Transform your iMessage conversations into a searchable knowledge base! Search through all your text messages, group chats, and conversations with friends, family, and colleagues.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m apps.imessage_rag --query "What did we discuss about the weekend plans?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Unlock your message history.&lt;/strong&gt; Never lose track of important conversations, shared links, or memorable moments from your iMessage history.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: How to Access iMessage Data&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;iMessage data location:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;iMessage conversations are stored in a SQLite database on your Mac at:&lt;/p&gt; 
 &lt;pre&gt;&lt;code&gt;~/Library/Messages/chat.db
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Important setup requirements:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Grant Full Disk Access&lt;/strong&gt; to your terminal or IDE:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Open &lt;strong&gt;System Preferences&lt;/strong&gt; ‚Üí &lt;strong&gt;Security &amp;amp; Privacy&lt;/strong&gt; ‚Üí &lt;strong&gt;Privacy&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;Select &lt;strong&gt;Full Disk Access&lt;/strong&gt; from the left sidebar&lt;/li&gt; 
    &lt;li&gt;Click the &lt;strong&gt;+&lt;/strong&gt; button and add your terminal app (Terminal, iTerm2) or IDE (VS Code, etc.)&lt;/li&gt; 
    &lt;li&gt;Restart your terminal/IDE after granting access&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Alternative: Use a backup database&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;If you have Time Machine backups or manual copies of the database&lt;/li&gt; 
    &lt;li&gt;Use &lt;code&gt;--db-path&lt;/code&gt; to specify a custom location&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;&lt;strong&gt;Supported formats:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Direct access to &lt;code&gt;~/Library/Messages/chat.db&lt;/code&gt; (default)&lt;/li&gt; 
  &lt;li&gt;Custom database path with &lt;code&gt;--db-path&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;Works with backup copies of the database&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: iMessage-Specific Arguments&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h4&gt;Parameters&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;--db-path PATH                    # Path to chat.db file (default: ~/Library/Messages/chat.db)
--concatenate-conversations       # Group messages by conversation (default: True)
--no-concatenate-conversations    # Process each message individually
--chunk-size N                    # Text chunk size (default: 1000)
--chunk-overlap N                 # Overlap between chunks (default: 200)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Example Commands&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Basic usage (requires Full Disk Access)
python -m apps.imessage_rag

# Search with specific query
python -m apps.imessage_rag --query "family dinner plans"

# Use custom database path
python -m apps.imessage_rag --db-path /path/to/backup/chat.db

# Process individual messages instead of conversations
python -m apps.imessage_rag --no-concatenate-conversations

# Limit processing for testing
python -m apps.imessage_rag --max-items 100 --query "weekend"
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üí° Click to expand: Example queries you can try&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Once your iMessage conversations are indexed, you can search with queries like:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;"What did we discuss about vacation plans?"&lt;/li&gt; 
  &lt;li&gt;"Find messages about restaurant recommendations"&lt;/li&gt; 
  &lt;li&gt;"Show me conversations with John about the project"&lt;/li&gt; 
  &lt;li&gt;"Search for shared links about technology"&lt;/li&gt; 
  &lt;li&gt;"Find group chat discussions about weekend events"&lt;/li&gt; 
  &lt;li&gt;"What did mom say about the family gathering?"&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;MCP Integration: RAG on Live Data from Any Platform&lt;/h3&gt; 
&lt;p&gt;Connect to live data sources through the Model Context Protocol (MCP). LEANN now supports real-time RAG on platforms like Slack, Twitter, and more through standardized MCP servers.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Key Benefits:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Live Data Access&lt;/strong&gt;: Fetch real-time data without manual exports&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Standardized Protocol&lt;/strong&gt;: Use any MCP-compatible server&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Easy Extension&lt;/strong&gt;: Add new platforms with minimal code&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Secure Access&lt;/strong&gt;: MCP servers handle authentication&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;üí¨ Slack Messages: Search Your Team Conversations&lt;/h4&gt; 
&lt;p&gt;Transform your Slack workspace into a searchable knowledge base! Find discussions, decisions, and shared knowledge across all your channels.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Test MCP server connection
python -m apps.slack_rag --mcp-server "slack-mcp-server" --test-connection

# Index and search Slack messages
python -m apps.slack_rag \
  --mcp-server "slack-mcp-server" \
  --workspace-name "my-team" \
  --channels general dev-team random \
  --query "What did we decide about the product launch?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;üìñ Comprehensive Setup Guide&lt;/strong&gt;: For detailed setup instructions, troubleshooting common issues (like "users cache is not ready yet"), and advanced configuration options, see our &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/docs/slack-setup-guide.md"&gt;&lt;strong&gt;Slack Setup Guide&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Quick Setup:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install a Slack MCP server (e.g., &lt;code&gt;npm install -g slack-mcp-server&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Create a Slack App and get API credentials (see detailed guide above)&lt;/li&gt; 
 &lt;li&gt;Set environment variables: &lt;pre&gt;&lt;code class="language-bash"&gt;export SLACK_BOT_TOKEN="xoxb-your-bot-token"
export SLACK_APP_TOKEN="xapp-your-app-token"  # Optional
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Test connection with &lt;code&gt;--test-connection&lt;/code&gt; flag&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Arguments:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--mcp-server&lt;/code&gt;: Command to start the Slack MCP server&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--workspace-name&lt;/code&gt;: Slack workspace name for organization&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--channels&lt;/code&gt;: Specific channels to index (optional)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--concatenate-conversations&lt;/code&gt;: Group messages by channel (default: true)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--max-messages-per-channel&lt;/code&gt;: Limit messages per channel (default: 100)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--max-retries&lt;/code&gt;: Maximum retries for cache sync issues (default: 5)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--retry-delay&lt;/code&gt;: Initial delay between retries in seconds (default: 2.0)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;üê¶ Twitter Bookmarks: Your Personal Tweet Library&lt;/h4&gt; 
&lt;p&gt;Search through your Twitter bookmarks! Find that perfect article, thread, or insight you saved for later.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Test MCP server connection
python -m apps.twitter_rag --mcp-server "twitter-mcp-server" --test-connection

# Index and search Twitter bookmarks
python -m apps.twitter_rag \
  --mcp-server "twitter-mcp-server" \
  --max-bookmarks 1000 \
  --query "What AI articles did I bookmark about machine learning?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Setup Requirements:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install a Twitter MCP server (e.g., &lt;code&gt;npm install -g twitter-mcp-server&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Get Twitter API credentials: 
  &lt;ul&gt; 
   &lt;li&gt;Apply for a Twitter Developer Account at &lt;a href="https://developer.twitter.com"&gt;developer.twitter.com&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Create a new app in the Twitter Developer Portal&lt;/li&gt; 
   &lt;li&gt;Generate API keys and access tokens with "Read" permissions&lt;/li&gt; 
   &lt;li&gt;For bookmarks access, you may need Twitter API v2 with appropriate scopes&lt;/li&gt; 
  &lt;/ul&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;export TWITTER_API_KEY="your-api-key"
export TWITTER_API_SECRET="your-api-secret"
export TWITTER_ACCESS_TOKEN="your-access-token"
export TWITTER_ACCESS_TOKEN_SECRET="your-access-token-secret"
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Test connection with &lt;code&gt;--test-connection&lt;/code&gt; flag&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Arguments:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--mcp-server&lt;/code&gt;: Command to start the Twitter MCP server&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--username&lt;/code&gt;: Filter bookmarks by username (optional)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--max-bookmarks&lt;/code&gt;: Maximum bookmarks to fetch (default: 1000)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--no-tweet-content&lt;/code&gt;: Exclude tweet content, only metadata&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--no-metadata&lt;/code&gt;: Exclude engagement metadata&lt;/li&gt; 
&lt;/ul&gt;  
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üí° Click to expand: Example queries you can try&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;Slack Queries:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;"What did the team discuss about the project deadline?"&lt;/li&gt; 
  &lt;li&gt;"Find messages about the new feature launch"&lt;/li&gt; 
  &lt;li&gt;"Show me conversations about budget planning"&lt;/li&gt; 
  &lt;li&gt;"What decisions were made in the dev-team channel?"&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;Twitter Queries:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;"What AI articles did I bookmark last month?"&lt;/li&gt; 
  &lt;li&gt;"Find tweets about machine learning techniques"&lt;/li&gt; 
  &lt;li&gt;"Show me bookmarked threads about startup advice"&lt;/li&gt; 
  &lt;li&gt;"What Python tutorials did I save?"&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;summary&gt;&lt;strong&gt;üîß Using MCP with CLI Commands&lt;/strong&gt;&lt;/summary&gt; 
&lt;p&gt;&lt;strong&gt;Want to use MCP data with regular LEANN CLI?&lt;/strong&gt; You can combine MCP apps with CLI commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Step 1: Use MCP app to fetch and index data
python -m apps.slack_rag --mcp-server "slack-mcp-server" --workspace-name "my-team"

# Step 2: The data is now indexed and available via CLI
leann search slack_messages "project deadline"
leann ask slack_messages "What decisions were made about the product launch?"

# Same for Twitter bookmarks
python -m apps.twitter_rag --mcp-server "twitter-mcp-server"
leann search twitter_bookmarks "machine learning articles"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;MCP vs Manual Export:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;MCP&lt;/strong&gt;: Live data, automatic updates, requires server setup&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Manual Export&lt;/strong&gt;: One-time setup, works offline, requires manual data export&lt;/li&gt; 
&lt;/ul&gt;  
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üîß Adding New MCP Platforms&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Want to add support for other platforms? LEANN's MCP integration is designed for easy extension:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;strong&gt;Find or create an MCP server&lt;/strong&gt; for your platform&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Create a reader class&lt;/strong&gt; following the pattern in &lt;code&gt;apps/slack_data/slack_mcp_reader.py&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Create a RAG application&lt;/strong&gt; following the pattern in &lt;code&gt;apps/slack_rag.py&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Test and contribute&lt;/strong&gt; back to the community!&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;&lt;strong&gt;Popular MCP servers to explore:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;GitHub repositories and issues&lt;/li&gt; 
  &lt;li&gt;Discord messages&lt;/li&gt; 
  &lt;li&gt;Notion pages&lt;/li&gt; 
  &lt;li&gt;Google Drive documents&lt;/li&gt; 
  &lt;li&gt;And many more in the MCP ecosystem!&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;üöÄ Claude Code Integration: Transform Your Development Workflow!&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;AST‚ÄëAware Code Chunking&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;LEANN features intelligent code chunking that preserves semantic boundaries (functions, classes, methods) for Python, Java, C#, and TypeScript, improving code understanding compared to text-based chunking.&lt;/p&gt; 
 &lt;p&gt;üìñ Read the &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/docs/ast_chunking_guide.md"&gt;AST Chunking Guide ‚Üí&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;p&gt;&lt;strong&gt;The future of code assistance is here.&lt;/strong&gt; Transform your development workflow with LEANN's native MCP integration for Claude Code. Index your entire codebase and get intelligent code assistance directly in your IDE.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Key features:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üîç &lt;strong&gt;Semantic code search&lt;/strong&gt; across your entire project, fully local index and lightweight&lt;/li&gt; 
 &lt;li&gt;üß† &lt;strong&gt;AST-aware chunking&lt;/strong&gt; preserves code structure (functions, classes)&lt;/li&gt; 
 &lt;li&gt;üìö &lt;strong&gt;Context-aware assistance&lt;/strong&gt; for debugging and development&lt;/li&gt; 
 &lt;li&gt;üöÄ &lt;strong&gt;Zero-config setup&lt;/strong&gt; with automatic language detection&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install LEANN globally for MCP integration
uv tool install leann-core --with leann
claude mcp add --scope user leann-server -- leann_mcp
# Setup is automatic - just start using Claude Code!
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Try our fully agentic pipeline with auto query rewriting, semantic search planning, and more:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/yichuan-w/LEANN/main/assets/mcp_leann.png" alt="LEANN MCP Integration" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;üî• Ready to supercharge your coding?&lt;/strong&gt; &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/packages/leann-mcp/README.md"&gt;Complete Setup Guide ‚Üí&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Command Line Interface&lt;/h2&gt; 
&lt;p&gt;LEANN includes a powerful CLI for document processing and search. Perfect for quick document indexing and interactive chat.&lt;/p&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;p&gt;If you followed the Quick Start, &lt;code&gt;leann&lt;/code&gt; is already installed in your virtual environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;source .venv/bin/activate
leann --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;To make it globally available:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install the LEANN CLI globally using uv tool
uv tool install leann-core --with leann


# Now you can use leann from anywhere without activating venv
leann --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Global installation is required for Claude Code integration. The &lt;code&gt;leann_mcp&lt;/code&gt; server depends on the globally available &lt;code&gt;leann&lt;/code&gt; command.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Usage Examples&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# build from a specific directory, and my_docs is the index name(Here you can also build from multiple dict or multiple files)
leann build my-docs --docs ./your_documents

# Search your documents
leann search my-docs "machine learning concepts"

# Interactive chat with your documents
leann ask my-docs --interactive

# Ask a single question (non-interactive)
leann ask my-docs "Where are prompts configured?"

# List all your indexes
leann list

# Remove an index
leann remove my-docs
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Key CLI features:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Auto-detects document formats (PDF, TXT, MD, DOCX, PPTX + code files)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üß† AST-aware chunking&lt;/strong&gt; for Python, Java, C#, TypeScript files&lt;/li&gt; 
 &lt;li&gt;Smart text chunking with overlap for all other content&lt;/li&gt; 
 &lt;li&gt;Multiple LLM providers (Ollama, OpenAI, HuggingFace)&lt;/li&gt; 
 &lt;li&gt;Organized index storage in &lt;code&gt;.leann/indexes/&lt;/code&gt; (project-local)&lt;/li&gt; 
 &lt;li&gt;Support for advanced search parameters&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: Complete CLI Reference&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;You can use &lt;code&gt;leann --help&lt;/code&gt;, or &lt;code&gt;leann build --help&lt;/code&gt;, &lt;code&gt;leann search --help&lt;/code&gt;, &lt;code&gt;leann ask --help&lt;/code&gt;, &lt;code&gt;leann list --help&lt;/code&gt;, &lt;code&gt;leann remove --help&lt;/code&gt; to get the complete CLI reference.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Build Command:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;leann build INDEX_NAME --docs DIRECTORY|FILE [DIRECTORY|FILE ...] [OPTIONS]

Options:
  --backend {hnsw,diskann}     Backend to use (default: hnsw)
  --embedding-model MODEL      Embedding model (default: facebook/contriever)
  --graph-degree N             Graph degree (default: 32)
  --complexity N               Build complexity (default: 64)
  --force                      Force rebuild existing index
  --compact / --no-compact     Use compact storage (default: true). Must be `no-compact` for `no-recompute` build.
  --recompute / --no-recompute Enable recomputation (default: true)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Search Command:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;leann search INDEX_NAME QUERY [OPTIONS]

Options:
  --top-k N                     Number of results (default: 5)
  --complexity N                Search complexity (default: 64)
  --recompute / --no-recompute  Enable/disable embedding recomputation (default: enabled). Should not do a `no-recompute` search in a `recompute` build.
  --pruning-strategy {global,local,proportional}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Ask Command:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;leann ask INDEX_NAME [OPTIONS]

Options:
  --llm {ollama,openai,hf,anthropic}    LLM provider (default: ollama)
  --model MODEL                         Model name (default: qwen3:8b)
  --interactive                         Interactive chat mode
  --top-k N                             Retrieval count (default: 20)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;List Command:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;leann list

# Lists all indexes across all projects with status indicators:
# ‚úÖ - Index is complete and ready to use
# ‚ùå - Index is incomplete or corrupted
# üìÅ - CLI-created index (in .leann/indexes/)
# üìÑ - App-created index (*.leann.meta.json files)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Remove Command:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;leann remove INDEX_NAME [OPTIONS]

Options:
  --force, -f    Force removal without confirmation

# Smart removal: automatically finds and safely removes indexes
# - Shows all matching indexes across projects
# - Requires confirmation for cross-project removal
# - Interactive selection when multiple matches found
# - Supports both CLI and app-created indexes
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;üöÄ Advanced Features&lt;/h2&gt; 
&lt;h3&gt;üéØ Metadata Filtering&lt;/h3&gt; 
&lt;p&gt;LEANN supports a simple metadata filtering system to enable sophisticated use cases like document filtering by date/type, code search by file extension, and content management based on custom criteria.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Add metadata during indexing
builder.add_text(
    "def authenticate_user(token): ...",
    metadata={"file_extension": ".py", "lines_of_code": 25}
)

# Search with filters
results = searcher.search(
    query="authentication function",
    metadata_filters={
        "file_extension": {"==": ".py"},
        "lines_of_code": {"&amp;lt;": 100}
    }
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Supported operators&lt;/strong&gt;: &lt;code&gt;==&lt;/code&gt;, &lt;code&gt;!=&lt;/code&gt;, &lt;code&gt;&amp;lt;&lt;/code&gt;, &lt;code&gt;&amp;lt;=&lt;/code&gt;, &lt;code&gt;&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;gt;=&lt;/code&gt;, &lt;code&gt;in&lt;/code&gt;, &lt;code&gt;not_in&lt;/code&gt;, &lt;code&gt;contains&lt;/code&gt;, &lt;code&gt;starts_with&lt;/code&gt;, &lt;code&gt;ends_with&lt;/code&gt;, &lt;code&gt;is_true&lt;/code&gt;, &lt;code&gt;is_false&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;üìñ &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/docs/metadata_filtering.md"&gt;Complete Metadata filtering guide ‚Üí&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h3&gt;üîç Grep Search&lt;/h3&gt; 
&lt;p&gt;For exact text matching instead of semantic search, use the &lt;code&gt;use_grep&lt;/code&gt; parameter:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Exact text search
results = searcher.search("banana‚Äëcrocodile", use_grep=True, top_k=1)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Use cases&lt;/strong&gt;: Finding specific code patterns, error messages, function names, or exact phrases where semantic similarity isn't needed.&lt;/p&gt; 
&lt;p&gt;üìñ &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/docs/grep_search.md"&gt;Complete grep search guide ‚Üí&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;üèóÔ∏è Architecture &amp;amp; How It Works&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/yichuan-w/LEANN/main/assets/arch.png" alt="LEANN Architecture" width="800" /&gt; &lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;The magic:&lt;/strong&gt; Most vector DBs store every single embedding (expensive). LEANN stores a pruned graph structure (cheap) and recomputes embeddings only when needed (fast).&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Core techniques:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Graph-based selective recomputation:&lt;/strong&gt; Only compute embeddings for nodes in the search path&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;High-degree preserving pruning:&lt;/strong&gt; Keep important "hub" nodes while removing redundant connections&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Dynamic batching:&lt;/strong&gt; Efficiently batch embedding computations for GPU utilization&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Two-level search:&lt;/strong&gt; Smart graph traversal that prioritizes promising nodes&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Backends:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;HNSW&lt;/strong&gt; (default): Ideal for most datasets with maximum storage savings through full recomputation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;DiskANN&lt;/strong&gt;: Advanced option with superior search performance, using PQ-based graph traversal with real-time reranking for the best speed-accuracy trade-off&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Benchmarks&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/benchmarks/diskann_vs_hnsw_speed_comparison.py"&gt;DiskANN vs HNSW Performance Comparison ‚Üí&lt;/a&gt;&lt;/strong&gt; - Compare search performance between both backends&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/benchmarks/compare_faiss_vs_leann.py"&gt;Simple Example: Compare LEANN vs FAISS ‚Üí&lt;/a&gt;&lt;/strong&gt; - See storage savings in action&lt;/p&gt; 
&lt;h3&gt;üìä Storage Comparison&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;System&lt;/th&gt; 
   &lt;th&gt;DPR (2.1M)&lt;/th&gt; 
   &lt;th&gt;Wiki (60M)&lt;/th&gt; 
   &lt;th&gt;Chat (400K)&lt;/th&gt; 
   &lt;th&gt;Email (780K)&lt;/th&gt; 
   &lt;th&gt;Browser (38K)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Traditional vector database (e.g., FAISS)&lt;/td&gt; 
   &lt;td&gt;3.8 GB&lt;/td&gt; 
   &lt;td&gt;201 GB&lt;/td&gt; 
   &lt;td&gt;1.8 GB&lt;/td&gt; 
   &lt;td&gt;2.4 GB&lt;/td&gt; 
   &lt;td&gt;130 MB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;LEANN&lt;/td&gt; 
   &lt;td&gt;324 MB&lt;/td&gt; 
   &lt;td&gt;6 GB&lt;/td&gt; 
   &lt;td&gt;64 MB&lt;/td&gt; 
   &lt;td&gt;79 MB&lt;/td&gt; 
   &lt;td&gt;6.4 MB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Savings&lt;/td&gt; 
   &lt;td&gt;91%&lt;/td&gt; 
   &lt;td&gt;97%&lt;/td&gt; 
   &lt;td&gt;97%&lt;/td&gt; 
   &lt;td&gt;97%&lt;/td&gt; 
   &lt;td&gt;95%&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Reproduce Our Results&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run benchmarks/run_evaluation.py    # Will auto-download evaluation data and run benchmarks
uv run benchmarks/run_evaluation.py benchmarks/data/indices/rpj_wiki/rpj_wiki --num-queries 2000    # After downloading data, you can run the benchmark with our biggest index
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The evaluation script downloads data automatically on first run. The last three results were tested with partial personal data, and you can reproduce them with your own data!&lt;/p&gt; 
&lt;h2&gt;üî¨ Paper&lt;/h2&gt; 
&lt;p&gt;If you find Leann useful, please cite:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://arxiv.org/abs/2506.08276"&gt;LEANN: A Low-Storage Vector Index&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{wang2025leannlowstoragevectorindex,
      title={LEANN: A Low-Storage Vector Index},
      author={Yichuan Wang and Shu Liu and Zhifei Li and Yongji Wu and Ziming Mao and Yilong Zhao and Xiao Yan and Zhiying Xu and Yang Zhou and Ion Stoica and Sewon Min and Matei Zaharia and Joseph E. Gonzalez},
      year={2025},
      eprint={2506.08276},
      archivePrefix={arXiv},
      primaryClass={cs.DB},
      url={https://arxiv.org/abs/2506.08276},
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;‚ú® &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/docs/features.md"&gt;Detailed Features ‚Üí&lt;/a&gt;&lt;/h2&gt; 
&lt;h2&gt;ü§ù &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/docs/CONTRIBUTING.md"&gt;CONTRIBUTING ‚Üí&lt;/a&gt;&lt;/h2&gt; 
&lt;h2&gt;‚ùì &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/docs/faq.md"&gt;FAQ ‚Üí&lt;/a&gt;&lt;/h2&gt; 
&lt;h2&gt;üìà &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/docs/roadmap.md"&gt;Roadmap ‚Üí&lt;/a&gt;&lt;/h2&gt; 
&lt;h2&gt;üìÑ License&lt;/h2&gt; 
&lt;p&gt;MIT License - see &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/LICENSE"&gt;LICENSE&lt;/a&gt; for details.&lt;/p&gt; 
&lt;h2&gt;üôè Acknowledgments&lt;/h2&gt; 
&lt;p&gt;Core Contributors: &lt;a href="https://yichuan-w.github.io/"&gt;Yichuan Wang&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/andylizf"&gt;Zhifei Li&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Active Contributors: &lt;a href="https://github.com/gabriel-dehan"&gt;Gabriel Dehan&lt;/a&gt;, &lt;a href="https://github.com/ASuresh0524"&gt;Aakash Suresh&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;We welcome more contributors! Feel free to open issues or submit PRs.&lt;/p&gt; 
&lt;p&gt;This work is done at &lt;a href="https://sky.cs.berkeley.edu/"&gt;&lt;strong&gt;Berkeley Sky Computing Lab&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#yichuan-w/LEANN&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=yichuan-w/LEANN&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;strong&gt;‚≠ê Star us on GitHub if Leann is useful for your research or applications!&lt;/strong&gt; &lt;/p&gt; 
&lt;p align="center"&gt; Made with ‚ù§Ô∏è by the Leann team &lt;/p&gt; 
&lt;h2&gt;ü§ñ Explore LEANN with AI&lt;/h2&gt; 
&lt;p&gt;LEANN is indexed on &lt;a href="https://deepwiki.com/yichuan-w/LEANN"&gt;DeepWiki&lt;/a&gt;, so you can ask questions to LLMs using Deep Research to explore the codebase and get help to add new features.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>3b1b/manim</title>
      <link>https://github.com/3b1b/manim</link>
      <description>&lt;p&gt;Animation engine for explanatory math videos&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://github.com/3b1b/manim"&gt; &lt;img src="https://raw.githubusercontent.com/3b1b/manim/master/logo/cropped.png" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;&lt;a href="https://pypi.org/project/manimgl/"&gt;&lt;img src="https://img.shields.io/pypi/v/manimgl?logo=pypi" alt="pypi version" /&gt;&lt;/a&gt; &lt;a href="http://choosealicense.com/licenses/mit/"&gt;&lt;img src="https://img.shields.io/badge/license-MIT-blue.svg?style=flat" alt="MIT License" /&gt;&lt;/a&gt; &lt;a href="https://www.reddit.com/r/manim/"&gt;&lt;img src="https://img.shields.io/reddit/subreddit-subscribers/manim.svg?color=ff4301&amp;amp;label=reddit&amp;amp;logo=reddit" alt="Manim Subreddit" /&gt;&lt;/a&gt; &lt;a href="https://discord.com/invite/bYCyhM9Kz2"&gt;&lt;img src="https://img.shields.io/discord/581738731934056449.svg?label=discord&amp;amp;logo=discord" alt="Manim Discord" /&gt;&lt;/a&gt; &lt;a href="https://3b1b.github.io/manim/"&gt;&lt;img src="https://github.com/3b1b/manim/workflows/docs/badge.svg?sanitize=true" alt="docs" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Manim is an engine for precise programmatic animations, designed for creating explanatory math videos.&lt;/p&gt; 
&lt;p&gt;Note, there are two versions of manim. This repository began as a personal project by the author of &lt;a href="https://www.3blue1brown.com/"&gt;3Blue1Brown&lt;/a&gt; for the purpose of animating those videos, with video-specific code available &lt;a href="https://github.com/3b1b/videos"&gt;here&lt;/a&gt;. In 2020 a group of developers forked it into what is now the &lt;a href="https://github.com/ManimCommunity/manim/"&gt;community edition&lt;/a&gt;, with a goal of being more stable, better tested, quicker to respond to community contributions, and all around friendlier to get started with. See &lt;a href="https://docs.manim.community/en/stable/faq/installation.html#different-versions"&gt;this page&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Warning] &lt;strong&gt;WARNING:&lt;/strong&gt; These instructions are for ManimGL &lt;em&gt;only&lt;/em&gt;. Trying to use these instructions to install &lt;a href="https://github.com/ManimCommunity/manim"&gt;Manim Community/manim&lt;/a&gt; or instructions there to install this version will cause problems. You should first decide which version you wish to install, then only follow the instructions for your desired version.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Note] &lt;strong&gt;Note&lt;/strong&gt;: To install manim directly through pip, please pay attention to the name of the installed package. This repository is ManimGL of 3b1b. The package name is &lt;code&gt;manimgl&lt;/code&gt; instead of &lt;code&gt;manim&lt;/code&gt; or &lt;code&gt;manimlib&lt;/code&gt;. Please use &lt;code&gt;pip install manimgl&lt;/code&gt; to install the version in this repository.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Manim runs on Python 3.7 or higher.&lt;/p&gt; 
&lt;p&gt;System requirements are &lt;a href="https://ffmpeg.org/"&gt;FFmpeg&lt;/a&gt;, &lt;a href="https://www.opengl.org/"&gt;OpenGL&lt;/a&gt; and &lt;a href="https://www.latex-project.org"&gt;LaTeX&lt;/a&gt; (optional, if you want to use LaTeX). For Linux, &lt;a href="https://pango.org"&gt;Pango&lt;/a&gt; along with its development headers are required. See instruction &lt;a href="https://github.com/ManimCommunity/ManimPango#building"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Directly&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Install manimgl
pip install manimgl

# Try it out
manimgl
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more options, take a look at the &lt;a href="https://raw.githubusercontent.com/3b1b/manim/master/#using-manim"&gt;Using manim&lt;/a&gt; sections further below.&lt;/p&gt; 
&lt;p&gt;If you want to hack on manimlib itself, clone this repository and in that directory execute:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Install manimgl
pip install -e .

# Try it out
manimgl example_scenes.py OpeningManimExample
# or
manim-render example_scenes.py OpeningManimExample
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Directly (Windows)&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://www.wikihow.com/Install-FFmpeg-on-Windows"&gt;Install FFmpeg&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Install a LaTeX distribution. &lt;a href="https://miktex.org/download"&gt;MiKTeX&lt;/a&gt; is recommended.&lt;/li&gt; 
 &lt;li&gt;Install the remaining Python packages. &lt;pre&gt;&lt;code class="language-sh"&gt;git clone https://github.com/3b1b/manim.git
cd manim
pip install -e .
manimgl example_scenes.py OpeningManimExample
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Mac OSX&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Install FFmpeg, LaTeX in terminal using homebrew.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;brew install ffmpeg mactex
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;If you are using an ARM-based processor, install Cairo.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;arch -arm64 brew install pkg-config cairo
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install latest version of manim using these command.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;git clone https://github.com/3b1b/manim.git
cd manim
pip install -e .
manimgl example_scenes.py OpeningManimExample (make sure to add manimgl to path first.)
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Anaconda Install&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install LaTeX as above.&lt;/li&gt; 
 &lt;li&gt;Create a conda environment using &lt;code&gt;conda create -n manim python=3.8&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Activate the environment using &lt;code&gt;conda activate manim&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Install manimgl using &lt;code&gt;pip install -e .&lt;/code&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Using manim&lt;/h2&gt; 
&lt;p&gt;Try running the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;manimgl example_scenes.py OpeningManimExample
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This should pop up a window playing a simple scene.&lt;/p&gt; 
&lt;p&gt;Look through the &lt;a href="https://3b1b.github.io/manim/getting_started/example_scenes.html"&gt;example scenes&lt;/a&gt; to see examples of the library's syntax, animation types and object types. In the &lt;a href="https://github.com/3b1b/videos"&gt;3b1b/videos&lt;/a&gt; repo, you can see all the code for 3blue1brown videos, though code from older videos may not be compatible with the most recent version of manim. The readme of that repo also outlines some details for how to set up a more interactive workflow, as shown in &lt;a href="https://www.youtube.com/watch?v=rbu7Zu5X1zI"&gt;this manim demo video&lt;/a&gt; for example.&lt;/p&gt; 
&lt;p&gt;When running in the CLI, some useful flags include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;-w&lt;/code&gt; to write the scene to a file&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-o&lt;/code&gt; to write the scene to a file and open the result&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-s&lt;/code&gt; to skip to the end and just show the final frame. 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;-so&lt;/code&gt; will save the final frame to an image and show it&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-n &amp;lt;number&amp;gt;&lt;/code&gt; to skip ahead to the &lt;code&gt;n&lt;/code&gt;'th animation of a scene.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-f&lt;/code&gt; to make the playback window fullscreen&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Take a look at custom_config.yml for further configuration. To add your customization, you can either edit this file, or add another file by the same name "custom_config.yml" to whatever directory you are running manim from. For example &lt;a href="https://github.com/3b1b/videos/raw/master/custom_config.yml"&gt;this is the one&lt;/a&gt; for 3blue1brown videos. There you can specify where videos should be output to, where manim should look for image files and sounds you want to read in, and other defaults regarding style and video quality.&lt;/p&gt; 
&lt;h3&gt;Documentation&lt;/h3&gt; 
&lt;p&gt;Documentation is in progress at &lt;a href="https://3b1b.github.io/manim/"&gt;3b1b.github.io/manim&lt;/a&gt;. And there is also a Chinese version maintained by &lt;a href="https://manim.org.cn"&gt;&lt;strong&gt;@manim-kindergarten&lt;/strong&gt;&lt;/a&gt;: &lt;a href="https://docs.manim.org.cn/"&gt;docs.manim.org.cn&lt;/a&gt; (in Chinese).&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/manim-kindergarten/"&gt;manim-kindergarten&lt;/a&gt; wrote and collected some useful extra classes and some codes of videos in &lt;a href="https://github.com/manim-kindergarten/manim_sandbox"&gt;manim_sandbox repo&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Is always welcome. As mentioned above, the &lt;a href="https://github.com/ManimCommunity/manim"&gt;community edition&lt;/a&gt; has the most active ecosystem for contributions, with testing and continuous integration, but pull requests are welcome here too. Please explain the motivation for a given change and examples of its effect.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project falls under the MIT license.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>QwenLM/Qwen-Image</title>
      <link>https://github.com/QwenLM/Qwen-Image</link>
      <description>&lt;p&gt;Qwen-Image is a powerful image generation foundation model capable of complex text rendering and precise image editing.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/qwen_image_logo.png" width="400" /&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p align="center"&gt;&amp;nbsp;&amp;nbsp;üíú &lt;a href="https://chat.qwen.ai/"&gt;Qwen Chat&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ü§ó &lt;a href="https://huggingface.co/Qwen/Qwen-Image"&gt;HuggingFace(T2I)&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ü§ó &lt;a href="https://huggingface.co/Qwen/Qwen-Image-Edit-2511"&gt;HuggingFace(Edit)&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ü§ñ &lt;a href="https://modelscope.cn/models/Qwen/Qwen-Image"&gt;ModelScope-T2I&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ü§ñ &lt;a href="https://modelscope.cn/models/Qwen/Qwen-Image-Edit-2511"&gt;ModelScope-Edit&lt;/a&gt;&amp;nbsp;&amp;nbsp;| &amp;nbsp;&amp;nbsp; üìë &lt;a href="https://arxiv.org/abs/2508.02324"&gt;Tech Report&lt;/a&gt; &amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; üìë &lt;a href="https://qwenlm.github.io/blog/qwen-image/"&gt;Blog(T2I)&lt;/a&gt; &amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; üìë &lt;a href="https://qwenlm.github.io/blog/qwen-image-edit-2511/"&gt;Blog(Edit)&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;br /&gt; üñ•Ô∏è &lt;a href="https://huggingface.co/spaces/Qwen/Qwen-Image"&gt;T2I Demo&lt;/a&gt;&amp;nbsp;&amp;nbsp; | üñ•Ô∏è &lt;a href="https://huggingface.co/spaces/Qwen/Qwen-Image-Edit-2511"&gt;Edit Demo&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;üí¨ &lt;a href="https://github.com/QwenLM/Qwen-Image/raw/main/assets/wechat.png"&gt;WeChat (ÂæÆ‰ø°)&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ü´® &lt;a href="https://discord.gg/CV4E9rpNSD"&gt;Discord&lt;/a&gt;&amp;nbsp;&amp;nbsp; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/merge3.jpg" width="1024" /&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;We are thrilled to release &lt;strong&gt;Qwen-Image&lt;/strong&gt;, a 20B MMDiT image foundation model that achieves significant advances in &lt;strong&gt;complex text rendering&lt;/strong&gt; and &lt;strong&gt;precise image editing&lt;/strong&gt;. Experiments show strong general capabilities in both image generation and editing, with exceptional performance in text rendering, especially for Chinese.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/bench.png#center" alt="" /&gt;&lt;/p&gt; 
&lt;h2&gt;News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;2025.12.31: We released Qwen-Image-2512 weights! Check at &lt;a href="https://huggingface.co/Qwen/Qwen-Image-2512"&gt;Huggingface&lt;/a&gt; and &lt;a href="https://modelscope.cn/models/Qwen/Qwen-Image-2512"&gt;ModelScope&lt;/a&gt;!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.12.31: We released Qwen-Image-2512! Check our &lt;a href="https://qwen.ai/blog?id=qwen-image-2512"&gt;Blog&lt;/a&gt; for more details! üöÄ Our December upgrade to Qwen-Image, just in time for the New Year.&lt;/p&gt; &lt;p&gt;‚ú® What‚Äôs new: ‚Ä¢ More realistic humans ‚Äî dramatically reduced ‚ÄúAI look,‚Äù richer facial &amp;amp; age details ‚Ä¢ Finer natural textures ‚Äî sharper landscapes, water, fur, and materials ‚Ä¢ Stronger text rendering ‚Äî better layout, higher accuracy in text‚Äìimage composition&lt;/p&gt; &lt;p&gt;üèÜ Tested in 10,000+ blind rounds on AI Arena, Qwen-Image-2512 ranks as the strongest open-source image model, while staying competitive with closed-source systems. &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/arena.png#center" alt="" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.12.31: &lt;a href="https://github.com/ModelTC/Qwen-Image-Lightning"&gt;Qwen-Image-Lightning&lt;/a&gt;, developed by &lt;a href="https://github.com/ModelTC/LightX2V"&gt;Lightx2v&lt;/a&gt;, provides &lt;a href="https://huggingface.co/lightx2v/Qwen-Image-2512-Lightning"&gt;Day 0 acceleration support for Qwen-Image-2512&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.12.31:vLLM-Omni supports high performance Qwen-Image-2512 inference from Day-0, with long sequence parallelism, cache acceleration and fast kernels, please check &lt;a href="https://github.com/vllm-project/vllm-omni/tree/main/examples/offline_inference/text_to_image"&gt;here&lt;/a&gt; for details.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.12.23: We released Qwen-Image-Edit-2511 weights! Check at &lt;a href="https://huggingface.co/Qwen/Qwen-Image-Edit-2511"&gt;Huggingface&lt;/a&gt; and &lt;a href="https://modelscope.cn/models/Qwen/Qwen-Image-Edit-2511"&gt;ModelScope&lt;/a&gt;!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.12.23: We released Qwen-Image-Edit-2511! Check our &lt;a href="https://qwen.ai/blog?id=qwen-image-edit-2511"&gt;Blog&lt;/a&gt; for more details!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.12.23: &lt;strong&gt;&lt;a href="https://github.com/ModelTC/LightX2V/"&gt;LightX2V&lt;/a&gt;&lt;/strong&gt; delivers Day 0 acceleration for Qwen-Image-Edit-2511, with native support for a wide range of hardware, including &lt;strong&gt;NVIDIA, Hygon, Metax, Ascend, and Cambricon&lt;/strong&gt;. By combining &lt;strong&gt;&lt;a href="https://github.com/ModelTC/Qwen-Image-Lightning"&gt;diffusion distillation&lt;/a&gt;&lt;/strong&gt; with cutting-edge inference optimizations, LightX2V achieves a &lt;strong&gt;25x reduction in DiT NFEs&lt;/strong&gt; and &lt;strong&gt;an order-of-magnitude 42.55x overall speedup&lt;/strong&gt;, enabling real-time image editing across diverse AI accelerators.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.12.23: &lt;strong&gt;vLLM-Omni&lt;/strong&gt; supports high performance &lt;code&gt;Qwen-Image-Edit-2511&lt;/code&gt;, &lt;code&gt;Qwen-Image-Layered&lt;/code&gt; inference from Day-0, with long sequence parallelism, cache acceleration and fast kernels, please check &lt;a href="https://github.com/vllm-project/vllm-omni/tree/main/examples/offline_inference/image_to_image"&gt;here&lt;/a&gt; for details.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.12.23: &lt;strong&gt;SGLang-Diffusion&lt;/strong&gt; provides day-0 support for Qwen-Image models. To play with &lt;code&gt;Qwen-Image-Edit-2511&lt;/code&gt; in SGlang, please check community supports section for details.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.12.19: We released Qwen-Image-Layered weights! Check at &lt;a href="https://huggingface.co/Qwen/Qwen-Image-Layered"&gt;Huggingface&lt;/a&gt; and &lt;a href="https://modelscope.cn/models/Qwen/Qwen-Image-Layered"&gt;ModelScope&lt;/a&gt;!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.12.19: We released Qwen-Image-Layered! Check our &lt;a href="https://qwenlm.github.io/blog/qwen-image-layered"&gt;Blog&lt;/a&gt; for more details!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.12.18: We released our &lt;a href="https://arxiv.org/abs/2512.15603"&gt;Research Paper&lt;/a&gt; on Arxiv!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.11.11: &lt;strong&gt;&lt;a href="https://t2i-corebench.github.io/"&gt;T2I-CoreBench&lt;/a&gt;&lt;/strong&gt; offers a comprehensive and complex evaluation of T2I models in real-world scenarios. On this benchmark, Qwen-Image achieves state-of-the-art performance under real-world complexities in both composition and reasoning T2I tasks, surpassing other open-source models and showing comparable results to closed-source ones.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.11.07: LeMiCa is a diffusion model inference acceleration solution developed by China Unicom Data Science and Artificial Intelligence Research Institute. By leveraging cache-based techniques and global denoising path optimization, LeMiCa provides efficient inference support for Qwen-Image, achieving nearly 3x lossless acceleration while maintaining visual consistency and quality. For more details, please visit the homepage: &lt;a href="https://unicomai.github.io/LeMiCa/"&gt;https://unicomai.github.io/LeMiCa/&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.09.22: This September, we are pleased to introduce Qwen-Image-Edit-2509, the monthly iteration of Qwen-Image-Edit. To experience the latest model, please visit &lt;a href="https://qwen.ai"&gt;Qwen Chat&lt;/a&gt; and select the "Image Editing" feature. Compared with Qwen-Image-Edit released in August, the main improvements of Qwen-Image-Edit-2509 include:&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.08.19: We have observed performance misalignments of Qwen-Image-Edit. To ensure optimal results, please update to the latest diffusers commit. Improvements are expected, especially in identity preservation and instruction following.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.08.18: We‚Äôre excited to announce the open-sourcing of Qwen-Image-Edit! üéâ Try it out in your local environment with the quick start guide below, or head over to &lt;a href="https://chat.qwen.ai/"&gt;Qwen Chat&lt;/a&gt; or &lt;a href="https://huggingface.co/spaces/Qwen/Qwen-Image-Edit"&gt;Huggingface Demo&lt;/a&gt; to experience the online demo right away! If you enjoy our work, please show your support by giving our repository a star. Your encouragement means a lot to us!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.08.09: Qwen-Image now supports a variety of LoRA models, such as MajicBeauty LoRA, enabling the generation of highly realistic beauty images. Check out the available weights on &lt;a href="https://modelscope.cn/models/merjic/majicbeauty-qwen1/summary"&gt;ModelScope&lt;/a&gt;. &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/magicbeauty.png#center" alt="" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.08.05: Qwen-Image is now natively supported in ComfyUI, see &lt;a href="https://blog.comfy.org/p/qwen-image-in-comfyui-new-era-of"&gt;Qwen-Image in ComfyUI: New Era of Text Generation in Images!&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.08.05: Qwen-Image is now on Qwen Chat. Click &lt;a href="https://chat.qwen.ai/"&gt;Qwen Chat&lt;/a&gt; and choose "Image Generation".&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.08.05: We released our &lt;a href="https://arxiv.org/abs/2508.02324"&gt;Technical Report&lt;/a&gt; on Arxiv!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.08.04: We released Qwen-Image weights! Check at &lt;a href="https://huggingface.co/Qwen/Qwen-Image"&gt;Huggingface&lt;/a&gt; and &lt;a href="https://modelscope.cn/models/Qwen/Qwen-Image"&gt;ModelScope&lt;/a&gt;!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.08.04: We released Qwen-Image! Check our &lt;a href="https://qwenlm.github.io/blog/qwen-image"&gt;Blog&lt;/a&gt; for more details!&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Due to heavy traffic, if you'd like to experience our demo online, we also recommend visiting DashScope, WaveSpeed, and LibLib. Please find the links below in the community support.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Make sure your transformers&amp;gt;=4.51.3 (Supporting Qwen2.5-VL)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install the latest version of diffusers&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code&gt;pip install git+https://github.com/huggingface/diffusers
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Qwen-Image-2512 (for Text to Image generation, better character realism/texture quality)&lt;/h3&gt; 
&lt;p&gt;We recommand use the latest prompt enhancing tools for Qwen-Image-2512, please check &lt;code&gt;src/examples/tools/prompt_utils_2512.py&lt;/code&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from diffusers import QwenImagePipeline
import torch
# Load the pipeline
if torch.cuda.is_available():
    torch_dtype = torch.bfloat16
    device = "cuda"
else:
    torch_dtype = torch.float32
    device = "cpu"

pipe = QwenImagePipeline.from_pretrained("Qwen/Qwen-Image-2512", torch_dtype=torch_dtype).to(device)

# Generate image
prompt = '''A 20-year-old East Asian girl with delicate, charming features and large, bright brown eyes‚Äîexpressive and lively, with a cheerful or subtly smiling expression. Her naturally wavy long hair is either loose or tied in twin ponytails. She has fair skin and light makeup accentuating her youthful freshness. She wears a modern, cute dress or relaxed outfit in bright, soft colors‚Äîlightweight fabric, minimalist cut. She stands indoors at an anime convention, surrounded by banners, posters, or stalls. Lighting is typical indoor illumination‚Äîno staged lighting‚Äîand the image resembles a casual iPhone snapshot: unpretentious composition, yet brimming with vivid, fresh, youthful charm.'''

negative_prompt = "‰ΩéÂàÜËæ®ÁéáÔºå‰ΩéÁîªË¥®ÔºåËÇ¢‰ΩìÁï∏ÂΩ¢ÔºåÊâãÊåáÁï∏ÂΩ¢ÔºåÁîªÈù¢ËøáÈ•±ÂíåÔºåËú°ÂÉèÊÑüÔºå‰∫∫ËÑ∏Êó†ÁªÜËäÇÔºåËøáÂ∫¶ÂÖâÊªëÔºåÁîªÈù¢ÂÖ∑ÊúâAIÊÑü„ÄÇÊûÑÂõæÊ∑∑‰π±„ÄÇÊñáÂ≠óÊ®°Á≥äÔºåÊâ≠Êõ≤„ÄÇ"


# Generate with different aspect ratios
aspect_ratios = {
    "1:1": (1328, 1328),
    "16:9": (1664, 928),
    "9:16": (928, 1664),
    "4:3": (1472, 1104),
    "3:4": (1104, 1472),
    "3:2": (1584, 1056),
    "2:3": (1056, 1584),
}

width, height = aspect_ratios["16:9"]

image = pipe(
    prompt=prompt,
    negative_prompt=negative_prompt,
    width=width,
    height=height,
    num_inference_steps=50,
    true_cfg_scale=4.0,
    generator=torch.Generator(device="cuda").manual_seed(42)
).images[0]

image.save("example.png")

&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Qwen-Image-Edit-2511 (for Image Editing, Multiple Image Support and Improved Consistency)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import os
import torch
from PIL import Image
from diffusers import QwenImageEditPlusPipeline
from io import BytesIO
import requests

pipeline = QwenImageEditPlusPipeline.from_pretrained("Qwen/Qwen-Image-Edit-2511", torch_dtype=torch.bfloat16)
print("pipeline loaded")

pipeline.to('cuda')
pipeline.set_progress_bar_config(disable=None)
image1 = Image.open(BytesIO(requests.get("https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen-Image/edit2511/edit2511input.png").content))
prompt = "Ëøô‰∏™Â•≥ÁîüÁúãÁùÄÈù¢ÂâçÁöÑÁîµËßÜÂ±èÂπïÔºåÂ±èÂπï‰∏äÈù¢ÂÜôÁùÄ‚ÄúÈòøÈáåÂ∑¥Â∑¥‚Äù"
inputs = {
    "image": [image1],
    "prompt": prompt,
    "generator": torch.manual_seed(0),
    "true_cfg_scale": 4.0,
    "negative_prompt": " ",
    "num_inference_steps": 40,
    "guidance_scale": 1.0,
    "num_images_per_prompt": 1,
}
with torch.inference_mode():
    output = pipeline(**inputs)
    output_image = output.images[0]
    output_image.save("output_image_edit_2511.png")
    print("image saved at", os.path.abspath("output_image_edit_2511.png"))
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt; Previous Version &lt;/summary&gt; 
 &lt;h3&gt;Qwen-Image (for Text-to-Image)&lt;/h3&gt; 
 &lt;p&gt;The following contains a code snippet illustrating how to use the model to generate images based on text prompts:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from diffusers import DiffusionPipeline
import torch

model_name = "Qwen/Qwen-Image"

# Load the pipeline
if torch.cuda.is_available():
    torch_dtype = torch.bfloat16
    device = "cuda"
else:
    torch_dtype = torch.float32
    device = "cpu"

pipe = DiffusionPipeline.from_pretrained(model_name, torch_dtype=torch_dtype).to(device)

positive_magic = {
    "en": ", Ultra HD, 4K, cinematic composition.", # for english prompt
    "zh": ", Ë∂ÖÊ∏ÖÔºå4KÔºåÁîµÂΩ±Á∫ßÊûÑÂõæ." # for chinese prompt
}

# Generate image
prompt = '''A coffee shop entrance features a chalkboard sign reading "Qwen Coffee üòä $2 per cup," with a neon light beside it displaying "ÈÄö‰πâÂçÉÈóÆ". Next to it hangs a poster showing a beautiful Chinese woman, and beneath the poster is written "œÄ‚âà3.1415926-53589793-23846264-33832795-02384197".'''

negative_prompt = " " # Recommended if you don't use a negative prompt.


# Generate with different aspect ratios
aspect_ratios = {
    "1:1": (1328, 1328),
    "16:9": (1664, 928),
    "9:16": (928, 1664),
    "4:3": (1472, 1104),
    "3:4": (1104, 1472),
    "3:2": (1584, 1056),
    "2:3": (1056, 1584),
}

width, height = aspect_ratios["16:9"]

image = pipe(
    prompt=prompt + positive_magic["en"],
    negative_prompt=negative_prompt,
    width=width,
    height=height,
    num_inference_steps=50,
    true_cfg_scale=4.0,
    generator=torch.Generator(device="cuda").manual_seed(42)
).images[0]

image.save("example.png")
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;Qwen-Image-Edit (for Image Editing, Only Support Single Image Input)&lt;/h3&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;[!NOTE] Qwen-Image-Edit-2509 has better consistency than Qwen-Image-Edit; it is recommended to use Qwen-Image-Edit-2509 directlyÔºåfor both single image input and multiple image inputs.&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import os
from PIL import Image
import torch

from diffusers import QwenImageEditPipeline

pipeline = QwenImageEditPipeline.from_pretrained("Qwen/Qwen-Image-Edit")
print("pipeline loaded")
pipeline.to(torch.bfloat16)
pipeline.to("cuda")
pipeline.set_progress_bar_config(disable=None)

image = Image.open("./input.png").convert("RGB")
prompt = "Change the rabbit's color to purple, with a flash light background."


inputs = {
    "image": image,
    "prompt": prompt,
    "generator": torch.manual_seed(0),
    "true_cfg_scale": 4.0,
    "negative_prompt": " ",
    "num_inference_steps": 50,
}

with torch.inference_mode():
    output = pipeline(**inputs)
    output_image = output.images[0]
    output_image.save("output_image_edit.png")
    print("image saved at", os.path.abspath("output_image_edit.png"))
&lt;/code&gt;&lt;/pre&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;[!NOTE] We have observed that editing results may become unstable if prompt rewriting is not used. Therefore, we strongly recommend applying prompt rewriting to improve the stability of editing tasks. For reference, please see our official &lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen-Image/main/src/examples/tools/prompt_utils.py"&gt;demo script&lt;/a&gt; or Advanced Usage below, which includes example system prompts. Qwen-Image-Edit is actively evolving with ongoing development. Stay tuned for future enhancements!&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;h3&gt;Qwen-Image-Edit-2509 (for Image Editing, Multiple Image Support and Improved Consistency)&lt;/h3&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import os
import torch
from PIL import Image
from diffusers import QwenImageEditPlusPipeline
from io import BytesIO
import requests

pipeline = QwenImageEditPlusPipeline.from_pretrained("Qwen/Qwen-Image-Edit-2509", torch_dtype=torch.bfloat16)
print("pipeline loaded")

pipeline.to('cuda')
pipeline.set_progress_bar_config(disable=None)
image1 = Image.open(BytesIO(requests.get("https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/edit2509_1.jpg").content))
image2 = Image.open(BytesIO(requests.get("https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/edit2509_2.jpg").content))
prompt = "The magician bear is on the left, the alchemist bear is on the right, facing each other in the central park square."
inputs = {
    "image": [image1, image2],
    "prompt": prompt,
    "generator": torch.manual_seed(0),
    "true_cfg_scale": 4.0,
    "negative_prompt": " ",
    "num_inference_steps": 40,
    "guidance_scale": 1.0,
    "num_images_per_prompt": 1,
}
with torch.inference_mode():
    output = pipeline(**inputs)
    output_image = output.images[0]
    output_image.save("output_image_edit_plus.png")
    print("image saved at", os.path.abspath("output_image_edit_plus.png"))
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Advanced Usage&lt;/h3&gt; 
&lt;h4&gt;Prompt Enhance for Text-to-Image&lt;/h4&gt; 
&lt;p&gt;For enhanced prompt optimization and multi-language support, we recommend using our official Prompt Enhancement Tool powered by Qwen-Plus .&lt;/p&gt; 
&lt;p&gt;You can integrate it directly into your code:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from tools.prompt_utils import rewrite
prompt = rewrite(prompt)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, run the example script from the command line:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd src
DASHSCOPE_API_KEY=sk-xxxxxxxxxxxxxxxxxxxx python examples/generate_w_prompt_enhance.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Prompt Enhance for Image Edit&lt;/h4&gt; 
&lt;p&gt;For enhanced stability, we recommend using our official Prompt Enhancement Tool powered by Qwen-VL-Max.&lt;/p&gt; 
&lt;p&gt;You can integrate it directly into your code:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from tools.prompt_utils import polish_edit_prompt
prompt = polish_edit_prompt(prompt, pil_image)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Deploy Qwen-Image&lt;/h2&gt; 
&lt;p&gt;Qwen-Image supports Multi-GPU API Server for local deployment:&lt;/p&gt; 
&lt;h3&gt;Multi-GPU API Server Pipeline &amp;amp; Usage&lt;/h3&gt; 
&lt;p&gt;The Multi-GPU API Server will start a Gradio-based web interface with:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Multi-GPU parallel processing&lt;/li&gt; 
 &lt;li&gt;Queue management for high concurrency&lt;/li&gt; 
 &lt;li&gt;Automatic prompt optimization&lt;/li&gt; 
 &lt;li&gt;Support for multiple aspect ratios&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Configuration via environment variables:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export NUM_GPUS_TO_USE=4          # Number of GPUs to use
export TASK_QUEUE_SIZE=100        # Task queue size
export TASK_TIMEOUT=300           # Task timeout in seconds
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Start the gradio demo server, api key for prompt enhance
cd src
DASHSCOPE_API_KEY=sk-xxxxxxxxxxxxxxxxx python examples/demo.py 
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Showcase&lt;/h2&gt; 
&lt;p&gt;For previous showcases, click the following links:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen-Image/main/Qwen-Image.md"&gt;Qwen-Image&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen-Image/main/Qwen-Image-Edit.md"&gt;Qwen-Image-Edit&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen-Image/main/Qwen-Image-Edit-2509.md"&gt;Qwen-Image-Edit-2509&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Showcase of Qwen-Image-2512&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Enhanced Huamn Realism&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;In Qwen-Image-2512, human depiction has been substantially refined. Compared to the August release, Qwen-Image-2512 adds significantly richer facial details and better environmental context. For example:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;A Chinese female college student, around 20 years old, with a very short haircut that conveys a gentle, artistic vibe. Her hair naturally falls to partially cover her cheeks, projecting a tomboyish yet charming demeanor. She has cool-toned fair skin and delicate features, with a slightly shy yet subtly confident expression‚Äîher mouth crooked in a playful, youthful smirk. She wears an off-shoulder top, revealing one shoulder, with a well-proportioned figure. The image is framed as a close-up selfie: she dominates the foreground, while the background clearly shows her dormitory‚Äîa neatly made bed with white linens on the top bunk, a tidy study desk with organized stationery, and wooden cabinets and drawers. The photo is captured on a smartphone under soft, even ambient lighting, with natural tones, high clarity, and a bright, lively atmosphere full of youthful, everyday energy.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/%E5%B9%BB%E7%81%AF%E7%89%871.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;For the same prompt, Qwen-Image-2512 yields notably more lifelike facial features, and background objects‚Äîe.g., the desk, stationery, and bedding‚Äîare rendered with significantly greater clarity than in Qwen-Image.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;A 20-year-old East Asian girl with delicate, charming features and large, bright brown eyes‚Äîexpressive and lively, with a cheerful or subtly smiling expression. Her naturally wavy long hair is either loose or tied in twin ponytails. She has fair skin and light makeup accentuating her youthful freshness. She wears a modern, cute dress or relaxed outfit in bright, soft colors‚Äîlightweight fabric, minimalist cut. She stands indoors at an anime convention, surrounded by banners, posters, or stalls. Lighting is typical indoor illumination‚Äîno staged lighting‚Äîand the image resembles a casual iPhone snapshot: unpretentious composition, yet brimming with vivid, fresh, youthful charm.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/%E5%B9%BB%E7%81%AF%E7%89%872.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;Here, hair strands serve as a key differentiator: Qwen-Image‚Äôs August version tends to blur them together, losing fine detail, whereas Qwen-Image-2512 renders individual strands with precision, resulting in a more natural and realistic appearance.&lt;/p&gt; 
&lt;p&gt;Another case:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;An East Asian teenage boy, aged 15‚Äì18, with soft, fluffy black short hair and refined facial contours. His large, warm brown eyes sparkle with energy. His fair skin and sunny, open smile convey an approachable, friendly demeanor‚Äîno makeup or blemishes. He wears a blue-and-white summer uniform shirt, slightly unbuttoned, made of thin breathable fabric, with black headphones hanging around his neck. His hands are in his pockets, body leaning slightly forward in a relaxed pose, as if engaged in conversation. Behind him lies a summer school playground: lush green grass and a red rubber track in the foreground, blurred school buildings in the distance, a clear blue sky with fluffy white clouds. The bright, airy lighting evokes a joyful, carefree adolescent atmosphere.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/%E5%B9%BB%E7%81%AF%E7%89%873.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;In this example, Qwen-Image-2512 better adheres to semantic instructions‚Äîfor instance, the prompt specifies ‚Äúbody leaning slightly forward,‚Äù and Qwen-Image-2512 accurately captures this posture, unlike its predecessor.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;An elderly Chinese couple in their 70s in a clean, organized home kitchen. The woman has a kind face and a warm smile, wearing a patterned apron; the man stands behind her, also smiling, as they both gaze at a steaming pot of buns on the stove. The kitchen is bright and tidy, exuding warmth and harmony. The scene is captured with a wide-angle lens to fully show the subjects and their surroundings.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/%E5%B9%BB%E7%81%AF%E7%89%874.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;This comparison starkly highlights the gap between the August and December models. The original Qwen-Image struggles to accurately render aged facial features (e.g., wrinkles), resulting in an artificial ‚ÄúAI look.‚Äù In contrast, Qwen-Image-2512 precisely captures age cues, dramatically boosting realism.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Finer Natural Detail&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Qwen-Image-2512‚Äôs enhanced detail rendering extends beyond humans‚Äîto landscapes, wildlife, and more. For instance:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;A turquoise river winds through a lush canyon. Thick moss and dense ferns blanket the rocky walls; multiple waterfalls cascade from above, enveloped in mist. At noon, sunlight filters through the dense canopy, dappling the river surface with shimmering light. The atmosphere is humid and fresh, pulsing with primal jungle vitality. No humans, text, or artificial traces present.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/%E5%B9%BB%E7%81%AF%E7%89%875.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;Side-by-side, Qwen-Image-2512 exhibits superior fidelity in water flow, foliage, and waterfall mist‚Äîand renders richer gradation in greens. Another example (wave rendering):&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;At dawn, a thin mist veils the sea. An ancient stone lighthouse stands at the cliff‚Äôs edge, its beacon faintly visible through the fog. Black rocks are pounded by waves, sending up bursts of white spray. The sky glows in soft blue-purple hues under cool, hazy light‚Äîevoking solitude and solemn grandeur.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/%E5%B9%BB%E7%81%AF%E7%89%876.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;Fur detail is another highlight‚Äîhere, a golden retriever portrait:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;An ultra-realistic close-up of a golden retriever outdoors under soft daylight. Hair is exquisitely detailed: strands distinct, color transitioning naturally from warm gold to light cream, light glinting delicately at the tips; a gentle breeze adds subtle volume. Undercoat is soft and dense; guard hairs are long and well-defined, with visible layering. Eyes are moist, expressive; nose is slightly damp with fine specular highlights. Background is softly blurred to emphasize the dog‚Äôs tangible texture and vivid expression.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/%E5%B9%BB%E7%81%AF%E7%89%877.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;Similarly, texture quality improves in depictions of rugged wildlife‚Äîfor example, a male argali sheep:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;A male argali stands atop a barren, rocky mountainside. Its coarse, dense grey-brown coat covers a powerful, muscular body. Most striking are its massive, thick, outward-spiraling horns‚Äîa symbol of wild strength. Its gaze is alert and sharp. The background reveals steep alpine terrain: jagged peaks, sparse low vegetation, and abundant sunlight‚Äîconveying the harsh yet majestic wilderness and the animal‚Äôs resilient vitality.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/%E5%B9%BB%E7%81%AF%E7%89%878.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Improved Text Rendering&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Qwen-Image-2512 further elevates text rendering‚Äîalready a strength of the original‚Äîby improving accuracy, layout, and multimodal integration.&lt;/p&gt; 
&lt;p&gt;For instance, this prompt requests a complete PPT slide illustrating Qwen-Image‚Äôs development roadmap (generation and editing tracks):&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ËøôÊòØ‰∏ÄÂº†Áé∞‰ª£È£éÊ†ºÁöÑÁßëÊäÄÊÑüÂπªÁÅØÁâáÔºåÊï¥‰ΩìÈááÁî®Ê∑±ËìùËâ≤Ê∏êÂèòËÉåÊôØ„ÄÇÊ†áÈ¢òÊòØ‚ÄúQwen-ImageÂèëÂ±ïÂéÜÁ®ã‚Äù„ÄÇ‰∏ãÊñπ‰∏ÄÊù°Ê∞¥Âπ≥Âª∂‰º∏ÁöÑÂèëÂÖâÊó∂Èó¥ËΩ¥ÔºåËΩ¥Á∫ø‰∏≠Èó¥ÂÜôÁùÄ‚ÄúÁîüÂõæË∑ØÁ∫ø‚Äù„ÄÇÁî±Â∑¶‰æßÊ∑°ËìùËâ≤Ê∏êÂèò‰∏∫Âè≥‰æßÊ∑±Á¥´Ëâ≤ÔºåÂπ∂‰ª•Á≤æËá¥ÁöÑÁÆ≠Â§¥Êî∂Â∞æ„ÄÇÊó∂Èó¥ËΩ¥‰∏äÊØè‰∏™ËäÇÁÇπÈÄöËøáËôöÁ∫øËøûÊé•Ëá≥‰∏ãÊñπÈÜíÁõÆÁöÑËìùËâ≤ÂúÜËßíÁü©ÂΩ¢Êó•ÊúüÊ†áÁ≠æÔºåÊ†áÁ≠æÂÜÖ‰∏∫Ê∏ÖÊô∞ÁôΩËâ≤Â≠ó‰ΩìÔºå‰ªéÂ∑¶ÂêëÂè≥‰æùÊ¨°ÂÜôÁùÄÔºö‚Äú2025Âπ¥5Êúà6Êó• Qwen-Image È°πÁõÆÂêØÂä®‚Äù‚Äú2025Âπ¥8Êúà4Êó• Qwen-Image ÂºÄÊ∫êÂèëÂ∏É‚Äù‚Äú2025Âπ¥12Êúà31Êó• Qwen-Image-2512 ÂºÄÊ∫êÂèëÂ∏É‚Äù ÔºàÂë®Âõ¥ÂÖâÊôïÊòæËëóÔºâÂú®‰∏ãÊñπ‰∏ÄÊù°Ê∞¥Âπ≥Âª∂‰º∏ÁöÑÂèëÂÖâÊó∂Èó¥ËΩ¥ÔºåËΩ¥Á∫ø‰∏≠Èó¥ÂÜôÁùÄ‚ÄúÁºñËæëË∑ØÁ∫ø‚Äù„ÄÇÁî±Â∑¶‰æßÊ∑°ËìùËâ≤Ê∏êÂèò‰∏∫Âè≥‰æßÊ∑±Á¥´Ëâ≤ÔºåÂπ∂‰ª•Á≤æËá¥ÁöÑÁÆ≠Â§¥Êî∂Â∞æ„ÄÇÊó∂Èó¥ËΩ¥‰∏äÊØè‰∏™ËäÇÁÇπÈÄöËøáËôöÁ∫øËøûÊé•Ëá≥‰∏ãÊñπÈÜíÁõÆÁöÑËìùËâ≤ÂúÜËßíÁü©ÂΩ¢Êó•ÊúüÊ†áÁ≠æÔºåÊ†áÁ≠æÂÜÖ‰∏∫Ê∏ÖÊô∞ÁôΩËâ≤Â≠ó‰ΩìÔºå‰ªéÂ∑¶ÂêëÂè≥‰æùÊ¨°ÂÜôÁùÄÔºö‚Äú2025Âπ¥8Êúà18Êó• Qwen-Image-Edit ÂºÄÊ∫êÂèëÂ∏É‚Äù‚Äú2025Âπ¥9Êúà22Êó• Qwen-Image-Edit-2509 ÂºÄÊ∫êÂèëÂ∏É‚Äù‚Äú2025Âπ¥12Êúà19Êó• Qwen-Image-Layered ÂºÄÊ∫êÂèëÂ∏É‚Äù‚Äú2025Âπ¥12Êúà23Êó• Qwen-Image-Edit-2511 ÂºÄÊ∫êÂèëÂ∏É‚Äù&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/%E5%B9%BB%E7%81%AF%E7%89%879.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;We can even generate a before-and-after comparison slide to highlight the leap from ‚ÄúAI-blurry‚Äù to ‚Äúphotorealistic‚Äù:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ËøôÊòØ‰∏ÄÂº†Áé∞‰ª£È£éÊ†ºÁöÑÁßëÊäÄÊÑüÂπªÁÅØÁâáÔºåÊï¥‰ΩìÈááÁî®Ê∑±ËìùËâ≤Ê∏êÂèòËÉåÊôØ„ÄÇÈ°∂ÈÉ®‰∏≠Â§Æ‰∏∫ÁôΩËâ≤Êó†Ë°¨Á∫øÁ≤ó‰ΩìÂ§ßÂ≠óÊ†áÈ¢ò‚ÄúQwen-Image-2512ÈáçÁ£ÖÂèëÂ∏É‚Äù„ÄÇÁîªÈù¢‰∏ª‰Ωì‰∏∫Ê®™ÂêëÂØπÊØîÂõæÔºåËßÜËßâÁÑ¶ÁÇπÈõÜ‰∏≠‰∫é‰∏≠Èó¥ÁöÑÂçáÁ∫ßÂØπÊØîÂå∫Âüü„ÄÇÂ∑¶‰æß‰∏∫Èù¢ÈÉ®ÂÖâÊªëÊ≤°Êúâ‰ªª‰ΩïÁªÜËäÇÁöÑÂ•≥ÊÄß‰∫∫ÂÉèÔºåË¥®ÊÑüÂ∑ÆÔºõÂè≥‰æß‰∏∫È´òÂ∫¶ÂÜôÂÆûÁöÑÂπ¥ËΩªÂ•≥ÊÄßËÇñÂÉèÔºåÁöÆËÇ§ÂëàÁé∞ÁúüÂÆûÊØõÂ≠îÁ∫πÁêÜ‰∏éÁªÜÂæÆÂÖâÂΩ±ÂèòÂåñÔºåÂèë‰∏ùÊ†πÊ†πÂàÜÊòéÔºåÁúºÁú∏ÈÄè‰∫ÆÔºåË°®ÊÉÖËá™ÁÑ∂ÔºåÊï¥‰ΩìË¥®ÊÑüÊé•ËøëÂÜôÂÆûÊëÑÂΩ±„ÄÇ‰∏§ÂõæÂÉè‰πãÈó¥‰ª•‰∏Ä‰∏™ÁªøËâ≤ÊµÅÁ∫øÂûãÁÆ≠Â§¥ÈìæÊé•„ÄÇÈÄ†ÂûãÁßëÊäÄÊÑüÂçÅË∂≥Ôºå‰∏≠ÈÉ®Ê†áÊ≥®‚Äú2512Ë¥®ÊÑüÂçáÁ∫ß‚ÄùÔºå‰ΩøÁî®ÁôΩËâ≤Âä†Á≤óÂ≠ó‰ΩìÔºåÂ±Ö‰∏≠ÊòæÁ§∫„ÄÇÁÆ≠Â§¥‰∏§‰æßÊúâÂæÆÂº±ÂÖâÊôïÊïàÊûúÔºåÂ¢ûÂº∫Âä®ÊÄÅÊÑü„ÄÇÂú®ÂõæÂÉè‰∏ãÊñπÔºå‰ª•ÁôΩËâ≤ÊñáÂ≠óÂëàÁé∞‰∏âË°åËØ¥ÊòéÔºö‚Äú‚óè Êõ¥ÁúüÂÆûÁöÑ‰∫∫Áâ©Ë¥®ÊÑü„ÄÇÂ§ßÂπÖÂ∫¶Èôç‰Ωé‰∫ÜÁîüÊàêÂõæÁâáÁöÑAIÊÑüÔºåÊèêÂçá‰∫ÜÂõæÂÉèÁúüÂÆûÊÄß ‚óè Êõ¥ÁªÜËÖªÁöÑËá™ÁÑ∂Á∫πÁêÜ„ÄÇÂ§ßÂπÖÂ∫¶ÊèêÂçá‰∫ÜÁîüÊàêÂõæÁâáÁöÑÁ∫πÁêÜÁªÜËäÇ„ÄÇÈ£éÊôØÂõæÔºåÂä®Áâ©ÊØõÂèëÂàªÁîªÊõ¥ÁªÜËÖª„ÄÇ‚óè Êõ¥Â§çÊùÇÁöÑÊñáÂ≠óÊ∏≤Êüì„ÄÇÂ§ßÂπÖÊèêÂçá‰∫ÜÊñáÂ≠óÊ∏≤ÊüìÁöÑË¥®Èáè„ÄÇÂõæÊñáÊ∑∑ÂêàÊ∏≤ÊüìÊõ¥ÂáÜÁ°ÆÔºåÊéíÁâàÊõ¥Â•Ω‚Äù&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/%E5%B9%BB%E7%81%AF%E7%89%8710.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;A more complex infographic example:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ËøôÊòØ‰∏ÄÂπÖ‰∏ì‰∏öÁ∫ßÂ∑•‰∏öÊäÄÊúØ‰ø°ÊÅØÂõæË°®ÔºåÊï¥‰ΩìÈááÁî®Ê∑±ËìùËâ≤ÁßëÊäÄÊÑüËÉåÊôØÔºåÂÖâÁ∫øÂùáÂåÄÊüîÂíåÔºåËê•ÈÄ†Âá∫ÂÜ∑Èùô„ÄÅÁ≤æÂáÜÁöÑÁé∞‰ª£Â∑•‰∏öÊ∞õÂõ¥„ÄÇÁîªÈù¢ÂàÜ‰∏∫Â∑¶Âè≥‰∏§Â§ßÊùøÂùóÔºåÂ∏ÉÂ±ÄÊ∏ÖÊô∞ÔºåËßÜËßâÂ±ÇÊ¨°ÂàÜÊòé„ÄÇÂ∑¶‰æßÊùøÂùóÊ†áÈ¢ò‰∏∫‚ÄúÂÆûÈôÖÂèëÁîüÁöÑÁé∞Ë±°‚ÄùÔºå‰ª•ÊµÖËìùËâ≤ÂúÜËßíÁü©ÂΩ¢Ê°ÜÁ™ÅÂá∫ÊòæÁ§∫ÔºåÂÜÖÈÉ®ÊéíÂàó‰∏â‰∏™Ê∑±ËìùËâ≤ÊåâÈíÆÂºèÊù°ÁõÆÔºåÁ¨¨‰∏Ä‰∏™Êù°ÁõÆÂ±ïÁ§∫‰∏ÄÂ†ÜÊ£ïËâ≤Á≤âÊú´Áä∂ÂéüÊñô‰∏äÊª¥ËêΩÊ∞¥Êª¥ÁöÑÂõæÊ†áÔºåÊñáÂ≠ó‰∏∫‚ÄúÂõ¢ËÅö/ÁªìÂùó‚ÄùÔºåÂêéÈù¢ÈÖçÊúâÁªøËâ≤ÂØπÈí©ÔºõÁ¨¨‰∫å‰∏™Êù°ÁõÆ‰∏∫‰∏Ä‰∏™Ë£ÖÊúâËìùËâ≤Ê∂≤‰ΩìÂπ∂ÂÜíÂá∫Ê∞îÊ≥°ÁöÑÈî•ÂΩ¢Áì∂ÔºåÊñáÂ≠ó‰∏∫‚Äú‰∫ßÁîüÊ∞îÊ≥°/Áº∫Èô∑‚ÄùÔºåÂêéÈù¢ÈÖçÊúâÁªøËâ≤ÂØπÈí©ÔºõÁ¨¨‰∏â‰∏™Êù°ÁõÆ‰∏∫‰∏§‰∏™ÁîüÈîàÁöÑÈΩøËΩÆÔºåÊñáÂ≠ó‰∏∫‚ÄúËÆæÂ§áËÖêËöÄ/ÂÇ¨ÂåñÂâÇÂ§±Ê¥ª‚ÄùÔºåÂêéÈù¢ÈÖçÊúâÁªøËâ≤ÂØπÈí©„ÄÇÂè≥‰æßÊùøÂùóÊ†áÈ¢ò‰∏∫‚Äú„Äê‰∏ç‰ºö„ÄëÂèëÁîüÁöÑÁé∞Ë±°‚ÄùÔºå‰ΩøÁî®Á±≥ÈªÑËâ≤ÂúÜËßíÁü©ÂΩ¢Ê°ÜÂëàÁé∞ÔºåÂÜÖÈÉ®Âõõ‰∏™Êù°ÁõÆÂùáÁΩÆ‰∫éÊ∑±ÁÅ∞Ëâ≤ËÉåÊôØÊñπÊ°Ü‰∏≠„ÄÇÂõæÊ†áÂàÜÂà´‰∏∫Ôºö‰∏ÄÁªÑÁ≤æÂØÜÂïÆÂêàÁöÑÈáëÂ±ûÈΩøËΩÆÔºåÊñáÂ≠ó‰∏∫‚ÄúÂèçÂ∫îÊïàÁéá„ÄêÊòæËëóÊèêÈ´ò„Äë‚ÄùÔºå‰∏äÊñπË¶ÜÁõñÈÜíÁõÆÁöÑÁ∫¢Ëâ≤ÂèâÂè∑Ôºõ‰∏ÄÊçÜÊï¥ÈΩêÊéíÂàóÁöÑÈáëÂ±ûÁÆ°ÊùêÔºåÊñáÂ≠ó‰∏∫‚ÄúÊàêÂìÅÂÜÖÈÉ®„ÄêÁªùÂØπÊó†Ê∞îÊ≥°/Â≠îÈöô„Äë‚ÄùÔºå‰∏äÊñπË¶ÜÁõñÈÜíÁõÆÁöÑÁ∫¢Ëâ≤ÂèâÂè∑Ôºõ‰∏ÄÊù°ÂùöÂõ∫ÁöÑÈáëÂ±ûÈìæÊù°Ê≠£Âú®ÊâøÂèóÊãâÂäõÔºåÊñáÂ≠ó‰∏∫‚ÄúÊùêÊñôÂº∫Â∫¶‰∏éËÄê‰πÖÊÄß„ÄêÂæóÂà∞Â¢ûÂº∫„Äë‚ÄùÔºå‰∏äÊñπË¶ÜÁõñÈÜíÁõÆÁöÑÁ∫¢Ëâ≤ÂèâÂè∑Ôºõ‰∏ÄÂ†ÜËÖêËöÄÁöÑÊâ≥ÊâãÔºåÊñáÂ≠ó‰∏∫‚ÄúÂä†Â∑•ËøáÁ®ã„ÄêÈõ∂ËÖêËöÄ/Èõ∂ÂâØÂèçÂ∫îÈ£éÈô©„Äë‚ÄùÔºå‰∏äÊñπË¶ÜÁõñÈÜíÁõÆÁöÑÁ∫¢Ëâ≤ÂèâÂè∑„ÄÇÂ∫ïÈÉ®‰∏≠Â§ÆÊúâ‰∏ÄË°åÂ∞èÂ≠óÊ≥®ÈáäÔºö‚ÄúÊ≥®ÔºöÊ∞¥ÂàÜÁöÑÂ≠òÂú®ÈÄöÂ∏∏‰ºöÂØºËá¥Ë¥üÈù¢ÊàñÂπ≤Êâ∞ÊÄßÁöÑÁªìÊûúÔºåËÄåÈùûÁêÜÊÉ≥ÊàñÂ¢ûÂº∫ÁöÑÁä∂ÊÄÅ‚ÄùÔºåÂ≠ó‰Ωì‰∏∫ÁôΩËâ≤ÔºåÊ∏ÖÊô∞ÂèØËØª„ÄÇÊï¥‰ΩìÈ£éÊ†ºÁé∞‰ª£ÁÆÄÁ∫¶ÔºåÈÖçËâ≤ÂØπÊØîÂº∫ÁÉàÔºåÂõæÂΩ¢Á¨¶Âè∑ÂáÜÁ°Æ‰º†ËææÊäÄÊúØÈÄªËæëÔºåÈÄÇÂêàÁî®‰∫éÂ∑•‰∏öÂüπËÆ≠ÊàñÁßëÊôÆÊºîÁ§∫Âú∫ÊôØ„ÄÇ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/%E5%B9%BB%E7%81%AF%E7%89%8711.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;Or even a full educational poster:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ËøôÊòØ‰∏ÄÂπÖÁî±ÂçÅ‰∫å‰∏™ÂàÜÊ†ºÁªÑÊàêÁöÑ3√ó4ÁΩëÊ†ºÂ∏ÉÂ±ÄÁöÑÂÜôÂÆûÊëÑÂΩ±‰ΩúÂìÅÔºåÊï¥‰ΩìÂëàÁé∞‚ÄúÂÅ•Â∫∑ÁöÑ‰∏ÄÂ§©‚Äù‰∏ªÈ¢òÔºåÁîªÈù¢È£éÊ†ºÁÆÄÊ¥ÅÊ∏ÖÊô∞ÔºåÊØè‰∏ÄÂàÜÊ†ºÁã¨Á´ãÊàêÊôØÂèàÁªü‰∏Ä‰∫éÁîüÊ¥ªËäÇÂ•èÁöÑÂèô‰∫ãËÑâÁªú„ÄÇÁ¨¨‰∏ÄË°åÂàÜÂà´ÊòØ‚Äú06:00 Êô®Ë∑ëÂî§ÈÜíË∫´‰Ωì‚ÄùÔºöÈù¢ÈÉ®ÁâπÂÜôÔºå‰∏Ä‰ΩçÂ•≥ÊÄßË∫´Á©øÁÅ∞Ëâ≤ËøêÂä®Â•óË£ÖÔºåËÉåÊôØÊòØÂàùÂçáÁöÑÊúùÈò≥‰∏éËë±ÈÉÅÁªøÊ†ëÔºõ‚Äú06:30 Âä®ÊÄÅÊãâ‰º∏ÊøÄÊ¥ªÂÖ≥ËäÇ‚ÄùÔºöÂ•≥ÊÄßË∫´ÁùÄÁëú‰ºΩÊúçÂú®Èò≥Âè∞ÂÅöÊô®Èó¥Êãâ‰º∏ÔºåË∫´‰ΩìËàíÂ±ïÔºåËÉåÊôØ‰∏∫Ê∑°Á≤âËâ≤Â§©Á©∫‰∏éËøúÂ±±ËΩÆÂªìÔºõ‚Äú07:30 ÂùáË°°Ëê•ÂÖªÊó©È§ê‚ÄùÔºöÊ°å‰∏äÊëÜÊîæÂÖ®È∫¶Èù¢ÂåÖ„ÄÅÁâõÊ≤πÊûúÂíå‰∏ÄÊùØÊ©ôÊ±ÅÔºåÂ•≥ÊÄßÂæÆÁ¨ëÁùÄÂáÜÂ§áÁî®È§êÔºõ‚Äú08:00 Ë°•Ê∞¥Ê∂¶Áá•‚ÄùÔºöÈÄèÊòéÁéªÁíÉÊ∞¥ÊùØ‰∏≠ÊµÆÊúâÊü†Ê™¨ÁâáÔºåÂ•≥ÊÄßÊâãÊåÅÊ∞¥ÊùØËΩªÂïúÔºåÈò≥ÂÖâ‰ªéÂ∑¶‰æßÊñúÁÖßÂÖ•ÂÆ§ÔºåÊùØÂ£ÅÊ∞¥Áè†ÊªëËêΩÔºõÁ¨¨‰∫åË°åÂàÜÂà´ÊòØÔºö‚Äú09:00 ‰∏ìÊ≥®È´òÊïàÂ∑•‰Ωú‚ÄùÔºöÂ•≥ÊÄß‰∏ìÊ≥®Êï≤ÂáªÈîÆÁõòÔºåÂ±èÂπïÊòæÁ§∫ÁÆÄÊ¥ÅÁïåÈù¢ÔºåË∫´ÊóÅÊîæÊúâ‰∏ÄÊùØÂíñÂï°‰∏é‰∏ÄÁõÜÁªøÊ§çÔºõ‚Äú12:00 ÈùôÂøÉÈòÖËØªÊó∂ÂÖâ‚ÄùÔºöÂ•≥ÊÄßÂùêÂú®‰π¶Ê°åÂâçÁøªÈòÖÁ∫∏Ë¥®‰π¶Á±çÔºåÂè∞ÁÅØÊï£ÂèëÊöñÂÖâÔºå‰π¶È°µÊ≥õÈªÑÔºåÊóÅÊîæÂçäÊùØÁ∫¢Ëå∂Ôºõ‚Äú12:30 ÂçàÂêéËΩªÊùæÊº´Ê≠•‚ÄùÔºöÂ•≥ÊÄßÂú®ÊûóËç´ÈÅì‰∏äÊº´Ê≠•ÔºåËÑ∏ÈÉ®ÁâπÂÜôÔºõ‚Äú15:00 Ëå∂È¶ô‰º¥ÂçàÂêé‚ÄùÔºöÂ•≥ÊÄßÁ´ØÁùÄÈ™®Áì∑Ëå∂ÊùØÁ´ôÂú®Á™óËæπÔºåÁ™óÂ§ñÊòØÂüéÂ∏ÇË°óÊôØ‰∏éÈ£òÂä®‰∫ëÊúµÔºåËå∂È¶ôË¢ÖË¢ÖÔºõÁ¨¨‰∏âË°åÂàÜÂà´ÊòØÔºö‚Äú18:00 ËøêÂä®ÈáäÊîæÂéãÂäõ‚ÄùÔºöÂÅ•Ë∫´ÊàøÂÜÖÔºåÂ•≥ÊÄßÊ≠£Âú®ÁªÉ‰π†Áëú‰ºΩÔºõ‚Äú19:00 ÁæéÂë≥ÊôöÈ§ê‚ÄùÔºöÂ•≥ÊÄßÂú®ÂºÄÊîæÂºèÂé®Êàø‰∏≠ÂàáËèúÔºåÁ†ßÊùø‰∏äÊúâÁï™ËåÑ‰∏éÈùíÊ§íÔºåÈîÖ‰∏≠ÁÉ≠Ê∞îÂçáËÖæÔºåÁÅØÂÖâÊ∏©ÊöñÔºõ‚Äú21:00 ÂÜ•ÊÉ≥Âä©Áú†‚ÄùÔºöÂ•≥ÊÄßÁõòËÖøÂùêÂú®ÊüîËΩØÂú∞ÊØØ‰∏äÂÜ•ÊÉ≥ÔºåÂèåÊâãËΩªÊîæËÜù‰∏äÔºåÈó≠ÁõÆÂÆÅÈùôÔºõ‚Äú21:30 ËøõÂÖ•Áù°Áú†‚ÄùÔºöÂ•≥ÊÄßË∫∫Âú®Â∫ä‰∏ä‰ºëÊÅØ„ÄÇÊï¥‰ΩìÈááÁî®Ëá™ÁÑ∂ÂÖâÁ∫ø‰∏∫‰∏ªÔºåËâ≤Ë∞É‰ª•ÊöñÁôΩ‰∏éÁ±≥ÁÅ∞‰∏∫Âü∫Ë∞ÉÔºåÂÖâÂΩ±Â±ÇÊ¨°ÂàÜÊòéÔºåÁîªÈù¢ÂÖÖÊª°Ê∏©È¶®ÁöÑÁîüÊ¥ªÊ∞îÊÅØ‰∏éËßÑÂæãÁöÑËäÇÂ•èÊÑü„ÄÇ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/%E5%B9%BB%E7%81%AF%E7%89%8712.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;These are the core enhancements in this update. We hope you enjoy using Qwen-Image-2512!&lt;/p&gt; 
&lt;h3&gt;Showcase of Qwen-Image-Edit-2511&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Qwen-Image-Edit-2511 Enhances Character Consistency&lt;/strong&gt; In Qwen-Image-Edit-2511, character consistency has been significantly improved. The model can perform imaginative edits based on an input portrait while preserving the identity and visual characteristics of the subject.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%871.JPG#center" alt="" /&gt; &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%872.JPG#center" alt="" /&gt; &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%873.JPG#center" alt="" /&gt; &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%874.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Improved Multi-Person Consistency&lt;/strong&gt; While Qwen-Image-Edit-2509 already improved consistency for single-subject editing, Qwen-Image-Edit-2511 further enhances consistency in multi-person group photos‚Äîenabling high-fidelity fusion of two separate person images into a coherent group shot: &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%875.JPG#center" alt="" /&gt; &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%876.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Built-in Support for Community-Created LoRAs&lt;/strong&gt; Since Qwen-Image-Edit‚Äôs release, the community has developed many creative and high-quality LoRAs‚Äîgreatly expanding its expressive potential. Qwen-Image-Edit-2511 integrates selected popular LoRAs directly into the base model, unlocking their effects without extra tuning.&lt;/p&gt; 
&lt;p&gt;For example, Lighting Enhancement LoRA Realistic lighting control is now achievable out-of-the-box: &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%877.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%878.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;Another example, generating new viewpoints can now be done directly with the base model:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%879.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%8710.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Industrial Design Applications&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;We‚Äôve paid special attention to practical engineering scenarios‚Äîfor instance, batch industrial product design:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%8711.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%8712.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;‚Ä¶and material replacement for industrial components: &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%8713.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%8714.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Enhanced Geometric Reasoning&lt;/strong&gt; Qwen-Image-Edit-2511 introduces stronger geometric reasoning capability‚Äîe.g., directly generating auxiliary construction lines for design or annotation purposes:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%8715.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%8716.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;h2&gt;AI Arena&lt;/h2&gt; 
&lt;p&gt;To comprehensively evaluate the general image generation capabilities of Qwen-Image and objectively compare it with state-of-the-art closed-source APIs, we introduce &lt;a href="https://aiarena.alibaba-inc.com"&gt;AI Arena&lt;/a&gt;, an open benchmarking platform built on the Elo rating system. AI Arena provides a fair, transparent, and dynamic environment for model evaluation.&lt;/p&gt; 
&lt;p&gt;In each round, two images‚Äîgenerated by randomly selected models from the same prompt‚Äîare anonymously presented to users for pairwise comparison. Users vote for the better image, and the results are used to update both personal and global leaderboards via the Elo algorithm, enabling developers, researchers, and the public to assess model performance in a robust and data-driven way. AI Arena is now publicly available, welcoming everyone to participate in model evaluations.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/figure_aiarena_website.png" alt="AI Arena" /&gt;&lt;/p&gt; 
&lt;p&gt;The latest leaderboard rankings can be viewed at &lt;a href="https://aiarena.alibaba-inc.com/corpora/arena/leaderboard?arenaType=text2image"&gt;AI Arena Learboard&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you wish to deploy your model on AI Arena and participate in the evaluation, please contact &lt;a href="mailto:weiyue.wy@alibaba-inc.com"&gt;weiyue.wy@alibaba-inc.com&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Community Support&lt;/h2&gt; 
&lt;h3&gt;Huggingface&lt;/h3&gt; 
&lt;p&gt;Diffusers has supported Qwen-Image since day 0. Support for LoRA and finetuning workflows is currently in development and will be available soon.&lt;/p&gt; 
&lt;h3&gt;ModelScope&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/modelscope/DiffSynth-Studio"&gt;DiffSynth-Studio&lt;/a&gt;&lt;/strong&gt; provides comprehensive support for Qwen-Image, including low-GPU-memory layer-by-layer offload (inference within 4GB VRAM), FP8 quantization, LoRA / full training.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/modelscope/DiffSynth-Engine"&gt;DiffSynth-Engine&lt;/a&gt;&lt;/strong&gt; delivers advanced optimizations for Qwen-Image inference and deployment, including FBCache-based acceleration, classifier-free guidance (CFG) parallel, and more.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://www.modelscope.cn/aigc"&gt;ModelScope AIGC Central&lt;/a&gt;&lt;/strong&gt; provides hands-on experiences on Qwen Image, including: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://www.modelscope.cn/aigc/imageGeneration"&gt;Image Generation&lt;/a&gt;: Generate high fidelity images using the Qwen Image model.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.modelscope.cn/aigc/modelTraining"&gt;LoRA Training&lt;/a&gt;: Easily train Qwen Image LoRAs for personalized concepts.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;SGLang&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;SGLang-Diffusion&lt;/strong&gt; provides day-0 support for Qwen-Image models. To play with &lt;code&gt;Qwen-Image-Edit-2511&lt;/code&gt;, use the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;sglang generate --model-path Qwen/Qwen-Image-Edit-2511 --prompt "make the girl in Figure 1 dance with the capybara in Figure 2."  --image-path "https://github.com/lm-sys/lm-sys.github.io/releases/download/test/TI2I_Qwen_Image_Edit_Input.jpg" "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/edit2509_2.jpg"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The output should be like &lt;img src="https://github.com/lm-sys/lm-sys.github.io/releases/download/test/SGLang_Diffusion_Qwen_Image_Edit_2511_example_output.jpg" alt="" /&gt;&lt;/p&gt; 
&lt;h3&gt;WaveSpeedAI&lt;/h3&gt; 
&lt;p&gt;WaveSpeed has deployed Qwen-Image on their platform from day 0, visit their &lt;a href="https://wavespeed.ai/models/wavespeed-ai/qwen-image/text-to-image"&gt;model page&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h3&gt;LiblibAI&lt;/h3&gt; 
&lt;p&gt;LiblibAI offers native support for Qwen-Image from day 0. Visit their &lt;a href="https://www.liblib.art/modelinfo/c62a103bd98a4246a2334e2d952f7b21?from=sd&amp;amp;versionUuid=75e0be0c93b34dd8baeec9c968013e0c"&gt;community&lt;/a&gt; page for more details and discussions.&lt;/p&gt; 
&lt;h3&gt;Inference Acceleration Method: cache-dit&lt;/h3&gt; 
&lt;p&gt;cache-dit offers cache acceleration support for Qwen-Image with DBCache, TaylorSeer and Cache CFG. Visit their &lt;a href="https://github.com/vipshop/cache-dit/raw/main/examples/pipeline/run_qwen_image.py"&gt;example&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;License Agreement&lt;/h2&gt; 
&lt;p&gt;Qwen-Image is licensed under Apache 2.0.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;We kindly encourage citation of our work if you find it useful.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{wu2025qwenimagetechnicalreport,
      title={Qwen-Image Technical Report}, 
      author={Chenfei Wu and Jiahao Li and Jingren Zhou and Junyang Lin and Kaiyuan Gao and Kun Yan and Sheng-ming Yin and Shuai Bai and Xiao Xu and Yilei Chen and Yuxiang Chen and Zecheng Tang and Zekai Zhang and Zhengyi Wang and An Yang and Bowen Yu and Chen Cheng and Dayiheng Liu and Deqing Li and Hang Zhang and Hao Meng and Hu Wei and Jingyuan Ni and Kai Chen and Kuan Cao and Liang Peng and Lin Qu and Minggang Wu and Peng Wang and Shuting Yu and Tingkun Wen and Wensen Feng and Xiaoxiao Xu and Yi Wang and Yichang Zhang and Yongqiang Zhu and Yujia Wu and Yuxuan Cai and Zenan Liu},
      year={2025},
      eprint={2508.02324},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2508.02324}, 
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contact and Join Us&lt;/h2&gt; 
&lt;p&gt;If you'd like to get in touch with our research team, we'd love to hear from you! Join our &lt;a href="https://discord.gg/z3GAxXZ9Ce"&gt;Discord&lt;/a&gt; or scan the QR code to connect via our &lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen-Image/main/assets/wechat.png"&gt;WeChat groups&lt;/a&gt; ‚Äî we're always open to discussion and collaboration.&lt;/p&gt; 
&lt;p&gt;If you have questions about this repository, feedback to share, or want to contribute directly, we welcome your issues and pull requests on GitHub. Your contributions help make Qwen-Image better for everyone.&lt;/p&gt; 
&lt;p&gt;If you're passionate about fundamental research, we're hiring full-time employees (FTEs) and research interns. Don't wait ‚Äî reach out to us at &lt;a href="mailto:fulai.hr@alibaba-inc.com"&gt;fulai.hr@alibaba-inc.com&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#QwenLM/Qwen-Image&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=QwenLM/Qwen-Image&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>alexta69/metube</title>
      <link>https://github.com/alexta69/metube</link>
      <description>&lt;p&gt;Self-hosted YouTube downloader (web UI for youtube-dl / yt-dlp)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MeTube&lt;/h1&gt; 
&lt;p&gt;&lt;img src="https://github.com/alexta69/metube/actions/workflows/main.yml/badge.svg?sanitize=true" alt="Build Status" /&gt; &lt;img src="https://img.shields.io/docker/pulls/alexta69/metube.svg?sanitize=true" alt="Docker Pulls" /&gt;&lt;/p&gt; 
&lt;p&gt;Web GUI for youtube-dl (using the &lt;a href="https://github.com/yt-dlp/yt-dlp"&gt;yt-dlp&lt;/a&gt; fork) with playlist support. Allows you to download videos from YouTube and &lt;a href="https://github.com/yt-dlp/yt-dlp/raw/master/supportedsites.md"&gt;dozens of other sites&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/alexta69/metube/raw/master/screenshot.gif" alt="screenshot1" /&gt;&lt;/p&gt; 
&lt;h2&gt;üê≥ Run using Docker&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -d -p 8081:8081 -v /path/to/downloads:/downloads ghcr.io/alexta69/metube
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;üê≥ Run using docker-compose&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;services:
  metube:
    image: ghcr.io/alexta69/metube
    container_name: metube
    restart: unless-stopped
    ports:
      - "8081:8081"
    volumes:
      - /path/to/downloads:/downloads
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;‚öôÔ∏è Configuration via environment variables&lt;/h2&gt; 
&lt;p&gt;Certain values can be set via environment variables, using the &lt;code&gt;-e&lt;/code&gt; parameter on the docker command line, or the &lt;code&gt;environment:&lt;/code&gt; section in docker-compose.&lt;/p&gt; 
&lt;h3&gt;‚¨áÔ∏è Download Behavior&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;DOWNLOAD_MODE&lt;/strong&gt;: This flag controls how downloads are scheduled and executed. Options are &lt;code&gt;sequential&lt;/code&gt;, &lt;code&gt;concurrent&lt;/code&gt;, and &lt;code&gt;limited&lt;/code&gt;. Defaults to &lt;code&gt;limited&lt;/code&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;sequential&lt;/code&gt;: Downloads are processed one at a time. A new download won't start until the previous one has finished. This mode is useful for conserving system resources or ensuring downloads occur in strict order.&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;concurrent&lt;/code&gt;: Downloads are started immediately as they are added, with no built-in limit on how many run simultaneously. This mode may overwhelm your system if too many downloads start at once.&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;limited&lt;/code&gt;: Downloads are started concurrently but are capped by a concurrency limit. In this mode, a semaphore is used so that at most a fixed number of downloads run at any given time.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MAX_CONCURRENT_DOWNLOADS&lt;/strong&gt;: This flag is used only when &lt;code&gt;DOWNLOAD_MODE&lt;/code&gt; is set to &lt;code&gt;limited&lt;/code&gt;.&lt;br /&gt; It specifies the maximum number of simultaneous downloads allowed. For example, if set to &lt;code&gt;5&lt;/code&gt;, then at most five downloads will run concurrently, and any additional downloads will wait until one of the active downloads completes. Defaults to &lt;code&gt;3&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;DELETE_FILE_ON_TRASHCAN&lt;/strong&gt;: if &lt;code&gt;true&lt;/code&gt;, downloaded files are deleted on the server, when they are trashed from the "Completed" section of the UI. Defaults to &lt;code&gt;false&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;DEFAULT_OPTION_PLAYLIST_STRICT_MODE&lt;/strong&gt;: if &lt;code&gt;true&lt;/code&gt;, the "Strict Playlist mode" switch will be enabled by default. In this mode the playlists will be downloaded only if the URL strictly points to a playlist. URLs to videos inside a playlist will be treated same as direct video URL. Defaults to &lt;code&gt;false&lt;/code&gt; .&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;DEFAULT_OPTION_PLAYLIST_ITEM_LIMIT&lt;/strong&gt;: Maximum number of playlist items that can be downloaded. Defaults to &lt;code&gt;0&lt;/code&gt; (no limit).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üìÅ Storage &amp;amp; Directories&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;DOWNLOAD_DIR&lt;/strong&gt;: Path to where the downloads will be saved. Defaults to &lt;code&gt;/downloads&lt;/code&gt; in the Docker image, and &lt;code&gt;.&lt;/code&gt; otherwise.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;AUDIO_DOWNLOAD_DIR&lt;/strong&gt;: Path to where audio-only downloads will be saved, if you wish to separate them from the video downloads. Defaults to the value of &lt;code&gt;DOWNLOAD_DIR&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;CUSTOM_DIRS&lt;/strong&gt;: Whether to enable downloading videos into custom directories within the &lt;strong&gt;DOWNLOAD_DIR&lt;/strong&gt; (or &lt;strong&gt;AUDIO_DOWNLOAD_DIR&lt;/strong&gt;). When enabled, a dropdown appears next to the Add button to specify the download directory. Defaults to &lt;code&gt;true&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;CREATE_CUSTOM_DIRS&lt;/strong&gt;: Whether to support automatically creating directories within the &lt;strong&gt;DOWNLOAD_DIR&lt;/strong&gt; (or &lt;strong&gt;AUDIO_DOWNLOAD_DIR&lt;/strong&gt;) if they do not exist. When enabled, the download directory selector supports free-text input, and the specified directory will be created recursively. Defaults to &lt;code&gt;true&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;CUSTOM_DIRS_EXCLUDE_REGEX&lt;/strong&gt;: Regular expression to exclude some custom directories from the dropdown. Empty regex disables exclusion. Defaults to &lt;code&gt;(^|/)[.@].*$&lt;/code&gt;, which means directories starting with &lt;code&gt;.&lt;/code&gt; or &lt;code&gt;@&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;DOWNLOAD_DIRS_INDEXABLE&lt;/strong&gt;: If &lt;code&gt;true&lt;/code&gt;, the download directories (&lt;strong&gt;DOWNLOAD_DIR&lt;/strong&gt; and &lt;strong&gt;AUDIO_DOWNLOAD_DIR&lt;/strong&gt;) are indexable on the web server. Defaults to &lt;code&gt;false&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;STATE_DIR&lt;/strong&gt;: Path to where the queue persistence files will be saved. Defaults to &lt;code&gt;/downloads/.metube&lt;/code&gt; in the Docker image, and &lt;code&gt;.&lt;/code&gt; otherwise.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;TEMP_DIR&lt;/strong&gt;: Path where intermediary download files will be saved. Defaults to &lt;code&gt;/downloads&lt;/code&gt; in the Docker image, and &lt;code&gt;.&lt;/code&gt; otherwise. 
  &lt;ul&gt; 
   &lt;li&gt;Set this to an SSD or RAM filesystem (e.g., &lt;code&gt;tmpfs&lt;/code&gt;) for better performance.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Note&lt;/strong&gt;: Using a RAM filesystem may prevent downloads from being resumed.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üìù File Naming &amp;amp; yt-dlp&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;OUTPUT_TEMPLATE&lt;/strong&gt;: The template for the filenames of the downloaded videos, formatted according to &lt;a href="https://github.com/yt-dlp/yt-dlp/raw/master/README.md#output-template"&gt;this spec&lt;/a&gt;. Defaults to &lt;code&gt;%(title)s.%(ext)s&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;OUTPUT_TEMPLATE_CHAPTER&lt;/strong&gt;: The template for the filenames of the downloaded videos when split into chapters via postprocessors. Defaults to &lt;code&gt;%(title)s - %(section_number)s %(section_title)s.%(ext)s&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;OUTPUT_TEMPLATE_PLAYLIST&lt;/strong&gt;: The template for the filenames of the downloaded videos when downloaded as a playlist. Defaults to &lt;code&gt;%(playlist_title)s/%(title)s.%(ext)s&lt;/code&gt;. When empty, then &lt;code&gt;OUTPUT_TEMPLATE&lt;/code&gt; is used.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;YTDL_OPTIONS&lt;/strong&gt;: Additional options to pass to yt-dlp in JSON format. &lt;a href="https://github.com/yt-dlp/yt-dlp/raw/master/yt_dlp/YoutubeDL.py#L222"&gt;See available options here&lt;/a&gt;. They roughly correspond to command-line options, though some do not have exact equivalents here. For example, &lt;code&gt;--recode-video&lt;/code&gt; has to be specified via &lt;code&gt;postprocessors&lt;/code&gt;. Also note that dashes are replaced with underscores. You may find &lt;a href="https://github.com/yt-dlp/yt-dlp/raw/master/devscripts/cli_to_api.py"&gt;this script&lt;/a&gt; helpful for converting from command-line options to &lt;code&gt;YTDL_OPTIONS&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;YTDL_OPTIONS_FILE&lt;/strong&gt;: A path to a JSON file that will be loaded and used for populating &lt;code&gt;YTDL_OPTIONS&lt;/code&gt; above. Please note that if both &lt;code&gt;YTDL_OPTIONS_FILE&lt;/code&gt; and &lt;code&gt;YTDL_OPTIONS&lt;/code&gt; are specified, the options in &lt;code&gt;YTDL_OPTIONS&lt;/code&gt; take precedence. The file will be monitored for changes and reloaded automatically when changes are detected.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üåê Web Server &amp;amp; URLs&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;URL_PREFIX&lt;/strong&gt;: Base path for the web server (for use when hosting behind a reverse proxy). Defaults to &lt;code&gt;/&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;PUBLIC_HOST_URL&lt;/strong&gt;: Base URL for the download links shown in the UI for completed files. By default, MeTube serves them under its own URL. If your download directory is accessible on another URL and you want the download links to be based there, use this variable to set it.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;PUBLIC_HOST_AUDIO_URL&lt;/strong&gt;: Same as PUBLIC_HOST_URL but for audio downloads.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;HTTPS&lt;/strong&gt;: Use &lt;code&gt;https&lt;/code&gt; instead of &lt;code&gt;http&lt;/code&gt; (&lt;strong&gt;CERTFILE&lt;/strong&gt; and &lt;strong&gt;KEYFILE&lt;/strong&gt; required). Defaults to &lt;code&gt;false&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;CERTFILE&lt;/strong&gt;: HTTPS certificate file path.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;KEYFILE&lt;/strong&gt;: HTTPS key file path.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ROBOTS_TXT&lt;/strong&gt;: A path to a &lt;code&gt;robots.txt&lt;/code&gt; file mounted in the container.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üè† Basic Setup&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;UID&lt;/strong&gt;: User under which MeTube will run. Defaults to &lt;code&gt;1000&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;GID&lt;/strong&gt;: Group under which MeTube will run. Defaults to &lt;code&gt;1000&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;UMASK&lt;/strong&gt;: Umask value used by MeTube. Defaults to &lt;code&gt;022&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;DEFAULT_THEME&lt;/strong&gt;: Default theme to use for the UI, can be set to &lt;code&gt;light&lt;/code&gt;, &lt;code&gt;dark&lt;/code&gt;, or &lt;code&gt;auto&lt;/code&gt;. Defaults to &lt;code&gt;auto&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;LOGLEVEL&lt;/strong&gt;: Log level, can be set to &lt;code&gt;DEBUG&lt;/code&gt;, &lt;code&gt;INFO&lt;/code&gt;, &lt;code&gt;WARNING&lt;/code&gt;, &lt;code&gt;ERROR&lt;/code&gt;, &lt;code&gt;CRITICAL&lt;/code&gt;, or &lt;code&gt;NONE&lt;/code&gt;. Defaults to &lt;code&gt;INFO&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ENABLE_ACCESSLOG&lt;/strong&gt;: Whether to enable access log. Defaults to &lt;code&gt;false&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The project's Wiki contains examples of useful configurations contributed by users of MeTube:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/alexta69/metube/wiki/YTDL_OPTIONS-Cookbook"&gt;YTDL_OPTIONS Cookbook&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/alexta69/metube/wiki/OUTPUT_TEMPLATE-Cookbook"&gt;OUTPUT_TEMPLATE Cookbook&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üç™ Using browser cookies&lt;/h2&gt; 
&lt;p&gt;In case you need to use your browser's cookies with MeTube, for example to download restricted or private videos:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Add the following to your docker-compose.yml:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;    volumes:
      - /path/to/cookies:/cookies
    environment:
      - YTDL_OPTIONS={"cookiefile":"/cookies/cookies.txt"}
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Install in your browser an extension to extract cookies: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://addons.mozilla.org/en-US/firefox/addon/export-cookies-txt/"&gt;Firefox&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://chrome.google.com/webstore/detail/get-cookiestxt-locally/cclelndahbckbenkjhflpdbgdldlbecc"&gt;Chrome&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Extract the cookies you need with the extension and rename the file &lt;code&gt;cookies.txt&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Drop the file in the folder you configured in the docker-compose.yml above&lt;/li&gt; 
 &lt;li&gt;Restart the container&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üîå Browser extensions&lt;/h2&gt; 
&lt;p&gt;Browser extensions allow right-clicking videos and sending them directly to MeTube. Please note that if you're on an HTTPS page, your MeTube instance must be behind an HTTPS reverse proxy (see below) for the extensions to work.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Chrome:&lt;/strong&gt; contributed by &lt;a href="https://github.com/rpsl"&gt;Rpsl&lt;/a&gt;. You can install it from &lt;a href="https://chrome.google.com/webstore/detail/metube-downloader/fbmkmdnlhacefjljljlbhkodfmfkijdh"&gt;Google Chrome Webstore&lt;/a&gt; or use developer mode and install &lt;a href="https://github.com/Rpsl/metube-browser-extension"&gt;from sources&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Firefox:&lt;/strong&gt; contributed by &lt;a href="https://github.com/nanocortex"&gt;nanocortex&lt;/a&gt;. You can install it from &lt;a href="https://addons.mozilla.org/en-US/firefox/addon/metube-downloader"&gt;Firefox Addons&lt;/a&gt; or get sources from &lt;a href="https://github.com/nanocortex/metube-firefox-addon"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;üì± iOS Shortcut&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/rithask"&gt;rithask&lt;/a&gt; created an iOS shortcut to send URLs to MeTube from Safari. Enter the MeTube instance address when prompted which will be saved for later use. You can run the shortcut from Safari‚Äôs share menu. The shortcut can be downloaded from &lt;a href="https://www.icloud.com/shortcuts/66627a9f334c467baabdb2769763a1a6"&gt;this iCloud link&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;üì± iOS Compatibility&lt;/h2&gt; 
&lt;p&gt;iOS has strict requirements for video files, requiring h264 or h265 video codec and aac audio codec in MP4 container. This can sometimes be a lower quality than the best quality available. To accommodate iOS requirements, when downloading a MP4 format you can choose "Best (iOS)" to get the best quality formats as compatible as possible with iOS requirements.&lt;/p&gt; 
&lt;p&gt;To force all downloads to be converted to an iOS-compatible codec, insert this as an environment variable:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;  environment:
    - 'YTDL_OPTIONS={"format": "best", "exec": "ffmpeg -i %(filepath)q -c:v libx264 -c:a aac %(filepath)q.h264.mp4"}'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;üîñ Bookmarklet&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/kushfest"&gt;kushfest&lt;/a&gt; has created a Chrome bookmarklet for sending the currently open webpage to MeTube. Please note that if you're on an HTTPS page, your MeTube instance must be configured with &lt;code&gt;HTTPS&lt;/code&gt; as &lt;code&gt;true&lt;/code&gt; in the environment, or be behind an HTTPS reverse proxy (see below) for the bookmarklet to work.&lt;/p&gt; 
&lt;p&gt;GitHub doesn't allow embedding JavaScript as a link, so the bookmarklet has to be created manually by copying the following code to a new bookmark you create on your bookmarks bar. Change the hostname in the URL below to point to your MeTube instance.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-javascript"&gt;javascript:!function(){xhr=new XMLHttpRequest();xhr.open("POST","https://metube.domain.com/add");xhr.withCredentials=true;xhr.send(JSON.stringify({"url":document.location.href,"quality":"best"}));xhr.onload=function(){if(xhr.status==200){alert("Sent to metube!")}else{alert("Send to metube failed. Check the javascript console for clues.")}}}();
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://github.com/shoonya75"&gt;shoonya75&lt;/a&gt; has contributed a Firefox version:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-javascript"&gt;javascript:(function(){xhr=new XMLHttpRequest();xhr.open("POST","https://metube.domain.com/add");xhr.send(JSON.stringify({"url":document.location.href,"quality":"best"}));xhr.onload=function(){if(xhr.status==200){alert("Sent to metube!")}else{alert("Send to metube failed. Check the javascript console for clues.")}}})();
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The above bookmarklets use &lt;code&gt;alert()&lt;/code&gt; as a success/failure notification. The following will show a toast message instead:&lt;/p&gt; 
&lt;p&gt;Chrome:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-javascript"&gt;javascript:!function(){function notify(msg) {var sc = document.scrollingElement.scrollTop; var text = document.createElement('span');text.innerHTML=msg;var ts = text.style;ts.all = 'revert';ts.color = '#000';ts.fontFamily = 'Verdana, sans-serif';ts.fontSize = '15px';ts.backgroundColor = 'white';ts.padding = '15px';ts.border = '1px solid gainsboro';ts.boxShadow = '3px 3px 10px';ts.zIndex = '100';document.body.appendChild(text);ts.position = 'absolute'; ts.top = 50 + sc + 'px'; ts.left = (window.innerWidth / 2)-(text.offsetWidth / 2) + 'px'; setTimeout(function () { text.style.visibility = "hidden"; }, 1500);}xhr=new XMLHttpRequest();xhr.open("POST","https://metube.domain.com/add");xhr.send(JSON.stringify({"url":document.location.href,"quality":"best"}));xhr.onload=function() { if(xhr.status==200){notify("Sent to metube!")}else {notify("Send to metube failed. Check the javascript console for clues.")}}}();
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Firefox:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-javascript"&gt;javascript:(function(){function notify(msg) {var sc = document.scrollingElement.scrollTop; var text = document.createElement('span');text.innerHTML=msg;var ts = text.style;ts.all = 'revert';ts.color = '#000';ts.fontFamily = 'Verdana, sans-serif';ts.fontSize = '15px';ts.backgroundColor = 'white';ts.padding = '15px';ts.border = '1px solid gainsboro';ts.boxShadow = '3px 3px 10px';ts.zIndex = '100';document.body.appendChild(text);ts.position = 'absolute'; ts.top = 50 + sc + 'px'; ts.left = (window.innerWidth / 2)-(text.offsetWidth / 2) + 'px'; setTimeout(function () { text.style.visibility = "hidden"; }, 1500);}xhr=new XMLHttpRequest();xhr.open("POST","https://metube.domain.com/add");xhr.send(JSON.stringify({"url":document.location.href,"quality":"best"}));xhr.onload=function() { if(xhr.status==200){notify("Sent to metube!")}else {notify("Send to metube failed. Check the javascript console for clues.")}}})();
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;‚ö° Raycast extension&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/dotvhs"&gt;dotvhs&lt;/a&gt; has created an &lt;a href="https://www.raycast.com/dot/metube"&gt;extension for Raycast&lt;/a&gt; that allows adding videos to MeTube directly from Raycast.&lt;/p&gt; 
&lt;h2&gt;üîí HTTPS support, and running behind a reverse proxy&lt;/h2&gt; 
&lt;p&gt;It's possible to configure MeTube to listen in HTTPS mode. &lt;code&gt;docker-compose&lt;/code&gt; example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;services:
  metube:
    image: ghcr.io/alexta69/metube
    container_name: metube
    restart: unless-stopped
    ports:
      - "8081:8081"
    volumes:
      - /path/to/downloads:/downloads
      - /path/to/ssl/crt:/ssl/crt.pem
      - /path/to/ssl/key:/ssl/key.pem
    environment:
      - HTTPS=true
      - CERTFILE=/ssl/crt.pem
      - KEYFILE=/ssl/key.pem
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It's also possible to run MeTube behind a reverse proxy, in order to support authentication. HTTPS support can also be added in this way.&lt;/p&gt; 
&lt;p&gt;When running behind a reverse proxy which remaps the URL (i.e. serves MeTube under a subdirectory and not under root), don't forget to set the URL_PREFIX environment variable to the correct value.&lt;/p&gt; 
&lt;p&gt;If you're using the &lt;a href="https://docs.linuxserver.io/general/swag"&gt;linuxserver/swag&lt;/a&gt; image for your reverse proxying needs (which I can heartily recommend), it already includes ready snippets for proxying MeTube both in &lt;a href="https://github.com/linuxserver/reverse-proxy-confs/raw/master/metube.subfolder.conf.sample"&gt;subfolder&lt;/a&gt; and &lt;a href="https://github.com/linuxserver/reverse-proxy-confs/raw/master/metube.subdomain.conf.sample"&gt;subdomain&lt;/a&gt; modes under the &lt;code&gt;nginx/proxy-confs&lt;/code&gt; directory in the configuration volume. It also includes Authelia which can be used for authentication.&lt;/p&gt; 
&lt;h3&gt;üåê NGINX&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-nginx"&gt;location /metube/ {
        proxy_pass http://metube:8081;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        proxy_set_header Host $host;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note: the extra &lt;code&gt;proxy_set_header&lt;/code&gt; directives are there to make WebSocket work.&lt;/p&gt; 
&lt;h3&gt;üåê Apache&lt;/h3&gt; 
&lt;p&gt;Contributed by &lt;a href="https://github.com/PIE-yt"&gt;PIE-yt&lt;/a&gt;. Source &lt;a href="https://gist.github.com/PIE-yt/29e7116588379032427f5bd446b2cac4"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-apache"&gt;# For putting in your Apache sites site.conf
# Serves MeTube under a /metube/ subdir (http://yourdomain.com/metube/)
&amp;lt;Location /metube/&amp;gt;
    ProxyPass http://localhost:8081/ retry=0 timeout=30
    ProxyPassReverse http://localhost:8081/
&amp;lt;/Location&amp;gt;

&amp;lt;Location /metube/socket.io&amp;gt;
    RewriteEngine On
    RewriteCond %{QUERY_STRING} transport=websocket    [NC]
    RewriteRule /(.*) ws://localhost:8081/socket.io/$1 [P,L]
    ProxyPass http://localhost:8081/socket.io retry=0 timeout=30
    ProxyPassReverse http://localhost:8081/socket.io
&amp;lt;/Location&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;üåê Caddy&lt;/h3&gt; 
&lt;p&gt;The following example Caddyfile gets a reverse proxy going behind &lt;a href="https://caddyserver.com"&gt;caddy&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-caddyfile"&gt;example.com {
  route /metube/* {
    uri strip_prefix metube
    reverse_proxy metube:8081
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;üîÑ Updating yt-dlp&lt;/h2&gt; 
&lt;p&gt;The engine which powers the actual video downloads in MeTube is &lt;a href="https://github.com/yt-dlp/yt-dlp"&gt;yt-dlp&lt;/a&gt;. Since video sites regularly change their layouts, frequent updates of yt-dlp are required to keep up.&lt;/p&gt; 
&lt;p&gt;There's an automatic nightly build of MeTube which looks for a new version of yt-dlp, and if one exists, the build pulls it and publishes an updated docker image. Therefore, in order to keep up with the changes, it's recommended that you update your MeTube container regularly with the latest image.&lt;/p&gt; 
&lt;p&gt;I recommend installing and setting up &lt;a href="https://github.com/nicholas-fedor/watchtower"&gt;watchtower&lt;/a&gt; for this purpose.&lt;/p&gt; 
&lt;h2&gt;üîß Troubleshooting and submitting issues&lt;/h2&gt; 
&lt;p&gt;Before asking a question or submitting an issue for MeTube, please remember that MeTube is only a UI for &lt;a href="https://github.com/yt-dlp/yt-dlp"&gt;yt-dlp&lt;/a&gt;. Any issues you might be experiencing with authentication to video websites, postprocessing, permissions, other &lt;code&gt;YTDL_OPTIONS&lt;/code&gt; configurations which seem not to work, or anything else that concerns the workings of the underlying yt-dlp library, need not be opened on the MeTube project. In order to debug and troubleshoot them, it's advised to try using the yt-dlp binary directly first, bypassing the UI, and once that is working, importing the options that worked for you into &lt;code&gt;YTDL_OPTIONS&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;In order to test with the yt-dlp command directly, you can either download it and run it locally, or for a better simulation of its actual conditions, you can run it within the MeTube container itself. Assuming your MeTube container is called &lt;code&gt;metube&lt;/code&gt;, run the following on your Docker host to get a shell inside the container:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker exec -ti metube sh
cd /downloads
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Once there, you can use the yt-dlp command freely.&lt;/p&gt; 
&lt;h2&gt;üí° Submitting feature requests&lt;/h2&gt; 
&lt;p&gt;MeTube development relies on code contributions by the community. The program as it currently stands fits my own use cases, and is therefore feature-complete as far as I'm concerned. If your use cases are different and require additional features, please feel free to submit PRs that implement those features. It's advisable to create an issue first to discuss the planned implementation, because in an effort to reduce bloat, some PRs may not be accepted. However, note that opening a feature request when you don't intend to implement the feature will rarely result in the request being fulfilled.&lt;/p&gt; 
&lt;h2&gt;üõ†Ô∏è Building and running locally&lt;/h2&gt; 
&lt;p&gt;Make sure you have Node.js 22+ and Python 3.13 installed.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd metube/ui
# install Angular and build the UI
pnpm install
pnpm run build
# install python dependencies
cd ..
curl -LsSf https://astral.sh/uv/install.sh | sh
uv sync
# run
uv run python3 app/main.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;A Docker image can be built locally (it will build the UI too):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker build -t metube .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note that if you're running the server in VSCode, your downloads will go to your user's Downloads folder (this is configured via the environment in &lt;code&gt;.vscode/launch.json&lt;/code&gt;).&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>mealie-recipes/mealie</title>
      <link>https://github.com/mealie-recipes/mealie</link>
      <description>&lt;p&gt;Mealie is a self hosted recipe manager and meal planner with a RestAPI backend and a reactive frontend application built in Vue for a pleasant user experience for the whole family. Easily add recipes into your database by providing the url and mealie will automatically import the relevant data or add a family recipe with the UI editor&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="https://github.com/mealie-recipes/mealie/releases"&gt;&lt;img src="https://img.shields.io/github/v/release/mealie-recipes/mealie?style=flat-square&amp;amp;label=latest%20release" alt="Latest Release" /&gt;&lt;/a&gt; &lt;a href="https://github.com/mealie-recipes/mealie/graphs/contributors"&gt;&lt;img src="https://img.shields.io/github/contributors/mealie-recipes/mealie.svg?style=flat-square" alt="Contributors" /&gt;&lt;/a&gt; &lt;a href="https://github.com/mealie-recipes/mealie/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/mealie-recipes/mealie.svg?style=flat-square" alt="Stargazers" /&gt;&lt;/a&gt; &lt;a href="https://github.com/mealie-recipes/mealie/issues"&gt;&lt;img src="https://img.shields.io/github/issues/mealie-recipes/mealie.svg?style=flat-square" alt="Issues" /&gt;&lt;/a&gt; &lt;a href="https://github.com/mealie-recipes/mealie/raw/mealie-next/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/mealie-recipes/mealie.svg?style=flat-square" alt="AGPL License" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/hkotel/mealie"&gt;&lt;img src="https://img.shields.io/docker/pulls/hkotel/mealie?style=flat-square" alt="Docker Pulls" /&gt;&lt;/a&gt; &lt;a href="https://github.com/mealie-recipes/mealie/pkgs/container/mealie"&gt;&lt;img src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fipitio.github.io%2Fbackage%2Fmealie-recipes%2Fmealie%2Fmealie.json&amp;amp;query=%24.downloads&amp;amp;style=flat-square&amp;amp;label=ghcr%20pulls" alt="GHCR Pulls" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;!-- PROJECT LOGO --&gt; 
&lt;br /&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/mealie-recipes/mealie"&gt; 
  &lt;svg style="width:100px;height:100px" viewbox="0 0 24 24"&gt; 
   &lt;path fill="currentColor" d="M8.1,13.34L3.91,9.16C2.35,7.59 2.35,5.06 3.91,3.5L10.93,10.5L8.1,13.34M13.41,13L20.29,19.88L18.88,21.29L12,14.41L5.12,21.29L3.71,19.88L13.36,10.22L13.16,10C12.38,9.23 12.38,7.97 13.16,7.19L17.5,2.82L18.43,3.74L15.19,7L16.15,7.94L19.39,4.69L20.31,5.61L17.06,8.85L18,9.81L21.26,6.56L22.18,7.5L17.81,11.84C17.03,12.62 15.77,12.62 15,11.84L14.78,11.64L13.41,13Z" /&gt; 
  &lt;/svg&gt; &lt;/a&gt; &lt;/p&gt;
&lt;h3 align="center"&gt;Mealie&lt;/h3&gt; 
&lt;p align="center"&gt; A Place For All Your Recipes &lt;br /&gt; &lt;a href="https://docs.mealie.io/"&gt;&lt;strong&gt;Explore the docs ¬ª&lt;/strong&gt;&lt;/a&gt; &lt;a href="https://github.com/mealie-recipes/mealie"&gt; &lt;/a&gt; &lt;br /&gt; &lt;a href="https://demo.mealie.io/"&gt;View Demo&lt;/a&gt; ¬∑ &lt;a href="https://github.com/mealie-recipes/mealie/issues"&gt;Report Bug&lt;/a&gt; ¬∑ &lt;a href="https://github.com/mealie-recipes/mealie/pkgs/container/mealie"&gt;GitHub Container Registry&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;&lt;a href="https://docs.mealie.io"&gt;&lt;img src="https://raw.githubusercontent.com/mealie-recipes/mealie/mealie-next/docs/docs/assets/img/home_screenshot.png" alt="Product Name Screen Shot" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;About The Project&lt;/h1&gt; 
&lt;p&gt;Mealie is a self hosted recipe manager, meal planner and shopping list with a RestAPI backend and a reactive frontend built in Vue for a pleasant user experience for the whole family. Easily add recipes into your database by providing the URL and Mealie will automatically import the relevant data, or add a family recipe with the UI editor. Mealie also provides an API for interactions from 3rd party applications.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://discord.gg/QuStdQGSGK"&gt;Remember to join the Discord&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.mealie.io/"&gt;Documentation&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Recipe imports: Create recipes, by &lt;strong&gt;importing from a URL&lt;/strong&gt; or entering data manually&lt;/li&gt; 
 &lt;li&gt;Meal Planner: Use the &lt;strong&gt;Meal Planner&lt;/strong&gt; to plan your what you'll cook for the next week&lt;/li&gt; 
 &lt;li&gt;Shopping List: Put the necessary ingredients on your &lt;strong&gt;Shopping List&lt;/strong&gt;, organised into sections of your local supermarket&lt;/li&gt; 
 &lt;li&gt;Cookbooks: Group recipes into &lt;strong&gt;Cookbooks&lt;/strong&gt; based on your own criteria&lt;/li&gt; 
 &lt;li&gt;Docker: Easy &lt;strong&gt;Docker&lt;/strong&gt; deployment&lt;/li&gt; 
 &lt;li&gt;Localisation: &lt;strong&gt;Translations&lt;/strong&gt; for 35+ languages&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- CONTRIBUTING --&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions are what make the open source community such an amazing place to learn, inspire, and create. Any contributions you make are &lt;strong&gt;greatly appreciated&lt;/strong&gt;. If you're going to be working on the code-base, you'll want to use the nightly documentation to ensure you get the latest information.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;See the &lt;a href="https://nightly.mealie.io/contributors/developers-guide/code-contributions/"&gt;Contributors Guide&lt;/a&gt; for help getting started.&lt;/li&gt; 
 &lt;li&gt;We use &lt;a href="https://code.visualstudio.com/docs/remote/containers"&gt;VSCode Dev Containers&lt;/a&gt; to make it easy for contributors to get started!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If you are not a coder, you can still contribute financially. Financial contributions help me prioritize working on this project over others and helps me know that there is a real demand for project development.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.buymeacoffee.com/haykot" target="_blank"&gt;&lt;img src="https://cdn.buymeacoffee.com/buttons/v2/default-green.png" alt="Buy Me A Coffee" style="height: 30px !important;width: 107px !important;" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Translations&lt;/h3&gt; 
&lt;p&gt;Translations can be a great way for &lt;strong&gt;non-coders&lt;/strong&gt; to contribute to the project. We use &lt;a href="https://crowdin.com/project/mealie"&gt;Crowdin&lt;/a&gt; to allow several contributors to work on translating Mealie. You can simply help by voting for your preferred translations, or even by completely translating Mealie into a new language.&lt;/p&gt; 
&lt;p&gt;For more information, check out the translation page on the &lt;a href="https://nightly.mealie.io/contributors/translating/"&gt;contributor's guide&lt;/a&gt;.&lt;/p&gt; 
&lt;!-- LICENSE --&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Distributed under the AGPL License. See &lt;code&gt;LICENSE&lt;/code&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;Sponsors&lt;/h2&gt; 
&lt;p&gt;Huge thanks to all the sponsors of this project on &lt;a href="https://github.com/sponsors/hay-kot"&gt;Github Sponsors&lt;/a&gt; and Buy Me a Coffee. Without you, this project would surely not be possible.&lt;/p&gt; 
&lt;p&gt;Thanks to Depot for providing build instances for our Docker image builds.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://depot.dev?utm_source=Mealie"&gt;&lt;img src="https://depot.dev/badges/built-with-depot.svg?sanitize=true" alt="Built with Depot" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;!-- MARKDOWN LINKS &amp; IMAGES --&gt; 
&lt;!-- https://www.markdownguide.org/basic-syntax/#reference-style-links --&gt;</description>
    </item>
    
  </channel>
</rss>