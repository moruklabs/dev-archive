<rss version="2.0">
  <channel>
    <title>GitHub Python Daily Trending</title>
    <description>Daily Trending of Python in GitHub</description>
    <pubDate>Wed, 07 Jan 2026 01:41:31 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>ahujasid/blender-mcp</title>
      <link>https://github.com/ahujasid/blender-mcp</link>
      <description>&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;BlenderMCP - Blender Model Context Protocol Integration&lt;/h1&gt; 
&lt;p&gt;BlenderMCP connects Blender to Claude AI through the Model Context Protocol (MCP), allowing Claude to directly interact with and control Blender. This integration enables prompt assisted 3D modeling, scene creation, and manipulation.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;We have no official website. Any website you see online is unofficial and has no affiliation with this project. Use them at your own risk.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=lCyQ717DuzQ"&gt;Full tutorial&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Join the Community&lt;/h3&gt; 
&lt;p&gt;Give feedback, get inspired, and build on top of the MCP: &lt;a href="https://discord.gg/z5apgR8TFU"&gt;Discord&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Supporters&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://www.coderabbit.ai/"&gt;CodeRabbit&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/satishgoda"&gt;Satish Goda&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;All supporters:&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/sponsors/ahujasid"&gt;Support this project&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Release notes (1.4.0)&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Added Hunyuan3D support&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Previously added features:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;View screenshots for Blender viewport to better understand the scene&lt;/li&gt; 
 &lt;li&gt;Search and download Sketchfab models&lt;/li&gt; 
 &lt;li&gt;Support for Poly Haven assets through their API&lt;/li&gt; 
 &lt;li&gt;Support to generate 3D models using Hyper3D Rodin&lt;/li&gt; 
 &lt;li&gt;Run Blender MCP on a remote host&lt;/li&gt; 
 &lt;li&gt;Telemetry for tools executed (completely anonymous)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Installating a new version (existing users)&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;For newcomers, you can go straight to Installation. For existing users, see the points below&lt;/li&gt; 
 &lt;li&gt;Download the latest addon.py file and replace the older one, then add it to Blender&lt;/li&gt; 
 &lt;li&gt;Delete the MCP server from Claude and add it back again, and you should be good to go!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Two-way communication&lt;/strong&gt;: Connect Claude AI to Blender through a socket-based server&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Object manipulation&lt;/strong&gt;: Create, modify, and delete 3D objects in Blender&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Material control&lt;/strong&gt;: Apply and modify materials and colors&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Scene inspection&lt;/strong&gt;: Get detailed information about the current Blender scene&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Code execution&lt;/strong&gt;: Run arbitrary Python code in Blender from Claude&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Components&lt;/h2&gt; 
&lt;p&gt;The system consists of two main components:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Blender Addon (&lt;code&gt;addon.py&lt;/code&gt;)&lt;/strong&gt;: A Blender addon that creates a socket server within Blender to receive and execute commands&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MCP Server (&lt;code&gt;src/blender_mcp/server.py&lt;/code&gt;)&lt;/strong&gt;: A Python server that implements the Model Context Protocol and connects to the Blender addon&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Blender 3.0 or newer&lt;/li&gt; 
 &lt;li&gt;Python 3.10 or newer&lt;/li&gt; 
 &lt;li&gt;uv package manager:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;If you're on Mac, please install uv as&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;brew install uv
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;On Windows&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;powershell -c "irm https://astral.sh/uv/install.ps1 | iex" 
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;and then add uv to the user path in Windows (you may need to restart Claude Desktop after):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;$localBin = "$env:USERPROFILE\.local\bin"
$userPath = [Environment]::GetEnvironmentVariable("Path", "User")
[Environment]::SetEnvironmentVariable("Path", "$userPath;$localBin", "User")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Otherwise installation instructions are on their website: &lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;Install uv&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;‚ö†Ô∏è Do not proceed before installing UV&lt;/strong&gt;&lt;/p&gt; 
&lt;h3&gt;Environment Variables&lt;/h3&gt; 
&lt;p&gt;The following environment variables can be used to configure the Blender connection:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;BLENDER_HOST&lt;/code&gt;: Host address for Blender socket server (default: "localhost")&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;BLENDER_PORT&lt;/code&gt;: Port number for Blender socket server (default: 9876)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export BLENDER_HOST='host.docker.internal'
export BLENDER_PORT=9876
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Claude for Desktop Integration&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=neoK_WMq92g"&gt;Watch the setup instruction video&lt;/a&gt; (Assuming you have already installed uv)&lt;/p&gt; 
&lt;p&gt;Go to Claude &amp;gt; Settings &amp;gt; Developer &amp;gt; Edit Config &amp;gt; claude_desktop_config.json to include the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
    "mcpServers": {
        "blender": {
            "command": "uvx",
            "args": [
                "blender-mcp"
            ]
        }
    }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Cursor integration&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://cursor.com/install-mcp?name=blender&amp;amp;config=eyJjb21tYW5kIjoidXZ4IGJsZW5kZXItbWNwIn0%3D"&gt;&lt;img src="https://cursor.com/deeplink/mcp-install-dark.svg?sanitize=true" alt="Install MCP Server" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;For Mac users, go to Settings &amp;gt; MCP and paste the following&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;To use as a global server, use "add new global MCP server" button and paste&lt;/li&gt; 
 &lt;li&gt;To use as a project specific server, create &lt;code&gt;.cursor/mcp.json&lt;/code&gt; in the root of the project and paste&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
    "mcpServers": {
        "blender": {
            "command": "uvx",
            "args": [
                "blender-mcp"
            ]
        }
    }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For Windows users, go to Settings &amp;gt; MCP &amp;gt; Add Server, add a new server with the following settings:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
    "mcpServers": {
        "blender": {
            "command": "cmd",
            "args": [
                "/c",
                "uvx",
                "blender-mcp"
            ]
        }
    }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=wgWsJshecac"&gt;Cursor setup video&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;‚ö†Ô∏è Only run one instance of the MCP server (either on Cursor or Claude Desktop), not both&lt;/strong&gt;&lt;/p&gt; 
&lt;h3&gt;Visual Studio Code Integration&lt;/h3&gt; 
&lt;p&gt;&lt;em&gt;Prerequisites&lt;/em&gt;: Make sure you have &lt;a href="https://code.visualstudio.com/"&gt;Visual Studio Code&lt;/a&gt; installed before proceeding.&lt;/p&gt; 
&lt;p&gt;&lt;a href="vscode:mcp/install?%7B%22name%22%3A%22blender-mcp%22%2C%22type%22%3A%22stdio%22%2C%22command%22%3A%22uvx%22%2C%22args%22%3A%5B%22blender-mcp%22%5D%7D"&gt;&lt;img src="https://img.shields.io/badge/VS_Code-Install_blender--mcp_server-0098FF?style=flat-square&amp;amp;logo=visualstudiocode&amp;amp;logoColor=ffffff" alt="Install in VS Code" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Installing the Blender Addon&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Download the &lt;code&gt;addon.py&lt;/code&gt; file from this repo&lt;/li&gt; 
 &lt;li&gt;Open Blender&lt;/li&gt; 
 &lt;li&gt;Go to Edit &amp;gt; Preferences &amp;gt; Add-ons&lt;/li&gt; 
 &lt;li&gt;Click "Install..." and select the &lt;code&gt;addon.py&lt;/code&gt; file&lt;/li&gt; 
 &lt;li&gt;Enable the addon by checking the box next to "Interface: Blender MCP"&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Starting the Connection&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/ahujasid/blender-mcp/main/assets/addon-instructions.png" alt="BlenderMCP in the sidebar" /&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;In Blender, go to the 3D View sidebar (press N if not visible)&lt;/li&gt; 
 &lt;li&gt;Find the "BlenderMCP" tab&lt;/li&gt; 
 &lt;li&gt;Turn on the Poly Haven checkbox if you want assets from their API (optional)&lt;/li&gt; 
 &lt;li&gt;Click "Connect to Claude"&lt;/li&gt; 
 &lt;li&gt;Make sure the MCP server is running in your terminal&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Using with Claude&lt;/h3&gt; 
&lt;p&gt;Once the config file has been set on Claude, and the addon is running on Blender, you will see a hammer icon with tools for the Blender MCP.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/ahujasid/blender-mcp/main/assets/hammer-icon.png" alt="BlenderMCP in the sidebar" /&gt;&lt;/p&gt; 
&lt;h4&gt;Capabilities&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Get scene and object information&lt;/li&gt; 
 &lt;li&gt;Create, delete and modify shapes&lt;/li&gt; 
 &lt;li&gt;Apply or create materials for objects&lt;/li&gt; 
 &lt;li&gt;Execute any Python code in Blender&lt;/li&gt; 
 &lt;li&gt;Download the right models, assets and HDRIs through &lt;a href="https://polyhaven.com/"&gt;Poly Haven&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;AI generated 3D models through &lt;a href="https://hyper3d.ai/"&gt;Hyper3D Rodin&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Example Commands&lt;/h3&gt; 
&lt;p&gt;Here are some examples of what you can ask Claude to do:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;"Create a low poly scene in a dungeon, with a dragon guarding a pot of gold" &lt;a href="https://www.youtube.com/watch?v=DqgKuLYUv00"&gt;Demo&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;"Create a beach vibe using HDRIs, textures, and models like rocks and vegetation from Poly Haven" &lt;a href="https://www.youtube.com/watch?v=I29rn92gkC4"&gt;Demo&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Give a reference image, and create a Blender scene out of it &lt;a href="https://www.youtube.com/watch?v=FDRb03XPiRo"&gt;Demo&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;"Generate a 3D model of a garden gnome through Hyper3D"&lt;/li&gt; 
 &lt;li&gt;"Get information about the current scene, and make a threejs sketch from it" &lt;a href="https://www.youtube.com/watch?v=jxbNI5L7AH8"&gt;Demo&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;"Make this car red and metallic"&lt;/li&gt; 
 &lt;li&gt;"Create a sphere and place it above the cube"&lt;/li&gt; 
 &lt;li&gt;"Make the lighting like a studio"&lt;/li&gt; 
 &lt;li&gt;"Point the camera at the scene, and make it isometric"&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Hyper3D integration&lt;/h2&gt; 
&lt;p&gt;Hyper3D's free trial key allows you to generate a limited number of models per day. If the daily limit is reached, you can wait for the next day's reset or obtain your own key from hyper3d.ai and fal.ai.&lt;/p&gt; 
&lt;h2&gt;Troubleshooting&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Connection issues&lt;/strong&gt;: Make sure the Blender addon server is running, and the MCP server is configured on Claude, DO NOT run the uvx command in the terminal. Sometimes, the first command won't go through but after that it starts working.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Timeout errors&lt;/strong&gt;: Try simplifying your requests or breaking them into smaller steps&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Poly Haven integration&lt;/strong&gt;: Claude is sometimes erratic with its behaviour&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Have you tried turning it off and on again?&lt;/strong&gt;: If you're still having connection errors, try restarting both Claude and the Blender server&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Technical Details&lt;/h2&gt; 
&lt;h3&gt;Communication Protocol&lt;/h3&gt; 
&lt;p&gt;The system uses a simple JSON-based protocol over TCP sockets:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Commands&lt;/strong&gt; are sent as JSON objects with a &lt;code&gt;type&lt;/code&gt; and optional &lt;code&gt;params&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Responses&lt;/strong&gt; are JSON objects with a &lt;code&gt;status&lt;/code&gt; and &lt;code&gt;result&lt;/code&gt; or &lt;code&gt;message&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Limitations &amp;amp; Security Considerations&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;The &lt;code&gt;execute_blender_code&lt;/code&gt; tool allows running arbitrary Python code in Blender, which can be powerful but potentially dangerous. Use with caution in production environments. ALWAYS save your work before using it.&lt;/li&gt; 
 &lt;li&gt;Poly Haven requires downloading models, textures, and HDRI images. If you do not want to use it, please turn it off in the checkbox in Blender.&lt;/li&gt; 
 &lt;li&gt;Complex operations might need to be broken down into smaller steps&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions are welcome! Please feel free to submit a Pull Request.&lt;/p&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;This is a third-party integration and not made by Blender. Made by &lt;a href="https://x.com/sidahuj"&gt;Siddharth&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>virattt/ai-hedge-fund</title>
      <link>https://github.com/virattt/ai-hedge-fund</link>
      <description>&lt;p&gt;An AI Hedge Fund Team&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AI Hedge Fund&lt;/h1&gt; 
&lt;p&gt;This is a proof of concept for an AI-powered hedge fund. The goal of this project is to explore the use of AI to make trading decisions. This project is for &lt;strong&gt;educational&lt;/strong&gt; purposes only and is not intended for real trading or investment.&lt;/p&gt; 
&lt;p&gt;This system employs several agents working together:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Aswath Damodaran Agent - The Dean of Valuation, focuses on story, numbers, and disciplined valuation&lt;/li&gt; 
 &lt;li&gt;Ben Graham Agent - The godfather of value investing, only buys hidden gems with a margin of safety&lt;/li&gt; 
 &lt;li&gt;Bill Ackman Agent - An activist investor, takes bold positions and pushes for change&lt;/li&gt; 
 &lt;li&gt;Cathie Wood Agent - The queen of growth investing, believes in the power of innovation and disruption&lt;/li&gt; 
 &lt;li&gt;Charlie Munger Agent - Warren Buffett's partner, only buys wonderful businesses at fair prices&lt;/li&gt; 
 &lt;li&gt;Michael Burry Agent - The Big Short contrarian who hunts for deep value&lt;/li&gt; 
 &lt;li&gt;Mohnish Pabrai Agent - The Dhandho investor, who looks for doubles at low risk&lt;/li&gt; 
 &lt;li&gt;Peter Lynch Agent - Practical investor who seeks "ten-baggers" in everyday businesses&lt;/li&gt; 
 &lt;li&gt;Phil Fisher Agent - Meticulous growth investor who uses deep "scuttlebutt" research&lt;/li&gt; 
 &lt;li&gt;Rakesh Jhunjhunwala Agent - The Big Bull of India&lt;/li&gt; 
 &lt;li&gt;Stanley Druckenmiller Agent - Macro legend who hunts for asymmetric opportunities with growth potential&lt;/li&gt; 
 &lt;li&gt;Warren Buffett Agent - The oracle of Omaha, seeks wonderful companies at a fair price&lt;/li&gt; 
 &lt;li&gt;Valuation Agent - Calculates the intrinsic value of a stock and generates trading signals&lt;/li&gt; 
 &lt;li&gt;Sentiment Agent - Analyzes market sentiment and generates trading signals&lt;/li&gt; 
 &lt;li&gt;Fundamentals Agent - Analyzes fundamental data and generates trading signals&lt;/li&gt; 
 &lt;li&gt;Technicals Agent - Analyzes technical indicators and generates trading signals&lt;/li&gt; 
 &lt;li&gt;Risk Manager - Calculates risk metrics and sets position limits&lt;/li&gt; 
 &lt;li&gt;Portfolio Manager - Makes final trading decisions and generates orders&lt;/li&gt; 
&lt;/ol&gt; 
&lt;img width="1042" alt="Screenshot 2025-03-22 at 6 19 07 PM" src="https://github.com/user-attachments/assets/cbae3dcf-b571-490d-b0ad-3f0f035ac0d4" /&gt; 
&lt;p&gt;Note: the system does not actually make any trades.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://twitter.com/virattt"&gt;&lt;img src="https://img.shields.io/twitter/follow/virattt?style=social" alt="Twitter Follow" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;This project is for &lt;strong&gt;educational and research purposes only&lt;/strong&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Not intended for real trading or investment&lt;/li&gt; 
 &lt;li&gt;No investment advice or guarantees provided&lt;/li&gt; 
 &lt;li&gt;Creator assumes no liability for financial losses&lt;/li&gt; 
 &lt;li&gt;Consult a financial advisor for investment decisions&lt;/li&gt; 
 &lt;li&gt;Past performance does not indicate future results&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;By using this software, you agree to use it solely for learning purposes.&lt;/p&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#how-to-install"&gt;How to Install&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#how-to-run"&gt;How to Run&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#%EF%B8%8F-command-line-interface"&gt;‚å®Ô∏è Command Line Interface&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#%EF%B8%8F-web-application"&gt;üñ•Ô∏è Web Application&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#how-to-contribute"&gt;How to Contribute&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#feature-requests"&gt;Feature Requests&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;How to Install&lt;/h2&gt; 
&lt;p&gt;Before you can run the AI Hedge Fund, you'll need to install it and set up your API keys. These steps are common to both the full-stack web application and command line interface.&lt;/p&gt; 
&lt;h3&gt;1. Clone the Repository&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/virattt/ai-hedge-fund.git
cd ai-hedge-fund
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Set up API keys&lt;/h3&gt; 
&lt;p&gt;Create a &lt;code&gt;.env&lt;/code&gt; file for your API keys:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Create .env file for your API keys (in the root directory)
cp .env.example .env
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Open and edit the &lt;code&gt;.env&lt;/code&gt; file to add your API keys:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# For running LLMs hosted by openai (gpt-4o, gpt-4o-mini, etc.)
OPENAI_API_KEY=your-openai-api-key

# For getting financial data to power the hedge fund
FINANCIAL_DATASETS_API_KEY=your-financial-datasets-api-key
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: You must set at least one LLM API key (e.g. &lt;code&gt;OPENAI_API_KEY&lt;/code&gt;, &lt;code&gt;GROQ_API_KEY&lt;/code&gt;, &lt;code&gt;ANTHROPIC_API_KEY&lt;/code&gt;, or &lt;code&gt;DEEPSEEK_API_KEY&lt;/code&gt;) for the hedge fund to work.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Financial Data&lt;/strong&gt;: Data for AAPL, GOOGL, MSFT, NVDA, and TSLA is free and does not require an API key. For any other ticker, you will need to set the &lt;code&gt;FINANCIAL_DATASETS_API_KEY&lt;/code&gt; in the .env file.&lt;/p&gt; 
&lt;h2&gt;How to Run&lt;/h2&gt; 
&lt;h3&gt;‚å®Ô∏è Command Line Interface&lt;/h3&gt; 
&lt;p&gt;You can run the AI Hedge Fund directly via terminal. This approach offers more granular control and is useful for automation, scripting, and integration purposes.&lt;/p&gt; 
&lt;img width="992" alt="Screenshot 2025-01-06 at 5 50 17 PM" src="https://github.com/user-attachments/assets/e8ca04bf-9989-4a7d-a8b4-34e04666663b" /&gt; 
&lt;h4&gt;Quick Start&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install Poetry (if not already installed):&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -sSL https://install.python-poetry.org | python3 -
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Install dependencies:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Run the AI Hedge Fund&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry run python src/main.py --ticker AAPL,MSFT,NVDA
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also specify a &lt;code&gt;--ollama&lt;/code&gt; flag to run the AI hedge fund using local LLMs.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry run python src/main.py --ticker AAPL,MSFT,NVDA --ollama
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can optionally specify the start and end dates to make decisions over a specific time period.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry run python src/main.py --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Run the Backtester&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Example Output:&lt;/strong&gt; &lt;img width="941" alt="Screenshot 2025-01-06 at 5 47 52 PM" src="https://github.com/user-attachments/assets/00e794ea-8628-44e6-9a84-8f8a31ad3b47" /&gt;&lt;/p&gt; 
&lt;p&gt;Note: The &lt;code&gt;--ollama&lt;/code&gt;, &lt;code&gt;--start-date&lt;/code&gt;, and &lt;code&gt;--end-date&lt;/code&gt; flags work for the backtester, as well!&lt;/p&gt; 
&lt;h3&gt;üñ•Ô∏è Web Application&lt;/h3&gt; 
&lt;p&gt;The new way to run the AI Hedge Fund is through our web application that provides a user-friendly interface. This is recommended for users who prefer visual interfaces over command line tools.&lt;/p&gt; 
&lt;p&gt;Please see detailed instructions on how to install and run the web application &lt;a href="https://github.com/virattt/ai-hedge-fund/tree/main/app"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;img width="1721" alt="Screenshot 2025-06-28 at 6 41 03‚ÄØPM" src="https://github.com/user-attachments/assets/b95ab696-c9f4-416c-9ad1-51feb1f5374b" /&gt; 
&lt;h2&gt;How to Contribute&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Fork the repository&lt;/li&gt; 
 &lt;li&gt;Create a feature branch&lt;/li&gt; 
 &lt;li&gt;Commit your changes&lt;/li&gt; 
 &lt;li&gt;Push to the branch&lt;/li&gt; 
 &lt;li&gt;Create a Pull Request&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: Please keep your pull requests small and focused. This will make it easier to review and merge.&lt;/p&gt; 
&lt;h2&gt;Feature Requests&lt;/h2&gt; 
&lt;p&gt;If you have a feature request, please open an &lt;a href="https://github.com/virattt/ai-hedge-fund/issues"&gt;issue&lt;/a&gt; and make sure it is tagged with &lt;code&gt;enhancement&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the MIT License - see the LICENSE file for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>isaac-sim/IsaacLab</title>
      <link>https://github.com/isaac-sim/IsaacLab</link>
      <description>&lt;p&gt;Unified framework for robot learning built on NVIDIA Isaac Sim&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/isaac-sim/IsaacLab/main/docs/source/_static/isaaclab.jpg" alt="Isaac Lab" /&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h1&gt;Isaac Lab&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://docs.isaacsim.omniverse.nvidia.com/latest/index.html"&gt;&lt;img src="https://img.shields.io/badge/IsaacSim-5.1.0-silver.svg?sanitize=true" alt="IsaacSim" /&gt;&lt;/a&gt; &lt;a href="https://docs.python.org/3/whatsnew/3.11.html"&gt;&lt;img src="https://img.shields.io/badge/python-3.11-blue.svg?sanitize=true" alt="Python" /&gt;&lt;/a&gt; &lt;a href="https://releases.ubuntu.com/22.04/"&gt;&lt;img src="https://img.shields.io/badge/platform-linux--64-orange.svg?sanitize=true" alt="Linux platform" /&gt;&lt;/a&gt; &lt;a href="https://www.microsoft.com/en-us/"&gt;&lt;img src="https://img.shields.io/badge/platform-windows--64-orange.svg?sanitize=true" alt="Windows platform" /&gt;&lt;/a&gt; &lt;a href="https://github.com/isaac-sim/IsaacLab/actions/workflows/pre-commit.yaml"&gt;&lt;img src="https://img.shields.io/github/actions/workflow/status/isaac-sim/IsaacLab/pre-commit.yaml?logo=pre-commit&amp;amp;logoColor=white&amp;amp;label=pre-commit&amp;amp;color=brightgreen" alt="pre-commit" /&gt;&lt;/a&gt; &lt;a href="https://github.com/isaac-sim/IsaacLab/actions/workflows/docs.yaml"&gt;&lt;img src="https://img.shields.io/github/actions/workflow/status/isaac-sim/IsaacLab/docs.yaml?label=docs&amp;amp;color=brightgreen" alt="docs status" /&gt;&lt;/a&gt; &lt;a href="https://opensource.org/licenses/BSD-3-Clause"&gt;&lt;img src="https://img.shields.io/badge/license-BSD--3-yellow.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://opensource.org/license/apache-2-0"&gt;&lt;img src="https://img.shields.io/badge/license-Apache--2.0-yellow.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Isaac Lab&lt;/strong&gt; is a GPU-accelerated, open-source framework designed to unify and simplify robotics research workflows, such as reinforcement learning, imitation learning, and motion planning. Built on &lt;a href="https://docs.isaacsim.omniverse.nvidia.com/latest/index.html"&gt;NVIDIA Isaac Sim&lt;/a&gt;, it combines fast and accurate physics and sensor simulation, making it an ideal choice for sim-to-real transfer in robotics.&lt;/p&gt; 
&lt;p&gt;Isaac Lab provides developers with a range of essential features for accurate sensor simulation, such as RTX-based cameras, LIDAR, or contact sensors. The framework's GPU acceleration enables users to run complex simulations and computations faster, which is key for iterative processes like reinforcement learning and data-intensive tasks. Moreover, Isaac Lab can run locally or be distributed across the cloud, offering flexibility for large-scale deployments.&lt;/p&gt; 
&lt;p&gt;A detailed description of Isaac Lab can be found in our &lt;a href="https://arxiv.org/abs/2511.04831"&gt;arXiv paper&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;p&gt;Isaac Lab offers a comprehensive set of tools and environments designed to facilitate robot learning:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Robots&lt;/strong&gt;: A diverse collection of robots, from manipulators, quadrupeds, to humanoids, with more than 16 commonly available models.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Environments&lt;/strong&gt;: Ready-to-train implementations of more than 30 environments, which can be trained with popular reinforcement learning frameworks such as RSL RL, SKRL, RL Games, or Stable Baselines. We also support multi-agent reinforcement learning.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Physics&lt;/strong&gt;: Rigid bodies, articulated systems, deformable objects&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Sensors&lt;/strong&gt;: RGB/depth/segmentation cameras, camera annotations, IMU, contact sensors, ray casters.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;h3&gt;Documentation&lt;/h3&gt; 
&lt;p&gt;Our &lt;a href="https://isaac-sim.github.io/IsaacLab"&gt;documentation page&lt;/a&gt; provides everything you need to get started, including detailed tutorials and step-by-step guides. Follow these links to learn more about:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://isaac-sim.github.io/IsaacLab/main/source/setup/installation/index.html#local-installation"&gt;Installation steps&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://isaac-sim.github.io/IsaacLab/main/source/overview/reinforcement-learning/rl_existing_scripts.html"&gt;Reinforcement learning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://isaac-sim.github.io/IsaacLab/main/source/tutorials/index.html"&gt;Tutorials&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://isaac-sim.github.io/IsaacLab/main/source/overview/environments.html"&gt;Available environments&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Isaac Sim Version Dependency&lt;/h2&gt; 
&lt;p&gt;Isaac Lab is built on top of Isaac Sim and requires specific versions of Isaac Sim that are compatible with each release of Isaac Lab. Below, we outline the recent Isaac Lab releases and GitHub branches and their corresponding dependency versions for Isaac Sim.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Isaac Lab Version&lt;/th&gt; 
   &lt;th&gt;Isaac Sim Version&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;main&lt;/code&gt; branch&lt;/td&gt; 
   &lt;td&gt;Isaac Sim 4.5 / 5.0 / 5.1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;v2.3.X&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Isaac Sim 4.5 / 5.0 / 5.1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;v2.2.X&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Isaac Sim 4.5 / 5.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;v2.1.X&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Isaac Sim 4.5&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;v2.0.X&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Isaac Sim 4.5&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Contributing to Isaac Lab&lt;/h2&gt; 
&lt;p&gt;We wholeheartedly welcome contributions from the community to make this framework mature and useful for everyone. These may happen as bug reports, feature requests, or code contributions. For details, please check our &lt;a href="https://isaac-sim.github.io/IsaacLab/main/source/refs/contributing.html"&gt;contribution guidelines&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Show &amp;amp; Tell: Share Your Inspiration&lt;/h2&gt; 
&lt;p&gt;We encourage you to utilize our &lt;a href="https://github.com/isaac-sim/IsaacLab/discussions/categories/show-and-tell"&gt;Show &amp;amp; Tell&lt;/a&gt; area in the &lt;code&gt;Discussions&lt;/code&gt; section of this repository. This space is designed for you to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Share the tutorials you've created&lt;/li&gt; 
 &lt;li&gt;Showcase your learning content&lt;/li&gt; 
 &lt;li&gt;Present exciting projects you've developed&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;By sharing your work, you'll inspire others and contribute to the collective knowledge of our community. Your contributions can spark new ideas and collaborations, fostering innovation in robotics and simulation.&lt;/p&gt; 
&lt;h2&gt;Troubleshooting&lt;/h2&gt; 
&lt;p&gt;Please see the &lt;a href="https://isaac-sim.github.io/IsaacLab/main/source/refs/troubleshooting.html"&gt;troubleshooting&lt;/a&gt; section for common fixes or &lt;a href="https://github.com/isaac-sim/IsaacLab/issues"&gt;submit an issue&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For issues related to Isaac Sim, we recommend checking its &lt;a href="https://docs.isaacsim.omniverse.nvidia.com/latest/index.html"&gt;documentation&lt;/a&gt; or opening a question on its &lt;a href="https://forums.developer.nvidia.com/c/agx-autonomous-machines/isaac/67"&gt;forums&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Support&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Please use GitHub &lt;a href="https://github.com/isaac-sim/IsaacLab/discussions"&gt;Discussions&lt;/a&gt; for discussing ideas, asking questions, and requests for new features.&lt;/li&gt; 
 &lt;li&gt;Github &lt;a href="https://github.com/isaac-sim/IsaacLab/issues"&gt;Issues&lt;/a&gt; should only be used to track executable pieces of work with a definite scope and a clear deliverable. These can be fixing bugs, documentation issues, new features, or general updates.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Connect with the NVIDIA Omniverse Community&lt;/h2&gt; 
&lt;p&gt;Do you have a project or resource you'd like to share more widely? We'd love to hear from you! Reach out to the NVIDIA Omniverse Community team at &lt;a href="mailto:OmniverseCommunity@nvidia.com"&gt;OmniverseCommunity@nvidia.com&lt;/a&gt; to explore opportunities to spotlight your work.&lt;/p&gt; 
&lt;p&gt;You can also join the conversation on the &lt;a href="https://discord.com/invite/nvidiaomniverse"&gt;Omniverse Discord&lt;/a&gt; to connect with other developers, share your projects, and help grow a vibrant, collaborative ecosystem where creativity and technology intersect. Your contributions can make a meaningful impact on the Isaac Lab community and beyond!&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;The Isaac Lab framework is released under &lt;a href="https://raw.githubusercontent.com/isaac-sim/IsaacLab/main/LICENSE"&gt;BSD-3 License&lt;/a&gt;. The &lt;code&gt;isaaclab_mimic&lt;/code&gt; extension and its corresponding standalone scripts are released under &lt;a href="https://raw.githubusercontent.com/isaac-sim/IsaacLab/main/LICENSE-mimic"&gt;Apache 2.0&lt;/a&gt;. The license files of its dependencies and assets are present in the &lt;a href="https://raw.githubusercontent.com/isaac-sim/IsaacLab/main/docs/licenses"&gt;&lt;code&gt;docs/licenses&lt;/code&gt;&lt;/a&gt; directory.&lt;/p&gt; 
&lt;p&gt;Note that Isaac Lab requires Isaac Sim, which includes components under proprietary licensing terms. Please see the &lt;a href="https://raw.githubusercontent.com/isaac-sim/IsaacLab/main/docs/licenses/dependencies/isaacsim-license.txt"&gt;Isaac Sim license&lt;/a&gt; for information on Isaac Sim licensing.&lt;/p&gt; 
&lt;p&gt;Note that the &lt;code&gt;isaaclab_mimic&lt;/code&gt; extension requires cuRobo, which has proprietary licensing terms that can be found in &lt;a href="https://raw.githubusercontent.com/isaac-sim/IsaacLab/main/docs/licenses/dependencies/cuRobo-license.txt"&gt;&lt;code&gt;docs/licenses/dependencies/cuRobo-license.txt&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you use Isaac Lab in your research, please cite the technical report:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@article{mittal2025isaaclab,
  title={Isaac Lab: A GPU-Accelerated Simulation Framework for Multi-Modal Robot Learning},
  author={Mayank Mittal and Pascal Roth and James Tigue and Antoine Richard and Octi Zhang and Peter Du and Antonio Serrano-Mu√±oz and Xinjie Yao and Ren√© Zurbr√ºgg and Nikita Rudin and Lukasz Wawrzyniak and Milad Rakhsha and Alain Denzler and Eric Heiden and Ales Borovicka and Ossama Ahmed and Iretiayo Akinola and Abrar Anwar and Mark T. Carlson and Ji Yuan Feng and Animesh Garg and Renato Gasoto and Lionel Gulich and Yijie Guo and M. Gussert and Alex Hansen and Mihir Kulkarni and Chenran Li and Wei Liu and Viktor Makoviychuk and Grzegorz Malczyk and Hammad Mazhar and Masoud Moghani and Adithyavairavan Murali and Michael Noseworthy and Alexander Poddubny and Nathan Ratliff and Welf Rehberg and Clemens Schwarke and Ritvik Singh and James Latham Smith and Bingjie Tang and Ruchik Thaker and Matthew Trepte and Karl Van Wyk and Fangzhou Yu and Alex Millane and Vikram Ramasamy and Remo Steiner and Sangeeta Subramanian and Clemens Volk and CY Chen and Neel Jawale and Ashwin Varghese Kuruttukulam and Michael A. Lin and Ajay Mandlekar and Karsten Patzwaldt and John Welsh and Huihua Zhao and Fatima Anes and Jean-Francois Lafleche and Nicolas Mo√´nne-Loccoz and Soowan Park and Rob Stepinski and Dirk Van Gelder and Chris Amevor and Jan Carius and Jumyung Chang and Anka He Chen and Pablo de Heras Ciechomski and Gilles Daviet and Mohammad Mohajerani and Julia von Muralt and Viktor Reutskyy and Michael Sauter and Simon Schirm and Eric L. Shi and Pierre Terdiman and Kenny Vilella and Tobias Widmer and Gordon Yeoman and Tiffany Chen and Sergey Grizan and Cathy Li and Lotus Li and Connor Smith and Rafael Wiltz and Kostas Alexis and Yan Chang and David Chu and Linxi "Jim" Fan and Farbod Farshidian and Ankur Handa and Spencer Huang and Marco Hutter and Yashraj Narang and Soha Pouya and Shiwei Sheng and Yuke Zhu and Miles Macklin and Adam Moravanszky and Philipp Reist and Yunrong Guo and David Hoeller and Gavriel State},
  journal={arXiv preprint arXiv:2511.04831},
  year={2025},
  url={https://arxiv.org/abs/2511.04831}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Acknowledgement&lt;/h2&gt; 
&lt;p&gt;Isaac Lab development initiated from the &lt;a href="https://isaac-orbit.github.io/"&gt;Orbit&lt;/a&gt; framework. We gratefully acknowledge the authors of Orbit for their foundational contributions.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>LuckyOne7777/ChatGPT-Micro-Cap-Experiment</title>
      <link>https://github.com/LuckyOne7777/ChatGPT-Micro-Cap-Experiment</link>
      <description>&lt;p&gt;This repo powers my experiment where ChatGPT manages a real-money micro-cap stock portfolio.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ChatGPT Micro-Cap Experiment&lt;/h1&gt; 
&lt;p&gt;Welcome to the repo behind my 6-month live trading experiment where ChatGPT manages a real-money micro-cap portfolio.&lt;/p&gt; 
&lt;h2&gt;Overview on getting started: &lt;a href="https://github.com/LuckyOne7777/ChatGPT-Micro-Cap-Experiment/raw/main/Start%20Your%20Own/README.md"&gt;Here&lt;/a&gt;&lt;/h2&gt; 
&lt;h2&gt;Repository Structure&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;trading_script.py&lt;/code&gt;&lt;/strong&gt; - Main trading engine with portfolio management and stop-loss automation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;Scripts and CSV Files/&lt;/code&gt;&lt;/strong&gt; - My personal portfolio (updates every trading day)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;Start Your Own/&lt;/code&gt;&lt;/strong&gt; - Template files and guide for starting your own experiment&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;Weekly Deep Research (MD|PDF)/&lt;/code&gt;&lt;/strong&gt; - Research summaries and performance reports&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;Experiment Details/&lt;/code&gt;&lt;/strong&gt; - Documentation, methodology, prompts, and Q&amp;amp;A&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;The Concept&lt;/h1&gt; 
&lt;p&gt;Every day, I kept seeing the same ad about having some A.I. pick undervalued stocks. It was obvious it was trying to get me to subscribe to some garbage, so I just rolled my eyes.&lt;br /&gt; Then I started wondering, "How well would that actually work?"&lt;/p&gt; 
&lt;p&gt;So, starting with just $100, I wanted to answer a simple but powerful question:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Can powerful large language models like ChatGPT actually generate alpha (or at least make smart trading decisions) using real-time data?&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Each trading day:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;I provide it trading data on the stocks in its portfolio.&lt;/li&gt; 
 &lt;li&gt;Strict stop-loss rules apply.&lt;/li&gt; 
 &lt;li&gt;Every week I allow it to use deep research to reevaluate its account.&lt;/li&gt; 
 &lt;li&gt;I track and publish performance data weekly on my blog: &lt;a href="https://nathanbsmith729.substack.com"&gt;Here&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Research &amp;amp; Documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/LuckyOne7777/ChatGPT-Micro-Cap-Experiment/raw/main/Experiment%20Details/Deep%20Research%20Index.md"&gt;Research Index&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/LuckyOne7777/ChatGPT-Micro-Cap-Experiment/raw/main/Experiment%20Details/Disclaimer.md"&gt;Disclaimer&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/LuckyOne7777/ChatGPT-Micro-Cap-Experiment/raw/main/Experiment%20Details/Q%26A.md"&gt;Q&amp;amp;A&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/LuckyOne7777/ChatGPT-Micro-Cap-Experiment/raw/main/Experiment%20Details/Prompts.md"&gt;Prompts&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/LuckyOne7777/ChatGPT-Micro-Cap-Experiment/raw/main/Start%20Your%20Own/README.md"&gt;Starting Your Own&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/LuckyOne7777/ChatGPT-Micro-Cap-Experiment/tree/main/Weekly%20Deep%20Research%20(MD)"&gt;Research Summaries (MD)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/LuckyOne7777/ChatGPT-Micro-Cap-Experiment/tree/main/Weekly%20Deep%20Research%20(PDF)"&gt;Full Deep Research Reports (PDF)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/LuckyOne7777/ChatGPT-Micro-Cap-Experiment/raw/main/Experiment%20Details/Chats.md"&gt;Chats&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Current Performance&lt;/h1&gt; 
&lt;!-- To update performance chart: 
     1. Replace the image file with updated results
     2. Update the dates and description below
     3. Update the "Last Updated" date --&gt; 
&lt;p&gt;&lt;strong&gt;Current Portfolio Results&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/LuckyOne7777/ChatGPT-Micro-Cap-Experiment/main/Results.png" alt="Latest Performance Results" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Current Status:&lt;/strong&gt; Portfolio is underperforming the S&amp;amp;P 500 benchmark&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Performance data is updated after each trading day. See the CSV files in &lt;code&gt;Scripts and CSV Files/&lt;/code&gt; for detailed daily tracking.&lt;/em&gt;&lt;/p&gt; 
&lt;h1&gt;Features of This Repo&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;Live trading scripts ‚Äî used to evaluate prices and update holdings daily&lt;/li&gt; 
 &lt;li&gt;LLM-powered decision engine ‚Äî ChatGPT picks the trades&lt;/li&gt; 
 &lt;li&gt;Performance tracking ‚Äî CSVs with daily PnL, total equity, and trade history&lt;/li&gt; 
 &lt;li&gt;Visualization tools ‚Äî Matplotlib graphs comparing ChatGPT vs. Index&lt;/li&gt; 
 &lt;li&gt;Logs &amp;amp; trade data ‚Äî auto-saved logs for transparency&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Want to Contribute?&lt;/h2&gt; 
&lt;p&gt;Contributions are very welcome! This project is community-oriented, and your help is invaluable.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Issues:&lt;/strong&gt; If you notice a bug or have an idea for improvement, please.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Pull Requests:&lt;/strong&gt; Feel free to submit a PR ‚Äî I usually review within a few days.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Collaboration:&lt;/strong&gt; High-value contributors may be invited as maintainers/admins to help shape the project‚Äôs future.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Whether it‚Äôs fixing a typo, adding features, or discussing new ideas, all contributions are appreciated!&lt;/p&gt; 
&lt;p&gt;For more information, check out: &lt;a href="https://github.com/LuckyOne7777/ChatGPT-Micro-Cap-Experiment/raw/main/Other/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Why This Matters&lt;/h1&gt; 
&lt;p&gt;AI is being hyped across every industry, but can it really manage money without guidance?&lt;/p&gt; 
&lt;p&gt;This project is an attempt to find out ‚Äî with transparency, data, and a real budget.&lt;/p&gt; 
&lt;h1&gt;Tech Stack &amp;amp; Features&lt;/h1&gt; 
&lt;h2&gt;Core Technologies&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Python&lt;/strong&gt; - Core scripting and automation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;pandas + yFinance&lt;/strong&gt; - Market data fetching and analysis&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Matplotlib&lt;/strong&gt; - Performance visualization and charting&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ChatGPT-5&lt;/strong&gt; - AI-powered trading decision engine&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Robust Data Sources&lt;/strong&gt; - Yahoo Finance primary, Stooq fallback for reliability&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Automated Stop-Loss&lt;/strong&gt; - Automatic position management with configurable stop-losses&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Interactive Trading&lt;/strong&gt; - Market-on-Open (MOO) and limit order support&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Backtesting Support&lt;/strong&gt; - ASOF_DATE override for historical analysis&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Performance Analytics&lt;/strong&gt; - CAPM analysis, Sharpe/Sortino ratios, drawdown metrics&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Trade Logging&lt;/strong&gt; - Complete transparency with detailed execution logs&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;System Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.11+&lt;/li&gt; 
 &lt;li&gt;Internet connection for market data&lt;/li&gt; 
 &lt;li&gt;~10MB storage for CSV data files&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Follow Along&lt;/h1&gt; 
&lt;p&gt;The experiment runs from June 2025 to December 2025.&lt;br /&gt; Every trading day I will update the portfolio CSV file.&lt;br /&gt; If you feel inspired to do something similar, feel free to use this as a blueprint.&lt;/p&gt; 
&lt;p&gt;Updates are posted weekly on my blog, more coming soon!&lt;/p&gt; 
&lt;p&gt;Blog: &lt;a href="https://nathanbsmith729.substack.com"&gt;A.I Controls Stock Account&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Have feature requests or any advice?&lt;/p&gt; 
&lt;p&gt;Please reach out here: &lt;strong&gt;&lt;a href="mailto:nathanbsmith.business@gmail.com"&gt;nathanbsmith.business@gmail.com&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>docling-project/docling</title>
      <link>https://github.com/docling-project/docling</link>
      <description>&lt;p&gt;Get your documents ready for gen AI&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://github.com/docling-project/docling"&gt; &lt;img loading="lazy" alt="Docling" src="https://github.com/docling-project/docling/raw/main/docs/assets/docling_processing.png" width="100%" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h1&gt;Docling&lt;/h1&gt; 
&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/12132" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/12132" alt="DS4SD%2Fdocling | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;&lt;a href="https://arxiv.org/abs/2408.09869"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2408.09869-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://docling-project.github.io/docling/"&gt;&lt;img src="https://img.shields.io/badge/docs-live-brightgreen" alt="Docs" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/docling/"&gt;&lt;img src="https://img.shields.io/pypi/v/docling" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/docling/"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/docling" alt="PyPI - Python Version" /&gt;&lt;/a&gt; &lt;a href="https://github.com/astral-sh/uv"&gt;&lt;img src="https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/uv/main/assets/badge/v0.json" alt="uv" /&gt;&lt;/a&gt; &lt;a href="https://github.com/astral-sh/ruff"&gt;&lt;img src="https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json" alt="Ruff" /&gt;&lt;/a&gt; &lt;a href="https://pydantic.dev"&gt;&lt;img src="https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/pydantic/pydantic/main/docs/badge/v2.json" alt="Pydantic v2" /&gt;&lt;/a&gt; &lt;a href="https://github.com/pre-commit/pre-commit"&gt;&lt;img src="https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&amp;amp;logoColor=white" alt="pre-commit" /&gt;&lt;/a&gt; &lt;a href="https://opensource.org/licenses/MIT"&gt;&lt;img src="https://img.shields.io/github/license/docling-project/docling" alt="License MIT" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/projects/docling"&gt;&lt;img src="https://static.pepy.tech/badge/docling/month" alt="PyPI Downloads" /&gt;&lt;/a&gt; &lt;a href="https://apify.com/vancura/docling"&gt;&lt;img src="https://apify.com/actor-badge?actor=vancura/docling?fpr=docling" alt="Docling Actor" /&gt;&lt;/a&gt; &lt;a href="https://app.dosu.dev/097760a8-135e-4789-8234-90c8837d7f1c/ask?utm_source=github"&gt;&lt;img src="https://dosu.dev/dosu-chat-badge.svg?sanitize=true" alt="Chat with Dosu" /&gt;&lt;/a&gt; &lt;a href="https://docling.ai/discord"&gt;&lt;img src="https://img.shields.io/discord/1399788921306746971?color=6A7EC2&amp;amp;logo=discord&amp;amp;logoColor=ffffff" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://www.bestpractices.dev/projects/10101"&gt;&lt;img src="https://www.bestpractices.dev/projects/10101/badge" alt="OpenSSF Best Practices" /&gt;&lt;/a&gt; &lt;a href="https://lfaidata.foundation/projects/"&gt;&lt;img src="https://img.shields.io/badge/LF%20AI%20%26%20Data-003778?logo=linuxfoundation&amp;amp;logoColor=fff&amp;amp;color=0094ff&amp;amp;labelColor=003778" alt="LF AI &amp;amp; Data" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Docling simplifies document processing, parsing diverse formats ‚Äî including advanced PDF understanding ‚Äî and providing seamless integrations with the gen AI ecosystem.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;üóÇÔ∏è Parsing of &lt;a href="https://docling-project.github.io/docling/usage/supported_formats/"&gt;multiple document formats&lt;/a&gt; incl. PDF, DOCX, PPTX, XLSX, HTML, WAV, MP3, VTT, images (PNG, TIFF, JPEG, ...), and more&lt;/li&gt; 
 &lt;li&gt;üìë Advanced PDF understanding incl. page layout, reading order, table structure, code, formulas, image classification, and more&lt;/li&gt; 
 &lt;li&gt;üß¨ Unified, expressive &lt;a href="https://docling-project.github.io/docling/concepts/docling_document/"&gt;DoclingDocument&lt;/a&gt; representation format&lt;/li&gt; 
 &lt;li&gt;‚Ü™Ô∏è Various &lt;a href="https://docling-project.github.io/docling/usage/supported_formats/"&gt;export formats&lt;/a&gt; and options, including Markdown, HTML, &lt;a href="https://arxiv.org/abs/2503.11576"&gt;DocTags&lt;/a&gt; and lossless JSON&lt;/li&gt; 
 &lt;li&gt;üîí Local execution capabilities for sensitive data and air-gapped environments&lt;/li&gt; 
 &lt;li&gt;ü§ñ Plug-and-play &lt;a href="https://docling-project.github.io/docling/integrations/"&gt;integrations&lt;/a&gt; incl. LangChain, LlamaIndex, Crew AI &amp;amp; Haystack for agentic AI&lt;/li&gt; 
 &lt;li&gt;üîç Extensive OCR support for scanned PDFs and images&lt;/li&gt; 
 &lt;li&gt;üëì Support of several Visual Language Models (&lt;a href="https://huggingface.co/ibm-granite/granite-docling-258M"&gt;GraniteDocling&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;üéôÔ∏è Audio support with Automatic Speech Recognition (ASR) models&lt;/li&gt; 
 &lt;li&gt;üîå Connect to any agent using the &lt;a href="https://docling-project.github.io/docling/usage/mcp/"&gt;MCP server&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üíª Simple and convenient CLI&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;What's new&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üì§ Structured &lt;a href="https://docling-project.github.io/docling/examples/extraction/"&gt;information extraction&lt;/a&gt; [üß™ beta]&lt;/li&gt; 
 &lt;li&gt;üìë New layout model (&lt;strong&gt;Heron&lt;/strong&gt;) by default, for faster PDF parsing&lt;/li&gt; 
 &lt;li&gt;üîå &lt;a href="https://docling-project.github.io/docling/usage/mcp/"&gt;MCP server&lt;/a&gt; for agentic applications&lt;/li&gt; 
 &lt;li&gt;üí¨ Parsing of Web Video Text Tracks (WebVTT) files&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Coming soon&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìù Metadata extraction, including title, authors, references &amp;amp; language&lt;/li&gt; 
 &lt;li&gt;üìù Chart understanding (Barchart, Piechart, LinePlot, etc)&lt;/li&gt; 
 &lt;li&gt;üìù Complex chemistry understanding (Molecular structures)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;To use Docling, simply install &lt;code&gt;docling&lt;/code&gt; from your package manager, e.g. pip:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install docling
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Works on macOS, Linux and Windows environments. Both x86_64 and arm64 architectures.&lt;/p&gt; 
&lt;p&gt;More &lt;a href="https://docling-project.github.io/docling/installation/"&gt;detailed installation instructions&lt;/a&gt; are available in the docs.&lt;/p&gt; 
&lt;h2&gt;Getting started&lt;/h2&gt; 
&lt;p&gt;To convert individual documents with python, use &lt;code&gt;convert()&lt;/code&gt;, for example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from docling.document_converter import DocumentConverter

source = "https://arxiv.org/pdf/2408.09869"  # document per local path or URL
converter = DocumentConverter()
result = converter.convert(source)
print(result.document.export_to_markdown())  # output: "## Docling Technical Report[...]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;More &lt;a href="https://docling-project.github.io/docling/usage/advanced_options/"&gt;advanced usage options&lt;/a&gt; are available in the docs.&lt;/p&gt; 
&lt;h2&gt;CLI&lt;/h2&gt; 
&lt;p&gt;Docling has a built-in CLI to run conversions.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docling https://arxiv.org/pdf/2206.01062
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also use ü•ö&lt;a href="https://huggingface.co/ibm-granite/granite-docling-258M"&gt;GraniteDocling&lt;/a&gt; and other VLMs via Docling CLI:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docling --pipeline vlm --vlm-model granite_docling https://arxiv.org/pdf/2206.01062
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will use MLX acceleration on supported Apple Silicon hardware.&lt;/p&gt; 
&lt;p&gt;Read more &lt;a href="https://docling-project.github.io/docling/usage/"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;Check out Docling's &lt;a href="https://docling-project.github.io/docling/"&gt;documentation&lt;/a&gt;, for details on installation, usage, concepts, recipes, extensions, and more.&lt;/p&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;p&gt;Go hands-on with our &lt;a href="https://docling-project.github.io/docling/examples/"&gt;examples&lt;/a&gt;, demonstrating how to address different application use cases with Docling.&lt;/p&gt; 
&lt;h2&gt;Integrations&lt;/h2&gt; 
&lt;p&gt;To further accelerate your AI application development, check out Docling's native &lt;a href="https://docling-project.github.io/docling/integrations/"&gt;integrations&lt;/a&gt; with popular frameworks and tools.&lt;/p&gt; 
&lt;h2&gt;Get help and support&lt;/h2&gt; 
&lt;p&gt;Please feel free to connect with us using the &lt;a href="https://github.com/docling-project/docling/discussions"&gt;discussion section&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Technical report&lt;/h2&gt; 
&lt;p&gt;For more details on Docling's inner workings, check out the &lt;a href="https://arxiv.org/abs/2408.09869"&gt;Docling Technical Report&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Please read &lt;a href="https://github.com/docling-project/docling/raw/main/CONTRIBUTING.md"&gt;Contributing to Docling&lt;/a&gt; for details.&lt;/p&gt; 
&lt;h2&gt;References&lt;/h2&gt; 
&lt;p&gt;If you use Docling in your projects, please consider citing the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bib"&gt;@techreport{Docling,
  author = {Deep Search Team},
  month = {8},
  title = {Docling Technical Report},
  url = {https://arxiv.org/abs/2408.09869},
  eprint = {2408.09869},
  doi = {10.48550/arXiv.2408.09869},
  version = {1.0.0},
  year = {2024}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;The Docling codebase is under MIT license. For individual model usage, please refer to the model licenses found in the original packages.&lt;/p&gt; 
&lt;h2&gt;LF AI &amp;amp; Data&lt;/h2&gt; 
&lt;p&gt;Docling is hosted as a project in the &lt;a href="https://lfaidata.foundation/projects/"&gt;LF AI &amp;amp; Data Foundation&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;IBM ‚ù§Ô∏è Open Source AI&lt;/h3&gt; 
&lt;p&gt;The project was started by the AI for knowledge team at IBM Research Zurich.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>OpenBB-finance/OpenBB</title>
      <link>https://github.com/OpenBB-finance/OpenBB</link>
      <description>&lt;p&gt;Financial data platform for analysts, quants and AI agents.&lt;/p&gt;&lt;hr&gt;&lt;br /&gt; 
&lt;img src="https://github.com/OpenBB-finance/OpenBB/raw/develop/images/odp-light.svg?raw=true#gh-light-mode-only" alt="Open Data Platform by OpenBB logo" width="600" /&gt; 
&lt;img src="https://github.com/OpenBB-finance/OpenBB/raw/develop/images/odp-dark.svg?raw=true#gh-dark-mode-only" alt="Open Data Platform by OpenBB logo" width="600" /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;a href="https://x.com/openbb_finance"&gt;&lt;img src="https://img.shields.io/twitter/url/https/twitter.com/openbb_finance.svg?style=social&amp;amp;label=Follow%20%40openbb_finance" alt="Twitter" /&gt;&lt;/a&gt; &lt;a href="https://discord.com/invite/xPHTuHCmuV"&gt;&lt;img src="https://img.shields.io/discord/831165782750789672" alt="Discord Shield" /&gt;&lt;/a&gt; &lt;a href="https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/OpenBB-finance/OpenBB"&gt;&lt;img src="https://img.shields.io/static/v1?label=Dev%20Containers&amp;amp;message=Open&amp;amp;color=blue&amp;amp;logo=visualstudiocode" alt="Open in Dev Containers" /&gt;&lt;/a&gt; &lt;a href="https://codespaces.new/OpenBB-finance/OpenBB"&gt; &lt;img src="https://github.com/codespaces/badge.svg?sanitize=true" height="20" /&gt; &lt;/a&gt; &lt;a target="_blank" href="https://colab.research.google.com/github/OpenBB-finance/OpenBB/blob/develop/examples/googleColab.ipynb"&gt; &lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt; &lt;/a&gt; &lt;a href="https://pypi.org/project/openbb/"&gt;&lt;img src="https://img.shields.io/pypi/v/openbb?color=blue&amp;amp;label=PyPI%20Package" alt="PyPI" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Open Data Platform by OpenBB (ODP) is the open-source toolset that helps data engineers integrate proprietary, licensed, and public data sources into downstream applications like AI copilots and research dashboards.&lt;/p&gt; 
&lt;p&gt;ODP operates as the "connect once, consume everywhere" infrastructure layer that consolidates and exposes data to multiple surfaces at once: Python environments for quants, OpenBB Workspace and Excel for analysts, MCP servers for AI agents, and REST APIs for other applications.&lt;/p&gt; 
&lt;a href="https://pro.openbb.co"&gt; 
 &lt;div align="center"&gt; 
  &lt;img src="https://openbb-cms.directus.app/assets/70b971ef-7a7e-486e-b5ae-1cc602f2162c.png" alt="Logo" width="1000" /&gt; 
 &lt;/div&gt; &lt;/a&gt; 
&lt;p&gt;Get started with: &lt;code&gt;pip install openbb&lt;/code&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openbb import obb
output = obb.equity.price.historical("AAPL")
df = output.to_dataframe()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Data integrations available can be found here: &lt;a href="https://docs.openbb.co/python/reference"&gt;https://docs.openbb.co/python/reference&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;OpenBB Workspace&lt;/h2&gt; 
&lt;p&gt;While the Open Data Platform provides the open-source data integration foundation, &lt;strong&gt;OpenBB Workspace&lt;/strong&gt; offers the enterprise UI for analysts to visualize datasets and leverage AI agents. The platform's "connect once, consume everywhere" architecture enables seamless integration between the two.&lt;/p&gt; 
&lt;p&gt;You can find OpenBB Workspace at &lt;a href="https://pro.openbb.co"&gt;https://pro.openbb.co&lt;/a&gt;. &lt;a href="https://pro.openbb.co"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;a href="https://pro.openbb.co"&gt; 
 &lt;div align="center"&gt; 
  &lt;img src="https://openbb-cms.directus.app/assets/f69b6aaf-0821-4bc8-a43c-715e03a924ef.png" alt="Logo" width="1000" /&gt; 
 &lt;/div&gt; &lt;/a&gt; 
&lt;p&gt;Data integration:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;You can learn more about adding data to the OpenBB workspace from the &lt;a href="https://docs.openbb.co/workspace"&gt;docs&lt;/a&gt; or &lt;a href="https://github.com/OpenBB-finance/backends-for-openbb"&gt;this open source repository&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;AI Agents integration:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;You can learn more about adding AI agents to the OpenBB workspace from &lt;a href="https://github.com/OpenBB-finance/agents-for-openbb"&gt;this open source repository&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Integrating Open Data Platform to the OpenBB Workspace&lt;/h3&gt; 
&lt;p&gt;Connect this library to the OpenBB Workspace with a few simple commands, in a Python (3.9.21 - 3.12) environment.&lt;/p&gt; 
&lt;h4&gt;Run an ODP backend&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Install the packages.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip install "openbb[all]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Start the API server over localhost.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;openbb-api
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will launch a FastAPI server, via Uvicorn, at &lt;code&gt;127.0.0.1:6900&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;You can check that it works by going to &lt;a href="http://127.0.0.1:6900"&gt;http://127.0.0.1:6900&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;Integrate the ODP Backend to OpenBB Workspace&lt;/h4&gt; 
&lt;p&gt;Sign-in to the &lt;a href="https://pro.openbb.co/"&gt;OpenBB Workspace&lt;/a&gt;, and follow the following steps:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/75cffb4a-5e95-470a-b9d0-6ffd4067e069" alt="CleanShot 2025-05-17 at 09 51 56@2x" /&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Go to the "Apps" tab&lt;/li&gt; 
 &lt;li&gt;Click on "Connect backend"&lt;/li&gt; 
 &lt;li&gt;Fill in the form with: Name: Open Data Platform URL: &lt;a href="http://127.0.0.1:6900"&gt;http://127.0.0.1:6900&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Click on "Test". You should get a "Test successful" with the number of apps found.&lt;/li&gt; 
 &lt;li&gt;Click on "Add".&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;That's it.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;!-- TABLE OF CONTENTS --&gt; 
&lt;details closed="closed"&gt; 
 &lt;summary&gt;&lt;h2 style="display: inline-block"&gt;Table of Contents&lt;/h2&gt;&lt;/summary&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#1-installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#2-contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#3-license"&gt;License&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#4-disclaimer"&gt;Disclaimer&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#5-contacts"&gt;Contacts&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#6-star-history"&gt;Star History&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#7-contributors"&gt;Contributors&lt;/a&gt;&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;h2&gt;1. Installation&lt;/h2&gt; 
&lt;p&gt;The ODP Python Package can be installed from &lt;a href="https://pypi.org/project/openbb/"&gt;PyPI package&lt;/a&gt; by running &lt;code&gt;pip install openbb&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;or by cloning the repository directly with &lt;code&gt;git clone https://github.com/OpenBB-finance/OpenBB.git&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Please find more about the installation process, in the &lt;a href="https://docs.openbb.co/python/installation"&gt;OpenBB Documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;ODP CLI installation&lt;/h3&gt; 
&lt;p&gt;The ODP CLI is a command-line interface that allows you to access the ODP directly from your command line.&lt;/p&gt; 
&lt;p&gt;It can be installed by running &lt;code&gt;pip install openbb-cli&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;or by cloning the repository directly with &lt;code&gt;git clone https://github.com/OpenBB-finance/OpenBB.git&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Please find more about the installation process in the &lt;a href="https://docs.openbb.co/cli/installation"&gt;OpenBB Documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;2. Contributing&lt;/h2&gt; 
&lt;p&gt;There are three main ways of contributing to this project. (Hopefully you have starred the project by now ‚≠êÔ∏è)&lt;/p&gt; 
&lt;h3&gt;Become a Contributor&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;More information on our &lt;a href="https://docs.openbb.co/python/developer"&gt;Developer Documentation&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Create a GitHub ticket&lt;/h3&gt; 
&lt;p&gt;Before creating a ticket make sure the one you are creating doesn't exist already &lt;a href="https://github.com/OpenBB-finance/OpenBB/issues"&gt;among the existing issues&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&amp;amp;labels=bug&amp;amp;template=bug_report.md&amp;amp;title=%5BBug%5D"&gt;Report bug&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&amp;amp;labels=enhancement&amp;amp;template=enhancement.md&amp;amp;title=%5BIMPROVE%5D"&gt;Suggest improvement&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&amp;amp;labels=new+feature&amp;amp;template=feature_request.md&amp;amp;title=%5BFR%5D"&gt;Request a feature&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Provide feedback&lt;/h3&gt; 
&lt;p&gt;We are most active on &lt;a href="https://openbb.co/discord"&gt;our Discord&lt;/a&gt;, but feel free to reach out to us in any of &lt;a href="https://openbb.co/links"&gt;our social media&lt;/a&gt; for feedback.&lt;/p&gt; 
&lt;h2&gt;3. License&lt;/h2&gt; 
&lt;p&gt;Distributed under the AGPLv3 License. See &lt;a href="https://github.com/OpenBB-finance/OpenBB/raw/main/LICENSE"&gt;LICENSE&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;4. Disclaimer&lt;/h2&gt; 
&lt;p&gt;Trading in financial instruments involves high risks including the risk of losing some, or all, of your investment amount, and may not be suitable for all investors.&lt;/p&gt; 
&lt;p&gt;Before deciding to trade in a financial instrument you should be fully informed of the risks and costs associated with trading the financial markets, carefully consider your investment objectives, level of experience, and risk appetite, and seek professional advice where needed.&lt;/p&gt; 
&lt;p&gt;The data contained in the Open Data Platform is not necessarily accurate.&lt;/p&gt; 
&lt;p&gt;OpenBB and any provider of the data contained in this website will not accept liability for any loss or damage as a result of your trading, or your reliance on the information displayed.&lt;/p&gt; 
&lt;p&gt;All names, logos, and brands of third parties that may be referenced in our sites, products or documentation are trademarks of their respective owners. Unless otherwise specified, OpenBB and its products and services are not endorsed by, sponsored by, or affiliated with these third parties.&lt;/p&gt; 
&lt;p&gt;Our use of these names, logos, and brands is for identification purposes only, and does not imply any such endorsement, sponsorship, or affiliation.&lt;/p&gt; 
&lt;h2&gt;5. Contacts&lt;/h2&gt; 
&lt;p&gt;If you have any questions about the platform or anything OpenBB, feel free to email us at &lt;code&gt;support@openbb.co&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;If you want to say hi, or are interested in partnering with us, feel free to reach us at &lt;code&gt;hello@openbb.co&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Any of our social media platforms: &lt;a href="https://openbb.co/links"&gt;openbb.co/links&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;6. Star History&lt;/h2&gt; 
&lt;p&gt;This is a proxy of our growth and that we are just getting started.&lt;/p&gt; 
&lt;p&gt;But for more metrics important to us check &lt;a href="https://openbb.co/open"&gt;openbb.co/open&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://api.star-history.com/svg?repos=openbb-finance/OpenBB&amp;amp;type=Date&amp;amp;theme=dark"&gt;&lt;img src="https://api.star-history.com/svg?repos=openbb-finance/OpenBB&amp;amp;type=Date&amp;amp;theme=dark" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;7. Contributors&lt;/h2&gt; 
&lt;p&gt;OpenBB wouldn't be OpenBB without you. If we are going to disrupt financial industry, every contribution counts. Thank you for being part of this journey.&lt;/p&gt; 
&lt;a href="https://github.com/OpenBB-finance/OpenBB/graphs/contributors"&gt; &lt;img src="https://contributors-img.web.app/image?repo=OpenBB-finance/OpenBB" width="800" /&gt; &lt;/a&gt; 
&lt;!-- MARKDOWN LINKS &amp; IMAGES --&gt; 
&lt;!-- https://www.markdownguide.org/basic-syntax/#reference-style-links --&gt;</description>
    </item>
    
    <item>
      <title>Asabeneh/30-Days-Of-Python</title>
      <link>https://github.com/Asabeneh/30-Days-Of-Python</link>
      <description>&lt;p&gt;The 30 Days of Python programming challenge is a step-by-step guide to learn the Python programming language in 30 days. This challenge may take more than 100 days. Follow your own pace. These videos may help too: https://www.youtube.com/channel/UC7PNRuno1rzYPb1xLa4yktw&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;üêç 30 Days Of Python&lt;/h1&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;# Day&lt;/th&gt; 
   &lt;th align="center"&gt;Topics&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;01&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/readme.md"&gt;Introduction&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;02&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/02_Day_Variables_builtin_functions/02_variables_builtin_functions.md"&gt;Variables, Built-in Functions&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;03&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/03_Day_Operators/03_operators.md"&gt;Operators&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;04&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/04_Day_Strings/04_strings.md"&gt;Strings&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;05&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/05_Day_Lists/05_lists.md"&gt;Lists&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;06&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/06_Day_Tuples/06_tuples.md"&gt;Tuples&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;07&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/07_Day_Sets/07_sets.md"&gt;Sets&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;08&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/08_Day_Dictionaries/08_dictionaries.md"&gt;Dictionaries&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;09&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/09_Day_Conditionals/09_conditionals.md"&gt;Conditionals&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;10&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/10_Day_Loops/10_loops.md"&gt;Loops&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;11&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/11_Day_Functions/11_functions.md"&gt;Functions&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;12&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/12_Day_Modules/12_modules.md"&gt;Modules&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;13&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/13_Day_List_comprehension/13_list_comprehension.md"&gt;List Comprehension&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;14&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/14_Day_Higher_order_functions/14_higher_order_functions.md"&gt;Higher Order Functions&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;15&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/15_Day_Python_type_errors/15_python_type_errors.md"&gt;Python Type Errors&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/16_Day_Python_date_time/16_python_datetime.md"&gt;Python Date time&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;17&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/17_Day_Exception_handling/17_exception_handling.md"&gt;Exception Handling&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;18&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/18_Day_Regular_expressions/18_regular_expressions.md"&gt;Regular Expressions&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;19&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/19_Day_File_handling/19_file_handling.md"&gt;File Handling&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;20&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/20_Day_Python_package_manager/20_python_package_manager.md"&gt;Python Package Manager&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;21&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/21_Day_Classes_and_objects/21_classes_and_objects.md"&gt;Classes and Objects&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;22&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/22_Day_Web_scraping/22_web_scraping.md"&gt;Web Scraping&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;23&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/23_Day_Virtual_environment/23_virtual_environment.md"&gt;Virtual Environment&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;24&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/24_Day_Statistics/24_statistics.md"&gt;Statistics&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;25&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/25_Day_Pandas/25_pandas.md"&gt;Pandas&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;26&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/26_Day_Python_web/26_python_web.md"&gt;Python web&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;27&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/27_Day_Python_with_mongodb/27_python_with_mongodb.md"&gt;Python with MongoDB&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;28&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/28_Day_API/28_API.md"&gt;API&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;29&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/29_Day_Building_API/29_building_API.md"&gt;Building API&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;30&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/30_Day_Conclusions/30_conclusions.md"&gt;Conclusions&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;small&gt;üß°üß°üß° HAPPY CODING üß°üß°üß°&lt;/small&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;div&gt; 
 &lt;h2&gt;üíñ Sponsors&lt;/h2&gt; 
 &lt;p&gt;Our amazing sponsors for supporting my open-source contribution and the &lt;strong&gt;30 Days of Challenge&lt;/strong&gt; series!&lt;/p&gt; 
 &lt;h3&gt;Current Sponsors&lt;/h3&gt; 
 &lt;hr /&gt; 
 &lt;div align="center"&gt; 
  &lt;a href="https://ref.wisprflow.ai/MPMzRGE" target="_blank" rel="noopener noreferrer"&gt; 
   &lt;picture&gt; 
    &lt;!-- Dark mode --&gt; 
    &lt;source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/Asabeneh/asabeneh/master/images/Wispr_Flow-Logo-white.png" /&gt; 
    &lt;!-- Light mode (fallback) --&gt; 
    &lt;img src="https://raw.githubusercontent.com/Asabeneh/asabeneh/master/images/Wispr_Flow-logo.png" width="400px" alt="Wispr Flow Logo" title="Wispr Flow" /&gt; 
   &lt;/picture&gt; &lt;/a&gt; 
  &lt;h1&gt; &lt;a href="https://ref.wisprflow.ai/MPMzRGE" target="_blank" rel="noopener noreferrer"&gt; Talk to code, stay in the Flow. &lt;/a&gt; &lt;/h1&gt; 
  &lt;h2&gt; &lt;a href="https://ref.wisprflow.ai/MPMzRGE" target="_blank" rel="noopener noreferrer"&gt; Flow is built for devs who live in their tools. Speak and give more context, get better results. &lt;/a&gt; &lt;/h2&gt; 
 &lt;/div&gt; 
 &lt;hr /&gt; 
 &lt;div align="center"&gt; 
  &lt;a href="https://client.petrosky.io/aff.php?aff=402" target="_blank" rel="noopener noreferrer"&gt; 
   &lt;picture&gt; 
    &lt;!-- Dark mode --&gt; 
    &lt;source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/Asabeneh/asabeneh/master/images/petrosky-logo-white.png" /&gt; 
    &lt;!-- Light mode (fallback) --&gt; 
    &lt;img src="https://raw.githubusercontent.com/Asabeneh/asabeneh/master/images/petrosky-logo-black.png" width="400px" alt="Petrosky Logo" title="Petrosky" /&gt; 
   &lt;/picture&gt; &lt;/a&gt; 
  &lt;h1&gt; &lt;a href="https://client.petrosky.io/aff.php?aff=402" target="_blank" rel="noopener noreferrer"&gt; A hosting for your entire journey! &lt;/a&gt; &lt;/h1&gt; 
  &lt;h2&gt; &lt;a href="https://client.petrosky.io/aff.php?aff=402" target="_blank" rel="noopener noreferrer"&gt; Affordable VPS Hosting Services For All Your Needs &lt;/a&gt; &lt;/h2&gt; 
 &lt;/div&gt; 
 &lt;hr /&gt; 
 &lt;h3&gt;üôå Become a Sponsor&lt;/h3&gt; 
 &lt;p&gt;You can support this project by becoming a sponsor on &lt;strong&gt;&lt;a href="https://github.com/sponsors/asabeneh"&gt;GitHub Sponsors&lt;/a&gt;&lt;/strong&gt; or through &lt;a href="https://www.paypal.me/asabeneh"&gt;PayPal&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;Every contribution, big or small, makes a huge difference. Thank you for your support! üåü&lt;/p&gt; 
 &lt;hr /&gt; 
 &lt;div align="center"&gt; 
  &lt;h1&gt; 30 Days Of Python: Day 1 - Introduction&lt;/h1&gt; 
  &lt;a class="header-badge" target="_blank" href="https://www.linkedin.com/in/asabeneh/"&gt; &lt;img src="https://img.shields.io/badge/style--5eba00.svg?label=LinkedIn&amp;amp;logo=linkedin&amp;amp;style=social" /&gt; &lt;/a&gt; 
  &lt;a class="header-badge" target="_blank" href="https://twitter.com/Asabeneh"&gt; &lt;img alt="Twitter Follow" src="https://img.shields.io/twitter/follow/asabeneh?style=social" /&gt; &lt;/a&gt; 
  &lt;p&gt;&lt;sub&gt;Author: &lt;a href="https://www.linkedin.com/in/asabeneh/" target="_blank"&gt;Asabeneh Yetayeh&lt;/a&gt;&lt;br /&gt; &lt;small&gt; Second Edition: July, 2021&lt;/small&gt; &lt;/sub&gt;&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;p&gt;üáßüá∑ &lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/Portuguese/README.md"&gt;Portuguese&lt;/a&gt; üá®üá≥ &lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/Chinese/README.md"&gt;‰∏≠Êñá&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/02_Day_Variables_builtin_functions/02_variables_builtin_functions.md"&gt;Day 2 &amp;gt;&amp;gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/30DaysOfPython_banner3@2x.png" alt="30DaysOfPython" /&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#-30-days-of-python"&gt;üêç 30 Days Of Python&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#-become-a-sponsor"&gt;üôå Become a Sponsor&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#-day-1"&gt;üìò Day 1&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#welcome"&gt;Welcome&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#why-python-"&gt;Why Python ?&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#environment-setup"&gt;Environment Setup&lt;/a&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#installing-python"&gt;Installing Python&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#python-shell"&gt;Python Shell&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#installing-visual-studio-code"&gt;Installing Visual Studio Code&lt;/a&gt; 
       &lt;ul&gt; 
        &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#how-to-use-visual-studio-code"&gt;How to use visual studio code&lt;/a&gt;&lt;/li&gt; 
       &lt;/ul&gt; &lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#basic-python"&gt;Basic Python&lt;/a&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#python-syntax"&gt;Python Syntax&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#python-indentation"&gt;Python Indentation&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#comments"&gt;Comments&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#data-types"&gt;Data types&lt;/a&gt; 
       &lt;ul&gt; 
        &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#number"&gt;Number&lt;/a&gt;&lt;/li&gt; 
        &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#string"&gt;String&lt;/a&gt;&lt;/li&gt; 
        &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#booleans"&gt;Booleans&lt;/a&gt;&lt;/li&gt; 
        &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#list"&gt;List&lt;/a&gt;&lt;/li&gt; 
        &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#dictionary"&gt;Dictionary&lt;/a&gt;&lt;/li&gt; 
        &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#tuple"&gt;Tuple&lt;/a&gt;&lt;/li&gt; 
        &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#set"&gt;Set&lt;/a&gt;&lt;/li&gt; 
       &lt;/ul&gt; &lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#checking-data-types"&gt;Checking Data types&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#python-file"&gt;Python File&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#-exercises---day-1"&gt;üíª Exercises - Day 1&lt;/a&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#exercise-level-1"&gt;Exercise: Level 1&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#exercise-level-2"&gt;Exercise: Level 2&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#exercise-level-3"&gt;Exercise: Level 3&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h1&gt;üìò Day 1&lt;/h1&gt; 
 &lt;h2&gt;Welcome&lt;/h2&gt; 
 &lt;p&gt;&lt;strong&gt;Congratulations&lt;/strong&gt; for deciding to participate in a &lt;em&gt;30 days of Python&lt;/em&gt; programming challenge. In this challenge, you will learn everything you need to be a python programmer and the whole concept of programming. At the end of the challenge, you will get a &lt;em&gt;30DaysOfPython&lt;/em&gt; programming challenge certificate.&lt;/p&gt; 
 &lt;p&gt;If you would like to actively engage in the challenge, you may join the &lt;a href="https://t.me/ThirtyDaysOfPython"&gt;30DaysOfPython challenge&lt;/a&gt; telegram group.&lt;/p&gt; 
 &lt;h2&gt;Introduction&lt;/h2&gt; 
 &lt;p&gt;Python is a high-level programming language for general-purpose programming. It is an open source, interpreted, objected-oriented programming language. Python was created by a Dutch programmer, Guido van Rossum. The name of the Python programming language was derived from a British sketch comedy series, &lt;em&gt;Monty Python's Flying Circus&lt;/em&gt;. The first version was released on February 20, 1991. This 30 days of Python challenge will help you learn the latest version of Python, Python 3 step by step. The topics are broken down into 30 days, where each day contains several topics with easy-to-understand explanations, real-world examples, and many hands on exercises and projects.&lt;/p&gt; 
 &lt;p&gt;This challenge is designed for beginners and professionals who want to learn python programming language. It may take 30 to 100 days to complete the challenge. People who actively participate in the telegram group have a high probability of completing the challenge.&lt;/p&gt; 
 &lt;p&gt;This challenge is easy to read, written in conversational English, engaging, motivating and at the same time, it is very demanding. You need to allocate much time to finish this challenge. If you are a visual learner, you may get the video lesson on &lt;a href="https://www.youtube.com/channel/UC7PNRuno1rzYPb1xLa4yktw"&gt; Washera&lt;/a&gt; YouTube channel. You may start from &lt;a href="https://youtu.be/OCCWZheOesI"&gt;Python for Absolute Beginners video&lt;/a&gt;. Subscribe the channel, comment and ask questions on YouTube vidoes and be proactive, the author will eventually notice you.&lt;/p&gt; 
 &lt;p&gt;The author likes to hear your opinion about the challenge, share the author by expressing your thoughts about the 30DaysOfPython challenge. You can leave your testimonial on this &lt;a href="https://www.asabeneh.com/testimonials"&gt;link&lt;/a&gt;&lt;/p&gt; 
 &lt;h2&gt;Why Python ?&lt;/h2&gt; 
 &lt;p&gt;It is a programming language which is very close to human language and because of that, it is easy to learn and use. Python is used by various industries and companies (including Google). It has been used to develop web applications, desktop applications, system administration, and machine learning libraries. Python is a highly embraced language in the data science and machine learning community. I hope this is enough to convince you to start learning Python. Python is eating the world and you are killing it before it eats you.&lt;/p&gt; 
 &lt;h2&gt;Environment Setup&lt;/h2&gt; 
 &lt;h3&gt;Installing Python&lt;/h3&gt; 
 &lt;p&gt;To run a python script you need to install python. Let's &lt;a href="https://www.python.org/"&gt;download&lt;/a&gt; python. If your are a windows user. Click the button encircled in red.&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://www.python.org/"&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/installing_on_windows.png" alt="installing on Windows" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;If you are a macOS user. Click the button encircled in red.&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://www.python.org/"&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/installing_on_macOS.png" alt="installing on Windows" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;To check if python is installed write the following command on your device terminal.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;python --version
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/python_versio.png" alt="Python Version" /&gt;&lt;/p&gt; 
 &lt;p&gt;As you can see from the terminal, I am using &lt;em&gt;Python 3.7.5&lt;/em&gt; version at the moment. Your version of Python might be different from mine by but it should be 3.6 or above. If you mange to see the python version, well done. Python has been installed on your machine. Continue to the next section.&lt;/p&gt; 
 &lt;h3&gt;Python Shell&lt;/h3&gt; 
 &lt;p&gt;Python is an interpreted scripting language, so it does not need to be compiled. It means it executes the code line by line. Python comes with a &lt;em&gt;Python Shell (Python Interactive Shell)&lt;/em&gt;. It is used to execute a single python command and get the result.&lt;/p&gt; 
 &lt;p&gt;Python Shell waits for the Python code from the user. When you enter the code, it interprets the code and shows the result in the next line. Open your terminal or command prompt(cmd) and write:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;python
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/opening_python_shell.png" alt="Python Scripting Shell" /&gt;&lt;/p&gt; 
 &lt;p&gt;The Python interactive shell is opened and it is waiting for you to write Python code(Python script). You will write your Python script next to this symbol &amp;gt;&amp;gt;&amp;gt; and then click Enter. Let us write our very first script on the Python scripting shell.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/adding_on_python_shell.png" alt="Python script on Python shell" /&gt;&lt;/p&gt; 
 &lt;p&gt;Well done, you wrote your first Python script on Python interactive shell. How do we close the Python interactive shell ? To close the shell, next to this symbol &amp;gt;&amp;gt; write &lt;strong&gt;exit()&lt;/strong&gt; command and press Enter.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/exit_from_shell.png" alt="Exit from python shell" /&gt;&lt;/p&gt; 
 &lt;p&gt;Now, you know how to open the Python interactive shell and how to exit from it.&lt;/p&gt; 
 &lt;p&gt;Python will give you results if you write scripts that Python understands, if not it returns errors. Let's make a deliberate mistake and see what Python will return.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/invalid_syntax_error.png" alt="Invalid Syntax Error" /&gt;&lt;/p&gt; 
 &lt;p&gt;As you can see from the returned error, Python is so clever that it knows the mistake we made and which was &lt;em&gt;Syntax Error: invalid syntax&lt;/em&gt;. Using x as multiplication in Python is a syntax error because (x) is not a valid syntax in Python. Instead of (&lt;strong&gt;x&lt;/strong&gt;) we use asterisk (*) for multiplication. The returned error clearly shows what to fix.&lt;/p&gt; 
 &lt;p&gt;The process of identifying and removing errors from a program is called &lt;em&gt;debugging&lt;/em&gt;. Let us debug it by putting * in place of &lt;strong&gt;x&lt;/strong&gt;.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/fixing_syntax_error.png" alt="Fixing Syntax Error" /&gt;&lt;/p&gt; 
 &lt;p&gt;Our bug was fixed, the code ran and we got a result we were expecting. As a programmer you will see such kind of errors on daily basis. It is good to know how to debug. To be good at debugging you should understand what kind of errors you are facing. Some of the Python errors you may encounter are &lt;em&gt;SyntaxError&lt;/em&gt;, &lt;em&gt;IndexError&lt;/em&gt;, &lt;em&gt;NameError&lt;/em&gt;, &lt;em&gt;ModuleNotFoundError&lt;/em&gt;, &lt;em&gt;KeyError&lt;/em&gt;, &lt;em&gt;ImportError&lt;/em&gt;, &lt;em&gt;AttributeError&lt;/em&gt;, &lt;em&gt;TypeError&lt;/em&gt;, &lt;em&gt;ValueError&lt;/em&gt;, &lt;em&gt;ZeroDivisionError&lt;/em&gt; etc. We will see more about different Python &lt;strong&gt;&lt;em&gt;error types&lt;/em&gt;&lt;/strong&gt; in later sections.&lt;/p&gt; 
 &lt;p&gt;Let us practice more how to use Python interactive shell. Go to your terminal or command prompt and write the word &lt;strong&gt;python&lt;/strong&gt;.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/opening_python_shell.png" alt="Python Scripting Shell" /&gt;&lt;/p&gt; 
 &lt;p&gt;The Python interactive shell is opened. Let us do some basic mathematical operations (addition, subtraction, multiplication, division, modulus, exponential).&lt;/p&gt; 
 &lt;p&gt;Let us do some maths first before we write any Python code:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;2 + 3 is 5&lt;/li&gt; 
  &lt;li&gt;3 - 2 is 1&lt;/li&gt; 
  &lt;li&gt;3 * 2 is 6&lt;/li&gt; 
  &lt;li&gt;3 / 2 is 1.5&lt;/li&gt; 
  &lt;li&gt;3 ** 2 is the same as 3 * 3&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;In python we have the following additional operations:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;3 % 2 = 1 =&amp;gt; which means finding the remainder&lt;/li&gt; 
  &lt;li&gt;3 // 2 = 1 =&amp;gt; which means removing the remainder&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;Let us change the above mathematical expressions to Python code. The Python shell has been opened and let us write a comment at the very beginning of the shell.&lt;/p&gt; 
 &lt;p&gt;A &lt;em&gt;comment&lt;/em&gt; is a part of the code which is not executed by python. So we can leave some text in our code to make our code more readable. Python does not run the comment part. A comment in python starts with hash(#) symbol. This is how you write a comment in python&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt; # comment starts with hash
 # this is a python comment, because it starts with a (#) symbol
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/maths_on_python_shell.png" alt="Maths on python shell" /&gt;&lt;/p&gt; 
 &lt;p&gt;Before we move on to the next section, let us practice more on the Python interactive shell. Close the opened shell by writing &lt;em&gt;exit()&lt;/em&gt; on the shell and open it again and let us practice how to write text on the Python shell.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/writing_string_on_shell.png" alt="Writing String on python shell" /&gt;&lt;/p&gt; 
 &lt;h3&gt;Installing Visual Studio Code&lt;/h3&gt; 
 &lt;p&gt;The Python interactive shell is good to try and test small script codes but it will not be for a big project. In real work environment, developers use different code editors to write codes. In this 30 days of Python programming challenge we will use visual studio code. Visual studio code is a very popular open source text editor. I am a fan of vscode and I would recommend to &lt;a href="https://code.visualstudio.com/"&gt;download&lt;/a&gt; visual studio code, but if you are in favor of other editors, feel free to follow with what you have.&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://code.visualstudio.com/"&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/vscode.png" alt="Visual Studio Code" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;If you installed visual studio code, let us see how to use it. If you prefer a video, you can follow this Visual Studio Code for Python &lt;a href="https://www.youtube.com/watch?v=bn7Cx4z-vSo"&gt;Video tutorial&lt;/a&gt;&lt;/p&gt; 
 &lt;h4&gt;How to use visual studio code&lt;/h4&gt; 
 &lt;p&gt;Open the visual studio code by double clicking the visual studio icon. When you open it you will get this kind of interface. Try to interact with the labeled icons.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/vscode_ui.png" alt="Visual studio Code" /&gt;&lt;/p&gt; 
 &lt;p&gt;Create a folder named 30DaysOfPython on your desktop. Then open it using visual studio code.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/how_to_open_project_on_vscode.png" alt="Opening Project on Visual studio" /&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/opening_project.png" alt="Opening a project" /&gt;&lt;/p&gt; 
 &lt;p&gt;After opening it you will see shortcuts for creating files and folders inside of 30DaysOfPython project's directory. As you can see below, I have created the very first file, helloworld.py. You can do the same.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/helloworld.png" alt="Creating a python file" /&gt;&lt;/p&gt; 
 &lt;p&gt;After a long day of coding, you want to close your code editor, right? This is how you will close the opened project.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/closing_opened_project.png" alt="Closing project" /&gt;&lt;/p&gt; 
 &lt;p&gt;Congratulations, you have finished setting up the development environment. Let us start coding.&lt;/p&gt; 
 &lt;h2&gt;Basic Python&lt;/h2&gt; 
 &lt;h3&gt;Python Syntax&lt;/h3&gt; 
 &lt;p&gt;A Python script can be written in Python interactive shell or in the code editor. A Python file has an extension .py.&lt;/p&gt; 
 &lt;h3&gt;Python Indentation&lt;/h3&gt; 
 &lt;p&gt;An indentation is a white space in a text. Indentation in many languages is used to increase code readability; however, Python uses indentation to create blocks of code. In other programming languages, curly brackets are used to create code blocks instead of indentation. One of the common bugs when writing Python code is incorrect indentation.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/indentation.png" alt="Indentation Error" /&gt;&lt;/p&gt; 
 &lt;h3&gt;Comments&lt;/h3&gt; 
 &lt;p&gt;Comments play a crucial role in enhancing code readability and allowing developers to leave notes within their code. In Python, any text preceded by a hash (#) symbol is considered a comment and is not executed when the code runs.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Example: Single Line Comment&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;    # This is the first comment
    # This is the second comment
    # Python is eating the world
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Example: Multiline Comment&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Triple quote can be used for multiline comment if it is not assigned to a variable&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;"""This is multiline comment
multiline comment takes multiple lines.
python is eating the world
"""
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;Data types&lt;/h3&gt; 
 &lt;p&gt;In Python there are several types of data types. Let us get started with the most common ones. Different data types will be covered in detail in other sections. For the time being, let us just go through the different data types and get familiar with them. You do not have to have a clear understanding now.&lt;/p&gt; 
 &lt;h4&gt;Number&lt;/h4&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Integer: Integer(negative, zero and positive) numbers Example: ... -3, -2, -1, 0, 1, 2, 3 ...&lt;/li&gt; 
  &lt;li&gt;Float: Decimal number Example ... -3.5, -2.25, -1.0, 0.0, 1.1, 2.2, 3.5 ...&lt;/li&gt; 
  &lt;li&gt;Complex Example 1 + j, 2 + 4j&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h4&gt;String&lt;/h4&gt; 
 &lt;p&gt;A collection of one or more characters under a single or double quote. If a string is more than one sentence then we use a triple quote.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-py"&gt;'Asabeneh'
'Finland'
'Python'
'I love teaching'
'I hope you are enjoying the first day of 30DaysOfPython Challenge'
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Booleans&lt;/h4&gt; 
 &lt;p&gt;A boolean data type is either a True or False value. T and F should be always uppercase.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;    True  #  Is the light on? If it is on, then the value is True
    False # Is the light on? If it is off, then the value is False
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;List&lt;/h4&gt; 
 &lt;p&gt;Python list is an ordered collection which allows to store different data type items. A list is similar to an array in JavaScript.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-py"&gt;[0, 1, 2, 3, 4, 5]  # all are the same data types - a list of numbers
['Banana', 'Orange', 'Mango', 'Avocado'] # all the same data types - a list of strings (fruits)
['Finland','Estonia', 'Sweden','Norway'] # all the same data types - a list of strings (countries)
['Banana', 10, False, 9.81] # different data types in the list - string, integer, boolean and float
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Dictionary&lt;/h4&gt; 
 &lt;p&gt;A Python dictionary object is an unordered collection of data in a key value pair format.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-py"&gt;{
'first_name':'Asabeneh',
'last_name':'Yetayeh',
'country':'Finland', 
'age':250, 
'is_married':True,
'skills':['JS', 'React', 'Node', 'Python']
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Tuple&lt;/h4&gt; 
 &lt;p&gt;A tuple is an ordered collection of different data types like list but tuples can not be modified once they are created. They are immutable.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-py"&gt;('Asabeneh', 'Pawel', 'Brook', 'Abraham', 'Lidiya') # Names
&lt;/code&gt;&lt;/pre&gt; 
 &lt;pre&gt;&lt;code class="language-py"&gt;('Earth', 'Jupiter', 'Neptune', 'Mars', 'Venus', 'Saturn', 'Uranus', 'Mercury') # planets
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Set&lt;/h4&gt; 
 &lt;p&gt;A set is a collection of data types similar to list and tuple. Unlike list and tuple, set is not an ordered collection of items. Like in Mathematics, set in Python stores only unique items.&lt;/p&gt; 
 &lt;p&gt;In later sections, we will go in detail about each and every Python data type.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-py"&gt;{2, 4, 3, 5}
{3.14, 9.81, 2.7} # order is not important in set
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;Checking Data types&lt;/h3&gt; 
 &lt;p&gt;To check the data type of certain data/variable we use the &lt;strong&gt;type&lt;/strong&gt; function. In the following terminal you will see different python data types:&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/checking_data_types.png" alt="Checking Data types" /&gt;&lt;/p&gt; 
 &lt;h3&gt;Python File&lt;/h3&gt; 
 &lt;p&gt;First open your project folder, 30DaysOfPython. If you don't have this folder, create a folder name called 30DaysOfPython. Inside this folder, create a file called helloworld.py. Now, let's do what we did on python interactive shell using visual studio code.&lt;/p&gt; 
 &lt;p&gt;The Python interactive shell was printing without using &lt;strong&gt;print&lt;/strong&gt; but on visual studio code to see our result we should use a built in function _print(). The &lt;em&gt;print()&lt;/em&gt; built-in function takes one or more arguments as follows &lt;em&gt;print('arument1', 'argument2', 'argument3')&lt;/em&gt;. See the examples below.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;The file name is helloworld.py&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-py"&gt;# Day 1 - 30DaysOfPython Challenge

print(2 + 3)             # addition(+)
print(3 - 1)             # subtraction(-)
print(2 * 3)             # multiplication(*)
print(3 / 2)             # division(/)
print(3 ** 2)            # exponential(**)
print(3 % 2)             # modulus(%)
print(3 // 2)            # Floor division operator(//)

# Checking data types
print(type(10))          # Int
print(type(3.14))        # Float
print(type(1 + 3j))      # Complex number
print(type('Asabeneh'))  # String
print(type([1, 2, 3]))   # List
print(type({'name':'Asabeneh'})) # Dictionary
print(type({9.8, 3.14, 2.7}))    # Set
print(type((9.8, 3.14, 2.7)))    # Tuple
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;To run the python file check the image below. You can run the python file either by running the green button on Visual Studio Code or by typing &lt;em&gt;python helloworld.py&lt;/em&gt; in the terminal .&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/running_python_script.png" alt="Running python script" /&gt;&lt;/p&gt; 
 &lt;p&gt;üåï You are amazing. You have just completed day 1 challenge and you are on your way to greatness. Now do some exercises for your brain and muscles.&lt;/p&gt; 
 &lt;h2&gt;üíª Exercises - Day 1&lt;/h2&gt; 
 &lt;h3&gt;Exercise: Level 1&lt;/h3&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Check the python version you are using&lt;/li&gt; 
  &lt;li&gt;Open the python interactive shell and do the following operations. The operands are 3 and 4. 
   &lt;ul&gt; 
    &lt;li&gt;addition(+)&lt;/li&gt; 
    &lt;li&gt;subtraction(-)&lt;/li&gt; 
    &lt;li&gt;multiplication(*)&lt;/li&gt; 
    &lt;li&gt;modulus(%)&lt;/li&gt; 
    &lt;li&gt;division(/)&lt;/li&gt; 
    &lt;li&gt;exponential(**)&lt;/li&gt; 
    &lt;li&gt;floor division operator(//)&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;Write strings on the python interactive shell. The strings are the following: 
   &lt;ul&gt; 
    &lt;li&gt;Your name&lt;/li&gt; 
    &lt;li&gt;Your family name&lt;/li&gt; 
    &lt;li&gt;Your country&lt;/li&gt; 
    &lt;li&gt;I am enjoying 30 days of python&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;Check the data types of the following data: 
   &lt;ul&gt; 
    &lt;li&gt;10&lt;/li&gt; 
    &lt;li&gt;9.8&lt;/li&gt; 
    &lt;li&gt;3.14&lt;/li&gt; 
    &lt;li&gt;4 - 4j&lt;/li&gt; 
    &lt;li&gt;['Asabeneh', 'Python', 'Finland']&lt;/li&gt; 
    &lt;li&gt;Your name&lt;/li&gt; 
    &lt;li&gt;Your family name&lt;/li&gt; 
    &lt;li&gt;Your country&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;h3&gt;Exercise: Level 2&lt;/h3&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Create a folder named day_1 inside 30DaysOfPython folder. Inside day_1 folder, create a python file helloworld.py and repeat questions 1, 2, 3 and 4. Remember to use &lt;em&gt;print()&lt;/em&gt; when you are working on a python file. Navigate to the directory where you have saved your file, and run it.&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;h3&gt;Exercise: Level 3&lt;/h3&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Write an example for different Python data types such as Number(Integer, Float, Complex), String, Boolean, List, Tuple, Set and Dictionary.&lt;/li&gt; 
  &lt;li&gt;Find an &lt;a href="https://en.wikipedia.org/wiki/Euclidean_distance#:~:text=In%20mathematics%2C%20the%20Euclidean%20distance,being%20called%20the%20Pythagorean%20distance."&gt;Euclidian distance&lt;/a&gt; between (2, 3) and (10, 8)&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;üéâ CONGRATULATIONS ! üéâ&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/02_Day_Variables_builtin_functions/02_variables_builtin_functions.md"&gt;Day 2 &amp;gt;&amp;gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>microsoft/BitNet</title>
      <link>https://github.com/microsoft/BitNet</link>
      <description>&lt;p&gt;Official inference framework for 1-bit LLMs&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;bitnet.cpp&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://opensource.org/licenses/MIT"&gt;&lt;img src="https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true" alt="License: MIT" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/badge/version-1.0-blue" alt="version" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/microsoft/BitNet-b1.58-2B-4T"&gt;&lt;img src="https://raw.githubusercontent.com/microsoft/BitNet/main/assets/header_model_release.png" alt="BitNet Model on Hugging Face" width="800" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Try it out via this &lt;a href="https://bitnet-demo.azurewebsites.net/"&gt;demo&lt;/a&gt;, or build and run it on your own &lt;a href="https://github.com/microsoft/BitNet?tab=readme-ov-file#build-from-source"&gt;CPU&lt;/a&gt; or &lt;a href="https://github.com/microsoft/BitNet/raw/main/gpu/README.md"&gt;GPU&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;bitnet.cpp is the official inference framework for 1-bit LLMs (e.g., BitNet b1.58). It offers a suite of optimized kernels, that support &lt;strong&gt;fast&lt;/strong&gt; and &lt;strong&gt;lossless&lt;/strong&gt; inference of 1.58-bit models on CPU and GPU (NPU support will coming next).&lt;/p&gt; 
&lt;p&gt;The first release of bitnet.cpp is to support inference on CPUs. bitnet.cpp achieves speedups of &lt;strong&gt;1.37x&lt;/strong&gt; to &lt;strong&gt;5.07x&lt;/strong&gt; on ARM CPUs, with larger models experiencing greater performance gains. Additionally, it reduces energy consumption by &lt;strong&gt;55.4%&lt;/strong&gt; to &lt;strong&gt;70.0%&lt;/strong&gt;, further boosting overall efficiency. On x86 CPUs, speedups range from &lt;strong&gt;2.37x&lt;/strong&gt; to &lt;strong&gt;6.17x&lt;/strong&gt; with energy reductions between &lt;strong&gt;71.9%&lt;/strong&gt; to &lt;strong&gt;82.2%&lt;/strong&gt;. Furthermore, bitnet.cpp can run a 100B BitNet b1.58 model on a single CPU, achieving speeds comparable to human reading (5-7 tokens per second), significantly enhancing the potential for running LLMs on local devices. Please refer to the &lt;a href="https://arxiv.org/abs/2410.16144"&gt;technical report&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/microsoft/BitNet/main/assets/m2_performance.jpg" alt="m2_performance" width="800" /&gt; 
&lt;img src="https://raw.githubusercontent.com/microsoft/BitNet/main/assets/intel_performance.jpg" alt="m2_performance" width="800" /&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;The tested models are dummy setups used in a research context to demonstrate the inference performance of bitnet.cpp.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Demo&lt;/h2&gt; 
&lt;p&gt;A demo of bitnet.cpp running a BitNet b1.58 3B model on Apple M2:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/7f46b736-edec-4828-b809-4be780a3e5b1"&gt;https://github.com/user-attachments/assets/7f46b736-edec-4828-b809-4be780a3e5b1&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;What's New:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;05/20/2025 &lt;a href="https://github.com/microsoft/BitNet/raw/main/gpu/README.md"&gt;BitNet Official GPU inference kernel&lt;/a&gt; &lt;img src="https://img.shields.io/badge/NEW-red" alt="NEW" /&gt;&lt;/li&gt; 
 &lt;li&gt;04/14/2025 &lt;a href="https://huggingface.co/microsoft/BitNet-b1.58-2B-4T"&gt;BitNet Official 2B Parameter Model on Hugging Face&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;02/18/2025 &lt;a href="https://arxiv.org/abs/2502.11880"&gt;Bitnet.cpp: Efficient Edge Inference for Ternary LLMs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;11/08/2024 &lt;a href="https://arxiv.org/abs/2411.04965"&gt;BitNet a4.8: 4-bit Activations for 1-bit LLMs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;10/21/2024 &lt;a href="https://arxiv.org/abs/2410.16144"&gt;1-bit AI Infra: Part 1.1, Fast and Lossless BitNet b1.58 Inference on CPUs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;10/17/2024 bitnet.cpp 1.0 released.&lt;/li&gt; 
 &lt;li&gt;03/21/2024 &lt;a href="https://github.com/microsoft/unilm/raw/master/bitnet/The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ.pdf"&gt;The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;02/27/2024 &lt;a href="https://arxiv.org/abs/2402.17764"&gt;The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;10/17/2023 &lt;a href="https://arxiv.org/abs/2310.11453"&gt;BitNet: Scaling 1-bit Transformers for Large Language Models&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;p&gt;This project is based on the &lt;a href="https://github.com/ggerganov/llama.cpp"&gt;llama.cpp&lt;/a&gt; framework. We would like to thank all the authors for their contributions to the open-source community. Also, bitnet.cpp's kernels are built on top of the Lookup Table methodologies pioneered in &lt;a href="https://github.com/microsoft/T-MAC/"&gt;T-MAC&lt;/a&gt;. For inference of general low-bit LLMs beyond ternary models, we recommend using T-MAC.&lt;/p&gt; 
&lt;h2&gt;Official Models&lt;/h2&gt; 
&lt;table&gt;  
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;th rowspan="2"&gt;Model&lt;/th&gt; 
   &lt;th rowspan="2"&gt;Parameters&lt;/th&gt; 
   &lt;th rowspan="2"&gt;CPU&lt;/th&gt; 
   &lt;th colspan="3"&gt;Kernel&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th&gt;I2_S&lt;/th&gt; 
   &lt;th&gt;TL1&lt;/th&gt; 
   &lt;th&gt;TL2&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/microsoft/BitNet-b1.58-2B-4T"&gt;BitNet-b1.58-2B-4T&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;2.4B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Supported Models&lt;/h2&gt; 
&lt;p&gt;‚ùóÔ∏è&lt;strong&gt;We use existing 1-bit LLMs available on &lt;a href="https://huggingface.co/"&gt;Hugging Face&lt;/a&gt; to demonstrate the inference capabilities of bitnet.cpp. We hope the release of bitnet.cpp will inspire the development of 1-bit LLMs in large-scale settings in terms of model size and training tokens.&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt;  
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;th rowspan="2"&gt;Model&lt;/th&gt; 
   &lt;th rowspan="2"&gt;Parameters&lt;/th&gt; 
   &lt;th rowspan="2"&gt;CPU&lt;/th&gt; 
   &lt;th colspan="3"&gt;Kernel&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th&gt;I2_S&lt;/th&gt; 
   &lt;th&gt;TL1&lt;/th&gt; 
   &lt;th&gt;TL2&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/1bitLLM/bitnet_b1_58-large"&gt;bitnet_b1_58-large&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;0.7B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/1bitLLM/bitnet_b1_58-3B"&gt;bitnet_b1_58-3B&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;3.3B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/HF1BitLLM/Llama3-8B-1.58-100B-tokens"&gt;Llama3-8B-1.58-100B-tokens&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;8.0B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/collections/tiiuae/falcon3-67605ae03578be86e4e87026"&gt;Falcon3 Family&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;1B-10B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/collections/tiiuae/falcon-edge-series-6804fd13344d6d8a8fa71130"&gt;Falcon-E Family&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;1B-3B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;python&amp;gt;=3.9&lt;/li&gt; 
 &lt;li&gt;cmake&amp;gt;=3.22&lt;/li&gt; 
 &lt;li&gt;clang&amp;gt;=18 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;For Windows users, install &lt;a href="https://visualstudio.microsoft.com/downloads/"&gt;Visual Studio 2022&lt;/a&gt;. In the installer, toggle on at least the following options(this also automatically installs the required additional tools like CMake):&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Desktop-development with C++&lt;/li&gt; 
     &lt;li&gt;C++-CMake Tools for Windows&lt;/li&gt; 
     &lt;li&gt;Git for Windows&lt;/li&gt; 
     &lt;li&gt;C++-Clang Compiler for Windows&lt;/li&gt; 
     &lt;li&gt;MS-Build Support for LLVM-Toolset (clang)&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;For Debian/Ubuntu users, you can download with &lt;a href="https://apt.llvm.org/"&gt;Automatic installation script&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash -c "$(wget -O - https://apt.llvm.org/llvm.sh)"&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;conda (highly recommend)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Build from source&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] If you are using Windows, please remember to always use a Developer Command Prompt / PowerShell for VS2022 for the following commands. Please refer to the FAQs below if you see any issues.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ol&gt; 
 &lt;li&gt;Clone the repo&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone --recursive https://github.com/microsoft/BitNet.git
cd BitNet
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Install the dependencies&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# (Recommended) Create a new conda environment
conda create -n bitnet-cpp python=3.9
conda activate bitnet-cpp

pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Build the project&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Manually download the model and run with local path
huggingface-cli download microsoft/BitNet-b1.58-2B-4T-gguf --local-dir models/BitNet-b1.58-2B-4T
python setup_env.py -md models/BitNet-b1.58-2B-4T -q i2_s

&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;
usage: setup_env.py [-h] [--hf-repo {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}] [--model-dir MODEL_DIR] [--log-dir LOG_DIR] [--quant-type {i2_s,tl1}] [--quant-embd]
                    [--use-pretuned]

Setup the environment for running inference

optional arguments:
  -h, --help            show this help message and exit
  --hf-repo {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}, -hr {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}
                        Model used for inference
  --model-dir MODEL_DIR, -md MODEL_DIR
                        Directory to save/load the model
  --log-dir LOG_DIR, -ld LOG_DIR
                        Directory to save the logging info
  --quant-type {i2_s,tl1}, -q {i2_s,tl1}
                        Quantization type
  --quant-embd          Quantize the embeddings to f16
  --use-pretuned, -p    Use the pretuned kernel parameters
&lt;/pre&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Basic usage&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run inference with the quantized model
python run_inference.py -m models/BitNet-b1.58-2B-4T/ggml-model-i2_s.gguf -p "You are a helpful assistant" -cnv
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;
usage: run_inference.py [-h] [-m MODEL] [-n N_PREDICT] -p PROMPT [-t THREADS] [-c CTX_SIZE] [-temp TEMPERATURE] [-cnv]

Run inference

optional arguments:
  -h, --help            show this help message and exit
  -m MODEL, --model MODEL
                        Path to model file
  -n N_PREDICT, --n-predict N_PREDICT
                        Number of tokens to predict when generating text
  -p PROMPT, --prompt PROMPT
                        Prompt to generate text from
  -t THREADS, --threads THREADS
                        Number of threads to use
  -c CTX_SIZE, --ctx-size CTX_SIZE
                        Size of the prompt context
  -temp TEMPERATURE, --temperature TEMPERATURE
                        Temperature, a hyperparameter that controls the randomness of the generated text
  -cnv, --conversation  Whether to enable chat mode or not (for instruct models.)
                        (When this option is turned on, the prompt specified by -p will be used as the system prompt.)
&lt;/pre&gt; 
&lt;h3&gt;Benchmark&lt;/h3&gt; 
&lt;p&gt;We provide scripts to run the inference benchmark providing a model.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;usage: e2e_benchmark.py -m MODEL [-n N_TOKEN] [-p N_PROMPT] [-t THREADS]  
   
Setup the environment for running the inference  
   
required arguments:  
  -m MODEL, --model MODEL  
                        Path to the model file. 
   
optional arguments:  
  -h, --help  
                        Show this help message and exit. 
  -n N_TOKEN, --n-token N_TOKEN  
                        Number of generated tokens. 
  -p N_PROMPT, --n-prompt N_PROMPT  
                        Prompt to generate text from. 
  -t THREADS, --threads THREADS  
                        Number of threads to use. 
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Here's a brief explanation of each argument:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;-m&lt;/code&gt;, &lt;code&gt;--model&lt;/code&gt;: The path to the model file. This is a required argument that must be provided when running the script.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-n&lt;/code&gt;, &lt;code&gt;--n-token&lt;/code&gt;: The number of tokens to generate during the inference. It is an optional argument with a default value of 128.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-p&lt;/code&gt;, &lt;code&gt;--n-prompt&lt;/code&gt;: The number of prompt tokens to use for generating text. This is an optional argument with a default value of 512.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-t&lt;/code&gt;, &lt;code&gt;--threads&lt;/code&gt;: The number of threads to use for running the inference. It is an optional argument with a default value of 2.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-h&lt;/code&gt;, &lt;code&gt;--help&lt;/code&gt;: Show the help message and exit. Use this argument to display usage information.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python utils/e2e_benchmark.py -m /path/to/model -n 200 -p 256 -t 4  
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command would run the inference benchmark using the model located at &lt;code&gt;/path/to/model&lt;/code&gt;, generating 200 tokens from a 256 token prompt, utilizing 4 threads.&lt;/p&gt; 
&lt;p&gt;For the model layout that do not supported by any public model, we provide scripts to generate a dummy model with the given model layout, and run the benchmark on your machine:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python utils/generate-dummy-bitnet-model.py models/bitnet_b1_58-large --outfile models/dummy-bitnet-125m.tl1.gguf --outtype tl1 --model-size 125M

# Run benchmark with the generated model, use -m to specify the model path, -p to specify the prompt processed, -n to specify the number of token to generate
python utils/e2e_benchmark.py -m models/dummy-bitnet-125m.tl1.gguf -p 512 -n 128
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Convert from &lt;code&gt;.safetensors&lt;/code&gt; Checkpoints&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Prepare the .safetensors model file
huggingface-cli download microsoft/bitnet-b1.58-2B-4T-bf16 --local-dir ./models/bitnet-b1.58-2B-4T-bf16

# Convert to gguf model
python ./utils/convert-helper-bitnet.py ./models/bitnet-b1.58-2B-4T-bf16
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;FAQ (Frequently Asked Questions)üìå&lt;/h3&gt; 
&lt;h4&gt;Q1: The build dies with errors building llama.cpp due to issues with std::chrono in log.cpp?&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; This is an issue introduced in recent version of llama.cpp. Please refer to this &lt;a href="https://github.com/tinglou/llama.cpp/commit/4e3db1e3d78cc1bcd22bcb3af54bd2a4628dd323"&gt;commit&lt;/a&gt; in the &lt;a href="https://github.com/abetlen/llama-cpp-python/issues/1942"&gt;discussion&lt;/a&gt; to fix this issue.&lt;/p&gt; 
&lt;h4&gt;Q2: How to build with clang in conda environment on windows?&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Before building the project, verify your clang installation and access to Visual Studio tools by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;clang -v
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command checks that you are using the correct version of clang and that the Visual Studio tools are available. If you see an error message such as:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;'clang' is not recognized as an internal or external command, operable program or batch file.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It indicates that your command line window is not properly initialized for Visual Studio tools.&lt;/p&gt; 
&lt;p&gt;‚Ä¢ If you are using Command Prompt, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;"C:\Program Files\Microsoft Visual Studio\2022\Professional\Common7\Tools\VsDevCmd.bat" -startdir=none -arch=x64 -host_arch=x64
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;‚Ä¢ If you are using Windows PowerShell, run the following commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Import-Module "C:\Program Files\Microsoft Visual Studio\2022\Professional\Common7\Tools\Microsoft.VisualStudio.DevShell.dll" Enter-VsDevShell 3f0e31ad -SkipAutomaticLocation -DevCmdArguments "-arch=x64 -host_arch=x64"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;These steps will initialize your environment and allow you to use the correct Visual Studio tools.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>microsoft/VibeVoice</title>
      <link>https://github.com/microsoft/VibeVoice</link>
      <description>&lt;p&gt;Open-Source Frontier Voice AI&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h2&gt;üéôÔ∏è VibeVoice: Open-Source Frontier Voice AI&lt;/h2&gt; 
 &lt;p&gt;&lt;a href="https://microsoft.github.io/VibeVoice"&gt;&lt;img src="https://img.shields.io/badge/Project-Page-blue?logo=microsoft" alt="Project Page" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/collections/microsoft/vibevoice-68a2ef24a875c44be47b034f"&gt;&lt;img src="https://img.shields.io/badge/HuggingFace-Collection-orange?logo=huggingface" alt="Hugging Face" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/pdf/2508.19205"&gt;&lt;img src="https://img.shields.io/badge/Technical-Report-red?logo=adobeacrobatreader" alt="Technical Report" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="Figures/VibeVoice_logo_white.png" /&gt; 
  &lt;img src="https://raw.githubusercontent.com/microsoft/VibeVoice/main/Figures/VibeVoice_logo.png" alt="VibeVoice Logo" width="300" /&gt; 
 &lt;/picture&gt; 
&lt;/div&gt; 
&lt;div align="left"&gt; 
 &lt;h3&gt;üì∞ News&lt;/h3&gt; 
 &lt;img src="https://img.shields.io/badge/Status-New-brightgreen?style=flat" alt="New" /&gt; 
 &lt;img src="https://img.shields.io/badge/Feature-Realtime_TTS-blue?style=flat&amp;amp;logo=soundcharts" alt="Realtime TTS" /&gt; 
 &lt;p&gt;&lt;strong&gt;2025-12-16: üì£ We added more experimental speakers for exploration, including multilingual voices and 11 distinct English style voices. &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-realtime-0.5b.md#optional-more-experimental-voices"&gt;Try it&lt;/a&gt;. More speaker types will be added over time.&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;2025-12-09: üì£ We added experimental speakers in nine languages (DE, FR, IT, JP, KR, NL, PL, PT, ES) for exploration‚Äîwelcome to try them out and share your feedback.&lt;/p&gt; 
 &lt;p&gt;2025-12-03: üì£ We open-sourced &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-realtime-0.5b.md"&gt;&lt;strong&gt;VibeVoice‚ÄëRealtime‚Äë0.5B&lt;/strong&gt;&lt;/a&gt;, a real‚Äëtime text‚Äëto‚Äëspeech model that supports streaming text input and robust long-form speech generation. Try it on &lt;a href="https://colab.research.google.com/github/microsoft/VibeVoice/blob/main/demo/vibevoice_realtime_colab.ipynb"&gt;Colab&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;To mitigate deepfake risks and ensure low latency for the first speech chunk, voice prompts are provided in an embedded format. For users requiring voice customization, please reach out to our team. We will also be expanding the range of available speakers. &lt;br /&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/0901d274-f6ae-46ef-a0fd-3c4fba4f76dc"&gt;https://github.com/user-attachments/assets/0901d274-f6ae-46ef-a0fd-3c4fba4f76dc&lt;/a&gt;&lt;/p&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;(Launch your own realtime demo via the websocket example in &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-realtime-0.5b.md#usage-1-launch-real-time-websocket-demo"&gt;Usage&lt;/a&gt;).&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/div&gt; 
&lt;p&gt;2025-09-05: VibeVoice is an open-source research framework intended to advance collaboration in the speech synthesis community. After release, we discovered instances where the tool was used in ways inconsistent with the stated intent. Since responsible use of AI is one of Microsoft‚Äôs guiding principles, we have disabled this repo until we are confident that out-of-scope use is no longer possible.&lt;/p&gt; 
&lt;h3&gt;Overview&lt;/h3&gt; 
&lt;p&gt;VibeVoice is a novel framework designed for generating &lt;strong&gt;expressive&lt;/strong&gt;, &lt;strong&gt;long-form&lt;/strong&gt;, &lt;strong&gt;multi-speaker&lt;/strong&gt; conversational audio, such as podcasts, from text. It addresses significant challenges in traditional Text-to-Speech (TTS) systems, particularly in scalability, speaker consistency, and natural turn-taking.&lt;/p&gt; 
&lt;p&gt;VibeVoice currently includes two model variants:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Long-form multi-speaker model&lt;/strong&gt;: Synthesizes conversational/single-speaker speech up to &lt;strong&gt;90 minutes&lt;/strong&gt; with up to &lt;strong&gt;4 distinct speakers&lt;/strong&gt;, surpassing the typical 1‚Äì2 speaker limits of many prior models.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-realtime-0.5b.md"&gt;Realtime streaming TTS model&lt;/a&gt;&lt;/strong&gt;: Produces initial audible speech in ~&lt;strong&gt;300 ms&lt;/strong&gt; and supports &lt;strong&gt;streaming text input&lt;/strong&gt; for single-speaker &lt;strong&gt;real-time&lt;/strong&gt; speech generation; designed for low-latency generation.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;A core innovation of VibeVoice is its use of continuous speech tokenizers (Acoustic and Semantic) operating at an ultra-low frame rate of 7.5 Hz. These tokenizers efficiently preserve audio fidelity while significantly boosting computational efficiency for processing long sequences. VibeVoice employs a &lt;a href="https://arxiv.org/abs/2412.08635"&gt;next-token diffusion&lt;/a&gt; framework, leveraging a Large Language Model (LLM) to understand textual context and dialogue flow, and a diffusion head to generate high-fidelity acoustic details.&lt;/p&gt; 
&lt;p align="left"&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/VibeVoice/main/Figures/MOS-preference.png" alt="MOS Preference Results" height="260px" /&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/VibeVoice/main/Figures/VibeVoice.jpg" alt="VibeVoice Overview" height="250px" style="margin-right: 10px;" /&gt; &lt;/p&gt; 
&lt;h3&gt;üéµ Demo Examples&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Video Demo&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;We produced this video with &lt;a href="https://github.com/Wan-Video/Wan2.2"&gt;Wan2.2&lt;/a&gt;. We sincerely appreciate the Wan-Video team for their great work.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;English&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/0967027c-141e-4909-bec8-091558b1b784"&gt;https://github.com/user-attachments/assets/0967027c-141e-4909-bec8-091558b1b784&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Chinese&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/322280b7-3093-4c67-86e3-10be4746c88f"&gt;https://github.com/user-attachments/assets/322280b7-3093-4c67-86e3-10be4746c88f&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Cross-Lingual&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/838d8ad9-a201-4dde-bb45-8cd3f59ce722"&gt;https://github.com/user-attachments/assets/838d8ad9-a201-4dde-bb45-8cd3f59ce722&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Spontaneous Singing&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/6f27a8a5-0c60-4f57-87f3-7dea2e11c730"&gt;https://github.com/user-attachments/assets/6f27a8a5-0c60-4f57-87f3-7dea2e11c730&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Long Conversation with 4 people&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/a357c4b6-9768-495c-a576-1618f6275727"&gt;https://github.com/user-attachments/assets/a357c4b6-9768-495c-a576-1618f6275727&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;For more examples, see the &lt;a href="https://microsoft.github.io/VibeVoice"&gt;Project Page&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Risks and limitations&lt;/h2&gt; 
&lt;p&gt;While efforts have been made to optimize it through various techniques, it may still produce outputs that are unexpected, biased, or inaccurate. VibeVoice inherits any biases, errors, or omissions produced by its base model (specifically, Qwen2.5 1.5b in this release). Potential for Deepfakes and Disinformation: High-quality synthetic speech can be misused to create convincing fake audio content for impersonation, fraud, or spreading disinformation. Users must ensure transcripts are reliable, check content accuracy, and avoid using generated content in misleading ways. Users are expected to use the generated content and to deploy the models in a lawful manner, in full compliance with all applicable laws and regulations in the relevant jurisdictions. It is best practice to disclose the use of AI when sharing AI-generated content.&lt;/p&gt; 
&lt;p&gt;English and Chinese only: Transcripts in languages other than English or Chinese may result in unexpected audio outputs.&lt;/p&gt; 
&lt;p&gt;Non-Speech Audio: The model focuses solely on speech synthesis and does not handle background noise, music, or other sound effects.&lt;/p&gt; 
&lt;p&gt;Overlapping Speech: The current model does not explicitly model or generate overlapping speech segments in conversations.&lt;/p&gt; 
&lt;p&gt;We do not recommend using VibeVoice in commercial or real-world applications without further testing and development. This model is intended for research and development purposes only. Please use responsibly.&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://api.star-history.com/svg?repos=Microsoft/vibevoice&amp;amp;type=date&amp;amp;legend=top-left" alt="Star History Chart" /&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>labmlai/annotated_deep_learning_paper_implementations</title>
      <link>https://github.com/labmlai/annotated_deep_learning_paper_implementations</link>
      <description>&lt;p&gt;üßë‚Äçüè´ 60+ Implementations/tutorials of deep learning papers with side-by-side notes üìù; including transformers (original, xl, switch, feedback, vit, ...), optimizers (adam, adabelief, sophia, ...), gans(cyclegan, stylegan2, ...), üéÆ reinforcement learning (ppo, dqn), capsnet, distillation, ... üß†&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="https://twitter.com/labmlai"&gt;&lt;img src="https://img.shields.io/twitter/follow/labmlai?style=social" alt="Twitter" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;&lt;a href="https://nn.labml.ai/index.html"&gt;labml.ai Deep Learning Paper Implementations&lt;/a&gt;&lt;/h1&gt; 
&lt;p&gt;This is a collection of simple PyTorch implementations of neural networks and related algorithms. These implementations are documented with explanations,&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://nn.labml.ai/index.html"&gt;The website&lt;/a&gt; renders these as side-by-side formatted notes. We believe these would help you understand these algorithms better.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://nn.labml.ai/dqn-light.png" alt="Screenshot" /&gt;&lt;/p&gt; 
&lt;p&gt;We are actively maintaining this repo and adding new implementations almost weekly. &lt;a href="https://twitter.com/labmlai"&gt;&lt;img src="https://img.shields.io/twitter/follow/labmlai?style=social" alt="Twitter" /&gt;&lt;/a&gt; for updates.&lt;/p&gt; 
&lt;h2&gt;Paper Implementations&lt;/h2&gt; 
&lt;h4&gt;‚ú® &lt;a href="https://nn.labml.ai/transformers/index.html"&gt;Transformers&lt;/a&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/transformers/jax_transformer/index.html"&gt;JAX implementation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/transformers/mha.html"&gt;Multi-headed attention&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/transformers/flash/index.html"&gt;Triton Flash Attention&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/transformers/models.html"&gt;Transformer building blocks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/transformers/xl/index.html"&gt;Transformer XL&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://nn.labml.ai/transformers/xl/relative_mha.html"&gt;Relative multi-headed attention&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/transformers/rope/index.html"&gt;Rotary Positional Embeddings&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/transformers/alibi/index.html"&gt;Attention with Linear Biases (ALiBi)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/transformers/retro/index.html"&gt;RETRO&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/transformers/compressive/index.html"&gt;Compressive Transformer&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/transformers/gpt/index.html"&gt;GPT Architecture&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/transformers/glu_variants/simple.html"&gt;GLU Variants&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/transformers/knn"&gt;kNN-LM: Generalization through Memorization&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/transformers/feedback/index.html"&gt;Feedback Transformer&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/transformers/switch/index.html"&gt;Switch Transformer&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/transformers/fast_weights/index.html"&gt;Fast Weights Transformer&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/transformers/fnet/index.html"&gt;FNet&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/transformers/aft/index.html"&gt;Attention Free Transformer&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/transformers/mlm/index.html"&gt;Masked Language Model&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/transformers/mlp_mixer/index.html"&gt;MLP-Mixer: An all-MLP Architecture for Vision&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/transformers/gmlp/index.html"&gt;Pay Attention to MLPs (gMLP)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/transformers/vit/index.html"&gt;Vision Transformer (ViT)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/transformers/primer_ez/index.html"&gt;Primer EZ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/transformers/hour_glass/index.html"&gt;Hourglass&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;‚ú® &lt;a href="https://nn.labml.ai/lora/index.html"&gt;Low-Rank Adaptation (LoRA)&lt;/a&gt;&lt;/h4&gt; 
&lt;h4&gt;‚ú® &lt;a href="https://nn.labml.ai/neox/index.html"&gt;Eleuther GPT-NeoX&lt;/a&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/neox/samples/generate.html"&gt;Generate on a 48GB GPU&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/neox/samples/finetune.html"&gt;Finetune on two 48GB GPUs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/neox/utils/llm_int8.html"&gt;LLM.int8()&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;‚ú® &lt;a href="https://nn.labml.ai/diffusion/index.html"&gt;Diffusion models&lt;/a&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/diffusion/ddpm/index.html"&gt;Denoising Diffusion Probabilistic Models (DDPM)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/diffusion/stable_diffusion/sampler/ddim.html"&gt;Denoising Diffusion Implicit Models (DDIM)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/diffusion/stable_diffusion/latent_diffusion.html"&gt;Latent Diffusion Models&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/diffusion/stable_diffusion/index.html"&gt;Stable Diffusion&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;‚ú® &lt;a href="https://nn.labml.ai/gan/index.html"&gt;Generative Adversarial Networks&lt;/a&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/gan/original/index.html"&gt;Original GAN&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/gan/dcgan/index.html"&gt;GAN with deep convolutional network&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/gan/cycle_gan/index.html"&gt;Cycle GAN&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/gan/wasserstein/index.html"&gt;Wasserstein GAN&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/gan/wasserstein/gradient_penalty/index.html"&gt;Wasserstein GAN with Gradient Penalty&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/gan/stylegan/index.html"&gt;StyleGAN 2&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;‚ú® &lt;a href="https://nn.labml.ai/recurrent_highway_networks/index.html"&gt;Recurrent Highway Networks&lt;/a&gt;&lt;/h4&gt; 
&lt;h4&gt;‚ú® &lt;a href="https://nn.labml.ai/lstm/index.html"&gt;LSTM&lt;/a&gt;&lt;/h4&gt; 
&lt;h4&gt;‚ú® &lt;a href="https://nn.labml.ai/hypernetworks/hyper_lstm.html"&gt;HyperNetworks - HyperLSTM&lt;/a&gt;&lt;/h4&gt; 
&lt;h4&gt;‚ú® &lt;a href="https://nn.labml.ai/resnet/index.html"&gt;ResNet&lt;/a&gt;&lt;/h4&gt; 
&lt;h4&gt;‚ú® &lt;a href="https://nn.labml.ai/conv_mixer/index.html"&gt;ConvMixer&lt;/a&gt;&lt;/h4&gt; 
&lt;h4&gt;‚ú® &lt;a href="https://nn.labml.ai/capsule_networks/index.html"&gt;Capsule Networks&lt;/a&gt;&lt;/h4&gt; 
&lt;h4&gt;‚ú® &lt;a href="https://nn.labml.ai/unet/index.html"&gt;U-Net&lt;/a&gt;&lt;/h4&gt; 
&lt;h4&gt;‚ú® &lt;a href="https://nn.labml.ai/sketch_rnn/index.html"&gt;Sketch RNN&lt;/a&gt;&lt;/h4&gt; 
&lt;h4&gt;‚ú® Graph Neural Networks&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/graphs/gat/index.html"&gt;Graph Attention Networks (GAT)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/graphs/gatv2/index.html"&gt;Graph Attention Networks v2 (GATv2)&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;‚ú® &lt;a href="https://nn.labml.ai/cfr/index.html"&gt;Counterfactual Regret Minimization (CFR)&lt;/a&gt;&lt;/h4&gt; 
&lt;p&gt;Solving games with incomplete information such as poker with CFR.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/cfr/kuhn/index.html"&gt;Kuhn Poker&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;‚ú® &lt;a href="https://nn.labml.ai/rl/index.html"&gt;Reinforcement Learning&lt;/a&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/rl/ppo/index.html"&gt;Proximal Policy Optimization&lt;/a&gt; with &lt;a href="https://nn.labml.ai/rl/ppo/gae.html"&gt;Generalized Advantage Estimation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/rl/dqn/index.html"&gt;Deep Q Networks&lt;/a&gt; with with &lt;a href="https://nn.labml.ai/rl/dqn/model.html"&gt;Dueling Network&lt;/a&gt;, &lt;a href="https://nn.labml.ai/rl/dqn/replay_buffer.html"&gt;Prioritized Replay&lt;/a&gt; and Double Q Network.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;‚ú® &lt;a href="https://nn.labml.ai/optimizers/index.html"&gt;Optimizers&lt;/a&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/optimizers/adam.html"&gt;Adam&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/optimizers/amsgrad.html"&gt;AMSGrad&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/optimizers/adam_warmup.html"&gt;Adam Optimizer with warmup&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/optimizers/noam.html"&gt;Noam Optimizer&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/optimizers/radam.html"&gt;Rectified Adam Optimizer&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/optimizers/ada_belief.html"&gt;AdaBelief Optimizer&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/optimizers/sophia.html"&gt;Sophia-G Optimizer&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;‚ú® &lt;a href="https://nn.labml.ai/normalization/index.html"&gt;Normalization Layers&lt;/a&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/normalization/batch_norm/index.html"&gt;Batch Normalization&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/normalization/layer_norm/index.html"&gt;Layer Normalization&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/normalization/instance_norm/index.html"&gt;Instance Normalization&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/normalization/group_norm/index.html"&gt;Group Normalization&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/normalization/weight_standardization/index.html"&gt;Weight Standardization&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/normalization/batch_channel_norm/index.html"&gt;Batch-Channel Normalization&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/normalization/deep_norm/index.html"&gt;DeepNorm&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;‚ú® &lt;a href="https://nn.labml.ai/distillation/index.html"&gt;Distillation&lt;/a&gt;&lt;/h4&gt; 
&lt;h4&gt;‚ú® &lt;a href="https://nn.labml.ai/adaptive_computation/index.html"&gt;Adaptive Computation&lt;/a&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/adaptive_computation/ponder_net/index.html"&gt;PonderNet&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;‚ú® &lt;a href="https://nn.labml.ai/uncertainty/index.html"&gt;Uncertainty&lt;/a&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/uncertainty/evidence/index.html"&gt;Evidential Deep Learning to Quantify Classification Uncertainty&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;‚ú® &lt;a href="https://nn.labml.ai/activations/index.html"&gt;Activations&lt;/a&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/activations/fta/index.html"&gt;Fuzzy Tiling Activations&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;‚ú® &lt;a href="https://nn.labml.ai/sampling/index.html"&gt;Langauge Model Sampling Techniques&lt;/a&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/sampling/greedy.html"&gt;Greedy Sampling&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/sampling/temperature.html"&gt;Temperature Sampling&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/sampling/top_k.html"&gt;Top-k Sampling&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/sampling/nucleus.html"&gt;Nucleus Sampling&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;‚ú® &lt;a href="https://nn.labml.ai/scaling/index.html"&gt;Scalable Training/Inference&lt;/a&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://nn.labml.ai/scaling/zero3/index.html"&gt;Zero3 memory optimizations&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install labml-nn
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>strands-agents/sdk-python</title>
      <link>https://github.com/strands-agents/sdk-python</link>
      <description>&lt;p&gt;A model-driven approach to building AI agents in just a few lines of code.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;div&gt; 
  &lt;a href="https://strandsagents.com"&gt; &lt;img src="https://strandsagents.com/latest/assets/logo-github.svg?sanitize=true" alt="Strands Agents" width="55px" height="105px" /&gt; &lt;/a&gt; 
 &lt;/div&gt; 
 &lt;h1&gt; Strands Agents &lt;/h1&gt; 
 &lt;h2&gt; A model-driven approach to building AI agents in just a few lines of code. &lt;/h2&gt; 
 &lt;div align="center"&gt; 
  &lt;a href="https://github.com/strands-agents/sdk-python/graphs/commit-activity"&gt;&lt;img alt="GitHub commit activity" src="https://img.shields.io/github/commit-activity/m/strands-agents/sdk-python" /&gt;&lt;/a&gt; 
  &lt;a href="https://github.com/strands-agents/sdk-python/issues"&gt;&lt;img alt="GitHub open issues" src="https://img.shields.io/github/issues/strands-agents/sdk-python" /&gt;&lt;/a&gt; 
  &lt;a href="https://github.com/strands-agents/sdk-python/pulls"&gt;&lt;img alt="GitHub open pull requests" src="https://img.shields.io/github/issues-pr/strands-agents/sdk-python" /&gt;&lt;/a&gt; 
  &lt;a href="https://github.com/strands-agents/sdk-python/raw/main/LICENSE"&gt;&lt;img alt="License" src="https://img.shields.io/github/license/strands-agents/sdk-python" /&gt;&lt;/a&gt; 
  &lt;a href="https://pypi.org/project/strands-agents/"&gt;&lt;img alt="PyPI version" src="https://img.shields.io/pypi/v/strands-agents" /&gt;&lt;/a&gt; 
  &lt;a href="https://python.org"&gt;&lt;img alt="Python versions" src="https://img.shields.io/pypi/pyversions/strands-agents" /&gt;&lt;/a&gt; 
 &lt;/div&gt; 
 &lt;p&gt; &lt;a href="https://strandsagents.com/"&gt;Documentation&lt;/a&gt; ‚óÜ &lt;a href="https://github.com/strands-agents/samples"&gt;Samples&lt;/a&gt; ‚óÜ &lt;a href="https://github.com/strands-agents/sdk-python"&gt;Python SDK&lt;/a&gt; ‚óÜ &lt;a href="https://github.com/strands-agents/tools"&gt;Tools&lt;/a&gt; ‚óÜ &lt;a href="https://github.com/strands-agents/agent-builder"&gt;Agent Builder&lt;/a&gt; ‚óÜ &lt;a href="https://github.com/strands-agents/mcp-server"&gt;MCP Server&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;Strands Agents is a simple yet powerful SDK that takes a model-driven approach to building and running AI agents. From simple conversational assistants to complex autonomous workflows, from local development to production deployment, Strands Agents scales with your needs.&lt;/p&gt; 
&lt;h2&gt;Feature Overview&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Lightweight &amp;amp; Flexible&lt;/strong&gt;: Simple agent loop that just works and is fully customizable&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Model Agnostic&lt;/strong&gt;: Support for Amazon Bedrock, Anthropic, Gemini, LiteLLM, Llama, Ollama, OpenAI, Writer, and custom providers&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Advanced Capabilities&lt;/strong&gt;: Multi-agent systems, autonomous agents, and streaming support&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Built-in MCP&lt;/strong&gt;: Native support for Model Context Protocol (MCP) servers, enabling access to thousands of pre-built tools&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install Strands Agents
pip install strands-agents strands-agents-tools
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from strands import Agent
from strands_tools import calculator
agent = Agent(tools=[calculator])
agent("What is the square root of 1764")
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: For the default Amazon Bedrock model provider, you'll need AWS credentials configured and model access enabled for Claude 4 Sonnet in the us-west-2 region. See the &lt;a href="https://strandsagents.com/"&gt;Quickstart Guide&lt;/a&gt; for details on configuring other model providers.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;Ensure you have Python 3.10+ installed, then:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Create and activate virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows use: .venv\Scripts\activate

# Install Strands and tools
pip install strands-agents strands-agents-tools
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Features at a Glance&lt;/h2&gt; 
&lt;h3&gt;Python-Based Tools&lt;/h3&gt; 
&lt;p&gt;Easily build tools using Python decorators:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from strands import Agent, tool

@tool
def word_count(text: str) -&amp;gt; int:
    """Count words in text.

    This docstring is used by the LLM to understand the tool's purpose.
    """
    return len(text.split())

agent = Agent(tools=[word_count])
response = agent("How many words are in this sentence?")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Hot Reloading from Directory:&lt;/strong&gt; Enable automatic tool loading and reloading from the &lt;code&gt;./tools/&lt;/code&gt; directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from strands import Agent

# Agent will watch ./tools/ directory for changes
agent = Agent(load_tools_from_directory=True)
response = agent("Use any tools you find in the tools directory")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;MCP Support&lt;/h3&gt; 
&lt;p&gt;Seamlessly integrate Model Context Protocol (MCP) servers:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from strands import Agent
from strands.tools.mcp import MCPClient
from mcp import stdio_client, StdioServerParameters

aws_docs_client = MCPClient(
    lambda: stdio_client(StdioServerParameters(command="uvx", args=["awslabs.aws-documentation-mcp-server@latest"]))
)

with aws_docs_client:
   agent = Agent(tools=aws_docs_client.list_tools_sync())
   response = agent("Tell me about Amazon Bedrock and how to use it with Python")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Multiple Model Providers&lt;/h3&gt; 
&lt;p&gt;Support for various model providers:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from strands import Agent
from strands.models import BedrockModel
from strands.models.ollama import OllamaModel
from strands.models.llamaapi import LlamaAPIModel
from strands.models.gemini import GeminiModel
from strands.models.llamacpp import LlamaCppModel

# Bedrock
bedrock_model = BedrockModel(
  model_id="us.amazon.nova-pro-v1:0",
  temperature=0.3,
  streaming=True, # Enable/disable streaming
)
agent = Agent(model=bedrock_model)
agent("Tell me about Agentic AI")

# Google Gemini
gemini_model = GeminiModel(
  client_args={
    "api_key": "your_gemini_api_key",
  },
  model_id="gemini-2.5-flash",
  params={"temperature": 0.7}
)
agent = Agent(model=gemini_model)
agent("Tell me about Agentic AI")

# Ollama
ollama_model = OllamaModel(
  host="http://localhost:11434",
  model_id="llama3"
)
agent = Agent(model=ollama_model)
agent("Tell me about Agentic AI")

# Llama API
llama_model = LlamaAPIModel(
    model_id="Llama-4-Maverick-17B-128E-Instruct-FP8",
)
agent = Agent(model=llama_model)
response = agent("Tell me about Agentic AI")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Built-in providers:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/user-guide/concepts/model-providers/amazon-bedrock/"&gt;Amazon Bedrock&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/user-guide/concepts/model-providers/anthropic/"&gt;Anthropic&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/user-guide/concepts/model-providers/gemini/"&gt;Gemini&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/user-guide/concepts/model-providers/cohere/"&gt;Cohere&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/user-guide/concepts/model-providers/litellm/"&gt;LiteLLM&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/user-guide/concepts/model-providers/llamacpp/"&gt;llama.cpp&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/user-guide/concepts/model-providers/llamaapi/"&gt;LlamaAPI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/user-guide/concepts/model-providers/mistral/"&gt;MistralAI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/user-guide/concepts/model-providers/ollama/"&gt;Ollama&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/user-guide/concepts/model-providers/openai/"&gt;OpenAI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/user-guide/concepts/model-providers/sagemaker/"&gt;SageMaker&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/user-guide/concepts/model-providers/writer/"&gt;Writer&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Custom providers can be implemented using &lt;a href="https://strandsagents.com/latest/user-guide/concepts/model-providers/custom_model_provider/"&gt;Custom Providers&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Example tools&lt;/h3&gt; 
&lt;p&gt;Strands offers an optional strands-agents-tools package with pre-built tools for quick experimentation:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from strands import Agent
from strands_tools import calculator
agent = Agent(tools=[calculator])
agent("What is the square root of 1764")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It's also available on GitHub via &lt;a href="https://github.com/strands-agents/tools"&gt;strands-agents/tools&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Bidirectional Streaming&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;‚ö†Ô∏è Experimental Feature&lt;/strong&gt;: Bidirectional streaming is currently in experimental status. APIs may change in future releases as we refine the feature based on user feedback and evolving model capabilities.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Build real-time voice and audio conversations with persistent streaming connections. Unlike traditional request-response patterns, bidirectional streaming maintains long-running conversations where users can interrupt, provide continuous input, and receive real-time audio responses. Get started with your first BidiAgent by following the &lt;a href="https://strandsagents.com/latest/documentation/docs/user-guide/concepts/experimental/bidirectional-streaming/quickstart"&gt;Quickstart&lt;/a&gt; guide.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Supported Model Providers:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Amazon Nova Sonic (&lt;code&gt;amazon.nova-sonic-v1:0&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Google Gemini Live (&lt;code&gt;gemini-2.5-flash-native-audio-preview-09-2025&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;OpenAI Realtime API (&lt;code&gt;gpt-realtime&lt;/code&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Quick Example:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import asyncio
from strands.experimental.bidi import BidiAgent
from strands.experimental.bidi.models import BidiNovaSonicModel
from strands.experimental.bidi.io import BidiAudioIO, BidiTextIO
from strands.experimental.bidi.tools import stop_conversation
from strands_tools import calculator

async def main():
    # Create bidirectional agent with audio model
    model = BidiNovaSonicModel()
    agent = BidiAgent(model=model, tools=[calculator, stop_conversation])

    # Setup audio and text I/O
    audio_io = BidiAudioIO()
    text_io = BidiTextIO()

    # Run with real-time audio streaming
    # Say "stop conversation" to gracefully end the conversation
    await agent.run(
        inputs=[audio_io.input()],
        outputs=[audio_io.output(), text_io.output()]
    )

if __name__ == "__main__":
    asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Configuration Options:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Configure audio settings
model = BidiNovaSonicModel(
    provider_config={
        "audio": {
            "input_rate": 16000,
            "output_rate": 16000,
            "voice": "matthew"
        },
        "inference": {
            "max_tokens": 2048,
            "temperature": 0.7
        }
    }
)

# Configure I/O devices
audio_io = BidiAudioIO(
    input_device_index=0,  # Specific microphone
    output_device_index=1,  # Specific speaker
    input_buffer_size=10,
    output_buffer_size=10
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;For detailed guidance &amp;amp; examples, explore our documentation:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/"&gt;User Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/user-guide/quickstart/"&gt;Quick Start Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/user-guide/concepts/agents/agent-loop/"&gt;Agent Loop&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/examples/"&gt;Examples&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/api-reference/agent/"&gt;API Reference&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/user-guide/deploy/operating-agents-in-production/"&gt;Production &amp;amp; Deployment Guide&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing ‚ù§Ô∏è&lt;/h2&gt; 
&lt;p&gt;We welcome contributions! See our &lt;a href="https://raw.githubusercontent.com/strands-agents/sdk-python/main/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt; for details on:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Reporting bugs &amp;amp; features&lt;/li&gt; 
 &lt;li&gt;Development setup&lt;/li&gt; 
 &lt;li&gt;Contributing via Pull Requests&lt;/li&gt; 
 &lt;li&gt;Code of Conduct&lt;/li&gt; 
 &lt;li&gt;Reporting of security issues&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the Apache License 2.0 - see the &lt;a href="https://raw.githubusercontent.com/strands-agents/sdk-python/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h2&gt;Security&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/strands-agents/sdk-python/main/CONTRIBUTING.md#security-issue-notifications"&gt;CONTRIBUTING&lt;/a&gt; for more information.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>zhaochenyang20/Awesome-ML-SYS-Tutorial</title>
      <link>https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial</link>
      <description>&lt;p&gt;My learning notes for ML SYS.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Awesome-ML-SYS-Tutorial&lt;/h1&gt; 
&lt;h2&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/README.md"&gt;English Version&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/README-cn.md"&gt;Chinese Version&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;My learning notes for ML SYS.&lt;/p&gt; 
&lt;p&gt;I've been writing this blog series intermittently for over a year now, and it's almost become an RL Infra Learning Note üòÇ&lt;/p&gt; 
&lt;p&gt;I often see discussions about whether ML SYS or AI Infra is worth getting into, and how to start. Everyone's choice is different. For me, I simply want to &lt;strong&gt;pursue the truth in algorithms&lt;/strong&gt;:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;A large number of RL conclusions derived from papers are based on RL infrastructure in the open-source community that may be extremely flawed. I've been involved in RL infra development for over a year, and I've seen numerous community experts diligently working, but the fact is that RL infra, whether open-source or within major companies, still has many problems. It is absolutely worth questioning whether the high-level conclusions drawn from this flawed infrastructure are correct. When I was reviewing for ICLR this year, I often asked the papers assigned to me, "If the framework you are using has implementation issues itself, can your conclusions still hold?" Although I never deducted points for this reason, no one could provide an answer that resolved my fundamental doubt.&lt;/p&gt; 
 &lt;p&gt;Therefore, some excellent researchers I know are keen to participate in infra development, spending most of their time on foundational work to rigorously ensure that the algorithm they plan to develop next has a correct basis. I greatly admire them and agree with such rigor‚Äîthey are my role models. The same is true for our SGLang RL community. With so much human power and time, we all hope to provide the most correct and concise RL foundation possible, whether it's for companies training models or researchers developing new algorithms, with the goal of genuinely serving everyone in the community. Thank you for your recognition, and I look forward to hearing from interested friends who wish to contact me and join us!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;After a year of going around in circles, this is the resolve that keeps me going in Infra: &lt;strong&gt;to make a contribution to the community by building a correct foundation, thereby helping to ensure correct conclusions.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Coming back to the topic, this series of podcasts started in August 2024, when I began learning ML SYS notes following the opportunity to use &lt;a href="https://github.com/sgl-project/sglang"&gt;SGLang&lt;/a&gt; during my research. It's largely written by me, with content focusing on &lt;strong&gt;RL infra, online/offline inference systems, and some fundamentals of AI Infra&lt;/strong&gt;. Over the past year, starting from two or three articles and thirty to fifty Github Stars, to now exceeding 4.5K Stars, I have become a minor technical influencer. I am deeply honored and grateful for the support.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;I would like to thank my advisors, Professor Quanquan Gu, Dr. Ying Sheng, and Dr. Linmin Zheng&lt;/strong&gt;, for the immense help and guidance they gave me in my study of AI Infra, career choices, and life path. Although I am no longer pursuing a Ph.D. at UCLA due to personal reasons, this journey after my undergraduate graduation has been an incredibly valuable experience. I have now joined RadixArk full-time, continuing my research in RL Infra. We will continue to share AI Infra-related technology and thoughts through my blog, via unofficial channels. &lt;strong&gt;I also hope readers interested in AI Infra reach out to us, join the SGLang open-source community, and together build open-source AI Infra that changes the world and is worth being proud of for a lifetime!&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;RLHF System Development Notes&lt;/h2&gt; 
&lt;h3&gt;slime Framework&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;„ÄêNot finished„Äë&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/slime/mismatch/blog-en.md"&gt;Achieving Speed and Accuracy: A Comprehensive Solution to Train-Inference Mismatch in RL&lt;/a&gt;: Introduces two solutions provided by the slime framework for the train-inference mismatch problem: achieving perfect True On-Policy training through kernel-level alignment, and mitigating the mismatch using algorithms like TIS/MIS. Also available in &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/slime/mismatch/blog-cn.md"&gt;Chinese version&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/slime/fsdp/readme_en.md"&gt;Support FSDP2 as A Training Backend for slime&lt;/a&gt;: Added FSDP as a training backend to slime, and aligned it with Megatron. FSDP is more flexible in supporting models with architectural innovations like Qwen3-Next/gpt-oss and helps us further support VLM RL. Also available in &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/slime/fsdp/readme.md"&gt;Chinese version&lt;/a&gt; and on &lt;a href="https://zhuanlan.zhihu.com/p/1979141713449742500"&gt;Zhihu&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/slime/fp8/readme_en.md"&gt;Unified FP8: Moving Beyond Mixed Precision for Stable and Accelerated MoE RL&lt;/a&gt;: Fully utilizing FP8 for both sampling (Rollout) and training (Training) in RL. Also available in &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/slime/fp8/readme.md"&gt;Chinese version&lt;/a&gt; and on &lt;a href="https://zhuanlan.zhihu.com/p/1974681194017865986"&gt;Zhihu&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/slime/spec/readme-en.md"&gt;Power Up Speculative Decoding In Reinforcement Learning&lt;/a&gt;: Introduces speculative decoding into the RL sampling process, significantly boosting sampling speed when the batch size is appropriate; moreover, the draft model is updated during training. Compared to freezing the draft model, the accepted length remains consistently high, yielding long-term stable positive returns. Also available in &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/slime/spec/readme.md"&gt;Chinese version&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/slime/code-walk-through/readme_en.md"&gt;An In-Depth Look at the Elegant Design and Source Code of the slime RL Framework&lt;/a&gt;: slime source code appreciation. Also available on &lt;a href="https://zhuanlan.zhihu.com/p/1946402397409740613"&gt;Zhihu&lt;/a&gt; and in &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/slime/code-walk-through/readme.md"&gt;Chinese version&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[Pending Review] &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/slime/fsdp/release_log/setup_fsdp.md"&gt;slime FSDP Setup Guide&lt;/a&gt;: Records how to test FSDP on slime, including H-cards and B-cards, and both Colocate and Disaggregated placement methods.&lt;/li&gt; 
 &lt;li&gt;[Pending Review] &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/slime/batch-GAE/ppo-gae-chunk.md"&gt;Chunked Parallel Computation of GAE in PPO (slime Implementation)&lt;/a&gt;: Rewrites the standard backward recurrence of GAE into chunk-based parallel prefix scanning, significantly mitigating the GAE computation bottleneck in long sequence scenarios, achieving about $100\times‚Äì300\times$ acceleration in slime. Also available on &lt;a href="https://zhuanlan.zhihu.com/p/1975237289425798560"&gt;Zhihu&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;AReal Framework&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/areal/code-walk-through_EN.md"&gt;AReal Code Walk Through&lt;/a&gt; AReal source code appreciation. Also available on &lt;a href="https://zhuanlan.zhihu.com/p/1983417813080236770"&gt;Zhihu&lt;/a&gt; and in &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/areal/code-walk-through_CN.md"&gt;Chinese version&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;System Design and Optimization&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/sys-design/readme-4-en.md"&gt;Deep Dive into DeepSeek MoE with Classic Secondary Development of EP on FSDP&lt;/a&gt;: Deep dive into DeepSeek MoE with classic secondary development of EP on FSDP. Also available in &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/sys-design/readme-4.md"&gt;Chinese version&lt;/a&gt; and &lt;a href="https://zhuanlan.zhihu.com/p/1990790333823481023"&gt;zhihu&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/sys-design/readme-1-EN.md"&gt;Deep Thoughts on RL Systems: In-Depth Understanding of Weight Update Mechanism&lt;/a&gt;: Summary of half a year's work, in-depth understanding of the weight update mechanism. Also available on &lt;a href="https://zhuanlan.zhihu.com/p/1925210722704531547"&gt;Zhihu&lt;/a&gt; and in &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/sys-design/readme-1.md"&gt;Chinese version&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/sys-design/readme-2-en.md"&gt;Deep Thoughts on RL Systems: FSDP Training Backend&lt;/a&gt;: Discusses the principles and implementation of FSDP, and analyzes verl's use of FSDP. Also available on &lt;a href="https://zhuanlan.zhihu.com/p/1929115059113693341"&gt;Zhihu&lt;/a&gt; and in &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/sys-design/readme-2.md"&gt;Chinese version&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[Pending Review] &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/sys-design/readme-3.md"&gt;Deep Thoughts on RL Systems: Megatron&lt;/a&gt;: Brief analysis of Megatron's basic features, focusing on its use in the RL framework.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/OpenRLHF/develop-log.md"&gt;Extending the OpenRLHF Inference Engine&lt;/a&gt;: Development notes on integrating SGLang into OpenRLHF. The entire process was very painful, and there's still an nccl hang error that a DeepSpeed core contributor is currently fixing.&lt;/li&gt; 
 &lt;li&gt;[Pending Review] &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/GRPO/SGLang_GRPO.md"&gt;SGLang as rollout engine of GRPO trainer&lt;/a&gt;: Introduction on how to use SGLang as the inference backend for the GRPO Trainer in TRL. GRPO is a PPO variant that optimizes PPO's memory usage while improving mathematical reasoning capabilities.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;verl Framework&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/torch/mem-snapshot/readme-en.md"&gt;Analyzing VLM RL Training Memory Leaks via Torch Memory Snapshot&lt;/a&gt;: Analysis of SGLang memory leak issues and solutions. Also available on &lt;a href="https://zhuanlan.zhihu.com/p/1943202817247519535"&gt;Zhihu&lt;/a&gt; and in &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/torch/mem-snapshot/readme.md"&gt;Chinese version&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/sglang/latency-accelerate-for-weight-updates/readme.md"&gt;Latency optimization for weight updates&lt;/a&gt;: A debug process for efficiency. Also available on &lt;a href="https://zhuanlan.zhihu.com/p/9908228168"&gt;Zhihu: A record of optimizing SGLang weight update latency&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/verl/multi-turn/code-walk-through/readme_EN.md"&gt;In-Depth Understanding of verl Source Code (Initialization)&lt;/a&gt;: Also available on &lt;a href="https://zhuanlan.zhihu.com/p/1920751852749849692"&gt;Zhihu&lt;/a&gt; and in &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/verl/multi-turn/code-walk-through/readme.md"&gt;Chinese version&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/verl/multi-turn/code-walk-through/readme-2-EN.md"&gt;In-Depth Understanding of verl Source Code (Rollout)&lt;/a&gt;: Also available on &lt;a href="https://zhuanlan.zhihu.com/p/1923349757566388159"&gt;Zhihu&lt;/a&gt; and in &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/verl/multi-turn/code-walk-through/readme-2.md"&gt;Chinese version&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[Pending Review] &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/verl/multi-turn/code-walk-through/readme-3.md"&gt;In-Depth Understanding of verl Source Code (Make Experience)&lt;/a&gt;: Analysis of the logic for the make experience part in verl.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/verl/multi-turn/code-walk-through/readme-6.md"&gt;AgentLoop Source Code Analysis&lt;/a&gt;: Analysis of the multi-turn RL implementation based on AgentLoop in verl.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/verl/multi-turn/code-walk-through/readme-5-EN.md"&gt;verl Parameter Quick Reference&lt;/a&gt;: Quick reference for verl parameters. Also available on &lt;a href="https://zhuanlan.zhihu.com/p/1925041836998783250"&gt;Zhihu&lt;/a&gt; and in &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/verl/multi-turn/code-walk-through/readme-5.md"&gt;Chinese version&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/verl/multi-turn/fast_tokenization/multiturn_tokenization_and_masking.md"&gt;Analyzing the Complexity of Agentic Multi-Turn Training from a Tokenizer Perspective&lt;/a&gt;: Also available on &lt;a href="https://zhuanlan.zhihu.com/p/1917126584806139373"&gt;Zhihu&lt;/a&gt; and in &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/verl/multi-turn/fast_tokenization/multiturn_tokenization_and_masking_ZH.md"&gt;Chinese version&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[Pending Review] &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/verl/multi-turn/code-walk-through/dapo.md"&gt;DAPO Dynamic Filtering Implementation and Batch Size Analysis&lt;/a&gt;: Exploring how to achieve higher parallelism by padding prompts to a smaller batch size.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/verl/multi-turn/tool_examples/profile_en.md"&gt;Systematic Analysis of Time Consumption in verl Multi-Turn Training&lt;/a&gt;: verl multi-turn interaction and tool call profile analysis. Also available in &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/verl/multi-turn/tool_examples/profile.md"&gt;Chinese version&lt;/a&gt; and on &lt;a href="https://zhuanlan.zhihu.com/p/1929748460212552414"&gt;Zhihu&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/verl/multi-turn/release_log/verl-multiturn-rollout-Release_ZH.md"&gt;SGLang, verl, OpenBMB, and Tsinghua University Team Jointly Open Source: First Support for Multi-Turn Interaction and Tool Calling in Mainstream RLHF Frameworks&lt;/a&gt;: First support for multi-turn interaction and tool calling in mainstream RLHF frameworks. Also available on &lt;a href="https://zhuanlan.zhihu.com/p/1906007821889283171"&gt;Zhihu&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/verl/multi-turn/tool_examples/verl-multiturn-searchR1-like_ZH.md"&gt;Search-R1 &amp;amp; veRL-SGLang: Train LLMs with Multi-Turn RL to Reason and Call a Search Engine&lt;/a&gt;: Integrating the Search-R1 framework into the verl-sglang ecosystem. Also available on &lt;a href="https://zhuanlan.zhihu.com/p/1912156329751081620"&gt;Zhihu&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/verl/server-based/veRL-server-based-rollout.md"&gt;SGLang-veRL Server: From Engine to Server, We Need More Flexible RLHF Rollout Interfaces&lt;/a&gt;: To implement more complex RLHF systems, we are gradually replacing the rollout engine in veRL with a rollout server. Also available on &lt;a href="https://zhuanlan.zhihu.com/p/1890631652486665464"&gt;Zhihu: SGLang-veRL Server&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/verl/readme.md"&gt;HybridFlow veRL Original Paper Analysis&lt;/a&gt;: Principles and implementation of SGLang's hybrid engine. Also available on &lt;a href="https://zhuanlan.zhihu.com/p/24682036412"&gt;Zhihu: HybridFlow veRL Original Paper Analysis&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;OpenRLHF Framework&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/677607581"&gt;Illustrated Series on LLM RLHF: PPO Principles and Source Code Interpretation for Everyone&lt;/a&gt; and &lt;a href="https://zhuanlan.zhihu.com/p/12871616401"&gt;Illustrated Distributed Training Process based on Ray in OpenRLHF&lt;/a&gt;: Excellent RLHF introductory resources by Ms. Mengyuan. After reading, you will have a good understanding of RLHF's computational flow and the OpenRLHF PPO framework. I have also added my own understanding in &lt;a href="https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/tree/main/rlhf/OpenRLHF#rlhf-%E7%9A%84%E8%AE%A1%E7%AE%97%E6%B5%81"&gt;RLHF Computational Flow&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/OpenRLHF/readme.md"&gt;Brief Analysis of the Computational Flow of Post-Training Systems Represented by OpenRLHF&lt;/a&gt;: Further complement to Ms. Mengyuan's article. The Github native rendering is terrible; you might as well look at &lt;a href="https://zhuanlan.zhihu.com/p/16370000391"&gt;Zhihu&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Algorithms and Theory&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/partial-rollout/readme.md"&gt;Kimi K1.5: Successful Practice of Long Context RL&lt;/a&gt;: Industrial implementation of Long Context RLHF. I have always liked the technical reports from the Kimi team. Also available on &lt;a href="https://zhuanlan.zhihu.com/p/1894282607325344277"&gt;Zhihu: Kimi K1.5: Successful Practice of Long Context RL&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/13211508979"&gt;Rule-based Reward&lt;/a&gt;: Only on Zhihu, a brief write-up. Honestly, I didn't particularly like the original paper, but determined reward is indeed charming.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/16292266518"&gt;SWE-Bench: How to Construct an Excellent Benchmark in the LLM Era&lt;/a&gt;: Reading notes on the SWE-Bench paper. How to construct a good benchmark to provide fine-grained reward for post-training is an eternal and beautiful topic.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/5220718268"&gt;Brief Analysis of Mainstream Alignment Algorithms and the NeMo-Aligner Framework&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;SGLang Learning Notes&lt;/h2&gt; 
&lt;h3&gt;SGLang Diffusion Learning Notes&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/sglang/diffusion-llm/readme-en.md"&gt;Power Up Diffusion LLMs: Day‚Äë0 Support for LLaDA‚ÄØ2.0&lt;/a&gt;: Introduction to the implementation of LLaDA2.0-flash-CAP in SGLang. Also available on &lt;a href="https://zhuanlan.zhihu.com/p/1985516215326749534"&gt;Zhihu&lt;/a&gt; and in &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/sglang/diffusion-llm/readme.md"&gt;Chinese version&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/sglang/code-walk-through/sgl_diffusion_en.md"&gt;SGLang Diffusion Code Walk Through&lt;/a&gt;: Basic principles of the diffusion model, and the entire process of a request being handled by SGLang-Diffusion. Also available on &lt;a href="https://zhuanlan.zhihu.com/p/1982441236066480797"&gt;Zhihu&lt;/a&gt; and in &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/sglang/code-walk-through/sgl_diffusion.md"&gt;Chinese version&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Core Architecture and Optimization&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/sglang/code-walk-through/readme.md"&gt;SGLang Code Walk Through&lt;/a&gt;: The entire process of a request being handled by the SGLang Engine. Some parts are unfinished, but most are okay and have served as a starting point for many SGLang beginners. &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/sglang/code-walk-through/readme-CN.md"&gt;Chinese version is here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/sglang/sglang-worker/readme.md"&gt;Walk Through SGLang / VLLM Worker&lt;/a&gt;: Incomplete analysis of SGLang code. Also available on &lt;a href="https://zhuanlan.zhihu.com/p/6363614076"&gt;Walk Through SGLang / VLLM Worker&lt;/a&gt;. We also thoughtfully provide an &lt;a href="https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/raw/main/sglang/sglang-worker/readme.md"&gt;English version&lt;/a&gt;. For a more detailed analysis, refer to &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/sglang/code-walk-through/readme.md"&gt;SGLang Code Walk Through&lt;/a&gt;; this one is just supplementary.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/sglang/sglang-scheduler/readme-CN.md"&gt;Walk Through SGLang Scheduler&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[Pending Review] &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/sglang/scheduler-evolution/SGLang%20Scheduler%20%E6%8A%80%E6%9C%AF%E5%8F%98%E8%BF%81.md"&gt;SGLang Scheduler Evolution&lt;/a&gt;: Detailed introduction to the technical evolution of the SGLang Scheduler from serial to CPU/GPU overlap, and related components, comparing the previous overlap Scheduler with the current one introducing multiple CUDA streams and FutureMap. Can be viewed on &lt;a href="https://zhuanlan.zhihu.com/p/1969077475129688722"&gt;Zhihu article&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[Pending Review] &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/sglang/kvcache-code-walk-through/readme.md"&gt;KV Cache Code Walkthrough&lt;/a&gt;: Overview of KV cache management implementation, starting from the Scheduler component, detailing the update process of KV cache and memory pool during prefill and decode stages.&lt;/li&gt; 
 &lt;li&gt;[Pending Review] &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/sglang/code-walk-through/multimodal_request_lifecycle.md"&gt;SGLang Multimodal Request Lifecycle: A Deep Architectural Analysis with Qwen2.5-VL as an Example&lt;/a&gt;: Provides a detailed analysis of the multimodal request processing flow within the SGLang framework, using Qwen2.5-VL as a reference model.&lt;/li&gt; 
 &lt;li&gt;[Pending Review] &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/sglang/how-model-is-loaded/readme.md"&gt;How A Model is Loaded in Hugging Face and SGLang&lt;/a&gt;: Documents the process of loading models in Hugging Face and SGLang to help understand the weight loading mechanism.&lt;/li&gt; 
 &lt;li&gt;[Pending Review] &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/sglang/speculative-decoding/speculative-decoding.md"&gt;Speculative Decoding&lt;/a&gt;: Introduces the speculative decoding optimization technique, which uses a smaller draft model to predict the next $K$ tokens, achieving up to $K$-fold acceleration.&lt;/li&gt; 
 &lt;li&gt;[Pending Review] &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/sglang/zero-overhead-scheduler/zero-overhead-batch-scheduler.md"&gt;Zero-Overhead Batch Scheduler&lt;/a&gt;: Introduces the zero-overhead batch scheduler, which solves the GPU Bubble problem caused by serial execution of CPU scheduling and GPU computation in traditional inference systems.&lt;/li&gt; 
 &lt;li&gt;[Pending Review] &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/sglang/dp-attention/readme.md"&gt;Data Parallelism Attention&lt;/a&gt;: Detailed introduction to the principles and implementation of DP Attention, specifically for models like DeepSeek that use MLA and only have one KV head, to avoid KV cache duplication caused by tensor parallelism.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/sglang/quantization/quantization_architecture_en.md"&gt;Brief Analysis of SGLang Framework's Quantization Design and Ideas&lt;/a&gt;: Also available on &lt;a href="https://zhuanlan.zhihu.com/p/1971183020338832111"&gt;Zhihu: Brief Analysis of SGLang Framework's Quantization Design and Ideas&lt;/a&gt; and in &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/sglang/quantization/quantization_architecture.md"&gt;Chinese version&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/sglang/constraint-decoding/readme.md"&gt;Constraint Decoding: Concepts, Methods, and Optimization&lt;/a&gt;: Also available on &lt;a href="https://zhuanlan.zhihu.com/p/18336995950"&gt;Zhihu: Understanding Constraint Decoding: Concepts, Methods, and Optimization in one article&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[Pending Review] &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/sglang/online-update-weights/readme.md"&gt;Online Update Weights&lt;/a&gt;: Introduction to the implementation of the &lt;code&gt;online_update_weights&lt;/code&gt; interface in SGLang. Unlike &lt;code&gt;update_weights&lt;/code&gt; which reads weights from the disk, this interface broadcasts new weights directly from the training engine via NCCL.&lt;/li&gt; 
 &lt;li&gt;[Pending Review] &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/sglang/sglang-verl-engine/readme.md"&gt;SGLang Verl Engine Optimization Analysis&lt;/a&gt;: Analysis of optimizations in the SGLang verl engine, including the implementation of interfaces like &lt;code&gt;update_weights_from_tensor&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/sglang/latency-accelerate-for-weight-updates/readme-CN.md"&gt;Latency Accelerate For Weight Updates&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[üî• Related Debugging] &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/torch/mem-snapshot/readme-en.md"&gt;Analyzing VLM RL Training Memory Leaks via Torch Memory Snapshot&lt;/a&gt;&lt;/strong&gt;: Analysis of SGLang memory leak issues and solutions. Also available on &lt;a href="https://zhuanlan.zhihu.com/p/1943202817247519535"&gt;Zhihu&lt;/a&gt; and in &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/torch/mem-snapshot/readme.md"&gt;Chinese version&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Usage and Practice&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;[Pending Review] &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/sglang/qwen/coder.md"&gt;Qwen3-Coder Usage&lt;/a&gt;: Introduction to using Qwen3-coder in SGLang, including the use of tool-parser.&lt;/li&gt; 
 &lt;li&gt;[Pending Review] &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/sglang/nvidia-dynamo/dynamo.md"&gt;NVIDIA Dynamo&lt;/a&gt;: Introduction to NVIDIA Dynamo, a high-throughput, low-latency inference framework designed for generative AI and inference model serving in multi-node distributed environments.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/9912733791"&gt;Viewing HuggingFace Model Structure&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/716543182"&gt;SGLang Backend Original Paper Analysis&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/4148050391"&gt;Brief Analysis of the Status Quo of Reward / Embed Model Server Engine&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/714833359"&gt;Newbie Perspective: Experience and Gains from Migrating vllm to SGLang&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/715805386"&gt;Newbie Perspective: Using SGL to Serve Embedding Model&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/715857723"&gt;Newbie Perspective: Using vllm to serve a new Embedding Model&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Scheduling and Routing&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/1711346141"&gt;Mooncake: Carrying the P/D Separation to the End&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/1280567902"&gt;Should Prefill and Decode be Separated onto Different Cards?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/718715866"&gt;Understanding Prefill and Decode Computation Characteristics Based on Chunked Prefill&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/718015016"&gt;ModelServer: A Frontend Distribution System Based on SGLang&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ML System Fundamentals&lt;/h2&gt; 
&lt;h3&gt;Transformers &amp;amp; Model Architecture&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;[Pending Review] &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/transformers/attention/cross_attention_en.md"&gt;Cross-Attention Mechanism in Transformer&lt;/a&gt;: Introduction to the cross-attention mechanism in Transformers, allowing the decoder to access and use relevant information from the encoder. Also available in &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/transformers/attention/cross_attention.md"&gt;Chinese version&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/transformers/special_tokens/special_tokens.md"&gt;Understanding Special Tokens and Chat Templates in One Article&lt;/a&gt;: Also recorded on Zhihu &lt;a href="https://zhuanlan.zhihu.com/p/17052593700"&gt;Understanding Special Tokens and Chat Templates in One Article&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;CUDA &amp;amp; GPU&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/torch/cuda-graph/readme_en.md"&gt;Brief Analysis of CUDA Graph Based on torch-memory-savor&lt;/a&gt;: Also available on &lt;a href="https://zhuanlan.zhihu.com/p/1921726788574360686"&gt;Zhihu: Brief Analysis of CUDA Graph Based on torch-memory-savor&lt;/a&gt; and in &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/torch/cuda-graph/readme.md"&gt;Chinese version&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Distributed Training &amp;amp; Communication&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;[Pending Review] &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/torch/tensor-parallelism/readme.md"&gt;Implementing Tensor Parallelism From Scratch&lt;/a&gt;: Implementation and practice of Tensor Parallelism.&lt;/li&gt; 
 &lt;li&gt;[Pending Review] &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/rlhf/sys-design/readme-4.md"&gt;Expert Parallelism&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/torch/nccl/readme.md"&gt;NCCL and NVIDIA TOPO&lt;/a&gt;: Introduction to NCCL and NVIDIA GPU detection. Also available on &lt;a href="https://zhuanlan.zhihu.com/p/6160835906"&gt;NCCL and NVIDIA TOPO&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/torch/nccl/readme_en.md"&gt;NCCL and SGLang&lt;/a&gt;: Application of NCCL in SGLang. This is very similar to the Chinese content but includes some additional notes on parallel strategies. I probably won't complete this note and will write a separate one to record parallel strategies.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/torch/torch-distributed/readme.md"&gt;PyTorch Distributed&lt;/a&gt;: Communication practice with &lt;code&gt;torch.distributed&lt;/code&gt;, details on GIL and &lt;code&gt;all_reduce&lt;/code&gt;. This part is also available on &lt;a href="https://zhuanlan.zhihu.com/p/5853094319"&gt;Zhihu: PyTorch Communication Practice&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/178402798"&gt;[Original][In-Depth][PyTorch] DDP Series Part 1: Introductory Tutorial&lt;/a&gt;: Although I didn't fully grasp the DDP content, I used this to learn about GIL and ring all reduce. This step is recorded in the &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/torch/torch-distributed/readme.md#gil"&gt;Postscript of torch-distributed&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.yourmetaverse.cn/deep_learning/199/"&gt;Detailed Explanation of nvidia-smi Command and Some Advanced Tips&lt;/a&gt;: Mainly about network topology; my local results are recorded in the &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/torch/nccl/readme.md#nvlink-%E6%9F%A5%E8%AF%A2"&gt;NCCL section&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Quantization&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/5485556270"&gt;Give me BF16 or Give Me Death: Comprehensive Evaluation of Current Quantization Methods&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/942485319"&gt;AWQ: Model Quantization Should Focus on Activation Values&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Developer Guide&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/engineer/how-to-use-docker/readme_en.md"&gt;How to use docker&lt;/a&gt;: How to use Docker to manage development environments. Please note that to collectively foster a good research environment and prevent others from being annoyed by the baseline "it runs on my machine," learning Docker is essential for everyone. We also have a &lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/engineer/how-to-use-docker/readme.md"&gt;Chinese version&lt;/a&gt; and &lt;a href="https://zhuanlan.zhihu.com/p/1916764175230801287"&gt;Zhihu&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/main/engineer/uv/readme.md"&gt;Setting up a Clean Development Environment&lt;/a&gt;: Setting up a clean development environment. Also available on &lt;a href="https://zhuanlan.zhihu.com/p/23440683394"&gt;Zhihu: Setting up a Clean Development Environment&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/2382351079"&gt;Compiling and Deploying Jupyter Notebooks as Documentation on CI&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>DrewThomasson/ebook2audiobook</title>
      <link>https://github.com/DrewThomasson/ebook2audiobook</link>
      <description>&lt;p&gt;Generate audiobooks from e-books, voice cloning &amp; 1158+ languages!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;üìö ebook2audiobook&lt;/h1&gt; 
&lt;p&gt;CPU/GPU Converter from eBooks to audiobooks with chapters and metadata&lt;br /&gt; using XTTSv2, Bark, Vits, Fairseq, YourTTS, Tacotron2 and more. Supports voice cloning and 1158 languages!&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] &lt;strong&gt;This tool is intended for use with non-DRM, legally acquired eBooks only.&lt;/strong&gt; &lt;br /&gt; The authors are not responsible for any misuse of this software or any resulting legal consequences. &lt;br /&gt; Use this tool responsibly and in accordance with all applicable laws.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;a href="https://discord.gg/63Tv3F65k6"&gt;&lt;img src="https://dcbadge.limes.pink/api/server/https://discord.gg/63Tv3F65k6" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Thanks to support ebook2audiobook developers!&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://ko-fi.com/athomasson2"&gt;&lt;img src="https://img.shields.io/badge/Ko--fi-F16061?style=for-the-badge&amp;amp;logo=ko-fi&amp;amp;logoColor=white" alt="Ko-Fi" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Run locally&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#launching-gradio-web-interface"&gt;&lt;img src="https://img.shields.io/badge/Quick%20Start-blue?style=for-the-badge" alt="Quick Start" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/DrewThomasson/ebook2audiobook/actions/workflows/Docker-Build.yml"&gt;&lt;img src="https://github.com/DrewThomasson/ebook2audiobook/actions/workflows/Docker-Build.yml/badge.svg?sanitize=true" alt="Docker Build" /&gt;&lt;/a&gt; &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/releases/latest"&gt;&lt;img src="https://img.shields.io/badge/Download-Now-blue.svg?sanitize=true" alt="Download" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;a href="https://github.com/DrewThomasson/ebook2audiobook"&gt; &lt;img src="https://img.shields.io/badge/Platform-mac%20|%20linux%20|%20windows-lightgrey" alt="Platform" /&gt; &lt;/a&gt;
&lt;a href="https://hub.docker.com/r/athomasson2/ebook2audiobook"&gt; &lt;img alt="Docker Pull Count" src="https://img.shields.io/docker/pulls/athomasson2/ebook2audiobook.svg?sanitize=true" /&gt; &lt;/a&gt; 
&lt;h3&gt;Run Remotely&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/spaces/drewThomasson/ebook2audiobook"&gt;&lt;img src="https://img.shields.io/badge/Hugging%20Face-Spaces-yellow?style=flat&amp;amp;logo=huggingface" alt="Hugging Face" /&gt;&lt;/a&gt; &lt;a href="https://colab.research.google.com/github/DrewThomasson/ebook2audiobook/blob/main/Notebooks/colab_ebook2audiobook.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Free Google Colab" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Rihcus/ebook2audiobookXTTS/raw/main/Notebooks/kaggle-ebook2audiobook.ipynb"&gt;&lt;img src="https://img.shields.io/badge/Kaggle-035a7d?style=flat&amp;amp;logo=kaggle&amp;amp;logoColor=white" alt="Kaggle" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;GUI Interface&lt;/h4&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/assets/demo_web_gui.gif" alt="demo_web_gui" /&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click to see images of Web GUI&lt;/summary&gt; 
 &lt;img width="1728" alt="GUI Screen 1" src="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/assets/gui_1.png" /&gt; 
 &lt;img width="1728" alt="GUI Screen 2" src="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/assets/gui_2.png" /&gt; 
 &lt;img width="1728" alt="GUI Screen 3" src="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/assets/gui_3.png" /&gt; 
&lt;/details&gt; 
&lt;h2&gt;Demos&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;New Default Voice Demo&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/750035dc-e355-46f1-9286-05c1d9e88cea"&gt;https://github.com/user-attachments/assets/750035dc-e355-46f1-9286-05c1d9e88cea&lt;/a&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;More Demos&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;ASMR Voice&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/68eee9a1-6f71-4903-aacd-47397e47e422"&gt;https://github.com/user-attachments/assets/68eee9a1-6f71-4903-aacd-47397e47e422&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Rainy Day Voice&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/d25034d9-c77f-43a9-8f14-0d167172b080"&gt;https://github.com/user-attachments/assets/d25034d9-c77f-43a9-8f14-0d167172b080&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Scarlett Voice&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/b12009ee-ec0d-45ce-a1ef-b3a52b9f8693"&gt;https://github.com/user-attachments/assets/b12009ee-ec0d-45ce-a1ef-b3a52b9f8693&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;David Attenborough Voice&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/81c4baad-117e-4db5-ac86-efc2b7fea921"&gt;https://github.com/user-attachments/assets/81c4baad-117e-4db5-ac86-efc2b7fea921&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://github.com/DrewThomasson/VoxNovel/raw/dc5197dff97252fa44c391dc0596902d71278a88/readme_files/example_in_app.jpeg" alt="Example" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;README.md&lt;/h2&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#-ebook2audiobook"&gt;ebook2audiobook&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#features"&gt;Features&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#gui-interface"&gt;GUI Interface&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#demos"&gt;Demos&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#supported-languages"&gt;Supported Languages&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#hardware-requirements"&gt;Minimum Requirements&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#launching-gradio-web-interface"&gt;Usage&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#launching-gradio-web-interface"&gt;Run Locally&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#launching-gradio-web-interface"&gt;Launching Gradio Web Interface&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#basic--usage"&gt;Basic Headless Usage&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#example-of-custom-model-zip-upload"&gt;Headless Custom XTTS Model Usage&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#help-command-output"&gt;Help command output&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#run-remotely"&gt;Run Remotely&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#docker"&gt;Docker&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#steps-to-run"&gt;Steps to Run&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#common-docker-issues"&gt;Common Docker Issues&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#fine-tuned-tts-models"&gt;Fine Tuned TTS models&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#fine-tuned-tts-collection"&gt;Collection of Fine-Tuned TTS Models&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#fine-tune-your-own-xttsv2-model"&gt;Train XTTSv2&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#supported-ebook-formats"&gt;Supported eBook Formats&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#output-formats"&gt;Output Formats&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#updating-to-latest-version"&gt;Updating to Latest Version&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#reverting-to-older-versions"&gt;Revert to older Version&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#common-issues"&gt;Common Issues&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#special-thanks"&gt;Special Thanks&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#table-of-contents"&gt;Table of Contents&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìö Splits eBook into chapters for organized audio.&lt;/li&gt; 
 &lt;li&gt;üéôÔ∏è High-quality text-to-speech with &lt;a href="https://huggingface.co/coqui/XTTS-v2"&gt;XTTSv2&lt;/a&gt;, &lt;a href="https://github.com/facebookresearch/fairseq/tree/main/examples/mms"&gt;Fairseq&lt;/a&gt; and much more.&lt;/li&gt; 
 &lt;li&gt;üó£Ô∏è Optional voice cloning with your own voice file.&lt;/li&gt; 
 &lt;li&gt;üó£Ô∏è Optional custom model with your own training model.&lt;/li&gt; 
 &lt;li&gt;üåç Supports 1158 languages. &lt;a href="https://dl.fbaipublicfiles.com/mms/tts/all-tts-languages.html"&gt;List of Supported languages&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üñ•Ô∏è Designed to run on 2GB RAM 1GB VRAM Min.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Supported Languages&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;&lt;strong&gt;Arabic (ar)&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;strong&gt;Chinese (zh)&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;strong&gt;English (en)&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;strong&gt;Spanish (es)&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;French (fr)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;German (de)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Italian (it)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Portuguese (pt)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Polish (pl)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Turkish (tr)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Russian (ru)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Dutch (nl)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Czech (cs)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Japanese (ja)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Hindi (hi)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Bengali (bn)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Hungarian (hu)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Korean (ko)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Vietnamese (vi)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Swedish (sv)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Persian (fa)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Yoruba (yo)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Swahili (sw)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Indonesian (id)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Slovak (sk)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Croatian (hr)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Tamil (ta)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Danish (da)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://dl.fbaipublicfiles.com/mms/tts/all-tts-languages.html"&gt;&lt;strong&gt;+1130 languages and dialects here&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Hardware Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;2GB RAM min, 8GB recommended.&lt;/li&gt; 
 &lt;li&gt;1GB VRAM min, 4GB recommended.&lt;/li&gt; 
 &lt;li&gt;Virtualization enabled if running on windows (Docker only).&lt;/li&gt; 
 &lt;li&gt;CPU (intel, AMD, ARM)*.&lt;/li&gt; 
 &lt;li&gt;GPU (CUDA, ROCm, XPU).&lt;/li&gt; 
 &lt;li&gt;MPS (Apple Silicon CPU).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;*&lt;i&gt; Modern TTS engines are very slow on CPU&lt;/i&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] &lt;strong&gt;Before to post an install or bug issue search carefully to the opened and closed issues TAB&lt;br /&gt; to be sure your issue does not exist already.&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] &lt;strong&gt;EPUB format lacks any standard structure like what is a chapter, paragraph, preface etc.&lt;br /&gt; So you should first remove manually any text you don't want to be converted in audio.&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Instructions&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Clone repo&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/DrewThomasson/ebook2audiobook.git
cd ebook2audiobook
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install / Run ebook2audiobook&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Linux/MacOS&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;./ebook2audiobook.sh  # Run launch script
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;i&gt;Note for MacOS users: homebrew is installed to install missing programs.&lt;/i&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Mac Launcher&lt;/strong&gt;&lt;br /&gt; Double click &lt;code&gt;Mac Ebook2Audiobook Launcher.command&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Windows&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;ebook2audiobook.cmd
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;or Double click &lt;code&gt;ebook2audiobook.cmd&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;i&gt;Note for Windows users: scoop is installed to install missing programs without administrator privileges.&lt;/i&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Open the Web App&lt;/strong&gt;: Click the URL provided in the terminal to access the web app and convert eBooks. &lt;code&gt;http://localhost:7860/&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;For Public Link&lt;/strong&gt;: &lt;code&gt;./ebook2audiobook.sh --share&lt;/code&gt; (Linux/MacOS) &lt;code&gt;ebook2audiobook.cmd --share&lt;/code&gt; (Windows) &lt;code&gt;python app.py --share&lt;/code&gt; (all OS)&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] &lt;strong&gt;If the script is stopped and run again, you need to refresh your gradio GUI interface&lt;br /&gt; to let the web page reconnect to the new connection socket.&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Basic Usage&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Linux/MacOS&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;./ebook2audiobook.sh --headless --ebook &amp;lt;path_to_ebook_file&amp;gt; --voice [path_to_voice_file] --language [language_code]
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Windows&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;ebook2audiobook.cmd --headless --ebook &amp;lt;path_to_ebook_file&amp;gt; --voice [path_to_voice_file] --language [language_code]
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;[--ebook]&lt;/strong&gt;: Path to your eBook file&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;[--voice]&lt;/strong&gt;: Voice cloning file path (optional)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;[--language]&lt;/strong&gt;: Language code in ISO-639-3 (i.e.: ita for italian, eng for english, deu for german...).&lt;br /&gt; Default language is eng and --language is optional for default language set in ./lib/lang.py.&lt;br /&gt; The ISO-639-1 2 letters codes are also supported.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Example of Custom Model Zip Upload&lt;/h3&gt; 
&lt;p&gt;(must be a .zip file containing the mandatory model files. Example for XTTSv2: config.json, model.pth, vocab.json and ref.wav)&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Linux/MacOS&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;./ebook2audiobook.sh --headless --ebook &amp;lt;ebook_file_path&amp;gt; --language &amp;lt;language&amp;gt; --custom_model &amp;lt;custom_model_path&amp;gt;
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Windows&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;ebook2audiobook.cmd --headless --ebook &amp;lt;ebook_file_path&amp;gt; --language &amp;lt;language&amp;gt; --custom_model &amp;lt;custom_model_path&amp;gt;
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;i&gt;Note: the ref.wav of your custom model is always the voice selected for the conversion&lt;/i&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&amp;lt;custom_model_path&amp;gt;&lt;/strong&gt;: Path to &lt;code&gt;model_name.zip&lt;/code&gt; file, which must contain (according to the tts engine) all the mandatory files&lt;br /&gt; (see ./lib/models.py).&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;For Detailed Guide with list of all Parameters to use&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Linux/MacOS&lt;/strong&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;./ebook2audiobook.sh --help
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;ebook2audiobook.cmd --help
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Or for all OS&lt;/strong&gt; &lt;code&gt;python app.py --help &lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a id="help-command-output"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;usage: app.py [-h] [--session SESSION] [--share] [--headless] [--ebook EBOOK] [--ebooks_dir EBOOKS_DIR]
              [--language LANGUAGE] [--voice VOICE] [--device {CPU,CUDA,MPS,ROCM,XPU,JETSON}]
              [--tts_engine {XTTSv2,BARK,VITS,FAIRSEQ,TACOTRON2,YOURTTS,xtts,bark,vits,fairseq,tacotron,yourtts}]
              [--custom_model CUSTOM_MODEL] [--fine_tuned FINE_TUNED] [--output_format OUTPUT_FORMAT]
              [--output_channel OUTPUT_CHANNEL] [--temperature TEMPERATURE] [--length_penalty LENGTH_PENALTY]
              [--num_beams NUM_BEAMS] [--repetition_penalty REPETITION_PENALTY] [--top_k TOP_K] [--top_p TOP_P]
              [--speed SPEED] [--enable_text_splitting] [--text_temp TEXT_TEMP] [--waveform_temp WAVEFORM_TEMP]
              [--output_dir OUTPUT_DIR] [--version]

Convert eBooks to Audiobooks using a Text-to-Speech model. You can either launch the Gradio interface or run the script in headless mode for direct conversion.

options:
  -h, --help            show this help message and exit
  --session SESSION     Session to resume the conversion in case of interruption, crash,
                            or reuse of custom models and custom cloning voices.

**** The following options are for all modes:
  Optional

**** The following option are for gradio/gui mode only:
  Optional

  --share               Enable a public shareable Gradio link.

**** The following options are for --headless mode only:
  --headless            Run the script in headless mode
  --ebook EBOOK         Path to the ebook file for conversion. Cannot be used when --ebooks_dir is present.
  --ebooks_dir EBOOKS_DIR
                        Relative or absolute path of the directory containing the files to convert.
                            Cannot be used when --ebook is present.
  --language LANGUAGE   Language of the e-book. Default language is set
                            in ./lib/lang.py sed as default if not present. All compatible language codes are in ./lib/lang.py

optional parameters:
  --voice VOICE         (Optional) Path to the voice cloning file for TTS engine.
                            Uses the default voice if not present.
  --device {CPU,CUDA,MPS,ROCM,XPU,JETSON}
                        (Optional) Processor unit type for the conversion.
                            Default is set in ./lib/conf.py if not present. Fall back to CPU if CUDA or MPS is not available.
  --tts_engine {XTTSv2,BARK,VITS,FAIRSEQ,TACOTRON2,YOURTTS,xtts,bark,vits,fairseq,tacotron,yourtts}
                        (Optional) Preferred TTS engine (available are: ['XTTSv2', 'BARK', 'VITS', 'FAIRSEQ', 'TACOTRON2', 'YOURTTS', 'xtts', 'bark', 'vits', 'fairseq', 'tacotron', 'yourtts'].
                            Default depends on the selected language. The tts engine should be compatible with the chosen language
  --custom_model CUSTOM_MODEL
                        (Optional) Path to the custom model zip file cntaining mandatory model files.
                            Please refer to ./lib/models.py
  --fine_tuned FINE_TUNED
                        (Optional) Fine tuned model path. Default is builtin model.
  --output_format OUTPUT_FORMAT
                        (Optional) Output audio format. Default is m4b set in ./lib/conf.py
  --output_channel OUTPUT_CHANNEL
                        (Optional) Output audio channel. Default is mono set in ./lib/conf.py
  --temperature TEMPERATURE
                        (xtts only, optional) Temperature for the model.
                            Default to config.json model. Higher temperatures lead to more creative outputs.
  --length_penalty LENGTH_PENALTY
                        (xtts only, optional) A length penalty applied to the autoregressive decoder.
                            Default to config.json model. Not applied to custom models.
  --num_beams NUM_BEAMS
                        (xtts only, optional) Controls how many alternative sequences the model explores. Must be equal or greater than length penalty.
                            Default to config.json model.
  --repetition_penalty REPETITION_PENALTY
                        (xtts only, optional) A penalty that prevents the autoregressive decoder from repeating itself.
                            Default to config.json model.
  --top_k TOP_K         (xtts only, optional) Top-k sampling.
                            Lower values mean more likely outputs and increased audio generation speed.
                            Default to config.json model.
  --top_p TOP_P         (xtts only, optional) Top-p sampling.
                            Lower values mean more likely outputs and increased audio generation speed. Default to config.json model.
  --speed SPEED         (xtts only, optional) Speed factor for the speech generation.
                            Default to config.json model.
  --enable_text_splitting
                        (xtts only, optional) Enable TTS text splitting. This option is known to not be very efficient.
                            Default to config.json model.
  --text_temp TEXT_TEMP
                        (bark only, optional) Text Temperature for the model.
                            Default to config.json model.
  --waveform_temp WAVEFORM_TEMP
                        (bark only, optional) Waveform Temperature for the model.
                            Default to config.json model.
  --output_dir OUTPUT_DIR
                        (Optional) Path to the output directory. Default is set in ./lib/conf.py
  --version             Show the version of the script and exit

Example usage:
Windows:
    Gradio/GUI:
    ebook2audiobook.cmd
    Headless mode:
    ebook2audiobook.cmd --headless --ebook '/path/to/file' --language eng
Linux/Mac:
    Gradio/GUI:
    ./ebook2audiobook.sh
    Headless mode:
    ./ebook2audiobook.sh --headless --ebook '/path/to/file' --language eng

Docker build image:
    Windows:
    ebook2audiobook.cmd --script_mode build_docker
    Linux/Mac
    ./ebook2audiobook.sh --script_mode build_docker
Docker run image:
    Gradio/GUI:
        CPU:
        docker run --rm -it -p 7860:7860 ebook2audiobook:cpu
        CUDA:
        docker run --gpus all --rm -it -p 7860:7860 ebook2audiobook:cu[118/121/128 etc..]
        ROCM:
        docker run --device=/dev/kfd --device=/dev/dri --rm -it -p 7860:7860 ebook2audiobook:rocm[5.5/6.1/6.4 etc..]
        XPU:
        docker run --device=/dev/dri --rm -it -p 7860:7860 ebook2audiobook:xpu
        JETSON:
        docker run --runtime nvidia  --rm -it -p 7860:7860 ebook2audiobook:jetson[51/60/61 etc...]
    Headless mode:
        CPU:
        docker run --rm -it -v "/my/real/ebooks/folder/absolute/path:/app/ebooks" -v "/my/real/output/folder/absolute/path:/app/audiobooks" -p 7860:7860 ebook2audiobook:cpu --headless --ebook "/app/ebooks/myfile.pdf" [--voice /app/my/voicepath/voice.mp3 etc..]
        CUDA:
        docker run --gpus all --rm -it -v "/my/real/ebooks/folder/absolute/path:/app/ebooks" -v "/my/real/output/folder/absolute/path:/app/audiobooks" -p 7860:7860 ebook2audiobook:cu[118/121/128 etc..] --headless --ebook "/app/ebooks/myfile.pdf" [--voice /app/my/voicepath/voice.mp3 etc..]
        ROCM:
        docker run --device=/dev/kfd --device=/dev/dri --rm -it -v "/my/real/ebooks/folder/absolute/path:/app/ebooks" -v "/my/real/output/folder/absolute/path:/app/audiobooks" -p 7860:7860 ebook2audiobook:rocm[5.5/6.1/6.4 etc..] --headless --ebook "/app/ebooks/myfile.pdf" [--voice /app/my/voicepath/voice.mp3 etc..]
        XPU:
        docker run --device=/dev/dri --rm -it -v "/my/real/ebooks/folder/absolute/path:/app/ebooks" -v "/my/real/output/folder/absolute/path:/app/audiobooks" -p 7860:7860 ebook2audiobook:xpu --headless --ebook "/app/ebooks/myfile.pdf" [--voice /app/my/voicepath/voice.mp3 etc..]
        JETSON:
        docker run --runtime nvidia --rm -it -v "/my/real/ebooks/folder/absolute/path:/app/ebooks" -v "/my/real/output/folder/absolute/path:/app/audiobooks" -p 7860:7860 ebook2audiobook:jetson[51/60/61 etc...] --headless --ebook "/app/ebooks/myfile.pdf" [--voice /app/my/voicepath/voice.mp3 etc..]

    Docker Compose (i.e. for cuda 11.8, add --build to rebuild):
        DEVICE_TAG=cu118 docker compose up -d

    Podman Compose (i.e. for cuda 12.4, add --build to rebuild):
        DEVICE_TAG=cu124 podman-compose up -d

    * MPS is not exposed in docker so CPU must be used.

Tip: to add of silence (random duration between 1.0 and 1.8 seconds) into your text just use "###" or "[pause]".

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;NOTE: in gradio/gui mode, to cancel a running conversion, just click on the [X] from the ebook upload component.&lt;/p&gt; 
&lt;p&gt;TIP: if it needs some more pauses, just add '###' or '[pause]' between the words you wish more pause. one [pause] is a random between 0.8 to 1.6 seconds&lt;/p&gt; 
&lt;h3&gt;Docker&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Clone the Repository&lt;/strong&gt;:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;   git clone https://github.com/DrewThomasson/ebook2audiobook.git
   cd ebook2audiobook
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;&lt;strong&gt;Build the container&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;   # Windows
   ebook2audiobook.cmd --script_mode build_docker

   # Linux/MacOS
   ./ebook2audiobook.sh --script_mode build_docker 
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="4"&gt; 
 &lt;li&gt;&lt;strong&gt;Run the Container:&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;	# Gradio/GUI:

	# CPU:
		docker run --rm -it -p 7860:7860 ebook2audiobook:cpu
	# CUDA:
		docker run --gpus all --rm -it -p 7860:7860 ebook2audiobook:cu[118/121/128 etc..]
	# ROCM:
		docker run --device=/dev/kfd --device=/dev/dri --rm -it -p 7860:7860 ebook2audiobook:rocm[5.5/6.1/6.4 etc..]
	# XPU:
		docker run --device=/dev/dri --rm -it -p 7860:7860 ebook2audiobook:xpu
	# JETSON:
		docker run --runtime nvidia  --rm -it -p 7860:7860 ebook2audiobook:jetson[51/60/61 etc...]
	
	# Headless mode examples:
	
	# CPU:
		docker run --rm -it -v "/my/real/ebooks/folder/absolute/path:/app/ebooks" -v "/my/real/output/folder/absolute/path:/app/audiobooks" -p 7860:7860 ebook2audiobook:cpu --headless --ebook "/app/ebooks/myfile.pdf" [--voice /app/my/voicepath/voice.mp3 etc..]
	# CUDA:
		docker run --gpus all --rm -it -v "/my/real/ebooks/folder/absolute/path:/app/ebooks" -v "/my/real/output/folder/absolute/path:/app/audiobooks" -p 7860:7860 ebook2audiobook:cu[118/121/128 etc..] --headless --ebook "/app/ebooks/myfile.pdf" [--voice /app/my/voicepath/voice.mp3 etc..]
	# ROCM:
		docker run --device=/dev/kfd --device=/dev/dri --rm -it -v "/my/real/ebooks/folder/absolute/path:/app/ebooks" -v "/my/real/output/folder/absolute/path:/app/audiobooks" -p 7860:7860 ebook2audiobook:rocm[5.5/6.1/6.4 etc..] --headless --ebook "/app/ebooks/myfile.pdf" [--voice /app/my/voicepath/voice.mp3 etc..]
	# XPU:
		docker run --device=/dev/dri --rm -it -v "/my/real/ebooks/folder/absolute/path:/app/ebooks" -v "/my/real/output/folder/absolute/path:/app/audiobooks" -p 7860:7860 ebook2audiobook:xpu --headless --ebook "/app/ebooks/myfile.pdf" [--voice /app/my/voicepath/voice.mp3 etc..]
	# JETSON:
		docker run --runtime nvidia --rm -it -v "/my/real/ebooks/folder/absolute/path:/app/ebooks" -v "/my/real/output/folder/absolute/path:/app/audiobooks" -p 7860:7860 ebook2audiobook:jetson[51/60/61 etc...] --headless --ebook "/app/ebooks/myfile.pdf" [--voice /app/my/voicepath/voice.mp3 etc..]

    # Docker Compose (example for cuda 12.9)
    docker-compose up -d
    DEVICE_TAG=cu128 docker compose up -d
    # To stop -&amp;gt; docker-compose down

    # Podman Compose (example for cuda 12.8)
    podman compose -f podman-compose.yml up
    DEVICE_TAG=cu128 podman-compose up -d
    # To stop -&amp;gt; podman compose -f podman-compose.yml down
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;NOTE: MPS is not exposed in docker so CPU must be used&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Common Docker Issues&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;My NVIDIA GPU isn't being detected?? -&amp;gt; &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/wiki/GPU-ISSUES"&gt;GPU ISSUES Wiki Page&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Fine Tuned TTS models&lt;/h2&gt; 
&lt;h4&gt;Fine Tune your own XTTSv2 model&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/spaces/drewThomasson/xtts-finetune-webui-gpu"&gt;&lt;img src="https://img.shields.io/badge/Hugging%20Face-Spaces-yellow?style=flat&amp;amp;logo=huggingface" alt="Hugging Face" /&gt;&lt;/a&gt; &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/raw/v25/Notebooks/finetune/xtts/kaggle-xtts-finetune-webui-gradio-gui.ipynb"&gt;&lt;img src="https://img.shields.io/badge/Kaggle-035a7d?style=flat&amp;amp;logo=kaggle&amp;amp;logoColor=white" alt="Kaggle" /&gt;&lt;/a&gt; &lt;a href="https://colab.research.google.com/github/DrewThomasson/ebook2audiobook/blob/v25/Notebooks/finetune/xtts/colab_xtts_finetune_webui.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;De-noise training data&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/spaces/drewThomasson/DeepFilterNet2_no_limit"&gt;&lt;img src="https://img.shields.io/badge/Hugging%20Face-Spaces-yellow?style=flat&amp;amp;logo=huggingface" alt="Hugging Face" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Rikorose/DeepFilterNet"&gt;&lt;img src="https://img.shields.io/badge/DeepFilterNet-181717?logo=github" alt="GitHub Repo" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Fine Tuned TTS Collection&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/drewThomasson/fineTunedTTSModels/tree/main"&gt;&lt;img src="https://img.shields.io/badge/Hugging%20Face-Models-yellow?style=flat&amp;amp;logo=huggingface" alt="Hugging Face" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;For an XTTSv2 custom model a ref audio clip of the voice reference is mandatory:&lt;/p&gt; 
&lt;h2&gt;Supported eBook Formats&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;.epub&lt;/code&gt;, &lt;code&gt;.pdf&lt;/code&gt;, &lt;code&gt;.mobi&lt;/code&gt;, &lt;code&gt;.txt&lt;/code&gt;, &lt;code&gt;.html&lt;/code&gt;, &lt;code&gt;.rtf&lt;/code&gt;, &lt;code&gt;.chm&lt;/code&gt;, &lt;code&gt;.lit&lt;/code&gt;, &lt;code&gt;.pdb&lt;/code&gt;, &lt;code&gt;.fb2&lt;/code&gt;, &lt;code&gt;.odt&lt;/code&gt;, &lt;code&gt;.cbr&lt;/code&gt;, &lt;code&gt;.cbz&lt;/code&gt;, &lt;code&gt;.prc&lt;/code&gt;, &lt;code&gt;.lrf&lt;/code&gt;, &lt;code&gt;.pml&lt;/code&gt;, &lt;code&gt;.snb&lt;/code&gt;, &lt;code&gt;.cbc&lt;/code&gt;, &lt;code&gt;.rb&lt;/code&gt;, &lt;code&gt;.tcr&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Best results&lt;/strong&gt;: &lt;code&gt;.epub&lt;/code&gt; or &lt;code&gt;.mobi&lt;/code&gt; for automatic chapter detection&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Output and process Formats&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;.m4b&lt;/code&gt;, &lt;code&gt;.m4a&lt;/code&gt;, &lt;code&gt;.mp4&lt;/code&gt;, &lt;code&gt;.webm&lt;/code&gt;, &lt;code&gt;.mov&lt;/code&gt;, &lt;code&gt;.mp3&lt;/code&gt;, &lt;code&gt;.flac&lt;/code&gt;, &lt;code&gt;.wav&lt;/code&gt;, &lt;code&gt;.ogg&lt;/code&gt;, &lt;code&gt;.aac&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Process format can be changed in lib/conf.py&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Your own Ebook2Audiobook customization&lt;/h2&gt; 
&lt;p&gt;You are free to modify libs/conf.py to add or remove the settings you wish. If you plan to do it just make a copy of the original conf.py so on each ebook2audiobook update you will backup your modified conf.py and put back the original one. You must plan the same process for models.py. If you wish to make your own custom model as an official ebook2audiobook fine tuned model so please contact us and we'll ad it to the models.py list.&lt;/p&gt; 
&lt;h2&gt;Reverting to older Versions&lt;/h2&gt; 
&lt;p&gt;Releases can be found -&amp;gt; &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/releases"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git checkout tags/VERSION_NUM # Locally/Compose -&amp;gt; Example: git checkout tags/v25.7.7
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Common Issues:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;My NVIDIA/ROCm/XPU/MPS GPU isn't being detected?? -&amp;gt; &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/wiki/GPU-ISSUES"&gt;GPU ISSUES Wiki Page&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;CPU is slow (better on server smp CPU) while GPU can have almost real time conversion. &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/discussions/19#discussioncomment-10879846"&gt;Discussion about this&lt;/a&gt; For faster multilingual generation I would suggest my other &lt;a href="https://github.com/DrewThomasson/ebook2audiobookpiper-tts"&gt;project that uses piper-tts&lt;/a&gt; instead (It doesn't have zero-shot voice cloning though, and is Siri quality voices, but it is much faster on cpu).&lt;/li&gt; 
 &lt;li&gt;"I'm having dependency issues" - Just use the docker, its fully self contained and has a headless mode, add &lt;code&gt;--help&lt;/code&gt; parameter at the end of the docker run command for more information.&lt;/li&gt; 
 &lt;li&gt;"Im getting a truncated audio issue!" - PLEASE MAKE AN ISSUE OF THIS, we don't speak every language and need advise from users to fine tune the sentence splitting logic.üòä&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;What we need help with! üôå&lt;/h2&gt; 
&lt;h2&gt;&lt;a href="https://github.com/DrewThomasson/ebook2audiobook/issues/32"&gt;Full list of things can be found here&lt;/a&gt;&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Any help from people speaking any of the supported languages to help us improve the models&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!--
## Do you need to rent a GPU to boost service from us?
- A poll is open here https://github.com/DrewThomasson/ebook2audiobook/discussions/889
--&gt; 
&lt;h2&gt;Special Thanks&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Coqui TTS&lt;/strong&gt;: &lt;a href="https://github.com/idiap/coqui-ai-TTS"&gt;Coqui TTS GitHub&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Calibre&lt;/strong&gt;: &lt;a href="https://calibre-ebook.com"&gt;Calibre Website&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;FFmpeg&lt;/strong&gt;: &lt;a href="https://ffmpeg.org"&gt;FFmpeg Website&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/DrewThomasson/ebook2audiobook/issues/8"&gt;@shakenbake15 for better chapter saving method&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>HKUDS/LightRAG</title>
      <link>https://github.com/HKUDS/LightRAG</link>
      <description>&lt;p&gt;[EMNLP2025] "LightRAG: Simple and Fast Retrieval-Augmented Generation"&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;div style="margin: 20px 0;"&gt; 
  &lt;img src="https://raw.githubusercontent.com/HKUDS/LightRAG/main/assets/logo.png" width="120" height="120" alt="LightRAG Logo" style="border-radius: 20px; box-shadow: 0 8px 32px rgba(0, 217, 255, 0.3);" /&gt; 
 &lt;/div&gt; 
 &lt;h1&gt;üöÄ LightRAG: Simple and Fast Retrieval-Augmented Generation&lt;/h1&gt; 
 &lt;div align="center"&gt; 
  &lt;a href="https://trendshift.io/repositories/13043" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/13043" alt="HKUDS%2FLightRAG | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; 
 &lt;/div&gt; 
 &lt;div align="center"&gt; 
  &lt;div style="width: 100%; height: 2px; margin: 20px 0; background: linear-gradient(90deg, transparent, #00d9ff, transparent);"&gt;&lt;/div&gt; 
 &lt;/div&gt; 
 &lt;div align="center"&gt; 
  &lt;div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; padding: 25px; text-align: center;"&gt; 
   &lt;p&gt; &lt;a href="https://github.com/HKUDS/LightRAG"&gt;&lt;img src="https://img.shields.io/badge/üî•Project-Page-00d9ff?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2410.05779"&gt;&lt;img src="https://img.shields.io/badge/üìÑarXiv-2410.05779-ff6b6b?style=for-the-badge&amp;amp;logo=arxiv&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;a href="https://github.com/HKUDS/LightRAG/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/HKUDS/LightRAG?color=00d9ff&amp;amp;style=for-the-badge&amp;amp;logo=star&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;/p&gt; 
   &lt;p&gt; &lt;img src="https://img.shields.io/badge/üêçPython-3.10-4ecdc4?style=for-the-badge&amp;amp;logo=python&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt; &lt;a href="https://pypi.org/project/lightrag-hku/"&gt;&lt;img src="https://img.shields.io/pypi/v/lightrag-hku.svg?style=for-the-badge&amp;amp;logo=pypi&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e&amp;amp;color=ff6b6b" /&gt;&lt;/a&gt; &lt;/p&gt; 
   &lt;p&gt; &lt;a href="https://discord.gg/yF2MmDJyGJ"&gt;&lt;img src="https://img.shields.io/badge/üí¨Discord-Community-7289da?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;a href="https://github.com/HKUDS/LightRAG/issues/285"&gt;&lt;img src="https://img.shields.io/badge/üí¨WeChat-Group-07c160?style=for-the-badge&amp;amp;logo=wechat&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;/p&gt; 
   &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/LightRAG/main/README-zh.md"&gt;&lt;img src="https://img.shields.io/badge/üá®üá≥‰∏≠ÊñáÁâà-1a1a2e?style=for-the-badge" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/LightRAG/main/README.md"&gt;&lt;img src="https://img.shields.io/badge/üá∫üá∏English-1a1a2e?style=for-the-badge" /&gt;&lt;/a&gt; &lt;/p&gt; 
   &lt;p&gt; &lt;a href="https://pepy.tech/projects/lightrag-hku"&gt;&lt;img src="https://static.pepy.tech/personalized-badge/lightrag-hku?period=total&amp;amp;units=INTERNATIONAL_SYSTEM&amp;amp;left_color=BLACK&amp;amp;right_color=GREEN&amp;amp;left_text=downloads" /&gt;&lt;/a&gt; &lt;/p&gt; 
  &lt;/div&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div align="center" style="margin: 30px 0;"&gt; 
 &lt;img src="https://user-images.githubusercontent.com/74038190/212284100-561aa473-3905-4a80-b561-0d28506553ee.gif" width="800" /&gt; 
&lt;/div&gt; 
&lt;div align="center" style="margin: 30px 0;"&gt; 
 &lt;img src="https://raw.githubusercontent.com/HKUDS/LightRAG/main/README.assets/b2aaf634151b4706892693ffb43d9093.png" width="800" alt="LightRAG Diagram" /&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üéâ News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;[2025.11]üéØ[New Feature]: Integrated &lt;strong&gt;RAGAS for Evaluation&lt;/strong&gt; and &lt;strong&gt;Langfuse for Tracing&lt;/strong&gt;. Updated the API to return retrieved contexts alongside query results to support context precision metrics.&lt;/li&gt; 
 &lt;li&gt;[2025.10]üéØ[Scalability Enhancement]: Eliminated processing bottlenecks to support &lt;strong&gt;Large-Scale Datasets Efficiently&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;[2025.09]üéØ[New Feature] Enhances knowledge graph extraction accuracy for &lt;strong&gt;Open-Sourced LLMs&lt;/strong&gt; such as Qwen3-30B-A3B.&lt;/li&gt; 
 &lt;li&gt;[2025.08]üéØ[New Feature] &lt;strong&gt;Reranker&lt;/strong&gt; is now supported, significantly boosting performance for mixed queries (set as default query mode).&lt;/li&gt; 
 &lt;li&gt;[2025.08]üéØ[New Feature] Added &lt;strong&gt;Document Deletion&lt;/strong&gt; with automatic KG regeneration to ensure optimal query performance.&lt;/li&gt; 
 &lt;li&gt;[2025.06]üéØ[New Release] Our team has released &lt;a href="https://github.com/HKUDS/RAG-Anything"&gt;RAG-Anything&lt;/a&gt; ‚Äî an &lt;strong&gt;All-in-One Multimodal RAG&lt;/strong&gt; system for seamless processing of text, images, tables, and equations.&lt;/li&gt; 
 &lt;li&gt;[2025.06]üéØ[New Feature] LightRAG now supports comprehensive multimodal data handling through &lt;a href="https://github.com/HKUDS/RAG-Anything"&gt;RAG-Anything&lt;/a&gt; integration, enabling seamless document parsing and RAG capabilities across diverse formats including PDFs, images, Office documents, tables, and formulas. Please refer to the new &lt;a href="https://github.com/HKUDS/LightRAG/?tab=readme-ov-file#multimodal-document-processing-rag-anything-integration"&gt;multimodal section&lt;/a&gt; for details.&lt;/li&gt; 
 &lt;li&gt;[2025.03]üéØ[New Feature] LightRAG now supports citation functionality, enabling proper source attribution and enhanced document traceability.&lt;/li&gt; 
 &lt;li&gt;[2025.02]üéØ[New Feature] You can now use MongoDB as an all-in-one storage solution for unified data management.&lt;/li&gt; 
 &lt;li&gt;[2025.02]üéØ[New Release] Our team has released &lt;a href="https://github.com/HKUDS/VideoRAG"&gt;VideoRAG&lt;/a&gt;-a RAG system for understanding extremely long-context videos&lt;/li&gt; 
 &lt;li&gt;[2025.01]üéØ[New Release] Our team has released &lt;a href="https://github.com/HKUDS/MiniRAG"&gt;MiniRAG&lt;/a&gt; making RAG simpler with small models.&lt;/li&gt; 
 &lt;li&gt;[2025.01]üéØYou can now use PostgreSQL as an all-in-one storage solution for data management.&lt;/li&gt; 
 &lt;li&gt;[2024.11]üéØ[New Resource] A comprehensive guide to LightRAG is now available on &lt;a href="https://learnopencv.com/lightrag"&gt;LearnOpenCV&lt;/a&gt;. ‚Äî explore in-depth tutorials and best practices. Many thanks to the blog author for this excellent contribution!&lt;/li&gt; 
 &lt;li&gt;[2024.11]üéØ[New Feature] Introducing the LightRAG WebUI ‚Äî an interface that allows you to insert, query, and visualize LightRAG knowledge through an intuitive web-based dashboard.&lt;/li&gt; 
 &lt;li&gt;[2024.11]üéØ[New Feature] You can now &lt;a href="https://github.com/HKUDS/LightRAG?tab=readme-ov-file#using-neo4j-for-storage"&gt;use Neo4J for Storage&lt;/a&gt;-enabling graph database support.&lt;/li&gt; 
 &lt;li&gt;[2024.10]üéØ[New Feature] We've added a link to a &lt;a href="https://youtu.be/oageL-1I0GE"&gt;LightRAG Introduction Video&lt;/a&gt;. ‚Äî a walkthrough of LightRAG's capabilities. Thanks to the author for this excellent contribution!&lt;/li&gt; 
 &lt;li&gt;[2024.10]üéØ[New Channel] We have created a &lt;a href="https://discord.gg/yF2MmDJyGJ"&gt;Discord channel&lt;/a&gt;!üí¨ Welcome to join our community for sharing, discussions, and collaboration! üéâüéâ&lt;/li&gt; 
 &lt;li&gt;[2024.10]üéØ[New Feature] LightRAG now supports &lt;a href="https://github.com/HKUDS/LightRAG?tab=readme-ov-file#quick-start"&gt;Ollama models&lt;/a&gt;!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary style="font-size: 1.4em; font-weight: bold; cursor: pointer; display: list-item;"&gt; Algorithm Flowchart &lt;/summary&gt; 
 &lt;p&gt;&lt;img src="https://learnopencv.com/wp-content/uploads/2024/11/LightRAG-VectorDB-Json-KV-Store-Indexing-Flowchart-scaled.jpg" alt="LightRAG Indexing Flowchart" /&gt; &lt;em&gt;Figure 1: LightRAG Indexing Flowchart - Img Caption : &lt;a href="https://learnopencv.com/lightrag/"&gt;Source&lt;/a&gt;&lt;/em&gt; &lt;img src="https://learnopencv.com/wp-content/uploads/2024/11/LightRAG-Querying-Flowchart-Dual-Level-Retrieval-Generation-Knowledge-Graphs-scaled.jpg" alt="LightRAG Retrieval and Querying Flowchart" /&gt; &lt;em&gt;Figure 2: LightRAG Retrieval and Querying Flowchart - Img Caption : &lt;a href="https://learnopencv.com/lightrag/"&gt;Source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;üí° Using uv for Package Management&lt;/strong&gt;: This project uses &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt; for fast and reliable Python package management. Install uv first: &lt;code&gt;curl -LsSf https://astral.sh/uv/install.sh | sh&lt;/code&gt; (Unix/macOS) or &lt;code&gt;powershell -c "irm https://astral.sh/uv/install.ps1 | iex"&lt;/code&gt; (Windows)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: You can also use pip if you prefer, but uv is recommended for better performance and more reliable dependency management.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;üì¶ Offline Deployment&lt;/strong&gt;: For offline or air-gapped environments, see the &lt;a href="https://raw.githubusercontent.com/HKUDS/LightRAG/main/docs/OfflineDeployment.md"&gt;Offline Deployment Guide&lt;/a&gt; for instructions on pre-installing all dependencies and cache files.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Install LightRAG Server&lt;/h3&gt; 
&lt;p&gt;The LightRAG Server is designed to provide Web UI and API support. The Web UI facilitates document indexing, knowledge graph exploration, and a simple RAG query interface. LightRAG Server also provide an Ollama compatible interfaces, aiming to emulate LightRAG as an Ollama chat model. This allows AI chat bot, such as Open WebUI, to access LightRAG easily.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Install from PyPI&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Using uv (recommended)
uv pip install "lightrag-hku[api]"
# Or using pip
# pip install "lightrag-hku[api]"

# Build front-end artifacts
cd lightrag_webui
bun install --frozen-lockfile
bun run build
cd ..

# setup env file
cp env.example .env  # Update the .env with your LLM and embedding configurations
# Launch the server
lightrag-server
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Installation from Source&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/HKUDS/LightRAG.git
cd LightRAG

# Using uv (recommended)
# Note: uv sync automatically creates a virtual environment in .venv/
uv sync --extra api
source .venv/bin/activate  # Activate the virtual environment (Linux/macOS)
# Or on Windows: .venv\Scripts\activate

### Or using pip with virtual environment
# python -m venv .venv
### source .venv/bin/activate  # Windows: .venv\Scripts\activate
# pip install -e ".[api]"

# Build front-end artifacts
cd lightrag_webui
bun install --frozen-lockfile
bun run build
cd ..

# setup env file
cp env.example .env  # Update the .env with your LLM and embedding configurations
# Launch API-WebUI server
lightrag-server
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Launching the LightRAG Server with Docker Compose&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/HKUDS/LightRAG.git
cd LightRAG
cp env.example .env  # Update the .env with your LLM and embedding configurations
# modify LLM and Embedding settings in .env
docker compose up
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Historical versions of LightRAG docker images can be found here: &lt;a href="https://github.com/HKUDS/LightRAG/pkgs/container/lightrag"&gt;LightRAG Docker Images&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Install LightRAG Core&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Install from source (Recommended)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd LightRAG
# Note: uv sync automatically creates a virtual environment in .venv/
uv sync
source .venv/bin/activate  # Activate the virtual environment (Linux/macOS)
# Or on Windows: .venv\Scripts\activate

# Or: pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Install from PyPI&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv pip install lightrag-hku
# Or: pip install lightrag-hku
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;h3&gt;LLM and Technology Stack Requirements for LightRAG&lt;/h3&gt; 
&lt;p&gt;LightRAG's demands on the capabilities of Large Language Models (LLMs) are significantly higher than those of traditional RAG, as it requires the LLM to perform entity-relationship extraction tasks from documents. Configuring appropriate Embedding and Reranker models is also crucial for improving query performance.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;LLM Selection&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;It is recommended to use an LLM with at least 32 billion parameters.&lt;/li&gt; 
   &lt;li&gt;The context length should be at least 32KB, with 64KB being recommended.&lt;/li&gt; 
   &lt;li&gt;It is not recommended to choose reasoning models during the document indexing stage.&lt;/li&gt; 
   &lt;li&gt;During the query stage, it is recommended to choose models with stronger capabilities than those used in the indexing stage to achieve better query results.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Embedding Model&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;A high-performance Embedding model is essential for RAG.&lt;/li&gt; 
   &lt;li&gt;We recommend using mainstream multilingual Embedding models, such as: &lt;code&gt;BAAI/bge-m3&lt;/code&gt; and &lt;code&gt;text-embedding-3-large&lt;/code&gt;.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Important Note&lt;/strong&gt;: The Embedding model must be determined before document indexing, and the same model must be used during the document query phase. For certain storage solutions (e.g., PostgreSQL), the vector dimension must be defined upon initial table creation. Therefore, when changing embedding models, it is necessary to delete the existing vector-related tables and allow LightRAG to recreate them with the new dimensions.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Reranker Model Configuration&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;Configuring a Reranker model can significantly enhance LightRAG's retrieval performance.&lt;/li&gt; 
   &lt;li&gt;When a Reranker model is enabled, it is recommended to set the "mix mode" as the default query mode.&lt;/li&gt; 
   &lt;li&gt;We recommend using mainstream Reranker models, such as: &lt;code&gt;BAAI/bge-reranker-v2-m3&lt;/code&gt; or models provided by services like Jina.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Quick Start for LightRAG Server&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;For more information about LightRAG Server, please refer to &lt;a href="https://raw.githubusercontent.com/HKUDS/LightRAG/main/lightrag/api/README.md"&gt;LightRAG Server&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Quick Start for LightRAG core&lt;/h3&gt; 
&lt;p&gt;To get started with LightRAG core, refer to the sample codes available in the &lt;code&gt;examples&lt;/code&gt; folder. Additionally, a &lt;a href="https://www.youtube.com/watch?v=g21royNJ4fw"&gt;video demo&lt;/a&gt; demonstration is provided to guide you through the local setup process. If you already possess an OpenAI API key, you can run the demo right away:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;### you should run the demo code with project folder
cd LightRAG
### provide your API-KEY for OpenAI
export OPENAI_API_KEY="sk-...your_opeai_key..."
### download the demo document of "A Christmas Carol" by Charles Dickens
curl https://raw.githubusercontent.com/gusye1234/nano-graphrag/main/tests/mock_data.txt &amp;gt; ./book.txt
### run the demo code
python examples/lightrag_openai_demo.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For a streaming response implementation example, please see &lt;code&gt;examples/lightrag_openai_compatible_demo.py&lt;/code&gt;. Prior to execution, ensure you modify the sample code's LLM and embedding configurations accordingly.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note 1&lt;/strong&gt;: When running the demo program, please be aware that different test scripts may use different embedding models. If you switch to a different embedding model, you must clear the data directory (&lt;code&gt;./dickens&lt;/code&gt;); otherwise, the program may encounter errors. If you wish to retain the LLM cache, you can preserve the &lt;code&gt;kv_store_llm_response_cache.json&lt;/code&gt; file while clearing the data directory.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note 2&lt;/strong&gt;: Only &lt;code&gt;lightrag_openai_demo.py&lt;/code&gt; and &lt;code&gt;lightrag_openai_compatible_demo.py&lt;/code&gt; are officially supported sample codes. Other sample files are community contributions that haven't undergone full testing and optimization.&lt;/p&gt; 
&lt;h2&gt;Programming with LightRAG Core&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;‚ö†Ô∏è &lt;strong&gt;If you would like to integrate LightRAG into your project, we recommend utilizing the REST API provided by the LightRAG Server&lt;/strong&gt;. LightRAG Core is typically intended for embedded applications or for researchers who wish to conduct studies and evaluations.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;‚ö†Ô∏è Important: Initialization Requirements&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;LightRAG requires explicit initialization before use.&lt;/strong&gt; You must call &lt;code&gt;await rag.initialize_storages()&lt;/code&gt; after creating a LightRAG instance, otherwise you will encounter errors.&lt;/p&gt; 
&lt;h3&gt;A Simple Program&lt;/h3&gt; 
&lt;p&gt;Use the below Python snippet to initialize LightRAG, insert text to it, and perform queries:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import os
import asyncio
from lightrag import LightRAG, QueryParam
from lightrag.llm.openai import gpt_4o_mini_complete, gpt_4o_complete, openai_embed
from lightrag.utils import setup_logger

setup_logger("lightrag", level="INFO")

WORKING_DIR = "./rag_storage"
if not os.path.exists(WORKING_DIR):
    os.mkdir(WORKING_DIR)

async def initialize_rag():
    rag = LightRAG(
        working_dir=WORKING_DIR,
        embedding_func=openai_embed,
        llm_model_func=gpt_4o_mini_complete,
    )
    # IMPORTANT: Both initialization calls are required!
    await rag.initialize_storages()  # Initialize storage backends
    return rag

async def main():
    try:
        # Initialize RAG instance
        rag = await initialize_rag()
        await rag.ainsert("Your text")

        # Perform hybrid search
        mode = "hybrid"
        print(
          await rag.aquery(
              "What are the top themes in this story?",
              param=QueryParam(mode=mode)
          )
        )

    except Exception as e:
        print(f"An error occurred: {e}")
    finally:
        if rag:
            await rag.finalize_storages()

if __name__ == "__main__":
    asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Important notes for the above snippet:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Export your OPENAI_API_KEY environment variable before running the script.&lt;/li&gt; 
 &lt;li&gt;This program uses the default storage settings for LightRAG, so all data will be persisted to WORKING_DIR/rag_storage.&lt;/li&gt; 
 &lt;li&gt;This program demonstrates only the simplest way to initialize a LightRAG object: Injecting the embedding and LLM functions, and initializing storage and pipeline status after creating the LightRAG object.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;LightRAG init parameters&lt;/h3&gt; 
&lt;p&gt;A full list of LightRAG init parameters:&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt; Parameters &lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;&lt;strong&gt;Parameter&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;Type&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;Explanation&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;Default&lt;/strong&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;working_dir&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;str&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Directory where the cache will be stored&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;lightrag_cache+timestamp&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;workspace&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;str&lt;/td&gt; 
    &lt;td&gt;Workspace name for data isolation between different LightRAG Instances&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;kv_storage&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;str&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Storage type for documents and text chunks. Supported types: &lt;code&gt;JsonKVStorage&lt;/code&gt;,&lt;code&gt;PGKVStorage&lt;/code&gt;,&lt;code&gt;RedisKVStorage&lt;/code&gt;,&lt;code&gt;MongoKVStorage&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;JsonKVStorage&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;vector_storage&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;str&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Storage type for embedding vectors. Supported types: &lt;code&gt;NanoVectorDBStorage&lt;/code&gt;,&lt;code&gt;PGVectorStorage&lt;/code&gt;,&lt;code&gt;MilvusVectorDBStorage&lt;/code&gt;,&lt;code&gt;ChromaVectorDBStorage&lt;/code&gt;,&lt;code&gt;FaissVectorDBStorage&lt;/code&gt;,&lt;code&gt;MongoVectorDBStorage&lt;/code&gt;,&lt;code&gt;QdrantVectorDBStorage&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;NanoVectorDBStorage&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;graph_storage&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;str&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Storage type for graph edges and nodes. Supported types: &lt;code&gt;NetworkXStorage&lt;/code&gt;,&lt;code&gt;Neo4JStorage&lt;/code&gt;,&lt;code&gt;PGGraphStorage&lt;/code&gt;,&lt;code&gt;AGEStorage&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;NetworkXStorage&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;doc_status_storage&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;str&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Storage type for documents process status. Supported types: &lt;code&gt;JsonDocStatusStorage&lt;/code&gt;,&lt;code&gt;PGDocStatusStorage&lt;/code&gt;,&lt;code&gt;MongoDocStatusStorage&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;JsonDocStatusStorage&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;chunk_token_size&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;int&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Maximum token size per chunk when splitting documents&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;1200&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;chunk_overlap_token_size&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;int&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Overlap token size between two chunks when splitting documents&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;100&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;tokenizer&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;Tokenizer&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;The function used to convert text into tokens (numbers) and back using .encode() and .decode() functions following &lt;code&gt;TokenizerInterface&lt;/code&gt; protocol. If you don't specify one, it will use the default Tiktoken tokenizer.&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;TiktokenTokenizer&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;tiktoken_model_name&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;str&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;If you're using the default Tiktoken tokenizer, this is the name of the specific Tiktoken model to use. This setting is ignored if you provide your own tokenizer.&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;gpt-4o-mini&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;entity_extract_max_gleaning&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;int&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Number of loops in the entity extraction process, appending history messages&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;node_embedding_algorithm&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;str&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Algorithm for node embedding (currently not used)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;node2vec&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;node2vec_params&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;dict&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Parameters for node embedding&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;{"dimensions": 1536,"num_walks": 10,"walk_length": 40,"window_size": 2,"iterations": 3,"random_seed": 3,}&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;embedding_func&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;EmbeddingFunc&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Function to generate embedding vectors from text&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;openai_embed&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;embedding_batch_num&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;int&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Maximum batch size for embedding processes (multiple texts sent per batch)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;32&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;embedding_func_max_async&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;int&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Maximum number of concurrent asynchronous embedding processes&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;16&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;llm_model_func&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;callable&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Function for LLM generation&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;gpt_4o_mini_complete&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;llm_model_name&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;str&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;LLM model name for generation&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;meta-llama/Llama-3.2-1B-Instruct&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;summary_context_size&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;int&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Maximum tokens send to LLM to generate summaries for entity relation merging&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;10000&lt;/code&gt;Ôºàconfigured by env var SUMMARY_CONTEXT_SIZE)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;summary_max_tokens&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;int&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Maximum token size for entity/relation description&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;500&lt;/code&gt;Ôºàconfigured by env var SUMMARY_MAX_TOKENS)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;llm_model_max_async&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;int&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Maximum number of concurrent asynchronous LLM processes&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;4&lt;/code&gt;Ôºàdefault value changed by env var MAX_ASYNC)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;llm_model_kwargs&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;dict&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Additional parameters for LLM generation&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;vector_db_storage_cls_kwargs&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;dict&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Additional parameters for vector database, like setting the threshold for nodes and relations retrieval&lt;/td&gt; 
    &lt;td&gt;cosine_better_than_threshold: 0.2Ôºàdefault value changed by env var COSINE_THRESHOLD)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;enable_llm_cache&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;bool&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;If &lt;code&gt;TRUE&lt;/code&gt;, stores LLM results in cache; repeated prompts return cached responses&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;TRUE&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;enable_llm_cache_for_entity_extract&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;bool&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;If &lt;code&gt;TRUE&lt;/code&gt;, stores LLM results in cache for entity extraction; Good for beginners to debug your application&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;TRUE&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;addon_params&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;dict&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Additional parameters, e.g., &lt;code&gt;{"language": "Simplified Chinese", "entity_types": ["organization", "person", "location", "event"]}&lt;/code&gt;: sets example limit, entity/relation extraction output language&lt;/td&gt; 
    &lt;td&gt;language: English`&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;embedding_cache_config&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;dict&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Configuration for question-answer caching. Contains three parameters: &lt;code&gt;enabled&lt;/code&gt;: Boolean value to enable/disable cache lookup functionality. When enabled, the system will check cached responses before generating new answers. &lt;code&gt;similarity_threshold&lt;/code&gt;: Float value (0-1), similarity threshold. When a new question's similarity with a cached question exceeds this threshold, the cached answer will be returned directly without calling the LLM. &lt;code&gt;use_llm_check&lt;/code&gt;: Boolean value to enable/disable LLM similarity verification. When enabled, LLM will be used as a secondary check to verify the similarity between questions before returning cached answers.&lt;/td&gt; 
    &lt;td&gt;Default: &lt;code&gt;{"enabled": False, "similarity_threshold": 0.95, "use_llm_check": False}&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h3&gt;Query Param&lt;/h3&gt; 
&lt;p&gt;Use QueryParam to control the behavior your query:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;class QueryParam:
    """Configuration parameters for query execution in LightRAG."""

    mode: Literal["local", "global", "hybrid", "naive", "mix", "bypass"] = "global"
    """Specifies the retrieval mode:
    - "local": Focuses on context-dependent information.
    - "global": Utilizes global knowledge.
    - "hybrid": Combines local and global retrieval methods.
    - "naive": Performs a basic search without advanced techniques.
    - "mix": Integrates knowledge graph and vector retrieval.
    """

    only_need_context: bool = False
    """If True, only returns the retrieved context without generating a response."""

    only_need_prompt: bool = False
    """If True, only returns the generated prompt without producing a response."""

    response_type: str = "Multiple Paragraphs"
    """Defines the response format. Examples: 'Multiple Paragraphs', 'Single Paragraph', 'Bullet Points'."""

    stream: bool = False
    """If True, enables streaming output for real-time responses."""

    top_k: int = int(os.getenv("TOP_K", "60"))
    """Number of top items to retrieve. Represents entities in 'local' mode and relationships in 'global' mode."""

    chunk_top_k: int = int(os.getenv("CHUNK_TOP_K", "20"))
    """Number of text chunks to retrieve initially from vector search and keep after reranking.
    If None, defaults to top_k value.
    """

    max_entity_tokens: int = int(os.getenv("MAX_ENTITY_TOKENS", "6000"))
    """Maximum number of tokens allocated for entity context in unified token control system."""

    max_relation_tokens: int = int(os.getenv("MAX_RELATION_TOKENS", "8000"))
    """Maximum number of tokens allocated for relationship context in unified token control system."""

    max_total_tokens: int = int(os.getenv("MAX_TOTAL_TOKENS", "30000"))
    """Maximum total tokens budget for the entire query context (entities + relations + chunks + system prompt)."""

    # History messages are only sent to LLM for context, not used for retrieval
    conversation_history: list[dict[str, str]] = field(default_factory=list)
    """Stores past conversation history to maintain context.
    Format: [{"role": "user/assistant", "content": "message"}].
    """

    # Deprecated (ids filter lead to potential hallucination effects)
    ids: list[str] | None = None
    """List of ids to filter the results."""

    model_func: Callable[..., object] | None = None
    """Optional override for the LLM model function to use for this specific query.
    If provided, this will be used instead of the global model function.
    This allows using different models for different query modes.
    """

    user_prompt: str | None = None
    """User-provided prompt for the query.
    Addition instructions for LLM. If provided, this will be inject into the prompt template.
    It's purpose is the let user customize the way LLM generate the response.
    """

    enable_rerank: bool = True
    """Enable reranking for retrieved text chunks. If True but no rerank model is configured, a warning will be issued.
    Default is True to enable reranking when rerank model is available.
    """
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;default value of Top_k can be change by environment variables TOP_K.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;LLM and Embedding Injection&lt;/h3&gt; 
&lt;p&gt;LightRAG requires the utilization of LLM and Embedding models to accomplish document indexing and querying tasks. During the initialization phase, it is necessary to inject the invocation methods of the relevant models into LightRAGÔºö&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt;Using Open AI-like APIs&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;LightRAG also supports Open AI-like chat/embeddings APIs:&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import os
import numpy as np
from lightrag.utils import wrap_embedding_func_with_attrs
from lightrag.llm.openai import openai_complete_if_cache, openai_embed

async def llm_model_func(
    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs
) -&amp;gt; str:
    return await openai_complete_if_cache(
        "solar-mini",
        prompt,
        system_prompt=system_prompt,
        history_messages=history_messages,
        api_key=os.getenv("UPSTAGE_API_KEY"),
        base_url="https://api.upstage.ai/v1/solar",
        **kwargs
    )

@wrap_embedding_func_with_attrs(embedding_dim=4096, max_token_size=8192, model_name="solar-embedding-1-large-query")
async def embedding_func(texts: list[str]) -&amp;gt; np.ndarray:
    return await openai_embed.func(
        texts,
        model="solar-embedding-1-large-query",
        api_key=os.getenv("UPSTAGE_API_KEY"),
        base_url="https://api.upstage.ai/v1/solar"
    )

async def initialize_rag():
    rag = LightRAG(
        working_dir=WORKING_DIR,
        llm_model_func=llm_model_func,
        embedding_func=embedding_func  # Pass the decorated function directly
    )

    await rag.initialize_storages()
    return rag
&lt;/code&gt;&lt;/pre&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;strong&gt;Important Note on Embedding Function Wrapping:&lt;/strong&gt;&lt;/p&gt; 
  &lt;p&gt;&lt;code&gt;EmbeddingFunc&lt;/code&gt; cannot be nested. Functions that have been decorated with &lt;code&gt;@wrap_embedding_func_with_attrs&lt;/code&gt; (such as &lt;code&gt;openai_embed&lt;/code&gt;, &lt;code&gt;ollama_embed&lt;/code&gt;, etc.) cannot be wrapped again using &lt;code&gt;EmbeddingFunc()&lt;/code&gt;. This is why we call &lt;code&gt;xxx_embed.func&lt;/code&gt; (the underlying unwrapped function) instead of &lt;code&gt;xxx_embed&lt;/code&gt; directly when creating custom embedding functions.&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt;Using Hugging Face Models&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;If you want to use Hugging Face models, you only need to set LightRAG as follows:&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;See &lt;code&gt;lightrag_hf_demo.py&lt;/code&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from functools import partial
from transformers import AutoTokenizer, AutoModel

# Pre-load tokenizer and model
tokenizer = AutoTokenizer.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")
embed_model = AutoModel.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")

# Initialize LightRAG with Hugging Face model
rag = LightRAG(
    working_dir=WORKING_DIR,
    llm_model_func=hf_model_complete,  # Use Hugging Face model for text generation
    llm_model_name='meta-llama/Llama-3.1-8B-Instruct',  # Model name from Hugging Face
    # Use Hugging Face embedding function
    embedding_func=EmbeddingFunc(
        embedding_dim=384,
        max_token_size=2048,
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        func=partial(
            hf_embed.func,  # Use .func to access the unwrapped function
            tokenizer=tokenizer,
            embed_model=embed_model
        )
    ),
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt;Using Ollama Models&lt;/b&gt; &lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;Overview&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;If you want to use Ollama models, you need to pull model you plan to use and embedding model, for example &lt;code&gt;nomic-embed-text&lt;/code&gt;.&lt;/p&gt; 
 &lt;p&gt;Then you only need to set LightRAG as follows:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import numpy as np
from lightrag.utils import wrap_embedding_func_with_attrs
from lightrag.llm.ollama import ollama_model_complete, ollama_embed

@wrap_embedding_func_with_attrs(embedding_dim=768, max_token_size=8192, model_name="nomic-embed-text")
async def embedding_func(texts: list[str]) -&amp;gt; np.ndarray:
    return await ollama_embed.func(texts, embed_model="nomic-embed-text")

# Initialize LightRAG with Ollama model
rag = LightRAG(
    working_dir=WORKING_DIR,
    llm_model_func=ollama_model_complete,  # Use Ollama model for text generation
    llm_model_name='your_model_name', # Your model name
    embedding_func=embedding_func,  # Pass the decorated function directly
)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Increasing context size&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;In order for LightRAG to work context should be at least 32k tokens. By default Ollama models have context size of 8k. You can achieve this using one of two ways:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Increasing the &lt;code&gt;num_ctx&lt;/code&gt; parameter in Modelfile&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Pull the model:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;ollama pull qwen2
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="2"&gt; 
  &lt;li&gt;Display the model file:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;ollama show --modelfile qwen2 &amp;gt; Modelfile
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="3"&gt; 
  &lt;li&gt;Edit the Modelfile by adding the following line:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;PARAMETER num_ctx 32768
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="4"&gt; 
  &lt;li&gt;Create the modified model:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;ollama create -f Modelfile qwen2m
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Setup &lt;code&gt;num_ctx&lt;/code&gt; via Ollama API&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;Tiy can use &lt;code&gt;llm_model_kwargs&lt;/code&gt; param to configure ollama:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import numpy as np
from lightrag.utils import wrap_embedding_func_with_attrs
from lightrag.llm.ollama import ollama_model_complete, ollama_embed

@wrap_embedding_func_with_attrs(embedding_dim=768, max_token_size=8192, model_name="nomic-embed-text")
async def embedding_func(texts: list[str]) -&amp;gt; np.ndarray:
    return await ollama_embed.func(texts, embed_model="nomic-embed-text")

rag = LightRAG(
    working_dir=WORKING_DIR,
    llm_model_func=ollama_model_complete,  # Use Ollama model for text generation
    llm_model_name='your_model_name', # Your model name
    llm_model_kwargs={"options": {"num_ctx": 32768}},
    embedding_func=embedding_func,  # Pass the decorated function directly
)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;strong&gt;Important Note on Embedding Function Wrapping:&lt;/strong&gt;&lt;/p&gt; 
  &lt;p&gt;&lt;code&gt;EmbeddingFunc&lt;/code&gt; cannot be nested. Functions that have been decorated with &lt;code&gt;@wrap_embedding_func_with_attrs&lt;/code&gt; (such as &lt;code&gt;openai_embed&lt;/code&gt;, &lt;code&gt;ollama_embed&lt;/code&gt;, etc.) cannot be wrapped again using &lt;code&gt;EmbeddingFunc()&lt;/code&gt;. This is why we call &lt;code&gt;xxx_embed.func&lt;/code&gt; (the underlying unwrapped function) instead of &lt;code&gt;xxx_embed&lt;/code&gt; directly when creating custom embedding functions.&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Low RAM GPUs&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;In order to run this experiment on low RAM GPU you should select small model and tune context window (increasing context increase memory consumption). For example, running this ollama example on repurposed mining GPU with 6Gb of RAM required to set context size to 26k while using &lt;code&gt;gemma2:2b&lt;/code&gt;. It was able to find 197 entities and 19 relations on &lt;code&gt;book.txt&lt;/code&gt;.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt;LlamaIndex&lt;/b&gt; &lt;/summary&gt; 
 &lt;p&gt;LightRAG supports integration with LlamaIndex (&lt;code&gt;llm/llama_index_impl.py&lt;/code&gt;):&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Integrates with OpenAI and other providers through LlamaIndex&lt;/li&gt; 
  &lt;li&gt;See &lt;a href="https://developers.llamaindex.ai/python/framework/"&gt;LlamaIndex Documentation&lt;/a&gt; for detailed setup or the &lt;a href="https://raw.githubusercontent.com/HKUDS/LightRAG/main/examples/unofficial-sample/"&gt;examples&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;Example Usage&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Using LlamaIndex with direct OpenAI access
import asyncio
from lightrag import LightRAG
from lightrag.llm.llama_index_impl import llama_index_complete_if_cache, llama_index_embed
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.llms.openai import OpenAI
from lightrag.utils import setup_logger

# Setup log handler for LightRAG
setup_logger("lightrag", level="INFO")

async def initialize_rag():
    rag = LightRAG(
        working_dir="your/path",
        llm_model_func=llama_index_complete_if_cache,  # LlamaIndex-compatible completion function
        embedding_func=EmbeddingFunc(    # LlamaIndex-compatible embedding function
            embedding_dim=1536,
            max_token_size=2048,
            model_name=embed_model,
            func=partial(llama_index_embed.func, embed_model=embed_model)  # Use .func to access the unwrapped function
        ),
    )

    await rag.initialize_storages()
    return rag

def main():
    # Initialize RAG instance
    rag = asyncio.run(initialize_rag())

    with open("./book.txt", "r", encoding="utf-8") as f:
        rag.insert(f.read())

    # Perform naive search
    print(
        rag.query("What are the top themes in this story?", param=QueryParam(mode="naive"))
    )

    # Perform local search
    print(
        rag.query("What are the top themes in this story?", param=QueryParam(mode="local"))
    )

    # Perform global search
    print(
        rag.query("What are the top themes in this story?", param=QueryParam(mode="global"))
    )

    # Perform hybrid search
    print(
        rag.query("What are the top themes in this story?", param=QueryParam(mode="hybrid"))
    )

if __name__ == "__main__":
    main()
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;For detailed documentation and examples, see:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://developers.llamaindex.ai/python/framework/"&gt;LlamaIndex Documentation&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/LightRAG/main/examples/unofficial-sample/lightrag_llamaindex_direct_demo.py"&gt;Direct OpenAI Example&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/LightRAG/main/examples/unofficial-sample/lightrag_llamaindex_litellm_demo.py"&gt;LiteLLM Proxy Example&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/LightRAG/main/examples/unofficial-sample/lightrag_llamaindex_litellm_opik_demo.py"&gt;LiteLLM Proxy with Opik Example&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt;Using Azure OpenAI Models&lt;/b&gt; &lt;/summary&gt; 
 &lt;p&gt;If you want to use Azure OpenAI models, you only need to set up LightRAG as follows:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import os
import numpy as np
from lightrag.utils import wrap_embedding_func_with_attrs
from lightrag.llm.azure_openai import azure_openai_complete_if_cache, azure_openai_embed

# Configure the generation model
async def llm_model_func(
    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs
) -&amp;gt; str:
    return await azure_openai_complete_if_cache(
        prompt,
        system_prompt=system_prompt,
        history_messages=history_messages,
        api_key=os.getenv("AZURE_OPENAI_API_KEY"),
        azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
        api_version=os.getenv("AZURE_OPENAI_API_VERSION"),
        deployment_name=os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME"),
        **kwargs
    )

# Configure the embedding model
@wrap_embedding_func_with_attrs(
    embedding_dim=1536,
    max_token_size=8192,
    model_name=os.getenv("AZURE_OPENAI_EMBEDDING_MODEL")
)
async def embedding_func(texts: list[str]) -&amp;gt; np.ndarray:
    return await azure_openai_embed.func(
        texts,
        api_key=os.getenv("AZURE_OPENAI_API_KEY"),
        azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
        api_version=os.getenv("AZURE_OPENAI_API_VERSION"),
        deployment_name=os.getenv("AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME")
    )

rag = LightRAG(
    working_dir=WORKING_DIR,
    llm_model_func=llm_model_func,
    embedding_func=embedding_func
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt;Using Google Gemini Models&lt;/b&gt; &lt;/summary&gt; 
 &lt;p&gt;If you want to use Google Gemini models, you only need to set up LightRAG as follows:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import os
import numpy as np
from lightrag.utils import wrap_embedding_func_with_attrs
from lightrag.llm.gemini import gemini_model_complete, gemini_embed

# Configure the generation model
async def llm_model_func(
    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs
) -&amp;gt; str:
    return await gemini_model_complete(
        prompt,
        system_prompt=system_prompt,
        history_messages=history_messages,
        api_key=os.getenv("GEMINI_API_KEY"),
        model_name="gemini-2.0-flash",
        **kwargs
    )

# Configure the embedding model
@wrap_embedding_func_with_attrs(
    embedding_dim=768,
    max_token_size=2048,
    model_name="models/text-embedding-004"
)
async def embedding_func(texts: list[str]) -&amp;gt; np.ndarray:
    return await gemini_embed.func(
        texts,
        api_key=os.getenv("GEMINI_API_KEY"),
        model="models/text-embedding-004"
    )

rag = LightRAG(
    working_dir=WORKING_DIR,
    llm_model_func=llm_model_func,
    llm_model_name="gemini-2.0-flash",
    embedding_func=embedding_func
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Rerank Function Injection&lt;/h3&gt; 
&lt;p&gt;To enhance retrieval quality, documents can be re-ranked based on a more effective relevance scoring model. The &lt;code&gt;rerank.py&lt;/code&gt; file provides three Reranker provider driver functions:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Cohere / vLLM&lt;/strong&gt;: &lt;code&gt;cohere_rerank&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Jina AI&lt;/strong&gt;: &lt;code&gt;jina_rerank&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Aliyun&lt;/strong&gt;: &lt;code&gt;ali_rerank&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can inject one of these functions into the &lt;code&gt;rerank_model_func&lt;/code&gt; attribute of the LightRAG object. This will enable LightRAG's query function to re-order retrieved text blocks using the injected function. For detailed usage, please refer to the &lt;code&gt;examples/rerank_example.py&lt;/code&gt; file.&lt;/p&gt; 
&lt;h3&gt;User Prompt vs. Query&lt;/h3&gt; 
&lt;p&gt;When using LightRAG for content queries, avoid combining the search process with unrelated output processing, as this significantly impacts query effectiveness. The &lt;code&gt;user_prompt&lt;/code&gt; parameter in Query Param is specifically designed to address this issue ‚Äî it does not participate in the RAG retrieval phase, but rather guides the LLM on how to process the retrieved results after the query is completed. Here's how to use it:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Create query parameters
query_param = QueryParam(
    mode = "hybrid",  # Other modesÔºölocal, global, hybrid, mix, naive
    user_prompt = "For diagrams, use mermaid format with English/Pinyin node names and Chinese display labels",
)

# Query and process
response_default = rag.query(
    "Please draw a character relationship diagram for Scrooge",
    param=query_param
)
print(response_default)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Insert&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt; Basic Insert &lt;/b&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Basic Insert
rag.insert("Text")
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt; Batch Insert &lt;/b&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Basic Batch Insert: Insert multiple texts at once
rag.insert(["TEXT1", "TEXT2",...])

# Batch Insert with custom batch size configuration
rag = LightRAG(
    ...
    working_dir=WORKING_DIR,
    max_parallel_insert = 4
)

rag.insert(["TEXT1", "TEXT2", "TEXT3", ...])  # Documents will be processed in batches of 4
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;The &lt;code&gt;max_parallel_insert&lt;/code&gt; parameter determines the number of documents processed concurrently in the document indexing pipeline. If unspecified, the default value is &lt;strong&gt;2&lt;/strong&gt;. We recommend keeping this setting &lt;strong&gt;below 10&lt;/strong&gt;, as the performance bottleneck typically lies with the LLM (Large Language Model) processing.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt; Insert with ID &lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;If you want to provide your own IDs for your documents, number of documents and number of IDs must be the same.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Insert single text, and provide ID for it
rag.insert("TEXT1", ids=["ID_FOR_TEXT1"])

# Insert multiple texts, and provide IDs for them
rag.insert(["TEXT1", "TEXT2",...], ids=["ID_FOR_TEXT1", "ID_FOR_TEXT2"])
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Insert using Pipeline&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;The &lt;code&gt;apipeline_enqueue_documents&lt;/code&gt; and &lt;code&gt;apipeline_process_enqueue_documents&lt;/code&gt; functions allow you to perform incremental insertion of documents into the graph.This is useful for scenarios where you want to process documents in the background while still allowing the main thread to continue executing.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;rag = LightRAG(..)

await rag.apipeline_enqueue_documents(input)
# Your routine in loop
await rag.apipeline_process_enqueue_documents(input)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Insert Multi-file Type Support&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;The &lt;code&gt;textract&lt;/code&gt; supports reading file types such as TXT, DOCX, PPTX, CSV, and PDF.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import textract

file_path = 'TEXT.pdf'
text_content = textract.process(file_path)

rag.insert(text_content.decode('utf-8'))
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Citation Functionality&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;By providing file paths, the system ensures that sources can be traced back to their original documents.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Define documents and their file paths
documents = ["Document content 1", "Document content 2"]
file_paths = ["path/to/doc1.txt", "path/to/doc2.txt"]

# Insert documents with file paths
rag.insert(documents, file_paths=file_paths)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Storage&lt;/h3&gt; 
&lt;p&gt;LightRAG uses 4 types of storage for different purposes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;KV_STORAGE: llm response cache, text chunks, document information&lt;/li&gt; 
 &lt;li&gt;VECTOR_STORAGE: entities vectors, relation vectors, chunks vectors&lt;/li&gt; 
 &lt;li&gt;GRAPH_STORAGE: entity relation graph&lt;/li&gt; 
 &lt;li&gt;DOC_STATUS_STORAGE: document indexing status&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Each storage type has several implementations:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;KV_STORAGE supported implementations:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;JsonKVStorage    JsonFile (default)
PGKVStorage      Postgres
RedisKVStorage   Redis
MongoKVStorage   MongoDB
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;GRAPH_STORAGE supported implementations:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;NetworkXStorage      NetworkX (default)
Neo4JStorage         Neo4J
PGGraphStorage       PostgreSQL with AGE plugin
MemgraphStorage.     Memgraph
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Testing has shown that Neo4J delivers superior performance in production environments compared to PostgreSQL with AGE plugin.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;VECTOR_STORAGE supported implementations:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;NanoVectorDBStorage         NanoVector (default)
PGVectorStorage             Postgres
MilvusVectorDBStorage       Milvus
FaissVectorDBStorage        Faiss
QdrantVectorDBStorage       Qdrant
MongoVectorDBStorage        MongoDB
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;DOC_STATUS_STORAGE: supported implementations:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;JsonDocStatusStorage        JsonFile (default)
PGDocStatusStorage          Postgres
MongoDocStatusStorage       MongoDB
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Example connection configurations for each storage type can be found in the &lt;code&gt;env.example&lt;/code&gt; file. The database instance in the connection string needs to be created by you on the database server beforehand. LightRAG is only responsible for creating tables within the database instance, not for creating the database instance itself. If using Redis as storage, remember to configure automatic data persistence rules for Redis, otherwise data will be lost after the Redis service restarts. If using PostgreSQL, it is recommended to use version 16.6 or above.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt;Using Neo4J Storage&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;For production level scenarios you will most likely want to leverage an enterprise solution&lt;/li&gt; 
  &lt;li&gt;for KG storage. Running Neo4J in Docker is recommended for seamless local testing.&lt;/li&gt; 
  &lt;li&gt;See: &lt;a href="https://hub.docker.com/_/neo4j"&gt;https://hub.docker.com/_/neo4j&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;export NEO4J_URI="neo4j://localhost:7687"
export NEO4J_USERNAME="neo4j"
export NEO4J_PASSWORD="password"

# Setup logger for LightRAG
setup_logger("lightrag", level="INFO")

# When you launch the project be sure to override the default KG by specifying graph_storage="Neo4JStorage".
# Initialize LightRAG with Neo4J implementation.
async def initialize_rag():
    rag = LightRAG(
        working_dir=WORKING_DIR,
        llm_model_func=gpt_4o_mini_complete,  # Use gpt_4o_mini_complete LLM model
        graph_storage="Neo4JStorage", #&amp;lt;-----------override KG default
    )

    # Initialize database connections
    await rag.initialize_storages()
    # Initialize pipeline status for document processing
    return rag
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;see test_neo4j.py for a working example.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt;Using PostgreSQL Storage&lt;/b&gt; &lt;/summary&gt; 
 &lt;p&gt;For production level scenarios you will most likely want to leverage an enterprise solution. PostgreSQL can provide a one-stop solution for you as KV store, VectorDB (pgvector) and GraphDB (apache AGE). PostgreSQL version 16.6 or higher is supported.&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;PostgreSQL is lightweight,the whole binary distribution including all necessary plugins can be zipped to 40MB: Ref to &lt;a href="https://github.com/ShanGor/apache-age-windows/releases/tag/PG17%2Fv1.5.0-rc0"&gt;Windows Release&lt;/a&gt; as it is easy to install for Linux/Mac.&lt;/li&gt; 
  &lt;li&gt;If you prefer docker, please start with this image if you are a beginner to avoid hiccups (Default user password:rag/rag): &lt;a href="https://hub.docker.com/r/gzdaniel/postgres-for-rag"&gt;https://hub.docker.com/r/gzdaniel/postgres-for-rag&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;How to start? Ref to: &lt;a href="https://github.com/HKUDS/LightRAG/raw/main/examples/lightrag_gemini_postgres_demo.py"&gt;examples/lightrag_gemini_postgres_demo.py&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;For high-performance graph database requirements, Neo4j is recommended as Apache AGE's performance is not as competitive.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt;Using Faiss Storage&lt;/b&gt; &lt;/summary&gt; Before using Faiss vector database, you must manually install `faiss-cpu` or `faiss-gpu`. 
 &lt;ul&gt; 
  &lt;li&gt;Install the required dependencies:&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code&gt;pip install faiss-cpu
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;You can also install &lt;code&gt;faiss-gpu&lt;/code&gt; if you have GPU support.&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Here we are using &lt;code&gt;sentence-transformers&lt;/code&gt; but you can also use &lt;code&gt;OpenAIEmbedding&lt;/code&gt; model with &lt;code&gt;3072&lt;/code&gt; dimensions.&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;async def embedding_func(texts: list[str]) -&amp;gt; np.ndarray:
    model = SentenceTransformer('all-MiniLM-L6-v2')
    embeddings = model.encode(texts, convert_to_numpy=True)
    return embeddings

# Initialize LightRAG with the LLM model function and embedding function
rag = LightRAG(
    working_dir=WORKING_DIR,
    llm_model_func=llm_model_func,
    embedding_func=EmbeddingFunc(
        embedding_dim=384,
        max_token_size=2048,
        model_name="all-MiniLM-L6-v2",
        func=embedding_func,
    ),
    vector_storage="FaissVectorDBStorage",
    vector_db_storage_cls_kwargs={
        "cosine_better_than_threshold": 0.3  # Your desired threshold
    }
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt;Using Memgraph for Storage&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Memgraph is a high-performance, in-memory graph database compatible with the Neo4j Bolt protocol.&lt;/li&gt; 
  &lt;li&gt;You can run Memgraph locally using Docker for easy testing:&lt;/li&gt; 
  &lt;li&gt;See: &lt;a href="https://memgraph.com/download"&gt;https://memgraph.com/download&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;export MEMGRAPH_URI="bolt://localhost:7687"

# Setup logger for LightRAG
setup_logger("lightrag", level="INFO")

# When you launch the project, override the default KG: NetworkX
# by specifying kg="MemgraphStorage".

# Note: Default settings use NetworkX
# Initialize LightRAG with Memgraph implementation.
async def initialize_rag():
    rag = LightRAG(
        working_dir=WORKING_DIR,
        llm_model_func=gpt_4o_mini_complete,  # Use gpt_4o_mini_complete LLM model
        graph_storage="MemgraphStorage", #&amp;lt;-----------override KG default
    )

    # Initialize database connections
    await rag.initialize_storages()
    # Initialize pipeline status for document processing
    return rag
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt;Using MongoDB Storage&lt;/b&gt; &lt;/summary&gt; 
 &lt;p&gt;MongoDB provides a one-stop storage solution for LightRAG. MongoDB offers native KV storage and vector storage. LightRAG uses MongoDB collections to implement a simple graph storage. MongoDB's official vector search functionality (&lt;code&gt;$vectorSearch&lt;/code&gt;) currently requires their official cloud service MongoDB Atlas. This functionality cannot be used on self-hosted MongoDB Community/Enterprise versions.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt;Using Redis Storage&lt;/b&gt; &lt;/summary&gt; 
 &lt;p&gt;LightRAG supports using Redis as KV storage. When using Redis storage, attention should be paid to persistence configuration and memory usage configuration. The following is the recommended Redis configuration:&lt;/p&gt; 
 &lt;pre&gt;&lt;code&gt;save 900 1
save 300 10
save 60 1000
stop-writes-on-bgsave-error yes
maxmemory 4gb
maxmemory-policy noeviction
maxclients 500
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Data Isolation Between LightRAG Instances&lt;/h3&gt; 
&lt;p&gt;The &lt;code&gt;workspace&lt;/code&gt; parameter ensures data isolation between different LightRAG instances. Once initialized, the &lt;code&gt;workspace&lt;/code&gt; is immutable and cannot be changed.Here is how workspaces are implemented for different types of storage:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;For local file-based databases, data isolation is achieved through workspace subdirectories:&lt;/strong&gt; &lt;code&gt;JsonKVStorage&lt;/code&gt;, &lt;code&gt;JsonDocStatusStorage&lt;/code&gt;, &lt;code&gt;NetworkXStorage&lt;/code&gt;, &lt;code&gt;NanoVectorDBStorage&lt;/code&gt;, &lt;code&gt;FaissVectorDBStorage&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;For databases that store data in collections, it's done by adding a workspace prefix to the collection name:&lt;/strong&gt; &lt;code&gt;RedisKVStorage&lt;/code&gt;, &lt;code&gt;RedisDocStatusStorage&lt;/code&gt;, &lt;code&gt;MilvusVectorDBStorage&lt;/code&gt;, &lt;code&gt;MongoKVStorage&lt;/code&gt;, &lt;code&gt;MongoDocStatusStorage&lt;/code&gt;, &lt;code&gt;MongoVectorDBStorage&lt;/code&gt;, &lt;code&gt;MongoGraphStorage&lt;/code&gt;, &lt;code&gt;PGGraphStorage&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;For Qdrant vector database, data isolation is achieved through payload-based partitioning (Qdrant's recommended multitenancy approach):&lt;/strong&gt; &lt;code&gt;QdrantVectorDBStorage&lt;/code&gt; uses shared collections with payload filtering for unlimited workspace scalability.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;For relational databases, data isolation is achieved by adding a &lt;code&gt;workspace&lt;/code&gt; field to the tables for logical data separation:&lt;/strong&gt; &lt;code&gt;PGKVStorage&lt;/code&gt;, &lt;code&gt;PGVectorStorage&lt;/code&gt;, &lt;code&gt;PGDocStatusStorage&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;For the Neo4j graph database, logical data isolation is achieved through labels:&lt;/strong&gt; &lt;code&gt;Neo4JStorage&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To maintain compatibility with legacy data, the default workspace for PostgreSQL non-graph storage is &lt;code&gt;default&lt;/code&gt; and, for PostgreSQL AGE graph storage is null, for Neo4j graph storage is &lt;code&gt;base&lt;/code&gt; when no workspace is configured. For all external storages, the system provides dedicated workspace environment variables to override the common &lt;code&gt;WORKSPACE&lt;/code&gt; environment variable configuration. These storage-specific workspace environment variables are: &lt;code&gt;REDIS_WORKSPACE&lt;/code&gt;, &lt;code&gt;MILVUS_WORKSPACE&lt;/code&gt;, &lt;code&gt;QDRANT_WORKSPACE&lt;/code&gt;, &lt;code&gt;MONGODB_WORKSPACE&lt;/code&gt;, &lt;code&gt;POSTGRES_WORKSPACE&lt;/code&gt;, &lt;code&gt;NEO4J_WORKSPACE&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;AGENTS.md -- Guiding Coding Agents&lt;/h3&gt; 
&lt;p&gt;AGENTS.md is a simple, open format for guiding coding agents (&lt;a href="https://agents.md/"&gt;https://agents.md/&lt;/a&gt;). It is a dedicated, predictable place to provide the context and instructions to help AI coding agents work on LightRAG project. Different AI coders should not maintain separate guidance files individually. If any AI coder cannot automatically recognize AGENTS.md, symbolic links can be used as a solution. After establishing symbolic links, you can prevent them from being committed to the Git repository by configuring your local &lt;code&gt;.gitignore_global&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Edit Entities and Relations&lt;/h2&gt; 
&lt;p&gt;LightRAG now supports comprehensive knowledge graph management capabilities, allowing you to create, edit, and delete entities and relationships within your knowledge graph.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt; Create Entities and Relations &lt;/b&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Create new entity
entity = rag.create_entity("Google", {
    "description": "Google is a multinational technology company specializing in internet-related services and products.",
    "entity_type": "company"
})

# Create another entity
product = rag.create_entity("Gmail", {
    "description": "Gmail is an email service developed by Google.",
    "entity_type": "product"
})

# Create relation between entities
relation = rag.create_relation("Google", "Gmail", {
    "description": "Google develops and operates Gmail.",
    "keywords": "develops operates service",
    "weight": 2.0
})
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt; Edit Entities and Relations &lt;/b&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Edit an existing entity
updated_entity = rag.edit_entity("Google", {
    "description": "Google is a subsidiary of Alphabet Inc., founded in 1998.",
    "entity_type": "tech_company"
})

# Rename an entity (with all its relationships properly migrated)
renamed_entity = rag.edit_entity("Gmail", {
    "entity_name": "Google Mail",
    "description": "Google Mail (formerly Gmail) is an email service."
})

# Edit a relation between entities
updated_relation = rag.edit_relation("Google", "Google Mail", {
    "description": "Google created and maintains Google Mail service.",
    "keywords": "creates maintains email service",
    "weight": 3.0
})
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;All operations are available in both synchronous and asynchronous versions. The asynchronous versions have the prefix "a" (e.g., &lt;code&gt;acreate_entity&lt;/code&gt;, &lt;code&gt;aedit_relation&lt;/code&gt;).&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt; Insert Custom KG &lt;/b&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;custom_kg = {
        "chunks": [
            {
                "content": "Alice and Bob are collaborating on quantum computing research.",
                "source_id": "doc-1",
                "file_path": "test_file",
            }
        ],
        "entities": [
            {
                "entity_name": "Alice",
                "entity_type": "person",
                "description": "Alice is a researcher specializing in quantum physics.",
                "source_id": "doc-1",
                "file_path": "test_file"
            },
            {
                "entity_name": "Bob",
                "entity_type": "person",
                "description": "Bob is a mathematician.",
                "source_id": "doc-1",
                "file_path": "test_file"
            },
            {
                "entity_name": "Quantum Computing",
                "entity_type": "technology",
                "description": "Quantum computing utilizes quantum mechanical phenomena for computation.",
                "source_id": "doc-1",
                "file_path": "test_file"
            }
        ],
        "relationships": [
            {
                "src_id": "Alice",
                "tgt_id": "Bob",
                "description": "Alice and Bob are research partners.",
                "keywords": "collaboration research",
                "weight": 1.0,
                "source_id": "doc-1",
                "file_path": "test_file"
            },
            {
                "src_id": "Alice",
                "tgt_id": "Quantum Computing",
                "description": "Alice conducts research on quantum computing.",
                "keywords": "research expertise",
                "weight": 1.0,
                "source_id": "doc-1",
                "file_path": "test_file"
            },
            {
                "src_id": "Bob",
                "tgt_id": "Quantum Computing",
                "description": "Bob researches quantum computing.",
                "keywords": "research application",
                "weight": 1.0,
                "source_id": "doc-1",
                "file_path": "test_file"
            }
        ]
    }

rag.insert_custom_kg(custom_kg)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt;Other Entity and Relation Operations&lt;/b&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;create_entity&lt;/strong&gt;: Creates a new entity with specified attributes&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;edit_entity&lt;/strong&gt;: Updates an existing entity's attributes or renames it&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;create_relation&lt;/strong&gt;: Creates a new relation between existing entities&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;edit_relation&lt;/strong&gt;: Updates an existing relation's attributes&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;These operations maintain data consistency across both the graph database and vector database components, ensuring your knowledge graph remains coherent.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;Delete Functions&lt;/h2&gt; 
&lt;p&gt;LightRAG provides comprehensive deletion capabilities, allowing you to delete documents, entities, and relationships.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt;Delete Entities&lt;/b&gt; &lt;/summary&gt; 
 &lt;p&gt;You can delete entities by their name along with all associated relationships:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Delete entity and all its relationships (synchronous version)
rag.delete_by_entity("Google")

# Asynchronous version
await rag.adelete_by_entity("Google")
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;When deleting an entity:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Removes the entity node from the knowledge graph&lt;/li&gt; 
  &lt;li&gt;Deletes all associated relationships&lt;/li&gt; 
  &lt;li&gt;Removes related embedding vectors from the vector database&lt;/li&gt; 
  &lt;li&gt;Maintains knowledge graph integrity&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt;Delete Relations&lt;/b&gt; &lt;/summary&gt; 
 &lt;p&gt;You can delete relationships between two specific entities:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Delete relationship between two entities (synchronous version)
rag.delete_by_relation("Google", "Gmail")

# Asynchronous version
await rag.adelete_by_relation("Google", "Gmail")
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;When deleting a relationship:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Removes the specified relationship edge&lt;/li&gt; 
  &lt;li&gt;Deletes the relationship's embedding vector from the vector database&lt;/li&gt; 
  &lt;li&gt;Preserves both entity nodes and their other relationships&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt;Delete by Document ID&lt;/b&gt; &lt;/summary&gt; 
 &lt;p&gt;You can delete an entire document and all its related knowledge through document ID:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Delete by document ID (asynchronous version)
await rag.adelete_by_doc_id("doc-12345")
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Optimized processing when deleting by document ID:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Smart Cleanup&lt;/strong&gt;: Automatically identifies and removes entities and relationships that belong only to this document&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Preserve Shared Knowledge&lt;/strong&gt;: If entities or relationships exist in other documents, they are preserved and their descriptions are rebuilt&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Cache Optimization&lt;/strong&gt;: Clears related LLM cache to reduce storage overhead&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Incremental Rebuilding&lt;/strong&gt;: Reconstructs affected entity and relationship descriptions from remaining documents&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;The deletion process includes:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Delete all text chunks related to the document&lt;/li&gt; 
  &lt;li&gt;Identify and delete entities and relationships that belong only to this document&lt;/li&gt; 
  &lt;li&gt;Rebuild entities and relationships that still exist in other documents&lt;/li&gt; 
  &lt;li&gt;Update all related vector indexes&lt;/li&gt; 
  &lt;li&gt;Clean up document status records&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;Note: Deletion by document ID is an asynchronous operation as it involves complex knowledge graph reconstruction processes.&lt;/p&gt; 
&lt;/details&gt; 
&lt;p&gt;&lt;strong&gt;Important Reminders:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Irreversible Operations&lt;/strong&gt;: All deletion operations are irreversible, please use with caution&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Performance Considerations&lt;/strong&gt;: Deleting large amounts of data may take some time, especially deletion by document ID&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Data Consistency&lt;/strong&gt;: Deletion operations automatically maintain consistency between the knowledge graph and vector database&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Backup Recommendations&lt;/strong&gt;: Consider backing up data before performing important deletion operations&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Batch Deletion Recommendations:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;For batch deletion operations, consider using asynchronous methods for better performance&lt;/li&gt; 
 &lt;li&gt;For large-scale deletions, consider processing in batches to avoid excessive system load&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Entity Merging&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt;Merge Entities and Their Relationships&lt;/b&gt; &lt;/summary&gt; 
 &lt;p&gt;LightRAG now supports merging multiple entities into a single entity, automatically handling all relationships:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Basic entity merging
rag.merge_entities(
    source_entities=["Artificial Intelligence", "AI", "Machine Intelligence"],
    target_entity="AI Technology"
)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;With custom merge strategy:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Define custom merge strategy for different fields
rag.merge_entities(
    source_entities=["John Smith", "Dr. Smith", "J. Smith"],
    target_entity="John Smith",
    merge_strategy={
        "description": "concatenate",  # Combine all descriptions
        "entity_type": "keep_first",   # Keep the entity type from the first entity
        "source_id": "join_unique"     # Combine all unique source IDs
    }
)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;With custom target entity data:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Specify exact values for the merged entity
rag.merge_entities(
    source_entities=["New York", "NYC", "Big Apple"],
    target_entity="New York City",
    target_entity_data={
        "entity_type": "LOCATION",
        "description": "New York City is the most populous city in the United States.",
    }
)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Advanced usage combining both approaches:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Merge company entities with both strategy and custom data
rag.merge_entities(
    source_entities=["Microsoft Corp", "Microsoft Corporation", "MSFT"],
    target_entity="Microsoft",
    merge_strategy={
        "description": "concatenate",  # Combine all descriptions
        "source_id": "join_unique"     # Combine source IDs
    },
    target_entity_data={
        "entity_type": "ORGANIZATION",
    }
)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;When merging entities:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;All relationships from source entities are redirected to the target entity&lt;/li&gt; 
  &lt;li&gt;Duplicate relationships are intelligently merged&lt;/li&gt; 
  &lt;li&gt;Self-relationships (loops) are prevented&lt;/li&gt; 
  &lt;li&gt;Source entities are removed after merging&lt;/li&gt; 
  &lt;li&gt;Relationship weights and attributes are preserved&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;Multimodal Document Processing (RAG-Anything Integration)&lt;/h2&gt; 
&lt;p&gt;LightRAG now seamlessly integrates with &lt;a href="https://github.com/HKUDS/RAG-Anything"&gt;RAG-Anything&lt;/a&gt;, a comprehensive &lt;strong&gt;All-in-One Multimodal Document Processing RAG system&lt;/strong&gt; built specifically for LightRAG. RAG-Anything enables advanced parsing and retrieval-augmented generation (RAG) capabilities, allowing you to handle multimodal documents seamlessly and extract structured content‚Äîincluding text, images, tables, and formulas‚Äîfrom various document formats for integration into your RAG pipeline.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;End-to-End Multimodal Pipeline&lt;/strong&gt;: Complete workflow from document ingestion and parsing to intelligent multimodal query answering&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Universal Document Support&lt;/strong&gt;: Seamless processing of PDFs, Office documents (DOC/DOCX/PPT/PPTX/XLS/XLSX), images, and diverse file formats&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Specialized Content Analysis&lt;/strong&gt;: Dedicated processors for images, tables, mathematical equations, and heterogeneous content types&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multimodal Knowledge Graph&lt;/strong&gt;: Automatic entity extraction and cross-modal relationship discovery for enhanced understanding&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Hybrid Intelligent Retrieval&lt;/strong&gt;: Advanced search capabilities spanning textual and multimodal content with contextual understanding&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Quick Start:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Install RAG-Anything:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install raganything
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Process multimodal documents:&lt;/p&gt; 
  &lt;details&gt; 
   &lt;summary&gt; &lt;b&gt; RAGAnything Usage Example &lt;/b&gt;&lt;/summary&gt; 
   &lt;pre&gt;&lt;code class="language-python"&gt;    import asyncio
    from raganything import RAGAnything
    from lightrag import LightRAG
    from lightrag.llm.openai import openai_complete_if_cache, openai_embed
    from lightrag.utils import EmbeddingFunc
    import os

    async def load_existing_lightrag():
        # First, create or load an existing LightRAG instance
        lightrag_working_dir = "./existing_lightrag_storage"

        # Check if previous LightRAG instance exists
        if os.path.exists(lightrag_working_dir) and os.listdir(lightrag_working_dir):
            print("‚úÖ Found existing LightRAG instance, loading...")
        else:
            print("‚ùå No existing LightRAG instance found, will create new one")

        from functools import partial

        # Create/Load LightRAG instance with your configurations
        lightrag_instance = LightRAG(
            working_dir=lightrag_working_dir,
            llm_model_func=lambda prompt, system_prompt=None, history_messages=[], **kwargs: openai_complete_if_cache(
                "gpt-4o-mini",
                prompt,
                system_prompt=system_prompt,
                history_messages=history_messages,
                api_key="your-api-key",
                **kwargs,
            ),
            embedding_func=EmbeddingFunc(
                embedding_dim=3072,
                max_token_size=8192,
                model="text-embedding-3-large",
                func=partial(
                    openai_embed.func,  # Use .func to access the unwrapped function
                    model="text-embedding-3-large",
                    api_key=api_key,
                    base_url=base_url,
                ),
            )
        )

        # Initialize storage (this will load existing data if available)
        await lightrag_instance.initialize_storages()

        # Now initialize RAGAnything with the existing LightRAG instance
        rag = RAGAnything(
            lightrag=lightrag_instance,  # Pass the existing LightRAG instance
            # Only need vision model for multimodal processing
            vision_model_func=lambda prompt, system_prompt=None, history_messages=[], image_data=None, **kwargs: openai_complete_if_cache(
                "gpt-4o",
                "",
                system_prompt=None,
                history_messages=[],
                messages=[
                    {"role": "system", "content": system_prompt} if system_prompt else None,
                    {"role": "user", "content": [
                        {"type": "text", "text": prompt},
                        {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}}
                    ]} if image_data else {"role": "user", "content": prompt}
                ],
                api_key="your-api-key",
                **kwargs,
            ) if image_data else openai_complete_if_cache(
                "gpt-4o-mini",
                prompt,
                system_prompt=system_prompt,
                history_messages=history_messages,
                api_key="your-api-key",
                **kwargs,
            )
            # Note: working_dir, llm_model_func, embedding_func, etc. are inherited from lightrag_instance
        )

        # Query the existing knowledge base
        result = await rag.query_with_multimodal(
            "What data has been processed in this LightRAG instance?",
            mode="hybrid"
        )
        print("Query result:", result)

        # Add new multimodal documents to the existing LightRAG instance
        await rag.process_document_complete(
            file_path="path/to/new/multimodal_document.pdf",
            output_dir="./output"
        )

    if __name__ == "__main__":
        asyncio.run(load_existing_lightrag())
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;For detailed documentation and advanced usage, please refer to the &lt;a href="https://github.com/HKUDS/RAG-Anything"&gt;RAG-Anything repository&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Token Usage Tracking&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt;Overview and Usage&lt;/b&gt; &lt;/summary&gt; 
 &lt;p&gt;LightRAG provides a TokenTracker tool to monitor and manage token consumption by large language models. This feature is particularly useful for controlling API costs and optimizing performance.&lt;/p&gt; 
 &lt;h3&gt;Usage&lt;/h3&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from lightrag.utils import TokenTracker

# Create TokenTracker instance
token_tracker = TokenTracker()

# Method 1: Using context manager (Recommended)
# Suitable for scenarios requiring automatic token usage tracking
with token_tracker:
    result1 = await llm_model_func("your question 1")
    result2 = await llm_model_func("your question 2")

# Method 2: Manually adding token usage records
# Suitable for scenarios requiring more granular control over token statistics
token_tracker.reset()

rag.insert()

rag.query("your question 1", param=QueryParam(mode="naive"))
rag.query("your question 2", param=QueryParam(mode="mix"))

# Display total token usage (including insert and query operations)
print("Token usage:", token_tracker.get_usage())
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;Usage Tips&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Use context managers for long sessions or batch operations to automatically track all token consumption&lt;/li&gt; 
  &lt;li&gt;For scenarios requiring segmented statistics, use manual mode and call reset() when appropriate&lt;/li&gt; 
  &lt;li&gt;Regular checking of token usage helps detect abnormal consumption early&lt;/li&gt; 
  &lt;li&gt;Actively use this feature during development and testing to optimize production costs&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;Practical Examples&lt;/h3&gt; 
 &lt;p&gt;You can refer to these examples for implementing token tracking:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;examples/lightrag_gemini_track_token_demo.py&lt;/code&gt;: Token tracking example using Google Gemini model&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;examples/lightrag_siliconcloud_track_token_demo.py&lt;/code&gt;: Token tracking example using SiliconCloud model&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;These examples demonstrate how to effectively use the TokenTracker feature with different models and scenarios.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;Data Export Functions&lt;/h2&gt; 
&lt;h3&gt;Overview&lt;/h3&gt; 
&lt;p&gt;LightRAG allows you to export your knowledge graph data in various formats for analysis, sharing, and backup purposes. The system supports exporting entities, relations, and relationship data.&lt;/p&gt; 
&lt;h3&gt;Export Functions&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt; Basic Usage &lt;/b&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Basic CSV export (default format)
rag.export_data("knowledge_graph.csv")

# Specify any format
rag.export_data("output.xlsx", file_format="excel")
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt; Different File Formats supported &lt;/b&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;#Export data in CSV format
rag.export_data("graph_data.csv", file_format="csv")

# Export data in Excel sheet
rag.export_data("graph_data.xlsx", file_format="excel")

# Export data in markdown format
rag.export_data("graph_data.md", file_format="md")

# Export data in Text
rag.export_data("graph_data.txt", file_format="txt")
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt; Additional Options &lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;Include vector embeddings in the export (optional):&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;rag.export_data("complete_data.csv", include_vector_data=True)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Data Included in Export&lt;/h3&gt; 
&lt;p&gt;All exports include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Entity information (names, IDs, metadata)&lt;/li&gt; 
 &lt;li&gt;Relation data (connections between entities)&lt;/li&gt; 
 &lt;li&gt;Relationship information from vector database&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Cache&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt;Clear Cache&lt;/b&gt; &lt;/summary&gt; 
 &lt;p&gt;You can clear the LLM response cache with different modes:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Clear all cache
await rag.aclear_cache()

# Clear local mode cache
await rag.aclear_cache(modes=["local"])

# Clear extraction cache
await rag.aclear_cache(modes=["default"])

# Clear multiple modes
await rag.aclear_cache(modes=["local", "global", "hybrid"])

# Synchronous version
rag.clear_cache(modes=["local"])
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Valid modes are:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;"default"&lt;/code&gt;: Extraction cache&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;"naive"&lt;/code&gt;: Naive search cache&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;"local"&lt;/code&gt;: Local search cache&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;"global"&lt;/code&gt;: Global search cache&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;"hybrid"&lt;/code&gt;: Hybrid search cache&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;"mix"&lt;/code&gt;: Mix search cache&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;Troubleshooting&lt;/h2&gt; 
&lt;h3&gt;Common Initialization Errors&lt;/h3&gt; 
&lt;p&gt;If you encounter these errors when using LightRAG:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;AttributeError: __aenter__&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Cause&lt;/strong&gt;: Storage backends not initialized&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Solution&lt;/strong&gt;: Call &lt;code&gt;await rag.initialize_storages()&lt;/code&gt; after creating the LightRAG instance&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;KeyError: 'history_messages'&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Cause&lt;/strong&gt;: Pipeline status not initialized&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Solution&lt;/strong&gt;: Call &lt;code&gt;await rag.initialize_storages()&lt;/code&gt; after creating the LightRAG instance&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Both errors in sequence&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Cause&lt;/strong&gt;: Neither initialization method was called&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Solution&lt;/strong&gt;: Always follow this pattern:&lt;/li&gt; 
  &lt;/ul&gt; &lt;pre&gt;&lt;code class="language-python"&gt;rag = LightRAG(...)
await rag.initialize_storages()
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Model Switching Issues&lt;/h3&gt; 
&lt;p&gt;When switching between different embedding models, you must clear the data directory to avoid errors. The only file you may want to preserve is &lt;code&gt;kv_store_llm_response_cache.json&lt;/code&gt; if you wish to retain the LLM cache.&lt;/p&gt; 
&lt;h2&gt;LightRAG API&lt;/h2&gt; 
&lt;p&gt;The LightRAG Server is designed to provide Web UI and API support. &lt;strong&gt;For more information about LightRAG Server, please refer to &lt;a href="https://raw.githubusercontent.com/HKUDS/LightRAG/main/lightrag/api/README.md"&gt;LightRAG Server&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Graph Visualization&lt;/h2&gt; 
&lt;p&gt;The LightRAG Server offers a comprehensive knowledge graph visualization feature. It supports various gravity layouts, node queries, subgraph filtering, and more. &lt;strong&gt;For more information about LightRAG Server, please refer to &lt;a href="https://raw.githubusercontent.com/HKUDS/LightRAG/main/lightrag/api/README.md"&gt;LightRAG Server&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/HKUDS/LightRAG/main/README.assets/iShot_2025-03-23_12.40.08.png" alt="iShot_2025-03-23_12.40.08" /&gt;&lt;/p&gt; 
&lt;h2&gt;Langfuse observability integration&lt;/h2&gt; 
&lt;p&gt;Langfuse provides a drop-in replacement for the OpenAI client that automatically tracks all LLM interactions, enabling developers to monitor, debug, and optimize their RAG systems without code changes.&lt;/p&gt; 
&lt;h3&gt;Installation with Langfuse option&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;pip install lightrag-hku
pip install lightrag-hku[observability]

# Or install from source code with debug mode enabled
pip install -e .
pip install -e ".[observability]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Config Langfuse env vars&lt;/h3&gt; 
&lt;p&gt;modify .env file:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;## Langfuse Observability (Optional)
# LLM observability and tracing platform
# Install with: pip install lightrag-hku[observability]
# Sign up at: https://cloud.langfuse.com or self-host
LANGFUSE_SECRET_KEY=""
LANGFUSE_PUBLIC_KEY=""
LANGFUSE_HOST="https://cloud.langfuse.com"  # or your self-hosted instance
LANGFUSE_ENABLE_TRACE=true
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Langfuse Usage&lt;/h3&gt; 
&lt;p&gt;Once installed and configured, Langfuse automatically traces all OpenAI LLM calls. Langfuse dashboard features include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Tracing&lt;/strong&gt;: View complete LLM call chains&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Analytics&lt;/strong&gt;: Token usage, latency, cost metrics&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Debugging&lt;/strong&gt;: Inspect prompts and responses&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Evaluation&lt;/strong&gt;: Compare model outputs&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Monitoring&lt;/strong&gt;: Real-time alerting&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Important Notice&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: LightRAG currently only integrates OpenAI-compatible API calls with Langfuse. APIs such as Ollama, Azure, and AWS Bedrock are not yet supported for Langfuse observability.&lt;/p&gt; 
&lt;h2&gt;RAGAS-based Evaluation&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;RAGAS&lt;/strong&gt; (Retrieval Augmented Generation Assessment) is a framework for reference-free evaluation of RAG systems using LLMs. There is an evaluation script based on RAGAS. For detailed information, please refer to &lt;a href="https://raw.githubusercontent.com/HKUDS/LightRAG/main/lightrag/evaluation/README_EVALUASTION_RAGAS.md"&gt;RAGAS-based Evaluation Framework&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Evaluation&lt;/h2&gt; 
&lt;h3&gt;Dataset&lt;/h3&gt; 
&lt;p&gt;The dataset used in LightRAG can be downloaded from &lt;a href="https://huggingface.co/datasets/TommyChien/UltraDomain"&gt;TommyChien/UltraDomain&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Generate Query&lt;/h3&gt; 
&lt;p&gt;LightRAG uses the following prompt to generate high-level queries, with the corresponding code in &lt;code&gt;examples/generate_query.py&lt;/code&gt;.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt; Prompt &lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;Given the following description of a dataset:

{description}

Please identify 5 potential users who would engage with this dataset. For each user, list 5 tasks they would perform with this dataset. Then, for each (user, task) combination, generate 5 questions that require a high-level understanding of the entire dataset.

Output the results in the following structure:
- User 1: [user description]
    - Task 1: [task description]
        - Question 1:
        - Question 2:
        - Question 3:
        - Question 4:
        - Question 5:
    - Task 2: [task description]
        ...
    - Task 5: [task description]
- User 2: [user description]
    ...
- User 5: [user description]
    ...
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Batch Eval&lt;/h3&gt; 
&lt;p&gt;To evaluate the performance of two RAG systems on high-level queries, LightRAG uses the following prompt, with the specific code available in &lt;code&gt;reproduce/batch_eval.py&lt;/code&gt;.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt; Prompt &lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;---Role---
You are an expert tasked with evaluating two answers to the same question based on three criteria: **Comprehensiveness**, **Diversity**, and **Empowerment**.
---Goal---
You will evaluate two answers to the same question based on three criteria: **Comprehensiveness**, **Diversity**, and **Empowerment**.

- **Comprehensiveness**: How much detail does the answer provide to cover all aspects and details of the question?
- **Diversity**: How varied and rich is the answer in providing different perspectives and insights on the question?
- **Empowerment**: How well does the answer help the reader understand and make informed judgments about the topic?

For each criterion, choose the better answer (either Answer 1 or Answer 2) and explain why. Then, select an overall winner based on these three categories.

Here is the question:
{query}

Here are the two answers:

**Answer 1:**
{answer1}

**Answer 2:**
{answer2}

Evaluate both answers using the three criteria listed above and provide detailed explanations for each criterion.

Output your evaluation in the following JSON format:

{{
    "Comprehensiveness": {{
        "Winner": "[Answer 1 or Answer 2]",
        "Explanation": "[Provide explanation here]"
    }},
    "Empowerment": {{
        "Winner": "[Answer 1 or Answer 2]",
        "Explanation": "[Provide explanation here]"
    }},
    "Overall Winner": {{
        "Winner": "[Answer 1 or Answer 2]",
        "Explanation": "[Summarize why this answer is the overall winner based on the three criteria]"
    }}
}}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Overall Performance Table&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Agriculture&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;CS&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Legal&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Mix&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;NaiveRAG&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;LightRAG&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;NaiveRAG&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;LightRAG&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;NaiveRAG&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;LightRAG&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;NaiveRAG&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;LightRAG&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Comprehensiveness&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;32.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;67.6%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;38.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;61.6%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;16.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;83.6%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;38.8%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;61.2%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Diversity&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;23.6%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;76.4%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;38.0%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;62.0%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;13.6%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;86.4%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;32.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;67.6%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Empowerment&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;32.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;67.6%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;38.8%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;61.2%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;16.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;83.6%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;42.8%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;57.2%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Overall&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;32.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;67.6%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;38.8%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;61.2%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;15.2%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;84.8%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;40.0%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;60.0%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;RQ-RAG&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;LightRAG&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;RQ-RAG&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;LightRAG&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;RQ-RAG&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;LightRAG&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;RQ-RAG&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;LightRAG&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Comprehensiveness&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;31.6%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;68.4%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;38.8%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;61.2%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;15.2%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;84.8%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;39.2%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;60.8%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Diversity&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;29.2%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;70.8%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;39.2%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;60.8%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;11.6%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;88.4%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;30.8%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;69.2%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Empowerment&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;31.6%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;68.4%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;36.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;63.6%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;15.2%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;84.8%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;42.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;57.6%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Overall&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;32.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;67.6%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;38.0%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;62.0%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;14.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;85.6%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;40.0%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;60.0%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;HyDE&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;LightRAG&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;HyDE&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;LightRAG&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;HyDE&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;LightRAG&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;HyDE&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;LightRAG&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Comprehensiveness&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;26.0%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;74.0%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;41.6%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;58.4%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;26.8%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;73.2%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;40.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;59.6%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Diversity&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;24.0%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;76.0%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;38.8%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;61.2%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;20.0%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;80.0%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;32.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;67.6%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Empowerment&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;25.2%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;74.8%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;40.8%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;59.2%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;26.0%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;74.0%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;46.0%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;54.0%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Overall&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;24.8%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;75.2%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;41.6%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;58.4%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;26.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;73.6%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;42.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;57.6%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;GraphRAG&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;LightRAG&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;GraphRAG&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;LightRAG&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;GraphRAG&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;LightRAG&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;GraphRAG&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;LightRAG&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Comprehensiveness&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;45.6%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;54.4%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;48.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;51.6%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;48.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;51.6%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;50.4%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;49.6%&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Diversity&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;22.8%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;77.2%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;40.8%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;59.2%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;26.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;73.6%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;36.0%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;64.0%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Empowerment&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;41.2%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;58.8%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;45.2%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;54.8%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;43.6%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;56.4%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;50.8%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;49.2%&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Overall&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;45.2%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;54.8%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;48.0%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;52.0%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;47.2%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;52.8%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;50.4%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;49.6%&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Reproduce&lt;/h2&gt; 
&lt;p&gt;All the code can be found in the &lt;code&gt;./reproduce&lt;/code&gt; directory.&lt;/p&gt; 
&lt;h3&gt;Step-0 Extract Unique Contexts&lt;/h3&gt; 
&lt;p&gt;First, we need to extract unique contexts in the datasets.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt; Code &lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;def extract_unique_contexts(input_directory, output_directory):

    os.makedirs(output_directory, exist_ok=True)

    jsonl_files = glob.glob(os.path.join(input_directory, '*.jsonl'))
    print(f"Found {len(jsonl_files)} JSONL files.")

    for file_path in jsonl_files:
        filename = os.path.basename(file_path)
        name, ext = os.path.splitext(filename)
        output_filename = f"{name}_unique_contexts.json"
        output_path = os.path.join(output_directory, output_filename)

        unique_contexts_dict = {}

        print(f"Processing file: {filename}")

        try:
            with open(file_path, 'r', encoding='utf-8') as infile:
                for line_number, line in enumerate(infile, start=1):
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        json_obj = json.loads(line)
                        context = json_obj.get('context')
                        if context and context not in unique_contexts_dict:
                            unique_contexts_dict[context] = None
                    except json.JSONDecodeError as e:
                        print(f"JSON decoding error in file {filename} at line {line_number}: {e}")
        except FileNotFoundError:
            print(f"File not found: {filename}")
            continue
        except Exception as e:
            print(f"An error occurred while processing file {filename}: {e}")
            continue

        unique_contexts_list = list(unique_contexts_dict.keys())
        print(f"There are {len(unique_contexts_list)} unique `context` entries in the file {filename}.")

        try:
            with open(output_path, 'w', encoding='utf-8') as outfile:
                json.dump(unique_contexts_list, outfile, ensure_ascii=False, indent=4)
            print(f"Unique `context` entries have been saved to: {output_filename}")
        except Exception as e:
            print(f"An error occurred while saving to the file {output_filename}: {e}")

    print("All files have been processed.")

&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Step-1 Insert Contexts&lt;/h3&gt; 
&lt;p&gt;For the extracted contexts, we insert them into the LightRAG system.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt; Code &lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;def insert_text(rag, file_path):
    with open(file_path, mode='r') as f:
        unique_contexts = json.load(f)

    retries = 0
    max_retries = 3
    while retries &amp;lt; max_retries:
        try:
            rag.insert(unique_contexts)
            break
        except Exception as e:
            retries += 1
            print(f"Insertion failed, retrying ({retries}/{max_retries}), error: {e}")
            time.sleep(10)
    if retries == max_retries:
        print("Insertion failed after exceeding the maximum number of retries")
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Step-2 Generate Queries&lt;/h3&gt; 
&lt;p&gt;We extract tokens from the first and the second half of each context in the dataset, then combine them as dataset descriptions to generate queries.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt; Code &lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

def get_summary(context, tot_tokens=2000):
    tokens = tokenizer.tokenize(context)
    half_tokens = tot_tokens // 2

    start_tokens = tokens[1000:1000 + half_tokens]
    end_tokens = tokens[-(1000 + half_tokens):1000]

    summary_tokens = start_tokens + end_tokens
    summary = tokenizer.convert_tokens_to_string(summary_tokens)

    return summary
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Step-3 Query&lt;/h3&gt; 
&lt;p&gt;For the queries generated in Step-2, we will extract them and query LightRAG.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt; Code &lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;def extract_queries(file_path):
    with open(file_path, 'r') as f:
        data = f.read()

    data = data.replace('**', '')

    queries = re.findall(r'- Question \d+: (.+)', data)

    return queries
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;üîó Related Projects&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;Ecosystem &amp;amp; Extensions&lt;/em&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td align="center"&gt; &lt;a href="https://github.com/HKUDS/RAG-Anything"&gt; 
      &lt;div style="width: 100px; height: 100px; background: linear-gradient(135deg, rgba(0, 217, 255, 0.1) 0%, rgba(0, 217, 255, 0.05) 100%); border-radius: 15px; border: 1px solid rgba(0, 217, 255, 0.2); display: flex; align-items: center; justify-content: center; margin-bottom: 10px;"&gt; 
       &lt;span style="font-size: 32px;"&gt;üì∏&lt;/span&gt; 
      &lt;/div&gt; &lt;b&gt;RAG-Anything&lt;/b&gt;&lt;br /&gt; &lt;sub&gt;Multimodal RAG&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt; 
    &lt;td align="center"&gt; &lt;a href="https://github.com/HKUDS/VideoRAG"&gt; 
      &lt;div style="width: 100px; height: 100px; background: linear-gradient(135deg, rgba(0, 217, 255, 0.1) 0%, rgba(0, 217, 255, 0.05) 100%); border-radius: 15px; border: 1px solid rgba(0, 217, 255, 0.2); display: flex; align-items: center; justify-content: center; margin-bottom: 10px;"&gt; 
       &lt;span style="font-size: 32px;"&gt;üé•&lt;/span&gt; 
      &lt;/div&gt; &lt;b&gt;VideoRAG&lt;/b&gt;&lt;br /&gt; &lt;sub&gt;Extreme Long-Context Video RAG&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt; 
    &lt;td align="center"&gt; &lt;a href="https://github.com/HKUDS/MiniRAG"&gt; 
      &lt;div style="width: 100px; height: 100px; background: linear-gradient(135deg, rgba(0, 217, 255, 0.1) 0%, rgba(0, 217, 255, 0.05) 100%); border-radius: 15px; border: 1px solid rgba(0, 217, 255, 0.2); display: flex; align-items: center; justify-content: center; margin-bottom: 10px;"&gt; 
       &lt;span style="font-size: 32px;"&gt;‚ú®&lt;/span&gt; 
      &lt;/div&gt; &lt;b&gt;MiniRAG&lt;/b&gt;&lt;br /&gt; &lt;sub&gt;Extremely Simple RAG&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;‚≠ê Star History&lt;/h2&gt; 
&lt;a href="https://star-history.com/#HKUDS/LightRAG&amp;amp;Date"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=HKUDS/LightRAG&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=HKUDS/LightRAG&amp;amp;type=Date" /&gt; 
  &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=HKUDS/LightRAG&amp;amp;type=Date" /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;h2&gt;ü§ù Contribution&lt;/h2&gt; 
&lt;div align="center"&gt;
  We thank all our contributors for their valuable contributions. 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/HKUDS/LightRAG/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=HKUDS/LightRAG" style="border-radius: 15px; box-shadow: 0 0 20px rgba(0, 217, 255, 0.3);" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìñ Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;@article{guo2024lightrag,
title={LightRAG: Simple and Fast Retrieval-Augmented Generation},
author={Zirui Guo and Lianghao Xia and Yanhua Yu and Tu Ao and Chao Huang},
year={2024},
eprint={2410.05779},
archivePrefix={arXiv},
primaryClass={cs.IR}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;div align="center" style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; padding: 30px; margin: 30px 0;"&gt; 
 &lt;div&gt; 
  &lt;img src="https://user-images.githubusercontent.com/74038190/212284100-561aa473-3905-4a80-b561-0d28506553ee.gif" width="500" /&gt; 
 &lt;/div&gt; 
 &lt;div style="margin-top: 20px;"&gt; 
  &lt;a href="https://github.com/HKUDS/LightRAG" style="text-decoration: none;"&gt; &lt;img src="https://img.shields.io/badge/‚≠ê%20Star%20us%20on%20GitHub-1a1a2e?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white" /&gt; &lt;/a&gt; 
  &lt;a href="https://github.com/HKUDS/LightRAG/issues" style="text-decoration: none;"&gt; &lt;img src="https://img.shields.io/badge/üêõ%20Report%20Issues-ff6b6b?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white" /&gt; &lt;/a&gt; 
  &lt;a href="https://github.com/HKUDS/LightRAG/discussions" style="text-decoration: none;"&gt; &lt;img src="https://img.shields.io/badge/üí¨%20Discussions-4ecdc4?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white" /&gt; &lt;/a&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;div style="width: 100%; max-width: 600px; margin: 20px auto; padding: 20px; background: linear-gradient(135deg, rgba(0, 217, 255, 0.1) 0%, rgba(0, 217, 255, 0.05) 100%); border-radius: 15px; border: 1px solid rgba(0, 217, 255, 0.2);"&gt; 
  &lt;div style="display: flex; justify-content: center; align-items: center; gap: 15px;"&gt; 
   &lt;span style="font-size: 24px;"&gt;‚≠ê&lt;/span&gt; 
   &lt;span style="color: #00d9ff; font-size: 18px;"&gt;Thank you for visiting LightRAG!&lt;/span&gt; 
   &lt;span style="font-size: 24px;"&gt;‚≠ê&lt;/span&gt; 
  &lt;/div&gt; 
 &lt;/div&gt; 
&lt;/div&gt;</description>
    </item>
    
  </channel>
</rss>