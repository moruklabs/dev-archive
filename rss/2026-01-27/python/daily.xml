<rss version="2.0">
  <channel>
    <title>GitHub Python Daily Trending</title>
    <description>Daily Trending of Python in GitHub</description>
    <pubDate>Mon, 26 Jan 2026 01:39:41 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>mikf/gallery-dl</title>
      <link>https://github.com/mikf/gallery-dl</link>
      <description>&lt;p&gt;Command-line program to download image galleries and collections from several image hosting sites&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;========== gallery-dl&lt;/h1&gt; 
&lt;p&gt;&lt;em&gt;gallery-dl&lt;/em&gt; is a command-line program to download image galleries and collections from several image hosting sites (see &lt;code&gt;Supported Sites &amp;lt;docs/supportedsites.md&amp;gt;&lt;/code&gt;&lt;strong&gt;). It is a cross-platform tool with many &lt;code&gt;configuration options &amp;lt;https://gdl-org.github.io/docs/configuration.html&amp;gt;&lt;/code&gt;&lt;/strong&gt; and powerful &lt;code&gt;filenaming capabilities &amp;lt;https://gdl-org.github.io/docs/formatting.html&amp;gt;&lt;/code&gt;__.&lt;/p&gt; 
&lt;p&gt;|pypi| |discord| |build|&lt;/p&gt; 
&lt;p&gt;.. contents::&lt;/p&gt; 
&lt;h1&gt;Dependencies&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python_ 3.8+&lt;/li&gt; 
 &lt;li&gt;Requests_&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Optional&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;yt-dlp_ or youtube-dl_: HLS/DASH video downloads, &lt;code&gt;ytdl&lt;/code&gt; integration&lt;/li&gt; 
 &lt;li&gt;FFmpeg_: Pixiv Ugoira conversion&lt;/li&gt; 
 &lt;li&gt;mkvmerge_: Accurate Ugoira frame timecodes&lt;/li&gt; 
 &lt;li&gt;PySocks_: SOCKS proxy support&lt;/li&gt; 
 &lt;li&gt;brotli_ or brotlicffi_: Brotli compression support&lt;/li&gt; 
 &lt;li&gt;zstandard_: Zstandard compression support&lt;/li&gt; 
 &lt;li&gt;PyYAML_: YAML configuration file support&lt;/li&gt; 
 &lt;li&gt;toml_: TOML configuration file support for Python&amp;lt;3.11&lt;/li&gt; 
 &lt;li&gt;SecretStorage_: GNOME keyring passwords for &lt;code&gt;--cookies-from-browser&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Psycopg_: PostgreSQL archive support&lt;/li&gt; 
 &lt;li&gt;truststore_: Native system certificate support&lt;/li&gt; 
 &lt;li&gt;Jinja_: Jinja template support&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Installation&lt;/h1&gt; 
&lt;h2&gt;Pip&lt;/h2&gt; 
&lt;p&gt;The stable releases of &lt;em&gt;gallery-dl&lt;/em&gt; are distributed on PyPI_ and can be easily installed or upgraded using pip_:&lt;/p&gt; 
&lt;p&gt;.. code:: bash&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python3 -m pip install -U gallery-dl
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Installing the latest dev version directly from GitHub can be done with pip_ as well:&lt;/p&gt; 
&lt;p&gt;.. code:: bash&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python3 -m pip install -U --force-reinstall --no-deps https://github.com/mikf/gallery-dl/archive/master.tar.gz
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Omit :code:&lt;code&gt;--no-deps&lt;/code&gt; if Requests_ hasn't been installed yet.&lt;/p&gt; 
&lt;p&gt;Note: Windows users should use :code:&lt;code&gt;py -3&lt;/code&gt; instead of :code:&lt;code&gt;python3&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;It is advised to use the latest version of pip_, including the essential packages :code:&lt;code&gt;setuptools&lt;/code&gt; and :code:&lt;code&gt;wheel&lt;/code&gt;. To ensure these packages are up-to-date, run&lt;/p&gt; 
&lt;p&gt;.. code:: bash&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python3 -m pip install --upgrade pip setuptools wheel
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Standalone Executable&lt;/h2&gt; 
&lt;p&gt;Prebuilt executable files with a Python interpreter and required Python packages included are available for&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;Windows &amp;lt;https://github.com/mikf/gallery-dl/releases/download/v1.31.4/gallery-dl.exe&amp;gt;&lt;/code&gt;__ (Requires &lt;code&gt;Microsoft Visual C++ Redistributable Package (x86) &amp;lt;https://aka.ms/vs/17/release/vc_redist.x86.exe&amp;gt;&lt;/code&gt;__)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Linux &amp;lt;https://github.com/mikf/gallery-dl/releases/download/v1.31.4/gallery-dl.bin&amp;gt;&lt;/code&gt;__&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Nightly Builds&lt;/h2&gt; 
&lt;p&gt;| Executables build from the latest commit can be found at | &lt;a href="https://github.com/gdl-org/builds/releases"&gt;https://github.com/gdl-org/builds/releases&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Snap&lt;/h2&gt; 
&lt;p&gt;Linux users that are using a distro that is supported by Snapd_ can install &lt;em&gt;gallery-dl&lt;/em&gt; from the Snap Store:&lt;/p&gt; 
&lt;p&gt;.. code:: bash&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;snap install gallery-dl
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Chocolatey&lt;/h2&gt; 
&lt;p&gt;Windows users that have Chocolatey_ installed can install &lt;em&gt;gallery-dl&lt;/em&gt; from the Chocolatey Community Packages repository:&lt;/p&gt; 
&lt;p&gt;.. code:: powershell&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;choco install gallery-dl
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Scoop&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;gallery-dl&lt;/em&gt; is also available in the Scoop_ "main" bucket for Windows users:&lt;/p&gt; 
&lt;p&gt;.. code:: powershell&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;scoop install gallery-dl
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Homebrew&lt;/h2&gt; 
&lt;p&gt;For macOS or Linux users using Homebrew:&lt;/p&gt; 
&lt;p&gt;.. code:: bash&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;brew install gallery-dl
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;MacPorts&lt;/h2&gt; 
&lt;p&gt;For macOS users with MacPorts:&lt;/p&gt; 
&lt;p&gt;.. code:: bash&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;sudo port install gallery-dl
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Docker&lt;/h2&gt; 
&lt;p&gt;Using the Dockerfile in the repository:&lt;/p&gt; 
&lt;p&gt;.. code:: bash&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;git clone https://github.com/mikf/gallery-dl.git
cd gallery-dl/
docker build -t gallery-dl:latest .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Pulling image from &lt;code&gt;Docker Hub &amp;lt;https://hub.docker.com/r/mikf123/gallery-dl&amp;gt;&lt;/code&gt;__:&lt;/p&gt; 
&lt;p&gt;.. code:: bash&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;docker pull mikf123/gallery-dl
docker tag mikf123/gallery-dl gallery-dl
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Pulling image from &lt;code&gt;GitHub Container Registry &amp;lt;https://github.com/mikf/gallery-dl/pkgs/container/gallery-dl&amp;gt;&lt;/code&gt;__:&lt;/p&gt; 
&lt;p&gt;.. code:: bash&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;docker pull ghcr.io/mikf/gallery-dl
docker tag ghcr.io/mikf/gallery-dl gallery-dl
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Pulling &lt;em&gt;Nightly Build&lt;/em&gt; images built from the latest commit by using the &lt;code&gt;dev&lt;/code&gt; tag:&lt;/p&gt; 
&lt;p&gt;.. code:: bash&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;docker pull mikf123/gallery-dl:dev
docker pull ghcr.io/mikf/gallery-dl:dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To run the container you will probably want to attach some directories on the host so that the config file and downloads can persist across runs.&lt;/p&gt; 
&lt;p&gt;Make sure to either download the example config file reference in the repo and place it in the mounted volume location or touch an empty file there.&lt;/p&gt; 
&lt;p&gt;If you gave the container a different tag or are using podman then make sure you adjust. Run &lt;code&gt;docker image ls&lt;/code&gt; to check the name if you are not sure.&lt;/p&gt; 
&lt;p&gt;This will remove the container after every use so you will always have a fresh environment for it to run. If you setup a ci-cd pipeline to autobuild the container you can also add a &lt;code&gt;--pull=newer&lt;/code&gt; flag so that when you run it docker will check to see if there is a newer container and download it before running.&lt;/p&gt; 
&lt;p&gt;.. code:: bash&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;docker run --rm  -v $HOME/Downloads/:/gallery-dl/ -v $HOME/.config/gallery-dl/gallery-dl.conf:/etc/gallery-dl.conf -it gallery-dl:latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also add an alias to your shell for "gallery-dl" or create a simple bash script and drop it somewhere in your $PATH to act as a shim for this command.&lt;/p&gt; 
&lt;h2&gt;Nix and Home Manager&lt;/h2&gt; 
&lt;p&gt;Adding &lt;em&gt;gallery-dl&lt;/em&gt; to your system environment:&lt;/p&gt; 
&lt;p&gt;.. code:: nix&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;environment.systemPackages = with pkgs; [
  gallery-dl
];
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Using :code:&lt;code&gt;nix-shell&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;.. code:: bash&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;nix-shell -p gallery-dl
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;.. code:: bash&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;nix-shell -p gallery-dl --run "gallery-dl &amp;lt;args&amp;gt;"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For Home Manager users, you can manage &lt;em&gt;gallery-dl&lt;/em&gt; declaratively:&lt;/p&gt; 
&lt;p&gt;.. code:: nix&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;programs.gallery-dl = {
  enable = true;
  settings = {
    extractor.base-directory = "~/Downloads";
  };
};
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, you can just add it to :code:&lt;code&gt;home.packages&lt;/code&gt; if you don't want to manage it declaratively:&lt;/p&gt; 
&lt;p&gt;.. code:: nix&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;home.packages = with pkgs; [
  gallery-dl
];
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After making these changes, simply rebuild your configuration and open a new shell to have &lt;em&gt;gallery-dl&lt;/em&gt; available.&lt;/p&gt; 
&lt;h1&gt;Usage&lt;/h1&gt; 
&lt;p&gt;To use &lt;em&gt;gallery-dl&lt;/em&gt; simply call it with the URLs you wish to download images from:&lt;/p&gt; 
&lt;p&gt;.. code:: bash&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;gallery-dl [OPTIONS]... URLS...
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Use :code:&lt;code&gt;gallery-dl --help&lt;/code&gt; or see &lt;code&gt;&amp;lt;docs/options.md&amp;gt;&lt;/code&gt;__ for a full list of all command-line options.&lt;/p&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;p&gt;Download images; in this case from danbooru via tag search for 'bonocho':&lt;/p&gt; 
&lt;p&gt;.. code:: bash&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;gallery-dl "https://danbooru.donmai.us/posts?tags=bonocho"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Get the direct URL of an image from a site supporting authentication with username &amp;amp; password:&lt;/p&gt; 
&lt;p&gt;.. code:: bash&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;gallery-dl -g -u "&amp;lt;username&amp;gt;" -p "&amp;lt;password&amp;gt;" "https://twitter.com/i/web/status/604341487988576256"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Filter manga chapters by chapter number and language:&lt;/p&gt; 
&lt;p&gt;.. code:: bash&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;gallery-dl --chapter-filter "10 &amp;lt;= chapter &amp;lt; 20" -o "lang=fr" "https://mangadex.org/title/59793dd0-a2d8-41a2-9758-8197287a8539"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;| Search a remote resource for URLs and download images from them: | (URLs for which no extractor can be found will be silently ignored)&lt;/p&gt; 
&lt;p&gt;.. code:: bash&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;gallery-dl "r:https://pastebin.com/raw/FLwrCYsT"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If a site's address is nonstandard for its extractor, you can prefix the URL with the extractor's name to force the use of a specific extractor:&lt;/p&gt; 
&lt;p&gt;.. code:: bash&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;gallery-dl "tumblr:https://sometumblrblog.example"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Configuration&lt;/h1&gt; 
&lt;p&gt;Configuration files for &lt;em&gt;gallery-dl&lt;/em&gt; use a JSON-based file format.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;A list of all available configuration options and their descriptions can be found at &lt;code&gt;&amp;lt;https://gdl-org.github.io/docs/configuration.html&amp;gt;&lt;/code&gt;__.&lt;/p&gt; 
&lt;p&gt;| For a default configuration file with available options set to their default values, see &lt;code&gt;&amp;lt;docs/gallery-dl.conf&amp;gt;&lt;/code&gt;__.&lt;/p&gt; 
&lt;p&gt;| For a commented example with more involved settings and option usage, see &lt;code&gt;&amp;lt;docs/gallery-dl-example.conf&amp;gt;&lt;/code&gt;__.&lt;/p&gt; 
&lt;h2&gt;Locations&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;gallery-dl&lt;/em&gt; searches for configuration files in the following places:&lt;/p&gt; 
&lt;p&gt;Windows: * &lt;code&gt;%APPDATA%\gallery-dl\config.json&lt;/code&gt; * &lt;code&gt;%USERPROFILE%\gallery-dl\config.json&lt;/code&gt; * &lt;code&gt;%USERPROFILE%\gallery-dl.conf&lt;/code&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;(``%USERPROFILE%`` usually refers to a user's home directory,
i.e. ``C:\Users\&amp;lt;username&amp;gt;\``)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Linux, macOS, etc.: * &lt;code&gt;/etc/gallery-dl.conf&lt;/code&gt; * &lt;code&gt;${XDG_CONFIG_HOME}/gallery-dl/config.json&lt;/code&gt; * &lt;code&gt;${HOME}/.config/gallery-dl/config.json&lt;/code&gt; * &lt;code&gt;${HOME}/.gallery-dl.conf&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;When run as &lt;code&gt;executable &amp;lt;Standalone Executable_&amp;gt;&lt;/code&gt;__, &lt;em&gt;gallery-dl&lt;/em&gt; will also look for a &lt;code&gt;gallery-dl.conf&lt;/code&gt; file in the same directory as said executable.&lt;/p&gt; 
&lt;p&gt;It is possible to use more than one configuration file at a time. In this case, any values from files after the first will get merged into the already loaded settings and potentially override previous ones.&lt;/p&gt; 
&lt;h1&gt;Authentication&lt;/h1&gt; 
&lt;h2&gt;Username &amp;amp; Password&lt;/h2&gt; 
&lt;p&gt;Some extractors require you to provide valid login credentials in the form of a username &amp;amp; password pair. This is necessary for &lt;code&gt;nijie&lt;/code&gt; and optional for &lt;code&gt;aryion&lt;/code&gt;, &lt;code&gt;danbooru&lt;/code&gt;, &lt;code&gt;e621&lt;/code&gt;, &lt;code&gt;exhentai&lt;/code&gt;, &lt;code&gt;idolcomplex&lt;/code&gt;, &lt;code&gt;imgbb&lt;/code&gt;, &lt;code&gt;inkbunny&lt;/code&gt;, &lt;code&gt;mangadex&lt;/code&gt;, &lt;code&gt;mangoxo&lt;/code&gt;, &lt;code&gt;pillowfort&lt;/code&gt;, &lt;code&gt;sankaku&lt;/code&gt;, &lt;code&gt;subscribestar&lt;/code&gt;, &lt;code&gt;tapas&lt;/code&gt;, &lt;code&gt;tsumino&lt;/code&gt;, &lt;code&gt;twitter&lt;/code&gt;, and &lt;code&gt;zerochan&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;You can set the necessary information in your &lt;code&gt;configuration file &amp;lt;Configuration_&amp;gt;&lt;/code&gt;__&lt;/p&gt; 
&lt;p&gt;.. code:: json&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;{
    "extractor": {
        "twitter": {
            "username": "&amp;lt;username&amp;gt;",
            "password": "&amp;lt;password&amp;gt;"
        }
    }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or you can provide them directly via the :code:&lt;code&gt;-u/--username&lt;/code&gt; and :code:&lt;code&gt;-p/--password&lt;/code&gt; or via the :code:&lt;code&gt;-o/--option&lt;/code&gt; command-line options&lt;/p&gt; 
&lt;p&gt;.. code:: bash&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;gallery-dl -u "&amp;lt;username&amp;gt;" -p "&amp;lt;password&amp;gt;" "URL"
gallery-dl -o "username=&amp;lt;username&amp;gt;" -o "password=&amp;lt;password&amp;gt;" "URL"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Cookies&lt;/h2&gt; 
&lt;p&gt;For sites where login with username &amp;amp; password is not possible due to CAPTCHA or similar, or has not been implemented yet, you can use the cookies from a browser login session and input them into &lt;em&gt;gallery-dl&lt;/em&gt;.&lt;/p&gt; 
&lt;p&gt;This can be done via the &lt;code&gt;cookies &amp;lt;https://gdl-org.github.io/docs/configuration.html#extractor-cookies&amp;gt;&lt;/code&gt;__ option in your configuration file by specifying&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;| the path to a Mozilla/Netscape format cookies.txt file exported by a browser addon | (e.g. &lt;code&gt;Get cookies.txt LOCALLY &amp;lt;https://chrome.google.com/webstore/detail/get-cookiestxt-locally/cclelndahbckbenkjhflpdbgdldlbecc&amp;gt;&lt;/code&gt;__ for Chrome, &lt;code&gt;Export Cookies &amp;lt;https://addons.mozilla.org/en-US/firefox/addon/export-cookies-txt/&amp;gt;&lt;/code&gt;__ for Firefox)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;| a list of name-value pairs gathered from your browser's web developer tools | (in &lt;code&gt;Chrome &amp;lt;https://developers.google.com/web/tools/chrome-devtools/storage/cookies&amp;gt;&lt;/code&gt;&lt;strong&gt;, in &lt;code&gt;Firefox &amp;lt;https://developer.mozilla.org/en-US/docs/Tools/Storage_Inspector&amp;gt;&lt;/code&gt;&lt;/strong&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;| the name of a browser to extract cookies from | (supported browsers are Chromium-based ones, Firefox, and Safari)&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For example:&lt;/p&gt; 
&lt;p&gt;.. code:: json&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;{
    "extractor": {
        "instagram": {
            "cookies": "$HOME/path/to/cookies.txt"
        },
        "patreon": {
            "cookies": {
                "session_id": "K1T57EKu19TR49C51CDjOJoXNQLF7VbdVOiBrC9ye0a"
            }
        },
        "twitter": {
            "cookies": ["firefox"]
        }
    }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;| You can also specify a cookies.txt file with the :code:&lt;code&gt;--cookies&lt;/code&gt; command-line option | or a browser to extract cookies from with :code:&lt;code&gt;--cookies-from-browser&lt;/code&gt;:&lt;/p&gt; 
&lt;p&gt;.. code:: bash&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;gallery-dl --cookies "$HOME/path/to/cookies.txt" "URL"
gallery-dl --cookies-from-browser firefox "URL"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;OAuth&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;gallery-dl&lt;/em&gt; supports user authentication via OAuth_ for some extractors. This is necessary for &lt;code&gt;pixiv&lt;/code&gt; and optional for &lt;code&gt;deviantart&lt;/code&gt;, &lt;code&gt;flickr&lt;/code&gt;, &lt;code&gt;reddit&lt;/code&gt;, &lt;code&gt;smugmug&lt;/code&gt;, &lt;code&gt;tumblr&lt;/code&gt;, and &lt;code&gt;mastodon&lt;/code&gt; instances.&lt;/p&gt; 
&lt;p&gt;Linking your account to &lt;em&gt;gallery-dl&lt;/em&gt; grants it the ability to issue requests on your account's behalf and enables it to access resources which would otherwise be unavailable to a public user.&lt;/p&gt; 
&lt;p&gt;To do so, start by invoking it with &lt;code&gt;oauth:&amp;lt;sitename&amp;gt;&lt;/code&gt; as an argument. For example:&lt;/p&gt; 
&lt;p&gt;.. code:: bash&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;gallery-dl oauth:flickr
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You will be sent to the site's authorization page and asked to grant read access to &lt;em&gt;gallery-dl&lt;/em&gt;. Authorize it and you will be shown one or more "tokens", which should be added to your configuration file.&lt;/p&gt; 
&lt;p&gt;To authenticate with a &lt;code&gt;mastodon&lt;/code&gt; instance, run &lt;em&gt;gallery-dl&lt;/em&gt; with &lt;code&gt;oauth:mastodon:&amp;lt;instance&amp;gt;&lt;/code&gt; as argument. For example:&lt;/p&gt; 
&lt;p&gt;.. code:: bash&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;gallery-dl oauth:mastodon:pawoo.net
gallery-dl oauth:mastodon:https://mastodon.social/
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;.. _Python: &lt;a href="https://www.python.org/downloads/"&gt;https://www.python.org/downloads/&lt;/a&gt; .. _PyPI: &lt;a href="https://pypi.org/"&gt;https://pypi.org/&lt;/a&gt; .. _pip: &lt;a href="https://pip.pypa.io/en/stable/"&gt;https://pip.pypa.io/en/stable/&lt;/a&gt; .. _Requests: &lt;a href="https://requests.readthedocs.io/en/latest/"&gt;https://requests.readthedocs.io/en/latest/&lt;/a&gt; .. _FFmpeg: &lt;a href="https://www.ffmpeg.org/"&gt;https://www.ffmpeg.org/&lt;/a&gt; .. _mkvmerge: &lt;a href="https://www.matroska.org/downloads/mkvtoolnix.html"&gt;https://www.matroska.org/downloads/mkvtoolnix.html&lt;/a&gt; .. _yt-dlp: &lt;a href="https://github.com/yt-dlp/yt-dlp"&gt;https://github.com/yt-dlp/yt-dlp&lt;/a&gt; .. _youtube-dl: &lt;a href="https://ytdl-org.github.io/youtube-dl/"&gt;https://ytdl-org.github.io/youtube-dl/&lt;/a&gt; .. _PySocks: &lt;a href="https://pypi.org/project/PySocks/"&gt;https://pypi.org/project/PySocks/&lt;/a&gt; .. _brotli: &lt;a href="https://github.com/google/brotli"&gt;https://github.com/google/brotli&lt;/a&gt; .. _brotlicffi: &lt;a href="https://github.com/python-hyper/brotlicffi"&gt;https://github.com/python-hyper/brotlicffi&lt;/a&gt; .. _zstandard: &lt;a href="https://github.com/indygreg/python-zstandard"&gt;https://github.com/indygreg/python-zstandard&lt;/a&gt; .. _PyYAML: &lt;a href="https://pyyaml.org/"&gt;https://pyyaml.org/&lt;/a&gt; .. _toml: &lt;a href="https://pypi.org/project/toml/"&gt;https://pypi.org/project/toml/&lt;/a&gt; .. _SecretStorage: &lt;a href="https://pypi.org/project/SecretStorage/"&gt;https://pypi.org/project/SecretStorage/&lt;/a&gt; .. _Psycopg: &lt;a href="https://www.psycopg.org/"&gt;https://www.psycopg.org/&lt;/a&gt; .. _truststore: &lt;a href="https://truststore.readthedocs.io/en/latest/"&gt;https://truststore.readthedocs.io/en/latest/&lt;/a&gt; .. _Jinja: &lt;a href="https://jinja.palletsprojects.com/"&gt;https://jinja.palletsprojects.com/&lt;/a&gt; .. _Snapd: &lt;a href="https://docs.snapcraft.io/installing-snapd"&gt;https://docs.snapcraft.io/installing-snapd&lt;/a&gt; .. _OAuth: &lt;a href="https://en.wikipedia.org/wiki/OAuth"&gt;https://en.wikipedia.org/wiki/OAuth&lt;/a&gt; .. _Chocolatey: &lt;a href="https://chocolatey.org/install"&gt;https://chocolatey.org/install&lt;/a&gt; .. _Scoop: &lt;a href="https://scoop.sh/"&gt;https://scoop.sh/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;.. |pypi| image:: &lt;a href="https://img.shields.io/pypi/v/gallery-dl?logo=pypi&amp;amp;label=PyPI"&gt;https://img.shields.io/pypi/v/gallery-dl?logo=pypi&amp;amp;label=PyPI&lt;/a&gt; :target: &lt;a href="https://pypi.org/project/gallery-dl/"&gt;https://pypi.org/project/gallery-dl/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;.. |build| image:: &lt;a href="https://github.com/mikf/gallery-dl/workflows/tests/badge.svg"&gt;https://github.com/mikf/gallery-dl/workflows/tests/badge.svg&lt;/a&gt; :target: &lt;a href="https://github.com/mikf/gallery-dl/actions"&gt;https://github.com/mikf/gallery-dl/actions&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;.. |gitter| image:: &lt;a href="https://badges.gitter.im/gallery-dl/main.svg"&gt;https://badges.gitter.im/gallery-dl/main.svg&lt;/a&gt; :target: &lt;a href="https://gitter.im/gallery-dl/main"&gt;https://gitter.im/gallery-dl/main&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;.. |discord| image:: &lt;a href="https://img.shields.io/discord/1067148002722062416?logo=discord&amp;amp;label=Discord&amp;amp;color=blue"&gt;https://img.shields.io/discord/1067148002722062416?logo=discord&amp;amp;label=Discord&amp;amp;color=blue&lt;/a&gt; :target: &lt;a href="https://discord.gg/rSzQwRvGnE"&gt;https://discord.gg/rSzQwRvGnE&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>bee-san/Ciphey</title>
      <link>https://github.com/bee-san/Ciphey</link>
      <description>&lt;p&gt;‚ö° Automatically decrypt encryptions without knowing the key or cipher, decode encodings, and crack hashes ‚ö°&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; Translations &lt;br /&gt; &lt;a href="https://github.com/Ciphey/Ciphey/tree/master/translations/de/README.md"&gt;üá©üá™ DE &lt;/a&gt; &lt;a href="https://github.com/Ciphey/Ciphey/tree/master/translations/fr/README.md"&gt;üá´üá∑ FR &lt;/a&gt; &lt;a href="https://github.com/Ciphey/Ciphey/tree/master/translations/hu/README.md"&gt;üá≠üá∫ HU &lt;/a&gt; &lt;a href="https://github.com/Ciphey/Ciphey/tree/master/translations/id/README.md"&gt;üáÆüá© ID &lt;/a&gt; &lt;a href="https://github.com/Ciphey/Ciphey/tree/master/translations/it/README.md"&gt;üáÆüáπ IT &lt;/a&gt; &lt;a href="https://github.com/Ciphey/Ciphey/tree/master/translations/nl/README.md"&gt;üá≥üá± NL &lt;/a&gt; &lt;a href="https://github.com/Ciphey/Ciphey/tree/master/translations/pt-br/README.md"&gt;üáßüá∑ PT-BR &lt;/a&gt; &lt;a href="https://github.com/Ciphey/Ciphey/tree/master/translations/ru/README.md"&gt;üá∑üá∫ RU &lt;/a&gt; &lt;a href="https://github.com/Ciphey/Ciphey/tree/master/translations/zh/README.md"&gt;üá®üá≥ ZH &lt;/a&gt; &lt;a href="https://github.com/Ciphey/Ciphey/tree/master/translations/th/README.md"&gt;üáπüá≠ TH &lt;/a&gt; &lt;br /&gt;&lt;br /&gt; ‚û°Ô∏è &lt;a href="https://github.com/Ciphey/Ciphey/wiki"&gt;Documentation&lt;/a&gt; | &lt;a href="https://discord.gg/zYTM3rZM4T"&gt;Discord&lt;/a&gt; | &lt;a href="https://github.com/Ciphey/Ciphey/wiki/Installation"&gt;Installation Guide&lt;/a&gt; ‚¨ÖÔ∏è &lt;br /&gt; &lt;img src="https://github.com/Ciphey/Ciphey/raw/master/Pictures_for_README/binoculars.png" alt="Ciphey" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://pepy.tech/badge/ciphey" /&gt; &lt;img src="https://pepy.tech/badge/ciphey/month" /&gt; &lt;a href="https://discord.gg/zYTM3rZM4T"&gt;&lt;img alt="Discord" src="https://img.shields.io/discord/754001738184392704" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/ciphey/"&gt;&lt;img src="https://img.shields.io/pypi/v/ciphey.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true" alt="Ciphey" /&gt; &lt;br /&gt; Fully automated decryption/decoding/cracking tool using natural language processing &amp;amp; artificial intelligence, along with some common sense. &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;&lt;a href="https://github.com/Ciphey/Ciphey/wiki/Installation"&gt;Installation Guide&lt;/a&gt;&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;p align="center"&gt;&lt;a href="https://pypi.org/project/ciphey"&gt;üêç Python&lt;/a&gt;&lt;/p&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;p align="center"&gt;&lt;a href="https://hub.docker.com/r/remnux/ciphey"&gt;üêã Docker (Universal)&lt;/a&gt;&lt;/p&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;p align="center"&gt;&lt;a href="https://ports.macports.org/port/ciphey/summary"&gt;üçé MacPorts (macOS)&lt;/a&gt;&lt;/p&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;p align="center"&gt;&lt;a href="https://formulae.brew.sh/formula/ciphey"&gt;üç∫ Homebrew (macOS/Linux)&lt;/a&gt;&lt;/p&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;p align="center"&gt;&lt;img src="https://github.com/Ciphey/Ciphey/raw/master/Pictures_for_README/python.png" /&gt;&lt;/p&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;p align="center"&gt;&lt;img src="https://github.com/Ciphey/Ciphey/raw/master/Pictures_for_README/docker.png" /&gt;&lt;/p&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;p align="center"&gt;&lt;img src="https://github.com/Ciphey/Ciphey/raw/master/Pictures_for_README/macports.png" /&gt;&lt;/p&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;p align="center"&gt;&lt;img src="https://github.com/Ciphey/Ciphey/raw/master/Pictures_for_README/homebrew.png" /&gt;&lt;/p&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;python3 -m pip install ciphey --upgrade&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;docker run -it --rm remnux/ciphey&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;sudo port install ciphey&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;brew install ciphey&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Linux&lt;/th&gt; 
   &lt;th&gt;Mac OS&lt;/th&gt; 
   &lt;th&gt;Windows&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/workflow/status/ciphey/ciphey/Python%20application?label=Linux" alt="GitHub Workflow Status" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/workflow/status/ciphey/ciphey/Python%20application?label=Mac%20OS" alt="GitHub Workflow Status" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/workflow/status/ciphey/ciphey/Python%20application?label=Windows" alt="GitHub Workflow Status" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h1&gt;ü§î What is this?&lt;/h1&gt; 
&lt;p&gt;Input encrypted text, get the decrypted text back.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;"What type of encryption?"&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;That's the point. You don't know, you just know it's possibly encrypted. Ciphey will figure it out for you.&lt;/p&gt; 
&lt;p&gt;Ciphey can solve most things in 3 seconds or less.&lt;/p&gt; 
&lt;p align="center" href="https://asciinema.org/a/336257"&gt; &lt;img src="https://github.com/Ciphey/Ciphey/raw/master/Pictures_for_README/index.gif" alt="Ciphey demo" /&gt; &lt;/p&gt; 
&lt;p&gt;Ciphey aims to be a tool to automate a lot of decryptions &amp;amp; decodings such as multiple base encodings, classical ciphers, hashes or more advanced cryptography.&lt;/p&gt; 
&lt;p&gt;If you don't know much about cryptography, or you want to quickly check the ciphertext before working on it yourself, Ciphey is for you.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;The technical part.&lt;/strong&gt; Ciphey uses a custom built artificial intelligence module (&lt;em&gt;AuSearch&lt;/em&gt;) with a &lt;em&gt;Cipher Detection Interface&lt;/em&gt; to approximate what something is encrypted with. And then a custom-built, customisable natural language processing &lt;em&gt;Language Checker Interface&lt;/em&gt;, which can detect when the given text becomes plaintext.&lt;/p&gt; 
&lt;p&gt;No neural networks or bloated AI here. We only use what is fast and minimal.&lt;/p&gt; 
&lt;p&gt;And that's just the tip of the iceberg. For the full technical explanation, check out our &lt;a href="https://github.com/Ciphey/Ciphey/wiki"&gt;documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;‚ú® Features&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;50+ encryptions/encodings supported&lt;/strong&gt; such as binary, Morse code and Base64. Classical ciphers like the Caesar cipher, Affine cipher and the Vigenere cipher. Along with modern encryption like repeating-key XOR and more. &lt;strong&gt;&lt;a href="https://github.com/Ciphey/Ciphey/wiki/Supported-Ciphers"&gt;For the full list, click here&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Custom Built Artificial Intelligence with Augmented Search (AuSearch) for answering the question "what encryption was used?"&lt;/strong&gt; Resulting in decryptions taking less than 3 seconds.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Custom built natural language processing module&lt;/strong&gt; Ciphey can determine whether something is plaintext or not. Whether that plaintext is JSON, a CTF flag, or English, Ciphey can get it in a couple of milliseconds.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi Language Support&lt;/strong&gt; at present, only German &amp;amp; English (with AU, UK, CAN, USA variants).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Supports encryptions and hashes&lt;/strong&gt; Which the alternatives such as CyberChef Magic do not.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/Ciphey/CipheyCore"&gt;C++ core&lt;/a&gt;&lt;/strong&gt; Blazingly fast.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;üî≠ Ciphey vs CyberChef&lt;/h1&gt; 
&lt;h2&gt;üîÅ Base64 Encoded 42 times&lt;/h2&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;th&gt;Name&lt;/th&gt; 
   &lt;th&gt;‚ö° Ciphey ‚ö° &lt;/th&gt; 
   &lt;th&gt;üê¢ CyberChef üê¢&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Gif&lt;/th&gt; 
   &lt;td&gt;&lt;img src="https://github.com/Ciphey/Ciphey/raw/master/Pictures_for_README/ciphey_gooder_cyberchef.gif" alt="The guy she tells you not to worry about" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/Ciphey/Ciphey/raw/master/Pictures_for_README/not_dying.gif" alt="You" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Time&lt;/th&gt; 
   &lt;td&gt;2 seconds&lt;/td&gt; 
   &lt;td&gt;6 seconds&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Setup&lt;/th&gt; 
   &lt;td&gt;
    &lt;ul&gt;
     &lt;li&gt;Run ciphey on the file&lt;/li&gt;
    &lt;/ul&gt;&lt;/td&gt; 
   &lt;td&gt;
    &lt;ul&gt;
     &lt;li&gt;Set the regex param to "{"&lt;/li&gt;
     &lt;li&gt;You need to know how many times to recurse&lt;/li&gt;
     &lt;li&gt;You need to know it's Base64 all the way down&lt;/li&gt;
     &lt;li&gt;You need to load CyberChef (it's a bloated JS app)&lt;/li&gt;
     &lt;li&gt;Know enough about CyberChef to create this pipeline&lt;/li&gt;
     &lt;li&gt;Invert the match&lt;/li&gt;
    &lt;/ul&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;p&gt;&lt;sub&gt;&lt;b&gt;Note&lt;/b&gt; The gifs may load at different times, so one may appear significantly faster than another.&lt;/sub&gt;&lt;br /&gt; &lt;sub&gt;&lt;b&gt;A note on magic &lt;/b&gt;CyberChef's most similar feature to Ciphey is Magic. Magic fails instantly on this input and crashes. The only way we could force CyberChef to compete was to manually define it.&lt;/sub&gt;&lt;/p&gt; 
&lt;p&gt;We also tested CyberChef and Ciphey with a &lt;strong&gt;6gb file&lt;/strong&gt;. Ciphey cracked it in &lt;strong&gt;5 minutes and 54 seconds&lt;/strong&gt;. CyberChef crashed before it even started.&lt;/p&gt; 
&lt;h2&gt;üìä Ciphey vs Katana vs CyberChef Magic&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;strong&gt;Name&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;‚ö° Ciphey ‚ö°&lt;/th&gt; 
   &lt;th&gt;üó°Ô∏è Katana üó°Ô∏è&lt;/th&gt; 
   &lt;th&gt;üê¢ CyberChef Magic üê¢&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Advanced Language Checker&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Supports Encryptions&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Releases named after Dystopian themes üåÉ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Supports hashes&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Easy to set up&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Can guess what something is encrypted with&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Created for hackers by hackers&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;üé¨ Getting Started&lt;/h1&gt; 
&lt;p&gt;If you're having trouble with installing Ciphey, &lt;a href="https://github.com/Ciphey/Ciphey/wiki/Common-Issues-&amp;amp;-Their-Solutions"&gt;read this.&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;‚ÄºÔ∏è Important Links (Docs, Installation guide, Discord Support)&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Installation Guide&lt;/th&gt; 
   &lt;th&gt;Documentation&lt;/th&gt; 
   &lt;th&gt;Discord&lt;/th&gt; 
   &lt;th&gt;Docker Image (from REMnux)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üìñ &lt;a href="https://github.com/Ciphey/Ciphey/wiki/Installation"&gt;Installation Guide&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;üìö &lt;a href="https://github.com/Ciphey/Ciphey/wiki"&gt;Documentation&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ü¶ú &lt;a href="https://discord.gg/zYTM3rZM4T"&gt;Discord&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;üêã &lt;a href="https://docs.remnux.org/run-tools-in-containers/remnux-containers#ciphey"&gt;Docker Documentation&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;üèÉ‚Äç‚ôÄÔ∏èRunning Ciphey&lt;/h2&gt; 
&lt;p&gt;There are 3 ways to run Ciphey.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;File Input &lt;code&gt;ciphey -f encrypted.txt&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Unqualified input &lt;code&gt;ciphey -- "Encrypted input"&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Normal way &lt;code&gt;ciphey -t "Encrypted input"&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;img src="https://github.com/Ciphey/Ciphey/raw/master/Pictures_for_README/3ways.gif" alt="Gif showing 3 ways to run Ciphey" /&gt;&lt;/p&gt; 
&lt;p&gt;To get rid of the progress bars, probability table, and all the noise use the quiet mode.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;ciphey -t "encrypted text here" -q&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;For a full list of arguments, run &lt;code&gt;ciphey --help&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;‚öóÔ∏è Importing Ciphey&lt;/h3&gt; 
&lt;p&gt;You can import Ciphey's main and use it in your own programs and code. &lt;code&gt;from Ciphey.__main__ import main&lt;/code&gt;&lt;/p&gt; 
&lt;h1&gt;üé™ Contributors&lt;/h1&gt; 
&lt;p&gt;Ciphey was invented by &lt;a href="https://github.com/bee-san"&gt;Bee&lt;/a&gt; in 2008, and revived in 2019. Ciphey wouldn't be where it was today without &lt;a href="https://github.com/Cyclic3"&gt;Cyclic3&lt;/a&gt; - president of UoL's Cyber Security Society.&lt;/p&gt; 
&lt;p&gt;Ciphey was revived &amp;amp; recreated by the &lt;a href="https://www.cybersoc.cf/"&gt;Cyber Security Society&lt;/a&gt; for use in CTFs. If you're ever in Liverpool, consider giving a talk or sponsoring our events. Email us at &lt;code&gt;cybersecurity@society.liverpoolguild.org&lt;/code&gt; to find out more ü§†&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Major Credit&lt;/strong&gt; to George H for working out how we could use proper algorithms to speed up the search process. &lt;strong&gt;Special thanks&lt;/strong&gt; to &lt;a href="https://www.facebook.com/varghalladesign"&gt;varghalladesign&lt;/a&gt; for designing the logo. Check out their other design work!&lt;/p&gt; 
&lt;h2&gt;üêï‚Äçü¶∫ &lt;a href="https://github.com/Ciphey/Ciphey/wiki/Contributing"&gt;Contributing&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;Don't be afraid to contribute! We have many, many things you can do to help out. Each of them labelled and easily explained with examples. If you're trying to contribute but stuck, tag @bee-san ‚ú®&lt;/p&gt; 
&lt;p&gt;Alternatively, join the Discord group and send a message there (link in &lt;a href="https://github.com/Ciphey/Ciphey/wiki/Contributing"&gt;contrib file&lt;/a&gt;) or at the top of this README as a badge.&lt;/p&gt; 
&lt;p&gt;Please read the &lt;a href="https://github.com/Ciphey/Ciphey/wiki/Contributing"&gt;contributing file&lt;/a&gt; for exact details on how to contribute ‚ú®&lt;/p&gt; 
&lt;p&gt;By doing so, you'll get your name added to the README below and get to be apart of an ever-growing project! &lt;a href="https://starchart.cc/Ciphey/Ciphey"&gt;&lt;img src="https://starchart.cc/Ciphey/Ciphey.svg?sanitize=true" alt="Stargazers over time" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üí∞ Financial Contributors&lt;/h2&gt; 
&lt;p&gt;The contributions will be used to fund not only the future of Ciphey and its authors, but also Cyber Security Society at the University of Liverpool.&lt;/p&gt; 
&lt;p&gt;GitHub doesn't support "sponsor this project and we'll evenly distribute the money", so pick a link and we'll sort it out on our end ü•∞&lt;/p&gt; 
&lt;h2&gt;‚ú® Contributors&lt;/h2&gt; 
&lt;p&gt;Thanks goes to these wonderful people (&lt;a href="https://allcontributors.org/docs/en/emoji-key"&gt;emoji key&lt;/a&gt;):&lt;/p&gt; 
&lt;!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section --&gt; 
&lt;!-- prettier-ignore-start --&gt; 
&lt;!-- markdownlint-disable --&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/Cyclic3"&gt;&lt;img src="https://avatars1.githubusercontent.com/u/15613874?v=4?s=100" width="100px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;cyclic3&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/bee-san/Ciphey/master/#design-cyclic3" title="Design"&gt;üé®&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/bee-san/Ciphey/master/#maintenance-cyclic3" title="Maintenance"&gt;üöß&lt;/a&gt; &lt;a href="https://github.com/Ciphey/Ciphey/commits?author=cyclic3" title="Code"&gt;üíª&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/bee-san/Ciphey/master/#ideas-cyclic3" title="Ideas, Planning, &amp;amp; Feedback"&gt;ü§î&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://skerritt.blog"&gt;&lt;img src="https://avatars3.githubusercontent.com/u/10378052?v=4?s=100" width="100px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Brandon&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/bee-san/Ciphey/master/#design-brandonskerritt" title="Design"&gt;üé®&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/bee-san/Ciphey/master/#maintenance-brandonskerritt" title="Maintenance"&gt;üöß&lt;/a&gt; &lt;a href="https://github.com/Ciphey/Ciphey/commits?author=brandonskerritt" title="Code"&gt;üíª&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/bee-san/Ciphey/master/#ideas-brandonskerritt" title="Ideas, Planning, &amp;amp; Feedback"&gt;ü§î&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/michalani"&gt;&lt;img src="https://avatars0.githubusercontent.com/u/27767884?v=4?s=100" width="100px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;michalani&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/Ciphey/Ciphey/commits?author=michalani" title="Code"&gt;üíª&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/ashb07"&gt;&lt;img src="https://avatars2.githubusercontent.com/u/24845568?v=4?s=100" width="100px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;ashb07&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/Ciphey/Ciphey/commits?author=ashb07" title="Code"&gt;üíª&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/TheAlcanian"&gt;&lt;img src="https://avatars3.githubusercontent.com/u/22127191?v=4?s=100" width="100px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Shardion&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/Ciphey/Ciphey/issues?q=author%3ATheAlcanian" title="Bug reports"&gt;üêõ&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/Bryzizzle"&gt;&lt;img src="https://avatars0.githubusercontent.com/u/57810197?v=4?s=100" width="100px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Bryan&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/bee-san/Ciphey/master/#translation-Bryzizzle" title="Translation"&gt;üåç&lt;/a&gt; &lt;a href="https://github.com/Ciphey/Ciphey/commits?author=Bryzizzle" title="Documentation"&gt;üìñ&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://lukasgabriel.net"&gt;&lt;img src="https://avatars0.githubusercontent.com/u/52338810?v=4?s=100" width="100px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Lukas Gabriel&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/Ciphey/Ciphey/commits?author=lukasgabriel" title="Code"&gt;üíª&lt;/a&gt; &lt;a href="https://github.com/Ciphey/Ciphey/issues?q=author%3Alukasgabriel" title="Bug reports"&gt;üêõ&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/bee-san/Ciphey/master/#translation-lukasgabriel" title="Translation"&gt;üåç&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/bee-san/Ciphey/master/#ideas-lukasgabriel" title="Ideas, Planning, &amp;amp; Feedback"&gt;ü§î&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/DarshanBhoi"&gt;&lt;img src="https://avatars2.githubusercontent.com/u/70128281?v=4?s=100" width="100px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Darshan&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/Ciphey/Ciphey/issues?q=author%3ADarshanBhoi" title="Bug reports"&gt;üêõ&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/SkeletalDemise"&gt;&lt;img src="https://avatars1.githubusercontent.com/u/29117662?v=4?s=100" width="100px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;SkeletalDemise&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/Ciphey/Ciphey/commits?author=SkeletalDemise" title="Code"&gt;üíª&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://www.patreon.com/cclauss"&gt;&lt;img src="https://avatars3.githubusercontent.com/u/3709715?v=4?s=100" width="100px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Christian Clauss&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/Ciphey/Ciphey/commits?author=cclauss" title="Code"&gt;üíª&lt;/a&gt; &lt;a href="https://github.com/Ciphey/Ciphey/issues?q=author%3Acclauss" title="Bug reports"&gt;üêõ&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="http://machinexa.xss.ht"&gt;&lt;img src="https://avatars1.githubusercontent.com/u/60662297?v=4?s=100" width="100px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Machinexa2&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/bee-san/Ciphey/master/#content-machinexa2" title="Content"&gt;üñã&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/anantverma275"&gt;&lt;img src="https://avatars1.githubusercontent.com/u/18184503?v=4?s=100" width="100px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Anant Verma&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/Ciphey/Ciphey/commits?author=anantverma275" title="Code"&gt;üíª&lt;/a&gt; &lt;a href="https://github.com/Ciphey/Ciphey/issues?q=author%3Aanantverma275" title="Bug reports"&gt;üêõ&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/XVXTOR"&gt;&lt;img src="https://avatars1.githubusercontent.com/u/40268197?v=4?s=100" width="100px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;XVXTOR&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/Ciphey/Ciphey/commits?author=XVXTOR" title="Documentation"&gt;üìñ&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/Itamikame"&gt;&lt;img src="https://avatars2.githubusercontent.com/u/59034423?v=4?s=100" width="100px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Itamikame&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/Ciphey/Ciphey/commits?author=Itamikame" title="Code"&gt;üíª&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/MikeMerz"&gt;&lt;img src="https://avatars3.githubusercontent.com/u/50526795?v=4?s=100" width="100px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;MikeMerz&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/Ciphey/Ciphey/commits?author=MikeMerz" title="Code"&gt;üíª&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/jacobggman"&gt;&lt;img src="https://avatars2.githubusercontent.com/u/30216976?v=4?s=100" width="100px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Jacob Galam&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/Ciphey/Ciphey/commits?author=jacobggman" title="Code"&gt;üíª&lt;/a&gt; &lt;a href="https://github.com/Ciphey/Ciphey/issues?q=author%3Ajacobggman" title="Bug reports"&gt;üêõ&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://tuxthexplorer.github.io/"&gt;&lt;img src="https://avatars1.githubusercontent.com/u/37508897?v=4?s=100" width="100px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;TuxTheXplorer&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/bee-san/Ciphey/master/#translation-TuxTheXplorer" title="Translation"&gt;üåç&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/Itamai"&gt;&lt;img src="https://avatars3.githubusercontent.com/u/53093696?v=4?s=100" width="100px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Itamai&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/Ciphey/Ciphey/commits?author=Itamai" title="Code"&gt;üíª&lt;/a&gt; &lt;a href="https://github.com/Ciphey/Ciphey/issues?q=author%3AItamai" title="Bug reports"&gt;üêõ&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/Termack"&gt;&lt;img src="https://avatars2.githubusercontent.com/u/26333901?v=4?s=100" width="100px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Filipe&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/bee-san/Ciphey/master/#translation-Termack" title="Translation"&gt;üåç&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/malathit"&gt;&lt;img src="https://avatars0.githubusercontent.com/u/2684148?v=4?s=100" width="100px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Malathi&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/Ciphey/Ciphey/commits?author=malathit" title="Code"&gt;üíª&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://hexchaos.xyz/"&gt;&lt;img src="https://avatars1.githubusercontent.com/u/8947820?v=4?s=100" width="100px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Jack&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/bee-san/Ciphey/master/#translation-HexChaos" title="Translation"&gt;üåç&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/yafkari"&gt;&lt;img src="https://avatars3.githubusercontent.com/u/41365655?v=4?s=100" width="100px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Younes&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/bee-san/Ciphey/master/#translation-yafkari" title="Translation"&gt;üåç&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://gitlab.com/Marnick39"&gt;&lt;img src="https://avatars2.githubusercontent.com/u/17315511?v=4?s=100" width="100px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Marnick Vandecauter&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/bee-san/Ciphey/master/#translation-Marnick39" title="Translation"&gt;üåç&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/mav8557"&gt;&lt;img src="https://avatars0.githubusercontent.com/u/47306745?v=4?s=100" width="100px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Michael V&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/Ciphey/Ciphey/commits?author=mav8557" title="Code"&gt;üíª&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/chuinzer"&gt;&lt;img src="https://avatars2.githubusercontent.com/u/64257785?v=4?s=100" width="100px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;chuinzer&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/bee-san/Ciphey/master/#translation-chuinzer" title="Translation"&gt;üåç&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/blackcat-917"&gt;&lt;img src="https://avatars1.githubusercontent.com/u/53786619?v=4?s=100" width="100px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;blackcat-917&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/bee-san/Ciphey/master/#translation-blackcat-917" title="Translation"&gt;üåç&lt;/a&gt; &lt;a href="https://github.com/Ciphey/Ciphey/commits?author=blackcat-917" title="Documentation"&gt;üìñ&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/Ozzyz"&gt;&lt;img src="https://avatars3.githubusercontent.com/u/6113447?v=4?s=100" width="100px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;√Ösmund Brekke&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/Ciphey/Ciphey/commits?author=Ozzyz" title="Code"&gt;üíª&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/sashreek1"&gt;&lt;img src="https://avatars1.githubusercontent.com/u/45600974?v=4?s=100" width="100px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Sashreek Shankar&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/Ciphey/Ciphey/commits?author=sashreek1" title="Code"&gt;üíª&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/cryptobadger"&gt;&lt;img src="https://avatars2.githubusercontent.com/u/26308101?v=4?s=100" width="100px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;cryptobadger&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/Ciphey/Ciphey/commits?author=cryptobadger" title="Code"&gt;üíª&lt;/a&gt; &lt;a href="https://github.com/Ciphey/Ciphey/issues?q=author%3Acryptobadger" title="Bug reports"&gt;üêõ&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/e1fy"&gt;&lt;img src="https://avatars3.githubusercontent.com/u/61194758?v=4?s=100" width="100px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;elf&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/Ciphey/Ciphey/commits?author=e1fy" title="Code"&gt;üíª&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/rogercyyu"&gt;&lt;img src="https://avatars0.githubusercontent.com/u/45835736?v=4?s=100" width="100px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Roger Yu&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/Ciphey/Ciphey/commits?author=rogercyyu" title="Code"&gt;üíª&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/JesseEmond"&gt;&lt;img src="https://avatars.githubusercontent.com/u/1843555?v=4?s=100" width="100px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;dysleixa&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/Ciphey/Ciphey/commits?author=JesseEmond" title="Code"&gt;üíª&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="http://mohzulfikar.me"&gt;&lt;img src="https://avatars.githubusercontent.com/u/48849323?v=4?s=100" width="100px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Mohammad Zulfikar&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/Ciphey/Ciphey/commits?author=mohzulfikar" title="Documentation"&gt;üìñ&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/AABur"&gt;&lt;img src="https://avatars.githubusercontent.com/u/41373199?v=4?s=100" width="100px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Alexander Burchenko&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/bee-san/Ciphey/master/#translation-AABur" title="Translation"&gt;üåç&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;!-- markdownlint-restore --&gt; 
&lt;!-- prettier-ignore-end --&gt; 
&lt;!-- ALL-CONTRIBUTORS-LIST:END --&gt; 
&lt;p&gt;This project follows the &lt;a href="https://github.com/all-contributors/all-contributors"&gt;all-contributors&lt;/a&gt; specification. Contributions of any kind welcome!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Shubhamsaboo/awesome-llm-apps</title>
      <link>https://github.com/Shubhamsaboo/awesome-llm-apps</link>
      <description>&lt;p&gt;Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="http://www.theunwindai.com"&gt; &lt;img src="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/unwind_black.png" width="900px" alt="Unwind AI" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://www.linkedin.com/in/shubhamsaboo/"&gt; &lt;img src="https://img.shields.io/badge/-Follow%20Shubham%20Saboo-blue?logo=linkedin&amp;amp;style=flat-square" alt="LinkedIn" /&gt; &lt;/a&gt; &lt;a href="https://twitter.com/Saboo_Shubham_"&gt; &lt;img src="https://img.shields.io/twitter/follow/Shubham_Saboo" alt="Twitter" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; 
 &lt;!-- Keep these links. Translations will automatically update with the README. --&gt; &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=de"&gt;Deutsch&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=es"&gt;Espa√±ol&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=fr"&gt;fran√ßais&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ja"&gt;Êó•Êú¨Ë™û&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ko"&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=pt"&gt;Portugu√™s&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ru"&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=zh"&gt;‰∏≠Êñá&lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h1&gt;üåü Awesome LLM Apps&lt;/h1&gt; 
&lt;p&gt;A curated collection of &lt;strong&gt;Awesome LLM apps built with RAG, AI Agents, Multi-agent Teams, MCP, Voice Agents, and more.&lt;/strong&gt; This repository features LLM apps that use models from &lt;img src="https://cdn.simpleicons.org/openai" alt="openai logo" width="25" height="15" /&gt;&lt;strong&gt;OpenAI&lt;/strong&gt; , &lt;img src="https://cdn.simpleicons.org/anthropic" alt="anthropic logo" width="25" height="15" /&gt;&lt;strong&gt;Anthropic&lt;/strong&gt;, &lt;img src="https://cdn.simpleicons.org/googlegemini" alt="google logo" width="25" height="18" /&gt;&lt;strong&gt;Google&lt;/strong&gt;, &lt;img src="https://cdn.simpleicons.org/x" alt="X logo" width="25" height="15" /&gt;&lt;strong&gt;xAI&lt;/strong&gt; and open-source models like &lt;img src="https://cdn.simpleicons.org/alibabacloud" alt="alibaba logo" width="25" height="15" /&gt;&lt;strong&gt;Qwen&lt;/strong&gt; or &lt;img src="https://cdn.simpleicons.org/meta" alt="meta logo" width="25" height="15" /&gt;&lt;strong&gt;Llama&lt;/strong&gt; that you can run locally on your computer.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/9876" target="_blank"&gt; &lt;img src="https://trendshift.io/api/badge/repositories/9876" alt="Shubhamsaboo%2Fawesome-llm-apps | Trendshift" style="width: 250px; height: 55px;" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;ü§î Why Awesome LLM Apps?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;üí° Discover practical and creative ways LLMs can be applied across different domains, from code repositories to email inboxes and more.&lt;/li&gt; 
 &lt;li&gt;üî• Explore apps that combine LLMs from OpenAI, Anthropic, Gemini, and open-source alternatives with AI Agents, Agent Teams, MCP &amp;amp; RAG.&lt;/li&gt; 
 &lt;li&gt;üéì Learn from well-documented projects and contribute to the growing open-source ecosystem of LLM-powered applications.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üôè Thanks to our sponsors&lt;/h2&gt; 
&lt;table align="center" cellpadding="16" cellspacing="12"&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td align="center"&gt; &lt;a href="https://tsdb.co/shubham-gh" target="_blank" rel="noopener" title="Tiger Data"&gt; &lt;img src="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/sponsors/tigerdata.png" alt="Tiger Data" width="500" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;a href="https://tsdb.co/shubham-gh" target="_blank" rel="noopener" style="text-decoration: none; color: #333; font-weight: bold; font-size: 18px;"&gt; Tiger Data MCP &lt;/a&gt; &lt;/td&gt; 
   &lt;td align="center"&gt; &lt;a href="https://github.com/speechmatics/speechmatics-academy" target="_blank" rel="noopener" title="Speechmatics"&gt; &lt;img src="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/sponsors/speechmatics.png" alt="Speechmatics" width="500" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;a href="https://github.com/speechmatics/speechmatics-academy" target="_blank" rel="noopener" style="text-decoration: none; color: #333; font-weight: bold; font-size: 18px;"&gt; Speechmatics &lt;/a&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt; &lt;a href="https://okara.ai/?utm_source=oss&amp;amp;utm_medium=sponsorship&amp;amp;utm_campaign=awesome-llm-apps" title="Okara"&gt; &lt;img src="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/sponsors/okara.png" alt="Okara" width="500" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;a href="https://okara.ai/?utm_source=oss&amp;amp;utm_medium=sponsorship&amp;amp;utm_campaign=awesome-llm-apps" style="text-decoration: none; color: #333; font-weight: bold; font-size: 18px;"&gt; Okara AI &lt;/a&gt; &lt;/td&gt; 
   &lt;td align="center"&gt; &lt;a href="https://sponsorunwindai.com/" title="Become a Sponsor"&gt; &lt;img src="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/sponsor_awesome_llm_apps.png" alt="Become a Sponsor" width="500" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;a href="https://sponsorunwindai.com/" style="text-decoration: none; color: #333; font-weight: bold; font-size: 18px;"&gt; Become a Sponsor &lt;/a&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;üìÇ Featured AI Projects&lt;/h2&gt; 
&lt;h3&gt;AI Agents&lt;/h3&gt; 
&lt;h3&gt;üå± Starter AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_blog_to_podcast_agent/"&gt;üéôÔ∏è AI Blog to Podcast Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_breakup_recovery_agent/"&gt;‚ù§Ô∏è‚Äçü©π AI Breakup Recovery Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_data_analysis_agent/"&gt;üìä AI Data Analysis Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_medical_imaging_agent/"&gt;ü©ª AI Medical Imaging Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_meme_generator_agent_browseruse/"&gt;üòÇ AI Meme Generator Agent (Browser)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_music_generator_agent/"&gt;üéµ AI Music Generator Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_travel_agent/"&gt;üõ´ AI Travel Agent (Local &amp;amp; Cloud)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/gemini_multimodal_agent_demo/"&gt;‚ú® Gemini Multimodal Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/mixture_of_agents/"&gt;üîÑ Mixture of Agents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/xai_finance_agent/"&gt;üìä xAI Finance Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/opeani_research_agent/"&gt;üîç OpenAI Research Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/web_scrapping_ai_agent/"&gt;üï∏Ô∏è Web Scraping AI Agent (Local &amp;amp; Cloud SDK)&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üöÄ Advanced AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_home_renovation_agent"&gt;üèöÔ∏è üçå AI Home Renovation Agent with Nano Banana Pro&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_deep_research_agent/"&gt;üîç AI Deep Research Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_vc_due_diligence_agent_team"&gt;üìä AI VC Due Diligence Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/research_agent_gemini_interaction_api"&gt;üî¨ AI Research Planner &amp;amp; Executor (Google Interactions API)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_consultant_agent"&gt;ü§ù AI Consultant Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_system_architect_r1/"&gt;üèóÔ∏è AI System Architect Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_financial_coach_agent/"&gt;üí∞ AI Financial Coach Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_movie_production_agent/"&gt;üé¨ AI Movie Production Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_investment_agent/"&gt;üìà AI Investment Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_health_fitness_agent/"&gt;üèãÔ∏è‚Äç‚ôÇÔ∏è AI Health &amp;amp; Fitness Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/product_launch_intelligence_agent"&gt;üöÄ AI Product Launch Intelligence Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_journalist_agent/"&gt;üóûÔ∏è AI Journalist Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_mental_wellbeing_agent/"&gt;üß† AI Mental Wellbeing Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_meeting_agent/"&gt;üìë AI Meeting Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_Self-Evolving_agent/"&gt;üß¨ AI Self-Evolving Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_sales_intelligence_agent_team"&gt;üë®üèª‚Äçüíº AI Sales Intelligence Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_news_and_podcast_agents/"&gt;üéß AI Social Media News and Podcast Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/accomplish-ai/openwork"&gt;üåê Openwork - Open Browser Automation Agent&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üéÆ Autonomous Game Playing Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/autonomous_game_playing_agent_apps/ai_3dpygame_r1/"&gt;üéÆ AI 3D Pygame Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/autonomous_game_playing_agent_apps/ai_chess_agent/"&gt;‚ôú AI Chess Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/autonomous_game_playing_agent_apps/ai_tic_tac_toe_agent/"&gt;üé≤ AI Tic-Tac-Toe Agent&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ü§ù Multi-agent Teams&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_competitor_intelligence_agent_team/"&gt;üß≤ AI Competitor Intelligence Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_finance_agent_team/"&gt;üí≤ AI Finance Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_game_design_agent_team/"&gt;üé® AI Game Design Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_legal_agent_team/"&gt;üë®‚Äç‚öñÔ∏è AI Legal Agent Team (Cloud &amp;amp; Local)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_recruitment_agent_team/"&gt;üíº AI Recruitment Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_real_estate_agent_team"&gt;üè† AI Real Estate Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_services_agency/"&gt;üë®‚Äçüíº AI Services Agency (CrewAI)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_teaching_agent_team/"&gt;üë®‚Äçüè´ AI Teaching Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_coding_agent_team/"&gt;üíª Multimodal Coding Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_design_agent_team/"&gt;‚ú® Multimodal Design Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_uiux_feedback_agent_team/"&gt;üé® üçå Multimodal UI/UX Feedback Agent Team with Nano Banana&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_travel_planner_agent_team/"&gt;üåè AI Travel Planner Agent Team&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üó£Ô∏è Voice AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/voice_ai_agents/ai_audio_tour_agent/"&gt;üó£Ô∏è AI Audio Tour Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/voice_ai_agents/customer_support_voice_agent/"&gt;üìû Customer Support Voice Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/voice_ai_agents/voice_rag_openaisdk/"&gt;üîä Voice RAG Agent (OpenAI SDK)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/akshayaggarwal99/jarvis-ai-assistant"&gt;üéôÔ∏è OpenSource Voice Dictation Agent (like Wispr Flow&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;img src="https://cdn.simpleicons.org/modelcontextprotocol" alt="mcp logo" width="25" height="20" /&gt; MCP AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/mcp_ai_agents/browser_mcp_agent/"&gt;‚ôæÔ∏è Browser MCP Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/mcp_ai_agents/github_mcp_agent/"&gt;üêô GitHub MCP Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/mcp_ai_agents/notion_mcp_agent"&gt;üìë Notion MCP Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/mcp_ai_agents/ai_travel_planner_mcp_agent_team"&gt;üåç AI Travel Planner MCP Agent&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üìÄ RAG (Retrieval Augmented Generation)&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/agentic_rag_embedding_gemma"&gt;üî• Agentic RAG with Embedding Gemma&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/agentic_rag_with_reasoning/"&gt;üßê Agentic RAG with Reasoning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/ai_blog_search/"&gt;üì∞ AI Blog Search (RAG)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/autonomous_rag/"&gt;üîç Autonomous RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/contextualai_rag_agent/"&gt;üîÑ Contextual AI RAG Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/corrective_rag/"&gt;üîÑ Corrective RAG (CRAG)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/deepseek_local_rag_agent/"&gt;üêã Deepseek Local RAG Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/gemini_agentic_rag/"&gt;ü§î Gemini Agentic RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/hybrid_search_rag/"&gt;üëÄ Hybrid Search RAG (Cloud)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/llama3.1_local_rag/"&gt;üîÑ Llama 3.1 Local RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/local_hybrid_search_rag/"&gt;üñ•Ô∏è Local Hybrid Search RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/local_rag_agent/"&gt;ü¶ô Local RAG Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/rag-as-a-service/"&gt;üß© RAG-as-a-Service&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/rag_agent_cohere/"&gt;‚ú® RAG Agent with Cohere&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/rag_chain/"&gt;‚õìÔ∏è Basic RAG Chain&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/rag_database_routing/"&gt;üì† RAG with Database Routing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/vision_rag/"&gt;üñºÔ∏è Vision RAG&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üíæ LLM Apps with Memory Tutorials&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/ai_arxiv_agent_memory/"&gt;üíæ AI ArXiv Agent with Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/ai_travel_agent_memory/"&gt;üõ©Ô∏è AI Travel Agent with Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/llama3_stateful_chat/"&gt;üí¨ Llama3 Stateful Chat&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/llm_app_personalized_memory/"&gt;üìù LLM App with Personalized Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/local_chatgpt_with_memory/"&gt;üóÑÔ∏è Local ChatGPT Clone with Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/multi_llm_memory/"&gt;üß† Multi-LLM Application with Shared Memory&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üí¨ Chat with X Tutorials&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_github/"&gt;üí¨ Chat with GitHub (GPT &amp;amp; Llama3)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_gmail/"&gt;üì® Chat with Gmail&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_pdf/"&gt;üìÑ Chat with PDF (GPT &amp;amp; Llama3)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_research_papers/"&gt;üìö Chat with Research Papers (ArXiv) (GPT &amp;amp; Llama3)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_substack/"&gt;üìù Chat with Substack&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_youtube_videos/"&gt;üìΩÔ∏è Chat with YouTube Videos&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üéØ LLM Optimization Tools&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_optimization_tools/toonify_token_optimization/"&gt;üéØ Toonify Token Optimization&lt;/a&gt; - Reduce LLM API costs by 30-60% using TOON format&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üîß LLM Fine-tuning Tutorials&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;img src="https://cdn.simpleicons.org/google" alt="google logo" width="20" height="15" /&gt; &lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_finetuning_tutorials/gemma3_finetuning/"&gt;Gemma 3 Fine-tuning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;img src="https://cdn.simpleicons.org/meta" alt="meta logo" width="25" height="15" /&gt; &lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_finetuning_tutorials/llama3.2_finetuning/"&gt;Llama 3.2 Fine-tuning&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üßë‚Äçüè´ AI Agent Framework Crash Course&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://cdn.simpleicons.org/google" alt="google logo" width="25" height="15" /&gt; &lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/ai_agent_framework_crash_course/google_adk_crash_course/"&gt;Google ADK Crash Course&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Starter agent; model‚Äëagnostic (OpenAI, Claude)&lt;/li&gt; 
 &lt;li&gt;Structured outputs (Pydantic)&lt;/li&gt; 
 &lt;li&gt;Tools: built‚Äëin, function, third‚Äëparty, MCP tools&lt;/li&gt; 
 &lt;li&gt;Memory; callbacks; Plugins&lt;/li&gt; 
 &lt;li&gt;Simple multi‚Äëagent; Multi‚Äëagent patterns&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://cdn.simpleicons.org/openai" alt="openai logo" width="25" height="15" /&gt; &lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/ai_agent_framework_crash_course/openai_sdk_crash_course/"&gt;OpenAI Agents SDK Crash Course&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Starter agent; function calling; structured outputs&lt;/li&gt; 
 &lt;li&gt;Tools: built‚Äëin, function, third‚Äëparty integrations&lt;/li&gt; 
 &lt;li&gt;Memory; callbacks; evaluation&lt;/li&gt; 
 &lt;li&gt;Multi‚Äëagent patterns; agent handoffs&lt;/li&gt; 
 &lt;li&gt;Swarm orchestration; routing logic&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üöÄ Getting Started&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Clone the repository&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/Shubhamsaboo/awesome-llm-apps.git 
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Navigate to the desired project directory&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cd awesome-llm-apps/starter_ai_agents/ai_travel_agent
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install the required dependencies&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Follow the project-specific instructions&lt;/strong&gt; in each project's &lt;code&gt;README.md&lt;/code&gt; file to set up and run the app.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;&lt;img src="https://cdn.simpleicons.org/github" alt="github logo" width="25" height="20" /&gt; Thank You, Community, for the Support! üôè&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#Shubhamsaboo/awesome-llm-apps&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=Shubhamsaboo/awesome-llm-apps&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;üåü &lt;strong&gt;Don‚Äôt miss out on future updates! Star the repo now and be the first to know about new and exciting LLM apps with RAG and AI Agents.&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>VectifyAI/PageIndex</title>
      <link>https://github.com/VectifyAI/PageIndex</link>
      <description>&lt;p&gt;üìë PageIndex: Document Index for Vectorless, Reasoning-based RAG&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;a href="https://vectify.ai/pageindex" target="_blank"&gt; &lt;img src="https://github.com/user-attachments/assets/46201e72-675b-43bc-bfbd-081cc6b65a1d" alt="PageIndex Banner" /&gt; &lt;/a&gt; 
 &lt;br /&gt; 
 &lt;br /&gt; 
 &lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/14736" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/14736" alt="VectifyAI%2FPageIndex | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;h1&gt;PageIndex: Vectorless, Reasoning-based RAG&lt;/h1&gt; 
 &lt;p align="center"&gt;&lt;b&gt;Reasoning-based RAG&amp;nbsp; ‚ó¶ &amp;nbsp;No Vector DB&amp;nbsp; ‚ó¶ &amp;nbsp;No Chunking&amp;nbsp; ‚ó¶ &amp;nbsp;Human-like Retrieval&lt;/b&gt;&lt;/p&gt; 
 &lt;h4 align="center"&gt; &lt;a href="https://vectify.ai"&gt;üè† Homepage&lt;/a&gt;&amp;nbsp; ‚Ä¢ &amp;nbsp; &lt;a href="https://chat.pageindex.ai"&gt;üñ•Ô∏è Chat Platform&lt;/a&gt;&amp;nbsp; ‚Ä¢ &amp;nbsp; &lt;a href="https://pageindex.ai/mcp"&gt;üîå MCP&lt;/a&gt;&amp;nbsp; ‚Ä¢ &amp;nbsp; &lt;a href="https://docs.pageindex.ai"&gt;üìö Docs&lt;/a&gt;&amp;nbsp; ‚Ä¢ &amp;nbsp; &lt;a href="https://discord.com/invite/VuXuf29EUj"&gt;üí¨ Discord&lt;/a&gt;&amp;nbsp; ‚Ä¢ &amp;nbsp; &lt;a href="https://ii2abc2jejf.typeform.com/to/tK3AXl8T"&gt;‚úâÔ∏è Contact&lt;/a&gt;&amp;nbsp; &lt;/h4&gt; 
&lt;/div&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;h3&gt;üì¢ Latest Updates&lt;/h3&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;üî• Releases:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://chat.pageindex.ai"&gt;&lt;strong&gt;PageIndex Chat&lt;/strong&gt;&lt;/a&gt;: The first human-like document-analysis agent &lt;a href="https://chat.pageindex.ai"&gt;platform&lt;/a&gt; built for professional long documents. Can also be integrated via &lt;a href="https://pageindex.ai/mcp"&gt;MCP&lt;/a&gt; or &lt;a href="https://docs.pageindex.ai/quickstart"&gt;API&lt;/a&gt; (beta).&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;!-- - [**PageIndex Chat API**](https://docs.pageindex.ai/quickstart): An API that brings PageIndex's advanced long-document intelligence directly into your applications and workflows. --&gt; 
 &lt;!-- - [PageIndex MCP](https://pageindex.ai/mcp): Bring PageIndex into Claude, Cursor, or any MCP-enabled agent. Chat with long PDFs in a reasoning-based, human-like way. --&gt; 
 &lt;p&gt;&lt;strong&gt;üìù Articles:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://pageindex.ai/blog/pageindex-intro"&gt;&lt;strong&gt;PageIndex Framework&lt;/strong&gt;&lt;/a&gt;: Introduces the PageIndex framework ‚Äî an &lt;em&gt;agentic, in-context&lt;/em&gt; &lt;em&gt;tree index&lt;/em&gt; that enables LLMs to perform &lt;em&gt;reasoning-based&lt;/em&gt;, &lt;em&gt;human-like retrieval&lt;/em&gt; over long documents, without vector DB or chunking.&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;!-- - [Do We Still Need OCR?](https://pageindex.ai/blog/do-we-need-ocr): Explores how vision-based, reasoning-native RAG challenges the traditional OCR pipeline, and why the future of document AI might be *vectorless* and *vision-based*. --&gt; 
 &lt;p&gt;&lt;strong&gt;üß™ Cookbooks:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://docs.pageindex.ai/cookbook/vectorless-rag-pageindex"&gt;Vectorless RAG&lt;/a&gt;: A minimal, hands-on example of reasoning-based RAG using PageIndex. No vectors, no chunking, and human-like retrieval.&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://docs.pageindex.ai/cookbook/vision-rag-pageindex"&gt;Vision-based Vectorless RAG&lt;/a&gt;: OCR-free, vision-only RAG with PageIndex's reasoning-native retrieval workflow that works directly over PDF page images.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h1&gt;üìë Introduction to PageIndex&lt;/h1&gt; 
&lt;p&gt;Are you frustrated with vector database retrieval accuracy for long professional documents? Traditional vector-based RAG relies on semantic &lt;em&gt;similarity&lt;/em&gt; rather than true &lt;em&gt;relevance&lt;/em&gt;. But &lt;strong&gt;similarity ‚â† relevance&lt;/strong&gt; ‚Äî what we truly need in retrieval is &lt;strong&gt;relevance&lt;/strong&gt;, and that requires &lt;strong&gt;reasoning&lt;/strong&gt;. When working with professional documents that demand domain expertise and multi-step reasoning, similarity search often falls short.&lt;/p&gt; 
&lt;p&gt;Inspired by AlphaGo, we propose &lt;strong&gt;&lt;a href="https://vectify.ai/pageindex"&gt;PageIndex&lt;/a&gt;&lt;/strong&gt; ‚Äî a &lt;strong&gt;vectorless&lt;/strong&gt;, &lt;strong&gt;reasoning-based RAG&lt;/strong&gt; system that builds a &lt;strong&gt;hierarchical tree index&lt;/strong&gt; from long documents and uses LLMs to &lt;strong&gt;reason&lt;/strong&gt; &lt;em&gt;over that index&lt;/em&gt; for &lt;strong&gt;agentic, context-aware retrieval&lt;/strong&gt;. It simulates how &lt;em&gt;human experts&lt;/em&gt; navigate and extract knowledge from complex documents through &lt;em&gt;tree search&lt;/em&gt;, enabling LLMs to &lt;em&gt;think&lt;/em&gt; and &lt;em&gt;reason&lt;/em&gt; their way to the most relevant document sections. PageIndex performs retrieval in two steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Generate a ‚ÄúTable-of-Contents‚Äù &lt;strong&gt;tree structure index&lt;/strong&gt; of documents&lt;/li&gt; 
 &lt;li&gt;Perform reasoning-based retrieval through &lt;strong&gt;tree search&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://pageindex.ai/blog/pageindex-intro" target="_blank" title="The PageIndex Framework"&gt; &lt;img src="https://docs.pageindex.ai/images/cookbook/vectorless-rag.png" width="70%" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h3&gt;üéØ Core Features&lt;/h3&gt; 
&lt;p&gt;Compared to traditional vector-based RAG, &lt;strong&gt;PageIndex&lt;/strong&gt; features:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;No Vector DB&lt;/strong&gt;: Uses document structure and LLM reasoning for retrieval, instead of vector similarity search.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;No Chunking&lt;/strong&gt;: Documents are organized into natural sections, not artificial chunks.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Human-like Retrieval&lt;/strong&gt;: Simulates how human experts navigate and extract knowledge from complex documents.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Better Explainability and Traceability&lt;/strong&gt;: Retrieval is based on reasoning ‚Äî traceable and interpretable, with page and section references. No more opaque, approximate vector search (‚Äúvibe retrieval‚Äù).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;PageIndex powers a reasoning-based RAG system that achieved &lt;strong&gt;state-of-the-art&lt;/strong&gt; &lt;a href="https://github.com/VectifyAI/Mafin2.5-FinanceBench"&gt;98.7% accuracy&lt;/a&gt; on FinanceBench, demonstrating superior performance over vector-based RAG solutions in professional document analysis (see our &lt;a href="https://vectify.ai/blog/Mafin2.5"&gt;blog post&lt;/a&gt; for details).&lt;/p&gt; 
&lt;h3&gt;üìç Explore PageIndex&lt;/h3&gt; 
&lt;p&gt;To learn more, please see a detailed introduction of the &lt;a href="https://pageindex.ai/blog/pageindex-intro"&gt;PageIndex framework&lt;/a&gt;. Check out this GitHub repo for open-source code, and the &lt;a href="https://docs.pageindex.ai/cookbook"&gt;cookbooks&lt;/a&gt;, &lt;a href="https://docs.pageindex.ai/tutorials"&gt;tutorials&lt;/a&gt;, and &lt;a href="https://pageindex.ai/blog"&gt;blog&lt;/a&gt; for additional usage guides and examples.&lt;/p&gt; 
&lt;p&gt;The PageIndex service is available as a ChatGPT-style &lt;a href="https://chat.pageindex.ai"&gt;chat platform&lt;/a&gt;, or can be integrated via &lt;a href="https://pageindex.ai/mcp"&gt;MCP&lt;/a&gt; or &lt;a href="https://docs.pageindex.ai/quickstart"&gt;API&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;üõ†Ô∏è Deployment Options&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Self-host ‚Äî run locally with this open-source repo.&lt;/li&gt; 
 &lt;li&gt;Cloud Service ‚Äî try instantly with our &lt;a href="https://chat.pageindex.ai/"&gt;Chat Platform&lt;/a&gt;, or integrate with &lt;a href="https://pageindex.ai/mcp"&gt;MCP&lt;/a&gt; or &lt;a href="https://docs.pageindex.ai/quickstart"&gt;API&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;Enterprise&lt;/em&gt; ‚Äî private or on-prem deployment. &lt;a href="https://ii2abc2jejf.typeform.com/to/tK3AXl8T"&gt;Contact us&lt;/a&gt; or &lt;a href="https://calendly.com/pageindex/meet"&gt;book a demo&lt;/a&gt; for more details.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üß™ Quick Hands-on&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Try the &lt;a href="https://github.com/VectifyAI/PageIndex/raw/main/cookbook/pageindex_RAG_simple.ipynb"&gt;&lt;strong&gt;Vectorless RAG&lt;/strong&gt;&lt;/a&gt; notebook ‚Äî a &lt;em&gt;minimal&lt;/em&gt;, hands-on example of reasoning-based RAG using PageIndex.&lt;/li&gt; 
 &lt;li&gt;Experiment with &lt;a href="https://github.com/VectifyAI/PageIndex/raw/main/cookbook/vision_RAG_pageindex.ipynb"&gt;&lt;em&gt;Vision-based Vectorless RAG&lt;/em&gt;&lt;/a&gt; ‚Äî no OCR; a minimal, reasoning-native RAG pipeline that works directly over page images.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://colab.research.google.com/github/VectifyAI/PageIndex/blob/main/cookbook/pageindex_RAG_simple.ipynb" target="_blank" rel="noopener"&gt; &lt;img src="https://img.shields.io/badge/Open_In_Colab-Vectorless_RAG-orange?style=for-the-badge&amp;amp;logo=googlecolab" alt="Open in Colab: Vectorless RAG" /&gt; &lt;/a&gt; &amp;nbsp;&amp;nbsp; 
 &lt;a href="https://colab.research.google.com/github/VectifyAI/PageIndex/blob/main/cookbook/vision_RAG_pageindex.ipynb" target="_blank" rel="noopener"&gt; &lt;img src="https://img.shields.io/badge/Open_In_Colab-Vision_RAG-orange?style=for-the-badge&amp;amp;logo=googlecolab" alt="Open in Colab: Vision RAG" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h1&gt;üå≤ PageIndex Tree Structure&lt;/h1&gt; 
&lt;p&gt;PageIndex can transform lengthy PDF documents into a semantic &lt;strong&gt;tree structure&lt;/strong&gt;, similar to a &lt;em&gt;"table of contents"&lt;/em&gt; but optimized for use with Large Language Models (LLMs). It's ideal for: financial reports, regulatory filings, academic textbooks, legal or technical manuals, and any document that exceeds LLM context limits.&lt;/p&gt; 
&lt;p&gt;Below is an example PageIndex tree structure. Also see more example &lt;a href="https://github.com/VectifyAI/PageIndex/tree/main/tests/pdfs"&gt;documents&lt;/a&gt; and generated &lt;a href="https://github.com/VectifyAI/PageIndex/tree/main/tests/results"&gt;tree structures&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsonc"&gt;...
{
  "title": "Financial Stability",
  "node_id": "0006",
  "start_index": 21,
  "end_index": 22,
  "summary": "The Federal Reserve ...",
  "nodes": [
    {
      "title": "Monitoring Financial Vulnerabilities",
      "node_id": "0007",
      "start_index": 22,
      "end_index": 28,
      "summary": "The Federal Reserve's monitoring ..."
    },
    {
      "title": "Domestic and International Cooperation and Coordination",
      "node_id": "0008",
      "start_index": 28,
      "end_index": 31,
      "summary": "In 2023, the Federal Reserve collaborated ..."
    }
  ]
}
...
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can generate the PageIndex tree structure with this open-source repo, or use our &lt;a href="https://docs.pageindex.ai/quickstart"&gt;API&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h1&gt;‚öôÔ∏è Package Usage&lt;/h1&gt; 
&lt;p&gt;You can follow these steps to generate a PageIndex tree from a PDF document.&lt;/p&gt; 
&lt;h3&gt;1. Install dependencies&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip3 install --upgrade -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Set your OpenAI API key&lt;/h3&gt; 
&lt;p&gt;Create a &lt;code&gt;.env&lt;/code&gt; file in the root directory and add your API key:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CHATGPT_API_KEY=your_openai_key_here
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. Run PageIndex on your PDF&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python3 run_pageindex.py --pdf_path /path/to/your/document.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Optional parameters&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; You can customize the processing with additional optional arguments: 
 &lt;pre&gt;&lt;code&gt;--model                 OpenAI model to use (default: gpt-4o-2024-11-20)
--toc-check-pages       Pages to check for table of contents (default: 20)
--max-pages-per-node    Max pages per node (default: 10)
--max-tokens-per-node   Max tokens per node (default: 20000)
--if-add-node-id        Add node ID (yes/no, default: yes)
--if-add-node-summary   Add node summary (yes/no, default: yes)
--if-add-doc-description Add doc description (yes/no, default: yes)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Markdown support&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; We also provide markdown support for PageIndex. You can use the `-md_path` flag to generate a tree structure for a markdown file. 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python3 run_pageindex.py --md_path /path/to/your/document.md
&lt;/code&gt;&lt;/pre&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;Note: in this function, we use "#" to determine node heading and their levels. For example, "##" is level 2, "###" is level 3, etc. Make sure your markdown file is formatted correctly. If your Markdown file was converted from a PDF or HTML, we don't recommend using this function, since most existing conversion tools cannot preserve the original hierarchy. Instead, use our &lt;a href="https://pageindex.ai/blog/ocr"&gt;PageIndex OCR&lt;/a&gt;, which is designed to preserve the original hierarchy, to convert the PDF to a markdown file and then use this function.&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/details&gt; 
&lt;!-- 
# ‚òÅÔ∏è Improved Tree Generation with PageIndex OCR

This repo is designed for generating PageIndex tree structure for simple PDFs, but many real-world use cases involve complex PDFs that are hard to parse by classic Python tools. However, extracting high-quality text from PDF documents remains a non-trivial challenge. Most OCR tools only extract page-level content, losing the broader document context and hierarchy.

To address this, we introduced PageIndex OCR ‚Äî the first long-context OCR model designed to preserve the global structure of documents. PageIndex OCR significantly outperforms other leading OCR tools, such as those from Mistral and Contextual AI, in recognizing true hierarchy and semantic relationships across document pages.

- Experience next-level OCR quality with PageIndex OCR at our [Dashboard](https://dash.pageindex.ai/).
- Integrate PageIndex OCR seamlessly into your stack via our [API](https://docs.pageindex.ai/quickstart).

&lt;p align="center"&gt;
  &lt;img src="https://github.com/user-attachments/assets/eb35d8ae-865c-4e60-a33b-ebbd00c41732" width="80%"&gt;
&lt;/p&gt;
--&gt; 
&lt;hr /&gt; 
&lt;h1&gt;üìà Case Study: PageIndex Leads Finance QA Benchmark&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://vectify.ai/mafin"&gt;Mafin 2.5&lt;/a&gt; is a reasoning-based RAG system for financial document analysis, powered by &lt;strong&gt;PageIndex&lt;/strong&gt;. It achieved a state-of-the-art &lt;a href="https://vectify.ai/blog/Mafin2.5"&gt;&lt;strong&gt;98.7% accuracy&lt;/strong&gt;&lt;/a&gt; on the &lt;a href="https://arxiv.org/abs/2311.11944"&gt;FinanceBench&lt;/a&gt; benchmark, significantly outperforming traditional vector-based RAG systems.&lt;/p&gt; 
&lt;p&gt;PageIndex's hierarchical indexing and reasoning-driven retrieval enable precise navigation and extraction of relevant context from complex financial reports, such as SEC filings and earnings disclosures.&lt;/p&gt; 
&lt;p&gt;Explore the full &lt;a href="https://github.com/VectifyAI/Mafin2.5-FinanceBench"&gt;benchmark results&lt;/a&gt; and our &lt;a href="https://vectify.ai/blog/Mafin2.5"&gt;blog post&lt;/a&gt; for detailed comparisons and performance metrics.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/VectifyAI/Mafin2.5-FinanceBench"&gt; &lt;img src="https://github.com/user-attachments/assets/571aa074-d803-43c7-80c4-a04254b782a3" width="70%" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h1&gt;üß≠ Resources&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;üß™ &lt;a href="https://docs.pageindex.ai/cookbook/vectorless-rag-pageindex"&gt;Cookbooks&lt;/a&gt;: hands-on, runnable examples and advanced use cases.&lt;/li&gt; 
 &lt;li&gt;üìñ &lt;a href="https://docs.pageindex.ai/doc-search"&gt;Tutorials&lt;/a&gt;: practical guides and strategies, including &lt;em&gt;Document Search&lt;/em&gt; and &lt;em&gt;Tree Search&lt;/em&gt;.&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://pageindex.ai/blog"&gt;Blog&lt;/a&gt;: technical articles, research insights, and product updates.&lt;/li&gt; 
 &lt;li&gt;üîå &lt;a href="https://pageindex.ai/mcp#quick-setup"&gt;MCP setup&lt;/a&gt; &amp;amp; &lt;a href="https://docs.pageindex.ai/quickstart"&gt;API docs&lt;/a&gt;: integration details and configuration options.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h1&gt;‚≠ê Support Us&lt;/h1&gt; 
&lt;p&gt;Leave us a star üåü if you like our project. Thank you!&lt;/p&gt; 
&lt;p&gt; &lt;img src="https://github.com/user-attachments/assets/eae4ff38-48ae-4a7c-b19f-eab81201d794" width="80%" /&gt; &lt;/p&gt; 
&lt;h3&gt;Connect with Us&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://x.com/PageIndexAI"&gt;&lt;img src="https://img.shields.io/badge/Twitter-000000?style=for-the-badge&amp;amp;logo=x&amp;amp;logoColor=white" alt="Twitter" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://www.linkedin.com/company/vectify-ai/"&gt;&lt;img src="https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&amp;amp;logo=linkedin&amp;amp;logoColor=white" alt="LinkedIn" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://discord.com/invite/VuXuf29EUj"&gt;&lt;img src="https://img.shields.io/badge/Discord-5865F2?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://ii2abc2jejf.typeform.com/to/tK3AXl8T"&gt;&lt;img src="https://img.shields.io/badge/Contact_Us-3B82F6?style=for-the-badge&amp;amp;logo=envelope&amp;amp;logoColor=white" alt="Contact Us" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;¬© 2025 &lt;a href="https://vectify.ai"&gt;Vectify AI&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>microsoft/VibeVoice</title>
      <link>https://github.com/microsoft/VibeVoice</link>
      <description>&lt;p&gt;Open-Source Frontier Voice AI&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h2&gt;üéôÔ∏è VibeVoice: Open-Source Frontier Voice AI&lt;/h2&gt; 
 &lt;p&gt;&lt;a href="https://microsoft.github.io/VibeVoice"&gt;&lt;img src="https://img.shields.io/badge/Project-Page-blue?logo=githubpages" alt="Project Page" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/collections/microsoft/vibevoice-68a2ef24a875c44be47b034f"&gt;&lt;img src="https://img.shields.io/badge/HuggingFace-Collection-orange?logo=huggingface" alt="Hugging Face" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/pdf/2508.19205"&gt;&lt;img src="https://img.shields.io/badge/TTS-Report-red?logo=arxiv" alt="TTS Report" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/VibeVoice-ASR-Report.pdf"&gt;&lt;img src="https://img.shields.io/badge/ASR-Report-yellow?logo=arxiv" alt="ASR Report" /&gt;&lt;/a&gt; &lt;a href="https://colab.research.google.com/github/microsoft/VibeVoice/blob/main/demo/VibeVoice_colab.ipynb"&gt;&lt;img src="https://img.shields.io/badge/StreamingTTS-Colab-green?logo=googlecolab" alt="Colab" /&gt;&lt;/a&gt; &lt;a href="https://aka.ms/vibevoice-asr"&gt;&lt;img src="https://img.shields.io/badge/ASR-Playground-6F42C1?logo=gradio" alt="ASR Playground" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="Figures/VibeVoice_logo_white.png" /&gt; 
  &lt;img src="https://raw.githubusercontent.com/microsoft/VibeVoice/main/Figures/VibeVoice_logo.png" alt="VibeVoice Logo" width="300" /&gt; 
 &lt;/picture&gt; 
&lt;/div&gt; 
&lt;div align="left"&gt; 
 &lt;h3&gt;üì∞ News&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;2026-01-21: üì£ We open-sourced &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-asr.md"&gt;&lt;strong&gt;VibeVoice-ASR&lt;/strong&gt;&lt;/a&gt;, a unified speech-to-text model designed to handle 60-minute long-form audio in a single pass, generating structured transcriptions containing Who (Speaker), When (Timestamps), and What (Content), with support for User-Customized Context. Try it in &lt;a href="https://aka.ms/vibevoice-asr"&gt;Playground&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;‚≠êÔ∏è VibeVoice-ASR is natively multilingual, supporting over 50 languages ‚Äî check the &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-asr.md#language-distribution"&gt;supported languages&lt;/a&gt; for details.&lt;/li&gt; 
  &lt;li&gt;üî• The VibeVoice-ASR &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/finetuning-asr/README.md"&gt;finetuning code&lt;/a&gt; is now available!&lt;/li&gt; 
  &lt;li&gt;‚ö°Ô∏è &lt;strong&gt;vLLM inference&lt;/strong&gt; is now supported for faster inference; see &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-vllm-asr.md"&gt;vllm-asr&lt;/a&gt; for more details.&lt;/li&gt; 
  &lt;li&gt;üìë &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/VibeVoice-ASR-Report.pdf"&gt;VibeVoice-ASR Technique Report&lt;/a&gt; is available.&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;2025-12-16: üì£ We added experimental speakers to &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-realtime-0.5b.md"&gt;&lt;strong&gt;VibeVoice‚ÄëRealtime‚Äë0.5B&lt;/strong&gt;&lt;/a&gt; for exploration, including multilingual voices in nine languages (DE, FR, IT, JP, KR, NL, PL, PT, ES) and 11 distinct English style voices. &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-realtime-0.5b.md#optional-more-experimental-voices"&gt;Try it&lt;/a&gt;. More speaker types will be added over time.&lt;/p&gt; 
 &lt;p&gt;2025-12-03: üì£ We open-sourced &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-realtime-0.5b.md"&gt;&lt;strong&gt;VibeVoice‚ÄëRealtime‚Äë0.5B&lt;/strong&gt;&lt;/a&gt;, a real‚Äëtime text‚Äëto‚Äëspeech model that supports streaming text input and robust long-form speech generation. Try it on &lt;a href="https://colab.research.google.com/github/microsoft/VibeVoice/blob/main/demo/vibevoice_realtime_colab.ipynb"&gt;Colab&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;2025-09-05: VibeVoice is an open-source research framework intended to advance collaboration in the speech synthesis community. After release, we discovered instances where the tool was used in ways inconsistent with the stated intent. Since responsible use of AI is one of Microsoft‚Äôs guiding principles, we have removed the VibeVoice-TTS code from this repository.&lt;/p&gt; 
 &lt;p&gt;2025-08-25: üì£ We open-sourced &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-tts.md"&gt;&lt;strong&gt;VibeVoice-TTS&lt;/strong&gt;&lt;/a&gt;, a long-form multi-speaker text-to-speech model that can synthesize speech up to 90 minutes long with up to 4 distinct speakers.&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;Overview&lt;/h2&gt; 
&lt;p&gt;VibeVoice is a &lt;strong&gt;family of open-source frontier voice AI models&lt;/strong&gt; that includes both Text-to-Speech (TTS) and Automatic Speech Recognition (ASR) models.&lt;/p&gt; 
&lt;p&gt;A core innovation of VibeVoice is its use of continuous speech tokenizers (Acoustic and Semantic) operating at an ultra-low frame rate of &lt;strong&gt;7.5 Hz&lt;/strong&gt;. These tokenizers efficiently preserve audio fidelity while significantly boosting computational efficiency for processing long sequences. VibeVoice employs a &lt;a href="https://arxiv.org/abs/2412.08635"&gt;next-token diffusion&lt;/a&gt; framework, leveraging a Large Language Model (LLM) to understand textual context and dialogue flow, and a diffusion head to generate high-fidelity acoustic details.&lt;/p&gt; 
&lt;p&gt;For more information, demos, and examples, please visit our &lt;a href="https://microsoft.github.io/VibeVoice"&gt;Project Page&lt;/a&gt;.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;Weight&lt;/th&gt; 
    &lt;th&gt;Quick Try&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;VibeVoice-ASR-7B&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://huggingface.co/microsoft/VibeVoice-ASR"&gt;HF Link&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://aka.ms/vibevoice-asr"&gt;Playground&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;VibeVoice-TTS-1.5B&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://huggingface.co/microsoft/VibeVoice-1.5B"&gt;HF Link&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Disabled&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;VibeVoice-Realtime-0.5B&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://huggingface.co/microsoft/VibeVoice-Realtime-0.5B"&gt;HF Link&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://colab.research.google.com/github/microsoft/VibeVoice/blob/main/demo/vibevoice_realtime_colab.ipynb"&gt;Colab&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;Models&lt;/h2&gt; 
&lt;h3&gt;1. üìñ &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-asr.md"&gt;VibeVoice-ASR&lt;/a&gt; - Long-form Speech Recognition&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;VibeVoice-ASR&lt;/strong&gt; is a unified speech-to-text model designed to handle &lt;strong&gt;60-minute long-form audio&lt;/strong&gt; in a single pass, generating structured transcriptions containing &lt;strong&gt;Who (Speaker), When (Timestamps), and What (Content)&lt;/strong&gt;, with support for &lt;strong&gt;Customized Hotwords&lt;/strong&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üïí 60-minute Single-Pass Processing&lt;/strong&gt;: Unlike conventional ASR models that slice audio into short chunks (often losing global context), VibeVoice ASR accepts up to &lt;strong&gt;60 minutes&lt;/strong&gt; of continuous audio input within 64K token length. This ensures consistent speaker tracking and semantic coherence across the entire hour.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üë§ Customized Hotwords&lt;/strong&gt;: Users can provide customized hotwords (e.g., specific names, technical terms, or background info) to guide the recognition process, significantly improving accuracy on domain-specific content.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üìù Rich Transcription (Who, When, What)&lt;/strong&gt;: The model jointly performs ASR, diarization, and timestamping, producing a structured output that indicates &lt;em&gt;who&lt;/em&gt; said &lt;em&gt;what&lt;/em&gt; and &lt;em&gt;when&lt;/em&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-asr.md"&gt;üìñ Documentation&lt;/a&gt; | &lt;a href="https://huggingface.co/microsoft/VibeVoice-ASR"&gt;ü§ó Hugging Face&lt;/a&gt; | &lt;a href="https://aka.ms/vibevoice-asr"&gt;üéÆ Playground&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/finetuning-asr/README.md"&gt;üõ†Ô∏è Finetuning&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/VibeVoice/main/Figures/DER.jpg" alt="DER" width="50%" /&gt;&lt;br /&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/VibeVoice/main/Figures/cpWER.jpg" alt="cpWER" width="50%" /&gt;&lt;br /&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/VibeVoice/main/Figures/tcpWER.jpg" alt="tcpWER" width="50%" /&gt; &lt;/p&gt; 
&lt;div align="center" id="vibevoice-asr"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/acde5602-dc17-4314-9e3b-c630bc84aefa"&gt;https://github.com/user-attachments/assets/acde5602-dc17-4314-9e3b-c630bc84aefa&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;h3&gt;2. üéôÔ∏è &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-tts.md"&gt;VibeVoice-TTS&lt;/a&gt; - Long-form Multi-speaker TTS&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Best for&lt;/strong&gt;: Long-form conversational audio, podcasts, multi-speaker dialogues&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;‚è±Ô∏è 90-minute Long-form Generation&lt;/strong&gt;: Synthesizes conversational/single-speaker speech up to &lt;strong&gt;90 minutes&lt;/strong&gt; in a single pass, maintaining speaker consistency and semantic coherence throughout.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üë• Multi-speaker Support&lt;/strong&gt;: Supports up to &lt;strong&gt;4 distinct speakers&lt;/strong&gt; in a single conversation, with natural turn-taking and speaker consistency across long dialogues.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üé≠ Expressive Speech&lt;/strong&gt;: Generates expressive, natural-sounding speech that captures conversational dynamics and emotional nuances.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üåê Multi-lingual Support&lt;/strong&gt;: Supports English, Chinese and other languages.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-tts.md"&gt;üìñ Documentation&lt;/a&gt; | &lt;a href="https://huggingface.co/microsoft/VibeVoice-1.5B"&gt;ü§ó Hugging Face&lt;/a&gt; | &lt;a href="https://arxiv.org/pdf/2508.19205"&gt;üìä Paper&lt;/a&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/microsoft/VibeVoice/main/Figures/VibeVoice-TTS-results.jpg" alt="VibeVoice Results" width="80%" /&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;English&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/0967027c-141e-4909-bec8-091558b1b784"&gt;https://github.com/user-attachments/assets/0967027c-141e-4909-bec8-091558b1b784&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Chinese&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/322280b7-3093-4c67-86e3-10be4746c88f"&gt;https://github.com/user-attachments/assets/322280b7-3093-4c67-86e3-10be4746c88f&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Cross-Lingual&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/838d8ad9-a201-4dde-bb45-8cd3f59ce722"&gt;https://github.com/user-attachments/assets/838d8ad9-a201-4dde-bb45-8cd3f59ce722&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Spontaneous Singing&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/6f27a8a5-0c60-4f57-87f3-7dea2e11c730"&gt;https://github.com/user-attachments/assets/6f27a8a5-0c60-4f57-87f3-7dea2e11c730&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Long Conversation with 4 people&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/a357c4b6-9768-495c-a576-1618f6275727"&gt;https://github.com/user-attachments/assets/a357c4b6-9768-495c-a576-1618f6275727&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;h3&gt;3. ‚ö° &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-realtime-0.5b.md"&gt;VibeVoice-Streaming&lt;/a&gt; - Real-time Streaming TTS&lt;/h3&gt; 
&lt;p&gt;VibeVoice-Realtime is a &lt;strong&gt;lightweight real‚Äëtime&lt;/strong&gt; text-to-speech model supporting &lt;strong&gt;streaming text input&lt;/strong&gt; and &lt;strong&gt;robust long-form speech generation&lt;/strong&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Parameter size: 0.5B (deployment-friendly)&lt;/li&gt; 
 &lt;li&gt;Real-time TTS (~300 milliseconds first audible latency)&lt;/li&gt; 
 &lt;li&gt;Streaming text input&lt;/li&gt; 
 &lt;li&gt;Robust long-form speech generation (~10 minutes)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-realtime-0.5b.md"&gt;üìñ Documentation&lt;/a&gt; | &lt;a href="https://huggingface.co/microsoft/VibeVoice-Realtime-0.5B"&gt;ü§ó Hugging Face&lt;/a&gt; | &lt;a href="https://colab.research.google.com/github/microsoft/VibeVoice/blob/main/demo/vibevoice_realtime_colab.ipynb"&gt;üöÄ Colab&lt;/a&gt;&lt;/p&gt; 
&lt;div align="center" id="generated-example-audio-vibevoice-realtime"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/0901d274-f6ae-46ef-a0fd-3c4fba4f76dc"&gt;https://github.com/user-attachments/assets/0901d274-f6ae-46ef-a0fd-3c4fba4f76dc&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;h2&gt;‚ö†Ô∏è Risks and Limitations&lt;/h2&gt; 
&lt;p&gt;While efforts have been made to optimize it through various techniques, it may still produce outputs that are unexpected, biased, or inaccurate. VibeVoice inherits any biases, errors, or omissions produced by its base model (specifically, Qwen2.5 1.5b in this release). Potential for Deepfakes and Disinformation: High-quality synthetic speech can be misused to create convincing fake audio content for impersonation, fraud, or spreading disinformation. Users must ensure transcripts are reliable, check content accuracy, and avoid using generated content in misleading ways. Users are expected to use the generated content and to deploy the models in a lawful manner, in full compliance with all applicable laws and regulations in the relevant jurisdictions. It is best practice to disclose the use of AI when sharing AI-generated content.&lt;/p&gt; 
&lt;p&gt;We do not recommend using VibeVoice in commercial or real-world applications without further testing and development. This model is intended for research and development purposes only. Please use responsibly.&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://api.star-history.com/svg?repos=Microsoft/vibevoice&amp;amp;type=date&amp;amp;legend=top-left" alt="Star History Chart" /&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Blaizzy/mlx-audio</title>
      <link>https://github.com/Blaizzy/mlx-audio</link>
      <description>&lt;p&gt;A text-to-speech (TTS), speech-to-text (STT) and speech-to-speech (STS) library built on Apple's MLX framework, providing efficient speech analysis on Apple Silicon.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MLX-Audio&lt;/h1&gt; 
&lt;p&gt;The best audio processing library built on Apple's MLX framework, providing fast and efficient text-to-speech (TTS), speech-to-text (STT), and speech-to-speech (STS) on Apple Silicon.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Fast inference optimized for Apple Silicon (M series chips)&lt;/li&gt; 
 &lt;li&gt;Multiple model architectures for TTS, STT, and STS&lt;/li&gt; 
 &lt;li&gt;Multilingual support across models&lt;/li&gt; 
 &lt;li&gt;Voice customization and cloning capabilities&lt;/li&gt; 
 &lt;li&gt;Adjustable speech speed control&lt;/li&gt; 
 &lt;li&gt;Interactive web interface with 3D audio visualization&lt;/li&gt; 
 &lt;li&gt;OpenAI-compatible REST API&lt;/li&gt; 
 &lt;li&gt;Quantization support (3-bit, 4-bit, 6-bit, 8-bit, and more) for optimized performance&lt;/li&gt; 
 &lt;li&gt;Swift package for iOS/macOS integration&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;Using pip&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install mlx-audio
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Using uv to install only the command line tools&lt;/h3&gt; 
&lt;p&gt;Latest release from pypi:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv tool install --force mlx-audio --prerelease=allow
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Latest code from github:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv tool install --force git+https://github.com/Blaizzy/mlx-audio.git --prerelease=allow
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;For development or web interface:&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/Blaizzy/mlx-audio.git
cd mlx-audio
pip install -e ".[dev]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;h3&gt;Command Line&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Basic TTS generation
mlx_audio.tts.generate --model mlx-community/Kokoro-82M-bf16 --text "Hello, world!" --lang_code a

# With voice selection and speed adjustment
mlx_audio.tts.generate --model mlx-community/Kokoro-82M-bf16 --text "Hello!" --voice af_heart --speed 1.2 --lang_code a

# Play audio immediately
mlx_audio.tts.generate --model mlx-community/Kokoro-82M-bf16 --text "Hello!" --play  --lang_code a

# Save to a specific directory
mlx_audio.tts.generate --model mlx-community/Kokoro-82M-bf16 --text "Hello!" --output_path ./my_audio  --lang_code a
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Python API&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_audio.tts.utils import load_model

# Load model
model = load_model("mlx-community/Kokoro-82M-bf16")

# Generate speech
for result in model.generate("Hello from MLX-Audio!", voice="af_heart"):
    print(f"Generated {result.audio.shape[0]} samples")
    # result.audio contains the waveform as mx.array
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Supported Models&lt;/h2&gt; 
&lt;h3&gt;Text-to-Speech (TTS)&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Languages&lt;/th&gt; 
   &lt;th&gt;Repo&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Kokoro&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Fast, high-quality multilingual TTS&lt;/td&gt; 
   &lt;td&gt;EN, JA, ZH, FR, ES, IT, PT, HI&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/Kokoro-82M-bf16"&gt;mlx-community/Kokoro-82M-bf16&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Qwen3-TTS&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Alibaba's multilingual TTS with voice design&lt;/td&gt; 
   &lt;td&gt;ZH, EN, JA, KO, + more&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-1.7B-VoiceDesign-bf16"&gt;mlx-community/Qwen3-TTS-12Hz-1.7B-VoiceDesign-bf16&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;CSM&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Conversational Speech Model with voice cloning&lt;/td&gt; 
   &lt;td&gt;EN&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/csm-1b"&gt;mlx-community/csm-1b&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Dia&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Dialogue-focused TTS&lt;/td&gt; 
   &lt;td&gt;EN&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/Dia-1.6B-bf16"&gt;mlx-community/Dia-1.6B-bf16&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;OuteTTS&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Efficient TTS model&lt;/td&gt; 
   &lt;td&gt;EN&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/OuteTTS-0.2-500M"&gt;mlx-community/OuteTTS-0.2-500M&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Spark&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;SparkTTS model&lt;/td&gt; 
   &lt;td&gt;EN, ZH&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/SparkTTS-0.5B-bf16"&gt;mlx-community/SparkTTS-0.5B-bf16&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Chatterbox&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Expressive multilingual TTS&lt;/td&gt; 
   &lt;td&gt;EN, ES, FR, DE, IT, PT, PL, TR, RU, NL, CS, AR, ZH, JA, HU, KO&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/Chatterbox-bf16"&gt;mlx-community/Chatterbox-bf16&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Soprano&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;High-quality TTS&lt;/td&gt; 
   &lt;td&gt;EN&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/Soprano-bf16"&gt;mlx-community/Soprano-bf16&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Speech-to-Text (STT)&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Languages&lt;/th&gt; 
   &lt;th&gt;Repo&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Whisper&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;OpenAI's robust STT model&lt;/td&gt; 
   &lt;td&gt;99+ languages&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/whisper-large-v3-turbo-asr-fp16"&gt;mlx-community/whisper-large-v3-turbo-asr-fp16&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Parakeet&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;NVIDIA's accurate STT&lt;/td&gt; 
   &lt;td&gt;EN&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/parakeet-tdt-0.6b-v2"&gt;mlx-community/parakeet-tdt-0.6b-v2&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Voxtral&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Mistral's speech model&lt;/td&gt; 
   &lt;td&gt;Multiple&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/Voxtral-Mini-3B-2507-bf16"&gt;mlx-community/Voxtral-Mini-3B-2507-bf16&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;VibeVoice-ASR&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Microsoft's 9B ASR with diarization &amp;amp; timestamps&lt;/td&gt; 
   &lt;td&gt;Multiple&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/VibeVoice-ASR-bf16"&gt;mlx-community/VibeVoice-ASR-bf16&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Speech-to-Speech (STS)&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Use Case&lt;/th&gt; 
   &lt;th&gt;Repo&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;SAM-Audio&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Text-guided source separation&lt;/td&gt; 
   &lt;td&gt;Extract specific sounds&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/sam-audio-large"&gt;mlx-community/sam-audio-large&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Liquid2.5-Audio&lt;/strong&gt;*&lt;/td&gt; 
   &lt;td&gt;Speech-to-Speech, Text-to-Speech and Speech-to-Text&lt;/td&gt; 
   &lt;td&gt;Speech interactions&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/LFM2.5-Audio-1.5B-8bit"&gt;mlx-community/LFM2.5-Audio-1.5B-8bit&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;MossFormer2 SE&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Speech enhancement&lt;/td&gt; 
   &lt;td&gt;Noise removal&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/starkdmi/MossFormer2_SE_48K_MLX"&gt;starkdmi/MossFormer2_SE_48K_MLX&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Model Examples&lt;/h2&gt; 
&lt;h3&gt;Kokoro TTS&lt;/h3&gt; 
&lt;p&gt;Kokoro is a fast, multilingual TTS model with 54 voice presets.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_audio.tts.utils import load_model

model = load_model("mlx-community/Kokoro-82M-bf16")

# Generate with different voices
for result in model.generate(
    text="Welcome to MLX-Audio!",
    voice="af_heart",  # American female
    speed=1.0,
    lang_code="a"  # American English
):
    audio = result.audio
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Available Voices:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;American English: &lt;code&gt;af_heart&lt;/code&gt;, &lt;code&gt;af_bella&lt;/code&gt;, &lt;code&gt;af_nova&lt;/code&gt;, &lt;code&gt;af_sky&lt;/code&gt;, &lt;code&gt;am_adam&lt;/code&gt;, &lt;code&gt;am_echo&lt;/code&gt;, etc.&lt;/li&gt; 
 &lt;li&gt;British English: &lt;code&gt;bf_alice&lt;/code&gt;, &lt;code&gt;bf_emma&lt;/code&gt;, &lt;code&gt;bm_daniel&lt;/code&gt;, &lt;code&gt;bm_george&lt;/code&gt;, etc.&lt;/li&gt; 
 &lt;li&gt;Japanese: &lt;code&gt;jf_alpha&lt;/code&gt;, &lt;code&gt;jm_kumo&lt;/code&gt;, etc.&lt;/li&gt; 
 &lt;li&gt;Chinese: &lt;code&gt;zf_xiaobei&lt;/code&gt;, &lt;code&gt;zm_yunxi&lt;/code&gt;, etc.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Language Codes:&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Code&lt;/th&gt; 
   &lt;th&gt;Language&lt;/th&gt; 
   &lt;th&gt;Note&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;a&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;American English&lt;/td&gt; 
   &lt;td&gt;Default&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;b&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;British English&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;j&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Japanese&lt;/td&gt; 
   &lt;td&gt;Requires &lt;code&gt;pip install misaki[ja]&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;z&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Mandarin Chinese&lt;/td&gt; 
   &lt;td&gt;Requires &lt;code&gt;pip install misaki[zh]&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;e&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Spanish&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;f&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;French&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Qwen3-TTS&lt;/h3&gt; 
&lt;p&gt;Alibaba's state-of-the-art multilingual TTS with voice cloning, emotion control, and voice design capabilities.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_audio.tts.utils import load_model

model = load_model("mlx-community/Qwen3-TTS-12Hz-0.6B-Base-bf16")
results = list(model.generate(
    text="Hello, welcome to MLX-Audio!",
    voice="Chelsie",
    language="English",
))

audio = results[0].audio  # mx.array
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/Blaizzy/mlx-audio/main/mlx_audio/tts/models/qwen3_tts/README.md"&gt;Qwen3-TTS README&lt;/a&gt; for voice cloning, CustomVoice, VoiceDesign, and all available models.&lt;/p&gt; 
&lt;h3&gt;CSM (Voice Cloning)&lt;/h3&gt; 
&lt;p&gt;Clone any voice using a reference audio sample:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mlx_audio.tts.generate \
    --model mlx-community/csm-1b \
    --text "Hello from Sesame." \
    --ref_audio ./reference_voice.wav \
    --play
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Whisper STT&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_audio.stt.generate import generate_transcription

result = generate_transcription(
    model="mlx-community/whisper-large-v3-turbo-asr-fp16",
    audio="audio.wav",
)
print(result.text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;VibeVoice-ASR&lt;/h3&gt; 
&lt;p&gt;Microsoft's 9B parameter speech-to-text model with speaker diarization and timestamps. Supports long-form audio (up to 60 minutes) and outputs structured JSON.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_audio.stt.utils import load

model = load("mlx-community/VibeVoice-ASR-bf16")

# Basic transcription
result = model.generate(audio="meeting.wav", max_tokens=8192, temperature=0.0)
print(result.text)
# [{"Start":0,"End":5.2,"Speaker":0,"Content":"Hello everyone, let's begin."},
#  {"Start":5.5,"End":9.8,"Speaker":1,"Content":"Thanks for joining today."}]

# Access parsed segments
for seg in result.segments:
    print(f"[{seg['start_time']:.1f}-{seg['end_time']:.1f}] Speaker {seg['speaker_id']}: {seg['text']}")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Streaming transcription:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Stream tokens as they are generated
for text in model.stream_transcribe(audio="speech.wav", max_tokens=4096):
    print(text, end="", flush=True)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;With context (hotwords/metadata):&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;result = model.generate(
    audio="technical_talk.wav",
    context="MLX, Apple Silicon, PyTorch, Transformer",
    max_tokens=8192,
    temperature=0.0,
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;CLI usage:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Basic transcription
python -m mlx_audio.stt.generate \
    --model mlx-community/VibeVoice-ASR-bf16 \
    --audio meeting.wav \
    --output-path output \
    --format json \
    --max-tokens 8192 \
    --verbose

# With context/hotwords
python -m mlx_audio.stt.generate \
    --model mlx-community/VibeVoice-ASR-bf16 \
    --audio technical_talk.wav \
    --output-path output \
    --format json \
    --max-tokens 8192 \
    --context "MLX, Apple Silicon, PyTorch, Transformer" \
    --verbose
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;SAM-Audio (Source Separation)&lt;/h3&gt; 
&lt;p&gt;Separate specific sounds from audio using text prompts:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_audio.sts import SAMAudio, SAMAudioProcessor, save_audio

model = SAMAudio.from_pretrained("mlx-community/sam-audio-large")
processor = SAMAudioProcessor.from_pretrained("mlx-community/sam-audio-large")

batch = processor(
    descriptions=["A person speaking"],
    audios=["mixed_audio.wav"],
)

result = model.separate_long(
    batch.audios,
    descriptions=batch.descriptions,
    anchors=batch.anchor_ids,
    chunk_seconds=10.0,
    overlap_seconds=3.0,
    ode_opt={"method": "midpoint", "step_size": 2/32},
)

save_audio(result.target[0], "voice.wav")
save_audio(result.residual[0], "background.wav")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;MossFormer2 (Speech Enhancement)&lt;/h3&gt; 
&lt;p&gt;Remove noise from speech recordings:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_audio.sts import MossFormer2SEModel, save_audio

model = MossFormer2SEModel.from_pretrained("starkdmi/MossFormer2_SE_48K_MLX")
enhanced = model.enhance("noisy_speech.wav")
save_audio(enhanced, "clean.wav", 48000)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Web Interface &amp;amp; API Server&lt;/h2&gt; 
&lt;p&gt;MLX-Audio includes a modern web interface and OpenAI-compatible API.&lt;/p&gt; 
&lt;h3&gt;Starting the Server&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Start API server
mlx_audio.server --host 0.0.0.0 --port 8000

# Start web UI (in another terminal)
cd mlx_audio/ui
npm install &amp;amp;&amp;amp; npm run dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;API Endpoints&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Text-to-Speech&lt;/strong&gt; (OpenAI-compatible):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -X POST http://localhost:8000/v1/audio/speech \
  -H "Content-Type: application/json" \
  -d '{"model": "mlx-community/Kokoro-82M-bf16", "input": "Hello!", "voice": "af_heart"}' \
  --output speech.wav
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Speech-to-Text&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -X POST http://localhost:8000/v1/audio/transcriptions \
  -F "file=@audio.wav" \
  -F "model=mlx-community/whisper-large-v3-turbo-asr-fp16"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Quantization&lt;/h2&gt; 
&lt;p&gt;Reduce model size and improve performance with quantization using the convert script:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Convert and quantize to 4-bit
python -m mlx_audio.convert \
    --hf-path prince-canuma/Kokoro-82M \
    --mlx-path ./Kokoro-82M-4bit \
    --quantize \
    --q-bits 4 \
    --upload-repo username/Kokoro-82M-4bit (optional: if you want to upload the model to Hugging Face)

# Convert with specific dtype (bfloat16)
python -m mlx_audio.convert \
    --hf-path prince-canuma/Kokoro-82M \
    --mlx-path ./Kokoro-82M-bf16 \
    --dtype bfloat16 \
    --upload-repo username/Kokoro-82M-bf16 (optional: if you want to upload the model to Hugging Face)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Options:&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Flag&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--hf-path&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Source Hugging Face model or local path&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--mlx-path&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Output directory for converted model&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-q, --quantize&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Enable quantization&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--q-bits&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Bits per weight (4, 6, or 8)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--q-group-size&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Group size for quantization (default: 64)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--dtype&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Weight dtype: &lt;code&gt;float16&lt;/code&gt;, &lt;code&gt;bfloat16&lt;/code&gt;, &lt;code&gt;float32&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--upload-repo&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Upload converted model to HF Hub&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Swift&lt;/h2&gt; 
&lt;p&gt;Looking for Swift/iOS support? Check out &lt;a href="https://github.com/Blaizzy/mlx-audio-swift"&gt;mlx-audio-swift&lt;/a&gt; for on-device TTS using MLX on macOS and iOS.&lt;/p&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.10+&lt;/li&gt; 
 &lt;li&gt;Apple Silicon Mac (M1/M2/M3/M4)&lt;/li&gt; 
 &lt;li&gt;MLX framework&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ffmpeg&lt;/strong&gt; (required for MP3/FLAC audio encoding)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Installing ffmpeg&lt;/h3&gt; 
&lt;p&gt;ffmpeg is required for saving audio in MP3 or FLAC format. Install it using:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# macOS (using Homebrew)
brew install ffmpeg

# Ubuntu/Debian
sudo apt install ffmpeg
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;WAV format works without ffmpeg.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/Blaizzy/mlx-audio/main/LICENSE"&gt;MIT License&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{mlx-audio,
  author = {Canuma, Prince},
  title = {MLX Audio},
  year = {2025},
  howpublished = {\url{https://github.com/Blaizzy/mlx-audio}},
  note = {Audio processing library for Apple Silicon with TTS, STT, and STS capabilities.}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ml-explore/mlx"&gt;Apple MLX Team&lt;/a&gt; for the MLX framework&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>commaai/openpilot</title>
      <link>https://github.com/commaai/openpilot</link>
      <description>&lt;p&gt;openpilot is an operating system for robotics. Currently, it upgrades the driver assistance system on 300+ supported cars.&lt;/p&gt;&lt;hr&gt;&lt;div align="center" style="text-align: center;"&gt; 
 &lt;h1&gt;openpilot&lt;/h1&gt; 
 &lt;p&gt; &lt;b&gt;openpilot is an operating system for robotics.&lt;/b&gt; &lt;br /&gt; Currently, it upgrades the driver assistance system in 300+ supported cars. &lt;/p&gt; 
 &lt;h3&gt; &lt;a href="https://docs.comma.ai"&gt;Docs&lt;/a&gt; &lt;span&gt; ¬∑ &lt;/span&gt; &lt;a href="https://docs.comma.ai/contributing/roadmap/"&gt;Roadmap&lt;/a&gt; &lt;span&gt; ¬∑ &lt;/span&gt; &lt;a href="https://github.com/commaai/openpilot/raw/master/docs/CONTRIBUTING.md"&gt;Contribute&lt;/a&gt; &lt;span&gt; ¬∑ &lt;/span&gt; &lt;a href="https://discord.comma.ai"&gt;Community&lt;/a&gt; &lt;span&gt; ¬∑ &lt;/span&gt; &lt;a href="https://comma.ai/shop"&gt;Try it on a comma 3X&lt;/a&gt; &lt;/h3&gt; 
 &lt;p&gt;Quick start: &lt;code&gt;bash &amp;lt;(curl -fsSL openpilot.comma.ai)&lt;/code&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/commaai/openpilot/actions/workflows/tests.yaml"&gt;&lt;img src="https://github.com/commaai/openpilot/actions/workflows/tests.yaml/badge.svg?sanitize=true" alt="openpilot tests" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/commaai/openpilot/master/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true" alt="License: MIT" /&gt;&lt;/a&gt; &lt;a href="https://x.com/comma_ai"&gt;&lt;img src="https://img.shields.io/twitter/follow/comma_ai" alt="X Follow" /&gt;&lt;/a&gt; &lt;a href="https://discord.comma.ai"&gt;&lt;img src="https://img.shields.io/discord/469524606043160576" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://youtu.be/NmBfgOanCyk" title="Video By Greer Viau"&gt;&lt;img src="https://github.com/commaai/openpilot/assets/8762862/2f7112ae-f748-4f39-b617-fabd689c3772" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://youtu.be/VHKyqZ7t8Gw" title="Video By Logan LeGrand"&gt;&lt;img src="https://github.com/commaai/openpilot/assets/8762862/92351544-2833-40d7-9e0b-7ef7ae37ec4c" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://youtu.be/SUIZYzxtMQs" title="A drive to Taco Bell"&gt;&lt;img src="https://github.com/commaai/openpilot/assets/8762862/05ceefc5-2628-439c-a9b2-89ce77dc6f63" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Using openpilot in a car&lt;/h2&gt; 
&lt;p&gt;To use openpilot in a car, you need four things:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Supported Device:&lt;/strong&gt; a comma 3X, available at &lt;a href="https://comma.ai/shop/comma-3x"&gt;comma.ai/shop&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Software:&lt;/strong&gt; The setup procedure for the comma 3X allows users to enter a URL for custom software. Use the URL &lt;code&gt;openpilot.comma.ai&lt;/code&gt; to install the release version.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Supported Car:&lt;/strong&gt; Ensure that you have one of &lt;a href="https://raw.githubusercontent.com/commaai/openpilot/master/docs/CARS.md"&gt;the 275+ supported cars&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Car Harness:&lt;/strong&gt; You will also need a &lt;a href="https://comma.ai/shop/car-harness"&gt;car harness&lt;/a&gt; to connect your comma 3X to your car.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;We have detailed instructions for &lt;a href="https://comma.ai/setup"&gt;how to install the harness and device in a car&lt;/a&gt;. Note that it's possible to run openpilot on &lt;a href="https://blog.comma.ai/self-driving-car-for-free/"&gt;other hardware&lt;/a&gt;, although it's not plug-and-play.&lt;/p&gt; 
&lt;h3&gt;Branches&lt;/h3&gt; 
&lt;p&gt;Running &lt;code&gt;master&lt;/code&gt; and other branches directly is supported, but it's recommended to run one of the following prebuilt branches:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;comma four branch&lt;/th&gt; 
   &lt;th&gt;comma 3X branch&lt;/th&gt; 
   &lt;th&gt;URL&lt;/th&gt; 
   &lt;th&gt;description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;release-mici&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;release-tizi&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;openpilot.comma.ai&lt;/td&gt; 
   &lt;td&gt;This is openpilot's release branch.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;release-mici-staging&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;release-tizi-staging&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;openpilot-test.comma.ai&lt;/td&gt; 
   &lt;td&gt;This is the staging branch for releases. Use it to get new releases slightly early.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;nightly&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;nightly&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;openpilot-nightly.comma.ai&lt;/td&gt; 
   &lt;td&gt;This is the bleeding edge development branch. Do not expect this to be stable.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;nightly-dev&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;nightly-dev&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;installer.comma.ai/commaai/nightly-dev&lt;/td&gt; 
   &lt;td&gt;Same as nightly, but includes experimental development features for some cars.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;To start developing openpilot&lt;/h2&gt; 
&lt;p&gt;openpilot is developed by &lt;a href="https://comma.ai/"&gt;comma&lt;/a&gt; and by users like you. We welcome both pull requests and issues on &lt;a href="http://github.com/commaai/openpilot"&gt;GitHub&lt;/a&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Join the &lt;a href="https://discord.comma.ai"&gt;community Discord&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Check out &lt;a href="https://raw.githubusercontent.com/commaai/openpilot/master/docs/CONTRIBUTING.md"&gt;the contributing docs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Check out the &lt;a href="https://raw.githubusercontent.com/commaai/openpilot/master/tools/"&gt;openpilot tools&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Code documentation lives at &lt;a href="https://docs.comma.ai"&gt;https://docs.comma.ai&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Information about running openpilot lives on the &lt;a href="https://github.com/commaai/openpilot/wiki"&gt;community wiki&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Want to get paid to work on openpilot? &lt;a href="https://comma.ai/jobs#open-positions"&gt;comma is hiring&lt;/a&gt; and offers lots of &lt;a href="https://comma.ai/bounties"&gt;bounties&lt;/a&gt; for external contributors.&lt;/p&gt; 
&lt;h2&gt;Safety and Testing&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;openpilot observes &lt;a href="https://en.wikipedia.org/wiki/ISO_26262"&gt;ISO26262&lt;/a&gt; guidelines, see &lt;a href="https://raw.githubusercontent.com/commaai/openpilot/master/docs/SAFETY.md"&gt;SAFETY.md&lt;/a&gt; for more details.&lt;/li&gt; 
 &lt;li&gt;openpilot has software-in-the-loop &lt;a href="https://raw.githubusercontent.com/commaai/openpilot/master/.github/workflows/tests.yaml"&gt;tests&lt;/a&gt; that run on every commit.&lt;/li&gt; 
 &lt;li&gt;The code enforcing the safety model lives in panda and is written in C, see &lt;a href="https://github.com/commaai/panda#code-rigor"&gt;code rigor&lt;/a&gt; for more details.&lt;/li&gt; 
 &lt;li&gt;panda has software-in-the-loop &lt;a href="https://github.com/commaai/panda/tree/master/tests/safety"&gt;safety tests&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Internally, we have a hardware-in-the-loop Jenkins test suite that builds and unit tests the various processes.&lt;/li&gt; 
 &lt;li&gt;panda has additional hardware-in-the-loop &lt;a href="https://github.com/commaai/panda/raw/master/Jenkinsfile"&gt;tests&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;We run the latest openpilot in a testing closet containing 10 comma devices continuously replaying routes.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;MIT Licensed&lt;/summary&gt; 
 &lt;p&gt;openpilot is released under the MIT license. Some parts of the software are released under other licenses as specified.&lt;/p&gt; 
 &lt;p&gt;Any user of this software shall indemnify and hold harmless Comma.ai, Inc. and its directors, officers, employees, agents, stockholders, affiliates, subcontractors and customers from and against all allegations, claims, actions, suits, demands, damages, liabilities, obligations, losses, settlements, judgments, costs and expenses (including without limitation attorneys‚Äô fees and costs) which arise out of, relate to or result from any use of this software by user.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;THIS IS ALPHA QUALITY SOFTWARE FOR RESEARCH PURPOSES ONLY. THIS IS NOT A PRODUCT. YOU ARE RESPONSIBLE FOR COMPLYING WITH LOCAL LAWS AND REGULATIONS. NO WARRANTY EXPRESSED OR IMPLIED.&lt;/strong&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;User Data and comma Account&lt;/summary&gt; 
 &lt;p&gt;By default, openpilot uploads the driving data to our servers. You can also access your data through &lt;a href="https://connect.comma.ai/"&gt;comma connect&lt;/a&gt;. We use your data to train better models and improve openpilot for everyone.&lt;/p&gt; 
 &lt;p&gt;openpilot is open source software: the user is free to disable data collection if they wish to do so.&lt;/p&gt; 
 &lt;p&gt;openpilot logs the road-facing cameras, CAN, GPS, IMU, magnetometer, thermal sensors, crashes, and operating system logs. The driver-facing camera and microphone are only logged if you explicitly opt-in in settings.&lt;/p&gt; 
 &lt;p&gt;By using openpilot, you agree to &lt;a href="https://comma.ai/privacy"&gt;our Privacy Policy&lt;/a&gt;. You understand that use of this software or its related services will generate certain types of user data, which may be logged and stored at the sole discretion of comma. By accepting this agreement, you grant an irrevocable, perpetual, worldwide right to comma for the use of this data.&lt;/p&gt; 
&lt;/details&gt;</description>
    </item>
    
    <item>
      <title>browser-use/browser-use</title>
      <link>https://github.com/browser-use/browser-use</link>
      <description>&lt;p&gt;üåê Make websites accessible for AI agents. Automate tasks online with ease.&lt;/p&gt;&lt;hr&gt;&lt;picture&gt; 
 &lt;source media="(prefers-color-scheme: light)" srcset="https://github.com/user-attachments/assets/2ccdb752-22fb-41c7-8948-857fc1ad7e24" " /&gt; 
 &lt;source media="(prefers-color-scheme: dark)" srcset="https://github.com/user-attachments/assets/774a46d5-27a0-490c-b7d0-e65fcbbfa358" /&gt; 
 &lt;img alt="Shows a black Browser Use Logo in light color mode and a white one in dark color mode." src="https://github.com/user-attachments/assets/2ccdb752-22fb-41c7-8948-857fc1ad7e24" width="full" /&gt; 
&lt;/picture&gt; 
&lt;div align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://github.com/user-attachments/assets/9955dda9-ede3-4971-8ee0-91cbc3850125" " /&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://github.com/user-attachments/assets/6797d09b-8ac3-4cb9-ba07-b289e080765a" /&gt; 
  &lt;img alt="The AI browser agent." src="https://github.com/user-attachments/assets/9955dda9-ede3-4971-8ee0-91cbc3850125" width="400" /&gt; 
 &lt;/picture&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://cloud.browser-use.com"&gt;&lt;img src="https://media.browser-use.tools/badges/package" height="48" alt="Browser-Use Package Download Statistics" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://raw.githubusercontent.com/browser-use/browser-use/main/#demos"&gt;&lt;img src="https://media.browser-use.tools/badges/demos" alt="Demos" /&gt;&lt;/a&gt; 
 &lt;img width="16" height="1" alt="" /&gt; 
 &lt;a href="https://docs.browser-use.com"&gt;&lt;img src="https://media.browser-use.tools/badges/docs" alt="Docs" /&gt;&lt;/a&gt; 
 &lt;img width="16" height="1" alt="" /&gt; 
 &lt;a href="https://browser-use.com/posts"&gt;&lt;img src="https://media.browser-use.tools/badges/blog" alt="Blog" /&gt;&lt;/a&gt; 
 &lt;img width="16" height="1" alt="" /&gt; 
 &lt;a href="https://browsermerch.com"&gt;&lt;img src="https://media.browser-use.tools/badges/merch" alt="Merch" /&gt;&lt;/a&gt; 
 &lt;img width="100" height="1" alt="" /&gt; 
 &lt;a href="https://github.com/browser-use/browser-use"&gt;&lt;img src="https://media.browser-use.tools/badges/github" alt="Github Stars" /&gt;&lt;/a&gt; 
 &lt;img width="4" height="1" alt="" /&gt; 
 &lt;a href="https://x.com/intent/user?screen_name=browser_use"&gt;&lt;img src="https://media.browser-use.tools/badges/twitter" alt="Twitter" /&gt;&lt;/a&gt; 
 &lt;img width="4 height=" 1" alt="" /&gt; 
 &lt;a href="https://link.browser-use.com/discord"&gt;&lt;img src="https://media.browser-use.tools/badges/discord" alt="Discord" /&gt;&lt;/a&gt; 
 &lt;img width="4" height="1" alt="" /&gt; 
 &lt;a href="https://cloud.browser-use.com"&gt;&lt;img src="https://media.browser-use.tools/badges/cloud" height="48" alt="Browser-Use Cloud" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;p&gt;üå§Ô∏è Want to skip the setup? Use our &lt;b&gt;&lt;a href="https://cloud.browser-use.com"&gt;cloud&lt;/a&gt;&lt;/b&gt; for faster, scalable, stealth-enabled browser automation!&lt;/p&gt; 
&lt;h1&gt;ü§ñ LLM Quickstart&lt;/h1&gt; 
&lt;ol&gt; 
 &lt;li&gt;Direct your favorite coding agent (Cursor, Claude Code, etc) to &lt;a href="https://docs.browser-use.com/llms-full.txt"&gt;Agents.md&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Prompt away!&lt;/li&gt; 
&lt;/ol&gt; 
&lt;br /&gt; 
&lt;h1&gt;üëã Human Quickstart&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;1. Create environment with &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt; (Python&amp;gt;=3.11):&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv init
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;2. Install Browser-Use package:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;#  We ship every day - use the latest version!
uv add browser-use
uv sync
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;3. Get your API key from &lt;a href="https://cloud.browser-use.com/new-api-key"&gt;Browser Use Cloud&lt;/a&gt; and add it to your &lt;code&gt;.env&lt;/code&gt; file (new signups get $10 free credits):&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;# .env
BROWSER_USE_API_KEY=your-key
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;4. Install Chromium browser:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uvx browser-use install
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;5. Run your first agent:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from browser_use import Agent, Browser, ChatBrowserUse
import asyncio

async def example():
    browser = Browser(
        # use_cloud=True,  # Uncomment to use a stealth browser on Browser Use Cloud
    )

    llm = ChatBrowserUse()

    agent = Agent(
        task="Find the number of stars of the browser-use repo",
        llm=llm,
        browser=browser,
    )

    history = await agent.run()
    return history

if __name__ == "__main__":
    history = asyncio.run(example())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Check out the &lt;a href="https://docs.browser-use.com"&gt;library docs&lt;/a&gt; and the &lt;a href="https://docs.cloud.browser-use.com"&gt;cloud docs&lt;/a&gt; for more!&lt;/p&gt; 
&lt;br /&gt; 
&lt;h1&gt;üî• Deploy on Sandboxes&lt;/h1&gt; 
&lt;p&gt;We handle agents, browsers, persistence, auth, cookies, and LLMs. The agent runs right next to the browser for minimal latency.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from browser_use import Browser, sandbox, ChatBrowserUse
from browser_use.agent.service import Agent
import asyncio

@sandbox()
async def my_task(browser: Browser):
    agent = Agent(task="Find the top HN post", browser=browser, llm=ChatBrowserUse())
    await agent.run()

# Just call it like any async function
asyncio.run(my_task())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href="https://docs.browser-use.com/production"&gt;Going to Production&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;br /&gt; 
&lt;h1&gt;üöÄ Template Quickstart&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;Want to get started even faster?&lt;/strong&gt; Generate a ready-to-run template:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uvx browser-use init --template default
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This creates a &lt;code&gt;browser_use_default.py&lt;/code&gt; file with a working example. Available templates:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;default&lt;/code&gt; - Minimal setup to get started quickly&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;advanced&lt;/code&gt; - All configuration options with detailed comments&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;tools&lt;/code&gt; - Examples of custom tools and extending the agent&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can also specify a custom output path:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uvx browser-use init --template default --output my_agent.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;br /&gt; 
&lt;h1&gt;üíª CLI&lt;/h1&gt; 
&lt;p&gt;Fast, persistent browser automation from the command line:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;browser-use open https://example.com    # Navigate to URL
browser-use state                       # See clickable elements
browser-use click 5                     # Click element by index
browser-use type "Hello"                # Type text
browser-use screenshot page.png         # Take screenshot
browser-use close                       # Close browser
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The CLI keeps the browser running between commands for fast iteration. See &lt;a href="https://raw.githubusercontent.com/browser-use/browser-use/main/browser_use/skill_cli/README.md"&gt;CLI docs&lt;/a&gt; for all commands.&lt;/p&gt; 
&lt;h3&gt;Claude Code Skill&lt;/h3&gt; 
&lt;p&gt;For &lt;a href="https://claude.ai/code"&gt;Claude Code&lt;/a&gt;, install the skill to enable AI-assisted browser automation:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mkdir -p ~/.claude/skills/browser-use
curl -o ~/.claude/skills/browser-use/SKILL.md \
  https://raw.githubusercontent.com/browser-use/browser-use/main/skills/browser-use/SKILL.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;br /&gt; 
&lt;h1&gt;Demos&lt;/h1&gt; 
&lt;h3&gt;üìã Form-Filling&lt;/h3&gt; 
&lt;h4&gt;Task = "Fill in this job application with my resume and information."&lt;/h4&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/57865ee6-6004-49d5-b2c2-6dff39ec2ba9" alt="Job Application Demo" /&gt; &lt;a href="https://github.com/browser-use/browser-use/raw/main/examples/use-cases/apply_to_job.py"&gt;Example code ‚Üó&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;üçé Grocery-Shopping&lt;/h3&gt; 
&lt;h4&gt;Task = "Put this list of items into my instacart."&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/a6813fa7-4a7c-40a6-b4aa-382bf88b1850"&gt;https://github.com/user-attachments/assets/a6813fa7-4a7c-40a6-b4aa-382bf88b1850&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/browser-use/browser-use/raw/main/examples/use-cases/buy_groceries.py"&gt;Example code ‚Üó&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;üíª Personal-Assistant.&lt;/h3&gt; 
&lt;h4&gt;Task = "Help me find parts for a custom PC."&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/ac34f75c-057a-43ef-ad06-5b2c9d42bf06"&gt;https://github.com/user-attachments/assets/ac34f75c-057a-43ef-ad06-5b2c9d42bf06&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/browser-use/browser-use/raw/main/examples/use-cases/pcpartpicker.py"&gt;Example code ‚Üó&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;üí°See &lt;a href="https://docs.browser-use.com/examples"&gt;more examples here ‚Üó&lt;/a&gt; and give us a star!&lt;/h3&gt; 
&lt;br /&gt; 
&lt;h2&gt;Integrations, hosting, custom tools, MCP, and more on our &lt;a href="https://docs.browser-use.com"&gt;Docs ‚Üó&lt;/a&gt;&lt;/h2&gt; 
&lt;br /&gt; 
&lt;h1&gt;FAQ&lt;/h1&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;What's the best model to use?&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;We optimized &lt;strong&gt;ChatBrowserUse()&lt;/strong&gt; specifically for browser automation tasks. On avg it completes tasks 3-5x faster than other models with SOTA accuracy.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Pricing (per 1M tokens):&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Input tokens: $0.20&lt;/li&gt; 
  &lt;li&gt;Cached input tokens: $0.02&lt;/li&gt; 
  &lt;li&gt;Output tokens: $2.00&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;For other LLM providers, see our &lt;a href="https://docs.browser-use.com/supported-models"&gt;supported models documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Can I use custom tools with the agent?&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;Yes! You can add custom tools to extend the agent's capabilities:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from browser_use import Tools

tools = Tools()

@tools.action(description='Description of what this tool does.')
def custom_tool(param: str) -&amp;gt; str:
    return f"Result: {param}"

agent = Agent(
    task="Your task",
    llm=llm,
    browser=browser,
    tools=tools,
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Can I use this for free?&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;Yes! Browser-Use is open source and free to use. You only need to choose an LLM provider (like OpenAI, Google, ChatBrowserUse, or run local models with Ollama).&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;How do I handle authentication?&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;Check out our authentication examples:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://github.com/browser-use/browser-use/raw/main/examples/browser/real_browser.py"&gt;Using real browser profiles&lt;/a&gt; - Reuse your existing Chrome profile with saved logins&lt;/li&gt; 
  &lt;li&gt;If you want to use temporary accounts with inbox, choose AgentMail&lt;/li&gt; 
  &lt;li&gt;To sync your auth profile with the remote browser, run &lt;code&gt;curl -fsSL https://browser-use.com/profile.sh | BROWSER_USE_API_KEY=XXXX sh&lt;/code&gt; (replace XXXX with your API key)&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;These examples show how to maintain sessions and handle authentication seamlessly.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;How do I solve CAPTCHAs?&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;For CAPTCHA handling, you need better browser fingerprinting and proxies. Use &lt;a href="https://cloud.browser-use.com"&gt;Browser Use Cloud&lt;/a&gt; which provides stealth browsers designed to avoid detection and CAPTCHA challenges.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;How do I go into production?&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;Chrome can consume a lot of memory, and running many agents in parallel can be tricky to manage.&lt;/p&gt; 
 &lt;p&gt;For production use cases, use our &lt;a href="https://cloud.browser-use.com"&gt;Browser Use Cloud API&lt;/a&gt; which handles:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Scalable browser infrastructure&lt;/li&gt; 
  &lt;li&gt;Memory management&lt;/li&gt; 
  &lt;li&gt;Proxy rotation&lt;/li&gt; 
  &lt;li&gt;Stealth browser fingerprinting&lt;/li&gt; 
  &lt;li&gt;High-performance parallel execution&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;strong&gt;Tell your computer what to do, and it gets it done.&lt;/strong&gt;&lt;/p&gt; 
 &lt;img src="https://github.com/user-attachments/assets/06fa3078-8461-4560-b434-445510c1766f" width="400" /&gt; 
 &lt;p&gt;&lt;a href="https://x.com/intent/user?screen_name=mamagnus00"&gt;&lt;img src="https://img.shields.io/twitter/follow/Magnus?style=social" alt="Twitter Follow" /&gt;&lt;/a&gt; ‚ÄÉ‚ÄÉ‚ÄÉ &lt;a href="https://x.com/intent/user?screen_name=gregpr07"&gt;&lt;img src="https://img.shields.io/twitter/follow/Gregor?style=social" alt="Twitter Follow" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt;
  Made with ‚ù§Ô∏è in Zurich and San Francisco 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>business-science/ai-data-science-team</title>
      <link>https://github.com/business-science/ai-data-science-team</link>
      <description>&lt;p&gt;An AI-powered data science team of agents to help you perform common data science tasks 10X faster.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;a href="https://github.com/business-science/ai-data-science-team"&gt; 
  &lt;picture&gt; 
   &lt;img src="https://raw.githubusercontent.com/business-science/ai-data-science-team/master/img/ai_data_science_logo.png" alt="AI Data Science Team" width="360" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;em&gt;AI Data Science Team + AI Pipeline Studio&lt;/em&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://pypi.python.org/pypi/ai-data-science-team"&gt;&lt;img src="https://img.shields.io/pypi/v/ai-data-science-team.svg?style=for-the-badge" alt="PyPI" /&gt;&lt;/a&gt; 
 &lt;a href="https://github.com/business-science/ai-data-science-team"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/ai-data-science-team.svg?style=for-the-badge" alt="versions" /&gt;&lt;/a&gt; 
 &lt;a href="https://github.com/business-science/ai-data-science-team/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/business-science/ai-data-science-team.svg?style=for-the-badge" alt="license" /&gt;&lt;/a&gt; 
 &lt;img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/business-science/ai-data-science-team?style=for-the-badge" /&gt; 
&lt;/div&gt; 
&lt;h1&gt;AI Data Science Team&lt;/h1&gt; 
&lt;p&gt;AI Data Science Team is a Python library of specialized agents for common data science workflows, plus a flagship app: &lt;strong&gt;AI Pipeline Studio&lt;/strong&gt;. The Studio turns your work into a visual, reproducible pipeline, while the AI team handles data loading, cleaning, visualization, and modeling.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Status:&lt;/strong&gt; Beta. Breaking changes may occur until 0.1.0.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/business-science/ai-data-science-team"&gt;&lt;strong&gt;Please ‚≠ê us on GitHub (it takes 2 seconds and means a lot).&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;AI Pipeline Studio (Flagship App)&lt;/h2&gt; 
&lt;p&gt;AI Pipeline Studio is the main example of the AI Data Science Team in action.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/business-science/ai-data-science-team/master/img/apps/ai_pipeline_studio_app.jpg" alt="AI Pipeline Studio" /&gt;&lt;/p&gt; 
&lt;p&gt;Highlights:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Pipeline-first workspace: Visual Editor, Table, Chart, EDA, Code, Model, Predictions, MLflow&lt;/li&gt; 
 &lt;li&gt;Manual + AI steps with lineage and reproducible scripts&lt;/li&gt; 
 &lt;li&gt;Multi-dataset handling and merge workflows&lt;/li&gt; 
 &lt;li&gt;Project saves: metadata-only or full-data&lt;/li&gt; 
 &lt;li&gt;Storage footprint controls and rehydrate workflows&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Run it:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;streamlit run apps/ai-pipeline-studio-app/app.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Full app docs: &lt;code&gt;apps/ai-pipeline-studio-app/README.md&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;h3&gt;Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.10+&lt;/li&gt; 
 &lt;li&gt;OpenAI API key (or Ollama for local models)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Install the app and library&lt;/h3&gt; 
&lt;p&gt;Clone the repo and install in editable mode:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Run the AI Pipeline Studio app&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;streamlit run apps/ai-pipeline-studio-app/app.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Library Overview&lt;/h2&gt; 
&lt;p&gt;The repository includes both the &lt;strong&gt;AI Pipeline Studio&lt;/strong&gt; app and the underlying &lt;strong&gt;AI Data Science Team&lt;/strong&gt; library. The library provides agent building blocks and multi-agent workflows for:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Data loading and inspection&lt;/li&gt; 
 &lt;li&gt;Cleaning, wrangling, and feature engineering&lt;/li&gt; 
 &lt;li&gt;Visualization and EDA&lt;/li&gt; 
 &lt;li&gt;Modeling and evaluation (H2O + MLflow tools)&lt;/li&gt; 
 &lt;li&gt;SQL database interaction&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Agents (Snapshot)&lt;/h3&gt; 
&lt;p&gt;Agent examples live in &lt;code&gt;examples/&lt;/code&gt;. Notable agents:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Data Loader Tools Agent&lt;/li&gt; 
 &lt;li&gt;Data Wrangling Agent&lt;/li&gt; 
 &lt;li&gt;Data Cleaning Agent&lt;/li&gt; 
 &lt;li&gt;Data Visualization Agent&lt;/li&gt; 
 &lt;li&gt;EDA Tools Agent&lt;/li&gt; 
 &lt;li&gt;Feature Engineering Agent&lt;/li&gt; 
 &lt;li&gt;SQL Database Agent&lt;/li&gt; 
 &lt;li&gt;H2O ML Agent&lt;/li&gt; 
 &lt;li&gt;MLflow Tools Agent&lt;/li&gt; 
 &lt;li&gt;Multi-agent workflows (e.g., Pandas Data Analyst, SQL Data Analyst)&lt;/li&gt; 
 &lt;li&gt;Supervisor Agent (oversees other agents)&lt;/li&gt; 
 &lt;li&gt;Custom tools for data science tasks&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Apps&lt;/h2&gt; 
&lt;p&gt;See all apps in &lt;code&gt;apps/&lt;/code&gt;. Notable apps:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;AI Pipeline Studio: &lt;code&gt;apps/ai-pipeline-studio-app/&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;EDA Explorer App: &lt;code&gt;apps/exploratory-copilot-app/&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Pandas Data Analyst App: &lt;code&gt;apps/pandas-data-analyst-app/&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Use OpenAI&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from langchain_openai import ChatOpenAI
llm = ChatOpenAI(
    model_name="gpt-4.1-mini",
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Use Ollama (Local LLM)&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;ollama serve
ollama pull llama3.1:8b
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from langchain_ollama import ChatOllama

llm = ChatOllama(
    model="llama3.1:8b",
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Next-Gen AI Agentic Workshop&lt;/h2&gt; 
&lt;p&gt;Want to learn how to build AI agents and AI apps for real data science workflows? Join my next‚Äëgen AI workshop: &lt;a href="https://learn.business-science.io/ai-register"&gt;https://learn.business-science.io/ai-register&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>PromtEngineer/localGPT</title>
      <link>https://github.com/PromtEngineer/localGPT</link>
      <description>&lt;p&gt;Chat with your documents on your local device using GPT models. No data leaves your device and 100% private.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LocalGPT - Private Document Intelligence Platform&lt;/h1&gt; 
&lt;div align="center"&gt; 
 &lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/2947" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/2947" alt="PromtEngineer%2FlocalGPT | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/PromtEngineer/localGPT/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/PromtEngineer/localGPT?style=flat-square" alt="GitHub Stars" /&gt;&lt;/a&gt; &lt;a href="https://github.com/PromtEngineer/localGPT/network/members"&gt;&lt;img src="https://img.shields.io/github/forks/PromtEngineer/localGPT?style=flat-square" alt="GitHub Forks" /&gt;&lt;/a&gt; &lt;a href="https://github.com/PromtEngineer/localGPT/issues"&gt;&lt;img src="https://img.shields.io/github/issues/PromtEngineer/localGPT?style=flat-square" alt="GitHub Issues" /&gt;&lt;/a&gt; &lt;a href="https://github.com/PromtEngineer/localGPT/pulls"&gt;&lt;img src="https://img.shields.io/github/issues-pr/PromtEngineer/localGPT?style=flat-square" alt="GitHub Pull Requests" /&gt;&lt;/a&gt; &lt;a href="https://www.python.org/downloads/"&gt;&lt;img src="https://img.shields.io/badge/python-3.8+-blue.svg?style=flat-square" alt="Python 3.8+" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/PromtEngineer/localGPT/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-MIT-green.svg?style=flat-square" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://www.docker.com/"&gt;&lt;img src="https://img.shields.io/badge/docker-supported-blue.svg?style=flat-square" alt="Docker" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;a href="https://x.com/engineerrprompt"&gt; &lt;img src="https://img.shields.io/badge/Follow%20on%20X-000000?style=for-the-badge&amp;amp;logo=x&amp;amp;logoColor=white" alt="Follow on X" /&gt; &lt;/a&gt; &lt;a href="https://discord.gg/tUDWAFGc"&gt; &lt;img src="https://img.shields.io/badge/Join%20our%20Discord-5865F2?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Join our Discord" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;üöÄ What is LocalGPT?&lt;/h2&gt; 
&lt;p&gt;LocalGPT is a &lt;strong&gt;fully private, on-premise Document Intelligence platform&lt;/strong&gt;. Ask questions, summarise, and uncover insights from your files with state-of-the-art AI‚Äîno data ever leaves your machine.&lt;/p&gt; 
&lt;p&gt;More than a traditional RAG (Retrieval-Augmented Generation) tool, LocalGPT features a &lt;strong&gt;hybrid search engine&lt;/strong&gt; that blends semantic similarity, keyword matching, and &lt;a href="https://jina.ai/news/late-chunking-in-long-context-embedding-models/"&gt;Late Chunking&lt;/a&gt; for long-context precision. A &lt;strong&gt;smart router&lt;/strong&gt; automatically selects between RAG and direct LLM answering for every query, while &lt;strong&gt;contextual enrichment&lt;/strong&gt; and sentence-level &lt;a href="https://huggingface.co/naver/provence-reranker-debertav3-v1"&gt;Context Pruning&lt;/a&gt; surface only the most relevant content. An independent &lt;strong&gt;verification&lt;/strong&gt; pass adds an extra layer of accuracy.&lt;/p&gt; 
&lt;p&gt;The architecture is &lt;strong&gt;modular and lightweight&lt;/strong&gt;‚Äîenable only the components you need. With a pure-Python core and minimal dependencies, LocalGPT is simple to deploy, run, and maintain on any infrastructure.The system has minimal dependencies on frameworks and libraries, making it easy to deploy and maintain. The RAG system is pure python and does not require any additional dependencies.&lt;/p&gt; 
&lt;h2&gt;‚ñ∂Ô∏è Video&lt;/h2&gt; 
&lt;p&gt;Watch this &lt;a href="https://youtu.be/JTbtGH3secI"&gt;video&lt;/a&gt; to get started with LocalGPT.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Home&lt;/th&gt; 
   &lt;th&gt;Create Index&lt;/th&gt; 
   &lt;th&gt;Chat&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/PromtEngineer/localGPT/main/Documentation/images/Home.png" alt="" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/PromtEngineer/localGPT/main/Documentation/images/Index%20Creation.png" alt="" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/PromtEngineer/localGPT/main/Documentation/images/Retrieval%20Process.png" alt="" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;‚ú® Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Utmost Privacy&lt;/strong&gt;: Your data remains on your computer, ensuring 100% security.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Versatile Model Support&lt;/strong&gt;: Seamlessly integrate a variety of open-source models via Ollama.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Diverse Embeddings&lt;/strong&gt;: Choose from a range of open-source embeddings.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Reuse Your LLM&lt;/strong&gt;: Once downloaded, reuse your LLM without the need for repeated downloads.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Chat History&lt;/strong&gt;: Remembers your previous conversations (in a session).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;API&lt;/strong&gt;: LocalGPT has an API that you can use for building RAG Applications.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;GPU, CPU, HPU &amp;amp; MPS Support&lt;/strong&gt;: Supports multiple platforms out of the box, Chat with your data using &lt;code&gt;CUDA&lt;/code&gt;, &lt;code&gt;CPU&lt;/code&gt;, &lt;code&gt;HPU (Intel¬Æ Gaudi¬Æ)&lt;/code&gt; or &lt;code&gt;MPS&lt;/code&gt; and more!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üìñ Document Processing&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-format Support&lt;/strong&gt;: PDF, DOCX, TXT, Markdown, and more (Currently only PDF is supported)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Contextual Enrichment&lt;/strong&gt;: Enhanced document understanding with AI-generated context, inspired by &lt;a href="https://www.anthropic.com/news/contextual-retrieval"&gt;Contextual Retrieval&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Batch Processing&lt;/strong&gt;: Handle multiple documents simultaneously&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ü§ñ AI-Powered Chat&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Natural Language Queries&lt;/strong&gt;: Ask questions in plain English&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Source Attribution&lt;/strong&gt;: Every answer includes document references&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Smart Routing&lt;/strong&gt;: Automatically chooses between RAG and direct LLM responses&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Query Decomposition&lt;/strong&gt;: Breaks complex queries into sub-questions for better answers&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Semantic Caching&lt;/strong&gt;: TTL-based caching with similarity matching for faster responses&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Session-Aware History&lt;/strong&gt;: Maintains conversation context across interactions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Answer Verification&lt;/strong&gt;: Independent verification pass for accuracy&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multiple AI Models&lt;/strong&gt;: Ollama for inference, HuggingFace for embeddings and reranking&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üõ†Ô∏è Developer-Friendly&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;RESTful APIs&lt;/strong&gt;: Complete API access for integration&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Real-time Progress&lt;/strong&gt;: Live updates during document processing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible Configuration&lt;/strong&gt;: Customize models, chunk sizes, and search parameters&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Extensible Architecture&lt;/strong&gt;: Plugin system for custom components&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üé® Modern Interface&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Intuitive Web UI&lt;/strong&gt;: Clean, responsive design&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Session Management&lt;/strong&gt;: Organize conversations by topic&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Index Management&lt;/strong&gt;: Easy document collection management&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Real-time Chat&lt;/strong&gt;: Streaming responses for immediate feedback&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üöÄ Quick Start&lt;/h2&gt; 
&lt;p&gt;Note: The installation is currently only tested on macOS.&lt;/p&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.8 or higher (tested with Python 3.11.5)&lt;/li&gt; 
 &lt;li&gt;Node.js 16+ and npm (tested with Node.js 23.10.0, npm 10.9.2)&lt;/li&gt; 
 &lt;li&gt;Docker (optional, for containerized deployment)&lt;/li&gt; 
 &lt;li&gt;8GB+ RAM (16GB+ recommended)&lt;/li&gt; 
 &lt;li&gt;Ollama (required for both deployment approaches)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;em&gt;&lt;strong&gt;NOTE&lt;/strong&gt;&lt;/em&gt;&lt;/h3&gt; 
&lt;p&gt;Before this brach is moved to the main branch, please clone this branch for instalation:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone -b localgpt-v2 https://github.com/PromtEngineer/localGPT.git
cd localGPT
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Option 1: Docker Deployment&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone the repository
git clone https://github.com/PromtEngineer/localGPT.git
cd localGPT

# Install Ollama locally (required even for Docker)
curl -fsSL https://ollama.ai/install.sh | sh
ollama pull qwen3:0.6b
ollama pull qwen3:8b

# Start Ollama
ollama serve

# Start with Docker (in a new terminal)
./start-docker.sh

# Access the application
open http://localhost:3000
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Docker Management Commands:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Check container status
docker compose ps

# View logs
docker compose logs -f

# Stop containers
./start-docker.sh stop
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Option 2: Direct Development (Recommended for Development)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone the repository
git clone https://github.com/PromtEngineer/localGPT.git
cd localGPT

# Install Python dependencies
pip install -r requirements.txt

# Key dependencies installed:
# - torch==2.4.1, transformers==4.51.0 (AI models)
# - lancedb (vector database)
# - rank_bm25, fuzzywuzzy (search algorithms)
# - sentence_transformers, rerankers (embedding/reranking)
# - docling (document processing)
# - colpali-engine (multimodal processing - support coming soon)

# Install Node.js dependencies
npm install

# Install and start Ollama
curl -fsSL https://ollama.ai/install.sh | sh
ollama pull qwen3:0.6b
ollama pull qwen3:8b
ollama serve

# Start the system (in a new terminal)
python run_system.py

# Access the application
open http://localhost:3000
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;System Management:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Check system health (comprehensive diagnostics)
python system_health_check.py

# Check service status and health
python run_system.py --health

# Start in production mode
python run_system.py --mode prod

# Skip frontend (backend + RAG API only)
python run_system.py --no-frontend

# View aggregated logs
python run_system.py --logs-only

# Stop all services
python run_system.py --stop
# Or press Ctrl+C in the terminal running python run_system.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Service Architecture:&lt;/strong&gt; The &lt;code&gt;run_system.py&lt;/code&gt; launcher manages four key services:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Ollama Server&lt;/strong&gt; (port 11434): AI model serving&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;RAG API Server&lt;/strong&gt; (port 8001): Document processing and retrieval&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Backend Server&lt;/strong&gt; (port 8000): Session management and API endpoints&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Frontend Server&lt;/strong&gt; (port 3000): React/Next.js web interface&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Option 3: Manual Component Startup&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Terminal 1: Start Ollama
ollama serve

# Terminal 2: Start RAG API
python -m rag_system.api_server

# Terminal 3: Start Backend
cd backend &amp;amp;&amp;amp; python server.py

# Terminal 4: Start Frontend
npm run dev

# Access at http://localhost:3000
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Detailed Installation&lt;/h3&gt; 
&lt;h4&gt;1. Install System Dependencies&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;Ubuntu/Debian:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt update
sudo apt install python3.8 python3-pip nodejs npm docker.io docker-compose
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;macOS:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;brew install python@3.8 node npm docker docker-compose
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Windows:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install Python 3.8+, Node.js, and Docker Desktop
# Then use PowerShell or WSL2
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;2. Install AI Models&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;Install Ollama (Recommended):&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Pull recommended models
ollama pull qwen3:0.6b          # Fast generation model
ollama pull qwen3:8b            # High-quality generation model
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;3. Configure Environment&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Copy environment template
cp .env.example .env

# Edit configuration
nano .env
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Key Configuration Options:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-env"&gt;# AI Models (referenced in rag_system/main.py)
OLLAMA_HOST=http://localhost:11434

# Database Paths (used by backend and RAG system)
DATABASE_PATH=./backend/chat_data.db
VECTOR_DB_PATH=./lancedb

# Server Settings (used by run_system.py)
BACKEND_PORT=8000
FRONTEND_PORT=3000
RAG_API_PORT=8001

# Optional: Override default models
GENERATION_MODEL=qwen3:8b
ENRICHMENT_MODEL=qwen3:0.6b
EMBEDDING_MODEL=Qwen/Qwen3-Embedding-0.6B
RERANKER_MODEL=answerdotai/answerai-colbert-small-v1
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;4. Initialize the System&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run system health check
python system_health_check.py

# Initialize databases
python -c "from backend.database import ChatDatabase; ChatDatabase().init_database()"

# Test installation
python -c "from rag_system.main import get_agent; print('‚úÖ Installation successful!')"

# Validate complete setup
python run_system.py --health
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üéØ Getting Started&lt;/h2&gt; 
&lt;h3&gt;1. Create Your First Index&lt;/h3&gt; 
&lt;p&gt;An &lt;strong&gt;index&lt;/strong&gt; is a collection of processed documents that you can chat with.&lt;/p&gt; 
&lt;h4&gt;Using the Web Interface:&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Open &lt;a href="http://localhost:3000"&gt;http://localhost:3000&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Click "Create New Index"&lt;/li&gt; 
 &lt;li&gt;Upload your documents (PDF, DOCX, TXT)&lt;/li&gt; 
 &lt;li&gt;Configure processing options&lt;/li&gt; 
 &lt;li&gt;Click "Build Index"&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Using Scripts:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Simple script approach
./simple_create_index.sh "My Documents" "path/to/document.pdf"

# Interactive script
python create_index_script.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Using API:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Create index
curl -X POST http://localhost:8000/indexes \
  -H "Content-Type: application/json" \
  -d '{"name": "My Index", "description": "My documents"}'

# Upload documents
curl -X POST http://localhost:8000/indexes/INDEX_ID/upload \
  -F "files=@document.pdf"

# Build index
curl -X POST http://localhost:8000/indexes/INDEX_ID/build
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Start Chatting&lt;/h3&gt; 
&lt;p&gt;Once your index is built:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Create a Chat Session&lt;/strong&gt;: Click "New Chat" or use an existing session&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Select Your Index&lt;/strong&gt;: Choose which document collection to query&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Ask Questions&lt;/strong&gt;: Type natural language questions about your documents&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Get Answers&lt;/strong&gt;: Receive AI-generated responses with source citations&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;3. Advanced Features&lt;/h3&gt; 
&lt;h4&gt;Custom Model Configuration&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Use different models for different tasks
curl -X POST http://localhost:8000/sessions \
  -H "Content-Type: application/json" \
  -d '{
    "title": "High Quality Session",
    "model": "qwen3:8b",
    "embedding_model": "Qwen/Qwen3-Embedding-4B"
  }'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Batch Document Processing&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Process multiple documents at once
python demo_batch_indexing.py --config batch_indexing_config.json
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;API Integration&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import requests

# Chat with your documents via API
response = requests.post('http://localhost:8000/chat', json={
    'query': 'What are the key findings in the research papers?',
    'session_id': 'your-session-id',
    'search_type': 'hybrid',
    'retrieval_k': 20
})

print(response.json()['response'])
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üîß Configuration&lt;/h2&gt; 
&lt;h3&gt;Model Configuration&lt;/h3&gt; 
&lt;p&gt;LocalGPT supports multiple AI model providers with centralized configuration:&lt;/p&gt; 
&lt;h4&gt;Ollama Models (Local Inference)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;OLLAMA_CONFIG = {
    "host": "http://localhost:11434",
    "generation_model": "qwen3:8b",        # Main text generation
    "enrichment_model": "qwen3:0.6b"       # Lightweight routing/enrichment
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;External Models (HuggingFace Direct)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;EXTERNAL_MODELS = {
    "embedding_model": "Qwen/Qwen3-Embedding-0.6B",           # 1024 dimensions
    "reranker_model": "answerdotai/answerai-colbert-small-v1", # ColBERT reranker
    "fallback_reranker": "BAAI/bge-reranker-base"             # Backup reranker
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Pipeline Configuration&lt;/h3&gt; 
&lt;p&gt;LocalGPT offers two main pipeline configurations:&lt;/p&gt; 
&lt;h4&gt;Default Pipeline (Production-Ready)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;"default": {
    "description": "Production-ready pipeline with hybrid search, AI reranking, and verification",
    "storage": {
        "lancedb_uri": "./lancedb",
        "text_table_name": "text_pages_v3",
        "bm25_path": "./index_store/bm25"
    },
    "retrieval": {
        "retriever": "multivector",
        "search_type": "hybrid",
        "late_chunking": {"enabled": True},
        "dense": {"enabled": True, "weight": 0.7},
        "bm25": {"enabled": True}
    },
    "reranker": {
        "enabled": True,
        "type": "ai",
        "strategy": "rerankers-lib",
        "model_name": "answerdotai/answerai-colbert-small-v1",
        "top_k": 10
    },
    "query_decomposition": {"enabled": True, "max_sub_queries": 3},
    "verification": {"enabled": True},
    "retrieval_k": 20,
    "contextual_enricher": {"enabled": True, "window_size": 1}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Fast Pipeline (Speed-Optimized)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;"fast": {
    "description": "Speed-optimized pipeline with minimal overhead",
    "retrieval": {
        "search_type": "vector_only",
        "late_chunking": {"enabled": False}
    },
    "reranker": {"enabled": False},
    "query_decomposition": {"enabled": False},
    "verification": {"enabled": False},
    "retrieval_k": 10,
    "contextual_enricher": {"enabled": False}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Search Configuration&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;SEARCH_CONFIG = {
    'hybrid': {
        'dense_weight': 0.7,
        'sparse_weight': 0.3,
        'retrieval_k': 20,
        'reranker_top_k': 10
    }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üõ†Ô∏è Troubleshooting&lt;/h2&gt; 
&lt;h3&gt;Common Issues&lt;/h3&gt; 
&lt;h4&gt;Installation Problems&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Check Python version
python --version  # Should be 3.8+

# Check dependencies
pip list | grep -E "(torch|transformers|lancedb)"

# Reinstall dependencies
pip install -r requirements.txt --force-reinstall
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Model Loading Issues&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Check Ollama status
ollama list
curl http://localhost:11434/api/tags

# Pull missing models
ollama pull qwen3:0.6b
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Database Issues&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Check database connectivity
python -c "from backend.database import ChatDatabase; db = ChatDatabase(); print('‚úÖ Database OK')"

# Reset database (WARNING: This deletes all data)
rm backend/chat_data.db
python -c "from backend.database import ChatDatabase; ChatDatabase().init_database()"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Performance Issues&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Check system resources
python system_health_check.py

# Monitor memory usage
htop  # or Task Manager on Windows

# Optimize for low-memory systems
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Getting Help&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Check Logs&lt;/strong&gt;: The system creates structured logs in the &lt;code&gt;logs/&lt;/code&gt; directory:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;logs/system.log&lt;/code&gt;: Main system events and errors&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;logs/ollama.log&lt;/code&gt;: Ollama server logs&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;logs/rag-api.log&lt;/code&gt;: RAG API processing logs&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;logs/backend.log&lt;/code&gt;: Backend server logs&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;logs/frontend.log&lt;/code&gt;: Frontend build and runtime logs&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;System Health&lt;/strong&gt;: Run comprehensive diagnostics:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;python system_health_check.py  # Full system diagnostics
python run_system.py --health  # Service status check
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Health Endpoints&lt;/strong&gt;: Check individual service health:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Backend: &lt;code&gt;http://localhost:8000/health&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;RAG API: &lt;code&gt;http://localhost:8001/health&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Ollama: &lt;code&gt;http://localhost:11434/api/tags&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Documentation&lt;/strong&gt;: Check the &lt;a href="https://raw.githubusercontent.com/PromtEngineer/localGPT/main/TECHNICAL_DOCS.md"&gt;Technical Documentation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;GitHub Issues&lt;/strong&gt;: Report bugs and request features&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Community&lt;/strong&gt;: Join our Discord/Slack community&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üîó API Reference&lt;/h2&gt; 
&lt;h3&gt;Core Endpoints&lt;/h3&gt; 
&lt;h4&gt;Chat API&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-http"&gt;# Session-based chat (recommended)
POST /sessions/{session_id}/chat
Content-Type: application/json

{
  "query": "What are the main topics discussed?",
  "search_type": "hybrid",
  "retrieval_k": 20,
  "ai_rerank": true,
  "context_window_size": 5
}

# Legacy chat endpoint
POST /chat
Content-Type: application/json

{
  "query": "What are the main topics discussed?",
  "session_id": "uuid",
  "search_type": "hybrid",
  "retrieval_k": 20
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Index Management&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-http"&gt;# Create index
POST /indexes
Content-Type: application/json
{
  "name": "My Index",
  "description": "Description",
  "config": "default"
}

# Get all indexes
GET /indexes

# Get specific index
GET /indexes/{id}

# Upload documents to index
POST /indexes/{id}/upload
Content-Type: multipart/form-data
files: [file1.pdf, file2.pdf, ...]

# Build index (process uploaded documents)
POST /indexes/{id}/build
Content-Type: application/json
{
  "config_mode": "default",
  "enable_enrich": true,
  "chunk_size": 512
}

# Delete index
DELETE /indexes/{id}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Session Management&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-http"&gt;# Create session
POST /sessions
Content-Type: application/json
{
  "title": "My Session",
  "model": "qwen3:0.6b"
}

# Get all sessions
GET /sessions

# Get specific session
GET /sessions/{session_id}

# Get session documents
GET /sessions/{session_id}/documents

# Get session indexes
GET /sessions/{session_id}/indexes

# Link index to session
POST /sessions/{session_id}/indexes/{index_id}

# Delete session
DELETE /sessions/{session_id}

# Rename session
POST /sessions/{session_id}/rename
Content-Type: application/json
{
  "new_title": "Updated Session Name"
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Advanced Features&lt;/h3&gt; 
&lt;h4&gt;Query Decomposition&lt;/h4&gt; 
&lt;p&gt;The system can break complex queries into sub-questions for better answers:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-http"&gt;POST /sessions/{session_id}/chat
Content-Type: application/json

{
  "query": "Compare the methodologies and analyze their effectiveness",
  "query_decompose": true,
  "compose_sub_answers": true
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Answer Verification&lt;/h4&gt; 
&lt;p&gt;Independent verification pass for accuracy using a separate verification model:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-http"&gt;POST /sessions/{session_id}/chat
Content-Type: application/json

{
  "query": "What are the key findings?",
  "verify": true
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Contextual Enrichment&lt;/h4&gt; 
&lt;p&gt;Document context enrichment during indexing for better understanding:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Enable during index building
POST /indexes/{id}/build
{
  "enable_enrich": true,
  "window_size": 2
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Late Chunking&lt;/h4&gt; 
&lt;p&gt;Better context preservation by chunking after embedding:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Configure in pipeline
"late_chunking": {"enabled": true}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Streaming Chat&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-http"&gt;POST /chat/stream
Content-Type: application/json

{
  "query": "Explain the methodology",
  "session_id": "uuid",
  "stream": true
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Batch Processing&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Using the batch indexing script
python demo_batch_indexing.py --config batch_indexing_config.json

# Example batch configuration (batch_indexing_config.json):
{
  "index_name": "Sample Batch Index",
  "index_description": "Example batch index configuration",
  "documents": [
    "./rag_system/documents/invoice_1039.pdf",
    "./rag_system/documents/invoice_1041.pdf"
  ],
  "processing": {
    "chunk_size": 512,
    "chunk_overlap": 64,
    "enable_enrich": true,
    "enable_latechunk": true,
    "enable_docling": true,
    "embedding_model": "Qwen/Qwen3-Embedding-0.6B",
    "generation_model": "qwen3:0.6b",
    "retrieval_mode": "hybrid",
    "window_size": 2
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-http"&gt;# API endpoint for batch processing
POST /batch/index
Content-Type: application/json

{
  "file_paths": ["doc1.pdf", "doc2.pdf"],
  "config": {
    "chunk_size": 512,
    "enable_enrich": true,
    "enable_latechunk": true,
    "enable_docling": true
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For complete API documentation, see &lt;a href="https://raw.githubusercontent.com/PromtEngineer/localGPT/main/API_REFERENCE.md"&gt;API_REFERENCE.md&lt;/a&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üèóÔ∏è Architecture&lt;/h2&gt; 
&lt;p&gt;LocalGPT is built with a modular, scalable architecture:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-mermaid"&gt;graph TB
    UI[Web Interface] --&amp;gt; API[Backend API]
    API --&amp;gt; Agent[RAG Agent]
    Agent --&amp;gt; Retrieval[Retrieval Pipeline]
    Agent --&amp;gt; Generation[Generation Pipeline]

    Retrieval --&amp;gt; Vector[Vector Search]
    Retrieval --&amp;gt; BM25[BM25 Search]
    Retrieval --&amp;gt; Rerank[Reranking]

    Vector --&amp;gt; LanceDB[(LanceDB)]
    BM25 --&amp;gt; BM25DB[(BM25 Index)]

    Generation --&amp;gt; Ollama[Ollama Models]
    Generation --&amp;gt; HF[Hugging Face Models]

    API --&amp;gt; SQLite[(SQLite DB)]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Overview of the Retrieval Agent&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-mermaid"&gt;graph TD
    classDef llmcall fill:#e6f3ff,stroke:#007bff;
    classDef pipeline fill:#e6ffe6,stroke:#28a745;
    classDef cache fill:#fff3e0,stroke:#fd7e14;
    classDef logic fill:#f8f9fa,stroke:#6c757d;
    classDef thread stroke-dasharray: 5 5;

    A(Start: Agent.run) --&amp;gt; B_asyncio.run(_run_async);
    B --&amp;gt; C{_run_async};

    C --&amp;gt; C1[Get Chat History];
    C1 --&amp;gt; T1[Build Triage Prompt &amp;lt;br/&amp;gt; Query + Doc Overviews ];
    T1 --&amp;gt; T2["(asyncio.to_thread)&amp;lt;br/&amp;gt;LLM Triage: RAG or LLM_DIRECT?"]; class T2 llmcall,thread;
    T2 --&amp;gt; T3{Decision?};

    T3 -- RAG --&amp;gt; RAG_Path;
    T3 -- LLM_DIRECT --&amp;gt; LLM_Path;

    subgraph RAG Path
        RAG_Path --&amp;gt; R1[Format Query + History];
        R1 --&amp;gt; R2["(asyncio.to_thread)&amp;lt;br/&amp;gt;Generate Query Embedding"]; class R2 pipeline,thread;
        R2 --&amp;gt; R3{{Check Semantic Cache}}; class R3 cache;
        R3 -- Hit --&amp;gt; R_Cache_Hit(Return Cached Result);
        R_Cache_Hit --&amp;gt; R_Hist_Update;
        R3 -- Miss --&amp;gt; R4{Decomposition &amp;lt;br/&amp;gt; Enabled?};

        R4 -- Yes --&amp;gt; R5["(asyncio.to_thread)&amp;lt;br/&amp;gt;Decompose Raw Query"]; class R5 llmcall,thread;
        R5 --&amp;gt; R6{{Run Sub-Queries &amp;lt;br/&amp;gt; Parallel RAG Pipeline}}; class R6 pipeline,thread;
        R6 --&amp;gt; R7[Collect Results &amp;amp; Docs];
        R7 --&amp;gt; R8["(asyncio.to_thread)&amp;lt;br/&amp;gt;Compose Final Answer"]; class R8 llmcall,thread;
        R8 --&amp;gt; V1(RAG Answer);

        R4 -- No --&amp;gt; R9["(asyncio.to_thread)&amp;lt;br/&amp;gt;Run Single Query &amp;lt;br/&amp;gt;(RAG Pipeline)"]; class R9 pipeline,thread;
        R9 --&amp;gt; V1;

        V1 --&amp;gt; V2{{Verification &amp;lt;br/&amp;gt; await verify_async}}; class V2 llmcall;
        V2 --&amp;gt; V3(Final RAG Result);
        V3 --&amp;gt; R_Cache_Store{{Store in Semantic Cache}}; class R_Cache_Store cache;
        R_Cache_Store --&amp;gt; FinalResult;
    end

    subgraph Direct LLM Path
        LLM_Path --&amp;gt; L1[Format Query + History];
        L1 --&amp;gt; L2["(asyncio.to_thread)&amp;lt;br/&amp;gt;Generate Direct LLM Answer &amp;lt;br/&amp;gt; (No RAG)"]; class L2 llmcall,thread;
        L2 --&amp;gt; FinalResult(Final Direct Result);
    end

    FinalResult --&amp;gt; R_Hist_Update(Update Chat History);
    R_Hist_Update --&amp;gt; ZZZ(End: Return Result);
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions from developers of all skill levels! LocalGPT is an open-source project that benefits from community involvement.&lt;/p&gt; 
&lt;h3&gt;üöÄ Quick Start for Contributors&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Fork and clone the repository
git clone https://github.com/PromtEngineer/localGPT.git
cd localGPT

# Set up development environment
pip install -r requirements.txt
npm install

# Install Ollama and models
curl -fsSL https://ollama.ai/install.sh | sh
ollama pull qwen3:0.6b qwen3:8b

# Verify setup
python system_health_check.py
python run_system.py --mode dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;üìã How to Contribute&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;üêõ Report Bugs&lt;/strong&gt;: Use our &lt;a href="https://raw.githubusercontent.com/PromtEngineer/localGPT/main/.github/ISSUE_TEMPLATE/bug_report.md"&gt;bug report template&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üí° Request Features&lt;/strong&gt;: Use our &lt;a href="https://raw.githubusercontent.com/PromtEngineer/localGPT/main/.github/ISSUE_TEMPLATE/feature_request.md"&gt;feature request template&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üîß Submit Code&lt;/strong&gt;: Follow our &lt;a href="https://raw.githubusercontent.com/PromtEngineer/localGPT/main/CONTRIBUTING.md#development-workflow"&gt;development workflow&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üìö Improve Docs&lt;/strong&gt;: Help make our documentation better&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;üìñ Detailed Guidelines&lt;/h3&gt; 
&lt;p&gt;For comprehensive contributing guidelines, including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Development setup and workflow&lt;/li&gt; 
 &lt;li&gt;Coding standards and best practices&lt;/li&gt; 
 &lt;li&gt;Testing requirements&lt;/li&gt; 
 &lt;li&gt;Documentation standards&lt;/li&gt; 
 &lt;li&gt;Release process&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;üëâ See our &lt;a href="https://raw.githubusercontent.com/PromtEngineer/localGPT/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; guide&lt;/strong&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìÑ License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the MIT License - see the &lt;a href="https://raw.githubusercontent.com/PromtEngineer/localGPT/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details. For models, please check their respective licenses.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìû Support&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Documentation&lt;/strong&gt;: &lt;a href="https://raw.githubusercontent.com/PromtEngineer/localGPT/main/TECHNICAL_DOCS.md"&gt;Technical Docs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Issues&lt;/strong&gt;: &lt;a href="https://github.com/PromtEngineer/localGPT/issues"&gt;GitHub Issues&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Discussions&lt;/strong&gt;: &lt;a href="https://github.com/PromtEngineer/localGPT/discussions"&gt;GitHub Discussions&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Business Deployment and Customization&lt;/strong&gt;: &lt;a href="https://tally.so/r/wv6R2d"&gt;Contact Us&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;h2&gt;Star History&lt;/h2&gt; 
 &lt;p&gt;&lt;a href="https://star-history.com/#PromtEngineer/localGPT&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=PromtEngineer/localGPT&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>OpenBMB/UltraRAG</title>
      <link>https://github.com/OpenBMB/UltraRAG</link>
      <description>&lt;p&gt;UltraRAG v3: A Low-Code MCP Framework for Building Complex and Innovative RAG Pipelines&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="./docs/ultrarag_dark.svg" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="./docs/ultrarag.svg" /&gt; 
  &lt;img alt="UltraRAG" src="https://raw.githubusercontent.com/OpenBMB/UltraRAG/main/docs/ultrarag.svg?sanitize=true" width="55%" /&gt; 
 &lt;/picture&gt; &lt;/p&gt; 
&lt;h3 align="center"&gt; Less Code, Lower Barrier, Faster Deployment &lt;/h3&gt; 
&lt;p align="center"&gt; | &lt;a href="https://ultrarag.openbmb.cn/pages/en/getting_started/introduction"&gt;&lt;b&gt;Documentation&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://modelscope.cn/datasets/UltraRAG/UltraRAG_Benchmark"&gt;&lt;b&gt;Dataset&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://github.com/OpenBMB/UltraRAG/tree/rag-paper-daily/rag-paper-daily"&gt;&lt;b&gt;Paper Daily&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/OpenBMB/UltraRAG/main/docs/README_zh.md"&gt;&lt;b&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/b&gt;&lt;/a&gt; | &lt;b&gt;English&lt;/b&gt; | &lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;em&gt;Latest News&lt;/em&gt; üî•&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;[2026.01.23] üéâ UltraRAG 3.0 Released: Say no to "black box" development‚Äîmake every line of reasoning logic clearly visible üëâ|&lt;a href="https://github.com/OpenBMB/UltraRAG/raw/page/project/blog/en/ultrarag3_0.md"&gt;üìñ Blog&lt;/a&gt;|&lt;/li&gt; 
 &lt;li&gt;[2026.01.20] üéâ AgentCPM-Report Model Released! DeepResearch is finally localized: 8B on-device writing agent AgentCPM-Report is open-sourced üëâ |&lt;a href="https://huggingface.co/openbmb/AgentCPM-Report"&gt;ü§ó Model&lt;/a&gt;|&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;Previous News&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;[2025.11.11] üéâ UltraRAG 2.1 Released: Enhanced knowledge ingestion &amp;amp; multimodal support, with a more complete unified evaluation system!&lt;/li&gt; 
  &lt;li&gt;[2025.09.23] New daily RAG paper digest, updated every day üëâ |&lt;a href="https://github.com/OpenBMB/UltraRAG/tree/rag-paper-daily/rag-paper-daily"&gt;üìñ Papers&lt;/a&gt;|&lt;/li&gt; 
  &lt;li&gt;[2025.09.09] Released a Lightweight DeepResearch Pipeline local setup tutorial üëâ |&lt;a href="https://www.bilibili.com/video/BV1p8JfziEwM"&gt;üì∫ bilibili&lt;/a&gt;|&lt;a href="https://github.com/OpenBMB/UltraRAG/raw/page/project/blog/en/01_build_light_deepresearch.md"&gt;üìñ Blog&lt;/a&gt;|&lt;/li&gt; 
  &lt;li&gt;[2025.09.01] Released a step-by-step UltraRAG installation and full RAG walkthrough video üëâ |&lt;a href="https://www.bilibili.com/video/BV1B9apz4E7K/?share_source=copy_web&amp;amp;vd_source=7035ae721e76c8149fb74ea7a2432710"&gt;üì∫ bilibili&lt;/a&gt;|&lt;a href="https://github.com/OpenBMB/UltraRAG/raw/page/project/blog/en/00_Installing_and_Running_RAG.md"&gt;üìñ Blog&lt;/a&gt;|&lt;/li&gt; 
  &lt;li&gt;[2025.08.28] üéâ UltraRAG 2.0 Released! UltraRAG 2.0 is fully upgraded: build a high-performance RAG with just a few dozen lines of code, empowering researchers to focus on ideas and innovation! We have preserved the UltraRAG v2 code, which can be viewed at &lt;a href="https://github.com/OpenBMB/UltraRAG/tree/v2"&gt;v2&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2025.01.23] UltraRAG Released! Enabling large models to better comprehend and utilize knowledge bases. The UltraRAG 1.0 code is still available at &lt;a href="https://github.com/OpenBMB/UltraRAG/tree/v1"&gt;v1&lt;/a&gt;.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h2&gt;About UltraRAG&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://trendshift.io/repositories/18747" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/18747" alt="OpenBMB%2FUltraRAG | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;UltraRAG is the first lightweight RAG development framework based on the &lt;a href="https://modelcontextprotocol.io/docs/getting-started/intro"&gt;Model Context Protocol (MCP)&lt;/a&gt; architecture design, jointly launched by &lt;a href="https://nlp.csai.tsinghua.edu.cn/"&gt;THUNLP&lt;/a&gt; at Tsinghua University, &lt;a href="https://neuir.github.io"&gt;NEUIR&lt;/a&gt; at Northeastern University, &lt;a href="https://www.openbmb.cn/home"&gt;OpenBMB&lt;/a&gt;, and &lt;a href="https://github.com/AI9Stars"&gt;AI9stars&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Designed for research exploration and industrial prototyping, UltraRAG standardizes core RAG components (Retriever, Generation, etc.) as independent &lt;strong&gt;MCP Servers&lt;/strong&gt;, combined with the powerful workflow orchestration capabilities of the &lt;strong&gt;MCP Client&lt;/strong&gt;. Developers can achieve precise orchestration of complex control structures such as conditional branches and loops simply through YAML configuration.&lt;/p&gt; 
&lt;p align="center"&gt; 
 &lt;picture&gt; 
  &lt;img alt="UltraRAG" src="https://raw.githubusercontent.com/OpenBMB/UltraRAG/main/docs/architecture.png" width="90%" /&gt; 
 &lt;/picture&gt; &lt;/p&gt; 
&lt;h3&gt;UltraRAG UI&lt;/h3&gt; 
&lt;p&gt;UltraRAG UI transcends the boundaries of traditional chat interfaces, evolving into a visual RAG Integrated Development Environment (IDE) that combines orchestration, debugging, and demonstration.&lt;/p&gt; 
&lt;p&gt;The system features a powerful built-in Pipeline Builder that supports bidirectional real-time synchronization between "Canvas Construction" and "Code Editing," allowing for granular online adjustments of pipeline parameters and prompts. Furthermore, it introduces an Intelligent AI Assistant to empower the entire development lifecycle, from pipeline structural design to parameter tuning and prompt generation. Once constructed, logic flows can be converted into interactive dialogue systems with a single click. The system seamlessly integrates Knowledge Base Management components, enabling users to build custom knowledge bases for document Q&amp;amp;A. This truly realizes a one-stop closed loop, spanning from underlying logic construction and data governance to final application deployment.&lt;/p&gt; 
&lt;!-- &lt;p align="center"&gt;
  &lt;picture&gt;
    &lt;img alt="UltraRAG_UI" src="./docs/chat_menu.png" width=80%&gt;
  &lt;/picture&gt;
&lt;/p&gt; --&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/fcf437b7-8b79-42f2-bf4e-e3b7c2a896b9"&gt;https://github.com/user-attachments/assets/fcf437b7-8b79-42f2-bf4e-e3b7c2a896b9&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Key Highlights&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;üöÄ &lt;strong&gt;Low-Code Orchestration of Complex Workflows&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Inference Orchestration&lt;/strong&gt;: Natively supports control structures such as sequential, loop, and conditional branches. Developers only need to write YAML configuration files to implement complex iterative RAG logic in dozens of lines of code.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;‚ö° &lt;strong&gt;Modular Extension and Reproduction&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Atomic Servers&lt;/strong&gt;: Based on the MCP architecture, functions are decoupled into independent Servers. New features only need to be registered as function-level Tools to seamlessly integrate into workflows, achieving extremely high reusability.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üìä &lt;strong&gt;Unified Evaluation and Benchmark Comparison&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Research Efficiency&lt;/strong&gt;: Built-in standardized evaluation workflows, ready-to-use mainstream research benchmarks. Through unified metric management and baseline integration, significantly improves experiment reproducibility and comparison efficiency.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;‚ú® &lt;strong&gt;Rapid Interactive Prototype Generation&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;One-Click Delivery&lt;/strong&gt;: Say goodbye to tedious UI development. With just one command, Pipeline logic can be instantly converted into an interactive conversational Web UI, shortening the distance from algorithm to demonstration.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;We provide two installation methods: local source code installation (recommended using &lt;code&gt;uv&lt;/code&gt; for package management) and Docker container deployment&lt;/p&gt; 
&lt;h3&gt;Method 1: Source Code Installation&lt;/h3&gt; 
&lt;p&gt;We strongly recommend using &lt;a href="https://github.com/astral-sh/uv"&gt;uv&lt;/a&gt; to manage Python environments and dependencies, as it can greatly improve installation speed.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Prepare Environment&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;If you haven't installed uv yet, please execute:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;## Direct installation
pip install uv
## Download
curl -LsSf https://astral.sh/uv/install.sh | sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Download Source Code&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;git clone https://github.com/OpenBMB/UltraRAG.git --depth 1
cd UltraRAG
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Install Dependencies&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Choose one of the following modes to install dependencies based on your use case:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;A: Create a New Environment&lt;/strong&gt; Use &lt;code&gt;uv sync&lt;/code&gt; to automatically create a virtual environment and synchronize dependencies:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Core dependencies: If you only need to run basic core functions, such as only using UltraRAG UI:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;uv sync
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Full installation: If you want to fully experience UltraRAG's retrieval, generation, corpus processing, and evaluation functions, please run:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;uv sync --all-extras
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;On-demand installation: If you only need to run specific modules, keep the corresponding &lt;code&gt;--extra&lt;/code&gt; as needed, for example:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;uv sync --extra retriever   # Retrieval module only
uv sync --extra generation  # Generation module only
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Once installed, activate the virtual environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# Windows CMD
.venv\Scripts\activate.bat

# Windows Powershell
.venv\Scripts\Activate.ps1

# macOS / Linux
source .venv/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;B: Install into an Existing Environment&lt;/strong&gt; To install UltraRAG into your currently active Python environment, use &lt;code&gt;uv pip&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# Core dependencies
uv pip install -e .

# Full installation
uv pip install -e ".[all]"

# On-demand installation
uv pip install -e ".[retriever]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Method 2: Docker Container Deployment&lt;/h3&gt; 
&lt;p&gt;If you prefer not to configure a local Python environment, you can deploy using Docker.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Get Code and Images&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# 1. Clone the repository
git clone https://github.com/OpenBMB/UltraRAG.git --depth 1
cd UltraRAG

# 2. Prepare the image (choose one)
# Option A: Pull from Docker Hub
docker pull hdxin2002/ultrarag:v0.3.0-base-cpu # Base version (CPU)
docker pull hdxin2002/ultrarag:v0.3.0-base-gpu # Base version (GPU)
docker pull hdxin2002/ultrarag:v0.3.0          # Full version (GPU)

# Option B: Build locally
docker build -t ultrarag:v0.3.0 .

# 3. Start container (port 5050 is automatically mapped)
docker run -it --gpus all -p 5050:5050 &amp;lt;docker_image_name&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Start the Container&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# Start the container (Port 5050 is mapped by default)
docker run -it --gpus all -p 5050:5050 &amp;lt;docker_image_name&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note: After the container starts, UltraRAG UI will run automatically. You can directly access &lt;code&gt;http://localhost:5050&lt;/code&gt; in your browser to use it.&lt;/p&gt; 
&lt;h3&gt;Verify Installation&lt;/h3&gt; 
&lt;p&gt;After installation, run the following example command to check if the environment is normal:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;ultrarag run examples/sayhello.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you see the following output, the installation is successful:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Hello, UltraRAG v3!
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;We provide complete tutorial examples from beginner to advanced. Whether you are conducting academic research or building industrial applications, you can find guidance here. Welcome to visit the &lt;a href="https://ultrarag.openbmb.cn/pages/en/getting_started/introduction"&gt;Documentation&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h3&gt;Research Experiments&lt;/h3&gt; 
&lt;p&gt;Designed for researchers, providing data, experimental workflows, and visualization analysis tools.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://ultrarag.openbmb.cn/pages/en/getting_started/quick_start"&gt;Getting Started&lt;/a&gt;: Learn how to quickly run standard RAG experimental workflows based on UltraRAG.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ultrarag.openbmb.cn/pages/en/develop_guide/dataset"&gt;Evaluation Data&lt;/a&gt;: Download the most commonly used public evaluation datasets in the RAG field and large-scale retrieval corpora, directly for research benchmark testing.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ultrarag.openbmb.cn/pages/en/develop_guide/case_study"&gt;Case Analysis&lt;/a&gt;: Provides a visual Case Study interface to deeply track each intermediate output of the workflow, assisting in analysis and error attribution.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ultrarag.openbmb.cn/pages/en/develop_guide/code_integration"&gt;Code Integration&lt;/a&gt;: Learn how to directly call UltraRAG components in Python code to achieve more flexible customized development.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Demo Systems&lt;/h3&gt; 
&lt;p&gt;Designed for developers and end users, providing complete UI interaction and complex application cases.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://ultrarag.openbmb.cn/pages/en/ui/start"&gt;Quick Start&lt;/a&gt;: Learn how to start UltraRAG UI and familiarize yourself with various advanced configurations in administrator mode.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ultrarag.openbmb.cn/pages/en/ui/prepare"&gt;Deployment Guide&lt;/a&gt;: Detailed production environment deployment tutorials, covering the setup of Retriever, Generation models (LLM), and Milvus vector database.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ultrarag.openbmb.cn/pages/en/demo/deepresearch"&gt;Deep Research&lt;/a&gt;: Flagship case, deploy a Deep Research Pipeline. Combined with the AgentCPM-Report model, it can automatically perform multi-step retrieval and integration to generate tens of thousands of words of survey reports.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Thanks to the following contributors for their code submissions and testing. We also welcome new members to join us in collectively building a comprehensive RAG ecosystem!&lt;/p&gt; 
&lt;p&gt;You can contribute by following the standard process: &lt;strong&gt;Fork this repository ‚Üí Submit Issues ‚Üí Create Pull Requests (PRs)&lt;/strong&gt;.&lt;/p&gt; 
&lt;a href="https://github.com/OpenBMB/UltraRAG/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=OpenBMB/UltraRAG&amp;amp;nocache=true" /&gt; &lt;/a&gt; 
&lt;h2&gt;Support Us&lt;/h2&gt; 
&lt;p&gt;If you find this repository helpful for your research, please consider giving us a ‚≠ê to show your support.&lt;/p&gt; 
&lt;a href="https://star-history.com/#OpenBMB/UltraRAG&amp;amp;Date"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=OpenBMB/UltraRAG&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=OpenBMB/UltraRAG&amp;amp;type=Date" /&gt; 
  &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=OpenBMB/UltraRAG&amp;amp;type=Date" /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;h2&gt;Contact Us&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;For technical issues and feature requests, please use &lt;a href="https://github.com/OpenBMB/UltraRAG/issues"&gt;GitHub Issues&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;For questions about usage, feedback, or any discussions related to RAG technologies, you are welcome to join our &lt;a href="https://github.com/OpenBMB/UltraRAG/raw/main/docs/wechat_qr.png"&gt;WeChat group&lt;/a&gt;, &lt;a href="https://github.com/OpenBMB/UltraRAG/raw/main/docs/feishu_qr.png"&gt;Feishu group&lt;/a&gt;, and &lt;a href="https://discord.gg/yRFFjjJnnS"&gt;Discord&lt;/a&gt; to exchange ideas with us.&lt;/li&gt; 
 &lt;li&gt;If you have any questions, feedback, or would like to get in touch, please feel free to reach out to us via email at &lt;a href="mailto:yanyk.thu@gmail.com"&gt;yanyk.thu@gmail.com&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/UltraRAG/main/docs/wechat_qr.png" alt="WeChat Group QR Code" width="220" /&gt;&lt;br /&gt; &lt;b&gt;WeChat Group&lt;/b&gt; &lt;/td&gt; 
   &lt;td align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/UltraRAG/main/docs/feishu_qr.png" alt="Feishu Group QR Code" width="220" /&gt;&lt;br /&gt; &lt;b&gt;Feishu Group&lt;/b&gt; &lt;/td&gt; 
   &lt;td align="center"&gt; &lt;a href="https://discord.gg/yRFFjjJnnS"&gt; &lt;img src="https://img.shields.io/badge/Discord-5865F2?logo=discord&amp;amp;logoColor=white" alt="Join Discord" /&gt; &lt;/a&gt;&lt;br /&gt; &lt;b&gt;Discord&lt;/b&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt;</description>
    </item>
    
    <item>
      <title>Asabeneh/30-Days-Of-Python</title>
      <link>https://github.com/Asabeneh/30-Days-Of-Python</link>
      <description>&lt;p&gt;The 30 Days of Python programming challenge is a step-by-step guide to learn the Python programming language in 30 days. This challenge may take more than 100 days. Follow your own pace. These videos may help too: https://www.youtube.com/channel/UC7PNRuno1rzYPb1xLa4yktw&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;üêç 30 Days Of Python&lt;/h1&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;# Day&lt;/th&gt; 
   &lt;th align="center"&gt;Topics&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;01&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/readme.md"&gt;Introduction&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;02&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/02_Day_Variables_builtin_functions/02_variables_builtin_functions.md"&gt;Variables, Built-in Functions&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;03&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/03_Day_Operators/03_operators.md"&gt;Operators&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;04&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/04_Day_Strings/04_strings.md"&gt;Strings&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;05&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/05_Day_Lists/05_lists.md"&gt;Lists&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;06&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/06_Day_Tuples/06_tuples.md"&gt;Tuples&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;07&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/07_Day_Sets/07_sets.md"&gt;Sets&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;08&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/08_Day_Dictionaries/08_dictionaries.md"&gt;Dictionaries&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;09&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/09_Day_Conditionals/09_conditionals.md"&gt;Conditionals&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;10&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/10_Day_Loops/10_loops.md"&gt;Loops&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;11&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/11_Day_Functions/11_functions.md"&gt;Functions&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;12&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/12_Day_Modules/12_modules.md"&gt;Modules&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;13&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/13_Day_List_comprehension/13_list_comprehension.md"&gt;List Comprehension&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;14&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/14_Day_Higher_order_functions/14_higher_order_functions.md"&gt;Higher Order Functions&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;15&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/15_Day_Python_type_errors/15_python_type_errors.md"&gt;Python Type Errors&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/16_Day_Python_date_time/16_python_datetime.md"&gt;Python Date time&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;17&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/17_Day_Exception_handling/17_exception_handling.md"&gt;Exception Handling&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;18&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/18_Day_Regular_expressions/18_regular_expressions.md"&gt;Regular Expressions&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;19&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/19_Day_File_handling/19_file_handling.md"&gt;File Handling&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;20&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/20_Day_Python_package_manager/20_python_package_manager.md"&gt;Python Package Manager&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;21&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/21_Day_Classes_and_objects/21_classes_and_objects.md"&gt;Classes and Objects&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;22&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/22_Day_Web_scraping/22_web_scraping.md"&gt;Web Scraping&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;23&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/23_Day_Virtual_environment/23_virtual_environment.md"&gt;Virtual Environment&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;24&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/24_Day_Statistics/24_statistics.md"&gt;Statistics&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;25&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/25_Day_Pandas/25_pandas.md"&gt;Pandas&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;26&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/26_Day_Python_web/26_python_web.md"&gt;Python web&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;27&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/27_Day_Python_with_mongodb/27_python_with_mongodb.md"&gt;Python with MongoDB&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;28&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/28_Day_API/28_API.md"&gt;API&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;29&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/29_Day_Building_API/29_building_API.md"&gt;Building API&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;30&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/30_Day_Conclusions/30_conclusions.md"&gt;Conclusions&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;small&gt;üß°üß°üß° HAPPY CODING üß°üß°üß°&lt;/small&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;div&gt; 
 &lt;h2&gt;üíñ Sponsors&lt;/h2&gt; 
 &lt;p&gt;Our amazing sponsors for supporting my open-source contribution and the &lt;strong&gt;30 Days of Challenge&lt;/strong&gt; series!&lt;/p&gt; 
 &lt;h3&gt;Current Sponsors&lt;/h3&gt; 
 &lt;hr /&gt; 
 &lt;div align="center"&gt; 
  &lt;a href="https://ref.wisprflow.ai/MPMzRGE" target="_blank" rel="noopener noreferrer"&gt; 
   &lt;picture&gt; 
    &lt;!-- Dark mode --&gt; 
    &lt;source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/Asabeneh/asabeneh/master/images/Wispr_Flow-Logo-white.png" /&gt; 
    &lt;!-- Light mode (fallback) --&gt; 
    &lt;img src="https://raw.githubusercontent.com/Asabeneh/asabeneh/master/images/Wispr_Flow-logo.png" width="400px" alt="Wispr Flow Logo" title="Wispr Flow" /&gt; 
   &lt;/picture&gt; &lt;/a&gt; 
  &lt;h1&gt; &lt;a href="https://ref.wisprflow.ai/MPMzRGE" target="_blank" rel="noopener noreferrer"&gt; Talk to code, stay in the Flow. &lt;/a&gt; &lt;/h1&gt; 
  &lt;h2&gt; &lt;a href="https://ref.wisprflow.ai/MPMzRGE" target="_blank" rel="noopener noreferrer"&gt; Flow is built for devs who live in their tools. Speak and give more context, get better results. &lt;/a&gt; &lt;/h2&gt; 
 &lt;/div&gt; 
 &lt;hr /&gt; 
 &lt;div align="center"&gt; 
  &lt;a href="https://client.petrosky.io/aff.php?aff=402" target="_blank" rel="noopener noreferrer"&gt; 
   &lt;picture&gt; 
    &lt;!-- Dark mode --&gt; 
    &lt;source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/Asabeneh/asabeneh/master/images/petrosky-logo-white.png" /&gt; 
    &lt;!-- Light mode (fallback) --&gt; 
    &lt;img src="https://raw.githubusercontent.com/Asabeneh/asabeneh/master/images/petrosky-logo-black.png" width="400px" alt="Petrosky Logo" title="Petrosky" /&gt; 
   &lt;/picture&gt; &lt;/a&gt; 
  &lt;h1&gt; &lt;a href="https://client.petrosky.io/aff.php?aff=402" target="_blank" rel="noopener noreferrer"&gt; A hosting for your entire journey! &lt;/a&gt; &lt;/h1&gt; 
  &lt;h2&gt; &lt;a href="https://client.petrosky.io/aff.php?aff=402" target="_blank" rel="noopener noreferrer"&gt; Affordable VPS Hosting Services For All Your Needs &lt;/a&gt; &lt;/h2&gt; 
 &lt;/div&gt; 
 &lt;hr /&gt; 
 &lt;h3&gt;üôå Become a Sponsor&lt;/h3&gt; 
 &lt;p&gt;You can support this project by becoming a sponsor on &lt;strong&gt;&lt;a href="https://github.com/sponsors/asabeneh"&gt;GitHub Sponsors&lt;/a&gt;&lt;/strong&gt; or through &lt;a href="https://www.paypal.me/asabeneh"&gt;PayPal&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;Every contribution, big or small, makes a huge difference. Thank you for your support! üåü&lt;/p&gt; 
 &lt;hr /&gt; 
 &lt;div align="center"&gt; 
  &lt;h1&gt; 30 Days Of Python: Day 1 - Introduction&lt;/h1&gt; 
  &lt;a class="header-badge" target="_blank" href="https://www.linkedin.com/in/asabeneh/"&gt; &lt;img src="https://img.shields.io/badge/style--5eba00.svg?label=LinkedIn&amp;amp;logo=linkedin&amp;amp;style=social" /&gt; &lt;/a&gt; 
  &lt;a class="header-badge" target="_blank" href="https://twitter.com/Asabeneh"&gt; &lt;img alt="Twitter Follow" src="https://img.shields.io/twitter/follow/asabeneh?style=social" /&gt; &lt;/a&gt; 
  &lt;p&gt;&lt;sub&gt;Author: &lt;a href="https://www.linkedin.com/in/asabeneh/" target="_blank"&gt;Asabeneh Yetayeh&lt;/a&gt;&lt;br /&gt; &lt;small&gt; Second Edition: July, 2021&lt;/small&gt; &lt;/sub&gt;&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;p&gt;üáßüá∑ &lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/Portuguese/README.md"&gt;Portuguese&lt;/a&gt; üá®üá≥ &lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/Chinese/README.md"&gt;‰∏≠Êñá&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/02_Day_Variables_builtin_functions/02_variables_builtin_functions.md"&gt;Day 2 &amp;gt;&amp;gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/30DaysOfPython_banner3@2x.png" alt="30DaysOfPython" /&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#-30-days-of-python"&gt;üêç 30 Days Of Python&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#-become-a-sponsor"&gt;üôå Become a Sponsor&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#-day-1"&gt;üìò Day 1&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#welcome"&gt;Welcome&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#why-python-"&gt;Why Python ?&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#environment-setup"&gt;Environment Setup&lt;/a&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#installing-python"&gt;Installing Python&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#python-shell"&gt;Python Shell&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#installing-visual-studio-code"&gt;Installing Visual Studio Code&lt;/a&gt; 
       &lt;ul&gt; 
        &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#how-to-use-visual-studio-code"&gt;How to use visual studio code&lt;/a&gt;&lt;/li&gt; 
       &lt;/ul&gt; &lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#basic-python"&gt;Basic Python&lt;/a&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#python-syntax"&gt;Python Syntax&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#python-indentation"&gt;Python Indentation&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#comments"&gt;Comments&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#data-types"&gt;Data types&lt;/a&gt; 
       &lt;ul&gt; 
        &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#number"&gt;Number&lt;/a&gt;&lt;/li&gt; 
        &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#string"&gt;String&lt;/a&gt;&lt;/li&gt; 
        &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#booleans"&gt;Booleans&lt;/a&gt;&lt;/li&gt; 
        &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#list"&gt;List&lt;/a&gt;&lt;/li&gt; 
        &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#dictionary"&gt;Dictionary&lt;/a&gt;&lt;/li&gt; 
        &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#tuple"&gt;Tuple&lt;/a&gt;&lt;/li&gt; 
        &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#set"&gt;Set&lt;/a&gt;&lt;/li&gt; 
       &lt;/ul&gt; &lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#checking-data-types"&gt;Checking Data types&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#python-file"&gt;Python File&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#-exercises---day-1"&gt;üíª Exercises - Day 1&lt;/a&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#exercise-level-1"&gt;Exercise: Level 1&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#exercise-level-2"&gt;Exercise: Level 2&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#exercise-level-3"&gt;Exercise: Level 3&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h1&gt;üìò Day 1&lt;/h1&gt; 
 &lt;h2&gt;Welcome&lt;/h2&gt; 
 &lt;p&gt;&lt;strong&gt;Congratulations&lt;/strong&gt; for deciding to participate in a &lt;em&gt;30 days of Python&lt;/em&gt; programming challenge. In this challenge, you will learn everything you need to be a python programmer and the whole concept of programming. In the end of the challenge you will get a &lt;em&gt;30DaysOfPython&lt;/em&gt; programming challenge certificate.&lt;/p&gt; 
 &lt;p&gt;If you would like to actively engage in the challenge, you may join the &lt;a href="https://t.me/ThirtyDaysOfPython"&gt;30DaysOfPython challenge&lt;/a&gt; telegram group.&lt;/p&gt; 
 &lt;h2&gt;Introduction&lt;/h2&gt; 
 &lt;p&gt;Python is a high-level programming language for general-purpose programming. It is an open source, interpreted, object-oriented programming language. Python was created by a Dutch programmer, Guido van Rossum. The name of the Python programming language was derived from a British sketch comedy series, &lt;em&gt;Monty Python's Flying Circus&lt;/em&gt;. The first version was released on February 20, 1991. This 30 days of Python challenge will help you learn the latest version of Python, Python 3 step by step. The topics are broken down into 30 days, where each day contains several topics with easy-to-understand explanations, real-world examples, and many hands on exercises and projects.&lt;/p&gt; 
 &lt;p&gt;This challenge is designed for beginners and professionals who want to learn python programming language. It may take 30 to 100 days to complete the challenge. People who actively participate in the telegram group have a high probability of completing the challenge.&lt;/p&gt; 
 &lt;p&gt;This challenge is easy to read, written in conversational English, engaging, motivating and at the same time, it is very demanding. You need to allocate much time to finish this challenge. If you are a visual learner, you may get the video lesson on &lt;a href="https://www.youtube.com/channel/UC7PNRuno1rzYPb1xLa4yktw"&gt; Washera&lt;/a&gt; YouTube channel. You may start from &lt;a href="https://youtu.be/OCCWZheOesI"&gt;Python for Absolute Beginners video&lt;/a&gt;. Subscribe the channel, comment and ask questions on YouTube videos and be proactive, the author will eventually notice you.&lt;/p&gt; 
 &lt;p&gt;The author likes to hear your opinion about the challenge, share the author by expressing your thoughts about the 30DaysOfPython challenge. You can leave your testimonial on this &lt;a href="https://www.asabeneh.com/testimonials"&gt;link&lt;/a&gt;&lt;/p&gt; 
 &lt;h2&gt;Why Python ?&lt;/h2&gt; 
 &lt;p&gt;It is a programming language which is very close to human language and because of that, it is easy to learn and use. Python is used by various industries and companies (including Google). It has been used to develop web applications, desktop applications, system administration, and machine learning libraries. Python is a highly embraced language in the data science and machine learning community. I hope this is enough to convince you to start learning Python. Python is eating the world and you are killing it before it eats you.&lt;/p&gt; 
 &lt;h2&gt;Environment Setup&lt;/h2&gt; 
 &lt;h3&gt;Installing Python&lt;/h3&gt; 
 &lt;p&gt;To run a python script you need to install python. Let's &lt;a href="https://www.python.org/"&gt;download&lt;/a&gt; python. If your are a windows user, click the button encircled in red.&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://www.python.org/"&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/installing_on_windows.png" alt="installing on Windows" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;If you are a macOS user, click the button encircled in red.&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://www.python.org/"&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/installing_on_macOS.png" alt="installing on Windows" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;To check if python is installed write the following command on your device terminal.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;python3 --version
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/python_versio.png" alt="Python Version" /&gt;&lt;/p&gt; 
 &lt;p&gt;As you can see from the terminal, I am using &lt;em&gt;Python 3.7.5&lt;/em&gt; version at the moment. Your version of Python might be different from mine by but it should be 3.6 or above. If you manage to see the python version, well done. Python has been installed on your machine. Continue to the next section.&lt;/p&gt; 
 &lt;h3&gt;Python Shell&lt;/h3&gt; 
 &lt;p&gt;Python is an interpreted scripting language, so it does not need to be compiled. It means it executes the code line by line. Python comes with a &lt;em&gt;Python Shell (Python Interactive Shell)&lt;/em&gt;. It is used to execute a single python command and get the result.&lt;/p&gt; 
 &lt;p&gt;Python Shell waits for the Python code from the user. When you enter the code, it interprets the code and shows the result in the next line. Open your terminal or command prompt(cmd) and write:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;python
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/opening_python_shell.png" alt="Python Scripting Shell" /&gt;&lt;/p&gt; 
 &lt;p&gt;The Python interactive shell is opened and it is waiting for you to write Python code(Python script). You will write your Python script next to this symbol &amp;gt;&amp;gt;&amp;gt; and then click Enter. Let us write our very first script on the Python scripting shell.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/adding_on_python_shell.png" alt="Python script on Python shell" /&gt;&lt;/p&gt; 
 &lt;p&gt;Well done, you wrote your first Python script on Python interactive shell. How do we close the Python interactive shell ? To close the shell, next to this symbol &amp;gt;&amp;gt;&amp;gt; write &lt;strong&gt;exit()&lt;/strong&gt; command and press Enter.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/exit_from_shell.png" alt="Exit from python shell" /&gt;&lt;/p&gt; 
 &lt;p&gt;Now, you know how to open the Python interactive shell and how to exit from it.&lt;/p&gt; 
 &lt;p&gt;Python will give you results if you write scripts that Python understands, if not it returns errors. Let's make a deliberate mistake and see what Python will return.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/invalid_syntax_error.png" alt="Invalid Syntax Error" /&gt;&lt;/p&gt; 
 &lt;p&gt;As you can see from the returned error, Python is so clever that it knows the mistake we made and which was &lt;em&gt;Syntax Error: invalid syntax&lt;/em&gt;. Using x as multiplication in Python is a syntax error because (x) is not a valid syntax in Python. Instead of (&lt;strong&gt;x&lt;/strong&gt;) we use asterisk (*) for multiplication. The returned error clearly shows what to fix.&lt;/p&gt; 
 &lt;p&gt;The process of identifying and removing errors from a program is called &lt;em&gt;debugging&lt;/em&gt;. Let us debug it by putting * in place of &lt;strong&gt;x&lt;/strong&gt;.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/fixing_syntax_error.png" alt="Fixing Syntax Error" /&gt;&lt;/p&gt; 
 &lt;p&gt;Our bug was fixed, the code ran and we got a result we were expecting. As a programmer you will see such kind of errors on daily basis. It is good to know how to debug. To be good at debugging you should understand what kind of errors you are facing. Some of the Python errors you may encounter are &lt;em&gt;SyntaxError&lt;/em&gt;, &lt;em&gt;IndexError&lt;/em&gt;, &lt;em&gt;NameError&lt;/em&gt;, &lt;em&gt;ModuleNotFoundError&lt;/em&gt;, &lt;em&gt;KeyError&lt;/em&gt;, &lt;em&gt;ImportError&lt;/em&gt;, &lt;em&gt;AttributeError&lt;/em&gt;, &lt;em&gt;TypeError&lt;/em&gt;, &lt;em&gt;ValueError&lt;/em&gt;, &lt;em&gt;ZeroDivisionError&lt;/em&gt; etc. We will see more about different Python &lt;strong&gt;&lt;em&gt;error types&lt;/em&gt;&lt;/strong&gt; in later sections.&lt;/p&gt; 
 &lt;p&gt;Let us practice more how to use Python interactive shell. Go to your terminal or command prompt and write the word &lt;strong&gt;python&lt;/strong&gt;.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/opening_python_shell.png" alt="Python Scripting Shell" /&gt;&lt;/p&gt; 
 &lt;p&gt;The Python interactive shell is opened. Let us do some basic mathematical operations (addition, subtraction, multiplication, division, modulus, exponentiation).&lt;/p&gt; 
 &lt;p&gt;Let us do some maths first before we write any Python code:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;2 + 3 = 5&lt;/li&gt; 
  &lt;li&gt;3 - 2 = 1&lt;/li&gt; 
  &lt;li&gt;3 * 2 = 6&lt;/li&gt; 
  &lt;li&gt;3 / 2 = 1.5&lt;/li&gt; 
  &lt;li&gt;3 ** 2 = 3 x 3 = 9&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;In python, we have the following additional operations:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;3 % 2 = 1 =&amp;gt; which means finding the remainder&lt;/li&gt; 
  &lt;li&gt;3 // 2 = 1 =&amp;gt; which means removing the remainder&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;Let us change the above mathematical expressions to Python code. The Python shell has been opened and let us write a comment at the very beginning of the shell.&lt;/p&gt; 
 &lt;p&gt;A &lt;em&gt;comment&lt;/em&gt; is a part of the code which is not executed by python. So we can leave some text in our code to make our code more readable. Python does not run the comment part. A comment in python starts with hash(#) symbol. This is how you write a comment in python&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt; # comment starts with hash
 # this is a python comment, because it starts with a (#) symbol
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/maths_on_python_shell.png" alt="Maths on python shell" /&gt;&lt;/p&gt; 
 &lt;p&gt;Before we move on to the next section, let us practice more on the Python interactive shell. Close the opened shell by writing &lt;em&gt;exit()&lt;/em&gt; on the shell and open it again and let us practice how to write text on the Python shell.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/writing_string_on_shell.png" alt="Writing String on python shell" /&gt;&lt;/p&gt; 
 &lt;h3&gt;Installing Visual Studio Code&lt;/h3&gt; 
 &lt;p&gt;The Python interactive shell is good to try and test small script codes but it will not be for a big project. In real work environment, developers use different code editors to write codes. In this 30 days of Python programming challenge, we will use Visual Studio Code. Visual Studio Code is a very popular open source text editor. I am a fan of vscode and I would recommend to &lt;a href="https://code.visualstudio.com/"&gt;download&lt;/a&gt; visual studio code, but if you are in favor of other editors, feel free to follow with what you have.&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://code.visualstudio.com/"&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/vscode.png" alt="Visual Studio Code" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;If you installed visual studio code, let us see how to use it. If you prefer a video, you can follow this Visual Studio Code for Python &lt;a href="https://www.youtube.com/watch?v=bn7Cx4z-vSo"&gt;Video tutorial&lt;/a&gt;&lt;/p&gt; 
 &lt;h4&gt;How to use visual studio code&lt;/h4&gt; 
 &lt;p&gt;Open the visual studio code by double clicking the visual studio icon. When you open it you will get this kind of interface. Try to interact with the labeled icons.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/vscode_ui.png" alt="Visual studio Code" /&gt;&lt;/p&gt; 
 &lt;p&gt;Create a folder named 30DaysOfPython on your desktop. Then open it using visual studio code.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/how_to_open_project_on_vscode.png" alt="Opening Project on Visual studio" /&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/opening_project.png" alt="Opening a project" /&gt;&lt;/p&gt; 
 &lt;p&gt;After opening it, you will see shortcuts for creating files and folders inside of 30DaysOfPython project's directory. As you can see below, I have created the very first file, &lt;code&gt;helloworld.py&lt;/code&gt;. You can do the same.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/helloworld.png" alt="Creating a python file" /&gt;&lt;/p&gt; 
 &lt;p&gt;After a long day of coding, you want to close your code editor, right? This is how you will close the opened project.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/closing_opened_project.png" alt="Closing project" /&gt;&lt;/p&gt; 
 &lt;p&gt;Congratulations, you have finished setting up the development environment. Let us start coding.&lt;/p&gt; 
 &lt;h2&gt;Basic Python&lt;/h2&gt; 
 &lt;h3&gt;Python Syntax&lt;/h3&gt; 
 &lt;p&gt;A Python script can be written in Python interactive shell or in the code editor. A Python file has an extension .py.&lt;/p&gt; 
 &lt;h3&gt;Python Indentation&lt;/h3&gt; 
 &lt;p&gt;An indentation is a white space in a text. Indentation in many languages is used to increase code readability; however, Python uses indentation to create blocks of code. In other programming languages, curly brackets are used to create code blocks instead of indentation. One of the common bugs when writing Python code is incorrect indentation.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/indentation.png" alt="Indentation Error" /&gt;&lt;/p&gt; 
 &lt;h3&gt;Comments&lt;/h3&gt; 
 &lt;p&gt;Comments play a crucial role in enhancing code readability and allowing developers to leave notes within their code. In Python, any text preceded by a hash (#) symbol is considered a comment and is not executed when the code runs.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Example: Single Line Comment&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;    # This is the first comment
    # This is the second comment
    # Python is eating the world
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Example: Multiline Comment&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Triple quote can be used for multiline comment if it is not assigned to a variable&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;"""This is multiline comment
multiline comment takes multiple lines.
python is eating the world
"""
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;Data types&lt;/h3&gt; 
 &lt;p&gt;In Python there are several types of data types. Let us get started with the most common ones. Different data types will be covered in detail in other sections. For the time being, let us just go through the different data types and get familiar with them. You do not have to have a clear understanding now.&lt;/p&gt; 
 &lt;h4&gt;Number&lt;/h4&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Integer: Integer(negative, zero and positive) numbers Example: ... -3, -2, -1, 0, 1, 2, 3 ...&lt;/li&gt; 
  &lt;li&gt;Float: Decimal number Example ... -3.5, -2.25, -1.0, 0.0, 1.1, 2.2, 3.5 ...&lt;/li&gt; 
  &lt;li&gt;Complex Example 1 + j, 2 + 4j&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h4&gt;String&lt;/h4&gt; 
 &lt;p&gt;A collection of one or more characters under a single or double quote. If a string is more than one sentence then we use a triple quote.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-py"&gt;'Asabeneh'
'Finland'
'Python'
'I love teaching'
'I hope you are enjoying the first day of 30DaysOfPython Challenge'
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Booleans&lt;/h4&gt; 
 &lt;p&gt;A boolean data type is either a True or False value. T and F should be always uppercase.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;    True  #  Is the light on? If it is on, then the value is True
    False # Is the light on? If it is off, then the value is False
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;List&lt;/h4&gt; 
 &lt;p&gt;Python list is an ordered collection which allows to store different data type items. A list is similar to an array in JavaScript.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-py"&gt;[0, 1, 2, 3, 4, 5]  # all are the same data types - a list of numbers
['Banana', 'Orange', 'Mango', 'Avocado'] # all the same data types - a list of strings (fruits)
['Finland','Estonia', 'Sweden','Norway'] # all the same data types - a list of strings (countries)
['Banana', 10, False, 9.81] # different data types in the list - string, integer, boolean and float
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Dictionary&lt;/h4&gt; 
 &lt;p&gt;A Python dictionary object is an unordered collection of data in a key value pair format.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-py"&gt;{
'first_name':'Asabeneh',
'last_name':'Yetayeh',
'country':'Finland',
'age':250,
'is_married':True,
'skills':['JS', 'React', 'Node', 'Python']
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Tuple&lt;/h4&gt; 
 &lt;p&gt;A tuple is an ordered collection of different data types like list but tuples can not be modified once they are created. They are immutable.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-py"&gt;('Asabeneh', 'Pawel', 'Brook', 'Abraham', 'Lidiya') # Names
&lt;/code&gt;&lt;/pre&gt; 
 &lt;pre&gt;&lt;code class="language-py"&gt;('Earth', 'Jupiter', 'Neptune', 'Mars', 'Venus', 'Saturn', 'Uranus', 'Mercury') # planets
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Set&lt;/h4&gt; 
 &lt;p&gt;A set is a collection of data types similar to list and tuple. Unlike list and tuple, set is not an ordered collection of items. Like in Mathematics, set in Python stores only unique items.&lt;/p&gt; 
 &lt;p&gt;In later sections, we will go in detail about each and every Python data type.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-py"&gt;{2, 4, 3, 5}
{3.14, 9.81, 2.7} # order is not important in set
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;Checking Data types&lt;/h3&gt; 
 &lt;p&gt;To check the data type of certain data/variable we use the &lt;strong&gt;type&lt;/strong&gt; function. In the following terminal you will see different python data types:&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/checking_data_types.png" alt="Checking Data types" /&gt;&lt;/p&gt; 
 &lt;h3&gt;Python File&lt;/h3&gt; 
 &lt;p&gt;First open your project folder, 30DaysOfPython. If you don't have this folder, create a folder name called 30DaysOfPython. Inside this folder, create a file called helloworld.py. Now, let's do what we did on python interactive shell using visual studio code.&lt;/p&gt; 
 &lt;p&gt;The Python interactive shell was printing without using &lt;strong&gt;print&lt;/strong&gt; but on visual studio code to see our result we should use a built in function &lt;em&gt;print()&lt;/em&gt;. The &lt;em&gt;print()&lt;/em&gt; built-in function takes one or more arguments as follows &lt;em&gt;print('arument1', 'argument2', 'argument3')&lt;/em&gt;. See the examples below.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;The file name is &lt;code&gt;helloworld.py&lt;/code&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-py"&gt;# Day 1 - 30DaysOfPython Challenge

print(2 + 3)             # addition(+)
print(3 - 1)             # subtraction(-)
print(2 * 3)             # multiplication(*)
print(3 / 2)             # division(/)
print(3 ** 2)            # exponential(**)
print(3 % 2)             # modulus(%)
print(3 // 2)            # Floor division operator(//)

# Checking data types
print(type(10))          # Int
print(type(3.14))        # Float
print(type(1 + 3j))      # Complex number
print(type('Asabeneh'))  # String
print(type([1, 2, 3]))   # List
print(type({'name':'Asabeneh'})) # Dictionary
print(type({9.8, 3.14, 2.7}))    # Set
print(type((9.8, 3.14, 2.7)))    # Tuple
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;To run the python file check the image below. You can run the python file either by running the green button on Visual Studio Code or by typing &lt;em&gt;python helloworld.py&lt;/em&gt; in the terminal .&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/running_python_script.png" alt="Running python script" /&gt;&lt;/p&gt; 
 &lt;p&gt;üåï You are amazing. You have just completed day 1 challenge and you are on your way to greatness. Now do some exercises for your brain and muscles.&lt;/p&gt; 
 &lt;h2&gt;üíª Exercises - Day 1&lt;/h2&gt; 
 &lt;h3&gt;Exercise: Level 1&lt;/h3&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Check the python version you are using&lt;/li&gt; 
  &lt;li&gt;Open the python interactive shell and do the following operations. The operands are 3 and 4. 
   &lt;ul&gt; 
    &lt;li&gt;addition(+)&lt;/li&gt; 
    &lt;li&gt;subtraction(-)&lt;/li&gt; 
    &lt;li&gt;multiplication(*)&lt;/li&gt; 
    &lt;li&gt;modulus(%)&lt;/li&gt; 
    &lt;li&gt;division(/)&lt;/li&gt; 
    &lt;li&gt;exponential(**)&lt;/li&gt; 
    &lt;li&gt;floor division operator(//)&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;Write strings on the python interactive shell. The strings are the following: 
   &lt;ul&gt; 
    &lt;li&gt;Your name&lt;/li&gt; 
    &lt;li&gt;Your family name&lt;/li&gt; 
    &lt;li&gt;Your country&lt;/li&gt; 
    &lt;li&gt;I am enjoying 30 days of python&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;Check the data types of the following data: 
   &lt;ul&gt; 
    &lt;li&gt;10&lt;/li&gt; 
    &lt;li&gt;9.8&lt;/li&gt; 
    &lt;li&gt;3.14&lt;/li&gt; 
    &lt;li&gt;4 - 4j&lt;/li&gt; 
    &lt;li&gt;['Asabeneh', 'Python', 'Finland']&lt;/li&gt; 
    &lt;li&gt;Your name&lt;/li&gt; 
    &lt;li&gt;Your family name&lt;/li&gt; 
    &lt;li&gt;Your country&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;h3&gt;Exercise: Level 2&lt;/h3&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Create a folder named day_1 inside 30DaysOfPython folder. Inside day_1 folder, create a python file helloworld.py and repeat questions 1, 2, 3 and 4. Remember to use &lt;em&gt;print()&lt;/em&gt; when you are working on a python file. Navigate to the directory where you have saved your file, and run it.&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;h3&gt;Exercise: Level 3&lt;/h3&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Write an example for different Python data types such as Number(Integer, Float, Complex), String, Boolean, List, Tuple, Set and Dictionary.&lt;/li&gt; 
  &lt;li&gt;Find an &lt;a href="https://en.wikipedia.org/wiki/Euclidean_distance#:~:text=In%20mathematics%2C%20the%20Euclidean%20distance,being%20called%20the%20Pythagorean%20distance."&gt;Euclidean distance&lt;/a&gt; between (2, 3) and (10, 8)&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;üéâ CONGRATULATIONS ! üéâ&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/02_Day_Variables_builtin_functions/02_variables_builtin_functions.md"&gt;Day 2 &amp;gt;&amp;gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>HKUDS/ViMax</title>
      <link>https://github.com/HKUDS/ViMax</link>
      <description>&lt;p&gt;"ViMax: Agentic Video Generation (Director, Screenwriter, Producer, and Video Generator All-in-One)"&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/HKUDS/ViMax/main/assets/vimax.png" /&gt; 
 &lt;br /&gt; 
 &lt;br /&gt; 
 &lt;a href="https://trendshift.io/repositories/15299" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/15299" alt="HKUDS%2FViMax | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; 
 &lt;h1 align="center"&gt;ViMax: Agentic Video Generation&lt;/h1&gt; 
 &lt;div align="center"&gt; 
 &lt;/div&gt; 
 &lt;p align="center"&gt; &lt;img src="https://img.shields.io/badge/üêçPython-3.12-00d9ff?style=for-the-badge&amp;amp;logo=python&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt; &lt;a href="https://github.com/astral-sh/uv"&gt;&lt;img src="https://img.shields.io/badge/‚ö°uv-Ready-ff6b6b?style=for-the-badge&amp;amp;logo=python&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/badge/License-MIT-4ecdc4?style=for-the-badge&amp;amp;logo=opensourceinitiative&amp;amp;logoColor=white" alt="MIT License" /&gt; &lt;/p&gt; 
 &lt;p align="center"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/ViMax/main/Communication.md"&gt;&lt;img src="https://img.shields.io/badge/üí¨Feishu-Group-07c160?style=for-the-badge&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/ViMax/main/Communication.md"&gt;&lt;img src="https://img.shields.io/badge/WeChat-Group-07c160?style=for-the-badge&amp;amp;logo=wechat&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;p align="center"&gt; &lt;a href="https://www.youtube.com/@AI-Creator-is-here"&gt;&lt;img src="https://badges.aleen42.com/src/youtube.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/ViMax/main/readme.md"&gt;&lt;img src="https://img.shields.io/badge/English-1a1a2e?style=for-the-badge" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/ViMax/main/README_ZH.md"&gt;&lt;img src="https://img.shields.io/badge/‰∏≠ÊñáÁâà-1a1a2e?style=for-the-badge" /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;a href="https://raw.githubusercontent.com/HKUDS/ViMax/main/#quick-start" style="text-decoration: none;"&gt; &lt;img src="https://img.shields.io/badge/Quick%20Start-Get%20Started%20Now-FFC107?style=for-the-badge&amp;amp;logo=rocket&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h3&gt;üö® Current Video Generation Limitations:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚ùå &lt;strong&gt;Limited to Short Clips&lt;/strong&gt; - Most AI tools generate only seconds of footage. &lt;br /&gt;&lt;/li&gt; 
 &lt;li&gt;‚ùå &lt;strong&gt;Consistency Chaos&lt;/strong&gt; - Characters and scenes change unpredictably across frames. &lt;br /&gt;&lt;/li&gt; 
 &lt;li&gt;‚ùå &lt;strong&gt;Visual-Only Focus&lt;/strong&gt; - Missing scripts, audio, narrative structure, and storytelling depth. &lt;br /&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üí° ViMax Solution:&lt;/h3&gt; 
&lt;p&gt;üé¨ &lt;strong&gt;Director&lt;/strong&gt;, &lt;strong&gt;Screenwriter&lt;/strong&gt;, &lt;strong&gt;Producer&lt;/strong&gt;, and &lt;strong&gt;Video Generator&lt;/strong&gt; &lt;strong&gt;All-in-One&lt;/strong&gt;! We're exploring a future where AI becomes a complete creative powerhouse. üí° Simply input your concept. ViMax autonomously handles the rest. It orchestrates scriptwriting, storyboarding, character creation, and final video generation‚Äîall end-to-end. üöÄ&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/5bad46b2-8276-4e1d-9480-3522640744b2"&gt;https://github.com/user-attachments/assets/5bad46b2-8276-4e1d-9480-3522640744b2&lt;/a&gt;&lt;/p&gt;  
&lt;hr /&gt; 
&lt;h2&gt;üìë Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/ViMax/main/#key-features"&gt;üí° Key Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/ViMax/main/#Video-Demos-Generated-from-Scratch"&gt;üîÆ Demos&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/ViMax/main/#%EF%B8%8F-architecture"&gt;üèóÔ∏è Architecture&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/ViMax/main/#quick-start"&gt;üöÄ Quick Start&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üí°Key Features&lt;/h2&gt; 
&lt;br /&gt; 
&lt;table align="center" width="100%" style="border: none; table-layout: fixed;"&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td width="25%" align="center" style="vertical-align: top; padding: 20px;"&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;h3 style="margin: 0; padding: 0;"&gt;üåü &lt;strong&gt;Idea2Video&lt;/strong&gt;&lt;/h3&gt; 
    &lt;/div&gt; 
    &lt;div align="center" style="margin: 15px 0;"&gt; 
     &lt;img src="https://img.shields.io/badge/IDEA-GENERATION-ff6b6b?style=for-the-badge&amp;amp;logo=algorithm&amp;amp;logoColor=white" alt="Algorithm Badge" /&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;&lt;strong&gt;From Spark to Screen&lt;/strong&gt;&lt;/p&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 60px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;Transform &lt;strong&gt; raw ideas &lt;/strong&gt; into complete video stories through intelligent multi-agent workflows automating &lt;strong&gt; storytelling, character design, and production &lt;/strong&gt;. &lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
   &lt;td width="25%" align="center" style="vertical-align: top; padding: 20px;"&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;h3 style="margin: 0; padding: 0;"&gt;üé® &lt;strong&gt;Novel2Video&lt;/strong&gt;&lt;/h3&gt; 
    &lt;/div&gt; 
    &lt;div align="center" style="margin: 15px 0;"&gt; 
     &lt;img src="https://img.shields.io/badge/NOVEL-ADAPTATION-4ecdc4?style=for-the-badge&amp;amp;logo=book&amp;amp;logoColor=white" alt="Frontend Badge" /&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;&lt;strong&gt;Smart Literary Adaptation Engine&lt;/strong&gt;&lt;/p&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 60px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;Transform &lt;strong&gt;complete novels&lt;/strong&gt; into &lt;strong&gt;episodic video content&lt;/strong&gt; with intelligent narrative compression, character tracking, and scene-by-scene visual adaptation&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
   &lt;td width="25%" align="center" style="vertical-align: top; padding: 20px;"&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;h3 style="margin: 0; padding: 0;"&gt;‚öôÔ∏è &lt;strong&gt;Script2Video&lt;/strong&gt;&lt;/h3&gt; 
    &lt;/div&gt; 
    &lt;div align="center" style="margin: 15px 0;"&gt; 
     &lt;img src="https://img.shields.io/badge/SCRIPT-ADAPTATION-9b59b6?style=for-the-badge&amp;amp;logo=server&amp;amp;logoColor=white" alt="Backend Badge" /&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;&lt;strong&gt;Unlimited Screenplay Video Creation&lt;/strong&gt;&lt;/p&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 60px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;Unleash your creativity by writing &lt;strong&gt;any screenplay&lt;/strong&gt; from personal stories to epic adventures, giving you complete control over every aspect of your visual storytelling.&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
   &lt;td width="25%" align="center" style="vertical-align: top; padding: 20px;"&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;h3 style="margin: 0; padding: 0;"&gt;ü§≥ &lt;strong&gt;AutoCameo&lt;/strong&gt;&lt;/h3&gt; 
    &lt;/div&gt; 
    &lt;div align="center" style="margin: 15px 0;"&gt; 
     &lt;img src="https://img.shields.io/badge/INTERACTIVE-GENERATION-FFC107?style=for-the-badge&amp;amp;logo=server&amp;amp;logoColor=white" alt="Backend Badge" /&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;&lt;strong&gt;Generate Video from Your Photo&lt;/strong&gt;&lt;/p&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 60px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt; &lt;strong&gt;Create your own cameo&lt;/strong&gt; video, transforming yourself/pet into a guest star who appears across limitless creative scripts, cinematic sequences, and interactive storylines.&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;br /&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üîÆVideo Demos Generated from Scratch&lt;/h2&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td align="center" width="33%"&gt; 
    &lt;video src="https://github.com/user-attachments/assets/c2fb27b0-218c-4976-b3d6-2abf8ea06be7" controls width="100%"&gt;&lt;/video&gt; &lt;/td&gt; 
   &lt;td align="center" width="33%"&gt; 
    &lt;video src="https://github.com/user-attachments/assets/bfa566a8-688d-4d53-a9e2-6cedeb4a399d" controls width="100%"&gt;&lt;/video&gt; &lt;/td&gt; 
   &lt;td align="center" width="33%"&gt; 
    &lt;video src="https://github.com/user-attachments/assets/49f61134-4f78-4285-9a9e-bb5e3e0c4abf" controls width="100%"&gt;&lt;/video&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" width="33%"&gt; 
    &lt;video src="https://github.com/user-attachments/assets/a950f449-a15c-449b-a1b8-c393951aa9be" controls width="100%"&gt;&lt;/video&gt; &lt;/td&gt; 
   &lt;td align="center" width="33%"&gt; 
    &lt;video src="https://github.com/user-attachments/assets/bb3ff0fd-9433-4806-886a-3f77b61d06ec" controls width="100%"&gt;&lt;/video&gt; &lt;/td&gt; 
   &lt;td align="center" width="33%"&gt; 
    &lt;video src="https://github.com/user-attachments/assets/2624a3f0-9f66-4fa4-b527-45c0ea0353fc" controls width="100%"&gt;&lt;/video&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" width="33%"&gt; 
    &lt;video src="https://github.com/user-attachments/assets/5dbb80f7-aff0-4211-940c-a898f91fb80c" controls width="100%"&gt;&lt;/video&gt; &lt;/td&gt; 
   &lt;td align="center" width="33%"&gt; 
    &lt;video src="https://github.com/user-attachments/assets/cc0b0bcd-e7db-4839-950b-0b03949637bd" controls width="100%"&gt;&lt;/video&gt; &lt;/td&gt; 
   &lt;td align="center" width="33%"&gt; 
    &lt;video src="https://github.com/user-attachments/assets/85919b59-80f0-461a-af7e-a93d3fb412fc" controls width="100%"&gt;&lt;/video&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h3&gt;üéØ &lt;strong&gt;End-to-End Video Creation Engine&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;The Challenges&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;üåÖ &lt;strong&gt;Reference Images&lt;/strong&gt;: Time-consuming acquisition, organization, and alignment of reference frames that accurately capture characters, objects, positions, and environments.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ü´† &lt;strong&gt;Consistency Check&lt;/strong&gt;: Sometimes, the image generator may generate unusable images even if it is given the correct characters, position, environment reference image and prompts.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üìÑ &lt;strong&gt;Scripts Generation&lt;/strong&gt;: Professional and high-quality videos need to have rich information density and structured design.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üìù &lt;strong&gt;Storyboard Design&lt;/strong&gt;: Converting stories into visual narratives requires expertise in cinematography, scene composition, and visual storytelling that most creators lack.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üé¨ &lt;strong&gt;Shot Design&lt;/strong&gt;: Creating coherent camera sequences with proper angles, transitions, and pacing while maintaining narrative flow across complex scenes.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üé® &lt;strong&gt;Development Delays&lt;/strong&gt;: Ensuring character appearances, environments, and artistic style remain consistent across hundreds of shots in long-form content.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;‚è±Ô∏è &lt;strong&gt;Production Efficiency&lt;/strong&gt;: Traditional video creation involves multiple specialists and lengthy workflows, creating barriers for independent creators and rapid prototyping.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üé• &lt;strong&gt;Scaling AI Generated Video&lt;/strong&gt;: AI-generated videos are usually only a few seconds long, high-quality long videos at the minute or even hour level require complex cross-scene continuity and multi-storyboards design and processing capabilities.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;ViMAX&lt;/strong&gt;: eliminates these production bottlenecks by automating the entire video creation pipeline from narrative input to final video output.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;üî• &lt;strong&gt;Why ViMax?&lt;/strong&gt;&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;üß† &lt;strong&gt;Effortless Production&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;üöÄ &lt;strong&gt;Complete Creative Freedom&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;üîä &lt;strong&gt;Audio and Video Binding&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;üé® &lt;strong&gt;Professional Quality&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;ü§© &lt;strong&gt;Interactive Video&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;One-Prompt to Finished Video&lt;/td&gt; 
   &lt;td align="center"&gt;From Any Narrative to Reality&lt;/td&gt; 
   &lt;td align="center"&gt;Synchronized Storytelling&lt;/td&gt; 
   &lt;td align="center"&gt;Movie-Grade Output&lt;/td&gt; 
   &lt;td align="center"&gt;Make Your Own Cameo Video&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Skip the technical complexity‚Äîjust describe your vision and let ViMax handle script generation, storyboarding, shot design, reference management, and consistency validation&lt;/td&gt; 
   &lt;td align="center"&gt;No creative limits‚Äîwhether it's a trailer, short story, novel chapter, or original concept, ViMax intelligently structures narratives and designs cinematography to bring any idea to life&lt;/td&gt; 
   &lt;td align="center"&gt;Seamlessly integrate character voice, and sound effects with visual content to create immersive experiences where audio and video work in perfect harmony&lt;/td&gt; 
   &lt;td align="center"&gt;Automated quality control ensures character consistency, proper scene composition, and professional visual standards across every frame of your video&lt;/td&gt; 
   &lt;td align="center"&gt;Interact in your own short stories by uploading your photo‚ÄîViMax intelligently integrates you as a character with consistent appearance and natural interactions throughout the entire video&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h3&gt;‚òÑÔ∏è &lt;strong&gt;Coming Soon&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üë®‚Äçüíª &lt;strong&gt;Google AI Studio API config‚úÖ&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;üìπ &lt;strong&gt;Dev mode branch&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;ü§≥ &lt;strong&gt;AutoCameo integrate&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;üì∫ &lt;strong&gt;More demos&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;üéûÔ∏è &lt;strong&gt;Shot planning&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;ü§ñ &lt;strong&gt;New features&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üèóÔ∏è Architecture&lt;/h2&gt; 
&lt;h3&gt;üìä &lt;strong&gt;System Overview&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;ViMax&lt;/strong&gt; is a multi-agent video framework that enables automated multi-shot video generation while ensuring character and scene consistency. Our system seamlessly translates your ideas into corresponding videos, allowing you to focus on storytelling rather than technical implementation.&lt;/p&gt; 
&lt;p&gt;üéØ &lt;strong&gt;Technical Capabilities&lt;/strong&gt;:&lt;/p&gt; 
&lt;p&gt;üß¨ &lt;strong&gt;Intelligent Long Script Generation&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;RAG-based long script design engine that intelligently analyzes lengthy, novel-like stories and automatically segments them into a multi-scene script format. The process meticulously ensures that all key plot developments and character dialogues are accurately retained within the new structure.&lt;/p&gt; 
&lt;p&gt;ü™Ñ &lt;strong&gt;Expressive Storyboard Design&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Shot-level storyboard design system that create expressive storyboards through cinematography language based on user requirements and target audiences, which establishs the narrative rhythm for subsequent video generation.&lt;/p&gt; 
&lt;p&gt;üîÆ &lt;strong&gt;Multi-camera Filming Simulation&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Simulates multi-camera filming to deliver an immersive viewing experience while maintaining consistent character positioning and backgrounds within the same scene.&lt;/p&gt; 
&lt;p&gt;üß∏ &lt;strong&gt;Intelligent Reference Images Selection&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Intelligently select the reference image required for the first frame of the current video, including the storyboards that occurred in the previous timeline, to ensure the accuracy of multiple characters and environmental elements as the video becomes longer.&lt;/p&gt; 
&lt;p&gt;‚öôÔ∏è &lt;strong&gt;Automated Images Generation&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Based on the selected reference image and the visual logical order on the previous timeline, the prompt of the image generator is automatically generated to reasonably arrange the spatial interaction position between the character and the environment.&lt;/p&gt; 
&lt;p&gt;‚úÖ &lt;strong&gt;Automated Image Generation Consistency Check&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Generate multiple images in parallel and select the best consistent image as the first frame through MLLM/VLM to imitate the workflow of human creators.&lt;/p&gt; 
&lt;p&gt;‚ö° &lt;strong&gt;High-efficiency Parallel Shot Generation&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Parallel processing for sequential shots captured from the same camera enables highly efficient video production.&lt;/p&gt; 
&lt;h3&gt;ü§ñ &lt;strong&gt;Multi-Agent Video Generation Pipeline&lt;/strong&gt;&lt;/h3&gt; 
&lt;div align="center"&gt; 
 &lt;table align="center" width="100%" style="border: none; border-collapse: collapse;"&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td colspan="3" align="center" style="padding: 20px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; color: white; font-weight: bold;"&gt; üß† &lt;strong&gt;INPUT LAYER&lt;/strong&gt;&lt;br /&gt; üìù Idea &amp;amp; Scripts &amp;amp; Novels ‚Ä¢ üí≠ Natural Language Prompts ‚Ä¢ üñºÔ∏è Reference Images ‚Ä¢ üé® Style Directives ‚Ä¢ üß© Configs &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="20"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td colspan="3" align="center" style="padding: 15px; background: linear-gradient(135deg, #ff6b6b 0%, #ee5a24 100%); border-radius: 12px; color: white; font-weight: bold;"&gt; üß≠ &lt;strong&gt;CENTRAL ORCHESTRATION&lt;/strong&gt;&lt;br /&gt; Agent Scheduling ‚Ä¢ Stage Transitions ‚Ä¢ Resource Management ‚Ä¢ Retry/Fallback Logic &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="15"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center" style="padding: 12px; background: linear-gradient(135deg, #3742fa 0%, #2f3542 100%); border-radius: 10px; color: white; width: 50%;"&gt; üßæ &lt;strong&gt;SCRIPT UNDERSTANDING&lt;/strong&gt;&lt;br /&gt; &lt;small&gt;Character/Environment Extraction ‚Ä¢ Scene Boundaries ‚Ä¢ Style Intent&lt;/small&gt; &lt;/td&gt; 
    &lt;td width="10"&gt;&lt;/td&gt; 
    &lt;td align="center" style="padding: 12px; background: linear-gradient(135deg, #8c7ae6 0%, #9c88ff 100%); border-radius: 10px; color: white; width: 50%;"&gt; üé• &lt;strong&gt;SCENE &amp;amp; SHOT PLANNING&lt;/strong&gt;&lt;br /&gt; &lt;small&gt;Storyboard Steps ‚Ä¢ Shot List ‚Ä¢ Key Frames &amp;amp; Beats&lt;/small&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="15"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td colspan="3" align="center" style="padding: 15px; background: linear-gradient(135deg, #00d2d3 0%, #54a0ff 100%); border-radius: 12px; color: white; font-weight: bold;"&gt; üß™ &lt;strong&gt;VISUAL ASSET PLANNING&lt;/strong&gt;&lt;br /&gt; Reference Image Selection ‚Ä¢ Look/Style Guidance ‚Ä¢ Prompt Conditioning &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="15"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;!-- Swapped: ASSET INDEXING is now on the left --&gt; 
    &lt;td align="center" style="padding: 12px; background: linear-gradient(135deg, #e056fd 0%, #f368e0 100%); border-radius: 10px; color: white; width: 50%;"&gt; üóÇÔ∏è &lt;strong&gt;ASSET INDEXING&lt;/strong&gt;&lt;br /&gt; &lt;small&gt;Frames/Refs Catalog ‚Ä¢ Embeddings ‚Ä¢ Retrieval for Reuse&lt;/small&gt; &lt;/td&gt; 
    &lt;td width="10"&gt;&lt;/td&gt; 
    &lt;!-- Swapped: CONSISTENCY &amp; CONTINUITY is now on the right --&gt; 
    &lt;td align="center" style="padding: 12px; background: linear-gradient(135deg, #ffa726 0%, #ff7043 100%); border-radius: 10px; color: white; width: 50%;"&gt; ‚ôªÔ∏è &lt;strong&gt;CONSISTENCY &amp;amp; CONTINUITY&lt;/strong&gt;&lt;br /&gt; &lt;small&gt;Character/Environment Tracking ‚Ä¢ Ref Matching ‚Ä¢ Temporal Coherence&lt;/small&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="15"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td colspan="3" align="center" style="padding: 15px; background: linear-gradient(135deg, #26de81 0%, #20bf6b 100%); border-radius: 12px; color: white; font-weight: bold;"&gt; ‚úÇÔ∏è &lt;strong&gt;VISUAL SYNTHESIS &amp;amp; ASSEMBLY&lt;/strong&gt;&lt;br /&gt; Image Generation ‚Ä¢ Best-Frame Selection ‚Ä¢ First/Last-Frame‚ÜíVideo ‚Ä¢ Cut &amp;amp; Timeline Assembly &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="15"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td colspan="3" align="center" style="padding: 20px; background: linear-gradient(135deg, #045de9 0%, #09c6f9 100%); border-radius: 15px; color: white; font-weight: bold;"&gt; üöÄ &lt;strong&gt;OUTPUT LAYER&lt;/strong&gt;&lt;br /&gt; üñºÔ∏è Frames ‚Ä¢ üéûÔ∏è Clips &amp;amp; Final Videos ‚Ä¢ üìú Logs ‚Ä¢ üì¶ Working Directory Artifacts &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;üöÄQuick Start&lt;/h2&gt; 
&lt;h3&gt;üñ•Ô∏è &lt;strong&gt;Environment&lt;/strong&gt;&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;OS: Linux, Windows
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;üì• &lt;strong&gt;Clone and Install&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;We use uv to manage the environment. For uv installation, please refer to the &lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;https://docs.astral.sh/uv/getting-started/installation/&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/HKUDS/ViMax.git
cd ViMax
uv sync
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;üéØ &lt;strong&gt;Usage&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;main_idea2video.py is used to convert your ideas into videos. You need to configure the model and API key information in the configs/idea2video.yaml file, including three parts‚Äîthe chat model, the image generator, and the video generator, as shown below&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;chat_model:
  init_args:
    model: google/gemini-2.5-flash-lite-preview-09-2025
    model_provider: openai
    api_key: &amp;lt;YOUR_API_KEY&amp;gt;
    base_url: https://openrouter.ai/api/v1

image_generator:
  class_path: tools.ImageGeneratorNanobananaGoogleAPI
  init_args:
    api_key: &amp;lt;YOUR_API_KEY&amp;gt;

video_generator:
  class_path: tools.VideoGeneratorVeoGoogleAPI
  init_args:
    api_key: &amp;lt;YOUR_API_KEY&amp;gt;

working_dir: .working_dir/idea2video
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then, provide a simple yet thoughtful idea and the corresponding creative requirements in main_idea2video.py.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;idea = \
"""
If a cat and a dog are best friends, what would happen when they meet a new cat?
"""
user_requirement = \
"""
For children, do not exceed 3 scenes.
"""
style = "Cartoon"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;main_script2video.py generates a video based on a specific script. You similarly need to set up the API configuration in configs/script2video.yaml file. Then, provide a scene script and the corresponding creative requirements in main_script2video.py, as shown below.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;script = \
"""
EXT. SCHOOL GYM - DAY
A group of students are practicing basketball in the gym. The gym is large and open, with a basketball hoop at one end and a large crowd of spectators at the other end. John (18, male, tall, athletic) is the star player, and he is practicing his dribble and shot. Jane (17, female, short, athletic) is the assistant coach, and she is helping John with his practice. The other students are watching the practice and cheering for John.
John: (dribbling the ball) I'm going to score a basket!
Jane: (smiling) Good job, John!
John: (shooting the ball) Yes!
...
"""
user_requirement = \
"""
Fast-paced with no more than 20 shots.
"""
style = "Animate Style"
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;üåü If this project helps you, please give us a Star!&lt;/strong&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;em&gt; ‚ù§Ô∏è Thanks for visiting ‚ú® ViMax!&lt;/em&gt;&lt;br /&gt;&lt;br /&gt; &lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>