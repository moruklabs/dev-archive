<rss version="2.0">
  <channel>
    <title>GitHub All Languages Daily Trending</title>
    <description>Daily Trending of All Languages in GitHub</description>
    <pubDate>Mon, 26 Jan 2026 01:32:55 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>openai/codex</title>
      <link>https://github.com/openai/codex</link>
      <description>&lt;p&gt;Lightweight coding agent that runs in your terminal&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt;&lt;code&gt;npm i -g @openai/codex&lt;/code&gt;&lt;br /&gt;or &lt;code&gt;brew install --cask codex&lt;/code&gt;&lt;/p&gt; 
&lt;p align="center"&gt;&lt;strong&gt;Codex CLI&lt;/strong&gt; is a coding agent from OpenAI that runs locally on your computer. &lt;/p&gt;
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/openai/codex/main/.github/codex-cli-splash.png" alt="Codex CLI splash" width="80%" /&gt; &lt;/p&gt; 
&lt;br /&gt; If you want Codex in your code editor (VS Code, Cursor, Windsurf), 
&lt;a href="https://developers.openai.com/codex/ide"&gt;install in your IDE.&lt;/a&gt; 
&lt;br /&gt;If you are looking for the 
&lt;em&gt;cloud-based agent&lt;/em&gt; from OpenAI, 
&lt;strong&gt;Codex Web&lt;/strong&gt;, go to 
&lt;a href="https://chatgpt.com/codex"&gt;chatgpt.com/codex&lt;/a&gt;.
&lt;p&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;h3&gt;Installing and running Codex CLI&lt;/h3&gt; 
&lt;p&gt;Install globally with your preferred package manager:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# Install using npm
npm install -g @openai/codex
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# Install using Homebrew
brew install --cask codex
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then simply run &lt;code&gt;codex&lt;/code&gt; to get started.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;You can also go to the &lt;a href="https://github.com/openai/codex/releases/latest"&gt;latest GitHub Release&lt;/a&gt; and download the appropriate binary for your platform.&lt;/summary&gt; 
 &lt;p&gt;Each GitHub Release contains many executables, but in practice, you likely want one of these:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;macOS 
   &lt;ul&gt; 
    &lt;li&gt;Apple Silicon/arm64: &lt;code&gt;codex-aarch64-apple-darwin.tar.gz&lt;/code&gt;&lt;/li&gt; 
    &lt;li&gt;x86_64 (older Mac hardware): &lt;code&gt;codex-x86_64-apple-darwin.tar.gz&lt;/code&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;Linux 
   &lt;ul&gt; 
    &lt;li&gt;x86_64: &lt;code&gt;codex-x86_64-unknown-linux-musl.tar.gz&lt;/code&gt;&lt;/li&gt; 
    &lt;li&gt;arm64: &lt;code&gt;codex-aarch64-unknown-linux-musl.tar.gz&lt;/code&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;Each archive contains a single entry with the platform baked into the name (e.g., &lt;code&gt;codex-x86_64-unknown-linux-musl&lt;/code&gt;), so you likely want to rename it to &lt;code&gt;codex&lt;/code&gt; after extracting it.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h3&gt;Using Codex with your ChatGPT plan&lt;/h3&gt; 
&lt;p&gt;Run &lt;code&gt;codex&lt;/code&gt; and select &lt;strong&gt;Sign in with ChatGPT&lt;/strong&gt;. We recommend signing into your ChatGPT account to use Codex as part of your Plus, Pro, Team, Edu, or Enterprise plan. &lt;a href="https://help.openai.com/en/articles/11369540-codex-in-chatgpt"&gt;Learn more about what's included in your ChatGPT plan&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You can also use Codex with an API key, but this requires &lt;a href="https://developers.openai.com/codex/auth#sign-in-with-an-api-key"&gt;additional setup&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Docs&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://developers.openai.com/codex"&gt;&lt;strong&gt;Codex Documentation&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/contributing.md"&gt;&lt;strong&gt;Contributing&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/install.md"&gt;&lt;strong&gt;Installing &amp;amp; building&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/open-source-fund.md"&gt;&lt;strong&gt;Open source fund&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This repository is licensed under the &lt;a href="https://raw.githubusercontent.com/openai/codex/main/LICENSE"&gt;Apache-2.0 License&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>VectifyAI/PageIndex</title>
      <link>https://github.com/VectifyAI/PageIndex</link>
      <description>&lt;p&gt;üìë PageIndex: Document Index for Vectorless, Reasoning-based RAG&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;a href="https://vectify.ai/pageindex" target="_blank"&gt; &lt;img src="https://github.com/user-attachments/assets/46201e72-675b-43bc-bfbd-081cc6b65a1d" alt="PageIndex Banner" /&gt; &lt;/a&gt; 
 &lt;br /&gt; 
 &lt;br /&gt; 
 &lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/14736" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/14736" alt="VectifyAI%2FPageIndex | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;h1&gt;PageIndex: Vectorless, Reasoning-based RAG&lt;/h1&gt; 
 &lt;p align="center"&gt;&lt;b&gt;Reasoning-based RAG&amp;nbsp; ‚ó¶ &amp;nbsp;No Vector DB&amp;nbsp; ‚ó¶ &amp;nbsp;No Chunking&amp;nbsp; ‚ó¶ &amp;nbsp;Human-like Retrieval&lt;/b&gt;&lt;/p&gt; 
 &lt;h4 align="center"&gt; &lt;a href="https://vectify.ai"&gt;üè† Homepage&lt;/a&gt;&amp;nbsp; ‚Ä¢ &amp;nbsp; &lt;a href="https://chat.pageindex.ai"&gt;üñ•Ô∏è Chat Platform&lt;/a&gt;&amp;nbsp; ‚Ä¢ &amp;nbsp; &lt;a href="https://pageindex.ai/mcp"&gt;üîå MCP&lt;/a&gt;&amp;nbsp; ‚Ä¢ &amp;nbsp; &lt;a href="https://docs.pageindex.ai"&gt;üìö Docs&lt;/a&gt;&amp;nbsp; ‚Ä¢ &amp;nbsp; &lt;a href="https://discord.com/invite/VuXuf29EUj"&gt;üí¨ Discord&lt;/a&gt;&amp;nbsp; ‚Ä¢ &amp;nbsp; &lt;a href="https://ii2abc2jejf.typeform.com/to/tK3AXl8T"&gt;‚úâÔ∏è Contact&lt;/a&gt;&amp;nbsp; &lt;/h4&gt; 
&lt;/div&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;h3&gt;üì¢ Latest Updates&lt;/h3&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;üî• Releases:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://chat.pageindex.ai"&gt;&lt;strong&gt;PageIndex Chat&lt;/strong&gt;&lt;/a&gt;: The first human-like document-analysis agent &lt;a href="https://chat.pageindex.ai"&gt;platform&lt;/a&gt; built for professional long documents. Can also be integrated via &lt;a href="https://pageindex.ai/mcp"&gt;MCP&lt;/a&gt; or &lt;a href="https://docs.pageindex.ai/quickstart"&gt;API&lt;/a&gt; (beta).&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;!-- - [**PageIndex Chat API**](https://docs.pageindex.ai/quickstart): An API that brings PageIndex's advanced long-document intelligence directly into your applications and workflows. --&gt; 
 &lt;!-- - [PageIndex MCP](https://pageindex.ai/mcp): Bring PageIndex into Claude, Cursor, or any MCP-enabled agent. Chat with long PDFs in a reasoning-based, human-like way. --&gt; 
 &lt;p&gt;&lt;strong&gt;üìù Articles:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://pageindex.ai/blog/pageindex-intro"&gt;&lt;strong&gt;PageIndex Framework&lt;/strong&gt;&lt;/a&gt;: Introduces the PageIndex framework ‚Äî an &lt;em&gt;agentic, in-context&lt;/em&gt; &lt;em&gt;tree index&lt;/em&gt; that enables LLMs to perform &lt;em&gt;reasoning-based&lt;/em&gt;, &lt;em&gt;human-like retrieval&lt;/em&gt; over long documents, without vector DB or chunking.&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;!-- - [Do We Still Need OCR?](https://pageindex.ai/blog/do-we-need-ocr): Explores how vision-based, reasoning-native RAG challenges the traditional OCR pipeline, and why the future of document AI might be *vectorless* and *vision-based*. --&gt; 
 &lt;p&gt;&lt;strong&gt;üß™ Cookbooks:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://docs.pageindex.ai/cookbook/vectorless-rag-pageindex"&gt;Vectorless RAG&lt;/a&gt;: A minimal, hands-on example of reasoning-based RAG using PageIndex. No vectors, no chunking, and human-like retrieval.&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://docs.pageindex.ai/cookbook/vision-rag-pageindex"&gt;Vision-based Vectorless RAG&lt;/a&gt;: OCR-free, vision-only RAG with PageIndex's reasoning-native retrieval workflow that works directly over PDF page images.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h1&gt;üìë Introduction to PageIndex&lt;/h1&gt; 
&lt;p&gt;Are you frustrated with vector database retrieval accuracy for long professional documents? Traditional vector-based RAG relies on semantic &lt;em&gt;similarity&lt;/em&gt; rather than true &lt;em&gt;relevance&lt;/em&gt;. But &lt;strong&gt;similarity ‚â† relevance&lt;/strong&gt; ‚Äî what we truly need in retrieval is &lt;strong&gt;relevance&lt;/strong&gt;, and that requires &lt;strong&gt;reasoning&lt;/strong&gt;. When working with professional documents that demand domain expertise and multi-step reasoning, similarity search often falls short.&lt;/p&gt; 
&lt;p&gt;Inspired by AlphaGo, we propose &lt;strong&gt;&lt;a href="https://vectify.ai/pageindex"&gt;PageIndex&lt;/a&gt;&lt;/strong&gt; ‚Äî a &lt;strong&gt;vectorless&lt;/strong&gt;, &lt;strong&gt;reasoning-based RAG&lt;/strong&gt; system that builds a &lt;strong&gt;hierarchical tree index&lt;/strong&gt; from long documents and uses LLMs to &lt;strong&gt;reason&lt;/strong&gt; &lt;em&gt;over that index&lt;/em&gt; for &lt;strong&gt;agentic, context-aware retrieval&lt;/strong&gt;. It simulates how &lt;em&gt;human experts&lt;/em&gt; navigate and extract knowledge from complex documents through &lt;em&gt;tree search&lt;/em&gt;, enabling LLMs to &lt;em&gt;think&lt;/em&gt; and &lt;em&gt;reason&lt;/em&gt; their way to the most relevant document sections. PageIndex performs retrieval in two steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Generate a ‚ÄúTable-of-Contents‚Äù &lt;strong&gt;tree structure index&lt;/strong&gt; of documents&lt;/li&gt; 
 &lt;li&gt;Perform reasoning-based retrieval through &lt;strong&gt;tree search&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://pageindex.ai/blog/pageindex-intro" target="_blank" title="The PageIndex Framework"&gt; &lt;img src="https://docs.pageindex.ai/images/cookbook/vectorless-rag.png" width="70%" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h3&gt;üéØ Core Features&lt;/h3&gt; 
&lt;p&gt;Compared to traditional vector-based RAG, &lt;strong&gt;PageIndex&lt;/strong&gt; features:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;No Vector DB&lt;/strong&gt;: Uses document structure and LLM reasoning for retrieval, instead of vector similarity search.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;No Chunking&lt;/strong&gt;: Documents are organized into natural sections, not artificial chunks.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Human-like Retrieval&lt;/strong&gt;: Simulates how human experts navigate and extract knowledge from complex documents.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Better Explainability and Traceability&lt;/strong&gt;: Retrieval is based on reasoning ‚Äî traceable and interpretable, with page and section references. No more opaque, approximate vector search (‚Äúvibe retrieval‚Äù).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;PageIndex powers a reasoning-based RAG system that achieved &lt;strong&gt;state-of-the-art&lt;/strong&gt; &lt;a href="https://github.com/VectifyAI/Mafin2.5-FinanceBench"&gt;98.7% accuracy&lt;/a&gt; on FinanceBench, demonstrating superior performance over vector-based RAG solutions in professional document analysis (see our &lt;a href="https://vectify.ai/blog/Mafin2.5"&gt;blog post&lt;/a&gt; for details).&lt;/p&gt; 
&lt;h3&gt;üìç Explore PageIndex&lt;/h3&gt; 
&lt;p&gt;To learn more, please see a detailed introduction of the &lt;a href="https://pageindex.ai/blog/pageindex-intro"&gt;PageIndex framework&lt;/a&gt;. Check out this GitHub repo for open-source code, and the &lt;a href="https://docs.pageindex.ai/cookbook"&gt;cookbooks&lt;/a&gt;, &lt;a href="https://docs.pageindex.ai/tutorials"&gt;tutorials&lt;/a&gt;, and &lt;a href="https://pageindex.ai/blog"&gt;blog&lt;/a&gt; for additional usage guides and examples.&lt;/p&gt; 
&lt;p&gt;The PageIndex service is available as a ChatGPT-style &lt;a href="https://chat.pageindex.ai"&gt;chat platform&lt;/a&gt;, or can be integrated via &lt;a href="https://pageindex.ai/mcp"&gt;MCP&lt;/a&gt; or &lt;a href="https://docs.pageindex.ai/quickstart"&gt;API&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;üõ†Ô∏è Deployment Options&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Self-host ‚Äî run locally with this open-source repo.&lt;/li&gt; 
 &lt;li&gt;Cloud Service ‚Äî try instantly with our &lt;a href="https://chat.pageindex.ai/"&gt;Chat Platform&lt;/a&gt;, or integrate with &lt;a href="https://pageindex.ai/mcp"&gt;MCP&lt;/a&gt; or &lt;a href="https://docs.pageindex.ai/quickstart"&gt;API&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;Enterprise&lt;/em&gt; ‚Äî private or on-prem deployment. &lt;a href="https://ii2abc2jejf.typeform.com/to/tK3AXl8T"&gt;Contact us&lt;/a&gt; or &lt;a href="https://calendly.com/pageindex/meet"&gt;book a demo&lt;/a&gt; for more details.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üß™ Quick Hands-on&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Try the &lt;a href="https://github.com/VectifyAI/PageIndex/raw/main/cookbook/pageindex_RAG_simple.ipynb"&gt;&lt;strong&gt;Vectorless RAG&lt;/strong&gt;&lt;/a&gt; notebook ‚Äî a &lt;em&gt;minimal&lt;/em&gt;, hands-on example of reasoning-based RAG using PageIndex.&lt;/li&gt; 
 &lt;li&gt;Experiment with &lt;a href="https://github.com/VectifyAI/PageIndex/raw/main/cookbook/vision_RAG_pageindex.ipynb"&gt;&lt;em&gt;Vision-based Vectorless RAG&lt;/em&gt;&lt;/a&gt; ‚Äî no OCR; a minimal, reasoning-native RAG pipeline that works directly over page images.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://colab.research.google.com/github/VectifyAI/PageIndex/blob/main/cookbook/pageindex_RAG_simple.ipynb" target="_blank" rel="noopener"&gt; &lt;img src="https://img.shields.io/badge/Open_In_Colab-Vectorless_RAG-orange?style=for-the-badge&amp;amp;logo=googlecolab" alt="Open in Colab: Vectorless RAG" /&gt; &lt;/a&gt; &amp;nbsp;&amp;nbsp; 
 &lt;a href="https://colab.research.google.com/github/VectifyAI/PageIndex/blob/main/cookbook/vision_RAG_pageindex.ipynb" target="_blank" rel="noopener"&gt; &lt;img src="https://img.shields.io/badge/Open_In_Colab-Vision_RAG-orange?style=for-the-badge&amp;amp;logo=googlecolab" alt="Open in Colab: Vision RAG" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h1&gt;üå≤ PageIndex Tree Structure&lt;/h1&gt; 
&lt;p&gt;PageIndex can transform lengthy PDF documents into a semantic &lt;strong&gt;tree structure&lt;/strong&gt;, similar to a &lt;em&gt;"table of contents"&lt;/em&gt; but optimized for use with Large Language Models (LLMs). It's ideal for: financial reports, regulatory filings, academic textbooks, legal or technical manuals, and any document that exceeds LLM context limits.&lt;/p&gt; 
&lt;p&gt;Below is an example PageIndex tree structure. Also see more example &lt;a href="https://github.com/VectifyAI/PageIndex/tree/main/tests/pdfs"&gt;documents&lt;/a&gt; and generated &lt;a href="https://github.com/VectifyAI/PageIndex/tree/main/tests/results"&gt;tree structures&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsonc"&gt;...
{
  "title": "Financial Stability",
  "node_id": "0006",
  "start_index": 21,
  "end_index": 22,
  "summary": "The Federal Reserve ...",
  "nodes": [
    {
      "title": "Monitoring Financial Vulnerabilities",
      "node_id": "0007",
      "start_index": 22,
      "end_index": 28,
      "summary": "The Federal Reserve's monitoring ..."
    },
    {
      "title": "Domestic and International Cooperation and Coordination",
      "node_id": "0008",
      "start_index": 28,
      "end_index": 31,
      "summary": "In 2023, the Federal Reserve collaborated ..."
    }
  ]
}
...
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can generate the PageIndex tree structure with this open-source repo, or use our &lt;a href="https://docs.pageindex.ai/quickstart"&gt;API&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h1&gt;‚öôÔ∏è Package Usage&lt;/h1&gt; 
&lt;p&gt;You can follow these steps to generate a PageIndex tree from a PDF document.&lt;/p&gt; 
&lt;h3&gt;1. Install dependencies&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip3 install --upgrade -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Set your OpenAI API key&lt;/h3&gt; 
&lt;p&gt;Create a &lt;code&gt;.env&lt;/code&gt; file in the root directory and add your API key:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CHATGPT_API_KEY=your_openai_key_here
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. Run PageIndex on your PDF&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python3 run_pageindex.py --pdf_path /path/to/your/document.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Optional parameters&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; You can customize the processing with additional optional arguments: 
 &lt;pre&gt;&lt;code&gt;--model                 OpenAI model to use (default: gpt-4o-2024-11-20)
--toc-check-pages       Pages to check for table of contents (default: 20)
--max-pages-per-node    Max pages per node (default: 10)
--max-tokens-per-node   Max tokens per node (default: 20000)
--if-add-node-id        Add node ID (yes/no, default: yes)
--if-add-node-summary   Add node summary (yes/no, default: yes)
--if-add-doc-description Add doc description (yes/no, default: yes)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Markdown support&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; We also provide markdown support for PageIndex. You can use the `-md_path` flag to generate a tree structure for a markdown file. 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python3 run_pageindex.py --md_path /path/to/your/document.md
&lt;/code&gt;&lt;/pre&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;Note: in this function, we use "#" to determine node heading and their levels. For example, "##" is level 2, "###" is level 3, etc. Make sure your markdown file is formatted correctly. If your Markdown file was converted from a PDF or HTML, we don't recommend using this function, since most existing conversion tools cannot preserve the original hierarchy. Instead, use our &lt;a href="https://pageindex.ai/blog/ocr"&gt;PageIndex OCR&lt;/a&gt;, which is designed to preserve the original hierarchy, to convert the PDF to a markdown file and then use this function.&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/details&gt; 
&lt;!-- 
# ‚òÅÔ∏è Improved Tree Generation with PageIndex OCR

This repo is designed for generating PageIndex tree structure for simple PDFs, but many real-world use cases involve complex PDFs that are hard to parse by classic Python tools. However, extracting high-quality text from PDF documents remains a non-trivial challenge. Most OCR tools only extract page-level content, losing the broader document context and hierarchy.

To address this, we introduced PageIndex OCR ‚Äî the first long-context OCR model designed to preserve the global structure of documents. PageIndex OCR significantly outperforms other leading OCR tools, such as those from Mistral and Contextual AI, in recognizing true hierarchy and semantic relationships across document pages.

- Experience next-level OCR quality with PageIndex OCR at our [Dashboard](https://dash.pageindex.ai/).
- Integrate PageIndex OCR seamlessly into your stack via our [API](https://docs.pageindex.ai/quickstart).

&lt;p align="center"&gt;
  &lt;img src="https://github.com/user-attachments/assets/eb35d8ae-865c-4e60-a33b-ebbd00c41732" width="80%"&gt;
&lt;/p&gt;
--&gt; 
&lt;hr /&gt; 
&lt;h1&gt;üìà Case Study: PageIndex Leads Finance QA Benchmark&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://vectify.ai/mafin"&gt;Mafin 2.5&lt;/a&gt; is a reasoning-based RAG system for financial document analysis, powered by &lt;strong&gt;PageIndex&lt;/strong&gt;. It achieved a state-of-the-art &lt;a href="https://vectify.ai/blog/Mafin2.5"&gt;&lt;strong&gt;98.7% accuracy&lt;/strong&gt;&lt;/a&gt; on the &lt;a href="https://arxiv.org/abs/2311.11944"&gt;FinanceBench&lt;/a&gt; benchmark, significantly outperforming traditional vector-based RAG systems.&lt;/p&gt; 
&lt;p&gt;PageIndex's hierarchical indexing and reasoning-driven retrieval enable precise navigation and extraction of relevant context from complex financial reports, such as SEC filings and earnings disclosures.&lt;/p&gt; 
&lt;p&gt;Explore the full &lt;a href="https://github.com/VectifyAI/Mafin2.5-FinanceBench"&gt;benchmark results&lt;/a&gt; and our &lt;a href="https://vectify.ai/blog/Mafin2.5"&gt;blog post&lt;/a&gt; for detailed comparisons and performance metrics.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/VectifyAI/Mafin2.5-FinanceBench"&gt; &lt;img src="https://github.com/user-attachments/assets/571aa074-d803-43c7-80c4-a04254b782a3" width="70%" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h1&gt;üß≠ Resources&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;üß™ &lt;a href="https://docs.pageindex.ai/cookbook/vectorless-rag-pageindex"&gt;Cookbooks&lt;/a&gt;: hands-on, runnable examples and advanced use cases.&lt;/li&gt; 
 &lt;li&gt;üìñ &lt;a href="https://docs.pageindex.ai/doc-search"&gt;Tutorials&lt;/a&gt;: practical guides and strategies, including &lt;em&gt;Document Search&lt;/em&gt; and &lt;em&gt;Tree Search&lt;/em&gt;.&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://pageindex.ai/blog"&gt;Blog&lt;/a&gt;: technical articles, research insights, and product updates.&lt;/li&gt; 
 &lt;li&gt;üîå &lt;a href="https://pageindex.ai/mcp#quick-setup"&gt;MCP setup&lt;/a&gt; &amp;amp; &lt;a href="https://docs.pageindex.ai/quickstart"&gt;API docs&lt;/a&gt;: integration details and configuration options.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h1&gt;‚≠ê Support Us&lt;/h1&gt; 
&lt;p&gt;Leave us a star üåü if you like our project. Thank you!&lt;/p&gt; 
&lt;p&gt; &lt;img src="https://github.com/user-attachments/assets/eae4ff38-48ae-4a7c-b19f-eab81201d794" width="80%" /&gt; &lt;/p&gt; 
&lt;h3&gt;Connect with Us&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://x.com/PageIndexAI"&gt;&lt;img src="https://img.shields.io/badge/Twitter-000000?style=for-the-badge&amp;amp;logo=x&amp;amp;logoColor=white" alt="Twitter" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://www.linkedin.com/company/vectify-ai/"&gt;&lt;img src="https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&amp;amp;logo=linkedin&amp;amp;logoColor=white" alt="LinkedIn" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://discord.com/invite/VuXuf29EUj"&gt;&lt;img src="https://img.shields.io/badge/Discord-5865F2?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://ii2abc2jejf.typeform.com/to/tK3AXl8T"&gt;&lt;img src="https://img.shields.io/badge/Contact_Us-3B82F6?style=for-the-badge&amp;amp;logo=envelope&amp;amp;logoColor=white" alt="Contact Us" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;¬© 2025 &lt;a href="https://vectify.ai"&gt;Vectify AI&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>OpenBMB/UltraRAG</title>
      <link>https://github.com/OpenBMB/UltraRAG</link>
      <description>&lt;p&gt;UltraRAG v3: A Low-Code MCP Framework for Building Complex and Innovative RAG Pipelines&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="./docs/ultrarag_dark.svg" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="./docs/ultrarag.svg" /&gt; 
  &lt;img alt="UltraRAG" src="https://raw.githubusercontent.com/OpenBMB/UltraRAG/main/docs/ultrarag.svg?sanitize=true" width="55%" /&gt; 
 &lt;/picture&gt; &lt;/p&gt; 
&lt;h3 align="center"&gt; Less Code, Lower Barrier, Faster Deployment &lt;/h3&gt; 
&lt;p align="center"&gt; | &lt;a href="https://ultrarag.openbmb.cn/pages/en/getting_started/introduction"&gt;&lt;b&gt;Documentation&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://modelscope.cn/datasets/UltraRAG/UltraRAG_Benchmark"&gt;&lt;b&gt;Dataset&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://github.com/OpenBMB/UltraRAG/tree/rag-paper-daily/rag-paper-daily"&gt;&lt;b&gt;Paper Daily&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/OpenBMB/UltraRAG/main/docs/README_zh.md"&gt;&lt;b&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/b&gt;&lt;/a&gt; | &lt;b&gt;English&lt;/b&gt; | &lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;em&gt;Latest News&lt;/em&gt; üî•&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;[2026.01.23] üéâ UltraRAG 3.0 Released: Say no to "black box" development‚Äîmake every line of reasoning logic clearly visible üëâ|&lt;a href="https://github.com/OpenBMB/UltraRAG/raw/page/project/blog/en/ultrarag3_0.md"&gt;üìñ Blog&lt;/a&gt;|&lt;/li&gt; 
 &lt;li&gt;[2026.01.20] üéâ AgentCPM-Report Model Released! DeepResearch is finally localized: 8B on-device writing agent AgentCPM-Report is open-sourced üëâ |&lt;a href="https://huggingface.co/openbmb/AgentCPM-Report"&gt;ü§ó Model&lt;/a&gt;|&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;Previous News&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;[2025.11.11] üéâ UltraRAG 2.1 Released: Enhanced knowledge ingestion &amp;amp; multimodal support, with a more complete unified evaluation system!&lt;/li&gt; 
  &lt;li&gt;[2025.09.23] New daily RAG paper digest, updated every day üëâ |&lt;a href="https://github.com/OpenBMB/UltraRAG/tree/rag-paper-daily/rag-paper-daily"&gt;üìñ Papers&lt;/a&gt;|&lt;/li&gt; 
  &lt;li&gt;[2025.09.09] Released a Lightweight DeepResearch Pipeline local setup tutorial üëâ |&lt;a href="https://www.bilibili.com/video/BV1p8JfziEwM"&gt;üì∫ bilibili&lt;/a&gt;|&lt;a href="https://github.com/OpenBMB/UltraRAG/raw/page/project/blog/en/01_build_light_deepresearch.md"&gt;üìñ Blog&lt;/a&gt;|&lt;/li&gt; 
  &lt;li&gt;[2025.09.01] Released a step-by-step UltraRAG installation and full RAG walkthrough video üëâ |&lt;a href="https://www.bilibili.com/video/BV1B9apz4E7K/?share_source=copy_web&amp;amp;vd_source=7035ae721e76c8149fb74ea7a2432710"&gt;üì∫ bilibili&lt;/a&gt;|&lt;a href="https://github.com/OpenBMB/UltraRAG/raw/page/project/blog/en/00_Installing_and_Running_RAG.md"&gt;üìñ Blog&lt;/a&gt;|&lt;/li&gt; 
  &lt;li&gt;[2025.08.28] üéâ UltraRAG 2.0 Released! UltraRAG 2.0 is fully upgraded: build a high-performance RAG with just a few dozen lines of code, empowering researchers to focus on ideas and innovation! We have preserved the UltraRAG v2 code, which can be viewed at &lt;a href="https://github.com/OpenBMB/UltraRAG/tree/v2"&gt;v2&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2025.01.23] UltraRAG Released! Enabling large models to better comprehend and utilize knowledge bases. The UltraRAG 1.0 code is still available at &lt;a href="https://github.com/OpenBMB/UltraRAG/tree/v1"&gt;v1&lt;/a&gt;.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h2&gt;About UltraRAG&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://trendshift.io/repositories/18747" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/18747" alt="OpenBMB%2FUltraRAG | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;UltraRAG is the first lightweight RAG development framework based on the &lt;a href="https://modelcontextprotocol.io/docs/getting-started/intro"&gt;Model Context Protocol (MCP)&lt;/a&gt; architecture design, jointly launched by &lt;a href="https://nlp.csai.tsinghua.edu.cn/"&gt;THUNLP&lt;/a&gt; at Tsinghua University, &lt;a href="https://neuir.github.io"&gt;NEUIR&lt;/a&gt; at Northeastern University, &lt;a href="https://www.openbmb.cn/home"&gt;OpenBMB&lt;/a&gt;, and &lt;a href="https://github.com/AI9Stars"&gt;AI9stars&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Designed for research exploration and industrial prototyping, UltraRAG standardizes core RAG components (Retriever, Generation, etc.) as independent &lt;strong&gt;MCP Servers&lt;/strong&gt;, combined with the powerful workflow orchestration capabilities of the &lt;strong&gt;MCP Client&lt;/strong&gt;. Developers can achieve precise orchestration of complex control structures such as conditional branches and loops simply through YAML configuration.&lt;/p&gt; 
&lt;p align="center"&gt; 
 &lt;picture&gt; 
  &lt;img alt="UltraRAG" src="https://raw.githubusercontent.com/OpenBMB/UltraRAG/main/docs/architecture.png" width="90%" /&gt; 
 &lt;/picture&gt; &lt;/p&gt; 
&lt;h3&gt;UltraRAG UI&lt;/h3&gt; 
&lt;p&gt;UltraRAG UI transcends the boundaries of traditional chat interfaces, evolving into a visual RAG Integrated Development Environment (IDE) that combines orchestration, debugging, and demonstration.&lt;/p&gt; 
&lt;p&gt;The system features a powerful built-in Pipeline Builder that supports bidirectional real-time synchronization between "Canvas Construction" and "Code Editing," allowing for granular online adjustments of pipeline parameters and prompts. Furthermore, it introduces an Intelligent AI Assistant to empower the entire development lifecycle, from pipeline structural design to parameter tuning and prompt generation. Once constructed, logic flows can be converted into interactive dialogue systems with a single click. The system seamlessly integrates Knowledge Base Management components, enabling users to build custom knowledge bases for document Q&amp;amp;A. This truly realizes a one-stop closed loop, spanning from underlying logic construction and data governance to final application deployment.&lt;/p&gt; 
&lt;!-- &lt;p align="center"&gt;
  &lt;picture&gt;
    &lt;img alt="UltraRAG_UI" src="./docs/chat_menu.png" width=80%&gt;
  &lt;/picture&gt;
&lt;/p&gt; --&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/fcf437b7-8b79-42f2-bf4e-e3b7c2a896b9"&gt;https://github.com/user-attachments/assets/fcf437b7-8b79-42f2-bf4e-e3b7c2a896b9&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Key Highlights&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;üöÄ &lt;strong&gt;Low-Code Orchestration of Complex Workflows&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Inference Orchestration&lt;/strong&gt;: Natively supports control structures such as sequential, loop, and conditional branches. Developers only need to write YAML configuration files to implement complex iterative RAG logic in dozens of lines of code.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;‚ö° &lt;strong&gt;Modular Extension and Reproduction&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Atomic Servers&lt;/strong&gt;: Based on the MCP architecture, functions are decoupled into independent Servers. New features only need to be registered as function-level Tools to seamlessly integrate into workflows, achieving extremely high reusability.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üìä &lt;strong&gt;Unified Evaluation and Benchmark Comparison&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Research Efficiency&lt;/strong&gt;: Built-in standardized evaluation workflows, ready-to-use mainstream research benchmarks. Through unified metric management and baseline integration, significantly improves experiment reproducibility and comparison efficiency.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;‚ú® &lt;strong&gt;Rapid Interactive Prototype Generation&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;One-Click Delivery&lt;/strong&gt;: Say goodbye to tedious UI development. With just one command, Pipeline logic can be instantly converted into an interactive conversational Web UI, shortening the distance from algorithm to demonstration.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;We provide two installation methods: local source code installation (recommended using &lt;code&gt;uv&lt;/code&gt; for package management) and Docker container deployment&lt;/p&gt; 
&lt;h3&gt;Method 1: Source Code Installation&lt;/h3&gt; 
&lt;p&gt;We strongly recommend using &lt;a href="https://github.com/astral-sh/uv"&gt;uv&lt;/a&gt; to manage Python environments and dependencies, as it can greatly improve installation speed.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Prepare Environment&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;If you haven't installed uv yet, please execute:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;## Direct installation
pip install uv
## Download
curl -LsSf https://astral.sh/uv/install.sh | sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Download Source Code&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;git clone https://github.com/OpenBMB/UltraRAG.git --depth 1
cd UltraRAG
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Install Dependencies&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Choose one of the following modes to install dependencies based on your use case:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;A: Create a New Environment&lt;/strong&gt; Use &lt;code&gt;uv sync&lt;/code&gt; to automatically create a virtual environment and synchronize dependencies:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Core dependencies: If you only need to run basic core functions, such as only using UltraRAG UI:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;uv sync
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Full installation: If you want to fully experience UltraRAG's retrieval, generation, corpus processing, and evaluation functions, please run:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;uv sync --all-extras
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;On-demand installation: If you only need to run specific modules, keep the corresponding &lt;code&gt;--extra&lt;/code&gt; as needed, for example:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;uv sync --extra retriever   # Retrieval module only
uv sync --extra generation  # Generation module only
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Once installed, activate the virtual environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# Windows CMD
.venv\Scripts\activate.bat

# Windows Powershell
.venv\Scripts\Activate.ps1

# macOS / Linux
source .venv/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;B: Install into an Existing Environment&lt;/strong&gt; To install UltraRAG into your currently active Python environment, use &lt;code&gt;uv pip&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# Core dependencies
uv pip install -e .

# Full installation
uv pip install -e ".[all]"

# On-demand installation
uv pip install -e ".[retriever]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Method 2: Docker Container Deployment&lt;/h3&gt; 
&lt;p&gt;If you prefer not to configure a local Python environment, you can deploy using Docker.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Get Code and Images&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# 1. Clone the repository
git clone https://github.com/OpenBMB/UltraRAG.git --depth 1
cd UltraRAG

# 2. Prepare the image (choose one)
# Option A: Pull from Docker Hub
docker pull hdxin2002/ultrarag:v0.3.0-base-cpu # Base version (CPU)
docker pull hdxin2002/ultrarag:v0.3.0-base-gpu # Base version (GPU)
docker pull hdxin2002/ultrarag:v0.3.0          # Full version (GPU)

# Option B: Build locally
docker build -t ultrarag:v0.3.0 .

# 3. Start container (port 5050 is automatically mapped)
docker run -it --gpus all -p 5050:5050 &amp;lt;docker_image_name&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Start the Container&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# Start the container (Port 5050 is mapped by default)
docker run -it --gpus all -p 5050:5050 &amp;lt;docker_image_name&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note: After the container starts, UltraRAG UI will run automatically. You can directly access &lt;code&gt;http://localhost:5050&lt;/code&gt; in your browser to use it.&lt;/p&gt; 
&lt;h3&gt;Verify Installation&lt;/h3&gt; 
&lt;p&gt;After installation, run the following example command to check if the environment is normal:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;ultrarag run examples/sayhello.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you see the following output, the installation is successful:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Hello, UltraRAG v3!
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;We provide complete tutorial examples from beginner to advanced. Whether you are conducting academic research or building industrial applications, you can find guidance here. Welcome to visit the &lt;a href="https://ultrarag.openbmb.cn/pages/en/getting_started/introduction"&gt;Documentation&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h3&gt;Research Experiments&lt;/h3&gt; 
&lt;p&gt;Designed for researchers, providing data, experimental workflows, and visualization analysis tools.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://ultrarag.openbmb.cn/pages/en/getting_started/quick_start"&gt;Getting Started&lt;/a&gt;: Learn how to quickly run standard RAG experimental workflows based on UltraRAG.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ultrarag.openbmb.cn/pages/en/develop_guide/dataset"&gt;Evaluation Data&lt;/a&gt;: Download the most commonly used public evaluation datasets in the RAG field and large-scale retrieval corpora, directly for research benchmark testing.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ultrarag.openbmb.cn/pages/en/develop_guide/case_study"&gt;Case Analysis&lt;/a&gt;: Provides a visual Case Study interface to deeply track each intermediate output of the workflow, assisting in analysis and error attribution.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ultrarag.openbmb.cn/pages/en/develop_guide/code_integration"&gt;Code Integration&lt;/a&gt;: Learn how to directly call UltraRAG components in Python code to achieve more flexible customized development.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Demo Systems&lt;/h3&gt; 
&lt;p&gt;Designed for developers and end users, providing complete UI interaction and complex application cases.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://ultrarag.openbmb.cn/pages/en/ui/start"&gt;Quick Start&lt;/a&gt;: Learn how to start UltraRAG UI and familiarize yourself with various advanced configurations in administrator mode.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ultrarag.openbmb.cn/pages/en/ui/prepare"&gt;Deployment Guide&lt;/a&gt;: Detailed production environment deployment tutorials, covering the setup of Retriever, Generation models (LLM), and Milvus vector database.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ultrarag.openbmb.cn/pages/en/demo/deepresearch"&gt;Deep Research&lt;/a&gt;: Flagship case, deploy a Deep Research Pipeline. Combined with the AgentCPM-Report model, it can automatically perform multi-step retrieval and integration to generate tens of thousands of words of survey reports.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Thanks to the following contributors for their code submissions and testing. We also welcome new members to join us in collectively building a comprehensive RAG ecosystem!&lt;/p&gt; 
&lt;p&gt;You can contribute by following the standard process: &lt;strong&gt;Fork this repository ‚Üí Submit Issues ‚Üí Create Pull Requests (PRs)&lt;/strong&gt;.&lt;/p&gt; 
&lt;a href="https://github.com/OpenBMB/UltraRAG/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=OpenBMB/UltraRAG&amp;amp;nocache=true" /&gt; &lt;/a&gt; 
&lt;h2&gt;Support Us&lt;/h2&gt; 
&lt;p&gt;If you find this repository helpful for your research, please consider giving us a ‚≠ê to show your support.&lt;/p&gt; 
&lt;a href="https://star-history.com/#OpenBMB/UltraRAG&amp;amp;Date"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=OpenBMB/UltraRAG&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=OpenBMB/UltraRAG&amp;amp;type=Date" /&gt; 
  &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=OpenBMB/UltraRAG&amp;amp;type=Date" /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;h2&gt;Contact Us&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;For technical issues and feature requests, please use &lt;a href="https://github.com/OpenBMB/UltraRAG/issues"&gt;GitHub Issues&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;For questions about usage, feedback, or any discussions related to RAG technologies, you are welcome to join our &lt;a href="https://github.com/OpenBMB/UltraRAG/raw/main/docs/wechat_qr.png"&gt;WeChat group&lt;/a&gt;, &lt;a href="https://github.com/OpenBMB/UltraRAG/raw/main/docs/feishu_qr.png"&gt;Feishu group&lt;/a&gt;, and &lt;a href="https://discord.gg/yRFFjjJnnS"&gt;Discord&lt;/a&gt; to exchange ideas with us.&lt;/li&gt; 
 &lt;li&gt;If you have any questions, feedback, or would like to get in touch, please feel free to reach out to us via email at &lt;a href="mailto:yanyk.thu@gmail.com"&gt;yanyk.thu@gmail.com&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/UltraRAG/main/docs/wechat_qr.png" alt="WeChat Group QR Code" width="220" /&gt;&lt;br /&gt; &lt;b&gt;WeChat Group&lt;/b&gt; &lt;/td&gt; 
   &lt;td align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/UltraRAG/main/docs/feishu_qr.png" alt="Feishu Group QR Code" width="220" /&gt;&lt;br /&gt; &lt;b&gt;Feishu Group&lt;/b&gt; &lt;/td&gt; 
   &lt;td align="center"&gt; &lt;a href="https://discord.gg/yRFFjjJnnS"&gt; &lt;img src="https://img.shields.io/badge/Discord-5865F2?logo=discord&amp;amp;logoColor=white" alt="Join Discord" /&gt; &lt;/a&gt;&lt;br /&gt; &lt;b&gt;Discord&lt;/b&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt;</description>
    </item>
    
    <item>
      <title>microsoft/VibeVoice</title>
      <link>https://github.com/microsoft/VibeVoice</link>
      <description>&lt;p&gt;Open-Source Frontier Voice AI&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h2&gt;üéôÔ∏è VibeVoice: Open-Source Frontier Voice AI&lt;/h2&gt; 
 &lt;p&gt;&lt;a href="https://microsoft.github.io/VibeVoice"&gt;&lt;img src="https://img.shields.io/badge/Project-Page-blue?logo=githubpages" alt="Project Page" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/collections/microsoft/vibevoice-68a2ef24a875c44be47b034f"&gt;&lt;img src="https://img.shields.io/badge/HuggingFace-Collection-orange?logo=huggingface" alt="Hugging Face" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/pdf/2508.19205"&gt;&lt;img src="https://img.shields.io/badge/TTS-Report-red?logo=arxiv" alt="TTS Report" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/VibeVoice-ASR-Report.pdf"&gt;&lt;img src="https://img.shields.io/badge/ASR-Report-yellow?logo=arxiv" alt="ASR Report" /&gt;&lt;/a&gt; &lt;a href="https://colab.research.google.com/github/microsoft/VibeVoice/blob/main/demo/VibeVoice_colab.ipynb"&gt;&lt;img src="https://img.shields.io/badge/StreamingTTS-Colab-green?logo=googlecolab" alt="Colab" /&gt;&lt;/a&gt; &lt;a href="https://aka.ms/vibevoice-asr"&gt;&lt;img src="https://img.shields.io/badge/ASR-Playground-6F42C1?logo=gradio" alt="ASR Playground" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="Figures/VibeVoice_logo_white.png" /&gt; 
  &lt;img src="https://raw.githubusercontent.com/microsoft/VibeVoice/main/Figures/VibeVoice_logo.png" alt="VibeVoice Logo" width="300" /&gt; 
 &lt;/picture&gt; 
&lt;/div&gt; 
&lt;div align="left"&gt; 
 &lt;h3&gt;üì∞ News&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;2026-01-21: üì£ We open-sourced &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-asr.md"&gt;&lt;strong&gt;VibeVoice-ASR&lt;/strong&gt;&lt;/a&gt;, a unified speech-to-text model designed to handle 60-minute long-form audio in a single pass, generating structured transcriptions containing Who (Speaker), When (Timestamps), and What (Content), with support for User-Customized Context. Try it in &lt;a href="https://aka.ms/vibevoice-asr"&gt;Playground&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;‚≠êÔ∏è VibeVoice-ASR is natively multilingual, supporting over 50 languages ‚Äî check the &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-asr.md#language-distribution"&gt;supported languages&lt;/a&gt; for details.&lt;/li&gt; 
  &lt;li&gt;üî• The VibeVoice-ASR &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/finetuning-asr/README.md"&gt;finetuning code&lt;/a&gt; is now available!&lt;/li&gt; 
  &lt;li&gt;‚ö°Ô∏è &lt;strong&gt;vLLM inference&lt;/strong&gt; is now supported for faster inference; see &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-vllm-asr.md"&gt;vllm-asr&lt;/a&gt; for more details.&lt;/li&gt; 
  &lt;li&gt;üìë &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/VibeVoice-ASR-Report.pdf"&gt;VibeVoice-ASR Technique Report&lt;/a&gt; is available.&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;2025-12-16: üì£ We added experimental speakers to &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-realtime-0.5b.md"&gt;&lt;strong&gt;VibeVoice‚ÄëRealtime‚Äë0.5B&lt;/strong&gt;&lt;/a&gt; for exploration, including multilingual voices in nine languages (DE, FR, IT, JP, KR, NL, PL, PT, ES) and 11 distinct English style voices. &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-realtime-0.5b.md#optional-more-experimental-voices"&gt;Try it&lt;/a&gt;. More speaker types will be added over time.&lt;/p&gt; 
 &lt;p&gt;2025-12-03: üì£ We open-sourced &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-realtime-0.5b.md"&gt;&lt;strong&gt;VibeVoice‚ÄëRealtime‚Äë0.5B&lt;/strong&gt;&lt;/a&gt;, a real‚Äëtime text‚Äëto‚Äëspeech model that supports streaming text input and robust long-form speech generation. Try it on &lt;a href="https://colab.research.google.com/github/microsoft/VibeVoice/blob/main/demo/vibevoice_realtime_colab.ipynb"&gt;Colab&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;2025-09-05: VibeVoice is an open-source research framework intended to advance collaboration in the speech synthesis community. After release, we discovered instances where the tool was used in ways inconsistent with the stated intent. Since responsible use of AI is one of Microsoft‚Äôs guiding principles, we have removed the VibeVoice-TTS code from this repository.&lt;/p&gt; 
 &lt;p&gt;2025-08-25: üì£ We open-sourced &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-tts.md"&gt;&lt;strong&gt;VibeVoice-TTS&lt;/strong&gt;&lt;/a&gt;, a long-form multi-speaker text-to-speech model that can synthesize speech up to 90 minutes long with up to 4 distinct speakers.&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;Overview&lt;/h2&gt; 
&lt;p&gt;VibeVoice is a &lt;strong&gt;family of open-source frontier voice AI models&lt;/strong&gt; that includes both Text-to-Speech (TTS) and Automatic Speech Recognition (ASR) models.&lt;/p&gt; 
&lt;p&gt;A core innovation of VibeVoice is its use of continuous speech tokenizers (Acoustic and Semantic) operating at an ultra-low frame rate of &lt;strong&gt;7.5 Hz&lt;/strong&gt;. These tokenizers efficiently preserve audio fidelity while significantly boosting computational efficiency for processing long sequences. VibeVoice employs a &lt;a href="https://arxiv.org/abs/2412.08635"&gt;next-token diffusion&lt;/a&gt; framework, leveraging a Large Language Model (LLM) to understand textual context and dialogue flow, and a diffusion head to generate high-fidelity acoustic details.&lt;/p&gt; 
&lt;p&gt;For more information, demos, and examples, please visit our &lt;a href="https://microsoft.github.io/VibeVoice"&gt;Project Page&lt;/a&gt;.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;Weight&lt;/th&gt; 
    &lt;th&gt;Quick Try&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;VibeVoice-ASR-7B&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://huggingface.co/microsoft/VibeVoice-ASR"&gt;HF Link&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://aka.ms/vibevoice-asr"&gt;Playground&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;VibeVoice-TTS-1.5B&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://huggingface.co/microsoft/VibeVoice-1.5B"&gt;HF Link&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Disabled&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;VibeVoice-Realtime-0.5B&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://huggingface.co/microsoft/VibeVoice-Realtime-0.5B"&gt;HF Link&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://colab.research.google.com/github/microsoft/VibeVoice/blob/main/demo/vibevoice_realtime_colab.ipynb"&gt;Colab&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;Models&lt;/h2&gt; 
&lt;h3&gt;1. üìñ &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-asr.md"&gt;VibeVoice-ASR&lt;/a&gt; - Long-form Speech Recognition&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;VibeVoice-ASR&lt;/strong&gt; is a unified speech-to-text model designed to handle &lt;strong&gt;60-minute long-form audio&lt;/strong&gt; in a single pass, generating structured transcriptions containing &lt;strong&gt;Who (Speaker), When (Timestamps), and What (Content)&lt;/strong&gt;, with support for &lt;strong&gt;Customized Hotwords&lt;/strong&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üïí 60-minute Single-Pass Processing&lt;/strong&gt;: Unlike conventional ASR models that slice audio into short chunks (often losing global context), VibeVoice ASR accepts up to &lt;strong&gt;60 minutes&lt;/strong&gt; of continuous audio input within 64K token length. This ensures consistent speaker tracking and semantic coherence across the entire hour.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üë§ Customized Hotwords&lt;/strong&gt;: Users can provide customized hotwords (e.g., specific names, technical terms, or background info) to guide the recognition process, significantly improving accuracy on domain-specific content.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üìù Rich Transcription (Who, When, What)&lt;/strong&gt;: The model jointly performs ASR, diarization, and timestamping, producing a structured output that indicates &lt;em&gt;who&lt;/em&gt; said &lt;em&gt;what&lt;/em&gt; and &lt;em&gt;when&lt;/em&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-asr.md"&gt;üìñ Documentation&lt;/a&gt; | &lt;a href="https://huggingface.co/microsoft/VibeVoice-ASR"&gt;ü§ó Hugging Face&lt;/a&gt; | &lt;a href="https://aka.ms/vibevoice-asr"&gt;üéÆ Playground&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/finetuning-asr/README.md"&gt;üõ†Ô∏è Finetuning&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/VibeVoice/main/Figures/DER.jpg" alt="DER" width="50%" /&gt;&lt;br /&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/VibeVoice/main/Figures/cpWER.jpg" alt="cpWER" width="50%" /&gt;&lt;br /&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/VibeVoice/main/Figures/tcpWER.jpg" alt="tcpWER" width="50%" /&gt; &lt;/p&gt; 
&lt;div align="center" id="vibevoice-asr"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/acde5602-dc17-4314-9e3b-c630bc84aefa"&gt;https://github.com/user-attachments/assets/acde5602-dc17-4314-9e3b-c630bc84aefa&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;h3&gt;2. üéôÔ∏è &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-tts.md"&gt;VibeVoice-TTS&lt;/a&gt; - Long-form Multi-speaker TTS&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Best for&lt;/strong&gt;: Long-form conversational audio, podcasts, multi-speaker dialogues&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;‚è±Ô∏è 90-minute Long-form Generation&lt;/strong&gt;: Synthesizes conversational/single-speaker speech up to &lt;strong&gt;90 minutes&lt;/strong&gt; in a single pass, maintaining speaker consistency and semantic coherence throughout.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üë• Multi-speaker Support&lt;/strong&gt;: Supports up to &lt;strong&gt;4 distinct speakers&lt;/strong&gt; in a single conversation, with natural turn-taking and speaker consistency across long dialogues.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üé≠ Expressive Speech&lt;/strong&gt;: Generates expressive, natural-sounding speech that captures conversational dynamics and emotional nuances.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üåê Multi-lingual Support&lt;/strong&gt;: Supports English, Chinese and other languages.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-tts.md"&gt;üìñ Documentation&lt;/a&gt; | &lt;a href="https://huggingface.co/microsoft/VibeVoice-1.5B"&gt;ü§ó Hugging Face&lt;/a&gt; | &lt;a href="https://arxiv.org/pdf/2508.19205"&gt;üìä Paper&lt;/a&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/microsoft/VibeVoice/main/Figures/VibeVoice-TTS-results.jpg" alt="VibeVoice Results" width="80%" /&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;English&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/0967027c-141e-4909-bec8-091558b1b784"&gt;https://github.com/user-attachments/assets/0967027c-141e-4909-bec8-091558b1b784&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Chinese&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/322280b7-3093-4c67-86e3-10be4746c88f"&gt;https://github.com/user-attachments/assets/322280b7-3093-4c67-86e3-10be4746c88f&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Cross-Lingual&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/838d8ad9-a201-4dde-bb45-8cd3f59ce722"&gt;https://github.com/user-attachments/assets/838d8ad9-a201-4dde-bb45-8cd3f59ce722&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Spontaneous Singing&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/6f27a8a5-0c60-4f57-87f3-7dea2e11c730"&gt;https://github.com/user-attachments/assets/6f27a8a5-0c60-4f57-87f3-7dea2e11c730&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Long Conversation with 4 people&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/a357c4b6-9768-495c-a576-1618f6275727"&gt;https://github.com/user-attachments/assets/a357c4b6-9768-495c-a576-1618f6275727&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;h3&gt;3. ‚ö° &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-realtime-0.5b.md"&gt;VibeVoice-Streaming&lt;/a&gt; - Real-time Streaming TTS&lt;/h3&gt; 
&lt;p&gt;VibeVoice-Realtime is a &lt;strong&gt;lightweight real‚Äëtime&lt;/strong&gt; text-to-speech model supporting &lt;strong&gt;streaming text input&lt;/strong&gt; and &lt;strong&gt;robust long-form speech generation&lt;/strong&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Parameter size: 0.5B (deployment-friendly)&lt;/li&gt; 
 &lt;li&gt;Real-time TTS (~300 milliseconds first audible latency)&lt;/li&gt; 
 &lt;li&gt;Streaming text input&lt;/li&gt; 
 &lt;li&gt;Robust long-form speech generation (~10 minutes)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-realtime-0.5b.md"&gt;üìñ Documentation&lt;/a&gt; | &lt;a href="https://huggingface.co/microsoft/VibeVoice-Realtime-0.5B"&gt;ü§ó Hugging Face&lt;/a&gt; | &lt;a href="https://colab.research.google.com/github/microsoft/VibeVoice/blob/main/demo/vibevoice_realtime_colab.ipynb"&gt;üöÄ Colab&lt;/a&gt;&lt;/p&gt; 
&lt;div align="center" id="generated-example-audio-vibevoice-realtime"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/0901d274-f6ae-46ef-a0fd-3c4fba4f76dc"&gt;https://github.com/user-attachments/assets/0901d274-f6ae-46ef-a0fd-3c4fba4f76dc&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;h2&gt;‚ö†Ô∏è Risks and Limitations&lt;/h2&gt; 
&lt;p&gt;While efforts have been made to optimize it through various techniques, it may still produce outputs that are unexpected, biased, or inaccurate. VibeVoice inherits any biases, errors, or omissions produced by its base model (specifically, Qwen2.5 1.5b in this release). Potential for Deepfakes and Disinformation: High-quality synthetic speech can be misused to create convincing fake audio content for impersonation, fraud, or spreading disinformation. Users must ensure transcripts are reliable, check content accuracy, and avoid using generated content in misleading ways. Users are expected to use the generated content and to deploy the models in a lawful manner, in full compliance with all applicable laws and regulations in the relevant jurisdictions. It is best practice to disclose the use of AI when sharing AI-generated content.&lt;/p&gt; 
&lt;p&gt;We do not recommend using VibeVoice in commercial or real-world applications without further testing and development. This model is intended for research and development purposes only. Please use responsibly.&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://api.star-history.com/svg?repos=Microsoft/vibevoice&amp;amp;type=date&amp;amp;legend=top-left" alt="Star History Chart" /&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>qarmin/czkawka</title>
      <link>https://github.com/qarmin/czkawka</link>
      <description>&lt;p&gt;Multi functional app to find duplicates, empty folders, similar images etc.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/567a7a38-d754-4a79-86b5-3cc898dbbade" alt="krokiet_logo" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Krokiet&lt;/strong&gt; ((IPA: [Ààkr…îc…õt]), "croquette" in Polish) new generation GUI frontend, simple, multiplatform, fast and free app to remove unnecessary files from your computer.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/41945903/102616149-66490400-4137-11eb-9cd6-813b2b070834.png" alt="czkawka_logo" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Czkawka&lt;/strong&gt; (&lt;em&gt;tch‚Ä¢kav‚Ä¢ka&lt;/em&gt; (IPA: [Àà ßÃëkafka]), "hiccup" in Polish) older gtk4 GUI frontend, superseded by Krokiet, but still receiving bugfix updates.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Written in memory-safe Rust - almost 100% unsafe code free&lt;/li&gt; 
 &lt;li&gt;Amazingly fast - due to using more or less advanced algorithms and multithreading&lt;/li&gt; 
 &lt;li&gt;Free, Open Source without ads&lt;/li&gt; 
 &lt;li&gt;Multiplatform - works on Linux, Windows, macOS, FreeBSD and many more&lt;/li&gt; 
 &lt;li&gt;Cache support - second and further scans should be much faster than the first one&lt;/li&gt; 
 &lt;li&gt;CLI frontend - for easy automation&lt;/li&gt; 
 &lt;li&gt;GUI frontend - uses Slint or GTK 4 frameworks&lt;/li&gt; 
 &lt;li&gt;Core library - allows to reuse functionality in other apps&lt;/li&gt; 
 &lt;li&gt;No spying - Czkawka does not have access to the Internet, nor does it collect any user information or statistics&lt;/li&gt; 
 &lt;li&gt;Multilingual - support multiple languages like Polish, English or Italian&lt;/li&gt; 
 &lt;li&gt;Multiple tools to use: 
  &lt;ul&gt; 
   &lt;li&gt;Duplicates - Finds duplicates based on file name, size or hash&lt;/li&gt; 
   &lt;li&gt;Empty Folders - Finds empty folders with the help of an advanced algorithm&lt;/li&gt; 
   &lt;li&gt;Big Files - Finds the provided number of the biggest files in given location&lt;/li&gt; 
   &lt;li&gt;Empty Files - Looks for empty files across the drive&lt;/li&gt; 
   &lt;li&gt;Temporary Files - Finds temporary files&lt;/li&gt; 
   &lt;li&gt;Similar Images - Finds images which are not exactly the same (different resolution, watermarks)&lt;/li&gt; 
   &lt;li&gt;Similar Videos - Looks for visually similar videos&lt;/li&gt; 
   &lt;li&gt;Same Music - Searches for similar music by tags or by reading content and comparing it&lt;/li&gt; 
   &lt;li&gt;Invalid Symbolic Links - Shows symbolic links which point to non-existent files/directories&lt;/li&gt; 
   &lt;li&gt;Broken Files - Finds files that are invalid or corrupted&lt;/li&gt; 
   &lt;li&gt;Bad Extensions - Lists files whose content not match with their extension&lt;/li&gt; 
   &lt;li&gt;Exif Remover - Removes Exif metadata from various file types&lt;/li&gt; 
   &lt;li&gt;Video Optimizer - Crops from static parts and converts videos to more efficient formats&lt;/li&gt; 
   &lt;li&gt;Bad Names - Finds files with names that may be not wanted (e.g., containing special characters)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/720e98c3-598a-41aa-a04b-0c0c1d8a28e6" alt="Krokiet" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/b0409515-1bec-4e13-8fac-7bdfa15f5848" alt="Czkawka" /&gt;&lt;/p&gt; 
&lt;p&gt;Changelog about each version can be found in &lt;a href="https://raw.githubusercontent.com/qarmin/czkawka/master/Changelog.md"&gt;CHANGELOG.md&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;New releases can be found in &lt;a href="https://github.com/qarmin/czkawka/releases"&gt;Github releases&lt;/a&gt; and nightly builds also in &lt;a href="https://github.com/qarmin/czkawka/releases/tag/Nightly"&gt;Nightly releases&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Usage, installation, compilation, requirements, license&lt;/h2&gt; 
&lt;p&gt;Each tool uses different technologies, so you can find instructions for each of them in the appropriate file:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/qarmin/czkawka/master/krokiet/README.md"&gt;Krokiet GUI (Slint frontend)&lt;/a&gt;&lt;br /&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/qarmin/czkawka/master/czkawka_gui/README.md"&gt;Czkawka GUI (GTK frontend)&lt;/a&gt;&lt;br /&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/qarmin/czkawka/master/czkawka_cli/README.md"&gt;Czkawka CLI&lt;/a&gt;&lt;br /&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/qarmin/czkawka/master/czkawka_core/README.md"&gt;Czkawka Core&lt;/a&gt;&lt;br /&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Comparison to other tools&lt;/h2&gt; 
&lt;p&gt;Bleachbit is a master at finding and removing temporary files, while Czkawka only finds the most basic ones. So these two apps shouldn't be compared directly or be considered as an alternative to one another.&lt;/p&gt; 
&lt;p&gt;In this comparison remember, that even if app have same features they may work different(e.g. one app may have more options to choose than other).&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;Czkawka&lt;/th&gt; 
   &lt;th align="center"&gt;Krokiet&lt;/th&gt; 
   &lt;th align="center"&gt;FSlint&lt;/th&gt; 
   &lt;th align="center"&gt;DupeGuru&lt;/th&gt; 
   &lt;th align="center"&gt;Bleachbit&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Language&lt;/td&gt; 
   &lt;td align="center"&gt;Rust&lt;/td&gt; 
   &lt;td align="center"&gt;Rust&lt;/td&gt; 
   &lt;td align="center"&gt;Python&lt;/td&gt; 
   &lt;td align="center"&gt;Python/Obj-C&lt;/td&gt; 
   &lt;td align="center"&gt;Python&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Framework base language&lt;/td&gt; 
   &lt;td align="center"&gt;C&lt;/td&gt; 
   &lt;td align="center"&gt;Rust&lt;/td&gt; 
   &lt;td align="center"&gt;C&lt;/td&gt; 
   &lt;td align="center"&gt;C/C++/Obj-C/Swift&lt;/td&gt; 
   &lt;td align="center"&gt;C&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Framework&lt;/td&gt; 
   &lt;td align="center"&gt;GTK 4&lt;/td&gt; 
   &lt;td align="center"&gt;Slint&lt;/td&gt; 
   &lt;td align="center"&gt;PyGTK2&lt;/td&gt; 
   &lt;td align="center"&gt;Qt 5 (PyQt)/Cocoa&lt;/td&gt; 
   &lt;td align="center"&gt;PyGTK3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;OS&lt;/td&gt; 
   &lt;td align="center"&gt;Lin,Mac,Win&lt;/td&gt; 
   &lt;td align="center"&gt;Lin,Mac,Win&lt;/td&gt; 
   &lt;td align="center"&gt;Lin&lt;/td&gt; 
   &lt;td align="center"&gt;Lin,Mac,Win&lt;/td&gt; 
   &lt;td align="center"&gt;Lin,Mac,Win&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Duplicate finder&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Empty files&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Empty folders&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Temporary files&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Big files&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Similar images&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Similar videos&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Music duplicates(tags)&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Music duplicates(content)&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Invalid symlinks&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Broken files&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Invalid names/extensions&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Exif cleaner&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Video optimizer&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Bad Names&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Names conflict&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Installed packages&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Bad ID&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Non stripped binaries&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Redundant whitespace&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Overwriting files&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Portable version&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Multiple languages&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Cache support&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;‚úî&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;In active development&lt;/td&gt; 
   &lt;td align="center"&gt;Yes&lt;sup&gt;**&lt;/sup&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;Yes&lt;/td&gt; 
   &lt;td align="center"&gt;No&lt;/td&gt; 
   &lt;td align="center"&gt;No&lt;sup&gt;*&lt;/sup&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;Yes&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;sup&gt;*&lt;/sup&gt; Few small commits added recently and last version released in 2023&lt;/p&gt; 
&lt;p&gt;&lt;sup&gt;**&lt;/sup&gt; Czkawka GTK is in maintenance mode receiving only bugfixes&lt;/p&gt; 
&lt;h2&gt;Other apps&lt;/h2&gt; 
&lt;p&gt;There are many similar applications to Czkawka on the Internet, which do some things better and some things worse:&lt;/p&gt; 
&lt;h3&gt;GUI&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/arsenetar/dupeguru"&gt;DupeGuru&lt;/a&gt; - Many options to customize; great photo compare tool&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pixelb/fslint"&gt;FSlint&lt;/a&gt; - A little outdated, but still have some tools not available in Czkawka&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ermig1979/AntiDupl"&gt;AntiDupl.NET&lt;/a&gt; - Shows a lot of metadata of compared images&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/0x90d/videoduplicatefinder"&gt;Video Duplicate Finder&lt;/a&gt; - Finds similar videos(surprising, isn't it), supports video thumbnails&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;CLI&lt;/h3&gt; 
&lt;p&gt;Due to limited time, the biggest emphasis is on the GUI version so if you are looking for really good and feature-packed console apps, then take a look at these:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pkolaczk/fclones"&gt;Fclones&lt;/a&gt; - One of the fastest tools to find duplicates; it is written also in Rust&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/sahib/rmlint"&gt;Rmlint&lt;/a&gt; - Nice console interface and also is feature packed&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pauldreik/rdfind"&gt;RdFind&lt;/a&gt; - Fast, but written in C++ ¬Ø\_(„ÉÑ)_/¬Ø&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Projects using Czkawka&lt;/h2&gt; 
&lt;p&gt;Czkawka exposes its common functionality through a crate called &lt;strong&gt;&lt;code&gt;czkawka_core&lt;/code&gt;&lt;/strong&gt;, which can be reused by other projects.&lt;/p&gt; 
&lt;p&gt;It is written in Rust and is used by all Czkawka frontends (&lt;code&gt;czkawka_gui&lt;/code&gt;, &lt;code&gt;czkawka_cli&lt;/code&gt;, &lt;code&gt;krokiet&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;It is also used by external projects, such as:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Czkawka Tauri&lt;/strong&gt; - &lt;a href="https://github.com/shixinhuang99/czkawka-tauri"&gt;https://github.com/shixinhuang99/czkawka-tauri&lt;/a&gt; - A Tauri-based GUI frontend for Czkawka.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;page-dewarp&lt;/strong&gt; ‚Äì &lt;a href="https://github.com/lmmx/page-dewarp"&gt;https://github.com/lmmx/page-dewarp&lt;/a&gt; - A library for dewarping document images using a cubic sheet model.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Bindings are also available for:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Python&lt;/strong&gt; ‚Äì &lt;a href="https://pypi.org/project/czkawka/"&gt;https://pypi.org/project/czkawka/&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Some projects work as wrappers around &lt;code&gt;czkawka_cli&lt;/code&gt;. Without directly depending on &lt;code&gt;czkawka_core&lt;/code&gt;, they allow simple scanning and retrieving results in JSON format:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Schluckauf&lt;/strong&gt; ‚Äì &lt;a href="https://github.com/fadykuzman/schluckauf"&gt;https://github.com/fadykuzman/schluckauf&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Thanks&lt;/h2&gt; 
&lt;p&gt;Big thanks to P√°draig Brady, creator of fantastic FSlint, because without his work I wouldn't create this tool.&lt;/p&gt; 
&lt;p&gt;Thanks also to all the people who create patches for this program, create and fix translations, make it available on other systems, create videos, articles about it etc.&lt;/p&gt; 
&lt;p&gt;Also, I really appreciate work of people that create crates on which Czkawka is based and for that I try to report bugs to make it even better.&lt;/p&gt; 
&lt;h2&gt;Officially Supported Projects&lt;/h2&gt; 
&lt;p&gt;Only this repository, &lt;a href="https://github.com/qarmin/czkawka/releases"&gt;prebuild-binaries&lt;/a&gt;, projects on &lt;a href="https://crates.io/crates/czkawka_gui"&gt;crates.io&lt;/a&gt; and &lt;a href="https://flathub.org/apps/com.github.qarmin.czkawka"&gt;flathub&lt;/a&gt; are directly maintained by me.&lt;/p&gt; 
&lt;p&gt;Czkawka does not have an official website, so do not trust any sites that claim to be the official one.&lt;/p&gt; 
&lt;p&gt;If you use packages from unofficial sources, make sure they are safe.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;The entire code in this repository is licensed under the &lt;a href="https://mit-license.org/"&gt;MIT&lt;/a&gt; license.&lt;/p&gt; 
&lt;p&gt;All images are licensed under the &lt;a href="https://creativecommons.org/licenses/by/4.0/"&gt;CC BY 4.0&lt;/a&gt; license.&lt;/p&gt; 
&lt;p&gt;The Czkawka GTK GUI and CLI applications are licensed under the &lt;a href="https://mit-license.org/"&gt;MIT&lt;/a&gt; license, while the Krokiet is licensed under the &lt;a href="https://www.gnu.org/licenses/gpl-3.0.en.html"&gt;GPL-3.0-only&lt;/a&gt; license.&lt;/p&gt; 
&lt;h2&gt;Donations&lt;/h2&gt; 
&lt;p&gt;If you are using the app, I would appreciate a donation for its further development, which can be done &lt;a href="https://github.com/sponsors/qarmin"&gt;here&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Psiphon-Inc/conduit</title>
      <link>https://github.com/Psiphon-Inc/conduit</link>
      <description>&lt;p&gt;Conduit Client&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Conduit App&lt;/h1&gt; 
&lt;p&gt;Conduit runs inproxy from &lt;a href="https://github.com/Psiphon-Labs/psiphon-tunnel-core"&gt;psiphon-tunnel-core&lt;/a&gt; in a mobile app. This repository targets Android, iOS and Mac (via Catalyst).&lt;/p&gt; 
&lt;p&gt;For more information about Conduit, &lt;a href="https://conduit.psiphon.ca"&gt;visit the web site&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Git LFS Usage&lt;/h2&gt; 
&lt;p&gt;This project uses &lt;a href="https://git-lfs.github.com/"&gt;Git LFS&lt;/a&gt; to manage large files such as the tunnel core libraries.&lt;/p&gt; 
&lt;h2&gt;Translations&lt;/h2&gt; 
&lt;p&gt;For information about pulling and verifying translations, see &lt;a href="https://raw.githubusercontent.com/Psiphon-Inc/conduit/main/i18n/README.md"&gt;i18n/README.md&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>remotion-dev/remotion</title>
      <link>https://github.com/remotion-dev/remotion</link>
      <description>&lt;p&gt;üé• Make videos programmatically with React&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://github.com/remotion-dev/logo"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="https://github.com/remotion-dev/logo/raw/main/animated-logo-banner-dark.apng" /&gt; 
   &lt;img alt="Animated Remotion Logo" src="https://github.com/remotion-dev/logo/raw/main/animated-logo-banner-light.gif" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;&lt;a href="https://remotion.dev/discord"&gt;&lt;img src="https://img.shields.io/discord/809501355504959528?color=000000&amp;amp;label=Discord&amp;amp;logo=fdgssdf" alt="Discord Shield" /&gt;&lt;/a&gt; &lt;a href="https://www.npmjs.org/package/remotion"&gt;&lt;img src="https://img.shields.io/npm/v/remotion.svg?style=flat&amp;amp;color=black" alt="NPM Version" /&gt;&lt;/a&gt; &lt;a href="https://npmcharts.com/compare/remotion?minimal=true"&gt;&lt;img src="https://img.shields.io/npm/dm/remotion.svg?style=flat&amp;amp;color=black&amp;amp;label=Downloads" alt="NPM Downloads" /&gt;&lt;/a&gt; &lt;a href="https://github.com/remotion-dev/remotion/issues?q=is%3Aopen+label%3A%22%F0%9F%92%8E+Bounty%22+sort%3Aupdated-desc"&gt;&lt;img src="https://img.shields.io/endpoint?url=https%3A%2F%2Fconsole.algora.io%2Fapi%2Fshields%2Fremotion%2Fbounties%3Fstatus%3Dopen&amp;amp;style=flat&amp;amp;color=black&amp;amp;labelColor=grey&amp;amp;label=Open+Bounties" alt="Open Bounties" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/remotion"&gt;&lt;img src="https://img.shields.io/twitter/follow/remotion?label=Twitter&amp;amp;color=black" alt="Twitter" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Remotion is a framework for &lt;strong&gt;creating videos programmatically using React.&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Why create videos in React?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Leverage web technologies&lt;/strong&gt;: Use all of CSS, Canvas, SVG, WebGL, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Leverage programming&lt;/strong&gt;: Use variables, functions, APIs, math and algorithms to create new effects&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Leverage React&lt;/strong&gt;: Reusable components, Powerful composition, Fast Refresh, Package ecosystem&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Created with Remotion&lt;/h2&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td align="center"&gt; &lt;img style="width: 290px" src="https://pub-646d808d9cb240cea53bedc76dd3cd0c.r2.dev/fireship-quick.gif" /&gt; &lt;p&gt;"This video was made with code" &lt;em&gt;- Fireship&lt;/em&gt; &lt;a href="https://youtu.be/deg8bOoziaE"&gt;Watch&lt;/a&gt; ‚Ä¢ &lt;a href="https://github.com/wcandillon/remotion-fireship"&gt;Source&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td align="center"&gt; &lt;img style="width: 240px" src="https://pub-646d808d9cb240cea53bedc76dd3cd0c.r2.dev/unwrapped-2023.gif" /&gt; &lt;p&gt;GitHub Unwrapped - Personalized Year in Review &lt;a href="https://www.githubunwrapped.com"&gt;Try&lt;/a&gt; ‚Ä¢ &lt;a href="https://github.com/remotion-dev/github-unwrapped"&gt;Source&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td align="center"&gt; &lt;em&gt;View more in the &lt;a href="https://remotion.dev/showcase"&gt;Remotion Showcase&lt;/a&gt;!&lt;/em&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Get started&lt;/h2&gt; 
&lt;p&gt;If you already have Node.JS installed, type&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;npx create-video@latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;to get started. Otherwise, read the &lt;a href="https://www.remotion.dev/docs/"&gt;installation page&lt;/a&gt; in the documentation.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;Documentation: &lt;a href="https://www.remotion.dev/docs"&gt;&lt;strong&gt;remotion.dev/docs&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt; API Reference: &lt;a href="https://www.remotion.dev/api"&gt;&lt;strong&gt;remotion.dev/api&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Be aware of that Remotion has a special license and requires obtaining a company license in some cases. Read the &lt;a href="https://raw.githubusercontent.com/remotion-dev/remotion/main/LICENSE.md"&gt;LICENSE&lt;/a&gt; page for more information.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Please read &lt;a href="https://raw.githubusercontent.com/remotion-dev/remotion/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; to learn about contributing to this project.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Blaizzy/mlx-audio</title>
      <link>https://github.com/Blaizzy/mlx-audio</link>
      <description>&lt;p&gt;A text-to-speech (TTS), speech-to-text (STT) and speech-to-speech (STS) library built on Apple's MLX framework, providing efficient speech analysis on Apple Silicon.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MLX-Audio&lt;/h1&gt; 
&lt;p&gt;The best audio processing library built on Apple's MLX framework, providing fast and efficient text-to-speech (TTS), speech-to-text (STT), and speech-to-speech (STS) on Apple Silicon.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Fast inference optimized for Apple Silicon (M series chips)&lt;/li&gt; 
 &lt;li&gt;Multiple model architectures for TTS, STT, and STS&lt;/li&gt; 
 &lt;li&gt;Multilingual support across models&lt;/li&gt; 
 &lt;li&gt;Voice customization and cloning capabilities&lt;/li&gt; 
 &lt;li&gt;Adjustable speech speed control&lt;/li&gt; 
 &lt;li&gt;Interactive web interface with 3D audio visualization&lt;/li&gt; 
 &lt;li&gt;OpenAI-compatible REST API&lt;/li&gt; 
 &lt;li&gt;Quantization support (3-bit, 4-bit, 6-bit, 8-bit, and more) for optimized performance&lt;/li&gt; 
 &lt;li&gt;Swift package for iOS/macOS integration&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;Using pip&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install mlx-audio
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Using uv to install only the command line tools&lt;/h3&gt; 
&lt;p&gt;Latest release from pypi:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv tool install --force mlx-audio --prerelease=allow
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Latest code from github:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv tool install --force git+https://github.com/Blaizzy/mlx-audio.git --prerelease=allow
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;For development or web interface:&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/Blaizzy/mlx-audio.git
cd mlx-audio
pip install -e ".[dev]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;h3&gt;Command Line&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Basic TTS generation
mlx_audio.tts.generate --model mlx-community/Kokoro-82M-bf16 --text "Hello, world!" --lang_code a

# With voice selection and speed adjustment
mlx_audio.tts.generate --model mlx-community/Kokoro-82M-bf16 --text "Hello!" --voice af_heart --speed 1.2 --lang_code a

# Play audio immediately
mlx_audio.tts.generate --model mlx-community/Kokoro-82M-bf16 --text "Hello!" --play  --lang_code a

# Save to a specific directory
mlx_audio.tts.generate --model mlx-community/Kokoro-82M-bf16 --text "Hello!" --output_path ./my_audio  --lang_code a
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Python API&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_audio.tts.utils import load_model

# Load model
model = load_model("mlx-community/Kokoro-82M-bf16")

# Generate speech
for result in model.generate("Hello from MLX-Audio!", voice="af_heart"):
    print(f"Generated {result.audio.shape[0]} samples")
    # result.audio contains the waveform as mx.array
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Supported Models&lt;/h2&gt; 
&lt;h3&gt;Text-to-Speech (TTS)&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Languages&lt;/th&gt; 
   &lt;th&gt;Repo&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Kokoro&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Fast, high-quality multilingual TTS&lt;/td&gt; 
   &lt;td&gt;EN, JA, ZH, FR, ES, IT, PT, HI&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/Kokoro-82M-bf16"&gt;mlx-community/Kokoro-82M-bf16&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Qwen3-TTS&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Alibaba's multilingual TTS with voice design&lt;/td&gt; 
   &lt;td&gt;ZH, EN, JA, KO, + more&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-1.7B-VoiceDesign-bf16"&gt;mlx-community/Qwen3-TTS-12Hz-1.7B-VoiceDesign-bf16&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;CSM&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Conversational Speech Model with voice cloning&lt;/td&gt; 
   &lt;td&gt;EN&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/csm-1b"&gt;mlx-community/csm-1b&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Dia&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Dialogue-focused TTS&lt;/td&gt; 
   &lt;td&gt;EN&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/Dia-1.6B-bf16"&gt;mlx-community/Dia-1.6B-bf16&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;OuteTTS&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Efficient TTS model&lt;/td&gt; 
   &lt;td&gt;EN&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/OuteTTS-0.2-500M"&gt;mlx-community/OuteTTS-0.2-500M&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Spark&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;SparkTTS model&lt;/td&gt; 
   &lt;td&gt;EN, ZH&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/SparkTTS-0.5B-bf16"&gt;mlx-community/SparkTTS-0.5B-bf16&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Chatterbox&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Expressive multilingual TTS&lt;/td&gt; 
   &lt;td&gt;EN, ES, FR, DE, IT, PT, PL, TR, RU, NL, CS, AR, ZH, JA, HU, KO&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/Chatterbox-bf16"&gt;mlx-community/Chatterbox-bf16&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Soprano&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;High-quality TTS&lt;/td&gt; 
   &lt;td&gt;EN&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/Soprano-bf16"&gt;mlx-community/Soprano-bf16&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Speech-to-Text (STT)&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Languages&lt;/th&gt; 
   &lt;th&gt;Repo&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Whisper&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;OpenAI's robust STT model&lt;/td&gt; 
   &lt;td&gt;99+ languages&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/whisper-large-v3-turbo-asr-fp16"&gt;mlx-community/whisper-large-v3-turbo-asr-fp16&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Parakeet&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;NVIDIA's accurate STT&lt;/td&gt; 
   &lt;td&gt;EN&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/parakeet-tdt-0.6b-v2"&gt;mlx-community/parakeet-tdt-0.6b-v2&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Voxtral&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Mistral's speech model&lt;/td&gt; 
   &lt;td&gt;Multiple&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/Voxtral-Mini-3B-2507-bf16"&gt;mlx-community/Voxtral-Mini-3B-2507-bf16&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;VibeVoice-ASR&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Microsoft's 9B ASR with diarization &amp;amp; timestamps&lt;/td&gt; 
   &lt;td&gt;Multiple&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/VibeVoice-ASR-bf16"&gt;mlx-community/VibeVoice-ASR-bf16&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Speech-to-Speech (STS)&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Use Case&lt;/th&gt; 
   &lt;th&gt;Repo&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;SAM-Audio&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Text-guided source separation&lt;/td&gt; 
   &lt;td&gt;Extract specific sounds&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/sam-audio-large"&gt;mlx-community/sam-audio-large&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Liquid2.5-Audio&lt;/strong&gt;*&lt;/td&gt; 
   &lt;td&gt;Speech-to-Speech, Text-to-Speech and Speech-to-Text&lt;/td&gt; 
   &lt;td&gt;Speech interactions&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/LFM2.5-Audio-1.5B-8bit"&gt;mlx-community/LFM2.5-Audio-1.5B-8bit&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;MossFormer2 SE&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Speech enhancement&lt;/td&gt; 
   &lt;td&gt;Noise removal&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/starkdmi/MossFormer2_SE_48K_MLX"&gt;starkdmi/MossFormer2_SE_48K_MLX&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Model Examples&lt;/h2&gt; 
&lt;h3&gt;Kokoro TTS&lt;/h3&gt; 
&lt;p&gt;Kokoro is a fast, multilingual TTS model with 54 voice presets.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_audio.tts.utils import load_model

model = load_model("mlx-community/Kokoro-82M-bf16")

# Generate with different voices
for result in model.generate(
    text="Welcome to MLX-Audio!",
    voice="af_heart",  # American female
    speed=1.0,
    lang_code="a"  # American English
):
    audio = result.audio
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Available Voices:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;American English: &lt;code&gt;af_heart&lt;/code&gt;, &lt;code&gt;af_bella&lt;/code&gt;, &lt;code&gt;af_nova&lt;/code&gt;, &lt;code&gt;af_sky&lt;/code&gt;, &lt;code&gt;am_adam&lt;/code&gt;, &lt;code&gt;am_echo&lt;/code&gt;, etc.&lt;/li&gt; 
 &lt;li&gt;British English: &lt;code&gt;bf_alice&lt;/code&gt;, &lt;code&gt;bf_emma&lt;/code&gt;, &lt;code&gt;bm_daniel&lt;/code&gt;, &lt;code&gt;bm_george&lt;/code&gt;, etc.&lt;/li&gt; 
 &lt;li&gt;Japanese: &lt;code&gt;jf_alpha&lt;/code&gt;, &lt;code&gt;jm_kumo&lt;/code&gt;, etc.&lt;/li&gt; 
 &lt;li&gt;Chinese: &lt;code&gt;zf_xiaobei&lt;/code&gt;, &lt;code&gt;zm_yunxi&lt;/code&gt;, etc.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Language Codes:&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Code&lt;/th&gt; 
   &lt;th&gt;Language&lt;/th&gt; 
   &lt;th&gt;Note&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;a&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;American English&lt;/td&gt; 
   &lt;td&gt;Default&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;b&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;British English&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;j&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Japanese&lt;/td&gt; 
   &lt;td&gt;Requires &lt;code&gt;pip install misaki[ja]&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;z&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Mandarin Chinese&lt;/td&gt; 
   &lt;td&gt;Requires &lt;code&gt;pip install misaki[zh]&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;e&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Spanish&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;f&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;French&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Qwen3-TTS&lt;/h3&gt; 
&lt;p&gt;Alibaba's state-of-the-art multilingual TTS with voice cloning, emotion control, and voice design capabilities.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_audio.tts.utils import load_model

model = load_model("mlx-community/Qwen3-TTS-12Hz-0.6B-Base-bf16")
results = list(model.generate(
    text="Hello, welcome to MLX-Audio!",
    voice="Chelsie",
    language="English",
))

audio = results[0].audio  # mx.array
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/Blaizzy/mlx-audio/main/mlx_audio/tts/models/qwen3_tts/README.md"&gt;Qwen3-TTS README&lt;/a&gt; for voice cloning, CustomVoice, VoiceDesign, and all available models.&lt;/p&gt; 
&lt;h3&gt;CSM (Voice Cloning)&lt;/h3&gt; 
&lt;p&gt;Clone any voice using a reference audio sample:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mlx_audio.tts.generate \
    --model mlx-community/csm-1b \
    --text "Hello from Sesame." \
    --ref_audio ./reference_voice.wav \
    --play
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Whisper STT&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_audio.stt.generate import generate_transcription

result = generate_transcription(
    model="mlx-community/whisper-large-v3-turbo-asr-fp16",
    audio="audio.wav",
)
print(result.text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;VibeVoice-ASR&lt;/h3&gt; 
&lt;p&gt;Microsoft's 9B parameter speech-to-text model with speaker diarization and timestamps. Supports long-form audio (up to 60 minutes) and outputs structured JSON.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_audio.stt.utils import load

model = load("mlx-community/VibeVoice-ASR-bf16")

# Basic transcription
result = model.generate(audio="meeting.wav", max_tokens=8192, temperature=0.0)
print(result.text)
# [{"Start":0,"End":5.2,"Speaker":0,"Content":"Hello everyone, let's begin."},
#  {"Start":5.5,"End":9.8,"Speaker":1,"Content":"Thanks for joining today."}]

# Access parsed segments
for seg in result.segments:
    print(f"[{seg['start_time']:.1f}-{seg['end_time']:.1f}] Speaker {seg['speaker_id']}: {seg['text']}")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Streaming transcription:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Stream tokens as they are generated
for text in model.stream_transcribe(audio="speech.wav", max_tokens=4096):
    print(text, end="", flush=True)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;With context (hotwords/metadata):&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;result = model.generate(
    audio="technical_talk.wav",
    context="MLX, Apple Silicon, PyTorch, Transformer",
    max_tokens=8192,
    temperature=0.0,
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;CLI usage:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Basic transcription
python -m mlx_audio.stt.generate \
    --model mlx-community/VibeVoice-ASR-bf16 \
    --audio meeting.wav \
    --output-path output \
    --format json \
    --max-tokens 8192 \
    --verbose

# With context/hotwords
python -m mlx_audio.stt.generate \
    --model mlx-community/VibeVoice-ASR-bf16 \
    --audio technical_talk.wav \
    --output-path output \
    --format json \
    --max-tokens 8192 \
    --context "MLX, Apple Silicon, PyTorch, Transformer" \
    --verbose
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;SAM-Audio (Source Separation)&lt;/h3&gt; 
&lt;p&gt;Separate specific sounds from audio using text prompts:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_audio.sts import SAMAudio, SAMAudioProcessor, save_audio

model = SAMAudio.from_pretrained("mlx-community/sam-audio-large")
processor = SAMAudioProcessor.from_pretrained("mlx-community/sam-audio-large")

batch = processor(
    descriptions=["A person speaking"],
    audios=["mixed_audio.wav"],
)

result = model.separate_long(
    batch.audios,
    descriptions=batch.descriptions,
    anchors=batch.anchor_ids,
    chunk_seconds=10.0,
    overlap_seconds=3.0,
    ode_opt={"method": "midpoint", "step_size": 2/32},
)

save_audio(result.target[0], "voice.wav")
save_audio(result.residual[0], "background.wav")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;MossFormer2 (Speech Enhancement)&lt;/h3&gt; 
&lt;p&gt;Remove noise from speech recordings:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_audio.sts import MossFormer2SEModel, save_audio

model = MossFormer2SEModel.from_pretrained("starkdmi/MossFormer2_SE_48K_MLX")
enhanced = model.enhance("noisy_speech.wav")
save_audio(enhanced, "clean.wav", 48000)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Web Interface &amp;amp; API Server&lt;/h2&gt; 
&lt;p&gt;MLX-Audio includes a modern web interface and OpenAI-compatible API.&lt;/p&gt; 
&lt;h3&gt;Starting the Server&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Start API server
mlx_audio.server --host 0.0.0.0 --port 8000

# Start web UI (in another terminal)
cd mlx_audio/ui
npm install &amp;amp;&amp;amp; npm run dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;API Endpoints&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Text-to-Speech&lt;/strong&gt; (OpenAI-compatible):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -X POST http://localhost:8000/v1/audio/speech \
  -H "Content-Type: application/json" \
  -d '{"model": "mlx-community/Kokoro-82M-bf16", "input": "Hello!", "voice": "af_heart"}' \
  --output speech.wav
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Speech-to-Text&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -X POST http://localhost:8000/v1/audio/transcriptions \
  -F "file=@audio.wav" \
  -F "model=mlx-community/whisper-large-v3-turbo-asr-fp16"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Quantization&lt;/h2&gt; 
&lt;p&gt;Reduce model size and improve performance with quantization using the convert script:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Convert and quantize to 4-bit
python -m mlx_audio.convert \
    --hf-path prince-canuma/Kokoro-82M \
    --mlx-path ./Kokoro-82M-4bit \
    --quantize \
    --q-bits 4 \
    --upload-repo username/Kokoro-82M-4bit (optional: if you want to upload the model to Hugging Face)

# Convert with specific dtype (bfloat16)
python -m mlx_audio.convert \
    --hf-path prince-canuma/Kokoro-82M \
    --mlx-path ./Kokoro-82M-bf16 \
    --dtype bfloat16 \
    --upload-repo username/Kokoro-82M-bf16 (optional: if you want to upload the model to Hugging Face)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Options:&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Flag&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--hf-path&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Source Hugging Face model or local path&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--mlx-path&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Output directory for converted model&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-q, --quantize&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Enable quantization&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--q-bits&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Bits per weight (4, 6, or 8)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--q-group-size&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Group size for quantization (default: 64)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--dtype&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Weight dtype: &lt;code&gt;float16&lt;/code&gt;, &lt;code&gt;bfloat16&lt;/code&gt;, &lt;code&gt;float32&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--upload-repo&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Upload converted model to HF Hub&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Swift&lt;/h2&gt; 
&lt;p&gt;Looking for Swift/iOS support? Check out &lt;a href="https://github.com/Blaizzy/mlx-audio-swift"&gt;mlx-audio-swift&lt;/a&gt; for on-device TTS using MLX on macOS and iOS.&lt;/p&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.10+&lt;/li&gt; 
 &lt;li&gt;Apple Silicon Mac (M1/M2/M3/M4)&lt;/li&gt; 
 &lt;li&gt;MLX framework&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ffmpeg&lt;/strong&gt; (required for MP3/FLAC audio encoding)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Installing ffmpeg&lt;/h3&gt; 
&lt;p&gt;ffmpeg is required for saving audio in MP3 or FLAC format. Install it using:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# macOS (using Homebrew)
brew install ffmpeg

# Ubuntu/Debian
sudo apt install ffmpeg
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;WAV format works without ffmpeg.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/Blaizzy/mlx-audio/main/LICENSE"&gt;MIT License&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{mlx-audio,
  author = {Canuma, Prince},
  title = {MLX Audio},
  year = {2025},
  howpublished = {\url{https://github.com/Blaizzy/mlx-audio}},
  note = {Audio processing library for Apple Silicon with TTS, STT, and STS capabilities.}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ml-explore/mlx"&gt;Apple MLX Team&lt;/a&gt; for the MLX framework&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>supermemoryai/supermemory</title>
      <link>https://github.com/supermemoryai/supermemory</link>
      <description>&lt;p&gt;Memory engine and app that is extremely fast, scalable. The Memory API for the AI era.&lt;/p&gt;&lt;hr&gt;&lt;p align="center" style="padding-bottom:20px;padding-top:20px"&gt; 
 &lt;picture&gt; 
  &lt;source srcset="apps/web/public/logo-fullmark.svg" media="(prefers-color-scheme: dark)" /&gt; 
  &lt;source srcset="apps/web/public/logo-light-fullmark.svg" media="(prefers-color-scheme: light)" /&gt; 
  &lt;img src="https://raw.githubusercontent.com/supermemoryai/supermemory/main/apps/web/public/logo-fullmark.svg?sanitize=true" alt="supermemory Logo" width="400" /&gt; 
 &lt;/picture&gt; &lt;br /&gt;&lt;br /&gt; &lt;em&gt;Your AI second brain for saving and organizing everything that matters.&lt;/em&gt; &lt;br /&gt;&lt;br /&gt; &lt;a href="https://app.supermemory.ai" style="text-decoration: none;"&gt; &lt;img src="https://img.shields.io/badge/Web-App-000000?style=for-the-badge" alt="Web App" /&gt; &lt;/a&gt; &amp;nbsp; &lt;a href="https://chromewebstore.google.com/detail/supermemory/afpgkkipfdpeaflnpoaffkcankadgjfc" style="text-decoration: none;"&gt; &lt;img src="https://img.shields.io/badge/Chrome-Extension-4285F4?style=for-the-badge&amp;amp;logo=googlechrome&amp;amp;logoColor=white" alt="Chrome Extension" /&gt; &lt;/a&gt; &amp;nbsp; &lt;a href="https://www.raycast.com/supermemory/supermemory" style="text-decoration: none;"&gt; &lt;img src="https://img.shields.io/badge/Raycast-Extension-FF6363?style=for-the-badge&amp;amp;logo=raycast&amp;amp;logoColor=white" alt="Raycast Extension" /&gt; &lt;/a&gt; &amp;nbsp; &lt;a href="https://supermemory.link/discord" style="text-decoration: none;"&gt; &lt;img src="https://img.shields.io/badge/Discord-5865F2?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p style="font-size: 0.9em; color: #666;"&gt; &lt;strong&gt;Building with Supermemory?&lt;/strong&gt; Check out the &lt;a href="https://console.supermemory.ai?utm_source=github&amp;amp;utm_medium=readme&amp;amp;utm_campaign=consumer_app"&gt;Developer Console&lt;/a&gt; and &lt;a href="https://docs.supermemory.ai?utm_source=github&amp;amp;utm_medium=readme&amp;amp;utm_campaign=consumer_app"&gt;Documentation&lt;/a&gt; for API access. &lt;/p&gt; 
&lt;p style="font-size: 0.9em; color: #666;"&gt; &lt;strong&gt;Want to self-host?&lt;/strong&gt; See our &lt;a href="https://supermemory.ai/docs/deployment/self-hosting#self-hosting"&gt;Self-Hosting Guide&lt;/a&gt; for enterprise deployment options. &lt;/p&gt; 
&lt;br /&gt; 
&lt;div align="center" style="padding-bottom:10px;padding-top:10px"&gt; 
 &lt;img src="https://raw.githubusercontent.com/supermemoryai/supermemory/main/apps/web/public/landing-page.jpeg" alt="supermemory" width="100%" /&gt; 
&lt;/div&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;h3&gt;Core Functionality&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/supermemoryai/supermemory/main/#add-memory"&gt;Add Memories from Any Content&lt;/a&gt;&lt;/strong&gt;: Easily add memories from URLs, PDFs, and plain text‚Äîjust paste, upload, or link.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/supermemoryai/supermemory/main/#chat-memories"&gt;Chat with Your Memories&lt;/a&gt;&lt;/strong&gt;: Converse with your stored content using natural language chat.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/supermemoryai/supermemory/main/#mcp-integration"&gt;Supermemory MCP Integration&lt;/a&gt;&lt;/strong&gt;: Seamlessly connect with all major AI tools (Claude, Cursor, etc.) via Supermemory MCP.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/supermemoryai/supermemory/main/#browser-extension"&gt;Browser Extension&lt;/a&gt;&lt;/strong&gt;: Save memories directly from your browser with integrations for ChatGPT, Claude, and Twitter/X.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/supermemoryai/supermemory/main/#raycast-extension"&gt;Raycast Extension&lt;/a&gt;&lt;/strong&gt;: Add and search memories directly from Raycast with keyboard shortcuts.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;How do I use this?&lt;/h2&gt; 
&lt;p&gt;Go to &lt;a href="https://app.supermemory.ai"&gt;app.supermemory.ai&lt;/a&gt; and sign in with your account&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a id="add-memory"&gt;&lt;/a&gt;Start Adding Memory with your choice of format (Note, Link, File)&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div align="center" style="padding-bottom:10px;padding-top:10px"&gt; 
 &lt;img src="https://raw.githubusercontent.com/supermemoryai/supermemory/main/apps/web/public/add-memory.png" alt="supermemory" width="100%" /&gt; 
&lt;/div&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;You can also Connect to your favourite services (Notion, Google Drive, OneDrive)&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div align="center" style="padding-bottom:10px;padding-top:10px"&gt; 
 &lt;img src="https://raw.githubusercontent.com/supermemoryai/supermemory/main/apps/web/public/add-connections.png" alt="supermemory" width="100%" /&gt; 
&lt;/div&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;&lt;a id="chat-memories"&gt;&lt;/a&gt;Once Memories are added, you can chat with Supermemory by clicking on "Open Chat" and retrieve info from your saved memories&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div align="center" style="padding-bottom:10px;padding-top:10px"&gt; 
 &lt;img src="https://raw.githubusercontent.com/supermemoryai/supermemory/main/apps/web/public/chat.png" alt="supermemory" width="100%" /&gt; 
&lt;/div&gt; 
&lt;ol start="4"&gt; 
 &lt;li&gt;&lt;a id="mcp-integration"&gt;&lt;/a&gt;Add MCP to your AI Tools (by clicking on "Connect to your AI" and select the AI tool you are trying to integrate)&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div align="center" style="padding-bottom:10px;padding-top:10px"&gt; 
 &lt;img src="https://raw.githubusercontent.com/supermemoryai/supermemory/main/apps/web/public/mcp.png" alt="supermemory" width="100%" /&gt; 
&lt;/div&gt; 
&lt;ol start="5"&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a id="browser-extension"&gt;&lt;/a&gt;&lt;strong&gt;Browser Extension&lt;/strong&gt;: Install the &lt;a href="https://chromewebstore.google.com/detail/supermemory/afpgkkipfdpeaflnpoaffkcankadgjfc"&gt;Chrome/Edge extension&lt;/a&gt; to save memories directly from any webpage, integrate with ChatGPT and Claude conversations, and import from Twitter/X. Right-click on any content or use the extension popup to save memories instantly.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a id="raycast-extension"&gt;&lt;/a&gt;&lt;strong&gt;Raycast Extension&lt;/strong&gt;: Install the &lt;a href="https://www.raycast.com/supermemory/supermemory"&gt;Raycast extension&lt;/a&gt; to add and search memories directly from Raycast. Use the "Add Memory" command to quickly save content, or "Search Memories" to find and retrieve your saved information with keyboard shortcuts.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Support&lt;/h2&gt; 
&lt;p&gt;Have questions or feedback? We're here to help:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Email: &lt;a href="mailto:support@supermemory.ai"&gt;support@supermemory.ai&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Discord: &lt;a href="https://supermemory.link/discord"&gt;Join our Discord server&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Documentation: &lt;a href="https://docs.supermemory.ai"&gt;docs.supermemory.ai&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions from developers of all skill levels! Whether you're fixing bugs, adding features, or improving documentation, your help makes supermemory better for everyone.&lt;/p&gt; 
&lt;p&gt;For detailed guidelines, development setup, coding standards, and the complete contribution workflow, please see our &lt;a href="https://raw.githubusercontent.com/supermemoryai/supermemory/main/CONTRIBUTING.md"&gt;&lt;strong&gt;Contributing Guide&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Ways to Contribute&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üêõ &lt;strong&gt;Bug fixes&lt;/strong&gt; - Help us squash those pesky issues&lt;/li&gt; 
 &lt;li&gt;‚ú® &lt;strong&gt;New features&lt;/strong&gt; - Add functionality that users will love&lt;/li&gt; 
 &lt;li&gt;üé® &lt;strong&gt;UI/UX improvements&lt;/strong&gt; - Make the interface more intuitive&lt;/li&gt; 
 &lt;li&gt;‚ö° &lt;strong&gt;Performance optimizations&lt;/strong&gt; - Help us make supermemory faster&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Check out our &lt;a href="https://github.com/supermemoryai/supermemory/issues"&gt;Issues&lt;/a&gt; page for &lt;code&gt;good first issue&lt;/code&gt; and &lt;code&gt;help wanted&lt;/code&gt; labels to get started!&lt;/p&gt; 
&lt;h2&gt;Updates &amp;amp; Roadmap&lt;/h2&gt; 
&lt;p&gt;Stay up to date with the latest improvements:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.supermemory.ai/changelog/overview"&gt;Changelog&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://x.com/supermemory"&gt;X&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
  </channel>
</rss>