<rss version="2.0">
  <channel>
    <title>GitHub Rust Daily Trending</title>
    <description>Daily Trending of Rust in GitHub</description>
    <pubDate>Fri, 05 Sep 2025 01:37:10 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>nexus-xyz/nexus-cli</title>
      <link>https://github.com/nexus-xyz/nexus-cli</link>
      <description>&lt;p&gt;Command line interface for supplying proofs to the Nexus network.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="https://github.com/nexus-xyz/nexus-cli/releases"&gt;&lt;img src="https://img.shields.io/github/v/release/nexus-xyz/nexus-cli.svg?sanitize=true" alt="Release" /&gt;&lt;/a&gt; &lt;a href="https://github.com/nexus-xyz/nexus-cli/actions"&gt;&lt;img src="https://github.com/nexus-xyz/nexus-cli/actions/workflows/ci.yml/badge.svg?sanitize=true" alt="CI" /&gt;&lt;/a&gt; &lt;a href="https://github.com/nexus-xyz/nexus-cli/raw/main/LICENSE-APACHE"&gt;&lt;img src="https://img.shields.io/badge/License-Apache_2.0-green.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://github.com/nexus-xyz/nexus-cli/raw/main/LICENSE-MIT"&gt;&lt;img src="https://img.shields.io/badge/License-MIT-green.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://x.com/NexusLabs"&gt;&lt;img src="https://img.shields.io/twitter/follow/NexusLabs" alt="Twitter" /&gt;&lt;/a&gt; &lt;a href="https://discord.com/invite/nexus-xyz"&gt;&lt;img src="https://img.shields.io/badge/Discord-Join-7289da.svg?logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Nexus CLI&lt;/h1&gt; 
&lt;p&gt;A high-performance command-line interface for contributing proofs to the Nexus network.&lt;/p&gt; 
&lt;figure&gt; 
 &lt;a href="https://nexus.xyz/"&gt; &lt;img src="https://raw.githubusercontent.com/nexus-xyz/nexus-cli/main/assets/images/nexus-network-image.png" alt="Nexus Network visualization showing a distributed network of interconnected nodes with a 'Launch Network' button in the center" /&gt; &lt;/a&gt; 
 &lt;figcaption&gt; 
  &lt;strong&gt;Verifiable Computation on a Global Scale&lt;/strong&gt;
  &lt;br /&gt; We're building a global distributed prover network to unite the world's computers and power a new and better Internet: the Verifiable Internet. Connect to the beta and give it a try today. 
 &lt;/figcaption&gt; 
&lt;/figure&gt; 
&lt;h2&gt;Nexus Network&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://nexus.xyz/"&gt;Nexus&lt;/a&gt; is a global distributed prover network that unites the world's computers to power a new and better Internet: the Verifiable Internet.&lt;/p&gt; 
&lt;p&gt;There have been several testnets so far:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Testnet 0: &lt;a href="https://blog.nexus.xyz/nexus-launches-worlds-first-open-prover-network/"&gt;October 8 – 28, 2024&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Testnet I: &lt;a href="https://blog.nexus.xyz/the-new-nexus-testnet-is-live/"&gt;December 9 – 13, 2024&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Testnet II: &lt;a href="https://blog.nexus.xyz/testnet-ii-is-open/"&gt;February 18 – 22, 2025&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Devnet: &lt;a href="https://docs.nexus.xyz/layer-1/testnet/devnet"&gt;February 22 - June 20, 2025&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Testnet III: &lt;a href="https://blog.nexus.xyz/live-everywhere/"&gt;Ongoing&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;h4&gt;Precompiled Binary (Recommended)&lt;/h4&gt; 
&lt;p&gt;For the simplest and most reliable installation:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl https://cli.nexus.xyz/ | sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Download and install the latest precompiled binary for your platform.&lt;/li&gt; 
 &lt;li&gt;Prompt you to accept the Terms of Use.&lt;/li&gt; 
 &lt;li&gt;Start the CLI in interactive mode.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;The template installation script is viewable &lt;a href="https://raw.githubusercontent.com/nexus-xyz/nexus-cli/main/public/install.sh.template"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;Non-Interactive Installation&lt;/h4&gt; 
&lt;p&gt;For automated installations (e.g., in CI):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -sSf https://cli.nexus.xyz/ -o install.sh
chmod +x install.sh
NONINTERACTIVE=1 ./install.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Proving&lt;/h3&gt; 
&lt;p&gt;Proving with the CLI is documented &lt;a href="https://docs.nexus.xyz/layer-1/testnet/cli-node"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To start with an existing node ID, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;nexus-cli start --node-id &amp;lt;your-node-id&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, you can register your wallet address and create a node ID with the CLI, or at &lt;a href="https://app.nexus.xyz"&gt;app.nexus.xyz&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;nexus-cli register-user --wallet-address &amp;lt;your-wallet-address&amp;gt;
nexus-cli register-node
nexus-cli start
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To run the CLI noninteractively, you can also opt to start it in headless mode.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;nexus-cli start --headless
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The &lt;code&gt;register-user&lt;/code&gt; and &lt;code&gt;register-node&lt;/code&gt; commands will save your credentials to &lt;code&gt;~/.nexus/config.json&lt;/code&gt;. To clear credentials, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;nexus-cli logout
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For troubleshooting or to see available command-line options, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;nexus-cli --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Use Docker&lt;/h3&gt; 
&lt;p&gt;Make sure Docker and Docker Compose have been installed on your machine. Check documentation here:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.docker.com/engine/install/"&gt;Install Docker&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.docker.com/compose/install/"&gt;Install Docker Compose&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Then, modify the node ID in the &lt;code&gt;docker-compose.yaml&lt;/code&gt; file, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker compose build --no-cache
docker compose up -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Check log&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker compose logs
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you want to shut down, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker compose down
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Terms of Use&lt;/h2&gt; 
&lt;p&gt;Use of the CLI is subject to the &lt;a href="https://nexus.xyz/terms-of-use"&gt;Terms of Use&lt;/a&gt;. First-time users running interactively will be prompted to accept these terms.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Node ID&lt;/h2&gt; 
&lt;p&gt;During the CLI's startup, you'll be asked for your node ID. To skip prompts in a non-interactive environment, manually create a &lt;code&gt;~/.nexus/config.json&lt;/code&gt; in the following format:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
   "node_id": "&amp;lt;YOUR NODE ID&amp;gt;"
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Get Help&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.nexus.xyz/layer-1/testnet/faq"&gt;Network FAQ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://discord.gg/nexus-xyz"&gt;Discord Community&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Technical issues? &lt;a href="https://github.com/nexus-xyz/nexus-cli/issues"&gt;Open an issue&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;To submit programs to the network for proving, contact &lt;a href="mailto:growth@nexus.xyz"&gt;growth@nexus.xyz&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Interested in contributing to the Nexus Network CLI? Check out our &lt;a href="https://raw.githubusercontent.com/nexus-xyz/nexus-cli/main/CONTRIBUTING.md"&gt;Contributor Guide&lt;/a&gt; for:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Development setup instructions&lt;/li&gt; 
 &lt;li&gt;How to report issues and submit pull requests&lt;/li&gt; 
 &lt;li&gt;Our code of conduct and community guidelines&lt;/li&gt; 
 &lt;li&gt;Tips for working with the codebase&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For most users, we recommend using the precompiled binaries as described above. The contributor guide is intended for those who want to modify or improve the CLI itself.&lt;/p&gt; 
&lt;h3&gt;🛠 Developer Guide&lt;/h3&gt; 
&lt;p&gt;The following steps may be required in order to set up a development environment for contributing to the project:&lt;/p&gt; 
&lt;h4&gt;Linux&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt update
sudo apt upgrade
sudo apt install build-essential pkg-config libssl-dev git-all
sudo apt install protobuf-compiler
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;macOS&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install using Homebrew
brew install protobuf

# Verify installation
protoc --version
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Windows&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://learn.microsoft.com/en-us/windows/wsl/install"&gt;Install WSL&lt;/a&gt;, then see Linux instructions above.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install using Chocolatey
choco install protobuf
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Building ProtoBuf files&lt;/h3&gt; 
&lt;p&gt;To build the ProtoBuf files, run the following command in the &lt;code&gt;clients/cli&lt;/code&gt; directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cargo build --features build_proto
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Creating a Release&lt;/h3&gt; 
&lt;p&gt;To create a release, update the package version in &lt;code&gt;Cargo.toml&lt;/code&gt;, then create and push a new (annotated) tag, e.g.:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git tag -a v0.1.2 -m "Release v0.1.2"
git push origin v0.1.2
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will trigger the GitHub Actions release workflow that compiles binaries and pushes the Docker image, in addition to creating release.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;WARNING&lt;/strong&gt;: Creating a release through the GitHub UI creates a new release but does &lt;strong&gt;NOT&lt;/strong&gt; trigger the workflow. This leads to a release without a Docker image or binaries, which breaks the installation script.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Nexus CLI is distributed under the terms of both the &lt;a href="https://raw.githubusercontent.com/nexus-xyz/nexus-cli/main/LICENSE-MIT"&gt;MIT License&lt;/a&gt; and the &lt;a href="https://raw.githubusercontent.com/nexus-xyz/nexus-cli/main/LICENSE-APACHE"&gt;Apache License (Version 2.0)&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>rathole-org/rathole</title>
      <link>https://github.com/rathole-org/rathole</link>
      <description>&lt;p&gt;A lightweight and high-performance reverse proxy for NAT traversal, written in Rust. An alternative to frp and ngrok.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;rathole&lt;/h1&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/rathole-org/rathole/main/docs/img/rathole-logo.png" alt="rathole-logo" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/rapiz1/rathole/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/rapiz1/rathole" alt="GitHub stars" /&gt;&lt;/a&gt; &lt;a href="https://github.com/rapiz1/rathole/releases"&gt;&lt;img src="https://img.shields.io/github/v/release/rapiz1/rathole" alt="GitHub release (latest SemVer)" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/github/actions/workflow/status/rapiz1/rathole/rust.yml?branch=main" alt="GitHub Workflow Status (branch)" /&gt; &lt;a href="https://github.com/rapiz1/rathole/releases"&gt;&lt;img src="https://img.shields.io/github/downloads/rapiz1/rathole/total" alt="GitHub all releases" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/rapiz1/rathole"&gt;&lt;img src="https://img.shields.io/docker/pulls/rapiz1/rathole" alt="Docker Pulls" /&gt;&lt;/a&gt; &lt;a href="https://gitter.im/rapiz1/rathole?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge"&gt;&lt;img src="https://badges.gitter.im/rapiz1/rathole.svg?sanitize=true" alt="Join the chat at https://gitter.im/rapiz1/rathole" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/rathole-org/rathole/main/README.md"&gt;English&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/rathole-org/rathole/main/README-zh.md"&gt;简体中文&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;A secure, stable and high-performance reverse proxy for NAT traversal, written in Rust&lt;/p&gt; 
&lt;p&gt;rathole, like &lt;a href="https://github.com/fatedier/frp"&gt;frp&lt;/a&gt; and &lt;a href="https://github.com/inconshreveable/ngrok"&gt;ngrok&lt;/a&gt;, can help to expose the service on the device behind the NAT to the Internet, via a server with a public IP.&lt;/p&gt; 
&lt;!-- TOC --&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rathole-org/rathole/main/#rathole"&gt;rathole&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rathole-org/rathole/main/#features"&gt;Features&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rathole-org/rathole/main/#quickstart"&gt;Quickstart&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rathole-org/rathole/main/#configuration"&gt;Configuration&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rathole-org/rathole/main/#logging"&gt;Logging&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rathole-org/rathole/main/#tuning"&gt;Tuning&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rathole-org/rathole/main/#benchmark"&gt;Benchmark&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rathole-org/rathole/main/#planning"&gt;Planning&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- /TOC --&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;High Performance&lt;/strong&gt; Much higher throughput can be achieved than frp, and more stable when handling a large volume of connections. See &lt;a href="https://raw.githubusercontent.com/rathole-org/rathole/main/#benchmark"&gt;Benchmark&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Low Resource Consumption&lt;/strong&gt; Consumes much fewer memory than similar tools. See &lt;a href="https://raw.githubusercontent.com/rathole-org/rathole/main/#benchmark"&gt;Benchmark&lt;/a&gt;. &lt;a href="https://raw.githubusercontent.com/rathole-org/rathole/main/docs/build-guide.md"&gt;The binary can be&lt;/a&gt; &lt;strong&gt;as small as ~500KiB&lt;/strong&gt; to fit the constraints of devices, like embedded devices as routers.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Security&lt;/strong&gt; Tokens of services are mandatory and service-wise. The server and clients are responsible for their own configs. With the optional Noise Protocol, encryption can be configured at ease. No need to create a self-signed certificate! TLS is also supported.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Hot Reload&lt;/strong&gt; Services can be added or removed dynamically by hot-reloading the configuration file. HTTP API is WIP.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;p&gt;A full-powered &lt;code&gt;rathole&lt;/code&gt; can be obtained from the &lt;a href="https://github.com/rapiz1/rathole/releases"&gt;release&lt;/a&gt; page. Or &lt;a href="https://raw.githubusercontent.com/rathole-org/rathole/main/docs/build-guide.md"&gt;build from source&lt;/a&gt; &lt;strong&gt;for other platforms and minimizing the binary&lt;/strong&gt;. A &lt;a href="https://hub.docker.com/r/rapiz1/rathole"&gt;Docker image&lt;/a&gt; is also available.&lt;/p&gt; 
&lt;p&gt;The usage of &lt;code&gt;rathole&lt;/code&gt; is very similar to frp. If you have experience with the latter, then the configuration is very easy for you. The only difference is that configuration of a service is split into the client side and the server side, and a token is mandatory.&lt;/p&gt; 
&lt;p&gt;To use &lt;code&gt;rathole&lt;/code&gt;, you need a server with a public IP, and a device behind the NAT, where some services that need to be exposed to the Internet.&lt;/p&gt; 
&lt;p&gt;Assuming you have a NAS at home behind the NAT, and want to expose its ssh service to the Internet:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;On the server which has a public IP&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Create &lt;code&gt;server.toml&lt;/code&gt; with the following content and accommodate it to your needs.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-toml"&gt;# server.toml
[server]
bind_addr = "0.0.0.0:2333" # `2333` specifies the port that rathole listens for clients

[server.services.my_nas_ssh]
token = "use_a_secret_that_only_you_know" # Token that is used to authenticate the client for the service. Change to an arbitrary value.
bind_addr = "0.0.0.0:5202" # `5202` specifies the port that exposes `my_nas_ssh` to the Internet
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;./rathole server.toml
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;On the host which is behind the NAT (your NAS)&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Create &lt;code&gt;client.toml&lt;/code&gt; with the following content and accommodate it to your needs.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-toml"&gt;# client.toml
[client]
remote_addr = "myserver.com:2333" # The address of the server. The port must be the same with the port in `server.bind_addr`

[client.services.my_nas_ssh]
token = "use_a_secret_that_only_you_know" # Must be the same with the server to pass the validation
local_addr = "127.0.0.1:22" # The address of the service that needs to be forwarded
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;./rathole client.toml
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Now the client will try to connect to the server &lt;code&gt;myserver.com&lt;/code&gt; on port &lt;code&gt;2333&lt;/code&gt;, and any traffic to &lt;code&gt;myserver.com:5202&lt;/code&gt; will be forwarded to the client's port &lt;code&gt;22&lt;/code&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;So you can &lt;code&gt;ssh myserver.com:5202&lt;/code&gt; to ssh to your NAS.&lt;/p&gt; 
&lt;p&gt;To run &lt;code&gt;rathole&lt;/code&gt; run as a background service on Linux, checkout the &lt;a href="https://raw.githubusercontent.com/rathole-org/rathole/main/examples/systemd"&gt;systemd examples&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Configuration&lt;/h2&gt; 
&lt;p&gt;&lt;code&gt;rathole&lt;/code&gt; can automatically determine to run in the server mode or the client mode, according to the content of the configuration file, if only one of &lt;code&gt;[server]&lt;/code&gt; and &lt;code&gt;[client]&lt;/code&gt; block is present, like the example in &lt;a href="https://raw.githubusercontent.com/rathole-org/rathole/main/#quickstart"&gt;Quickstart&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;But the &lt;code&gt;[client]&lt;/code&gt; and &lt;code&gt;[server]&lt;/code&gt; block can also be put in one file. Then on the server side, run &lt;code&gt;rathole --server config.toml&lt;/code&gt; and on the client side, run &lt;code&gt;rathole --client config.toml&lt;/code&gt; to explicitly tell &lt;code&gt;rathole&lt;/code&gt; the running mode.&lt;/p&gt; 
&lt;p&gt;Before heading to the full configuration specification, it's recommend to skim &lt;a href="https://raw.githubusercontent.com/rathole-org/rathole/main/examples"&gt;the configuration examples&lt;/a&gt; to get a feeling of the configuration format.&lt;/p&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/rathole-org/rathole/main/docs/transport.md"&gt;Transport&lt;/a&gt; for more details about encryption and the &lt;code&gt;transport&lt;/code&gt; block.&lt;/p&gt; 
&lt;p&gt;Here is the full configuration specification:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-toml"&gt;[client]
remote_addr = "example.com:2333" # Necessary. The address of the server
default_token = "default_token_if_not_specify" # Optional. The default token of services, if they don't define their own ones
heartbeat_timeout = 40 # Optional. Set to 0 to disable the application-layer heartbeat test. The value must be greater than `server.heartbeat_interval`. Default: 40 seconds
retry_interval = 1 # Optional. The interval between retry to connect to the server. Default: 1 second

[client.transport] # The whole block is optional. Specify which transport to use
type = "tcp" # Optional. Possible values: ["tcp", "tls", "noise"]. Default: "tcp"

[client.transport.tcp] # Optional. Also affects `noise` and `tls`
proxy = "socks5://user:passwd@127.0.0.1:1080" # Optional. The proxy used to connect to the server. `http` and `socks5` is supported.
nodelay = true # Optional. Determine whether to enable TCP_NODELAY, if applicable, to improve the latency but decrease the bandwidth. Default: true
keepalive_secs = 20 # Optional. Specify `tcp_keepalive_time` in `tcp(7)`, if applicable. Default: 20 seconds
keepalive_interval = 8 # Optional. Specify `tcp_keepalive_intvl` in `tcp(7)`, if applicable. Default: 8 seconds

[client.transport.tls] # Necessary if `type` is "tls"
trusted_root = "ca.pem" # Necessary. The certificate of CA that signed the server's certificate
hostname = "example.com" # Optional. The hostname that the client uses to validate the certificate. If not set, fallback to `client.remote_addr`

[client.transport.noise] # Noise protocol. See `docs/transport.md` for further explanation
pattern = "Noise_NK_25519_ChaChaPoly_BLAKE2s" # Optional. Default value as shown
local_private_key = "key_encoded_in_base64" # Optional
remote_public_key = "key_encoded_in_base64" # Optional

[client.transport.websocket] # Necessary if `type` is "websocket"
tls = true # If `true` then it will use settings in `client.transport.tls`

[client.services.service1] # A service that needs forwarding. The name `service1` can change arbitrarily, as long as identical to the name in the server's configuration
type = "tcp" # Optional. The protocol that needs forwarding. Possible values: ["tcp", "udp"]. Default: "tcp"
token = "whatever" # Necessary if `client.default_token` not set
local_addr = "127.0.0.1:1081" # Necessary. The address of the service that needs to be forwarded
nodelay = true # Optional. Override the `client.transport.nodelay` per service
retry_interval = 1 # Optional. The interval between retry to connect to the server. Default: inherits the global config

[client.services.service2] # Multiple services can be defined
local_addr = "127.0.0.1:1082"

[server]
bind_addr = "0.0.0.0:2333" # Necessary. The address that the server listens for clients. Generally only the port needs to be change.
default_token = "default_token_if_not_specify" # Optional
heartbeat_interval = 30 # Optional. The interval between two application-layer heartbeat. Set to 0 to disable sending heartbeat. Default: 30 seconds

[server.transport] # Same as `[client.transport]`
type = "tcp"

[server.transport.tcp] # Same as the client
nodelay = true
keepalive_secs = 20
keepalive_interval = 8

[server.transport.tls] # Necessary if `type` is "tls"
pkcs12 = "identify.pfx" # Necessary. pkcs12 file of server's certificate and private key
pkcs12_password = "password" # Necessary. Password of the pkcs12 file

[server.transport.noise] # Same as `[client.transport.noise]`
pattern = "Noise_NK_25519_ChaChaPoly_BLAKE2s"
local_private_key = "key_encoded_in_base64"
remote_public_key = "key_encoded_in_base64"

[server.transport.websocket] # Necessary if `type` is "websocket"
tls = true # If `true` then it will use settings in `server.transport.tls`

[server.services.service1] # The service name must be identical to the client side
type = "tcp" # Optional. Same as the client `[client.services.X.type]
token = "whatever" # Necessary if `server.default_token` not set
bind_addr = "0.0.0.0:8081" # Necessary. The address of the service is exposed at. Generally only the port needs to be change.
nodelay = true # Optional. Same as the client

[server.services.service2]
bind_addr = "0.0.0.1:8082"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Logging&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;rathole&lt;/code&gt;, like many other Rust programs, use environment variables to control the logging level. &lt;code&gt;info&lt;/code&gt;, &lt;code&gt;warn&lt;/code&gt;, &lt;code&gt;error&lt;/code&gt;, &lt;code&gt;debug&lt;/code&gt;, &lt;code&gt;trace&lt;/code&gt; are available.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;RUST_LOG=error ./rathole config.toml
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;will run &lt;code&gt;rathole&lt;/code&gt; with only error level logging.&lt;/p&gt; 
&lt;p&gt;If &lt;code&gt;RUST_LOG&lt;/code&gt; is not present, the default logging level is &lt;code&gt;info&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Tuning&lt;/h3&gt; 
&lt;p&gt;From v0.4.7, rathole enables TCP_NODELAY by default, which should benefit the latency and interactive applications like rdp, Minecraft servers. However, it slightly decreases the bandwidth.&lt;/p&gt; 
&lt;p&gt;If the bandwidth is more important, TCP_NODELAY can be opted out with &lt;code&gt;nodelay = false&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Benchmark&lt;/h2&gt; 
&lt;p&gt;rathole has similar latency to &lt;a href="https://github.com/fatedier/frp"&gt;frp&lt;/a&gt;, but can handle a more connections, provide larger bandwidth, with less memory usage.&lt;/p&gt; 
&lt;p&gt;For more details, see the separate page &lt;a href="https://raw.githubusercontent.com/rathole-org/rathole/main/docs/benchmark.md"&gt;Benchmark&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;However, don't take it from here that &lt;code&gt;rathole&lt;/code&gt; can magically make your forwarded service faster several times than before.&lt;/strong&gt; The benchmark is done on local loopback, indicating the performance when the task is cpu-bounded. One can gain quite a improvement if the network is not the bottleneck. Unfortunately, that's not true for many users. In that case, the main benefit is lower resource consumption, while the bandwidth and the latency may not improved significantly.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/rathole-org/rathole/main/docs/img/http_throughput.svg?sanitize=true" alt="http_throughput" /&gt; &lt;img src="https://raw.githubusercontent.com/rathole-org/rathole/main/docs/img/tcp_bitrate.svg?sanitize=true" alt="tcp_bitrate" /&gt; &lt;img src="https://raw.githubusercontent.com/rathole-org/rathole/main/docs/img/udp_bitrate.svg?sanitize=true" alt="udp_bitrate" /&gt; &lt;img src="https://raw.githubusercontent.com/rathole-org/rathole/main/docs/img/mem-graph.png" alt="mem" /&gt;&lt;/p&gt; 
&lt;h2&gt;Planning&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; HTTP APIs for configuration&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/rathole-org/rathole/main/docs/out-of-scope.md"&gt;Out of Scope&lt;/a&gt; lists features that are not planned to be implemented and why.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>tracel-ai/burn</title>
      <link>https://github.com/tracel-ai/burn</link>
      <description>&lt;p&gt;Burn is a next generation Deep Learning Framework that doesn't compromise on flexibility, efficiency and portability.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/tracel-ai/burn/main/assets/logo-burn-neutral.webp" width="350px" /&gt; 
 &lt;p&gt;&lt;a href="https://discord.gg/uPEBbYYDB6"&gt;&lt;img src="https://img.shields.io/discord/1038839012602941528.svg?color=7289da&amp;amp;&amp;amp;logo=discord" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://crates.io/crates/burn"&gt;&lt;img src="https://img.shields.io/crates/v/burn.svg?sanitize=true" alt="Current Crates.io Version" /&gt;&lt;/a&gt; &lt;a href="https://crates.io/crates/burn"&gt;&lt;img src="https://img.shields.io/crates/msrv/burn" alt="Minimum Supported Rust Version" /&gt;&lt;/a&gt; &lt;a href="https://burn.dev/docs/burn"&gt;&lt;img src="https://img.shields.io/badge/docs-latest-blue" alt="Documentation" /&gt;&lt;/a&gt; &lt;a href="https://github.com/tracel-ai/burn/actions/workflows/test.yml"&gt;&lt;img src="https://github.com/tracel-ai/burn/actions/workflows/test.yml/badge.svg?sanitize=true" alt="Test Status" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/#license"&gt;&lt;img src="https://shields.io/badge/license-MIT%2FApache--2.0-blue" alt="license" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/tracel-ai/burn"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://www.runblaze.dev"&gt;&lt;img src="https://www.runblaze.dev/ci-blaze-powered.png" width="125px" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;hr /&gt; 
 &lt;p&gt;&lt;strong&gt;Burn is a next generation Deep Learning Framework that doesn't compromise on &lt;br /&gt; flexibility, efficiency and portability.&lt;/strong&gt;&lt;/p&gt; 
 &lt;br /&gt; 
&lt;/div&gt; 
&lt;div align="left"&gt; 
 &lt;h2&gt;Performance&lt;/h2&gt; 
 &lt;div align="left"&gt; 
  &lt;img align="right" src="https://raw.githubusercontent.com/tracel-ai/burn/main/assets/ember-blazingly-fast.png" height="96px" /&gt; 
  &lt;p&gt;Because we believe the goal of a deep learning framework is to convert computation into useful intelligence, we have made performance a core pillar of Burn. We strive to achieve top efficiency by leveraging multiple optimization techniques described below.&lt;/p&gt; 
  &lt;p&gt;&lt;strong&gt;Click on each section for more details&lt;/strong&gt; 👇&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;br /&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Automatic kernel fusion 💥 &lt;/summary&gt; 
  &lt;br /&gt; 
  &lt;p&gt;Using Burn means having your models optimized on any backend. When possible, we provide a way to automatically and dynamically create custom kernels that minimize data relocation between different memory spaces, extremely useful when moving memory is the bottleneck.&lt;/p&gt; 
  &lt;p&gt;As an example, you could write your own GELU activation function with the high level tensor api (see Rust code snippet below).&lt;/p&gt; 
  &lt;pre&gt;&lt;code class="language-rust"&gt;fn gelu_custom&amp;lt;B: Backend, const D: usize&amp;gt;(x: Tensor&amp;lt;B, D&amp;gt;) -&amp;gt; Tensor&amp;lt;B, D&amp;gt; {
    let x = x.clone() * ((x / SQRT_2).erf() + 1);
    x / 2
}
&lt;/code&gt;&lt;/pre&gt; 
  &lt;p&gt;Then, at runtime, a custom low-level kernel will be automatically created for your specific implementation and will rival a handcrafted GPU implementation. The kernel consists of about 60 lines of WGSL &lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/%22https://www.w3.org/TR/WGSL/https://www.w3.org/TR/WGSL/%22"&gt;WebGPU Shading Language&lt;/a&gt;, an extremely verbose lower level shader language you probably don't want to program your deep learning models in!&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Asynchronous execution ❤️‍🔥 &lt;/summary&gt; 
  &lt;br /&gt; 
  &lt;p&gt;For &lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/#backends"&gt;first-party backends&lt;/a&gt;, an asynchronous execution style is used, which allows to perform various optimizations, such as the previously mentioned automatic kernel fusion.&lt;/p&gt; 
  &lt;p&gt;Asynchronous execution also ensures that the normal execution of the framework does not block the model computations, which implies that the framework overhead won't impact the speed of execution significantly. Conversely, the intense computations in the model do not interfere with the responsiveness of the framework. For more information about our asynchronous backends, see &lt;a href="https://burn.dev/blog/creating-high-performance-asynchronous-backends-with-burn-compute"&gt;this blog post&lt;/a&gt;.&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Thread-safe building blocks 🦞 &lt;/summary&gt; 
  &lt;br /&gt; 
  &lt;p&gt;Burn emphasizes thread safety by leveraging the &lt;a href="https://doc.rust-lang.org/book/ch04-00-understanding-ownership.html"&gt;ownership system of Rust&lt;/a&gt;. With Burn, each module is the owner of its weights. It is therefore possible to send a module to another thread for computing the gradients, then send the gradients to the main thread that can aggregate them, and &lt;em&gt;voilà&lt;/em&gt;, you get multi-device training.&lt;/p&gt; 
  &lt;p&gt;This is a very different approach from what PyTorch does, where backpropagation actually mutates the &lt;em&gt;grad&lt;/em&gt; attribute of each tensor parameter. This is not a thread-safe operation and therefore requires lower level synchronization primitives, see &lt;a href="https://pytorch.org/docs/stable/distributed.html"&gt;distributed training&lt;/a&gt; for reference. Note that this is still very fast, but not compatible across different backends and quite hard to implement.&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Intelligent memory management 🦀 &lt;/summary&gt; 
  &lt;br /&gt; 
  &lt;p&gt;One of the main roles of a deep learning framework is to reduce the amount of memory necessary to run models. The naive way of handling memory is that each tensor has its own memory space, which is allocated when the tensor is created then deallocated as the tensor gets out of scope. However, allocating and deallocating data is very costly, so a memory pool is often required to achieve good throughput. Burn offers an infrastructure that allows for easily creating and selecting memory management strategies for backends. For more details on memory management in Burn, see &lt;a href="https://burn.dev/blog/creating-high-performance-asynchronous-backends-with-burn-compute"&gt;this blog post&lt;/a&gt;.&lt;/p&gt; 
  &lt;p&gt;Another very important memory optimization of Burn is that we keep track of when a tensor can be mutated in-place just by using the ownership system well. Even though it is a rather small memory optimization on its own, it adds up considerably when training or running inference with larger models and contributes to reduce the memory usage even more. For more information, see &lt;a href="https://burn.dev/blog/burn-rusty-approach-to-tensor-handling"&gt;this blog post about tensor handling&lt;/a&gt;.&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Automatic kernel selection 🎯 &lt;/summary&gt; 
  &lt;br /&gt; 
  &lt;p&gt;A good deep learning framework should ensure that models run smoothly on all hardware. However, not all hardware share the same behavior in terms of execution speed. For instance, a matrix multiplication kernel can be launched with many different parameters, which are highly sensitive to the size of the matrices and the hardware. Using the wrong configuration could reduce the speed of execution by a large factor (10 times or even more in extreme cases), so choosing the right kernels becomes a priority.&lt;/p&gt; 
  &lt;p&gt;With our home-made backends, we run benchmarks automatically and choose the best configuration for the current hardware and matrix sizes with a reasonable caching strategy.&lt;/p&gt; 
  &lt;p&gt;This adds a small overhead by increasing the warmup execution time, but stabilizes quickly after a few forward and backward passes, saving lots of time in the long run. Note that this feature isn't mandatory, and can be disabled when cold starts are a priority over optimized throughput.&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Hardware specific features 🔥 &lt;/summary&gt; 
  &lt;br /&gt; 
  &lt;p&gt;It is no secret that deep learning is mostly relying on matrix multiplication as its core operation, since this is how fully-connected neural networks are modeled.&lt;/p&gt; 
  &lt;p&gt;More and more, hardware manufacturers optimize their chips specifically for matrix multiplication workloads. For instance, Nvidia has its &lt;em&gt;Tensor Cores&lt;/em&gt; and today most cellphones have AI specialized chips. As of this moment, we support Tensor Cores with our LibTorch, Candle, CUDA, Metal and WGPU/SPIR-V backends, but not other accelerators yet. We hope &lt;a href="https://github.com/gpuweb/gpuweb/issues/4195"&gt;this issue&lt;/a&gt; gets resolved at some point to bring support to our WGPU backend.&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Custom Backend Extension 🎒 &lt;/summary&gt; 
  &lt;br /&gt; 
  &lt;p&gt;Burn aims to be the most flexible deep learning framework. While it's crucial to maintain compatibility with a wide variety of backends, Burn also provides the ability to extend the functionalities of a backend implementation to suit your personal modeling requirements.&lt;/p&gt; 
  &lt;p&gt;This versatility is advantageous in numerous ways, such as supporting custom operations like flash attention or manually writing your own kernel for a specific backend to enhance performance. See &lt;a href="https://burn.dev/books/burn/advanced/backend-extension/index.html"&gt;this section&lt;/a&gt; in the Burn Book 🔥 for more details.&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;h2&gt;Backend&lt;/h2&gt; 
 &lt;div align="left"&gt; 
  &lt;img align="right" src="https://raw.githubusercontent.com/tracel-ai/burn/main/assets/backend-chip.png" height="96px" /&gt; 
  &lt;p&gt;Burn strives to be as fast as possible on as many hardwares as possible, with robust implementations. We believe this flexibility is crucial for modern needs where you may train your models in the cloud, then deploy on customer hardwares, which vary from user to user.&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;Supported Backends&lt;/strong&gt;&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Backend&lt;/th&gt; 
    &lt;th&gt;Devices&lt;/th&gt; 
    &lt;th&gt;Class&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CUDA&lt;/td&gt; 
    &lt;td&gt;NVIDIA GPUs&lt;/td&gt; 
    &lt;td&gt;First-Party&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;ROCm&lt;/td&gt; 
    &lt;td&gt;AMD GPUs&lt;/td&gt; 
    &lt;td&gt;First-Party&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Metal&lt;/td&gt; 
    &lt;td&gt;Apple GPUs&lt;/td&gt; 
    &lt;td&gt;First-Party&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Vulkan&lt;/td&gt; 
    &lt;td&gt;Most GPUs on Linux &amp;amp; Windows&lt;/td&gt; 
    &lt;td&gt;First-Party&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Wgpu&lt;/td&gt; 
    &lt;td&gt;Most GPUs&lt;/td&gt; 
    &lt;td&gt;First-Party&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;NdArray&lt;/td&gt; 
    &lt;td&gt;Most CPUs&lt;/td&gt; 
    &lt;td&gt;Third-Party&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;LibTorch&lt;/td&gt; 
    &lt;td&gt;Most GPUs &amp;amp; CPUs&lt;/td&gt; 
    &lt;td&gt;Third-Party&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Candle&lt;/td&gt; 
    &lt;td&gt;Nvidia, Apple GPUs &amp;amp; CPUs&lt;/td&gt; 
    &lt;td&gt;Third-Party&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;br /&gt; 
 &lt;p&gt;Compared to other frameworks, Burn has a very different approach to supporting many backends. By design, most code is generic over the Backend trait, which allows us to build Burn with swappable backends. This makes composing backend possible, augmenting them with additional functionalities such as autodifferentiation and automatic kernel fusion.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Autodiff: Backend decorator that brings backpropagation to any backend 🔄 &lt;/summary&gt; 
  &lt;br /&gt; 
  &lt;p&gt;Contrary to the aforementioned backends, Autodiff is actually a backend &lt;em&gt;decorator&lt;/em&gt;. This means that it cannot exist by itself; it must encapsulate another backend.&lt;/p&gt; 
  &lt;p&gt;The simple act of wrapping a base backend with Autodiff transparently equips it with autodifferentiation support, making it possible to call backward on your model.&lt;/p&gt; 
  &lt;pre&gt;&lt;code class="language-rust"&gt;use burn::backend::{Autodiff, Wgpu};
use burn::tensor::{Distribution, Tensor};

fn main() {
    type Backend = Autodiff&amp;lt;Wgpu&amp;gt;;

    let device = Default::default();

    let x: Tensor&amp;lt;Backend, 2&amp;gt; = Tensor::random([32, 32], Distribution::Default, &amp;amp;device);
    let y: Tensor&amp;lt;Backend, 2&amp;gt; = Tensor::random([32, 32], Distribution::Default, &amp;amp;device).require_grad();

    let tmp = x.clone() + y.clone();
    let tmp = tmp.matmul(x);
    let tmp = tmp.exp();

    let grads = tmp.backward();
    let y_grad = y.grad(&amp;amp;grads).unwrap();
    println!("{y_grad}");
}
&lt;/code&gt;&lt;/pre&gt; 
  &lt;p&gt;Of note, it is impossible to make the mistake of calling backward on a model that runs on a backend that does not support autodiff (for inference), as this method is only offered by an Autodiff backend.&lt;/p&gt; 
  &lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/crates/burn-autodiff/README.md"&gt;Autodiff Backend README&lt;/a&gt; for more details.&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Fusion: Backend decorator that brings kernel fusion to all first-party backends &lt;/summary&gt; 
  &lt;br /&gt; 
  &lt;p&gt;This backend decorator enhances a backend with kernel fusion, provided that the inner backend supports it. Note that you can compose this backend with other backend decorators such as Autodiff. All first-party accelerated backends (like WGPU and CUDA) use Fusion by default (&lt;code&gt;burn/fusion&lt;/code&gt; feature flag), so you typically don't need to apply it manually.&lt;/p&gt; 
  &lt;pre&gt;&lt;code class="language-rust"&gt;#[cfg(not(feature = "fusion"))]
pub type Cuda&amp;lt;F = f32, I = i32&amp;gt; = CubeBackend&amp;lt;CudaRuntime, F, I, u8&amp;gt;;

#[cfg(feature = "fusion")]
pub type Cuda&amp;lt;F = f32, I = i32&amp;gt; = burn_fusion::Fusion&amp;lt;CubeBackend&amp;lt;CudaRuntime, F, I, u8&amp;gt;&amp;gt;;
&lt;/code&gt;&lt;/pre&gt; 
  &lt;p&gt;Of note, we plan to implement automatic gradient checkpointing based on compute bound and memory bound operations, which will work gracefully with the fusion backend to make your code run even faster during training, see &lt;a href="https://github.com/tracel-ai/burn/issues/936"&gt;this issue&lt;/a&gt;.&lt;/p&gt; 
  &lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/crates/burn-fusion/README.md"&gt;Fusion Backend README&lt;/a&gt; for more details.&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Router (Beta): Backend decorator that composes multiple backends into a single one &lt;/summary&gt; 
  &lt;br /&gt; 
  &lt;p&gt;That backend simplifies hardware operability, if for instance you want to execute some operations on the CPU and other operations on the GPU.&lt;/p&gt; 
  &lt;pre&gt;&lt;code class="language-rust"&gt;use burn::tensor::{Distribution, Tensor};
use burn::backend::{
    NdArray, Router, Wgpu, ndarray::NdArrayDevice, router::duo::MultiDevice, wgpu::WgpuDevice,
};

fn main() {
    type Backend = Router&amp;lt;(Wgpu, NdArray)&amp;gt;;

    let device_0 = MultiDevice::B1(WgpuDevice::DiscreteGpu(0));
    let device_1 = MultiDevice::B2(NdArrayDevice::Cpu);

    let tensor_gpu =
        Tensor::&amp;lt;Backend, 2&amp;gt;::random([3, 3], burn::tensor::Distribution::Default, &amp;amp;device_0);
    let tensor_cpu =
        Tensor::&amp;lt;Backend, 2&amp;gt;::random([3, 3], burn::tensor::Distribution::Default, &amp;amp;device_1);
}

&lt;/code&gt;&lt;/pre&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Remote (Beta): Backend decorator for remote backend execution, useful for distributed computations &lt;/summary&gt; 
  &lt;br /&gt; 
  &lt;p&gt;That backend has two parts, one client and one server. The client sends tensor operations over the network to a remote compute backend. You can use any first-party backend as server in a single line of code:&lt;/p&gt; 
  &lt;pre&gt;&lt;code class="language-rust"&gt;fn main_server() {
    // Start a server on port 3000.
    burn::server::start::&amp;lt;burn::backend::Cuda&amp;gt;(Default::default(), 3000);
}

fn main_client() {
    // Create a client that communicate with the server on port 3000.
    use burn::backend::{Autodiff, RemoteBackend};

    type Backend = Autodiff&amp;lt;RemoteDevice&amp;gt;;

    let device = RemoteDevice::new("ws://localhost:3000");
    let tensor_gpu =
        Tensor::&amp;lt;Backend, 2&amp;gt;::random([3, 3], Distribution::Default, &amp;amp;device);
}

&lt;/code&gt;&lt;/pre&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;h2&gt;Training &amp;amp; Inference&lt;/h2&gt; 
 &lt;div align="left"&gt; 
  &lt;img align="right" src="https://raw.githubusercontent.com/tracel-ai/burn/main/assets/ember-wall.png" height="96px" /&gt; 
  &lt;p&gt;The whole deep learning workflow is made easy with Burn, as you can monitor your training progress with an ergonomic dashboard, and run inference everywhere from embedded devices to large GPU clusters.&lt;/p&gt; 
  &lt;p&gt;Burn was built from the ground up with training and inference in mind. It's also worth noting how Burn, in comparison to frameworks like PyTorch, simplifies the transition from training to deployment, eliminating the need for code changes.&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;div align="center"&gt; 
  &lt;br /&gt; 
  &lt;a href="https://www.youtube.com/watch?v=N9RM5CQbNQc" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/tracel-ai/burn/main/assets/burn-train-tui.png" alt="Burn Train TUI" width="75%" /&gt; &lt;/a&gt; 
 &lt;/div&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;Click on the following sections to expand 👇&lt;/strong&gt;&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Training Dashboard 📈 &lt;/summary&gt; 
  &lt;br /&gt; 
  &lt;p&gt;As you can see in the previous video (click on the picture!), a new terminal UI dashboard based on the &lt;a href="https://github.com/ratatui-org/ratatui"&gt;Ratatui&lt;/a&gt; crate allows users to follow their training with ease without having to connect to any external application.&lt;/p&gt; 
  &lt;p&gt;You can visualize your training and validation metrics updating in real-time and analyze the lifelong progression or recent history of any registered metrics using only the arrow keys. Break from the training loop without crashing, allowing potential checkpoints to be fully written or important pieces of code to complete without interruption 🛡&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; ONNX Support 🐫 &lt;/summary&gt; 
  &lt;br /&gt; 
  &lt;p&gt;ONNX (Open Neural Network Exchange) is an open-standard format that exports both the architecture and the weights of a deep learning model.&lt;/p&gt; 
  &lt;p&gt;Burn supports the importation of models that follow the ONNX standard so you can easily port a model you have written in another framework like TensorFlow or PyTorch to Burn to benefit from all the advantages our framework offers.&lt;/p&gt; 
  &lt;p&gt;Our ONNX support is further described in &lt;a href="https://burn.dev/books/burn/import/onnx-model.html"&gt;this section of the Burn Book 🔥&lt;/a&gt;.&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: This crate is in active development and currently supports a &lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/crates/burn-import/SUPPORTED-ONNX-OPS.md"&gt;limited set of ONNX operators&lt;/a&gt;.&lt;/p&gt; 
  &lt;/blockquote&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Importing PyTorch or Safetensors Models 🚚 &lt;/summary&gt; 
  &lt;br /&gt; 
  &lt;p&gt;You can load weights from PyTorch or Safetensors formats directly into your Burn-defined models. This makes it easy to reuse existing models while benefiting from Burn's performance and deployment features.&lt;/p&gt; 
  &lt;p&gt;Learn more:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://burn.dev/books/burn/import/pytorch-model.html"&gt;Import pre-trained PyTorch models into Burn&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://burn.dev/books/burn/import/safetensors-model.html"&gt;Load models from Safetensors format&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Inference in the Browser 🌐 &lt;/summary&gt; 
  &lt;br /&gt; 
  &lt;p&gt;Several of our backends can run in WebAssembly environments: Candle and NdArray for CPU execution, and WGPU for GPU acceleration via WebGPU. This means that you can run inference directly within a browser. We provide several examples of this:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/mnist-inference-web"&gt;MNIST&lt;/a&gt; where you can draw digits and a small convnet tries to find which one it is! 2️⃣ 7️⃣ 😰&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/image-classification-web"&gt;Image Classification&lt;/a&gt; where you can upload images and classify them! 🌄&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Embedded: &lt;i&gt;no_std&lt;/i&gt; support ⚙️ &lt;/summary&gt; 
  &lt;br /&gt; 
  &lt;p&gt;Burn's core components support &lt;a href="https://docs.rust-embedded.org/book/intro/no-std.html"&gt;no_std&lt;/a&gt;. This means it can run in bare metal environment such as embedded devices without an operating system.&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;As of now, only the NdArray backend can be used in a &lt;em&gt;no_std&lt;/em&gt; environment.&lt;/p&gt; 
  &lt;/blockquote&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;h3&gt;Benchmarks&lt;/h3&gt; 
 &lt;p&gt;To evaluate performance across different backends and track improvements over time, we provide a dedicated benchmarking suite.&lt;/p&gt; 
 &lt;p&gt;Run and compare benchmarks using &lt;a href="https://github.com/tracel-ai/burn-bench"&gt;burn-bench&lt;/a&gt;.&lt;/p&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;⚠️ &lt;strong&gt;Warning&lt;/strong&gt; When using one of the &lt;code&gt;wgpu&lt;/code&gt; backends, you may encounter compilation errors related to recursive type evaluation. This is due to complex type nesting within the &lt;code&gt;wgpu&lt;/code&gt; dependency chain. To resolve this issue, add the following line at the top of your &lt;code&gt;main.rs&lt;/code&gt; or &lt;code&gt;lib.rs&lt;/code&gt; file:&lt;/p&gt; 
  &lt;pre&gt;&lt;code class="language-rust"&gt;#![recursion_limit = "256"]
&lt;/code&gt;&lt;/pre&gt; 
  &lt;p&gt;The default recursion limit (128) is often just below the required depth (typically 130-150) due to deeply nested associated types and trait bounds.&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;h2&gt;Getting Started&lt;/h2&gt; 
 &lt;div align="left"&gt; 
  &lt;img align="right" src="https://raw.githubusercontent.com/tracel-ai/burn/main/assets/ember-walking.png" height="96px" /&gt; 
  &lt;p&gt;Just heard of Burn? You are at the right place! Just continue reading this section and we hope you can get on board really quickly.&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;details&gt; 
  &lt;summary&gt; The Burn Book 🔥 &lt;/summary&gt; 
  &lt;br /&gt; 
  &lt;p&gt;To begin working effectively with Burn, it is crucial to understand its key components and philosophy. This is why we highly recommend new users to read the first sections of &lt;a href="https://burn.dev/books/burn/"&gt;The Burn Book 🔥&lt;/a&gt;. It provides detailed examples and explanations covering every facet of the framework, including building blocks like tensors, modules, and optimizers, all the way to advanced usage, like coding your own GPU kernels.&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;The project is constantly evolving, and we try as much as possible to keep the book up to date with new additions. However, we might miss some details sometimes, so if you see something weird, let us know! We also gladly accept Pull Requests 😄&lt;/p&gt; 
  &lt;/blockquote&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Examples 🙏 &lt;/summary&gt; 
  &lt;br /&gt; 
  &lt;p&gt;Let's start with a code snippet that shows how intuitive the framework is to use! In the following, we declare a neural network module with some parameters along with its forward pass.&lt;/p&gt; 
  &lt;pre&gt;&lt;code class="language-rust"&gt;use burn::nn;
use burn::module::Module;
use burn::tensor::backend::Backend;

#[derive(Module, Debug)]
pub struct PositionWiseFeedForward&amp;lt;B: Backend&amp;gt; {
    linear_inner: nn::Linear&amp;lt;B&amp;gt;,
    linear_outer: nn::Linear&amp;lt;B&amp;gt;,
    dropout: nn::Dropout,
    gelu: nn::Gelu,
}

impl&amp;lt;B: Backend&amp;gt; PositionWiseFeedForward&amp;lt;B&amp;gt; {
    pub fn forward&amp;lt;const D: usize&amp;gt;(&amp;amp;self, input: Tensor&amp;lt;B, D&amp;gt;) -&amp;gt; Tensor&amp;lt;B, D&amp;gt; {
        let x = self.linear_inner.forward(input);
        let x = self.gelu.forward(x);
        let x = self.dropout.forward(x);

        self.linear_outer.forward(x)
    }
}
&lt;/code&gt;&lt;/pre&gt; 
  &lt;p&gt;We have a somewhat large amount of &lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples"&gt;examples&lt;/a&gt; in the repository that shows how to use the framework in different scenarios.&lt;/p&gt; 
  &lt;p&gt;Following &lt;a href="https://burn.dev/books/burn/"&gt;the book&lt;/a&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/guide"&gt;Basic Workflow&lt;/a&gt; : Creates a custom CNN &lt;code&gt;Module&lt;/code&gt; to train on the MNIST dataset and use for inference.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/custom-training-loop"&gt;Custom Training Loop&lt;/a&gt; : Implements a basic training loop instead of using the &lt;code&gt;Learner&lt;/code&gt;.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/custom-wgpu-kernel"&gt;Custom WGPU Kernel&lt;/a&gt; : Learn how to create your own custom operation with the WGPU backend.&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;p&gt;Additional examples:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/custom-csv-dataset"&gt;Custom CSV Dataset&lt;/a&gt; : Implements a dataset to parse CSV data for a regression task.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/simple-regression"&gt;Regression&lt;/a&gt; : Trains a simple MLP on the California Housing dataset to predict the median house value for a district.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/custom-image-dataset"&gt;Custom Image Dataset&lt;/a&gt; : Trains a simple CNN on custom image dataset following a simple folder structure.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/custom-renderer"&gt;Custom Renderer&lt;/a&gt; : Implements a custom renderer to display the &lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/building-blocks/learner.md"&gt;&lt;code&gt;Learner&lt;/code&gt;&lt;/a&gt; progress.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/image-classification-web"&gt;Image Classification Web&lt;/a&gt; : Image classification web browser demo using Burn, WGPU and WebAssembly.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/mnist-inference-web"&gt;MNIST Inference on Web&lt;/a&gt; : An interactive MNIST inference demo in the browser. The demo is available &lt;a href="https://burn.dev/demo/"&gt;online&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/mnist"&gt;MNIST Training&lt;/a&gt; : Demonstrates how to train a custom &lt;code&gt;Module&lt;/code&gt; (MLP) with the &lt;code&gt;Learner&lt;/code&gt; configured to log metrics and keep training checkpoints.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/named-tensor"&gt;Named Tensor&lt;/a&gt; : Performs operations with the experimental &lt;code&gt;NamedTensor&lt;/code&gt; feature.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/onnx-inference"&gt;ONNX Import Inference&lt;/a&gt; : Imports an ONNX model pre-trained on MNIST to perform inference on a sample image with Burn.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/import-model-weights"&gt;PyTorch Import Inference&lt;/a&gt; : Imports a PyTorch model pre-trained on MNIST to perform inference on a sample image with Burn.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/text-classification"&gt;Text Classification&lt;/a&gt; : Trains a text classification transformer model on the AG News or DbPedia dataset. The trained model can then be used to classify a text sample.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/text-generation"&gt;Text Generation&lt;/a&gt; : Trains a text generation transformer model on the DbPedia dataset.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/examples/wgan"&gt;Wasserstein GAN MNIST&lt;/a&gt; : Trains a WGAN model to generate new handwritten digits based on MNIST.&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;p&gt;For more practical insights, you can clone the repository and run any of them directly on your computer!&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Pre-trained Models 🤖 &lt;/summary&gt; 
  &lt;br /&gt; 
  &lt;p&gt;We keep an updated and curated list of models and examples built with Burn, see the &lt;a href="https://github.com/tracel-ai/models"&gt;tracel-ai/models repository&lt;/a&gt; for more details.&lt;/p&gt; 
  &lt;p&gt;Don't see the model you want? Don't hesitate to open an issue, and we may prioritize it. Built a model using Burn and want to share it? You can also open a Pull Request and add your model under the community section!&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; Why use Rust for Deep Learning? 🦀 &lt;/summary&gt; 
  &lt;br /&gt; 
  &lt;p&gt;Deep Learning is a special form of software where you need very high level abstractions as well as extremely fast execution time. Rust is the perfect candidate for that use case since it provides zero-cost abstractions to easily create neural network modules, and fine-grained control over memory to optimize every detail.&lt;/p&gt; 
  &lt;p&gt;It's important that a framework be easy to use at a high level so that its users can focus on innovating in the AI field. However, since running models relies so heavily on computations, performance can't be neglected.&lt;/p&gt; 
  &lt;p&gt;To this day, the mainstream solution to this problem has been to offer APIs in Python, but rely on bindings to low-level languages such as C/C++. This reduces portability, increases complexity and creates frictions between researchers and engineers. We feel like Rust's approach to abstractions makes it versatile enough to tackle this two languages dichotomy.&lt;/p&gt; 
  &lt;p&gt;Rust also comes with the Cargo package manager, which makes it incredibly easy to build, test, and deploy from any environment, which is usually a pain in Python.&lt;/p&gt; 
  &lt;p&gt;Although Rust has the reputation of being a difficult language at first, we strongly believe it leads to more reliable, bug-free solutions built faster (after some practice 😅)!&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;strong&gt;Deprecation Note&lt;/strong&gt;&lt;br /&gt;Since &lt;code&gt;0.14.0&lt;/code&gt;, the internal structure for tensor data has changed. The previous &lt;code&gt;Data&lt;/code&gt; struct was deprecated and officially removed since &lt;code&gt;0.17.0&lt;/code&gt; in favor of the new &lt;code&gt;TensorData&lt;/code&gt; struct, which allows for more flexibility by storing the underlying data as bytes and keeping the data type as a field. If you are using &lt;code&gt;Data&lt;/code&gt; in your code, make sure to switch to &lt;code&gt;TensorData&lt;/code&gt;.&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;!-- &gt;
&gt; In the event that you are trying to load a model record saved in a previous version, make sure to
&gt; enable the `record-backward-compat` feature using a previous version of burn (&lt;=0.16.0). Otherwise,
&gt; the record won't be deserialized correctly and you will get an error message (which will also point
&gt; you to the backward compatible feature flag). The backward compatibility was maintained for
&gt; deserialization (loading), so as soon as you have saved the record again it will be saved according
&gt; to the new structure and you will be able to upgrade to this version. Please note that binary formats
&gt; are not backward compatible. Thus, you will need to load your record in a previous version and save it
&gt; to another of the self-describing record formats before using a compatible version (as described) with the
&gt; `record-backward-compat` feature flag. --&gt; 
 &lt;details id="deprecation"&gt; 
  &lt;summary&gt; Loading Model Records From Previous Versions ⚠️ &lt;/summary&gt; 
  &lt;br /&gt; 
  &lt;p&gt;In the event that you are trying to load a model record saved in a version older than &lt;code&gt;0.14.0&lt;/code&gt;, make sure to use a compatible version (&lt;code&gt;0.14&lt;/code&gt;, &lt;code&gt;0.15&lt;/code&gt; or &lt;code&gt;0.16&lt;/code&gt;) with the &lt;code&gt;record-backward-compat&lt;/code&gt; feature flag.&lt;/p&gt; 
  &lt;pre&gt;&lt;code&gt;features = [..., "record-backward-compat"]
&lt;/code&gt;&lt;/pre&gt; 
  &lt;p&gt;Otherwise, the record won't be deserialized correctly and you will get an error message. This error will also point you to the backward compatible feature flag.&lt;/p&gt; 
  &lt;p&gt;The backward compatibility was maintained for deserialization when loading records. Therefore, as soon as you have saved the record again it will be saved according to the new structure and you can upgrade back to the current version&lt;/p&gt; 
  &lt;p&gt;Please note that binary formats are not backward compatible. Thus, you will need to load your record in a previous version and save it in any of the other self-describing record format (e.g., using the &lt;code&gt;NamedMpkFileRecorder&lt;/code&gt;) before using a compatible version (as described) with the &lt;code&gt;record-backward-compat&lt;/code&gt; feature flag.&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;h2&gt;Community&lt;/h2&gt; 
 &lt;div align="left"&gt; 
  &lt;img align="right" src="https://raw.githubusercontent.com/tracel-ai/burn/main/assets/ember-community.png" height="96px" /&gt; 
  &lt;p&gt;If you are excited about the project, don't hesitate to join our &lt;a href="https://discord.gg/uPEBbYYDB6"&gt;Discord&lt;/a&gt;! We try to be as welcoming as possible to everybody from any background. You can ask your questions and share what you built with the community!&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;Contributing&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Before contributing, please take a moment to review our &lt;a href="https://github.com/tracel-ai/burn/tree/main/CODE-OF-CONDUCT.md"&gt;code of conduct&lt;/a&gt;. It's also highly recommended to read the &lt;a href="https://github.com/tracel-ai/burn/tree/main/contributor-book/src/project-architecture"&gt;architecture overview&lt;/a&gt;, which explains some of our architectural decisions. Refer to our &lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/CONTRIBUTING.md"&gt;contributing guide&lt;/a&gt; for more details.&lt;/p&gt; 
 &lt;h2&gt;Status&lt;/h2&gt; 
 &lt;p&gt;Burn is currently in active development, and there will be breaking changes. While any resulting issues are likely to be easy to fix, there are no guarantees at this stage.&lt;/p&gt; 
 &lt;h2&gt;License&lt;/h2&gt; 
 &lt;p&gt;Burn is distributed under the terms of both the MIT license and the Apache License (Version 2.0). See &lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/LICENSE-APACHE"&gt;LICENSE-APACHE&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/tracel-ai/burn/main/LICENSE-MIT"&gt;LICENSE-MIT&lt;/a&gt; for details. Opening a pull request is assumed to signal agreement with these licensing terms.&lt;/p&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>tensorzero/tensorzero</title>
      <link>https://github.com/tensorzero/tensorzero</link>
      <description>&lt;p&gt;TensorZero is an open-source stack for industrial-grade LLM applications. It unifies an LLM gateway, observability, optimization, evaluation, and experimentation.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;
 &lt;picture&gt;
  &lt;img src="https://github.com/user-attachments/assets/47d67430-386d-4675-82ad-d4734d3262d9" alt="TensorZero Logo" width="128" height="128" /&gt;
 &lt;/picture&gt;&lt;/p&gt; 
&lt;h1&gt;TensorZero&lt;/h1&gt; 
&lt;p&gt;
 &lt;picture&gt;
  &lt;img src="https://www.tensorzero.com/github-trending-badge.svg?sanitize=true" alt="#1 Repository Of The Day" /&gt;
 &lt;/picture&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;TensorZero is an open-source stack for &lt;em&gt;industrial-grade LLM applications&lt;/em&gt;:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Gateway:&lt;/strong&gt; access every LLM provider through a unified API, built for performance (&amp;lt;1ms p99 latency)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Observability:&lt;/strong&gt; store inferences and feedback in your database, available programmatically or in the UI&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Optimization:&lt;/strong&gt; collect metrics and human feedback to optimize prompts, models, and inference strategies&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Evaluation:&lt;/strong&gt; benchmark individual inferences or end-to-end workflows using heuristics, LLM judges, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Experimentation:&lt;/strong&gt; ship with confidence with built-in A/B testing, routing, fallbacks, retries, etc.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Take what you need, adopt incrementally, and complement with other tools.&lt;/p&gt; 
&lt;p&gt;
 &lt;video src="https://github.com/user-attachments/assets/04a8466e-27d8-4189-b305-e7cecb6881ee"&gt;&lt;/video&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p align="center"&gt; &lt;b&gt;&lt;a href="https://www.tensorzero.com/" target="_blank"&gt;Website&lt;/a&gt;&lt;/b&gt; · &lt;b&gt;&lt;a href="https://www.tensorzero.com/docs" target="_blank"&gt;Docs&lt;/a&gt;&lt;/b&gt; · &lt;b&gt;&lt;a href="https://www.x.com/tensorzero" target="_blank"&gt;Twitter&lt;/a&gt;&lt;/b&gt; · &lt;b&gt;&lt;a href="https://www.tensorzero.com/slack" target="_blank"&gt;Slack&lt;/a&gt;&lt;/b&gt; · &lt;b&gt;&lt;a href="https://www.tensorzero.com/discord" target="_blank"&gt;Discord&lt;/a&gt;&lt;/b&gt; &lt;br /&gt; &lt;br /&gt; &lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/quickstart" target="_blank"&gt;Quick Start (5min)&lt;/a&gt;&lt;/b&gt; · &lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/deployment" target="_blank"&gt;Deployment Guide&lt;/a&gt;&lt;/b&gt; · &lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/api-reference" target="_blank"&gt;API Reference&lt;/a&gt;&lt;/b&gt; · &lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/deployment" target="_blank"&gt;Configuration Reference&lt;/a&gt;&lt;/b&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td width="30%" valign="top"&gt;&lt;b&gt;What is TensorZero?&lt;/b&gt;&lt;/td&gt; 
   &lt;td width="70%" valign="top"&gt;TensorZero is an open-source stack for industrial-grade LLM applications. It unifies an LLM gateway, observability, optimization, evaluation, and experimentation.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="30%" valign="top"&gt;&lt;b&gt;How is TensorZero different from other LLM frameworks?&lt;/b&gt;&lt;/td&gt; 
   &lt;td width="70%" valign="top"&gt; 1. TensorZero enables you to optimize complex LLM applications based on production metrics and human feedback.&lt;br /&gt; 2. TensorZero supports the needs of industrial-grade LLM applications: low latency, high throughput, type safety, self-hosted, GitOps, customizability, etc.&lt;br /&gt; 3. TensorZero unifies the entire LLMOps stack, creating compounding benefits. For example, LLM evaluations can be used for fine-tuning models alongside AI judges. &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="30%" valign="top"&gt;&lt;b&gt;Can I use TensorZero with ___?&lt;/b&gt;&lt;/td&gt; 
   &lt;td width="70%" valign="top"&gt;Yes. Every major programming language is supported. You can use TensorZero with our Python client, any OpenAI SDK or OpenAI-compatible client, or our HTTP API.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="30%" valign="top"&gt;&lt;b&gt;Is TensorZero production-ready?&lt;/b&gt;&lt;/td&gt; 
   &lt;td width="70%" valign="top"&gt;Yes. Here's a case study: &lt;b&gt;&lt;a href="https://www.tensorzero.com/blog/case-study-automating-code-changelogs-at-a-large-bank-with-llms"&gt;Automating Code Changelogs at a Large Bank with LLMs&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="30%" valign="top"&gt;&lt;b&gt;How much does TensorZero cost?&lt;/b&gt;&lt;/td&gt; 
   &lt;td width="70%" valign="top"&gt;Nothing. TensorZero is 100% self-hosted and open-source. There are no paid features.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="30%" valign="top"&gt;&lt;b&gt;Who is building TensorZero?&lt;/b&gt;&lt;/td&gt; 
   &lt;td width="70%" valign="top"&gt;Our technical team includes a former Rust compiler maintainer, machine learning researchers (Stanford, CMU, Oxford, Columbia) with thousands of citations, and the chief product officer of a decacorn startup. We're backed by the same investors as leading open-source projects (e.g. ClickHouse, CockroachDB) and AI labs (e.g. OpenAI, Anthropic). See our &lt;b&gt;&lt;a href="https://www.tensorzero.com/blog/tensorzero-raises-7-3m-seed-round-to-build-an-open-source-stack-for-industrial-grade-llm-applications/"&gt;$7.3M seed round announcement&lt;/a&gt;&lt;/b&gt; and &lt;b&gt;&lt;a href="https://venturebeat.com/ai/tensorzero-nabs-7-3m-seed-to-solve-the-messy-world-of-enterprise-llm-development/" rel="nofollow" target="_blank"&gt;coverage from VentureBeat&lt;/a&gt;&lt;/b&gt;. We're &lt;b&gt;&lt;a href="https://www.tensorzero.com/jobs" rel="nofollow" target="_blank"&gt;hiring in NYC&lt;/a&gt;&lt;/b&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="30%" valign="top"&gt;&lt;b&gt;How do I get started?&lt;/b&gt;&lt;/td&gt; 
   &lt;td width="70%" valign="top"&gt;You can adopt TensorZero incrementally. Our &lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/quickstart"&gt;Quick Start&lt;/a&gt;&lt;/b&gt; goes from a vanilla OpenAI wrapper to a production-ready LLM application with observability and fine-tuning in just 5 minutes.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;h3&gt;🌐 LLM Gateway&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Integrate with TensorZero once and access every major LLM provider.&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Access every major LLM provider (API or self-hosted) through a single unified API&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Infer with streaming, tool use, structured generation (JSON mode), batch, embeddings, multimodal (VLMs), file inputs, caching, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Define prompt templates and schemas to enforce a consistent, typed interface between your application and the LLMs&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Satisfy extreme throughput and latency needs, thanks to 🦀 Rust: &amp;lt;1ms p99 latency overhead at 10k+ QPS&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Integrate using our Python client, any OpenAI SDK or OpenAI-compatible client, or our HTTP API (use any programming language)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Ensure high availability with routing, retries, fallbacks, load balancing, granular timeouts, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Soon: rate limits, spend tracking and budgeting, service accounts&lt;/li&gt; 
&lt;/ul&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt;&lt;/tr&gt; 
  &lt;!-- flip highlight order --&gt; 
  &lt;tr&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;b&gt;Model Providers&lt;/b&gt;&lt;/td&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;b&gt;Features&lt;/b&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%" align="left" valign="top"&gt; &lt;p&gt; The TensorZero Gateway natively supports: &lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/anthropic"&gt;Anthropic&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/aws-bedrock"&gt;AWS Bedrock&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/aws-sagemaker"&gt;AWS SageMaker&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/azure"&gt;Azure OpenAI Service&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/deepseek"&gt;DeepSeek&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/fireworks"&gt;Fireworks&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/gcp-vertex-ai-anthropic"&gt;GCP Vertex AI Anthropic&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/gcp-vertex-ai-gemini"&gt;GCP Vertex AI Gemini&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/google-ai-studio-gemini"&gt;Google AI Studio (Gemini API)&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/groq"&gt;Groq&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/hyperbolic"&gt;Hyperbolic&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/mistral"&gt;Mistral&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/openai"&gt;OpenAI&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/openrouter"&gt;OpenRouter&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/sglang"&gt;SGLang&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/tgi"&gt;TGI&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/together"&gt;Together AI&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/vllm"&gt;vLLM&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/xai"&gt;xAI (Grok)&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;p&gt; &lt;em&gt; Need something else? Your provider is most likely supported because TensorZero integrates with &lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/openai-compatible"&gt;any OpenAI-compatible API (e.g. Ollama)&lt;/a&gt;&lt;/b&gt;. &lt;/em&gt; &lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%" align="left" valign="top"&gt; &lt;p&gt; The TensorZero Gateway supports advanced features like: &lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/retries-fallbacks"&gt;Retries &amp;amp; Fallbacks&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations"&gt;Inference-Time Optimizations&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/prompt-templates-schemas"&gt;Prompt Templates &amp;amp; Schemas&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/experimentation/"&gt;Experimentation (A/B Testing)&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/configuration-reference"&gt;Configuration-as-Code (GitOps)&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/batch-inference"&gt;Batch Inference&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/multimodal-inference"&gt;Multimodal Inference (VLMs)&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/inference-caching"&gt;Inference Caching&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/metrics-feedback"&gt;Metrics &amp;amp; Feedback&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/episodes"&gt;Multi-Step LLM Workflows (Episodes)&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;em&gt;&amp;amp; a lot more...&lt;/em&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;p&gt; The TensorZero Gateway is written in Rust 🦀 with &lt;b&gt;performance&lt;/b&gt; in mind (&amp;lt;1ms p99 latency overhead @ 10k QPS). See &lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/benchmarks"&gt;Benchmarks&lt;/a&gt;&lt;/b&gt;.&lt;br /&gt; &lt;/p&gt; &lt;p&gt; You can run inference using the &lt;b&gt;TensorZero client&lt;/b&gt; (recommended), the &lt;b&gt;OpenAI client&lt;/b&gt;, or the &lt;b&gt;HTTP API&lt;/b&gt;. &lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;br /&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;b&gt;Usage: Python — TensorZero Client (Recommended)&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;You can access any provider using the TensorZero Python client.&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;code&gt;pip install tensorzero&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;Optional: Set up the TensorZero configuration.&lt;/li&gt; 
  &lt;li&gt;Run inference:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from tensorzero import TensorZeroGateway  # or AsyncTensorZeroGateway


with TensorZeroGateway.build_embedded(clickhouse_url="...", config_file="...") as client:
    response = client.inference(
        model_name="openai::gpt-4o-mini",
        # Try other providers easily: "anthropic::claude-3-7-sonnet-20250219"
        input={
            "messages": [
                {
                    "role": "user",
                    "content": "Write a haiku about artificial intelligence.",
                }
            ]
        },
    )
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;See &lt;strong&gt;&lt;a href="https://www.tensorzero.com/docs/quickstart"&gt;Quick Start&lt;/a&gt;&lt;/strong&gt; for more information.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Usage: Python — OpenAI Client&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;You can access any provider using the OpenAI Python client with TensorZero.&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;code&gt;pip install tensorzero&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;Optional: Set up the TensorZero configuration.&lt;/li&gt; 
  &lt;li&gt;Run inference:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from openai import OpenAI  # or AsyncOpenAI
from tensorzero import patch_openai_client

client = OpenAI()

patch_openai_client(
    client,
    clickhouse_url="http://chuser:chpassword@localhost:8123/tensorzero",
    config_file="config/tensorzero.toml",
    async_setup=False,
)

response = client.chat.completions.create(
    model="tensorzero::model_name::openai::gpt-4o-mini",
    # Try other providers easily: "tensorzero::model_name::anthropic::claude-3-7-sonnet-20250219"
    messages=[
        {
            "role": "user",
            "content": "Write a haiku about artificial intelligence.",
        }
    ],
)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;See &lt;strong&gt;&lt;a href="https://www.tensorzero.com/docs/quickstart"&gt;Quick Start&lt;/a&gt;&lt;/strong&gt; for more information.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Usage: JavaScript / TypeScript (Node) — OpenAI Client&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;You can access any provider using the OpenAI Node client with TensorZero.&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Deploy &lt;code&gt;tensorzero/gateway&lt;/code&gt; using Docker. &lt;strong&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/deployment"&gt;Detailed instructions →&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;li&gt;Set up the TensorZero configuration.&lt;/li&gt; 
  &lt;li&gt;Run inference:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-ts"&gt;import OpenAI from "openai";

const client = new OpenAI({
  baseURL: "http://localhost:3000/openai/v1",
});

const response = await client.chat.completions.create({
  model: "tensorzero::model_name::openai::gpt-4o-mini",
  // Try other providers easily: "tensorzero::model_name::anthropic::claude-3-7-sonnet-20250219"
  messages: [
    {
      role: "user",
      content: "Write a haiku about artificial intelligence.",
    },
  ],
});
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;See &lt;strong&gt;&lt;a href="https://www.tensorzero.com/docs/quickstart"&gt;Quick Start&lt;/a&gt;&lt;/strong&gt; for more information.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Usage: Other Languages &amp;amp; Platforms — HTTP API&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;TensorZero supports virtually any programming language or platform via its HTTP API.&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Deploy &lt;code&gt;tensorzero/gateway&lt;/code&gt; using Docker. &lt;strong&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/deployment"&gt;Detailed instructions →&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;li&gt;Optional: Set up the TensorZero configuration.&lt;/li&gt; 
  &lt;li&gt;Run inference:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;curl -X POST "http://localhost:3000/inference" \
  -H "Content-Type: application/json" \
  -d '{
    "model_name": "openai::gpt-4o-mini",
    "input": {
      "messages": [
        {
          "role": "user",
          "content": "Write a haiku about artificial intelligence."
        }
      ]
    }
  }'
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;See &lt;strong&gt;&lt;a href="https://www.tensorzero.com/docs/quickstart"&gt;Quick Start&lt;/a&gt;&lt;/strong&gt; for more information.&lt;/p&gt; 
&lt;/details&gt; 
&lt;br /&gt; 
&lt;h3&gt;🔍 LLM Observability&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Zoom in to debug individual API calls, or zoom out to monitor metrics across models and prompts over time — all using the open-source TensorZero UI.&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Store inferences and feedback (metrics, human edits, etc.) in your own database&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Dive into individual inferences or high-level aggregate patterns using the TensorZero UI or programmatically&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Build datasets for optimization, evaluation, and other workflows&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Replay historical inferences with new prompts, models, inference strategies, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Export OpenTelemetry (OTLP) traces to your favorite general-purpose observability tool&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Soon: AI-assisted debugging and root cause analysis; AI-assisted data labeling&lt;/li&gt; 
&lt;/ul&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt;&lt;/tr&gt; 
  &lt;!-- flip highlight order --&gt; 
  &lt;tr&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;b&gt;Observability » UI&lt;/b&gt;&lt;/td&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;b&gt;Observability » Programmatic&lt;/b&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;
    &lt;video src="https://github.com/user-attachments/assets/a23e4c95-18fa-482c-8423-6078fb4cf285"&gt;&lt;/video&gt;&lt;/td&gt; 
   &lt;td width="50%" align="left" valign="middle"&gt; &lt;pre&gt;&lt;code class="language-python"&gt;t0.experimental_list_inferences(
  function_name="sales_agent",
  variant_name="qwen3-promptv2",
  filters=BooleanMetricFilter(
      metric_name="converted_sale",
      value=True,
  ),
  order_by=[OrderBy(by="timestamp", direction="DESC")],
  limit=100_000,
  # ... and more ...
)
&lt;/code&gt;&lt;/pre&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;br /&gt; 
&lt;h3&gt;📈 LLM Optimization&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Send production metrics and human feedback to easily optimize your prompts, models, and inference strategies — using the UI or programmatically.&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Optimize your models with supervised fine-tuning, RLHF, and other techniques&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Optimize your prompts with automated prompt engineering algorithms like MIPROv2&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Optimize your inference strategy with dynamic in-context learning, chain of thought, best/mixture-of-N sampling, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Enable a feedback loop for your LLMs: a data &amp;amp; learning flywheel turning production data into smarter, faster, and cheaper models&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Soon: synthetic data generation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Model Optimization&lt;/h4&gt; 
&lt;p&gt;Optimize closed-source and open-source models using supervised fine-tuning (SFT) and preference fine-tuning (DPO).&lt;/p&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt;&lt;/tr&gt; 
  &lt;!-- flip highlight order --&gt; 
  &lt;tr&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;b&gt;Supervised Fine-tuning — UI&lt;/b&gt;&lt;/td&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;b&gt;Preference Fine-tuning (DPO) — Jupyter Notebook&lt;/b&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;
    &lt;video src="https://github.com/user-attachments/assets/82f76be7-5e02-4ada-b503-69dfa209a442"&gt;&lt;/video&gt;&lt;/td&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;img src="https://github.com/user-attachments/assets/a67a0634-04a7-42b0-b934-9130cb7cdf51" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h4&gt;Inference-Time Optimization&lt;/h4&gt; 
&lt;p&gt;Boost performance by dynamically updating your prompts with relevant examples, combining responses from multiple inferences, and more.&lt;/p&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt;&lt;/tr&gt; 
  &lt;!-- flip highlight order --&gt; 
  &lt;tr&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#best-of-n-sampling"&gt;Best-of-N Sampling&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#mixture-of-n-sampling"&gt;Mixture-of-N Sampling&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;img src="https://github.com/user-attachments/assets/c0edfa4c-713c-4996-9964-50c0d26e6970" /&gt;&lt;/td&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;img src="https://github.com/user-attachments/assets/75b5bf05-4c1f-43c4-b158-d69d1b8d05be" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#dynamic-in-context-learning-dicl"&gt;Dynamic In-Context Learning (DICL)&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#chain-of-thought-cot"&gt;Chain-of-Thought (CoT)&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;img src="https://github.com/user-attachments/assets/d8489e92-ce93-46ac-9aab-289ce19bb67d" /&gt;&lt;/td&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;img src="https://github.com/user-attachments/assets/ea13d73c-76a4-4e0c-a35b-0c648f898311" height="320" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;p&gt;&lt;em&gt;More coming soon...&lt;/em&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;h4&gt;Prompt Optimization&lt;/h4&gt; 
&lt;p&gt;Optimize your prompts programmatically using research-driven optimization techniques.&lt;/p&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt;&lt;/tr&gt; 
  &lt;!-- flip highlight order --&gt; 
  &lt;tr&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#best-of-n-sampling"&gt;MIPROv2&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;b&gt;&lt;a href="https://github.com/tensorzero/tensorzero/tree/main/examples/gsm8k-custom-recipe-dspy"&gt;DSPy Integration&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;img src="https://github.com/user-attachments/assets/d81a7c37-382f-4c46-840f-e6c2593301db" alt="MIPROv2 diagram" /&gt;&lt;/td&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt; TensorZero comes with several optimization recipes, but you can also easily create your own. This example shows how to optimize a TensorZero function using an arbitrary tool — here, DSPy, a popular library for automated prompt engineering. &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;p&gt;&lt;em&gt;More coming soon...&lt;/em&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;h3&gt;📊 LLM Evaluation&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Compare prompts, models, and inference strategies using evaluations powered by heuristics and LLM judges.&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Evaluate individual inferences with &lt;em&gt;static evaluations&lt;/em&gt; powered by heuristics or LLM judges (≈ unit tests for LLMs)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Evaluate end-to-end workflows with &lt;em&gt;dynamic evaluations&lt;/em&gt; with complete flexibility (≈ integration tests for LLMs)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Optimize LLM judges just like any other TensorZero function to align them to human preferences&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Soon: more built-in evaluators; headless evaluations&lt;/li&gt; 
&lt;/ul&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt;&lt;/tr&gt; 
  &lt;!-- flip highlight order --&gt; 
  &lt;tr&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;b&gt;Evaluation » UI&lt;/b&gt;&lt;/td&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;b&gt;Evaluation » CLI&lt;/b&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;img src="https://github.com/user-attachments/assets/f4bf54e3-1b63-46c8-be12-2eaabf615699" /&gt;&lt;/td&gt; 
   &lt;td width="50%" align="left" valign="middle"&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;docker compose run --rm evaluations \
  --evaluation-name extract_data \
  --dataset-name hard_test_cases \
  --variant-name gpt_4o \
  --concurrency 5&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;Run ID: 01961de9-c8a4-7c60-ab8d-15491a9708e4
Number of datapoints: 100
██████████████████████████████████████ 100/100
exact_match: 0.83 ± 0.03
semantic_match: 0.98 ± 0.01
item_count: 7.15 ± 0.39&lt;/code&gt;&lt;/pre&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;🧪 LLM Experimentation&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Ship with confidence with built-in A/B testing, routing, fallbacks, retries, etc.&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Ship with confidence with built-in A/B testing for models, prompts, providers, hyperparameters, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Enforce principled experiments (RCTs) in complex workflows, including multi-turn and compound LLM systems&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Soon: multi-armed bandits; AI-managed experiments&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&amp;amp; more!&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Build with an open-source stack well-suited for prototypes but designed from the ground up to support the most complex LLM applications and deployments.&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Build simple applications or massive deployments with GitOps-friendly orchestration&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Extend TensorZero with built-in escape hatches, programmatic-first usage, direct database access, and more&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Integrate with third-party tools: specialized observability and evaluations, model providers, agent orchestration frameworks, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Iterate quickly by experimenting with prompts interactively using the Playground UI&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Demo&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Watch LLMs get better at data extraction in real-time with TensorZero!&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#dynamic-in-context-learning-dicl"&gt;Dynamic in-context learning (DICL)&lt;/a&gt;&lt;/strong&gt; is a powerful inference-time optimization available out of the box with TensorZero. It enhances LLM performance by automatically incorporating relevant historical examples into the prompt, without the need for model fine-tuning.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/4df1022e-886e-48c2-8f79-6af3cdad79cb"&gt;https://github.com/user-attachments/assets/4df1022e-886e-48c2-8f79-6af3cdad79cb&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Get Started&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Start building today.&lt;/strong&gt; The &lt;strong&gt;&lt;a href="https://www.tensorzero.com/docs/quickstart"&gt;Quick Start&lt;/a&gt;&lt;/strong&gt; shows it's easy to set up an LLM application with TensorZero.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Questions?&lt;/strong&gt; Ask us on &lt;strong&gt;&lt;a href="https://www.tensorzero.com/slack"&gt;Slack&lt;/a&gt;&lt;/strong&gt; or &lt;strong&gt;&lt;a href="https://www.tensorzero.com/discord"&gt;Discord&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Using TensorZero at work?&lt;/strong&gt; Email us at &lt;strong&gt;&lt;a href="mailto:hello@tensorzero.com"&gt;hello@tensorzero.com&lt;/a&gt;&lt;/strong&gt; to set up a Slack or Teams channel with your team (free).&lt;/p&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;p&gt;We are working on a series of &lt;strong&gt;complete runnable examples&lt;/strong&gt; illustrating TensorZero's data &amp;amp; learning flywheel.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/tensorzero/tensorzero/tree/main/examples/data-extraction-ner"&gt;Optimizing Data Extraction (NER) with TensorZero&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;This example shows how to use TensorZero to optimize a data extraction pipeline. We demonstrate techniques like fine-tuning and dynamic in-context learning (DICL). In the end, an optimized GPT-4o Mini model outperforms GPT-4o on this task — at a fraction of the cost and latency — using a small amount of training data.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/tensorzero/tensorzero/tree/main/examples/rag-retrieval-augmented-generation/simple-agentic-rag/"&gt;Agentic RAG — Multi-Hop Question Answering with LLMs&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;This example shows how to build a multi-hop retrieval agent using TensorZero. The agent iteratively searches Wikipedia to gather information, and decides when it has enough context to answer a complex question.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/tensorzero/tensorzero/tree/main/examples/haiku-hidden-preferences"&gt;Writing Haikus to Satisfy a Judge with Hidden Preferences&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;This example fine-tunes GPT-4o Mini to generate haikus tailored to a specific taste. You'll see TensorZero's "data flywheel in a box" in action: better variants leads to better data, and better data leads to better variants. You'll see progress by fine-tuning the LLM multiple times.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/tensorzero/tensorzero/tree/main/examples/multimodal-vision-finetuning"&gt;Image Data Extraction — Multimodal (Vision) Fine-tuning&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;This example shows how to fine-tune multimodal models (VLMs) like GPT-4o to improve their performance on vision-language tasks. Specifically, we'll build a system that categorizes document images (screenshots of computer science research papers).&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/tensorzero/tensorzero/tree/main/examples/chess-puzzles/"&gt;Improving LLM Chess Ability with Best-of-N Sampling&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;This example showcases how best-of-N sampling can significantly enhance an LLM's chess-playing abilities by selecting the most promising moves from multiple generated options.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/tensorzero/tensorzero/tree/main/examples/gsm8k-custom-recipe-dspy"&gt;Improving Math Reasoning with a Custom Recipe for Automated Prompt Engineering (DSPy)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;TensorZero provides a number of pre-built optimization recipes covering common LLM engineering workflows. But you can also easily create your own recipes and workflows! This example shows how to optimize a TensorZero function using an arbitrary tool — here, DSPy.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;em&gt;&amp;amp; many more on the way!&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>paradigmxyz/reth</title>
      <link>https://github.com/paradigmxyz/reth</link>
      <description>&lt;p&gt;Modular, contributor-friendly and blazing-fast implementation of the Ethereum protocol, in Rust&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;reth&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/paradigmxyz/reth/actions/workflows/bench.yml"&gt;&lt;img src="https://github.com/paradigmxyz/reth/actions/workflows/bench.yml/badge.svg?sanitize=true" alt="bench status" /&gt;&lt;/a&gt; &lt;a href="https://github.com/paradigmxyz/reth/actions/workflows/unit.yml"&gt;&lt;img src="https://github.com/paradigmxyz/reth/workflows/unit/badge.svg?sanitize=true" alt="CI status" /&gt;&lt;/a&gt; &lt;a href="https://github.com/paradigmxyz/reth/actions/workflows/lint.yml"&gt;&lt;img src="https://github.com/paradigmxyz/reth/actions/workflows/lint.yml/badge.svg?sanitize=true" alt="cargo-lint status" /&gt;&lt;/a&gt; &lt;a href="https://t.me/paradigm_reth"&gt;&lt;img src="https://img.shields.io/endpoint?color=neon&amp;amp;logo=telegram&amp;amp;label=chat&amp;amp;url=https%3A%2F%2Ftg.sumanjay.workers.dev%2Fparadigm%5Freth" alt="Telegram Chat" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Modular, contributor-friendly and blazing-fast implementation of the Ethereum protocol&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/paradigmxyz/reth/main/assets/reth-prod.png" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://paradigmxyz.github.io/reth/installation/installation.html"&gt;Install&lt;/a&gt;&lt;/strong&gt; | &lt;a href="https://reth.rs"&gt;User Docs&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/paradigmxyz/reth/main/docs"&gt;Developer Docs&lt;/a&gt; | &lt;a href="https://reth.rs/docs"&gt;Crate Docs&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;What is Reth?&lt;/h2&gt; 
&lt;p&gt;Reth (short for Rust Ethereum, &lt;a href="https://twitter.com/kelvinfichter/status/1597653609411268608"&gt;pronunciation&lt;/a&gt;) is a new Ethereum full node implementation that is focused on being user-friendly, highly modular, as well as being fast and efficient. Reth is an Execution Layer (EL) and is compatible with all Ethereum Consensus Layer (CL) implementations that support the &lt;a href="https://github.com/ethereum/execution-apis/tree/a0d03086564ab1838b462befbc083f873dcf0c0f/src/engine"&gt;Engine API&lt;/a&gt;. It is originally built and driven forward by &lt;a href="https://paradigm.xyz/"&gt;Paradigm&lt;/a&gt;, and is licensed under the Apache and MIT licenses.&lt;/p&gt; 
&lt;h2&gt;Goals&lt;/h2&gt; 
&lt;p&gt;As a full Ethereum node, Reth allows users to connect to the Ethereum network and interact with the Ethereum blockchain. This includes sending and receiving transactions/logs/traces, as well as accessing and interacting with smart contracts. Building a successful Ethereum node requires creating a high-quality implementation that is both secure and efficient, as well as being easy to use on consumer hardware. It also requires building a strong community of contributors who can help support and improve the software.&lt;/p&gt; 
&lt;p&gt;More concretely, our goals are:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Modularity&lt;/strong&gt;: Every component of Reth is built to be used as a library: well-tested, heavily documented and benchmarked. We envision that developers will import the node's crates, mix and match, and innovate on top of them. Examples of such usage include but are not limited to spinning up standalone P2P networks, talking directly to a node's database, or "unbundling" the node into the components you need. To achieve that, we are licensing Reth under the Apache/MIT permissive license. You can learn more about the project's components &lt;a href="https://raw.githubusercontent.com/paradigmxyz/reth/main/docs/repo/layout.md"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Performance&lt;/strong&gt;: Reth aims to be fast, so we use Rust and the &lt;a href="https://erigon.substack.com/p/erigon-stage-sync-and-control-flows"&gt;Erigon staged-sync&lt;/a&gt; node architecture. We also use our Ethereum libraries (including &lt;a href="https://github.com/alloy-rs/alloy/"&gt;Alloy&lt;/a&gt; and &lt;a href="https://github.com/bluealloy/revm/"&gt;revm&lt;/a&gt;) which we've battle-tested and optimized via &lt;a href="https://github.com/foundry-rs/foundry/"&gt;Foundry&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Free for anyone to use any way they want&lt;/strong&gt;: Reth is free open source software, built for the community, by the community. By licensing the software under the Apache/MIT license, we want developers to use it without being bound by business licenses, or having to think about the implications of GPL-like licenses.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Client Diversity&lt;/strong&gt;: The Ethereum protocol becomes more antifragile when no node implementation dominates. This ensures that if there's a software bug, the network does not finalize a bad block. By building a new client, we hope to contribute to Ethereum's antifragility.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Support as many EVM chains as possible&lt;/strong&gt;: We aspire that Reth can full-sync not only Ethereum, but also other chains like Optimism, Polygon, BNB Smart Chain, and more. If you're working on any of these projects, please reach out.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Configurability&lt;/strong&gt;: We want to solve for node operators that care about fast historical queries, but also for hobbyists who cannot operate on large hardware. We also want to support teams and individuals who want both sync from genesis and via "fast sync". We envision that Reth will be configurable enough and provide configurable "profiles" for the tradeoffs that each team faces.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Status&lt;/h2&gt; 
&lt;p&gt;Reth is production ready, and suitable for usage in mission-critical environments such as staking or high-uptime services. We also actively recommend professional node operators to switch to Reth in production for performance and cost reasons in use cases where high performance with great margins is required such as RPC, MEV, Indexing, Simulations, and P2P activities.&lt;/p&gt; 
&lt;p&gt;More historical context below:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;We released 1.0 "production-ready" stable Reth in June 2024. 
  &lt;ul&gt; 
   &lt;li&gt;Reth completed an audit with &lt;a href="https://sigmaprime.io/"&gt;Sigma Prime&lt;/a&gt;, the developers of &lt;a href="https://github.com/sigp/lighthouse"&gt;Lighthouse&lt;/a&gt;, the Rust Consensus Layer implementation. Find it &lt;a href="https://raw.githubusercontent.com/paradigmxyz/reth/main/audit/sigma_prime_audit_v2.pdf"&gt;here&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;Revm (the EVM used in Reth) underwent an audit with &lt;a href="https://twitter.com/guidovranken"&gt;Guido Vranken&lt;/a&gt; (#1 &lt;a href="https://ethereum.org/en/bug-bounty"&gt;Ethereum Bug Bounty&lt;/a&gt;). We will publish the results soon.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;We released multiple iterative beta versions, up to &lt;a href="https://github.com/paradigmxyz/reth/releases/tag/v0.2.0-beta.9"&gt;beta.9&lt;/a&gt; on Monday June 3, 2024,the last beta release.&lt;/li&gt; 
 &lt;li&gt;We released &lt;a href="https://github.com/paradigmxyz/reth/releases/tag/v0.2.0-beta.1"&gt;beta&lt;/a&gt; on Monday March 4, 2024, our first breaking change to the database model, providing faster query speed, smaller database footprint, and allowing "history" to be mounted on separate drives.&lt;/li&gt; 
 &lt;li&gt;We shipped iterative improvements until the last alpha release on February 28, 2024, &lt;a href="https://github.com/paradigmxyz/reth/releases/tag/v0.1.0-alpha.21"&gt;0.1.0-alpha.21&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;We &lt;a href="https://www.paradigm.xyz/2023/06/reth-alpha"&gt;initially announced&lt;/a&gt; &lt;a href="https://github.com/paradigmxyz/reth/releases/tag/v0.1.0-alpha.1"&gt;0.1.0-alpha.1&lt;/a&gt; on June 20, 2023.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Database compatibility&lt;/h3&gt; 
&lt;p&gt;We do not have any breaking database changes since beta.1, and we do not plan any in the near future.&lt;/p&gt; 
&lt;p&gt;Reth &lt;a href="https://github.com/paradigmxyz/reth/releases/tag/v0.2.0-beta.1"&gt;v0.2.0-beta.1&lt;/a&gt; includes a &lt;a href="https://github.com/paradigmxyz/reth/pull/5191"&gt;set of breaking database changes&lt;/a&gt; that makes it impossible to use database files produced by earlier versions.&lt;/p&gt; 
&lt;p&gt;If you had a database produced by alpha versions of Reth, you need to drop it with &lt;code&gt;reth db drop&lt;/code&gt; (using the same arguments such as &lt;code&gt;--config&lt;/code&gt; or &lt;code&gt;--datadir&lt;/code&gt; that you passed to &lt;code&gt;reth node&lt;/code&gt;), and resync using the same &lt;code&gt;reth node&lt;/code&gt; command you've used before.&lt;/p&gt; 
&lt;h2&gt;For Users&lt;/h2&gt; 
&lt;p&gt;See the &lt;a href="https://paradigmxyz.github.io/reth"&gt;Reth documentation&lt;/a&gt; for instructions on how to install and run Reth.&lt;/p&gt; 
&lt;h2&gt;For Developers&lt;/h2&gt; 
&lt;h3&gt;Using reth as a library&lt;/h3&gt; 
&lt;p&gt;You can use individual crates of reth in your project.&lt;/p&gt; 
&lt;p&gt;The crate docs can be found &lt;a href="https://paradigmxyz.github.io/reth/docs"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For a general overview of the crates, see &lt;a href="https://raw.githubusercontent.com/paradigmxyz/reth/main/docs/repo/layout.md"&gt;Project Layout&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Contributing&lt;/h3&gt; 
&lt;p&gt;If you want to contribute, or follow along with contributor discussion, you can use our &lt;a href="https://t.me/paradigm_reth"&gt;main telegram&lt;/a&gt; to chat with us about the development of Reth!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Our contributor guidelines can be found in &lt;a href="https://raw.githubusercontent.com/paradigmxyz/reth/main/CONTRIBUTING.md"&gt;&lt;code&gt;CONTRIBUTING.md&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;See our &lt;a href="https://raw.githubusercontent.com/paradigmxyz/reth/main/docs"&gt;contributor docs&lt;/a&gt; for more information on the project. A good starting point is &lt;a href="https://raw.githubusercontent.com/paradigmxyz/reth/main/docs/repo/layout.md"&gt;Project Layout&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Building and testing&lt;/h3&gt; 
&lt;!--
When updating this, also update:
- Cargo.toml
- .github/workflows/lint.yml
--&gt; 
&lt;p&gt;The Minimum Supported Rust Version (MSRV) of this project is &lt;a href="https://blog.rust-lang.org/2025/06/26/Rust-1.88.0/"&gt;1.88.0&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;See the docs for detailed instructions on how to &lt;a href="https://paradigmxyz.github.io/reth/installation/source"&gt;build from source&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To fully test Reth, you will need to have &lt;a href="https://geth.ethereum.org/docs/getting-started/installing-geth"&gt;Geth installed&lt;/a&gt;, but it is possible to run a subset of tests without Geth.&lt;/p&gt; 
&lt;p&gt;First, clone the repository:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;git clone https://github.com/paradigmxyz/reth
cd reth
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Next, run the tests:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;cargo nextest run --workspace

# Run the Ethereum Foundation tests
make ef-tests
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We highly recommend using &lt;a href="https://nexte.st/"&gt;&lt;code&gt;cargo nextest&lt;/code&gt;&lt;/a&gt; to speed up testing. Using &lt;code&gt;cargo test&lt;/code&gt; to run tests may work fine, but this is not tested and does not support more advanced features like retries for spurious failures.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Some tests use random number generators to generate test data. If you want to use a deterministic seed, you can set the &lt;code&gt;SEED&lt;/code&gt; environment variable.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Getting Help&lt;/h2&gt; 
&lt;p&gt;If you have any questions, first see if the answer to your question can be found in the &lt;a href="https://paradigmxyz.github.io/reth/"&gt;docs&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If the answer is not there:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Join the &lt;a href="https://t.me/paradigm_reth"&gt;Telegram&lt;/a&gt; to get help, or&lt;/li&gt; 
 &lt;li&gt;Open a &lt;a href="https://github.com/paradigmxyz/reth/discussions/new"&gt;discussion&lt;/a&gt; with your question, or&lt;/li&gt; 
 &lt;li&gt;Open an issue with &lt;a href="https://github.com/paradigmxyz/reth/issues/new?assignees=&amp;amp;labels=C-bug%2CS-needs-triage&amp;amp;projects=&amp;amp;template=bug.yml"&gt;the bug&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Security&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/paradigmxyz/reth/main/SECURITY.md"&gt;&lt;code&gt;SECURITY.md&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;p&gt;Reth is a new implementation of the Ethereum protocol. In the process of developing the node we investigated the design decisions other nodes have made to understand what is done well, what is not, and where we can improve the status quo.&lt;/p&gt; 
&lt;p&gt;None of this would have been possible without them, so big shoutout to the teams below:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ethereum/go-ethereum/"&gt;Geth&lt;/a&gt;: We would like to express our heartfelt gratitude to the go-ethereum team for their outstanding contributions to Ethereum over the years. Their tireless efforts and dedication have helped to shape the Ethereum ecosystem and make it the vibrant and innovative community it is today. Thank you for your hard work and commitment to the project.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ledgerwatch/erigon"&gt;Erigon&lt;/a&gt; (fka Turbo-Geth): Erigon pioneered the &lt;a href="https://erigon.substack.com/p/erigon-stage-sync-and-control-flows"&gt;"Staged Sync" architecture&lt;/a&gt; that Reth is using, as well as &lt;a href="https://github.com/ledgerwatch/erigon/wiki/Choice-of-storage-engine"&gt;introduced MDBX&lt;/a&gt; as the database of choice. We thank Erigon for pushing the state of the art research on the performance limits of Ethereum nodes.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/akula-bft/akula/"&gt;Akula&lt;/a&gt;: Reth uses forks of the Apache versions of Akula's &lt;a href="https://github.com/paradigmxyz/reth/pull/132"&gt;MDBX Bindings&lt;/a&gt;, &lt;a href="https://github.com/paradigmxyz/reth/pull/63"&gt;FastRLP&lt;/a&gt; and &lt;a href="https://github.com/paradigmxyz/reth/pull/80"&gt;ECIES&lt;/a&gt;. Given that these packages were already released under the Apache License, and they implement standardized solutions, we decided not to reimplement them to iterate faster. We thank the Akula team for their contributions to the Rust Ethereum ecosystem and for publishing these packages.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Warning&lt;/h2&gt; 
&lt;p&gt;The &lt;code&gt;NippyJar&lt;/code&gt; and &lt;code&gt;Compact&lt;/code&gt; encoding formats and their implementations are designed for storing and retrieving data internally. They are not hardened to safely read potentially malicious data.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>google/comprehensive-rust</title>
      <link>https://github.com/google/comprehensive-rust</link>
      <description>&lt;p&gt;This is the Rust course used by the Android team at Google. It provides you the material to quickly teach Rust.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Comprehensive Rust 🦀&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/google/comprehensive-rust/actions/workflows/build.yml?query=branch%3Amain"&gt;&lt;img src="https://img.shields.io/github/actions/workflow/status/google/comprehensive-rust/build.yml?style=flat-square" alt="Build workflow" /&gt;&lt;/a&gt; &lt;a href="https://github.com/google/comprehensive-rust/graphs/contributors"&gt;&lt;img src="https://img.shields.io/github/contributors/google/comprehensive-rust?style=flat-square" alt="GitHub contributors" /&gt;&lt;/a&gt; &lt;a href="https://github.com/google/comprehensive-rust/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/google/comprehensive-rust?style=flat-square" alt="GitHub stars" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;This repository has the source code for Comprehensive Rust 🦀, a multi-day Rust course developed by the Android team. The course covers all aspects of Rust, from basic syntax to generics and error handling. It also includes deep dives on &lt;a href="https://google.github.io/comprehensive-rust/android.html"&gt;Android&lt;/a&gt;, &lt;a href="https://google.github.io/comprehensive-rust/chromium.html"&gt;Chromium&lt;/a&gt;, &lt;a href="https://google.github.io/comprehensive-rust/bare-metal.html"&gt;bare-metal&lt;/a&gt;, and &lt;a href="https://google.github.io/comprehensive-rust/concurrency.html"&gt;concurrency&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Read the course at &lt;strong&gt;&lt;a href="https://google.github.io/comprehensive-rust/"&gt;https://google.github.io/comprehensive-rust/&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; 
&lt;h2&gt;Course Format and Target Audience&lt;/h2&gt; 
&lt;p&gt;The course is used internally at Google when teaching Rust to experienced software engineers. They typically have a background in C++ or Java.&lt;/p&gt; 
&lt;p&gt;The course is taught in a classroom setting and we hope it will be useful for others who want to teach Rust to their team. The course will be less useful for self-study since you miss out on the discussions happening in the classroom. You don't see the questions and answers and you don't see the compiler errors we trigger when going through the code samples. We hope to improve on this via &lt;a href="https://github.com/google/comprehensive-rust/issues/53"&gt;speaker notes&lt;/a&gt; and by &lt;a href="https://github.com/google/comprehensive-rust/issues/52"&gt;publishing videos&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Press&lt;/h2&gt; 
&lt;p&gt;Articles and blog posts from around the web which cover Comprehensive Rust:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;2023-09-08: &lt;em&gt;&lt;a href="https://mo8it.com/blog/teaching-rust/"&gt;Teaching Rust in 5 days&lt;/a&gt;&lt;/em&gt;. Comprehensive Rust was used as a base for a 5-day university class on Rust.&lt;/li&gt; 
 &lt;li&gt;2023-09-21: &lt;em&gt;&lt;a href="https://security.googleblog.com/2023/09/scaling-rust-adoption-through-training.html"&gt;Scaling Rust Adoption Through Training&lt;/a&gt;&lt;/em&gt;. We published a blog post with details on the development of the course.&lt;/li&gt; 
 &lt;li&gt;2023-10-02: &lt;em&gt;&lt;a href="https://www.darkreading.com/application-security/seeking-rust-developers-in-house-training"&gt;In Search of Rust Developers, Companies Turn to In-House Training&lt;/a&gt;&lt;/em&gt;. About how Microsoft, Google, and others are training people in Rust.&lt;/li&gt; 
 &lt;li&gt;2024-10-18: &lt;em&gt;&lt;a href="https://youtu.be/7h5KyMqt2-Q?si=4M99HdWWxMaqN8Zr"&gt;Rust Training at Scale | Rust Global @ RustConf 2024&lt;/a&gt;&lt;/em&gt;. What Google learnt from teaching Comprehensive Rust for more than two years.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Setup&lt;/h2&gt; 
&lt;p&gt;The course is built using a few tools:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/rust-lang/mdBook"&gt;mdbook&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/boozook/mdbook-svgbob"&gt;mdbook-svgbob&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/google/mdbook-i18n-helpers"&gt;mdbook-i18n-helpers and i18n-report&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/comprehensive-rust/main/mdbook-exerciser/"&gt;mdbook-exerciser&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/comprehensive-rust/main/mdbook-course/"&gt;mdbook-course&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/marxin/mdbook-linkcheck2"&gt;mdbook-linkcheck2&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;First install Rust by following the instructions on &lt;a href="https://rustup.rs/"&gt;https://rustup.rs/&lt;/a&gt;. Then clone this repository:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;git clone https://github.com/google/comprehensive-rust/
cd comprehensive-rust
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then install these tools with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;cargo xtask install-tools
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; We use &lt;code&gt;xtask&lt;/code&gt; for task automation within the project (e.g. installing required tools). Xtask is not a package that you should install. Visit &lt;a href="https://github.com/matklad/cargo-xtask"&gt;https://github.com/matklad/cargo-xtask&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Commands&lt;/h2&gt; 
&lt;p&gt;Here are some of the commonly used commands you can run in the project. Run &lt;code&gt;cargo xtask&lt;/code&gt; to view all available commands.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Command&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;cargo xtask install-tools&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Install all the tools the project depends on.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;cargo xtask serve&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Start a web server with the course. You'll find the content on &lt;a href="http://localhost:3000"&gt;http://localhost:3000&lt;/a&gt;. To serve any of the translated versions of the course, add the language flag (--language or -l) followed by xx, where xx is the ISO 639 language code (e.g. cargo xtask serve -l da for the Danish translation).&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;cargo xtask rust-tests&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Test the included Rust snippets.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;cargo xtask web-tests&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Run the web driver tests in the tests directory.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;cargo xtask build&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Create a static version of the course in the &lt;code&gt;book/&lt;/code&gt; directory. Note that you have to separately build and zip exercises and add them to book/html. To build any of the translated versions of the course, add the language flag (--language or -l) followed by xx, where xx is the ISO 639 language code (e.g. cargo xtask build -l da for the Danish translation). &lt;a href="https://raw.githubusercontent.com/google/comprehensive-rust/main/TRANSLATIONS.md"&gt;TRANSLATIONS.md&lt;/a&gt; contains further instructions.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; On Windows, you need to enable symlinks (&lt;code&gt;git config --global core.symlinks true&lt;/code&gt;) and Developer Mode.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We would like to receive your contributions. Please see &lt;a href="https://raw.githubusercontent.com/google/comprehensive-rust/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for details.&lt;/p&gt; 
&lt;h2&gt;Contact&lt;/h2&gt; 
&lt;p&gt;For questions or comments, please contact &lt;a href="mailto:mgeisler@google.com"&gt;Martin Geisler&lt;/a&gt; or start a &lt;a href="https://github.com/google/comprehensive-rust/discussions"&gt;discussion on GitHub&lt;/a&gt;. We would love to hear from you.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>mainmatter/100-exercises-to-learn-rust</title>
      <link>https://github.com/mainmatter/100-exercises-to-learn-rust</link>
      <description>&lt;p&gt;A self-paced course to learn Rust, one exercise at a time.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Learn Rust, one exercise at a time&lt;/h1&gt; 
&lt;p&gt;You've heard about Rust, but you never had the chance to try it out?&lt;br /&gt; This course is for you!&lt;/p&gt; 
&lt;p&gt;You'll learn Rust by solving 100 exercises.&lt;br /&gt; You'll go from knowing nothing about Rust to being able to start writing your own programs, one exercise at a time.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] This course has been written by &lt;a href="https://mainmatter.com/rust-consulting/"&gt;Mainmatter&lt;/a&gt;.&lt;br /&gt; It's one of the trainings in &lt;a href="https://mainmatter.com/services/workshops/rust/"&gt;our portfolio of Rust workshops&lt;/a&gt;.&lt;br /&gt; Check out our &lt;a href="https://mainmatter.com/rust-consulting/"&gt;landing page&lt;/a&gt; if you're looking for Rust consulting or training!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Getting started&lt;/h2&gt; 
&lt;p&gt;Go to &lt;a href="https://rust-exercises.com"&gt;rust-exercises.com&lt;/a&gt; and follow the instructions there to get started with the course.&lt;/p&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Rust&lt;/strong&gt; (follow instructions &lt;a href="https://www.rust-lang.org/tools/install"&gt;here&lt;/a&gt;).&lt;br /&gt; If &lt;code&gt;rustup&lt;/code&gt; is already installed on your system, run &lt;code&gt;rustup update&lt;/code&gt; (or another appropriate command depending on how you installed Rust on your system) to make sure you're running on the latest stable version.&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;(Optional but recommended)&lt;/em&gt; An IDE with Rust autocompletion support. We recommend one of the following: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://www.jetbrains.com/rust/"&gt;RustRover&lt;/a&gt;;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://code.visualstudio.com"&gt;Visual Studio Code&lt;/a&gt; with the &lt;a href="https://marketplace.visualstudio.com/items?itemName=matklad.rust-analyzer"&gt;&lt;code&gt;rust-analyzer&lt;/code&gt;&lt;/a&gt; extension.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Solutions&lt;/h2&gt; 
&lt;p&gt;You can find the solutions to the exercises in the &lt;a href="https://github.com/mainmatter/100-exercises-to-learn-rust/tree/solutions"&gt;&lt;code&gt;solutions&lt;/code&gt; branch&lt;/a&gt; of this repository.&lt;/p&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;p&gt;Copyright © 2024- Mainmatter GmbH (&lt;a href="https://mainmatter.com"&gt;https://mainmatter.com&lt;/a&gt;), released under the &lt;a href="https://creativecommons.org/licenses/by-nc/4.0/"&gt;Creative Commons Attribution-NonCommercial 4.0 International license&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ai-dynamo/dynamo</title>
      <link>https://github.com/ai-dynamo/dynamo</link>
      <description>&lt;p&gt;A Datacenter Scale Distributed Inference Serving Framework&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/docs/images/frontpage-banner.png" alt="Dynamo banner" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://opensource.org/licenses/Apache-2.0"&gt;&lt;img src="https://img.shields.io/badge/License-Apache_2.0-blue.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://github.com/ai-dynamo/dynamo/releases/latest"&gt;&lt;img src="https://img.shields.io/github/v/release/ai-dynamo/dynamo" alt="GitHub Release" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/D92uqZRjCZ"&gt;&lt;img src="https://dcbadge.limes.pink/api/server/D92uqZRjCZ?style=flat" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/ai-dynamo/dynamo"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;| &lt;strong&gt;&lt;a href="https://github.com/ai-dynamo/dynamo/issues/762"&gt;Roadmap&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href="https://docs.nvidia.com/dynamo/latest/index.html"&gt;Documentation&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href="https://github.com/ai-dynamo/dynamo/tree/main/examples"&gt;Examples&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href="https://github.com/ai-dynamo/enhancements"&gt;Design Proposals&lt;/a&gt;&lt;/strong&gt; |&lt;/p&gt; 
&lt;h1&gt;NVIDIA Dynamo&lt;/h1&gt; 
&lt;p&gt;High-throughput, low-latency inference framework designed for serving generative AI and reasoning models in multi-node distributed environments.&lt;/p&gt; 
&lt;h2&gt;Latest News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;[08/05] Deploy &lt;code&gt;openai/gpt-oss-120b&lt;/code&gt; with disaggregated serving on NVIDIA Blackwell GPUs using Dynamo &lt;a href="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/components/backends/trtllm/gpt-oss.md"&gt;➡️ link&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;The Era of Multi-GPU, Multi-Node&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/docs/images/frontpage-gpu-vertical.png" alt="Multi Node Multi-GPU topology" width="600" /&gt; &lt;/p&gt; 
&lt;p&gt;Large language models are quickly outgrowing the memory and compute budget of any single GPU. Tensor-parallelism solves the capacity problem by spreading each layer across many GPUs—and sometimes many servers—but it creates a new one: how do you coordinate those shards, route requests, and share KV cache fast enough to feel like one accelerator? This orchestration gap is exactly what NVIDIA Dynamo is built to close.&lt;/p&gt; 
&lt;p&gt;Dynamo is designed to be inference engine agnostic (supports TRT-LLM, vLLM, SGLang or others) and captures LLM-specific capabilities such as:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Disaggregated prefill &amp;amp; decode inference&lt;/strong&gt; – Maximizes GPU throughput and facilitates trade off between throughput and latency.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Dynamic GPU scheduling&lt;/strong&gt; – Optimizes performance based on fluctuating demand&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;LLM-aware request routing&lt;/strong&gt; – Eliminates unnecessary KV cache re-computation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Accelerated data transfer&lt;/strong&gt; – Reduces inference response time using NIXL.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;KV cache offloading&lt;/strong&gt; – Leverages multiple memory hierarchies for higher system throughput&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/docs/images/frontpage-architecture.png" alt="Dynamo architecture" width="600" /&gt; &lt;/p&gt; 
&lt;h2&gt;Framework Support Matrix&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Feature&lt;/th&gt; 
   &lt;th&gt;vLLM&lt;/th&gt; 
   &lt;th&gt;SGLang&lt;/th&gt; 
   &lt;th&gt;TensorRT-LLM&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/docs/architecture/disagg_serving.md"&gt;&lt;strong&gt;Disaggregated Serving&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/docs/architecture/disagg_serving.md#conditional-disaggregation"&gt;&lt;strong&gt;Conditional Disaggregation&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;🚧&lt;/td&gt; 
   &lt;td&gt;🚧&lt;/td&gt; 
   &lt;td&gt;🚧&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/docs/architecture/kv_cache_routing.md"&gt;&lt;strong&gt;KV-Aware Routing&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/docs/architecture/load_planner.md"&gt;&lt;strong&gt;Load Based Planner&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;🚧&lt;/td&gt; 
   &lt;td&gt;🚧&lt;/td&gt; 
   &lt;td&gt;🚧&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/docs/architecture/sla_planner.md"&gt;&lt;strong&gt;SLA-Based Planner&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;🚧&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/docs/architecture/kvbm_architecture.md"&gt;&lt;strong&gt;KVBM&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;🚧&lt;/td&gt; 
   &lt;td&gt;🚧&lt;/td&gt; 
   &lt;td&gt;🚧&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;To learn more about each framework and their capabilities, check out each framework's README!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/components/backends/vllm/README.md"&gt;vLLM&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/components/backends/sglang/README.md"&gt;SGLang&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/components/backends/trtllm/README.md"&gt;TensorRT-LLM&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Built in Rust for performance and in Python for extensibility, Dynamo is fully open-source and driven by a transparent, OSS (Open Source Software) first development approach.&lt;/p&gt; 
&lt;h1&gt;Installation&lt;/h1&gt; 
&lt;p&gt;The following examples require a few system level packages. Recommended to use Ubuntu 24.04 with a x86_64 CPU. See &lt;a href="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/docs/support_matrix.md"&gt;docs/support_matrix.md&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;1. Initial setup&lt;/h2&gt; 
&lt;p&gt;The Dynamo team recommends the &lt;code&gt;uv&lt;/code&gt; Python package manager, although any way works. Install uv:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;curl -LsSf https://astral.sh/uv/install.sh | sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Install etcd and NATS (required)&lt;/h3&gt; 
&lt;p&gt;To coordinate across a data center, Dynamo relies on etcd and NATS. To run Dynamo locally, these need to be available.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://etcd.io/"&gt;etcd&lt;/a&gt; can be run directly as &lt;code&gt;./etcd&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nats.io/"&gt;nats&lt;/a&gt; needs jetstream enabled: &lt;code&gt;nats-server -js&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To quickly setup etcd &amp;amp; NATS, you can also run:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;# At the root of the repository:
docker compose -f deploy/docker-compose.yml up -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;2. Select an engine&lt;/h2&gt; 
&lt;p&gt;We publish Python wheels specialized for each of our supported engines: vllm, sglang, trtllm, and llama.cpp. The examples that follow use SGLang; continue reading for other engines.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;uv venv venv
source venv/bin/activate
uv pip install pip

# Choose one
uv pip install "ai-dynamo[sglang]"  #replace with [vllm], [trtllm], etc.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;3. Run Dynamo&lt;/h2&gt; 
&lt;h3&gt;Running an LLM API server&lt;/h3&gt; 
&lt;p&gt;Dynamo provides a simple way to spin up a local set of inference components including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;OpenAI Compatible Frontend&lt;/strong&gt; – High performance OpenAI compatible http api server written in Rust.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Basic and Kv Aware Router&lt;/strong&gt; – Route and load balance traffic to a set of workers.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Workers&lt;/strong&gt; – Set of pre-configured LLM serving engines.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;# Start an OpenAI compatible HTTP server, a pre-processor (prompt templating and tokenization) and a router.
# Pass the TLS certificate and key paths to use HTTPS instead of HTTP.
python -m dynamo.frontend --http-port 8000 [--tls-cert-path cert.pem] [--tls-key-path key.pem]

# Start the SGLang engine, connecting to NATS and etcd to receive requests. You can run several of these,
# both for the same model and for multiple models. The frontend node will discover them.
python -m dynamo.sglang.worker --model deepseek-ai/DeepSeek-R1-Distill-Llama-8B --skip-tokenizer-init
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Send a Request&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl localhost:8000/v1/chat/completions   -H "Content-Type: application/json"   -d '{
    "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
    "messages": [
    {
        "role": "user",
        "content": "Hello, how are you?"
    }
    ],
    "stream":false,
    "max_tokens": 300
  }' | jq
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Rerun with &lt;code&gt;curl -N&lt;/code&gt; and change &lt;code&gt;stream&lt;/code&gt; in the request to &lt;code&gt;true&lt;/code&gt; to get the responses as soon as the engine issues them.&lt;/p&gt; 
&lt;h3&gt;Deploying Dynamo&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Follow the &lt;a href="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/docs/guides/dynamo_deploy/README.md"&gt;Quickstart Guide&lt;/a&gt; to deploy on Kubernetes.&lt;/li&gt; 
 &lt;li&gt;Check out &lt;a href="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/components/backends"&gt;Backends&lt;/a&gt; to deploy various workflow configurations (e.g. SGLang with router, vLLM with disaggregated serving, etc.)&lt;/li&gt; 
 &lt;li&gt;Run some &lt;a href="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/examples"&gt;Examples&lt;/a&gt; to learn about building components in Dynamo and exploring various integrations.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Benchmarking Dynamo&lt;/h3&gt; 
&lt;p&gt;Dynamo provides comprehensive benchmarking tools to evaluate and optimize your deployments:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/docs/benchmarks/benchmarking.md"&gt;Benchmarking Guide&lt;/a&gt;&lt;/strong&gt; – Compare deployment topologies (aggregated vs. disaggregated vs. vanilla vLLM) using GenAI-Perf&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/docs/benchmarks/pre_deployment_profiling.md"&gt;Pre-Deployment Profiling&lt;/a&gt;&lt;/strong&gt; – Optimize configurations before deployment to meet SLA requirements&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Engines&lt;/h1&gt; 
&lt;p&gt;Dynamo is designed to be inference engine agnostic. To use any engine with Dynamo, NATS and etcd need to be installed, along with a Dynamo frontend (&lt;code&gt;python -m dynamo.frontend [--interactive]&lt;/code&gt;).&lt;/p&gt; 
&lt;h2&gt;vLLM&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;uv pip install ai-dynamo[vllm]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run the backend/worker like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python -m dynamo.vllm --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;vLLM attempts to allocate enough KV cache for the full context length at startup. If that does not fit in your available memory pass &lt;code&gt;--context-length &amp;lt;value&amp;gt;&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;To specify which GPUs to use set environment variable &lt;code&gt;CUDA_VISIBLE_DEVICES&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;SGLang&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;# Install libnuma
apt install -y libnuma-dev

uv pip install ai-dynamo[sglang]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run the backend/worker like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python -m dynamo.sglang.worker --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can pass any sglang flags directly to this worker, see &lt;a href="https://docs.sglang.ai/advanced_features/server_arguments.html"&gt;https://docs.sglang.ai/advanced_features/server_arguments.html&lt;/a&gt; . See there to use multiple GPUs.&lt;/p&gt; 
&lt;h2&gt;TensorRT-LLM&lt;/h2&gt; 
&lt;p&gt;It is recommended to use &lt;a href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch"&gt;NGC PyTorch Container&lt;/a&gt; for running the TensorRT-LLM engine.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Note] Ensure that you select a PyTorch container image version that matches the version of TensorRT-LLM you are using. For example, if you are using &lt;code&gt;tensorrt-llm==1.0.0rc6&lt;/code&gt;, use the PyTorch container image version &lt;code&gt;25.06&lt;/code&gt;. To find the correct PyTorch container version for your desired &lt;code&gt;tensorrt-llm&lt;/code&gt; release, visit the &lt;a href="https://github.com/NVIDIA/TensorRT-LLM/raw/main/docker/Dockerfile.multi"&gt;TensorRT-LLM Dockerfile.multi&lt;/a&gt; on GitHub. Switch to the branch that matches your &lt;code&gt;tensorrt-llm&lt;/code&gt; version, and look for the &lt;code&gt;BASE_TAG&lt;/code&gt; line to identify the recommended PyTorch container tag.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Important] Launch container with the following additional settings &lt;code&gt;--shm-size=1g --ulimit memlock=-1&lt;/code&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Install prerequisites&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;# Optional step: Only required for Blackwell and Grace Hopper
uv pip install torch==2.7.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128

# Required until the trtllm version is bumped to include this pinned dependency itself
uv pip install "cuda-python&amp;gt;=12,&amp;lt;13"

sudo apt-get -y install libopenmpi-dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Tip] You can learn more about these prequisites and known issues with TensorRT-LLM pip based installation &lt;a href="https://nvidia.github.io/TensorRT-LLM/installation/linux.html"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;After installing the pre-requisites above, install Dynamo&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;uv pip install ai-dynamo[trtllm]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run the backend/worker like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python -m dynamo.trtllm --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To specify which GPUs to use set environment variable &lt;code&gt;CUDA_VISIBLE_DEVICES&lt;/code&gt;.&lt;/p&gt; 
&lt;h1&gt;Developing Locally&lt;/h1&gt; 
&lt;h2&gt;1. Install libraries&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Ubuntu:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;sudo apt install -y build-essential libhwloc-dev libudev-dev pkg-config libclang-dev protobuf-compiler python3-dev cmake
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;macOS:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://brew.sh/"&gt;Homebrew&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;# if brew is not installed on your system, install it
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://developer.apple.com/xcode/"&gt;Xcode&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;brew install cmake protobuf

## Check that Metal is accessible
xcrun -sdk macosx metal
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If Metal is accessible, you should see an error like &lt;code&gt;metal: error: no input files&lt;/code&gt;, which confirms it is installed correctly.&lt;/p&gt; 
&lt;h2&gt;2. Install Rust&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
source $HOME/.cargo/env
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;3. Create a Python virtual env:&lt;/h2&gt; 
&lt;p&gt;Follow the instructions in &lt;a href="https://docs.astral.sh/uv/#installation"&gt;uv installation&lt;/a&gt; guide to install uv if you don't have &lt;code&gt;uv&lt;/code&gt; installed. Once uv is installed, create a virtual environment and activate it.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Install uv&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -LsSf https://astral.sh/uv/install.sh | sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Create a virtual environment&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv venv dynamo
source dynamo/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;4. Install build tools&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;uv pip install pip maturin
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://github.com/PyO3/maturin"&gt;Maturin&lt;/a&gt; is the Rust&amp;lt;-&amp;gt;Python bindings build tool.&lt;/p&gt; 
&lt;h2&gt;5. Build the Rust bindings&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;cd lib/bindings/python
maturin develop --uv
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;6. Install the wheel&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;cd $PROJECT_ROOT
uv pip install .
# For development, use
export PYTHONPATH="${PYTHONPATH}:$(pwd)/components/frontend/src:$(pwd)/components/planner/src:$(pwd)/components/backends/vllm/src:$(pwd)/components/backends/sglang/src:$(pwd)/components/backends/trtllm/src:$(pwd)/components/backends/llama_cpp/src:$(pwd)/components/backends/mocker/src"
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Note] Editable (&lt;code&gt;-e&lt;/code&gt;) does not work because the &lt;code&gt;dynamo&lt;/code&gt; package is split over multiple directories, one per backend.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;You should now be able to run &lt;code&gt;python -m dynamo.frontend&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Remember that nats and etcd must be running (see earlier).&lt;/p&gt; 
&lt;p&gt;Set the environment variable &lt;code&gt;DYN_LOG&lt;/code&gt; to adjust the logging level; for example, &lt;code&gt;export DYN_LOG=debug&lt;/code&gt;. It has the same syntax as &lt;code&gt;RUST_LOG&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;If you use vscode or cursor, we have a .devcontainer folder built on &lt;a href="https://code.visualstudio.com/docs/devcontainers/containers"&gt;Microsofts Extension&lt;/a&gt;. For instructions see the &lt;a href="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/.devcontainer/README.md"&gt;ReadMe&lt;/a&gt; for more details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>gitbutlerapp/gitbutler</title>
      <link>https://github.com/gitbutlerapp/gitbutler</link>
      <description>&lt;p&gt;The GitButler version control client, backed by Git, powered by Tauri/Rust/Svelte&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img align="center" width="128px" src="https://raw.githubusercontent.com/gitbutlerapp/gitbutler/master/crates/gitbutler-tauri/icons/128x128@2x.png" /&gt; 
 &lt;h1 align="center"&gt;&lt;b&gt;GitButler&lt;/b&gt;&lt;/h1&gt; 
 &lt;p align="center"&gt; Git branch management tool, built from the ground up for modern workflows &lt;br /&gt; &lt;a href="https://gitbutler.com"&gt;&lt;strong&gt;gitbutler.com »&lt;/strong&gt;&lt;/a&gt; &lt;br /&gt; &lt;br /&gt; &lt;b&gt;Download for &lt;/b&gt; macOS (&lt;a href="https://app.gitbutler.com/downloads/release/darwin/aarch64/dmg"&gt;Apple Silicon&lt;/a&gt; | &lt;a href="https://app.gitbutler.com/downloads/release/darwin/x86_64/dmg"&gt;Intel&lt;/a&gt;) · Linux (&lt;a href="https://app.gitbutler.com/downloads/release/linux/x86_64/gz"&gt;AppImage&lt;/a&gt; | &lt;a href="https://app.gitbutler.com/downloads/release/linux/x86_64/deb"&gt;deb&lt;/a&gt;) · Windows (&lt;a href="https://app.gitbutler.com/downloads/release/windows/x86_64/msi"&gt;msi&lt;/a&gt;) &lt;br /&gt; &lt;br /&gt; (Unstable Nightly releases can be found &lt;a href="https://app.gitbutler.com/downloads"&gt;here&lt;/a&gt;) &lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/bf9bdb33-a979-47a0-b2b2-8fff5ea53afb" alt="gitbutler_client" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/gitbutlerapp/gitbutler/actions/workflows/push.yaml"&gt;&lt;img src="https://github.com/gitbutlerapp/gitbutler/actions/workflows/push.yaml/badge.svg?sanitize=true" alt="CI" /&gt;&lt;/a&gt; &lt;a href="https://gitbutler.com/"&gt;&lt;img src="https://img.shields.io/badge/GitButler-%23B9F4F2?logo=data%3Aimage%2Fsvg%2Bxml%3Bbase64%2CPHN2ZyB3aWR0aD0iMzkiIGhlaWdodD0iMjgiIHZpZXdCb3g9IjAgMCAzOSAyOCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggZD0iTTI1LjIxNDUgMTIuMTk5N0wyLjg3MTA3IDEuMzg5MTJDMS41NDI5NSAwLjc0NjUzMiAwIDEuNzE0MDYgMCAzLjE4OTQ3VjI0LjgxMDVDMCAyNi4yODU5IDEuNTQyOTUgMjcuMjUzNSAyLjg3MTA3IDI2LjYxMDlMMjUuMjE0NSAxNS44MDAzQzI2LjcxOTcgMTUuMDcyMSAyNi43MTk3IDEyLjkyNzkgMjUuMjE0NSAxMi4xOTk3WiIgZmlsbD0iYmxhY2siLz4KPHBhdGggZD0iTTEzLjc4NTUgMTIuMTk5N0wzNi4xMjg5IDEuMzg5MTJDMzcuNDU3MSAwLjc0NjUzMiAzOSAxLjcxNDA2IDM5IDMuMTg5NDdWMjQuODEwNUMzOSAyNi4yODU5IDM3LjQ1NzEgMjcuMjUzNSAzNi4xMjg5IDI2LjYxMDlMMTMuNzg1NSAxNS44MDAzQzEyLjI4MDMgMTUuMDcyMSAxMi4yODAzIDEyLjkyNzkgMTMuNzg1NSAxMi4xOTk3WiIgZmlsbD0idXJsKCNwYWludDBfcmFkaWFsXzMxMF8xMjkpIi8%2BCjxkZWZzPgo8cmFkaWFsR3JhZGllbnQgaWQ9InBhaW50MF9yYWRpYWxfMzEwXzEyOSIgY3g9IjAiIGN5PSIwIiByPSIxIiBncmFkaWVudFVuaXRzPSJ1c2VyU3BhY2VPblVzZSIgZ3JhZGllbnRUcmFuc2Zvcm09InRyYW5zbGF0ZSgxNi41NzAxIDE0KSBzY2FsZSgxOS44NjQxIDE5LjgzODMpIj4KPHN0b3Agb2Zmc2V0PSIwLjMwMTA1NiIgc3RvcC1vcGFjaXR5PSIwIi8%2BCjxzdG9wIG9mZnNldD0iMSIvPgo8L3JhZGlhbEdyYWRpZW50Pgo8L2RlZnM%2BCjwvc3ZnPgo%3D" alt="BADGE" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/intent/follow?screen_name=gitbutler"&gt;&lt;img src="https://img.shields.io/badge/Twitter-black?logo=x&amp;amp;logoColor=white" alt="TWEET" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/MmFkmaJ42D"&gt;&lt;img src="https://img.shields.io/discord/1060193121130000425?label=Discord&amp;amp;color=5865F2" alt="DISCORD" /&gt;&lt;/a&gt; &lt;a href="https://www.instagram.com/gitbutler/"&gt;&lt;img src="https://img.shields.io/badge/Instagram-E4405F?logo=instagram&amp;amp;logoColor=white" alt="INSTA" /&gt;&lt;/a&gt; &lt;a href="https://www.youtube.com/@gitbutlerapp"&gt;&lt;img src="https://img.shields.io/youtube/channel/subscribers/UCEwkZIHGqsTGYvX8wgD0LoQ" alt="YOUTUBE" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/gitbutlerapp/gitbutler"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="DEEPWIKI" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://repobeats.axiom.co/api/embed/fb23382bcf57c609832661874d3019a43555d6ae.svg?sanitize=true" alt="Alt" title="Repobeats analytics for GitButler" /&gt;&lt;/p&gt; 
&lt;p&gt;GitButler is a git client that lets you work on multiple branches at the same time. It allows you to quickly organize file changes into separate branches while still having them applied to your working directory. You can then push branches individually to your remote, or directly create pull requests.&lt;/p&gt; 
&lt;p&gt;In a nutshell, it's a more flexible version of &lt;code&gt;git add -p&lt;/code&gt; and &lt;code&gt;git rebase -i&lt;/code&gt;, allowing you to efficiently multitask across branches.&lt;/p&gt; 
&lt;h2&gt;How Does It Work?&lt;/h2&gt; 
&lt;p&gt;GitButler keeps track of uncommitted changes in a layer on top of Git. Changes to files or parts of files can be grouped into what we call virtual branches. Whenever you are happy with the contents of a virtual branch, you can push it to a remote. GitButler makes sure that the state of other virtual branches is kept separate.&lt;/p&gt; 
&lt;h2&gt;How Do GB's Virtual Branches Differ From Git Branches?&lt;/h2&gt; 
&lt;p&gt;The branches that we know and love in Git are separate universes, and switching between them is a full context switch. GitButler allows you to work with multiple branches in parallel in the same working directory. This effectively means having the content of multiple branches available at the same time.&lt;/p&gt; 
&lt;p&gt;GitButler is aware of changes before they are committed. This allows it to keep a record of which virtual branch each individual diff belongs to. Effectively, this means that you can separate out individual branches with their content at any time to push them to a remote or to unapply them from your working directory.&lt;/p&gt; 
&lt;p&gt;And finally, while in Git it is preferable that you create your desired branch ahead of time, using GitButler you can move changes between virtual branches at any point during development.&lt;/p&gt; 
&lt;h2&gt;Why GitButler?&lt;/h2&gt; 
&lt;p&gt;We love Git. Our own &lt;a href="https://github.com/schacon"&gt;@schacon&lt;/a&gt; has even published the &lt;a href="https://git-scm.com/book/en/v2"&gt;Pro Git&lt;/a&gt; book. At the same time, Git's user interface hasn't been fundamentally changed for 15 years. While it was written for Linux kernel devs sending patches to each other over mailing lists, most developers today have different workflows and needs.&lt;/p&gt; 
&lt;p&gt;Instead of trying to fit the semantics of the Git CLI into a graphical interface, we are starting with the developer workflow and mapping it back to Git.&lt;/p&gt; 
&lt;h2&gt;Tech&lt;/h2&gt; 
&lt;p&gt;GitButler is a &lt;a href="https://tauri.app/"&gt;Tauri&lt;/a&gt;-based application. Its UI is written in &lt;a href="https://svelte.dev/"&gt;Svelte&lt;/a&gt; using &lt;a href="https://www.typescriptlang.org"&gt;TypeScript&lt;/a&gt; and its backend is written in &lt;a href="https://www.rust-lang.org/"&gt;Rust&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Main Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Virtual Branches&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Organize work on multiple branches simultaneously, rather than constantly switching branches&lt;/li&gt; 
   &lt;li&gt;Automatically create new branches when needed&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Easy Commit Management&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Undo, Amend and Squash commits by dragging and dropping&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Undo Timeline&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Logs all operations and changes and allows you to easily undo or revert any operation&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;GitHub Integration&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Authenticate to GitHub to open Pull Requests, list branches and statuses and more&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Easy SSH Key Management&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;GitButler can generate an SSH key to upload to GitHub automatically&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;AI Tooling&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Automatically write commit messages based on your work in progress&lt;/li&gt; 
   &lt;li&gt;Automatically create descriptive branch names&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Commit Signing&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Easy commit signing with GPG or SSH&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Example Uses&lt;/h2&gt; 
&lt;h3&gt;Fixing a Bug While Working on a Feature&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Say that while developing a feature, you encounter a bug that you wish to fix. It's often desirable that you ship the fix as a separate contribution (Pull request).&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Using Git you can stash your changes and switch to another branch, where you can commit, and push your fix.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;With GitButler&lt;/em&gt; you simply assign your fix to a separate virtual branch, which you can individually push (or directly create a PR). An additional benefit is that you can retain the fix in your working directory while waiting for CI and/or code review.&lt;/p&gt; 
&lt;h3&gt;Trying Someone Else's Branch Together With My Work in Progress&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Say you want to test a branch from someone else for the purpose of code review.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Using Git trying out someone else's branch is a full context switch away from your own work. &lt;em&gt;With GitButler&lt;/em&gt; you can apply and unapply (add / remove) any remote branch directly into your working directory.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;You can find our end user documentation at: &lt;a href="https://docs.gitbutler.com"&gt;https://docs.gitbutler.com&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Bugs and Feature Requests&lt;/h2&gt; 
&lt;p&gt;If you have a bug or feature request, feel free to open an &lt;a href="https://github.com/gitbutlerapp/gitbutler/issues/new"&gt;issue&lt;/a&gt;, or &lt;a href="https://discord.gg/MmFkmaJ42D"&gt;join our Discord server&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;AI Commit Message Generation&lt;/h2&gt; 
&lt;p&gt;Commit message generation is an opt-in feature. You can enable it while adding your repository for the first time or later in the project settings.&lt;/p&gt; 
&lt;p&gt;Currently, GitButler uses OpenAI's API for diff summarization, which means that if enabled, code diffs would be sent to OpenAI's servers.&lt;/p&gt; 
&lt;p&gt;Our goal is to make this feature more modular such that in the future you can modify the prompt as well as plug a different LLM endpoints (including local ones).&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;So you want to help out? Please check out the &lt;a href="https://raw.githubusercontent.com/gitbutlerapp/gitbutler/master/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; document.&lt;/p&gt; 
&lt;p&gt;If you want to skip right to getting the code to actually compile, take a look at the &lt;a href="https://raw.githubusercontent.com/gitbutlerapp/gitbutler/master/DEVELOPMENT.md"&gt;DEVELOPMENT.md&lt;/a&gt; file.&lt;/p&gt; 
&lt;p&gt;Want to show your support? Add a GitButler badge to your project's README:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-md"&gt;[![GitButler](https://img.shields.io/badge/GitButler-%23B9F4F2?logo=data%3Aimage%2Fsvg%2Bxml%3Bbase64%2CPHN2ZyB3aWR0aD0iMzkiIGhlaWdodD0iMjgiIHZpZXdCb3g9IjAgMCAzOSAyOCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggZD0iTTI1LjIxNDUgMTIuMTk5N0wyLjg3MTA3IDEuMzg5MTJDMS41NDI5NSAwLjc0NjUzMiAwIDEuNzE0MDYgMCAzLjE4OTQ3VjI0LjgxMDVDMCAyNi4yODU5IDEuNTQyOTUgMjcuMjUzNSAyLjg3MTA3IDI2LjYxMDlMMjUuMjE0NSAxNS44MDAzQzI2LjcxOTcgMTUuMDcyMSAyNi43MTk3IDEyLjkyNzkgMjUuMjE0NSAxMi4xOTk3WiIgZmlsbD0iYmxhY2siLz4KPHBhdGggZD0iTTEzLjc4NTUgMTIuMTk5N0wzNi4xMjg5IDEuMzg5MTJDMzcuNDU3MSAwLjc0NjUzMiAzOSAxLjcxNDA2IDM5IDMuMTg5NDdWMjQuODEwNUMzOSAyNi4yODU5IDM3LjQ1NzEgMjcuMjUzNSAzNi4xMjg5IDI2LjYxMDlMMTMuNzg1NSAxNS44MDAzQzEyLjI4MDMgMTUuMDcyMSAxMi4yODAzIDEyLjkyNzkgMTMuNzg1NSAxMi4xOTk3WiIgZmlsbD0idXJsKCNwYWludDBfcmFkaWFsXzMxMF8xMjkpIi8%2BCjxkZWZzPgo8cmFkaWFsR3JhZGllbnQgaWQ9InBhaW50MF9yYWRpYWxfMzEwXzEyOSIgY3g9IjAiIGN5PSIwIiByPSIxIiBncmFkaWVudFVuaXRzPSJ1c2VyU3BhY2VPblVzZSIgZ3JhZGllbnRUcmFuc2Zvcm09InRyYW5zbGF0ZSgxNi41NzAxIDE0KSBzY2FsZSgxOS44NjQxIDE5LjgzODMpIj4KPHN0b3Agb2Zmc2V0PSIwLjMwMTA1NiIgc3RvcC1vcGFjaXR5PSIwIi8%2BCjxzdG9wIG9mZnNldD0iMSIvPgo8L3JhZGlhbEdyYWRpZW50Pgo8L2RlZnM%2BCjwvc3ZnPgo%3D)](https://gitbutler.com/)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://gitbutler.com/"&gt;&lt;img src="https://img.shields.io/badge/GitButler-%23B9F4F2?logo=data%3Aimage%2Fsvg%2Bxml%3Bbase64%2CPHN2ZyB3aWR0aD0iMzkiIGhlaWdodD0iMjgiIHZpZXdCb3g9IjAgMCAzOSAyOCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggZD0iTTI1LjIxNDUgMTIuMTk5N0wyLjg3MTA3IDEuMzg5MTJDMS41NDI5NSAwLjc0NjUzMiAwIDEuNzE0MDYgMCAzLjE4OTQ3VjI0LjgxMDVDMCAyNi4yODU5IDEuNTQyOTUgMjcuMjUzNSAyLjg3MTA3IDI2LjYxMDlMMjUuMjE0NSAxNS44MDAzQzI2LjcxOTcgMTUuMDcyMSAyNi43MTk3IDEyLjkyNzkgMjUuMjE0NSAxMi4xOTk3WiIgZmlsbD0iYmxhY2siLz4KPHBhdGggZD0iTTEzLjc4NTUgMTIuMTk5N0wzNi4xMjg5IDEuMzg5MTJDMzcuNDU3MSAwLjc0NjUzMiAzOSAxLjcxNDA2IDM5IDMuMTg5NDdWMjQuODEwNUMzOSAyNi4yODU5IDM3LjQ1NzEgMjcuMjUzNSAzNi4xMjg5IDI2LjYxMDlMMTMuNzg1NSAxNS44MDAzQzEyLjI4MDMgMTUuMDcyMSAxMi4yODAzIDEyLjkyNzkgMTMuNzg1NSAxMi4xOTk3WiIgZmlsbD0idXJsKCNwYWludDBfcmFkaWFsXzMxMF8xMjkpIi8%2BCjxkZWZzPgo8cmFkaWFsR3JhZGllbnQgaWQ9InBhaW50MF9yYWRpYWxfMzEwXzEyOSIgY3g9IjAiIGN5PSIwIiByPSIxIiBncmFkaWVudFVuaXRzPSJ1c2VyU3BhY2VPblVzZSIgZ3JhZGllbnRUcmFuc2Zvcm09InRyYW5zbGF0ZSgxNi41NzAxIDE0KSBzY2FsZSgxOS44NjQxIDE5LjgzODMpIj4KPHN0b3Agb2Zmc2V0PSIwLjMwMTA1NiIgc3RvcC1vcGFjaXR5PSIwIi8%2BCjxzdG9wIG9mZnNldD0iMSIvPgo8L3JhZGlhbEdyYWRpZW50Pgo8L2RlZnM%2BCjwvc3ZnPgo%3D" alt="BADGE" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ast-grep/ast-grep</title>
      <link>https://github.com/ast-grep/ast-grep</link>
      <description>&lt;p&gt;⚡A CLI tool for code structural search, lint and rewriting. Written in Rust&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://ast-grep.github.io/logo.svg?sanitize=true" alt="ast-grep" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://github.com/ast-grep/ast-grep/actions/workflows/coverage.yaml/badge.svg?sanitize=true" alt="coverage badge" /&gt; &lt;a href="https://app.codecov.io/gh/ast-grep/ast-grep"&gt;&lt;img src="https://codecov.io/gh/ast-grep/ast-grep/branch/main/graph/badge.svg?token=37VX8H2EWV" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/4YZjf6htSQ" target="_blank"&gt;&lt;img alt="Discord" src="https://img.shields.io/discord/1107749847722889217?label=Discord" /&gt;&lt;/a&gt; &lt;a href="https://repology.org/project/ast-grep/versions" target="_blank"&gt;&lt;img alt="Repology" src="https://repology.org/badge/tiny-repos/ast-grep.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/github/stars/ast-grep/ast-grep?style=social" alt="Badge" /&gt; &lt;img src="https://img.shields.io/github/forks/ast-grep/ast-grep?style=social" alt="Badge" /&gt; &lt;img alt="GitHub Sponsors" src="https://img.shields.io/github/sponsors/HerringtonDarkholme?style=social" /&gt; &lt;a href="https://gurubase.io/g/ast-grep"&gt;&lt;img alt="Gurubase" src="https://img.shields.io/badge/Gurubase-Ask%20ast--grep%20Guru-006BFF" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;ast-grep(sg)&lt;/h2&gt; 
&lt;p&gt;ast-grep(sg) is a CLI tool for code structural search, lint, and rewriting.&lt;/p&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;ast-grep is an &lt;a href="https://dev.to/balapriya/abstract-syntax-tree-ast-explained-in-plain-english-1h38"&gt;abstract syntax tree&lt;/a&gt; based tool to search code by pattern code. Think of it as your old-friend &lt;a href="https://en.wikipedia.org/wiki/Grep#:~:text=grep%20is%20a%20command%2Dline,which%20has%20the%20same%20effect."&gt;&lt;code&gt;grep&lt;/code&gt;&lt;/a&gt;, but matching AST nodes instead of text. You can write patterns as if you are writing ordinary code. It will match all code that has the same syntactical structure. You can use &lt;code&gt;$&lt;/code&gt; sign + upper case letters as a &lt;a href="https://en.wikipedia.org/wiki/Wildcard_character"&gt;wildcard&lt;/a&gt;, e.g. &lt;code&gt;$MATCH&lt;/code&gt;, to match any single AST node. Think of it as &lt;a href="https://regexone.com/lesson/wildcards_dot"&gt;regular expression dot&lt;/a&gt; &lt;code&gt;.&lt;/code&gt;, except it is not textual.&lt;/p&gt; 
&lt;p&gt;Try the &lt;a href="https://ast-grep.github.io/playground.html"&gt;online playground&lt;/a&gt; for a taste!&lt;/p&gt; 
&lt;h2&gt;Screenshot&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://ast-grep.github.io/image/search-replace.png" alt="demo" /&gt;&lt;/p&gt; 
&lt;p&gt;See more screenshots on the &lt;a href="https://ast-grep.github.io/"&gt;website&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;You can install it from &lt;a href="https://docs.npmjs.com/downloading-and-installing-node-js-and-npm"&gt;npm&lt;/a&gt;, &lt;a href="https://pypi.org/"&gt;pip&lt;/a&gt;, &lt;a href="https://doc.rust-lang.org/cargo/getting-started/installation.html"&gt;cargo&lt;/a&gt;, &lt;a href="https://github.com/cargo-bins/cargo-binstall"&gt;cargo-binstall&lt;/a&gt;, &lt;a href="https://brew.sh/"&gt;homebrew&lt;/a&gt;, &lt;a href="https://scoop.sh/"&gt;scoop&lt;/a&gt; or &lt;a href="https://www.macports.org"&gt;MacPorts&lt;/a&gt;!&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npm install --global @ast-grep/cli
pip install ast-grep-cli
brew install ast-grep
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click for more installation methods&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;cargo install ast-grep --locked
cargo binstall ast-grep

# install via scoop, thank @brian6932
scoop install main/ast-grep

# install via MacPorts
sudo port install ast-grep

# try ast-grep in nix-shell
nix-shell -p ast-grep
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p&gt;Or you can build ast-grep from source. You need to install rustup, clone the repository and then&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cargo install --path ./crates/cli --locked
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://repology.org/project/ast-grep/versions"&gt;Packages&lt;/a&gt; are available on other platforms too.&lt;/p&gt; 
&lt;h2&gt;Command line usage example&lt;/h2&gt; 
&lt;p&gt;ast-grep has following form.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;ast-grep --pattern 'var code = $PATTERN' --rewrite 'let code = new $PATTERN' --lang ts
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Example&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://twitter.com/Hchan_mgn/status/1547061516993699841?s=20&amp;amp;t=ldDoj4U2nq-FRKQkU5GWXA"&gt;Rewrite code in null coalescing operator&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;ast-grep -p '$A &amp;amp;&amp;amp; $A()' -l ts -r '$A?.()'
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://twitter.com/Hchan_mgn/status/1561802312846278657"&gt;Rewrite&lt;/a&gt; &lt;a href="https://github.com/ecyrbe/zodios#migrate-to-v8"&gt;Zodios&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;ast-grep -p 'new Zodios($URL,  $CONF as const,)' -l ts -r 'new Zodios($URL, $CONF)' -i
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://twitter.com/Hchan_mgn/status/1560108625460355073"&gt;Implement eslint rule using YAML.&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Sponsor&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/HerringtonDarkholme/sponsors/main/sponsorkit/sponsors.svg?sanitize=true" alt="Sponsors" /&gt;&lt;/p&gt; 
&lt;p&gt;If you find ast-grep interesting and useful for your work, please &lt;a href="https://github.com/sponsors/HerringtonDarkholme"&gt;buy me a coffee&lt;/a&gt; so I can spend more time on the project!&lt;/p&gt; 
&lt;h2&gt;Feature Highlight&lt;/h2&gt; 
&lt;p&gt;ast-grep's core is an algorithm to search and replace code based on abstract syntax tree produced by tree-sitter. It can help you to do lightweight static analysis and massive scale code manipulation in an intuitive way.&lt;/p&gt; 
&lt;p&gt;Key highlights:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;An intuitive pattern to find and replace AST. ast-grep's pattern looks like ordinary code you would write every day (you could say the pattern is isomorphic to code).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;jQuery like API for AST traversal and manipulation.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;YAML configuration to write new linting rules or code modification.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Written in compiled language, with tree-sitter based parsing and utilizing multiple cores.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Beautiful command line interface :)&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;ast-grep's vision is to democratize abstract syntax tree magic and to liberate one from cumbersome AST programming!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;If you are an open-source library author, ast-grep can help your library users adopt breaking changes more easily.&lt;/li&gt; 
 &lt;li&gt;if you are a tech lead in your team, ast-grep can help you enforce code best practice tailored to your business need.&lt;/li&gt; 
 &lt;li&gt;If you are a security researcher, ast-grep can help you write rules much faster.&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>rustdesk/rustdesk</title>
      <link>https://github.com/rustdesk/rustdesk</link>
      <description>&lt;p&gt;An open-source remote desktop application designed for self-hosting, as an alternative to TeamViewer.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/rustdesk/rustdesk/master/res/logo-header.svg?sanitize=true" alt="RustDesk - Your remote desktop" /&gt;&lt;br /&gt; &lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/#raw-steps-to-build"&gt;Build&lt;/a&gt; • &lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/#how-to-build-with-docker"&gt;Docker&lt;/a&gt; • &lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/#file-structure"&gt;Structure&lt;/a&gt; • &lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/#snapshot"&gt;Snapshot&lt;/a&gt;&lt;br /&gt; [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-UA.md"&gt;Українська&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-CS.md"&gt;česky&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-ZH.md"&gt;中文&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-HU.md"&gt;Magyar&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-ES.md"&gt;Español&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-FA.md"&gt;فارسی&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-FR.md"&gt;Français&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-DE.md"&gt;Deutsch&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-PL.md"&gt;Polski&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-ID.md"&gt;Indonesian&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-FI.md"&gt;Suomi&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-ML.md"&gt;മലയാളം&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-JP.md"&gt;日本語&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-NL.md"&gt;Nederlands&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-IT.md"&gt;Italiano&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-RU.md"&gt;Русский&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-PTBR.md"&gt;Português (Brasil)&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-EO.md"&gt;Esperanto&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-KR.md"&gt;한국어&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-AR.md"&gt;العربي&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-VN.md"&gt;Tiếng Việt&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-DA.md"&gt;Dansk&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-GR.md"&gt;Ελληνικά&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-TR.md"&gt;Türkçe&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-NO.md"&gt;Norsk&lt;/a&gt;]&lt;br /&gt; &lt;b&gt;We need your help to translate this README, &lt;a href="https://github.com/rustdesk/rustdesk/tree/master/src/lang"&gt;RustDesk UI&lt;/a&gt; and &lt;a href="https://github.com/rustdesk/doc.rustdesk.com"&gt;RustDesk Doc&lt;/a&gt; to your native language&lt;/b&gt; &lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Caution] &lt;strong&gt;Misuse Disclaimer:&lt;/strong&gt; &lt;br /&gt; The developers of RustDesk do not condone or support any unethical or illegal use of this software. Misuse, such as unauthorized access, control or invasion of privacy, is strictly against our guidelines. The authors are not responsible for any misuse of the application.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Chat with us: &lt;a href="https://discord.gg/nDceKgxnkV"&gt;Discord&lt;/a&gt; | &lt;a href="https://twitter.com/rustdesk"&gt;Twitter&lt;/a&gt; | &lt;a href="https://www.reddit.com/r/rustdesk"&gt;Reddit&lt;/a&gt; | &lt;a href="https://www.youtube.com/@rustdesk"&gt;YouTube&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://rustdesk.com/pricing.html"&gt;&lt;img src="https://img.shields.io/badge/RustDesk%20Server%20Pro-Advanced%20Features-blue" alt="RustDesk Server Pro" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Yet another remote desktop solution, written in Rust. Works out of the box with no configuration required. You have full control of your data, with no concerns about security. You can use our rendezvous/relay server, &lt;a href="https://rustdesk.com/server"&gt;set up your own&lt;/a&gt;, or &lt;a href="https://github.com/rustdesk/rustdesk-server-demo"&gt;write your own rendezvous/relay server&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/71636191/171661982-430285f0-2e12-4b1d-9957-4a58e375304d.png" alt="image" /&gt;&lt;/p&gt; 
&lt;p&gt;RustDesk welcomes contribution from everyone. See &lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for help getting started.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/rustdesk/rustdesk/wiki/FAQ"&gt;&lt;strong&gt;FAQ&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/rustdesk/rustdesk/releases"&gt;&lt;strong&gt;BINARY DOWNLOAD&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/rustdesk/rustdesk/releases/tag/nightly"&gt;&lt;strong&gt;NIGHTLY BUILD&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://f-droid.org/en/packages/com.carriez.flutter_hbb"&gt;&lt;img src="https://f-droid.org/badge/get-it-on.png" alt="Get it on F-Droid" height="80" /&gt;&lt;/a&gt; &lt;a href="https://flathub.org/apps/com.rustdesk.RustDesk"&gt;&lt;img src="https://flathub.org/api/badge?svg&amp;amp;locale=en" alt="Get it on Flathub" height="80" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Dependencies&lt;/h2&gt; 
&lt;p&gt;Desktop versions use Flutter or Sciter (deprecated) for GUI, this tutorial is for Sciter only, since it is easier and more friendly to start. Check out our &lt;a href="https://github.com/rustdesk/rustdesk/raw/master/.github/workflows/flutter-build.yml"&gt;CI&lt;/a&gt; for building Flutter version.&lt;/p&gt; 
&lt;p&gt;Please download Sciter dynamic library yourself.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/c-smile/sciter-sdk/master/bin.win/x64/sciter.dll"&gt;Windows&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/c-smile/sciter-sdk/master/bin.lnx/x64/libsciter-gtk.so"&gt;Linux&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/c-smile/sciter-sdk/master/bin.osx/libsciter.dylib"&gt;macOS&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Raw Steps to build&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Prepare your Rust development env and C++ build env&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install &lt;a href="https://github.com/microsoft/vcpkg"&gt;vcpkg&lt;/a&gt;, and set &lt;code&gt;VCPKG_ROOT&lt;/code&gt; env variable correctly&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Windows: vcpkg install libvpx:x64-windows-static libyuv:x64-windows-static opus:x64-windows-static aom:x64-windows-static&lt;/li&gt; 
   &lt;li&gt;Linux/macOS: vcpkg install libvpx libyuv opus aom&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;run &lt;code&gt;cargo run&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;a href="https://rustdesk.com/docs/en/dev/build/"&gt;Build&lt;/a&gt;&lt;/h2&gt; 
&lt;h2&gt;How to Build on Linux&lt;/h2&gt; 
&lt;h3&gt;Ubuntu 18 (Debian 10)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;sudo apt install -y zip g++ gcc git curl wget nasm yasm libgtk-3-dev clang libxcb-randr0-dev libxdo-dev \
        libxfixes-dev libxcb-shape0-dev libxcb-xfixes0-dev libasound2-dev libpulse-dev cmake make \
        libclang-dev ninja-build libgstreamer1.0-dev libgstreamer-plugins-base1.0-dev libpam0g-dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;openSUSE Tumbleweed&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;sudo zypper install gcc-c++ git curl wget nasm yasm gcc gtk3-devel clang libxcb-devel libXfixes-devel cmake alsa-lib-devel gstreamer-devel gstreamer-plugins-base-devel xdotool-devel pam-devel
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Fedora 28 (CentOS 8)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;sudo yum -y install gcc-c++ git curl wget nasm yasm gcc gtk3-devel clang libxcb-devel libxdo-devel libXfixes-devel pulseaudio-libs-devel cmake alsa-lib-devel gstreamer1-devel gstreamer1-plugins-base-devel pam-devel
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Arch (Manjaro)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;sudo pacman -Syu --needed unzip git cmake gcc curl wget yasm nasm zip make pkg-config clang gtk3 xdotool libxcb libxfixes alsa-lib pipewire
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Install vcpkg&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;git clone https://github.com/microsoft/vcpkg
cd vcpkg
git checkout 2023.04.15
cd ..
vcpkg/bootstrap-vcpkg.sh
export VCPKG_ROOT=$HOME/vcpkg
vcpkg/vcpkg install libvpx libyuv opus aom
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Fix libvpx (For Fedora)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;cd vcpkg/buildtrees/libvpx/src
cd *
./configure
sed -i 's/CFLAGS+=-I/CFLAGS+=-fPIC -I/g' Makefile
sed -i 's/CXXFLAGS+=-I/CXXFLAGS+=-fPIC -I/g' Makefile
make
cp libvpx.a $HOME/vcpkg/installed/x64-linux/lib/
cd
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Build&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
source $HOME/.cargo/env
git clone --recurse-submodules https://github.com/rustdesk/rustdesk
cd rustdesk
mkdir -p target/debug
wget https://raw.githubusercontent.com/c-smile/sciter-sdk/master/bin.lnx/x64/libsciter-gtk.so
mv libsciter-gtk.so target/debug
VCPKG_ROOT=$HOME/vcpkg cargo run
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;How to build with Docker&lt;/h2&gt; 
&lt;p&gt;Begin by cloning the repository and building the Docker container:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;git clone https://github.com/rustdesk/rustdesk
cd rustdesk
git submodule update --init --recursive
docker build -t "rustdesk-builder" .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then, each time you need to build the application, run the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;docker run --rm -it -v $PWD:/home/user/rustdesk -v rustdesk-git-cache:/home/user/.cargo/git -v rustdesk-registry-cache:/home/user/.cargo/registry -e PUID="$(id -u)" -e PGID="$(id -g)" rustdesk-builder
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note that the first build may take longer before dependencies are cached, subsequent builds will be faster. Additionally, if you need to specify different arguments to the build command, you may do so at the end of the command in the &lt;code&gt;&amp;lt;OPTIONAL-ARGS&amp;gt;&lt;/code&gt; position. For instance, if you wanted to build an optimized release version, you would run the command above followed by &lt;code&gt;--release&lt;/code&gt;. The resulting executable will be available in the target folder on your system, and can be run with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;target/debug/rustdesk
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or, if you're running a release executable:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;target/release/rustdesk
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Please ensure that you run these commands from the root of the RustDesk repository, or the application may not find the required resources. Also note that other cargo subcommands such as &lt;code&gt;install&lt;/code&gt; or &lt;code&gt;run&lt;/code&gt; are not currently supported via this method as they would install or run the program inside the container instead of the host.&lt;/p&gt; 
&lt;h2&gt;File Structure&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rustdesk/rustdesk/tree/master/libs/hbb_common"&gt;libs/hbb_common&lt;/a&gt;&lt;/strong&gt;: video codec, config, tcp/udp wrapper, protobuf, fs functions for file transfer, and some other utility functions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rustdesk/rustdesk/tree/master/libs/scrap"&gt;libs/scrap&lt;/a&gt;&lt;/strong&gt;: screen capture&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rustdesk/rustdesk/tree/master/libs/enigo"&gt;libs/enigo&lt;/a&gt;&lt;/strong&gt;: platform specific keyboard/mouse control&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rustdesk/rustdesk/tree/master/libs/clipboard"&gt;libs/clipboard&lt;/a&gt;&lt;/strong&gt;: file copy and paste implementation for Windows, Linux, macOS.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rustdesk/rustdesk/tree/master/src/ui"&gt;src/ui&lt;/a&gt;&lt;/strong&gt;: obsolete Sciter UI (deprecated)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rustdesk/rustdesk/tree/master/src/server"&gt;src/server&lt;/a&gt;&lt;/strong&gt;: audio/clipboard/input/video services, and network connections&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rustdesk/rustdesk/tree/master/src/client.rs"&gt;src/client.rs&lt;/a&gt;&lt;/strong&gt;: start a peer connection&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rustdesk/rustdesk/tree/master/src/rendezvous_mediator.rs"&gt;src/rendezvous_mediator.rs&lt;/a&gt;&lt;/strong&gt;: Communicate with &lt;a href="https://github.com/rustdesk/rustdesk-server"&gt;rustdesk-server&lt;/a&gt;, wait for remote direct (TCP hole punching) or relayed connection&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rustdesk/rustdesk/tree/master/src/platform"&gt;src/platform&lt;/a&gt;&lt;/strong&gt;: platform specific code&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rustdesk/rustdesk/tree/master/flutter"&gt;flutter&lt;/a&gt;&lt;/strong&gt;: Flutter code for desktop and mobile&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rustdesk/rustdesk/tree/master/flutter/web/v1/js"&gt;flutter/web/js&lt;/a&gt;&lt;/strong&gt;: JavaScript for Flutter web client&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Screenshots&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://github.com/rustdesk/rustdesk/assets/28412477/db82d4e7-c4bc-4823-8e6f-6af7eadf7651" alt="Connection Manager" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/rustdesk/rustdesk/assets/28412477/9baa91e9-3362-4d06-aa1a-7518edcbd7ea" alt="Connected to a Windows PC" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/rustdesk/rustdesk/assets/28412477/39511ad3-aa9a-4f8c-8947-1cce286a46ad" alt="File Transfer" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/rustdesk/rustdesk/assets/28412477/78e8708f-e87e-4570-8373-1360033ea6c5" alt="TCP Tunneling" /&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>carthage-software/mago</title>
      <link>https://github.com/carthage-software/mago</link>
      <description>&lt;p&gt;Mago is a toolchain for PHP that aims to provide a set of tools to help developers write better code.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/carthage-software/mago/main/docs/public/assets/banner.svg?sanitize=true" alt="Mago Banner" width="600" /&gt; &lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;strong&gt;An extremely fast PHP linter, formatter, and static analyzer, written in Rust.&lt;/strong&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/carthage-software/mago/actions/workflows/ci.yml"&gt;&lt;img src="https://github.com/carthage-software/mago/actions/workflows/ci.yml/badge.svg?sanitize=true" alt="CI Status" /&gt;&lt;/a&gt; &lt;a href="https://github.com/carthage-software/mago/actions/workflows/cd.yml"&gt;&lt;img src="https://github.com/carthage-software/mago/actions/workflows/cd.yml/badge.svg?sanitize=true" alt="CD Status" /&gt;&lt;/a&gt; &lt;a href="https://crates.io/crates/mago"&gt;&lt;img src="https://img.shields.io/crates/v/mago.svg?sanitize=true" alt="Crates.io" /&gt;&lt;/a&gt; &lt;a href="https://packagist.org/packages/carthage-software/mago"&gt;&lt;img src="https://poser.pugx.org/carthage-software/mago/v" alt="Latest Stable Version for PHP" /&gt;&lt;/a&gt; &lt;a href="https://packagist.org/packages/carthage-software/mago"&gt;&lt;img src="https://poser.pugx.org/carthage-software/mago/v/unstable" alt="Latest Unstable Version for PHP" /&gt;&lt;/a&gt; &lt;a href="https://packagist.org/packages/carthage-software/mago"&gt;&lt;img src="http://poser.pugx.org/carthage-software/mago/downloads" alt="Total Composer Downloads" /&gt;&lt;/a&gt; &lt;a href="https://github.com/carthage-software/mago/raw/main/LICENSE-MIT"&gt;&lt;img src="https://img.shields.io/crates/l/mago.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Mago&lt;/strong&gt; is a comprehensive toolchain for PHP that helps developers write better code. Inspired by the Rust ecosystem, Mago brings speed, reliability, and an exceptional developer experience to PHP projects of all sizes.&lt;/p&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/carthage-software/mago/main/#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/carthage-software/mago/main/#getting-started"&gt;Getting Started&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/carthage-software/mago/main/#features"&gt;Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/carthage-software/mago/main/#our-sponsors"&gt;Our Sponsors&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/carthage-software/mago/main/#contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/carthage-software/mago/main/#inspiration--acknowledgements"&gt;Inspiration &amp;amp; Acknowledgements&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/carthage-software/mago/main/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;The most common way to install Mago on macOS and Linux is by using our shell script:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;curl --proto '=https' --tlsv1.2 -sSf https://carthage.software/mago.sh | bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For all other installation methods, including Homebrew, Composer, and Cargo, please refer to our official &lt;strong&gt;&lt;a href="https://mago.carthage.software/guide/installation"&gt;Installation Guide&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;To get started with Mago and learn how to configure your project, please visit our &lt;strong&gt;&lt;a href="https://mago.carthage.software/guide/getting-started"&gt;Getting Started Guide&lt;/a&gt;&lt;/strong&gt; in the official documentation.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;⚡️ Extremely Fast: Built in Rust for maximum performance.&lt;/li&gt; 
 &lt;li&gt;🔍 Lint: Identify issues in your codebase with customizable rules.&lt;/li&gt; 
 &lt;li&gt;🔬 Static Analysis: Perform deep analysis of your codebase to catch potential type errors and bugs.&lt;/li&gt; 
 &lt;li&gt;🛠️ Automated Fixes: Apply fixes for many lint issues automatically.&lt;/li&gt; 
 &lt;li&gt;📜 Formatting: Automatically format your code to adhere to best practices and style guides.&lt;/li&gt; 
 &lt;li&gt;🧠 Semantic Checks: Ensure code correctness with robust semantic analysis.&lt;/li&gt; 
 &lt;li&gt;🌳 AST Visualization: Explore your code’s structure with Abstract Syntax Tree (AST) parsing.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- START-SPONSORS --&gt; 
&lt;h2&gt;Our Sponsors&lt;/h2&gt; 
&lt;p align="center"&gt;&lt;a href="https://github.com/jasonrm" title="Jason R. McNeil"&gt;&lt;kbd&gt;&lt;img src="https://avatars.githubusercontent.com/u/39949?u=69c0e4fb08c439250978d41dbc3371d2f0609b98&amp;amp;v=4&amp;amp;s=160" width="80" height="80" alt="Jason R. McNeil" /&gt;&lt;/kbd&gt;&lt;/a&gt;&lt;a href="https://github.com/vvvinceocam" title="Vincent Berset"&gt;&lt;kbd&gt;&lt;img src="https://avatars.githubusercontent.com/u/5173120?u=95efc76cd8fc804536dc6dd25781a95b650bf902&amp;amp;v=4&amp;amp;s=160" width="80" height="80" alt="Vincent Berset" /&gt;&lt;/kbd&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p align="center"&gt;&lt;a href="https://github.com/TicketSwap" title="TicketSwap"&gt;&lt;kbd&gt;&lt;img src="https://avatars.githubusercontent.com/u/5766233?v=4&amp;amp;s=120" width="60" height="60" alt="TicketSwap" /&gt;&lt;/kbd&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/carthage-software/mago/main/SPONSORS.md"&gt;See all sponsors&lt;/a&gt;&lt;/p&gt; 
&lt;!-- END-SPONSORS --&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Mago is a community-driven project, and we welcome contributions! Whether you're reporting bugs, suggesting features, writing documentation, or submitting code, your help is valued.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;See our &lt;a href="https://raw.githubusercontent.com/carthage-software/mago/main/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt; to get started.&lt;/li&gt; 
 &lt;li&gt;Join the discussion on &lt;a href="https://discord.gg/mwyyjr27eu"&gt;Discord&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Inspiration &amp;amp; Acknowledgements&lt;/h2&gt; 
&lt;p&gt;Mago stands on the shoulders of giants. Our design and functionality are heavily inspired by pioneering tools in both the Rust and PHP ecosystems.&lt;/p&gt; 
&lt;h3&gt;Inspirations:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/rust-lang/rust-clippy"&gt;Clippy&lt;/a&gt;: For its comprehensive linting approach.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/oxc-project/oxc/"&gt;OXC&lt;/a&gt;: A major inspiration for building a high-performance toolchain in Rust.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/slackhq/hakana/"&gt;Hakana&lt;/a&gt;: For its deep static analysis capabilities.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Acknowledgements:&lt;/h3&gt; 
&lt;p&gt;We deeply respect the foundational work of tools like &lt;a href="https://github.com/PHP-CS-Fixer/PHP-CS-Fixer"&gt;PHP-CS-Fixer&lt;/a&gt;, &lt;a href="https://github.com/vimeo/psalm"&gt;Psalm&lt;/a&gt;, &lt;a href="https://github.com/phpstan/phpstan"&gt;PHPStan&lt;/a&gt;, and &lt;a href="https://github.com/squizlabs/PHP_CodeSniffer"&gt;PHP_CodeSniffer&lt;/a&gt;. While Mago aims to offer a unified and faster alternative, these tools paved the way for modern PHP development.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Mago is dual-licensed under your choice of the following:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;MIT License (&lt;a href="https://raw.githubusercontent.com/carthage-software/mago/main/LICENSE-MIT"&gt;LICENSE-MIT&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;Apache License, Version 2.0 (&lt;a href="https://raw.githubusercontent.com/carthage-software/mago/main/LICENSE-APACHE"&gt;LICENSE-APACHE&lt;/a&gt;)&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>ankitects/anki</title>
      <link>https://github.com/ankitects/anki</link>
      <description>&lt;p&gt;Anki is a smart spaced repetition flashcard program&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Anki®&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://buildkite.com/ankitects/anki-ci"&gt;&lt;img src="https://badge.buildkite.com/c9edf020a4aec976f9835e54751cc5409d843adbb66d043bd3.svg?branch=main" alt="Build status" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;This repo contains the source code for the computer version of &lt;a href="https://apps.ankiweb.net"&gt;Anki&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;About&lt;/h1&gt; 
&lt;p&gt;Anki is a spaced repetition program. Please see the &lt;a href="https://apps.ankiweb.net"&gt;website&lt;/a&gt; to learn more.&lt;/p&gt; 
&lt;h1&gt;Getting Started&lt;/h1&gt; 
&lt;h3&gt;Anki Betas&lt;/h3&gt; 
&lt;p&gt;If you'd like to try development builds of Anki but don't feel comfortable building the code, please see &lt;a href="https://betas.ankiweb.net/"&gt;Anki betas&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Developing&lt;/h3&gt; 
&lt;p&gt;For more information on building and developing, please see &lt;a href="https://raw.githubusercontent.com/ankitects/anki/main/docs/development.md"&gt;Development&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Contributing&lt;/h3&gt; 
&lt;p&gt;Want to contribute to Anki? Check out the &lt;a href="https://raw.githubusercontent.com/ankitects/anki/main/docs/contributing.md"&gt;Contribution Guidelines&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Anki Contributors&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/ankitects/anki/main/CONTRIBUTORS"&gt;CONTRIBUTORS&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;p&gt;Anki's license: &lt;a href="https://raw.githubusercontent.com/ankitects/anki/main/LICENSE"&gt;LICENSE&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>facebook/pyrefly</title>
      <link>https://github.com/facebook/pyrefly</link>
      <description>&lt;p&gt;A fast type checker and IDE for Python&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Pyrefly: A fast type checker and IDE for Python&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://pypi.python.org/pypi/pyrefly"&gt;&lt;img src="https://img.shields.io/pypi/v/pyrefly.svg?color=blue" alt="PyPI" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/Cf7mFQtW7W"&gt;&lt;img src="https://img.shields.io/badge/Discord-%235865F2.svg?logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://opensource.org/licenses/MIT"&gt;&lt;img src="https://img.shields.io/badge/License-MIT-blue.svg?sanitize=true" alt="License: MIT" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Currently under active development with known issues. Please open an issue if you find bugs.&lt;/p&gt; 
&lt;p&gt;Pyrefly is a fast type checker for Python. It's designed to replace the existing Pyre type checker at Meta by the end of 2025. This README describes basic usage. See the &lt;a href="https://pyrefly.org"&gt;Pyrefly website&lt;/a&gt; for full documentation and a tool for checking code.&lt;/p&gt; 
&lt;h3&gt;Getting Started&lt;/h3&gt; 
&lt;p&gt;Pyrefly aims to increase development velocity with IDE features and by checking your Python code.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Try out pyrefly in your browser: &lt;a href="https://pyrefly.org/sandbox/"&gt;Sandbox&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Get the command-line tool: &lt;code&gt;pip install pyrefly&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Get the VSCode extension: &lt;a href="https://marketplace.visualstudio.com/items?itemName=meta.pyrefly"&gt;Link&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Key Features:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Type Inference: Pyrefly infers types in most locations, apart from function parameters. It can infer types of variables and return types.&lt;/li&gt; 
 &lt;li&gt;Flow Types: Pyrefly can understand your program's control flow to refine static types.&lt;/li&gt; 
 &lt;li&gt;Incrementality: Pyrefly aims for large-scale incrementality at the module level, with optimized checking and parallelism.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Getting Involved&lt;/h2&gt; 
&lt;p&gt;If you have questions or would like to report a bug, please &lt;a href="https://github.com/facebook/pyrefly/issues"&gt;create an issue&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;See our &lt;a href="https://github.com/facebook/pyrefly/raw/main/CONTRIBUTING.md"&gt;contributing guide&lt;/a&gt; for information on how to contribute to Pyrefly.&lt;/p&gt; 
&lt;p&gt;Join our &lt;a href="https://discord.com/invite/Cf7mFQtW7W"&gt;Discord&lt;/a&gt; to chat about Pyrefly and types. This is also where we hold biweekly office hours.&lt;/p&gt; 
&lt;h2&gt;Choices&lt;/h2&gt; 
&lt;p&gt;There are a number of choices when writing a Python type checker. We are taking inspiration from &lt;a href="https://pyre-check.org/"&gt;Pyre1&lt;/a&gt;, &lt;a href="https://github.com/microsoft/pyright"&gt;Pyright&lt;/a&gt; and &lt;a href="https://mypy.readthedocs.io/en/stable/"&gt;MyPy&lt;/a&gt;. Some notable choices:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;We infer types in most locations, apart from parameters to functions. We do infer types of variables and return types. As an example, &lt;code&gt;def foo(x): return True&lt;/code&gt; would result in something equivalent to had you written &lt;code&gt;def foo(x: Any) -&amp;gt; bool: ...&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;We attempt to infer the type of &lt;code&gt;[]&lt;/code&gt; to however it is used first, then fix it after. For example &lt;code&gt;xs = []; xs.append(1); xs.append("")&lt;/code&gt; will infer that &lt;code&gt;xs: List[int]&lt;/code&gt; and then error on the final statement.&lt;/li&gt; 
 &lt;li&gt;We use flow types which refine static types, e.g. &lt;code&gt;x: int = 4&lt;/code&gt; will both know that &lt;code&gt;x&lt;/code&gt; has type &lt;code&gt;int&lt;/code&gt;, but also that the immediately next usage of &lt;code&gt;x&lt;/code&gt; will be aware the type is &lt;code&gt;Literal[4]&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;We aim for large-scale incrementality (at the module level) and optimized checking with parallelism, aiming to use the advantages of Rust to keep the code a bit simpler.&lt;/li&gt; 
 &lt;li&gt;We expect large strongly connected components of modules, and do not attempt to take advantage of a DAG-shape in the source code.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Code layout&lt;/h2&gt; 
&lt;p&gt;Pyrefly is split into a number of crates (mostly under &lt;code&gt;crates/&lt;/code&gt;):&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;pyrefly_util&lt;/code&gt; are general purpose utilities, which have nothing to do with Python or type checking. Examples include IO wrappers, locking, command line helpers etc.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;pyrefly_derive&lt;/code&gt; are proc-macros for deriving traits such as &lt;code&gt;TypeEq&lt;/code&gt; and &lt;code&gt;Visit&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;pyrefly_python&lt;/code&gt; are Python utilities with no type-checking aspects, such as modelling modules or &lt;code&gt;sys.info&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;pyrefly_bundled&lt;/code&gt; are the third-party &lt;a href="https://github.com/python/typeshed"&gt;typeshed stubs&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;pyrefly_config&lt;/code&gt; defines the Pyrefly configuration, along with support for reading Mypy/Pyright configuration.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;pyrefly_types&lt;/code&gt; defines the Pyrefly type along with operations on it.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;pyrefly_wasm&lt;/code&gt; defines the sandbox code that compiles to WASM.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;pyrefly&lt;/code&gt; itself is the type checker and everything else.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Design&lt;/h2&gt; 
&lt;p&gt;There are many nuances of design that change on a regular basis. But the basic substrate on which the checker is built involves three steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Figure out what each module exports. That requires solving all &lt;code&gt;import *&lt;/code&gt; statements transitively.&lt;/li&gt; 
 &lt;li&gt;For each module in isolation, convert it to bindings, dealing with all statements and scope information (both static and flow).&lt;/li&gt; 
 &lt;li&gt;Solve those bindings, which may require the solutions of bindings in other modules.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;If we encounter unknowable information (e.g. recursion) we use &lt;code&gt;Type::Var&lt;/code&gt; to insert placeholders which are filled in later.&lt;/p&gt; 
&lt;p&gt;For each module, we solve the steps sequentially and completely. In particular, we do not try and solve a specific identifier first (like &lt;a href="https://github.com/dotnet/roslyn"&gt;Roslyn&lt;/a&gt; or &lt;a href="https://www.typescriptlang.org/"&gt;TypeScript&lt;/a&gt;), and do not use fine-grained incrementality (like &lt;a href="https://github.com/rust-lang/rust-analyzer"&gt;Rust Analyzer&lt;/a&gt; using &lt;a href="https://github.com/salsa-rs/salsa"&gt;Salsa&lt;/a&gt;). Instead, we aim for raw performance and a simpler module-centric design - there's no need to solve a single binding in isolation if solving all bindings in a module is fast enough.&lt;/p&gt; 
&lt;h3&gt;Example of bindings&lt;/h3&gt; 
&lt;p&gt;Given the program:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;1: x: int = 4
2: print(x)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We might produce the bindings:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;define int@0&lt;/code&gt; = &lt;code&gt;from builtins import int&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;define x@1&lt;/code&gt; = &lt;code&gt;4: int@0&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;use x@2&lt;/code&gt; = &lt;code&gt;x@1&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;anon @2&lt;/code&gt; = &lt;code&gt;print(x@2)&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;export x&lt;/code&gt; = &lt;code&gt;x@2&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Of note:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;The keys are things like &lt;code&gt;define&lt;/code&gt; (the definition of something), &lt;code&gt;use&lt;/code&gt; (a usage of a thing) and &lt;code&gt;anon&lt;/code&gt; (a statement we need to type check, but don't care about the result of).&lt;/li&gt; 
 &lt;li&gt;In many cases the value of a key refers to other keys.&lt;/li&gt; 
 &lt;li&gt;Some keys are imported from other modules, via &lt;code&gt;export&lt;/code&gt; keys and &lt;code&gt;import&lt;/code&gt; values.&lt;/li&gt; 
 &lt;li&gt;In order to disambiguate identifiers we use the textual position at which they occur (in the example we've used &lt;code&gt;@line&lt;/code&gt;, but in reality it's the byte offset in the file).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Example of &lt;code&gt;Var&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;Given the program:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;1: x = 1
2: while test():
3:     x = x
4: print(x)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We end up with the bindings:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;x@1&lt;/code&gt; = &lt;code&gt;1&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;x@3&lt;/code&gt; = &lt;code&gt;phi(x@1, x@3)&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;x@4&lt;/code&gt; = &lt;code&gt;phi(x@1, x@3)&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The expression &lt;code&gt;phi&lt;/code&gt; is the join point of the two values, e.g. &lt;code&gt;phi(int, str)&lt;/code&gt; would be &lt;code&gt;int | str&lt;/code&gt;. We skip the distinction between &lt;code&gt;define&lt;/code&gt; and &lt;code&gt;use&lt;/code&gt;, since it is not necessary for this example.&lt;/p&gt; 
&lt;p&gt;When solving &lt;code&gt;x@3&lt;/code&gt; we encounter recursion. Operationally:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;We start solving &lt;code&gt;x@3&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;That requires us to solve &lt;code&gt;x@1&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;We solve &lt;code&gt;x@1&lt;/code&gt; to be &lt;code&gt;Literal[1]&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;We start solving &lt;code&gt;x@3&lt;/code&gt;. But we are currently solving &lt;code&gt;x@3&lt;/code&gt;, so we invent a fresh &lt;code&gt;Var&lt;/code&gt; (let's call it &lt;code&gt;?1&lt;/code&gt;) and return that.&lt;/li&gt; 
 &lt;li&gt;We conclude that &lt;code&gt;x@3&lt;/code&gt; must be &lt;code&gt;Literal[1] | ?1&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Since &lt;code&gt;?1&lt;/code&gt; was introduced by &lt;code&gt;x@3&lt;/code&gt; we record that &lt;code&gt;?1 = Literal[1] | ?1&lt;/code&gt;. We can take the upper reachable bound of that and conclude that &lt;code&gt;?1 = Literal[1]&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;We simplify &lt;code&gt;x@3&lt;/code&gt; to just &lt;code&gt;Literal[1]&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>openai/codex</title>
      <link>https://github.com/openai/codex</link>
      <description>&lt;p&gt;Lightweight coding agent that runs in your terminal&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt;OpenAI Codex CLI&lt;/h1&gt; 
&lt;p align="center"&gt;&lt;code&gt;npm i -g @openai/codex&lt;/code&gt;&lt;br /&gt;or &lt;code&gt;brew install codex&lt;/code&gt;&lt;/p&gt; 
&lt;p align="center"&gt;&lt;strong&gt;Codex CLI&lt;/strong&gt; is a coding agent from OpenAI that runs locally on your computer.&lt;br /&gt;If you are looking for the &lt;em&gt;cloud-based agent&lt;/em&gt; from OpenAI, &lt;strong&gt;Codex Web&lt;/strong&gt;, see &lt;a href="https://chatgpt.com/codex"&gt;chatgpt.com/codex&lt;/a&gt;.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/openai/codex/main/.github/codex-cli-splash.png" alt="Codex CLI splash" width="80%" /&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;h3&gt;Installing and running Codex CLI&lt;/h3&gt; 
&lt;p&gt;Install globally with your preferred package manager. If you use npm:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;npm install -g @openai/codex
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, if you use Homebrew:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;brew install codex
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then simply run &lt;code&gt;codex&lt;/code&gt; to get started:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;codex
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;You can also go to the &lt;a href="https://github.com/openai/codex/releases/latest"&gt;latest GitHub Release&lt;/a&gt; and download the appropriate binary for your platform.&lt;/summary&gt; 
 &lt;p&gt;Each GitHub Release contains many executables, but in practice, you likely want one of these:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;macOS 
   &lt;ul&gt; 
    &lt;li&gt;Apple Silicon/arm64: &lt;code&gt;codex-aarch64-apple-darwin.tar.gz&lt;/code&gt;&lt;/li&gt; 
    &lt;li&gt;x86_64 (older Mac hardware): &lt;code&gt;codex-x86_64-apple-darwin.tar.gz&lt;/code&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;Linux 
   &lt;ul&gt; 
    &lt;li&gt;x86_64: &lt;code&gt;codex-x86_64-unknown-linux-musl.tar.gz&lt;/code&gt;&lt;/li&gt; 
    &lt;li&gt;arm64: &lt;code&gt;codex-aarch64-unknown-linux-musl.tar.gz&lt;/code&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;Each archive contains a single entry with the platform baked into the name (e.g., &lt;code&gt;codex-x86_64-unknown-linux-musl&lt;/code&gt;), so you likely want to rename it to &lt;code&gt;codex&lt;/code&gt; after extracting it.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h3&gt;Using Codex with your ChatGPT plan&lt;/h3&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/openai/codex/main/.github/codex-cli-login.png" alt="Codex CLI login" width="80%" /&gt; &lt;/p&gt; 
&lt;p&gt;Run &lt;code&gt;codex&lt;/code&gt; and select &lt;strong&gt;Sign in with ChatGPT&lt;/strong&gt;. We recommend signing into your ChatGPT account to use Codex as part of your Plus, Pro, Team, Edu, or Enterprise plan. &lt;a href="https://help.openai.com/en/articles/11369540-codex-in-chatgpt"&gt;Learn more about what's included in your ChatGPT plan&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You can also use Codex with an API key, but this requires &lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/authentication.md#usage-based-billing-alternative-use-an-openai-api-key"&gt;additional setup&lt;/a&gt;. If you previously used an API key for usage-based billing, see the &lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/authentication.md#migrating-from-usage-based-billing-api-key"&gt;migration steps&lt;/a&gt;. If you're having trouble with login, please comment on &lt;a href="https://github.com/openai/codex/issues/1243"&gt;this issue&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Model Context Protocol (MCP)&lt;/h3&gt; 
&lt;p&gt;Codex CLI supports &lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/advanced.md#model-context-protocol-mcp"&gt;MCP servers&lt;/a&gt;. Enable by adding an &lt;code&gt;mcp_servers&lt;/code&gt; section to your &lt;code&gt;~/.codex/config.toml&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Configuration&lt;/h3&gt; 
&lt;p&gt;Codex CLI supports a rich set of configuration options, with preferences stored in &lt;code&gt;~/.codex/config.toml&lt;/code&gt;. For full configuration options, see &lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/config.md"&gt;Configuration&lt;/a&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Docs &amp;amp; FAQ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/getting-started.md"&gt;&lt;strong&gt;Getting started&lt;/strong&gt;&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/getting-started.md#cli-usage"&gt;CLI usage&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/getting-started.md#running-with-a-prompt-as-input"&gt;Running with a prompt as input&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/getting-started.md#example-prompts"&gt;Example prompts&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/getting-started.md#memory--project-docs"&gt;Memory with AGENTS.md&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/config.md"&gt;Configuration&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/sandbox.md"&gt;&lt;strong&gt;Sandbox &amp;amp; approvals&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/authentication.md"&gt;&lt;strong&gt;Authentication&lt;/strong&gt;&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/authentication.md#forcing-a-specific-auth-method-advanced"&gt;Auth methods&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/authentication.md#connecting-on-a-headless-machine"&gt;Login on a "Headless" machine&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/advanced.md"&gt;&lt;strong&gt;Advanced&lt;/strong&gt;&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/advanced.md#non-interactive--ci-mode"&gt;Non-interactive / CI mode&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/advanced.md#tracing--verbose-logging"&gt;Tracing / verbose logging&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/advanced.md#model-context-protocol-mcp"&gt;Model Context Protocol (MCP)&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/zdr.md"&gt;&lt;strong&gt;Zero data retention (ZDR)&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/contributing.md"&gt;&lt;strong&gt;Contributing&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/install.md"&gt;&lt;strong&gt;Install &amp;amp; build&lt;/strong&gt;&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/install.md#system-requirements"&gt;System Requirements&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/install.md#dotslash"&gt;DotSlash&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/install.md#build-from-source"&gt;Build from source&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/faq.md"&gt;&lt;strong&gt;FAQ&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/open-source-fund.md"&gt;&lt;strong&gt;Open source fund&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This repository is licensed under the &lt;a href="https://raw.githubusercontent.com/openai/codex/main/LICENSE"&gt;Apache-2.0 License&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>dbt-labs/dbt-fusion</title>
      <link>https://github.com/dbt-labs/dbt-fusion</link>
      <description>&lt;p&gt;The next-generation engine for dbt&lt;/p&gt;&lt;hr&gt;&lt;div style="text-align: center;"&gt; 
 &lt;img src="https://raw.githubusercontent.com/dbt-labs/dbt-fusion/main/assets/dbt-fusion-engine.png" alt="dbt Fusion Engine" width="400" style="border-radius: 6px;" /&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h1&gt;dbt F✦SION engine (BETA)&lt;/h1&gt; 
&lt;p&gt;This repo hosts components of the dbt Fusion engine, the foundation for future innovation in &lt;code&gt;dbt&lt;/code&gt;. The dbt Fusion engine is written in Rust and is designed for speed, correctness, and has a native understanding of SQL across multiple data warehouse SQL dialects.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT]&lt;br /&gt; &lt;strong&gt;Note: the dbt Fusion Engine is in Beta!&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Bugs and missing functionality compared to dbt Core will be resolved continuously in the leadup to a final release (more info: &lt;a href="https://docs.getdbt.com/blog/dbt-fusion-engine-path-to-ga"&gt;the dbt Fusion Engine: the Path to GA&lt;/a&gt;).&lt;/p&gt; 
&lt;p&gt;The dbt Fusion engine is a ground up, first principles rewrite of the dbt Core execution engine, built to be interoperable with the standard dbt authoring layer. Fusion enforces some ambiguous areas of the authoring spec more strictly than dbt Core to ensure correctness (for example, dbt Core does not proactively validate most YAML configurations). Many of these discrepancies can be fixed automatically with the &lt;a href="https://github.com/dbt-labs/dbt-autofix"&gt;dbt Autofix&lt;/a&gt; tool.&lt;/p&gt; 
&lt;p&gt;Beyond conformance with dbt Core, Fusion also contains new SQL Comprehension capabilities, a language server, modern ADBC drivers for warehouse connections, and more. While dbt Core was written in Python, the dbt Fusion engine is written in Rust, and compiled to a single application binary.&lt;/p&gt; 
&lt;p&gt;You can install dbt-fusion onto your local machine, a docker container, or a machine in the cloud. It is designed for flexible installation, with no dependencies on other libraries. The only libraries that dbt Fusion will load are it's corresponding database drivers.&lt;/p&gt; 
&lt;p&gt;The dbt Fusion engine is being released to this repository incrementally, so, until this note is removed this repository contains only a subset of the crates that make the core of the engine work. These crates are published incrementally starting on May 28.&lt;/p&gt; 
&lt;h2&gt;Getting Started with the dbt Fusion engine&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP]&lt;br /&gt; You don't have to build this project from source to use the new dbt! We recommend using the precompiled binary with additional capabilities:&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;There are several ways to get started with Fusion (for more, see the &lt;a href="https://docs.getdbt.com/guides/fusion?step=1"&gt;Quickstart for the dbt Fusion engine&lt;/a&gt;)&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Download dbt the vs-code extension&lt;/strong&gt; - For most people the best experience. This will install the dbt fusion CLI and Language Server on your system - see the docs page: &lt;a href="https://docs.getdbt.com/docs/install-dbt-extension"&gt;Install the dbt VS Code extension&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Install Fusion Directly&lt;/strong&gt; Install just the fusion CLI with the command below or see dbt's documentation: &lt;a href="https://docs.getdbt.com/docs/fusion/install-fusion"&gt;About Fusion installation&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -fsSL https://public.cdn.getdbt.com/fs/install/install.sh | sh -s -- --update
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;&lt;strong&gt;Build Fusion from Source&lt;/strong&gt; - See the below section: &lt;a href="https://raw.githubusercontent.com/dbt-labs/dbt-fusion/main/#compiling-from-source"&gt;Compiling from Source&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Supported Operating Systems and CPU Microarchitectures&lt;/h3&gt; 
&lt;p&gt;Fusion &amp;amp; associated drivers are compiled for each CPU microarchitecture and operating system independently. This allows for hardware level optimization.&lt;/p&gt; 
&lt;p&gt;Legend:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;🟢 - Supported today&lt;/li&gt; 
 &lt;li&gt;🟡 - Unsupported today&lt;/li&gt; 
&lt;/ul&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Operating System&lt;/th&gt; 
   &lt;th&gt;X86-64&lt;/th&gt; 
   &lt;th&gt;ARM&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MacOS&lt;/td&gt; 
   &lt;td&gt;🟢&lt;/td&gt; 
   &lt;td&gt;🟢&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Linux&lt;/td&gt; 
   &lt;td&gt;🟢&lt;/td&gt; 
   &lt;td&gt;🟢&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Windows&lt;/td&gt; 
   &lt;td&gt;🟢&lt;/td&gt; 
   &lt;td&gt;🟡&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Timeline&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Target Date&lt;/th&gt; 
   &lt;th&gt;Milestone&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2025-05-28&lt;/td&gt; 
   &lt;td&gt;Initial release of Fusion&lt;/td&gt; 
   &lt;td&gt;Published source code of parser, schemas, dbt-jinja, and Snowflake ADBC driver.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2025-06-09&lt;/td&gt; 
   &lt;td&gt;Databricks Adapter release&lt;/td&gt; 
   &lt;td&gt;Databricks ADBC driver, and adapter for Fusion&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2025-06-30&lt;/td&gt; 
   &lt;td&gt;BigQuery Adapter release&lt;/td&gt; 
   &lt;td&gt;BigQuery ADBC driver, and adapter for Fusion&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2025-07-31&lt;/td&gt; 
   &lt;td&gt;Redshift Adapter release&lt;/td&gt; 
   &lt;td&gt;Redshift ADBC driver, and adapter for Fusion&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2025-08-30&lt;/td&gt; 
   &lt;td&gt;ANTLR Grammars release + SQL Parser&lt;/td&gt; 
   &lt;td&gt;The SQL grammar used by the ANTLR parser generator.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Top Level Components Released to Date&lt;/h3&gt; 
&lt;p&gt;Releases of various Fusion components will be iterative as each component reaches maturity &amp;amp; readiness for contribution.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;code&gt;dbt-jinja&lt;/code&gt; - All Rust extension of mini-jinja to support dbt's jinja functions &amp;amp; other capabilities&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;code&gt;dbt-parser&lt;/code&gt; - Rust parser for dbt projects&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;code&gt;dbt-snowflake&lt;/code&gt; - database driver&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;code&gt;dbt-schemas&lt;/code&gt; - complete, correct, machine generated json schemas for dbt's authoring surface&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;code&gt;dbt-sql&lt;/code&gt; - ANTLR grammars and generated parsers 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" disabled /&gt; snowflake.g4&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" disabled /&gt; bigquery.g4&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" disabled /&gt; redshift.g4&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" disabled /&gt; databricks.g4&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Fusion: the comprehensive dbt fusion engine release.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;FAQ&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;i&gt;Can I contribute to the dbt Fusion engine?&lt;/i&gt;&lt;/summary&gt; 
 &lt;p&gt;Yes absolutely!. Please see &lt;a href="https://raw.githubusercontent.com/dbt-labs/dbt-fusion/main/CONTRIBUTING.md"&gt;&lt;code&gt;CONTRIBUTING.md&lt;/code&gt;&lt;/a&gt; for contribution guidelines&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;i&gt;How is dbt Fusion different from dbt Core?&lt;/i&gt;&lt;/summary&gt; The dbt Fusion engine is a ground-up rewrite of dbt Core, with many additional capabilities. *Things that are the same:* * The YML authoring format including profiles, configuration, seeds, data tests, and unit tests * The materialization libraries * dbt's library managemenet system (although `dbt deps` are installed automatically) 
 &lt;p&gt;&lt;em&gt;Additional capabilities provided by Fusion:&lt;/em&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;All new Arrow Database Connector (ADBC) drivers for faster data transfers and unified connection handling&lt;/li&gt; 
  &lt;li&gt;A language server and corresponding VS-Code extension (compatible with Cursor) for ease of development&lt;/li&gt; 
  &lt;li&gt;Multi-dialect SQL compilation, validation, &amp;amp; static analysis&lt;/li&gt; 
  &lt;li&gt;Standalone distribution. No JVM, or python required.&lt;/li&gt; 
  &lt;li&gt;Automatic installation of dependencies, whether that's a dbt package, or database driver&lt;/li&gt; 
  &lt;li&gt;dbt code-signed &amp;amp; secure distributions&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;i&gt;This repo doesn't have all of dbt's functionality, when will the rest come?&lt;/i&gt;&lt;/summary&gt; dbt Fusion's source code is being published as components are finalized. Please see the above section: [Timeline](#timeline). 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;i&gt;Can I use dbt Fusion today?&lt;/i&gt;&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;State&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;Workaround&lt;/th&gt; 
    &lt;th&gt;Resolvable by&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Unblocked&lt;/td&gt; 
    &lt;td&gt;You can adopt the dbt Fusion engine with no changes to your project&lt;/td&gt; 
    &lt;td&gt;---&lt;/td&gt; 
    &lt;td&gt;---&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Soft blocked&lt;/td&gt; 
    &lt;td&gt;Your project contains functionality (for more info: &lt;a href="https://www.getdbt.com/blog/how-to-get-ready-for-the-new-dbt-engine"&gt;How to get ready for the new dbt engine&lt;/a&gt;.&lt;/td&gt; 
    &lt;td&gt;Resolve deprecations with the dbt-autofix script or workflow in dbt Studio&lt;/td&gt; 
    &lt;td&gt;Users&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Hard blocked&lt;/td&gt; 
    &lt;td&gt;Your project contains Python models or uses a not-yet-supported adapter&lt;/td&gt; 
    &lt;td&gt;Remove unsupported functionality if possible&lt;/td&gt; 
    &lt;td&gt;dbt Labs&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h2&gt;Compiling from Source&lt;/h2&gt; 
 &lt;p&gt;The primary CLI in this repository is the &lt;code&gt;dbt-sa-cli&lt;/code&gt;. To compile the CLI, you need the Rust toolchain.&lt;/p&gt; 
 &lt;p&gt;Let's start with Rust, run the following command to install Rust on your machine:&lt;/p&gt; 
 &lt;p&gt;Linux:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;sudo ./scripts/setup_dev_env_linux.sh
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Mac:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Great! We have Rust installed. To confirm, run the following command:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;cargo -v
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;You should see a printout like:&lt;/p&gt; 
 &lt;pre&gt;&lt;code&gt;Rust's package manager

Usage: cargo [+toolchain] [OPTIONS] [COMMAND]
       cargo [+toolchain] [OPTIONS] -Zscript &amp;lt;MANIFEST_RS&amp;gt; [ARGS]...

Options:
  -V, --version                  Print version info and exit
...
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Cargo is Rust's build system and package manager. If you're familiar with Python, pip would be a sufficient comparison. We'll use cargo to run command to build the local &lt;code&gt;dbt-sa-cli&lt;/code&gt; binary and run helper scripts via &lt;code&gt;cargo xtask&lt;/code&gt;. More on that later.&lt;/p&gt; 
 &lt;p&gt;To build the binary locally, &lt;code&gt;cd&lt;/code&gt; to the this repo's directory and run:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;cargo build
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;This will compile our Rust code into the &lt;code&gt;dbt-sa-cli&lt;/code&gt; binary. After this completes, you should see a new executable in &lt;code&gt;target/debug/dbt-sa-cli&lt;/code&gt;. You can run this executable by passing the path directly into the CLI, so if you're in the root of this git repo, you can run:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;target/debug/dbt-sa-cli
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;If built correctly, you should see output like:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;&amp;gt; ./target/debug/dbt
Usage: dbt &amp;lt;COMMAND&amp;gt;

Commands:
  parse    Parse models
  ...
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;You might be wondering why it was built into the &lt;code&gt;debug&lt;/code&gt; directory - this is because our default profile is &lt;code&gt;debug&lt;/code&gt; when running &lt;code&gt;cargo build&lt;/code&gt;. Our &lt;code&gt;debug&lt;/code&gt; profile compiles the code faster, but sacrifices optimizations to do so. Therefore, if you want to benchmark the parser, build with the flag &lt;code&gt;cargo build --release&lt;/code&gt;. The compile will take longer, but the build will mimic the experience of the end user.&lt;/p&gt; 
 &lt;p&gt;If you expect to use this executable often, we recommend creating an alias for it in your &lt;code&gt;~/.zshrc&lt;/code&gt;. To do so, start by getting the absolute path to the executable with:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;cd target/debug &amp;amp;&amp;amp; pwd
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h2&gt;Running Tests&lt;/h2&gt; 
 &lt;p&gt;To run tests, increase the stack size and use nextest.&lt;/p&gt; 
 &lt;pre&gt;&lt;code&gt; RUST_MIN_STACK=8388608 cargo nextest run --no-fail-fast
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h1&gt;License&lt;/h1&gt; 
 &lt;p&gt;The dbt Fusion engine is a monorepo and contains more than one License. Most code is licensed under ELv2. For more, please see &lt;a href="https://raw.githubusercontent.com/dbt-labs/dbt-fusion/main/LICENSES.md"&gt;&lt;code&gt;LICENSES.md&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
 &lt;h1&gt;Acknowledgments&lt;/h1&gt; 
 &lt;p&gt;&lt;em&gt;To the dbt community:&lt;/em&gt; dbt the tool &amp;amp; dbt Labs the company would not be here without the incredible community of authors, contributors, practitioners, and enthusiasts. dbt Fusion is an evolution of that work &amp;amp; stands on the shoulders of what has come before.&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;To the Arrow Community:&lt;/em&gt; dbt Labs is committing fully to the Arrow ecosystem. Fusion exclusively uses the Arrow type system from drivers through adapters into the internals of the compiler &amp;amp; runtime.&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;To the DataFusion Community:&lt;/em&gt; The intermediate representation of the SQL compiler is the DataFusion logical plan which has proven to be pragmatic, extensible, and easy to work with in all the right ways.&lt;/p&gt; 
 &lt;p&gt;Thank you all. dbt, Arrow, and DataFusion have become truly global software projects. dbt Labs is committed to contributing meaningfully to these efforts over the coming months and years.&lt;/p&gt; 
&lt;/details&gt;</description>
    </item>
    
    <item>
      <title>AppFlowy-IO/AppFlowy-Cloud</title>
      <link>https://github.com/AppFlowy-IO/AppFlowy-Cloud</link>
      <description>&lt;p&gt;Bring projects, wikis, and teams together with AI. AppFlowy is the AI collaborative workspace where you achieve more without losing control of your data. The leading open source Notion alternative.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; 
 &lt;picture&gt; 
  &lt;source srcset="assets/logos/appflowy_logo_white.svg" media="(prefers-color-scheme: dark)" /&gt; 
  &lt;img src="https://raw.githubusercontent.com/AppFlowy-IO/AppFlowy-Cloud/main/assets/logos/appflowy_logo_black.svg?sanitize=true" width="500" height="200" /&gt; 
 &lt;/picture&gt; &lt;/p&gt; 
&lt;h4 align="center"&gt; &lt;a href="https://discord.gg/9Q2xaN37tV"&gt;&lt;img src="https://img.shields.io/badge/AppFlowy.IO-discord-orange" /&gt;&lt;/a&gt; &lt;a href="https://opensource.org/licenses/AGPL-3.0"&gt;&lt;img src="https://img.shields.io/badge/license-AGPL-purple.svg?sanitize=true" alt="License: AGPL" /&gt;&lt;/a&gt; &lt;/h4&gt; 
&lt;p align="center"&gt; &lt;a href="https://www.appflowy.com"&gt;&lt;b&gt;Website&lt;/b&gt;&lt;/a&gt; • &lt;a href="https://twitter.com/appflowy"&gt;&lt;b&gt;Twitter&lt;/b&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt;⚡ The AppFlowy Cloud written with Rust 🦀&lt;/p&gt; 
&lt;h1&gt;AppFlowy Cloud&lt;/h1&gt; 
&lt;p&gt;AppFlowy Cloud is part of the AppFlowy ecosystem, offering secure user authentication, file storage, and real-time WebSocket communication for an efficient and collaborative user experience.&lt;/p&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AppFlowy-IO/AppFlowy-Cloud/main/#deployment"&gt;🚀 Deployment&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AppFlowy-IO/AppFlowy-Cloud/main/#development"&gt;💻 Development&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AppFlowy-IO/AppFlowy-Cloud/main/#debugging"&gt;🐞 Debugging&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/AppFlowy-IO/AppFlowy-Cloud/main/#%EF%B8%8Fcontributing"&gt;⚙️ Contributing&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🚀Deployment&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;See &lt;a href="https://raw.githubusercontent.com/AppFlowy-IO/AppFlowy-Cloud/main/doc/DEPLOYMENT.md"&gt;deployment guide&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;💻Development&lt;/h2&gt; 
&lt;h3&gt;Pre-requisites&lt;/h3&gt; 
&lt;p&gt;You'll need to install:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.rust-lang.org/tools/install"&gt;Rust&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.docker.com/get-docker/"&gt;Docker&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Environment Configuration&lt;/h3&gt; 
&lt;p&gt;To get started, you need to set up your environment variables. We've made this easy with an interactive script:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;./script/generate_env.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The script will ask you to choose between development (&lt;code&gt;dev.env&lt;/code&gt;) or production (&lt;code&gt;deploy.env&lt;/code&gt;) settings, then generate a &lt;code&gt;.env&lt;/code&gt; file for you. If you have sensitive values like API keys, you can put them in environment-specific secret files and the script will safely merge them in.&lt;/p&gt; 
&lt;h4&gt;Quick Setup with Secrets (Recommended)&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;You don't need to understand all the environment variables.&lt;/strong&gt; For most development setups, simply:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Copy the development secrets template:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cp env.dev.secret.example .env.dev.secret
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Edit &lt;code&gt;.env.dev.secret&lt;/code&gt; and fill in only the values you need (like API keys, passwords, etc.)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run the generator:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;./script/generate_env.sh
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;The script will automatically use your secrets file and generate a complete &lt;code&gt;.env&lt;/code&gt; with sensible defaults for everything else.&lt;/p&gt; 
&lt;h4&gt;Manual Setup&lt;/h4&gt; 
&lt;p&gt;If you prefer doing it manually, just copy one of the template files:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cp dev.env .env    # for development
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then edit the &lt;code&gt;.env&lt;/code&gt; file with your specific settings. &lt;strong&gt;Choose ONE of the following commands&lt;/strong&gt; to start the AppFlowy Cloud server locally(make sure you are in the root directory of the project):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# For new setup - RECOMMENDED FOR FIRST TIME
./script/run_local_server.sh --reset

# Basic run (interactive prompts for container management)
./script/run_local_server.sh

# With SQLx metadata preparation (useful for clean builds)
./script/run_local_server.sh --sqlx

# Combined: reset database and prepare SQLx metadata
./script/run_local_server.sh --reset --sqlx
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Interactive Features:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Prompts before stopping existing containers (data is preserved)&lt;/li&gt; 
 &lt;li&gt;Automatically checks for sqlx-cli and offers to install if missing&lt;/li&gt; 
 &lt;li&gt;Color-coded output for better visibility&lt;/li&gt; 
 &lt;li&gt;Clear warnings about data-affecting operations&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Command Line Flags:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--sqlx&lt;/code&gt;: Prepare SQLx metadata (takes a few minutes, required for clean builds)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--reset&lt;/code&gt;: Reset database schema and data (no prompt)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Environment Variables:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;SKIP_SQLX_PREPARE=true&lt;/code&gt;: Skip SQLx preparation (faster restarts)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;SKIP_APPFLOWY_CLOUD=true&lt;/code&gt;: Skip AppFlowy Cloud build&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;SQLX_OFFLINE=false&lt;/code&gt;: Connect to DB during build (default: true)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This process will execute all dependencies and start the AppFlowy-Cloud server with an interactive setup experience.&lt;/p&gt; 
&lt;h3&gt;Manual Setup (Step-by-Step)&lt;/h3&gt; 
&lt;p&gt;If you cannot run the &lt;code&gt;run_local_server.sh&lt;/code&gt; script, follow these manual steps:&lt;/p&gt; 
&lt;h4&gt;1. Prerequisites Check&lt;/h4&gt; 
&lt;p&gt;Ensure you have installed:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.rust-lang.org/tools/install"&gt;Rust&lt;/a&gt; and Cargo toolchain&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.docker.com/get-docker/"&gt;Docker&lt;/a&gt; and Docker Compose&lt;/li&gt; 
 &lt;li&gt;PostgreSQL client (psql)&lt;/li&gt; 
 &lt;li&gt;sqlx-cli: &lt;code&gt;cargo install sqlx-cli&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;2. Configuration Setup&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Copy the configuration template
cp dev.env .env

# Edit the .env file as required (such as SMTP configurations)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;3. Start Docker Services&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Set environment variables
export GOTRUE_MAILER_AUTOCONFIRM=true
export GOTRUE_EXTERNAL_GOOGLE_ENABLED=true

# Start Docker Compose services
docker compose --file ./docker-compose-dev.yml up -d --build
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;4. Wait for Services to Start&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Wait for PostgreSQL to be ready (adjust connection details as needed)
# Keep trying until connection succeeds
PGPASSWORD="password" psql -h "localhost" -U "postgres" -p "5432" -d "postgres" -c '\q'

# Wait for AppFlowy Cloud health check
# Keep trying until health endpoint responds
curl localhost:9999/health
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;5. Database Setup (Optional)&lt;/h4&gt; 
&lt;p&gt;If you need to reset/setup the database:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Generate protobuf files for collab-rt-entity crate
./script/code_gen.sh

# Create database and run migrations
cargo sqlx database create
cargo sqlx migrate run
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;6. SQLx Preparation (Optional)&lt;/h4&gt; 
&lt;p&gt;If you need to prepare SQLx metadata:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Prepare SQLx metadata (takes a few minutes)
cargo sqlx prepare --workspace
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;7. Build AppFlowy Cloud&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Build and run AppFlowy Cloud
cargo run --package xtask
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;⚙️Contributing&lt;/h2&gt; 
&lt;p&gt;Any new contribution is more than welcome in this project! If you want to know more about the development workflow or want to contribute, please visit our &lt;a href="https://raw.githubusercontent.com/AppFlowy-IO/AppFlowy-Cloud/main/doc/CONTRIBUTING.md"&gt;contributing guidelines&lt;/a&gt; for detailed instructions!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>pola-rs/polars</title>
      <link>https://github.com/pola-rs/polars</link>
      <description>&lt;p&gt;Dataframes powered by a multithreaded, vectorized query engine, written in Rust&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt; &lt;a href="https://pola.rs"&gt; &lt;img src="https://raw.githubusercontent.com/pola-rs/polars-static/master/banner/polars_github_banner.svg?sanitize=true" alt="Polars logo" /&gt; &lt;/a&gt; &lt;/h1&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://crates.io/crates/polars"&gt; &lt;img src="https://img.shields.io/crates/v/polars.svg?sanitize=true" alt="crates.io Latest Release" /&gt; &lt;/a&gt; 
 &lt;a href="https://pypi.org/project/polars/"&gt; &lt;img src="https://img.shields.io/pypi/v/polars.svg?sanitize=true" alt="PyPi Latest Release" /&gt; &lt;/a&gt; 
 &lt;a href="https://www.npmjs.com/package/nodejs-polars"&gt; &lt;img src="https://img.shields.io/npm/v/nodejs-polars.svg?sanitize=true" alt="NPM Latest Release" /&gt; &lt;/a&gt; 
 &lt;a href="https://community.r-multiverse.org/polars"&gt; &lt;img src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fcommunity.r-multiverse.org%2Fapi%2Fpackages%2Fpolars&amp;amp;query=%24.Version&amp;amp;label=r-multiverse" alt="R-multiverse Latest Release" /&gt; &lt;/a&gt; 
 &lt;a href="https://doi.org/10.5281/zenodo.7697217"&gt; &lt;img src="https://zenodo.org/badge/DOI/10.5281/zenodo.7697217.svg?sanitize=true" alt="DOI Latest Release" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;p align="center"&gt; &lt;b&gt;Documentation&lt;/b&gt;: &lt;a href="https://docs.pola.rs/api/python/stable/reference/index.html"&gt;Python&lt;/a&gt; - &lt;a href="https://docs.rs/polars/latest/polars/"&gt;Rust&lt;/a&gt; - &lt;a href="https://pola-rs.github.io/nodejs-polars/index.html"&gt;Node.js&lt;/a&gt; - &lt;a href="https://pola-rs.github.io/r-polars/index.html"&gt;R&lt;/a&gt; | &lt;b&gt;StackOverflow&lt;/b&gt;: &lt;a href="https://stackoverflow.com/questions/tagged/python-polars"&gt;Python&lt;/a&gt; - &lt;a href="https://stackoverflow.com/questions/tagged/rust-polars"&gt;Rust&lt;/a&gt; - &lt;a href="https://stackoverflow.com/questions/tagged/nodejs-polars"&gt;Node.js&lt;/a&gt; - &lt;a href="https://stackoverflow.com/questions/tagged/r-polars"&gt;R&lt;/a&gt; | &lt;a href="https://docs.pola.rs/"&gt;User guide&lt;/a&gt; | &lt;a href="https://discord.gg/4UfP5cfBE7"&gt;Discord&lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;Polars: Blazingly fast DataFrames in Rust, Python, Node.js, R, and SQL&lt;/h2&gt; 
&lt;p&gt;Polars is a DataFrame interface on top of an OLAP Query Engine implemented in Rust using &lt;a href="https://arrow.apache.org/docs/format/Columnar.html"&gt;Apache Arrow Columnar Format&lt;/a&gt; as the memory model.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Lazy | eager execution&lt;/li&gt; 
 &lt;li&gt;Multi-threaded&lt;/li&gt; 
 &lt;li&gt;SIMD&lt;/li&gt; 
 &lt;li&gt;Query optimization&lt;/li&gt; 
 &lt;li&gt;Powerful expression API&lt;/li&gt; 
 &lt;li&gt;Hybrid Streaming (larger-than-RAM datasets)&lt;/li&gt; 
 &lt;li&gt;Rust | Python | NodeJS | R | ...&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To learn more, read the &lt;a href="https://docs.pola.rs/"&gt;user guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Python&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;&amp;gt;&amp;gt;&amp;gt; import polars as pl
&amp;gt;&amp;gt;&amp;gt; df = pl.DataFrame(
...     {
...         "A": [1, 2, 3, 4, 5],
...         "fruits": ["banana", "banana", "apple", "apple", "banana"],
...         "B": [5, 4, 3, 2, 1],
...         "cars": ["beetle", "audi", "beetle", "beetle", "beetle"],
...     }
... )

# embarrassingly parallel execution &amp;amp; very expressive query language
&amp;gt;&amp;gt;&amp;gt; df.sort("fruits").select(
...     "fruits",
...     "cars",
...     pl.lit("fruits").alias("literal_string_fruits"),
...     pl.col("B").filter(pl.col("cars") == "beetle").sum(),
...     pl.col("A").filter(pl.col("B") &amp;gt; 2).sum().over("cars").alias("sum_A_by_cars"),
...     pl.col("A").sum().over("fruits").alias("sum_A_by_fruits"),
...     pl.col("A").reverse().over("fruits").alias("rev_A_by_fruits"),
...     pl.col("A").sort_by("B").over("fruits").alias("sort_A_by_B_by_fruits"),
... )
shape: (5, 8)
┌──────────┬──────────┬──────────────┬─────┬─────────────┬─────────────┬─────────────┬─────────────┐
│ fruits   ┆ cars     ┆ literal_stri ┆ B   ┆ sum_A_by_ca ┆ sum_A_by_fr ┆ rev_A_by_fr ┆ sort_A_by_B │
│ ---      ┆ ---      ┆ ng_fruits    ┆ --- ┆ rs          ┆ uits        ┆ uits        ┆ _by_fruits  │
│ str      ┆ str      ┆ ---          ┆ i64 ┆ ---         ┆ ---         ┆ ---         ┆ ---         │
│          ┆          ┆ str          ┆     ┆ i64         ┆ i64         ┆ i64         ┆ i64         │
╞══════════╪══════════╪══════════════╪═════╪═════════════╪═════════════╪═════════════╪═════════════╡
│ "apple"  ┆ "beetle" ┆ "fruits"     ┆ 11  ┆ 4           ┆ 7           ┆ 4           ┆ 4           │
│ "apple"  ┆ "beetle" ┆ "fruits"     ┆ 11  ┆ 4           ┆ 7           ┆ 3           ┆ 3           │
│ "banana" ┆ "beetle" ┆ "fruits"     ┆ 11  ┆ 4           ┆ 8           ┆ 5           ┆ 5           │
│ "banana" ┆ "audi"   ┆ "fruits"     ┆ 11  ┆ 2           ┆ 8           ┆ 2           ┆ 2           │
│ "banana" ┆ "beetle" ┆ "fruits"     ┆ 11  ┆ 4           ┆ 8           ┆ 1           ┆ 1           │
└──────────┴──────────┴──────────────┴─────┴─────────────┴─────────────┴─────────────┴─────────────┘
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;SQL&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;&amp;gt;&amp;gt;&amp;gt; df = pl.scan_csv("docs/assets/data/iris.csv")
&amp;gt;&amp;gt;&amp;gt; ## OPTION 1
&amp;gt;&amp;gt;&amp;gt; # run SQL queries on frame-level
&amp;gt;&amp;gt;&amp;gt; df.sql("""
...	SELECT species,
...	  AVG(sepal_length) AS avg_sepal_length
...	FROM self
...	GROUP BY species
...	""").collect()
shape: (3, 2)
┌────────────┬──────────────────┐
│ species    ┆ avg_sepal_length │
│ ---        ┆ ---              │
│ str        ┆ f64              │
╞════════════╪══════════════════╡
│ Virginica  ┆ 6.588            │
│ Versicolor ┆ 5.936            │
│ Setosa     ┆ 5.006            │
└────────────┴──────────────────┘
&amp;gt;&amp;gt;&amp;gt; ## OPTION 2
&amp;gt;&amp;gt;&amp;gt; # use pl.sql() to operate on the global context
&amp;gt;&amp;gt;&amp;gt; df2 = pl.LazyFrame({
...    "species": ["Setosa", "Versicolor", "Virginica"],
...    "blooming_season": ["Spring", "Summer", "Fall"]
...})
&amp;gt;&amp;gt;&amp;gt; pl.sql("""
... SELECT df.species,
...     AVG(df.sepal_length) AS avg_sepal_length,
...     df2.blooming_season
... FROM df
... LEFT JOIN df2 ON df.species = df2.species
... GROUP BY df.species, df2.blooming_season
... """).collect()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;SQL commands can also be run directly from your terminal using the Polars CLI:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# run an inline SQL query
&amp;gt; polars -c "SELECT species, AVG(sepal_length) AS avg_sepal_length, AVG(sepal_width) AS avg_sepal_width FROM read_csv('docs/assets/data/iris.csv') GROUP BY species;"

# run interactively
&amp;gt; polars
Polars CLI v0.3.0
Type .help for help.

&amp;gt; SELECT species, AVG(sepal_length) AS avg_sepal_length, AVG(sepal_width) AS avg_sepal_width FROM read_csv('docs/assets/data/iris.csv') GROUP BY species;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Refer to the &lt;a href="https://github.com/pola-rs/polars-cli"&gt;Polars CLI repository&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;Performance 🚀🚀&lt;/h2&gt; 
&lt;h3&gt;Blazingly fast&lt;/h3&gt; 
&lt;p&gt;Polars is very fast. In fact, it is one of the best performing solutions available. See the &lt;a href="https://www.pola.rs/benchmarks.html"&gt;PDS-H benchmarks&lt;/a&gt; results.&lt;/p&gt; 
&lt;h3&gt;Lightweight&lt;/h3&gt; 
&lt;p&gt;Polars is also very lightweight. It comes with zero required dependencies, and this shows in the import times:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;polars: 70ms&lt;/li&gt; 
 &lt;li&gt;numpy: 104ms&lt;/li&gt; 
 &lt;li&gt;pandas: 520ms&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Handles larger-than-RAM data&lt;/h3&gt; 
&lt;p&gt;If you have data that does not fit into memory, Polars' query engine is able to process your query (or parts of your query) in a streaming fashion. This drastically reduces memory requirements, so you might be able to process your 250GB dataset on your laptop. Collect with &lt;code&gt;collect(engine='streaming')&lt;/code&gt; to run the query streaming. (This might be a little slower, but it is still very fast!)&lt;/p&gt; 
&lt;h2&gt;Setup&lt;/h2&gt; 
&lt;h3&gt;Python&lt;/h3&gt; 
&lt;p&gt;Install the latest Polars version with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip install polars
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We also have a conda package (&lt;code&gt;conda install -c conda-forge polars&lt;/code&gt;), however pip is the preferred way to install Polars.&lt;/p&gt; 
&lt;p&gt;Install Polars with all optional dependencies.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip install 'polars[all]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also install a subset of all optional dependencies.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip install 'polars[numpy,pandas,pyarrow]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See the &lt;a href="https://docs.pola.rs/user-guide/installation/#feature-flags"&gt;User Guide&lt;/a&gt; for more details on optional dependencies&lt;/p&gt; 
&lt;p&gt;To see the current Polars version and a full list of its optional dependencies, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;pl.show_versions()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Releases happen quite often (weekly / every few days) at the moment, so updating Polars regularly to get the latest bugfixes / features might not be a bad idea.&lt;/p&gt; 
&lt;h3&gt;Rust&lt;/h3&gt; 
&lt;p&gt;You can take latest release from &lt;code&gt;crates.io&lt;/code&gt;, or if you want to use the latest features / performance improvements point to the &lt;code&gt;main&lt;/code&gt; branch of this repo.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-toml"&gt;polars = { git = "https://github.com/pola-rs/polars", rev = "&amp;lt;optional git tag&amp;gt;" }
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Requires Rust version &lt;code&gt;&amp;gt;=1.80&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Want to contribute? Read our &lt;a href="https://docs.pola.rs/development/contributing/"&gt;contributing guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Python: compile Polars from source&lt;/h2&gt; 
&lt;p&gt;If you want a bleeding edge release or maximal performance you should compile Polars from source.&lt;/p&gt; 
&lt;p&gt;This can be done by going through the following steps in sequence:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install the latest &lt;a href="https://www.rust-lang.org/tools/install"&gt;Rust compiler&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Install &lt;a href="https://maturin.rs/"&gt;maturin&lt;/a&gt;: &lt;code&gt;pip install maturin&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;cd py-polars&lt;/code&gt; and choose one of the following: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;make build&lt;/code&gt;, slow binary with debug assertions and symbols, fast compile times&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;make build-release&lt;/code&gt;, fast binary without debug assertions, minimal debug symbols, long compile times&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;make build-nodebug-release&lt;/code&gt;, same as build-release but without any debug symbols, slightly faster to compile&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;make build-debug-release&lt;/code&gt;, same as build-release but with full debug symbols, slightly slower to compile&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;make build-dist-release&lt;/code&gt;, fastest binary, extreme compile times&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;By default the binary is compiled with optimizations turned on for a modern CPU. Specify &lt;code&gt;LTS_CPU=1&lt;/code&gt; with the command if your CPU is older and does not support e.g. AVX2.&lt;/p&gt; 
&lt;p&gt;Note that the Rust crate implementing the Python bindings is called &lt;code&gt;py-polars&lt;/code&gt; to distinguish from the wrapped Rust crate &lt;code&gt;polars&lt;/code&gt; itself. However, both the Python package and the Python module are named &lt;code&gt;polars&lt;/code&gt;, so you can &lt;code&gt;pip install polars&lt;/code&gt; and &lt;code&gt;import polars&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Using custom Rust functions in Python&lt;/h2&gt; 
&lt;p&gt;Extending Polars with UDFs compiled in Rust is easy. We expose PyO3 extensions for &lt;code&gt;DataFrame&lt;/code&gt; and &lt;code&gt;Series&lt;/code&gt; data structures. See more in &lt;a href="https://github.com/pola-rs/polars/tree/main/pyo3-polars"&gt;https://github.com/pola-rs/polars/tree/main/pyo3-polars&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Going big...&lt;/h2&gt; 
&lt;p&gt;Do you expect more than 2^32 (~4.2 billion) rows? Compile Polars with the &lt;code&gt;bigidx&lt;/code&gt; feature flag or, for Python users, install &lt;code&gt;pip install polars-u64-idx&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Don't use this unless you hit the row boundary as the default build of Polars is faster and consumes less memory.&lt;/p&gt; 
&lt;h2&gt;Legacy&lt;/h2&gt; 
&lt;p&gt;Do you want Polars to run on an old CPU (e.g. dating from before 2011), or on an &lt;code&gt;x86-64&lt;/code&gt; build of Python on Apple Silicon under Rosetta? Install &lt;code&gt;pip install polars-lts-cpu&lt;/code&gt;. This version of Polars is compiled without &lt;a href="https://en.wikipedia.org/wiki/Advanced_Vector_Extensions"&gt;AVX&lt;/a&gt; target features.&lt;/p&gt; 
&lt;h2&gt;Sponsors&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.jetbrains.com"&gt;&lt;img src="https://www.jetbrains.com/company/brand/img/jetbrains_logo.png" height="50" alt="JetBrains logo" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ArthurBrussee/brush</title>
      <link>https://github.com/ArthurBrussee/brush</link>
      <description>&lt;p&gt;3D Reconstruction for all&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Brush&lt;/h1&gt; 
&lt;p&gt;
 &lt;video src="https://github.com/user-attachments/assets/5756967a-846c-44cf-bde9-3ca4c86f1a4d"&gt;
  A video showing various Brush features and scenes
 &lt;/video&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;i&gt; Massive thanks to &lt;a href="https://www.youtube.com/@gradeeterna"&gt;@GradeEterna&lt;/a&gt; for the beautiful scenes &lt;/i&gt; &lt;/p&gt; 
&lt;p&gt;Brush is a 3D reconstruction engine using &lt;a href="https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/"&gt;Gaussian splatting&lt;/a&gt;. It works on a wide range of systems: &lt;strong&gt;macOS/windows/linux&lt;/strong&gt;, &lt;strong&gt;AMD/Nvidia/Intel&lt;/strong&gt; cards, &lt;strong&gt;Android&lt;/strong&gt;, and in a &lt;strong&gt;browser&lt;/strong&gt;. To achieve this, it uses WebGPU compatible tech and the &lt;a href="https://github.com/tracel-ai/burn"&gt;Burn&lt;/a&gt; machine learning framework.&lt;/p&gt; 
&lt;p&gt;Machine learning for real time rendering has tons of potential, but most ML tools don't work well with it: Rendering requires realtime interactivity, usually involve dynamic shapes &amp;amp; computations, don't run on most platforms, and it can be cumbersome to ship apps with large CUDA deps. Brush on the other hand produces simple dependency free binaries, runs on nearly all devices, without any setup.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://arthurbrussee.github.io/brush-demo"&gt;&lt;strong&gt;Try the web demo&lt;/strong&gt; &lt;img src="https://cdn-icons-png.flaticon.com/256/888/888846.png" alt="chrome logo" width="24" /&gt; &lt;/a&gt; &lt;em&gt;NOTE: Only works on Chrome 135+ as of June 2025. Firefox and Safari are hopefully supported &lt;a href="https://caniuse.com/webgpu"&gt;soon&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://discord.gg/TbxJST2BbC"&gt;&lt;img src="https://dcbadge.limes.pink/api/server/https://discord.gg/TbxJST2BbC" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Features&lt;/h1&gt; 
&lt;h2&gt;Training&lt;/h2&gt; 
&lt;p&gt;Brush takes in &lt;em&gt;posed&lt;/em&gt; image data. It can load COLMAP data or datasets in the Nerfstudio format. Training is fully supported natively, on mobile, and in a browser.&lt;/p&gt; 
&lt;p&gt;While training you can interact with the scene and see the training dynamics live, and compare the current rendering to training or eval views as the training progresses.&lt;/p&gt; 
&lt;p&gt;It also supports masking images:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Images with transparency. This will force the final splat to match the transparency of the input.&lt;/li&gt; 
 &lt;li&gt;A folder of images called 'masks'. This ignores parts of the image that are masked out.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Viewer&lt;/h2&gt; 
&lt;p&gt;Brush also works well as a splat viewer, including on the web. It can load .ply &amp;amp; .compressed.ply files. You can stream in data from a URL (for a web app, simply append &lt;code&gt;?url=&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;Brush also can load .zip of splat files to display them as an animation, or a special ply that includes delta frames (see &lt;a href="https://cat-4d.github.io/"&gt;cat-4D&lt;/a&gt; and &lt;a href="https://felixtaubner.github.io/cap4d/"&gt;Cap4D&lt;/a&gt;!).&lt;/p&gt; 
&lt;h2&gt;CLI&lt;/h2&gt; 
&lt;p&gt;Brush can be used as a CLI. Run &lt;code&gt;brush --help&lt;/code&gt; to get an overview. Every CLI command can work with &lt;code&gt;--with-viewer&lt;/code&gt; which also opens the UI, for easy debugging.&lt;/p&gt; 
&lt;h2&gt;Rerun&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/f679fec0-935d-4dd2-87e1-c301db9cdc2c"&gt;https://github.com/user-attachments/assets/f679fec0-935d-4dd2-87e1-c301db9cdc2c&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;While training, additional data can be visualized with the excellent &lt;a href="https://rerun.io/"&gt;rerun&lt;/a&gt;. To install rerun on your machine, please follow their &lt;a href="https://rerun.io/docs/getting-started/installing-viewer"&gt;instructions&lt;/a&gt;. Open the ./brush_blueprint.rbl in the viewer for best results.&lt;/p&gt; 
&lt;h2&gt;Building Brush&lt;/h2&gt; 
&lt;p&gt;First install rust 1.85+. You can run tests with &lt;code&gt;cargo test --all&lt;/code&gt;. Brush uses the wonderful &lt;a href="https://rerun.io/"&gt;rerun&lt;/a&gt; for additional visualizations while training, run &lt;code&gt;cargo install rerun-cli&lt;/code&gt; if you want to use it.&lt;/p&gt; 
&lt;h3&gt;Windows/macOS/Linux&lt;/h3&gt; 
&lt;p&gt;Simply &lt;code&gt;cargo run&lt;/code&gt; or &lt;code&gt;cargo run --release&lt;/code&gt; from the workspace root. Brush can also be used as a CLI, run &lt;code&gt;cargo run --release -- --help&lt;/code&gt; to use the CLI directly from source. See the notes about the CLI in the features section.&lt;/p&gt; 
&lt;h3&gt;Web&lt;/h3&gt; 
&lt;p&gt;Brush can be compiled to WASM. Run &lt;code&gt;npm run dev&lt;/code&gt; to start the demo website using Next.js, see the brush_nextjs directory.&lt;/p&gt; 
&lt;p&gt;Brush uses &lt;a href="https://rustwasm.github.io/wasm-bindgen/introduction.html"&gt;&lt;code&gt;wasm-pack&lt;/code&gt;&lt;/a&gt; to build the WASM bundle. You can also use it without a bundler, see &lt;a href="hhttps://rustwasm.github.io/wasm-bindgen/examples/without-a-bundler.html"&gt;wasm-pack's documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;WebGPU is still an upcoming standard, and as such, only Chrome 134+ on Windows and macOS is currently supported.&lt;/p&gt; 
&lt;h3&gt;Android&lt;/h3&gt; 
&lt;p&gt;As a one time setup, make sure you have the Android SDK &amp;amp; NDK installed.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Check if ANDROID_NDK_HOME and ANDROID_HOME are set&lt;/li&gt; 
 &lt;li&gt;Add the Android target to rust &lt;code&gt;rustup target add aarch64-linux-android&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Install cargo-ndk to manage building a lib &lt;code&gt;cargo install cargo-ndk&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Each time you change the rust code, run&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;cargo ndk -t arm64-v8a -o crates/brush-app/app/src/main/jniLibs/ build&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Nb: Nb, for best performance, build in release mode. This is separate from the Android Studio app build configuration.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;cargo ndk -t arm64-v8a -o crates/brush-app/app/src/main/jniLibs/ build --release&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can now either run the project from Android Studio (Android Studio does NOT build the rust code), or run it from the command line:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;./gradlew build
./gradlew installDebug
adb shell am start -n com.splats.app/.MainActivity
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also open this folder as a project in Android Studio and run things from there.&lt;/p&gt; 
&lt;p&gt;Nb: Running in Android Studio does &lt;em&gt;not&lt;/em&gt; rebuild the rust code automatically.&lt;/p&gt; 
&lt;h2&gt;Results&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Metric&lt;/th&gt; 
   &lt;th&gt;bicycle&lt;/th&gt; 
   &lt;th&gt;garden&lt;/th&gt; 
   &lt;th&gt;stump&lt;/th&gt; 
   &lt;th&gt;room&lt;/th&gt; 
   &lt;th&gt;counter&lt;/th&gt; 
   &lt;th&gt;kitchen&lt;/th&gt; 
   &lt;th&gt;bonsai&lt;/th&gt; 
   &lt;th&gt;Average&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;PSNR ↑&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;inria 30K&lt;/td&gt; 
   &lt;td&gt;25.25&lt;/td&gt; 
   &lt;td&gt;27.41&lt;/td&gt; 
   &lt;td&gt;26.55&lt;/td&gt; 
   &lt;td&gt;30.63&lt;/td&gt; 
   &lt;td&gt;28.70&lt;/td&gt; 
   &lt;td&gt;30.32&lt;/td&gt; 
   &lt;td&gt;31.98&lt;/td&gt; 
   &lt;td&gt;28.69&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;gsplat 30K&lt;/td&gt; 
   &lt;td&gt;25.22&lt;/td&gt; 
   &lt;td&gt;27.32&lt;/td&gt; 
   &lt;td&gt;26.53&lt;/td&gt; 
   &lt;td&gt;31.36&lt;/td&gt; 
   &lt;td&gt;29.02&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;31.16&lt;/strong&gt;⭐&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;32.06&lt;/strong&gt;⭐&lt;/td&gt; 
   &lt;td&gt;28.95&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;brush 30K&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;25.55&lt;/strong&gt;⭐&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;27.42&lt;/strong&gt;⭐&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;26.88&lt;/strong&gt;⭐&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;31.45&lt;/strong&gt;⭐&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;29.17&lt;/strong&gt;⭐&lt;/td&gt; 
   &lt;td&gt;30.55&lt;/td&gt; 
   &lt;td&gt;32.02&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;29.01&lt;/strong&gt;⭐&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;SSIM ↑&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;inria 30k&lt;/td&gt; 
   &lt;td&gt;0.763&lt;/td&gt; 
   &lt;td&gt;0.863&lt;/td&gt; 
   &lt;td&gt;0.771&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;0.918&lt;/strong&gt;⭐&lt;/td&gt; 
   &lt;td&gt;0.906&lt;/td&gt; 
   &lt;td&gt;0.925&lt;/td&gt; 
   &lt;td&gt;0.941&lt;/td&gt; 
   &lt;td&gt;0.870&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;gsplat&lt;/td&gt; 
   &lt;td&gt;0.764&lt;/td&gt; 
   &lt;td&gt;0.865&lt;/td&gt; 
   &lt;td&gt;0.768&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;0.918&lt;/strong&gt;⭐&lt;/td&gt; 
   &lt;td&gt;0.907&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;0.926&lt;/strong&gt;⭐&lt;/td&gt; 
   &lt;td&gt;0.941&lt;/td&gt; 
   &lt;td&gt;0.870&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;brush&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;0.781&lt;/strong&gt;⭐&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;0.869&lt;/strong&gt;⭐&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;0.791&lt;/strong&gt;⭐&lt;/td&gt; 
   &lt;td&gt;0.916&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;0.909&lt;/strong&gt;⭐&lt;/td&gt; 
   &lt;td&gt;0.920&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;0.942&lt;/strong&gt;⭐&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;0.875&lt;/strong&gt;⭐&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Splat Count (millions) ↓&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;inira&lt;/td&gt; 
   &lt;td&gt;6.06&lt;/td&gt; 
   &lt;td&gt;5.71&lt;/td&gt; 
   &lt;td&gt;4.82&lt;/td&gt; 
   &lt;td&gt;1.55&lt;/td&gt; 
   &lt;td&gt;1.19&lt;/td&gt; 
   &lt;td&gt;1.78&lt;/td&gt; 
   &lt;td&gt;1.24&lt;/td&gt; 
   &lt;td&gt;3.19&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;gsplat&lt;/td&gt; 
   &lt;td&gt;6.26&lt;/td&gt; 
   &lt;td&gt;5.84&lt;/td&gt; 
   &lt;td&gt;4.81&lt;/td&gt; 
   &lt;td&gt;1.59&lt;/td&gt; 
   &lt;td&gt;1.21&lt;/td&gt; 
   &lt;td&gt;1.79&lt;/td&gt; 
   &lt;td&gt;1.25&lt;/td&gt; 
   &lt;td&gt;3.25&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;brush&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;3.30&lt;/strong&gt;⭐&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;2.90&lt;/strong&gt;⭐&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;2.55&lt;/strong&gt;⭐&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;0.75&lt;/strong&gt;⭐&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;0.60&lt;/strong&gt;⭐&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;0.79&lt;/strong&gt;⭐&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;0.68&lt;/strong&gt;⭐&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;1.65&lt;/strong&gt;⭐&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Minutes (4070 ti)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;brush&lt;/td&gt; 
   &lt;td&gt;35&lt;/td&gt; 
   &lt;td&gt;35&lt;/td&gt; 
   &lt;td&gt;28&lt;/td&gt; 
   &lt;td&gt;18&lt;/td&gt; 
   &lt;td&gt;19&lt;/td&gt; 
   &lt;td&gt;18&lt;/td&gt; 
   &lt;td&gt;18&lt;/td&gt; 
   &lt;td&gt;24.43&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Numbers taken from &lt;a href="https://docs.gsplat.studio/main/tests/eval.html"&gt;here&lt;/a&gt;. Note that Brush by default regularizes opacity slightly.&lt;/p&gt; 
&lt;h2&gt;Benchmarks&lt;/h2&gt; 
&lt;p&gt;Rendering is generally faster than gsplat, while end-to-end training speeds are similar. You can run benchmarks of some of the kernels using &lt;code&gt;cargo bench&lt;/code&gt;.&lt;/p&gt; 
&lt;h1&gt;Acknowledgements&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/nerfstudio-project/gsplat"&gt;&lt;strong&gt;gSplat&lt;/strong&gt;&lt;/a&gt;, for their reference version of the kernels&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Peter Hedman, George Kopanas &amp;amp; Bernhard Kerbl&lt;/strong&gt;, for the many discussions &amp;amp; pointers.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;The Burn team&lt;/strong&gt;, for help &amp;amp; improvements to Burn along the way&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Raph Levien&lt;/strong&gt;, for the &lt;a href="https://github.com/googlefonts/compute-shader-101/pull/31"&gt;original version&lt;/a&gt; of the GPU radix sort.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;GradeEterna&lt;/strong&gt;, for feedback and displaying their scenes.&lt;/p&gt; 
&lt;h1&gt;Disclaimer&lt;/h1&gt; 
&lt;p&gt;This is &lt;em&gt;not&lt;/em&gt; an official Google product. This repository is a forked public version of &lt;a href="https://github.com/google-research/google-research/tree/master/brush_splat"&gt;the google-research repository&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>sunface/rust-by-practice</title>
      <link>https://github.com/sunface/rust-by-practice</link>
      <description>&lt;p&gt;Learning Rust By Practice, narrowing the gap between beginner and skilled-dev through challenging examples, exercises and projects.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://github.com/sunface/rust-by-practice/raw/master/en/assets/header.jpg?raw=true" /&gt; 
&lt;/div&gt; 
&lt;p align="center"&gt; &lt;span&gt;English&lt;/span&gt; &lt;span&gt; | &lt;/span&gt; &lt;a href="https://github.com/sunface/rust-by-practice/raw/master/zh-CN/src/why-exercise.md"&gt;中文&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt;Practice Rust with challenging examples, exercises and projects&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/sunface/rust-by-practice/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/sunface/rust-by-practice?style=flat" alt="Stars Count" /&gt;&lt;/a&gt; &lt;a href="https://hirust.cn"&gt;&lt;img src="https://img.shields.io/badge/RustCn-orange" alt="studyrut" /&gt;&lt;/a&gt; &lt;a href="https://github.com/sunface/rust-by-practice/raw/master/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-CC_BY_4.0-green?style=flat" alt="LICENSE" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;This book was designed for easily diving into and getting skilled with Rust It's very easy to use. All you need to do is to make each exercise compile without ERRORS and Panics!&lt;/p&gt; 
&lt;h2&gt;Reading online&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://practice.rs"&gt;https://practice.rs&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;p&gt;Part of our examples and exercises are borrowed from &lt;a href="https://github.com/rust-lang/rust-by-example"&gt;Rust By Example&lt;/a&gt;, thanks for your great works!&lt;/p&gt; 
&lt;p&gt;Although they are so awesome, we have our own secret weapons :)&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;There are three parts in each chapter: examples, exercises and practices&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Besides examples, we have &lt;code&gt;a lot of exercises&lt;/code&gt;, you can Read, Edit and Run them ONLINE&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Covering nearly all aspects of Rust, such as async/await, threads, sync primitives, optimizing, standard libraries, tool chain, data structures and algorithms etc.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Every exercise has its own solutions&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;The overall difficulties are a bit higher and from easy to super hard: easy 🌟 medium 🌟🌟 hard 🌟🌟🌟 super hard 🌟🌟🌟🌟&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;What we want to do is fill in the gap between learning and getting started with real projects.&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;🏅 Contributors&lt;/h2&gt; 
&lt;p&gt;Thanks to all of our &lt;a href="https://github.com/sunface/rust-by-practice/graphs/contributors"&gt;contributors&lt;/a&gt;!&lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;strong&gt;🏆 Special thanks to our English editor:&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td align="center"&gt; &lt;a href="https://github.com/Tanish-Eagle"&gt; &lt;img src="https://avatars.githubusercontent.com/u/71984506?v=4?s=100" width="160px" alt="" /&gt; &lt;br /&gt; &lt;sub&gt;&lt;b&gt;Tanish-Eagle&lt;/b&gt;&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;br /&gt; 
&lt;h2&gt;Running locally&lt;/h2&gt; 
&lt;p&gt;We use &lt;a href="https://rust-lang.github.io/mdBook/"&gt;mdbook&lt;/a&gt; building our exercises. You can run locally with below steps:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Clone the repo&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;$ git clone https://github.com/sunface/rust-by-practice
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Install mdbook using Cargo&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;$ cargo install mdbook
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;For Book in English&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;$ cd rust-by-practice &amp;amp;&amp;amp; mdbook serve en/
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;For Book in Chinese&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;$ cd rust-by-practice &amp;amp;&amp;amp; mdbook serve zh-CN/
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Some of our exercises&lt;/h2&gt; 
&lt;p&gt;🌟🌟🌟 Tuple struct looks similar to tuples, it has added meaning the struct name provides but has no named fields. It's useful when you want give the whole tuple a name, but don't care the fields's names.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-rust"&gt;
// fix the error and fill the blanks
struct Color(i32, i32, i32);
struct Point(i32, i32, i32);
fn main() {
    let v = Point(___, ___, ___);
    check_color(v);
}

fn check_color(p: Color) {
    let (x, _, _) = p;
    assert_eq!(x, 0);
    assert_eq!(p.1, 127);
    assert_eq!(___, 255);
 }
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;🌟🌟 Within the destructuring of a single variable, both by-move and by-reference pattern bindings can be used at the same time. Doing this will result in a partial move of the variable, which means that parts of the variable will be moved while other parts stay. In such a case, the parent variable cannot be used afterwards as a whole, however the parts that are only referenced (and not moved) can still be used.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-rust"&gt;
// fix errors to make it work
#[derive(Debug)]
struct File {
    name: String,
    data: String,
}
fn main() {
    let f = File {
        name: String::from("readme.md"),
        data: "Rust By Practice".to_string()
    };

    let _name = f.name;

    // ONLY modify this line
    println!("{}, {}, {:?}",f.name, f.data, f);
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;🌟🌟 A match guard is an additional if condition specified after the pattern in a match arm that must also match, along with the pattern matching, for that arm to be chosen.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-rust,editable"&gt;
// fill in the blank to make the code work, `split` MUST be used
fn main() {
    let num = Some(4);
    let split = 5;
    match num {
        Some(x) __ =&amp;gt; assert!(x &amp;lt; split),
        Some(x) =&amp;gt; assert!(x &amp;gt;= split),
        None =&amp;gt; (),
    }
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>ekzhang/bore</title>
      <link>https://github.com/ekzhang/bore</link>
      <description>&lt;p&gt;🕳 bore is a simple CLI tool for making tunnels to localhost&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;bore&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/ekzhang/bore/actions"&gt;&lt;img src="https://img.shields.io/github/actions/workflow/status/ekzhang/bore/ci.yml" alt="Build status" /&gt;&lt;/a&gt; &lt;a href="https://crates.io/crates/bore-cli"&gt;&lt;img src="https://img.shields.io/crates/v/bore-cli.svg?sanitize=true" alt="Crates.io" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;A modern, simple TCP tunnel in Rust that exposes local ports to a remote server, bypassing standard NAT connection firewalls. &lt;strong&gt;That's all it does: no more, and no less.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://i.imgur.com/vDeGsmx.gif" alt="Video demo" /&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# Installation (requires Rust, see alternatives below)
cargo install bore-cli

# On your local machine
bore local 8000 --to bore.pub
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will expose your local port at &lt;code&gt;localhost:8000&lt;/code&gt; to the public internet at &lt;code&gt;bore.pub:&amp;lt;PORT&amp;gt;&lt;/code&gt;, where the port number is assigned randomly.&lt;/p&gt; 
&lt;p&gt;Similar to &lt;a href="https://github.com/localtunnel/localtunnel"&gt;localtunnel&lt;/a&gt; and &lt;a href="https://ngrok.io/"&gt;ngrok&lt;/a&gt;, except &lt;code&gt;bore&lt;/code&gt; is intended to be a highly efficient, unopinionated tool for forwarding TCP traffic that is simple to install and easy to self-host, with no frills attached.&lt;/p&gt; 
&lt;p&gt;(&lt;code&gt;bore&lt;/code&gt; totals about 400 lines of safe, async Rust code and is trivial to set up — just run a single binary for the client and server.)&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;macOS&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;bore&lt;/code&gt; is packaged as a Homebrew core formula.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;brew install bore-cli
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Linux&lt;/h3&gt; 
&lt;h4&gt;Arch Linux&lt;/h4&gt; 
&lt;p&gt;&lt;code&gt;bore&lt;/code&gt; is available in the AUR as &lt;code&gt;bore&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;yay -S bore # or your favorite AUR helper
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Gentoo Linux&lt;/h4&gt; 
&lt;p&gt;&lt;code&gt;bore&lt;/code&gt; is available in the &lt;a href="https://github.com/microcai/gentoo-zh"&gt;gentoo-zh&lt;/a&gt; overlay.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;sudo eselect repository enable gentoo-zh
sudo emerge --sync gentoo-zh
sudo emerge net-proxy/bore
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Binary Distribution&lt;/h3&gt; 
&lt;p&gt;Otherwise, the easiest way to install bore is from prebuilt binaries. These are available on the &lt;a href="https://github.com/ekzhang/bore/releases"&gt;releases page&lt;/a&gt; for macOS, Windows, and Linux. Just unzip the appropriate file for your platform and move the &lt;code&gt;bore&lt;/code&gt; executable into a folder on your PATH.&lt;/p&gt; 
&lt;h3&gt;Cargo&lt;/h3&gt; 
&lt;p&gt;You also can build &lt;code&gt;bore&lt;/code&gt; from source using &lt;a href="https://doc.rust-lang.org/cargo/"&gt;Cargo&lt;/a&gt;, the Rust package manager. This command installs the &lt;code&gt;bore&lt;/code&gt; binary at a user-accessible path.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;cargo install bore-cli
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Docker&lt;/h3&gt; 
&lt;p&gt;We also publish versioned Docker images for each release. The image is built for an AMD 64-bit architecture. They're tagged with the specific version and allow you to run the statically-linked &lt;code&gt;bore&lt;/code&gt; binary from a minimal "scratch" container.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;docker run -it --init --rm --network host ekzhang/bore &amp;lt;ARGS&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Detailed Usage&lt;/h2&gt; 
&lt;p&gt;This section describes detailed usage for the &lt;code&gt;bore&lt;/code&gt; CLI command.&lt;/p&gt; 
&lt;h3&gt;Local Forwarding&lt;/h3&gt; 
&lt;p&gt;You can forward a port on your local machine by using the &lt;code&gt;bore local&lt;/code&gt; command. This takes a positional argument, the local port to forward, as well as a mandatory &lt;code&gt;--to&lt;/code&gt; option, which specifies the address of the remote server.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;bore local 5000 --to bore.pub
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can optionally pass in a &lt;code&gt;--port&lt;/code&gt; option to pick a specific port on the remote to expose, although the command will fail if this port is not available. Also, passing &lt;code&gt;--local-host&lt;/code&gt; allows you to expose a different host on your local area network besides the loopback address &lt;code&gt;localhost&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;The full options are shown below.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;Starts a local proxy to the remote server

Usage: bore local [OPTIONS] --to &amp;lt;TO&amp;gt; &amp;lt;LOCAL_PORT&amp;gt;

Arguments:
  &amp;lt;LOCAL_PORT&amp;gt;  The local port to expose [env: BORE_LOCAL_PORT=]

Options:
  -l, --local-host &amp;lt;HOST&amp;gt;  The local host to expose [default: localhost]
  -t, --to &amp;lt;TO&amp;gt;            Address of the remote server to expose local ports to [env: BORE_SERVER=]
  -p, --port &amp;lt;PORT&amp;gt;        Optional port on the remote server to select [default: 0]
  -s, --secret &amp;lt;SECRET&amp;gt;    Optional secret for authentication [env: BORE_SECRET]
  -h, --help               Print help
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Self-Hosting&lt;/h3&gt; 
&lt;p&gt;As mentioned in the startup instructions, there is a public instance of the &lt;code&gt;bore&lt;/code&gt; server running at &lt;code&gt;bore.pub&lt;/code&gt;. However, if you want to self-host &lt;code&gt;bore&lt;/code&gt; on your own network, you can do so with the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;bore server
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;That's all it takes! After the server starts running at a given address, you can then update the &lt;code&gt;bore local&lt;/code&gt; command with option &lt;code&gt;--to &amp;lt;ADDRESS&amp;gt;&lt;/code&gt; to forward a local port to this remote server.&lt;/p&gt; 
&lt;p&gt;It's possible to specify different IP addresses for the control server and for the tunnels. This setup is useful for cases where you might want the control server to be on a private network while allowing tunnel connections over a public interface, or vice versa.&lt;/p&gt; 
&lt;p&gt;The full options for the &lt;code&gt;bore server&lt;/code&gt; command are shown below.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;Runs the remote proxy server

Usage: bore server [OPTIONS]

Options:
      --min-port &amp;lt;MIN_PORT&amp;gt;          Minimum accepted TCP port number [env: BORE_MIN_PORT=] [default: 1024]
      --max-port &amp;lt;MAX_PORT&amp;gt;          Maximum accepted TCP port number [env: BORE_MAX_PORT=] [default: 65535]
  -s, --secret &amp;lt;SECRET&amp;gt;              Optional secret for authentication [env: BORE_SECRET]
      --bind-addr &amp;lt;BIND_ADDR&amp;gt;        IP address to bind to, clients must reach this [default: 0.0.0.0]
      --bind-tunnels &amp;lt;BIND_TUNNELS&amp;gt;  IP address where tunnels will listen on, defaults to --bind-addr
  -h, --help                         Print help
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Protocol&lt;/h2&gt; 
&lt;p&gt;There is an implicit &lt;em&gt;control port&lt;/em&gt; at &lt;code&gt;7835&lt;/code&gt;, used for creating new connections on demand. At initialization, the client sends a "Hello" message to the server on the TCP control port, asking to proxy a selected remote port. The server then responds with an acknowledgement and begins listening for external TCP connections.&lt;/p&gt; 
&lt;p&gt;Whenever the server obtains a connection on the remote port, it generates a secure &lt;a href="https://en.wikipedia.org/wiki/Universally_unique_identifier"&gt;UUID&lt;/a&gt; for that connection and sends it back to the client. The client then opens a separate TCP stream to the server and sends an "Accept" message containing the UUID on that stream. The server then proxies the two connections between each other.&lt;/p&gt; 
&lt;p&gt;For correctness reasons and to avoid memory leaks, incoming connections are only stored by the server for up to 10 seconds before being discarded if the client does not accept them.&lt;/p&gt; 
&lt;h2&gt;Authentication&lt;/h2&gt; 
&lt;p&gt;On a custom deployment of &lt;code&gt;bore server&lt;/code&gt;, you can optionally require a &lt;em&gt;secret&lt;/em&gt; to prevent the server from being used by others. The protocol requires clients to verify possession of the secret on each TCP connection by answering random challenges in the form of HMAC codes. (This secret is only used for the initial handshake, and no further traffic is encrypted by default.)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# on the server
bore server --secret my_secret_string

# on the client
bore local &amp;lt;LOCAL_PORT&amp;gt; --to &amp;lt;TO&amp;gt; --secret my_secret_string
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If a secret is not present in the arguments, &lt;code&gt;bore&lt;/code&gt; will also attempt to read from the &lt;code&gt;BORE_SECRET&lt;/code&gt; environment variable.&lt;/p&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;p&gt;Created by Eric Zhang (&lt;a href="https://twitter.com/ekzhang1"&gt;@ekzhang1&lt;/a&gt;). Licensed under the &lt;a href="https://raw.githubusercontent.com/ekzhang/bore/main/LICENSE"&gt;MIT license&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The author would like to thank the contributors and maintainers of the &lt;a href="https://tokio.rs/"&gt;Tokio&lt;/a&gt; project for making it possible to write ergonomic and efficient network services in Rust.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>slint-ui/slint</title>
      <link>https://github.com/slint-ui/slint</link>
      <description>&lt;p&gt;Slint is an open-source declarative GUI toolkit to build native user interfaces for Rust, C++, JavaScript, or Python apps.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/slint-ui/slint/master/logo/slint-logo-full-light.svg#gh-light-mode-only" alt="Slint" /&gt; &lt;img src="https://raw.githubusercontent.com/slint-ui/slint/master/logo/slint-logo-full-dark.svg#gh-dark-mode-only" alt="Slint" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/slint-ui/slint/actions"&gt;&lt;img src="https://github.com/slint-ui/slint/workflows/CI/badge.svg?sanitize=true" alt="Build Status" /&gt;&lt;/a&gt; &lt;a href="https://api.reuse.software/info/github.com/slint-ui/slint"&gt;&lt;img src="https://api.reuse.software/badge/github.com/slint-ui/slint" alt="REUSE status" /&gt;&lt;/a&gt; &lt;a href="https://github.com/slint-ui/slint/discussions"&gt;&lt;img src="https://img.shields.io/github/discussions/slint-ui/slint" alt="Discussions" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Slint&lt;/strong&gt; is an open-source declarative GUI toolkit for building native user interfaces for embedded systems, desktops, and mobile platforms.&lt;/p&gt; 
&lt;p&gt;Write your UI once in &lt;code&gt;.slint&lt;/code&gt;, a simple markup language. Connect it to business logic written in Rust, C++, JavaScript, or Python.&lt;/p&gt; 
&lt;h2&gt;Why Slint?&lt;/h2&gt; 
&lt;p&gt;The name &lt;em&gt;Slint&lt;/em&gt; is derived from our design goals:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Scalable&lt;/strong&gt;: Slint should support responsive UI design, allow cross-platform usage across operating systems and processor architectures and support multiple programming languages.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Lightweight&lt;/strong&gt;: Slint should require minimal resources, in terms of memory and processing power, and yet deliver a smooth, smartphone-like user experience on any device.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Intuitive&lt;/strong&gt;: Designers and developers should feel productive while enjoying the GUI design and development process. The design creation tools should be intuitive to use for the designers. Similarly for the developers, the APIs should be consistent and easy to use, no matter which programming language they choose.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Native&lt;/strong&gt;: GUI built with Slint should match the end users' expectations of a native application irrespective of the platform - desktop, mobile, web or embedded system. The UI design should be compiled to machine code and provide flexibility that only a native application can offer: Access full operating system APIs, utilize all CPU and GPU cores, connect to any peripheral.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Beyond the design goals, here’s what makes Slint stand out:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Independent UI Design&lt;/strong&gt;: Use a declarative language similar to separate your UI from business logic. Designers can work in parallel with developers.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Tooling&lt;/strong&gt;: Iterate quickly with our Live Preview &amp;amp; editor integrations. Integrate from Figma with the &lt;a href="https://www.figma.com/community/plugin/1474418299182276871/figma-to-slint"&gt;Slint To Figma plugin&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Stable APIs&lt;/strong&gt;: Slint follows a stable 1.x API. We evolve carefully without breaking your code.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See what others have built: &lt;a href="https://madewithslint.com"&gt;#MadeWithSlint&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;h3&gt;Embedded&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;RaspberryPi&lt;/th&gt; 
   &lt;th&gt;STM32&lt;/th&gt; 
   &lt;th&gt;RP2040&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=_BDbNHrjK7g"&gt;Video of Slint on Raspberry Pi&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=NNNOJJsOAis"&gt;Video of Slint on STM32&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=dkBwNocItGs"&gt;Video of Slint on RP2040&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Desktop&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Windows&lt;/th&gt; 
   &lt;th&gt;macOS&lt;/th&gt; 
   &lt;th&gt;Linux&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src="https://slint.dev/resources/gallery_win_screenshot.png" alt="Screenshot of the Gallery on Windows" title="Gallery" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://slint.dev/resources/gallery_mac_screenshot.png" alt="Screenshot of the Gallery on macOS" title="Gallery" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://slint.dev/resources/gallery_linux_screenshot.png" alt="Screenshot of the Gallery on Linux" title="Gallery" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Web using WebAssembly&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Printer Demo&lt;/th&gt; 
   &lt;th&gt;Slide Puzzle&lt;/th&gt; 
   &lt;th&gt;Energy Monitor&lt;/th&gt; 
   &lt;th&gt;Widget Gallery&lt;/th&gt; 
   &lt;th&gt;Weather demo&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://slint.dev/demos/printerdemo/"&gt;&lt;img src="https://slint.dev/resources/printerdemo_screenshot.png" alt="Screenshot of the Printer Demo" title="Printer Demo" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://slint.dev/demos/slide_puzzle/"&gt;&lt;img src="https://slint.dev/resources/puzzle_screenshot.png" alt="Screenshot of the Slide Puzzle" title="Slide Puzzle" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://slint.dev/demos/energy-monitor/"&gt;&lt;img src="https://slint.dev/resources/energy-monitor-screenshot.png" alt="Screenshot of the Energy Monitor Demo" title="Energy Monitor Demo" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://slint.dev/demos/gallery/"&gt;&lt;img src="https://slint.dev/resources/gallery_screenshot.png" alt="Screenshot of the Gallery Demo" title="Gallery Demo" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://slint.dev/demos/weather-demo/"&gt;&lt;img src="https://raw.githubusercontent.com/slint-ui/slint/master/demos/weather-demo/docs/img/desktop-preview.png" alt="Screenshot of the weather Demo" title="Weather Demo" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;More examples and demos in the &lt;a href="https://raw.githubusercontent.com/slint-ui/slint/master/examples#examples"&gt;examples folder&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Get Started&lt;/h2&gt; 
&lt;h3&gt;Hello World&lt;/h3&gt; 
&lt;p&gt;The UI is defined in a Domain Specific Language that is declarative, easy to use, intuitive, and provides a powerful way to describe graphical elements, their placement, their hierarchy, property bindings, and the flow of data through the different states.&lt;/p&gt; 
&lt;p&gt;Here's the obligatory "Hello World":&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-slint"&gt;export component HelloWorld inherits Window {
    width: 400px;
    height: 400px;

    Text {
       y: parent.width / 2;
       x: parent.x + 200px;
       text: "Hello, world";
       color: blue;
    }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Documentation&lt;/h3&gt; 
&lt;p&gt;For more details, check out the &lt;a href="https://slint.dev/docs/slint"&gt;Slint Language Documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The &lt;a href="https://raw.githubusercontent.com/slint-ui/slint/master/examples"&gt;examples&lt;/a&gt; folder contains examples and demos, showing how to use the Slint markup language and how to interact with a Slint user interface from supported programming languages.&lt;/p&gt; 
&lt;p&gt;The &lt;code&gt;docs&lt;/code&gt; folder contains a lot more information, including &lt;a href="https://raw.githubusercontent.com/slint-ui/slint/master/docs/building.md"&gt;build instructions&lt;/a&gt;, and &lt;a href="https://raw.githubusercontent.com/slint-ui/slint/master/docs/development.md"&gt;internal developer docs&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Refer to the README of each language directory in the &lt;code&gt;api&lt;/code&gt; folder:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/slint-ui/slint/master/api/cpp"&gt;C++&lt;/a&gt; (&lt;a href="https://slint.dev/latest/docs/cpp"&gt;Documentation&lt;/a&gt; | &lt;a href="https://github.com/slint-ui/slint-cpp-template"&gt;Getting Started Template&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/slint-ui/slint/master/api/rs/slint"&gt;Rust&lt;/a&gt; &lt;a href="https://crates.io/crates/slint"&gt;&lt;img src="https://img.shields.io/crates/v/slint" alt="Crates.io" /&gt;&lt;/a&gt; (&lt;a href="https://slint.dev/latest/docs/rust/slint/"&gt;Documentation&lt;/a&gt; | &lt;a href="https://youtu.be/WBcv4V-whHk"&gt;Tutorial Video&lt;/a&gt; | &lt;a href="https://github.com/slint-ui/slint-rust-template"&gt;Getting Started Template&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/slint-ui/slint/master/api/node"&gt;JavaScript/NodeJS (Beta)&lt;/a&gt; &lt;a href="https://www.npmjs.com/package/slint-ui"&gt;&lt;img src="https://img.shields.io/npm/v/slint-ui" alt="npm" /&gt;&lt;/a&gt; (&lt;a href="https://slint.dev/latest/docs/node"&gt;Documentation&lt;/a&gt; | &lt;a href="https://github.com/slint-ui/slint-nodejs-template"&gt;Getting Started Template&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/slint-ui/slint/master/api/python/slint"&gt;Python (Beta)&lt;/a&gt; &lt;a href="https://pypi.org/project/slint/"&gt;&lt;img src="https://img.shields.io/pypi/v/slint" alt="pypi" /&gt;&lt;/a&gt; (&lt;a href="http://snapshots.slint.dev/master/docs/python/"&gt;Documentation&lt;/a&gt; | &lt;a href="https://github.com/slint-ui/slint-python-template"&gt;Getting Started Template&lt;/a&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Architecture&lt;/h2&gt; 
&lt;p&gt;An application is composed of the business logic written in Rust, C++, or JavaScript and the &lt;code&gt;.slint&lt;/code&gt; user interface design markup, which is compiled to native code.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://slint.dev/resources/architecture.drawio.svg?sanitize=true" alt="Architecture Overview" /&gt;&lt;/p&gt; 
&lt;h3&gt;Compiler&lt;/h3&gt; 
&lt;p&gt;The &lt;code&gt;.slint&lt;/code&gt; files are compiled ahead of time. The expressions in the &lt;code&gt;.slint&lt;/code&gt; are pure functions that the compiler can optimize. For example, the compiler could choose to "inline" properties and remove those that are constant or unchanged.&lt;/p&gt; 
&lt;p&gt;The compiler uses the typical compiler phases of lexing, parsing, optimization, and finally code generation. It provides different back-ends for code generation in the target language. The C++ code generator produces a C++ header file, the Rust generator produces Rust code, and so on. An interpreter for dynamic languages is also included.&lt;/p&gt; 
&lt;h3&gt;Runtime&lt;/h3&gt; 
&lt;p&gt;The runtime library consists of an engine that supports properties declared in the &lt;code&gt;.slint&lt;/code&gt; language. Components with their elements, items, and properties are laid out in a single memory region, to reduce memory allocations.&lt;/p&gt; 
&lt;p&gt;Rendering backends and styles are configurable at compile time:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;The &lt;code&gt;femtovg&lt;/code&gt; renderer uses OpenGL ES 2.0 for rendering.&lt;/li&gt; 
 &lt;li&gt;The &lt;code&gt;skia&lt;/code&gt; renderer uses &lt;a href="https://skia.org"&gt;Skia&lt;/a&gt; for rendering.&lt;/li&gt; 
 &lt;li&gt;The &lt;code&gt;software&lt;/code&gt; renderer uses the CPU with no additional dependencies.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;NOTE: When Qt is installed on the system, the &lt;code&gt;qt&lt;/code&gt; style becomes available, using Qt's QStyle to achieve native looking widgets.&lt;/p&gt; 
&lt;h3&gt;Tooling&lt;/h3&gt; 
&lt;p&gt;We have a few tools to help with the development of .slint files:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;A &lt;a href="https://raw.githubusercontent.com/slint-ui/slint/master/tools/lsp"&gt;&lt;strong&gt;LSP Server&lt;/strong&gt;&lt;/a&gt; that adds features like auto-complete and live preview of the .slint files to many editors.&lt;/li&gt; 
 &lt;li&gt;It is bundled in a &lt;a href="https://raw.githubusercontent.com/slint-ui/slint/master/editors/vscode"&gt;&lt;strong&gt;Visual Studio Code Extension&lt;/strong&gt;&lt;/a&gt; available from the market place.&lt;/li&gt; 
 &lt;li&gt;A &lt;a href="https://raw.githubusercontent.com/slint-ui/slint/master/tools/viewer"&gt;&lt;strong&gt;slint-viewer&lt;/strong&gt;&lt;/a&gt; tool which displays the .slint files. The &lt;code&gt;--auto-reload&lt;/code&gt; argument makes it easy to preview your UI while you are working on it (when using the LSP preview is not possible).&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://slintpad.com/"&gt;&lt;strong&gt;SlintPad&lt;/strong&gt;&lt;/a&gt;, an online editor to try out .slint syntax without installing anything (&lt;a href="https://raw.githubusercontent.com/slint-ui/slint/master/tools/slintpad"&gt;sources&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;A &lt;a href="https://www.figma.com/community/plugin/1474418299182276871/figma-to-slint"&gt;&lt;strong&gt;Figma to Slint&lt;/strong&gt;&lt;/a&gt; plugin.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Please check our &lt;a href="https://raw.githubusercontent.com/slint-ui/slint/master/editors/README.md"&gt;Editors README&lt;/a&gt; for tips on how to configure your favorite editor to work well with Slint.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;You can use Slint under &lt;em&gt;&lt;strong&gt;any&lt;/strong&gt;&lt;/em&gt; of the following licenses, at your choice:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Build proprietary desktop, mobile, or web applications for free with the &lt;a href="https://raw.githubusercontent.com/slint-ui/slint/master/LICENSES/LicenseRef-Slint-Royalty-free-2.0.md"&gt;Royalty-free License&lt;/a&gt;,&lt;/li&gt; 
 &lt;li&gt;Build open source embedded, desktop, mobile, or web applications for free with the &lt;a href="https://raw.githubusercontent.com/slint-ui/slint/master/LICENSES/GPL-3.0-only.txt"&gt;GNU GPLv3&lt;/a&gt;,&lt;/li&gt; 
 &lt;li&gt;Build proprietary embedded, desktop, mobile, or web applications with the &lt;a href="https://raw.githubusercontent.com/slint-ui/slint/master/LICENSES/LicenseRef-Slint-Software-3.0.md"&gt;Paid license&lt;/a&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;See the &lt;a href="https://slint.dev/pricing.html"&gt;Slint licensing options on the website&lt;/a&gt; and the &lt;a href="https://raw.githubusercontent.com/slint-ui/slint/master/FAQ.md#licensing"&gt;Licensing FAQ&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributions&lt;/h2&gt; 
&lt;p&gt;We welcome your contributions: in the form of code, bug reports or feedback. For contribution guidelines see &lt;a href="https://raw.githubusercontent.com/slint-ui/slint/master/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Frequently Asked Questions&lt;/h2&gt; 
&lt;p&gt;Please see our separate &lt;a href="https://raw.githubusercontent.com/slint-ui/slint/master/FAQ.md"&gt;FAQ&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;About us (SixtyFPS GmbH)&lt;/h2&gt; 
&lt;p&gt;We are passionate about software - API design, cross-platform software development and user interface components. Our aim is to make developing user interfaces fun for everyone: from Python, JavaScript, C++, or Rust developers all the way to UI/UX designers. We believe that software grows organically and keeping it open source is the best way to sustain that growth. Our team members are located remotely in Germany, Finland, and US.&lt;/p&gt; 
&lt;h3&gt;Stay up to date&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Follow &lt;a href="https://twitter.com/slint_ui"&gt;@slint_ui&lt;/a&gt; on X/Twitter.&lt;/li&gt; 
 &lt;li&gt;Follow &lt;a href="https://mastodon.social/@slint@fosstodon.org"&gt;@slint@fosstodon.org&lt;/a&gt; on Mastodon.&lt;/li&gt; 
 &lt;li&gt;Follow &lt;a href="https://www.linkedin.com/company/slint-ui/"&gt;@slint-ui&lt;/a&gt; on LinkedIn.&lt;/li&gt; 
 &lt;li&gt;Follow &lt;a href="https://bsky.app/profile/slint.dev"&gt;@slint.dev&lt;/a&gt; on Bluesky&lt;/li&gt; 
 &lt;li&gt;Subscribe to our &lt;a href="https://www.youtube.com/@Slint-UI"&gt;YouTube channel&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Contact us&lt;/h3&gt; 
&lt;p&gt;Feel free to join &lt;a href="https://github.com/slint-ui/slint/discussions"&gt;Github discussions&lt;/a&gt; for general chat or questions. Use &lt;a href="https://github.com/slint-ui/slint/issues"&gt;Github issues&lt;/a&gt; to report public suggestions or bugs.&lt;/p&gt; 
&lt;p&gt;We chat in &lt;a href="https://chat.slint.dev"&gt;our Mattermost instance&lt;/a&gt; where you are welcome to listen in or ask your questions.&lt;/p&gt; 
&lt;p&gt;You can of course also contact us privately via email to &lt;a href="mailto://info@slint.dev"&gt;info@slint.dev&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Eventual-Inc/Daft</title>
      <link>https://github.com/Eventual-Inc/Daft</link>
      <description>&lt;p&gt;Distributed query engine providing simple and reliable data processing for any modality and scale&lt;/p&gt;&lt;hr&gt;&lt;p&gt;|Banner|&lt;/p&gt; 
&lt;p&gt;|CI| |PyPI| |Latest Tag| |Coverage| |Slack|&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;Website &amp;lt;https://www.daft.ai&amp;gt;&lt;/code&gt;_ • &lt;code&gt;Docs &amp;lt;https://docs.daft.ai&amp;gt;&lt;/code&gt;_ • &lt;code&gt;Installation &amp;lt;https://docs.daft.ai/en/stable/install/&amp;gt;&lt;/code&gt;_ • &lt;code&gt;Daft Quickstart &amp;lt;https://docs.daft.ai/en/stable/quickstart/&amp;gt;&lt;/code&gt;_ • &lt;code&gt;Community and Support &amp;lt;https://github.com/Eventual-Inc/Daft/discussions&amp;gt;&lt;/code&gt;_&lt;/p&gt; 
&lt;h1&gt;Daft: Unified Engine for Data Analytics, Engineering &amp;amp; ML/AI&lt;/h1&gt; 
&lt;p&gt;&lt;code&gt;Daft &amp;lt;https://www.daft.ai&amp;gt;&lt;/code&gt;_ is a distributed query engine for large-scale data processing using Python or SQL, implemented in Rust.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Familiar interactive API:&lt;/strong&gt; Lazy Python Dataframe for rapid and interactive iteration, or SQL for analytical queries&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Focus on the what:&lt;/strong&gt; Powerful Query Optimizer that rewrites queries to be as efficient as possible&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Data Catalog integrations:&lt;/strong&gt; Full integration with data catalogs such as Apache Iceberg&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Rich multimodal type-system:&lt;/strong&gt; Supports multimodal types such as Images, URLs, Tensors and more&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Seamless Interchange&lt;/strong&gt;: Built on the &lt;code&gt;Apache Arrow &amp;lt;https://arrow.apache.org/docs/index.html&amp;gt;&lt;/code&gt;_ In-Memory Format&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Built for the cloud:&lt;/strong&gt; &lt;code&gt;Record-setting &amp;lt;https://www.daft.ai/blog/announcing-daft-02&amp;gt;&lt;/code&gt;_ I/O performance for integrations with S3 cloud storage&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;About Daft&lt;/code&gt;_&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Getting Started&lt;/code&gt;_&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Benchmarks&lt;/code&gt;_&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Contributing&lt;/code&gt;_&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Telemetry&lt;/code&gt;_&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Related Projects&lt;/code&gt;_&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;License&lt;/code&gt;_&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;About Daft&lt;/h2&gt; 
&lt;p&gt;Daft was designed with the following principles in mind:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Any Data&lt;/strong&gt;: Beyond the usual strings/numbers/dates, Daft columns can also hold complex or nested multimodal data such as Images, Embeddings and Python objects efficiently with its Arrow based memory representation. Ingestion and basic transformations of multimodal data is extremely easy and performant in Daft.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Interactive Computing&lt;/strong&gt;: Daft is built for the interactive developer experience through notebooks or REPLs - intelligent caching/query optimizations accelerates your experimentation and data exploration.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Distributed Computing&lt;/strong&gt;: Some workloads can quickly outgrow your local laptop's computational resources - Daft integrates natively with &lt;code&gt;Ray &amp;lt;https://www.ray.io&amp;gt;&lt;/code&gt;_ for running dataframes on large clusters of machines with thousands of CPUs/GPUs.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;Installation ^^^^^^^^^^^^&lt;/p&gt; 
&lt;p&gt;Install Daft with &lt;code&gt;pip install daft&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;For more advanced installations (e.g. installing from source or with extra dependencies such as Ray and AWS utilities), please see our &lt;code&gt;Installation Guide &amp;lt;https://docs.daft.ai/en/stable/install/&amp;gt;&lt;/code&gt;_&lt;/p&gt; 
&lt;p&gt;Quickstart ^^^^^^^^^^&lt;/p&gt; 
&lt;p&gt;Check out our &lt;code&gt;quickstart &amp;lt;https://docs.daft.ai/en/stable/quickstart/&amp;gt;&lt;/code&gt;_!&lt;/p&gt; 
&lt;p&gt;In this example, we load images from an AWS S3 bucket's URLs and resize each image in the dataframe:&lt;/p&gt; 
&lt;p&gt;.. code:: python&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;import daft

# Load a dataframe from filepaths in an S3 bucket
df = daft.from_glob_path("s3://daft-public-data/laion-sample-images/*")

# 1. Download column of image URLs as a column of bytes
# 2. Decode the column of bytes into a column of images
df = df.with_column("image", df["path"].url.download().image.decode())

# Resize each image into 32x32
df = df.with_column("resized", df["image"].image.resize(32, 32))

df.show(3)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;|Quickstart Image|&lt;/p&gt; 
&lt;h2&gt;Benchmarks&lt;/h2&gt; 
&lt;p&gt;|Benchmark Image|&lt;/p&gt; 
&lt;p&gt;To see the full benchmarks, detailed setup, and logs, check out our &lt;code&gt;benchmarking page. &amp;lt;https://docs.daft.ai/en/stable/resources/benchmarks/tpch/&amp;gt;&lt;/code&gt;_&lt;/p&gt; 
&lt;p&gt;More Resources ^^^^^^^^^^^^^^&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;Daft Quickstart &amp;lt;https://docs.daft.ai/en/stable/quickstart/&amp;gt;&lt;/code&gt;_ - learn more about Daft's full range of capabilities including dataloading from URLs, joins, user-defined functions (UDF), groupby, aggregations and more.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;User Guide &amp;lt;https://docs.daft.ai/en/stable/&amp;gt;&lt;/code&gt;_ - take a deep-dive into each topic within Daft&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;API Reference &amp;lt;https://docs.daft.ai/en/stable/api/&amp;gt;&lt;/code&gt;_ - API reference for public classes/functions of Daft&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;SQL Reference &amp;lt;https://docs.daft.ai/en/stable/sql/&amp;gt;&lt;/code&gt;_ - Daft SQL reference&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We &amp;lt;3 developers! To start contributing to Daft, please read &lt;code&gt;CONTRIBUTING.md &amp;lt;https://github.com/Eventual-Inc/Daft/blob/main/CONTRIBUTING.md&amp;gt;&lt;/code&gt;_. This document describes the development lifecycle and toolchain for working on Daft. It also details how to add new functionality to the core engine and expose it through a Python API.&lt;/p&gt; 
&lt;p&gt;Here's a list of &lt;code&gt;good first issues &amp;lt;https://github.com/Eventual-Inc/Daft/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22&amp;gt;&lt;/code&gt;_ to get yourself warmed up with Daft. Comment in the issue to pick it up, and feel free to ask any questions!&lt;/p&gt; 
&lt;h2&gt;Telemetry&lt;/h2&gt; 
&lt;p&gt;To help improve Daft, we collect non-identifiable data via Scarf (&lt;a href="https://scarf.sh"&gt;https://scarf.sh&lt;/a&gt;).&lt;/p&gt; 
&lt;p&gt;To disable this behavior, set the environment variable &lt;code&gt;DO_NOT_TRACK=true&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;The data that we collect is:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Non-identifiable:&lt;/strong&gt; Events are keyed by a session ID which is generated on import of Daft&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Metadata-only:&lt;/strong&gt; We do not collect any of our users’ proprietary code or data&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;For development only:&lt;/strong&gt; We do not buy or sell any user data&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Please see our &lt;code&gt;documentation &amp;lt;https://docs.daft.ai/en/stable/resources/telemetry/&amp;gt;&lt;/code&gt;_ for more details.&lt;/p&gt; 
&lt;p&gt;.. image:: &lt;a href="https://static.scarf.sh/a.png?x-pxid=31f8d5ba-7e09-4d75-8895-5252bbf06cf6"&gt;https://static.scarf.sh/a.png?x-pxid=31f8d5ba-7e09-4d75-8895-5252bbf06cf6&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Related Projects&lt;/h2&gt; 
&lt;p&gt;+---------------------------------------------------+-----------------+---------------+-------------+-----------------+-----------------------------+-------------+ | Engine | Query Optimizer | Multimodal | Distributed | Arrow Backed | Vectorized Execution Engine | Out-of-core | +===================================================+=================+===============+=============+=================+=============================+=============+ | Daft | Yes | Yes | Yes | Yes | Yes | Yes | +---------------------------------------------------+-----------------+---------------+-------------+-----------------+-----------------------------+-------------+ | &lt;code&gt;Pandas &amp;lt;https://github.com/pandas-dev/pandas&amp;gt;&lt;/code&gt;_ | No | Python object | No | optional &amp;gt;= 2.0 | Some(Numpy) | No | +---------------------------------------------------+-----------------+---------------+-------------+-----------------+-----------------------------+-------------+ | &lt;code&gt;Polars &amp;lt;https://github.com/pola-rs/polars&amp;gt;&lt;/code&gt;_ | Yes | Python object | No | Yes | Yes | Yes | +---------------------------------------------------+-----------------+---------------+-------------+-----------------+-----------------------------+-------------+ | &lt;code&gt;Modin &amp;lt;https://github.com/modin-project/modin&amp;gt;&lt;/code&gt;_ | Yes | Python object | Yes | No | Some(Pandas) | Yes | +---------------------------------------------------+-----------------+---------------+-------------+-----------------+-----------------------------+-------------+ | &lt;code&gt;Pyspark &amp;lt;https://github.com/apache/spark&amp;gt;&lt;/code&gt;_ | Yes | No | Yes | Pandas UDF/IO | Pandas UDF | Yes | +---------------------------------------------------+-----------------+---------------+-------------+-----------------+-----------------------------+-------------+ | &lt;code&gt;Dask DF &amp;lt;https://github.com/dask/dask&amp;gt;&lt;/code&gt;_ | No | Python object | Yes | No | Some(Pandas) | Yes | +---------------------------------------------------+-----------------+---------------+-------------+-----------------+-----------------------------+-------------+&lt;/p&gt; 
&lt;p&gt;Check out our &lt;code&gt;engine comparison page &amp;lt;https://docs.daft.ai/en/stable/resources/engine_comparison/&amp;gt;&lt;/code&gt;_ for more details!&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Daft has an Apache 2.0 license - please see the LICENSE file.&lt;/p&gt; 
&lt;p&gt;.. |Quickstart Image| image:: &lt;a href="https://github.com/Eventual-Inc/Daft/assets/17691182/dea2f515-9739-4f3e-ac58-cd96d51e44a8"&gt;https://github.com/Eventual-Inc/Daft/assets/17691182/dea2f515-9739-4f3e-ac58-cd96d51e44a8&lt;/a&gt; :alt: Dataframe code to load a folder of images from AWS S3 and create thumbnails :height: 256&lt;/p&gt; 
&lt;p&gt;.. |Benchmark Image| image:: &lt;a href="https://github-production-user-asset-6210df.s3.amazonaws.com/2550285/243524430-338e427d-f049-40b3-b555-4059d6be7bfd.png"&gt;https://github-production-user-asset-6210df.s3.amazonaws.com/2550285/243524430-338e427d-f049-40b3-b555-4059d6be7bfd.png&lt;/a&gt; :alt: Benchmarks for SF100 TPCH&lt;/p&gt; 
&lt;p&gt;.. |Banner| image:: &lt;a href="https://daft.ai/images/diagram.png"&gt;https://daft.ai/images/diagram.png&lt;/a&gt; :target: &lt;a href="https://www.daft.ai"&gt;https://www.daft.ai&lt;/a&gt; :alt: Daft dataframes can load any data such as PDF documents, images, protobufs, csv, parquet and audio files into a table dataframe structure for easy querying&lt;/p&gt; 
&lt;p&gt;.. |CI| image:: &lt;a href="https://github.com/Eventual-Inc/Daft/actions/workflows/pr-test-suite.yml/badge.svg"&gt;https://github.com/Eventual-Inc/Daft/actions/workflows/pr-test-suite.yml/badge.svg&lt;/a&gt; :target: &lt;a href="https://github.com/Eventual-Inc/Daft/actions/workflows/pr-test-suite.yml?query=branch:main"&gt;https://github.com/Eventual-Inc/Daft/actions/workflows/pr-test-suite.yml?query=branch:main&lt;/a&gt; :alt: GitHub Actions tests&lt;/p&gt; 
&lt;p&gt;.. |PyPI| image:: &lt;a href="https://img.shields.io/pypi/v/daft.svg?label=pip&amp;amp;logo=PyPI&amp;amp;logoColor=white"&gt;https://img.shields.io/pypi/v/daft.svg?label=pip&amp;amp;logo=PyPI&amp;amp;logoColor=white&lt;/a&gt; :target: &lt;a href="https://pypi.org/project/daft"&gt;https://pypi.org/project/daft&lt;/a&gt; :alt: PyPI&lt;/p&gt; 
&lt;p&gt;.. |Latest Tag| image:: &lt;a href="https://img.shields.io/github/v/tag/Eventual-Inc/Daft?label=latest&amp;amp;logo=GitHub"&gt;https://img.shields.io/github/v/tag/Eventual-Inc/Daft?label=latest&amp;amp;logo=GitHub&lt;/a&gt; :target: &lt;a href="https://github.com/Eventual-Inc/Daft/tags"&gt;https://github.com/Eventual-Inc/Daft/tags&lt;/a&gt; :alt: latest tag&lt;/p&gt; 
&lt;p&gt;.. |Coverage| image:: &lt;a href="https://codecov.io/gh/Eventual-Inc/Daft/branch/main/graph/badge.svg?token=J430QVFE89"&gt;https://codecov.io/gh/Eventual-Inc/Daft/branch/main/graph/badge.svg?token=J430QVFE89&lt;/a&gt; :target: &lt;a href="https://codecov.io/gh/Eventual-Inc/Daft"&gt;https://codecov.io/gh/Eventual-Inc/Daft&lt;/a&gt; :alt: Coverage&lt;/p&gt; 
&lt;p&gt;.. |Slack| image:: &lt;a href="https://img.shields.io/badge/slack-@distdata-purple.svg?logo=slack"&gt;https://img.shields.io/badge/slack-@distdata-purple.svg?logo=slack&lt;/a&gt; :target: &lt;a href="https://join.slack.com/t/dist-data/shared_invite/zt-2e77olvxw-uyZcPPV1SRchhi8ah6ZCtg"&gt;https://join.slack.com/t/dist-data/shared_invite/zt-2e77olvxw-uyZcPPV1SRchhi8ah6ZCtg&lt;/a&gt; :alt: slack community&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>