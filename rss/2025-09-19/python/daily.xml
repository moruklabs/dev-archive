<rss version="2.0">
  <channel>
    <title>GitHub Python Daily Trending</title>
    <description>Daily Trending of Python in GitHub</description>
    <pubDate>Thu, 18 Sep 2025 01:37:12 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>HKUDS/DeepCode</title>
      <link>https://github.com/HKUDS/DeepCode</link>
      <description>&lt;p&gt;"DeepCode: Open Agentic Coding (Paper2Code &amp; Text2Web &amp; Text2Backend)"&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;table style="border: none; margin: 0 auto; padding: 0; border-collapse: collapse;"&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td align="center" style="vertical-align: middle; padding: 10px; border: none; width: 250px;"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/DeepCode/main/assets/logo.png" alt="DeepCode Logo" width="200" style="margin: 0; padding: 0; display: block;" /&gt; &lt;/td&gt; 
    &lt;td align="left" style="vertical-align: middle; padding: 10px 0 10px 30px; border: none;"&gt; &lt;pre style="font-family: 'Courier New', monospace; font-size: 16px; color: #0EA5E9; margin: 0; padding: 0; text-shadow: 0 0 10px #0EA5E9, 0 0 20px rgba(14,165,233,0.5); line-height: 1.2; transform: skew(-1deg, 0deg); display: block;"&gt;    ██████╗ ███████╗███████╗██████╗  ██████╗ ██████╗ ██████╗ ███████╗
    ██╔══██╗██╔════╝██╔════╝██╔══██╗██╔════╝██╔═══██╗██╔══██╗██╔════╝
    ██║  ██║█████╗  █████╗  ██████╔╝██║     ██║   ██║██║  ██║█████╗
    ██║  ██║██╔══╝  ██╔══╝  ██╔═══╝ ██║     ██║   ██║██║  ██║██╔══╝
    ██████╔╝███████╗███████╗██║     ╚██████╗╚██████╔╝██████╔╝███████╗
    ╚═════╝ ╚══════╝╚══════╝╚═╝      ╚═════╝ ╚═════╝ ╚═════╝ ╚══════╝&lt;/pre&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
 &lt;div align="center"&gt; 
  &lt;a href="https://trendshift.io/repositories/14665" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/14665" alt="HKUDS%2FDeepCode | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; 
 &lt;/div&gt; 
 &lt;!-- &lt;img src="https://readme-typing-svg.herokuapp.com?font=Russo+One&amp;size=28&amp;duration=2000&amp;pause=800&amp;color=06B6D4&amp;background=00000000&amp;center=true&amp;vCenter=true&amp;width=800&amp;height=50&amp;lines=%E2%9A%A1+OPEN+AGENTIC+CODING+%E2%9A%A1" alt="DeepCode Tech Subtitle" style="margin-top: 5px; filter: drop-shadow(0 0 12px #06B6D4) drop-shadow(0 0 24px rgba(6,182,212,0.4));"/&gt; --&gt; 
 &lt;h1&gt;&lt;img src="https://github.com/Zongwei9888/Experiment_Images/raw/43c585dca3d21b8e4b6390d835cdd34dc4b4b23d/DeepCode_images/title_logo.svg?sanitize=true" alt="DeepCode Logo" width="32" height="32" style="vertical-align: middle; margin-right: 8px;" /&gt; DeepCode: Open Agentic Coding&lt;/h1&gt; 
 &lt;h3&gt;&lt;em&gt;Advancing Code Generation with Multi-Agent Systems&lt;/em&gt;&lt;/h3&gt; 
 &lt;!-- &lt;p align="center"&gt;
  &lt;img src="https://img.shields.io/badge/Version-1.0.0-00d4ff?style=for-the-badge&amp;logo=rocket&amp;logoColor=white" alt="Version"&gt;

  &lt;img src="https://img.shields.io/badge/License-MIT-4ecdc4?style=for-the-badge&amp;logo=opensourceinitiative&amp;logoColor=white" alt="License"&gt;
  &lt;img src="https://img.shields.io/badge/AI-Multi--Agent-9b59b6?style=for-the-badge&amp;logo=brain&amp;logoColor=white" alt="AI"&gt;
  &lt;img src="https://img.shields.io/badge/HKU-Data_Intelligence_Lab-f39c12?style=for-the-badge&amp;logo=university&amp;logoColor=white" alt="HKU"&gt;
&lt;/p&gt; --&gt; 
 &lt;p&gt; &lt;a href="https://github.com/HKUDS/DeepCode/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/HKUDS/DeepCode?color=00d9ff&amp;amp;style=for-the-badge&amp;amp;logo=star&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/badge/🐍Python-3.13-4ecdc4?style=for-the-badge&amp;amp;logo=python&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt; &lt;a href="https://pypi.org/project/deepcode-hku/"&gt;&lt;img src="https://img.shields.io/pypi/v/deepcode-hku.svg?style=for-the-badge&amp;amp;logo=pypi&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e&amp;amp;color=ff6b6b" /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;p&gt; &lt;a href="https://discord.gg/yF2MmDJyGJ"&gt;&lt;img src="https://img.shields.io/badge/💬Discord-Community-7289da?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;a href="https://github.com/HKUDS/DeepCode/issues/11"&gt;&lt;img src="https://img.shields.io/badge/💬WeChat-Group-07c160?style=for-the-badge&amp;amp;logo=wechat&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;div style="width: 100%; height: 2px; margin: 20px 0; background: linear-gradient(90deg, transparent, #00d9ff, transparent);"&gt;&lt;/div&gt; 
 &lt;/div&gt; 
 &lt;div align="center"&gt; 
  &lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-quick-start" style="text-decoration: none;"&gt; &lt;img src="https://img.shields.io/badge/Quick%20Start-Get%20Started%20Now-00d9ff?style=for-the-badge&amp;amp;logo=rocket&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt; &lt;/a&gt; 
 &lt;/div&gt; 
 &lt;h3&gt;🖥️ &lt;strong&gt;Interface Showcase&lt;/strong&gt;&lt;/h3&gt; 
 &lt;table align="center" width="100%" style="border: none; border-collapse: collapse; margin: 30px 0;"&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td width="50%" align="center" style="vertical-align: top; padding: 20px;"&gt; &lt;h4&gt;🖥️ &lt;strong&gt;CLI Interface&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;Terminal-Based Development&lt;/strong&gt;&lt;/p&gt; 
     &lt;div align="center"&gt; 
      &lt;img src="https://github.com/Zongwei9888/Experiment_Images/raw/8882a7313c504ca97ead6e7b36c51aa761b6a4f3/DeepCode_images/CLI.gif" alt="CLI Interface Demo" width="100%" style="border-radius: 10px; box-shadow: 0 8px 20px rgba(45,55,72,0.3); margin: 15px 0;" /&gt; 
      &lt;div style="background: linear-gradient(135deg, #2D3748 0%, #4A5568 100%); border-radius: 12px; padding: 15px; margin: 15px 0; color: white;"&gt; 
       &lt;strong&gt;🚀 Advanced Terminal Experience&lt;/strong&gt;
       &lt;br /&gt; 
       &lt;small&gt;⚡ Fast command-line workflow&lt;br /&gt;🔧 Developer-friendly interface&lt;br /&gt;📊 Real-time progress tracking&lt;/small&gt; 
      &lt;/div&gt; 
      &lt;p&gt;&lt;em&gt;Professional terminal interface for advanced users and CI/CD integration&lt;/em&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
    &lt;td width="50%" align="center" style="vertical-align: top; padding: 20px;"&gt; &lt;h4&gt;🌐 &lt;strong&gt;Web Interface&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;Visual Interactive Experience&lt;/strong&gt;&lt;/p&gt; 
     &lt;div align="center"&gt; 
      &lt;img src="https://github.com/Zongwei9888/Experiment_Images/raw/8882a7313c504ca97ead6e7b36c51aa761b6a4f3/DeepCode_images/UI.gif" alt="Web Interface Demo" width="100%" style="border-radius: 10px; box-shadow: 0 8px 20px rgba(14,165,233,0.3); margin: 15px 0;" /&gt; 
      &lt;div style="background: linear-gradient(135deg, #0EA5E9 0%, #00D4FF 100%); border-radius: 12px; padding: 15px; margin: 15px 0; color: white;"&gt; 
       &lt;strong&gt;🎨 Modern Web Dashboard&lt;/strong&gt;
       &lt;br /&gt; 
       &lt;small&gt;🖱️ Intuitive drag-and-drop&lt;br /&gt;📱 Responsive design&lt;br /&gt;🎯 Visual progress tracking&lt;/small&gt; 
      &lt;/div&gt; 
      &lt;p&gt;&lt;em&gt;Beautiful web interface with streamlined workflow for all skill levels&lt;/em&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
 &lt;hr /&gt; 
 &lt;div align="center"&gt; 
  &lt;h3&gt;🎬 &lt;strong&gt;Introduction Video&lt;/strong&gt;&lt;/h3&gt; 
  &lt;div style="margin: 20px 0;"&gt; 
   &lt;a href="https://youtu.be/PRgmP8pOI08" target="_blank"&gt; &lt;img src="https://img.youtube.com/vi/PRgmP8pOI08/maxresdefault.jpg" alt="DeepCode Introduction Video" width="75%" style="border-radius: 12px; box-shadow: 0 8px 25px rgba(0,0,0,0.15); transition: transform 0.3s ease;" /&gt; &lt;/a&gt; 
  &lt;/div&gt; 
  &lt;p&gt;&lt;em&gt;🎯 &lt;strong&gt;Watch our complete introduction&lt;/strong&gt; - See how DeepCode transforms research papers and natural language into production-ready code&lt;/em&gt;&lt;/p&gt; 
  &lt;p&gt; &lt;a href="https://youtu.be/PRgmP8pOI08" target="_blank"&gt; &lt;img src="https://img.shields.io/badge/▶️_Watch_Video-FF0000?style=for-the-badge&amp;amp;logo=youtube&amp;amp;logoColor=white" alt="Watch Video" /&gt; &lt;/a&gt; &lt;/p&gt; 
 &lt;/div&gt; 
 &lt;hr /&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;em&gt;"Where AI Agents Transform Ideas into Production-Ready Code"&lt;/em&gt;&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;📑 Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-key-features"&gt;🚀 Key Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#%EF%B8%8F-architecture"&gt;🏗️ Architecture&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-quick-start"&gt;🚀 Quick Start&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-examples"&gt;💡 Examples&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-live-demonstrations"&gt;🎬 Live Demonstrations&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-star-history"&gt;⭐ Star History&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-license"&gt;📄 License&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🚀 Key Features&lt;/h2&gt; 
&lt;br /&gt; 
&lt;table align="center" width="100%" style="border: none; table-layout: fixed;"&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td width="30%" align="center" style="vertical-align: top; padding: 20px;"&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;h3 style="margin: 0; padding: 0;"&gt;🚀 &lt;strong&gt;Paper2Code&lt;/strong&gt;&lt;/h3&gt; 
    &lt;/div&gt; 
    &lt;div align="center" style="margin: 15px 0;"&gt; 
     &lt;img src="https://img.shields.io/badge/ALGORITHM-IMPLEMENTATION-ff6b6b?style=for-the-badge&amp;amp;logo=algorithm&amp;amp;logoColor=white" alt="Algorithm Badge" /&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;&lt;strong&gt;Automated Implementation of Complex Algorithms&lt;/strong&gt;&lt;/p&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 60px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;Effortlessly converts complex algorithms from research papers into &lt;strong&gt;high-quality&lt;/strong&gt;, &lt;strong&gt;production-ready&lt;/strong&gt; code, accelerating algorithm reproduction.&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
   &lt;td width="30%" align="center" style="vertical-align: top; padding: 20px;"&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;h3 style="margin: 0; padding: 0;"&gt;🎨 &lt;strong&gt;Text2Web&lt;/strong&gt;&lt;/h3&gt; 
    &lt;/div&gt; 
    &lt;div align="center" style="margin: 15px 0;"&gt; 
     &lt;img src="https://img.shields.io/badge/FRONTEND-DEVELOPMENT-4ecdc4?style=for-the-badge&amp;amp;logo=react&amp;amp;logoColor=white" alt="Frontend Badge" /&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;&lt;strong&gt;Automated Front-End Web Development&lt;/strong&gt;&lt;/p&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 60px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;Translates plain textual descriptions into &lt;strong&gt;fully functional&lt;/strong&gt;, &lt;strong&gt;visually appealing&lt;/strong&gt; front-end web code for rapid interface creation.&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
   &lt;td width="30%" align="center" style="vertical-align: top; padding: 20px;"&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;h3 style="margin: 0; padding: 0;"&gt;⚙️ &lt;strong&gt;Text2Backend&lt;/strong&gt;&lt;/h3&gt; 
    &lt;/div&gt; 
    &lt;div align="center" style="margin: 15px 0;"&gt; 
     &lt;img src="https://img.shields.io/badge/BACKEND-DEVELOPMENT-9b59b6?style=for-the-badge&amp;amp;logo=server&amp;amp;logoColor=white" alt="Backend Badge" /&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;&lt;strong&gt;Automated Back-End Development&lt;/strong&gt;&lt;/p&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 60px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;Generates &lt;strong&gt;efficient&lt;/strong&gt;, &lt;strong&gt;scalable&lt;/strong&gt;, and &lt;strong&gt;feature-rich&lt;/strong&gt; back-end code from simple text inputs, streamlining server-side development.&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;br /&gt; 
&lt;h3&gt;🎯 &lt;strong&gt;Autonomous Multi-Agent Workflow&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;The Challenges&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;📄 &lt;strong&gt;Implementation Complexity&lt;/strong&gt;: Converting academic papers and complex algorithms into working code requires significant technical effort and domain expertise&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;🔬 &lt;strong&gt;Research Bottleneck&lt;/strong&gt;: Researchers spend valuable time implementing algorithms instead of focusing on their core research and discovery work&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;⏱️ &lt;strong&gt;Development Delays&lt;/strong&gt;: Product teams experience long wait times between concept and testable prototypes, slowing down innovation cycles&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;🔄 &lt;strong&gt;Repetitive Coding&lt;/strong&gt;: Developers repeatedly implement similar patterns and functionality instead of building on existing solutions&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;DeepCode&lt;/strong&gt; addresses these workflow inefficiencies by providing reliable automation for common development tasks, streamlining your development workflow from concept to code.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;pre&gt;&lt;code class="language-mermaid"&gt;flowchart LR
    A["📄 Research Papers&amp;lt;br/&amp;gt;💬 Text Prompts&amp;lt;br/&amp;gt;🌐 URLs &amp;amp; Document&amp;lt;br/&amp;gt;📎 Files: PDF, DOC, PPTX, TXT, HTML"] --&amp;gt; B["🧠 DeepCode&amp;lt;br/&amp;gt;Multi-Agent Engine"]
    B --&amp;gt; C["🚀 Algorithm Implementation &amp;lt;br/&amp;gt;🎨 Frontend Development &amp;lt;br/&amp;gt;⚙️ Backend Development"]

    style A fill:#ff6b6b,stroke:#c0392b,stroke-width:2px,color:#000
    style B fill:#00d4ff,stroke:#0984e3,stroke-width:3px,color:#000
    style C fill:#00b894,stroke:#00a085,stroke-width:2px,color:#000
&lt;/code&gt;&lt;/pre&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🏗️ Architecture&lt;/h2&gt; 
&lt;h3&gt;📊 &lt;strong&gt;System Overview&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;DeepCode&lt;/strong&gt; is an AI-powered development platform that automates code generation and implementation tasks. Our multi-agent system handles the complexity of translating requirements into functional, well-structured code, allowing you to focus on innovation rather than implementation details.&lt;/p&gt; 
&lt;p&gt;🎯 &lt;strong&gt;Technical Capabilities&lt;/strong&gt;:&lt;/p&gt; 
&lt;p&gt;🧬 &lt;strong&gt;Research-to-Production Pipeline&lt;/strong&gt;&lt;br /&gt; Multi-modal document analysis engine that extracts algorithmic logic and mathematical models from academic papers. Generates optimized implementations with proper data structures while preserving computational complexity characteristics.&lt;/p&gt; 
&lt;p&gt;🪄 &lt;strong&gt;Natural Language Code Synthesis&lt;/strong&gt;&lt;br /&gt; Context-aware code generation using fine-tuned language models trained on curated code repositories. Maintains architectural consistency across modules while supporting multiple programming languages and frameworks.&lt;/p&gt; 
&lt;p&gt;⚡ &lt;strong&gt;Automated Prototyping Engine&lt;/strong&gt;&lt;br /&gt; Intelligent scaffolding system generating complete application structures including database schemas, API endpoints, and frontend components. Uses dependency analysis to ensure scalable architecture from initial generation.&lt;/p&gt; 
&lt;p&gt;💎 &lt;strong&gt;Quality Assurance Automation&lt;/strong&gt;&lt;br /&gt; Integrated static analysis with automated unit test generation and documentation synthesis. Employs AST analysis for code correctness and property-based testing for comprehensive coverage.&lt;/p&gt; 
&lt;p&gt;🔮 &lt;strong&gt;CodeRAG Integration System&lt;/strong&gt;&lt;br /&gt; Advanced retrieval-augmented generation combining semantic vector embeddings with graph-based dependency analysis. Automatically discovers optimal libraries and implementation patterns from large-scale code corpus.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;🔧 &lt;strong&gt;Core Techniques&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;🧠 &lt;strong&gt;Intelligent Orchestration Agent&lt;/strong&gt;: Central decision-making system that coordinates workflow phases and analyzes requirements. Employs dynamic planning algorithms to adapt execution strategies in real-time based on evolving project complexity. Dynamically selects optimal processing strategies for each implementation step. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;💾 &lt;strong&gt;Efficient Memory Mechanism&lt;/strong&gt;: Advanced context engineering system that manages large-scale code contexts efficiently. Implements hierarchical memory structures with intelligent compression for handling complex codebases. This component enables instant retrieval of implementation patterns and maintains semantic coherence across extended development sessions. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;🔍 &lt;strong&gt;Advanced CodeRAG System&lt;/strong&gt;: Global code comprehension engine that analyzes complex inter-dependencies across repositories. Performs cross-codebase relationship mapping to understand architectural patterns from a holistic perspective. This module leverages dependency graphs and semantic analysis to provide globally-aware code recommendations during implementation.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;🤖 &lt;strong&gt;Multi-Agent Architecture of DeepCode&lt;/strong&gt;:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;🎯 Central Orchestrating Agent&lt;/strong&gt;: Orchestrates entire workflow execution and makes strategic decisions. Coordinates specialized agents based on input complexity analysis. Implements dynamic task planning and resource allocation algorithms. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;📝 Intent Understanding Agent&lt;/strong&gt;: Performs deep semantic analysis of user requirements to decode complex intentions. Extracts functional specifications and technical constraints through advanced NLP processing. Transforms ambiguous human descriptions into precise, actionable development specifications with structured task decomposition. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;📄 Document Parsing Agent&lt;/strong&gt;: Processes complex technical documents and research papers with advanced parsing capabilities. Extracts algorithms and methodologies using document understanding models. Converts academic concepts into practical implementation specifications through intelligent content analysis. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;🏗️ Code Planning Agent&lt;/strong&gt;: Performs architectural design and technology stack optimization. Dynamic planning for adaptive development roadmaps. Enforces coding standards and generates modular structures through automated design pattern selection.&lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;🔍 Code Reference Mining Agent&lt;/strong&gt;: Discovers relevant repositories and frameworks through intelligent search algorithms. Analyzes codebases for compatibility and integration potential. Provides recommendations based on similarity metrics and automated dependency analysis. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;📚 Code Indexing Agent&lt;/strong&gt;: Builds comprehensive knowledge graphs of discovered codebases. Maintains semantic relationships between code components. Enables intelligent retrieval and cross-reference capabilities. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;🧬 Code Generation Agent&lt;/strong&gt;: Synthesizes gathered information into executable code implementations. Creates functional interfaces and integrates discovered components. Generates comprehensive test suites and documentation for reproducibility.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h4&gt;🛠️ &lt;strong&gt;Implementation Tools Matrix&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;🔧 Powered by MCP (Model Context Protocol)&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;DeepCode leverages the &lt;strong&gt;Model Context Protocol (MCP)&lt;/strong&gt; standard to seamlessly integrate with various tools and services. This standardized approach ensures reliable communication between AI agents and external systems, enabling powerful automation capabilities.&lt;/p&gt; 
&lt;h5&gt;📡 &lt;strong&gt;MCP Servers &amp;amp; Tools&lt;/strong&gt;&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;🛠️ &lt;strong&gt;MCP Server&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;🔧 &lt;strong&gt;Primary Function&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;💡 &lt;strong&gt;Purpose &amp;amp; Capabilities&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;🔍 brave&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Web Search Engine&lt;/td&gt; 
   &lt;td&gt;Real-time information retrieval via Brave Search API&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;🌐 bocha-mcp&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Alternative Search&lt;/td&gt; 
   &lt;td&gt;Secondary search option with independent API access&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;📂 filesystem&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;File System Operations&lt;/td&gt; 
   &lt;td&gt;Local file and directory management, read/write operations&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;🌐 fetch&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Web Content Retrieval&lt;/td&gt; 
   &lt;td&gt;Fetch and extract content from URLs and web resources&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;📥 github-downloader&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Repository Management&lt;/td&gt; 
   &lt;td&gt;Clone and download GitHub repositories for analysis&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;📋 file-downloader&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Document Processing&lt;/td&gt; 
   &lt;td&gt;Download and convert files (PDF, DOCX, etc.) to Markdown&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;⚡ command-executor&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;System Commands&lt;/td&gt; 
   &lt;td&gt;Execute bash/shell commands for environment management&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;🧬 code-implementation&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Code Generation Hub&lt;/td&gt; 
   &lt;td&gt;Comprehensive code reproduction with execution and testing&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;📚 code-reference-indexer&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Smart Code Search&lt;/td&gt; 
   &lt;td&gt;Intelligent indexing and search of code repositories&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;📄 document-segmentation&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Smart Document Analysis&lt;/td&gt; 
   &lt;td&gt;Intelligent document segmentation for large papers and technical documents&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h5&gt;🔧 &lt;strong&gt;Legacy Tool Functions&lt;/strong&gt; &lt;em&gt;(for reference)&lt;/em&gt;&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;🛠️ &lt;strong&gt;Function&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;🎯 &lt;strong&gt;Usage Context&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;📄 read_code_mem&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Efficient code context retrieval from memory&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;✍️ write_file&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Direct file content generation and modification&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;🐍 execute_python&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Python code testing and validation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;📁 get_file_structure&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Project structure analysis and organization&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;⚙️ set_workspace&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Dynamic workspace and environment configuration&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;📊 get_operation_history&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Process monitoring and operation tracking&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;p&gt;🎛️ &lt;strong&gt;Multi-Interface Framework&lt;/strong&gt;&lt;br /&gt; RESTful API with CLI and web frontends featuring real-time code streaming, interactive debugging, and extensible plugin architecture for CI/CD integration.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;🚀 Multi-Agent Intelligent Pipeline:&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;h3&gt;🌟 &lt;strong&gt;Intelligence Processing Flow&lt;/strong&gt;&lt;/h3&gt; 
 &lt;table align="center" width="100%" style="border: none; border-collapse: collapse;"&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td colspan="3" align="center" style="padding: 20px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; color: white; font-weight: bold;"&gt; 💡 &lt;strong&gt;INPUT LAYER&lt;/strong&gt;&lt;br /&gt; 📄 Research Papers • 💬 Natural Language • 🌐 URLs • 📋 Requirements &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="20"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td colspan="3" align="center" style="padding: 15px; background: linear-gradient(135deg, #ff6b6b 0%, #ee5a24 100%); border-radius: 12px; color: white; font-weight: bold;"&gt; 🎯 &lt;strong&gt;CENTRAL ORCHESTRATION&lt;/strong&gt;&lt;br /&gt; Strategic Decision Making • Workflow Coordination • Agent Management &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="15"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center" style="padding: 12px; background: linear-gradient(135deg, #3742fa 0%, #2f3542 100%); border-radius: 10px; color: white; width: 50%;"&gt; 📝 &lt;strong&gt;TEXT ANALYSIS&lt;/strong&gt;&lt;br /&gt; &lt;small&gt;Requirement Processing&lt;/small&gt; &lt;/td&gt; 
    &lt;td width="10"&gt;&lt;/td&gt; 
    &lt;td align="center" style="padding: 12px; background: linear-gradient(135deg, #8c7ae6 0%, #9c88ff 100%); border-radius: 10px; color: white; width: 50%;"&gt; 📄 &lt;strong&gt;DOCUMENT ANALYSIS&lt;/strong&gt;&lt;br /&gt; &lt;small&gt;Paper &amp;amp; Spec Processing&lt;/small&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="15"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td colspan="3" align="center" style="padding: 15px; background: linear-gradient(135deg, #00d2d3 0%, #54a0ff 100%); border-radius: 12px; color: white; font-weight: bold;"&gt; 📋 &lt;strong&gt;REPRODUCTION PLANNING&lt;/strong&gt;&lt;br /&gt; Deep Paper Analysis • Code Requirements Parsing • Reproduction Strategy Development &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="15"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center" style="padding: 12px; background: linear-gradient(135deg, #ffa726 0%, #ff7043 100%); border-radius: 10px; color: white; width: 50%;"&gt; 🔍 &lt;strong&gt;REFERENCE ANALYSIS&lt;/strong&gt;&lt;br /&gt; &lt;small&gt;Repository Discovery&lt;/small&gt; &lt;/td&gt; 
    &lt;td width="10"&gt;&lt;/td&gt; 
    &lt;td align="center" style="padding: 12px; background: linear-gradient(135deg, #e056fd 0%, #f368e0 100%); border-radius: 10px; color: white; width: 50%;"&gt; 📚 &lt;strong&gt;CODE INDEXING&lt;/strong&gt;&lt;br /&gt; &lt;small&gt;Knowledge Graph Building&lt;/small&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="15"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td colspan="3" align="center" style="padding: 15px; background: linear-gradient(135deg, #26de81 0%, #20bf6b 100%); border-radius: 12px; color: white; font-weight: bold;"&gt; 🧬 &lt;strong&gt;CODE IMPLEMENTATION&lt;/strong&gt;&lt;br /&gt; Implementation Generation • Testing • Documentation &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="15"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td colspan="3" align="center" style="padding: 20px; background: linear-gradient(135deg, #045de9 0%, #09c6f9 100%); border-radius: 15px; color: white; font-weight: bold;"&gt; ⚡ &lt;strong&gt;OUTPUT DELIVERY&lt;/strong&gt;&lt;br /&gt; 📦 Complete Codebase • 🧪 Test Suite • 📚 Documentation • 🚀 Deployment Ready &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;br /&gt; 
 &lt;h3&gt;🔄 &lt;strong&gt;Process Intelligence Features&lt;/strong&gt;&lt;/h3&gt; 
 &lt;table align="center" style="border: none;"&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td align="center" width="25%" style="padding: 15px;"&gt; 
     &lt;div style="background: #f8f9fa; border-radius: 10px; padding: 15px; border-left: 4px solid #ff6b6b;"&gt; 
      &lt;h4&gt;🎯 Adaptive Flow&lt;/h4&gt; 
      &lt;p&gt;&lt;small&gt;Dynamic agent selection based on input complexity&lt;/small&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
    &lt;td align="center" width="25%" style="padding: 15px;"&gt; 
     &lt;div style="background: #f8f9fa; border-radius: 10px; padding: 15px; border-left: 4px solid #4ecdc4;"&gt; 
      &lt;h4&gt;🧠 Smart Coordination&lt;/h4&gt; 
      &lt;p&gt;&lt;small&gt;Intelligent task distribution and parallel processing&lt;/small&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
    &lt;td align="center" width="25%" style="padding: 15px;"&gt; 
     &lt;div style="background: #f8f9fa; border-radius: 10px; padding: 15px; border-left: 4px solid #45b7d1;"&gt; 
      &lt;h4&gt;🔍 Context Awareness&lt;/h4&gt; 
      &lt;p&gt;&lt;small&gt;Deep understanding through CodeRAG integration&lt;/small&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
    &lt;td align="center" width="25%" style="padding: 15px;"&gt; 
     &lt;div style="background: #f8f9fa; border-radius: 10px; padding: 15px; border-left: 4px solid #96ceb4;"&gt; 
      &lt;h4&gt;⚡ Quality Assurance&lt;/h4&gt; 
      &lt;p&gt;&lt;small&gt;Automated testing and validation throughout&lt;/small&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🚀 Quick Start&lt;/h2&gt; 
&lt;h3&gt;📦 &lt;strong&gt;Step 1: Installation&lt;/strong&gt;&lt;/h3&gt; 
&lt;h4&gt;⚡ &lt;strong&gt;Direct Installation (Recommended)&lt;/strong&gt;&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 🚀 Install DeepCode package directly
pip install deepcode-hku

# 🔑 Download configuration files
curl -O https://raw.githubusercontent.com/HKUDS/DeepCode/main/mcp_agent.config.yaml
curl -O https://raw.githubusercontent.com/HKUDS/DeepCode/main/mcp_agent.secrets.yaml

# 🔑 Configure API keys (required)
# Edit mcp_agent.secrets.yaml with your API keys and base_url:
# - openai: api_key, base_url (for OpenAI/custom endpoints)
# - anthropic: api_key (for Claude models)

# 🔑 Configure search API keys for web search (optional)
# Edit mcp_agent.config.yaml to set your API keys:
# - For Brave Search: Set BRAVE_API_KEY: "your_key_here" in brave.env section (line ~28)
# - For Bocha-MCP: Set BOCHA_API_KEY: "your_key_here" in bocha-mcp.env section (line ~74)

# 📄 Configure document segmentation (optional)
# Edit mcp_agent.config.yaml to control document processing:
# - enabled: true/false (whether to use intelligent document segmentation)
# - size_threshold_chars: 50000 (document size threshold to trigger segmentation)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;🔧 &lt;strong&gt;Development Installation (From Source)&lt;/strong&gt;&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;📂 Click to expand development installation options&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h5&gt;🔥 &lt;strong&gt;Using UV (Recommended for Development)&lt;/strong&gt;&lt;/h5&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# 🔽 Clone the repository
git clone https://github.com/HKUDS/DeepCode.git
cd DeepCode/

# 📦 Install UV package manager
curl -LsSf https://astral.sh/uv/install.sh | sh

# 🔧 Install dependencies with UV
uv venv --python=3.13
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
uv pip install -r requirements.txt

# 🔑 Configure API keys (required)
# Edit mcp_agent.secrets.yaml with your API keys and base_url:
# - openai: api_key, base_url (for OpenAI/custom endpoints)
# - anthropic: api_key (for Claude models)

# 🔑 Configure search API keys for web search (optional)
# Edit mcp_agent.config.yaml to set your API keys:
# - For Brave Search: Set BRAVE_API_KEY: "your_key_here" in brave.env section (line ~28)
# - For Bocha-MCP: Set BOCHA_API_KEY: "your_key_here" in bocha-mcp.env section (line ~74)

# 📄 Configure document segmentation (optional)
# Edit mcp_agent.config.yaml to control document processing:
# - enabled: true/false (whether to use intelligent document segmentation)
# - size_threshold_chars: 50000 (document size threshold to trigger segmentation)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h5&gt;🐍 &lt;strong&gt;Using Traditional pip&lt;/strong&gt;&lt;/h5&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# 🔽 Clone the repository
git clone https://github.com/HKUDS/DeepCode.git
cd DeepCode/

# 📦 Install dependencies
pip install -r requirements.txt

# 🔑 Configure API keys (required)
# Edit mcp_agent.secrets.yaml with your API keys and base_url:
# - openai: api_key, base_url (for OpenAI/custom endpoints)
# - anthropic: api_key (for Claude models)

# 🔑 Configure search API keys for web search (optional)
# Edit mcp_agent.config.yaml to set your API keys:
# - For Brave Search: Set BRAVE_API_KEY: "your_key_here" in brave.env section (line ~28)
# - For Bocha-MCP: Set BOCHA_API_KEY: "your_key_here" in bocha-mcp.env section (line ~74)

# 📄 Configure document segmentation (optional)
# Edit mcp_agent.config.yaml to control document processing:
# - enabled: true/false (whether to use intelligent document segmentation)
# - size_threshold_chars: 50000 (document size threshold to trigger segmentation)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h4&gt;🪟 &lt;strong&gt;Windows Users: Additional MCP Server Configuration&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;If you're using Windows, you may need to configure MCP servers manually in &lt;code&gt;mcp_agent.config.yaml&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 1. Install MCP servers globally
npm i -g @modelcontextprotocol/server-brave-search
npm i -g @modelcontextprotocol/server-filesystem

# 2. Find your global node_modules path
npm -g root
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then update your &lt;code&gt;mcp_agent.config.yaml&lt;/code&gt; to use absolute paths:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;mcp:
  servers:
    brave:
      command: "node"
      args: ["C:/Program Files/nodejs/node_modules/@modelcontextprotocol/server-brave-search/dist/index.js"]
    filesystem:
      command: "node"
      args: ["C:/Program Files/nodejs/node_modules/@modelcontextprotocol/server-filesystem/dist/index.js", "."]
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Replace the path with your actual global node_modules path from step 2.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;🔍 &lt;strong&gt;Search Server Configuration (Optional)&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;DeepCode supports multiple search servers for web search functionality. You can configure your preferred option in &lt;code&gt;mcp_agent.config.yaml&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;# Default search server configuration
# Options: "brave" or "bocha-mcp"
default_search_server: "brave"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Available Options:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;🔍 Brave Search&lt;/strong&gt; (&lt;code&gt;"brave"&lt;/code&gt;):&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Default option with high-quality search results&lt;/li&gt; 
   &lt;li&gt;Requires BRAVE_API_KEY configuration&lt;/li&gt; 
   &lt;li&gt;Recommended for most users&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;🌐 Bocha-MCP&lt;/strong&gt; (&lt;code&gt;"bocha-mcp"&lt;/code&gt;):&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Alternative search server option&lt;/li&gt; 
   &lt;li&gt;Requires BOCHA_API_KEY configuration&lt;/li&gt; 
   &lt;li&gt;Uses local Python server implementation&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;API Key Configuration in mcp_agent.config.yaml:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;# For Brave Search (default) - around line 28
brave:
  command: "npx"
  args: ["-y", "@modelcontextprotocol/server-brave-search"]
  env:
    BRAVE_API_KEY: "your_brave_api_key_here"

# For Bocha-MCP (alternative) - around line 74
bocha-mcp:
  command: "python"
  args: ["tools/bocha_search_server.py"]
  env:
    PYTHONPATH: "."
    BOCHA_API_KEY: "your_bocha_api_key_here"
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;💡 Tip&lt;/strong&gt;: Both search servers require API key configuration. Choose the one that best fits your API access and requirements.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;⚡ &lt;strong&gt;Step 2: Launch Application&lt;/strong&gt;&lt;/h3&gt; 
&lt;h4&gt;🚀 &lt;strong&gt;Using Installed Package (Recommended)&lt;/strong&gt;&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 🌐 Launch web interface directly
deepcode

# The application will automatically start at http://localhost:8501
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;🛠️ &lt;strong&gt;Using Source Code&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;Choose your preferred interface:&lt;/p&gt; 
&lt;h5&gt;🌐 &lt;strong&gt;Web Interface&lt;/strong&gt; (Recommended)&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Using UV
uv run streamlit run ui/streamlit_app.py
# Or using traditional Python
streamlit run ui/streamlit_app.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://img.shields.io/badge/Access-localhost:8501-00d4ff?style=flat-square&amp;amp;logo=streamlit&amp;amp;logoColor=white" alt="Web Access" /&gt; 
&lt;/div&gt; 
&lt;h5&gt;🖥️ &lt;strong&gt;CLI Interface&lt;/strong&gt; (Advanced Users)&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Using UV
uv run python cli/main_cli.py
# Or using traditional Python
python cli/main_cli.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://img.shields.io/badge/Mode-Interactive_Terminal-9b59b6?style=flat-square&amp;amp;logo=terminal&amp;amp;logoColor=white" alt="CLI Mode" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;🎯 &lt;strong&gt;Step 3: Generate Code&lt;/strong&gt;&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;📄 Input&lt;/strong&gt;: Upload your research paper, provide requirements, or paste a URL&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🤖 Processing&lt;/strong&gt;: Watch the multi-agent system analyze and plan&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;⚡ Output&lt;/strong&gt;: Receive production-ready code with tests and documentation&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr /&gt; 
&lt;h2&gt;💡 Examples&lt;/h2&gt; 
&lt;h3&gt;🎬 &lt;strong&gt;Live Demonstrations&lt;/strong&gt;&lt;/h3&gt; 
&lt;table align="center"&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td width="33%" align="center"&gt; &lt;h4&gt;📄 &lt;strong&gt;Paper2Code Demo&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;Research to Implementation&lt;/strong&gt;&lt;/p&gt; 
    &lt;div align="center"&gt; 
     &lt;a href="https://www.youtube.com/watch?v=MQZYpLkzsbw"&gt; &lt;img src="https://img.youtube.com/vi/MQZYpLkzsbw/maxresdefault.jpg" alt="Paper2Code Demo" width="100%" style="border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);" /&gt; &lt;/a&gt; 
     &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.youtube.com/watch?v=MQZYpLkzsbw"&gt;▶️ Watch Demo&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
     &lt;p&gt;&lt;em&gt;Transform academic papers into production-ready code automatically&lt;/em&gt;&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
   &lt;td width="33%" align="center"&gt; &lt;h4&gt;🖼️ &lt;strong&gt;Image Processing Demo&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;AI-Powered Image Tools&lt;/strong&gt;&lt;/p&gt; 
    &lt;div align="center"&gt; 
     &lt;a href="https://www.youtube.com/watch?v=nFt5mLaMEac"&gt; &lt;img src="https://img.youtube.com/vi/nFt5mLaMEac/maxresdefault.jpg" alt="Image Processing Demo" width="100%" style="border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);" /&gt; &lt;/a&gt; 
     &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.youtube.com/watch?v=nFt5mLaMEac"&gt;▶️ Watch Demo&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
     &lt;p&gt;&lt;em&gt;Intelligent image processing with background removal and enhancement&lt;/em&gt;&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
   &lt;td width="33%" align="center"&gt; &lt;h4&gt;🌐 &lt;strong&gt;Frontend Implementation&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;Complete Web Application&lt;/strong&gt;&lt;/p&gt; 
    &lt;div align="center"&gt; 
     &lt;a href="https://www.youtube.com/watch?v=78wx3dkTaAU"&gt; &lt;img src="https://img.youtube.com/vi/78wx3dkTaAU/maxresdefault.jpg" alt="Frontend Demo" width="100%" style="border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);" /&gt; &lt;/a&gt; 
     &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.youtube.com/watch?v=78wx3dkTaAU"&gt;▶️ Watch Demo&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
     &lt;p&gt;&lt;em&gt;Full-stack web development from concept to deployment&lt;/em&gt;&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;🆕 &lt;strong&gt;Recent Updates&lt;/strong&gt;&lt;/h3&gt; 
&lt;h4&gt;📄 &lt;strong&gt;Smart Document Segmentation (v1.2.0)&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Intelligent Processing&lt;/strong&gt;: Automatically handles large research papers and technical documents that exceed LLM token limits&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Configurable Control&lt;/strong&gt;: Toggle segmentation via configuration with size-based thresholds&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Semantic Analysis&lt;/strong&gt;: Advanced content understanding with algorithm, concept, and formula preservation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Backward Compatibility&lt;/strong&gt;: Seamlessly falls back to traditional processing for smaller documents&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;🚀 &lt;strong&gt;Coming Soon&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;We're continuously enhancing DeepCode with exciting new features:&lt;/p&gt; 
&lt;h4&gt;🔧 &lt;strong&gt;Enhanced Code Reliability &amp;amp; Validation&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Automated Testing&lt;/strong&gt;: Comprehensive functionality testing with execution verification and error detection.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Code Quality Assurance&lt;/strong&gt;: Multi-level validation through static analysis, dynamic testing, and performance benchmarking.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Smart Debugging&lt;/strong&gt;: AI-powered error detection with automatic correction suggestions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;📊 &lt;strong&gt;PaperBench Performance Showcase&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Benchmark Dashboard&lt;/strong&gt;: Comprehensive performance metrics on the PaperBench evaluation suite.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Accuracy Metrics&lt;/strong&gt;: Detailed comparison with state-of-the-art paper reproduction systems.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Success Analytics&lt;/strong&gt;: Statistical analysis across paper categories and complexity levels.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;⚡ &lt;strong&gt;System-wide Optimizations&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Performance Boost&lt;/strong&gt;: Multi-threaded processing and optimized agent coordination for faster generation.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Enhanced Reasoning&lt;/strong&gt;: Advanced reasoning capabilities with improved context understanding.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Expanded Support&lt;/strong&gt;: Extended compatibility with additional programming languages and frameworks.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;⭐ Star History&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;em&gt;Community Growth Trajectory&lt;/em&gt;&lt;/p&gt; 
 &lt;a href="https://star-history.com/#HKUDS/DeepCode&amp;amp;Date"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=HKUDS/DeepCode&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
   &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=HKUDS/DeepCode&amp;amp;type=Date" /&gt; 
   &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=HKUDS/DeepCode&amp;amp;type=Date" style="border-radius: 15px; box-shadow: 0 0 30px rgba(0, 217, 255, 0.3);" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h3&gt;🚀 &lt;strong&gt;Ready to Transform Development?&lt;/strong&gt;&lt;/h3&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-quick-start"&gt;&lt;img src="https://img.shields.io/badge/🚀_Get_Started-00d4ff?style=for-the-badge&amp;amp;logo=rocket&amp;amp;logoColor=white" alt="Get Started" /&gt;&lt;/a&gt; &lt;a href="https://github.com/HKUDS"&gt;&lt;img src="https://img.shields.io/badge/🏛️_View_on_GitHub-00d4ff?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white" alt="View on GitHub" /&gt;&lt;/a&gt; &lt;a href="https://github.com/HKUDS/deepcode-agent"&gt;&lt;img src="https://img.shields.io/badge/⭐_Star_Project-00d4ff?style=for-the-badge&amp;amp;logo=star&amp;amp;logoColor=white" alt="Star Project" /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;hr /&gt; 
 &lt;h3&gt;📄 &lt;strong&gt;License&lt;/strong&gt;&lt;/h3&gt; 
 &lt;img src="https://img.shields.io/badge/License-MIT-4ecdc4?style=for-the-badge&amp;amp;logo=opensourceinitiative&amp;amp;logoColor=white" alt="MIT License" /&gt; 
 &lt;p&gt;&lt;strong&gt;MIT License&lt;/strong&gt; - Copyright (c) 2025 Data Intelligence Lab, The University of Hong Kong&lt;/p&gt; 
 &lt;hr /&gt; 
 &lt;img src="https://visitor-badge.laobi.icu/badge?page_id=deepcode.readme&amp;amp;style=for-the-badge&amp;amp;color=00d4ff" alt="Visitors" /&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>dataease/SQLBot</title>
      <link>https://github.com/dataease/SQLBot</link>
      <description>&lt;p&gt;基于大模型和 RAG 的智能问数系统。Text-to-SQL Generation via LLMs using RAG.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt;&lt;img src="https://resource-fit2cloud-com.oss-cn-hangzhou.aliyuncs.com/sqlbot/sqlbot.png" alt="SQLBot" width="300" /&gt;&lt;/p&gt; 
&lt;h3 align="center"&gt;基于大模型和 RAG 的智能问数系统&lt;/h3&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/dataease/SQLBot/releases/latest"&gt;&lt;img src="https://img.shields.io/github/v/release/dataease/SQLBot" alt="Latest release" /&gt;&lt;/a&gt; &lt;a href="https://github.com/dataease/SQLBot"&gt;&lt;img src="https://img.shields.io/github/stars/dataease/SQLBot?color=%231890FF&amp;amp;style=flat-square" alt="Stars" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/dataease/SQLbot"&gt;&lt;img src="https://img.shields.io/docker/pulls/dataease/sqlbot?label=downloads" alt="Download" /&gt;&lt;/a&gt;&lt;br /&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;SQLBot 是一款基于大模型和 RAG 的智能问数系统。SQLBot 的优势包括：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;开箱即用&lt;/strong&gt;: 只需配置大模型和数据源即可开启问数之旅，通过大模型和 RAG 的结合来实现高质量的 text2sql；&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;易于集成&lt;/strong&gt;: 支持快速嵌入到第三方业务系统，也支持被 n8n、MaxKB、Dify、Coze 等 AI 应用开发平台集成调用，让各类应用快速拥有智能问数能力；&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;安全可控&lt;/strong&gt;: 提供基于工作空间的资源隔离机制，能够实现细粒度的数据权限控制。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;工作原理&lt;/h2&gt; 
&lt;img width="1189" height="624" alt="system-arch" src="https://github.com/user-attachments/assets/cde40783-369e-493e-bb59-44ce43c2e7c5" /&gt; 
&lt;h2&gt;快速开始&lt;/h2&gt; 
&lt;h3&gt;安装部署&lt;/h3&gt; 
&lt;p&gt;准备一台 Linux 服务器，安装好 &lt;a href="https://docs.docker.com/get-docker/"&gt;Docker&lt;/a&gt;，执行以下一键安装脚本：&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -d \
  --name sqlbot \
  --restart unless-stopped \
  -p 8000:8000 \
  -p 8001:8001 \
  -v ./data/sqlbot/excel:/opt/sqlbot/data/excel \
  -v ./data/sqlbot/images:/opt/sqlbot/images \
  -v ./data/sqlbot/logs:/opt/sqlbot/logs \
  -v ./data/postgresql:/var/lib/postgresql/data \
  --privileged=true \
  dataease/sqlbot
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;你也可以通过 &lt;a href="https://apps.fit2cloud.com/1panel"&gt;1Panel 应用商店&lt;/a&gt; 快速部署 SQLBot。&lt;/p&gt; 
&lt;p&gt;如果是内网环境，你可以通过 &lt;a href="https://community.fit2cloud.com/#/products/sqlbot/downloads"&gt;离线安装包方式&lt;/a&gt; 部署 SQLBot。&lt;/p&gt; 
&lt;h3&gt;访问方式&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;在浏览器中打开: http://&amp;lt;你的服务器IP&amp;gt;:8000/&lt;/li&gt; 
 &lt;li&gt;用户名: admin&lt;/li&gt; 
 &lt;li&gt;密码: SQLBot@123456&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;联系我们&lt;/h3&gt; 
&lt;p&gt;如你有更多问题，可以加入我们的技术交流群与我们交流。&lt;/p&gt; 
&lt;img width="180" height="180" alt="contact_me_qr" src="https://github.com/user-attachments/assets/2594ff29-5426-4457-b051-279855610030" /&gt; 
&lt;h2&gt;UI 展示&lt;/h2&gt;  
&lt;img alt="q&amp;amp;a" src="https://github.com/user-attachments/assets/55526514-52f3-4cfe-98ec-08a986259280" /&gt;  
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#dataease/sqlbot&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=dataease/sqlbot&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;飞致云旗下的其他明星项目&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/dataease/dataease/"&gt;DataEase&lt;/a&gt; - 人人可用的开源 BI 工具&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/1panel-dev/1panel/"&gt;1Panel&lt;/a&gt; - 现代化、开源的 Linux 服务器运维管理面板&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/1panel-dev/MaxKB/"&gt;MaxKB&lt;/a&gt; - 强大易用的企业级智能体平台&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/jumpserver/jumpserver/"&gt;JumpServer&lt;/a&gt; - 广受欢迎的开源堡垒机&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/halo-dev/halo/"&gt;Halo&lt;/a&gt; - 强大易用的开源建站工具&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/metersphere/metersphere/"&gt;MeterSphere&lt;/a&gt; - 新一代的开源持续测试工具&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;本仓库遵循 &lt;a href="https://raw.githubusercontent.com/dataease/SQLBot/main/LICENSE"&gt;FIT2CLOUD Open Source License&lt;/a&gt; 开源协议，该许可证本质上是 GPLv3，但有一些额外的限制。&lt;/p&gt; 
&lt;p&gt;你可以基于 SQLBot 的源代码进行二次开发，但是需要遵守以下规定：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;不能替换和修改 SQLBot 的 Logo 和版权信息；&lt;/li&gt; 
 &lt;li&gt;二次开发后的衍生作品必须遵守 GPL V3 的开源义务。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;如需商业授权，请联系 &lt;a href="mailto:support@fit2cloud.com"&gt;support@fit2cloud.com&lt;/a&gt; 。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>swisskyrepo/PayloadsAllTheThings</title>
      <link>https://github.com/swisskyrepo/PayloadsAllTheThings</link>
      <description>&lt;p&gt;A list of useful payloads and bypass for Web Application Security and Pentest/CTF&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Payloads All The Things&lt;/h1&gt; 
&lt;p&gt;A list of useful payloads and bypasses for Web Application Security. Feel free to improve with your payloads and techniques !&lt;/p&gt; 
&lt;p&gt;You can also contribute with a &lt;span&gt;🍻&lt;/span&gt; IRL, or using the sponsor button.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/sponsors/swisskyrepo"&gt;&lt;img src="https://img.shields.io/static/v1?label=Sponsor&amp;amp;message=%E2%9D%A4&amp;amp;logo=GitHub&amp;amp;link=https://github.com/sponsors/swisskyrepo" alt="Sponsor" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/intent/tweet?text=Payloads%20All%20The%20Things,%20a%20list%20of%20useful%20payloads%20and%20bypasses%20for%20Web%20Application%20Security%20-%20by%20@pentest_swissky&amp;amp;url=https://github.com/swisskyrepo/PayloadsAllTheThings/"&gt;&lt;img src="https://img.shields.io/twitter/url/http/shields.io.svg?style=social" alt="Tweet" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;An alternative display version is available at &lt;a href="https://swisskyrepo.github.io/PayloadsAllTheThings/"&gt;PayloadsAllTheThingsWeb&lt;/a&gt;.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/swisskyrepo/PayloadsAllTheThings/master/.github/banner.png" alt="banner" /&gt; &lt;/p&gt; 
&lt;h2&gt;&lt;span&gt;📖&lt;/span&gt; Documentation&lt;/h2&gt; 
&lt;p&gt;Every section contains the following files, you can use the &lt;code&gt;_template_vuln&lt;/code&gt; folder to create a new chapter:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;README.md - vulnerability description and how to exploit it, including several payloads&lt;/li&gt; 
 &lt;li&gt;Intruder - a set of files to give to Burp Intruder&lt;/li&gt; 
 &lt;li&gt;Images - pictures for the README.md&lt;/li&gt; 
 &lt;li&gt;Files - some files referenced in the README.md&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You might also like the other projects from the AllTheThings family :&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://swisskyrepo.github.io/InternalAllTheThings/"&gt;InternalAllTheThings&lt;/a&gt; - Active Directory and Internal Pentest Cheatsheets&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://swisskyrepo.github.io/HardwareAllTheThings/"&gt;HardwareAllTheThings&lt;/a&gt; - Hardware/IOT Pentesting Wiki&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You want more ? Check the &lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/raw/master/_LEARNING_AND_SOCIALS/BOOKS.md"&gt;Books&lt;/a&gt; and &lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/raw/master/_LEARNING_AND_SOCIALS/YOUTUBE.md"&gt;Youtube channel&lt;/a&gt; selections.&lt;/p&gt; 
&lt;h2&gt;&lt;span&gt;🧑💻&lt;/span&gt; Contributions&lt;/h2&gt; 
&lt;p&gt;Be sure to read &lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/raw/master/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=swisskyrepo/PayloadsAllTheThings&amp;amp;max=36" alt="sponsors-list" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;Thanks again for your contribution! &lt;span&gt;❤️&lt;/span&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;span&gt;🍻&lt;/span&gt; Sponsors&lt;/h2&gt; 
&lt;p&gt;This project is proudly sponsored by these companies.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Logo&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://serpapi.com"&gt;&lt;img src="https://avatars.githubusercontent.com/u/34724717?s=40&amp;amp;v=4" alt="sponsor-serpapi" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;SerpApi&lt;/strong&gt; is a real time API to access Google search results. It solves the issues of having to rent proxies, solving captchas, and JSON parsing.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://projectdiscovery.io/"&gt;&lt;img src="https://avatars.githubusercontent.com/u/50994705?s=40&amp;amp;v=4" alt="sponsor-projectdiscovery" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;ProjectDiscovery&lt;/strong&gt; - Detect real, exploitable vulnerabilities. Harness the power of Nuclei for fast and accurate findings without false positives.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.vaadata.com/"&gt;&lt;img src="https://avatars.githubusercontent.com/u/48131541?s=40&amp;amp;v=4" alt="sponsor-vaadata" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;VAADATA&lt;/strong&gt; - Ethical Hacking Services&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt;</description>
    </item>
    
    <item>
      <title>TheAlgorithms/Python</title>
      <link>https://github.com/TheAlgorithms/Python</link>
      <description>&lt;p&gt;All Algorithms implemented in Python&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;!-- Title: --&gt; 
 &lt;a href="https://github.com/TheAlgorithms/"&gt; &lt;img src="https://raw.githubusercontent.com/TheAlgorithms/website/1cd824df116b27029f17c2d1b42d81731f28a920/public/logo.svg?sanitize=true" height="100" /&gt; &lt;/a&gt; 
 &lt;h1&gt;&lt;a href="https://github.com/TheAlgorithms/"&gt;The Algorithms&lt;/a&gt; - Python&lt;/h1&gt; 
 &lt;!-- Labels: --&gt; 
 &lt;!-- First row: --&gt; 
 &lt;a href="https://gitpod.io/#https://github.com/TheAlgorithms/Python"&gt; &lt;img src="https://img.shields.io/badge/Gitpod-Ready--to--Code-blue?logo=gitpod&amp;amp;style=flat-square" height="20" alt="Gitpod Ready-to-Code" /&gt; &lt;/a&gt; 
 &lt;a href="https://github.com/TheAlgorithms/Python/raw/master/CONTRIBUTING.md"&gt; &lt;img src="https://img.shields.io/static/v1.svg?label=Contributions&amp;amp;message=Welcome&amp;amp;color=0059b3&amp;amp;style=flat-square" height="20" alt="Contributions Welcome" /&gt; &lt;/a&gt; 
 &lt;img src="https://img.shields.io/github/repo-size/TheAlgorithms/Python.svg?label=Repo%20size&amp;amp;style=flat-square" height="20" /&gt; 
 &lt;a href="https://the-algorithms.com/discord"&gt; &lt;img src="https://img.shields.io/discord/808045925556682782.svg?logo=discord&amp;amp;colorB=7289DA&amp;amp;style=flat-square" height="20" alt="Discord chat" /&gt; &lt;/a&gt; 
 &lt;a href="https://gitter.im/TheAlgorithms/community"&gt; &lt;img src="https://img.shields.io/badge/Chat-Gitter-ff69b4.svg?label=Chat&amp;amp;logo=gitter&amp;amp;style=flat-square" height="20" alt="Gitter chat" /&gt; &lt;/a&gt; 
 &lt;!-- Second row: --&gt; 
 &lt;br /&gt; 
 &lt;a href="https://github.com/TheAlgorithms/Python/actions"&gt; &lt;img src="https://img.shields.io/github/actions/workflow/status/TheAlgorithms/Python/build.yml?branch=master&amp;amp;label=CI&amp;amp;logo=github&amp;amp;style=flat-square" height="20" alt="GitHub Workflow Status" /&gt; &lt;/a&gt; 
 &lt;a href="https://github.com/pre-commit/pre-commit"&gt; &lt;img src="https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&amp;amp;logoColor=white&amp;amp;style=flat-square" height="20" alt="pre-commit" /&gt; &lt;/a&gt; 
 &lt;a href="https://docs.astral.sh/ruff/formatter/"&gt; &lt;img src="https://img.shields.io/static/v1?label=code%20style&amp;amp;message=ruff&amp;amp;color=black&amp;amp;style=flat-square" height="20" alt="code style: black" /&gt; &lt;/a&gt; 
 &lt;!-- Short description: --&gt; 
 &lt;h3&gt;All algorithms implemented in Python - for education 📚&lt;/h3&gt; 
&lt;/div&gt; 
&lt;p&gt;Implementations are for learning purposes only. They may be less efficient than the implementations in the Python standard library. Use them at your discretion.&lt;/p&gt; 
&lt;h2&gt;🚀 Getting Started&lt;/h2&gt; 
&lt;p&gt;📋 Read through our &lt;a href="https://raw.githubusercontent.com/TheAlgorithms/Python/master/CONTRIBUTING.md"&gt;Contribution Guidelines&lt;/a&gt; before you contribute.&lt;/p&gt; 
&lt;h2&gt;🌐 Community Channels&lt;/h2&gt; 
&lt;p&gt;We are on &lt;a href="https://the-algorithms.com/discord"&gt;Discord&lt;/a&gt; and &lt;a href="https://gitter.im/TheAlgorithms/community"&gt;Gitter&lt;/a&gt;! Community channels are a great way for you to ask questions and get help. Please join us!&lt;/p&gt; 
&lt;h2&gt;📜 List of Algorithms&lt;/h2&gt; 
&lt;p&gt;See our &lt;a href="https://raw.githubusercontent.com/TheAlgorithms/Python/master/DIRECTORY.md"&gt;directory&lt;/a&gt; for easier navigation and a better overview of the project.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>microsoft/markitdown</title>
      <link>https://github.com/microsoft/markitdown</link>
      <description>&lt;p&gt;Python tool for converting files and office documents to Markdown.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MarkItDown&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://pypi.org/project/markitdown/"&gt;&lt;img src="https://img.shields.io/pypi/v/markitdown.svg?sanitize=true" alt="PyPI" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/pypi/dd/markitdown" alt="PyPI - Downloads" /&gt; &lt;a href="https://github.com/microsoft/autogen"&gt;&lt;img src="https://img.shields.io/badge/Built%20by-AutoGen%20Team-blue" alt="Built by AutoGen Team" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] MarkItDown now offers an MCP (Model Context Protocol) server for integration with LLM applications like Claude Desktop. See &lt;a href="https://github.com/microsoft/markitdown/tree/main/packages/markitdown-mcp"&gt;markitdown-mcp&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] Breaking changes between 0.0.1 to 0.1.0:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Dependencies are now organized into optional feature-groups (further details below). Use &lt;code&gt;pip install 'markitdown[all]'&lt;/code&gt; to have backward-compatible behavior.&lt;/li&gt; 
  &lt;li&gt;convert_stream() now requires a binary file-like object (e.g., a file opened in binary mode, or an io.BytesIO object). This is a breaking change from the previous version, where it previously also accepted text file-like objects, like io.StringIO.&lt;/li&gt; 
  &lt;li&gt;The DocumentConverter class interface has changed to read from file-like streams rather than file paths. &lt;em&gt;No temporary files are created anymore&lt;/em&gt;. If you are the maintainer of a plugin, or custom DocumentConverter, you likely need to update your code. Otherwise, if only using the MarkItDown class or CLI (as in these examples), you should not need to change anything.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;MarkItDown is a lightweight Python utility for converting various files to Markdown for use with LLMs and related text analysis pipelines. To this end, it is most comparable to &lt;a href="https://github.com/deanmalmgren/textract"&gt;textract&lt;/a&gt;, but with a focus on preserving important document structure and content as Markdown (including: headings, lists, tables, links, etc.) While the output is often reasonably presentable and human-friendly, it is meant to be consumed by text analysis tools -- and may not be the best option for high-fidelity document conversions for human consumption.&lt;/p&gt; 
&lt;p&gt;MarkItDown currently supports the conversion from:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;PDF&lt;/li&gt; 
 &lt;li&gt;PowerPoint&lt;/li&gt; 
 &lt;li&gt;Word&lt;/li&gt; 
 &lt;li&gt;Excel&lt;/li&gt; 
 &lt;li&gt;Images (EXIF metadata and OCR)&lt;/li&gt; 
 &lt;li&gt;Audio (EXIF metadata and speech transcription)&lt;/li&gt; 
 &lt;li&gt;HTML&lt;/li&gt; 
 &lt;li&gt;Text-based formats (CSV, JSON, XML)&lt;/li&gt; 
 &lt;li&gt;ZIP files (iterates over contents)&lt;/li&gt; 
 &lt;li&gt;Youtube URLs&lt;/li&gt; 
 &lt;li&gt;EPubs&lt;/li&gt; 
 &lt;li&gt;... and more!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Why Markdown?&lt;/h2&gt; 
&lt;p&gt;Markdown is extremely close to plain text, with minimal markup or formatting, but still provides a way to represent important document structure. Mainstream LLMs, such as OpenAI's GPT-4o, natively "&lt;em&gt;speak&lt;/em&gt;" Markdown, and often incorporate Markdown into their responses unprompted. This suggests that they have been trained on vast amounts of Markdown-formatted text, and understand it well. As a side benefit, Markdown conventions are also highly token-efficient.&lt;/p&gt; 
&lt;h2&gt;Prerequisites&lt;/h2&gt; 
&lt;p&gt;MarkItDown requires Python 3.10 or higher. It is recommended to use a virtual environment to avoid dependency conflicts.&lt;/p&gt; 
&lt;p&gt;With the standard Python installation, you can create and activate a virtual environment using the following commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m venv .venv
source .venv/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If using &lt;code&gt;uv&lt;/code&gt;, you can create a virtual environment with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv venv --python=3.12 .venv
source .venv/bin/activate
# NOTE: Be sure to use 'uv pip install' rather than just 'pip install' to install packages in this virtual environment
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you are using Anaconda, you can create a virtual environment with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;conda create -n markitdown python=3.12
conda activate markitdown
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;To install MarkItDown, use pip: &lt;code&gt;pip install 'markitdown[all]'&lt;/code&gt;. Alternatively, you can install it from the source:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone git@github.com:microsoft/markitdown.git
cd markitdown
pip install -e 'packages/markitdown[all]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Command-Line&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown path-to-file.pdf &amp;gt; document.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or use &lt;code&gt;-o&lt;/code&gt; to specify the output file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown path-to-file.pdf -o document.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also pipe content:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cat path-to-file.pdf | markitdown
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Optional Dependencies&lt;/h3&gt; 
&lt;p&gt;MarkItDown has optional dependencies for activating various file formats. Earlier in this document, we installed all optional dependencies with the &lt;code&gt;[all]&lt;/code&gt; option. However, you can also install them individually for more control. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install 'markitdown[pdf, docx, pptx]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;will install only the dependencies for PDF, DOCX, and PPTX files.&lt;/p&gt; 
&lt;p&gt;At the moment, the following optional dependencies are available:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;[all]&lt;/code&gt; Installs all optional dependencies&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[pptx]&lt;/code&gt; Installs dependencies for PowerPoint files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[docx]&lt;/code&gt; Installs dependencies for Word files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[xlsx]&lt;/code&gt; Installs dependencies for Excel files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[xls]&lt;/code&gt; Installs dependencies for older Excel files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[pdf]&lt;/code&gt; Installs dependencies for PDF files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[outlook]&lt;/code&gt; Installs dependencies for Outlook messages&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[az-doc-intel]&lt;/code&gt; Installs dependencies for Azure Document Intelligence&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[audio-transcription]&lt;/code&gt; Installs dependencies for audio transcription of wav and mp3 files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[youtube-transcription]&lt;/code&gt; Installs dependencies for fetching YouTube video transcription&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Plugins&lt;/h3&gt; 
&lt;p&gt;MarkItDown also supports 3rd-party plugins. Plugins are disabled by default. To list installed plugins:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown --list-plugins
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To enable plugins use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown --use-plugins path-to-file.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To find available plugins, search GitHub for the hashtag &lt;code&gt;#markitdown-plugin&lt;/code&gt;. To develop a plugin, see &lt;code&gt;packages/markitdown-sample-plugin&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Azure Document Intelligence&lt;/h3&gt; 
&lt;p&gt;To use Microsoft Document Intelligence for conversion:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown path-to-file.pdf -o document.md -d -e "&amp;lt;document_intelligence_endpoint&amp;gt;"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;More information about how to set up an Azure Document Intelligence Resource can be found &lt;a href="https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/how-to-guides/create-document-intelligence-resource?view=doc-intel-4.0.0"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Python API&lt;/h3&gt; 
&lt;p&gt;Basic usage in Python:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from markitdown import MarkItDown

md = MarkItDown(enable_plugins=False) # Set to True to enable plugins
result = md.convert("test.xlsx")
print(result.text_content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Document Intelligence conversion in Python:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from markitdown import MarkItDown

md = MarkItDown(docintel_endpoint="&amp;lt;document_intelligence_endpoint&amp;gt;")
result = md.convert("test.pdf")
print(result.text_content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To use Large Language Models for image descriptions (currently only for pptx and image files), provide &lt;code&gt;llm_client&lt;/code&gt; and &lt;code&gt;llm_model&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from markitdown import MarkItDown
from openai import OpenAI

client = OpenAI()
md = MarkItDown(llm_client=client, llm_model="gpt-4o", llm_prompt="optional custom prompt")
result = md.convert("example.jpg")
print(result.text_content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Docker&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;docker build -t markitdown:latest .
docker run --rm -i markitdown:latest &amp;lt; ~/your-file.pdf &amp;gt; output.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href="https://cla.opensource.microsoft.com"&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; 
&lt;p&gt;This project has adopted the &lt;a href="https://opensource.microsoft.com/codeofconduct/"&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href="https://opensource.microsoft.com/codeofconduct/faq/"&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href="mailto:opencode@microsoft.com"&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; 
&lt;h3&gt;How to Contribute&lt;/h3&gt; 
&lt;p&gt;You can help by looking at issues or helping review PRs. Any issue or PR is welcome, but we have also marked some as 'open for contribution' and 'open for reviewing' to help facilitate community contributions. These are of course just suggestions and you are welcome to contribute in any way you like.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;&lt;/th&gt; 
    &lt;th&gt;All&lt;/th&gt; 
    &lt;th&gt;Especially Needs Help from Community&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Issues&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/markitdown/issues"&gt;All Issues&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/markitdown/issues?q=is%3Aissue+is%3Aopen+label%3A%22open+for+contribution%22"&gt;Issues open for contribution&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;PRs&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/markitdown/pulls"&gt;All PRs&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/markitdown/pulls?q=is%3Apr+is%3Aopen+label%3A%22open+for+reviewing%22"&gt;PRs open for reviewing&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h3&gt;Running Tests and Checks&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Navigate to the MarkItDown package:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;cd packages/markitdown
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install &lt;code&gt;hatch&lt;/code&gt; in your environment and run tests:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;pip install hatch  # Other ways of installing hatch: https://hatch.pypa.io/dev/install/
hatch shell
hatch test
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;(Alternative) Use the Devcontainer which has all the dependencies installed:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;# Reopen the project in Devcontainer and run:
hatch test
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run pre-commit checks before submitting a PR: &lt;code&gt;pre-commit run --all-files&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Contributing 3rd-party Plugins&lt;/h3&gt; 
&lt;p&gt;You can also contribute by creating and sharing 3rd party plugins. See &lt;code&gt;packages/markitdown-sample-plugin&lt;/code&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Trademarks&lt;/h2&gt; 
&lt;p&gt;This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow &lt;a href="https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general"&gt;Microsoft's Trademark &amp;amp; Brand Guidelines&lt;/a&gt;. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>google-research/timesfm</title>
      <link>https://github.com/google-research/timesfm</link>
      <description>&lt;p&gt;TimesFM (Time Series Foundation Model) is a pretrained time-series foundation model developed by Google Research for time-series forecasting.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TimesFM&lt;/h1&gt; 
&lt;p&gt;TimesFM (Time Series Foundation Model) is a pretrained time-series foundation model developed by Google Research for time-series forecasting.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/2310.10688"&gt;A decoder-only foundation model for time-series forecasting&lt;/a&gt;, ICML 2024.&lt;/li&gt; 
 &lt;li&gt;All checkpoints: &lt;a href="https://huggingface.co/collections/google/timesfm-release-66e4be5fdb56e960c1e482a6"&gt;TimesFM Hugging Face Collection&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://research.google/blog/a-decoder-only-foundation-model-for-time-series-forecasting/"&gt;Google Research blog&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://cloud.google.com/bigquery/docs/timesfm-model"&gt;TimesFM in BigQuery&lt;/a&gt;: an official Google product.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This open version is not an officially supported Google product.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Latest Model Version:&lt;/strong&gt; TimesFM 2.5&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Archived Model Versions:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;1.0 and 2.0: relevant code archived in the sub directory &lt;code&gt;v1&lt;/code&gt;. You can &lt;code&gt;pip install timesfm==1.3.0&lt;/code&gt; to install an older version of this package to load them.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Update - Sept. 15, 2025&lt;/h2&gt; 
&lt;p&gt;TimesFM 2.5 is out!&lt;/p&gt; 
&lt;p&gt;Comparing to TimesFM 2.0, this new 2.5 model:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;uses 200M parameters, down from 500M.&lt;/li&gt; 
 &lt;li&gt;supports up to 16k context length, up from 2048.&lt;/li&gt; 
 &lt;li&gt;supports continuous quantile forecast up to 1k horizon via an optional 30M quantile head.&lt;/li&gt; 
 &lt;li&gt;gets rid of the &lt;code&gt;frequency&lt;/code&gt; indicator.&lt;/li&gt; 
 &lt;li&gt;has a couple of new forecasting flags.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Along with the model upgrade we have also upgraded the inference API. This repo will be under construction over the next few weeks to&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;add support for an upcoming Flax version of the model (faster inference).&lt;/li&gt; 
 &lt;li&gt;add back covariate support.&lt;/li&gt; 
 &lt;li&gt;populate more docstrings, docs and notebook.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Install&lt;/h3&gt; 
&lt;p&gt;TODO(siriuz42): Package timesfm==2.0.0 and upload to PyPI .&lt;/p&gt; 
&lt;p&gt;Run&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;git clone https://github.com/google-research/timesfm.git
cd timesfm
pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Code Example&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import numpy as np
import timesfm
model = timesfm.TimesFM_2p5_200M_torch()
model.load_checkpoint()
model.compile(
    timesfm.ForecastConfig(
        max_context=1024,
        max_horizon=256,
        normalize_inputs=True,
        use_continuous_quantile_head=True,
        force_flip_invariance=True,
        infer_is_positive=True,
        fix_quantile_crossing=True,
    )
)
point_forecast, quantile_forecast = model.forecast(
    horizon=12,
    inputs=[
        np.linspace(0, 1, 100),
        np.sin(np.linspace(0, 20, 67)),
    ],  # Two dummy inputs
)
point_forecast.shape  # (2, 12)
quantile_forecast.shape  # (2, 12, 10): mean, then 10th to 90th quantiles.
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>ROCm/TheRock</title>
      <link>https://github.com/ROCm/TheRock</link>
      <description>&lt;p&gt;The HIP Environment and ROCm Kit - A lightweight open source build system for HIP and ROCm&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TheRock&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/pre-commit/pre-commit"&gt;&lt;img src="https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit" alt="pre-commit" /&gt;&lt;/a&gt; &lt;a href="https://github.com/ROCm/TheRock/actions/workflows/ci.yml?query=branch%3Amain"&gt;&lt;img src="https://github.com/ROCm/TheRock/actions/workflows/ci.yml/badge.svg?branch=main&amp;amp;event=push" alt="CI" /&gt;&lt;/a&gt; &lt;a href="https://github.com/ROCm/TheRock/actions/workflows/ci_nightly.yml?query=branch%3Amain"&gt;&lt;img src="https://github.com/ROCm/TheRock/actions/workflows/ci_nightly.yml/badge.svg?branch=main" alt="CI Nightly" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;TheRock (The HIP Environment and ROCm Kit) is a lightweight open source build platform for HIP and ROCm. The project is currently in an &lt;strong&gt;early preview state&lt;/strong&gt; but is under active development and welcomes contributors. Come try us out! Please see &lt;a href="https://raw.githubusercontent.com/ROCm/TheRock/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for more info.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;p&gt;TheRock includes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Nightly releases of ROCm and PyTorch&lt;/li&gt; 
 &lt;li&gt;A CMake super-project for HIP and ROCm source builds&lt;/li&gt; 
 &lt;li&gt;Support for building PyTorch with ROCm from source 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://github.com/ROCm/TheRock/issues/247"&gt;JAX support&lt;/a&gt; and other external project builds are in the works!&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Operating system support including multiple Linux distributions and native Windows&lt;/li&gt; 
 &lt;li&gt;Tools for developing individual ROCm components&lt;/li&gt; 
 &lt;li&gt;Comprehensive CI/CD pipelines for building, testing, and releasing supported components&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installing from releases&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] See the &lt;a href="https://raw.githubusercontent.com/ROCm/TheRock/main/RELEASES.md"&gt;Releases Page&lt;/a&gt; for instructions on how to install prebuilt ROCm and PyTorch packages.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Nightly release status&lt;/h3&gt; 
&lt;p&gt;Packages and Python wheels:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Platform&lt;/th&gt; 
   &lt;th align="right"&gt;Prebuilt tarballs and ROCm Python packages&lt;/th&gt; 
   &lt;th align="right"&gt;PyTorch Python packages&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Linux&lt;/td&gt; 
   &lt;td align="right"&gt;&lt;a href="https://github.com/ROCm/TheRock/actions/workflows/release_portable_linux_packages.yml?query=branch%3Amain"&gt;&lt;img src="https://github.com/ROCm/TheRock/actions/workflows/release_portable_linux_packages.yml/badge.svg?branch=main" alt="Release portable Linux packages" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="right"&gt;&lt;a href="https://github.com/ROCm/TheRock/actions/workflows/release_portable_linux_pytorch_wheels.yml?query=branch%3Amain"&gt;&lt;img src="https://github.com/ROCm/TheRock/actions/workflows/release_portable_linux_pytorch_wheels.yml/badge.svg?branch=main" alt="Release Portable Linux PyTorch Wheels" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Windows&lt;/td&gt; 
   &lt;td align="right"&gt;&lt;a href="https://github.com/ROCm/TheRock/actions/workflows/release_windows_packages.yml?query=branch%3Amain"&gt;&lt;img src="https://github.com/ROCm/TheRock/actions/workflows/release_windows_packages.yml/badge.svg?branch=main" alt="Release Windows packages" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="right"&gt;&lt;a href="https://github.com/ROCm/TheRock/actions/workflows/release_windows_pytorch_wheels.yml?query=branch%3Amain"&gt;&lt;img src="https://github.com/ROCm/TheRock/actions/workflows/release_windows_pytorch_wheels.yml/badge.svg?branch=main" alt="Release Windows PyTorch Wheels" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Building from source&lt;/h2&gt; 
&lt;p&gt;We keep the following instructions for recent, commonly used operating system versions. Most build failures are due to minor operating system differences in dependencies and project setup. Refer to the &lt;a href="https://raw.githubusercontent.com/ROCm/TheRock/main/docs/environment_setup_guide.md"&gt;Environment Setup Guide&lt;/a&gt; for contributed instructions and configurations for alternatives.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] While building from source offers the greatest flexibility, &lt;a href="https://raw.githubusercontent.com/ROCm/TheRock/main/#installing-from-releases"&gt;installing from releases&lt;/a&gt; in supported configurations is often faster and easier.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Setup - Ubuntu (24.04)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install Ubuntu dependencies
sudo apt update
sudo apt install gfortran git git-lfs ninja-build cmake g++ pkg-config xxd patchelf automake libtool python3-venv python3-dev libegl1-mesa-dev

# Clone the repository
git clone https://github.com/ROCm/TheRock.git
cd TheRock

# Init python virtual environment and install python dependencies
python3 -m venv .venv &amp;amp;&amp;amp; source .venv/bin/activate
pip install -r requirements.txt

# Download submodules and apply patches
python ./build_tools/fetch_sources.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Setup - Windows 11 (VS 2022)&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] See &lt;a href="https://raw.githubusercontent.com/ROCm/TheRock/main/docs/development/windows_support.md"&gt;windows_support.md&lt;/a&gt; for setup instructions on Windows, in particular the section for &lt;a href="https://raw.githubusercontent.com/ROCm/TheRock/main/docs/development/windows_support.md#install-tools"&gt;installing tools&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;If the build system is a non-English system. Make sure to switch to &lt;code&gt;utf-8&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-cmd"&gt;chcp 65001
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install dependencies following the Windows support guide

# Clone the repository
git clone https://github.com/ROCm/TheRock.git
cd TheRock

# Init python virtual environment and install python dependencies
python -m venv .venv
.venv\Scripts\Activate.bat
pip install -r requirements.txt

# Download submodules and apply patches
# Note that dvc is used for pulling large files
python ./build_tools/fetch_sources.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Build configuration&lt;/h3&gt; 
&lt;p&gt;The build can be customized through cmake feature flags.&lt;/p&gt; 
&lt;h4&gt;Required configuration flags&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;-DTHEROCK_AMDGPU_FAMILIES=&lt;/code&gt;&lt;/p&gt; &lt;p&gt;or&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;-DTHEROCK_AMDGPU_TARGETS=&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Not all family and targets are currently supported. See &lt;a href="https://raw.githubusercontent.com/ROCm/TheRock/main/cmake/therock_amdgpu_targets.cmake"&gt;therock_amdgpu_targets.cmake&lt;/a&gt; file for available options.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Optional configuration flags&lt;/h4&gt; 
&lt;p&gt;By default, the project builds everything available. The following group flags enable/disable selected subsets:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Group flag&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-DTHEROCK_ENABLE_ALL=OFF&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Disables all optional components&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-DTHEROCK_ENABLE_CORE=OFF&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Disables all core components&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-DTHEROCK_ENABLE_COMM_LIBS=OFF&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Disables all communication libraries&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-DTHEROCK_ENABLE_MATH_LIBS=OFF&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Disables all math libraries&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-DTHEROCK_ENABLE_ML_LIBS=OFF&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Disables all ML libraries&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-DTHEROCK_ENABLE_PROFILER=OFF&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Disables profilers&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Individual features can be controlled separately (typically in combination with &lt;code&gt;-DTHEROCK_ENABLE_ALL=OFF&lt;/code&gt; or &lt;code&gt;-DTHEROCK_RESET_FEATURES=ON&lt;/code&gt; to force a minimal build):&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Component flag&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-DTHEROCK_ENABLE_COMPILER=ON&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Enables the GPU+host compiler toolchain&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-DTHEROCK_ENABLE_HIPIFY=ON&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Enables the hipify tool&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-DTHEROCK_ENABLE_CORE_RUNTIME=ON&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Enables the core runtime components and tools&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-DTHEROCK_ENABLE_HIP_RUNTIME=ON&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Enables the HIP runtime components&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-DTHEROCK_ENABLE_ROCPROFV3=ON&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Enables rocprofv3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-DTHEROCK_ENABLE_RCCL=ON&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Enables RCCL&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-DTHEROCK_ENABLE_PRIM=ON&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Enables the PRIM library&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-DTHEROCK_ENABLE_BLAS=ON&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Enables the BLAS libraries&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-DTHEROCK_ENABLE_RAND=ON&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Enables the RAND libraries&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-DTHEROCK_ENABLE_SOLVER=ON&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Enables the SOLVER libraries&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-DTHEROCK_ENABLE_SPARSE=ON&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Enables the SPARSE libraries&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-DTHEROCK_ENABLE_MIOPEN=ON&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Enables MIOpen&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] Enabling any features will implicitly enable their &lt;em&gt;minimum&lt;/em&gt; dependencies. Some libraries (like MIOpen) have a number of &lt;em&gt;optional&lt;/em&gt; dependencies, which must be enabled manually if enabling/disabling individual features.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] A report of enabled/disabled features and flags will be printed on every CMake configure.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;By default, components are built from the sources fetched via the submodules. For some components, external sources can be used instead.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;External source settings&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-DTHEROCK_USE_EXTERNAL_COMPOSABLE_KERNEL=OFF&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Use external composable-kernel source location&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-DTHEROCK_USE_EXTERNAL_RCCL=OFF&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Use external rccl source location&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-DTHEROCK_USE_EXTERNAL_RCCL_TESTS=OFF&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Use external rccl-tests source location&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-DTHEROCK_COMPOSABLE_KERNEL_SOURCE_DIR=&amp;lt;PATH&amp;gt;&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Path to composable-kernel sources&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-DTHEROCK_RCCL_SOURCE_DIR=&amp;lt;PATH&amp;gt;&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Path to rccl sources&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-DTHEROCK_RCCL_TESTS_SOURCE_DIR=&amp;lt;PATH&amp;gt;&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Path to rccl-tests sources&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Further flags allow to build components with specific features enabled.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Other flags&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-DTHEROCK_ENABLE_MPI=OFF&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Enables building components with Message Passing Interface (MPI) support&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Building components with MPI support, currently requires MPI to be pre-installed until &lt;a href="https://github.com/ROCm/TheRock/issues/1284"&gt;issue #1284&lt;/a&gt; is resolved.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;CMake build usage&lt;/h3&gt; 
&lt;p&gt;To build ROCm/HIP:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cmake -B build -GNinja . -DTHEROCK_AMDGPU_FAMILIES=gfx110X-dgpu
cmake --build build
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;CCache usage on Linux&lt;/h4&gt; 
&lt;p&gt;To build with the &lt;a href="https://ccache.dev/"&gt;ccache&lt;/a&gt; compiler cache:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;You must have a recent ccache (&amp;gt;= 4.11 at the time of writing) that supports proper caching with the &lt;code&gt;--offload-compress&lt;/code&gt; option used for compressing AMDGPU device code.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;export CCACHE_SLOPPINESS=include_file_ctime&lt;/code&gt; to support hard-linking&lt;/li&gt; 
 &lt;li&gt;Proper setup of the &lt;code&gt;compiler_check&lt;/code&gt; directive to do safe caching in the presence of compiler bootstrapping&lt;/li&gt; 
 &lt;li&gt;Set the C/CXX compiler launcher options to cmake appropriately.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Since these options are very fiddly and prone to change over time, we recommend using the &lt;code&gt;./build_tools/setup_ccache.py&lt;/code&gt; script to create a &lt;code&gt;.ccache&lt;/code&gt; directory in the repository root with hard coded configuration suitable for the project.&lt;/p&gt; 
&lt;p&gt;Example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Any shell used to build must eval setup_ccache.py to set environment
# variables.
eval "$(./build_tools/setup_ccache.py)"
cmake -B build -GNinja -DTHEROCK_AMDGPU_FAMILIES=gfx110X-dgpu \
  -DCMAKE_C_COMPILER_LAUNCHER=ccache \
  -DCMAKE_CXX_COMPILER_LAUNCHER=ccache \
  .

cmake --build build
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;CCache usage on Windows&lt;/h4&gt; 
&lt;p&gt;We are still investigating the exact proper options for ccache on Windows and do not currently recommend that end users enable it.&lt;/p&gt; 
&lt;h3&gt;Running tests&lt;/h3&gt; 
&lt;p&gt;Project-wide testing can be controlled with the standard CMake &lt;code&gt;-DBUILD_TESTING=ON|OFF&lt;/code&gt; flag. This gates both setup of build tests and compilation of installed testing artifacts.&lt;/p&gt; 
&lt;p&gt;Tests of the integrity of the build are enabled by default and can be run with ctest:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;ctest --test-dir build
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Testing functionality on an actual GPU is in progress and will be documented separately.&lt;/p&gt; 
&lt;h2&gt;Development manuals&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ROCm/TheRock/main/CONTRIBUTING.md"&gt;Contribution Guidelines&lt;/a&gt;: Documentation for the process of contributing to this project including a quick pointer to its governance.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ROCm/TheRock/main/docs/development/development_guide.md"&gt;Development Guide&lt;/a&gt;: Documentation on how to use TheRock as a daily driver for developing any of its contained ROCm components (i.e. vs interacting with each component build individually).&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ROCm/TheRock/main/docs/development/build_system.md"&gt;Build System&lt;/a&gt;: More detailed information about TheRock's build system relevant to people looking to extend TheRock, add components, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ROCm/TheRock/main/docs/environment_setup_guide.md"&gt;Environment Setup Guide&lt;/a&gt;: Comprehensive guide for setting up a build environment, known workarounds, and other operating specific information.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ROCm/TheRock/main/docs/development/git_chores.md"&gt;Git Chores&lt;/a&gt;: Procedures for managing the codebase, specifically focused on version control, upstream/downstream, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ROCm/TheRock/main/docs/development/dependencies.md"&gt;Dependencies&lt;/a&gt;: Further specifications on ROCm-wide standards for depending on various components.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ROCm/TheRock/main/docs/development/build_containers.md"&gt;Build Containers&lt;/a&gt;: Further information about containers used for building TheRock on CI.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ROCm/TheRock/main/docs/development/artifacts.md"&gt;Build Artifacts&lt;/a&gt;: Documentation about the outputs of the build system.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ROCm/TheRock/main/RELEASES.md"&gt;Releases Page&lt;/a&gt;: Documentation for how to leverage our build artifacts.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ROCm/TheRock/main/ROADMAP.md"&gt;Roadmap for Support&lt;/a&gt;: Documentation for our prioritized roadmap to support AMD GPUs.&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Lightricks/LTX-Video</title>
      <link>https://github.com/Lightricks/LTX-Video</link>
      <description>&lt;p&gt;Official repository for LTX-Video&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h1&gt;LTX-Video&lt;/h1&gt; 
 &lt;p&gt;&lt;a href="https://www.lightricks.com/ltxv"&gt;&lt;img src="https://img.shields.io/badge/Website-LTXV-181717?logo=google-chrome" alt="Website" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/Lightricks/LTX-Video"&gt;&lt;img src="https://img.shields.io/badge/HuggingFace-Model-orange?logo=huggingface" alt="Model" /&gt;&lt;/a&gt; &lt;a href="https://app.ltx.studio/motion-workspace?videoModel=ltxv-13b"&gt;&lt;img src="https://img.shields.io/badge/Demo-Try%20Now-brightgreen?logo=vercel" alt="Demo" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2501.00103"&gt;&lt;img src="https://img.shields.io/badge/Paper-arXiv-B31B1B?logo=arxiv" alt="Paper" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Lightricks/LTX-Video-Trainer"&gt;&lt;img src="https://img.shields.io/badge/LTXV-Trainer-9146FF?logo=github" alt="Trainer" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/Mn8BRgUKKy"&gt;&lt;img src="https://img.shields.io/badge/Join-Discord-5865F2?logo=discord" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;This is the official repository for LTX-Video.&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#news"&gt;What's new&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#models"&gt;Models&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#quick-start-guide"&gt;Quick Start Guide&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#online-inference"&gt;Online demo&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#run-locally"&gt;Run locally&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#inference"&gt;Inference&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#comfyui-integration"&gt;ComfyUI Integration&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#diffusers-integration"&gt;Diffusers Integration&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#model-user-guide"&gt;Model User Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#community-contribution"&gt;Community Contribution&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#training"&gt;Training&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#control-models"&gt;Control Models&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#join-us-"&gt;Join Us!&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#acknowledgement"&gt;Acknowledgement&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Introduction&lt;/h1&gt; 
&lt;p&gt;LTX-Video is the first DiT-based video generation model that can generate high-quality videos in &lt;em&gt;real-time&lt;/em&gt;. It can generate 30 FPS videos at 1216×704 resolution, faster than it takes to watch them. The model is trained on a large-scale dataset of diverse videos and can generate high-resolution videos with realistic and diverse content.&lt;/p&gt; 
&lt;p&gt;The model supports image-to-video, keyframe-based animation, video extension (both forward and backward), video-to-video transformations, and any combination of these features.&lt;/p&gt; 
&lt;h3&gt;Image-to-video examples&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00001.gif" alt="example1" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00002.gif" alt="example2" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00003.gif" alt="example3" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00004.gif" alt="example4" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00005.gif" alt="example5" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00006.gif" alt="example6" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00007.gif" alt="example7" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00008.gif" alt="example8" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00009.gif" alt="example9" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Controlled video examples&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_ic_2v_example_00000.gif" alt="control0" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_ic_2v_example_00001.gif" alt="control1" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_ic_2v_example_00002.gif" alt="control2" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_ic_2v_example_00003.gif" alt="control3" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_ic_2v_example_00004.gif" alt="control4" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;News&lt;/h1&gt; 
&lt;h2&gt;July, 16th, 2025: New Distilled models v0.9.8 with up to 60 seconds of video:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Long shot generation in LTXV-13B! 
  &lt;ul&gt; 
   &lt;li&gt;LTX-Video now supports up to 60 seconds of video.&lt;/li&gt; 
   &lt;li&gt;Compatible also with the official IC-LoRAs.&lt;/li&gt; 
   &lt;li&gt;Try now in &lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/tree/master/example_workflows/ltxv-13b-i2v-long-multi-prompt.json"&gt;ComfyUI&lt;/a&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Release a new distilled models: 
  &lt;ul&gt; 
   &lt;li&gt;13B distilled model &lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-13b-0.9.8-distilled.yaml"&gt;ltxv-13b-0.9.8-distilled&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;2B distilled model &lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-2b-0.9.8-distilled.yaml"&gt;ltxv-2b-0.9.8-distilled&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Both models are distilled from the same base model &lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-13b-0.9.8-dev.yaml"&gt;ltxv-13b-0.9.8-dev&lt;/a&gt; and are compatible for use together in the same multiscale pipeline.&lt;/li&gt; 
   &lt;li&gt;Improved prompt understanding and detail generation&lt;/li&gt; 
   &lt;li&gt;Includes corresponding FP8 weights and workflows.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Release a new detailer model &lt;a href="https://huggingface.co/Lightricks/LTX-Video-ICLoRA-detailer-13b-0.9.8"&gt;LTX-Video-ICLoRA-detailer-13B-0.9.8&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Available in &lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/tree/master/example_workflows/ltxv-13b-upscale.json"&gt;ComfyUI&lt;/a&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;July, 8th, 2025: New Control Models Released!&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Released three new control models for LTX-Video on HuggingFace: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Depth Control&lt;/strong&gt;: &lt;a href="https://huggingface.co/Lightricks/LTX-Video-ICLoRA-depth-13b-0.9.7"&gt;LTX-Video-ICLoRA-depth-13b-0.9.7&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Pose Control&lt;/strong&gt;: &lt;a href="https://huggingface.co/Lightricks/LTX-Video-ICLoRA-pose-13b-0.9.7"&gt;LTX-Video-ICLoRA-pose-13b-0.9.7&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Canny Control&lt;/strong&gt;: &lt;a href="https://huggingface.co/Lightricks/LTX-Video-ICLoRA-canny-13b-0.9.7"&gt;LTX-Video-ICLoRA-canny-13b-0.9.7&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;May, 14th, 2025: New distilled model 13B v0.9.7:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Release a new 13B distilled model &lt;a href="https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-distilled.safetensors"&gt;ltxv-13b-0.9.7-distilled&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Amazing for iterative work - generates HD videos in 10 seconds, with low-res preview after just 3 seconds (on H100)!&lt;/li&gt; 
   &lt;li&gt;Does not require classifier-free guidance and spatio-temporal guidance.&lt;/li&gt; 
   &lt;li&gt;Supports sampling with 8 (recommended), or less diffusion steps.&lt;/li&gt; 
   &lt;li&gt;Also released a LoRA version of the distilled model, &lt;a href="https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-distilled-lora128.safetensors"&gt;ltxv-13b-0.9.7-distilled-lora128&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Requires only 1GB of VRAM&lt;/li&gt; 
     &lt;li&gt;Can be used with the full 13B model for fast inference&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Release a new quantized distilled model &lt;a href="https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-distilled-fp8.safetensors"&gt;ltxv-13b-0.9.7-distilled-fp8&lt;/a&gt; for &lt;em&gt;real-time&lt;/em&gt; generation (on H100) with even less VRAM&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;May, 5th, 2025: New model 13B v0.9.7:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Release a new 13B model &lt;a href="https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-dev.safetensors"&gt;ltxv-13b-0.9.7-dev&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Release a new quantized model &lt;a href="https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-dev-fp8.safetensors"&gt;ltxv-13b-0.9.7-dev-fp8&lt;/a&gt; for faster inference with less VRam&lt;/li&gt; 
 &lt;li&gt;Release a new upscalers 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-temporal-upscaler-0.9.7.safetensors"&gt;ltxv-temporal-upscaler-0.9.7&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-spatial-upscaler-0.9.7.safetensors"&gt;ltxv-spatial-upscaler-0.9.7&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Breakthrough prompt adherence and physical understanding.&lt;/li&gt; 
 &lt;li&gt;New Pipeline for multi-scale video rendering for fast and high quality results&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;April, 15th, 2025: New checkpoints v0.9.6:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Release a new checkpoint &lt;a href="https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-2b-0.9.6-dev-04-25.safetensors"&gt;ltxv-2b-0.9.6-dev-04-25&lt;/a&gt; with improved quality&lt;/li&gt; 
 &lt;li&gt;Release a new distilled model &lt;a href="https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-2b-0.9.6-distilled-04-25.safetensors"&gt;ltxv-2b-0.9.6-distilled-04-25&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;15x faster inference than non-distilled model.&lt;/li&gt; 
   &lt;li&gt;Does not require classifier-free guidance and spatio-temporal guidance.&lt;/li&gt; 
   &lt;li&gt;Supports sampling with 8 (recommended), or less diffusion steps.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Improved prompt adherence, motion quality and fine details.&lt;/li&gt; 
 &lt;li&gt;New default resolution and FPS: 1216 × 704 pixels at 30 FPS 
  &lt;ul&gt; 
   &lt;li&gt;Still real time on H100 with the distilled model.&lt;/li&gt; 
   &lt;li&gt;Other resolutions and FPS are still supported.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Support stochastic inference (can improve visual quality when using the distilled model)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;March, 5th, 2025: New checkpoint v0.9.5&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;New license for commercial use (&lt;a href="https://huggingface.co/Lightricks/LTX-Video/ltx-video-2b-v0.9.5.license.txt"&gt;OpenRail-M&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;Release a new checkpoint v0.9.5 with improved quality&lt;/li&gt; 
 &lt;li&gt;Support keyframes and video extension&lt;/li&gt; 
 &lt;li&gt;Support higher resolutions&lt;/li&gt; 
 &lt;li&gt;Improved prompt understanding&lt;/li&gt; 
 &lt;li&gt;Improved VAE&lt;/li&gt; 
 &lt;li&gt;New online web app in &lt;a href="https://app.ltx.studio/ltx-video"&gt;LTX-Studio&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Automatic prompt enhancement&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;February, 20th, 2025: More inference options&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Improve STG (Spatiotemporal Guidance) for LTX-Video&lt;/li&gt; 
 &lt;li&gt;Support MPS on macOS with PyTorch 2.3.0&lt;/li&gt; 
 &lt;li&gt;Add support for 8-bit model, LTX-VideoQ8&lt;/li&gt; 
 &lt;li&gt;Add TeaCache for LTX-Video&lt;/li&gt; 
 &lt;li&gt;Add &lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#comfyui-integration"&gt;ComfyUI-LTXTricks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Add Diffusion-Pipe&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;December 31st, 2024: Research paper&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Release the &lt;a href="https://arxiv.org/abs/2501.00103"&gt;research paper&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;December 20th, 2024: New checkpoint v0.9.1&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Release a new checkpoint v0.9.1 with improved quality&lt;/li&gt; 
 &lt;li&gt;Support for STG / PAG&lt;/li&gt; 
 &lt;li&gt;Support loading checkpoints of LTX-Video in Diffusers format (conversion is done on-the-fly)&lt;/li&gt; 
 &lt;li&gt;Support offloading unused parts to CPU&lt;/li&gt; 
 &lt;li&gt;Support the new timestep-conditioned VAE decoder&lt;/li&gt; 
 &lt;li&gt;Reference contributions from the community in the readme file&lt;/li&gt; 
 &lt;li&gt;Relax transformers dependency&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;November 21th, 2024: Initial release v0.9.0&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Initial release of LTX-Video&lt;/li&gt; 
 &lt;li&gt;Support text-to-video and image-to-video generation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Models &amp;amp; Workflows&lt;/h1&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Name&lt;/th&gt; 
   &lt;th&gt;Notes&lt;/th&gt; 
   &lt;th&gt;inference.py config&lt;/th&gt; 
   &lt;th&gt;ComfyUI workflow (Recommended)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ltxv-13b-0.9.8-dev&lt;/td&gt; 
   &lt;td&gt;Highest quality, requires more VRAM&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-13b-0.9.8-dev.yaml"&gt;ltxv-13b-0.9.8-dev.yaml&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/ltxv-13b-i2v-base.json"&gt;ltxv-13b-i2v-base.json&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://app.ltx.studio/motion-workspace?videoModel=ltxv-13b"&gt;ltxv-13b-0.9.8-mix&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Mix ltxv-13b-dev and ltxv-13b-distilled in the same multi-scale rendering workflow for balanced speed-quality&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/ltxv-13b-i2v-mixed-multiscale.json"&gt;ltxv-13b-i2v-mixed-multiscale.json&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://app.ltx.studio/motion-workspace?videoModel=ltxv"&gt;ltxv-13b-0.9.8-distilled&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Faster, less VRAM usage, slight quality reduction compared to 13b. Ideal for rapid iterations&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-13b-0.9.8-distilled.yaml"&gt;ltxv-13b-0.9.8-distilled.yaml&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/13b-distilled/ltxv-13b-dist-i2v-base.json"&gt;ltxv-13b-dist-i2v-base.json&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ltxv-2b-0.9.8-distilled&lt;/td&gt; 
   &lt;td&gt;Smaller model, slight quality reduction compared to 13b distilled. Ideal for fast generation with light VRAM usage&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-2b-0.9.8-distilled.yaml"&gt;ltxv-2b-0.9.8-distilled.yaml&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ltxv-13b-0.9.8-dev-fp8&lt;/td&gt; 
   &lt;td&gt;Quantized version of ltxv-13b&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-13b-0.9.8-dev-fp8.yaml"&gt;ltxv-13b-0.9.8-dev-fp8.yaml&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/ltxv-13b-i2v-base-fp8.json"&gt;ltxv-13b-i2v-base-fp8.json&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ltxv-13b-0.9.8-distilled-fp8&lt;/td&gt; 
   &lt;td&gt;Quantized version of ltxv-13b-distilled&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-13b-0.9.8-distilled-fp8.yaml"&gt;ltxv-13b-0.9.8-distilled-fp8.yaml&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/13b-distilled/ltxv-13b-dist-i2v-base-fp8.json"&gt;ltxv-13b-dist-i2v-base-fp8.json&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ltxv-2b-0.9.8-distilled-fp8&lt;/td&gt; 
   &lt;td&gt;Quantized version of ltxv-2b-distilled&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-2b-0.9.8-distilled-fp8.yaml"&gt;ltxv-2b-0.9.8-distilled-fp8.yaml&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ltxv-2b-0.9.6&lt;/td&gt; 
   &lt;td&gt;Good quality, lower VRAM requirement than ltxv-13b&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-2b-0.9.6-dev.yaml"&gt;ltxv-2b-0.9.6-dev.yaml&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/low_level/ltxvideo-i2v.json"&gt;ltxvideo-i2v.json&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ltxv-2b-0.9.6-distilled&lt;/td&gt; 
   &lt;td&gt;15× faster, real-time capable, fewer steps needed, no STG/CFG required&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-2b-0.9.6-distilled.yaml"&gt;ltxv-2b-0.9.6-distilled.yaml&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/low_level/ltxvideo-i2v-distilled.json"&gt;ltxvideo-i2v-distilled.json&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;Quick Start Guide&lt;/h1&gt; 
&lt;h2&gt;Online inference&lt;/h2&gt; 
&lt;p&gt;The model is accessible right away via the following links:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://app.ltx.studio/motion-workspace?videoModel=ltxv-13b"&gt;LTX-Studio image-to-video (13B-mix)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://app.ltx.studio/motion-workspace?videoModel=ltxv"&gt;LTX-Studio image-to-video (13B distilled)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://fal.ai/models/fal-ai/ltx-video-13b-dev/image-to-video"&gt;Fal.ai image-to-video (13B full)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://fal.ai/models/fal-ai/ltx-video-13b-distilled/image-to-video"&gt;Fal.ai image-to-video (13B distilled)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://replicate.com/lightricks/ltx-video"&gt;Replicate image-to-video&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Run locally&lt;/h2&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;p&gt;The codebase was tested with Python 3.10.5, CUDA version 12.2, and supports PyTorch &amp;gt;= 2.1.2. On macOS, MPS was tested with PyTorch 2.3.0, and should support PyTorch == 2.3 or &amp;gt;= 2.6.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/Lightricks/LTX-Video.git
cd LTX-Video

# create env
python -m venv env
source env/bin/activate
python -m pip install -e .\[inference\]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;FP8 Kernels (optional)&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://github.com/Lightricks/LTXVideo-Q8-Kernels"&gt;FP8 kernels&lt;/a&gt; developed for LTX-Video provide performance boost on supported graphics cards (Ada architecture and later). To install FP8 kernels, follow the instructions in that repository.&lt;/p&gt; 
&lt;h3&gt;Inference&lt;/h3&gt; 
&lt;p&gt;📝 &lt;strong&gt;Note:&lt;/strong&gt; For best results, we recommend using our &lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#comfyui-integration"&gt;ComfyUI&lt;/a&gt; workflow. We're working on updating the inference.py script to match the high quality and output fidelity of ComfyUI.&lt;/p&gt; 
&lt;p&gt;To use our model, please follow the inference code in &lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/inference.py"&gt;inference.py&lt;/a&gt;:&lt;/p&gt; 
&lt;h4&gt;For image-to-video generation:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python inference.py --prompt "PROMPT" --conditioning_media_paths IMAGE_PATH --conditioning_start_frames 0 --height HEIGHT --width WIDTH --num_frames NUM_FRAMES --seed SEED --pipeline_config configs/ltxv-13b-0.9.8-distilled.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Extending a video:&lt;/h4&gt; 
&lt;p&gt;📝 &lt;strong&gt;Note:&lt;/strong&gt; Input video segments must contain a multiple of 8 frames plus 1 (e.g., 9, 17, 25, etc.), and the target frame number should be a multiple of 8.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python inference.py --prompt "PROMPT" --conditioning_media_paths VIDEO_PATH --conditioning_start_frames START_FRAME --height HEIGHT --width WIDTH --num_frames NUM_FRAMES --seed SEED --pipeline_config configs/ltxv-13b-0.9.8-distilled.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;For video generation with multiple conditions:&lt;/h4&gt; 
&lt;p&gt;You can now generate a video conditioned on a set of images and/or short video segments. Simply provide a list of paths to the images or video segments you want to condition on, along with their target frame numbers in the generated video. You can also specify the conditioning strength for each item (default: 1.0).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python inference.py --prompt "PROMPT" --conditioning_media_paths IMAGE_OR_VIDEO_PATH_1 IMAGE_OR_VIDEO_PATH_2 --conditioning_start_frames TARGET_FRAME_1 TARGET_FRAME_2 --height HEIGHT --width WIDTH --num_frames NUM_FRAMES --seed SEED --pipeline_config configs/ltxv-13b-0.9.8-distilled.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Using as a library&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from ltx_video.inference import infer, InferenceConfig

infer(
    InferenceConfig(
        pipeline_config="configs/ltxv-13b-0.9.8-distilled.yaml",
        prompt=PROMPT,
        height=HEIGHT,
        width=WIDTH,
        num_frames=NUM_FRAMES,
        output_path="output.mp4",
    )
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ComfyUI Integration&lt;/h2&gt; 
&lt;p&gt;To use our model with ComfyUI, please follow the instructions at &lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/"&gt;https://github.com/Lightricks/ComfyUI-LTXVideo/&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Diffusers Integration&lt;/h2&gt; 
&lt;p&gt;To use our model with the Diffusers Python library, check out the &lt;a href="https://huggingface.co/docs/diffusers/main/en/api/pipelines/ltx_video"&gt;official documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Diffusers also support an 8-bit version of LTX-Video, &lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#ltx-videoq8"&gt;see details below&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Model User Guide&lt;/h1&gt; 
&lt;h2&gt;📝 Prompt Engineering&lt;/h2&gt; 
&lt;p&gt;When writing prompts, focus on detailed, chronological descriptions of actions and scenes. Include specific movements, appearances, camera angles, and environmental details - all in a single flowing paragraph. Start directly with the action, and keep descriptions literal and precise. Think like a cinematographer describing a shot list. Keep within 200 words. For best results, build your prompts using this structure:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Start with main action in a single sentence&lt;/li&gt; 
 &lt;li&gt;Add specific details about movements and gestures&lt;/li&gt; 
 &lt;li&gt;Describe character/object appearances precisely&lt;/li&gt; 
 &lt;li&gt;Include background and environment details&lt;/li&gt; 
 &lt;li&gt;Specify camera angles and movements&lt;/li&gt; 
 &lt;li&gt;Describe lighting and colors&lt;/li&gt; 
 &lt;li&gt;Note any changes or sudden events&lt;/li&gt; 
 &lt;li&gt;See &lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#introduction"&gt;examples&lt;/a&gt; for more inspiration.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Automatic Prompt Enhancement&lt;/h3&gt; 
&lt;p&gt;When using &lt;code&gt;LTXVideoPipeline&lt;/code&gt; directly, you can enable prompt enhancement by setting &lt;code&gt;enhance_prompt=True&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;🎮 Parameter Guide&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Resolution Preset: Higher resolutions for detailed scenes, lower for faster generation and simpler scenes. The model works on resolutions that are divisible by 32 and number of frames that are divisible by 8 + 1 (e.g. 257). In case the resolution or number of frames are not divisible by 32 or 8 + 1, the input will be padded with -1 and then cropped to the desired resolution and number of frames. The model works best on resolutions under 720 x 1280 and number of frames below 257&lt;/li&gt; 
 &lt;li&gt;Seed: Save seed values to recreate specific styles or compositions you like&lt;/li&gt; 
 &lt;li&gt;Guidance Scale: 3-3.5 are the recommended values&lt;/li&gt; 
 &lt;li&gt;Inference Steps: More steps (40+) for quality, fewer steps (20-30) for speed&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;📝 For advanced parameters usage, please see &lt;code&gt;python inference.py --help&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;Community Contribution&lt;/h2&gt; 
&lt;h3&gt;ComfyUI-LTXTricks 🛠️&lt;/h3&gt; 
&lt;p&gt;A community project providing additional nodes for enhanced control over the LTX Video model. It includes implementations of advanced techniques like RF-Inversion, RF-Edit, FlowEdit, and more. These nodes enable workflows such as Image and Video to Video (I+V2V), enhanced sampling via Spatiotemporal Skip Guidance (STG), and interpolation with precise frame settings.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Repository:&lt;/strong&gt; &lt;a href="https://github.com/logtd/ComfyUI-LTXTricks"&gt;ComfyUI-LTXTricks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Features:&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;🔄 &lt;strong&gt;RF-Inversion:&lt;/strong&gt; Implements &lt;a href="https://rf-inversion.github.io/"&gt;RF-Inversion&lt;/a&gt; with an &lt;a href="https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltx_inversion.json"&gt;example workflow here&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;✂️ &lt;strong&gt;RF-Edit:&lt;/strong&gt; Implements &lt;a href="https://github.com/wangjiangshan0725/RF-Solver-Edit"&gt;RF-Solver-Edit&lt;/a&gt; with an &lt;a href="https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltx_rf_edit.json"&gt;example workflow here&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;🌊 &lt;strong&gt;FlowEdit:&lt;/strong&gt; Implements &lt;a href="https://github.com/fallenshock/FlowEdit"&gt;FlowEdit&lt;/a&gt; with an &lt;a href="https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltx_flow_edit.json"&gt;example workflow here&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;🎥 &lt;strong&gt;I+V2V:&lt;/strong&gt; Enables Video to Video with a reference image. &lt;a href="https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltx_iv2v.json"&gt;Example workflow&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;✨ &lt;strong&gt;Enhance:&lt;/strong&gt; Partial implementation of &lt;a href="https://junhahyung.github.io/STGuidance/"&gt;STGuidance&lt;/a&gt;. &lt;a href="https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltxv_stg.json"&gt;Example workflow&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;🖼️ &lt;strong&gt;Interpolation and Frame Setting:&lt;/strong&gt; Nodes for precise control of latents per frame. &lt;a href="https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltx_interpolation.json"&gt;Example workflow&lt;/a&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;LTX-VideoQ8 🎱 &lt;a id="ltx-videoq8"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;LTX-VideoQ8&lt;/strong&gt; is an 8-bit optimized version of &lt;a href="https://github.com/Lightricks/LTX-Video"&gt;LTX-Video&lt;/a&gt;, designed for faster performance on NVIDIA ADA GPUs.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Repository:&lt;/strong&gt; &lt;a href="https://github.com/KONAKONA666/LTX-Video"&gt;LTX-VideoQ8&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Features:&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;🚀 Up to 3X speed-up with no accuracy loss&lt;/li&gt; 
   &lt;li&gt;🎥 Generate 720x480x121 videos in under a minute on RTX 4060 (8GB VRAM)&lt;/li&gt; 
   &lt;li&gt;🛠️ Fine-tune 2B transformer models with precalculated latents&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Community Discussion:&lt;/strong&gt; &lt;a href="https://www.reddit.com/r/StableDiffusion/comments/1h79ks2/fast_ltx_video_on_rtx_4060_and_other_ada_gpus/"&gt;Reddit Thread&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Diffusers integration:&lt;/strong&gt; A diffusers integration for the 8-bit model is already out! &lt;a href="https://github.com/sayakpaul/q8-ltx-video"&gt;Details here&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;TeaCache for LTX-Video 🍵 &lt;a id="TeaCache"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;TeaCache&lt;/strong&gt; is a training-free caching approach that leverages timestep differences across model outputs to accelerate LTX-Video inference by up to 2x without significant visual quality degradation.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Repository:&lt;/strong&gt; &lt;a href="https://github.com/ali-vilab/TeaCache/tree/main/TeaCache4LTX-Video"&gt;TeaCache4LTX-Video&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Features:&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;🚀 Speeds up LTX-Video inference.&lt;/li&gt; 
   &lt;li&gt;📊 Adjustable trade-offs between speed (up to 2x) and visual quality using configurable parameters.&lt;/li&gt; 
   &lt;li&gt;🛠️ No retraining required: Works directly with existing models.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Your Contribution&lt;/h3&gt; 
&lt;p&gt;...is welcome! If you have a project or tool that integrates with LTX-Video, please let us know by opening an issue or pull request.&lt;/p&gt; 
&lt;h1&gt;⚡️ Training&lt;/h1&gt; 
&lt;p&gt;We provide an open-source repository for fine-tuning the LTX-Video model: &lt;a href="https://github.com/Lightricks/LTX-Video-Trainer"&gt;LTX-Video-Trainer&lt;/a&gt;. This repository supports both the 2B and 13B model variants, enabling full fine-tuning as well as LoRA (Low-Rank Adaptation) fine-tuning for more efficient training. This includes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Control LoRAs&lt;/strong&gt;: Train custom control models like depth, pose, and canny control&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Effect LoRAs&lt;/strong&gt;: Create specialized effects and transformations for video generation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Explore the repository to customize the model for your specific use cases! More information and training instructions can be found in the &lt;a href="https://github.com/Lightricks/LTX-Video-Trainer/raw/main/README.md"&gt;README&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;🎬 Control Models&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo"&gt;ComfyUI-LTXVideo&lt;/a&gt; repository now contains workflows and models for 3 specialized models that enable precise control over LTX-Video generation:&lt;/p&gt; 
&lt;p&gt;Pose Control, Depth Control and Canny Control&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Example ComfyUI Workflow (for all control types):&lt;/strong&gt; &lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/ic_lora/ic-lora.json"&gt;ic-lora.json&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;🚀 Join Us&lt;/h1&gt; 
&lt;p&gt;Want to work on cutting-edge AI research and make a real impact on millions of users worldwide?&lt;/p&gt; 
&lt;p&gt;At &lt;strong&gt;Lightricks&lt;/strong&gt;, an AI-first company, we're revolutionizing how visual content is created.&lt;/p&gt; 
&lt;p&gt;If you are passionate about AI, computer vision, and video generation, we would love to hear from you!&lt;/p&gt; 
&lt;p&gt;Please visit our &lt;a href="https://careers.lightricks.com/careers?query=&amp;amp;office=all&amp;amp;department=R%26D"&gt;careers page&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h1&gt;Acknowledgement&lt;/h1&gt; 
&lt;p&gt;We are grateful for the following awesome projects when implementing LTX-Video:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/facebookresearch/DiT"&gt;DiT&lt;/a&gt; and &lt;a href="https://github.com/PixArt-alpha/PixArt-alpha"&gt;PixArt-alpha&lt;/a&gt;: vision transformers for image generation.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;📄 Our tech report is out! If you find our work helpful, please ⭐️ star the repository and cite our paper.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@article{HaCohen2024LTXVideo,
  title={LTX-Video: Realtime Video Latent Diffusion},
  author={HaCohen, Yoav and Chiprut, Nisan and Brazowski, Benny and Shalem, Daniel and Moshe, Dudu and Richardson, Eitan and Levin, Eran and Shiran, Guy and Zabari, Nir and Gordon, Ori and Panet, Poriya and Weissbuch, Sapir and Kulikov, Victor and Bitterman, Yaki and Melumian, Zeev and Bibi, Ofir},
  journal={arXiv preprint arXiv:2501.00103},
  year={2024}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>MODSetter/SurfSense</title>
      <link>https://github.com/MODSetter/SurfSense</link>
      <description>&lt;p&gt;Open Source Alternative to NotebookLM / Perplexity, connected to external sources such as Search Engines, Slack, Linear, Jira, ClickUp, Confluence, Notion, YouTube, GitHub, Discord and more. Join our discord: https://discord.gg/ejRNvftDp9&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/e236b764-0ddc-42ff-a1f1-8fbb3d2e0e65" alt="new_header" /&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://discord.gg/ejRNvftDp9"&gt; &lt;img src="https://img.shields.io/discord/1359368468260192417" alt="Discord" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h1&gt;SurfSense&lt;/h1&gt; 
&lt;p&gt;While tools like NotebookLM and Perplexity are impressive and highly effective for conducting research on any topic/query, SurfSense elevates this capability by integrating with your personal knowledge base. It is a highly customizable AI research agent, connected to external sources such as Search Engines (Tavily, LinkUp), Slack, Linear, Jira, ClickUp, Confluence, Gmail, Notion, YouTube, GitHub, Discord, Airtable, Google Calendar and more to come.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://trendshift.io/repositories/13606" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/13606" alt="MODSetter%2FSurfSense | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;h1&gt;Video&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/d9221908-e0de-4b2f-ac3a-691cf4b202da"&gt;https://github.com/user-attachments/assets/d9221908-e0de-4b2f-ac3a-691cf4b202da&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Podcast Sample&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/a0a16566-6967-4374-ac51-9b3e07fbecd7"&gt;https://github.com/user-attachments/assets/a0a16566-6967-4374-ac51-9b3e07fbecd7&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;h3&gt;💡 &lt;strong&gt;Idea&lt;/strong&gt;:&lt;/h3&gt; 
&lt;p&gt;Have your own highly customizable private NotebookLM and Perplexity integrated with external sources.&lt;/p&gt; 
&lt;h3&gt;📁 &lt;strong&gt;Multiple File Format Uploading Support&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Save content from your own personal files &lt;em&gt;(Documents, images, videos and supports &lt;strong&gt;50+ file extensions&lt;/strong&gt;)&lt;/em&gt; to your own personal knowledge base .&lt;/p&gt; 
&lt;h3&gt;🔍 &lt;strong&gt;Powerful Search&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Quickly research or find anything in your saved content .&lt;/p&gt; 
&lt;h3&gt;💬 &lt;strong&gt;Chat with your Saved Content&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Interact in Natural Language and get cited answers.&lt;/p&gt; 
&lt;h3&gt;📄 &lt;strong&gt;Cited Answers&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Get Cited answers just like Perplexity.&lt;/p&gt; 
&lt;h3&gt;🔔 &lt;strong&gt;Privacy &amp;amp; Local LLM Support&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Works Flawlessly with Ollama local LLMs.&lt;/p&gt; 
&lt;h3&gt;🏠 &lt;strong&gt;Self Hostable&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Open source and easy to deploy locally.&lt;/p&gt; 
&lt;h3&gt;🎙️ Podcasts&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Blazingly fast podcast generation agent. (Creates a 3-minute podcast in under 20 seconds.)&lt;/li&gt; 
 &lt;li&gt;Convert your chat conversations into engaging audio content&lt;/li&gt; 
 &lt;li&gt;Support for local TTS providers (Kokoro TTS)&lt;/li&gt; 
 &lt;li&gt;Support for multiple TTS providers (OpenAI, Azure, Google Vertex AI)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;📊 &lt;strong&gt;Advanced RAG Techniques&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Supports 100+ LLM's&lt;/li&gt; 
 &lt;li&gt;Supports 6000+ Embedding Models.&lt;/li&gt; 
 &lt;li&gt;Supports all major Rerankers (Pinecode, Cohere, Flashrank etc)&lt;/li&gt; 
 &lt;li&gt;Uses Hierarchical Indices (2 tiered RAG setup).&lt;/li&gt; 
 &lt;li&gt;Utilizes Hybrid Search (Semantic + Full Text Search combined with Reciprocal Rank Fusion).&lt;/li&gt; 
 &lt;li&gt;RAG as a Service API Backend.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ℹ️ &lt;strong&gt;External Sources&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Search Engines (Tavily, LinkUp)&lt;/li&gt; 
 &lt;li&gt;Slack&lt;/li&gt; 
 &lt;li&gt;Linear&lt;/li&gt; 
 &lt;li&gt;Jira&lt;/li&gt; 
 &lt;li&gt;ClickUp&lt;/li&gt; 
 &lt;li&gt;Confluence&lt;/li&gt; 
 &lt;li&gt;Notion&lt;/li&gt; 
 &lt;li&gt;Gmail&lt;/li&gt; 
 &lt;li&gt;Youtube Videos&lt;/li&gt; 
 &lt;li&gt;GitHub&lt;/li&gt; 
 &lt;li&gt;Discord&lt;/li&gt; 
 &lt;li&gt;Airtable&lt;/li&gt; 
 &lt;li&gt;Google Calendar&lt;/li&gt; 
 &lt;li&gt;and more to come.....&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;📄 &lt;strong&gt;Supported File Extensions&lt;/strong&gt;&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: File format support depends on your ETL service configuration. LlamaCloud supports 50+ formats, Unstructured supports 34+ core formats, and Docling (core formats, local processing, privacy-focused, no API key).&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Documents &amp;amp; Text&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;LlamaCloud&lt;/strong&gt;: &lt;code&gt;.pdf&lt;/code&gt;, &lt;code&gt;.doc&lt;/code&gt;, &lt;code&gt;.docx&lt;/code&gt;, &lt;code&gt;.docm&lt;/code&gt;, &lt;code&gt;.dot&lt;/code&gt;, &lt;code&gt;.dotm&lt;/code&gt;, &lt;code&gt;.rtf&lt;/code&gt;, &lt;code&gt;.txt&lt;/code&gt;, &lt;code&gt;.xml&lt;/code&gt;, &lt;code&gt;.epub&lt;/code&gt;, &lt;code&gt;.odt&lt;/code&gt;, &lt;code&gt;.wpd&lt;/code&gt;, &lt;code&gt;.pages&lt;/code&gt;, &lt;code&gt;.key&lt;/code&gt;, &lt;code&gt;.numbers&lt;/code&gt;, &lt;code&gt;.602&lt;/code&gt;, &lt;code&gt;.abw&lt;/code&gt;, &lt;code&gt;.cgm&lt;/code&gt;, &lt;code&gt;.cwk&lt;/code&gt;, &lt;code&gt;.hwp&lt;/code&gt;, &lt;code&gt;.lwp&lt;/code&gt;, &lt;code&gt;.mw&lt;/code&gt;, &lt;code&gt;.mcw&lt;/code&gt;, &lt;code&gt;.pbd&lt;/code&gt;, &lt;code&gt;.sda&lt;/code&gt;, &lt;code&gt;.sdd&lt;/code&gt;, &lt;code&gt;.sdp&lt;/code&gt;, &lt;code&gt;.sdw&lt;/code&gt;, &lt;code&gt;.sgl&lt;/code&gt;, &lt;code&gt;.sti&lt;/code&gt;, &lt;code&gt;.sxi&lt;/code&gt;, &lt;code&gt;.sxw&lt;/code&gt;, &lt;code&gt;.stw&lt;/code&gt;, &lt;code&gt;.sxg&lt;/code&gt;, &lt;code&gt;.uof&lt;/code&gt;, &lt;code&gt;.uop&lt;/code&gt;, &lt;code&gt;.uot&lt;/code&gt;, &lt;code&gt;.vor&lt;/code&gt;, &lt;code&gt;.wps&lt;/code&gt;, &lt;code&gt;.zabw&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Unstructured&lt;/strong&gt;: &lt;code&gt;.doc&lt;/code&gt;, &lt;code&gt;.docx&lt;/code&gt;, &lt;code&gt;.odt&lt;/code&gt;, &lt;code&gt;.rtf&lt;/code&gt;, &lt;code&gt;.pdf&lt;/code&gt;, &lt;code&gt;.xml&lt;/code&gt;, &lt;code&gt;.txt&lt;/code&gt;, &lt;code&gt;.md&lt;/code&gt;, &lt;code&gt;.markdown&lt;/code&gt;, &lt;code&gt;.rst&lt;/code&gt;, &lt;code&gt;.html&lt;/code&gt;, &lt;code&gt;.org&lt;/code&gt;, &lt;code&gt;.epub&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Docling&lt;/strong&gt;: &lt;code&gt;.pdf&lt;/code&gt;, &lt;code&gt;.docx&lt;/code&gt;, &lt;code&gt;.html&lt;/code&gt;, &lt;code&gt;.htm&lt;/code&gt;, &lt;code&gt;.xhtml&lt;/code&gt;, &lt;code&gt;.adoc&lt;/code&gt;, &lt;code&gt;.asciidoc&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Presentations&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;LlamaCloud&lt;/strong&gt;: &lt;code&gt;.ppt&lt;/code&gt;, &lt;code&gt;.pptx&lt;/code&gt;, &lt;code&gt;.pptm&lt;/code&gt;, &lt;code&gt;.pot&lt;/code&gt;, &lt;code&gt;.potm&lt;/code&gt;, &lt;code&gt;.potx&lt;/code&gt;, &lt;code&gt;.odp&lt;/code&gt;, &lt;code&gt;.key&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Unstructured&lt;/strong&gt;: &lt;code&gt;.ppt&lt;/code&gt;, &lt;code&gt;.pptx&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Docling&lt;/strong&gt;: &lt;code&gt;.pptx&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Spreadsheets &amp;amp; Data&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;LlamaCloud&lt;/strong&gt;: &lt;code&gt;.xlsx&lt;/code&gt;, &lt;code&gt;.xls&lt;/code&gt;, &lt;code&gt;.xlsm&lt;/code&gt;, &lt;code&gt;.xlsb&lt;/code&gt;, &lt;code&gt;.xlw&lt;/code&gt;, &lt;code&gt;.csv&lt;/code&gt;, &lt;code&gt;.tsv&lt;/code&gt;, &lt;code&gt;.ods&lt;/code&gt;, &lt;code&gt;.fods&lt;/code&gt;, &lt;code&gt;.numbers&lt;/code&gt;, &lt;code&gt;.dbf&lt;/code&gt;, &lt;code&gt;.123&lt;/code&gt;, &lt;code&gt;.dif&lt;/code&gt;, &lt;code&gt;.sylk&lt;/code&gt;, &lt;code&gt;.slk&lt;/code&gt;, &lt;code&gt;.prn&lt;/code&gt;, &lt;code&gt;.et&lt;/code&gt;, &lt;code&gt;.uos1&lt;/code&gt;, &lt;code&gt;.uos2&lt;/code&gt;, &lt;code&gt;.wk1&lt;/code&gt;, &lt;code&gt;.wk2&lt;/code&gt;, &lt;code&gt;.wk3&lt;/code&gt;, &lt;code&gt;.wk4&lt;/code&gt;, &lt;code&gt;.wks&lt;/code&gt;, &lt;code&gt;.wq1&lt;/code&gt;, &lt;code&gt;.wq2&lt;/code&gt;, &lt;code&gt;.wb1&lt;/code&gt;, &lt;code&gt;.wb2&lt;/code&gt;, &lt;code&gt;.wb3&lt;/code&gt;, &lt;code&gt;.qpw&lt;/code&gt;, &lt;code&gt;.xlr&lt;/code&gt;, &lt;code&gt;.eth&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Unstructured&lt;/strong&gt;: &lt;code&gt;.xls&lt;/code&gt;, &lt;code&gt;.xlsx&lt;/code&gt;, &lt;code&gt;.csv&lt;/code&gt;, &lt;code&gt;.tsv&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Docling&lt;/strong&gt;: &lt;code&gt;.xlsx&lt;/code&gt;, &lt;code&gt;.csv&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Images&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;LlamaCloud&lt;/strong&gt;: &lt;code&gt;.jpg&lt;/code&gt;, &lt;code&gt;.jpeg&lt;/code&gt;, &lt;code&gt;.png&lt;/code&gt;, &lt;code&gt;.gif&lt;/code&gt;, &lt;code&gt;.bmp&lt;/code&gt;, &lt;code&gt;.svg&lt;/code&gt;, &lt;code&gt;.tiff&lt;/code&gt;, &lt;code&gt;.webp&lt;/code&gt;, &lt;code&gt;.html&lt;/code&gt;, &lt;code&gt;.htm&lt;/code&gt;, &lt;code&gt;.web&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Unstructured&lt;/strong&gt;: &lt;code&gt;.jpg&lt;/code&gt;, &lt;code&gt;.jpeg&lt;/code&gt;, &lt;code&gt;.png&lt;/code&gt;, &lt;code&gt;.bmp&lt;/code&gt;, &lt;code&gt;.tiff&lt;/code&gt;, &lt;code&gt;.heic&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Docling&lt;/strong&gt;: &lt;code&gt;.jpg&lt;/code&gt;, &lt;code&gt;.jpeg&lt;/code&gt;, &lt;code&gt;.png&lt;/code&gt;, &lt;code&gt;.bmp&lt;/code&gt;, &lt;code&gt;.tiff&lt;/code&gt;, &lt;code&gt;.tif&lt;/code&gt;, &lt;code&gt;.webp&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Audio &amp;amp; Video &lt;em&gt;(Always Supported)&lt;/em&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;.mp3&lt;/code&gt;, &lt;code&gt;.mpga&lt;/code&gt;, &lt;code&gt;.m4a&lt;/code&gt;, &lt;code&gt;.wav&lt;/code&gt;, &lt;code&gt;.mp4&lt;/code&gt;, &lt;code&gt;.mpeg&lt;/code&gt;, &lt;code&gt;.webm&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Email &amp;amp; Communication&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Unstructured&lt;/strong&gt;: &lt;code&gt;.eml&lt;/code&gt;, &lt;code&gt;.msg&lt;/code&gt;, &lt;code&gt;.p7s&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;🔖 Cross Browser Extension&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;The SurfSense extension can be used to save any webpage you like.&lt;/li&gt; 
 &lt;li&gt;Its main usecase is to save any webpages protected beyond authentication.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;FEATURE REQUESTS AND FUTURE&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;SurfSense is actively being developed.&lt;/strong&gt; While it's not yet production-ready, you can help us speed up the process.&lt;/p&gt; 
&lt;p&gt;Join the &lt;a href="https://discord.gg/ejRNvftDp9"&gt;SurfSense Discord&lt;/a&gt; and help shape the future of SurfSense!&lt;/p&gt; 
&lt;h2&gt;🚀 Roadmap&lt;/h2&gt; 
&lt;p&gt;Stay up to date with our development progress and upcoming features!&lt;br /&gt; Check out our public roadmap and contribute your ideas or feedback:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;View the Roadmap:&lt;/strong&gt; &lt;a href="https://github.com/users/MODSetter/projects/2"&gt;SurfSense Roadmap on GitHub Projects&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;How to get started?&lt;/h2&gt; 
&lt;h3&gt;Installation Options&lt;/h3&gt; 
&lt;p&gt;SurfSense provides two installation methods:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.surfsense.net/docs/docker-installation"&gt;Docker Installation&lt;/a&gt;&lt;/strong&gt; - The easiest way to get SurfSense up and running with all dependencies containerized.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Includes pgAdmin for database management through a web UI&lt;/li&gt; 
   &lt;li&gt;Supports environment variable customization via &lt;code&gt;.env&lt;/code&gt; file&lt;/li&gt; 
   &lt;li&gt;Flexible deployment options (full stack or core services only)&lt;/li&gt; 
   &lt;li&gt;No need to manually edit configuration files between environments&lt;/li&gt; 
   &lt;li&gt;See &lt;a href="https://raw.githubusercontent.com/MODSetter/SurfSense/main/DOCKER_SETUP.md"&gt;Docker Setup Guide&lt;/a&gt; for detailed instructions&lt;/li&gt; 
   &lt;li&gt;For deployment scenarios and options, see &lt;a href="https://raw.githubusercontent.com/MODSetter/SurfSense/main/DEPLOYMENT_GUIDE.md"&gt;Deployment Guide&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.surfsense.net/docs/manual-installation"&gt;Manual Installation (Recommended)&lt;/a&gt;&lt;/strong&gt; - For users who prefer more control over their setup or need to customize their deployment.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Both installation guides include detailed OS-specific instructions for Windows, macOS, and Linux.&lt;/p&gt; 
&lt;p&gt;Before installation, make sure to complete the &lt;a href="https://www.surfsense.net/docs/"&gt;prerequisite setup steps&lt;/a&gt; including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;PGVector setup&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;File Processing ETL Service&lt;/strong&gt; (choose one): 
  &lt;ul&gt; 
   &lt;li&gt;Unstructured.io API key (supports 34+ formats)&lt;/li&gt; 
   &lt;li&gt;LlamaIndex API key (enhanced parsing, supports 50+ formats)&lt;/li&gt; 
   &lt;li&gt;Docling (local processing, no API key required, supports PDF, Office docs, images, HTML, CSV)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Other required API keys&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Screenshots&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Research Agent&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/e22c5d86-f511-4c72-8c50-feba0c1561b4" alt="updated_researcher" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Search Spaces&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/e254c38c-f937-44b6-9e9d-770db583d099" alt="search_spaces" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Manage Documents&lt;/strong&gt; &lt;img src="https://github.com/user-attachments/assets/7001e306-eb06-4009-89c6-8fadfdc3fc4d" alt="documents" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Podcast Agent&lt;/strong&gt; &lt;img src="https://github.com/user-attachments/assets/6cb82ffd-9e14-4172-bc79-67faf34c4c1c" alt="podcasts" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Agent Chat&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/bb352d52-1c6d-4020-926b-722d0b98b491" alt="git_chat" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Browser Extension&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/1f042b7a-6349-422b-94fb-d40d0df16c40" alt="ext1" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/a9b9f1aa-2677-404d-b0a0-c1b2dddf24a7" alt="ext2" /&gt;&lt;/p&gt; 
&lt;h2&gt;Tech Stack&lt;/h2&gt; 
&lt;h3&gt;&lt;strong&gt;BackEnd&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;FastAPI&lt;/strong&gt;: Modern, fast web framework for building APIs with Python&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;PostgreSQL with pgvector&lt;/strong&gt;: Database with vector search capabilities for similarity searches&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;SQLAlchemy&lt;/strong&gt;: SQL toolkit and ORM (Object-Relational Mapping) for database interactions&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Alembic&lt;/strong&gt;: A database migrations tool for SQLAlchemy.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;FastAPI Users&lt;/strong&gt;: Authentication and user management with JWT and OAuth support&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;LangGraph&lt;/strong&gt;: Framework for developing AI-agents.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;LangChain&lt;/strong&gt;: Framework for developing AI-powered applications.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;LLM Integration&lt;/strong&gt;: Integration with LLM models through LiteLLM&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Rerankers&lt;/strong&gt;: Advanced result ranking for improved search relevance&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Hybrid Search&lt;/strong&gt;: Combines vector similarity and full-text search for optimal results using Reciprocal Rank Fusion (RRF)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Vector Embeddings&lt;/strong&gt;: Document and text embeddings for semantic search&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;pgvector&lt;/strong&gt;: PostgreSQL extension for efficient vector similarity operations&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Chonkie&lt;/strong&gt;: Advanced document chunking and embedding library&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Uses &lt;code&gt;AutoEmbeddings&lt;/code&gt; for flexible embedding model selection&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;LateChunker&lt;/code&gt; for optimized document chunking based on embedding model's max sequence length&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;&lt;strong&gt;FrontEnd&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Next.js 15.2.3&lt;/strong&gt;: React framework featuring App Router, server components, automatic code-splitting, and optimized rendering.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;React 19.0.0&lt;/strong&gt;: JavaScript library for building user interfaces.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;TypeScript&lt;/strong&gt;: Static type-checking for JavaScript, enhancing code quality and developer experience.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Vercel AI SDK Kit UI Stream Protocol&lt;/strong&gt;: To create scalable chat UI.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Tailwind CSS 4.x&lt;/strong&gt;: Utility-first CSS framework for building custom UI designs.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Shadcn&lt;/strong&gt;: Headless components library.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Lucide React&lt;/strong&gt;: Icon set implemented as React components.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Framer Motion&lt;/strong&gt;: Animation library for React.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Sonner&lt;/strong&gt;: Toast notification library.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Geist&lt;/strong&gt;: Font family from Vercel.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;React Hook Form&lt;/strong&gt;: Form state management and validation.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Zod&lt;/strong&gt;: TypeScript-first schema validation with static type inference.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;@hookform/resolvers&lt;/strong&gt;: Resolvers for using validation libraries with React Hook Form.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;@tanstack/react-table&lt;/strong&gt;: Headless UI for building powerful tables &amp;amp; datagrids.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;strong&gt;DevOps&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Docker&lt;/strong&gt;: Container platform for consistent deployment across environments&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Docker Compose&lt;/strong&gt;: Tool for defining and running multi-container Docker applications&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;pgAdmin&lt;/strong&gt;: Web-based PostgreSQL administration tool included in Docker setup&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;strong&gt;Extension&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Manifest v3 on Plasmo&lt;/p&gt; 
&lt;h2&gt;Future Work&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Add More Connectors.&lt;/li&gt; 
 &lt;li&gt;Patch minor bugs.&lt;/li&gt; 
 &lt;li&gt;Document Podcasts&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contribute&lt;/h2&gt; 
&lt;p&gt;Contributions are very welcome! A contribution can be as small as a ⭐ or even finding and creating issues. Fine-tuning the Backend is always desired.&lt;/p&gt; 
&lt;p&gt;For detailed contribution guidelines, please see our &lt;a href="https://raw.githubusercontent.com/MODSetter/SurfSense/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; file.&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;a href="https://www.star-history.com/#MODSetter/SurfSense&amp;amp;Date"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=MODSetter/SurfSense&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=MODSetter/SurfSense&amp;amp;type=Date" /&gt; 
  &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=MODSetter/SurfSense&amp;amp;type=Date" /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;hr /&gt; 
&lt;hr /&gt; 
&lt;p align="center"&gt; &lt;img src="https://github.com/user-attachments/assets/329c9bc2-6005-4aed-a629-700b5ae296b4" alt="Catalyst Project" width="200" /&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;hr /&gt;</description>
    </item>
    
    <item>
      <title>cheahjs/free-llm-api-resources</title>
      <link>https://github.com/cheahjs/free-llm-api-resources</link>
      <description>&lt;p&gt;A list of free LLM inference resources accessible via API.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Free LLM API resources&lt;/h1&gt; 
&lt;p&gt;This lists various services that provide free access or credits towards API-based LLM usage.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;br /&gt; Please don't abuse these services, else we might lose them.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING]&lt;br /&gt; This list explicitly excludes any services that are not legitimate (eg reverse engineers an existing chatbot)&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cheahjs/free-llm-api-resources/main/#free-providers"&gt;Free Providers&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cheahjs/free-llm-api-resources/main/#openrouter"&gt;OpenRouter&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cheahjs/free-llm-api-resources/main/#google-ai-studio"&gt;Google AI Studio&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cheahjs/free-llm-api-resources/main/#nvidia-nim"&gt;NVIDIA NIM&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cheahjs/free-llm-api-resources/main/#mistral-la-plateforme"&gt;Mistral (La Plateforme)&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cheahjs/free-llm-api-resources/main/#mistral-codestral"&gt;Mistral (Codestral)&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cheahjs/free-llm-api-resources/main/#huggingface-inference-providers"&gt;HuggingFace Inference Providers&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cheahjs/free-llm-api-resources/main/#vercel-ai-gateway"&gt;Vercel AI Gateway&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cheahjs/free-llm-api-resources/main/#cerebras"&gt;Cerebras&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cheahjs/free-llm-api-resources/main/#groq"&gt;Groq&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cheahjs/free-llm-api-resources/main/#together-free"&gt;Together (Free)&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cheahjs/free-llm-api-resources/main/#cohere"&gt;Cohere&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cheahjs/free-llm-api-resources/main/#github-models"&gt;GitHub Models&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cheahjs/free-llm-api-resources/main/#cloudflare-workers-ai"&gt;Cloudflare Workers AI&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cheahjs/free-llm-api-resources/main/#google-cloud-vertex-ai"&gt;Google Cloud Vertex AI&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cheahjs/free-llm-api-resources/main/#providers-with-trial-credits"&gt;Providers with trial credits&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cheahjs/free-llm-api-resources/main/#together"&gt;Together&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cheahjs/free-llm-api-resources/main/#fireworks"&gt;Fireworks&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cheahjs/free-llm-api-resources/main/#baseten"&gt;Baseten&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cheahjs/free-llm-api-resources/main/#nebius"&gt;Nebius&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cheahjs/free-llm-api-resources/main/#novita"&gt;Novita&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cheahjs/free-llm-api-resources/main/#ai21"&gt;AI21&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cheahjs/free-llm-api-resources/main/#upstage"&gt;Upstage&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cheahjs/free-llm-api-resources/main/#nlp-cloud"&gt;NLP Cloud&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cheahjs/free-llm-api-resources/main/#alibaba-cloud-international-model-studio"&gt;Alibaba Cloud (International) Model Studio&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cheahjs/free-llm-api-resources/main/#modal"&gt;Modal&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cheahjs/free-llm-api-resources/main/#inferencenet"&gt;Inference.net&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cheahjs/free-llm-api-resources/main/#ncompass"&gt;nCompass&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cheahjs/free-llm-api-resources/main/#hyperbolic"&gt;Hyperbolic&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cheahjs/free-llm-api-resources/main/#sambanova-cloud"&gt;SambaNova Cloud&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cheahjs/free-llm-api-resources/main/#scaleway-generative-apis"&gt;Scaleway Generative APIs&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Free Providers&lt;/h2&gt; 
&lt;h3&gt;&lt;a href="https://openrouter.ai"&gt;OpenRouter&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Limits:&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://openrouter.ai/docs/api-reference/limits"&gt;20 requests/minute&lt;br /&gt;50 requests/day&lt;br /&gt;1000 requests/day with $10 lifetime topup&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Models share a common quota.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://openrouter.ai/agentica-org/deepcoder-14b-preview:free"&gt;DeepCoder 14B Preview&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openrouter.ai/nousresearch/deephermes-3-llama-3-8b-preview:free"&gt;DeepHermes 3 Llama 3 8B Preview&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openrouter.ai/deepseek/deepseek-r1:free"&gt;DeepSeek R1&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openrouter.ai/deepseek/deepseek-r1-distill-llama-70b:free"&gt;DeepSeek R1 Distill Llama 70B&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openrouter.ai/deepseek/deepseek-r1-distill-qwen-14b:free"&gt;DeepSeek R1 Distill Qwen 14B&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openrouter.ai/deepseek/deepseek-chat-v3-0324:free"&gt;DeepSeek V3 0324&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openrouter.ai/cognitivecomputations/dolphin3.0-mistral-24b:free"&gt;Dolphin 3.0 Mistral 24B&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openrouter.ai/cognitivecomputations/dolphin3.0-r1-mistral-24b:free"&gt;Dolphin 3.0 R1 Mistral 24B&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openrouter.ai/google/gemma-2-9b-it:free"&gt;Gemma 2 9B Instruct&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openrouter.ai/google/gemma-3-12b-it:free"&gt;Gemma 3 12B Instruct&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openrouter.ai/google/gemma-3-27b-it:free"&gt;Gemma 3 27B Instruct&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openrouter.ai/google/gemma-3-4b-it:free"&gt;Gemma 3 4B Instruct&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openrouter.ai/moonshotai/kimi-vl-a3b-thinking:free"&gt;Kimi VL A3B Thinking&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openrouter.ai/meta-llama/llama-3.1-405b-instruct:free"&gt;Llama 3.1 405B Instruct&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openrouter.ai/nvidia/llama-3.1-nemotron-ultra-253b-v1:free"&gt;Llama 3.1 Nemotron Ultra 253B v1&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openrouter.ai/meta-llama/llama-3.2-3b-instruct:free"&gt;Llama 3.2 3B Instruct&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openrouter.ai/meta-llama/llama-3.3-70b-instruct:free"&gt;Llama 3.3 70B Instruct&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openrouter.ai/meta-llama/llama-4-maverick:free"&gt;Llama 4 Maverick&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openrouter.ai/meta-llama/llama-4-scout:free"&gt;Llama 4 Scout&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openrouter.ai/mistralai/mistral-7b-instruct:free"&gt;Mistral 7B Instruct&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openrouter.ai/mistralai/mistral-nemo:free"&gt;Mistral Nemo&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openrouter.ai/mistralai/mistral-small-24b-instruct-2501:free"&gt;Mistral Small 24B Instruct 2501&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openrouter.ai/mistralai/mistral-small-3.1-24b-instruct:free"&gt;Mistral Small 3.1 24B Instruct&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openrouter.ai/arliai/qwq-32b-arliai-rpr-v1:free"&gt;QwQ 32B ArliAI RpR v1&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openrouter.ai/qwen/qwen-2.5-72b-instruct:free"&gt;Qwen 2.5 72B Instruct&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openrouter.ai/qwen/qwen2.5-vl-32b-instruct:free"&gt;Qwen 2.5 VL 32B Instruct&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openrouter.ai/qwen/qwq-32b:free"&gt;Qwen QwQ 32B&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openrouter.ai/qwen/qwen-2.5-coder-32b-instruct:free"&gt;Qwen2.5 Coder 32B Instruct&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openrouter.ai/qwen/qwen2.5-vl-72b-instruct:free"&gt;Qwen2.5 VL 72B Instruct&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openrouter.ai/rekaai/reka-flash-3:free"&gt;Reka Flash 3&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openrouter.ai/shisa-ai/shisa-v2-llama3.3-70b:free"&gt;Shisa V2 Llama 3.3 70B&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openrouter.ai/cognitivecomputations/dolphin-mistral-24b-venice-edition:free"&gt;cognitivecomputations/dolphin-mistral-24b-venice-edition:free&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openrouter.ai/deepseek/deepseek-chat-v3.1:free"&gt;deepseek/deepseek-chat-v3.1:free&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openrouter.ai/deepseek/deepseek-r1-0528-qwen3-8b:free"&gt;deepseek/deepseek-r1-0528-qwen3-8b:free&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openrouter.ai/deepseek/deepseek-r1-0528:free"&gt;deepseek/deepseek-r1-0528:free&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openrouter.ai/google/gemma-3n-e2b-it:free"&gt;google/gemma-3n-e2b-it:free&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openrouter.ai/google/gemma-3n-e4b-it:free"&gt;google/gemma-3n-e4b-it:free&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openrouter.ai/meta-llama/llama-3.3-8b-instruct:free"&gt;meta-llama/llama-3.3-8b-instruct:free&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openrouter.ai/microsoft/mai-ds-r1:free"&gt;microsoft/mai-ds-r1:free&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openrouter.ai/mistralai/devstral-small-2505:free"&gt;mistralai/devstral-small-2505:free&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openrouter.ai/mistralai/mistral-small-3.2-24b-instruct:free"&gt;mistralai/mistral-small-3.2-24b-instruct:free&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openrouter.ai/moonshotai/kimi-dev-72b:free"&gt;moonshotai/kimi-dev-72b:free&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openrouter.ai/moonshotai/kimi-k2:free"&gt;moonshotai/kimi-k2:free&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openrouter.ai/nvidia/nemotron-nano-9b-v2:free"&gt;nvidia/nemotron-nano-9b-v2:free&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openrouter.ai/openai/gpt-oss-120b:free"&gt;openai/gpt-oss-120b:free&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openrouter.ai/openai/gpt-oss-20b:free"&gt;openai/gpt-oss-20b:free&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openrouter.ai/qwen/qwen3-14b:free"&gt;qwen/qwen3-14b:free&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openrouter.ai/qwen/qwen3-235b-a22b:free"&gt;qwen/qwen3-235b-a22b:free&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openrouter.ai/qwen/qwen3-30b-a3b:free"&gt;qwen/qwen3-30b-a3b:free&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openrouter.ai/qwen/qwen3-4b:free"&gt;qwen/qwen3-4b:free&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openrouter.ai/qwen/qwen3-8b:free"&gt;qwen/qwen3-8b:free&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openrouter.ai/qwen/qwen3-coder:free"&gt;qwen/qwen3-coder:free&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openrouter.ai/tencent/hunyuan-a13b-instruct:free"&gt;tencent/hunyuan-a13b-instruct:free&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openrouter.ai/tngtech/deepseek-r1t-chimera:free"&gt;tngtech/deepseek-r1t-chimera:free&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openrouter.ai/tngtech/deepseek-r1t2-chimera:free"&gt;tngtech/deepseek-r1t2-chimera:free&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openrouter.ai/z-ai/glm-4.5-air:free"&gt;z-ai/glm-4.5-air:free&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;a href="https://aistudio.google.com"&gt;Google AI Studio&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Data is used for training when used outside of the UK/CH/EEA/EU.&lt;/p&gt; 
&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th&gt;Model Name&lt;/th&gt;
   &lt;th&gt;Model Limits&lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt; 
  &lt;tr&gt;
   &lt;td&gt;Gemini 2.5 Pro&lt;/td&gt;
   &lt;td&gt;3,000,000 tokens/day&lt;br /&gt;125,000 tokens/minute&lt;br /&gt;50 requests/day&lt;br /&gt;2 requests/minute&lt;/td&gt;
  &lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;Gemini 2.5 Flash&lt;/td&gt;
   &lt;td&gt;250,000 tokens/minute&lt;br /&gt;250 requests/day&lt;br /&gt;10 requests/minute&lt;/td&gt;
  &lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;Gemini 2.5 Flash-Lite&lt;/td&gt;
   &lt;td&gt;250,000 tokens/minute&lt;br /&gt;1,000 requests/day&lt;br /&gt;15 requests/minute&lt;/td&gt;
  &lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;Gemini 2.5 Flash Image Preview (Nano Banana)&lt;/td&gt;
   &lt;td&gt;&lt;/td&gt;
  &lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;Gemini 2.0 Flash&lt;/td&gt;
   &lt;td&gt;1,000,000 tokens/minute&lt;br /&gt;200 requests/day&lt;br /&gt;15 requests/minute&lt;/td&gt;
  &lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;Gemini 2.0 Flash-Lite&lt;/td&gt;
   &lt;td&gt;1,000,000 tokens/minute&lt;br /&gt;200 requests/day&lt;br /&gt;30 requests/minute&lt;/td&gt;
  &lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;Gemini 2.0 Flash (Experimental)&lt;/td&gt;
   &lt;td&gt;250,000 tokens/minute&lt;br /&gt;50 requests/day&lt;br /&gt;10 requests/minute&lt;/td&gt;
  &lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;Gemini 1.5 Flash&lt;/td&gt;
   &lt;td&gt;250,000 tokens/minute&lt;br /&gt;50 requests/day&lt;br /&gt;15 requests/minute&lt;/td&gt;
  &lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;Gemini 1.5 Flash-8B&lt;/td&gt;
   &lt;td&gt;250,000 tokens/minute&lt;br /&gt;50 requests/day&lt;br /&gt;15 requests/minute&lt;/td&gt;
  &lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;LearnLM 2.0 Flash (Experimental)&lt;/td&gt;
   &lt;td&gt;1,500 requests/day&lt;br /&gt;15 requests/minute&lt;/td&gt;
  &lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;Gemma 3 27B Instruct&lt;/td&gt;
   &lt;td&gt;15,000 tokens/minute&lt;br /&gt;14,400 requests/day&lt;br /&gt;30 requests/minute&lt;/td&gt;
  &lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;Gemma 3 12B Instruct&lt;/td&gt;
   &lt;td&gt;15,000 tokens/minute&lt;br /&gt;14,400 requests/day&lt;br /&gt;30 requests/minute&lt;/td&gt;
  &lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;Gemma 3 4B Instruct&lt;/td&gt;
   &lt;td&gt;15,000 tokens/minute&lt;br /&gt;14,400 requests/day&lt;br /&gt;30 requests/minute&lt;/td&gt;
  &lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;Gemma 3 1B Instruct&lt;/td&gt;
   &lt;td&gt;15,000 tokens/minute&lt;br /&gt;14,400 requests/day&lt;br /&gt;30 requests/minute&lt;/td&gt;
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;&lt;a href="https://build.nvidia.com/explore/discover"&gt;NVIDIA NIM&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Phone number verification required. Models tend to be context window limited.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Limits:&lt;/strong&gt; 40 requests/minute&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://build.nvidia.com/models"&gt;Various open models&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;a href="https://console.mistral.ai/"&gt;Mistral (La Plateforme)&lt;/a&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Free tier (Experiment plan) requires opting into data training&lt;/li&gt; 
 &lt;li&gt;Requires phone number verification.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Limits (per-model):&lt;/strong&gt; 1 request/second, 500,000 tokens/minute, 1,000,000,000 tokens/month&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.mistral.ai/getting-started/models/models_overview/"&gt;Open and Proprietary Mistral models&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;a href="https://codestral.mistral.ai/"&gt;Mistral (Codestral)&lt;/a&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Currently free to use&lt;/li&gt; 
 &lt;li&gt;Monthly subscription based&lt;/li&gt; 
 &lt;li&gt;Requires phone number verification&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Limits:&lt;/strong&gt; 30 requests/minute, 2,000 requests/day&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Codestral&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;a href="https://huggingface.co/docs/inference-providers/en/index"&gt;HuggingFace Inference Providers&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;HuggingFace Serverless Inference limited to models smaller than 10GB. Some popular models are supported even if they exceed 10GB.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Limits:&lt;/strong&gt; &lt;a href="https://huggingface.co/docs/inference-providers/en/pricing"&gt;$0.10/month in credits&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Various open models across supported providers&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;a href="https://vercel.com/docs/ai-gateway"&gt;Vercel AI Gateway&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Routes to various supported providers.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Limits:&lt;/strong&gt; &lt;a href="https://vercel.com/docs/ai-gateway/pricing"&gt;$5/month&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;&lt;a href="https://cloud.cerebras.ai/"&gt;Cerebras&lt;/a&gt;&lt;/h3&gt; 
&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th&gt;Model Name&lt;/th&gt;
   &lt;th&gt;Model Limits&lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt; 
  &lt;tr&gt;
   &lt;td&gt;gpt-oss-120b&lt;/td&gt;
   &lt;td&gt;30 requests/minute&lt;br /&gt;60,000 tokens/minute&lt;br /&gt;900 requests/hour&lt;br /&gt;1,000,000 tokens/hour&lt;br /&gt;14,400 requests/day&lt;br /&gt;1,000,000 tokens/day&lt;/td&gt;
  &lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;Qwen 3 235B A22B Instruct&lt;/td&gt;
   &lt;td&gt;30 requests/minute&lt;br /&gt;60,000 tokens/minute&lt;br /&gt;900 requests/hour&lt;br /&gt;1,000,000 tokens/hour&lt;br /&gt;14,400 requests/day&lt;br /&gt;1,000,000 tokens/day&lt;/td&gt;
  &lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;Qwen 3 235B A22B Thinking&lt;/td&gt;
   &lt;td&gt;30 requests/minute&lt;br /&gt;60,000 tokens/minute&lt;br /&gt;900 requests/hour&lt;br /&gt;1,000,000 tokens/hour&lt;br /&gt;14,400 requests/day&lt;br /&gt;1,000,000 tokens/day&lt;/td&gt;
  &lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;Qwen 3 Coder 480B&lt;/td&gt;
   &lt;td&gt;10 requests/minute&lt;br /&gt;150,000 tokens/minute&lt;br /&gt;100 requests/hour&lt;br /&gt;1,000,000 tokens/hour&lt;br /&gt;100 requests/day&lt;br /&gt;1,000,000 tokens/day&lt;/td&gt;
  &lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;Llama 3.3 70B&lt;/td&gt;
   &lt;td&gt;30 requests/minute&lt;br /&gt;64,000 tokens/minute&lt;br /&gt;900 requests/hour&lt;br /&gt;1,000,000 tokens/hour&lt;br /&gt;14,400 requests/day&lt;br /&gt;1,000,000 tokens/day&lt;/td&gt;
  &lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;Qwen 3 32B&lt;/td&gt;
   &lt;td&gt;30 requests/minute&lt;br /&gt;64,000 tokens/minute&lt;br /&gt;900 requests/hour&lt;br /&gt;1,000,000 tokens/hour&lt;br /&gt;14,400 requests/day&lt;br /&gt;1,000,000 tokens/day&lt;/td&gt;
  &lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;Llama 3.1 8B&lt;/td&gt;
   &lt;td&gt;30 requests/minute&lt;br /&gt;60,000 tokens/minute&lt;br /&gt;900 requests/hour&lt;br /&gt;1,000,000 tokens/hour&lt;br /&gt;14,400 requests/day&lt;br /&gt;1,000,000 tokens/day&lt;/td&gt;
  &lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;Llama 4 Scout&lt;/td&gt;
   &lt;td&gt;30 requests/minute&lt;br /&gt;60,000 tokens/minute&lt;br /&gt;900 requests/hour&lt;br /&gt;1,000,000 tokens/hour&lt;br /&gt;14,400 requests/day&lt;br /&gt;1,000,000 tokens/day&lt;/td&gt;
  &lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;Llama 4 Maverick&lt;/td&gt;
   &lt;td&gt;30 requests/minute&lt;br /&gt;60,000 tokens/minute&lt;br /&gt;900 requests/hour&lt;br /&gt;1,000,000 tokens/hour&lt;br /&gt;14,400 requests/day&lt;br /&gt;1,000,000 tokens/day&lt;/td&gt;
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;&lt;a href="https://console.groq.com"&gt;Groq&lt;/a&gt;&lt;/h3&gt; 
&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th&gt;Model Name&lt;/th&gt;
   &lt;th&gt;Model Limits&lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt; 
  &lt;tr&gt;
   &lt;td&gt;Allam 2 7B&lt;/td&gt;
   &lt;td&gt;7,000 requests/day&lt;br /&gt;6,000 tokens/minute&lt;/td&gt;
  &lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;DeepSeek R1 Distill Llama 70B&lt;/td&gt;
   &lt;td&gt;1,000 requests/day&lt;br /&gt;6,000 tokens/minute&lt;/td&gt;
  &lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;Gemma 2 9B Instruct&lt;/td&gt;
   &lt;td&gt;14,400 requests/day&lt;br /&gt;15,000 tokens/minute&lt;/td&gt;
  &lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;Llama 3.1 8B&lt;/td&gt;
   &lt;td&gt;14,400 requests/day&lt;br /&gt;6,000 tokens/minute&lt;/td&gt;
  &lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;Llama 3.3 70B&lt;/td&gt;
   &lt;td&gt;1,000 requests/day&lt;br /&gt;12,000 tokens/minute&lt;/td&gt;
  &lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;Llama 4 Maverick 17B 128E Instruct&lt;/td&gt;
   &lt;td&gt;1,000 requests/day&lt;br /&gt;6,000 tokens/minute&lt;/td&gt;
  &lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;Llama 4 Scout Instruct&lt;/td&gt;
   &lt;td&gt;1,000 requests/day&lt;br /&gt;30,000 tokens/minute&lt;/td&gt;
  &lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;Whisper Large v3&lt;/td&gt;
   &lt;td&gt;7,200 audio-seconds/minute&lt;br /&gt;2,000 requests/day&lt;/td&gt;
  &lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;Whisper Large v3 Turbo&lt;/td&gt;
   &lt;td&gt;7,200 audio-seconds/minute&lt;br /&gt;2,000 requests/day&lt;/td&gt;
  &lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;groq/compound&lt;/td&gt;
   &lt;td&gt;250 requests/day&lt;br /&gt;70,000 tokens/minute&lt;/td&gt;
  &lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;groq/compound-mini&lt;/td&gt;
   &lt;td&gt;250 requests/day&lt;br /&gt;70,000 tokens/minute&lt;/td&gt;
  &lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;meta-llama/llama-guard-4-12b&lt;/td&gt;
   &lt;td&gt;14,400 requests/day&lt;br /&gt;15,000 tokens/minute&lt;/td&gt;
  &lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;meta-llama/llama-prompt-guard-2-22m&lt;/td&gt;
   &lt;td&gt;&lt;/td&gt;
  &lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;meta-llama/llama-prompt-guard-2-86m&lt;/td&gt;
   &lt;td&gt;&lt;/td&gt;
  &lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;moonshotai/kimi-k2-instruct&lt;/td&gt;
   &lt;td&gt;1,000 requests/day&lt;br /&gt;10,000 tokens/minute&lt;/td&gt;
  &lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;moonshotai/kimi-k2-instruct-0905&lt;/td&gt;
   &lt;td&gt;1,000 requests/day&lt;br /&gt;10,000 tokens/minute&lt;/td&gt;
  &lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;openai/gpt-oss-120b&lt;/td&gt;
   &lt;td&gt;1,000 requests/day&lt;br /&gt;8,000 tokens/minute&lt;/td&gt;
  &lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;openai/gpt-oss-20b&lt;/td&gt;
   &lt;td&gt;1,000 requests/day&lt;br /&gt;8,000 tokens/minute&lt;/td&gt;
  &lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;qwen/qwen3-32b&lt;/td&gt;
   &lt;td&gt;1,000 requests/day&lt;br /&gt;6,000 tokens/minute&lt;/td&gt;
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;&lt;a href="https://together.ai"&gt;Together (Free)&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Limits:&lt;/strong&gt; Up to 60 requests/minute&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://together.ai/models/llama-3-3-70b-free"&gt;Llama 3.3 70B Instruct&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://together.ai/models/deepseek-r1-distilled-llama-70b-free"&gt;DeepSeek R1 Distil Llama 70B&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;a href="https://cohere.com"&gt;Cohere&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Limits:&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://docs.cohere.com/docs/rate-limits"&gt;20 requests/minute&lt;br /&gt;1,000 requests/month&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Models share a common quota.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Command-A&lt;/li&gt; 
 &lt;li&gt;Command-R7B&lt;/li&gt; 
 &lt;li&gt;Command-R+&lt;/li&gt; 
 &lt;li&gt;Command-R&lt;/li&gt; 
 &lt;li&gt;Aya Expanse 8B&lt;/li&gt; 
 &lt;li&gt;Aya Expanse 32B&lt;/li&gt; 
 &lt;li&gt;Aya Vision 8B&lt;/li&gt; 
 &lt;li&gt;Aya Vision 32B&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;a href="https://github.com/marketplace/models"&gt;GitHub Models&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Extremely restrictive input/output token limits.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Limits:&lt;/strong&gt; &lt;a href="https://docs.github.com/en/github-models/prototyping-with-ai-models#rate-limits"&gt;Dependent on Copilot subscription tier (Free/Pro/Pro+/Business/Enterprise)&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;AI21 Jamba 1.5 Large&lt;/li&gt; 
 &lt;li&gt;AI21 Jamba 1.5 Mini&lt;/li&gt; 
 &lt;li&gt;Codestral 25.01&lt;/li&gt; 
 &lt;li&gt;Cohere Command A&lt;/li&gt; 
 &lt;li&gt;Cohere Command R 08-2024&lt;/li&gt; 
 &lt;li&gt;Cohere Command R+ 08-2024&lt;/li&gt; 
 &lt;li&gt;Cohere Embed v3 English&lt;/li&gt; 
 &lt;li&gt;Cohere Embed v3 Multilingual&lt;/li&gt; 
 &lt;li&gt;DeepSeek-R1&lt;/li&gt; 
 &lt;li&gt;DeepSeek-R1-0528&lt;/li&gt; 
 &lt;li&gt;DeepSeek-V3-0324&lt;/li&gt; 
 &lt;li&gt;Grok 3&lt;/li&gt; 
 &lt;li&gt;Grok 3 Mini&lt;/li&gt; 
 &lt;li&gt;JAIS 30b Chat&lt;/li&gt; 
 &lt;li&gt;Llama 4 Maverick 17B 128E Instruct FP8&lt;/li&gt; 
 &lt;li&gt;Llama 4 Scout 17B 16E Instruct&lt;/li&gt; 
 &lt;li&gt;Llama-3.2-11B-Vision-Instruct&lt;/li&gt; 
 &lt;li&gt;Llama-3.2-90B-Vision-Instruct&lt;/li&gt; 
 &lt;li&gt;Llama-3.3-70B-Instruct&lt;/li&gt; 
 &lt;li&gt;MAI-DS-R1&lt;/li&gt; 
 &lt;li&gt;Meta-Llama-3.1-405B-Instruct&lt;/li&gt; 
 &lt;li&gt;Meta-Llama-3.1-8B-Instruct&lt;/li&gt; 
 &lt;li&gt;Ministral 3B&lt;/li&gt; 
 &lt;li&gt;Mistral Large 24.11&lt;/li&gt; 
 &lt;li&gt;Mistral Medium 3 (25.05)&lt;/li&gt; 
 &lt;li&gt;Mistral Nemo&lt;/li&gt; 
 &lt;li&gt;Mistral Small 3.1&lt;/li&gt; 
 &lt;li&gt;OpenAI GPT-4.1&lt;/li&gt; 
 &lt;li&gt;OpenAI GPT-4.1-mini&lt;/li&gt; 
 &lt;li&gt;OpenAI GPT-4.1-nano&lt;/li&gt; 
 &lt;li&gt;OpenAI GPT-4o&lt;/li&gt; 
 &lt;li&gt;OpenAI GPT-4o mini&lt;/li&gt; 
 &lt;li&gt;OpenAI Text Embedding 3 (large)&lt;/li&gt; 
 &lt;li&gt;OpenAI Text Embedding 3 (small)&lt;/li&gt; 
 &lt;li&gt;OpenAI gpt-5&lt;/li&gt; 
 &lt;li&gt;OpenAI gpt-5-chat (preview)&lt;/li&gt; 
 &lt;li&gt;OpenAI gpt-5-mini&lt;/li&gt; 
 &lt;li&gt;OpenAI gpt-5-nano&lt;/li&gt; 
 &lt;li&gt;OpenAI o1&lt;/li&gt; 
 &lt;li&gt;OpenAI o1-mini&lt;/li&gt; 
 &lt;li&gt;OpenAI o1-preview&lt;/li&gt; 
 &lt;li&gt;OpenAI o3&lt;/li&gt; 
 &lt;li&gt;OpenAI o3-mini&lt;/li&gt; 
 &lt;li&gt;OpenAI o4-mini&lt;/li&gt; 
 &lt;li&gt;Phi-4&lt;/li&gt; 
 &lt;li&gt;Phi-4-mini-instruct&lt;/li&gt; 
 &lt;li&gt;Phi-4-mini-reasoning&lt;/li&gt; 
 &lt;li&gt;Phi-4-multimodal-instruct&lt;/li&gt; 
 &lt;li&gt;Phi-4-reasoning&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;a href="https://developers.cloudflare.com/workers-ai"&gt;Cloudflare Workers AI&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Limits:&lt;/strong&gt; &lt;a href="https://developers.cloudflare.com/workers-ai/platform/pricing/#free-allocation"&gt;10,000 neurons/day&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;@cf/openai/gpt-oss-120b&lt;/li&gt; 
 &lt;li&gt;@cf/openai/gpt-oss-20b&lt;/li&gt; 
 &lt;li&gt;DeepSeek R1 Distill Qwen 32B&lt;/li&gt; 
 &lt;li&gt;Deepseek Coder 6.7B Base (AWQ)&lt;/li&gt; 
 &lt;li&gt;Deepseek Coder 6.7B Instruct (AWQ)&lt;/li&gt; 
 &lt;li&gt;Deepseek Math 7B Instruct&lt;/li&gt; 
 &lt;li&gt;Discolm German 7B v1 (AWQ)&lt;/li&gt; 
 &lt;li&gt;Falcom 7B Instruct&lt;/li&gt; 
 &lt;li&gt;Gemma 2B Instruct (LoRA)&lt;/li&gt; 
 &lt;li&gt;Gemma 3 12B Instruct&lt;/li&gt; 
 &lt;li&gt;Gemma 7B Instruct&lt;/li&gt; 
 &lt;li&gt;Gemma 7B Instruct (LoRA)&lt;/li&gt; 
 &lt;li&gt;Hermes 2 Pro Mistral 7B&lt;/li&gt; 
 &lt;li&gt;Llama 2 13B Chat (AWQ)&lt;/li&gt; 
 &lt;li&gt;Llama 2 7B Chat (FP16)&lt;/li&gt; 
 &lt;li&gt;Llama 2 7B Chat (INT8)&lt;/li&gt; 
 &lt;li&gt;Llama 2 7B Chat (LoRA)&lt;/li&gt; 
 &lt;li&gt;Llama 3 8B Instruct&lt;/li&gt; 
 &lt;li&gt;Llama 3 8B Instruct&lt;/li&gt; 
 &lt;li&gt;Llama 3 8B Instruct (AWQ)&lt;/li&gt; 
 &lt;li&gt;Llama 3.1 8B Instruct (AWQ)&lt;/li&gt; 
 &lt;li&gt;Llama 3.1 8B Instruct (FP8)&lt;/li&gt; 
 &lt;li&gt;Llama 3.2 11B Vision Instruct&lt;/li&gt; 
 &lt;li&gt;Llama 3.2 1B Instruct&lt;/li&gt; 
 &lt;li&gt;Llama 3.2 3B Instruct&lt;/li&gt; 
 &lt;li&gt;Llama 3.3 70B Instruct (FP8)&lt;/li&gt; 
 &lt;li&gt;Llama 4 Scout Instruct&lt;/li&gt; 
 &lt;li&gt;Llama Guard 3 8B&lt;/li&gt; 
 &lt;li&gt;LlamaGuard 7B (AWQ)&lt;/li&gt; 
 &lt;li&gt;Mistral 7B Instruct v0.1&lt;/li&gt; 
 &lt;li&gt;Mistral 7B Instruct v0.1 (AWQ)&lt;/li&gt; 
 &lt;li&gt;Mistral 7B Instruct v0.2&lt;/li&gt; 
 &lt;li&gt;Mistral 7B Instruct v0.2 (LoRA)&lt;/li&gt; 
 &lt;li&gt;Mistral Small 3.1 24B Instruct&lt;/li&gt; 
 &lt;li&gt;Neural Chat 7B v3.1 (AWQ)&lt;/li&gt; 
 &lt;li&gt;OpenChat 3.5 0106&lt;/li&gt; 
 &lt;li&gt;OpenHermes 2.5 Mistral 7B (AWQ)&lt;/li&gt; 
 &lt;li&gt;Phi-2&lt;/li&gt; 
 &lt;li&gt;Qwen 1.5 0.5B Chat&lt;/li&gt; 
 &lt;li&gt;Qwen 1.5 1.8B Chat&lt;/li&gt; 
 &lt;li&gt;Qwen 1.5 14B Chat (AWQ)&lt;/li&gt; 
 &lt;li&gt;Qwen 1.5 7B Chat (AWQ)&lt;/li&gt; 
 &lt;li&gt;Qwen 2.5 Coder 32B Instruct&lt;/li&gt; 
 &lt;li&gt;Qwen QwQ 32B&lt;/li&gt; 
 &lt;li&gt;SQLCoder 7B 2&lt;/li&gt; 
 &lt;li&gt;Starling LM 7B Beta&lt;/li&gt; 
 &lt;li&gt;TinyLlama 1.1B Chat v1.0&lt;/li&gt; 
 &lt;li&gt;Una Cybertron 7B v2 (BF16)&lt;/li&gt; 
 &lt;li&gt;Zephyr 7B Beta (AWQ)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;a href="https://console.cloud.google.com/vertex-ai/model-garden"&gt;Google Cloud Vertex AI&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Very stringent payment verification for Google Cloud.&lt;/p&gt; 
&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th&gt;Model Name&lt;/th&gt;
   &lt;th&gt;Model Limits&lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt; 
  &lt;tr&gt;
   &lt;td&gt;&lt;a href="https://console.cloud.google.com/vertex-ai/publishers/meta/model-garden/llama-3-2-90b-vision-instruct-maas" target="_blank"&gt;Llama 3.2 90B Vision Instruct&lt;/a&gt;&lt;/td&gt;
   &lt;td&gt;30 requests/minute&lt;br /&gt;Free during preview&lt;/td&gt;
  &lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;&lt;a href="https://console.cloud.google.com/vertex-ai/publishers/meta/model-garden/llama-3-1-405b-instruct-maas" target="_blank"&gt;Llama 3.1 70B Instruct&lt;/a&gt;&lt;/td&gt;
   &lt;td&gt;60 requests/minute&lt;br /&gt;Free during preview&lt;/td&gt;
  &lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;&lt;a href="https://console.cloud.google.com/vertex-ai/publishers/meta/model-garden/llama-3-1-405b-instruct-maas" target="_blank"&gt;Llama 3.1 8B Instruct&lt;/a&gt;&lt;/td&gt;
   &lt;td&gt;60 requests/minute&lt;br /&gt;Free during preview&lt;/td&gt;
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Providers with trial credits&lt;/h2&gt; 
&lt;h3&gt;&lt;a href="https://together.ai"&gt;Together&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Credits:&lt;/strong&gt; $1 when you add a payment method&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Models:&lt;/strong&gt; &lt;a href="https://together.ai/models"&gt;Various open models&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;&lt;a href="https://fireworks.ai/"&gt;Fireworks&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Credits:&lt;/strong&gt; $1&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Models:&lt;/strong&gt; &lt;a href="https://fireworks.ai/models"&gt;Various open models&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;&lt;a href="https://app.baseten.co/"&gt;Baseten&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Credits:&lt;/strong&gt; $30&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Models:&lt;/strong&gt; &lt;a href="https://www.baseten.co/library/"&gt;Any supported model - pay by compute time&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;&lt;a href="https://studio.nebius.com/"&gt;Nebius&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Credits:&lt;/strong&gt; $1&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Models:&lt;/strong&gt; &lt;a href="https://studio.nebius.ai/models"&gt;Various open models&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;&lt;a href="https://novita.ai/?ref=ytblmjc&amp;amp;utm_source=affiliate"&gt;Novita&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Credits:&lt;/strong&gt; $0.5 for 1 year&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Models:&lt;/strong&gt; &lt;a href="https://novita.ai/models"&gt;Various open models&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;&lt;a href="https://studio.ai21.com/"&gt;AI21&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Credits:&lt;/strong&gt; $10 for 3 months&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Models:&lt;/strong&gt; Jamba family of models&lt;/p&gt; 
&lt;h3&gt;&lt;a href="https://console.upstage.ai/"&gt;Upstage&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Credits:&lt;/strong&gt; $10 for 3 months&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Models:&lt;/strong&gt; Solar Pro/Mini&lt;/p&gt; 
&lt;h3&gt;&lt;a href="https://nlpcloud.com/home"&gt;NLP Cloud&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Credits:&lt;/strong&gt; $15&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Requirements:&lt;/strong&gt; Phone number verification&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Models:&lt;/strong&gt; Various open models&lt;/p&gt; 
&lt;h3&gt;&lt;a href="https://bailian.console.alibabacloud.com/"&gt;Alibaba Cloud (International) Model Studio&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Credits:&lt;/strong&gt; 1 million tokens/model&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Models:&lt;/strong&gt; &lt;a href="https://www.alibabacloud.com/en/product/modelstudio"&gt;Various open and proprietary Qwen models&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;&lt;a href="https://modal.com"&gt;Modal&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Credits:&lt;/strong&gt; $5/month upon sign up, $30/month with payment method added&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Models:&lt;/strong&gt; Any supported model - pay by compute time&lt;/p&gt; 
&lt;h3&gt;&lt;a href="https://inference.net"&gt;Inference.net&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Credits:&lt;/strong&gt; $1, $25 on responding to email survey&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Models:&lt;/strong&gt; Various open models&lt;/p&gt; 
&lt;h3&gt;&lt;a href="https://ncompass.tech"&gt;nCompass&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Credits:&lt;/strong&gt; $1&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Models:&lt;/strong&gt; Various open models&lt;/p&gt; 
&lt;h3&gt;&lt;a href="https://app.hyperbolic.xyz/"&gt;Hyperbolic&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Credits:&lt;/strong&gt; $1&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Models:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;DeepSeek V3&lt;/li&gt; 
 &lt;li&gt;DeepSeek V3 0324&lt;/li&gt; 
 &lt;li&gt;Hermes 3 Llama 3.1 70B&lt;/li&gt; 
 &lt;li&gt;Llama 3 70B Instruct&lt;/li&gt; 
 &lt;li&gt;Llama 3.1 405B Base&lt;/li&gt; 
 &lt;li&gt;Llama 3.1 405B Base (FP8)&lt;/li&gt; 
 &lt;li&gt;Llama 3.1 405B Instruct&lt;/li&gt; 
 &lt;li&gt;Llama 3.1 70B Instruct&lt;/li&gt; 
 &lt;li&gt;Llama 3.1 8B Instruct&lt;/li&gt; 
 &lt;li&gt;Llama 3.2 3B Instruct&lt;/li&gt; 
 &lt;li&gt;Llama 3.3 70B Instruct&lt;/li&gt; 
 &lt;li&gt;Pixtral 12B (2409)&lt;/li&gt; 
 &lt;li&gt;Qwen QwQ 32B&lt;/li&gt; 
 &lt;li&gt;Qwen QwQ 32B Preview&lt;/li&gt; 
 &lt;li&gt;Qwen2.5 72B Instruct&lt;/li&gt; 
 &lt;li&gt;Qwen2.5 Coder 32B Instruct&lt;/li&gt; 
 &lt;li&gt;Qwen2.5 VL 72B Instruct&lt;/li&gt; 
 &lt;li&gt;Qwen2.5 VL 7B Instruct&lt;/li&gt; 
 &lt;li&gt;openai/gpt-oss-120b&lt;/li&gt; 
 &lt;li&gt;openai/gpt-oss-120b-turbo&lt;/li&gt; 
 &lt;li&gt;openai/gpt-oss-20b&lt;/li&gt; 
 &lt;li&gt;qwen/qwen3-235b-a22b-instruct-2507-fp8&lt;/li&gt; 
 &lt;li&gt;qwen/qwen3-coder-480b-a35b-instruct-fp8&lt;/li&gt; 
 &lt;li&gt;qwen/qwen3-next-80b-a3b-instruct&lt;/li&gt; 
 &lt;li&gt;qwen/qwen3-next-80b-a3b-thinking&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;a href="https://cloud.sambanova.ai/"&gt;SambaNova Cloud&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Credits:&lt;/strong&gt; $5 for 3 months&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Models:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;E5-Mistral-7B-Instruct&lt;/li&gt; 
 &lt;li&gt;Llama 3.1 405B&lt;/li&gt; 
 &lt;li&gt;Llama 3.1 8B&lt;/li&gt; 
 &lt;li&gt;Llama 3.2 1B&lt;/li&gt; 
 &lt;li&gt;Llama 3.2 3B&lt;/li&gt; 
 &lt;li&gt;Llama 3.3 70B&lt;/li&gt; 
 &lt;li&gt;Llama-4-Maverick-17B-128E-Instruct&lt;/li&gt; 
 &lt;li&gt;Llama-4-Scout-17B-16E-Instruct&lt;/li&gt; 
 &lt;li&gt;Llama-Guard-3-8B&lt;/li&gt; 
 &lt;li&gt;Qwen/QwQ-32B&lt;/li&gt; 
 &lt;li&gt;Qwen/Qwen2-Audio-7B-Instruct&lt;/li&gt; 
 &lt;li&gt;Qwen/Qwen3-32B&lt;/li&gt; 
 &lt;li&gt;deepseek-ai/DeepSeek-R1&lt;/li&gt; 
 &lt;li&gt;deepseek-ai/DeepSeek-R1-0528&lt;/li&gt; 
 &lt;li&gt;deepseek-ai/DeepSeek-R1-Distill-Llama-70B&lt;/li&gt; 
 &lt;li&gt;deepseek-ai/DeepSeek-V3-0324&lt;/li&gt; 
 &lt;li&gt;deepseek-ai/DeepSeek-V3.1&lt;/li&gt; 
 &lt;li&gt;test-model&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;a href="https://console.scaleway.com/generative-api/models"&gt;Scaleway Generative APIs&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Credits:&lt;/strong&gt; 1,000,000 free tokens&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Models:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;BGE-Multilingual-Gemma2&lt;/li&gt; 
 &lt;li&gt;DeepSeek R1 Distill Llama 70B&lt;/li&gt; 
 &lt;li&gt;Gemma 3 27B Instruct&lt;/li&gt; 
 &lt;li&gt;Llama 3.1 8B Instruct&lt;/li&gt; 
 &lt;li&gt;Llama 3.3 70B Instruct&lt;/li&gt; 
 &lt;li&gt;Mistral Nemo 2407&lt;/li&gt; 
 &lt;li&gt;Mistral Small 3.1 24B Instruct 2503&lt;/li&gt; 
 &lt;li&gt;Pixtral 12B (2409)&lt;/li&gt; 
 &lt;li&gt;Qwen2.5 Coder 32B Instruct&lt;/li&gt; 
 &lt;li&gt;devstral-small-2505&lt;/li&gt; 
 &lt;li&gt;gpt-oss-120b&lt;/li&gt; 
 &lt;li&gt;mistral-small-3.2-24b-instruct-2506&lt;/li&gt; 
 &lt;li&gt;qwen3-235b-a22b-instruct-2507&lt;/li&gt; 
 &lt;li&gt;qwen3-coder-30b-a3b-instruct&lt;/li&gt; 
 &lt;li&gt;voxtral-small-24b-2507&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>GoogleCloudPlatform/agent-starter-pack</title>
      <link>https://github.com/GoogleCloudPlatform/agent-starter-pack</link>
      <description>&lt;p&gt;A collection of production-ready Generative AI Agent templates built for Google Cloud. It accelerates development by providing a holistic, production-ready solution, addressing common challenges (Deployment &amp; Operations, Evaluation, Customization, Observability) in building and deploying GenAI agents.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;🚀 Agent Starter Pack&lt;/h1&gt; 
&lt;p&gt;&lt;img src="https://img.shields.io/pypi/v/agent-starter-pack?color=blue" alt="Version" /&gt; &lt;a href="https://youtu.be/jHt-ZVD660g"&gt;&lt;img src="https://img.shields.io/badge/1--Minute%20Overview-gray" alt="1-Minute Video Overview" /&gt;&lt;/a&gt; &lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/"&gt;&lt;img src="https://img.shields.io/badge/Documentation-gray" alt="Docs" /&gt;&lt;/a&gt; &lt;a href="https://studio.firebase.google.com/new?template=https%3A%2F%2Fgithub.com%2FGoogleCloudPlatform%2Fagent-starter-pack%2Ftree%2Fmain%2Fsrc%2Fresources%2Fidx"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="https://cdn.firebasestudio.dev/btn/try_light_20.svg" /&gt; 
   &lt;source media="(prefers-color-scheme: light)" srcset="https://cdn.firebasestudio.dev/btn/try_dark_20.svg" /&gt; 
   &lt;img height="20" alt="Try in Firebase Studio" src="https://cdn.firebasestudio.dev/btn/try_blue_20.svg?sanitize=true" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; &lt;a href="https://shell.cloud.google.com/cloudshell/editor?cloudshell_git_repo=https%3A%2F%2Fgithub.com%2Feliasecchig%2Fasp-open-in-cloud-shell&amp;amp;cloudshell_print=open-in-cs"&gt;&lt;img src="https://img.shields.io/badge/Launch-in_Cloud_Shell-white" alt="Launch in Cloud Shell" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/github/stars/GoogleCloudPlatform/agent-starter-pack?color=yellow" alt="Stars" /&gt;&lt;/p&gt; 
&lt;p&gt;The &lt;code&gt;agent-starter-pack&lt;/code&gt; is a Python package that provides a collection of production-ready Generative AI Agent templates built for Google Cloud. &lt;br /&gt; It accelerates development by providing a holistic, production-ready solution, addressing common challenges (Deployment &amp;amp; Operations, Evaluation, Customization, Observability) in building and deploying GenAI agents.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;⚡️ Launch&lt;/th&gt; 
   &lt;th&gt;🧪 Experiment&lt;/th&gt; 
   &lt;th&gt;✅ Deploy&lt;/th&gt; 
   &lt;th&gt;🛠️ Customize&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/GoogleCloudPlatform/agent-starter-pack/main/agents/"&gt;Pre-built agent templates&lt;/a&gt; (ReAct, RAG, multi-agent, Live API).&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://cloud.google.com/vertex-ai/generative-ai/docs/models/evaluation-overview"&gt;Vertex AI evaluation&lt;/a&gt; and an interactive playground.&lt;/td&gt; 
   &lt;td&gt;Production-ready infra with &lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/guide/observability"&gt;monitoring, observability&lt;/a&gt;, and &lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/guide/deployment"&gt;CI/CD&lt;/a&gt; on &lt;a href="https://cloud.google.com/run"&gt;Cloud Run&lt;/a&gt; or &lt;a href="https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/overview"&gt;Agent Engine&lt;/a&gt;.&lt;/td&gt; 
   &lt;td&gt;Extend and customize templates according to your needs. 🆕 Now integrating with &lt;a href="https://github.com/google-gemini/gemini-cli"&gt;Gemini CLI&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h2&gt;⚡ Get Started in 1 Minute&lt;/h2&gt; 
&lt;p&gt;Ready to build your AI agent? Simply run this command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Create and activate a Python virtual environment
python -m venv .venv &amp;amp;&amp;amp; source .venv/bin/activate

# Install the agent starter pack
pip install --upgrade agent-starter-pack

# Create a new agent project
agent-starter-pack create my-awesome-agent
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt; ✨ Alternative: Using uv&lt;/summary&gt; 
 &lt;p&gt;If you have &lt;a href="https://github.com/astral-sh/uv"&gt;&lt;code&gt;uv&lt;/code&gt;&lt;/a&gt; installed, you can create and set up your project with a single command:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;uvx agent-starter-pack create my-awesome-agent
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;This command handles creating the project without needing to pre-install the package into a virtual environment.&lt;/p&gt; 
&lt;/details&gt; 
&lt;p&gt;&lt;strong&gt;That's it!&lt;/strong&gt; You now have a fully functional agent project—complete with backend, frontend, and deployment infrastructure—ready for you to explore and customize.&lt;/p&gt; 
&lt;h3&gt;🔧 Enhance Existing Agents&lt;/h3&gt; 
&lt;p&gt;Already have an agent? Add production-ready deployment and infrastructure by running this command in your project's root folder:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uvx agent-starter-pack enhance
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/guide/installation"&gt;Installation Guide&lt;/a&gt; for more options, or try with zero setup in &lt;a href="https://studio.firebase.google.com/new?template=https%3A%2F%2Fgithub.com%2FGoogleCloudPlatform%2Fagent-starter-pack%2Ftree%2Fmain%2Fsrc%2Fresources%2Fidx"&gt;Firebase Studio&lt;/a&gt; or &lt;a href="https://shell.cloud.google.com/cloudshell/editor?cloudshell_git_repo=https%3A%2F%2Fgithub.com%2Feliasecchig%2Fasp-open-in-cloud-shell&amp;amp;cloudshell_print=open-in-cs"&gt;Cloud Shell&lt;/a&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🤖 Agents&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Agent Name&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;adk_base&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;A base ReAct agent implemented using Google's &lt;a href="https://github.com/google/adk-python"&gt;Agent Development Kit&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;agentic_rag&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;A RAG agent for document retrieval and Q&amp;amp;A. Supporting &lt;a href="https://cloud.google.com/generative-ai-app-builder/docs/enterprise-search-introduction"&gt;Vertex AI Search&lt;/a&gt; and &lt;a href="https://cloud.google.com/vertex-ai/docs/vector-search/overview"&gt;Vector Search&lt;/a&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;langgraph_base_react&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;An agent implementing a base ReAct agent using LangGraph&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;crewai_coding_crew&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;A multi-agent system implemented with CrewAI created to support coding activities&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;live_api&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;A real-time multimodal RAG agent powered by Gemini, supporting audio/video/text chat with vector DB-backed responses&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;More agents are on the way!&lt;/strong&gt; We are continuously expanding our &lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/agents/overview"&gt;agent library&lt;/a&gt;. Have a specific agent type in mind? &lt;a href="https://github.com/GoogleCloudPlatform/agent-starter-pack/issues/new?labels=enhancement"&gt;Raise an issue as a feature request!&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;🔍 ADK Samples&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Looking to explore more ADK examples? Check out the &lt;a href="https://github.com/google/adk-samples"&gt;ADK Samples Repository&lt;/a&gt; for additional examples and use cases demonstrating ADK's capabilities.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🌟 Community Showcase&lt;/h2&gt; 
&lt;p&gt;Explore amazing projects built with the Agent Starter Pack!&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/guide/community-showcase"&gt;View Community Showcase →&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h4&gt;Extra Features&lt;/h4&gt; 
&lt;p&gt;The &lt;code&gt;agent-starter-pack&lt;/code&gt; offers two key features to accelerate and simplify the development of your agent:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;🔄 &lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/cli/setup_cicd"&gt;CI/CD Automation&lt;/a&gt;&lt;/strong&gt; - A single command to set up a complete CI/CD pipeline for all environments, supporting both &lt;strong&gt;Google Cloud Build&lt;/strong&gt; and &lt;strong&gt;GitHub Actions&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;📥 &lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/guide/data-ingestion"&gt;Data Pipeline for RAG with Terraform/CI-CD&lt;/a&gt;&lt;/strong&gt; - Seamlessly integrate a data pipeline to process embeddings for RAG into your agent system. Supporting &lt;a href="https://cloud.google.com/generative-ai-app-builder/docs/enterprise-search-introduction"&gt;Vertex AI Search&lt;/a&gt; and &lt;a href="https://cloud.google.com/vertex-ai/docs/vector-search/overview"&gt;Vector Search&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/GoogleCloudPlatform/agent-starter-pack/main/docs/guide/remote-templating.md"&gt;Remote Templates&lt;/a&gt;&lt;/strong&gt;: Create and share your own agent starter packs templates from any Git repository.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🤖 Gemini CLI Integration&lt;/strong&gt; - Use the &lt;a href="https://github.com/google-gemini/gemini-cli"&gt;Gemini CLI&lt;/a&gt; and the included &lt;code&gt;GEMINI.md&lt;/code&gt; context file to ask questions about your template, agent architecture, and the path to production. Get instant guidance and code examples directly in your terminal.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;High-Level Architecture&lt;/h2&gt; 
&lt;p&gt;This starter pack covers all aspects of Agent development, from prototyping and evaluation to deployment and monitoring.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/GoogleCloudPlatform/agent-starter-pack/main/docs/images/ags_high_level_architecture.png" alt="High Level Architecture" title="Architecture" /&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🔧 Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.10+&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://cloud.google.com/sdk/docs/install"&gt;Google Cloud SDK&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://developer.hashicorp.com/terraform/downloads"&gt;Terraform&lt;/a&gt; (for deployment)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.gnu.org/software/make/"&gt;Make&lt;/a&gt; (for development tasks)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;📚 Documentation&lt;/h2&gt; 
&lt;p&gt;Visit our &lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/"&gt;documentation site&lt;/a&gt; for comprehensive guides and references!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/guide/getting-started"&gt;Getting Started Guide&lt;/a&gt; - First steps with agent-starter-pack&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/guide/installation"&gt;Installation Guide&lt;/a&gt; - Setting up your environment&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/guide/deployment"&gt;Deployment Guide&lt;/a&gt; - Taking your agent to production&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/agents/overview"&gt;Agent Templates Overview&lt;/a&gt; - Explore available agent patterns&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/cli/"&gt;CLI Reference&lt;/a&gt; - Command-line tool documentation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Video Walkthrough:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.youtube.com/watch?v=9zqwym-N3lg"&gt;Exploring the Agent Starter Pack&lt;/a&gt;&lt;/strong&gt;: A comprehensive tutorial demonstrating how to rapidly deploy AI Agents using the Agent Starter Pack, covering architecture, templates, and step-by-step deployment.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.youtube.com/live/eZ-8UQ_t4YM?feature=shared&amp;amp;t=2791"&gt;6-minute introduction&lt;/a&gt;&lt;/strong&gt; (April 2024): Explaining the Agent Starter Pack and demonstrating its key features. Part of the Kaggle GenAI intensive course.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.youtube.com/watch?v=yIRIT_EtALs&amp;amp;t=235s"&gt;120-minute livestream demo&lt;/a&gt;&lt;/strong&gt; (March 6, 2025): Watch us build 3 Agents in under 30 minutes using the &lt;code&gt;agent-starter-pack&lt;/code&gt;!&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Looking for more examples and resources for Generative AI on Google Cloud? Check out the &lt;a href="https://github.com/GoogleCloudPlatform/generative-ai"&gt;GoogleCloudPlatform/generative-ai&lt;/a&gt; repository for notebooks, code samples, and more!&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions are welcome! See the &lt;a href="https://raw.githubusercontent.com/GoogleCloudPlatform/agent-starter-pack/main/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Feedback&lt;/h2&gt; 
&lt;p&gt;We value your input! Your feedback helps us improve this starter pack and make it more useful for the community.&lt;/p&gt; 
&lt;h3&gt;Getting Help&lt;/h3&gt; 
&lt;p&gt;If you encounter any issues or have specific suggestions, please first consider &lt;a href="https://github.com/GoogleCloudPlatform/generative-ai/issues"&gt;raising an issue&lt;/a&gt; on our GitHub repository.&lt;/p&gt; 
&lt;h3&gt;Share Your Experience&lt;/h3&gt; 
&lt;p&gt;For other types of feedback, or if you'd like to share a positive experience or success story using this starter pack, we'd love to hear from you! You can reach out to us at &lt;a href="mailto:agent-starter-pack@google.com"&gt;&lt;/a&gt;&lt;a href="mailto:agent-starter-pack@google.com"&gt;agent-starter-pack@google.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Thank you for your contributions!&lt;/p&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;This repository is for demonstrative purposes only and is not an officially supported Google product.&lt;/p&gt; 
&lt;h2&gt;Terms of Service&lt;/h2&gt; 
&lt;p&gt;The agent-starter-pack templating CLI and the templates in this starter pack leverage Google Cloud APIs. When you use this starter pack, you'll be deploying resources in your own Google Cloud project and will be responsible for those resources. Please review the &lt;a href="https://cloud.google.com/terms/service-terms"&gt;Google Cloud Service Terms&lt;/a&gt; for details on the terms of service associated with these APIs.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>virattt/ai-hedge-fund</title>
      <link>https://github.com/virattt/ai-hedge-fund</link>
      <description>&lt;p&gt;An AI Hedge Fund Team&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AI Hedge Fund&lt;/h1&gt; 
&lt;p&gt;This is a proof of concept for an AI-powered hedge fund. The goal of this project is to explore the use of AI to make trading decisions. This project is for &lt;strong&gt;educational&lt;/strong&gt; purposes only and is not intended for real trading or investment.&lt;/p&gt; 
&lt;p&gt;This system employs several agents working together:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Aswath Damodaran Agent - The Dean of Valuation, focuses on story, numbers, and disciplined valuation&lt;/li&gt; 
 &lt;li&gt;Ben Graham Agent - The godfather of value investing, only buys hidden gems with a margin of safety&lt;/li&gt; 
 &lt;li&gt;Bill Ackman Agent - An activist investor, takes bold positions and pushes for change&lt;/li&gt; 
 &lt;li&gt;Cathie Wood Agent - The queen of growth investing, believes in the power of innovation and disruption&lt;/li&gt; 
 &lt;li&gt;Charlie Munger Agent - Warren Buffett's partner, only buys wonderful businesses at fair prices&lt;/li&gt; 
 &lt;li&gt;Michael Burry Agent - The Big Short contrarian who hunts for deep value&lt;/li&gt; 
 &lt;li&gt;Mohnish Pabrai Agent - The Dhandho investor, who looks for doubles at low risk&lt;/li&gt; 
 &lt;li&gt;Peter Lynch Agent - Practical investor who seeks "ten-baggers" in everyday businesses&lt;/li&gt; 
 &lt;li&gt;Phil Fisher Agent - Meticulous growth investor who uses deep "scuttlebutt" research&lt;/li&gt; 
 &lt;li&gt;Rakesh Jhunjhunwala Agent - The Big Bull of India&lt;/li&gt; 
 &lt;li&gt;Stanley Druckenmiller Agent - Macro legend who hunts for asymmetric opportunities with growth potential&lt;/li&gt; 
 &lt;li&gt;Warren Buffett Agent - The oracle of Omaha, seeks wonderful companies at a fair price&lt;/li&gt; 
 &lt;li&gt;Valuation Agent - Calculates the intrinsic value of a stock and generates trading signals&lt;/li&gt; 
 &lt;li&gt;Sentiment Agent - Analyzes market sentiment and generates trading signals&lt;/li&gt; 
 &lt;li&gt;Fundamentals Agent - Analyzes fundamental data and generates trading signals&lt;/li&gt; 
 &lt;li&gt;Technicals Agent - Analyzes technical indicators and generates trading signals&lt;/li&gt; 
 &lt;li&gt;Risk Manager - Calculates risk metrics and sets position limits&lt;/li&gt; 
 &lt;li&gt;Portfolio Manager - Makes final trading decisions and generates orders&lt;/li&gt; 
&lt;/ol&gt; 
&lt;img width="1042" alt="Screenshot 2025-03-22 at 6 19 07 PM" src="https://github.com/user-attachments/assets/cbae3dcf-b571-490d-b0ad-3f0f035ac0d4" /&gt; 
&lt;p&gt;Note: the system does not actually make any trades.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://twitter.com/virattt"&gt;&lt;img src="https://img.shields.io/twitter/follow/virattt?style=social" alt="Twitter Follow" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;This project is for &lt;strong&gt;educational and research purposes only&lt;/strong&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Not intended for real trading or investment&lt;/li&gt; 
 &lt;li&gt;No investment advice or guarantees provided&lt;/li&gt; 
 &lt;li&gt;Creator assumes no liability for financial losses&lt;/li&gt; 
 &lt;li&gt;Consult a financial advisor for investment decisions&lt;/li&gt; 
 &lt;li&gt;Past performance does not indicate future results&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;By using this software, you agree to use it solely for learning purposes.&lt;/p&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#how-to-install"&gt;How to Install&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#how-to-run"&gt;How to Run&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#%EF%B8%8F-command-line-interface"&gt;⌨️ Command Line Interface&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#%EF%B8%8F-web-application"&gt;🖥️ Web Application&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#how-to-contribute"&gt;How to Contribute&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#feature-requests"&gt;Feature Requests&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;How to Install&lt;/h2&gt; 
&lt;p&gt;Before you can run the AI Hedge Fund, you'll need to install it and set up your API keys. These steps are common to both the full-stack web application and command line interface.&lt;/p&gt; 
&lt;h3&gt;1. Clone the Repository&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/virattt/ai-hedge-fund.git
cd ai-hedge-fund
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Set up API keys&lt;/h3&gt; 
&lt;p&gt;Create a &lt;code&gt;.env&lt;/code&gt; file for your API keys:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Create .env file for your API keys (in the root directory)
cp .env.example .env
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Open and edit the &lt;code&gt;.env&lt;/code&gt; file to add your API keys:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# For running LLMs hosted by openai (gpt-4o, gpt-4o-mini, etc.)
OPENAI_API_KEY=your-openai-api-key

# For getting financial data to power the hedge fund
FINANCIAL_DATASETS_API_KEY=your-financial-datasets-api-key
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: You must set at least one LLM API key (e.g. &lt;code&gt;OPENAI_API_KEY&lt;/code&gt;, &lt;code&gt;GROQ_API_KEY&lt;/code&gt;, &lt;code&gt;ANTHROPIC_API_KEY&lt;/code&gt;, or &lt;code&gt;DEEPSEEK_API_KEY&lt;/code&gt;) for the hedge fund to work.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Financial Data&lt;/strong&gt;: Data for AAPL, GOOGL, MSFT, NVDA, and TSLA is free and does not require an API key. For any other ticker, you will need to set the &lt;code&gt;FINANCIAL_DATASETS_API_KEY&lt;/code&gt; in the .env file.&lt;/p&gt; 
&lt;h2&gt;How to Run&lt;/h2&gt; 
&lt;h3&gt;⌨️ Command Line Interface&lt;/h3&gt; 
&lt;p&gt;You can run the AI Hedge Fund directly via terminal. This approach offers more granular control and is useful for automation, scripting, and integration purposes.&lt;/p&gt; 
&lt;img width="992" alt="Screenshot 2025-01-06 at 5 50 17 PM" src="https://github.com/user-attachments/assets/e8ca04bf-9989-4a7d-a8b4-34e04666663b" /&gt; 
&lt;h4&gt;Quick Start&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install Poetry (if not already installed):&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -sSL https://install.python-poetry.org | python3 -
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Install dependencies:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Run the AI Hedge Fund&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry run python src/main.py --ticker AAPL,MSFT,NVDA
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also specify a &lt;code&gt;--ollama&lt;/code&gt; flag to run the AI hedge fund using local LLMs.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry run python src/main.py --ticker AAPL,MSFT,NVDA --ollama
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can optionally specify the start and end dates to make decisions over a specific time period.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry run python src/main.py --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Run the Backtester&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Example Output:&lt;/strong&gt; &lt;img width="941" alt="Screenshot 2025-01-06 at 5 47 52 PM" src="https://github.com/user-attachments/assets/00e794ea-8628-44e6-9a84-8f8a31ad3b47" /&gt;&lt;/p&gt; 
&lt;p&gt;Note: The &lt;code&gt;--ollama&lt;/code&gt;, &lt;code&gt;--start-date&lt;/code&gt;, and &lt;code&gt;--end-date&lt;/code&gt; flags work for the backtester, as well!&lt;/p&gt; 
&lt;h3&gt;🖥️ Web Application&lt;/h3&gt; 
&lt;p&gt;The new way to run the AI Hedge Fund is through our web application that provides a user-friendly interface. This is recommended for users who prefer visual interfaces over command line tools.&lt;/p&gt; 
&lt;p&gt;Please see detailed instructions on how to install and run the web application &lt;a href="https://github.com/virattt/ai-hedge-fund/tree/main/app"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;img width="1721" alt="Screenshot 2025-06-28 at 6 41 03 PM" src="https://github.com/user-attachments/assets/b95ab696-c9f4-416c-9ad1-51feb1f5374b" /&gt; 
&lt;h2&gt;How to Contribute&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Fork the repository&lt;/li&gt; 
 &lt;li&gt;Create a feature branch&lt;/li&gt; 
 &lt;li&gt;Commit your changes&lt;/li&gt; 
 &lt;li&gt;Push to the branch&lt;/li&gt; 
 &lt;li&gt;Create a Pull Request&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: Please keep your pull requests small and focused. This will make it easier to review and merge.&lt;/p&gt; 
&lt;h2&gt;Feature Requests&lt;/h2&gt; 
&lt;p&gt;If you have a feature request, please open an &lt;a href="https://github.com/virattt/ai-hedge-fund/issues"&gt;issue&lt;/a&gt; and make sure it is tagged with &lt;code&gt;enhancement&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the MIT License - see the LICENSE file for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>openai/openai-agents-python</title>
      <link>https://github.com/openai/openai-agents-python</link>
      <description>&lt;p&gt;A lightweight, powerful framework for multi-agent workflows&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OpenAI Agents SDK&lt;/h1&gt; 
&lt;p&gt;The OpenAI Agents SDK is a lightweight yet powerful framework for building multi-agent workflows. It is provider-agnostic, supporting the OpenAI Responses and Chat Completions APIs, as well as 100+ other LLMs.&lt;/p&gt; 
&lt;img src="https://cdn.openai.com/API/docs/images/orchestration.png" alt="Image of the Agents Tracing UI" style="max-height: 803px;" /&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Looking for the JavaScript/TypeScript version? Check out &lt;a href="https://github.com/openai/openai-agents-js"&gt;Agents SDK JS/TS&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Core concepts:&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://openai.github.io/openai-agents-python/agents"&gt;&lt;strong&gt;Agents&lt;/strong&gt;&lt;/a&gt;: LLMs configured with instructions, tools, guardrails, and handoffs&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openai.github.io/openai-agents-python/handoffs/"&gt;&lt;strong&gt;Handoffs&lt;/strong&gt;&lt;/a&gt;: A specialized tool call used by the Agents SDK for transferring control between agents&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openai.github.io/openai-agents-python/guardrails/"&gt;&lt;strong&gt;Guardrails&lt;/strong&gt;&lt;/a&gt;: Configurable safety checks for input and output validation&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/openai-agents-python/main/#sessions"&gt;&lt;strong&gt;Sessions&lt;/strong&gt;&lt;/a&gt;: Automatic conversation history management across agent runs&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openai.github.io/openai-agents-python/tracing/"&gt;&lt;strong&gt;Tracing&lt;/strong&gt;&lt;/a&gt;: Built-in tracking of agent runs, allowing you to view, debug and optimize your workflows&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Explore the &lt;a href="https://raw.githubusercontent.com/openai/openai-agents-python/main/examples"&gt;examples&lt;/a&gt; directory to see the SDK in action, and read our &lt;a href="https://openai.github.io/openai-agents-python/"&gt;documentation&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Get started&lt;/h2&gt; 
&lt;p&gt;To get started, set up your Python environment (Python 3.9 or newer required), and then install OpenAI Agents SDK package.&lt;/p&gt; 
&lt;h3&gt;venv&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
pip install openai-agents
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For voice support, install with the optional &lt;code&gt;voice&lt;/code&gt; group: &lt;code&gt;pip install 'openai-agents[voice]'&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;uv&lt;/h3&gt; 
&lt;p&gt;If you're familiar with &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt;, using the tool would be even similar:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv init
uv add openai-agents
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For voice support, install with the optional &lt;code&gt;voice&lt;/code&gt; group: &lt;code&gt;uv add 'openai-agents[voice]'&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Hello world example&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from agents import Agent, Runner

agent = Agent(name="Assistant", instructions="You are a helpful assistant")

result = Runner.run_sync(agent, "Write a haiku about recursion in programming.")
print(result.final_output)

# Code within the code,
# Functions calling themselves,
# Infinite loop's dance.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;(&lt;em&gt;If running this, ensure you set the &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; environment variable&lt;/em&gt;)&lt;/p&gt; 
&lt;p&gt;(&lt;em&gt;For Jupyter notebook users, see &lt;a href="https://raw.githubusercontent.com/openai/openai-agents-python/main/examples/basic/hello_world_jupyter.ipynb"&gt;hello_world_jupyter.ipynb&lt;/a&gt;&lt;/em&gt;)&lt;/p&gt; 
&lt;h2&gt;Handoffs example&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from agents import Agent, Runner
import asyncio

spanish_agent = Agent(
    name="Spanish agent",
    instructions="You only speak Spanish.",
)

english_agent = Agent(
    name="English agent",
    instructions="You only speak English",
)

triage_agent = Agent(
    name="Triage agent",
    instructions="Handoff to the appropriate agent based on the language of the request.",
    handoffs=[spanish_agent, english_agent],
)


async def main():
    result = await Runner.run(triage_agent, input="Hola, ¿cómo estás?")
    print(result.final_output)
    # ¡Hola! Estoy bien, gracias por preguntar. ¿Y tú, cómo estás?


if __name__ == "__main__":
    asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Functions example&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import asyncio

from agents import Agent, Runner, function_tool


@function_tool
def get_weather(city: str) -&amp;gt; str:
    return f"The weather in {city} is sunny."


agent = Agent(
    name="Hello world",
    instructions="You are a helpful agent.",
    tools=[get_weather],
)


async def main():
    result = await Runner.run(agent, input="What's the weather in Tokyo?")
    print(result.final_output)
    # The weather in Tokyo is sunny.


if __name__ == "__main__":
    asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;The agent loop&lt;/h2&gt; 
&lt;p&gt;When you call &lt;code&gt;Runner.run()&lt;/code&gt;, we run a loop until we get a final output.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;We call the LLM, using the model and settings on the agent, and the message history.&lt;/li&gt; 
 &lt;li&gt;The LLM returns a response, which may include tool calls.&lt;/li&gt; 
 &lt;li&gt;If the response has a final output (see below for more on this), we return it and end the loop.&lt;/li&gt; 
 &lt;li&gt;If the response has a handoff, we set the agent to the new agent and go back to step 1.&lt;/li&gt; 
 &lt;li&gt;We process the tool calls (if any) and append the tool responses messages. Then we go to step 1.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;There is a &lt;code&gt;max_turns&lt;/code&gt; parameter that you can use to limit the number of times the loop executes.&lt;/p&gt; 
&lt;h3&gt;Final output&lt;/h3&gt; 
&lt;p&gt;Final output is the last thing the agent produces in the loop.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;If you set an &lt;code&gt;output_type&lt;/code&gt; on the agent, the final output is when the LLM returns something of that type. We use &lt;a href="https://platform.openai.com/docs/guides/structured-outputs"&gt;structured outputs&lt;/a&gt; for this.&lt;/li&gt; 
 &lt;li&gt;If there's no &lt;code&gt;output_type&lt;/code&gt; (i.e. plain text responses), then the first LLM response without any tool calls or handoffs is considered as the final output.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;As a result, the mental model for the agent loop is:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;If the current agent has an &lt;code&gt;output_type&lt;/code&gt;, the loop runs until the agent produces structured output matching that type.&lt;/li&gt; 
 &lt;li&gt;If the current agent does not have an &lt;code&gt;output_type&lt;/code&gt;, the loop runs until the current agent produces a message without any tool calls/handoffs.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Common agent patterns&lt;/h2&gt; 
&lt;p&gt;The Agents SDK is designed to be highly flexible, allowing you to model a wide range of LLM workflows including deterministic flows, iterative loops, and more. See examples in &lt;a href="https://raw.githubusercontent.com/openai/openai-agents-python/main/examples/agent_patterns"&gt;&lt;code&gt;examples/agent_patterns&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Tracing&lt;/h2&gt; 
&lt;p&gt;The Agents SDK automatically traces your agent runs, making it easy to track and debug the behavior of your agents. Tracing is extensible by design, supporting custom spans and a wide variety of external destinations, including &lt;a href="https://logfire.pydantic.dev/docs/integrations/llms/openai/#openai-agents"&gt;Logfire&lt;/a&gt;, &lt;a href="https://docs.agentops.ai/v1/integrations/agentssdk"&gt;AgentOps&lt;/a&gt;, &lt;a href="https://braintrust.dev/docs/guides/traces/integrations#openai-agents-sdk"&gt;Braintrust&lt;/a&gt;, &lt;a href="https://docs.scorecard.io/docs/documentation/features/tracing#openai-agents-sdk-integration"&gt;Scorecard&lt;/a&gt;, and &lt;a href="https://docs.keywordsai.co/integration/development-frameworks/openai-agent"&gt;Keywords AI&lt;/a&gt;. For more details about how to customize or disable tracing, see &lt;a href="http://openai.github.io/openai-agents-python/tracing"&gt;Tracing&lt;/a&gt;, which also includes a larger list of &lt;a href="http://openai.github.io/openai-agents-python/tracing/#external-tracing-processors-list"&gt;external tracing processors&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Long running agents &amp;amp; human-in-the-loop&lt;/h2&gt; 
&lt;p&gt;You can use the Agents SDK &lt;a href="https://temporal.io/"&gt;Temporal&lt;/a&gt; integration to run durable, long-running workflows, including human-in-the-loop tasks. View a demo of Temporal and the Agents SDK working in action to complete long-running tasks &lt;a href="https://www.youtube.com/watch?v=fFBZqzT4DD8"&gt;in this video&lt;/a&gt;, and &lt;a href="https://github.com/temporalio/sdk-python/tree/main/temporalio/contrib/openai_agents"&gt;view docs here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Sessions&lt;/h2&gt; 
&lt;p&gt;The Agents SDK provides built-in session memory to automatically maintain conversation history across multiple agent runs, eliminating the need to manually handle &lt;code&gt;.to_input_list()&lt;/code&gt; between turns.&lt;/p&gt; 
&lt;h3&gt;Quick start&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from agents import Agent, Runner, SQLiteSession

# Create agent
agent = Agent(
    name="Assistant",
    instructions="Reply very concisely.",
)

# Create a session instance
session = SQLiteSession("conversation_123")

# First turn
result = await Runner.run(
    agent,
    "What city is the Golden Gate Bridge in?",
    session=session
)
print(result.final_output)  # "San Francisco"

# Second turn - agent automatically remembers previous context
result = await Runner.run(
    agent,
    "What state is it in?",
    session=session
)
print(result.final_output)  # "California"

# Also works with synchronous runner
result = Runner.run_sync(
    agent,
    "What's the population?",
    session=session
)
print(result.final_output)  # "Approximately 39 million"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Session options&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;No memory&lt;/strong&gt; (default): No session memory when session parameter is omitted&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;session: Session = DatabaseSession(...)&lt;/code&gt;&lt;/strong&gt;: Use a Session instance to manage conversation history&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from agents import Agent, Runner, SQLiteSession

# Custom SQLite database file
session = SQLiteSession("user_123", "conversations.db")
agent = Agent(name="Assistant")

# Different session IDs maintain separate conversation histories
result1 = await Runner.run(
    agent,
    "Hello",
    session=session
)
result2 = await Runner.run(
    agent,
    "Hello",
    session=SQLiteSession("user_456", "conversations.db")
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Custom session implementations&lt;/h3&gt; 
&lt;p&gt;You can implement your own session memory by creating a class that follows the &lt;code&gt;Session&lt;/code&gt; protocol:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from agents.memory import Session
from typing import List

class MyCustomSession:
    """Custom session implementation following the Session protocol."""

    def __init__(self, session_id: str):
        self.session_id = session_id
        # Your initialization here

    async def get_items(self, limit: int | None = None) -&amp;gt; List[dict]:
        # Retrieve conversation history for the session
        pass

    async def add_items(self, items: List[dict]) -&amp;gt; None:
        # Store new items for the session
        pass

    async def pop_item(self) -&amp;gt; dict | None:
        # Remove and return the most recent item from the session
        pass

    async def clear_session(self) -&amp;gt; None:
        # Clear all items for the session
        pass

# Use your custom session
agent = Agent(name="Assistant")
result = await Runner.run(
    agent,
    "Hello",
    session=MyCustomSession("my_session")
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Development (only needed if you need to edit the SDK/examples)&lt;/h2&gt; 
&lt;ol start="0"&gt; 
 &lt;li&gt;Ensure you have &lt;a href="https://docs.astral.sh/uv/"&gt;&lt;code&gt;uv&lt;/code&gt;&lt;/a&gt; installed.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv --version
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install dependencies&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;make sync
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;(After making changes) lint/test&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code&gt;make check # run tests linter and typechecker
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or to run them individually:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;make tests  # run tests
make mypy   # run typechecker
make lint   # run linter
make format-check # run style checker
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;p&gt;We'd like to acknowledge the excellent work of the open-source community, especially:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.pydantic.dev/latest/"&gt;Pydantic&lt;/a&gt; (data validation) and &lt;a href="https://ai.pydantic.dev/"&gt;PydanticAI&lt;/a&gt; (advanced agent framework)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/BerriAI/litellm"&gt;LiteLLM&lt;/a&gt; (unified interface for 100+ LLMs)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/squidfunk/mkdocs-material"&gt;MkDocs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mkdocstrings/griffe"&gt;Griffe&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/astral-sh/uv"&gt;uv&lt;/a&gt; and &lt;a href="https://github.com/astral-sh/ruff"&gt;ruff&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We're committed to continuing to build the Agents SDK as an open source framework so others in the community can expand on our approach.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>mem0ai/mem0</title>
      <link>https://github.com/mem0ai/mem0</link>
      <description>&lt;p&gt;Universal memory layer for AI Agents; Announcing OpenMemory MCP - local and secure memory management.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://github.com/mem0ai/mem0"&gt; &lt;img src="https://raw.githubusercontent.com/mem0ai/mem0/main/docs/images/banner-sm.png" width="800px" alt="Mem0 - The Memory Layer for Personalized AI" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center" style="display: flex; justify-content: center; gap: 20px; align-items: center;"&gt; &lt;a href="https://trendshift.io/repositories/11194" target="blank"&gt; &lt;img src="https://trendshift.io/api/badge/repositories/11194" alt="mem0ai%2Fmem0 | Trendshift" width="250" height="55" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://mem0.ai"&gt;Learn more&lt;/a&gt; · &lt;a href="https://mem0.dev/DiG"&gt;Join Discord&lt;/a&gt; · &lt;a href="https://mem0.dev/demo"&gt;Demo&lt;/a&gt; · &lt;a href="https://mem0.dev/openmemory"&gt;OpenMemory&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://mem0.dev/DiG"&gt; &lt;img src="https://img.shields.io/badge/Discord-%235865F2.svg?&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Mem0 Discord" /&gt; &lt;/a&gt; &lt;a href="https://pepy.tech/project/mem0ai"&gt; &lt;img src="https://img.shields.io/pypi/dm/mem0ai" alt="Mem0 PyPI - Downloads" /&gt; &lt;/a&gt; &lt;a href="https://github.com/mem0ai/mem0"&gt; &lt;img src="https://img.shields.io/github/commit-activity/m/mem0ai/mem0?style=flat-square" alt="GitHub commit activity" /&gt; &lt;/a&gt; &lt;a href="https://pypi.org/project/mem0ai" target="blank"&gt; &lt;img src="https://img.shields.io/pypi/v/mem0ai?color=%2334D058&amp;amp;label=pypi%20package" alt="Package version" /&gt; &lt;/a&gt; &lt;a href="https://www.npmjs.com/package/mem0ai" target="blank"&gt; &lt;img src="https://img.shields.io/npm/v/mem0ai" alt="Npm package" /&gt; &lt;/a&gt; &lt;a href="https://www.ycombinator.com/companies/mem0"&gt; &lt;img src="https://img.shields.io/badge/Y%20Combinator-S24-orange?style=flat-square" alt="Y Combinator S24" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://mem0.ai/research"&gt;&lt;strong&gt;📄 Building Production-Ready AI Agents with Scalable Long-Term Memory →&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;strong&gt;⚡ +26% Accuracy vs. OpenAI Memory • 🚀 91% Faster • 💰 90% Fewer Tokens&lt;/strong&gt; &lt;/p&gt; 
&lt;h2&gt;🔥 Research Highlights&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;+26% Accuracy&lt;/strong&gt; over OpenAI Memory on the LOCOMO benchmark&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;91% Faster Responses&lt;/strong&gt; than full-context, ensuring low-latency at scale&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;90% Lower Token Usage&lt;/strong&gt; than full-context, cutting costs without compromise&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mem0.ai/research"&gt;Read the full paper&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Introduction&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://mem0.ai"&gt;Mem0&lt;/a&gt; ("mem-zero") enhances AI assistants and agents with an intelligent memory layer, enabling personalized AI interactions. It remembers user preferences, adapts to individual needs, and continuously learns over time—ideal for customer support chatbots, AI assistants, and autonomous systems.&lt;/p&gt; 
&lt;h3&gt;Key Features &amp;amp; Use Cases&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Core Capabilities:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-Level Memory&lt;/strong&gt;: Seamlessly retains User, Session, and Agent state with adaptive personalization&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Developer-Friendly&lt;/strong&gt;: Intuitive API, cross-platform SDKs, and a fully managed service option&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Applications:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;AI Assistants&lt;/strong&gt;: Consistent, context-rich conversations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Customer Support&lt;/strong&gt;: Recall past tickets and user history for tailored help&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Healthcare&lt;/strong&gt;: Track patient preferences and history for personalized care&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Productivity &amp;amp; Gaming&lt;/strong&gt;: Adaptive workflows and environments based on user behavior&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🚀 Quickstart Guide &lt;a name="quickstart"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;Choose between our hosted platform or self-hosted package:&lt;/p&gt; 
&lt;h3&gt;Hosted Platform&lt;/h3&gt; 
&lt;p&gt;Get up and running in minutes with automatic updates, analytics, and enterprise security.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Sign up on &lt;a href="https://app.mem0.ai"&gt;Mem0 Platform&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Embed the memory layer via SDK or API keys&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Self-Hosted (Open Source)&lt;/h3&gt; 
&lt;p&gt;Install the sdk via pip:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install mem0ai
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Install sdk via npm:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npm install mem0ai
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Basic Usage&lt;/h3&gt; 
&lt;p&gt;Mem0 requires an LLM to function, with &lt;code&gt;gpt-4o-mini&lt;/code&gt; from OpenAI as the default. However, it supports a variety of LLMs; for details, refer to our &lt;a href="https://docs.mem0.ai/components/llms/overview"&gt;Supported LLMs documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;First step is to instantiate the memory:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openai import OpenAI
from mem0 import Memory

openai_client = OpenAI()
memory = Memory()

def chat_with_memories(message: str, user_id: str = "default_user") -&amp;gt; str:
    # Retrieve relevant memories
    relevant_memories = memory.search(query=message, user_id=user_id, limit=3)
    memories_str = "\n".join(f"- {entry['memory']}" for entry in relevant_memories["results"])

    # Generate Assistant response
    system_prompt = f"You are a helpful AI. Answer the question based on query and memories.\nUser Memories:\n{memories_str}"
    messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": message}]
    response = openai_client.chat.completions.create(model="gpt-4o-mini", messages=messages)
    assistant_response = response.choices[0].message.content

    # Create new memories from the conversation
    messages.append({"role": "assistant", "content": assistant_response})
    memory.add(messages, user_id=user_id)

    return assistant_response

def main():
    print("Chat with AI (type 'exit' to quit)")
    while True:
        user_input = input("You: ").strip()
        if user_input.lower() == 'exit':
            print("Goodbye!")
            break
        print(f"AI: {chat_with_memories(user_input)}")

if __name__ == "__main__":
    main()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For detailed integration steps, see the &lt;a href="https://docs.mem0.ai/quickstart"&gt;Quickstart&lt;/a&gt; and &lt;a href="https://docs.mem0.ai/api-reference"&gt;API Reference&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;🔗 Integrations &amp;amp; Demos&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ChatGPT with Memory&lt;/strong&gt;: Personalized chat powered by Mem0 (&lt;a href="https://mem0.dev/demo"&gt;Live Demo&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Browser Extension&lt;/strong&gt;: Store memories across ChatGPT, Perplexity, and Claude (&lt;a href="https://chromewebstore.google.com/detail/onihkkbipkfeijkadecaafbgagkhglop?utm_source=item-share-cb"&gt;Chrome Extension&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Langgraph Support&lt;/strong&gt;: Build a customer bot with Langgraph + Mem0 (&lt;a href="https://docs.mem0.ai/integrations/langgraph"&gt;Guide&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;CrewAI Integration&lt;/strong&gt;: Tailor CrewAI outputs with Mem0 (&lt;a href="https://docs.mem0.ai/integrations/crewai"&gt;Example&lt;/a&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;📚 Documentation &amp;amp; Support&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Full docs: &lt;a href="https://docs.mem0.ai"&gt;https://docs.mem0.ai&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Community: &lt;a href="https://mem0.dev/DiG"&gt;Discord&lt;/a&gt; · &lt;a href="https://x.com/mem0ai"&gt;Twitter&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Contact: &lt;a href="mailto:founders@mem0.ai"&gt;founders@mem0.ai&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;We now have a paper you can cite:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@article{mem0,
  title={Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory},
  author={Chhikara, Prateek and Khant, Dev and Aryan, Saket and Singh, Taranjeet and Yadav, Deshraj},
  journal={arXiv preprint arXiv:2504.19413},
  year={2025}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;⚖️ License&lt;/h2&gt; 
&lt;p&gt;Apache 2.0 — see the &lt;a href="https://raw.githubusercontent.com/mem0ai/mem0/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>PaddlePaddle/PaddleOCR</title>
      <link>https://github.com/PaddlePaddle/PaddleOCR</link>
      <description>&lt;p&gt;Turn any PDF or image document into structured data for your AI. A powerful, lightweight OCR toolkit that bridges the gap between images/PDFs and LLMs. Supports 80+ languages.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/docs/images/Banner.png" alt="PaddleOCR Banner" /&gt; &lt;/p&gt; 
 &lt;p&gt;English | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_cn.md"&gt;简体中文&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_tcn.md"&gt;繁體中文&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_ja.md"&gt;日本語&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_ko.md"&gt;한국어&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_fr.md"&gt;Français&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_ru.md"&gt;Русский&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_es.md"&gt;Español&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_ar.md"&gt;العربية&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/PaddlePaddle/PaddleOCR"&gt;&lt;img src="https://img.shields.io/github/stars/PaddlePaddle/PaddleOCR?color=ccf" alt="stars" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2507.05595"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2507.05595-b31b1b.svg?logo=arXiv" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/paddleocr"&gt;&lt;img src="https://static.pepy.tech/badge/paddleocr/month" alt="PyPI Downloads" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/paddleocr"&gt;&lt;img src="https://static.pepy.tech/badge/paddleocr" alt="PyPI Downloads" /&gt;&lt;/a&gt; &lt;a href="https://github.com/PaddlePaddle/PaddleOCR/network/dependents"&gt;&lt;img src="https://img.shields.io/badge/Used%20by-5.9k%2B%20repositories-blue" alt="Used by" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://img.shields.io/badge/python-3.8~3.12-aff.svg?sanitize=true" alt="python" /&gt; &lt;img src="https://img.shields.io/badge/os-linux%2C%20win%2C%20mac-pink.svg?sanitize=true" alt="os" /&gt; &lt;img src="https://img.shields.io/badge/hardware-cpu%2C%20gpu%2C%20xpu%2C%20npu-yellow.svg?sanitize=true" alt="hardware" /&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-Apache_2.0-green" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/PaddlePaddle/PaddleOCR"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;PaddleOCR is an industry-leading, production-ready OCR and document AI engine, offering end-to-end solutions from text extraction to intelligent document understanding&lt;/strong&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h1&gt;PaddleOCR&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://www.paddlepaddle.org.cn/en"&gt;&lt;img src="https://img.shields.io/badge/PaddlePaddle-3.0-orange" alt="Framework" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/#"&gt;&lt;img src="https://img.shields.io/badge/Recognition%20Accuracy-%F0%9F%8F%86-green" alt="Accuracy" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/#"&gt;&lt;img src="https://img.shields.io/badge/Support_Languages-80+-brightgreen" alt="Multi-Language" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/#"&gt;&lt;img src="https://img.shields.io/badge/Handwriting-%E2%9C%93-success" alt="Handwriting" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/#"&gt;&lt;img src="https://img.shields.io/badge/Heterogeneous%20Hardware-Kunlunxin%20%7C%20Ascend_NPU-red" alt="Hardware" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] PaddleOCR now provides an MCP server that supports integration with Agent applications like Claude Desktop. For details, please refer to &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/mcp_server.html"&gt;PaddleOCR MCP Server&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;The PaddleOCR 3.0 Technical Report is now available. See details at: &lt;a href="https://arxiv.org/abs/2507.05595"&gt;PaddleOCR 3.0 Technical Report&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;PaddleOCR&lt;/strong&gt; converts documents and images into &lt;strong&gt;structured, AI-friendly data&lt;/strong&gt; (like JSON and Markdown) with &lt;strong&gt;industry-leading accuracy&lt;/strong&gt;—powering AI applications for everyone from indie developers and startups to large enterprises worldwide. With over &lt;strong&gt;50,000 stars&lt;/strong&gt; and deep integration into leading projects like &lt;strong&gt;MinerU, RAGFlow, and OmniParser&lt;/strong&gt;, PaddleOCR has become the &lt;strong&gt;premier solution&lt;/strong&gt; for developers building intelligent document applications in the &lt;strong&gt;AI era&lt;/strong&gt;.&lt;/p&gt; 
&lt;h3&gt;PaddleOCR 3.0 Core Features&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aistudio.baidu.com/community/app/91660/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP_OCRv5-Demo_on_AI_Studio-green" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://aistudio.baidu.com/community/app/518494/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP_StructureV3-Demo_on_AI_Studio-green" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://aistudio.baidu.com/community/app/518493/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP_ChatOCRv4-Demo_on_AI_Studio-green" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://www.modelscope.cn/organization/PaddlePaddle"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%A4%96_Demo_on_ModelScope-purple" alt="ModelScope" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/PaddlePaddle"&gt;&lt;img src="https://img.shields.io/badge/Demo_on_HuggingFace-purple.svg?logo=huggingface" alt="HuggingFace" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-OCRv5 — Universal Scene Text Recognition&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;Single model supports five text types&lt;/strong&gt; (Simplified Chinese, Traditional Chinese, English, Japanese, and Pinyin) with &lt;strong&gt;13% accuracy improvement&lt;/strong&gt;. Solves multilingual mixed document recognition challenges.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-StructureV3 — Complex Document Parsing&lt;/strong&gt;&lt;br /&gt; Intelligently converts complex PDFs and document images into &lt;strong&gt;Markdown and JSON files that preserve original structure&lt;/strong&gt;. &lt;strong&gt;Outperforms&lt;/strong&gt; numerous commercial solutions in public benchmarks. &lt;strong&gt;Perfectly maintains document layout and hierarchical structure&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-ChatOCRv4 — Intelligent Information Extraction&lt;/strong&gt;&lt;br /&gt; Natively integrates ERNIE 4.5 to &lt;strong&gt;precisely extract key information&lt;/strong&gt; from massive documents, with 15% accuracy improvement over previous generation. Makes documents "&lt;strong&gt;understand&lt;/strong&gt;" your questions and provide accurate answers.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;In addition to providing an outstanding model library, PaddleOCR 3.0 also offers user-friendly tools covering model training, inference, and service deployment, so developers can rapidly bring AI applications to production.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/main/images/paddleocr/README/Arch.jpg" alt="PaddleOCR Architecture" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Special Note&lt;/strong&gt;: PaddleOCR 3.x introduces several significant interface changes. &lt;strong&gt;Old code written based on PaddleOCR 2.x is likely incompatible with PaddleOCR 3.x&lt;/strong&gt;. Please ensure that the documentation you are reading matches the version of PaddleOCR you are using. &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/update/upgrade_notes.html"&gt;This document&lt;/a&gt; explains the reasons for the upgrade and the major changes from PaddleOCR 2.x to 3.x.&lt;/p&gt; 
&lt;h2&gt;📣 Recent updates&lt;/h2&gt; 
&lt;h3&gt;🔥🔥2025.08.21: Release of PaddleOCR 3.2.0, includes:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Significant Model Additions:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Introduced training, inference, and deployment for PP-OCRv5 recognition models in English, Thai, and Greek. &lt;strong&gt;The PP-OCRv5 English model delivers an 11% improvement in English scenarios compared to the main PP-OCRv5 model, with the Thai and Greek recognition models achieving accuracies of 82.68% and 89.28%, respectively.&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Deployment Capability Upgrades:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Full support for PaddlePaddle framework versions 3.1.0 and 3.1.1.&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Comprehensive upgrade of the PP-OCRv5 C++ local deployment solution, now supporting both Linux and Windows, with feature parity and identical accuracy to the Python implementation.&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;High-performance inference now supports CUDA 12, and inference can be performed using either the Paddle Inference or ONNX Runtime backends.&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;The high-stability service-oriented deployment solution is now fully open-sourced, allowing users to customize Docker images and SDKs as required.&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;The high-stability service-oriented deployment solution also supports invocation via manually constructed HTTP requests, enabling client-side code development in any programming language.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Benchmark Support:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;All production lines now support fine-grained benchmarking, enabling measurement of end-to-end inference time as well as per-layer and per-module latency data to assist with performance analysis. &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/docs/version3.x/pipeline_usage/instructions/benchmark.en.md"&gt;Here's&lt;/a&gt; how to set up and use the benchmark feature.&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Documentation has been updated to include key metrics for commonly used configurations on mainstream hardware, such as inference latency and memory usage, providing deployment references for users.&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Bug Fixes:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Resolved the issue of failed log saving during model training.&lt;/li&gt; 
   &lt;li&gt;Upgraded the data augmentation component for formula models for compatibility with newer versions of the albumentations dependency, and fixed deadlock warnings when using the tokenizers package in multi-process scenarios.&lt;/li&gt; 
   &lt;li&gt;Fixed inconsistencies in switch behaviors (e.g., &lt;code&gt;use_chart_parsing&lt;/code&gt;) in the PP-StructureV3 configuration files compared to other pipelines.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Other Enhancements:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Separated core and optional dependencies. Only minimal core dependencies are required for basic text recognition; additional dependencies for document parsing and information extraction can be installed as needed.&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Enabled support for NVIDIA RTX 50 series graphics cards on Windows; users can refer to the &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/docs/version3.x/installation.en.md"&gt;installation guide&lt;/a&gt; for the corresponding PaddlePaddle framework versions.&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;PP-OCR series models now support returning single-character coordinates.&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;Added AIStudio, ModelScope, and other model download sources, allowing users to specify the source for model downloads.&lt;/li&gt; 
   &lt;li&gt;Added support for chart-to-table conversion via the PP-Chart2Table module.&lt;/li&gt; 
   &lt;li&gt;Optimized documentation descriptions to improve usability.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;2025.08.15: PaddleOCR 3.1.1 Released&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Bug Fixes:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Added the missing methods &lt;code&gt;save_vector&lt;/code&gt;, &lt;code&gt;save_visual_info_list&lt;/code&gt;, &lt;code&gt;load_vector&lt;/code&gt;, and &lt;code&gt;load_visual_info_list&lt;/code&gt; in the &lt;code&gt;PP-ChatOCRv4&lt;/code&gt; class.&lt;/li&gt; 
    &lt;li&gt;Added the missing parameters &lt;code&gt;glossary&lt;/code&gt; and &lt;code&gt;llm_request_interval&lt;/code&gt; to the &lt;code&gt;translate&lt;/code&gt; method in the &lt;code&gt;PPDocTranslation&lt;/code&gt; class.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Documentation Improvements:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Added a demo to the MCP documentation.&lt;/li&gt; 
    &lt;li&gt;Added information about the PaddlePaddle and PaddleOCR version used for performance metrics testing in the documentation.&lt;/li&gt; 
    &lt;li&gt;Fixed errors and omissions in the production line document translation.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Others:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Changed the MCP server dependency to use the pure Python library &lt;code&gt;puremagic&lt;/code&gt; instead of &lt;code&gt;python-magic&lt;/code&gt; to reduce installation issues.&lt;/li&gt; 
    &lt;li&gt;Retested PP-OCRv5 performance metrics with PaddleOCR version 3.1.0 and updated the documentation.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;2025.06.29: PaddleOCR 3.1.0 Released&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Key Models and Pipelines:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Added PP-OCRv5 Multilingual Text Recognition Model&lt;/strong&gt;, which supports the training and inference process for text recognition models in 37 languages, including French, Spanish, Portuguese, Russian, Korean, etc. &lt;strong&gt;Average accuracy improved by over 30%.&lt;/strong&gt; &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/algorithm/PP-OCRv5/PP-OCRv5_multi_languages.html"&gt;Details&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;Upgraded the &lt;strong&gt;PP-Chart2Table model&lt;/strong&gt; in PP-StructureV3, further enhancing the capability of converting charts to tables. On internal custom evaluation sets, the metric (RMS-F1) &lt;strong&gt;increased by 9.36 percentage points (71.24% -&amp;gt; 80.60%).&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;Newly launched &lt;strong&gt;document translation pipeline, PP-DocTranslation, based on PP-StructureV3 and ERNIE 4.5&lt;/strong&gt;, which supports the translation of Markdown format documents, various complex-layout PDF documents, and document images, with the results saved as Markdown format documents. &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/pipeline_usage/PP-DocTranslation.html"&gt;Details&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;New MCP server:&lt;/strong&gt; &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/mcp_server.html"&gt;Details&lt;/a&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Supports both OCR and PP-StructureV3 pipelines.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;Supports three working modes: local Python library, AIStudio Community Cloud Service, and self-hosted service.&lt;/li&gt; 
    &lt;li&gt;Supports invoking local services via stdio and remote services via Streamable HTTP.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Documentation Optimization:&lt;/strong&gt; Improved the descriptions in some user guides for a smoother reading experience.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;2025.06.26: PaddleOCR 3.0.3 Released&lt;/strong&gt;&lt;/summary&gt; - Bug Fix: Resolved the issue where the `enable_mkldnn` parameter was not effective, restoring the default behavior of using MKL-DNN for CPU inference. 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;2025.06.19: PaddleOCR 3.0.2 Released&lt;/strong&gt;&lt;/summary&gt; - **New Features:** 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;The default download source has been changed from &lt;code&gt;BOS&lt;/code&gt; to &lt;code&gt;HuggingFace&lt;/code&gt;. Users can also change the environment variable &lt;code&gt;PADDLE_PDX_MODEL_SOURCE&lt;/code&gt; to &lt;code&gt;BOS&lt;/code&gt; to set the model download source back to Baidu Object Storage (BOS).&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Added service invocation examples for six languages—C++, Java, Go, C#, Node.js, and PHP—for pipelines like PP-OCRv5, PP-StructureV3, and PP-ChatOCRv4.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Improved the layout partition sorting algorithm in the PP-StructureV3 pipeline, enhancing the sorting logic for complex vertical layouts to deliver better results.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Enhanced model selection logic: when a language is specified but a model version is not, the system will automatically select the latest model version supporting that language.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Set a default upper limit for MKL-DNN cache size to prevent unlimited growth, while also allowing users to configure cache capacity.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Updated default configurations for high-performance inference to support Paddle MKL-DNN acceleration and optimized the logic for automatic configuration selection for smarter choices.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Adjusted the logic for obtaining the default device to consider the actual support for computing devices by the installed Paddle framework, making program behavior more intuitive.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Added Android example for PP-OCRv5. &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/on_device_deployment.html"&gt;Details&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Bug Fixes:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Fixed an issue with some CLI parameters in PP-StructureV3 not taking effect.&lt;/li&gt; 
    &lt;li&gt;Resolved an issue where &lt;code&gt;export_paddlex_config_to_yaml&lt;/code&gt; would not function correctly in certain cases.&lt;/li&gt; 
    &lt;li&gt;Corrected the discrepancy between the actual behavior of &lt;code&gt;save_path&lt;/code&gt; and its documentation description.&lt;/li&gt; 
    &lt;li&gt;Fixed potential multithreading errors when using MKL-DNN in basic service deployment.&lt;/li&gt; 
    &lt;li&gt;Corrected channel order errors in image preprocessing for the Latex-OCR model.&lt;/li&gt; 
    &lt;li&gt;Fixed channel order errors in saving visualized images within the text recognition module.&lt;/li&gt; 
    &lt;li&gt;Resolved channel order errors in visualized table results within PP-StructureV3 pipeline.&lt;/li&gt; 
    &lt;li&gt;Fixed an overflow issue in the calculation of &lt;code&gt;overlap_ratio&lt;/code&gt; under extremely special circumstances in the PP-StructureV3 pipeline.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Documentation Improvements:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Updated the description of the &lt;code&gt;enable_mkldnn&lt;/code&gt; parameter in the documentation to accurately reflect the program's actual behavior.&lt;/li&gt; 
    &lt;li&gt;Fixed errors in the documentation regarding the &lt;code&gt;lang&lt;/code&gt; and &lt;code&gt;ocr_version&lt;/code&gt; parameters.&lt;/li&gt; 
    &lt;li&gt;Added instructions for exporting pipeline configuration files via CLI.&lt;/li&gt; 
    &lt;li&gt;Fixed missing columns in the performance data table for PP-OCRv5.&lt;/li&gt; 
    &lt;li&gt;Refined benchmark metrics for PP-StructureV3 across different configurations.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Others:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Relaxed version restrictions on dependencies like numpy and pandas, restoring support for Python 3.12.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;History Log&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;2025.06.05: &lt;strong&gt;PaddleOCR 3.0.1 Released&lt;/strong&gt;, includes:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Optimisation of certain models and model configurations:&lt;/strong&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Updated the default model configuration for PP-OCRv5, changing both detection and recognition from mobile to server models. To improve default performance in most scenarios, the parameter &lt;code&gt;limit_side_len&lt;/code&gt; in the configuration has been changed from 736 to 64.&lt;/li&gt; 
    &lt;li&gt;Added a new text line orientation classification model &lt;code&gt;PP-LCNet_x1_0_textline_ori&lt;/code&gt; with an accuracy of 99.42%. The default text line orientation classifier for OCR, PP-StructureV3, and PP-ChatOCRv4 pipelines has been updated to this model.&lt;/li&gt; 
    &lt;li&gt;Optimized the text line orientation classification model &lt;code&gt;PP-LCNet_x0_25_textline_ori&lt;/code&gt;, improving accuracy by 3.3 percentage points to a current accuracy of 98.85%.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Optimizations and fixes for some issues in version 3.0.0, &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/update/update.html"&gt;details&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;🔥🔥2025.05.20: Official Release of &lt;strong&gt;PaddleOCR v3.0&lt;/strong&gt;, including:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-OCRv5&lt;/strong&gt;: High-Accuracy Text Recognition Model for All Scenarios - Instant Text from Images/PDFs.&lt;/p&gt; 
   &lt;ol&gt; 
    &lt;li&gt;🌐 Single-model support for &lt;strong&gt;five&lt;/strong&gt; text types - Seamlessly process &lt;strong&gt;Simplified Chinese, Traditional Chinese, Simplified Chinese Pinyin, English&lt;/strong&gt; and &lt;strong&gt;Japanese&lt;/strong&gt; within a single model.&lt;/li&gt; 
    &lt;li&gt;✍️ Improved &lt;strong&gt;handwriting recognition&lt;/strong&gt;: Significantly better at complex cursive scripts and non-standard handwriting.&lt;/li&gt; 
    &lt;li&gt;🎯 &lt;strong&gt;13-point accuracy gain&lt;/strong&gt; over PP-OCRv4, achieving state-of-the-art performance across a variety of real-world scenarios.&lt;/li&gt; 
   &lt;/ol&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-StructureV3&lt;/strong&gt;: General-Purpose Document Parsing – Unleash SOTA Images/PDFs Parsing for Real-World Scenarios!&lt;/p&gt; 
   &lt;ol&gt; 
    &lt;li&gt;🧮 &lt;strong&gt;High-Accuracy multi-scene PDF parsing&lt;/strong&gt;, leading both open- and closed-source solutions on the OmniDocBench benchmark.&lt;/li&gt; 
    &lt;li&gt;🧠 Specialized capabilities include &lt;strong&gt;seal recognition&lt;/strong&gt;, &lt;strong&gt;chart-to-table conversion&lt;/strong&gt;, &lt;strong&gt;table recognition with nested formulas/images&lt;/strong&gt;, &lt;strong&gt;vertical text document parsing&lt;/strong&gt;, and &lt;strong&gt;complex table structure analysis&lt;/strong&gt;.&lt;/li&gt; 
   &lt;/ol&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-ChatOCRv4&lt;/strong&gt;: Intelligent Document Understanding – Extract Key Information, not just text from Images/PDFs.&lt;/p&gt; 
   &lt;ol&gt; 
    &lt;li&gt;🔥 &lt;strong&gt;15-point accuracy gain&lt;/strong&gt; in key-information extraction on PDF/PNG/JPG files over the previous generation.&lt;/li&gt; 
    &lt;li&gt;💻 Native support for &lt;strong&gt;ERNIE 4.5&lt;/strong&gt;, with compatibility for large-model deployments via PaddleNLP, Ollama, vLLM, and more.&lt;/li&gt; 
    &lt;li&gt;🤝 Integrated &lt;a href="https://github.com/PaddlePaddle/PaddleMIX/tree/develop/paddlemix/examples/ppdocbee2"&gt;PP-DocBee2&lt;/a&gt;, enabling extraction and understanding of printed text, handwriting, seals, tables, charts, and other common elements in complex documents.&lt;/li&gt; 
   &lt;/ol&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/update/update.html"&gt;History Log&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;⚡ Quick Start&lt;/h2&gt; 
&lt;h3&gt;1. Run online demo&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aistudio.baidu.com/community/app/91660/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP_OCRv5-AI_Studio-green" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://aistudio.baidu.com/community/app/518494/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP_StructureV3-AI_Studio-green" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://aistudio.baidu.com/community/app/518493/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP_ChatOCRv4-AI_Studio-green" alt="AI Studio" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;2. Installation&lt;/h3&gt; 
&lt;p&gt;Install PaddlePaddle refer to &lt;a href="https://www.paddlepaddle.org.cn/en/install/quick?docurl=/documentation/docs/en/develop/install/pip/linux-pip_en.html"&gt;Installation Guide&lt;/a&gt;, after then, install the PaddleOCR toolkit.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# If you only want to use the basic text recognition feature (returns text position coordinates and content), including the PP-OCR series
python -m pip install paddleocr
# If you want to use all features such as document parsing, document understanding, document translation, key information extraction, etc.
# python -m pip install "paddleocr[all]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Starting from version 3.2.0, in addition to the &lt;code&gt;all&lt;/code&gt; dependency group demonstrated above, PaddleOCR also supports installing partial optional features by specifying other dependency groups. All dependency groups provided by PaddleOCR are as follows:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Dependency Group Name&lt;/th&gt; 
   &lt;th&gt;Corresponding Functionality&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;doc-parser&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Document parsing: can be used to extract layout elements such as tables, formulas, stamps, images, etc. from documents; includes models like PP-StructureV3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ie&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Information extraction: can be used to extract key information from documents, such as names, dates, addresses, amounts, etc.; includes models like PP-ChatOCRv4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;trans&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Document translation: can be used to translate documents from one language to another; includes models like PP-DocTranslation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;all&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Complete functionality&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;3. Run inference by CLI&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run PP-OCRv5 inference
paddleocr ocr -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_ocr_002.png --use_doc_orientation_classify False --use_doc_unwarping False --use_textline_orientation False  

# Run PP-StructureV3 inference
paddleocr pp_structurev3 -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/pp_structure_v3_demo.png --use_doc_orientation_classify False --use_doc_unwarping False

# Get the Qianfan API Key at first, and then run PP-ChatOCRv4 inference
paddleocr pp_chatocrv4_doc -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/vehicle_certificate-1.png -k 驾驶室准乘人数 --qianfan_api_key your_api_key --use_doc_orientation_classify False --use_doc_unwarping False 

# Get more information about "paddleocr ocr"
paddleocr ocr --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;4. Run inference by API&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;4.1 PP-OCRv5 Example&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Initialize PaddleOCR instance
from paddleocr import PaddleOCR
ocr = PaddleOCR(
    use_doc_orientation_classify=False,
    use_doc_unwarping=False,
    use_textline_orientation=False)

# Run OCR inference on a sample image 
result = ocr.predict(
    input="https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_ocr_002.png")

# Visualize the results and save the JSON results
for res in result:
    res.print()
    res.save_to_img("output")
    res.save_to_json("output")
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;4.2 PP-StructureV3 Example&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from pathlib import Path
from paddleocr import PPStructureV3

pipeline = PPStructureV3(
    use_doc_orientation_classify=False,
    use_doc_unwarping=False
)

# For Image
output = pipeline.predict(
    input="https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/pp_structure_v3_demo.png",
)

# Visualize the results and save the JSON results
for res in output:
    res.print() 
    res.save_to_json(save_path="output") 
    res.save_to_markdown(save_path="output")           
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;4.3 PP-ChatOCRv4 Example&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from paddleocr import PPChatOCRv4Doc

chat_bot_config = {
    "module_name": "chat_bot",
    "model_name": "ernie-3.5-8k",
    "base_url": "https://qianfan.baidubce.com/v2",
    "api_type": "openai",
    "api_key": "api_key",  # your api_key
}

retriever_config = {
    "module_name": "retriever",
    "model_name": "embedding-v1",
    "base_url": "https://qianfan.baidubce.com/v2",
    "api_type": "qianfan",
    "api_key": "api_key",  # your api_key
}

pipeline = PPChatOCRv4Doc(
    use_doc_orientation_classify=False,
    use_doc_unwarping=False
)

visual_predict_res = pipeline.visual_predict(
    input="https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/vehicle_certificate-1.png",
    use_common_ocr=True,
    use_seal_recognition=True,
    use_table_recognition=True,
)

mllm_predict_info = None
use_mllm = False
# If a multimodal large model is used, the local mllm service needs to be started. You can refer to the documentation: https://github.com/PaddlePaddle/PaddleX/blob/release/3.0/docs/pipeline_usage/tutorials/vlm_pipelines/doc_understanding.en.md performs deployment and updates the mllm_chat_bot_config configuration.
if use_mllm:
    mllm_chat_bot_config = {
        "module_name": "chat_bot",
        "model_name": "PP-DocBee",
        "base_url": "http://127.0.0.1:8080/",  # your local mllm service url
        "api_type": "openai",
        "api_key": "api_key",  # your api_key
    }

    mllm_predict_res = pipeline.mllm_pred(
        input="https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/vehicle_certificate-1.png",
        key_list=["驾驶室准乘人数"],
        mllm_chat_bot_config=mllm_chat_bot_config,
    )
    mllm_predict_info = mllm_predict_res["mllm_res"]

visual_info_list = []
for res in visual_predict_res:
    visual_info_list.append(res["visual_info"])
    layout_parsing_result = res["layout_parsing_result"]

vector_info = pipeline.build_vector(
    visual_info_list, flag_save_bytes_vector=True, retriever_config=retriever_config
)
chat_result = pipeline.chat(
    key_list=["驾驶室准乘人数"],
    visual_info=visual_info_list,
    vector_info=vector_info,
    mllm_predict_info=mllm_predict_info,
    chat_bot_config=chat_bot_config,
    retriever_config=retriever_config,
)
print(chat_result)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;5. Chinese Heterogeneous AI Accelerators&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/other_devices_support/paddlepaddle_install_NPU.html"&gt;Huawei Ascend&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/other_devices_support/paddlepaddle_install_XPU.html"&gt;KUNLUNXIN&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🧩 More Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Convert models to ONNX format: &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/obtaining_onnx_models.html"&gt;Obtaining ONNX Models&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Accelerate inference using engines like OpenVINO, ONNX Runtime, TensorRT, or perform inference using ONNX format models: &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/high_performance_inference.html"&gt;High-Performance Inference&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Accelerate inference using multi-GPU and multi-process: &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/pipeline_usage/instructions/parallel_inference.html"&gt;Parallel Inference for Pipelines&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Integrate PaddleOCR into applications written in C++, C#, Java, etc.: &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/serving.html"&gt;Serving&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;⛰️ Advanced Tutorials&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/pipeline_usage/OCR.html"&gt;PP-OCRv5 Tutorial&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/pipeline_usage/PP-StructureV3.html"&gt;PP-StructureV3 Tutorial&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/pipeline_usage/PP-ChatOCRv4.html"&gt;PP-ChatOCRv4 Tutorial&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🔄 Quick Overview of Execution Results&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/docs/images/demo.gif" alt="PP-OCRv5 Demo" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/docs/images/blue_v3.gif" alt="PP-StructureV3 Demo" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;✨ Stay Tuned&lt;/h2&gt; 
&lt;p&gt;⭐ &lt;strong&gt;Star this repository to keep up with exciting updates and new releases, including powerful OCR and document parsing capabilities!&lt;/strong&gt; ⭐&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="1200" src="https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/main/images/paddleocr/README/star_paddleocr.en.gif" alt="Star-Project" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;👩‍👩‍👧‍👦 Community&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;PaddlePaddle WeChat official account&lt;/th&gt; 
    &lt;th align="center"&gt;Join the tech discussion group&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/refs/heads/main/images/paddleocr/README/qrcode_for_paddlepaddle_official_account.jpg" width="150" /&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/refs/heads/main/images/paddleocr/README/qr_code_for_the_questionnaire.jpg" width="150" /&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;😃 Awesome Projects Leveraging PaddleOCR&lt;/h2&gt; 
&lt;p&gt;PaddleOCR wouldn't be where it is today without its incredible community! 💗 A massive thank you to all our longtime partners, new collaborators, and everyone who's poured their passion into PaddleOCR — whether we've named you or not. Your support fuels our fire!&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Project Name&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/infiniflow/ragflow"&gt;RAGFlow&lt;/a&gt; &lt;a href="https://github.com/infiniflow/ragflow"&gt;&lt;img src="https://img.shields.io/github/stars/infiniflow/ragflow" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;RAG engine based on deep document understanding.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/opendatalab/MinerU"&gt;MinerU&lt;/a&gt; &lt;a href="https://github.com/opendatalab/MinerU"&gt;&lt;img src="https://img.shields.io/github/stars/opendatalab/MinerU" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Multi-type Document to Markdown Conversion Tool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/hiroi-sora/Umi-OCR"&gt;Umi-OCR&lt;/a&gt; &lt;a href="https://github.com/hiroi-sora/Umi-OCR"&gt;&lt;img src="https://img.shields.io/github/stars/hiroi-sora/Umi-OCR" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Free, Open-source, Batch Offline OCR Software.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/OmniParser"&gt;OmniParser&lt;/a&gt;&lt;a href="https://github.com/microsoft/OmniParser"&gt;&lt;img src="https://img.shields.io/github/stars/microsoft/OmniParser" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;OmniParser: Screen Parsing tool for Pure Vision Based GUI Agent.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/netease-youdao/QAnything"&gt;QAnything&lt;/a&gt;&lt;a href="https://github.com/netease-youdao/QAnything"&gt;&lt;img src="https://img.shields.io/github/stars/netease-youdao/QAnything" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Question and Answer based on Anything.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/opendatalab/PDF-Extract-Kit"&gt;PDF-Extract-Kit&lt;/a&gt; &lt;a href="https://github.com/opendatalab/PDF-Extract-Kit"&gt;&lt;img src="https://img.shields.io/github/stars/opendatalab/PDF-Extract-Kit" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;A powerful open-source toolkit designed to efficiently extract high-quality content from complex and diverse PDF documents.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/PantsuDango/Dango-Translator"&gt;Dango-Translator&lt;/a&gt;&lt;a href="https://github.com/PantsuDango/Dango-Translator"&gt;&lt;img src="https://img.shields.io/github/stars/PantsuDango/Dango-Translator" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Recognize text on the screen, translate it and show the translation results in real time.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/awesome_projects.md"&gt;Learn more projects&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/awesome_projects.md"&gt;More projects based on PaddleOCR&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;👩‍👩‍👧‍👦 Contributors&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/PaddlePaddle/PaddleOCR/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=PaddlePaddle/PaddleOCR&amp;amp;max=400&amp;amp;columns=20" width="800" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;🌟 Star&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="800" src="https://api.star-history.com/svg?repos=PaddlePaddle/PaddleOCR&amp;amp;type=Date" alt="Star-history" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;📄 License&lt;/h2&gt; 
&lt;p&gt;This project is released under the &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/LICENSE"&gt;Apache 2.0 license&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;🎓 Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;@misc{cui2025paddleocr30technicalreport,
      title={PaddleOCR 3.0 Technical Report}, 
      author={Cheng Cui and Ting Sun and Manhui Lin and Tingquan Gao and Yubo Zhang and Jiaxuan Liu and Xueqing Wang and Zelun Zhang and Changda Zhou and Hongen Liu and Yue Zhang and Wenyu Lv and Kui Huang and Yichao Zhang and Jing Zhang and Jun Zhang and Yi Liu and Dianhai Yu and Yanjun Ma},
      year={2025},
      eprint={2507.05595},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2507.05595}, 
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>dortania/OpenCore-Legacy-Patcher</title>
      <link>https://github.com/dortania/OpenCore-Legacy-Patcher</link>
      <description>&lt;p&gt;Experience macOS just like before&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/dortania/OpenCore-Legacy-Patcher/main/docs/images/OC-Patcher.png" alt="OpenCore Patcher Logo" width="256" /&gt; 
 &lt;h1&gt;OpenCore Legacy Patcher&lt;/h1&gt; 
&lt;/div&gt; 
&lt;p&gt;A Python-based project revolving around &lt;a href="https://github.com/acidanthera/OpenCorePkg"&gt;Acidanthera's OpenCorePkg&lt;/a&gt; and &lt;a href="https://github.com/acidanthera/Lilu"&gt;Lilu&lt;/a&gt; for both running and unlocking features in macOS on supported and unsupported Macs.&lt;/p&gt; 
&lt;p&gt;Our project's main goal is to breathe new life into Macs no longer supported by Apple, allowing for the installation and usage of macOS Big Sur and newer on machines as old as 2007.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;img src="https://img.shields.io/github/downloads/dortania/OpenCore-Legacy-Patcher/total?color=white&amp;amp;style=plastic" alt="GitHub all releases" /&gt; &lt;img src="https://img.shields.io/github/languages/top/dortania/OpenCore-Legacy-Patcher?color=4B8BBE&amp;amp;style=plastic" alt="GitHub top language" /&gt; &lt;img src="https://img.shields.io/discord/417165963327176704?color=7289da&amp;amp;label=discord&amp;amp;style=plastic" alt="Discord" /&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;Noteworthy features of OpenCore Legacy Patcher:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Support for macOS Big Sur, Monterey, Ventura, Sonoma and Sequoia&lt;/li&gt; 
 &lt;li&gt;Native Over the Air (OTA) System Updates&lt;/li&gt; 
 &lt;li&gt;Supports Penryn and newer Macs&lt;/li&gt; 
 &lt;li&gt;Full support for WPA Wi-Fi and Personal Hotspot on BCM943224 and newer wireless chipsets&lt;/li&gt; 
 &lt;li&gt;System Integrity Protection, FileVault 2, .im4m Secure Boot and Vaulting&lt;/li&gt; 
 &lt;li&gt;Recovery OS, Safe Mode and Single-user Mode booting on non-native OSes&lt;/li&gt; 
 &lt;li&gt;Unlocks features such as Sidecar and AirPlay to Mac even on native Macs&lt;/li&gt; 
 &lt;li&gt;Enables enhanced SATA and NVMe power management on non-Apple storage devices&lt;/li&gt; 
 &lt;li&gt;Zero firmware patching required (ie. APFS ROM patching)&lt;/li&gt; 
 &lt;li&gt;Graphics acceleration for both Metal and non-Metal GPUs&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;Note: Only clean-installs and upgrades are supported. macOS Big Sur installs already patched with other patchers, such as &lt;a href="https://github.com/BenSova/Patched-Sur"&gt;Patched Sur&lt;/a&gt; or &lt;a href="https://github.com/StarPlayrX/bigmac"&gt;bigmac&lt;/a&gt;, cannot be used due to broken file integrity with APFS snapshots and SIP.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;You can, however, reinstall macOS with this patcher and retain your original data&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Note 2: Currently, OpenCore Legacy Patcher officially supports patching to run macOS Big Sur through Sonoma installs. For older OSes, OpenCore may function; however, support is currently not provided from Dortania.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;For macOS Mojave and Catalina support, we recommend the use of &lt;a href="http://dosdude1.com"&gt;dosdude1's patchers&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;To start using the project, please see our in-depth guide:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://dortania.github.io/OpenCore-Legacy-Patcher/"&gt;OpenCore Legacy Patcher Guide&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Support&lt;/h2&gt; 
&lt;p&gt;This project is offered on an AS-IS basis, we do not guarantee support for any issues that may arise. However, there is a community server with other passionate users and developers that can aid you:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://discord.gg/rqdPgH8xSN"&gt;OpenCore Patcher Paradise Discord Server&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Keep in mind that the Discord server is maintained by the community, so we ask everyone to be respectful.&lt;/li&gt; 
   &lt;li&gt;Please review our docs on &lt;a href="https://dortania.github.io/OpenCore-Legacy-Patcher/DEBUG.html"&gt;how to debug with OpenCore&lt;/a&gt; to gather important information to help others with troubleshooting.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Running from source&lt;/h2&gt; 
&lt;p&gt;To run the project from source, see here: &lt;a href="https://raw.githubusercontent.com/dortania/OpenCore-Legacy-Patcher/main/SOURCE.md"&gt;Build and run from source&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Credits&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Acidanthera"&gt;Acidanthera&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;OpenCorePkg, as well as many of the core kexts and tools&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/DhinakG"&gt;DhinakG&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Main co-author&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Khronokernel"&gt;Khronokernel&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Main co-author&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Ausdauersportler"&gt;Ausdauersportler&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;iMacs Metal GPUs Upgrade Patch set and documentation&lt;/li&gt; 
   &lt;li&gt;Great amounts of help with debugging, and code suggestions&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vit9696"&gt;vit9696&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Endless amount of help troubleshooting, determining fixes and writing patches&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/covasedu"&gt;EduCovas&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://github.com/moraea/non-metal-frameworks"&gt;non-Metal patch set&lt;/a&gt; for nVidia Tesla/Fermi/Maxwell/Pascal, AMD TeraScale 1/2, and Intel Core 1st/2nd Generation GPUs&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/moraea/misc-patches/tree/main/3802-Metal-15"&gt;3802 Metal patch set&lt;/a&gt; and &lt;a href="https://github.com/dortania/MetallibSupportPkg"&gt;MetallibSupportPkg&lt;/a&gt; for nVidia Kepler and Intel Core 3rd/4th Generation GPUs&lt;/li&gt; 
   &lt;li&gt;Metal bundle patches and shims for &lt;a href="https://github.com/moraea/misc-patches/tree/main/Kepler%2013%2B"&gt;nVidia Kepler&lt;/a&gt;, &lt;a href="https://github.com/moraea/misc-patches/tree/main/GCN%2013%2B"&gt;AMD GCN 1 - 4&lt;/a&gt;, and &lt;a href="https://github.com/moraea/misc-patches/tree/main/vega%2013%2B"&gt;AMD GCN 5 (Vega)&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/moraea/misc-patches/tree/main/Sonoma%2014.4%20IOSurface"&gt;IOSurface offset patches&lt;/a&gt; for nVidia Kepler, AMD GCN 1 - 5, and Intel Core 3rd - 6th Generation GPUs&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/moraea/unsupported-wifi-patches"&gt;legacy Wi-Fi patch set&lt;/a&gt; restores functionality for Wi-Fi cards in all 2007 - 2017 models&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/moraea/misc-patches/tree/main/T1-Patch"&gt;T1 patch set&lt;/a&gt; restores Touch ID, Apple Pay, and other secure functionality in 2016 - 2017 models&lt;/li&gt; 
   &lt;li&gt;AppleGVA downgrade for accelerated video decoding on 2012 - 2016 models&lt;/li&gt; 
   &lt;li&gt;OpenCL and OpenGL downgrade for AMD GCN&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/moraea/misc-patches/tree/main/IOUSBHostFamily-14.4"&gt;USB 1 patch&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/moosethegoose2213"&gt;ASentientHedgehog&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://github.com/moraea/non-metal-frameworks"&gt;non-Metal patch set&lt;/a&gt; for nVidia Tesla/Fermi/Maxwell/Pascal, AMD TeraScale 1/2, and Intel Core 1st/2nd Generation GPUs&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ASentientBot"&gt;ASentientBot&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://github.com/moraea/non-metal-frameworks"&gt;non-Metal patch set&lt;/a&gt; for nVidia Tesla/Fermi/Maxwell/Pascal, AMD TeraScale 1/2, and Intel Core 1st/2nd Generation GPUs&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/moraea/misc-patches/tree/main/sequoia%2031001%20interposer"&gt;Metal bundle interposer&lt;/a&gt; for AMD GCN 1 - 5 and Intel Core 5th/6th Generation GPUs&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/moraea/dsce"&gt;dsce&lt;/a&gt; and &lt;a href="https://github.com/moraea/moraea-common"&gt;shared code&lt;/a&gt; used by some other patches&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/cdf"&gt;cdf&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Mac Pro on OpenCore Patch set and documentation&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/cdf/Innie"&gt;Innie&lt;/a&gt; and &lt;a href="https://github.com/cdf/NightShiftEnabler"&gt;NightShiftEnabler&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://forums.macrumors.com/members/syncretic.1173816/"&gt;Syncretic&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://forums.macrumors.com/threads/mp3-1-others-sse-4-2-emulation-to-enable-amd-metal-driver.2206682/"&gt;AAAMouSSE&lt;/a&gt;, &lt;a href="https://forums.macrumors.com/threads/mp3-1-others-sse-4-2-emulation-to-enable-amd-metal-driver.2206682/post-28447707"&gt;telemetrap&lt;/a&gt; and &lt;a href="https://github.com/reenigneorcim/SurPlus"&gt;SurPlus&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/dosdude1"&gt;dosdude1&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Main author of the &lt;a href="https://github.com/dortania/OCLP-GUI"&gt;original GUI&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Development of previous patchers, laying out much of what needs to be patched&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/parrotgeek1"&gt;parrotgeek1&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://github.com/dortania/OpenCore-Legacy-Patcher/raw/4a8f61a01da72b38a4b2250386cc4b497a31a839/payloads/Config/config.plist#L1222-L1281"&gt;VMM Patch Set&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/BarryKN"&gt;BarryKN&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Development of previous patchers, laying out much of what needs to be patched&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mariobrostech"&gt;mario_bros_tech&lt;/a&gt; and the rest of the Unsupported Mac Discord 
  &lt;ul&gt; 
   &lt;li&gt;Catalyst that started OpenCore Legacy Patcher&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/arter97/"&gt;arter97&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://github.com/arter97/SimpleMSR/"&gt;SimpleMSR&lt;/a&gt; to disable firmware throttling in Nehalem+ MacBooks without batteries&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mrmacintosh.com"&gt;Mr.Macintosh&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Endless hours helping architect and troubleshoot many portions of the project&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/flagersgit"&gt;flagers&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Aid with Nvidia Web Driver research and development&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/moraea/non-metal-frameworks"&gt;non-Metal patch set&lt;/a&gt; for nVidia Tesla/Fermi/Maxwell/Pascal, AMD TeraScale 1/2, and Intel Core 1st/2nd Generation GPUs&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/moraea/misc-patches/tree/main/sequoia%2031001%20interposer"&gt;Metal bundle interposer&lt;/a&gt; for AMD GCN 1 - 5 and Intel Core 5th/6th Generation GPUs&lt;/li&gt; 
   &lt;li&gt;LegacyRVPL, SnapshotIsKill, etc. to aid in rapid testing and development&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/joevt"&gt;joevt&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://github.com/joevt/joevtApps"&gt;FixPCIeLinkrate&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Jazzzny"&gt;Jazzzny&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Research and various contributions to the project&lt;/li&gt; 
   &lt;li&gt;UEFI Legacy XHCI research and development&lt;/li&gt; 
   &lt;li&gt;NVIDIA OpenCL research and development&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;MacBook5,2&lt;/code&gt; research and development 
    &lt;ul&gt; 
     &lt;li&gt;LegacyKeyboardInjector&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Pre-Ivy Bridge Aquantia Ethernet Patch&lt;/li&gt; 
   &lt;li&gt;Non-Metal Photo Booth Patch for Monterey+&lt;/li&gt; 
   &lt;li&gt;GUI and Backend Development 
    &lt;ul&gt; 
     &lt;li&gt;Updater UI&lt;/li&gt; 
     &lt;li&gt;macOS Downloader UI&lt;/li&gt; 
     &lt;li&gt;Downloader UI&lt;/li&gt; 
     &lt;li&gt;USB Top Case probing&lt;/li&gt; 
     &lt;li&gt;Developer root patching&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Vaulting implementation&lt;/li&gt; 
   &lt;li&gt;macOS 15 3802 Helios Research&lt;/li&gt; 
   &lt;li&gt;UEFI bootx64.efi research&lt;/li&gt; 
   &lt;li&gt;universal2 build research&lt;/li&gt; 
   &lt;li&gt;Various documentation contributions&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Amazing users who've graciously donate hardware: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://forums.macrumors.com/members/johnd.53633/"&gt;JohnD&lt;/a&gt; - 2013 Mac Pro&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/SpiGAndromeda"&gt;SpiGAndromeda&lt;/a&gt; - AMD Vega 64&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/turbomacs"&gt;turbomacs&lt;/a&gt; - 2014 5k iMac&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://forums.macrumors.com/members/vinaypundith.1212357/"&gt;vinaypundith&lt;/a&gt; - MacBook7,1&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/ThatStella7922"&gt;ThatStella7922&lt;/a&gt; - 2017 13" MacBook Pro (A1708)&lt;/li&gt; 
   &lt;li&gt;zephar - 2008 Mac Pro&lt;/li&gt; 
   &lt;li&gt;jazo97 - 2011 15" MacBook Pro&lt;/li&gt; 
   &lt;li&gt;And others (reach out if we forgot you!)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;MacRumors and Unsupported Mac Communities 
  &lt;ul&gt; 
   &lt;li&gt;Endless testing and reporting issues&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Apple 
  &lt;ul&gt; 
   &lt;li&gt;for macOS and many of the kexts, frameworks and other binaries we reimplemented into newer OSes&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>ccxt/ccxt</title>
      <link>https://github.com/ccxt/ccxt</link>
      <description>&lt;p&gt;A cryptocurrency trading API with more than 100 exchanges in JavaScript / TypeScript / Python / C# / PHP / Go&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CCXT – CryptoCurrency eXchange Trading Library&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://www.npmjs.com/package/ccxt"&gt;&lt;img src="https://img.shields.io/npm/dy/ccxt.svg?sanitize=true" alt="NPM Downloads" /&gt;&lt;/a&gt; &lt;a href="https://npmjs.com/package/ccxt"&gt;&lt;img src="https://img.shields.io/npm/v/ccxt.svg?sanitize=true" alt="npm" /&gt;&lt;/a&gt; &lt;a href="https://pypi.python.org/pypi/ccxt"&gt;&lt;img src="https://img.shields.io/pypi/v/ccxt.svg?sanitize=true" alt="PyPI" /&gt;&lt;/a&gt; &lt;a href="https://www.nuget.org/packages/ccxt"&gt;&lt;img src="https://img.shields.io/nuget/v/ccxt" alt="NuGet version" /&gt;&lt;/a&gt; &lt;a href="https://godoc.org/github.com/ccxt/ccxt/go/v4"&gt;&lt;img src="https://pkg.go.dev/badge/github.com/ccxt/ccxt/go/v4?utm_source=godoc" alt="GoDoc" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/ccxt"&gt;&lt;img src="https://img.shields.io/discord/690203284119617602?logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://github.com/ccxt/ccxt/wiki/Exchange-Markets"&gt;&lt;img src="https://img.shields.io/badge/exchanges-106-blue.svg?sanitize=true" alt="Supported Exchanges" /&gt;&lt;/a&gt; &lt;a href="https://x.com/ccxt_official"&gt;&lt;img src="https://img.shields.io/twitter/follow/ccxt_official.svg?style=social&amp;amp;label=CCXT" alt="Follow CCXT at x.com" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;A cryptocurrency trading API with more than 100 exchanges in JavaScript / TypeScript / Python / C# / PHP / Go.&lt;/p&gt; 
&lt;h3&gt;&lt;a href="https://raw.githubusercontent.com/ccxt/ccxt/master/#install"&gt;Install&lt;/a&gt; · &lt;a href="https://raw.githubusercontent.com/ccxt/ccxt/master/#usage"&gt;Usage&lt;/a&gt; · &lt;a href="https://github.com/ccxt/ccxt/wiki"&gt;Manual&lt;/a&gt; · &lt;a href="https://github.com/ccxt/ccxt/wiki/FAQ"&gt;FAQ&lt;/a&gt; · &lt;a href="https://github.com/ccxt/ccxt/tree/master/examples"&gt;Examples&lt;/a&gt; · &lt;a href="https://github.com/ccxt/ccxt/raw/master/CONTRIBUTING.md"&gt;Contributing&lt;/a&gt; · &lt;a href="https://raw.githubusercontent.com/ccxt/ccxt/master/#disclaimer"&gt;Disclaimer&lt;/a&gt; · &lt;a href="https://raw.githubusercontent.com/ccxt/ccxt/master/#social"&gt;Social&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;The &lt;strong&gt;CCXT&lt;/strong&gt; library is used to connect and trade with cryptocurrency exchanges and payment processing services worldwide. It provides quick access to market data for storage, analysis, visualization, indicator development, algorithmic trading, strategy backtesting, bot programming, and related software engineering.&lt;/p&gt; 
&lt;p&gt;It is intended to be used by &lt;strong&gt;coders, developers, technically-skilled traders, data-scientists and financial analysts&lt;/strong&gt; for building trading algorithms.&lt;/p&gt; 
&lt;p&gt;Current feature list:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;support for many cryptocurrency exchanges — more coming soon&lt;/li&gt; 
 &lt;li&gt;fully implemented public and private APIs&lt;/li&gt; 
 &lt;li&gt;optional normalized data for cross-exchange analytics and arbitrage&lt;/li&gt; 
 &lt;li&gt;an out of the box unified API that is extremely easy to integrate&lt;/li&gt; 
 &lt;li&gt;works in Node 10.4+, Python 3, PHP 8.1+, netstandard2.0/2.1, Go 1.20+ and web browsers&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;See Also&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;sub&gt;&lt;a href="https://tab-trader.com/?utm_source=ccxt"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/66755907-9c3e8880-eea1-11e9-846e-0bff349ceb87.png" alt="TabTrader" /&gt;&lt;/a&gt;&lt;/sub&gt; &lt;strong&gt;&lt;a href="https://tab-trader.com/?utm_source=ccxt"&gt;TabTrader&lt;/a&gt;&lt;/strong&gt; – trading on all exchanges in one app. Available on &lt;strong&gt;&lt;a href="https://play.google.com/store/apps/details?id=com.tabtrader.android&amp;amp;referrer=utm_source%3Dccxt"&gt;Android&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href="https://itunes.apple.com/app/apple-store/id1095716562?mt=8"&gt;iOS&lt;/a&gt;&lt;/strong&gt;!&lt;/li&gt; 
 &lt;li&gt;&lt;sub&gt;&lt;a href="https://www.freqtrade.io"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/114340585-8e35fa80-9b60-11eb-860f-4379125e2db6.png" alt="Freqtrade" /&gt;&lt;/a&gt;&lt;/sub&gt; &lt;strong&gt;&lt;a href="https://www.freqtrade.io"&gt;Freqtrade&lt;/a&gt;&lt;/strong&gt; – leading opensource cryptocurrency algorithmic trading software!&lt;/li&gt; 
 &lt;li&gt;&lt;sub&gt;&lt;a href="https://www.octobot.online"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/132113722-007fc092-7530-4b41-b929-b8ed380b7b2e.png" alt="OctoBot" /&gt;&lt;/a&gt;&lt;/sub&gt; &lt;strong&gt;&lt;a href="https://www.octobot.online"&gt;OctoBot&lt;/a&gt;&lt;/strong&gt; – cryptocurrency trading bot with an advanced web interface.&lt;/li&gt; 
 &lt;li&gt;&lt;sub&gt;&lt;a href="https://tokenbot.com/?utm_source=github&amp;amp;utm_medium=ccxt&amp;amp;utm_campaign=algodevs"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/152720975-0522b803-70f0-4f18-a305-3c99b37cd990.png" alt="TokenBot" /&gt;&lt;/a&gt;&lt;/sub&gt; &lt;strong&gt;&lt;a href="https://tokenbot.com/?utm_source=github&amp;amp;utm_medium=ccxt&amp;amp;utm_campaign=algodevs"&gt;TokenBot&lt;/a&gt;&lt;/strong&gt; – discover and copy the best algorithmic traders in the world.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Certified Cryptocurrency Exchanges&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;logo&lt;/th&gt; 
   &lt;th&gt;id&lt;/th&gt; 
   &lt;th&gt;name&lt;/th&gt; 
   &lt;th align="center"&gt;ver&lt;/th&gt; 
   &lt;th&gt;type&lt;/th&gt; 
   &lt;th&gt;certified&lt;/th&gt; 
   &lt;th align="center"&gt;pro&lt;/th&gt; 
   &lt;th&gt;discount&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://accounts.binance.com/en/register?ref=D7YA7CLY"&gt;&lt;img src="https://github.com/user-attachments/assets/e9419b93-ccb0-46aa-9bff-c883f096274b" alt="binance" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;binance&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://accounts.binance.com/en/register?ref=D7YA7CLY"&gt;Binance&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://developers.binance.com/en"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://accounts.binance.com/en/register?ref=D7YA7CLY"&gt;&lt;img src="https://img.shields.io/static/v1?label=Fee&amp;amp;message=%2d10%25&amp;amp;color=orange" alt="Sign up with Binance using CCXT's referral link for a 10% discount!" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://accounts.binance.com/en/register?ref=D7YA7CLY"&gt;&lt;img src="https://github.com/user-attachments/assets/871cbea7-eebb-4b28-b260-c1c91df0487a" alt="binanceusdm" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;binanceusdm&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://accounts.binance.com/en/register?ref=D7YA7CLY"&gt;Binance USDⓈ-M&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://binance-docs.github.io/apidocs/futures/en/"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://accounts.binance.com/en/register?ref=D7YA7CLY"&gt;&lt;img src="https://img.shields.io/static/v1?label=Fee&amp;amp;message=%2d10%25&amp;amp;color=orange" alt="Sign up with Binance USDⓈ-M using CCXT's referral link for a 10% discount!" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://accounts.binance.com/en/register?ref=D7YA7CLY"&gt;&lt;img src="https://github.com/user-attachments/assets/387cfc4e-5f33-48cd-8f5c-cd4854dabf0c" alt="binancecoinm" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;binancecoinm&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://accounts.binance.com/en/register?ref=D7YA7CLY"&gt;Binance COIN-M&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://binance-docs.github.io/apidocs/delivery/en/"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://accounts.binance.com/en/register?ref=D7YA7CLY"&gt;&lt;img src="https://img.shields.io/static/v1?label=Fee&amp;amp;message=%2d10%25&amp;amp;color=orange" alt="Sign up with Binance COIN-M using CCXT's referral link for a 10% discount!" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.bybit.com/register?affiliate_id=35953"&gt;&lt;img src="https://github.com/user-attachments/assets/97a5d0b3-de10-423d-90e1-6620960025ed" alt="bybit" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bybit&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.bybit.com/register?affiliate_id=35953"&gt;Bybit&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://bybit-exchange.github.io/docs/inverse/"&gt;&lt;img src="https://img.shields.io/badge/5-lightgray" alt="API Version 5" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.okx.com/join/CCXT2023"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/152485636-38b19e4a-bece-4dec-979a-5982859ffc04.jpg" alt="okx" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;okx&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.okx.com/join/CCXT2023"&gt;OKX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://www.okx.com/docs-v5/en/"&gt;&lt;img src="https://img.shields.io/badge/5-lightgray" alt="API Version 5" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.okx.com/join/CCXT2023"&gt;&lt;img src="https://img.shields.io/static/v1?label=Fee&amp;amp;message=%2d20%25&amp;amp;color=orange" alt="Sign up with OKX using CCXT's referral link for a 20% discount!" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.gate.io/signup/2436035"&gt;&lt;img src="https://github.com/user-attachments/assets/64f988c5-07b6-4652-b5c1-679a6bf67c85" alt="gate" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;gate&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.gate.io/signup/2436035"&gt;Gate.io&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://www.gate.io/docs/developers/apiv4/en/"&gt;&lt;img src="https://img.shields.io/badge/4-lightgray" alt="API Version 4" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.gate.io/signup/2436035"&gt;&lt;img src="https://img.shields.io/static/v1?label=Fee&amp;amp;message=%2d20%25&amp;amp;color=orange" alt="Sign up with Gate.io using CCXT's referral link for a 20% discount!" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.kucoin.com/ucenter/signup?rcode=E5wkqe"&gt;&lt;img src="https://user-images.githubusercontent.com/51840849/87295558-132aaf80-c50e-11ea-9801-a2fb0c57c799.jpg" alt="kucoin" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;kucoin&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.kucoin.com/ucenter/signup?rcode=E5wkqe"&gt;KuCoin&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.kucoin.com"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://futures.kucoin.com/?rcode=E5wkqe"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/147508995-9e35030a-d046-43a1-a006-6fabd981b554.jpg" alt="kucoinfutures" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;kucoinfutures&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://futures.kucoin.com/?rcode=E5wkqe"&gt;KuCoin Futures&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.kucoin.com/futures"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.bitget.com/expressly?languageType=0&amp;amp;channelCode=ccxt&amp;amp;vipCode=tg9j"&gt;&lt;img src="https://github.com/user-attachments/assets/fbaa10cc-a277-441d-a5b7-997dd9a87658" alt="bitget" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bitget&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.bitget.com/expressly?languageType=0&amp;amp;channelCode=ccxt&amp;amp;vipCode=tg9j"&gt;Bitget&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://www.bitget.com/api-doc/common/intro"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://app.hyperliquid.xyz/"&gt;&lt;img src="https://github.com/ccxt/ccxt/assets/43336371/b371bc6c-4a8c-489f-87f4-20a913dd8d4b" alt="hyperliquid" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;hyperliquid&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://app.hyperliquid.xyz/"&gt;Hyperliquid&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://hyperliquid.gitbook.io/hyperliquid-docs/for-developers/api"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/DEX-blue.svg?sanitize=true" alt="DEX - Distributed EXchange" title="DEX - Distributed EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.bitmex.com/app/register/NZTR1q"&gt;&lt;img src="https://github.com/user-attachments/assets/c78425ab-78d5-49d6-bd14-db7734798f04" alt="bitmex" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bitmex&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.bitmex.com/app/register/NZTR1q"&gt;BitMEX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://www.bitmex.com/app/apiOverview"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.bitmex.com/app/register/NZTR1q"&gt;&lt;img src="https://img.shields.io/static/v1?label=Fee&amp;amp;message=%2d10%25&amp;amp;color=orange" alt="Sign up with BitMEX using CCXT's referral link for a 10% discount!" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://bingx.com/invite/OHETOM"&gt;&lt;img src="https://github-production-user-asset-6210df.s3.amazonaws.com/1294454/253675376-6983b72e-4999-4549-b177-33b374c195e3.jpg" alt="bingx" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bingx&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://bingx.com/invite/OHETOM"&gt;BingX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://bingx-api.github.io/docs/"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.htx.com.vc/invite/en-us/1h?invite_code=6rmm2223"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/76137448-22748a80-604e-11ea-8069-6e389271911d.jpg" alt="htx" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;htx&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.htx.com.vc/invite/en-us/1h?invite_code=6rmm2223"&gt;HTX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huobiapi.github.io/docs/spot/v1/en/"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.htx.com.vc/invite/en-us/1h?invite_code=6rmm2223"&gt;&lt;img src="https://img.shields.io/static/v1?label=Fee&amp;amp;message=%2d15%25&amp;amp;color=orange" alt="Sign up with HTX using CCXT's referral link for a 15% discount!" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.mexc.com/register?inviteCode=mexc-1FQ1GNu1"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/137283979-8b2a818d-8633-461b-bfca-de89e8c446b2.jpg" alt="mexc" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;mexc&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.mexc.com/register?inviteCode=mexc-1FQ1GNu1"&gt;MEXC Global&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://mexcdevelop.github.io/apidocs/"&gt;&lt;img src="https://img.shields.io/badge/3-lightgray" alt="API Version 3" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="http://www.bitmart.com/?r=rQCFLh"&gt;&lt;img src="https://github.com/user-attachments/assets/0623e9c4-f50e-48c9-82bd-65c3908c3a14" alt="bitmart" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bitmart&lt;/td&gt; 
   &lt;td&gt;&lt;a href="http://www.bitmart.com/?r=rQCFLh"&gt;BitMart&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://developer-pro.bitmart.com/"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="http://www.bitmart.com/?r=rQCFLh"&gt;&lt;img src="https://img.shields.io/static/v1?label=Fee&amp;amp;message=%2d30%25&amp;amp;color=orange" alt="Sign up with BitMart using CCXT's referral link for a 30% discount!" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://crypto.com/exch/kdacthrnxt"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/147792121-38ed5e36-c229-48d6-b49a-48d05fc19ed4.jpeg" alt="cryptocom" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;cryptocom&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://crypto.com/exch/kdacthrnxt"&gt;Crypto.com&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://exchange-docs.crypto.com/exchange/v1/rest-ws/index.html"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://crypto.com/exch/kdacthrnxt"&gt;&lt;img src="https://img.shields.io/static/v1?label=Fee&amp;amp;message=%2d75%25&amp;amp;color=orange" alt="Sign up with Crypto.com using CCXT's referral link for a 75% discount!" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.coinex.com/register?refer_code=yw5fz"&gt;&lt;img src="https://user-images.githubusercontent.com/51840849/87182089-1e05fa00-c2ec-11ea-8da9-cc73b45abbbc.jpg" alt="coinex" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;coinex&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.coinex.com/register?refer_code=yw5fz"&gt;CoinEx&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.coinex.com/api/v2"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://global.hashkey.com/en-US/register/invite?invite_code=82FQUN"&gt;&lt;img src="https://github.com/user-attachments/assets/6dd6127b-cc19-4a13-9b29-a98d81f80e98" alt="hashkey" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;hashkey&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://global.hashkey.com/en-US/register/invite?invite_code=82FQUN"&gt;HashKey Global&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://hashkeyglobal-apidoc.readme.io/"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://woox.io/register?ref=DIJT0CNL"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/150730761-1a00e5e0-d28c-480f-9e65-089ce3e6ef3b.jpg" alt="woo" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;woo&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://woox.io/register?ref=DIJT0CNL"&gt;WOO X&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.woox.io/"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://woox.io/register?ref=DIJT0CNL"&gt;&lt;img src="https://img.shields.io/static/v1?label=Fee&amp;amp;message=%2d35%25&amp;amp;color=orange" alt="Sign up with WOO X using CCXT's referral link for a 35% discount!" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://dex.woo.org/en/trade?ref=CCXT"&gt;&lt;img src="https://github.com/user-attachments/assets/9ba21b8a-a9c7-4770-b7f1-ce3bcbde68c1" alt="woofipro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;woofipro&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://dex.woo.org/en/trade?ref=CCXT"&gt;WOOFI PRO&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://orderly.network/docs/build-on-evm/building-on-evm"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/DEX-blue.svg?sanitize=true" alt="DEX - Distributed EXchange" title="DEX - Distributed EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://dex.woo.org/en/trade?ref=CCXT"&gt;&lt;img src="https://img.shields.io/static/v1?label=Fee&amp;amp;message=%2d5%25&amp;amp;color=orange" alt="Sign up with WOOFI PRO using CCXT's referral link for a 5% discount!" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Supported Cryptocurrency Exchanges&lt;/h2&gt; 
&lt;!-- init list --&gt;The CCXT library currently supports the following 103 cryptocurrency exchange markets and trading APIs: 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;logo&lt;/th&gt; 
   &lt;th&gt;id&lt;/th&gt; 
   &lt;th&gt;name&lt;/th&gt; 
   &lt;th align="center"&gt;ver&lt;/th&gt; 
   &lt;th&gt;type&lt;/th&gt; 
   &lt;th&gt;certified&lt;/th&gt; 
   &lt;th&gt;pro&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://alpaca.markets"&gt;&lt;img src="https://github.com/user-attachments/assets/e9476df8-a450-4c3e-ab9a-1a7794219e1b" alt="alpaca" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;alpaca&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://alpaca.markets"&gt;Alpaca&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://alpaca.markets/docs/"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://omni.apex.exchange/trade"&gt;&lt;img src="https://github.com/user-attachments/assets/fef8f2f7-4265-46aa-965e-33a91881cb00" alt="apex" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;apex&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://omni.apex.exchange/trade"&gt;Apex&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://api-docs.pro.apex.exchange"&gt;&lt;img src="https://img.shields.io/badge/3-lightgray" alt="API Version 3" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/DEX-blue.svg?sanitize=true" alt="DEX - Distributed EXchange" title="DEX - Distributed EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://ascendex.com/en-us/register?inviteCode=EL6BXBQM"&gt;&lt;img src="https://github.com/user-attachments/assets/55bab6b9-d4ca-42a8-a0e6-fac81ae557f1" alt="ascendex" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ascendex&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ascendex.com/en-us/register?inviteCode=EL6BXBQM"&gt;AscendEX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ascendex.github.io/ascendex-pro-api/#ascendex-pro-api-documentation"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://backpack.exchange/join/ib8qxwyl"&gt;&lt;img src="https://github.com/user-attachments/assets/cc04c278-679f-4554-9f72-930dd632b80f" alt="backpack" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;backpack&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://backpack.exchange/join/ib8qxwyl"&gt;Backpack&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.backpack.exchange/"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://bequant.io/referral/dd104e3bee7634ec"&gt;&lt;img src="https://github.com/user-attachments/assets/0583ef1f-29fe-4b7c-8189-63565a0e2867" alt="bequant" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bequant&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://bequant.io/referral/dd104e3bee7634ec"&gt;Bequant&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://api.bequant.io/"&gt;&lt;img src="https://img.shields.io/badge/3-lightgray" alt="API Version 3" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://b1.run/users/new?code=D3LLBVFT"&gt;&lt;img src="https://github.com/user-attachments/assets/4e5cfd53-98cc-4b90-92cd-0d7b512653d1" alt="bigone" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bigone&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://b1.run/users/new?code=D3LLBVFT"&gt;BigONE&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://open.big.one/docs/api.html"&gt;&lt;img src="https://img.shields.io/badge/3-lightgray" alt="API Version 3" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://accounts.binance.com/en/register?ref=D7YA7CLY"&gt;&lt;img src="https://github.com/user-attachments/assets/e9419b93-ccb0-46aa-9bff-c883f096274b" alt="binance" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;binance&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://accounts.binance.com/en/register?ref=D7YA7CLY"&gt;Binance&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://developers.binance.com/en"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://accounts.binance.com/en/register?ref=D7YA7CLY"&gt;&lt;img src="https://github.com/user-attachments/assets/387cfc4e-5f33-48cd-8f5c-cd4854dabf0c" alt="binancecoinm" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;binancecoinm&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://accounts.binance.com/en/register?ref=D7YA7CLY"&gt;Binance COIN-M&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://binance-docs.github.io/apidocs/delivery/en/"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.binance.us/?ref=35005074"&gt;&lt;img src="https://github.com/user-attachments/assets/a9667919-b632-4d52-a832-df89f8a35e8c" alt="binanceus" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;binanceus&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.binance.us/?ref=35005074"&gt;Binance US&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/binance-us/binance-official-api-docs"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://accounts.binance.com/en/register?ref=D7YA7CLY"&gt;&lt;img src="https://github.com/user-attachments/assets/871cbea7-eebb-4b28-b260-c1c91df0487a" alt="binanceusdm" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;binanceusdm&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://accounts.binance.com/en/register?ref=D7YA7CLY"&gt;Binance USDⓈ-M&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://binance-docs.github.io/apidocs/futures/en/"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://bingx.com/invite/OHETOM"&gt;&lt;img src="https://github-production-user-asset-6210df.s3.amazonaws.com/1294454/253675376-6983b72e-4999-4549-b177-33b374c195e3.jpg" alt="bingx" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bingx&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://bingx.com/invite/OHETOM"&gt;BingX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://bingx-api.github.io/docs/"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://bit2c.co.il/Aff/63bfed10-e359-420c-ab5a-ad368dab0baf"&gt;&lt;img src="https://github.com/user-attachments/assets/db0bce50-6842-4c09-a1d5-0c87d22118aa" alt="bit2c" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bit2c&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://bit2c.co.il/Aff/63bfed10-e359-420c-ab5a-ad368dab0baf"&gt;Bit2C&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://www.bit2c.co.il/home/api"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://bitbank.cc/"&gt;&lt;img src="https://github.com/user-attachments/assets/9d616de0-8a88-4468-8e38-d269acab0348" alt="bitbank" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bitbank&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://bitbank.cc/"&gt;bitbank&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.bitbank.cc/"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://ref.bitbns.com/1090961"&gt;&lt;img src="https://github.com/user-attachments/assets/a5b9a562-cdd8-4bea-9fa7-fd24c1dad3d9" alt="bitbns" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bitbns&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ref.bitbns.com/1090961"&gt;Bitbns&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://bitbns.com/trade/#/api-trading/"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.bitfinex.com"&gt;&lt;img src="https://github.com/user-attachments/assets/4a8e947f-ab46-481a-a8ae-8b20e9b03178" alt="bitfinex" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bitfinex&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.bitfinex.com"&gt;Bitfinex&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.bitfinex.com/v2/docs/"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://bitflyer.com"&gt;&lt;img src="https://github.com/user-attachments/assets/d0217747-e54d-4533-8416-0d553dca74bb" alt="bitflyer" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bitflyer&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://bitflyer.com"&gt;bitFlyer&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://lightning.bitflyer.com/docs?lang=en"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.bitget.com/expressly?languageType=0&amp;amp;channelCode=ccxt&amp;amp;vipCode=tg9j"&gt;&lt;img src="https://github.com/user-attachments/assets/fbaa10cc-a277-441d-a5b7-997dd9a87658" alt="bitget" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bitget&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.bitget.com/expressly?languageType=0&amp;amp;channelCode=ccxt&amp;amp;vipCode=tg9j"&gt;Bitget&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://www.bitget.com/api-doc/common/intro"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.bithumb.com"&gt;&lt;img src="https://github.com/user-attachments/assets/c9e0eefb-4777-46b9-8f09-9d7f7c4af82d" alt="bithumb" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bithumb&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.bithumb.com"&gt;Bithumb&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://apidocs.bithumb.com"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="http://www.bitmart.com/?r=rQCFLh"&gt;&lt;img src="https://github.com/user-attachments/assets/0623e9c4-f50e-48c9-82bd-65c3908c3a14" alt="bitmart" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bitmart&lt;/td&gt; 
   &lt;td&gt;&lt;a href="http://www.bitmart.com/?r=rQCFLh"&gt;BitMart&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://developer-pro.bitmart.com/"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.bitmex.com/app/register/NZTR1q"&gt;&lt;img src="https://github.com/user-attachments/assets/c78425ab-78d5-49d6-bd14-db7734798f04" alt="bitmex" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bitmex&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.bitmex.com/app/register/NZTR1q"&gt;BitMEX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://www.bitmex.com/app/apiOverview"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.bitopro.com"&gt;&lt;img src="https://github.com/user-attachments/assets/affc6337-b95a-44bf-aacd-04f9722364f6" alt="bitopro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bitopro&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.bitopro.com"&gt;BitoPro&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/bitoex/bitopro-offical-api-docs/raw/master/v3-1/rest-1/rest.md"&gt;&lt;img src="https://img.shields.io/badge/3-lightgray" alt="API Version 3" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.bitrue.com/affiliate/landing?cn=600000&amp;amp;inviteCode=EZWETQE"&gt;&lt;img src="https://github.com/user-attachments/assets/67abe346-1273-461a-bd7c-42fa32907c8e" alt="bitrue" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bitrue&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.bitrue.com/affiliate/landing?cn=600000&amp;amp;inviteCode=EZWETQE"&gt;Bitrue&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/Bitrue-exchange/bitrue-official-api-docs"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://bitso.com/?ref=itej"&gt;&lt;img src="https://github.com/user-attachments/assets/178c8e56-9054-4107-b192-5e5053d4f975" alt="bitso" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bitso&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://bitso.com/?ref=itej"&gt;Bitso&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://bitso.com/api_info"&gt;&lt;img src="https://img.shields.io/badge/3-lightgray" alt="API Version 3" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.bitstamp.net"&gt;&lt;img src="https://github.com/user-attachments/assets/d5480572-1fee-43cb-b900-d38c522d0024" alt="bitstamp" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bitstamp&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.bitstamp.net"&gt;Bitstamp&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://www.bitstamp.net/api"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://bit.team/auth/sign-up?ref=bitboy2023"&gt;&lt;img src="https://github.com/user-attachments/assets/b41b5e0d-98e5-4bd3-8a6e-aeb230a4a135" alt="bitteam" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bitteam&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://bit.team/auth/sign-up?ref=bitboy2023"&gt;BIT.TEAM&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://bit.team/trade/api/documentation"&gt;&lt;img src="https://img.shields.io/badge/2.0.6-lightgray" alt="API Version 2.0.6" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.bittrade.co.jp/register/?invite_code=znnq3"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/85734211-85755480-b705-11ea-8b35-0b7f1db33a2f.jpg" alt="bittrade" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bittrade&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.bittrade.co.jp/register/?invite_code=znnq3"&gt;BitTrade&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://api-doc.bittrade.co.jp"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://bitvavo.com/?a=24F34952F7"&gt;&lt;img src="https://github.com/user-attachments/assets/d213155c-8c71-4701-9bd5-45351febc2a8" alt="bitvavo" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bitvavo&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://bitvavo.com/?a=24F34952F7"&gt;Bitvavo&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.bitvavo.com/"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://blockchain.com"&gt;&lt;img src="https://github.com/user-attachments/assets/975e3054-3399-4363-bcee-ec3c6d63d4e8" alt="blockchaincom" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;blockchaincom&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://blockchain.com"&gt;Blockchain.com&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://api.blockchain.com/v3"&gt;&lt;img src="https://img.shields.io/badge/3-lightgray" alt="API Version 3" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://blofin.com/register?referral_code=f79EsS"&gt;&lt;img src="https://github.com/user-attachments/assets/518cdf80-f05d-4821-a3e3-d48ceb41d73b" alt="blofin" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;blofin&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://blofin.com/register?referral_code=f79EsS"&gt;BloFin&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://blofin.com/docs"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://btc-alpha.com/?r=123788"&gt;&lt;img src="https://github.com/user-attachments/assets/dce49f3a-61e5-4ba0-a2fe-41d192fd0e5d" alt="btcalpha" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;btcalpha&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://btc-alpha.com/?r=123788"&gt;BTC-Alpha&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://btc-alpha.github.io/api-docs"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.btcbox.co.jp/"&gt;&lt;img src="https://github.com/user-attachments/assets/1e2cb499-8d0f-4f8f-9464-3c015cfbc76b" alt="btcbox" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;btcbox&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.btcbox.co.jp/"&gt;BtcBox&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://blog.btcbox.jp/en/archives/8762"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://btcmarkets.net"&gt;&lt;img src="https://github.com/user-attachments/assets/8c8d6907-3873-4cc4-ad20-e22fba28247e" alt="btcmarkets" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;btcmarkets&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://btcmarkets.net"&gt;BTC Markets&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://api.btcmarkets.net/doc/v3"&gt;&lt;img src="https://img.shields.io/badge/3-lightgray" alt="API Version 3" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.btcturk.com"&gt;&lt;img src="https://github.com/user-attachments/assets/10e0a238-9f60-4b06-9dda-edfc7602f1d6" alt="btcturk" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;btcturk&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.btcturk.com"&gt;BTCTurk&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/BTCTrader/broker-api-docs"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.bybit.com/register?affiliate_id=35953"&gt;&lt;img src="https://github.com/user-attachments/assets/97a5d0b3-de10-423d-90e1-6620960025ed" alt="bybit" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bybit&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.bybit.com/register?affiliate_id=35953"&gt;Bybit&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://bybit-exchange.github.io/docs/inverse/"&gt;&lt;img src="https://img.shields.io/badge/5-lightgray" alt="API Version 5" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://cex.io/r/0/up105393824/0/"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/27766442-8ddc33b0-5ed8-11e7-8b98-f786aef0f3c9.jpg" alt="cex" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;cex&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://cex.io/r/0/up105393824/0/"&gt;CEX.IO&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://trade.cex.io/docs/"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.coinbase.com/join/58cbe25a355148797479dbd2"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/40811661-b6eceae2-653a-11e8-829e-10bfadb078cf.jpg" alt="coinbase" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;coinbase&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.coinbase.com/join/58cbe25a355148797479dbd2"&gt;Coinbase Advanced&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://developers.coinbase.com/api/v2"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://coinbase.com/"&gt;&lt;img src="https://github.com/ccxt/ccxt/assets/43336371/34a65553-88aa-4a38-a714-064bd228b97e" alt="coinbaseexchange" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;coinbaseexchange&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://coinbase.com/"&gt;Coinbase Exchange&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.cloud.coinbase.com/exchange/docs/"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://international.coinbase.com"&gt;&lt;img src="https://github.com/ccxt/ccxt/assets/43336371/866ae638-6ab5-4ebf-ab2c-cdcce9545625" alt="coinbaseinternational" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;coinbaseinternational&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://international.coinbase.com"&gt;Coinbase International&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.cloud.coinbase.com/intx/docs"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://partner.coincatch.cc/bg/92hy70391729607848548"&gt;&lt;img src="https://github.com/user-attachments/assets/3d49065f-f05d-4573-88a2-1b5201ec6ff3" alt="coincatch" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;coincatch&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://partner.coincatch.cc/bg/92hy70391729607848548"&gt;CoinCatch&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://coincatch.github.io/github.io/en/"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://coincheck.com"&gt;&lt;img src="https://user-images.githubusercontent.com/51840849/87182088-1d6d6380-c2ec-11ea-9c64-8ab9f9b289f5.jpg" alt="coincheck" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;coincheck&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://coincheck.com"&gt;coincheck&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://coincheck.com/documents/exchange/api"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.coinex.com/register?refer_code=yw5fz"&gt;&lt;img src="https://user-images.githubusercontent.com/51840849/87182089-1e05fa00-c2ec-11ea-8da9-cc73b45abbbc.jpg" alt="coinex" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;coinex&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.coinex.com/register?refer_code=yw5fz"&gt;CoinEx&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.coinex.com/api/v2"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://coinmate.io?referral=YTFkM1RsOWFObVpmY1ZjMGREQmpTRnBsWjJJNVp3PT0"&gt;&lt;img src="https://user-images.githubusercontent.com/51840849/87460806-1c9f3f00-c616-11ea-8c46-a77018a8f3f4.jpg" alt="coinmate" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;coinmate&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://coinmate.io?referral=YTFkM1RsOWFObVpmY1ZjMGREQmpTRnBsWjJJNVp3PT0"&gt;CoinMate&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://coinmate.docs.apiary.io"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://go.coinmetro.com/?ref=crypto24"&gt;&lt;img src="https://github.com/ccxt/ccxt/assets/43336371/e86f87ec-6ba3-4410-962b-f7988c5db539" alt="coinmetro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;coinmetro&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://go.coinmetro.com/?ref=crypto24"&gt;Coinmetro&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://documenter.getpostman.com/view/3653795/SVfWN6KS"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://coinone.co.kr"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/38003300-adc12fba-323f-11e8-8525-725f53c4a659.jpg" alt="coinone" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;coinone&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://coinone.co.kr"&gt;CoinOne&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://doc.coinone.co.kr"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://coins.ph/"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/225719995-48ab2026-4ddb-496c-9da7-0d7566617c9b.jpg" alt="coinsph" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;coinsph&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://coins.ph/"&gt;Coins.ph&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://coins-docs.github.io/rest-api"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.coinspot.com.au/register?code=PJURCU"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/28208429-3cacdf9a-6896-11e7-854e-4c79a772a30f.jpg" alt="coinspot" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;coinspot&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.coinspot.com.au/register?code=PJURCU"&gt;CoinSpot&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://www.coinspot.com.au/api"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://crypto.com/exch/kdacthrnxt"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/147792121-38ed5e36-c229-48d6-b49a-48d05fc19ed4.jpeg" alt="cryptocom" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;cryptocom&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://crypto.com/exch/kdacthrnxt"&gt;Crypto.com&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://exchange-docs.crypto.com/exchange/v1/rest-ws/index.html"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://app.cryptomus.com/signup/?ref=JRP4yj"&gt;&lt;img src="https://github.com/user-attachments/assets/8e0b1c48-7c01-4177-9224-f1b01d89d7e7" alt="cryptomus" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;cryptomus&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://app.cryptomus.com/signup/?ref=JRP4yj"&gt;Cryptomus&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://doc.cryptomus.com/personal"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://app.defx.com/join/6I2CZ7"&gt;&lt;img src="https://github.com/user-attachments/assets/4e92bace-d7a9-45ea-92be-122168dc87e4" alt="defx" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;defx&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://app.defx.com/join/6I2CZ7"&gt;Defx X&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.defx.com/docs"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/DEX-blue.svg?sanitize=true" alt="DEX - Distributed EXchange" title="DEX - Distributed EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.delta.exchange/app/signup/?code=IULYNB"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/99450025-3be60a00-2931-11eb-9302-f4fd8d8589aa.jpg" alt="delta" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;delta&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.delta.exchange/app/signup/?code=IULYNB"&gt;Delta Exchange&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.delta.exchange"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.deribit.com/reg-1189.4038"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/41933112-9e2dd65a-798b-11e8-8440-5bab2959fcb8.jpg" alt="deribit" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;deribit&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.deribit.com/reg-1189.4038"&gt;Deribit&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.deribit.com/v2"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.derive.xyz/invite/3VB0B"&gt;&lt;img src="https://github.com/user-attachments/assets/f835b95f-033a-43dd-b6bb-24e698fc498c" alt="derive" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;derive&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.derive.xyz/invite/3VB0B"&gt;derive&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.derive.xyz/docs/"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/DEX-blue.svg?sanitize=true" alt="DEX - Distributed EXchange" title="DEX - Distributed EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.digifinex.com/en-ww/from/DhOzBg?channelCode=ljaUPp"&gt;&lt;img src="https://user-images.githubusercontent.com/51840849/87443315-01283a00-c5fe-11ea-8628-c2a0feaf07ac.jpg" alt="digifinex" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;digifinex&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.digifinex.com/en-ww/from/DhOzBg?channelCode=ljaUPp"&gt;DigiFinex&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.digifinex.com"&gt;&lt;img src="https://img.shields.io/badge/3-lightgray" alt="API Version 3" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://exmo.me/?ref=131685"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/27766491-1b0ea956-5eda-11e7-9225-40d67b481b8d.jpg" alt="exmo" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;exmo&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://exmo.me/?ref=131685"&gt;EXMO&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://exmo.me/en/api_doc?ref=131685"&gt;&lt;img src="https://img.shields.io/badge/1.1-lightgray" alt="API Version 1.1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://fmfw.io/referral/da948b21d6c92d69"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/159177712-b685b40c-5269-4cea-ac83-f7894c49525d.jpg" alt="fmfwio" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;fmfwio&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://fmfw.io/referral/da948b21d6c92d69"&gt;FMFW.io&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://api.fmfw.io/"&gt;&lt;img src="https://img.shields.io/badge/3-lightgray" alt="API Version 3" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://app.foxbit.com.br"&gt;&lt;img src="https://github.com/user-attachments/assets/1f8faca2-ae2f-4222-b33e-5671e7d873dd" alt="foxbit" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;foxbit&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://app.foxbit.com.br"&gt;Foxbit&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.foxbit.com.br"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.gate.io/signup/2436035"&gt;&lt;img src="https://github.com/user-attachments/assets/64f988c5-07b6-4652-b5c1-679a6bf67c85" alt="gate" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;gate&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.gate.io/signup/2436035"&gt;Gate.io&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://www.gate.io/docs/developers/apiv4/en/"&gt;&lt;img src="https://img.shields.io/badge/4-lightgray" alt="API Version 4" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://gemini.com/"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/27816857-ce7be644-6096-11e7-82d6-3c257263229c.jpg" alt="gemini" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;gemini&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://gemini.com/"&gt;Gemini&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.gemini.com/rest-api"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://global.hashkey.com/en-US/register/invite?invite_code=82FQUN"&gt;&lt;img src="https://github.com/user-attachments/assets/6dd6127b-cc19-4a13-9b29-a98d81f80e98" alt="hashkey" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;hashkey&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://global.hashkey.com/en-US/register/invite?invite_code=82FQUN"&gt;HashKey Global&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://hashkeyglobal-apidoc.readme.io/"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/ccxt/ccxt/master/hibachi.xyz/r/ZBL2YFWIHU"&gt;&lt;img src="https://github.com/user-attachments/assets/7301bbb1-4f27-4167-8a55-75f74b14e973" alt="hibachi" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;hibachi&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/ccxt/ccxt/master/hibachi.xyz/r/ZBL2YFWIHU"&gt;Hibachi&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/ccxt/ccxt/master/undefined"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/DEX-blue.svg?sanitize=true" alt="DEX - Distributed EXchange" title="DEX - Distributed EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://hitbtc.com/?ref_id=5a5d39a65d466"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/27766555-8eaec20e-5edc-11e7-9c5b-6dc69fc42f5e.jpg" alt="hitbtc" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;hitbtc&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://hitbtc.com/?ref_id=5a5d39a65d466"&gt;HitBTC&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://api.hitbtc.com"&gt;&lt;img src="https://img.shields.io/badge/3-lightgray" alt="API Version 3" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://pro.hollaex.com/signup?affiliation_code=QSWA6G"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/75841031-ca375180-5ddd-11ea-8417-b975674c23cb.jpg" alt="hollaex" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;hollaex&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://pro.hollaex.com/signup?affiliation_code=QSWA6G"&gt;HollaEx&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://apidocs.hollaex.com"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.htx.com.vc/invite/en-us/1h?invite_code=6rmm2223"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/76137448-22748a80-604e-11ea-8069-6e389271911d.jpg" alt="htx" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;htx&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.htx.com.vc/invite/en-us/1h?invite_code=6rmm2223"&gt;HTX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huobiapi.github.io/docs/spot/v1/en/"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://app.hyperliquid.xyz/"&gt;&lt;img src="https://github.com/ccxt/ccxt/assets/43336371/b371bc6c-4a8c-489f-87f4-20a913dd8d4b" alt="hyperliquid" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;hyperliquid&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://app.hyperliquid.xyz/"&gt;Hyperliquid&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://hyperliquid.gitbook.io/hyperliquid-docs/for-developers/api"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/DEX-blue.svg?sanitize=true" alt="DEX - Distributed EXchange" title="DEX - Distributed EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.independentreserve.com"&gt;&lt;img src="https://user-images.githubusercontent.com/51840849/87182090-1e9e9080-c2ec-11ea-8e49-563db9a38f37.jpg" alt="independentreserve" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;independentreserve&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.independentreserve.com"&gt;Independent Reserve&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://www.independentreserve.com/API"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://indodax.com/ref/testbitcoincoid/1"&gt;&lt;img src="https://user-images.githubusercontent.com/51840849/87070508-9358c880-c221-11ea-8dc5-5391afbbb422.jpg" alt="indodax" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;indodax&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://indodax.com/ref/testbitcoincoid/1"&gt;INDODAX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/btcid/indodax-official-api-docs"&gt;&lt;img src="https://img.shields.io/badge/2.0-lightgray" alt="API Version 2.0" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.kraken.com"&gt;&lt;img src="https://user-images.githubusercontent.com/51840849/76173629-fc67fb00-61b1-11ea-84fe-f2de582f58a3.jpg" alt="kraken" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;kraken&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.kraken.com"&gt;Kraken&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.kraken.com/rest/"&gt;&lt;img src="https://img.shields.io/badge/0-lightgray" alt="API Version 0" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://futures.kraken.com/"&gt;&lt;img src="https://user-images.githubusercontent.com/24300605/81436764-b22fd580-9172-11ea-9703-742783e6376d.jpg" alt="krakenfutures" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;krakenfutures&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://futures.kraken.com/"&gt;Kraken Futures&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.kraken.com/api/docs/futures-api/trading/market-data/"&gt;&lt;img src="https://img.shields.io/badge/3-lightgray" alt="API Version 3" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.kucoin.com/ucenter/signup?rcode=E5wkqe"&gt;&lt;img src="https://user-images.githubusercontent.com/51840849/87295558-132aaf80-c50e-11ea-9801-a2fb0c57c799.jpg" alt="kucoin" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;kucoin&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.kucoin.com/ucenter/signup?rcode=E5wkqe"&gt;KuCoin&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.kucoin.com"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://futures.kucoin.com/?rcode=E5wkqe"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/147508995-9e35030a-d046-43a1-a006-6fabd981b554.jpg" alt="kucoinfutures" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;kucoinfutures&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://futures.kucoin.com/?rcode=E5wkqe"&gt;KuCoin Futures&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.kucoin.com/futures"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://latoken.com/invite?r=mvgp2djk"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/61511972-24c39f00-aa01-11e9-9f7c-471f1d6e5214.jpg" alt="latoken" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;latoken&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://latoken.com/invite?r=mvgp2djk"&gt;Latoken&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://api.latoken.com"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.lbank.com/login/?icode=7QCY"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/38063602-9605e28a-3302-11e8-81be-64b1e53c4cfb.jpg" alt="lbank" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;lbank&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.lbank.com/login/?icode=7QCY"&gt;LBank&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://www.lbank.com/en-US/docs/index.html"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.luno.com/invite/44893A"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/27766607-8c1a69d8-5ede-11e7-930c-540b5eb9be24.jpg" alt="luno" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;luno&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.luno.com/invite/44893A"&gt;luno&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://www.luno.com/en/api"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.mercadobitcoin.com.br"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/27837060-e7c58714-60ea-11e7-9192-f05e86adb83f.jpg" alt="mercado" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;mercado&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.mercadobitcoin.com.br"&gt;Mercado Bitcoin&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://www.mercadobitcoin.com.br/api-doc"&gt;&lt;img src="https://img.shields.io/badge/3-lightgray" alt="API Version 3" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.mexc.com/register?inviteCode=mexc-1FQ1GNu1"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/137283979-8b2a818d-8633-461b-bfca-de89e8c446b2.jpg" alt="mexc" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;mexc&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.mexc.com/register?inviteCode=mexc-1FQ1GNu1"&gt;MEXC Global&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://mexcdevelop.github.io/apidocs/"&gt;&lt;img src="https://img.shields.io/badge/3-lightgray" alt="API Version 3" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://trade.mode.network?ref=MODETRADE"&gt;&lt;img src="https://github.com/user-attachments/assets/cec2b7f1-3b2b-4502-971b-447ee1937d6b" alt="modetrade" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;modetrade&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://trade.mode.network?ref=MODETRADE"&gt;Mode Trade&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/ccxt/ccxt/master/undefined"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/DEX-blue.svg?sanitize=true" alt="DEX - Distributed EXchange" title="DEX - Distributed EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.my.okx.com/join/CCXT2023"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/152485636-38b19e4a-bece-4dec-979a-5982859ffc04.jpg" alt="myokx" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;myokx&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.my.okx.com/join/CCXT2023"&gt;MyOKX (EEA)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://my.okx.com/docs-v5/en/#overview"&gt;&lt;img src="https://img.shields.io/badge/5-lightgray" alt="API Version 5" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://one.ndax.io/bfQiSL"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/108623144-67a3ef00-744e-11eb-8140-75c6b851e945.jpg" alt="ndax" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ndax&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://one.ndax.io/bfQiSL"&gt;NDAX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://apidoc.ndax.io/"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.novadax.com.br/?s=ccxt"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/92337550-2b085500-f0b3-11ea-98e7-5794fb07dd3b.jpg" alt="novadax" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;novadax&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.novadax.com.br/?s=ccxt"&gt;NovaDAX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://doc.novadax.com/pt-BR/"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://oceanex.pro/signup?referral=VE24QX"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/58385970-794e2d80-8001-11e9-889c-0567cd79b78e.jpg" alt="oceanex" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;oceanex&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://oceanex.pro/signup?referral=VE24QX"&gt;OceanEx&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://api.oceanex.pro/doc/v1"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.okcoin.com/account/register?flag=activity&amp;amp;channelId=600001513"&gt;&lt;img src="https://user-images.githubusercontent.com/51840849/87295551-102fbf00-c50e-11ea-90a9-462eebba5829.jpg" alt="okcoin" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;okcoin&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.okcoin.com/account/register?flag=activity&amp;amp;channelId=600001513"&gt;OKCoin&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://www.okcoin.com/docs/en/"&gt;&lt;img src="https://img.shields.io/badge/5-lightgray" alt="API Version 5" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.okx.com/join/CCXT2023"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/152485636-38b19e4a-bece-4dec-979a-5982859ffc04.jpg" alt="okx" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;okx&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.okx.com/join/CCXT2023"&gt;OKX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://www.okx.com/docs-v5/en/"&gt;&lt;img src="https://img.shields.io/badge/5-lightgray" alt="API Version 5" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.app.okx.com/join/CCXT2023"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/152485636-38b19e4a-bece-4dec-979a-5982859ffc04.jpg" alt="okxus" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;okxus&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.app.okx.com/join/CCXT2023"&gt;OKX (US)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://app.okx.com/docs-v5/en/#overview"&gt;&lt;img src="https://img.shields.io/badge/5-lightgray" alt="API Version 5" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://onetrading.com/"&gt;&lt;img src="https://github.com/ccxt/ccxt/assets/43336371/bdbc26fd-02f2-4ca7-9f1e-17333690bb1c" alt="onetrading" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;onetrading&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://onetrading.com/"&gt;One Trading&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.onetrading.com"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://ox.fun/register?shareAccountId=5ZUD4a7G"&gt;&lt;img src="https://github.com/ccxt/ccxt/assets/43336371/6a196124-c1ee-4fae-8573-962071b61a85" alt="oxfun" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;oxfun&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ox.fun/register?shareAccountId=5ZUD4a7G"&gt;OXFUN&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.ox.fun/"&gt;&lt;img src="https://img.shields.io/badge/3-lightgray" alt="API Version 3" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://p2pb2b.com?referral=ee784c53"&gt;&lt;img src="https://github.com/ccxt/ccxt/assets/43336371/8da13a80-1f0a-49be-bb90-ff8b25164755" alt="p2b" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;p2b&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://p2pb2b.com?referral=ee784c53"&gt;p2b&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/P2B-team/p2b-api-docs/raw/master/api-doc.md"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://app.paradex.trade/r/ccxt24"&gt;&lt;img src="https://github.com/user-attachments/assets/84628770-784e-4ec4-a759-ec2fbb2244ea" alt="paradex" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;paradex&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://app.paradex.trade/r/ccxt24"&gt;Paradex&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.api.testnet.paradex.trade/"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/DEX-blue.svg?sanitize=true" alt="DEX - Distributed EXchange" title="DEX - Distributed EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.paymium.com/page/sign-up?referral=eDAzPoRQFMvaAB8sf-qj"&gt;&lt;img src="https://user-images.githubusercontent.com/51840849/87153930-f0f02200-c2c0-11ea-9c0a-40337375ae89.jpg" alt="paymium" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;paymium&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.paymium.com/page/sign-up?referral=eDAzPoRQFMvaAB8sf-qj"&gt;Paymium&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/Paymium/api-documentation"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://phemex.com/register?referralCode=EDNVJ"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/85225056-221eb600-b3d7-11ea-930d-564d2690e3f6.jpg" alt="phemex" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;phemex&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://phemex.com/register?referralCode=EDNVJ"&gt;Phemex&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://phemex-docs.github.io/#overview"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://poloniex.com/signup?c=UBFZJRPJ"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/27766817-e9456312-5ee6-11e7-9b3c-b628ca5626a5.jpg" alt="poloniex" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;poloniex&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://poloniex.com/signup?c=UBFZJRPJ"&gt;Poloniex&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://api-docs.poloniex.com/spot/"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.probit.com/r/34608773"&gt;&lt;img src="https://user-images.githubusercontent.com/51840849/79268032-c4379480-7ea2-11ea-80b3-dd96bb29fd0d.jpg" alt="probit" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;probit&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.probit.com/r/34608773"&gt;ProBit&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs-en.probit.com"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://timex.io/?refcode=1x27vNkTbP1uwkCck"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/70423869-6839ab00-1a7f-11ea-8f94-13ae72c31115.jpg" alt="timex" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;timex&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://timex.io/?refcode=1x27vNkTbP1uwkCck"&gt;TimeX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://plasma-relay-backend.timex.io/swagger-ui/index.html"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://tokocrypto.com"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/183870484-d3398d0c-f6a1-4cce-91b8-d58792308716.jpg" alt="tokocrypto" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;tokocrypto&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://tokocrypto.com"&gt;Tokocrypto&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://www.tokocrypto.com/apidocs/"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://tradeogre.com"&gt;&lt;img src="https://github.com/ccxt/ccxt/assets/43336371/3aa748b7-ea44-45e9-a9e7-b1d207a2578a" alt="tradeogre" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;tradeogre&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://tradeogre.com"&gt;tradeogre&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://tradeogre.com/help/api"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://upbit.com"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/49245610-eeaabe00-f423-11e8-9cba-4b0aed794799.jpg" alt="upbit" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;upbit&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://upbit.com"&gt;Upbit&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.upbit.com/docs/%EC%9A%94%EC%B2%AD-%EC%88%98-%EC%A0%9C%ED%95%9C"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://wx.network"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/84547058-5fb27d80-ad0b-11ea-8711-78ac8b3c7f31.jpg" alt="wavesexchange" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;wavesexchange&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://wx.network"&gt;Waves.Exchange&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.wx.network"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/DEX-blue.svg?sanitize=true" alt="DEX - Distributed EXchange" title="DEX - Distributed EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://whitebit.com/referral/d9bdf40e-28f2-4b52-b2f9-cd1415d82963"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/66732963-8eb7dd00-ee66-11e9-849b-10d9282bb9e0.jpg" alt="whitebit" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;whitebit&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://whitebit.com/referral/d9bdf40e-28f2-4b52-b2f9-cd1415d82963"&gt;WhiteBit&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/whitebit-exchange/api-docs"&gt;&lt;img src="https://img.shields.io/badge/4-lightgray" alt="API Version 4" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://woox.io/register?ref=DIJT0CNL"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/150730761-1a00e5e0-d28c-480f-9e65-089ce3e6ef3b.jpg" alt="woo" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;woo&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://woox.io/register?ref=DIJT0CNL"&gt;WOO X&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.woox.io/"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://dex.woo.org/en/trade?ref=CCXT"&gt;&lt;img src="https://github.com/user-attachments/assets/9ba21b8a-a9c7-4770-b7f1-ce3bcbde68c1" alt="woofipro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;woofipro&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://dex.woo.org/en/trade?ref=CCXT"&gt;WOOFI PRO&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://orderly.network/docs/build-on-evm/building-on-evm"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/DEX-blue.svg?sanitize=true" alt="DEX - Distributed EXchange" title="DEX - Distributed EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.xt.com/en/accounts/register?ref=9PTM9VW"&gt;&lt;img src="https://user-images.githubusercontent.com/14319357/232636712-466df2fc-560a-4ca4-aab2-b1d954a58e24.jpg" alt="xt" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;xt&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.xt.com/en/accounts/register?ref=9PTM9VW"&gt;XT&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://doc.xt.com/"&gt;&lt;img src="https://img.shields.io/badge/4-lightgray" alt="API Version 4" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.yobit.net"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/27766910-cdcbfdae-5eea-11e7-9859-03fea873272d.jpg" alt="yobit" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;yobit&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.yobit.net"&gt;YoBit&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://www.yobit.net/en/api/"&gt;&lt;img src="https://img.shields.io/badge/3-lightgray" alt="API Version 3" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://zaif.jp"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/27766927-39ca2ada-5eeb-11e7-972f-1b4199518ca6.jpg" alt="zaif" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;zaif&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://zaif.jp"&gt;Zaif&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://techbureau-api-document.readthedocs.io/ja/latest/index.html"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://auth.zondaglobal.com/ref/jHlbB4mIkdS1"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/159202310-a0e38007-5e7c-4ba9-a32f-c8263a0291fe.jpg" alt="zonda" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;zonda&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://auth.zondaglobal.com/ref/jHlbB4mIkdS1"&gt;Zonda&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.zondacrypto.exchange/"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX – Centralized EXchange" title="CEX – Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;!-- end list --&gt; 
&lt;p&gt;The list above is updated frequently, new crypto markets, exchanges, bug fixes, and API endpoints are introduced on a regular basis. See the &lt;a href="https://github.com/ccxt/ccxt/wiki/"&gt;Manual&lt;/a&gt; for more details. If you can't find a cryptocurrency exchange in the list above and want it to be added, post a link to it by opening an issue here on GitHub or send us an email.&lt;/p&gt; 
&lt;p&gt;The library is under &lt;a href="https://github.com/ccxt/ccxt/raw/master/LICENSE.txt"&gt;MIT license&lt;/a&gt;, that means it's absolutely free for any developer to build commercial and opensource software on top of it, but use it at your own risk with no warranties, as is.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Install&lt;/h2&gt; 
&lt;p&gt;The easiest way to install the CCXT library is to use a package manager:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.npmjs.com/package/ccxt"&gt;ccxt in &lt;strong&gt;NPM&lt;/strong&gt;&lt;/a&gt; (JavaScript / Node v7.6+)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pypi.python.org/pypi/ccxt"&gt;ccxt in &lt;strong&gt;PyPI&lt;/strong&gt;&lt;/a&gt; (Python 3.7.0+)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://packagist.org/packages/ccxt/ccxt"&gt;ccxt in &lt;strong&gt;Packagist/Composer&lt;/strong&gt;&lt;/a&gt; (PHP 8.1+)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.nuget.org/packages/ccxt"&gt;ccxt in &lt;strong&gt;Nuget&lt;/strong&gt;&lt;/a&gt; (netstandard 2.0)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pkg.go.dev/github.com/ccxt/ccxt/go/v4"&gt;ccxt in &lt;strong&gt;GO&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This library is shipped as an all-in-one module implementation with minimalistic dependencies and requirements:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ccxt/ccxt/raw/master/js/"&gt;js/&lt;/a&gt; in JavaScript&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ccxt/ccxt/raw/master/python/"&gt;python/&lt;/a&gt; in Python (generated from TS)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ccxt/ccxt/raw/master/php/"&gt;php/&lt;/a&gt; in PHP (generated from TS)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ccxt/ccxt/raw/master/cs/"&gt;cs/&lt;/a&gt; in C# (generated from TS)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ccxt/ccxt/raw/master/go/"&gt;go/&lt;/a&gt; in Go (generated from TS)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can also clone it into your project directory from &lt;a href="https://github.com/ccxt/ccxt"&gt;ccxt GitHub repository&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;git clone https://github.com/ccxt/ccxt.git  # including 1GB of commit history

# or

git clone https://github.com/ccxt/ccxt.git --depth 1  # avoid downloading 1GB of commit history
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;JavaScript (NPM)&lt;/h3&gt; 
&lt;p&gt;JavaScript version of CCXT works in both Node and web browsers. Requires ES6 and &lt;code&gt;async/await&lt;/code&gt; syntax support (Node 7.6.0+). When compiling with Webpack and Babel, make sure it is &lt;a href="https://github.com/ccxt/ccxt/issues/225#issuecomment-331905178"&gt;not excluded&lt;/a&gt; in your &lt;code&gt;babel-loader&lt;/code&gt; config.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.npmjs.com/package/ccxt"&gt;ccxt in &lt;strong&gt;NPM&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;npm install ccxt
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-JavaScript"&gt;//cjs
var ccxt = require ('ccxt')
console.log (ccxt.exchanges) // print all available exchanges
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-Javascript"&gt;//esm
import {version, exchanges} from 'ccxt';
console.log(version, Object.keys(exchanges));
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;JavaScript (for use with the &lt;code&gt;&amp;lt;script&amp;gt;&lt;/code&gt; tag):&lt;/h3&gt; 
&lt;p&gt;All-in-one browser bundle (dependencies included), served from a CDN of your choice:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;jsDelivr: &lt;a href="https://cdn.jsdelivr.net/npm/ccxt@4.5.5/dist/ccxt.browser.min.js"&gt;https://cdn.jsdelivr.net/npm/ccxt@4.5.5/dist/ccxt.browser.min.js&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;unpkg: &lt;a href="https://unpkg.com/ccxt@4.5.5/dist/ccxt.browser.min.js"&gt;https://unpkg.com/ccxt@4.5.5/dist/ccxt.browser.min.js&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;CDNs are not updated in real-time and may have delays. Defaulting to the most recent version without specifying the version number is not recommended. Please, keep in mind that we are not responsible for the correct operation of those CDN servers.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-HTML"&gt;&amp;lt;script type="text/javascript" src="https://cdn.jsdelivr.net/npm/ccxt@4.5.5/dist/ccxt.browser.min.js"&amp;gt;&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Creates a global &lt;code&gt;ccxt&lt;/code&gt; object:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-JavaScript"&gt;console.log (ccxt.exchanges) // print all available exchanges
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Python&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://pypi.python.org/pypi/ccxt"&gt;ccxt in &lt;strong&gt;PyPI&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install ccxt
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-Python"&gt;import ccxt
print(ccxt.exchanges) # print a list of all available exchange classes
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The library supports concurrent asynchronous mode with asyncio and async/await in Python 3.7.0+&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-Python"&gt;import ccxt.async_support as ccxt # link against the asynchronous version of ccxt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;orjson support&lt;/h4&gt; 
&lt;p&gt;CCXT also supports &lt;code&gt;orjson&lt;/code&gt; for parsing JSON since it is much faster than the builtin library. This is especially important when using websockets because some exchanges return big messages that need to be parsed and dispatched as quickly as possible.&lt;/p&gt; 
&lt;p&gt;However, &lt;code&gt;orjson&lt;/code&gt; is not enabled by default because it is not supported by every python interpreter. If you want to opt-in, you just need to install it (&lt;code&gt;pip install orjson&lt;/code&gt;) on your local environment. CCXT will detect the installion and pick it up automatically.&lt;/p&gt; 
&lt;h3&gt;PHP&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://packagist.org/packages/ccxt/ccxt"&gt;ccxt in PHP with &lt;strong&gt;Packagist/Composer&lt;/strong&gt;&lt;/a&gt; (PHP 8.1+)&lt;/p&gt; 
&lt;p&gt;It requires common PHP modules:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;cURL&lt;/li&gt; 
 &lt;li&gt;mbstring (using UTF-8 is highly recommended)&lt;/li&gt; 
 &lt;li&gt;PCRE&lt;/li&gt; 
 &lt;li&gt;iconv&lt;/li&gt; 
 &lt;li&gt;gmp&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-PHP"&gt;include "ccxt.php";
var_dump (\ccxt\Exchange::$exchanges); // print a list of all available exchange classes
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The library supports concurrent asynchronous mode using tools from &lt;a href="https://reactphp.org/"&gt;ReactPHP&lt;/a&gt; in PHP 8.1+. Read the &lt;a href="https://github.com/ccxt/ccxt/wiki/"&gt;Manual&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h3&gt;.net/C#&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://www.nuget.org/packages/ccxt"&gt;ccxt in C# with &lt;strong&gt;Nuget&lt;/strong&gt;&lt;/a&gt; (netstandard 2.0 and netstandard 2.1)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-c#"&gt;using ccxt;
Console.WriteLine(ccxt.Exchanges) // check this later
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Go&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://pkg.go.dev/github.com/ccxt/ccxt/go/v4"&gt;ccxt in GO with &lt;strong&gt;PKG&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;go install github.com/ccxt/ccxt/go/v4@latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-Go"&gt;import "ccxt"
fmt.Println(ccxt.Exchanges)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Docker&lt;/h3&gt; 
&lt;p&gt;You can get CCXT installed in a container along with all the supported languages and dependencies. This may be useful if you want to contribute to CCXT (e.g. run the build scripts and tests — please see the &lt;a href="https://github.com/ccxt/ccxt/raw/master/CONTRIBUTING.md"&gt;Contributing&lt;/a&gt; document for the details on that).&lt;/p&gt; 
&lt;p&gt;Using &lt;code&gt;docker-compose&lt;/code&gt; (in the cloned CCXT repository):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;docker-compose run --rm ccxt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You don't need the Docker image if you're not going to develop CCXT. If you just want to use CCXT –&amp;nbsp;just install it as a regular package into your project.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Intro&lt;/h3&gt; 
&lt;p&gt;The CCXT library consists of a public part and a private part. Anyone can use the public part immediately after installation. Public APIs provide unrestricted access to public information for all exchange markets without the need to register a user account or have an API key.&lt;/p&gt; 
&lt;p&gt;Public APIs include the following:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;market data&lt;/li&gt; 
 &lt;li&gt;instruments/trading pairs&lt;/li&gt; 
 &lt;li&gt;price feeds (exchange rates)&lt;/li&gt; 
 &lt;li&gt;order books&lt;/li&gt; 
 &lt;li&gt;trade history&lt;/li&gt; 
 &lt;li&gt;tickers&lt;/li&gt; 
 &lt;li&gt;OHLC(V) for charting&lt;/li&gt; 
 &lt;li&gt;other public endpoints&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;In order to trade with private APIs you need to obtain API keys from an exchange's website. It usually means signing up to the exchange and creating API keys for your account. Some exchanges require personal info or identification. Sometimes verification may be necessary as well. In this case you will need to register yourself, this library will not create accounts or API keys for you. Some exchanges expose API endpoints for registering an account, but most exchanges don't. You will have to sign up and create API keys on their websites.&lt;/p&gt; 
&lt;p&gt;Private APIs allow the following:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;manage personal account info&lt;/li&gt; 
 &lt;li&gt;query account balances&lt;/li&gt; 
 &lt;li&gt;trade by making market and limit orders&lt;/li&gt; 
 &lt;li&gt;deposit and withdraw fiat and crypto funds&lt;/li&gt; 
 &lt;li&gt;query personal orders&lt;/li&gt; 
 &lt;li&gt;get ledger history&lt;/li&gt; 
 &lt;li&gt;transfer funds between accounts&lt;/li&gt; 
 &lt;li&gt;use merchant services&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This library implements full public and private REST and WebSocket APIs for all exchanges in TypeScript, JavaScript, PHP and Python.&lt;/p&gt; 
&lt;p&gt;The CCXT library supports both camelcase notation (preferred in TypeScript and JavaScript) and underscore notation (preferred in Python and PHP), therefore all methods can be called in either notation or coding style in any language.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-JavaScript"&gt;// both of these notations work in JavaScript/Python/PHP
exchange.methodName ()  // camelcase pseudocode
exchange.method_name () // underscore pseudocode
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Read the &lt;a href="https://github.com/ccxt/ccxt/wiki/"&gt;Manual&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h3&gt;JavaScript&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;CCXT now supports ESM and CJS modules&lt;/strong&gt;&lt;/p&gt; 
&lt;h4&gt;CJS&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-JavaScript"&gt;// cjs example
'use strict';
const ccxt = require ('ccxt');

(async function () {
    let kraken    = new ccxt.kraken ()
    let bitfinex  = new ccxt.bitfinex ({ verbose: true })
    let huobipro  = new ccxt.huobipro ()
    let okcoinusd = new ccxt.okcoin ({
        apiKey: 'YOUR_PUBLIC_API_KEY',
        secret: 'YOUR_SECRET_PRIVATE_KEY',
    })

    const exchangeId = 'binance'
        , exchangeClass = ccxt[exchangeId]
        , exchange = new exchangeClass ({
            'apiKey': 'YOUR_API_KEY',
            'secret': 'YOUR_SECRET',
        })

    console.log (kraken.id,    await kraken.loadMarkets ())
    console.log (bitfinex.id,  await bitfinex.loadMarkets  ())
    console.log (huobipro.id,  await huobipro.loadMarkets ())

    console.log (kraken.id,    await kraken.fetchOrderBook (kraken.symbols[0]))
    console.log (bitfinex.id,  await bitfinex.fetchTicker ('BTC/USD'))
    console.log (huobipro.id,  await huobipro.fetchTrades ('ETH/USDT'))

    console.log (okcoinusd.id, await okcoinusd.fetchBalance ())

    // sell 1 BTC/USD for market price, sell a bitcoin for dollars immediately
    console.log (okcoinusd.id, await okcoinusd.createMarketSellOrder ('BTC/USD', 1))

    // buy 1 BTC/USD for $2500, you pay $2500 and receive ฿1 when the order is closed
    console.log (okcoinusd.id, await okcoinusd.createLimitBuyOrder ('BTC/USD', 1, 2500.00))

    // pass/redefine custom exchange-specific order params: type, amount, price or whatever
    // use a custom order type
    bitfinex.createLimitSellOrder ('BTC/USD', 1, 10, { 'type': 'trailing-stop' })

}) ();
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;ESM&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-Javascript"&gt;//esm example
import {version, binance} from 'ccxt';

console.log(version);
const exchange = new binance();
const ticker = await exchange.fetchTicker('BTC/USDT');
console.log(ticker);
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Python&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-Python"&gt;# coding=utf-8

import ccxt

hitbtc   = ccxt.hitbtc({'verbose': True})
bitmex   = ccxt.bitmex()
huobipro = ccxt.huobipro()
exmo     = ccxt.exmo({
    'apiKey': 'YOUR_PUBLIC_API_KEY',
    'secret': 'YOUR_SECRET_PRIVATE_KEY',
})
kraken = ccxt.kraken({
    'apiKey': 'YOUR_PUBLIC_API_KEY',
    'secret': 'YOUR_SECRET_PRIVATE_KEY',
})

exchange_id = 'binance'
exchange_class = getattr(ccxt, exchange_id)
exchange = exchange_class({
    'apiKey': 'YOUR_API_KEY',
    'secret': 'YOUR_SECRET',
})

hitbtc_markets = hitbtc.load_markets()

print(hitbtc.id, hitbtc_markets)
print(bitmex.id, bitmex.load_markets())
print(huobipro.id, huobipro.load_markets())

print(hitbtc.fetch_order_book(hitbtc.symbols[0]))
print(bitmex.fetch_ticker('BTC/USD'))
print(huobipro.fetch_trades('LTC/USDT'))

print(exmo.fetch_balance())

# sell one ฿ for market price and receive $ right now
print(exmo.id, exmo.create_market_sell_order('BTC/USD', 1))

# limit buy BTC/EUR, you pay €2500 and receive ฿1  when the order is closed
print(exmo.id, exmo.create_limit_buy_order('BTC/EUR', 1, 2500.00))

# pass/redefine custom exchange-specific order params: type, amount, price, flags, etc...
kraken.create_market_buy_order('BTC/USD', 1, {'trading_agreement': 'agree'})
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;PHP&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-PHP"&gt;include 'ccxt.php';

$poloniex = new \ccxt\poloniex ();
$bittrex  = new \ccxt\bittrex  (array ('verbose' =&amp;gt; true));
$quoinex  = new \ccxt\quoinex   ();
$zaif     = new \ccxt\zaif     (array (
    'apiKey' =&amp;gt; 'YOUR_PUBLIC_API_KEY',
    'secret' =&amp;gt; 'YOUR_SECRET_PRIVATE_KEY',
));
$hitbtc   = new \ccxt\hitbtc   (array (
    'apiKey' =&amp;gt; 'YOUR_PUBLIC_API_KEY',
    'secret' =&amp;gt; 'YOUR_SECRET_PRIVATE_KEY',
));

$exchange_id = 'binance';
$exchange_class = "\\ccxt\\$exchange_id";
$exchange = new $exchange_class (array (
    'apiKey' =&amp;gt; 'YOUR_API_KEY',
    'secret' =&amp;gt; 'YOUR_SECRET',
));

$poloniex_markets = $poloniex-&amp;gt;load_markets ();

var_dump ($poloniex_markets);
var_dump ($bittrex-&amp;gt;load_markets ());
var_dump ($quoinex-&amp;gt;load_markets ());

var_dump ($poloniex-&amp;gt;fetch_order_book ($poloniex-&amp;gt;symbols[0]));
var_dump ($bittrex-&amp;gt;fetch_trades ('BTC/USD'));
var_dump ($quoinex-&amp;gt;fetch_ticker ('ETH/EUR'));
var_dump ($zaif-&amp;gt;fetch_ticker ('BTC/JPY'));

var_dump ($zaif-&amp;gt;fetch_balance ());

// sell 1 BTC/JPY for market price, you pay ¥ and receive ฿ immediately
var_dump ($zaif-&amp;gt;id, $zaif-&amp;gt;create_market_sell_order ('BTC/JPY', 1));

// buy BTC/JPY, you receive ฿1 for ¥285000 when the order closes
var_dump ($zaif-&amp;gt;id, $zaif-&amp;gt;create_limit_buy_order ('BTC/JPY', 1, 285000));

// set a custom user-defined id to your order
$hitbtc-&amp;gt;create_order ('BTC/USD', 'limit', 'buy', 1, 3000, array ('clientOrderId' =&amp;gt; '123'));
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;.net/C#&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-C#"&gt;using ccxt; // importing ccxt
namespace Project;
class Project {
    public async static Task CreateOrder() {
        var exchange = new Binance();
        exchange.apiKey = "my api key";
        exchange.secret = "my secret";
        // always use the capitalized method (CreateOrder instead of createOrder)
        var order = await exchange.CreateOrder("BTC/USDT", "limit", "buy", 1, 50);
        Console.WriteLine("Placed Order, order id: " + order.id);
    }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Go&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-Go"&gt;package main
import (
	"github.com/ccxt/ccxt/go/v4/go"
	"fmt"
)

func main() {
	exchange := ccxt.NewBinance(map[string]interface{}{
		"apiKey": "MY KEY",
		"secret": "MY SECRET",
	})
	orderParams := map[string]interface{}{
		"clientOrderId": "myOrderId68768678",
	}

    &amp;lt;-exchange.LoadMarkets()

	order, err := exchange.CreateOrder("BTC/USDT", "limit", "buy", 0.001, ccxt.WithCreateOrderPrice(6000), ccxt.WithCreateOrderParams(orderParams))
	if err != nil {
		if ccxtError, ok := err.(*ccxt.Error); ok {
			if ccxtError.Type == "InvalidOrder" {
				fmt.Println("Invalid order")
			} else {
				fmt.Println("Some other error")
			}
		}
	} else {
		fmt.Println(*order.Id)
	}


    // fetching OHLCV
	ohlcv, err := exchange.FetchOHLCV("BTC/USDT", ccxt.WithFetchOHLCVTimeframe("5m"), ccxt.WithFetchOHLCVLimit(100))

	if err != nil {
		fmt.Println("Error: ", err)
	} else {
		fmt.Println("Got OHLCV!")
	}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Optional parameters&lt;/h4&gt; 
&lt;p&gt;Unlike Javascript/Python/PHP/C# Go does not support "traditional" optional parameters like &lt;code&gt;function a(optional = false)&lt;/code&gt;. However, the CCXT language and structure have some methods with optional params, and since the Go language is transpiled from the Typescript source, we had to find a way of representing them.&lt;/p&gt; 
&lt;p&gt;We have decided to "go" (pun intended) with Option structs and the &lt;code&gt;WithX&lt;/code&gt; methods.&lt;/p&gt; 
&lt;p&gt;For example, this function &lt;code&gt;FetchMyTrades&lt;/code&gt; supports 4 different "optional" parameters, symbol, since, limit, and params.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-Golang"&gt;func (this *Binance) FetchMyTrades(options ...FetchMyTradesOptions) ([]Trade, error)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;And we can provide them by doing&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-Golang"&gt;trades, error := exchange.FetchMyTrades(ccxt.withFetchMyTradesSymbol("BTC/USDT"), ccxt.WithFetchOHLCVLimit(5), ccxt.WithFetchMyTradesParams(orderParams))
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Lastly, just because the signature dictates that some argument like &lt;code&gt;symbol&lt;/code&gt; is optional, it will depend from exchange to exchange and you might need to provide it to avoid getting a &lt;code&gt;SymbolRequired&lt;/code&gt; error.&lt;/p&gt; 
&lt;p&gt;You can check different examples in the &lt;code&gt;examples/go&lt;/code&gt; folder.&lt;/p&gt; 
&lt;h2&gt;CCXT CLI&lt;/h2&gt; 
&lt;p&gt;Read the documentation for more information and details: &lt;a href="https://github.com/ccxt/ccxt/tree/master/cli/README.md"&gt;docs&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;CCXT also provides a command-line interface (CLI) that enables direct interaction with any supported exchange from the terminal. You can quickly check balances, place orders, or fetch trade data—without the need to write or execute custom code. This is especially useful for simple or time-sensitive tasks that don’t warrant the overhead of building a full application.&lt;/p&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;p&gt;The CLI is available as a npm package and can be installed by doing&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;npm i ccxt-cli -g
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Usage&lt;/h3&gt; 
&lt;p&gt;You can use the &lt;code&gt;--help&lt;/code&gt; option to view a general overview of how the CLI works. The tool allows you to invoke any CCXT method by specifying the exchange id, the methodName, and any required arguments.&lt;/p&gt; 
&lt;p&gt;Examples:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;ccxt binance createOrder BTC/USDT market buy 0.1 // places an order
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you are not sure which arguments should be provided you can always use the &lt;code&gt;explain&lt;/code&gt; command.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;ccxt explain createOrder
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;result:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Method: createOrder
Usage:
  binance createOrder &amp;lt;symbol&amp;gt; &amp;lt;type&amp;gt; &amp;lt;side&amp;gt; &amp;lt;amount&amp;gt; [price] [params]

Arguments:
  - symbol       (required) — Market symbol e.g., BTC/USDT
  - type         (required) — (no description available)
  - side         (required) — order side e.g., buy or sell
  - amount       (required) — (no description available)
  - price        (optional) — Price per unit of asset e.g., 26000.50
  - params       (optional) — Extra parameters for the exchange e.g., { "recvWindow": 5000 }
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can easily provide API keys by setting them as environment varibales eg: &lt;code&gt;BINANCE_APIKEY="XXXX"&lt;/code&gt; or adding them to the config file located at &lt;code&gt;$CACHE/config.json&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Please read the &lt;a href="https://github.com/ccxt/ccxt/raw/master/CONTRIBUTING.md"&gt;CONTRIBUTING&lt;/a&gt; document before making changes that you would like adopted in the code. Also, read the &lt;a href="https://github.com/ccxt/ccxt/wiki"&gt;Manual&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Support Developer Team&lt;/h2&gt; 
&lt;p&gt;We are investing a significant amount of time into the development of this library. If CCXT made your life easier and you want to help us improve it further, or if you want to speed up development of new features and exchanges, please support us with a tip. We appreciate all contributions!&lt;/p&gt; 
&lt;h3&gt;Sponsors&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://vitalitycrypto.com/"&gt;&lt;img src="https://github.com/user-attachments/assets/0981aae2-3e12-4b57-8d2f-c5ae2b3b8b1c" alt="Vitality" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Support this project by becoming a sponsor.&lt;/p&gt; 
&lt;p&gt;[&lt;a href="https://opencollective.com/ccxt#sponsor"&gt;Become a sponsor&lt;/a&gt;]&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://opencollective.com/ccxt/tiers/sponsor/0/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/sponsor/0/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/ccxt/tiers/sponsor/1/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/sponsor/1/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/ccxt/tiers/sponsor/2/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/sponsor/2/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/ccxt/tiers/sponsor/3/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/sponsor/3/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/ccxt/tiers/sponsor/4/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/sponsor/4/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/ccxt/tiers/sponsor/5/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/sponsor/5/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/ccxt/tiers/sponsor/6/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/sponsor/6/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/ccxt/tiers/sponsor/7/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/sponsor/7/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/ccxt/tiers/sponsor/8/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/sponsor/8/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/ccxt/tiers/sponsor/9/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/sponsor/9/avatar.svg?sanitize=true" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Supporters&lt;/h3&gt; 
&lt;p&gt;Support this project by becoming a supporter. Your avatar will show up here with a link to your website.&lt;/p&gt; 
&lt;p&gt;[&lt;a href="https://opencollective.com/ccxt#supporter"&gt;Become a supporter&lt;/a&gt;]&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://opencollective.com/ccxt/tiers/supporter/0/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/supporter/0/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/ccxt/tiers/supporter/1/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/supporter/1/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/ccxt/tiers/supporter/2/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/supporter/2/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/ccxt/tiers/supporter/3/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/supporter/3/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/ccxt/tiers/supporter/4/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/supporter/4/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/ccxt/tiers/supporter/5/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/supporter/5/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/ccxt/tiers/supporter/6/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/supporter/6/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/ccxt/tiers/supporter/7/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/supporter/7/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/ccxt/tiers/supporter/8/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/supporter/8/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/ccxt/tiers/supporter/9/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/supporter/9/avatar.svg?sanitize=true" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Backers&lt;/h3&gt; 
&lt;p&gt;Thank you to all our backers! [&lt;a href="https://opencollective.com/ccxt#backer"&gt;Become a backer&lt;/a&gt;]&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://opencollective.com/ccxt#backers" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/backer.svg?width=890" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Thank you!&lt;/p&gt; 
&lt;h2&gt;Social&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;sub&gt;&lt;a href="https://twitter.com/ccxt_official"&gt;&lt;img src="https://img.shields.io/twitter/follow/ccxt_official?style=social" alt="Twitter" /&gt;&lt;/a&gt;&lt;/sub&gt; Follow us on Twitter&lt;/li&gt; 
 &lt;li&gt;&lt;sub&gt;&lt;a href="https://medium.com/@ccxt"&gt;&lt;img src="https://img.shields.io/badge/read-our%20blog-black?logo=medium" alt="Medium" /&gt;&lt;/a&gt;&lt;/sub&gt; Read our blog on Medium&lt;/li&gt; 
 &lt;li&gt;&lt;sub&gt;&lt;a href="https://discord.gg/dhzSKYU"&gt;&lt;img src="https://img.shields.io/discord/690203284119617602?logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt;&lt;/sub&gt; Join our Discord&lt;/li&gt; 
 &lt;li&gt;&lt;sub&gt;&lt;a href="https://t.me/ccxt_announcements"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Channel-blue?logo=telegram" alt="Telegram Announcements" /&gt;&lt;/a&gt;&lt;/sub&gt; CCXT Channel on Telegram (important announcements)&lt;/li&gt; 
 &lt;li&gt;&lt;sub&gt;&lt;a href="https://t.me/ccxt_chat"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Chat-blue?logo=telegram" alt="Telegram Chat" /&gt;&lt;/a&gt;&lt;/sub&gt; CCXT Chat on Telegram (technical support)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#ccxt/ccxt&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=ccxt/ccxt&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;CCXT is not a service nor a server. CCXT is a software. &lt;strong&gt;CCXT is a free open source non-custodian API broker software under MIT license&lt;/strong&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Non-custodian&lt;/strong&gt; means CCXT is not an intermediary in trading, it does not hold traders' money at any point in time, traders install CCXT and use CCXT to talk to exchanges directly.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MIT license&lt;/strong&gt; means CCXT can be used for any purpose, but use at your own risk without any warranties.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;API broker&lt;/strong&gt; means CCXT is funded with rebates from exchanges' API broker programs and it is an official API broker with many exchanges, all rebates and related fees are handled by the exchanges solely in accordance with exchanges' respective terms and conditions established by each partner exchange.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Free software&lt;/strong&gt; means CCXT is free to use and has no hidden fees, with CCXT traders pay the same trading fees they would pay to the exchanges directly.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Open source&lt;/strong&gt; means anyone is allowed to use it, to look inside the code and to change everything, including other brokers.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contact Us&lt;/h2&gt; 
&lt;p&gt;For business inquiries: &lt;a href="mailto:info@ccxt.trade"&gt;info@ccxt.trade&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>apache/airflow</title>
      <link>https://github.com/apache/airflow</link>
      <description>&lt;p&gt;Apache Airflow - A platform to programmatically author, schedule, and monitor workflows&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Apache Airflow&lt;/h1&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Badges&lt;/th&gt; 
   &lt;th&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;License&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.apache.org/licenses/LICENSE-2.0.txt"&gt;&lt;img src="https://img.shields.io/:license-Apache%202-blue.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;PyPI&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://badge.fury.io/py/apache-airflow"&gt;&lt;img src="https://badge.fury.io/py/apache-airflow.svg?sanitize=true" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/apache-airflow/"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/apache-airflow.svg?sanitize=true" alt="PyPI - Python Version" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/apache-airflow/"&gt;&lt;img src="https://img.shields.io/pypi/dm/apache-airflow" alt="PyPI - Downloads" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Containers&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://hub.docker.com/r/apache/airflow"&gt;&lt;img src="https://img.shields.io/docker/pulls/apache/airflow.svg?sanitize=true" alt="Docker Pulls" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/apache/airflow"&gt;&lt;img src="https://img.shields.io/docker/stars/apache/airflow.svg?sanitize=true" alt="Docker Stars" /&gt;&lt;/a&gt; &lt;a href="https://artifacthub.io/packages/search?repo=apache-airflow"&gt;&lt;img src="https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/apache-airflow" alt="Artifact HUB" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Community&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/apache/airflow/graphs/contributors"&gt;&lt;img src="https://img.shields.io/github/contributors/apache/airflow" alt="Contributors" /&gt;&lt;/a&gt; &lt;a href="https://s.apache.org/airflow-slack"&gt;&lt;img src="https://img.shields.io/badge/slack-join_chat-white.svg?logo=slack&amp;amp;style=social" alt="Slack Status" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/github/commit-activity/m/apache/airflow" alt="Commit Activity" /&gt; &lt;a href="https://ossrank.com/p/6"&gt;&lt;img src="https://shields.io/endpoint?url=https://ossrank.com/shield/6" alt="OSSRank" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Version&lt;/th&gt; 
   &lt;th&gt;Build Status&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Main&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/apache/airflow/actions"&gt;&lt;img src="https://github.com/apache/airflow/actions/workflows/ci-amd.yml/badge.svg?sanitize=true" alt="GitHub Build main" /&gt;&lt;/a&gt; &lt;a href="https://github.com/apache/airflow/actions"&gt;&lt;img src="https://github.com/apache/airflow/actions/workflows/ci-arm.yml/badge.svg?sanitize=true" alt="GitHub Build main" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;3.x&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/apache/airflow/actions"&gt;&lt;img src="https://github.com/apache/airflow/actions/workflows/ci-amd.yml/badge.svg?branch=v3-1-test" alt="GitHub Build 3.1" /&gt;&lt;/a&gt; &lt;a href="https://github.com/apache/airflow/actions"&gt;&lt;img src="https://github.com/apache/airflow/actions/workflows/ci-arm.yml/badge.svg?branch=v3-1-test" alt="GitHub Build 3.1" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2.x&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/apache/airflow/actions"&gt;&lt;img src="https://github.com/apache/airflow/actions/workflows/ci.yml/badge.svg?branch=v2-11-test" alt="GitHub Build 2.11" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;picture width="500"&gt; 
 &lt;img src="https://github.com/apache/airflow/raw/19ebcac2395ef9a6b6ded3a2faa29dc960c1e635/docs/apache-airflow/img/logos/wordmark_1.png?raw=true" alt="Apache Airflow logo" /&gt; 
&lt;/picture&gt; 
&lt;p&gt;&lt;a href="https://airflow.apache.org/docs/apache-airflow/stable/"&gt;Apache Airflow&lt;/a&gt; (or simply Airflow) is a platform to programmatically author, schedule, and monitor workflows.&lt;/p&gt; 
&lt;p&gt;When workflows are defined as code, they become more maintainable, versionable, testable, and collaborative.&lt;/p&gt; 
&lt;p&gt;Use Airflow to author workflows (Dags) that orchestrate tasks. The Airflow scheduler executes your tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed.&lt;/p&gt; 
&lt;!-- END Apache Airflow, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;!-- START doctoc generated TOC please keep comment here to allow auto update --&gt; 
&lt;!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE --&gt; 
&lt;p&gt;&lt;strong&gt;Table of contents&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#project-focus"&gt;Project Focus&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#principles"&gt;Principles&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#requirements"&gt;Requirements&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#getting-started"&gt;Getting started&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#installing-from-pypi"&gt;Installing from PyPI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#official-source-code"&gt;Official source code&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#convenience-packages"&gt;Convenience packages&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#user-interface"&gt;User Interface&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#semantic-versioning"&gt;Semantic versioning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#version-life-cycle"&gt;Version Life Cycle&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#support-for-python-and-kubernetes-versions"&gt;Support for Python and Kubernetes versions&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#base-os-support-for-reference-airflow-images"&gt;Base OS support for reference Airflow images&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#approach-to-dependencies-of-airflow"&gt;Approach to dependencies of Airflow&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#voting-policy"&gt;Voting Policy&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#who-uses-apache-airflow"&gt;Who uses Apache Airflow?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#who-maintains-apache-airflow"&gt;Who maintains Apache Airflow?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#what-goes-into-the-next-release"&gt;What goes into the next release?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#can-i-use-the-apache-airflow-logo-in-my-presentation"&gt;Can I use the Apache Airflow logo in my presentation?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#links"&gt;Links&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#sponsors"&gt;Sponsors&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- END doctoc generated TOC please keep comment here to allow auto update --&gt; 
&lt;h2&gt;Project Focus&lt;/h2&gt; 
&lt;p&gt;Airflow works best with workflows that are mostly static and slowly changing. When the DAG structure is similar from one run to the next, it clarifies the unit of work and continuity. Other similar projects include &lt;a href="https://github.com/spotify/luigi"&gt;Luigi&lt;/a&gt;, &lt;a href="https://oozie.apache.org/"&gt;Oozie&lt;/a&gt; and &lt;a href="https://azkaban.github.io/"&gt;Azkaban&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Airflow is commonly used to process data, but has the opinion that tasks should ideally be idempotent (i.e., results of the task will be the same, and will not create duplicated data in a destination system), and should not pass large quantities of data from one task to the next (though tasks can pass metadata using Airflow's &lt;a href="https://airflow.apache.org/docs/apache-airflow/stable/concepts/xcoms.html"&gt;XCom feature&lt;/a&gt;). For high-volume, data-intensive tasks, a best practice is to delegate to external services specializing in that type of work.&lt;/p&gt; 
&lt;p&gt;Airflow is not a streaming solution, but it is often used to process real-time data, pulling data off streams in batches.&lt;/p&gt; 
&lt;h2&gt;Principles&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Dynamic&lt;/strong&gt;: Pipelines are defined in code, enabling dynamic dag generation and parameterization.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Extensible&lt;/strong&gt;: The Airflow framework includes a wide range of built-in operators and can be extended to fit your needs.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible&lt;/strong&gt;: Airflow leverages the &lt;a href="https://jinja.palletsprojects.com"&gt;&lt;strong&gt;Jinja&lt;/strong&gt;&lt;/a&gt; templating engine, allowing rich customizations.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- START Requirements, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;p&gt;Apache Airflow is tested with:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;Main version (dev)&lt;/th&gt; 
   &lt;th&gt;Stable version (3.0.6)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Python&lt;/td&gt; 
   &lt;td&gt;3.10, 3.11, 3.12, 3.13&lt;/td&gt; 
   &lt;td&gt;3.9, 3.10, 3.11, 3.12&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Platform&lt;/td&gt; 
   &lt;td&gt;AMD64/ARM64(*)&lt;/td&gt; 
   &lt;td&gt;AMD64/ARM64(*)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Kubernetes&lt;/td&gt; 
   &lt;td&gt;1.30, 1.31, 1.32, 1.33&lt;/td&gt; 
   &lt;td&gt;1.30, 1.31, 1.32, 1.33&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;PostgreSQL&lt;/td&gt; 
   &lt;td&gt;13, 14, 15, 16, 17&lt;/td&gt; 
   &lt;td&gt;13, 14, 15, 16, 17&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MySQL&lt;/td&gt; 
   &lt;td&gt;8.0, 8.4, Innovation&lt;/td&gt; 
   &lt;td&gt;8.0, 8.4, Innovation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;SQLite&lt;/td&gt; 
   &lt;td&gt;3.15.0+&lt;/td&gt; 
   &lt;td&gt;3.15.0+&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;* Experimental&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: MariaDB is not tested/recommended.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: SQLite is used in Airflow tests. Do not use it in production. We recommend using the latest stable version of SQLite for local development.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Airflow currently can be run on POSIX-compliant Operating Systems. For development, it is regularly tested on fairly modern Linux Distros and recent versions of macOS. On Windows you can run it via WSL2 (Windows Subsystem for Linux 2) or via Linux Containers. The work to add Windows support is tracked via &lt;a href="https://github.com/apache/airflow/issues/10388"&gt;#10388&lt;/a&gt;, but it is not a high priority. You should only use Linux-based distros as "Production" execution environment as this is the only environment that is supported. The only distro that is used in our CI tests and that is used in the &lt;a href="https://hub.docker.com/p/apache/airflow"&gt;Community managed DockerHub image&lt;/a&gt; is &lt;code&gt;Debian Bookworm&lt;/code&gt;.&lt;/p&gt; 
&lt;!-- END Requirements, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;!-- START Getting started, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;h2&gt;Getting started&lt;/h2&gt; 
&lt;p&gt;Visit the official Airflow website documentation (latest &lt;strong&gt;stable&lt;/strong&gt; release) for help with &lt;a href="https://airflow.apache.org/docs/apache-airflow/stable/installation/"&gt;installing Airflow&lt;/a&gt;, &lt;a href="https://airflow.apache.org/docs/apache-airflow/stable/start.html"&gt;getting started&lt;/a&gt;, or walking through a more complete &lt;a href="https://airflow.apache.org/docs/apache-airflow/stable/tutorial/"&gt;tutorial&lt;/a&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Note: If you're looking for documentation for the main branch (latest development branch): you can find it on &lt;a href="https://s.apache.org/airflow-docs/"&gt;s.apache.org/airflow-docs&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;For more information on Airflow Improvement Proposals (AIPs), visit the &lt;a href="https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals"&gt;Airflow Wiki&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Documentation for dependent projects like provider distributions, Docker image, Helm Chart, you'll find it in &lt;a href="https://airflow.apache.org/docs/"&gt;the documentation index&lt;/a&gt;.&lt;/p&gt; 
&lt;!-- END Getting started, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;!-- START Installing from PyPI, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;h2&gt;Installing from PyPI&lt;/h2&gt; 
&lt;p&gt;We publish Apache Airflow as &lt;code&gt;apache-airflow&lt;/code&gt; package in PyPI. Installing it however might be sometimes tricky because Airflow is a bit of both a library and application. Libraries usually keep their dependencies open, and applications usually pin them, but we should do neither and both simultaneously. We decided to keep our dependencies as open as possible (in &lt;code&gt;pyproject.toml&lt;/code&gt;) so users can install different versions of libraries if needed. This means that &lt;code&gt;pip install apache-airflow&lt;/code&gt; will not work from time to time or will produce unusable Airflow installation.&lt;/p&gt; 
&lt;p&gt;To have repeatable installation, however, we keep a set of "known-to-be-working" constraint files in the orphan &lt;code&gt;constraints-main&lt;/code&gt; and &lt;code&gt;constraints-2-0&lt;/code&gt; branches. We keep those "known-to-be-working" constraints files separately per major/minor Python version. You can use them as constraint files when installing Airflow from PyPI. Note that you have to specify correct Airflow tag/version/branch and Python versions in the URL.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Installing just Airflow:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Note: Only &lt;code&gt;pip&lt;/code&gt; installation is currently officially supported.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;While it is possible to install Airflow with tools like &lt;a href="https://python-poetry.org"&gt;Poetry&lt;/a&gt; or &lt;a href="https://pypi.org/project/pip-tools"&gt;pip-tools&lt;/a&gt;, they do not share the same workflow as &lt;code&gt;pip&lt;/code&gt; - especially when it comes to constraint vs. requirements management. Installing via &lt;code&gt;Poetry&lt;/code&gt; or &lt;code&gt;pip-tools&lt;/code&gt; is not currently supported.&lt;/p&gt; 
&lt;p&gt;There are known issues with &lt;code&gt;bazel&lt;/code&gt; that might lead to circular dependencies when using it to install Airflow. Please switch to &lt;code&gt;pip&lt;/code&gt; if you encounter such problems. &lt;code&gt;Bazel&lt;/code&gt; community works on fixing the problem in &lt;code&gt;this PR &amp;lt;https://github.com/bazelbuild/rules_python/pull/1166&amp;gt;&lt;/code&gt;_ so it might be that newer versions of &lt;code&gt;bazel&lt;/code&gt; will handle it.&lt;/p&gt; 
&lt;p&gt;If you wish to install Airflow using those tools, you should use the constraint files and convert them to the appropriate format and workflow that your tool requires.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install 'apache-airflow==3.0.6' \
 --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-3.0.6/constraints-3.10.txt"
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Installing with extras (i.e., postgres, google)&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install 'apache-airflow[postgres,google]==3.0.6' \
 --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-3.0.6/constraints-3.10.txt"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For information on installing provider distributions, check &lt;a href="http://airflow.apache.org/docs/apache-airflow-providers/index.html"&gt;providers&lt;/a&gt;.&lt;/p&gt; 
&lt;!-- END Installing from PyPI, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;For comprehensive instructions on setting up your local development environment and installing Apache Airflow, please refer to the &lt;a href="https://raw.githubusercontent.com/apache/airflow/main/INSTALLING.md"&gt;INSTALLING.md&lt;/a&gt; file.&lt;/p&gt; 
&lt;!-- START Official source code, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;h2&gt;Official source code&lt;/h2&gt; 
&lt;p&gt;Apache Airflow is an &lt;a href="https://www.apache.org"&gt;Apache Software Foundation&lt;/a&gt; (ASF) project, and our official source code releases:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Follow the &lt;a href="https://www.apache.org/legal/release-policy.html"&gt;ASF Release Policy&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Can be downloaded from &lt;a href="https://downloads.apache.org/airflow"&gt;the ASF Distribution Directory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Are cryptographically signed by the release manager&lt;/li&gt; 
 &lt;li&gt;Are officially voted on by the PMC members during the &lt;a href="https://www.apache.org/legal/release-policy.html#release-approval"&gt;Release Approval Process&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Following the ASF rules, the source packages released must be sufficient for a user to build and test the release provided they have access to the appropriate platform and tools.&lt;/p&gt; 
&lt;!-- END Official source code, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;h2&gt;Convenience packages&lt;/h2&gt; 
&lt;p&gt;There are other ways of installing and using Airflow. Those are "convenience" methods - they are not "official releases" as stated by the &lt;code&gt;ASF Release Policy&lt;/code&gt;, but they can be used by the users who do not want to build the software themselves.&lt;/p&gt; 
&lt;p&gt;Those are - in the order of most common ways people install Airflow:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/apache-airflow/"&gt;PyPI releases&lt;/a&gt; to install Airflow using standard &lt;code&gt;pip&lt;/code&gt; tool&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://hub.docker.com/r/apache/airflow"&gt;Docker Images&lt;/a&gt; to install airflow via &lt;code&gt;docker&lt;/code&gt; tool, use them in Kubernetes, Helm Charts, &lt;code&gt;docker-compose&lt;/code&gt;, &lt;code&gt;docker swarm&lt;/code&gt;, etc. You can read more about using, customizing, and extending the images in the &lt;a href="https://airflow.apache.org/docs/docker-stack/index.html"&gt;Latest docs&lt;/a&gt;, and learn details on the internals in the &lt;a href="https://airflow.apache.org/docs/docker-stack/index.html"&gt;images&lt;/a&gt; document.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/apache/airflow/tags"&gt;Tags in GitHub&lt;/a&gt; to retrieve the git project sources that were used to generate official source packages via git&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;All those artifacts are not official releases, but they are prepared using officially released sources. Some of those artifacts are "development" or "pre-release" ones, and they are clearly marked as such following the ASF Policy.&lt;/p&gt; 
&lt;h2&gt;User Interface&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;DAGs&lt;/strong&gt;: Overview of all DAGs in your environment.&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/dags.png" alt="DAGs" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Assets&lt;/strong&gt;: Overview of Assets with dependencies.&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/assets_graph.png" alt="Asset Dependencies" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Grid&lt;/strong&gt;: Grid representation of a DAG that spans across time.&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/grid.png" alt="Grid" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Graph&lt;/strong&gt;: Visualization of a DAG's dependencies and their current status for a specific run.&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/graph.png" alt="Graph" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Home&lt;/strong&gt;: Summary statistics of your Airflow environment.&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/home.png" alt="Home" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Backfill&lt;/strong&gt;: Backfilling a DAG for a specific date range.&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/backfill.png" alt="Backfill" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Code&lt;/strong&gt;: Quick way to view source code of a DAG.&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/code.png" alt="Code" /&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Semantic versioning&lt;/h2&gt; 
&lt;p&gt;As of Airflow 2.0.0, we support a strict &lt;a href="https://semver.org/"&gt;SemVer&lt;/a&gt; approach for all packages released.&lt;/p&gt; 
&lt;p&gt;There are few specific rules that we agreed to that define details of versioning of the different packages:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Airflow&lt;/strong&gt;: SemVer rules apply to core airflow only (excludes any changes to providers). Changing limits for versions of Airflow dependencies is not a breaking change on its own.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Airflow Providers&lt;/strong&gt;: SemVer rules apply to changes in the particular provider's code only. SemVer MAJOR and MINOR versions for the packages are independent of the Airflow version. For example, &lt;code&gt;google 4.1.0&lt;/code&gt; and &lt;code&gt;amazon 3.0.6&lt;/code&gt; providers can happily be installed with &lt;code&gt;Airflow 2.1.2&lt;/code&gt;. If there are limits of cross-dependencies between providers and Airflow packages, they are present in providers as &lt;code&gt;install_requires&lt;/code&gt; limitations. We aim to keep backwards compatibility of providers with all previously released Airflow 2 versions but there will sometimes be breaking changes that might make some, or all providers, have minimum Airflow version specified.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Airflow Helm Chart&lt;/strong&gt;: SemVer rules apply to changes in the chart only. SemVer MAJOR and MINOR versions for the chart are independent of the Airflow version. We aim to keep backwards compatibility of the Helm Chart with all released Airflow 2 versions, but some new features might only work starting from specific Airflow releases. We might however limit the Helm Chart to depend on minimal Airflow version.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Airflow API clients&lt;/strong&gt;: Their versioning is independent from Airflow versions. They follow their own SemVer rules for breaking changes and new features - which for example allows to change the way we generate the clients.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Version Life Cycle&lt;/h2&gt; 
&lt;p&gt;Apache Airflow version life cycle:&lt;/p&gt; 
&lt;!-- This table is automatically updated by prek scripts/ci/prek/supported_versions.py --&gt; 
&lt;!-- Beginning of auto-generated table --&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Version&lt;/th&gt; 
   &lt;th&gt;Current Patch/Minor&lt;/th&gt; 
   &lt;th&gt;State&lt;/th&gt; 
   &lt;th&gt;First Release&lt;/th&gt; 
   &lt;th&gt;Limited Maintenance&lt;/th&gt; 
   &lt;th&gt;EOL/Terminated&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;3&lt;/td&gt; 
   &lt;td&gt;3.0.6&lt;/td&gt; 
   &lt;td&gt;Supported&lt;/td&gt; 
   &lt;td&gt;Apr 22, 2025&lt;/td&gt; 
   &lt;td&gt;TBD&lt;/td&gt; 
   &lt;td&gt;TBD&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;2.11.0&lt;/td&gt; 
   &lt;td&gt;Supported&lt;/td&gt; 
   &lt;td&gt;Dec 17, 2020&lt;/td&gt; 
   &lt;td&gt;Oct 22, 2025&lt;/td&gt; 
   &lt;td&gt;Apr 22, 2026&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;1.10&lt;/td&gt; 
   &lt;td&gt;1.10.15&lt;/td&gt; 
   &lt;td&gt;EOL&lt;/td&gt; 
   &lt;td&gt;Aug 27, 2018&lt;/td&gt; 
   &lt;td&gt;Dec 17, 2020&lt;/td&gt; 
   &lt;td&gt;June 17, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;1.9&lt;/td&gt; 
   &lt;td&gt;1.9.0&lt;/td&gt; 
   &lt;td&gt;EOL&lt;/td&gt; 
   &lt;td&gt;Jan 03, 2018&lt;/td&gt; 
   &lt;td&gt;Aug 27, 2018&lt;/td&gt; 
   &lt;td&gt;Aug 27, 2018&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;1.8&lt;/td&gt; 
   &lt;td&gt;1.8.2&lt;/td&gt; 
   &lt;td&gt;EOL&lt;/td&gt; 
   &lt;td&gt;Mar 19, 2017&lt;/td&gt; 
   &lt;td&gt;Jan 03, 2018&lt;/td&gt; 
   &lt;td&gt;Jan 03, 2018&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;1.7&lt;/td&gt; 
   &lt;td&gt;1.7.1.2&lt;/td&gt; 
   &lt;td&gt;EOL&lt;/td&gt; 
   &lt;td&gt;Mar 28, 2016&lt;/td&gt; 
   &lt;td&gt;Mar 19, 2017&lt;/td&gt; 
   &lt;td&gt;Mar 19, 2017&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;!-- End of auto-generated table --&gt; 
&lt;p&gt;Limited support versions will be supported with security and critical bug fix only. EOL versions will not get any fixes nor support. We always recommend that all users run the latest available minor release for whatever major version is in use. We &lt;strong&gt;highly&lt;/strong&gt; recommend upgrading to the latest Airflow major release at the earliest convenient time and before the EOL date.&lt;/p&gt; 
&lt;h2&gt;Support for Python and Kubernetes versions&lt;/h2&gt; 
&lt;p&gt;As of Airflow 2.0, we agreed to certain rules we follow for Python and Kubernetes support. They are based on the official release schedule of Python and Kubernetes, nicely summarized in the &lt;a href="https://devguide.python.org/#status-of-python-branches"&gt;Python Developer's Guide&lt;/a&gt; and &lt;a href="https://kubernetes.io/docs/setup/release/version-skew-policy/"&gt;Kubernetes version skew policy&lt;/a&gt;.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;We drop support for Python and Kubernetes versions when they reach EOL. Except for Kubernetes, a version stays supported by Airflow if two major cloud providers still provide support for it. We drop support for those EOL versions in main right after EOL date, and it is effectively removed when we release the first new MINOR (Or MAJOR if there is no new MINOR version) of Airflow. For example, for Python 3.10 it means that we will drop support in main right after 27.06.2023, and the first MAJOR or MINOR version of Airflow released after will not have it.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;We support a new version of Python/Kubernetes in main after they are officially released, as soon as we make them work in our CI pipeline (which might not be immediate due to dependencies catching up with new versions of Python mostly) we release new images/support in Airflow based on the working CI setup.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;This policy is best-effort which means there may be situations where we might terminate support earlier if circumstances require it.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Base OS support for reference Airflow images&lt;/h2&gt; 
&lt;p&gt;The Airflow Community provides conveniently packaged container images that are published whenever we publish an Apache Airflow release. Those images contain:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Base OS with necessary packages to install Airflow (stable Debian OS)&lt;/li&gt; 
 &lt;li&gt;Base Python installation in versions supported at the time of release for the MINOR version of Airflow released (so there could be different versions for 2.3 and 2.2 line for example)&lt;/li&gt; 
 &lt;li&gt;Libraries required to connect to supported Databases (again the set of databases supported depends on the MINOR version of Airflow)&lt;/li&gt; 
 &lt;li&gt;Predefined set of popular providers (for details see the &lt;a href="https://raw.githubusercontent.com/apache/airflow/main/Dockerfile"&gt;Dockerfile&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;Possibility of building your own, custom image where the user can choose their own set of providers and libraries (see &lt;a href="https://airflow.apache.org/docs/docker-stack/build.html"&gt;Building the image&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;In the future Airflow might also support a "slim" version without providers nor database clients installed&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The version of the base OS image is the stable version of Debian. Airflow supports using all currently active stable versions - as soon as all Airflow dependencies support building, and we set up the CI pipeline for building and testing the OS version. Approximately 6 months before the end-of-regular support of a previous stable version of the OS, Airflow switches the images released to use the latest supported version of the OS.&lt;/p&gt; 
&lt;p&gt;For example switch from &lt;code&gt;Debian Bullseye&lt;/code&gt; to &lt;code&gt;Debian Bookworm&lt;/code&gt; has been implemented before 2.8.0 release in October 2023 and &lt;code&gt;Debian Bookworm&lt;/code&gt; will be the only option supported as of Airflow 2.10.0.&lt;/p&gt; 
&lt;p&gt;Users will continue to be able to build their images using stable Debian releases until the end of regular support and building and verifying of the images happens in our CI but no unit tests were executed using this image in the &lt;code&gt;main&lt;/code&gt; branch.&lt;/p&gt; 
&lt;h2&gt;Approach to dependencies of Airflow&lt;/h2&gt; 
&lt;p&gt;Airflow has a lot of dependencies - direct and transitive, also Airflow is both - library and application, therefore our policies to dependencies has to include both - stability of installation of application, but also ability to install newer version of dependencies for those users who develop DAGs. We developed the approach where &lt;code&gt;constraints&lt;/code&gt; are used to make sure airflow can be installed in a repeatable way, while we do not limit our users to upgrade most of the dependencies. As a result we decided not to upper-bound version of Airflow dependencies by default, unless we have good reasons to believe upper-bounding them is needed because of importance of the dependency as well as risk it involves to upgrade specific dependency. We also upper-bound the dependencies that we know cause problems.&lt;/p&gt; 
&lt;p&gt;The constraint mechanism of ours takes care about finding and upgrading all the non-upper bound dependencies automatically (providing that all the tests pass). Our &lt;code&gt;main&lt;/code&gt; build failures will indicate in case there are versions of dependencies that break our tests - indicating that we should either upper-bind them or that we should fix our code/tests to account for the upstream changes from those dependencies.&lt;/p&gt; 
&lt;p&gt;Whenever we upper-bound such a dependency, we should always comment why we are doing it - i.e. we should have a good reason why dependency is upper-bound. And we should also mention what is the condition to remove the binding.&lt;/p&gt; 
&lt;h3&gt;Approach for dependencies for Airflow Core&lt;/h3&gt; 
&lt;p&gt;Those dependencies are maintained in &lt;code&gt;pyproject.toml&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;There are few dependencies that we decided are important enough to upper-bound them by default, as they are known to follow predictable versioning scheme, and we know that new versions of those are very likely to bring breaking changes. We commit to regularly review and attempt to upgrade to the newer versions of the dependencies as they are released, but this is manual process.&lt;/p&gt; 
&lt;p&gt;The important dependencies are:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;SQLAlchemy&lt;/code&gt;: upper-bound to specific MINOR version (SQLAlchemy is known to remove deprecations and introduce breaking changes especially that support for different Databases varies and changes at various speed)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Alembic&lt;/code&gt;: it is important to handle our migrations in predictable and performant way. It is developed together with SQLAlchemy. Our experience with Alembic is that it very stable in MINOR version&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Flask&lt;/code&gt;: We are using Flask as the back-bone of our web UI and API. We know major version of Flask are very likely to introduce breaking changes across those so limiting it to MAJOR version makes sense&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;werkzeug&lt;/code&gt;: the library is known to cause problems in new versions. It is tightly coupled with Flask libraries, and we should update them together&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;celery&lt;/code&gt;: Celery is a crucial component of Airflow as it used for CeleryExecutor (and similar). Celery &lt;a href="https://docs.celeryq.dev/en/stable/contributing.html?highlight=semver#versions"&gt;follows SemVer&lt;/a&gt;, so we should upper-bound it to the next MAJOR version. Also, when we bump the upper version of the library, we should make sure Celery Provider minimum Airflow version is updated.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;kubernetes&lt;/code&gt;: Kubernetes is a crucial component of Airflow as it is used for the KubernetesExecutor (and similar). Kubernetes Python library &lt;a href="https://github.com/kubernetes-client/python#compatibility"&gt;follows SemVer&lt;/a&gt;, so we should upper-bound it to the next MAJOR version. Also, when we bump the upper version of the library, we should make sure Kubernetes Provider minimum Airflow version is updated.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Approach for dependencies in Airflow Providers and extras&lt;/h3&gt; 
&lt;p&gt;The main part of the Airflow is the Airflow Core, but the power of Airflow also comes from a number of providers that extend the core functionality and are released separately, even if we keep them (for now) in the same monorepo for convenience. You can read more about the providers in the &lt;a href="https://airflow.apache.org/docs/apache-airflow-providers/index.html"&gt;Providers documentation&lt;/a&gt;. We also have set of policies implemented for maintaining and releasing community-managed providers as well as the approach for community vs. 3rd party providers in the &lt;a href="https://github.com/apache/airflow/raw/main/PROVIDERS.rst"&gt;providers&lt;/a&gt; document.&lt;/p&gt; 
&lt;p&gt;Those &lt;code&gt;extras&lt;/code&gt; and &lt;code&gt;providers&lt;/code&gt; dependencies are maintained in &lt;code&gt;provider.yaml&lt;/code&gt; of each provider.&lt;/p&gt; 
&lt;p&gt;By default, we should not upper-bound dependencies for providers, however each provider's maintainer might decide to add additional limits (and justify them with comment).&lt;/p&gt; 
&lt;!-- START Contributing, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Want to help build Apache Airflow? Check out our &lt;a href="https://github.com/apache/airflow/raw/main/contributing-docs/README.rst"&gt;contributors' guide&lt;/a&gt; for a comprehensive overview of how to contribute, including setup instructions, coding standards, and pull request guidelines.&lt;/p&gt; 
&lt;p&gt;If you can't wait to contribute, and want to get started asap, check out the &lt;a href="https://github.com/apache/airflow/raw/main/contributing-docs/03_contributors_quick_start.rst"&gt;contribution quickstart&lt;/a&gt; here!&lt;/p&gt; 
&lt;p&gt;Official Docker (container) images for Apache Airflow are described in &lt;a href="https://github.com/apache/airflow/raw/main/dev/breeze/doc/ci/02_images.md"&gt;images&lt;/a&gt;.&lt;/p&gt; 
&lt;!-- END Contributing, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;!-- START Who uses Apache Airflow, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;h2&gt;Voting Policy&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Commits need a +1 vote from a committer who is not the author&lt;/li&gt; 
 &lt;li&gt;When we do AIP voting, both PMC member's and committer's &lt;code&gt;+1s&lt;/code&gt; are considered a binding vote.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Who uses Apache Airflow?&lt;/h2&gt; 
&lt;p&gt;We know about around 500 organizations that are using Apache Airflow (but there are likely many more) &lt;a href="https://github.com/apache/airflow/raw/main/INTHEWILD.md"&gt;in the wild&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you use Airflow - feel free to make a PR to add your organisation to the list.&lt;/p&gt; 
&lt;!-- END Who uses Apache Airflow, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;!-- START Who maintains Apache Airflow, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;h2&gt;Who maintains Apache Airflow?&lt;/h2&gt; 
&lt;p&gt;Airflow is the work of the &lt;a href="https://github.com/apache/airflow/graphs/contributors"&gt;community&lt;/a&gt;, but the &lt;a href="https://people.apache.org/committers-by-project.html#airflow"&gt;core committers/maintainers&lt;/a&gt; are responsible for reviewing and merging PRs as well as steering conversations around new feature requests. If you would like to become a maintainer, please review the Apache Airflow &lt;a href="https://github.com/apache/airflow/raw/main/COMMITTERS.rst#guidelines-to-become-an-airflow-committer"&gt;committer requirements&lt;/a&gt;.&lt;/p&gt; 
&lt;!-- END Who maintains Apache Airflow, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;h2&gt;What goes into the next release?&lt;/h2&gt; 
&lt;p&gt;Often you will see an issue that is assigned to specific milestone with Airflow version, or a PR that gets merged to the main branch and you might wonder which release the merged PR(s) will be released in or which release the fixed issues will be in. The answer to this is as usual - it depends on various scenarios. The answer is different for PRs and Issues.&lt;/p&gt; 
&lt;p&gt;To add a bit of context, we are following the &lt;a href="https://semver.org/"&gt;Semver&lt;/a&gt; versioning scheme as described in &lt;a href="https://airflow.apache.org/docs/apache-airflow/stable/release-process.html"&gt;Airflow release process&lt;/a&gt;. More details are explained in detail in this README under the &lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#semantic-versioning"&gt;Semantic versioning&lt;/a&gt; chapter, but in short, we have &lt;code&gt;MAJOR.MINOR.PATCH&lt;/code&gt; versions of Airflow.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;MAJOR&lt;/code&gt; version is incremented in case of breaking changes&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;MINOR&lt;/code&gt; version is incremented when there are new features added&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;PATCH&lt;/code&gt; version is incremented when there are only bug-fixes and doc-only changes&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Generally we release &lt;code&gt;MINOR&lt;/code&gt; versions of Airflow from a branch that is named after the MINOR version. For example &lt;code&gt;2.7.*&lt;/code&gt; releases are released from &lt;code&gt;v2-7-stable&lt;/code&gt; branch, &lt;code&gt;2.8.*&lt;/code&gt; releases are released from &lt;code&gt;v2-8-stable&lt;/code&gt; branch, etc.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Most of the time in our release cycle, when the branch for next &lt;code&gt;MINOR&lt;/code&gt; branch is not yet created, all PRs merged to &lt;code&gt;main&lt;/code&gt; (unless they get reverted), will find their way to the next &lt;code&gt;MINOR&lt;/code&gt; release. For example if the last release is &lt;code&gt;2.7.3&lt;/code&gt; and &lt;code&gt;v2-8-stable&lt;/code&gt; branch is not created yet, the next &lt;code&gt;MINOR&lt;/code&gt; release is &lt;code&gt;2.8.0&lt;/code&gt; and all PRs merged to main will be released in &lt;code&gt;2.8.0&lt;/code&gt;. However, some PRs (bug-fixes and doc-only changes) when merged, can be cherry-picked to current &lt;code&gt;MINOR&lt;/code&gt; branch and released in the next &lt;code&gt;PATCHLEVEL&lt;/code&gt; release. For example, if &lt;code&gt;2.8.1&lt;/code&gt; is already released and we are working on &lt;code&gt;2.9.0dev&lt;/code&gt;, then marking a PR with &lt;code&gt;2.8.2&lt;/code&gt; milestone means that it will be cherry-picked to &lt;code&gt;v2-8-test&lt;/code&gt; branch and released in &lt;code&gt;2.8.2rc1&lt;/code&gt;, and eventually in &lt;code&gt;2.8.2&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;When we prepare for the next &lt;code&gt;MINOR&lt;/code&gt; release, we cut new &lt;code&gt;v2-*-test&lt;/code&gt; and &lt;code&gt;v2-*-stable&lt;/code&gt; branch and prepare &lt;code&gt;alpha&lt;/code&gt;, &lt;code&gt;beta&lt;/code&gt; releases for the next &lt;code&gt;MINOR&lt;/code&gt; version, the PRs merged to main will still be released in the next &lt;code&gt;MINOR&lt;/code&gt; release until &lt;code&gt;rc&lt;/code&gt; version is cut. This is happening because the &lt;code&gt;v2-*-test&lt;/code&gt; and &lt;code&gt;v2-*-stable&lt;/code&gt; branches are rebased on top of main when next &lt;code&gt;beta&lt;/code&gt; and &lt;code&gt;rc&lt;/code&gt; releases are prepared. For example, when we cut &lt;code&gt;2.10.0beta1&lt;/code&gt; version, anything merged to main before &lt;code&gt;2.10.0rc1&lt;/code&gt; is released, will find its way to 2.10.0rc1.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Then, once we prepare the first RC candidate for the MINOR release, we stop moving the &lt;code&gt;v2-*-test&lt;/code&gt; and &lt;code&gt;v2-*-stable&lt;/code&gt; branches and the PRs merged to main will be released in the next &lt;code&gt;MINOR&lt;/code&gt; release. However, some PRs (bug-fixes and doc-only changes) when merged, can be cherry-picked to current &lt;code&gt;MINOR&lt;/code&gt; branch and released in the next &lt;code&gt;PATCHLEVEL&lt;/code&gt; release - for example when the last released version from &lt;code&gt;v2-10-stable&lt;/code&gt; branch is &lt;code&gt;2.10.0rc1&lt;/code&gt;, some of the PRs from main can be marked as &lt;code&gt;2.10.0&lt;/code&gt; milestone by committers, the release manager will try to cherry-pick them into the release branch. If successful, they will be released in &lt;code&gt;2.10.0rc2&lt;/code&gt; and subsequently in &lt;code&gt;2.10.0&lt;/code&gt;. This also applies to subsequent &lt;code&gt;PATCHLEVEL&lt;/code&gt; versions. When for example &lt;code&gt;2.10.1&lt;/code&gt; is already released, marking a PR with &lt;code&gt;2.10.2&lt;/code&gt; milestone will mean that it will be cherry-picked to &lt;code&gt;v2-10-stable&lt;/code&gt; branch and released in &lt;code&gt;2.10.2rc1&lt;/code&gt; and eventually in &lt;code&gt;2.10.2&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;The final decision about cherry-picking is made by the release manager.&lt;/p&gt; 
&lt;p&gt;Marking issues with a milestone is a bit different. Maintainers do not mark issues with a milestone usually, normally they are only marked in PRs. If PR linked to the issue (and "fixing it") gets merged and released in a specific version following the process described above, the issue will be automatically closed, no milestone will be set for the issue, you need to check the PR that fixed the issue to see which version it was released in.&lt;/p&gt; 
&lt;p&gt;However, sometimes maintainers mark issues with specific milestone, which means that the issue is important to become a candidate to take a look when the release is being prepared. Since this is an Open-Source project, where basically all contributors volunteer their time, there is no guarantee that specific issue will be fixed in specific version. We do not want to hold the release because some issue is not fixed, so in such case release manager will reassign such unfixed issues to the next milestone in case they are not fixed in time for the current release. Therefore, the milestone for issue is more of an intent that it should be looked at, than promise it will be fixed in the version.&lt;/p&gt; 
&lt;p&gt;More context and &lt;strong&gt;FAQ&lt;/strong&gt; about the patchlevel release can be found in the &lt;a href="https://raw.githubusercontent.com/apache/airflow/main/dev/WHAT_GOES_INTO_THE_NEXT_RELEASE.md"&gt;What goes into the next release&lt;/a&gt; document in the &lt;code&gt;dev&lt;/code&gt; folder of the repository.&lt;/p&gt; 
&lt;h2&gt;Can I use the Apache Airflow logo in my presentation?&lt;/h2&gt; 
&lt;p&gt;Yes! Be sure to abide by the Apache Foundation &lt;a href="https://www.apache.org/foundation/marks/#books"&gt;trademark policies&lt;/a&gt; and the Apache Airflow &lt;a href="https://cwiki.apache.org/confluence/display/AIRFLOW/Brandbook"&gt;Brandbook&lt;/a&gt;. The most up-to-date logos are found in &lt;a href="https://github.com/apache/airflow/tree/main/airflow-core/docs/img/logos/"&gt;this repo&lt;/a&gt; and on the Apache Software Foundation &lt;a href="https://www.apache.org/logos/about.html"&gt;website&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Links&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://airflow.apache.org/docs/apache-airflow/stable/"&gt;Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://s.apache.org/airflow-slack"&gt;Chat&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://airflow.apache.org/community/"&gt;Community Information&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Sponsors&lt;/h2&gt; 
&lt;p&gt;The CI infrastructure for Apache Airflow has been sponsored by:&lt;/p&gt; 
&lt;!-- Ordered by most recently "funded" --&gt; 
&lt;p&gt;&lt;a href="https://astronomer.io"&gt;&lt;img src="https://assets2.astronomer.io/logos/logoForLIGHTbackground.png" alt="astronomer.io" width="250px" /&gt;&lt;/a&gt; &lt;a href="https://aws.amazon.com/opensource/"&gt;&lt;img src="https://github.com/apache/airflow/raw/main/providers/amazon/docs/integration-logos/AWS-Cloud-alt_light-bg@4x.png?raw=true" alt="AWS OpenSource" width="130px" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ultralytics/ultralytics</title>
      <link>https://github.com/ultralytics/ultralytics</link>
      <description>&lt;p&gt;Ultralytics YOLO 🚀&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt; &lt;a href="https://www.ultralytics.com/events/yolovision?utm_source=github&amp;amp;utm_medium=org&amp;amp;utm_campaign=yv25_event" target="_blank"&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/banner-yolov8.png" alt="Ultralytics YOLO banner" /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://docs.ultralytics.com/zh/"&gt;中文&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/ko/"&gt;한국어&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/ja/"&gt;日本語&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/ru/"&gt;Русский&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/de/"&gt;Deutsch&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/fr/"&gt;Français&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/es"&gt;Español&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/pt/"&gt;Português&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/tr/"&gt;Türkçe&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/vi/"&gt;Tiếng Việt&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/ar/"&gt;العربية&lt;/a&gt; &lt;br /&gt;&lt;/p&gt; 
 &lt;div&gt; 
  &lt;a href="https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml"&gt;&lt;img src="https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg?sanitize=true" alt="Ultralytics CI" /&gt;&lt;/a&gt; 
  &lt;a href="https://clickpy.clickhouse.com/dashboard/ultralytics"&gt;&lt;img src="https://static.pepy.tech/badge/ultralytics" alt="Ultralytics Downloads" /&gt;&lt;/a&gt; 
  &lt;a href="https://zenodo.org/badge/latestdoi/264818686"&gt;&lt;img src="https://zenodo.org/badge/264818686.svg?sanitize=true" alt="Ultralytics YOLO Citation" /&gt;&lt;/a&gt; 
  &lt;a href="https://discord.com/invite/ultralytics"&gt;&lt;img alt="Ultralytics Discord" src="https://img.shields.io/discord/1089800235347353640?logo=discord&amp;amp;logoColor=white&amp;amp;label=Discord&amp;amp;color=blue" /&gt;&lt;/a&gt; 
  &lt;a href="https://community.ultralytics.com/"&gt;&lt;img alt="Ultralytics Forums" src="https://img.shields.io/discourse/users?server=https%3A%2F%2Fcommunity.ultralytics.com&amp;amp;logo=discourse&amp;amp;label=Forums&amp;amp;color=blue" /&gt;&lt;/a&gt; 
  &lt;a href="https://www.reddit.com/r/ultralytics/"&gt;&lt;img alt="Ultralytics Reddit" src="https://img.shields.io/reddit/subreddit-subscribers/ultralytics?style=flat&amp;amp;logo=reddit&amp;amp;logoColor=white&amp;amp;label=Reddit&amp;amp;color=blue" /&gt;&lt;/a&gt; 
  &lt;br /&gt; 
  &lt;a href="https://console.paperspace.com/github/ultralytics/ultralytics"&gt;&lt;img src="https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true" alt="Run Ultralytics on Gradient" /&gt;&lt;/a&gt; 
  &lt;a href="https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open Ultralytics In Colab" /&gt;&lt;/a&gt; 
  &lt;a href="https://www.kaggle.com/models/ultralytics/yolo11"&gt;&lt;img src="https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true" alt="Open Ultralytics In Kaggle" /&gt;&lt;/a&gt; 
  &lt;a href="https://mybinder.org/v2/gh/ultralytics/ultralytics/HEAD?labpath=examples%2Ftutorial.ipynb"&gt;&lt;img src="https://mybinder.org/badge_logo.svg?sanitize=true" alt="Open Ultralytics In Binder" /&gt;&lt;/a&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;a href="https://www.ultralytics.com/"&gt;Ultralytics&lt;/a&gt; creates cutting-edge, state-of-the-art (SOTA) &lt;a href="https://www.ultralytics.com/yolo"&gt;YOLO models&lt;/a&gt; built on years of foundational research in computer vision and AI. Constantly updated for performance and flexibility, our models are &lt;strong&gt;fast&lt;/strong&gt;, &lt;strong&gt;accurate&lt;/strong&gt;, and &lt;strong&gt;easy to use&lt;/strong&gt;. They excel at &lt;a href="https://docs.ultralytics.com/tasks/detect/"&gt;object detection&lt;/a&gt;, &lt;a href="https://docs.ultralytics.com/modes/track/"&gt;tracking&lt;/a&gt;, &lt;a href="https://docs.ultralytics.com/tasks/segment/"&gt;instance segmentation&lt;/a&gt;, &lt;a href="https://docs.ultralytics.com/tasks/classify/"&gt;image classification&lt;/a&gt;, and &lt;a href="https://docs.ultralytics.com/tasks/pose/"&gt;pose estimation&lt;/a&gt; tasks.&lt;/p&gt; 
&lt;p&gt;Find detailed documentation in the &lt;a href="https://docs.ultralytics.com/"&gt;Ultralytics Docs&lt;/a&gt;. Get support via &lt;a href="https://github.com/ultralytics/ultralytics/issues/new/choose"&gt;GitHub Issues&lt;/a&gt;. Join discussions on &lt;a href="https://discord.com/invite/ultralytics"&gt;Discord&lt;/a&gt;, &lt;a href="https://www.reddit.com/r/ultralytics/"&gt;Reddit&lt;/a&gt;, and the &lt;a href="https://community.ultralytics.com/"&gt;Ultralytics Community Forums&lt;/a&gt;!&lt;/p&gt; 
&lt;p&gt;Request an Enterprise License for commercial use at &lt;a href="https://www.ultralytics.com/license"&gt;Ultralytics Licensing&lt;/a&gt;.&lt;/p&gt; 
&lt;a href="https://docs.ultralytics.com/models/yolo11/" target="_blank"&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/ultralytics/assets/refs/heads/main/yolo/performance-comparison.png" alt="YOLO11 performance plots" /&gt; &lt;/a&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/ultralytics"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-github.png" width="2%" alt="Ultralytics GitHub" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="2%" alt="space" /&gt; 
 &lt;a href="https://www.linkedin.com/company/ultralytics/"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-linkedin.png" width="2%" alt="Ultralytics LinkedIn" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="2%" alt="space" /&gt; 
 &lt;a href="https://twitter.com/ultralytics"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-twitter.png" width="2%" alt="Ultralytics Twitter" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="2%" alt="space" /&gt; 
 &lt;a href="https://youtube.com/ultralytics?sub_confirmation=1"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-youtube.png" width="2%" alt="Ultralytics YouTube" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="2%" alt="space" /&gt; 
 &lt;a href="https://www.tiktok.com/@ultralytics"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-tiktok.png" width="2%" alt="Ultralytics TikTok" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="2%" alt="space" /&gt; 
 &lt;a href="https://ultralytics.com/bilibili"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-bilibili.png" width="2%" alt="Ultralytics BiliBili" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="2%" alt="space" /&gt; 
 &lt;a href="https://discord.com/invite/ultralytics"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-discord.png" width="2%" alt="Ultralytics Discord" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;📄 Documentation&lt;/h2&gt; 
&lt;p&gt;See below for quickstart installation and usage examples. For comprehensive guidance on training, validation, prediction, and deployment, refer to our full &lt;a href="https://docs.ultralytics.com/"&gt;Ultralytics Docs&lt;/a&gt;.&lt;/p&gt; 
&lt;details open&gt; 
 &lt;summary&gt;Install&lt;/summary&gt; 
 &lt;p&gt;Install the &lt;code&gt;ultralytics&lt;/code&gt; package, including all &lt;a href="https://github.com/ultralytics/ultralytics/raw/main/pyproject.toml"&gt;requirements&lt;/a&gt;, in a &lt;a href="https://www.python.org/"&gt;&lt;strong&gt;Python&amp;gt;=3.8&lt;/strong&gt;&lt;/a&gt; environment with &lt;a href="https://pytorch.org/get-started/locally/"&gt;&lt;strong&gt;PyTorch&amp;gt;=1.8&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://pypi.org/project/ultralytics/"&gt;&lt;img src="https://img.shields.io/pypi/v/ultralytics?logo=pypi&amp;amp;logoColor=white" alt="PyPI - Version" /&gt;&lt;/a&gt; &lt;a href="https://clickpy.clickhouse.com/dashboard/ultralytics"&gt;&lt;img src="https://static.pepy.tech/badge/ultralytics" alt="Ultralytics Downloads" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/ultralytics/"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/ultralytics?logo=python&amp;amp;logoColor=gold" alt="PyPI - Python Version" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip install ultralytics
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;For alternative installation methods, including &lt;a href="https://anaconda.org/conda-forge/ultralytics"&gt;Conda&lt;/a&gt;, &lt;a href="https://hub.docker.com/r/ultralytics/ultralytics"&gt;Docker&lt;/a&gt;, and building from source via Git, please consult the &lt;a href="https://docs.ultralytics.com/quickstart/"&gt;Quickstart Guide&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://anaconda.org/conda-forge/ultralytics"&gt;&lt;img src="https://img.shields.io/conda/vn/conda-forge/ultralytics?logo=condaforge" alt="Conda Version" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/ultralytics/ultralytics"&gt;&lt;img src="https://img.shields.io/docker/v/ultralytics/ultralytics?sort=semver&amp;amp;logo=docker" alt="Docker Image Version" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/ultralytics/ultralytics"&gt;&lt;img src="https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker" alt="Ultralytics Docker Pulls" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details open&gt; 
 &lt;summary&gt;Usage&lt;/summary&gt; 
 &lt;h3&gt;CLI&lt;/h3&gt; 
 &lt;p&gt;You can use Ultralytics YOLO directly from the Command Line Interface (CLI) with the &lt;code&gt;yolo&lt;/code&gt; command:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Predict using a pretrained YOLO model (e.g., YOLO11n) on an image
yolo predict model=yolo11n.pt source='https://ultralytics.com/images/bus.jpg'
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;The &lt;code&gt;yolo&lt;/code&gt; command supports various tasks and modes, accepting additional arguments like &lt;code&gt;imgsz=640&lt;/code&gt;. Explore the YOLO &lt;a href="https://docs.ultralytics.com/usage/cli/"&gt;CLI Docs&lt;/a&gt; for more examples.&lt;/p&gt; 
 &lt;h3&gt;Python&lt;/h3&gt; 
 &lt;p&gt;Ultralytics YOLO can also be integrated directly into your Python projects. It accepts the same &lt;a href="https://docs.ultralytics.com/usage/cfg/"&gt;configuration arguments&lt;/a&gt; as the CLI:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from ultralytics import YOLO

# Load a pretrained YOLO11n model
model = YOLO("yolo11n.pt")

# Train the model on the COCO8 dataset for 100 epochs
train_results = model.train(
    data="coco8.yaml",  # Path to dataset configuration file
    epochs=100,  # Number of training epochs
    imgsz=640,  # Image size for training
    device="cpu",  # Device to run on (e.g., 'cpu', 0, [0,1,2,3])
)

# Evaluate the model's performance on the validation set
metrics = model.val()

# Perform object detection on an image
results = model("path/to/image.jpg")  # Predict on an image
results[0].show()  # Display results

# Export the model to ONNX format for deployment
path = model.export(format="onnx")  # Returns the path to the exported model
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Discover more examples in the YOLO &lt;a href="https://docs.ultralytics.com/usage/python/"&gt;Python Docs&lt;/a&gt;.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;✨ Models&lt;/h2&gt; 
&lt;p&gt;Ultralytics supports a wide range of YOLO models, from early versions like &lt;a href="https://docs.ultralytics.com/models/yolov3/"&gt;YOLOv3&lt;/a&gt; to the latest &lt;a href="https://docs.ultralytics.com/models/yolo11/"&gt;YOLO11&lt;/a&gt;. The tables below showcase YOLO11 models pretrained on the &lt;a href="https://docs.ultralytics.com/datasets/detect/coco/"&gt;COCO&lt;/a&gt; dataset for &lt;a href="https://docs.ultralytics.com/tasks/detect/"&gt;Detection&lt;/a&gt;, &lt;a href="https://docs.ultralytics.com/tasks/segment/"&gt;Segmentation&lt;/a&gt;, and &lt;a href="https://docs.ultralytics.com/tasks/pose/"&gt;Pose Estimation&lt;/a&gt;. Additionally, &lt;a href="https://docs.ultralytics.com/tasks/classify/"&gt;Classification&lt;/a&gt; models pretrained on the &lt;a href="https://docs.ultralytics.com/datasets/classify/imagenet/"&gt;ImageNet&lt;/a&gt; dataset are available. &lt;a href="https://docs.ultralytics.com/modes/track/"&gt;Tracking&lt;/a&gt; mode is compatible with all Detection, Segmentation, and Pose models. All &lt;a href="https://docs.ultralytics.com/models/"&gt;Models&lt;/a&gt; are automatically downloaded from the latest Ultralytics &lt;a href="https://github.com/ultralytics/assets/releases"&gt;release&lt;/a&gt; upon first use.&lt;/p&gt; 
&lt;a href="https://docs.ultralytics.com/tasks/" target="_blank"&gt; &lt;img width="100%" src="https://github.com/ultralytics/docs/releases/download/0/ultralytics-yolov8-tasks-banner.avif" alt="Ultralytics YOLO supported tasks" /&gt; &lt;/a&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;details open&gt;
 &lt;summary&gt;Detection (COCO)&lt;/summary&gt; 
 &lt;p&gt;Explore the &lt;a href="https://docs.ultralytics.com/tasks/detect/"&gt;Detection Docs&lt;/a&gt; for usage examples. These models are trained on the &lt;a href="https://cocodataset.org/"&gt;COCO dataset&lt;/a&gt;, featuring 80 object classes.&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;size&lt;br /&gt;&lt;sup&gt;(pixels)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;mAP&lt;sup&gt;val&lt;br /&gt;50-95&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Speed&lt;br /&gt;&lt;sup&gt;CPU ONNX&lt;br /&gt;(ms)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Speed&lt;br /&gt;&lt;sup&gt;T4 TensorRT10&lt;br /&gt;(ms)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;params&lt;br /&gt;&lt;sup&gt;(M)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;FLOPs&lt;br /&gt;&lt;sup&gt;(B)&lt;/sup&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt"&gt;YOLO11n&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;39.5&lt;/td&gt; 
    &lt;td&gt;56.1 ± 0.8&lt;/td&gt; 
    &lt;td&gt;1.5 ± 0.0&lt;/td&gt; 
    &lt;td&gt;2.6&lt;/td&gt; 
    &lt;td&gt;6.5&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11s.pt"&gt;YOLO11s&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;47.0&lt;/td&gt; 
    &lt;td&gt;90.0 ± 1.2&lt;/td&gt; 
    &lt;td&gt;2.5 ± 0.0&lt;/td&gt; 
    &lt;td&gt;9.4&lt;/td&gt; 
    &lt;td&gt;21.5&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m.pt"&gt;YOLO11m&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;51.5&lt;/td&gt; 
    &lt;td&gt;183.2 ± 2.0&lt;/td&gt; 
    &lt;td&gt;4.7 ± 0.1&lt;/td&gt; 
    &lt;td&gt;20.1&lt;/td&gt; 
    &lt;td&gt;68.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11l.pt"&gt;YOLO11l&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;53.4&lt;/td&gt; 
    &lt;td&gt;238.6 ± 1.4&lt;/td&gt; 
    &lt;td&gt;6.2 ± 0.1&lt;/td&gt; 
    &lt;td&gt;25.3&lt;/td&gt; 
    &lt;td&gt;86.9&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11x.pt"&gt;YOLO11x&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;54.7&lt;/td&gt; 
    &lt;td&gt;462.8 ± 6.7&lt;/td&gt; 
    &lt;td&gt;11.3 ± 0.2&lt;/td&gt; 
    &lt;td&gt;56.9&lt;/td&gt; 
    &lt;td&gt;194.9&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;mAP&lt;sup&gt;val&lt;/sup&gt;&lt;/strong&gt; values refer to single-model single-scale performance on the &lt;a href="https://cocodataset.org/"&gt;COCO val2017&lt;/a&gt; dataset. See &lt;a href="https://docs.ultralytics.com/guides/yolo-performance-metrics/"&gt;YOLO Performance Metrics&lt;/a&gt; for details. &lt;br /&gt;Reproduce with &lt;code&gt;yolo val detect data=coco.yaml device=0&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Speed&lt;/strong&gt; metrics are averaged over COCO val images using an &lt;a href="https://aws.amazon.com/ec2/instance-types/p4/"&gt;Amazon EC2 P4d&lt;/a&gt; instance. CPU speeds measured with &lt;a href="https://onnx.ai/"&gt;ONNX&lt;/a&gt; export. GPU speeds measured with &lt;a href="https://developer.nvidia.com/tensorrt"&gt;TensorRT&lt;/a&gt; export. &lt;br /&gt;Reproduce with &lt;code&gt;yolo val detect data=coco.yaml batch=1 device=0|cpu&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Segmentation (COCO)&lt;/summary&gt; 
 &lt;p&gt;Refer to the &lt;a href="https://docs.ultralytics.com/tasks/segment/"&gt;Segmentation Docs&lt;/a&gt; for usage examples. These models are trained on &lt;a href="https://docs.ultralytics.com/datasets/segment/coco/"&gt;COCO-Seg&lt;/a&gt;, including 80 classes.&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;size&lt;br /&gt;&lt;sup&gt;(pixels)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;mAP&lt;sup&gt;box&lt;br /&gt;50-95&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;mAP&lt;sup&gt;mask&lt;br /&gt;50-95&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Speed&lt;br /&gt;&lt;sup&gt;CPU ONNX&lt;br /&gt;(ms)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Speed&lt;br /&gt;&lt;sup&gt;T4 TensorRT10&lt;br /&gt;(ms)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;params&lt;br /&gt;&lt;sup&gt;(M)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;FLOPs&lt;br /&gt;&lt;sup&gt;(B)&lt;/sup&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-seg.pt"&gt;YOLO11n-seg&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;38.9&lt;/td&gt; 
    &lt;td&gt;32.0&lt;/td&gt; 
    &lt;td&gt;65.9 ± 1.1&lt;/td&gt; 
    &lt;td&gt;1.8 ± 0.0&lt;/td&gt; 
    &lt;td&gt;2.9&lt;/td&gt; 
    &lt;td&gt;10.4&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11s-seg.pt"&gt;YOLO11s-seg&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;46.6&lt;/td&gt; 
    &lt;td&gt;37.8&lt;/td&gt; 
    &lt;td&gt;117.6 ± 4.9&lt;/td&gt; 
    &lt;td&gt;2.9 ± 0.0&lt;/td&gt; 
    &lt;td&gt;10.1&lt;/td&gt; 
    &lt;td&gt;35.5&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m-seg.pt"&gt;YOLO11m-seg&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;51.5&lt;/td&gt; 
    &lt;td&gt;41.5&lt;/td&gt; 
    &lt;td&gt;281.6 ± 1.2&lt;/td&gt; 
    &lt;td&gt;6.3 ± 0.1&lt;/td&gt; 
    &lt;td&gt;22.4&lt;/td&gt; 
    &lt;td&gt;123.3&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11l-seg.pt"&gt;YOLO11l-seg&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;53.4&lt;/td&gt; 
    &lt;td&gt;42.9&lt;/td&gt; 
    &lt;td&gt;344.2 ± 3.2&lt;/td&gt; 
    &lt;td&gt;7.8 ± 0.2&lt;/td&gt; 
    &lt;td&gt;27.6&lt;/td&gt; 
    &lt;td&gt;142.2&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11x-seg.pt"&gt;YOLO11x-seg&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;54.7&lt;/td&gt; 
    &lt;td&gt;43.8&lt;/td&gt; 
    &lt;td&gt;664.5 ± 3.2&lt;/td&gt; 
    &lt;td&gt;15.8 ± 0.7&lt;/td&gt; 
    &lt;td&gt;62.1&lt;/td&gt; 
    &lt;td&gt;319.0&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;mAP&lt;sup&gt;val&lt;/sup&gt;&lt;/strong&gt; values are for single-model single-scale on the &lt;a href="https://cocodataset.org/"&gt;COCO val2017&lt;/a&gt; dataset. See &lt;a href="https://docs.ultralytics.com/guides/yolo-performance-metrics/"&gt;YOLO Performance Metrics&lt;/a&gt; for details. &lt;br /&gt;Reproduce with &lt;code&gt;yolo val segment data=coco.yaml device=0&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Speed&lt;/strong&gt; metrics are averaged over COCO val images using an &lt;a href="https://aws.amazon.com/ec2/instance-types/p4/"&gt;Amazon EC2 P4d&lt;/a&gt; instance. CPU speeds measured with &lt;a href="https://onnx.ai/"&gt;ONNX&lt;/a&gt; export. GPU speeds measured with &lt;a href="https://developer.nvidia.com/tensorrt"&gt;TensorRT&lt;/a&gt; export. &lt;br /&gt;Reproduce with &lt;code&gt;yolo val segment data=coco.yaml batch=1 device=0|cpu&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Classification (ImageNet)&lt;/summary&gt; 
 &lt;p&gt;Consult the &lt;a href="https://docs.ultralytics.com/tasks/classify/"&gt;Classification Docs&lt;/a&gt; for usage examples. These models are trained on &lt;a href="https://docs.ultralytics.com/datasets/classify/imagenet/"&gt;ImageNet&lt;/a&gt;, covering 1000 classes.&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;size&lt;br /&gt;&lt;sup&gt;(pixels)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;acc&lt;br /&gt;&lt;sup&gt;top1&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;acc&lt;br /&gt;&lt;sup&gt;top5&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Speed&lt;br /&gt;&lt;sup&gt;CPU ONNX&lt;br /&gt;(ms)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Speed&lt;br /&gt;&lt;sup&gt;T4 TensorRT10&lt;br /&gt;(ms)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;params&lt;br /&gt;&lt;sup&gt;(M)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;FLOPs&lt;br /&gt;&lt;sup&gt;(B) at 224&lt;/sup&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-cls.pt"&gt;YOLO11n-cls&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;224&lt;/td&gt; 
    &lt;td&gt;70.0&lt;/td&gt; 
    &lt;td&gt;89.4&lt;/td&gt; 
    &lt;td&gt;5.0 ± 0.3&lt;/td&gt; 
    &lt;td&gt;1.1 ± 0.0&lt;/td&gt; 
    &lt;td&gt;1.6&lt;/td&gt; 
    &lt;td&gt;0.5&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11s-cls.pt"&gt;YOLO11s-cls&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;224&lt;/td&gt; 
    &lt;td&gt;75.4&lt;/td&gt; 
    &lt;td&gt;92.7&lt;/td&gt; 
    &lt;td&gt;7.9 ± 0.2&lt;/td&gt; 
    &lt;td&gt;1.3 ± 0.0&lt;/td&gt; 
    &lt;td&gt;5.5&lt;/td&gt; 
    &lt;td&gt;1.6&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m-cls.pt"&gt;YOLO11m-cls&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;224&lt;/td&gt; 
    &lt;td&gt;77.3&lt;/td&gt; 
    &lt;td&gt;93.9&lt;/td&gt; 
    &lt;td&gt;17.2 ± 0.4&lt;/td&gt; 
    &lt;td&gt;2.0 ± 0.0&lt;/td&gt; 
    &lt;td&gt;10.4&lt;/td&gt; 
    &lt;td&gt;5.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11l-cls.pt"&gt;YOLO11l-cls&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;224&lt;/td&gt; 
    &lt;td&gt;78.3&lt;/td&gt; 
    &lt;td&gt;94.3&lt;/td&gt; 
    &lt;td&gt;23.2 ± 0.3&lt;/td&gt; 
    &lt;td&gt;2.8 ± 0.0&lt;/td&gt; 
    &lt;td&gt;12.9&lt;/td&gt; 
    &lt;td&gt;6.2&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11x-cls.pt"&gt;YOLO11x-cls&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;224&lt;/td&gt; 
    &lt;td&gt;79.5&lt;/td&gt; 
    &lt;td&gt;94.9&lt;/td&gt; 
    &lt;td&gt;41.4 ± 0.9&lt;/td&gt; 
    &lt;td&gt;3.8 ± 0.0&lt;/td&gt; 
    &lt;td&gt;28.4&lt;/td&gt; 
    &lt;td&gt;13.7&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;acc&lt;/strong&gt; values represent model accuracy on the &lt;a href="https://www.image-net.org/"&gt;ImageNet&lt;/a&gt; dataset validation set. &lt;br /&gt;Reproduce with &lt;code&gt;yolo val classify data=path/to/ImageNet device=0&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Speed&lt;/strong&gt; metrics are averaged over ImageNet val images using an &lt;a href="https://aws.amazon.com/ec2/instance-types/p4/"&gt;Amazon EC2 P4d&lt;/a&gt; instance. CPU speeds measured with &lt;a href="https://onnx.ai/"&gt;ONNX&lt;/a&gt; export. GPU speeds measured with &lt;a href="https://developer.nvidia.com/tensorrt"&gt;TensorRT&lt;/a&gt; export. &lt;br /&gt;Reproduce with &lt;code&gt;yolo val classify data=path/to/ImageNet batch=1 device=0|cpu&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Pose (COCO)&lt;/summary&gt; 
 &lt;p&gt;See the &lt;a href="https://docs.ultralytics.com/tasks/pose/"&gt;Pose Estimation Docs&lt;/a&gt; for usage examples. These models are trained on &lt;a href="https://docs.ultralytics.com/datasets/pose/coco/"&gt;COCO-Pose&lt;/a&gt;, focusing on the 'person' class.&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;size&lt;br /&gt;&lt;sup&gt;(pixels)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;mAP&lt;sup&gt;pose&lt;br /&gt;50-95&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;mAP&lt;sup&gt;pose&lt;br /&gt;50&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Speed&lt;br /&gt;&lt;sup&gt;CPU ONNX&lt;br /&gt;(ms)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Speed&lt;br /&gt;&lt;sup&gt;T4 TensorRT10&lt;br /&gt;(ms)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;params&lt;br /&gt;&lt;sup&gt;(M)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;FLOPs&lt;br /&gt;&lt;sup&gt;(B)&lt;/sup&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-pose.pt"&gt;YOLO11n-pose&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;50.0&lt;/td&gt; 
    &lt;td&gt;81.0&lt;/td&gt; 
    &lt;td&gt;52.4 ± 0.5&lt;/td&gt; 
    &lt;td&gt;1.7 ± 0.0&lt;/td&gt; 
    &lt;td&gt;2.9&lt;/td&gt; 
    &lt;td&gt;7.6&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11s-pose.pt"&gt;YOLO11s-pose&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;58.9&lt;/td&gt; 
    &lt;td&gt;86.3&lt;/td&gt; 
    &lt;td&gt;90.5 ± 0.6&lt;/td&gt; 
    &lt;td&gt;2.6 ± 0.0&lt;/td&gt; 
    &lt;td&gt;9.9&lt;/td&gt; 
    &lt;td&gt;23.2&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m-pose.pt"&gt;YOLO11m-pose&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;64.9&lt;/td&gt; 
    &lt;td&gt;89.4&lt;/td&gt; 
    &lt;td&gt;187.3 ± 0.8&lt;/td&gt; 
    &lt;td&gt;4.9 ± 0.1&lt;/td&gt; 
    &lt;td&gt;20.9&lt;/td&gt; 
    &lt;td&gt;71.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11l-pose.pt"&gt;YOLO11l-pose&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;66.1&lt;/td&gt; 
    &lt;td&gt;89.9&lt;/td&gt; 
    &lt;td&gt;247.7 ± 1.1&lt;/td&gt; 
    &lt;td&gt;6.4 ± 0.1&lt;/td&gt; 
    &lt;td&gt;26.2&lt;/td&gt; 
    &lt;td&gt;90.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11x-pose.pt"&gt;YOLO11x-pose&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;69.5&lt;/td&gt; 
    &lt;td&gt;91.1&lt;/td&gt; 
    &lt;td&gt;488.0 ± 13.9&lt;/td&gt; 
    &lt;td&gt;12.1 ± 0.2&lt;/td&gt; 
    &lt;td&gt;58.8&lt;/td&gt; 
    &lt;td&gt;203.3&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;mAP&lt;sup&gt;val&lt;/sup&gt;&lt;/strong&gt; values are for single-model single-scale on the &lt;a href="https://docs.ultralytics.com/datasets/pose/coco/"&gt;COCO Keypoints val2017&lt;/a&gt; dataset. See &lt;a href="https://docs.ultralytics.com/guides/yolo-performance-metrics/"&gt;YOLO Performance Metrics&lt;/a&gt; for details. &lt;br /&gt;Reproduce with &lt;code&gt;yolo val pose data=coco-pose.yaml device=0&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Speed&lt;/strong&gt; metrics are averaged over COCO val images using an &lt;a href="https://aws.amazon.com/ec2/instance-types/p4/"&gt;Amazon EC2 P4d&lt;/a&gt; instance. CPU speeds measured with &lt;a href="https://onnx.ai/"&gt;ONNX&lt;/a&gt; export. GPU speeds measured with &lt;a href="https://developer.nvidia.com/tensorrt"&gt;TensorRT&lt;/a&gt; export. &lt;br /&gt;Reproduce with &lt;code&gt;yolo val pose data=coco-pose.yaml batch=1 device=0|cpu&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Oriented Bounding Boxes (DOTAv1)&lt;/summary&gt; 
 &lt;p&gt;Check the &lt;a href="https://docs.ultralytics.com/tasks/obb/"&gt;OBB Docs&lt;/a&gt; for usage examples. These models are trained on &lt;a href="https://docs.ultralytics.com/datasets/obb/dota-v2/#dota-v10/"&gt;DOTAv1&lt;/a&gt;, including 15 classes.&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;size&lt;br /&gt;&lt;sup&gt;(pixels)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;mAP&lt;sup&gt;test&lt;br /&gt;50&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Speed&lt;br /&gt;&lt;sup&gt;CPU ONNX&lt;br /&gt;(ms)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Speed&lt;br /&gt;&lt;sup&gt;T4 TensorRT10&lt;br /&gt;(ms)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;params&lt;br /&gt;&lt;sup&gt;(M)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;FLOPs&lt;br /&gt;&lt;sup&gt;(B)&lt;/sup&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-obb.pt"&gt;YOLO11n-obb&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;1024&lt;/td&gt; 
    &lt;td&gt;78.4&lt;/td&gt; 
    &lt;td&gt;117.6 ± 0.8&lt;/td&gt; 
    &lt;td&gt;4.4 ± 0.0&lt;/td&gt; 
    &lt;td&gt;2.7&lt;/td&gt; 
    &lt;td&gt;17.2&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11s-obb.pt"&gt;YOLO11s-obb&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;1024&lt;/td&gt; 
    &lt;td&gt;79.5&lt;/td&gt; 
    &lt;td&gt;219.4 ± 4.0&lt;/td&gt; 
    &lt;td&gt;5.1 ± 0.0&lt;/td&gt; 
    &lt;td&gt;9.7&lt;/td&gt; 
    &lt;td&gt;57.5&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m-obb.pt"&gt;YOLO11m-obb&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;1024&lt;/td&gt; 
    &lt;td&gt;80.9&lt;/td&gt; 
    &lt;td&gt;562.8 ± 2.9&lt;/td&gt; 
    &lt;td&gt;10.1 ± 0.4&lt;/td&gt; 
    &lt;td&gt;20.9&lt;/td&gt; 
    &lt;td&gt;183.5&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11l-obb.pt"&gt;YOLO11l-obb&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;1024&lt;/td&gt; 
    &lt;td&gt;81.0&lt;/td&gt; 
    &lt;td&gt;712.5 ± 5.0&lt;/td&gt; 
    &lt;td&gt;13.5 ± 0.6&lt;/td&gt; 
    &lt;td&gt;26.2&lt;/td&gt; 
    &lt;td&gt;232.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11x-obb.pt"&gt;YOLO11x-obb&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;1024&lt;/td&gt; 
    &lt;td&gt;81.3&lt;/td&gt; 
    &lt;td&gt;1408.6 ± 7.7&lt;/td&gt; 
    &lt;td&gt;28.6 ± 1.0&lt;/td&gt; 
    &lt;td&gt;58.8&lt;/td&gt; 
    &lt;td&gt;520.2&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;mAP&lt;sup&gt;test&lt;/sup&gt;&lt;/strong&gt; values are for single-model multiscale performance on the &lt;a href="https://captain-whu.github.io/DOTA/dataset.html"&gt;DOTAv1 test set&lt;/a&gt;. &lt;br /&gt;Reproduce by &lt;code&gt;yolo val obb data=DOTAv1.yaml device=0 split=test&lt;/code&gt; and submit merged results to the &lt;a href="https://captain-whu.github.io/DOTA/evaluation.html"&gt;DOTA evaluation server&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Speed&lt;/strong&gt; metrics are averaged over &lt;a href="https://docs.ultralytics.com/datasets/obb/dota-v2/#dota-v10"&gt;DOTAv1 val images&lt;/a&gt; using an &lt;a href="https://aws.amazon.com/ec2/instance-types/p4/"&gt;Amazon EC2 P4d&lt;/a&gt; instance. CPU speeds measured with &lt;a href="https://onnx.ai/"&gt;ONNX&lt;/a&gt; export. GPU speeds measured with &lt;a href="https://developer.nvidia.com/tensorrt"&gt;TensorRT&lt;/a&gt; export. &lt;br /&gt;Reproduce by &lt;code&gt;yolo val obb data=DOTAv1.yaml batch=1 device=0|cpu&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;🧩 Integrations&lt;/h2&gt; 
&lt;p&gt;Our key integrations with leading AI platforms extend the functionality of Ultralytics' offerings, enhancing tasks like dataset labeling, training, visualization, and model management. Discover how Ultralytics, in collaboration with partners like &lt;a href="https://docs.ultralytics.com/integrations/weights-biases/"&gt;Weights &amp;amp; Biases&lt;/a&gt;, &lt;a href="https://docs.ultralytics.com/integrations/comet/"&gt;Comet ML&lt;/a&gt;, &lt;a href="https://docs.ultralytics.com/integrations/roboflow/"&gt;Roboflow&lt;/a&gt;, and &lt;a href="https://docs.ultralytics.com/integrations/openvino/"&gt;Intel OpenVINO&lt;/a&gt;, can optimize your AI workflow. Explore more at &lt;a href="https://docs.ultralytics.com/integrations/"&gt;Ultralytics Integrations&lt;/a&gt;.&lt;/p&gt; 
&lt;a href="https://docs.ultralytics.com/integrations/" target="_blank"&gt; &lt;img width="100%" src="https://github.com/ultralytics/assets/raw/main/yolov8/banner-integrations.png" alt="Ultralytics active learning integrations" /&gt; &lt;/a&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://www.ultralytics.com/hub"&gt; &lt;img src="https://github.com/ultralytics/assets/raw/main/partners/logo-ultralytics-hub.png" width="10%" alt="Ultralytics HUB logo" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="15%" height="0" alt="space" /&gt; 
 &lt;a href="https://docs.ultralytics.com/integrations/weights-biases/"&gt; &lt;img src="https://github.com/ultralytics/assets/raw/main/partners/logo-wb.png" width="10%" alt="Weights &amp;amp; Biases logo" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="15%" height="0" alt="space" /&gt; 
 &lt;a href="https://docs.ultralytics.com/integrations/comet/"&gt; &lt;img src="https://github.com/ultralytics/assets/raw/main/partners/logo-comet.png" width="10%" alt="Comet ML logo" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="15%" height="0" alt="space" /&gt; 
 &lt;a href="https://docs.ultralytics.com/integrations/neural-magic/"&gt; &lt;img src="https://github.com/ultralytics/assets/raw/main/partners/logo-neuralmagic.png" width="10%" alt="Neural Magic logo" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Ultralytics HUB 🌟&lt;/th&gt; 
   &lt;th align="center"&gt;Weights &amp;amp; Biases&lt;/th&gt; 
   &lt;th align="center"&gt;Comet&lt;/th&gt; 
   &lt;th align="center"&gt;Neural Magic&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Streamline YOLO workflows: Label, train, and deploy effortlessly with &lt;a href="https://hub.ultralytics.com/"&gt;Ultralytics HUB&lt;/a&gt;. Try now!&lt;/td&gt; 
   &lt;td align="center"&gt;Track experiments, hyperparameters, and results with &lt;a href="https://docs.ultralytics.com/integrations/weights-biases/"&gt;Weights &amp;amp; Biases&lt;/a&gt;.&lt;/td&gt; 
   &lt;td align="center"&gt;Free forever, &lt;a href="https://docs.ultralytics.com/integrations/comet/"&gt;Comet ML&lt;/a&gt; lets you save YOLO models, resume training, and interactively visualize predictions.&lt;/td&gt; 
   &lt;td align="center"&gt;Run YOLO inference up to 6x faster with &lt;a href="https://docs.ultralytics.com/integrations/neural-magic/"&gt;Neural Magic DeepSparse&lt;/a&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;🌟 Ultralytics HUB&lt;/h2&gt; 
&lt;p&gt;Experience seamless AI with &lt;a href="https://hub.ultralytics.com/"&gt;Ultralytics HUB&lt;/a&gt;, the all-in-one platform for data visualization, training YOLO models, and deployment—no coding required. Transform images into actionable insights and bring your AI visions to life effortlessly using our cutting-edge platform and user-friendly &lt;a href="https://www.ultralytics.com/app-install"&gt;Ultralytics App&lt;/a&gt;. Start your journey for &lt;strong&gt;Free&lt;/strong&gt; today!&lt;/p&gt; 
&lt;a href="https://www.ultralytics.com/hub" target="_blank"&gt; &lt;img width="100%" src="https://github.com/ultralytics/assets/raw/main/im/ultralytics-hub.png" alt="Ultralytics HUB preview image" /&gt;&lt;/a&gt; 
&lt;h2&gt;🤝 Contribute&lt;/h2&gt; 
&lt;p&gt;We thrive on community collaboration! Ultralytics YOLO wouldn't be the SOTA framework it is without contributions from developers like you. Please see our &lt;a href="https://docs.ultralytics.com/help/contributing/"&gt;Contributing Guide&lt;/a&gt; to get started. We also welcome your feedback—share your experience by completing our &lt;a href="https://www.ultralytics.com/survey?utm_source=github&amp;amp;utm_medium=social&amp;amp;utm_campaign=Survey"&gt;Survey&lt;/a&gt;. A huge &lt;strong&gt;Thank You&lt;/strong&gt; 🙏 to everyone who contributes!&lt;/p&gt; 
&lt;!-- SVG image from https://opencollective.com/ultralytics/contributors.svg?width=1280 --&gt; 
&lt;p&gt;&lt;a href="https://github.com/ultralytics/ultralytics/graphs/contributors"&gt;&lt;img src="https://raw.githubusercontent.com/ultralytics/assets/main/im/image-contributors.png" alt="Ultralytics open-source contributors" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;We look forward to your contributions to help make the Ultralytics ecosystem even better!&lt;/p&gt; 
&lt;h2&gt;📜 License&lt;/h2&gt; 
&lt;p&gt;Ultralytics offers two licensing options to suit different needs:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;AGPL-3.0 License&lt;/strong&gt;: This &lt;a href="https://opensource.org/license"&gt;OSI-approved&lt;/a&gt; open-source license is perfect for students, researchers, and enthusiasts. It encourages open collaboration and knowledge sharing. See the &lt;a href="https://github.com/ultralytics/ultralytics/raw/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for full details.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Ultralytics Enterprise License&lt;/strong&gt;: Designed for commercial use, this license allows for the seamless integration of Ultralytics software and AI models into commercial products and services, bypassing the open-source requirements of AGPL-3.0. If your use case involves commercial deployment, please contact us via &lt;a href="https://www.ultralytics.com/license"&gt;Ultralytics Licensing&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;📞 Contact&lt;/h2&gt; 
&lt;p&gt;For bug reports and feature requests related to Ultralytics software, please visit &lt;a href="https://github.com/ultralytics/ultralytics/issues"&gt;GitHub Issues&lt;/a&gt;. For questions, discussions, and community support, join our active communities on &lt;a href="https://discord.com/invite/ultralytics"&gt;Discord&lt;/a&gt;, &lt;a href="https://www.reddit.com/r/ultralytics/"&gt;Reddit&lt;/a&gt;, and the &lt;a href="https://community.ultralytics.com/"&gt;Ultralytics Community Forums&lt;/a&gt;. We're here to help with all things Ultralytics!&lt;/p&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/ultralytics"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-github.png" width="3%" alt="Ultralytics GitHub" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="3%" alt="space" /&gt; 
 &lt;a href="https://www.linkedin.com/company/ultralytics/"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-linkedin.png" width="3%" alt="Ultralytics LinkedIn" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="3%" alt="space" /&gt; 
 &lt;a href="https://twitter.com/ultralytics"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-twitter.png" width="3%" alt="Ultralytics Twitter" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="3%" alt="space" /&gt; 
 &lt;a href="https://youtube.com/ultralytics?sub_confirmation=1"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-youtube.png" width="3%" alt="Ultralytics YouTube" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="3%" alt="space" /&gt; 
 &lt;a href="https://www.tiktok.com/@ultralytics"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-tiktok.png" width="3%" alt="Ultralytics TikTok" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="3%" alt="space" /&gt; 
 &lt;a href="https://ultralytics.com/bilibili"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-bilibili.png" width="3%" alt="Ultralytics BiliBili" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="3%" alt="space" /&gt; 
 &lt;a href="https://discord.com/invite/ultralytics"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-discord.png" width="3%" alt="Ultralytics Discord" /&gt;&lt;/a&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>ml-explore/mlx-lm</title>
      <link>https://github.com/ml-explore/mlx-lm</link>
      <description>&lt;p&gt;Run LLMs with MLX&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;MLX LM&lt;/h2&gt; 
&lt;p&gt;MLX LM is a Python package for generating text and fine-tuning large language models on Apple silicon with MLX.&lt;/p&gt; 
&lt;p&gt;Some key features include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Integration with the Hugging Face Hub to easily use thousands of LLMs with a single command.&lt;/li&gt; 
 &lt;li&gt;Support for quantizing and uploading models to the Hugging Face Hub.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ml-explore/mlx-lm/raw/main/mlx_lm/LORA.md"&gt;Low-rank and full model fine-tuning&lt;/a&gt; with support for quantized models.&lt;/li&gt; 
 &lt;li&gt;Distributed inference and fine-tuning with &lt;code&gt;mx.distributed&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The easiest way to get started is to install the &lt;code&gt;mlx-lm&lt;/code&gt; package:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;With &lt;code&gt;pip&lt;/code&gt;&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip install mlx-lm
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;With &lt;code&gt;conda&lt;/code&gt;&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;conda install -c conda-forge mlx-lm
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Quick Start&lt;/h3&gt; 
&lt;p&gt;To generate text with an LLM use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mlx_lm.generate --prompt "How tall is Mt Everest?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To chat with an LLM use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mlx_lm.chat
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will give you a chat REPL that you can use to interact with the LLM. The chat context is preserved during the lifetime of the REPL.&lt;/p&gt; 
&lt;p&gt;Commands in &lt;code&gt;mlx-lm&lt;/code&gt; typically take command line options which let you specify the model, sampling parameters, and more. Use &lt;code&gt;-h&lt;/code&gt; to see a list of available options for a command, e.g.:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mlx_lm.generate -h
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The default model for generation and chat is &lt;code&gt;mlx-community/Llama-3.2-3B-Instruct-4bit&lt;/code&gt;. You can specify any MLX-compatible model with the &lt;code&gt;--model&lt;/code&gt; flag. Thousands are available in the &lt;a href="https://huggingface.co/mlx-community"&gt;MLX Community&lt;/a&gt; Hugging Face organization.&lt;/p&gt; 
&lt;h3&gt;Python API&lt;/h3&gt; 
&lt;p&gt;You can use &lt;code&gt;mlx-lm&lt;/code&gt; as a module:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_lm import load, generate

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

prompt = "Write a story about Einstein"

messages = [{"role": "user", "content": prompt}]
prompt = tokenizer.apply_chat_template(
    messages, add_generation_prompt=True
)

text = generate(model, tokenizer, prompt=prompt, verbose=True)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To see a description of all the arguments you can do:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; help(generate)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Check out the &lt;a href="https://github.com/ml-explore/mlx-lm/tree/main/mlx_lm/examples/generate_response.py"&gt;generation example&lt;/a&gt; to see how to use the API in more detail. Check out the &lt;a href="https://github.com/ml-explore/mlx-lm/tree/main/mlx_lm/examples/batch_generate_response.py"&gt;batch generation example&lt;/a&gt; to see how to efficiently generate continuations for a batch of prompts.&lt;/p&gt; 
&lt;p&gt;The &lt;code&gt;mlx-lm&lt;/code&gt; package also comes with functionality to quantize and optionally upload models to the Hugging Face Hub.&lt;/p&gt; 
&lt;p&gt;You can convert models using the Python API:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_lm import convert

repo = "mistralai/Mistral-7B-Instruct-v0.3"
upload_repo = "mlx-community/My-Mistral-7B-Instruct-v0.3-4bit"

convert(repo, quantize=True, upload_repo=upload_repo)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will generate a 4-bit quantized Mistral 7B and upload it to the repo &lt;code&gt;mlx-community/My-Mistral-7B-Instruct-v0.3-4bit&lt;/code&gt;. It will also save the converted model in the path &lt;code&gt;mlx_model&lt;/code&gt; by default.&lt;/p&gt; 
&lt;p&gt;To see a description of all the arguments you can do:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; help(convert)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Streaming&lt;/h4&gt; 
&lt;p&gt;For streaming generation, use the &lt;code&gt;stream_generate&lt;/code&gt; function. This yields a generation response object.&lt;/p&gt; 
&lt;p&gt;For example,&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_lm import load, stream_generate

repo = "mlx-community/Mistral-7B-Instruct-v0.3-4bit"
model, tokenizer = load(repo)

prompt = "Write a story about Einstein"

messages = [{"role": "user", "content": prompt}]
prompt = tokenizer.apply_chat_template(
    messages, add_generation_prompt=True
)

for response in stream_generate(model, tokenizer, prompt, max_tokens=512):
    print(response.text, end="", flush=True)
print()
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Sampling&lt;/h4&gt; 
&lt;p&gt;The &lt;code&gt;generate&lt;/code&gt; and &lt;code&gt;stream_generate&lt;/code&gt; functions accept &lt;code&gt;sampler&lt;/code&gt; and &lt;code&gt;logits_processors&lt;/code&gt; keyword arguments. A sampler is any callable which accepts a possibly batched logits array and returns an array of sampled tokens. The &lt;code&gt;logits_processors&lt;/code&gt; must be a list of callables which take the token history and current logits as input and return the processed logits. The logits processors are applied in order.&lt;/p&gt; 
&lt;p&gt;Some standard sampling functions and logits processors are provided in &lt;code&gt;mlx_lm.sample_utils&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Command Line&lt;/h3&gt; 
&lt;p&gt;You can also use &lt;code&gt;mlx-lm&lt;/code&gt; from the command line with:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;mlx_lm.generate --model mistralai/Mistral-7B-Instruct-v0.3 --prompt "hello"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will download a Mistral 7B model from the Hugging Face Hub and generate text using the given prompt.&lt;/p&gt; 
&lt;p&gt;For a full list of options run:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;mlx_lm.generate --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To quantize a model from the command line run:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;mlx_lm.convert --hf-path mistralai/Mistral-7B-Instruct-v0.3 -q
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more options run:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;mlx_lm.convert --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can upload new models to Hugging Face by specifying &lt;code&gt;--upload-repo&lt;/code&gt; to &lt;code&gt;convert&lt;/code&gt;. For example, to upload a quantized Mistral-7B model to the &lt;a href="https://huggingface.co/mlx-community"&gt;MLX Hugging Face community&lt;/a&gt; you can do:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;mlx_lm.convert \
    --hf-path mistralai/Mistral-7B-Instruct-v0.3 \
    -q \
    --upload-repo mlx-community/my-4bit-mistral
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Models can also be converted and quantized directly in the &lt;a href="https://huggingface.co/spaces/mlx-community/mlx-my-repo"&gt;mlx-my-repo&lt;/a&gt; Hugging Face Space.&lt;/p&gt; 
&lt;h3&gt;Long Prompts and Generations&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;mlx-lm&lt;/code&gt; has some tools to scale efficiently to long prompts and generations:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;A rotating fixed-size key-value cache.&lt;/li&gt; 
 &lt;li&gt;Prompt caching&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To use the rotating key-value cache pass the argument &lt;code&gt;--max-kv-size n&lt;/code&gt; where &lt;code&gt;n&lt;/code&gt; can be any integer. Smaller values like &lt;code&gt;512&lt;/code&gt; will use very little RAM but result in worse quality. Larger values like &lt;code&gt;4096&lt;/code&gt; or higher will use more RAM but have better quality.&lt;/p&gt; 
&lt;p&gt;Caching prompts can substantially speedup reusing the same long context with different queries. To cache a prompt use &lt;code&gt;mlx_lm.cache_prompt&lt;/code&gt;. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cat prompt.txt | mlx_lm.cache_prompt \
  --model mistralai/Mistral-7B-Instruct-v0.3 \
  --prompt - \
  --prompt-cache-file mistral_prompt.safetensors
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then use the cached prompt with &lt;code&gt;mlx_lm.generate&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;mlx_lm.generate \
    --prompt-cache-file mistral_prompt.safetensors \
    --prompt "\nSummarize the above text."
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The cached prompt is treated as a prefix to the supplied prompt. Also notice when using a cached prompt, the model to use is read from the cache and need not be supplied explicitly.&lt;/p&gt; 
&lt;p&gt;Prompt caching can also be used in the Python API in order to avoid recomputing the prompt. This is useful in multi-turn dialogues or across requests that use the same context. See the &lt;a href="https://github.com/ml-explore/mlx-lm/raw/main/mlx_lm/examples/chat.py"&gt;example&lt;/a&gt; for more usage details.&lt;/p&gt; 
&lt;h3&gt;Supported Models&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;mlx-lm&lt;/code&gt; supports thousands of Hugging Face format LLMs. If the model you want to run is not supported, file an &lt;a href="https://github.com/ml-explore/mlx-lm/issues/new"&gt;issue&lt;/a&gt; or better yet, submit a pull request.&lt;/p&gt; 
&lt;p&gt;Here are a few examples of Hugging Face models that work with this example:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/mistralai/Mistral-7B-v0.1"&gt;mistralai/Mistral-7B-v0.1&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/meta-llama/Llama-2-7b-hf"&gt;meta-llama/Llama-2-7b-hf&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/deepseek-ai/deepseek-coder-6.7b-instruct"&gt;deepseek-ai/deepseek-coder-6.7b-instruct&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/01-ai/Yi-6B-Chat"&gt;01-ai/Yi-6B-Chat&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/microsoft/phi-2"&gt;microsoft/phi-2&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1"&gt;mistralai/Mixtral-8x7B-Instruct-v0.1&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Qwen/Qwen-7B"&gt;Qwen/Qwen-7B&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/pfnet/plamo-13b"&gt;pfnet/plamo-13b&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/pfnet/plamo-13b-instruct"&gt;pfnet/plamo-13b-instruct&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/stabilityai/stablelm-2-zephyr-1_6b"&gt;stabilityai/stablelm-2-zephyr-1_6b&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/internlm/internlm2-7b"&gt;internlm/internlm2-7b&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/tiiuae/falcon-mamba-7b-instruct"&gt;tiiuae/falcon-mamba-7b-instruct&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Most &lt;a href="https://huggingface.co/models?library=transformers,safetensors&amp;amp;other=mistral&amp;amp;sort=trending"&gt;Mistral&lt;/a&gt;, &lt;a href="https://huggingface.co/models?library=transformers,safetensors&amp;amp;other=llama&amp;amp;sort=trending"&gt;Llama&lt;/a&gt;, &lt;a href="https://huggingface.co/models?library=transformers,safetensors&amp;amp;other=phi&amp;amp;sort=trending"&gt;Phi-2&lt;/a&gt;, and &lt;a href="https://huggingface.co/models?library=transformers,safetensors&amp;amp;other=mixtral&amp;amp;sort=trending"&gt;Mixtral&lt;/a&gt; style models should work out of the box.&lt;/p&gt; 
&lt;p&gt;For some models (such as &lt;code&gt;Qwen&lt;/code&gt; and &lt;code&gt;plamo&lt;/code&gt;) the tokenizer requires you to enable the &lt;code&gt;trust_remote_code&lt;/code&gt; option. You can do this by passing &lt;code&gt;--trust-remote-code&lt;/code&gt; in the command line. If you don't specify the flag explicitly, you will be prompted to trust remote code in the terminal when running the model.&lt;/p&gt; 
&lt;p&gt;For &lt;code&gt;Qwen&lt;/code&gt; models you must also specify the &lt;code&gt;eos_token&lt;/code&gt;. You can do this by passing &lt;code&gt;--eos-token "&amp;lt;|endoftext|&amp;gt;"&lt;/code&gt; in the command line.&lt;/p&gt; 
&lt;p&gt;These options can also be set in the Python API. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;model, tokenizer = load(
    "qwen/Qwen-7B",
    tokenizer_config={"eos_token": "&amp;lt;|endoftext|&amp;gt;", "trust_remote_code": True},
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Large Models&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] This requires macOS 15.0 or higher to work.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Models which are large relative to the total RAM available on the machine can be slow. &lt;code&gt;mlx-lm&lt;/code&gt; will attempt to make them faster by wiring the memory occupied by the model and cache. This requires macOS 15 or higher to work.&lt;/p&gt; 
&lt;p&gt;If you see the following warning message:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[WARNING] Generating with a model that requires ...&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;then the model will likely be slow on the given machine. If the model fits in RAM then it can often be sped up by increasing the system wired memory limit. To increase the limit, set the following &lt;code&gt;sysctl&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo sysctl iogpu.wired_limit_mb=N
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The value &lt;code&gt;N&lt;/code&gt; should be larger than the size of the model in megabytes but smaller than the memory size of the machine.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Alibaba-NLP/DeepResearch</title>
      <link>https://github.com/Alibaba-NLP/DeepResearch</link>
      <description>&lt;p&gt;Tongyi DeepResearch, the Leading Open-source DeepResearch Agent&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;picture&gt; 
  &lt;img src="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/logo.png" width="100%" /&gt; 
 &lt;/picture&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;div align="center" style="line-height: 1;"&gt; 
 &lt;p&gt;&lt;a href="https://huggingface.co/Alibaba-NLP/Tongyi-DeepResearch-30B-A3B"&gt;&lt;img src="https://img.shields.io/badge/Models-5EDDD2?style=for-the-badge&amp;amp;logo=huggingface&amp;amp;logoColor=ffffff&amp;amp;labelColor" alt="MODELS" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Alibaba-NLP/DeepResearch"&gt;&lt;img src="https://img.shields.io/badge/Github-24292F?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white" alt="GITHUB" /&gt;&lt;/a&gt; &lt;a href="https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/"&gt;&lt;img src="https://img.shields.io/badge/Blog-4285F4?style=for-the-badge&amp;amp;logo=google-chrome&amp;amp;logoColor=white" alt="Blog" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p align="center"&gt; 🤗 &lt;a href="https://huggingface.co/Alibaba-NLP/Tongyi-DeepResearch-30B-A3B" target="_blank"&gt;HuggingFace&lt;/a&gt; ｜ &lt;img src="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/tongyi.png" width="14px" style="display:inline;" /&gt; &lt;a href="https://modelscope.cn/models/iic/Tongyi-DeepResearch-30B-A3B" target="_blank"&gt;ModelScope&lt;/a&gt; &lt;/p&gt;
&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/14217" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/14217" alt="Alibaba-NLP%2FWebAgent | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; &lt;/p&gt;
&lt;h1&gt;Introduction&lt;/h1&gt; 
&lt;p&gt;We present &lt;img src="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/tongyi.png" width="14px" style="display:inline;" /&gt; &lt;strong&gt;Tongyi DeepResearch&lt;/strong&gt;, an agentic large language model featuring 30.5 billion total parameters, with only 3.3 billion activated per token. Developed by Tongyi Lab, the model is specifically designed for &lt;strong&gt;long-horizon, deep information-seeking&lt;/strong&gt; tasks. Tongyi DeepResearch demonstrates state-of-the-art performance across a range of agentic search benchmarks, including Humanity's Last Exam, BrowserComp, BrowserComp-ZH, WebWalkerQA,xbench-DeepSearch, FRAMES and SimpleQA.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Tongyi DeepResearch builds upon our previous work on the &lt;img src="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/tongyi.png" width="14px" style="display:inline;" /&gt; &lt;a href="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/WebAgent/"&gt;WebAgent&lt;/a&gt; project.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;More details can be found in our 📰&amp;nbsp;&lt;a href="https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/"&gt;Tech Blog&lt;/a&gt;.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/performance.png" /&gt; &lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;⚙️ &lt;strong&gt;Fully automated synthetic data generation pipeline&lt;/strong&gt;: We design a highly scalable data synthesis pipeline, which is fully automatic and empowers agentic pre-training, supervised fine-tuning, and reinforcement learning.&lt;/li&gt; 
 &lt;li&gt;🔄 &lt;strong&gt;Large-scale continual pre-training on agentic data&lt;/strong&gt;: Leveraging diverse, high-quality agentic interaction data to extend model capabilities, maintain freshness, and strengthen reasoning performance.&lt;/li&gt; 
 &lt;li&gt;🔁 &lt;strong&gt;End-to-end reinforcement learning&lt;/strong&gt;: We employ a strictly on-policy RL approach based on a customized Group Relative Policy Optimization framework, with token-level policy gradients, leave-one-out advantage estimation, and selective filtering of negative samples to stabilize training in a non‑stationary environment.&lt;/li&gt; 
 &lt;li&gt;🤖 &lt;strong&gt;Agent Inference Paradigm Compatibility&lt;/strong&gt;: At inference, Tongyi DeepResearch is compatible with two inference paradigms: ReAct, for rigorously evaluating the model's core intrinsic abilities, and an IterResearch-based 'Heavy' mode, which uses a test-time scaling strategy to unlock the model's maximum performance ceiling.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Model Download&lt;/h1&gt; 
&lt;p&gt;You can directly download the model by following the links below.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Model&lt;/th&gt; 
   &lt;th align="center"&gt;Download Links&lt;/th&gt; 
   &lt;th align="center"&gt;Model Size&lt;/th&gt; 
   &lt;th align="center"&gt;Context Length&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Tongyi-DeepResearch-30B-A3B&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/Alibaba-NLP/Tongyi-DeepResearch-30B-A3B"&gt;🤗 HuggingFace&lt;/a&gt;&lt;br /&gt; &lt;a href="https://modelscope.cn/models/iic/Tongyi-DeepResearch-30B-A3B"&gt;🤖 ModelScope&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;30B-A3B&lt;/td&gt; 
   &lt;td align="center"&gt;128K&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;News&lt;/h1&gt; 
&lt;p&gt;[2025/09/17]🔥 We have released &lt;strong&gt;Tongyi-DeepResearch-30B-A3B&lt;/strong&gt;.&lt;/p&gt; 
&lt;h1&gt;Deep Research Benchmark Results&lt;/h1&gt; 
&lt;p align="center"&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/benchmark.png" /&gt; &lt;/p&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;This guide provides instructions for setting up the environment and running inference scripts located in the &lt;a href="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/inference/"&gt;inference&lt;/a&gt; folder.&lt;/p&gt; 
&lt;h3&gt;1. Environment Setup&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Recommended Python version: &lt;strong&gt;3.10.0&lt;/strong&gt; (using other versions may cause dependency issues).&lt;/li&gt; 
 &lt;li&gt;It is strongly advised to create an isolated environment using &lt;code&gt;conda&lt;/code&gt; or &lt;code&gt;virtualenv&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Example with Conda
conda create -n react_infer_env python=3.10.0 
conda activate react_infer_env
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Installation&lt;/h3&gt; 
&lt;p&gt;Install the required dependencies:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. Prepare Evaluation Data&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Create a folder named &lt;code&gt;eval_data/&lt;/code&gt; in the project root.&lt;/li&gt; 
 &lt;li&gt;Place your QA file in &lt;strong&gt;JSONL&lt;/strong&gt; format inside this directory, e.g. &lt;code&gt;eval_data/example.jsonl&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Each line must be a JSON object that includes &lt;strong&gt;both&lt;/strong&gt; of the following keys: &lt;pre&gt;&lt;code class="language-json"&gt;{"question": "...","answer": "..."}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;A sample file is provided in the &lt;code&gt;eval_data&lt;/code&gt; folder for reference.&lt;/li&gt; 
 &lt;li&gt;If you plan to use the &lt;em&gt;file parser&lt;/em&gt; tool, &lt;strong&gt;prepend the file name to the &lt;code&gt;question&lt;/code&gt; field&lt;/strong&gt; and place the referenced file inside the &lt;code&gt;eval_data/file_corpus/&lt;/code&gt; directory.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;4. Configure the Inference Script&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Open &lt;code&gt;run_react_infer.sh&lt;/code&gt; and modify the following variables as instructed in the comments: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;MODEL_PATH&lt;/code&gt; - path to the local or remote model weights.&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;DATASET&lt;/code&gt; - path to the evaluation set, e.g. &lt;code&gt;example&lt;/code&gt;.&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;OUTPUT_PATH&lt;/code&gt; - path for saving the prediction results, e.g. &lt;code&gt;./outputs&lt;/code&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Depending on the tools you enable (retrieval, calculator, web search, etc.), provide the required &lt;code&gt;API_KEY&lt;/code&gt;, &lt;code&gt;BASE_URL&lt;/code&gt;, or other credentials. Each key is explained inline in the bash script.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;5. Run the Inference Script&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;bash run_react_infer.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;p&gt;With these steps, you can fully prepare the environment, configure the dataset, and run the model. For more details, consult the inline comments in each script or open an issue.&lt;/p&gt; 
&lt;h2&gt;Benchmark Evaluation&lt;/h2&gt; 
&lt;p&gt;We provide benchmark evaluation scripts for various datasets. Please refer to the &lt;a href="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/evaluation/"&gt;evaluation scripts&lt;/a&gt; directory for more details.&lt;/p&gt; 
&lt;h2&gt;Deep Research Agent Family&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/family.png" /&gt; &lt;/p&gt; 
&lt;p&gt;Tongyi DeepResearch also has an extensive deep research agent family. You can find more information in the following paper:&lt;/p&gt; 
&lt;p&gt;[1] &lt;a href="https://arxiv.org/pdf/2501.07572"&gt;WebWalker: Benchmarking LLMs in Web Traversal&lt;/a&gt;&lt;br /&gt; [2] &lt;a href="https://arxiv.org/pdf/2505.22648"&gt;WebDancer: Towards Autonomous Information Seeking Agency&lt;/a&gt;&lt;br /&gt; [3] &lt;a href="https://arxiv.org/pdf/2507.02592"&gt;WebSailor: Navigating Super-human Reasoning for Web Agent&lt;/a&gt;&lt;br /&gt; [4] &lt;a href="https://arxiv.org/pdf/2507.15061"&gt;WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization&lt;/a&gt;&lt;br /&gt; [5] &lt;a href="https://arxiv.org/pdf/2508.05748"&gt;WebWatcher: Breaking New Frontier of Vision-Language Deep Research Agent&lt;/a&gt;&lt;br /&gt; [6] &lt;a href="https://arxiv.org/pdf/2509.13309"&gt;WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon Agents&lt;/a&gt;&lt;br /&gt; [7] &lt;a href="https://arxiv.org/pdf/2509.13313"&gt;ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization&lt;/a&gt;&lt;br /&gt; [8] &lt;a href="https://arxiv.org/pdf/2509.13312"&gt;WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep Research&lt;/a&gt;&lt;br /&gt; [9] &lt;a href="https://arxiv.org/pdf/2509.13305"&gt;WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning&lt;/a&gt;&lt;br /&gt; [10] &lt;a href="https://arxiv.org/pdf/2509.13310"&gt;Scaling Agents via Continual Pre-training&lt;/a&gt;&lt;br /&gt; [11] &lt;a href="https://arxiv.org/pdf/2509.13311"&gt;Towards General Agentic Intelligence via Environment Scaling&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;🌟 Misc&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://www.star-history.com/#Alibaba-NLP/DeepResearch&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=Alibaba-NLP/DeepResearch&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;🚩 Talent Recruitment&lt;/h2&gt; 
&lt;p&gt;🔥🔥🔥 We are hiring! Research intern positions are open (based in Hangzhou、Beijing、Shanghai)&lt;/p&gt; 
&lt;p&gt;📚 &lt;strong&gt;Research Area&lt;/strong&gt;：Web Agent, Search Agent, Agent RL, MultiAgent RL, Agentic RAG&lt;/p&gt; 
&lt;p&gt;☎️ &lt;strong&gt;Contact&lt;/strong&gt;：&lt;a href=""&gt;yongjiang.jy@alibaba-inc.com&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Contact Information&lt;/h2&gt; 
&lt;p&gt;For communications, please contact Yong Jiang (&lt;a href="mailto:yongjiang.jy@alibaba-inc.com"&gt;yongjiang.jy@alibaba-inc.com&lt;/a&gt;).&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{tongyidr,
  author={Tongyi DeepResearch Team},
  title={Tongyi-DeepResearch},
  year={2025},
  howpublished={\url{https://github.com/Alibaba-NLP/DeepResearch}}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>facebookresearch/detectron2</title>
      <link>https://github.com/facebookresearch/detectron2</link>
      <description>&lt;p&gt;Detectron2 is a platform for object detection, segmentation and other visual recognition tasks.&lt;/p&gt;&lt;hr&gt;&lt;img src="https://raw.githubusercontent.com/facebookresearch/detectron2/main/.github/Detectron2-Logo-Horz.svg?sanitize=true" width="300" /&gt; 
&lt;p&gt;Detectron2 is Facebook AI Research's next generation library that provides state-of-the-art detection and segmentation algorithms. It is the successor of &lt;a href="https://github.com/facebookresearch/Detectron/"&gt;Detectron&lt;/a&gt; and &lt;a href="https://github.com/facebookresearch/maskrcnn-benchmark/"&gt;maskrcnn-benchmark&lt;/a&gt;. It supports a number of computer vision research projects and production applications in Facebook.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://user-images.githubusercontent.com/1381301/66535560-d3422200-eace-11e9-9123-5535d469db19.png" /&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;h2&gt;Learn More about Detectron2&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Includes new capabilities such as panoptic segmentation, Densepose, Cascade R-CNN, rotated bounding boxes, PointRend, DeepLab, ViTDet, MViTv2 etc.&lt;/li&gt; 
 &lt;li&gt;Used as a library to support building &lt;a href="https://raw.githubusercontent.com/facebookresearch/detectron2/main/projects/"&gt;research projects&lt;/a&gt; on top of it.&lt;/li&gt; 
 &lt;li&gt;Models can be exported to TorchScript format or Caffe2 format for deployment.&lt;/li&gt; 
 &lt;li&gt;It &lt;a href="https://detectron2.readthedocs.io/notes/benchmarks.html"&gt;trains much faster&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See our &lt;a href="https://ai.meta.com/blog/-detectron2-a-pytorch-based-modular-object-detection-library-/"&gt;blog post&lt;/a&gt; to see more demos. See this &lt;a href="https://ai.meta.com/blog/detectron-everingham-prize/"&gt;interview&lt;/a&gt; to learn more about the stories behind detectron2.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://detectron2.readthedocs.io/tutorials/install.html"&gt;installation instructions&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://detectron2.readthedocs.io/tutorials/getting_started.html"&gt;Getting Started with Detectron2&lt;/a&gt;, and the &lt;a href="https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5"&gt;Colab Notebook&lt;/a&gt; to learn about basic usage.&lt;/p&gt; 
&lt;p&gt;Learn more at our &lt;a href="https://detectron2.readthedocs.org"&gt;documentation&lt;/a&gt;. And see &lt;a href="https://raw.githubusercontent.com/facebookresearch/detectron2/main/projects/"&gt;projects/&lt;/a&gt; for some projects that are built on top of detectron2.&lt;/p&gt; 
&lt;h2&gt;Model Zoo and Baselines&lt;/h2&gt; 
&lt;p&gt;We provide a large set of baseline results and trained models available for download in the &lt;a href="https://raw.githubusercontent.com/facebookresearch/detectron2/main/MODEL_ZOO.md"&gt;Detectron2 Model Zoo&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Detectron2 is released under the &lt;a href="https://raw.githubusercontent.com/facebookresearch/detectron2/main/LICENSE"&gt;Apache 2.0 license&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Citing Detectron2&lt;/h2&gt; 
&lt;p&gt;If you use Detectron2 in your research or wish to refer to the baseline results published in the &lt;a href="https://raw.githubusercontent.com/facebookresearch/detectron2/main/MODEL_ZOO.md"&gt;Model Zoo&lt;/a&gt;, please use the following BibTeX entry.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-BibTeX"&gt;@misc{wu2019detectron2,
  author =       {Yuxin Wu and Alexander Kirillov and Francisco Massa and
                  Wan-Yen Lo and Ross Girshick},
  title =        {Detectron2},
  howpublished = {\url{https://github.com/facebookresearch/detectron2}},
  year =         {2019}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>sansan0/TrendRadar</title>
      <link>https://github.com/sansan0/TrendRadar</link>
      <description>&lt;p&gt;🎯 告别信息过载，只看真正关心的新闻 - 多平台热点聚合工具，趋势分析工具，一键监控今日头条、百度热搜、微博、抖音、知乎、B站等35个平台，智能关键词筛选，自动生成热点分析报告。支持企业微信、飞书、钉钉、Telegram推送，30秒网页部署，1分钟手机通知，无需编程基础。也支持docker私人部署⭐ 让算法为你服务，而非被算法绑架&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h1&gt;🎯TrendRadar&lt;/h1&gt; 
 &lt;p&gt;🚀 最快&lt;strong&gt;30秒&lt;/strong&gt;部署的热点助手 —— 告别无效刷屏，只看真正关心的新闻资讯&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/sansan0/TrendRadar/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/sansan0/TrendRadar?style=flat-square&amp;amp;logo=github&amp;amp;color=yellow" alt="GitHub Stars" /&gt;&lt;/a&gt; &lt;a href="https://github.com/sansan0/TrendRadar/network/members"&gt;&lt;img src="https://img.shields.io/github/forks/sansan0/TrendRadar?style=flat-square&amp;amp;logo=github&amp;amp;color=blue" alt="GitHub Forks" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-GPL--3.0-blue.svg?style=flat-square" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://github.com/sansan0/TrendRadar"&gt;&lt;img src="https://img.shields.io/badge/version-v2.2.0-green.svg?style=flat-square" alt="Version" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://work.weixin.qq.com/"&gt;&lt;img src="https://img.shields.io/badge/%E4%BC%81%E4%B8%9A%E5%BE%AE%E4%BF%A1-%E9%80%9A%E7%9F%A5%E6%94%AF%E6%8C%81-00D4AA?style=flat-square" alt="企业微信通知" /&gt;&lt;/a&gt; &lt;a href="https://telegram.org/"&gt;&lt;img src="https://img.shields.io/badge/Telegram-%E9%80%9A%E7%9F%A5%E6%94%AF%E6%8C%81-00D4AA?style=flat-square" alt="Telegram通知" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#"&gt;&lt;img src="https://img.shields.io/badge/%E9%92%89%E9%92%89-%E9%80%9A%E7%9F%A5%E6%94%AF%E6%8C%81-00D4AA?style=flat-square" alt="dingtalk通知" /&gt;&lt;/a&gt; &lt;a href="https://www.feishu.cn/"&gt;&lt;img src="https://img.shields.io/badge/%E9%A3%9E%E4%B9%A6-%E9%80%9A%E7%9F%A5%E6%94%AF%E6%8C%81-00D4AA?style=flat-square" alt="飞书通知" /&gt;&lt;/a&gt; &lt;a href="https://github.com/sansan0/TrendRadar"&gt;&lt;img src="https://img.shields.io/badge/GitHub_Actions-%E8%87%AA%E5%8A%A8%E5%8C%96-2088FF?style=flat-square&amp;amp;logo=github-actions&amp;amp;logoColor=white" alt="GitHub Actions" /&gt;&lt;/a&gt; &lt;a href="https://sansan0.github.io/TrendRadar"&gt;&lt;img src="https://img.shields.io/badge/GitHub_Pages-%E9%83%A8%E7%BD%B2-4285F4?style=flat-square&amp;amp;logo=github&amp;amp;logoColor=white" alt="GitHub Pages" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/"&gt;&lt;img src="https://img.shields.io/badge/Docker-%E9%83%A8%E7%BD%B2-2496ED?style=flat-square&amp;amp;logo=docker&amp;amp;logoColor=white" alt="Docker" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;本项目以轻量，易部署为目标，主要处理 issues&lt;/p&gt; 
 &lt;p&gt;遇到问题提 issues，或【硅基茶水间】公众号留言&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;details&gt; 
 &lt;summary&gt;👉 点击查看&lt;strong&gt;致谢名单&lt;/strong&gt; (当前 &lt;strong&gt;🔥19🔥&lt;/strong&gt; 位)&lt;/summary&gt; 
 &lt;h3&gt;数据支持&lt;/h3&gt; 
 &lt;p&gt;本项目使用了 &lt;a href="https://github.com/ourongxing/newsnow"&gt;newsnow&lt;/a&gt; 项目提供的 API 接口获取多平台数据&lt;/p&gt; 
 &lt;h3&gt;推广助力&lt;/h3&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;感谢以下平台和个人的推荐(按时间排列)，以及各微信群，qq群等给到这个项目帮助的人&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://mp.weixin.qq.com/s/fvutkJ_NPUelSW9OGK39aA"&gt;小众软件&lt;/a&gt; - 开源软件推荐平台&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://linux.do/"&gt;LinuxDo 社区&lt;/a&gt; - 技术爱好者的聚集地&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/ruanyf/weekly"&gt;阮一峰周刊&lt;/a&gt; - 技术圈有影响力的周刊&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;观众支持&lt;/h3&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;感谢以下热心观众的信任与支持&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;点赞人&lt;/th&gt; 
    &lt;th align="center"&gt;金额&lt;/th&gt; 
    &lt;th align="center"&gt;日期&lt;/th&gt; 
    &lt;th align="center"&gt;备注&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;**昊&lt;/td&gt; 
    &lt;td align="center"&gt;5&lt;/td&gt; 
    &lt;td align="center"&gt;2025.9.17&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*号&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.9.15&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;T*T&lt;/td&gt; 
    &lt;td align="center"&gt;2&lt;/td&gt; 
    &lt;td align="center"&gt;2025.9.15&lt;/td&gt; 
    &lt;td align="center"&gt;点赞&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*家&lt;/td&gt; 
    &lt;td align="center"&gt;10&lt;/td&gt; 
    &lt;td align="center"&gt;2025.9.10&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*X&lt;/td&gt; 
    &lt;td align="center"&gt;1.11&lt;/td&gt; 
    &lt;td align="center"&gt;2025.9.3&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*飙&lt;/td&gt; 
    &lt;td align="center"&gt;20&lt;/td&gt; 
    &lt;td align="center"&gt;2025.8.31&lt;/td&gt; 
    &lt;td align="center"&gt;来自老童谢谢&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*下&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.8.30&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;2*D&lt;/td&gt; 
    &lt;td align="center"&gt;88&lt;/td&gt; 
    &lt;td align="center"&gt;2025.8.13 下午&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;2*D&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.8.13 上午&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;S*o&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.8.05&lt;/td&gt; 
    &lt;td align="center"&gt;支持一下&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*侠&lt;/td&gt; 
    &lt;td align="center"&gt;10&lt;/td&gt; 
    &lt;td align="center"&gt;2025.8.04&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;x*x&lt;/td&gt; 
    &lt;td align="center"&gt;2&lt;/td&gt; 
    &lt;td align="center"&gt;2025.8.03&lt;/td&gt; 
    &lt;td align="center"&gt;trendRadar 好项目 点赞&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*远&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.8.01&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*邪&lt;/td&gt; 
    &lt;td align="center"&gt;5&lt;/td&gt; 
    &lt;td align="center"&gt;2025.8.01&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*梦&lt;/td&gt; 
    &lt;td align="center"&gt;0.1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.7.30&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;**龙&lt;/td&gt; 
    &lt;td align="center"&gt;10&lt;/td&gt; 
    &lt;td align="center"&gt;2025.7.29&lt;/td&gt; 
    &lt;td align="center"&gt;支持一下&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h2&gt;✨ 核心功能&lt;/h2&gt; 
&lt;h3&gt;&lt;strong&gt;全网热点聚合&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;今日头条&lt;/li&gt; 
 &lt;li&gt;百度热搜&lt;/li&gt; 
 &lt;li&gt;华尔街见闻&lt;/li&gt; 
 &lt;li&gt;澎湃新闻&lt;/li&gt; 
 &lt;li&gt;bilibili 热搜&lt;/li&gt; 
 &lt;li&gt;财联社热门&lt;/li&gt; 
 &lt;li&gt;凤凰网&lt;/li&gt; 
 &lt;li&gt;贴吧&lt;/li&gt; 
 &lt;li&gt;微博&lt;/li&gt; 
 &lt;li&gt;抖音&lt;/li&gt; 
 &lt;li&gt;知乎&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;默认监控 11 个主流平台，如想额外增加，可看最下方的&lt;strong&gt;自定义监控平台&lt;/strong&gt;&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;智能推送策略&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;三种推送模式&lt;/strong&gt;：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;📈 投资者/交易员&lt;/strong&gt; → 选择 &lt;code&gt;incremental&lt;/code&gt;，及时获取新增资讯&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;📰 自媒体人/内容创作者&lt;/strong&gt; → 选择 &lt;code&gt;current&lt;/code&gt;，掌握实时热点趋势&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;📋 企业管理者/普通用户&lt;/strong&gt; → 选择 &lt;code&gt;daily&lt;/code&gt;，定时获取完整日报&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;静默推送模式&lt;/strong&gt;：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;时间范围控制&lt;/strong&gt;：设定推送时间窗口（如 9:00-18:00），仅在指定时间内推送&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;适用场景&lt;/strong&gt;： 
  &lt;ul&gt; 
   &lt;li&gt;时间内每次执行都推送&lt;/li&gt; 
   &lt;li&gt;时间范围内只推送一次&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;strong&gt;精准内容筛选&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;设置个人关键词（如：AI、比亚迪、教育政策），只推送相关热点，过滤无关信息&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;支持普通词、必须词(+)、过滤词(!)三种语法，具体见【frequency_words.txt 配置教程】&lt;/li&gt; 
 &lt;li&gt;词组化管理，独立统计不同主题热点&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;也可以不做筛选，完整的推送所有热点，具体见【历史更新】中的 v2.0.1&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;&lt;strong&gt;个性化热点算法&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;不再被各个平台的算法牵着走，TrendRadar 会重新整理全网热搜：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;看重排名高的新闻&lt;/strong&gt;（占60%）：各平台前几名的新闻优先显示&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;关注持续出现的话题&lt;/strong&gt;（占30%）：反复出现的新闻更重要&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;考虑排名质量&lt;/strong&gt;（占10%）：不仅多次出现，还经常排在前列&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;实际效果&lt;/strong&gt;：把分散在各个平台的热搜合并起来，按照你关心的热度重新排序&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;这三个比例可以选择适合自己的场景进行调整，具体见【热点权重调整】&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;&lt;strong&gt;多渠道实时推送&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;支持&lt;strong&gt;企业微信&lt;/strong&gt;、&lt;strong&gt;飞书&lt;/strong&gt;、&lt;strong&gt;钉钉&lt;/strong&gt;、&lt;strong&gt;Telegram&lt;/strong&gt;，消息直达手机&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;多端适配&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;GitHub Pages&lt;/strong&gt;：自动生成精美网页报告，PC/移动端适配&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Docker部署&lt;/strong&gt;：支持多架构容器化运行&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;数据持久化&lt;/strong&gt;：HTML/TXT多格式历史记录保存&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;strong&gt;零技术门槛部署&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;GitHub 一键 Fork 即可使用，无需编程基础。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;30秒部署： GitHub Pages（网页浏览）支持一键保存成图片，随时分享给他人&lt;/p&gt; 
 &lt;p&gt;1分钟部署： 企业微信（手机通知）&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;💡 提示：&lt;/strong&gt; 想要&lt;strong&gt;实时更新&lt;/strong&gt;的网页版？fork 后，进入你的仓库 Settings → Pages，启用 GitHub Pages。&lt;a href="https://sansan0.github.io/TrendRadar/"&gt;效果预览&lt;/a&gt;。&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;减少 APP 依赖&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;从"被算法推荐绑架"变成"主动获取自己想要的信息"&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;适合人群：&lt;/strong&gt; 投资者、自媒体人、企业公关、关心时事的普通用户&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;典型场景：&lt;/strong&gt; 股市投资监控、品牌舆情追踪、行业动态关注、生活资讯获取&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Github Pages 网页效果(手机端也适配)&lt;/th&gt; 
   &lt;th align="center"&gt;飞书推送效果&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/sansan0/TrendRadar/master/_image/github-pages.png" alt="Github Pages效果" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/sansan0/TrendRadar/master/_image/feishu.jpg" alt="飞书推送效果" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;👉 推送格式说明&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h2&gt;&lt;strong&gt;通知示例：&lt;/strong&gt;&lt;/h2&gt; 
 &lt;p&gt;📊 热点词汇统计&lt;/p&gt; 
 &lt;p&gt;🔥 [1/3] AI ChatGPT : 2 条&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;[百度热搜] 🆕 ChatGPT-5正式发布 [&lt;strong&gt;1&lt;/strong&gt;] - 09时15分 (1次)&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[今日头条] AI芯片概念股暴涨 [&lt;strong&gt;3&lt;/strong&gt;] - [08时30分 ~ 10时45分] (3次)&lt;/p&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;━━━━━━━━━━━━━━━━━━━&lt;/p&gt; 
 &lt;p&gt;📈 [2/3] 比亚迪 特斯拉 : 2 条&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;[微博] 🆕 比亚迪月销量破纪录 [&lt;strong&gt;2&lt;/strong&gt;] - 10时20分 (1次)&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[抖音] 特斯拉降价促销 [&lt;strong&gt;4&lt;/strong&gt;] - [07时45分 ~ 09时15分] (2次)&lt;/p&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;━━━━━━━━━━━━━━━━━━━&lt;/p&gt; 
 &lt;p&gt;📌 [3/3] A股 股市 : 1 条&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;[华尔街见闻] A股午盘点评分析 [&lt;strong&gt;5&lt;/strong&gt;] - [11时30分 ~ 12时00分] (2次)&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;🆕 本次新增热点新闻 (共 2 条)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;百度热搜&lt;/strong&gt; (1 条):&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;ChatGPT-5正式发布 [&lt;strong&gt;1&lt;/strong&gt;]&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;&lt;strong&gt;微博&lt;/strong&gt; (1 条):&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;比亚迪月销量破纪录 [&lt;strong&gt;2&lt;/strong&gt;]&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;更新时间：2025-01-15 12:30:15&lt;/p&gt; 
 &lt;h2&gt;&lt;strong&gt;消息格式说明&lt;/strong&gt;&lt;/h2&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;格式元素&lt;/th&gt; 
    &lt;th&gt;示例&lt;/th&gt; 
    &lt;th&gt;含义&lt;/th&gt; 
    &lt;th&gt;说明&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;🔥📈📌&lt;/td&gt; 
    &lt;td&gt;🔥 [1/3] AI ChatGPT&lt;/td&gt; 
    &lt;td&gt;热度等级&lt;/td&gt; 
    &lt;td&gt;🔥高热度(≥10条) 📈中热度(5-9条) 📌普通热度(&amp;lt;5条)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;[序号/总数]&lt;/td&gt; 
    &lt;td&gt;[1/3]&lt;/td&gt; 
    &lt;td&gt;排序位置&lt;/td&gt; 
    &lt;td&gt;当前词组在所有匹配词组中的排名&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;频率词组&lt;/td&gt; 
    &lt;td&gt;AI ChatGPT&lt;/td&gt; 
    &lt;td&gt;关键词组&lt;/td&gt; 
    &lt;td&gt;配置文件中的词组，标题必须包含其中词汇&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;: N 条&lt;/td&gt; 
    &lt;td&gt;: 2 条&lt;/td&gt; 
    &lt;td&gt;匹配数量&lt;/td&gt; 
    &lt;td&gt;该词组匹配的新闻总数&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;[平台名]&lt;/td&gt; 
    &lt;td&gt;[百度热搜]&lt;/td&gt; 
    &lt;td&gt;来源平台&lt;/td&gt; 
    &lt;td&gt;新闻所属的平台名称&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;🆕&lt;/td&gt; 
    &lt;td&gt;🆕 ChatGPT-5正式发布&lt;/td&gt; 
    &lt;td&gt;新增标记&lt;/td&gt; 
    &lt;td&gt;本轮抓取中首次出现的热点&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;[&lt;strong&gt;数字&lt;/strong&gt;]&lt;/td&gt; 
    &lt;td&gt;[&lt;strong&gt;1&lt;/strong&gt;]&lt;/td&gt; 
    &lt;td&gt;高排名&lt;/td&gt; 
    &lt;td&gt;排名≤阈值的热搜，红色加粗显示&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;[数字]&lt;/td&gt; 
    &lt;td&gt;[7]&lt;/td&gt; 
    &lt;td&gt;普通排名&lt;/td&gt; 
    &lt;td&gt;排名&amp;gt;阈值的热搜，普通显示&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;- 时间&lt;/td&gt; 
    &lt;td&gt;- 09时15分&lt;/td&gt; 
    &lt;td&gt;首次时间&lt;/td&gt; 
    &lt;td&gt;该新闻首次被发现的时间&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;[时间~时间]&lt;/td&gt; 
    &lt;td&gt;[08时30分 ~ 10时45分]&lt;/td&gt; 
    &lt;td&gt;持续时间&lt;/td&gt; 
    &lt;td&gt;从首次出现到最后出现的时间范围&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;(N次)&lt;/td&gt; 
    &lt;td&gt;(3次)&lt;/td&gt; 
    &lt;td&gt;出现频率&lt;/td&gt; 
    &lt;td&gt;在监控期间出现的总次数&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;新增区域&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;🆕 &lt;strong&gt;本次新增热点新闻&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;新话题汇总&lt;/td&gt; 
    &lt;td&gt;单独展示本轮新出现的热点话题&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h2&gt;📝 更新日志&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;升级说明：&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;提示1&lt;/strong&gt;：请通过以下方式更新项目(或根据&lt;strong&gt;更新提示&lt;/strong&gt;升级)，不要通过 &lt;strong&gt;Sync fork&lt;/strong&gt; 更新&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;提示2&lt;/strong&gt;：比如你当前是v2.0.1，想升级，建议查看【历史更新】，明确升级的方式和更新的功能&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;小版本更新&lt;/strong&gt;：一般情况，直接在 GitHub 网页编辑器中，用本项目的 &lt;code&gt;main.py&lt;/code&gt; 代码替换你 fork 仓库中的对应文件&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;大版本升级&lt;/strong&gt;：从 v1.x 升级到 v2.0 建议删除现有 fork 后重新 fork，这样更省力且避免配置冲突&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;感谢各位朋友的支持与厚爱，特别感谢：&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;fork 并为项目点 star&lt;/strong&gt; 的观众们，你们的认可是我前进的动力&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;关注公众号并积极互动&lt;/strong&gt; 的读者们，你们的留言和点赞让内容更有温度&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;给予资金点赞支持&lt;/strong&gt; 的朋友们，你们的慷慨让项目得以持续发展&lt;/p&gt; 
 &lt;p&gt;下一次&lt;strong&gt;新功能&lt;/strong&gt;，大概会是 ai 分析功能(大概(●'◡'●)&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;2025/09/17 - v2.2.0&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;新增一键保存新闻图片功能，让你轻松分享关注的热点&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;使用说明&lt;/strong&gt;：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;适用场景：当你按照教程开启了网页版功能后(GitHub Pages)&lt;/li&gt; 
 &lt;li&gt;使用方法：用手机或电脑打开该网页链接，点击页面顶部的"保存为图片"按钮&lt;/li&gt; 
 &lt;li&gt;实际效果：系统会自动将当前的新闻报告制作成一张精美图片，保存到你的手机相册或电脑桌面&lt;/li&gt; 
 &lt;li&gt;分享便利：你可以直接把这张图片发给朋友、发到朋友圈，或分享到工作群，让别人也能看到你发现的重要资讯&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;👉 历史更新&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h3&gt;2025/09/13 - v2.1.2&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;解决钉钉的推送容量限制导致的新闻推送失败问题(采用分批推送)&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/09/04 - v2.1.1&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;修复docker在某些架构中无法正常运行的问题&lt;/li&gt; 
  &lt;li&gt;正式发布官方 Docker 镜像 wantcat/trendradar，支持多架构&lt;/li&gt; 
  &lt;li&gt;优化 Docker 部署流程，无需本地构建即可快速使用&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/08/30 - v2.1.0&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;核心改进&lt;/strong&gt;：&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;推送逻辑优化&lt;/strong&gt;：从"每次执行都推送"改为"时间窗口内可控推送"&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;时间窗口控制&lt;/strong&gt;：可设定推送时间范围，避免非工作时间打扰&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;推送频率可选&lt;/strong&gt;：时间段内支持单次推送或多次推送&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;更新提示&lt;/strong&gt;：&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;本功能默认关闭，需手动在 config.yaml 中开启静默推送模式&lt;/li&gt; 
  &lt;li&gt;升级需同时更新 main.py 和 config.yaml 两个文件&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/08/27 - v2.0.4&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;本次版本不是功能修复，而是重要提醒&lt;/li&gt; 
  &lt;li&gt;请务必妥善保管好 webhooks，不要公开，不要公开，不要公开&lt;/li&gt; 
  &lt;li&gt;如果你以 fork 的方式将本项目部署在 GitHub 上，请将 webhooks 填入 GitHub Secret，而非 config.yaml&lt;/li&gt; 
  &lt;li&gt;如果你已经暴露了 webhooks 或将其填入了 config.yaml，建议删除后重新生成&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/08/06 - v2.0.3&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;优化 github page 的网页版效果，方便移动端使用&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/07/28 - v2.0.2&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;重构代码&lt;/li&gt; 
  &lt;li&gt;解决版本号容易被遗漏修改的问题&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/07/27 - v2.0.1&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;修复问题&lt;/strong&gt;:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;docker 的 shell 脚本的换行符为 CRLF 导致的执行异常问题&lt;/li&gt; 
  &lt;li&gt;frequency_words.txt 为空时，导致新闻发送也为空的逻辑问题&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;ul&gt; 
  &lt;li&gt;修复后，当你选择 frequency_words.txt 为空时，将&lt;strong&gt;推送所有新闻&lt;/strong&gt;，但受限于消息推送大小限制，请做如下调整 
   &lt;ul&gt; 
    &lt;li&gt;方案一：关闭手机推送，只选择 Github Pages 布置(这是能获得最完整信息的方案，将把所有平台的热点按照你&lt;strong&gt;自定义的热搜算法&lt;/strong&gt;进行重新排序)&lt;/li&gt; 
    &lt;li&gt;方案二：减少推送平台，优先选择&lt;strong&gt;企业微信&lt;/strong&gt;或&lt;strong&gt;Telegram&lt;/strong&gt;，这两个推送我做了分批推送功能(因为分批推送影响推送体验，且只有这两个平台只给一点点推送容量，所以才不得已做了分批推送功能，但至少能保证获得的信息完整)&lt;/li&gt; 
    &lt;li&gt;方案三：可与方案二结合，模式选择 current 或 incremental 可有效减少一次性推送的内容&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/07/17 - v2.0.0&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;重大重构&lt;/strong&gt;：&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;配置管理重构：所有配置现在通过 &lt;code&gt;config/config.yaml&lt;/code&gt; 文件管理（main.py 我依旧没拆分，方便你们复制升级）&lt;/li&gt; 
  &lt;li&gt;运行模式升级：支持三种模式 - &lt;code&gt;daily&lt;/code&gt;（当日汇总）、&lt;code&gt;current&lt;/code&gt;（当前榜单）、&lt;code&gt;incremental&lt;/code&gt;（增量监控）&lt;/li&gt; 
  &lt;li&gt;Docker 支持：完整的 Docker 部署方案，支持容器化运行&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;配置文件说明&lt;/strong&gt;：&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;config/config.yaml&lt;/code&gt; - 主配置文件（应用设置、爬虫配置、通知配置、平台配置等）&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;config/frequency_words.txt&lt;/code&gt; - 关键词配置（监控词汇设置）&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/07/09 - v1.4.1&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;功能新增&lt;/strong&gt;：增加增量推送(在 main.py 头部配置 FOCUS_NEW_ONLY)，该开关只关心新话题而非持续热度，只在有新内容时才发通知。&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;修复问题&lt;/strong&gt;: 某些情况下，由于新闻本身含有特殊符号导致的偶发性排版异常。&lt;/p&gt; 
 &lt;h3&gt;2025/06/23 - v1.3.0&lt;/h3&gt; 
 &lt;p&gt;企业微信 和 Telegram 的推送消息有长度限制，对此我采用将消息拆分推送的方式。开发文档详见&lt;a href="https://developer.work.weixin.qq.com/document/path/91770"&gt;企业微信&lt;/a&gt; 和 &lt;a href="https://core.telegram.org/bots/api"&gt;Telegram&lt;/a&gt;&lt;/p&gt; 
 &lt;h3&gt;2025/06/21 - v1.2.1&lt;/h3&gt; 
 &lt;p&gt;在本版本之前的旧版本，不仅 main.py 需要复制替换， crawler.yml 也需要你复制替换 &lt;a href="https://github.com/sansan0/TrendRadar/raw/master/.github/workflows/crawler.yml"&gt;https://github.com/sansan0/TrendRadar/blob/master/.github/workflows/crawler.yml&lt;/a&gt;&lt;/p&gt; 
 &lt;h3&gt;2025/06/19 - v1.2.0&lt;/h3&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;感谢 claude research 整理的各平台 api ,让我快速完成各平台适配（虽然代码更多冗余了~&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;ol&gt; 
  &lt;li&gt;支持 telegram ，企业微信，钉钉推送渠道, 支持多渠道配置和同时推送&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;h3&gt;2025/06/18 - v1.1.0&lt;/h3&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;strong&gt;200 star⭐&lt;/strong&gt; 了, 继续给大伙儿助兴~近期，在我的"怂恿"下，挺多人在我公众号点赞分享推荐助力了我，我都在后台看见了具体账号的鼓励数据，很多都成了天使轮老粉（我玩公众号才一个多月，虽然注册是七八年前的事了哈哈，属于上车早，发车晚），但因为你们没有留言或私信我，所以我也无法一一回应并感谢支持，在此一并谢谢！&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;ol&gt; 
  &lt;li&gt;重要的更新，加了权重，你现在看到的新闻都是最热点最有关注度的出现在最上面&lt;/li&gt; 
  &lt;li&gt;更新文档使用，因为近期更新了很多功能，而且之前的使用文档我偷懒写的简单（见下面的 ⚙️ frequency_words.txt 配置完整教程）&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;h3&gt;2025/06/16 - v1.0.0&lt;/h3&gt; 
 &lt;ol&gt; 
  &lt;li&gt;增加了一个项目新版本更新提示，默认打开，如要关掉，可以在 main.py 中把 "FEISHU_SHOW_VERSION_UPDATE": True 中的 True 改成 False 即可&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;h3&gt;2025/06/13+14&lt;/h3&gt; 
 &lt;ol&gt; 
  &lt;li&gt;去掉了兼容代码，之前 fork 的同学，直接复制代码会在当天显示异常（第二天会恢复正常）&lt;/li&gt; 
  &lt;li&gt;feishu 和 html 底部增加一个新增新闻显示&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;h3&gt;2025/06/09&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;100 star⭐&lt;/strong&gt; 了，写个小功能给大伙儿助助兴 frequency_words.txt 文件增加了一个【必须词】功能，使用 + 号&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;必须词语法如下：&lt;br /&gt; 唐僧或者猪八戒必须在标题里同时出现，才会收录到推送新闻中&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code&gt;+唐僧
+猪八戒
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="2"&gt; 
  &lt;li&gt;过滤词的优先级更高：&lt;br /&gt; 如果标题中过滤词匹配到唐僧念经，那么即使必须词里有唐僧，也不显示&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code&gt;+唐僧
!唐僧念经
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;2025/06/02&lt;/h3&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;strong&gt;网页&lt;/strong&gt;和&lt;strong&gt;飞书消息&lt;/strong&gt;支持手机直接跳转详情新闻&lt;/li&gt; 
  &lt;li&gt;优化显示效果 + 1&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;h3&gt;2025/05/26&lt;/h3&gt; 
 &lt;ol&gt; 
  &lt;li&gt;飞书消息显示效果优化&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;table&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td align="center"&gt; 优化前&lt;br /&gt; &lt;img src="https://raw.githubusercontent.com/sansan0/TrendRadar/master/_image/before.jpg" alt="飞书消息界面 - 优化前" width="400" /&gt; &lt;/td&gt; 
    &lt;td align="center"&gt; 优化后&lt;br /&gt; &lt;img src="https://raw.githubusercontent.com/sansan0/TrendRadar/master/_image/after.jpg" alt="飞书消息界面 - 优化后" width="400" /&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h2&gt;🚀 使用方式&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Fork 本项目&lt;/strong&gt;到你的 GitHub 账户&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;点击本页面右上角的"Fork"按钮&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;设置 GitHub Secrets（选择你需要的平台）&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;在你 Fork 后的仓库中，进入 &lt;code&gt;Settings&lt;/code&gt; &amp;gt; &lt;code&gt;Secrets and variables&lt;/code&gt; &amp;gt; &lt;code&gt;Actions&lt;/code&gt; &amp;gt; &lt;code&gt;New repository secret&lt;/code&gt;，然后根据需要配置以下任一或多个通知平台：&lt;/p&gt; &lt;p&gt;可以同时配置多个平台，系统会向所有配置的平台发送通知。&lt;/p&gt; 
  &lt;details&gt; 
   &lt;summary&gt; &lt;strong&gt;👉 企业微信机器人&lt;/strong&gt;（配置最简单最迅速）&lt;/summary&gt; 
   &lt;br /&gt; 
   &lt;p&gt;&lt;strong&gt;GitHub Secret 配置：&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;名称：&lt;code&gt;WEWORK_WEBHOOK_URL&lt;/code&gt;&lt;/li&gt; 
    &lt;li&gt;值：你的企业微信机器人 Webhook 地址&lt;/li&gt; 
   &lt;/ul&gt; 
   &lt;p&gt;&lt;strong&gt;机器人设置步骤：&lt;/strong&gt;&lt;/p&gt; 
   &lt;h4&gt;手机端设置：&lt;/h4&gt; 
   &lt;ol&gt; 
    &lt;li&gt;打开企业微信 App → 进入目标内部群聊&lt;/li&gt; 
    &lt;li&gt;点击右上角"…"按钮 → 选择"群机器人"&lt;/li&gt; 
    &lt;li&gt;点击"添加" → 点击"新建" → 设置机器人昵称&lt;/li&gt; 
    &lt;li&gt;复制 Webhook 地址，配置到上方的 GitHub Secret 中&lt;/li&gt; 
   &lt;/ol&gt; 
   &lt;h4&gt;PC 端设置流程类似&lt;/h4&gt; 
  &lt;/details&gt; 
  &lt;details&gt; 
   &lt;summary&gt; &lt;strong&gt;👉 飞书机器人&lt;/strong&gt;（消息显示最友好）&lt;/summary&gt; 
   &lt;br /&gt; 
   &lt;p&gt;&lt;strong&gt;GitHub Secret 配置：&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;名称：&lt;code&gt;FEISHU_WEBHOOK_URL&lt;/code&gt;&lt;/li&gt; 
    &lt;li&gt;值：你的飞书机器人 Webhook 地址&lt;/li&gt; 
   &lt;/ul&gt; 
   &lt;p&gt;&lt;strong&gt;机器人设置步骤：&lt;/strong&gt;&lt;/p&gt; 
   &lt;ol&gt; 
    &lt;li&gt; &lt;p&gt;电脑浏览器打开 &lt;a href="https://botbuilder.feishu.cn/home/my-app"&gt;https://botbuilder.feishu.cn/home/my-app&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;点击"新建机器人应用"&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;进入创建的应用后，点击"流程涉及" &amp;gt; "创建流程" &amp;gt; "选择触发器"&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;往下滑动，点击"Webhook 触发"&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;此时你会看到"Webhook 地址"，把这个链接先复制到本地记事本暂存，继续接下来的操作&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;"参数"里面放上下面的内容，然后点击"完成"&lt;/p&gt; &lt;/li&gt; 
   &lt;/ol&gt; 
   &lt;pre&gt;&lt;code class="language-json"&gt;{
  "message_type": "text",
  "content": {
    "total_titles": "{{内容}}",
    "timestamp": "{{内容}}",
    "report_type": "{{内容}}",
    "text": "{{内容}}"
  }
}
&lt;/code&gt;&lt;/pre&gt; 
   &lt;ol start="7"&gt; 
    &lt;li&gt; &lt;p&gt;点击"选择操作" &amp;gt; "发送飞书消息"，勾选 "群消息"，然后点击下面的输入框，点击"我管理的群组"（如果没有群组，你可以在飞书 app 上创建群组）&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;消息标题填写"TrendRadar 热点监控"&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;最关键的部分来了，点击 + 按钮，选择"Webhook 触发"，然后按照下面的图片摆放&lt;/p&gt; &lt;/li&gt; 
   &lt;/ol&gt; 
   &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/sansan0/TrendRadar/master/_image/image.png" alt="飞书机器人配置示例" /&gt;&lt;/p&gt; 
   &lt;ol start="10"&gt; 
    &lt;li&gt;配置完成后，将第 5 步复制的 Webhook 地址配置到 GitHub Secrets 中的 &lt;code&gt;FEISHU_WEBHOOK_URL&lt;/code&gt;&lt;/li&gt; 
   &lt;/ol&gt; 
  &lt;/details&gt; 
  &lt;details&gt; 
   &lt;summary&gt; &lt;strong&gt;👉 钉钉机器人&lt;/strong&gt;&lt;/summary&gt; 
   &lt;br /&gt; 
   &lt;p&gt;&lt;strong&gt;GitHub Secret 配置：&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;名称：&lt;code&gt;DINGTALK_WEBHOOK_URL&lt;/code&gt;&lt;/li&gt; 
    &lt;li&gt;值：你的钉钉机器人 Webhook 地址&lt;/li&gt; 
   &lt;/ul&gt; 
   &lt;p&gt;&lt;strong&gt;机器人设置步骤：&lt;/strong&gt;&lt;/p&gt; 
   &lt;ol&gt; 
    &lt;li&gt; &lt;p&gt;&lt;strong&gt;创建机器人（仅 PC 端支持）&lt;/strong&gt;：&lt;/p&gt; 
     &lt;ul&gt; 
      &lt;li&gt;打开钉钉 PC 客户端，进入目标群聊&lt;/li&gt; 
      &lt;li&gt;点击群设置图标（⚙️）→ 往下翻找到"机器人"点开&lt;/li&gt; 
      &lt;li&gt;选择"添加机器人" → "自定义"&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;&lt;strong&gt;配置机器人&lt;/strong&gt;：&lt;/p&gt; 
     &lt;ul&gt; 
      &lt;li&gt;设置机器人名称&lt;/li&gt; 
      &lt;li&gt;&lt;strong&gt;安全设置&lt;/strong&gt;： 
       &lt;ul&gt; 
        &lt;li&gt;&lt;strong&gt;自定义关键词&lt;/strong&gt;：设置 "热点"&lt;/li&gt; 
       &lt;/ul&gt; &lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;&lt;strong&gt;完成设置&lt;/strong&gt;：&lt;/p&gt; 
     &lt;ul&gt; 
      &lt;li&gt;勾选服务条款协议 → 点击"完成"&lt;/li&gt; 
      &lt;li&gt;复制获得的 Webhook URL&lt;/li&gt; 
      &lt;li&gt;将 URL 配置到 GitHub Secrets 中的 &lt;code&gt;DINGTALK_WEBHOOK_URL&lt;/code&gt;&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
   &lt;/ol&gt; 
   &lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;：移动端只能接收消息，无法创建新机器人。&lt;/p&gt; 
  &lt;/details&gt; 
  &lt;details&gt; 
   &lt;summary&gt; &lt;strong&gt;👉 Telegram Bot&lt;/strong&gt;&lt;/summary&gt; 
   &lt;br /&gt; 
   &lt;p&gt;&lt;strong&gt;GitHub Secret 配置：&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;名称：&lt;code&gt;TELEGRAM_BOT_TOKEN&lt;/code&gt; - 你的 Telegram Bot Token&lt;/li&gt; 
    &lt;li&gt;名称：&lt;code&gt;TELEGRAM_CHAT_ID&lt;/code&gt; - 你的 Telegram Chat ID&lt;/li&gt; 
   &lt;/ul&gt; 
   &lt;p&gt;&lt;strong&gt;机器人设置步骤：&lt;/strong&gt;&lt;/p&gt; 
   &lt;ol&gt; 
    &lt;li&gt; &lt;p&gt;&lt;strong&gt;创建机器人&lt;/strong&gt;：&lt;/p&gt; 
     &lt;ul&gt; 
      &lt;li&gt;在 Telegram 中搜索 &lt;code&gt;@BotFather&lt;/code&gt;（大小写注意，有蓝色徽章勾勾，有类似 37849827 monthly users，这个才是官方的，有一些仿官方的账号注意辨别）&lt;/li&gt; 
      &lt;li&gt;发送 &lt;code&gt;/newbot&lt;/code&gt; 命令创建新机器人&lt;/li&gt; 
      &lt;li&gt;设置机器人名称（必须以"bot"结尾，很容易遇到重复名字，所以你要绞尽脑汁想不同的名字）&lt;/li&gt; 
      &lt;li&gt;获取 Bot Token（格式如：&lt;code&gt;123456789:AAHfiqksKZ8WmR2zSjiQ7_v4TMAKdiHm9T0&lt;/code&gt;）&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;&lt;strong&gt;获取 Chat ID&lt;/strong&gt;：&lt;/p&gt; &lt;p&gt;&lt;strong&gt;方法一：通过官方 API 获取&lt;/strong&gt;&lt;/p&gt; 
     &lt;ul&gt; 
      &lt;li&gt;先向你的机器人发送一条消息&lt;/li&gt; 
      &lt;li&gt;访问：&lt;code&gt;https://api.telegram.org/bot&amp;lt;你的Bot Token&amp;gt;/getUpdates&lt;/code&gt;&lt;/li&gt; 
      &lt;li&gt;在返回的 JSON 中找到 &lt;code&gt;"chat":{"id":数字}&lt;/code&gt; 中的数字&lt;/li&gt; 
     &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;方法二：使用第三方工具&lt;/strong&gt;&lt;/p&gt; 
     &lt;ul&gt; 
      &lt;li&gt;搜索 &lt;code&gt;@userinfobot&lt;/code&gt; 并发送 &lt;code&gt;/start&lt;/code&gt;&lt;/li&gt; 
      &lt;li&gt;获取你的用户 ID 作为 Chat ID&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;&lt;strong&gt;配置到 GitHub&lt;/strong&gt;：&lt;/p&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;code&gt;TELEGRAM_BOT_TOKEN&lt;/code&gt;：填入第 1 步获得的 Bot Token&lt;/li&gt; 
      &lt;li&gt;&lt;code&gt;TELEGRAM_CHAT_ID&lt;/code&gt;：填入第 2 步获得的 Chat ID&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
   &lt;/ol&gt; 
  &lt;/details&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;主要配置&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;推送设置：&lt;/strong&gt; : 在 &lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/config/config.yaml"&gt;config/config.yaml&lt;/a&gt; 中进行，可根据里面的描述文字操作，这里不重复了&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;比如: 在 &lt;code&gt;config/config.yaml&lt;/code&gt; 中修改 &lt;code&gt;report.mode&lt;/code&gt; 设置：&lt;/p&gt; 
    &lt;table&gt; 
     &lt;thead&gt; 
      &lt;tr&gt; 
       &lt;th&gt;模式&lt;/th&gt; 
       &lt;th&gt;推送时机&lt;/th&gt; 
       &lt;th&gt;显示内容&lt;/th&gt; 
       &lt;th&gt;适用场景&lt;/th&gt; 
      &lt;/tr&gt; 
     &lt;/thead&gt; 
     &lt;tbody&gt; 
      &lt;tr&gt; 
       &lt;td&gt;&lt;strong&gt;当日汇总模式&lt;/strong&gt;&lt;br /&gt;&lt;code&gt;daily&lt;/code&gt;&lt;/td&gt; 
       &lt;td&gt;按时推送&lt;/td&gt; 
       &lt;td&gt;当日所有匹配新闻&lt;br /&gt;+ 新增新闻区域&lt;/td&gt; 
       &lt;td&gt;日报总结&lt;br /&gt;全面了解当日热点趋势&lt;/td&gt; 
      &lt;/tr&gt; 
      &lt;tr&gt; 
       &lt;td&gt;&lt;strong&gt;当前榜单模式&lt;/strong&gt;&lt;br /&gt;&lt;code&gt;current&lt;/code&gt;&lt;/td&gt; 
       &lt;td&gt;按时推送&lt;/td&gt; 
       &lt;td&gt;当前榜单匹配新闻&lt;br /&gt;+ 新增新闻区域&lt;/td&gt; 
       &lt;td&gt;实时热点追踪&lt;br /&gt;了解当前最火的内容&lt;/td&gt; 
      &lt;/tr&gt; 
      &lt;tr&gt; 
       &lt;td&gt;&lt;strong&gt;增量监控模式&lt;/strong&gt;&lt;br /&gt;&lt;code&gt;incremental&lt;/code&gt;&lt;/td&gt; 
       &lt;td&gt;有新增才推送&lt;/td&gt; 
       &lt;td&gt;新出现的匹配频率词新闻&lt;/td&gt; 
       &lt;td&gt;避免重复信息干扰&lt;br /&gt;高频监控场景&lt;/td&gt; 
      &lt;/tr&gt; 
     &lt;/tbody&gt; 
    &lt;/table&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;关键词配置&lt;/strong&gt;: 修改 &lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/config/frequency_words.txt"&gt;config/frequency_words.txt&lt;/a&gt; 文件，添加你关心的关键词&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;details&gt; 
   &lt;summary&gt;&lt;strong&gt;👉 frequency_words.txt 配置教程&lt;/strong&gt;&lt;/summary&gt; 
   &lt;br /&gt; 
   &lt;p&gt;在 &lt;code&gt;frequency_words.txt&lt;/code&gt; 文件中配置监控的关键词，支持三种语法和词组功能。&lt;/p&gt; 
   &lt;p&gt;关键词越靠前，新闻的优先级越高，你可以根据自己的关注度调整关键词顺序&lt;/p&gt; 
   &lt;h3&gt;📋 基础语法说明&lt;/h3&gt; 
   &lt;h4&gt;1. &lt;strong&gt;普通关键词&lt;/strong&gt; - 基础匹配&lt;/h4&gt; 
   &lt;pre&gt;&lt;code class="language-txt"&gt;华为
OPPO
苹果
&lt;/code&gt;&lt;/pre&gt; 
   &lt;p&gt;&lt;strong&gt;作用：&lt;/strong&gt; 新闻标题包含其中&lt;strong&gt;任意一个词&lt;/strong&gt;就会被捕获&lt;/p&gt; 
   &lt;h4&gt;2. &lt;strong&gt;必须词&lt;/strong&gt; &lt;code&gt;+词汇&lt;/code&gt; - 限定范围&lt;/h4&gt; 
   &lt;pre&gt;&lt;code class="language-txt"&gt;华为
OPPO
+手机
&lt;/code&gt;&lt;/pre&gt; 
   &lt;p&gt;&lt;strong&gt;作用：&lt;/strong&gt; 必须同时包含普通词&lt;strong&gt;和&lt;/strong&gt;必须词才会被捕获&lt;/p&gt; 
   &lt;h4&gt;3. &lt;strong&gt;过滤词&lt;/strong&gt; &lt;code&gt;!词汇&lt;/code&gt; - 排除干扰&lt;/h4&gt; 
   &lt;pre&gt;&lt;code class="language-txt"&gt;苹果
华为
!水果
!价格
&lt;/code&gt;&lt;/pre&gt; 
   &lt;p&gt;&lt;strong&gt;作用：&lt;/strong&gt; 包含过滤词的新闻会被&lt;strong&gt;直接排除&lt;/strong&gt;，即使包含关键词&lt;/p&gt; 
   &lt;h3&gt;🔗 词组功能 - 空行分隔的重要作用&lt;/h3&gt; 
   &lt;p&gt;&lt;strong&gt;核心规则：&lt;/strong&gt; 用&lt;strong&gt;空行&lt;/strong&gt;分隔不同的词组，每个词组独立统计&lt;/p&gt; 
   &lt;h4&gt;示例配置：&lt;/h4&gt; 
   &lt;pre&gt;&lt;code class="language-txt"&gt;iPhone
华为
OPPO
+发布

A股
上证
深证
+涨跌
!预测

世界杯
欧洲杯
亚洲杯
+比赛
&lt;/code&gt;&lt;/pre&gt; 
   &lt;h4&gt;词组解释及匹配效果：&lt;/h4&gt; 
   &lt;p&gt;&lt;strong&gt;第1组 - 手机新品类：&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;关键词：iPhone、华为、OPPO&lt;/li&gt; 
    &lt;li&gt;必须词：发布&lt;/li&gt; 
    &lt;li&gt;效果：必须包含手机品牌名，同时包含"发布"&lt;/li&gt; 
   &lt;/ul&gt; 
   &lt;p&gt;&lt;strong&gt;匹配示例：&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;✅ "iPhone 15正式发布售价公布" ← 有"iPhone"+"发布"&lt;/li&gt; 
    &lt;li&gt;✅ "华为Mate60系列发布会直播" ← 有"华为"+"发布"&lt;/li&gt; 
    &lt;li&gt;✅ "OPPO Find X7发布时间确定" ← 有"OPPO"+"发布"&lt;/li&gt; 
    &lt;li&gt;❌ "iPhone销量创新高" ← 有"iPhone"但缺少"发布"&lt;/li&gt; 
   &lt;/ul&gt; 
   &lt;p&gt;&lt;strong&gt;第2组 - 股市行情类：&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;关键词：A股、上证、深证&lt;/li&gt; 
    &lt;li&gt;必须词：涨跌&lt;/li&gt; 
    &lt;li&gt;过滤词：预测&lt;/li&gt; 
    &lt;li&gt;效果：包含股市相关词，同时包含"涨跌"，但排除包含"预测"的内容&lt;/li&gt; 
   &lt;/ul&gt; 
   &lt;p&gt;&lt;strong&gt;匹配示例：&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;✅ "A股今日大幅涨跌分析" ← 有"A股"+"涨跌"&lt;/li&gt; 
    &lt;li&gt;✅ "上证指数涨跌原因解读" ← 有"上证"+"涨跌"&lt;/li&gt; 
    &lt;li&gt;❌ "专家预测A股涨跌趋势" ← 有"A股"+"涨跌"但包含"预测"&lt;/li&gt; 
    &lt;li&gt;❌ "A股成交量创新高" ← 有"A股"但缺少"涨跌"&lt;/li&gt; 
   &lt;/ul&gt; 
   &lt;p&gt;&lt;strong&gt;第3组 - 足球赛事类：&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;关键词：世界杯、欧洲杯、亚洲杯&lt;/li&gt; 
    &lt;li&gt;必须词：比赛&lt;/li&gt; 
    &lt;li&gt;效果：必须包含杯赛名称，同时包含"比赛"&lt;/li&gt; 
   &lt;/ul&gt; 
   &lt;p&gt;&lt;strong&gt;匹配示例：&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;✅ "世界杯小组赛比赛结果" ← 有"世界杯"+"比赛"&lt;/li&gt; 
    &lt;li&gt;✅ "欧洲杯决赛比赛时间" ← 有"欧洲杯"+"比赛"&lt;/li&gt; 
    &lt;li&gt;❌ "世界杯门票开售" ← 有"世界杯"但缺少"比赛"&lt;/li&gt; 
   &lt;/ul&gt; 
   &lt;h3&gt;🎯 配置技巧&lt;/h3&gt; 
   &lt;h4&gt;1. &lt;strong&gt;从宽到严的配置策略&lt;/strong&gt;&lt;/h4&gt; 
   &lt;pre&gt;&lt;code class="language-txt"&gt;# 第一步：先用宽泛关键词测试
人工智能
AI
ChatGPT

# 第二步：发现误匹配后，加入必须词限定
人工智能  
AI
ChatGPT
+技术

# 第三步：发现干扰内容后，加入过滤词
人工智能
AI  
ChatGPT
+技术
!广告
!培训
&lt;/code&gt;&lt;/pre&gt; 
   &lt;h4&gt;2. &lt;strong&gt;避免过度复杂&lt;/strong&gt;&lt;/h4&gt; 
   &lt;p&gt;❌ &lt;strong&gt;不推荐：&lt;/strong&gt; 一个词组包含太多词汇&lt;/p&gt; 
   &lt;pre&gt;&lt;code class="language-txt"&gt;华为
OPPO
苹果
三星
vivo
一加
魅族
+手机
+发布
+销量
!假货
!维修
!二手
&lt;/code&gt;&lt;/pre&gt; 
   &lt;p&gt;✅ &lt;strong&gt;推荐：&lt;/strong&gt; 拆分成多个精确的词组&lt;/p&gt; 
   &lt;pre&gt;&lt;code class="language-txt"&gt;华为
OPPO
+新品

苹果
三星  
+发布

手机
销量
+市场
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;👉 自定义监控平台&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h3&gt;🔧 自定义监控平台&lt;/h3&gt; 
 &lt;p&gt;本项目的资讯数据来源于 &lt;a href="https://github.com/ourongxing/newsnow"&gt;newsnow&lt;/a&gt; ，你可以点击&lt;a href="https://newsnow.busiyi.world/"&gt;网站&lt;/a&gt;，点击[更多]，查看是否有你想要的平台。&lt;/p&gt; 
 &lt;p&gt;具体添加可访问 &lt;a href="https://github.com/ourongxing/newsnow/tree/main/server/sources"&gt;项目源代码&lt;/a&gt;，根据里面的文件名，在 &lt;code&gt;config/config.yaml&lt;/code&gt; 文件中修改 &lt;code&gt;platforms&lt;/code&gt; 配置：&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-yaml"&gt;platforms:
  - id: "toutiao"
    name: "今日头条"
  - id: "baidu"  
    name: "百度热搜"
  - id: "wallstreetcn-hot"
    name: "华尔街见闻"
  # 添加更多平台...
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;👉 Docker 部署&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h3&gt;🐳 Docker 部署&lt;/h3&gt; 
 &lt;h4&gt;方式一：快速体验（一行命令）&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# 直接运行，使用默认配置（仅体验功能，无推送通知）
docker run -d --name trend-radar \
  -v ./config:/app/config:ro \
  -v ./output:/app/output \
  -e CRON_SCHEDULE="*/30 * * * *" \
  -e RUN_MODE="cron" \
  -e IMMEDIATE_RUN="true" \
  wantcat/trendradar:latest

# 或者启用手机应用推送通知
docker run -d --name trend-radar \
  -v ./config:/app/config:ro \
  -v ./output:/app/output \
  -e FEISHU_WEBHOOK_URL="你的飞书webhook" \
  -e DINGTALK_WEBHOOK_URL="你的钉钉webhook" \
  -e WEWORK_WEBHOOK_URL="你的企业微信webhook" \
  -e TELEGRAM_BOT_TOKEN="你的telegram_bot_token" \
  -e TELEGRAM_CHAT_ID="你的telegram_chat_id" \
  -e CRON_SCHEDULE="*/30 * * * *" \
  -e RUN_MODE="cron" \
  -e IMMEDIATE_RUN="true" \
  wantcat/trendradar:latest
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;：快速体验模式需要先准备配置文件：&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Linux/macOS 系统：&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# 创建配置目录并下载配置文件
mkdir -p config output
wget https://raw.githubusercontent.com/sansan0/TrendRadar/master/config/config.yaml -P config/
wget https://raw.githubusercontent.com/sansan0/TrendRadar/master/config/frequency_words.txt -P config/
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;或者&lt;strong&gt;手动创建&lt;/strong&gt;：&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;在当前目录下创建两个文件夹：&lt;code&gt;config&lt;/code&gt; 和 &lt;code&gt;output&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;下载配置文件到对应位置： 
   &lt;ul&gt; 
    &lt;li&gt;访问 &lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/config/config.yaml"&gt;https://raw.githubusercontent.com/sansan0/TrendRadar/master/config/config.yaml&lt;/a&gt; → 右键"另存为" → 保存到 &lt;code&gt;config\config.yaml&lt;/code&gt;&lt;/li&gt; 
    &lt;li&gt;访问 &lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/config/frequency_words.txt"&gt;https://raw.githubusercontent.com/sansan0/TrendRadar/master/config/frequency_words.txt&lt;/a&gt; → 右键"另存为" → 保存到 &lt;code&gt;config\frequency_words.txt&lt;/code&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;完成后的目录结构应该是：&lt;/p&gt; 
 &lt;pre&gt;&lt;code&gt;当前目录/
├── config/
│   ├── config.yaml
│   └── frequency_words.txt
└── output/
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;方式二：使用 docker-compose（推荐）&lt;/h4&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;strong&gt;创建项目目录和配置&lt;/strong&gt;: &lt;pre&gt;&lt;code class="language-bash"&gt;# 创建目录结构
mkdir -p trendradar/{config,docker}
cd trendradar

# 下载配置文件模板
wget https://raw.githubusercontent.com/sansan0/TrendRadar/master/config/config.yaml -P config/
wget https://raw.githubusercontent.com/sansan0/TrendRadar/master/config/frequency_words.txt -P config/

# 下载 docker-compose 配置
wget https://raw.githubusercontent.com/sansan0/TrendRadar/master/docker/.env
wget https://raw.githubusercontent.com/sansan0/TrendRadar/master/docker/docker-compose.yml
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;完成后的目录结构应该是：&lt;/p&gt; 
 &lt;pre&gt;&lt;code&gt;当前目录/
├── config/
│   ├── config.yaml
│   └── frequency_words.txt
└── docker/
    ├── .env
    └── docker-compose.yml
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="2"&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;配置文件说明&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;code&gt;config/config.yaml&lt;/code&gt; - 应用主配置（报告模式、推送设置等）&lt;/li&gt; 
    &lt;li&gt;&lt;code&gt;config/frequency_words.txt&lt;/code&gt; - 关键词配置（设置你关心的热点词汇）&lt;/li&gt; 
    &lt;li&gt;&lt;code&gt;.env&lt;/code&gt; - 环境变量配置（webhook URLs 和定时任务）&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;启动服务&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# 拉取最新镜像并启动
docker-compose pull
docker-compose up -d
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;查看运行状态&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# 查看日志
docker logs -f trend-radar

# 查看容器状态
docker ps | grep trend-radar
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;h4&gt;方式三：本地构建（开发者选项）&lt;/h4&gt; 
 &lt;p&gt;如果需要自定义修改代码或构建自己的镜像：&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# 克隆项目
git clone https://github.com/sansan0/TrendRadar.git
cd TrendRadar

# 修改配置文件
vim config/config.yaml
vim config/frequency_words.txt

# 使用构建版本的 docker-compose
cd docker
cp docker-compose-build.yml docker-compose.yml

# 构建并启动
docker-compose build
docker-compose up -d
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;镜像更新&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# 方式一：手动更新
docker pull wantcat/trendradar:latest
docker-compose down
docker-compose up -d

# 方式二：使用 docker-compose 更新
docker-compose pull
docker-compose up -d
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;服务管理命令&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# 查看运行状态
docker exec -it trend-radar python manage.py status

# 手动执行一次爬虫
docker exec -it trend-radar python manage.py run

# 查看实时日志
docker exec -it trend-radar python manage.py logs

# 显示当前配置
docker exec -it trend-radar python manage.py config

# 显示输出文件
docker exec -it trend-radar python manage.py files

# 查看帮助信息
docker exec -it trend-radar python manage.py help

# 重启容器
docker restart trend-radar

# 停止容器
docker stop trend-radar

# 删除容器（保留数据）
docker rm trend-radar
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;数据持久化&lt;/h4&gt; 
 &lt;p&gt;生成的报告和数据默认保存在 &lt;code&gt;./output&lt;/code&gt; 目录下，即使容器重启或删除，数据也会保留。&lt;/p&gt; 
 &lt;h4&gt;故障排查&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# 检查容器状态
docker inspect trend-radar

# 查看容器日志
docker logs --tail 100 trend-radar

# 进入容器调试
docker exec -it trend-radar /bin/bash

# 验证配置文件
docker exec -it trend-radar ls -la /app/config/
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;👉 热点权重调整&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;当前默认的配置是平衡性配置&lt;/p&gt; 
 &lt;h3&gt;两个核心场景&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;追实时热点型&lt;/strong&gt;：&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-yaml"&gt;weight:
  rank_weight: 0.8    # 主要看排名
  frequency_weight: 0.1  # 不太在乎持续性
  hotness_weight: 0.1
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;适用人群&lt;/strong&gt;：自媒体博主、营销人员、想快速了解当下最火话题的用户&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;追深度话题型&lt;/strong&gt;：&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-yaml"&gt;weight:
  rank_weight: 0.4    # 适度看排名
  frequency_weight: 0.5  # 重视当天内的持续热度
  hotness_weight: 0.1
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;适用人群&lt;/strong&gt;：投资者、研究人员、新闻工作者、需要深度分析趋势的用户&lt;/p&gt; 
 &lt;h3&gt;调整的方法&lt;/h3&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;strong&gt;三个数字加起来必须等于 1.0&lt;/strong&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;哪个重要就调大哪个&lt;/strong&gt;：在乎排名就调大 rank_weight，在乎持续性就调大 frequency_weight&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;建议每次只调 0.1-0.2&lt;/strong&gt;，观察效果&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;核心思路：追求速度和时效性的用户提高排名权重，追求深度和稳定性的用户提高频次权重。&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;☕ 学习交流与1元点赞&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;心意到就行，收到的点赞用于提高开发者开源的积极性&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;公众号关注&lt;/th&gt; 
    &lt;th align="center"&gt;微信点赞&lt;/th&gt; 
    &lt;th align="center"&gt;支付宝点赞&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/sansan0/TrendRadar/master/_image/weixin.png" width="300" title="硅基茶水间" /&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;img src="https://cdn-1258574687.cos.ap-shanghai.myqcloud.com/img/%2F2025%2F07%2F17%2F2ae0a88d98079f7e876c2b4dc85233c6-9e8025.JPG" width="300" title="微信支付" /&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;img src="https://cdn-1258574687.cos.ap-shanghai.myqcloud.com/img/%2F2025%2F07%2F17%2Fed4f20ab8e35be51f8e84c94e6e239b4-fe4947.JPG" width="300" title="支付宝支付" /&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h3&gt;项目相关推荐&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;附项目相关的两篇文章，欢迎留言交流&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://mp.weixin.qq.com/s/jzn0vLiQFX408opcfpPPxQ"&gt;2个月破 1000 star，我的GitHub项目推广实战经验&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mp.weixin.qq.com/s/8ghyfDAtQZjLrnWTQabYOQ"&gt;基于本项目，如何开展公众号或者新闻资讯类文章写作&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;AI 开发：&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;如果你有小众需求，完全可以基于我的项目自行开发，零编程基础的也可以试试&lt;/li&gt; 
 &lt;li&gt;我所有的开源项目或多或少都使用了自己写的&lt;strong&gt;AI辅助软件&lt;/strong&gt;来提升开发效率，这款工具已开源&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;核心功能&lt;/strong&gt;：迅速筛选项目代码喂给AI，你只需要补充个人需求即可&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;项目地址&lt;/strong&gt;：&lt;a href="https://github.com/sansan0/ai-code-context-helper"&gt;https://github.com/sansan0/ai-code-context-helper&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;其余项目&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;📍 毛主席足迹地图 - 交互式动态展示1893-1976年完整轨迹。欢迎诸位同志贡献数据&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/sansan0/mao-map"&gt;https://github.com/sansan0/mao-map&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;哔哩哔哩(bilibili)评论区数据可视化分析软件&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/sansan0/bilibili-comment-analyzer"&gt;https://github.com/sansan0/bilibili-comment-analyzer&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;👉 微信推送通知方案&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;由于该方案是基于企业微信的插件机制，推送样式也十分不同，所以相关实现我暂时不准备纳入当前项目&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;ul&gt; 
  &lt;li&gt;fork 这位兄台的项目 &lt;a href="https://github.com/jayzqj/TrendRadar"&gt;https://github.com/jayzqj/TrendRadar&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;完成上方的企业微信推送设置&lt;/li&gt; 
  &lt;li&gt;按照下面图片操作&lt;/li&gt; 
  &lt;li&gt;配置好后，手机上的企业微信 app 删除掉也没事&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;img src="https://raw.githubusercontent.com/sansan0/TrendRadar/master/_image/wework.png" title="github" /&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;👉 本项目流程图&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-mermaid"&gt;flowchart TD
    A[👤 用户开始] --&amp;gt; B[🍴 Fork 项目]
    B --&amp;gt; C[⚙️ 选择通知方式]
    
    C --&amp;gt; D1[📱 企业微信群机器人&amp;lt;br/&amp;gt;最简单快速]
    C --&amp;gt; D2[💬 飞书机器人&amp;lt;br/&amp;gt;显示效果最佳]
    C --&amp;gt; D3[🔔 钉钉机器人&amp;lt;br/&amp;gt;]
    C --&amp;gt; D4[📟 Telegram Bot&amp;lt;br/&amp;gt;]
    
    D1 --&amp;gt; E[🔑 配置 GitHub Secrets&amp;lt;br/&amp;gt;填入机器人 Webhook 地址]
    D2 --&amp;gt; E
    D3 --&amp;gt; E  
    D4 --&amp;gt; E
    
    E --&amp;gt; F[📝 编辑关键词配置&amp;lt;br/&amp;gt;config/frequency_words.txt&amp;lt;br/&amp;gt;添加你关心的词汇]
    F --&amp;gt; G[🎯 选择运行模式&amp;lt;br/&amp;gt;config/config.yaml&amp;lt;br/&amp;gt;daily/current/incremental]
    
    G --&amp;gt; H[✅ 配置完成]
    H --&amp;gt; I[🤖 系统根据设定时间自动运行]
    
    I --&amp;gt; J[📊 爬取各大平台热点]
    J --&amp;gt; K[🔍 根据关键词筛选]
    K --&amp;gt; L[📱 推送到你的手机]
    
    L --&amp;gt; M[📈 查看推送结果]
    M --&amp;gt; N{满意效果?}
    N --&amp;gt;|不满意| F
    N --&amp;gt;|满意| O[🎉 持续接收精准推送]
    
    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style E fill:#fff3e0
    style F fill:#e8f5e8
    style G fill:#e8f5e8
    style L fill:#ffebee
    style O fill:#e8f5e8
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#sansan0/TrendRadar&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=sansan0/TrendRadar&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;📄 许可证&lt;/h2&gt; 
&lt;p&gt;GPL-3.0 License&lt;/p&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;strong&gt;⭐ 如果这个工具对你有帮助，请给项目点个 Star 支持开发！&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#trendradar"&gt;🔝 回到顶部&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>unslothai/unsloth</title>
      <link>https://github.com/unslothai/unsloth</link>
      <description>&lt;p&gt;Fine-tuning &amp; Reinforcement Learning for LLMs. 🦥 Train OpenAI gpt-oss, Qwen3, Llama 4, DeepSeek-R1, Gemma 3, TTS 2x faster with 70% less VRAM.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://unsloth.ai"&gt;
   &lt;picture&gt; 
    &lt;source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20white%20text.png" /&gt; 
    &lt;source media="(prefers-color-scheme: light)" srcset="https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png" /&gt; 
    &lt;img alt="unsloth logo" src="https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png" height="110" style="max-width: 100%;" /&gt; 
   &lt;/picture&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb"&gt;&lt;img src="https://raw.githubusercontent.com/unslothai/unsloth/main/images/start free finetune button.png" width="154" /&gt;&lt;/a&gt; &lt;a href="https://discord.com/invite/unsloth"&gt;&lt;img src="https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord button.png" width="165" /&gt;&lt;/a&gt; &lt;a href="https://docs.unsloth.ai"&gt;&lt;img src="https://raw.githubusercontent.com/unslothai/unsloth/refs/heads/main/images/Documentation%20Button.png" width="137" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;h3&gt;Finetune gpt-oss, Gemma 3n, Qwen3, Llama 4, &amp;amp; Mistral 2x faster with 80% less VRAM!&lt;/h3&gt; 
 &lt;p&gt;&lt;img src="https://i.ibb.co/sJ7RhGG/image-41.png" alt="" /&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;✨ Finetune for Free&lt;/h2&gt; 
&lt;p&gt;Notebooks are beginner friendly. Read our &lt;a href="https://docs.unsloth.ai/get-started/fine-tuning-guide"&gt;guide&lt;/a&gt;. Add your dataset, click "Run All", and export your finetuned model to GGUF, Ollama, vLLM or Hugging Face.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Unsloth supports&lt;/th&gt; 
   &lt;th&gt;Free Notebooks&lt;/th&gt; 
   &lt;th&gt;Performance&lt;/th&gt; 
   &lt;th&gt;Memory use&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;gpt-oss (20B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-Fine-tuning.ipynb"&gt;▶️ Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1.5x faster&lt;/td&gt; 
   &lt;td&gt;70% less&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Gemma 3n (4B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3N_(4B)-Conversational.ipynb"&gt;▶️ Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1.5x faster&lt;/td&gt; 
   &lt;td&gt;50% less&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Qwen3 (14B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(14B)-Reasoning-Conversational.ipynb"&gt;▶️ Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2x faster&lt;/td&gt; 
   &lt;td&gt;70% less&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Qwen3 (4B): GRPO&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(4B)-GRPO.ipynb"&gt;▶️ Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2x faster&lt;/td&gt; 
   &lt;td&gt;80% less&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Qwen2.5-VL (7B): GSPO&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2_5_7B_VL_GRPO.ipynb"&gt;▶️ Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1.5x faster&lt;/td&gt; 
   &lt;td&gt;80% less&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Phi-4 (14B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4-Conversational.ipynb"&gt;▶️ Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2x faster&lt;/td&gt; 
   &lt;td&gt;70% less&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Llama 3.2 Vision (11B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb"&gt;▶️ Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2x faster&lt;/td&gt; 
   &lt;td&gt;50% less&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Llama 3.1 (8B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb"&gt;▶️ Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2x faster&lt;/td&gt; 
   &lt;td&gt;70% less&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Mistral v0.3 (7B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_(7B)-Conversational.ipynb"&gt;▶️ Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2.2x faster&lt;/td&gt; 
   &lt;td&gt;75% less&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Orpheus-TTS (3B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Orpheus_(3B)-TTS.ipynb"&gt;▶️ Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1.5x faster&lt;/td&gt; 
   &lt;td&gt;50% less&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;See all our notebooks for: &lt;a href="https://github.com/unslothai/notebooks?tab=readme-ov-file#-kaggle-notebooks"&gt;Kaggle&lt;/a&gt;, &lt;a href="https://docs.unsloth.ai/get-started/unsloth-notebooks#grpo-reasoning-rl-notebooks"&gt;GRPO&lt;/a&gt;, &lt;strong&gt;&lt;a href="https://docs.unsloth.ai/get-started/unsloth-notebooks#text-to-speech-tts-notebooks"&gt;TTS&lt;/a&gt;&lt;/strong&gt; &amp;amp; &lt;a href="https://docs.unsloth.ai/get-started/unsloth-notebooks#vision-multimodal-notebooks"&gt;Vision&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;See &lt;a href="https://docs.unsloth.ai/get-started/all-our-models"&gt;all our models&lt;/a&gt; and &lt;a href="https://github.com/unslothai/notebooks"&gt;all our notebooks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;See detailed documentation for Unsloth &lt;a href="https://docs.unsloth.ai/"&gt;here&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;⚡ Quickstart&lt;/h2&gt; 
&lt;h3&gt;Linux or WSL&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install unsloth
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Windows&lt;/h3&gt; 
&lt;p&gt;For Windows, &lt;code&gt;pip install unsloth&lt;/code&gt; works only if you have Pytorch installed. For more info, read our &lt;a href="https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation"&gt;Windows Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Docker&lt;/h3&gt; 
&lt;p&gt;Use our official &lt;a href="https://hub.docker.com/r/unsloth/unsloth"&gt;Unsloth Docker image&lt;/a&gt; &lt;code&gt;unsloth/unsloth&lt;/code&gt; container. Read our &lt;a href="https://docs.unsloth.ai/get-started/install-and-update/docker"&gt;Docker Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Blackwell&lt;/h3&gt; 
&lt;p&gt;For RTX 50x, B200, 6000 GPUs, simply do &lt;code&gt;pip install unsloth&lt;/code&gt;. Read our &lt;a href="https://docs.unsloth.ai/basics/training-llms-with-blackwell-rtx-50-series-and-unsloth"&gt;Blackwell Guide&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;🦥 Unsloth.ai News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;📣 &lt;strong&gt;Vision RL&lt;/strong&gt; You can now train VLMs with GRPO or GSPO in Unsloth! &lt;a href="https://docs.unsloth.ai/new/vision-reinforcement-learning-vlm-rl"&gt;Read guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;📣 &lt;strong&gt;Memory-efficient RL&lt;/strong&gt; We're introducing even better RL. Our new kernels &amp;amp; algos allows faster RL with 50% less VRAM &amp;amp; 10× more context. &lt;a href="https://docs.unsloth.ai/new/memory-efficient-rl"&gt;Read blog&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;📣 &lt;strong&gt;gpt-oss&lt;/strong&gt; by OpenAI: For details on &lt;a href="https://docs.unsloth.ai/new/long-context-gpt-oss-training"&gt;Unsloth Flex Attention&lt;/a&gt;, long-context training, bug fixes, &lt;a href="https://docs.unsloth.ai/basics/gpt-oss"&gt;Read our Guide&lt;/a&gt;. 20B works on a 14GB GPU and 120B on 65GB VRAM. &lt;a href="https://huggingface.co/collections/unsloth/gpt-oss-6892433695ce0dee42f31681"&gt;gpt-oss uploads&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;📣 &lt;strong&gt;Gemma 3n&lt;/strong&gt; by Google: &lt;a href="https://docs.unsloth.ai/basics/gemma-3n-how-to-run-and-fine-tune"&gt;Read Blog&lt;/a&gt;. We &lt;a href="https://huggingface.co/collections/unsloth/gemma-3n-685d3874830e49e1c93f9339"&gt;uploaded GGUFs, 4-bit models&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;📣 &lt;strong&gt;&lt;a href="https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning"&gt;Text-to-Speech (TTS)&lt;/a&gt;&lt;/strong&gt; is now supported, including &lt;code&gt;sesame/csm-1b&lt;/code&gt; and STT &lt;code&gt;openai/whisper-large-v3&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;📣 &lt;strong&gt;&lt;a href="https://docs.unsloth.ai/basics/qwen3-how-to-run-and-fine-tune"&gt;Qwen3&lt;/a&gt;&lt;/strong&gt; is now supported. Qwen3-30B-A3B fits on 17.5GB VRAM.&lt;/li&gt; 
 &lt;li&gt;📣 Introducing &lt;strong&gt;&lt;a href="https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs"&gt;Dynamic 2.0&lt;/a&gt;&lt;/strong&gt; quants that set new benchmarks on 5-shot MMLU &amp;amp; KL Divergence.&lt;/li&gt; 
 &lt;li&gt;📣 &lt;a href="https://unsloth.ai/blog/gemma3#everything"&gt;&lt;strong&gt;EVERYTHING&lt;/strong&gt; is now supported&lt;/a&gt; - all models (BERT, diffusion, Cohere, Mamba), FFT, etc. MultiGPU coming soon. Enable FFT with &lt;code&gt;full_finetuning = True&lt;/code&gt;, 8-bit with &lt;code&gt;load_in_8bit = True&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click for more news&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;📣 &lt;a href="https://unsloth.ai/blog/deepseek-r1"&gt;DeepSeek-R1&lt;/a&gt; - run or fine-tune them &lt;a href="https://unsloth.ai/blog/deepseek-r1"&gt;with our guide&lt;/a&gt;. All model uploads: &lt;a href="https://huggingface.co/collections/unsloth/deepseek-r1-all-versions-678e1c48f5d2fce87892ace5"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;📣 Introducing Long-context &lt;a href="https://unsloth.ai/blog/grpo"&gt;Reasoning (GRPO)&lt;/a&gt; in Unsloth. Train your own reasoning model with just 5GB VRAM. Transform Llama, Phi, Mistral etc. into reasoning LLMs!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;📣 Introducing Unsloth &lt;a href="https://unsloth.ai/blog/dynamic-4bit"&gt;Dynamic 4-bit Quantization&lt;/a&gt;! We dynamically opt not to quantize certain parameters and this greatly increases accuracy while only using &amp;lt;10% more VRAM than BnB 4-bit. See our collection on &lt;a href="https://huggingface.co/collections/unsloth/unsloth-4-bit-dynamic-quants-67503bb873f89e15276c44e7"&gt;Hugging Face here.&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;📣 &lt;strong&gt;&lt;a href="https://unsloth.ai/blog/llama4"&gt;Llama 4&lt;/a&gt;&lt;/strong&gt; by Meta, including Scout &amp;amp; Maverick are now supported.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;📣 &lt;a href="https://unsloth.ai/blog/phi4"&gt;Phi-4&lt;/a&gt; by Microsoft: We also &lt;a href="https://unsloth.ai/blog/phi4"&gt;fixed bugs&lt;/a&gt; in Phi-4 and &lt;a href="https://huggingface.co/collections/unsloth/phi-4-all-versions-677eecf93784e61afe762afa"&gt;uploaded GGUFs, 4-bit&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;📣 &lt;a href="https://unsloth.ai/blog/vision"&gt;Vision models&lt;/a&gt; now supported! &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb"&gt;Llama 3.2 Vision (11B)&lt;/a&gt;, &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2_VL_(7B)-Vision.ipynb"&gt;Qwen 2.5 VL (7B)&lt;/a&gt; and &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Pixtral_(12B)-Vision.ipynb"&gt;Pixtral (12B) 2409&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;📣 &lt;a href="https://huggingface.co/collections/unsloth/llama-33-all-versions-67535d7d994794b9d7cf5e9f"&gt;Llama 3.3 (70B)&lt;/a&gt;, Meta's latest model is supported.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;📣 We worked with Apple to add &lt;a href="https://arxiv.org/abs/2411.09009"&gt;Cut Cross Entropy&lt;/a&gt;. Unsloth now supports 89K context for Meta's Llama 3.3 (70B) on a 80GB GPU - 13x longer than HF+FA2. For Llama 3.1 (8B), Unsloth enables 342K context, surpassing its native 128K support.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;📣 We found and helped fix a &lt;a href="https://unsloth.ai/blog/gradient"&gt;gradient accumulation bug&lt;/a&gt;! Please update Unsloth and transformers.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;📣 We cut memory usage by a &lt;a href="https://unsloth.ai/blog/long-context"&gt;further 30%&lt;/a&gt; and now support &lt;a href="https://unsloth.ai/blog/long-context"&gt;4x longer context windows&lt;/a&gt;!&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;🔗 Links and Resources&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Links&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;📚 &lt;strong&gt;Documentation &amp;amp; Wiki&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.unsloth.ai"&gt;Read Our Docs&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img width="16" src="https://upload.wikimedia.org/wikipedia/commons/6/6f/Logo_of_Twitter.svg?sanitize=true" /&gt;&amp;nbsp; &lt;strong&gt;Twitter (aka X)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://twitter.com/unslothai"&gt;Follow us on X&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;💾 &lt;strong&gt;Installation&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.unsloth.ai/get-started/installing-+-updating"&gt;Pip install&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;🔮 &lt;strong&gt;Our Models&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.unsloth.ai/get-started/all-our-models"&gt;Unsloth Releases&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;✍️ &lt;strong&gt;Blog&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://unsloth.ai/blog"&gt;Read our Blogs&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img width="15" src="https://redditinc.com/hs-fs/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png" /&gt;&amp;nbsp; &lt;strong&gt;Reddit&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://reddit.com/r/unsloth"&gt;Join our Reddit&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;⭐ Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Supports &lt;strong&gt;full-finetuning&lt;/strong&gt;, pretraining, 4b-bit, 16-bit and &lt;strong&gt;8-bit&lt;/strong&gt; training&lt;/li&gt; 
 &lt;li&gt;Supports &lt;strong&gt;all transformer-style models&lt;/strong&gt; including &lt;a href="https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning"&gt;TTS, STT&lt;/a&gt;, multimodal, diffusion, &lt;a href="https://docs.unsloth.ai/get-started/unsloth-notebooks#other-important-notebooks"&gt;BERT&lt;/a&gt; and more!&lt;/li&gt; 
 &lt;li&gt;All kernels written in &lt;a href="https://openai.com/index/triton/"&gt;OpenAI's Triton&lt;/a&gt; language. &lt;strong&gt;Manual backprop engine&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;0% loss in accuracy&lt;/strong&gt; - no approximation methods - all exact.&lt;/li&gt; 
 &lt;li&gt;No change of hardware. Supports NVIDIA GPUs since 2018+. Minimum CUDA Capability 7.0 (V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc) &lt;a href="https://developer.nvidia.com/cuda-gpus"&gt;Check your GPU!&lt;/a&gt; GTX 1070, 1080 works, but is slow.&lt;/li&gt; 
 &lt;li&gt;Works on &lt;strong&gt;Linux&lt;/strong&gt; and &lt;strong&gt;Windows&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;If you trained a model with 🦥Unsloth, you can use this cool sticker! &amp;nbsp; &lt;img src="https://raw.githubusercontent.com/unslothai/unsloth/main/images/made with unsloth.png" width="200" align="center" /&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;💾 Install Unsloth&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!warning] Python 3.14 does not support Unsloth. Use 3.13 or lower.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;You can also see our documentation for more detailed installation and updating instructions &lt;a href="https://docs.unsloth.ai/get-started/installing-+-updating"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Pip Installation&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Install with pip (recommended) for Linux devices:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install unsloth
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;To update Unsloth:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install --upgrade --force-reinstall --no-cache-dir unsloth unsloth_zoo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href="https://github.com/unslothai/unsloth/edit/main/README.md#advanced-pip-installation"&gt;here&lt;/a&gt; for advanced pip install instructions.&lt;/p&gt; 
&lt;h3&gt;Windows Installation&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install NVIDIA Video Driver:&lt;/strong&gt; You should install the latest version of your GPUs driver. Download drivers here: &lt;a href="https://www.nvidia.com/Download/index.aspx"&gt;NVIDIA GPU Drive&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install Visual Studio C++:&lt;/strong&gt; You will need Visual Studio, with C++ installed. By default, C++ is not installed with &lt;a href="https://visualstudio.microsoft.com/vs/community/"&gt;Visual Studio&lt;/a&gt;, so make sure you select all of the C++ options. Also select options for Windows 10/11 SDK. For detailed instructions with options, see &lt;a href="https://docs.unsloth.ai/get-started/installing-+-updating"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install CUDA Toolkit:&lt;/strong&gt; Follow the instructions to install &lt;a href="https://developer.nvidia.com/cuda-toolkit-archive"&gt;CUDA Toolkit&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install PyTorch:&lt;/strong&gt; You will need the correct version of PyTorch that is compatible with your CUDA drivers, so make sure to select them carefully. &lt;a href="https://pytorch.org/get-started/locally/"&gt;Install PyTorch&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install Unsloth:&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;pip install unsloth
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Notes&lt;/h4&gt; 
&lt;p&gt;To run Unsloth directly on Windows:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Install Triton from this Windows fork and follow the instructions &lt;a href="https://github.com/woct0rdho/triton-windows"&gt;here&lt;/a&gt; (be aware that the Windows fork requires PyTorch &amp;gt;= 2.4 and CUDA 12)&lt;/li&gt; 
 &lt;li&gt;In the &lt;code&gt;SFTConfig&lt;/code&gt;, set &lt;code&gt;dataset_num_proc=1&lt;/code&gt; to avoid a crashing issue:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;SFTConfig(
    dataset_num_proc=1,
    ...
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Advanced/Troubleshooting&lt;/h4&gt; 
&lt;p&gt;For &lt;strong&gt;advanced installation instructions&lt;/strong&gt; or if you see weird errors during installations:&lt;/p&gt; 
&lt;p&gt;First try using an isolated environment via then &lt;code&gt;pip install unsloth&lt;/code&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m venv unsloth
source unsloth/bin/activate
pip install unsloth
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install &lt;code&gt;torch&lt;/code&gt; and &lt;code&gt;triton&lt;/code&gt;. Go to &lt;a href="https://pytorch.org"&gt;https://pytorch.org&lt;/a&gt; to install it. For example &lt;code&gt;pip install torch torchvision torchaudio triton&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Confirm if CUDA is installed correctly. Try &lt;code&gt;nvcc&lt;/code&gt;. If that fails, you need to install &lt;code&gt;cudatoolkit&lt;/code&gt; or CUDA drivers.&lt;/li&gt; 
 &lt;li&gt;Install &lt;code&gt;xformers&lt;/code&gt; manually via:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install ninja
pip install -v --no-build-isolation -U git+https://github.com/facebookresearch/xformers.git@main#egg=xformers
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code&gt;Check if `xformers` succeeded with `python -m xformers.info` Go to https://github.com/facebookresearch/xformers. Another option is to install `flash-attn` for Ampere GPUs and ignore `xformers`
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="5"&gt; 
 &lt;li&gt;For GRPO runs, you can try installing &lt;code&gt;vllm&lt;/code&gt; and seeing if &lt;code&gt;pip install vllm&lt;/code&gt; succeeds.&lt;/li&gt; 
 &lt;li&gt;Double check that your versions of Python, CUDA, CUDNN, &lt;code&gt;torch&lt;/code&gt;, &lt;code&gt;triton&lt;/code&gt;, and &lt;code&gt;xformers&lt;/code&gt; are compatible with one another. The &lt;a href="https://github.com/pytorch/pytorch/raw/main/RELEASE.md#release-compatibility-matrix"&gt;PyTorch Compatibility Matrix&lt;/a&gt; may be useful.&lt;/li&gt; 
 &lt;li&gt;Finally, install &lt;code&gt;bitsandbytes&lt;/code&gt; and check it with &lt;code&gt;python -m bitsandbytes&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Conda Installation (Optional)&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;⚠️Only use Conda if you have it. If not, use Pip&lt;/code&gt;. Select either &lt;code&gt;pytorch-cuda=11.8,12.1&lt;/code&gt; for CUDA 11.8 or CUDA 12.1. We support &lt;code&gt;python=3.10,3.11,3.12&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;conda create --name unsloth_env \
    python=3.11 \
    pytorch-cuda=12.1 \
    pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers \
    -y
conda activate unsloth_env

pip install unsloth
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;If you're looking to install Conda in a Linux environment, &lt;a href="https://docs.anaconda.com/miniconda/"&gt;read here&lt;/a&gt;, or run the below 🔽&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;mkdir -p ~/miniconda3
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh
bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3
rm -rf ~/miniconda3/miniconda.sh
~/miniconda3/bin/conda init bash
~/miniconda3/bin/conda init zsh
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Advanced Pip Installation&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;⚠️Do **NOT** use this if you have Conda.&lt;/code&gt; Pip is a bit more complex since there are dependency issues. The pip command is different for &lt;code&gt;torch 2.2,2.3,2.4,2.5&lt;/code&gt; and CUDA versions.&lt;/p&gt; 
&lt;p&gt;For other torch versions, we support &lt;code&gt;torch211&lt;/code&gt;, &lt;code&gt;torch212&lt;/code&gt;, &lt;code&gt;torch220&lt;/code&gt;, &lt;code&gt;torch230&lt;/code&gt;, &lt;code&gt;torch240&lt;/code&gt; and for CUDA versions, we support &lt;code&gt;cu118&lt;/code&gt; and &lt;code&gt;cu121&lt;/code&gt; and &lt;code&gt;cu124&lt;/code&gt;. For Ampere devices (A100, H100, RTX3090) and above, use &lt;code&gt;cu118-ampere&lt;/code&gt; or &lt;code&gt;cu121-ampere&lt;/code&gt; or &lt;code&gt;cu124-ampere&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;For example, if you have &lt;code&gt;torch 2.4&lt;/code&gt; and &lt;code&gt;CUDA 12.1&lt;/code&gt;, use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install --upgrade pip
pip install "unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Another example, if you have &lt;code&gt;torch 2.5&lt;/code&gt; and &lt;code&gt;CUDA 12.4&lt;/code&gt;, use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install --upgrade pip
pip install "unsloth[cu124-torch250] @ git+https://github.com/unslothai/unsloth.git"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;And other examples:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install "unsloth[cu121-ampere-torch240] @ git+https://github.com/unslothai/unsloth.git"
pip install "unsloth[cu118-ampere-torch240] @ git+https://github.com/unslothai/unsloth.git"
pip install "unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git"
pip install "unsloth[cu118-torch240] @ git+https://github.com/unslothai/unsloth.git"

pip install "unsloth[cu121-torch230] @ git+https://github.com/unslothai/unsloth.git"
pip install "unsloth[cu121-ampere-torch230] @ git+https://github.com/unslothai/unsloth.git"

pip install "unsloth[cu121-torch250] @ git+https://github.com/unslothai/unsloth.git"
pip install "unsloth[cu124-ampere-torch250] @ git+https://github.com/unslothai/unsloth.git"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or, run the below in a terminal to get the &lt;strong&gt;optimal&lt;/strong&gt; pip installation command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;wget -qO- https://raw.githubusercontent.com/unslothai/unsloth/main/unsloth/_auto_install.py | python -
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or, run the below manually in a Python REPL:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;try: import torch
except: raise ImportError('Install torch via `pip install torch`')
from packaging.version import Version as V
import re
v = V(re.match(r"[0-9\.]{3,}", torch.__version__).group(0))
cuda = str(torch.version.cuda)
is_ampere = torch.cuda.get_device_capability()[0] &amp;gt;= 8
USE_ABI = torch._C._GLIBCXX_USE_CXX11_ABI
if cuda not in ("11.8", "12.1", "12.4", "12.6", "12.8"): raise RuntimeError(f"CUDA = {cuda} not supported!")
if   v &amp;lt;= V('2.1.0'): raise RuntimeError(f"Torch = {v} too old!")
elif v &amp;lt;= V('2.1.1'): x = 'cu{}{}-torch211'
elif v &amp;lt;= V('2.1.2'): x = 'cu{}{}-torch212'
elif v  &amp;lt; V('2.3.0'): x = 'cu{}{}-torch220'
elif v  &amp;lt; V('2.4.0'): x = 'cu{}{}-torch230'
elif v  &amp;lt; V('2.5.0'): x = 'cu{}{}-torch240'
elif v  &amp;lt; V('2.5.1'): x = 'cu{}{}-torch250'
elif v &amp;lt;= V('2.5.1'): x = 'cu{}{}-torch251'
elif v  &amp;lt; V('2.7.0'): x = 'cu{}{}-torch260'
elif v  &amp;lt; V('2.7.9'): x = 'cu{}{}-torch270'
elif v  &amp;lt; V('2.8.0'): x = 'cu{}{}-torch271'
elif v  &amp;lt; V('2.8.9'): x = 'cu{}{}-torch280'
else: raise RuntimeError(f"Torch = {v} too new!")
if v &amp;gt; V('2.6.9') and cuda not in ("11.8", "12.6", "12.8"): raise RuntimeError(f"CUDA = {cuda} not supported!")
x = x.format(cuda.replace(".", ""), "-ampere" if is_ampere else "")
print(f'pip install --upgrade pip &amp;amp;&amp;amp; pip install "unsloth[{x}] @ git+https://github.com/unslothai/unsloth.git"')
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Docker Installation&lt;/h3&gt; 
&lt;p&gt;You can use our pre-built Docker container with all dependencies to use Unsloth instantly with no setup required. &lt;a href="https://docs.unsloth.ai/get-started/install-and-update/docker"&gt;Read our guide&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;This container requires installing &lt;a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html"&gt;NVIDIA's Container Toolkit&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -d -e JUPYTER_PASSWORD="mypassword" \
  -p 8888:8888 -p 2222:22 \
  -v $(pwd)/work:/workspace/work \
  --gpus all \
  unsloth/unsloth
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Access Jupyter Lab at &lt;code&gt;http://localhost:8888&lt;/code&gt; and start fine-tuning!&lt;/p&gt; 
&lt;h2&gt;📜 Documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Go to our official &lt;a href="https://docs.unsloth.ai"&gt;Documentation&lt;/a&gt; for saving to GGUF, checkpointing, evaluation and more!&lt;/li&gt; 
 &lt;li&gt;We support Huggingface's TRL, Trainer, Seq2SeqTrainer or even Pytorch code!&lt;/li&gt; 
 &lt;li&gt;We're in 🤗Hugging Face's official docs! Check out the &lt;a href="https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth"&gt;SFT docs&lt;/a&gt; and &lt;a href="https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth"&gt;DPO docs&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;If you want to download models from the ModelScope community, please use an environment variable: &lt;code&gt;UNSLOTH_USE_MODELSCOPE=1&lt;/code&gt;, and install the modelscope library by: &lt;code&gt;pip install modelscope -U&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;unsloth_cli.py also supports &lt;code&gt;UNSLOTH_USE_MODELSCOPE=1&lt;/code&gt; to download models and datasets. please remember to use the model and dataset id in the ModelScope community.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from unsloth import FastLanguageModel, FastModel
import torch
from trl import SFTTrainer, SFTConfig
from datasets import load_dataset
max_seq_length = 2048 # Supports RoPE Scaling internally, so choose any!
# Get LAION dataset
url = "https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl"
dataset = load_dataset("json", data_files = {"train" : url}, split = "train")

# 4bit pre quantized models we support for 4x faster downloading + no OOMs.
fourbit_models = [
    "unsloth/Meta-Llama-3.1-8B-bnb-4bit",      # Llama-3.1 2x faster
    "unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit",
    "unsloth/Meta-Llama-3.1-70B-bnb-4bit",
    "unsloth/Meta-Llama-3.1-405B-bnb-4bit",    # 4bit for 405b!
    "unsloth/Mistral-Small-Instruct-2409",     # Mistral 22b 2x faster!
    "unsloth/mistral-7b-instruct-v0.3-bnb-4bit",
    "unsloth/Phi-3.5-mini-instruct",           # Phi-3.5 2x faster!
    "unsloth/Phi-3-medium-4k-instruct",
    "unsloth/gemma-2-9b-bnb-4bit",
    "unsloth/gemma-2-27b-bnb-4bit",            # Gemma 2x faster!

    "unsloth/Llama-3.2-1B-bnb-4bit",           # NEW! Llama 3.2 models
    "unsloth/Llama-3.2-1B-Instruct-bnb-4bit",
    "unsloth/Llama-3.2-3B-bnb-4bit",
    "unsloth/Llama-3.2-3B-Instruct-bnb-4bit",

    "unsloth/Llama-3.3-70B-Instruct-bnb-4bit" # NEW! Llama 3.3 70B!
] # More models at https://huggingface.co/unsloth

model, tokenizer = FastModel.from_pretrained(
    model_name = "unsloth/gemma-3-4B-it",
    max_seq_length = 2048, # Choose any for long context!
    load_in_4bit = True,  # 4 bit quantization to reduce memory
    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory
    full_finetuning = False, # [NEW!] We have full finetuning now!
    # token = "hf_...", # use one if using gated models
)

# Do model patching and add fast LoRA weights
model = FastLanguageModel.get_peft_model(
    model,
    r = 16,
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 16,
    lora_dropout = 0, # Supports any, but = 0 is optimized
    bias = "none",    # Supports any, but = "none" is optimized
    # [NEW] "unsloth" uses 30% less VRAM, fits 2x larger batch sizes!
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
    random_state = 3407,
    max_seq_length = max_seq_length,
    use_rslora = False,  # We support rank stabilized LoRA
    loftq_config = None, # And LoftQ
)

trainer = SFTTrainer(
    model = model,
    train_dataset = dataset,
    tokenizer = tokenizer,
    args = SFTConfig(
        max_seq_length = max_seq_length,
        per_device_train_batch_size = 2,
        gradient_accumulation_steps = 4,
        warmup_steps = 10,
        max_steps = 60,
        logging_steps = 1,
        output_dir = "outputs",
        optim = "adamw_8bit",
        seed = 3407,
    ),
)
trainer.train()

# Go to https://github.com/unslothai/unsloth/wiki for advanced tips like
# (1) Saving to GGUF / merging to 16bit for vLLM
# (2) Continued training from a saved LoRA adapter
# (3) Adding an evaluation loop / OOMs
# (4) Customized chat templates
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a name="RL"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;💡 Reinforcement Learning&lt;/h2&gt; 
&lt;p&gt;RL including DPO, GRPO, PPO, Reward Modelling, Online DPO all work with Unsloth. We're in 🤗Hugging Face's official docs! We're on the &lt;a href="https://huggingface.co/learn/nlp-course/en/chapter12/6"&gt;GRPO docs&lt;/a&gt; and the &lt;a href="https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth"&gt;DPO docs&lt;/a&gt;! List of RL notebooks:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Advanced Qwen3 GRPO notebook: &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(4B)-GRPO.ipynb"&gt;Link&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ORPO notebook: &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-ORPO.ipynb"&gt;Link&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;DPO Zephyr notebook: &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Zephyr_(7B)-DPO.ipynb"&gt;Link&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;KTO notebook: &lt;a href="https://colab.research.google.com/drive/1MRgGtLWuZX4ypSfGguFgC-IblTvO2ivM?usp=sharing"&gt;Link&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;SimPO notebook: &lt;a href="https://colab.research.google.com/drive/1Hs5oQDovOay4mFA6Y9lQhVJ8TnbFLFh2?usp=sharing"&gt;Link&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click for DPO code&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import os
os.environ["CUDA_VISIBLE_DEVICES"] = "0" # Optional set GPU device ID

from unsloth import FastLanguageModel
import torch
from trl import DPOTrainer, DPOConfig
max_seq_length = 2048

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/zephyr-sft-bnb-4bit",
    max_seq_length = max_seq_length,
    load_in_4bit = True,
)

# Do model patching and add fast LoRA weights
model = FastLanguageModel.get_peft_model(
    model,
    r = 64,
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 64,
    lora_dropout = 0, # Supports any, but = 0 is optimized
    bias = "none",    # Supports any, but = "none" is optimized
    # [NEW] "unsloth" uses 30% less VRAM, fits 2x larger batch sizes!
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
    random_state = 3407,
    max_seq_length = max_seq_length,
)

dpo_trainer = DPOTrainer(
    model = model,
    ref_model = None,
    train_dataset = YOUR_DATASET_HERE,
    # eval_dataset = YOUR_DATASET_HERE,
    tokenizer = tokenizer,
    args = DPOConfig(
        per_device_train_batch_size = 4,
        gradient_accumulation_steps = 8,
        warmup_ratio = 0.1,
        num_train_epochs = 3,
        logging_steps = 1,
        optim = "adamw_8bit",
        seed = 42,
        output_dir = "outputs",
        max_length = 1024,
        max_prompt_length = 512,
        beta = 0.1,
    ),
)
dpo_trainer.train()
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;🥇 Performance Benchmarking&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;For our most detailed benchmarks, read our &lt;a href="https://unsloth.ai/blog/llama3-3"&gt;Llama 3.3 Blog&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Benchmarking of Unsloth was also conducted by &lt;a href="https://huggingface.co/blog/unsloth-trl"&gt;🤗Hugging Face&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We tested using the Alpaca Dataset, a batch size of 2, gradient accumulation steps of 4, rank = 32, and applied QLoRA on all linear layers (q, k, v, o, gate, up, down):&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;VRAM&lt;/th&gt; 
   &lt;th&gt;🦥 Unsloth speed&lt;/th&gt; 
   &lt;th&gt;🦥 VRAM reduction&lt;/th&gt; 
   &lt;th&gt;🦥 Longer context&lt;/th&gt; 
   &lt;th&gt;😊 Hugging Face + FA2&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Llama 3.3 (70B)&lt;/td&gt; 
   &lt;td&gt;80GB&lt;/td&gt; 
   &lt;td&gt;2x&lt;/td&gt; 
   &lt;td&gt;&amp;gt;75%&lt;/td&gt; 
   &lt;td&gt;13x longer&lt;/td&gt; 
   &lt;td&gt;1x&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Llama 3.1 (8B)&lt;/td&gt; 
   &lt;td&gt;80GB&lt;/td&gt; 
   &lt;td&gt;2x&lt;/td&gt; 
   &lt;td&gt;&amp;gt;70%&lt;/td&gt; 
   &lt;td&gt;12x longer&lt;/td&gt; 
   &lt;td&gt;1x&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Context length benchmarks&lt;/h3&gt; 
&lt;h4&gt;Llama 3.1 (8B) max. context length&lt;/h4&gt; 
&lt;p&gt;We tested Llama 3.1 (8B) Instruct and did 4bit QLoRA on all linear layers (Q, K, V, O, gate, up and down) with rank = 32 with a batch size of 1. We padded all sequences to a certain maximum sequence length to mimic long context finetuning workloads.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;GPU VRAM&lt;/th&gt; 
   &lt;th&gt;🦥Unsloth context length&lt;/th&gt; 
   &lt;th&gt;Hugging Face + FA2&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;8 GB&lt;/td&gt; 
   &lt;td&gt;2,972&lt;/td&gt; 
   &lt;td&gt;OOM&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;12 GB&lt;/td&gt; 
   &lt;td&gt;21,848&lt;/td&gt; 
   &lt;td&gt;932&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;16 GB&lt;/td&gt; 
   &lt;td&gt;40,724&lt;/td&gt; 
   &lt;td&gt;2,551&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;24 GB&lt;/td&gt; 
   &lt;td&gt;78,475&lt;/td&gt; 
   &lt;td&gt;5,789&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;40 GB&lt;/td&gt; 
   &lt;td&gt;153,977&lt;/td&gt; 
   &lt;td&gt;12,264&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;48 GB&lt;/td&gt; 
   &lt;td&gt;191,728&lt;/td&gt; 
   &lt;td&gt;15,502&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;80 GB&lt;/td&gt; 
   &lt;td&gt;342,733&lt;/td&gt; 
   &lt;td&gt;28,454&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h4&gt;Llama 3.3 (70B) max. context length&lt;/h4&gt; 
&lt;p&gt;We tested Llama 3.3 (70B) Instruct on a 80GB A100 and did 4bit QLoRA on all linear layers (Q, K, V, O, gate, up and down) with rank = 32 with a batch size of 1. We padded all sequences to a certain maximum sequence length to mimic long context finetuning workloads.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;GPU VRAM&lt;/th&gt; 
   &lt;th&gt;🦥Unsloth context length&lt;/th&gt; 
   &lt;th&gt;Hugging Face + FA2&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;48 GB&lt;/td&gt; 
   &lt;td&gt;12,106&lt;/td&gt; 
   &lt;td&gt;OOM&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;80 GB&lt;/td&gt; 
   &lt;td&gt;89,389&lt;/td&gt; 
   &lt;td&gt;6,916&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://i.ibb.co/sJ7RhGG/image-41.png" alt="" /&gt; &lt;br /&gt;&lt;/p&gt; 
&lt;h3&gt;Citation&lt;/h3&gt; 
&lt;p&gt;You can cite the Unsloth repo as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@software{unsloth,
  author = {Daniel Han, Michael Han and Unsloth team},
  title = {Unsloth},
  url = {http://github.com/unslothai/unsloth},
  year = {2023}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Thank You to&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;The &lt;a href="https://github.com/ggml-org/llama.cpp"&gt;llama.cpp library&lt;/a&gt; that lets users save models with Unsloth&lt;/li&gt; 
 &lt;li&gt;The Hugging Face team and their &lt;a href="https://github.com/huggingface/trl"&gt;TRL library&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/erikwijmans"&gt;Erik&lt;/a&gt; for his help adding &lt;a href="https://github.com/apple/ml-cross-entropy"&gt;Apple's ML Cross Entropy&lt;/a&gt; in Unsloth&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Etherll"&gt;Etherl&lt;/a&gt; for adding support for &lt;a href="https://github.com/unslothai/notebooks/pull/34"&gt;TTS, diffusion and BERT models&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;And of course for every single person who has contributed or has used Unsloth!&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
  </channel>
</rss>