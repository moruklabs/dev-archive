<rss version="2.0">
  <channel>
    <title>GitHub All Languages Weekly Trending</title>
    <description>Weekly Trending of All Languages in GitHub</description>
    <pubDate>Thu, 18 Sep 2025 01:42:06 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>ItzCrazyKns/Perplexica</title>
      <link>https://github.com/ItzCrazyKns/Perplexica</link>
      <description>&lt;p&gt;Perplexica is an AI-powered search engine. It is an Open source alternative to Perplexity AI&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ðŸš€ Perplexica - An AI-powered search engine ðŸ”Ž 
 &lt;!-- omit in toc --&gt;&lt;/h1&gt; 
&lt;div align="center" markdown="1"&gt; 
 &lt;sup&gt;Special thanks to:&lt;/sup&gt; 
 &lt;br /&gt; 
 &lt;br /&gt; 
 &lt;a href="https://www.warp.dev/perplexica"&gt; &lt;img alt="Warp sponsorship" width="400" src="https://github.com/user-attachments/assets/775dd593-9b5f-40f1-bf48-479faff4c27b" /&gt; &lt;/a&gt; 
 &lt;h3&gt;&lt;a href="https://www.warp.dev/perplexica"&gt;Warp, the AI Devtool that lives in your terminal&lt;/a&gt;&lt;/h3&gt; 
 &lt;p&gt;&lt;a href="https://www.warp.dev/perplexica"&gt;Available for MacOS, Linux, &amp;amp; Windows&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;a href="https://discord.gg/26aArMy8tT"&gt;&lt;img src="https://dcbadge.limes.pink/api/server/26aArMy8tT?style=flat" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/.assets/perplexica-screenshot.png?" alt="preview" /&gt;&lt;/p&gt; 
&lt;h2&gt;Table of Contents 
 &lt;!-- omit in toc --&gt;&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#overview"&gt;Overview&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#preview"&gt;Preview&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#features"&gt;Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#installation"&gt;Installation&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#getting-started-with-docker-recommended"&gt;Getting Started with Docker (Recommended)&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#non-docker-installation"&gt;Non-Docker Installation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#ollama-connection-errors"&gt;Ollama Connection Errors&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#using-as-a-search-engine"&gt;Using as a Search Engine&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#using-perplexicas-api"&gt;Using Perplexica's API&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#expose-perplexica-to-network"&gt;Expose Perplexica to a network&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#one-click-deployment"&gt;One-Click Deployment&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#upcoming-features"&gt;Upcoming Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#support-us"&gt;Support Us&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#donations"&gt;Donations&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#contribution"&gt;Contribution&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#help-and-support"&gt;Help and Support&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Overview&lt;/h2&gt; 
&lt;p&gt;Perplexica is an open-source AI-powered searching tool or an AI-powered search engine that goes deep into the internet to find answers. Inspired by Perplexity AI, it's an open-source option that not just searches the web but understands your questions. It uses advanced machine learning algorithms like similarity searching and embeddings to refine results and provides clear answers with sources cited.&lt;/p&gt; 
&lt;p&gt;Using SearxNG to stay current and fully open source, Perplexica ensures you always get the most up-to-date information without compromising your privacy.&lt;/p&gt; 
&lt;p&gt;Want to know more about its architecture and how it works? You can read it &lt;a href="https://github.com/ItzCrazyKns/Perplexica/tree/master/docs/architecture/README.md"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Preview&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/.assets/perplexica-preview.gif" alt="video-preview" /&gt;&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Local LLMs&lt;/strong&gt;: You can utilize local LLMs such as Qwen, DeepSeek, Llama, and Mistral.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Two Main Modes:&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Copilot Mode:&lt;/strong&gt; (In development) Boosts search by generating different queries to find more relevant internet sources. Like normal search instead of just using the context by SearxNG, it visits the top matches and tries to find relevant sources to the user's query directly from the page.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Normal Mode:&lt;/strong&gt; Processes your query and performs a web search.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Focus Modes:&lt;/strong&gt; Special modes to better answer specific types of questions. Perplexica currently has 6 focus modes: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;All Mode:&lt;/strong&gt; Searches the entire web to find the best results.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Writing Assistant Mode:&lt;/strong&gt; Helpful for writing tasks that do not require searching the web.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Academic Search Mode:&lt;/strong&gt; Finds articles and papers, ideal for academic research.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;YouTube Search Mode:&lt;/strong&gt; Finds YouTube videos based on the search query.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Wolfram Alpha Search Mode:&lt;/strong&gt; Answers queries that need calculations or data analysis using Wolfram Alpha.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Reddit Search Mode:&lt;/strong&gt; Searches Reddit for discussions and opinions related to the query.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Current Information:&lt;/strong&gt; Some search tools might give you outdated info because they use data from crawling bots and convert them into embeddings and store them in a index. Unlike them, Perplexica uses SearxNG, a metasearch engine to get the results and rerank and get the most relevant source out of it, ensuring you always get the latest information without the overhead of daily data updates.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;API&lt;/strong&gt;: Integrate Perplexica into your existing applications and make use of its capibilities.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;It has many more features like image and video search. Some of the planned features are mentioned in &lt;a href="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#upcoming-features"&gt;upcoming features&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;There are mainly 2 ways of installing Perplexica - With Docker, Without Docker. Using Docker is highly recommended.&lt;/p&gt; 
&lt;h3&gt;Getting Started with Docker (Recommended)&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Ensure Docker is installed and running on your system.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Clone the Perplexica repository:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/ItzCrazyKns/Perplexica.git
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;After cloning, navigate to the directory containing the project files.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Rename the &lt;code&gt;sample.config.toml&lt;/code&gt; file to &lt;code&gt;config.toml&lt;/code&gt;. For Docker setups, you need only fill in the following fields:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;&lt;code&gt;OPENAI&lt;/code&gt;: Your OpenAI API key. &lt;strong&gt;You only need to fill this if you wish to use OpenAI's models&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;code&gt;CUSTOM_OPENAI&lt;/code&gt;: Your OpenAI-API-compliant local server URL, model name, and API key. You should run your local server with host set to &lt;code&gt;0.0.0.0&lt;/code&gt;, take note of which port number it is running on, and then use that port number to set &lt;code&gt;API_URL = http://host.docker.internal:PORT_NUMBER&lt;/code&gt;. You must specify the model name, such as &lt;code&gt;MODEL_NAME = "unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF:Q4_K_XL"&lt;/code&gt;. Finally, set &lt;code&gt;API_KEY&lt;/code&gt; to the appropriate value. If you have not defined an API key, just put anything you want in-between the quotation marks: &lt;code&gt;API_KEY = "whatever-you-want-but-not-blank"&lt;/code&gt; &lt;strong&gt;You only need to configure these settings if you want to use a local OpenAI-compliant server, such as Llama.cpp's &lt;a href="https://github.com/ggml-org/llama.cpp/raw/master/tools/server/README.md"&gt;&lt;code&gt;llama-server&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;code&gt;OLLAMA&lt;/code&gt;: Your Ollama API URL. You should enter it as &lt;code&gt;http://host.docker.internal:PORT_NUMBER&lt;/code&gt;. If you installed Ollama on port 11434, use &lt;code&gt;http://host.docker.internal:11434&lt;/code&gt;. For other ports, adjust accordingly. &lt;strong&gt;You need to fill this if you wish to use Ollama's models instead of OpenAI's&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;code&gt;GROQ&lt;/code&gt;: Your Groq API key. &lt;strong&gt;You only need to fill this if you wish to use Groq's hosted models&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;code&gt;ANTHROPIC&lt;/code&gt;: Your Anthropic API key. &lt;strong&gt;You only need to fill this if you wish to use Anthropic models&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;code&gt;Gemini&lt;/code&gt;: Your Gemini API key. &lt;strong&gt;You only need to fill this if you wish to use Google's models&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;code&gt;DEEPSEEK&lt;/code&gt;: Your Deepseek API key. &lt;strong&gt;Only needed if you want Deepseek models.&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;code&gt;AIMLAPI&lt;/code&gt;: Your AI/ML API key. &lt;strong&gt;Only needed if you want to use AI/ML API models and embeddings.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: You can change these after starting Perplexica from the settings dialog.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;code&gt;SIMILARITY_MEASURE&lt;/code&gt;: The similarity measure to use (This is filled by default; you can leave it as is if you are unsure about it.)&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Ensure you are in the directory containing the &lt;code&gt;docker-compose.yaml&lt;/code&gt; file and execute:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;docker compose up -d
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Wait a few minutes for the setup to complete. You can access Perplexica at &lt;a href="http://localhost:3000"&gt;http://localhost:3000&lt;/a&gt; in your web browser.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: After the containers are built, you can start Perplexica directly from Docker without having to open a terminal.&lt;/p&gt; 
&lt;h3&gt;Non-Docker Installation&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install SearXNG and allow &lt;code&gt;JSON&lt;/code&gt; format in the SearXNG settings.&lt;/li&gt; 
 &lt;li&gt;Clone the repository and rename the &lt;code&gt;sample.config.toml&lt;/code&gt; file to &lt;code&gt;config.toml&lt;/code&gt; in the root directory. Ensure you complete all required fields in this file.&lt;/li&gt; 
 &lt;li&gt;After populating the configuration run &lt;code&gt;npm i&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Install the dependencies and then execute &lt;code&gt;npm run build&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Finally, start the app by running &lt;code&gt;npm run start&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Using Docker is recommended as it simplifies the setup process, especially for managing environment variables and dependencies.&lt;/p&gt; 
&lt;p&gt;See the &lt;a href="https://github.com/ItzCrazyKns/Perplexica/tree/master/docs/installation"&gt;installation documentation&lt;/a&gt; for more information like updating, etc.&lt;/p&gt; 
&lt;h3&gt;Troubleshooting&lt;/h3&gt; 
&lt;h4&gt;Local OpenAI-API-Compliant Servers&lt;/h4&gt; 
&lt;p&gt;If Perplexica tells you that you haven't configured any chat model providers, ensure that:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Your server is running on &lt;code&gt;0.0.0.0&lt;/code&gt; (not &lt;code&gt;127.0.0.1&lt;/code&gt;) and on the same port you put in the API URL.&lt;/li&gt; 
 &lt;li&gt;You have specified the correct model name loaded by your local LLM server.&lt;/li&gt; 
 &lt;li&gt;You have specified the correct API key, or if one is not defined, you have put &lt;em&gt;something&lt;/em&gt; in the API key field and not left it empty.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Ollama Connection Errors&lt;/h4&gt; 
&lt;p&gt;If you're encountering an Ollama connection error, it is likely due to the backend being unable to connect to Ollama's API. To fix this issue you can:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Check your Ollama API URL:&lt;/strong&gt; Ensure that the API URL is correctly set in the settings menu.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Update API URL Based on OS:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Windows:&lt;/strong&gt; Use &lt;code&gt;http://host.docker.internal:11434&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Mac:&lt;/strong&gt; Use &lt;code&gt;http://host.docker.internal:11434&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Linux:&lt;/strong&gt; Use &lt;code&gt;http://&amp;lt;private_ip_of_host&amp;gt;:11434&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;Adjust the port number if you're using a different one.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Linux Users - Expose Ollama to Network:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;Inside &lt;code&gt;/etc/systemd/system/ollama.service&lt;/code&gt;, you need to add &lt;code&gt;Environment="OLLAMA_HOST=0.0.0.0:11434"&lt;/code&gt;. (Change the port number if you are using a different one.) Then reload the systemd manager configuration with &lt;code&gt;systemctl daemon-reload&lt;/code&gt;, and restart Ollama by &lt;code&gt;systemctl restart ollama&lt;/code&gt;. For more information see &lt;a href="https://github.com/ollama/ollama/raw/main/docs/faq.md#setting-environment-variables-on-linux"&gt;Ollama docs&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Ensure that the port (default is 11434) is not blocked by your firewall.&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Using as a Search Engine&lt;/h2&gt; 
&lt;p&gt;If you wish to use Perplexica as an alternative to traditional search engines like Google or Bing, or if you want to add a shortcut for quick access from your browser's search bar, follow these steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Open your browser's settings.&lt;/li&gt; 
 &lt;li&gt;Navigate to the 'Search Engines' section.&lt;/li&gt; 
 &lt;li&gt;Add a new site search with the following URL: &lt;code&gt;http://localhost:3000/?q=%s&lt;/code&gt;. Replace &lt;code&gt;localhost&lt;/code&gt; with your IP address or domain name, and &lt;code&gt;3000&lt;/code&gt; with the port number if Perplexica is not hosted locally.&lt;/li&gt; 
 &lt;li&gt;Click the add button. Now, you can use Perplexica directly from your browser's search bar.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Using Perplexica's API&lt;/h2&gt; 
&lt;p&gt;Perplexica also provides an API for developers looking to integrate its powerful search engine into their own applications. You can run searches, use multiple models and get answers to your queries.&lt;/p&gt; 
&lt;p&gt;For more details, check out the full documentation &lt;a href="https://github.com/ItzCrazyKns/Perplexica/tree/master/docs/API/SEARCH.md"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Expose Perplexica to network&lt;/h2&gt; 
&lt;p&gt;Perplexica runs on Next.js and handles all API requests. It works right away on the same network and stays accessible even with port forwarding.&lt;/p&gt; 
&lt;h2&gt;One-Click Deployment&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://usw.sealos.io/?openapp=system-template%3FtemplateName%3Dperplexica"&gt;&lt;img src="https://raw.githubusercontent.com/labring-actions/templates/main/Deploy-on-Sealos.svg?sanitize=true" alt="Deploy to Sealos" /&gt;&lt;/a&gt; &lt;a href="https://repocloud.io/details/?app_id=267"&gt;&lt;img src="https://d16t0pc4846x52.cloudfront.net/deploylobe.svg?sanitize=true" alt="Deploy to RepoCloud" /&gt;&lt;/a&gt; &lt;a href="https://template.run.claw.cloud/?referralCode=U11MRQ8U9RM4&amp;amp;openapp=system-fastdeploy%3FtemplateName%3Dperplexica"&gt;&lt;img src="https://raw.githubusercontent.com/ClawCloud/Run-Template/refs/heads/main/Run-on-ClawCloud.svg?sanitize=true" alt="Run on ClawCloud" /&gt;&lt;/a&gt; &lt;a href="https://www.hostinger.com/vps/docker-hosting?compose_url=https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/refs/heads/master/docker-compose.yaml"&gt;&lt;img src="https://assets.hostinger.com/vps/deploy.svg?sanitize=true" alt="Deploy on Hostinger" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Upcoming Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Add settings page&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Adding support for local LLMs&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; History Saving features&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Introducing various Focus Modes&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Adding API support&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Adding Discover&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Finalizing Copilot Mode&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Support Us&lt;/h2&gt; 
&lt;p&gt;If you find Perplexica useful, consider giving us a star on GitHub. This helps more people discover Perplexica and supports the development of new features. Your support is greatly appreciated.&lt;/p&gt; 
&lt;h3&gt;Donations&lt;/h3&gt; 
&lt;p&gt;We also accept donations to help sustain our project. If you would like to contribute, you can use the following options to donate. Thank you for your support!&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Ethereum&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Address: &lt;code&gt;0xB025a84b2F269570Eb8D4b05DEdaA41D8525B6DD&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Contribution&lt;/h2&gt; 
&lt;p&gt;Perplexica is built on the idea that AI and large language models should be easy for everyone to use. If you find bugs or have ideas, please share them in via GitHub Issues. For more information on contributing to Perplexica you can read the &lt;a href="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; file to learn more about Perplexica and how you can contribute to it.&lt;/p&gt; 
&lt;h2&gt;Help and Support&lt;/h2&gt; 
&lt;p&gt;If you have any questions or feedback, please feel free to reach out to us. You can create an issue on GitHub or join our Discord server. There, you can connect with other users, share your experiences and reviews, and receive more personalized help. &lt;a href="https://discord.gg/EFwsmQDgAu"&gt;Click here&lt;/a&gt; to join the Discord server. To discuss matters outside of regular support, feel free to contact me on Discord at &lt;code&gt;itzcrazykns&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Thank you for exploring Perplexica, the AI-powered search engine designed to enhance your search experience. We are constantly working to improve Perplexica and expand its capabilities. We value your feedback and contributions which help us make Perplexica even better. Don't forget to check back for updates and new features!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>CorentinJ/Real-Time-Voice-Cloning</title>
      <link>https://github.com/CorentinJ/Real-Time-Voice-Cloning</link>
      <description>&lt;p&gt;Clone a voice in 5 seconds to generate arbitrary speech in real-time&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Real-Time Voice Cloning&lt;/h1&gt; 
&lt;p&gt;This repository is an implementation of &lt;a href="https://arxiv.org/pdf/1806.04558.pdf"&gt;Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis&lt;/a&gt; (SV2TTS) with a vocoder that works in real-time. This was my &lt;a href="https://matheo.uliege.be/handle/2268.2/6801"&gt;master's thesis&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;SV2TTS is a deep learning framework in three stages. In the first stage, one creates a digital representation of a voice from a few seconds of audio. In the second and third stages, this representation is used as reference to generate speech given arbitrary text.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Video demonstration&lt;/strong&gt; (click the picture):&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=-O_hYhToKoA"&gt;&lt;img src="https://i.imgur.com/8lFUlgz.png" alt="Toolbox demo" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Papers implemented&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;URL&lt;/th&gt; 
   &lt;th&gt;Designation&lt;/th&gt; 
   &lt;th&gt;Title&lt;/th&gt; 
   &lt;th&gt;Implementation source&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/1806.04558.pdf"&gt;&lt;strong&gt;1806.04558&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;SV2TTS&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;This repo&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/1802.08435.pdf"&gt;1802.08435&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;WaveRNN (vocoder)&lt;/td&gt; 
   &lt;td&gt;Efficient Neural Audio Synthesis&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/fatchord/WaveRNN"&gt;fatchord/WaveRNN&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/1703.10135.pdf"&gt;1703.10135&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Tacotron (synthesizer)&lt;/td&gt; 
   &lt;td&gt;Tacotron: Towards End-to-End Speech Synthesis&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/fatchord/WaveRNN"&gt;fatchord/WaveRNN&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/1710.10467.pdf"&gt;1710.10467&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;GE2E (encoder)&lt;/td&gt; 
   &lt;td&gt;Generalized End-To-End Loss for Speaker Verification&lt;/td&gt; 
   &lt;td&gt;This repo&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Heads up&lt;/h2&gt; 
&lt;p&gt;Like everything else in Deep Learning, this repo has quickly gotten old. Many SaaS apps (often paying) will give you a better audio quality than this repository will. If you wish for an open-source solution with a high voice quality:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Check out &lt;a href="https://paperswithcode.com/task/speech-synthesis/"&gt;paperswithcode&lt;/a&gt; for other repositories and recent research in the field of speech synthesis.&lt;/li&gt; 
 &lt;li&gt;Check out &lt;a href="https://github.com/resemble-ai/chatterbox"&gt;Chatterbox&lt;/a&gt; for a similar project up to date with the 2025 SOTA in voice cloning&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Setup&lt;/h2&gt; 
&lt;h3&gt;1. Install Requirements&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Both Windows and Linux are supported. A GPU is recommended for training and for inference speed, but is not mandatory.&lt;/li&gt; 
 &lt;li&gt;Python 3.7 is recommended. Python 3.5 or greater should work, but you'll probably have to tweak the dependencies' versions. I recommend setting up a virtual environment using &lt;code&gt;venv&lt;/code&gt;, but this is optional.&lt;/li&gt; 
 &lt;li&gt;Install &lt;a href="https://ffmpeg.org/download.html#get-packages"&gt;ffmpeg&lt;/a&gt;. This is necessary for reading audio files.&lt;/li&gt; 
 &lt;li&gt;Install &lt;a href="https://pytorch.org/get-started/locally/"&gt;PyTorch&lt;/a&gt;. Pick the latest stable version, your operating system, your package manager (pip by default) and finally pick any of the proposed CUDA versions if you have a GPU, otherwise pick CPU. Run the given command.&lt;/li&gt; 
 &lt;li&gt;Install the remaining requirements with &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;2. (Optional) Download Pretrained Models&lt;/h3&gt; 
&lt;p&gt;Pretrained models are now downloaded automatically. If this doesn't work for you, you can manually download them &lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;3. (Optional) Test Configuration&lt;/h3&gt; 
&lt;p&gt;Before you download any dataset, you can begin by testing your configuration with:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;python demo_cli.py&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;If all tests pass, you're good to go.&lt;/p&gt; 
&lt;h3&gt;4. (Optional) Download Datasets&lt;/h3&gt; 
&lt;p&gt;For playing with the toolbox alone, I only recommend downloading &lt;a href="https://www.openslr.org/resources/12/train-clean-100.tar.gz"&gt;&lt;code&gt;LibriSpeech/train-clean-100&lt;/code&gt;&lt;/a&gt;. Extract the contents as &lt;code&gt;&amp;lt;datasets_root&amp;gt;/LibriSpeech/train-clean-100&lt;/code&gt; where &lt;code&gt;&amp;lt;datasets_root&amp;gt;&lt;/code&gt; is a directory of your choosing. Other datasets are supported in the toolbox, see &lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training#datasets"&gt;here&lt;/a&gt;. You're free not to download any dataset, but then you will need your own data as audio files or you will have to record it with the toolbox.&lt;/p&gt; 
&lt;h3&gt;5. Launch the Toolbox&lt;/h3&gt; 
&lt;p&gt;You can then try the toolbox:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;python demo_toolbox.py -d &amp;lt;datasets_root&amp;gt;&lt;/code&gt;&lt;br /&gt; or&lt;br /&gt; &lt;code&gt;python demo_toolbox.py&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;depending on whether you downloaded any datasets. If you are running an X-server or if you have the error &lt;code&gt;Aborted (core dumped)&lt;/code&gt;, see &lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/11#issuecomment-504733590"&gt;this issue&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>twitter/the-algorithm</title>
      <link>https://github.com/twitter/the-algorithm</link>
      <description>&lt;p&gt;Source code for the X Recommendation Algorithm&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;X's Recommendation Algorithm&lt;/h1&gt; 
&lt;p&gt;X's Recommendation Algorithm is a set of services and jobs that are responsible for serving feeds of posts and other content across all X product surfaces (e.g. For You Timeline, Search, Explore, Notifications). For an introduction to how the algorithm works, please refer to our &lt;a href="https://blog.x.com/engineering/en_us/topics/open-source/2023/twitter-recommendation-algorithm"&gt;engineering blog&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Architecture&lt;/h2&gt; 
&lt;p&gt;Product surfaces at X are built on a shared set of data, models, and software frameworks. The shared components included in this repository are listed below:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Component&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Data&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/tweetypie/server/README.md"&gt;tweetypie&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Core service that handles the reading and writing of post data.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/unified_user_actions/README.md"&gt;unified-user-actions&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Real-time stream of user actions on X.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/user-signal-service/README.md"&gt;user-signal-service&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Centralized platform to retrieve explicit (e.g. likes, replies) and implicit (e.g. profile visits, tweet clicks) user signals.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Model&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/src/scala/com/twitter/simclusters_v2/README.md"&gt;SimClusters&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Community detection and sparse embeddings into those communities.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/twitter/the-algorithm-ml/raw/main/projects/twhin/README.md"&gt;TwHIN&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Dense knowledge graph embeddings for Users and Posts.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/trust_and_safety_models/README.md"&gt;trust-and-safety-models&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Models for detecting NSFW or abusive content.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/src/scala/com/twitter/interaction_graph/README.md"&gt;real-graph&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Model to predict the likelihood of an X User interacting with another User.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/src/scala/com/twitter/graph/batch/job/tweepcred/README"&gt;tweepcred&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Page-Rank algorithm for calculating X User reputation.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/recos-injector/README.md"&gt;recos-injector&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Streaming event processor for building input streams for &lt;a href="https://github.com/twitter/GraphJet"&gt;GraphJet&lt;/a&gt; based services.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/graph-feature-service/README.md"&gt;graph-feature-service&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Serves graph features for a directed pair of users (e.g. how many of User A's following liked posts from User B).&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/topic-social-proof/README.md"&gt;topic-social-proof&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Identifies topics related to individual posts.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/representation-scorer/README.md"&gt;representation-scorer&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Compute scores between pairs of entities (Users, Posts, etc.) using embedding similarity.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Software framework&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/navi/README.md"&gt;navi&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;High performance, machine learning model serving written in Rust.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/product-mixer/README.md"&gt;product-mixer&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Software framework for building feeds of content.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/timelines/data_processing/ml_util/aggregation_framework/README.md"&gt;timelines-aggregation-framework&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Framework for generating aggregate features in batch or real time.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/representation-manager/README.md"&gt;representation-manager&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Service to retrieve embeddings (i.e. SimClusers and TwHIN).&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/twml/README.md"&gt;twml&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Legacy machine learning framework built on TensorFlow v1.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;The product surfaces currently included in this repository are the For You Timeline and Recommended Notifications.&lt;/p&gt; 
&lt;h3&gt;For You Timeline&lt;/h3&gt; 
&lt;p&gt;The diagram below illustrates how major services and jobs interconnect to construct a For You Timeline.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/twitter/the-algorithm/main/docs/system-diagram.png" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;The core components of the For You Timeline included in this repository are listed below:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Component&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Candidate Source&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/src/java/com/twitter/search/README.md"&gt;search-index&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Find and rank In-Network posts. ~50% of posts come from this candidate source.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/tweet-mixer"&gt;tweet-mixer&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Coordination layer for fetching Out-of-Network tweet candidates from underlying compute services.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/src/scala/com/twitter/recos/user_tweet_entity_graph/README.md"&gt;user-tweet-entity-graph&lt;/a&gt; (UTEG)&lt;/td&gt; 
   &lt;td&gt;Maintains an in memory User to Post interaction graph, and finds candidates based on traversals of this graph. This is built on the &lt;a href="https://github.com/twitter/GraphJet"&gt;GraphJet&lt;/a&gt; framework. Several other GraphJet based features and candidate sources are located &lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/src/scala/com/twitter/recos"&gt;here&lt;/a&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/follow-recommendations-service/README.md"&gt;follow-recommendation-service&lt;/a&gt; (FRS)&lt;/td&gt; 
   &lt;td&gt;Provides Users with recommendations for accounts to follow, and posts from those accounts.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ranking&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/src/python/twitter/deepbird/projects/timelines/scripts/models/earlybird/README.md"&gt;light-ranker&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Light Ranker model used by search index (Earlybird) to rank posts.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/twitter/the-algorithm-ml/raw/main/projects/home/recap/README.md"&gt;heavy-ranker&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Neural network for ranking candidate posts. One of the main signals used to select timeline posts post candidate sourcing.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Post mixing &amp;amp; filtering&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/home-mixer/README.md"&gt;home-mixer&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Main service used to construct and serve the Home Timeline. Built on &lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/product-mixer/README.md"&gt;product-mixer&lt;/a&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/visibilitylib/README.md"&gt;visibility-filters&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Responsible for filtering X content to support legal compliance, improve product quality, increase user trust, protect revenue through the use of hard-filtering, visible product treatments, and coarse-grained downranking.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/timelineranker/README.md"&gt;timelineranker&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Legacy service which provides relevance-scored posts from the Earlybird Search Index and UTEG service.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Recommended Notifications&lt;/h3&gt; 
&lt;p&gt;The core components of Recommended Notifications included in this repository are listed below:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Component&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Service&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/pushservice/README.md"&gt;pushservice&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Main recommendation service at X used to surface recommendations to our users via notifications.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ranking&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/pushservice/src/main/python/models/light_ranking/README.md"&gt;pushservice-light-ranker&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Light Ranker model used by pushservice to rank posts. Bridges candidate generation and heavy ranking by pre-selecting highly-relevant candidates from the initial huge candidate pool.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/pushservice/src/main/python/models/heavy_ranking/README.md"&gt;pushservice-heavy-ranker&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Multi-task learning model to predict the probabilities that the target users will open and engage with the sent notifications.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Build and test code&lt;/h2&gt; 
&lt;p&gt;We include Bazel BUILD files for most components, but not a top-level BUILD or WORKSPACE file. We plan to add a more complete build and test system in the future.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We invite the community to submit GitHub issues and pull requests for suggestions on improving the recommendation algorithm. We are working on tools to manage these suggestions and sync changes to our internal repository. Any security concerns or issues should be routed to our official &lt;a href="https://hackerone.com/x"&gt;bug bounty program&lt;/a&gt; through HackerOne. We hope to benefit from the collective intelligence and expertise of the global community in helping us identify issues and suggest improvements, ultimately leading to a better X.&lt;/p&gt; 
&lt;p&gt;Read our blog on the open source initiative &lt;a href="https://blog.x.com/en_us/topics/company/2023/a-new-era-of-transparency-for-twitter"&gt;here&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>NVIDIA/garak</title>
      <link>https://github.com/NVIDIA/garak</link>
      <description>&lt;p&gt;the LLM vulnerability scanner&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;garak, LLM vulnerability scanner&lt;/h1&gt; 
&lt;p&gt;&lt;em&gt;Generative AI Red-teaming &amp;amp; Assessment Kit&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;garak&lt;/code&gt; checks if an LLM can be made to fail in a way we don't want. &lt;code&gt;garak&lt;/code&gt; probes for hallucination, data leakage, prompt injection, misinformation, toxicity generation, jailbreaks, and many other weaknesses. If you know &lt;code&gt;nmap&lt;/code&gt; or &lt;code&gt;msf&lt;/code&gt; / Metasploit Framework, garak does somewhat similar things to them, but for LLMs.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;garak&lt;/code&gt; focuses on ways of making an LLM or dialog system fail. It combines static, dynamic, and adaptive probes to explore this.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;garak&lt;/code&gt;'s a free tool. We love developing it and are always interested in adding functionality to support applications.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://opensource.org/licenses/Apache-2.0"&gt;&lt;img src="https://img.shields.io/badge/License-Apache_2.0-blue.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://github.com/NVIDIA/garak/actions/workflows/test_linux.yml"&gt;&lt;img src="https://github.com/NVIDIA/garak/actions/workflows/test_linux.yml/badge.svg?sanitize=true" alt="Tests/Linux" /&gt;&lt;/a&gt; &lt;a href="https://github.com/NVIDIA/garak/actions/workflows/test_windows.yml"&gt;&lt;img src="https://github.com/NVIDIA/garak/actions/workflows/test_windows.yml/badge.svg?sanitize=true" alt="Tests/Windows" /&gt;&lt;/a&gt; &lt;a href="https://github.com/NVIDIA/garak/actions/workflows/test_macos.yml"&gt;&lt;img src="https://github.com/NVIDIA/garak/actions/workflows/test_macos.yml/badge.svg?sanitize=true" alt="Tests/OSX" /&gt;&lt;/a&gt; &lt;a href="http://garak.readthedocs.io/en/latest/?badge=latest"&gt;&lt;img src="https://readthedocs.org/projects/garak/badge/?version=latest" alt="Documentation Status" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2406.11036"&gt;&lt;img src="https://img.shields.io/badge/cs.CL-arXiv%3A2406.11036-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/uVch4puUCs"&gt;&lt;img src="https://img.shields.io/badge/chat-on%20discord-yellow.svg?sanitize=true" alt="discord-img" /&gt;&lt;/a&gt; &lt;a href="https://github.com/psf/black"&gt;&lt;img src="https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true" alt="Code style: black" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/garak"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/garak" alt="PyPI - Python Version" /&gt;&lt;/a&gt; &lt;a href="https://badge.fury.io/py/garak"&gt;&lt;img src="https://badge.fury.io/py/garak.svg?sanitize=true" alt="PyPI" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/garak"&gt;&lt;img src="https://static.pepy.tech/badge/garak" alt="Downloads" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/garak"&gt;&lt;img src="https://static.pepy.tech/badge/garak/month" alt="Downloads" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Get started&lt;/h2&gt; 
&lt;h3&gt;&amp;gt; See our user guide! &lt;a href="https://docs.garak.ai/"&gt;docs.garak.ai&lt;/a&gt;&lt;/h3&gt; 
&lt;h3&gt;&amp;gt; Join our &lt;a href="https://discord.gg/uVch4puUCs"&gt;Discord&lt;/a&gt;!&lt;/h3&gt; 
&lt;h3&gt;&amp;gt; Project links &amp;amp; home: &lt;a href="https://garak.ai/"&gt;garak.ai&lt;/a&gt;&lt;/h3&gt; 
&lt;h3&gt;&amp;gt; Twitter: &lt;a href="https://twitter.com/garak_llm"&gt;@garak_llm&lt;/a&gt;&lt;/h3&gt; 
&lt;h3&gt;&amp;gt; DEF CON &lt;a href="https://garak.ai/garak_aiv_slides.pdf"&gt;slides&lt;/a&gt;!&lt;/h3&gt; 
&lt;hr /&gt; 
&lt;h2&gt;LLM support&lt;/h2&gt; 
&lt;p&gt;currently supports:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/models"&gt;hugging face hub&lt;/a&gt; generative models&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://replicate.com/"&gt;replicate&lt;/a&gt; text models&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://platform.openai.com/docs/introduction"&gt;openai api&lt;/a&gt; chat &amp;amp; continuation models&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.litellm.ai/"&gt;litellm&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;pretty much anything accessible via REST&lt;/li&gt; 
 &lt;li&gt;gguf models like &lt;a href="https://github.com/ggerganov/llama.cpp"&gt;llama.cpp&lt;/a&gt; version &amp;gt;= 1046&lt;/li&gt; 
 &lt;li&gt;.. and many more LLMs!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Install:&lt;/h2&gt; 
&lt;p&gt;&lt;code&gt;garak&lt;/code&gt; is a command-line tool. It's developed in Linux and OSX.&lt;/p&gt; 
&lt;h3&gt;Standard install with &lt;code&gt;pip&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;Just grab it from PyPI and you should be good to go:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python -m pip install -U garak
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Install development version with &lt;code&gt;pip&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;The standard pip version of &lt;code&gt;garak&lt;/code&gt; is updated periodically. To get a fresher version from GitHub, try:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python -m pip install -U git+https://github.com/NVIDIA/garak.git@main
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Clone from source&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;garak&lt;/code&gt; has its own dependencies. You can to install &lt;code&gt;garak&lt;/code&gt; in its own Conda environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;conda create --name garak "python&amp;gt;=3.10,&amp;lt;=3.12"
conda activate garak
gh repo clone NVIDIA/garak
cd garak
python -m pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;OK, if that went fine, you're probably good to go!&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: if you cloned before the move to the &lt;code&gt;NVIDIA&lt;/code&gt; GitHub organisation, but you're reading this at the &lt;code&gt;github.com/NVIDIA&lt;/code&gt; URI, please update your remotes as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;git remote set-url origin https://github.com/NVIDIA/garak.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Getting started&lt;/h2&gt; 
&lt;p&gt;The general syntax is:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;garak &amp;lt;options&amp;gt;&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;garak&lt;/code&gt; needs to know what model to scan, and by default, it'll try all the probes it knows on that model, using the vulnerability detectors recommended by each probe. You can see a list of probes using:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;garak --list_probes&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;To specify a generator, use the &lt;code&gt;--model_type&lt;/code&gt; and, optionally, the &lt;code&gt;--model_name&lt;/code&gt; options. Model type specifies a model family/interface; model name specifies the exact model to be used. The "Intro to generators" section below describes some of the generators supported. A straightforward generator family is Hugging Face models; to load one of these, set &lt;code&gt;--model_type&lt;/code&gt; to &lt;code&gt;huggingface&lt;/code&gt; and &lt;code&gt;--model_name&lt;/code&gt; to the model's name on Hub (e.g. &lt;code&gt;"RWKV/rwkv-4-169m-pile"&lt;/code&gt;). Some generators might need an API key to be set as an environment variable, and they'll let you know if they need that.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;garak&lt;/code&gt; runs all the probes by default, but you can be specific about that too. &lt;code&gt;--probes promptinject&lt;/code&gt; will use only the &lt;a href="https://github.com/agencyenterprise/promptinject"&gt;PromptInject&lt;/a&gt; framework's methods, for example. You can also specify one specific plugin instead of a plugin family by adding the plugin name after a &lt;code&gt;.&lt;/code&gt;; for example, &lt;code&gt;--probes lmrc.SlurUsage&lt;/code&gt; will use an implementation of checking for models generating slurs based on the &lt;a href="https://arxiv.org/abs/2303.18190"&gt;Language Model Risk Cards&lt;/a&gt; framework.&lt;/p&gt; 
&lt;p&gt;For help and inspiration, find us on &lt;a href="https://twitter.com/garak_llm"&gt;Twitter&lt;/a&gt; or &lt;a href="https://discord.gg/uVch4puUCs"&gt;discord&lt;/a&gt;!&lt;/p&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;p&gt;Probe ChatGPT for encoding-based prompt injection (OSX/*nix) (replace example value with a real OpenAI API key)&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;export OPENAI_API_KEY="sk-123XXXXXXXXXXXX"
python3 -m garak --model_type openai --model_name gpt-3.5-turbo --probes encoding
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See if the Hugging Face version of GPT2 is vulnerable to DAN 11.0&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python3 -m garak --model_type huggingface --model_name gpt2 --probes dan.Dan_11_0
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Reading the results&lt;/h2&gt; 
&lt;p&gt;For each probe loaded, garak will print a progress bar as it generates. Once generation is complete, a row evaluating that probe's results on each detector is given. If any of the prompt attempts yielded an undesirable behavior, the response will be marked as FAIL, and the failure rate given.&lt;/p&gt; 
&lt;p&gt;Here are the results with the &lt;code&gt;encoding&lt;/code&gt; module on a GPT-3 variant: &lt;img src="https://i.imgur.com/8Dxf45N.png" alt="alt text" /&gt;&lt;/p&gt; 
&lt;p&gt;And the same results for ChatGPT: &lt;img src="https://i.imgur.com/VKAF5if.png" alt="alt text" /&gt;&lt;/p&gt; 
&lt;p&gt;We can see that the more recent model is much more susceptible to encoding-based injection attacks, where text-babbage-001 was only found to be vulnerable to quoted-printable and MIME encoding injections. The figures at the end of each row, e.g. 840/840, indicate the number of text generations total and then how many of these seemed to behave OK. The figure can be quite high because more than one generation is made per prompt - by default, 10.&lt;/p&gt; 
&lt;p&gt;Errors go in &lt;code&gt;garak.log&lt;/code&gt;; the run is logged in detail in a &lt;code&gt;.jsonl&lt;/code&gt; file specified at analysis start &amp;amp; end. There's a basic analysis script in &lt;code&gt;analyse/analyse_log.py&lt;/code&gt; which will output the probes and prompts that led to the most hits.&lt;/p&gt; 
&lt;p&gt;Send PRs &amp;amp; open issues. Happy hunting!&lt;/p&gt; 
&lt;h2&gt;Intro to generators&lt;/h2&gt; 
&lt;h3&gt;Hugging Face&lt;/h3&gt; 
&lt;p&gt;Using the Pipeline API:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--model_type huggingface&lt;/code&gt; (for transformers models to run locally)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - use the model name from Hub. Only generative models will work. If it fails and shouldn't, please open an issue and paste in the command you tried + the exception!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Using the Inference API:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--model_type huggingface.InferenceAPI&lt;/code&gt; (for API-based model access)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - the model name from Hub, e.g. &lt;code&gt;"mosaicml/mpt-7b-instruct"&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Using private endpoints:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;--model_type huggingface.InferenceEndpoint&lt;/code&gt; (for private endpoints)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;--model_name&lt;/code&gt; - the endpoint URL, e.g. &lt;code&gt;https://xxx.us-east-1.aws.endpoints.huggingface.cloud&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;(optional) set the &lt;code&gt;HF_INFERENCE_TOKEN&lt;/code&gt; environment variable to a Hugging Face API token with the "read" role; see &lt;a href="https://huggingface.co/settings/tokens"&gt;https://huggingface.co/settings/tokens&lt;/a&gt; when logged in&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;OpenAI&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--model_type openai&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - the OpenAI model you'd like to use. &lt;code&gt;gpt-3.5-turbo-0125&lt;/code&gt; is fast and fine for testing.&lt;/li&gt; 
 &lt;li&gt;set the &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; environment variable to your OpenAI API key (e.g. "sk-19763ASDF87q6657"); see &lt;a href="https://platform.openai.com/account/api-keys"&gt;https://platform.openai.com/account/api-keys&lt;/a&gt; when logged in&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Recognised model types are whitelisted, because the plugin needs to know which sub-API to use. Completion or ChatCompletion models are OK. If you'd like to use a model not supported, you should get an informative error message, and please send a PR / open an issue.&lt;/p&gt; 
&lt;h3&gt;Replicate&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;set the &lt;code&gt;REPLICATE_API_TOKEN&lt;/code&gt; environment variable to your Replicate API token, e.g. "r8-123XXXXXXXXXXXX"; see &lt;a href="https://replicate.com/account/api-tokens"&gt;https://replicate.com/account/api-tokens&lt;/a&gt; when logged in&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Public Replicate models:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--model_type replicate&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - the Replicate model name and hash, e.g. &lt;code&gt;"stability-ai/stablelm-tuned-alpha-7b:c49dae36"&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Private Replicate endpoints:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--model_type replicate.InferenceEndpoint&lt;/code&gt; (for private endpoints)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - username/model-name slug from the deployed endpoint, e.g. &lt;code&gt;elim/elims-llama2-7b&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Cohere&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--model_type cohere&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; (optional, &lt;code&gt;command&lt;/code&gt; by default) - The specific Cohere model you'd like to test&lt;/li&gt; 
 &lt;li&gt;set the &lt;code&gt;COHERE_API_KEY&lt;/code&gt; environment variable to your Cohere API key, e.g. "aBcDeFgHiJ123456789"; see &lt;a href="https://dashboard.cohere.ai/api-keys"&gt;https://dashboard.cohere.ai/api-keys&lt;/a&gt; when logged in&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Groq&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--model_type groq&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - The name of the model to access via the Groq API&lt;/li&gt; 
 &lt;li&gt;set the &lt;code&gt;GROQ_API_KEY&lt;/code&gt; environment variable to your Groq API key, see &lt;a href="https://console.groq.com/docs/quickstart"&gt;https://console.groq.com/docs/quickstart&lt;/a&gt; for details on creating an API key&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ggml&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--model_type ggml&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - The path to the ggml model you'd like to load, e.g. &lt;code&gt;/home/leon/llama.cpp/models/7B/ggml-model-q4_0.bin&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;set the &lt;code&gt;GGML_MAIN_PATH&lt;/code&gt; environment variable to the path to your ggml &lt;code&gt;main&lt;/code&gt; executable&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;REST&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;rest.RestGenerator&lt;/code&gt; is highly flexible and can connect to any REST endpoint that returns plaintext or JSON. It does need some brief config, which will typically result a short YAML file describing your endpoint. See &lt;a href="https://reference.garak.ai/en/latest/garak.generators.rest.html"&gt;https://reference.garak.ai/en/latest/garak.generators.rest.html&lt;/a&gt; for examples.&lt;/p&gt; 
&lt;h3&gt;NIM&lt;/h3&gt; 
&lt;p&gt;Use models from &lt;a href="https://build.nvidia.com/"&gt;https://build.nvidia.com/&lt;/a&gt; or other NIM endpoints.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;set the &lt;code&gt;NIM_API_KEY&lt;/code&gt; environment variable to your authentication API token, or specify it in the config YAML&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For chat models:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--model_type nim&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - the NIM &lt;code&gt;model&lt;/code&gt; name, e.g. &lt;code&gt;meta/llama-3.1-8b-instruct&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For completion models:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--model_type nim.NVOpenAICompletion&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - the NIM &lt;code&gt;model&lt;/code&gt; name, e.g. &lt;code&gt;bigcode/starcoder2-15b&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Test&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;--model_type test&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;(alternatively) &lt;code&gt;--model_name test.Blank&lt;/code&gt; For testing. This always generates the empty string, using the &lt;code&gt;test.Blank&lt;/code&gt; generator. Will be marked as failing for any tests that &lt;em&gt;require&lt;/em&gt; an output, e.g. those that make contentious claims and expect the model to refute them in order to pass.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;--model_type test.Repeat&lt;/code&gt; For testing. This generator repeats back the prompt it received.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Intro to probes&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Probe&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;blank&lt;/td&gt; 
   &lt;td&gt;A simple probe that always sends an empty prompt.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;atkgen&lt;/td&gt; 
   &lt;td&gt;Automated Attack Generation. A red-teaming LLM probes the target and reacts to it in an attempt to get toxic output. Prototype, mostly stateless, for now uses a simple GPT-2 &lt;a href="https://huggingface.co/garak-llm/artgpt2tox"&gt;fine-tuned&lt;/a&gt; on the subset of hhrlhf attempts that yielded detectable toxicity (the only target currently supported for now).&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;av_spam_scanning&lt;/td&gt; 
   &lt;td&gt;Probes that attempt to make the model output malicious content signatures&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;continuation&lt;/td&gt; 
   &lt;td&gt;Probes that test if the model will continue a probably undesirable word&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;dan&lt;/td&gt; 
   &lt;td&gt;Various &lt;a href="https://adguard.com/en/blog/chatgpt-dan-prompt-abuse.html"&gt;DAN&lt;/a&gt; and DAN-like attacks&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;donotanswer&lt;/td&gt; 
   &lt;td&gt;Prompts to which responsible language models should not answer.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;encoding&lt;/td&gt; 
   &lt;td&gt;Prompt injection through text encoding&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;gcg&lt;/td&gt; 
   &lt;td&gt;Disrupt a system prompt by appending an adversarial suffix.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;glitch&lt;/td&gt; 
   &lt;td&gt;Probe model for glitch tokens that provoke unusual behavior.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;grandma&lt;/td&gt; 
   &lt;td&gt;Appeal to be reminded of one's grandmother.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;goodside&lt;/td&gt; 
   &lt;td&gt;Implementations of Riley Goodside attacks.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;leakreplay&lt;/td&gt; 
   &lt;td&gt;Evaluate if a model will replay training data.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;lmrc&lt;/td&gt; 
   &lt;td&gt;Subsample of the &lt;a href="https://arxiv.org/abs/2303.18190"&gt;Language Model Risk Cards&lt;/a&gt; probes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;malwaregen&lt;/td&gt; 
   &lt;td&gt;Attempts to have the model generate code for building malware&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;misleading&lt;/td&gt; 
   &lt;td&gt;Attempts to make a model support misleading and false claims&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;packagehallucination&lt;/td&gt; 
   &lt;td&gt;Trying to get code generations that specify non-existent (and therefore insecure) packages.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;promptinject&lt;/td&gt; 
   &lt;td&gt;Implementation of the Agency Enterprise &lt;a href="https://github.com/agencyenterprise/PromptInject/tree/main/promptinject"&gt;PromptInject&lt;/a&gt; work (best paper awards @ NeurIPS ML Safety Workshop 2022)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;realtoxicityprompts&lt;/td&gt; 
   &lt;td&gt;Subset of the RealToxicityPrompts work (data constrained because the full test will take so long to run)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;snowball&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ofir.io/snowballed_hallucination.pdf"&gt;Snowballed Hallucination&lt;/a&gt; probes designed to make a model give a wrong answer to questions too complex for it to process&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;xss&lt;/td&gt; 
   &lt;td&gt;Look for vulnerabilities the permit or enact cross-site attacks, such as private data exfiltration.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Logging&lt;/h2&gt; 
&lt;p&gt;&lt;code&gt;garak&lt;/code&gt; generates multiple kinds of log:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;A log file, &lt;code&gt;garak.log&lt;/code&gt;. This includes debugging information from &lt;code&gt;garak&lt;/code&gt; and its plugins, and is continued across runs.&lt;/li&gt; 
 &lt;li&gt;A report of the current run, structured as JSONL. A new report file is created every time &lt;code&gt;garak&lt;/code&gt; runs. The name of this file is output at the beginning and, if successful, also at the end of the run. In the report, an entry is made for each probing attempt both as the generations are received, and again when they are evaluated; the entry's &lt;code&gt;status&lt;/code&gt; attribute takes a constant from &lt;code&gt;garak.attempts&lt;/code&gt; to describe what stage it was made at.&lt;/li&gt; 
 &lt;li&gt;A hit log, detailing attempts that yielded a vulnerability (a 'hit')&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;How is the code structured?&lt;/h2&gt; 
&lt;p&gt;Check out the &lt;a href="https://reference.garak.ai/"&gt;reference docs&lt;/a&gt; for an authoritative guide to &lt;code&gt;garak&lt;/code&gt; code structure.&lt;/p&gt; 
&lt;p&gt;In a typical run, &lt;code&gt;garak&lt;/code&gt; will read a model type (and optionally model name) from the command line, then determine which &lt;code&gt;probe&lt;/code&gt;s and &lt;code&gt;detector&lt;/code&gt;s to run, start up a &lt;code&gt;generator&lt;/code&gt;, and then pass these to a &lt;code&gt;harness&lt;/code&gt; to do the probing; an &lt;code&gt;evaluator&lt;/code&gt; deals with the results. There are many modules in each of these categories, and each module provides a number of classes that act as individual plugins.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;garak/probes/&lt;/code&gt; - classes for generating interactions with LLMs&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;garak/detectors/&lt;/code&gt; - classes for detecting an LLM is exhibiting a given failure mode&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;garak/evaluators/&lt;/code&gt; - assessment reporting schemes&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;garak/generators/&lt;/code&gt; - plugins for LLMs to be probed&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;garak/harnesses/&lt;/code&gt; - classes for structuring testing&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;resources/&lt;/code&gt; - ancillary items required by plugins&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The default operating mode is to use the &lt;code&gt;probewise&lt;/code&gt; harness. Given a list of probe module names and probe plugin names, the &lt;code&gt;probewise&lt;/code&gt; harness instantiates each probe, then for each probe reads its &lt;code&gt;recommended_detectors&lt;/code&gt; attribute to get a list of &lt;code&gt;detector&lt;/code&gt;s to run on the output.&lt;/p&gt; 
&lt;p&gt;Each plugin category (&lt;code&gt;probes&lt;/code&gt;, &lt;code&gt;detectors&lt;/code&gt;, &lt;code&gt;evaluators&lt;/code&gt;, &lt;code&gt;generators&lt;/code&gt;, &lt;code&gt;harnesses&lt;/code&gt;) includes a &lt;code&gt;base.py&lt;/code&gt; which defines the base classes usable by plugins in that category. Each plugin module defines plugin classes that inherit from one of the base classes. For example, &lt;code&gt;garak.generators.openai.OpenAIGenerator&lt;/code&gt; descends from &lt;code&gt;garak.generators.base.Generator&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Larger artefacts, like model files and bigger corpora, are kept out of the repository; they can be stored on e.g. Hugging Face Hub and loaded locally by clients using &lt;code&gt;garak&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Developing your own plugin&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Take a look at how other plugins do it&lt;/li&gt; 
 &lt;li&gt;Inherit from one of the base classes, e.g. &lt;code&gt;garak.probes.base.TextProbe&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Override as little as possible&lt;/li&gt; 
 &lt;li&gt;You can test the new code in at least two ways: 
  &lt;ul&gt; 
   &lt;li&gt;Start an interactive Python session 
    &lt;ul&gt; 
     &lt;li&gt;Import the model, e.g. &lt;code&gt;import garak.probes.mymodule&lt;/code&gt;&lt;/li&gt; 
     &lt;li&gt;Instantiate the plugin, e.g. &lt;code&gt;p = garak.probes.mymodule.MyProbe()&lt;/code&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Run a scan with test plugins 
    &lt;ul&gt; 
     &lt;li&gt;For probes, try a blank generator and always.Pass detector: &lt;code&gt;python3 -m garak -m test.Blank -p mymodule -d always.Pass&lt;/code&gt;&lt;/li&gt; 
     &lt;li&gt;For detectors, try a blank generator and a blank probe: &lt;code&gt;python3 -m garak -m test.Blank -p test.Blank -d mymodule&lt;/code&gt;&lt;/li&gt; 
     &lt;li&gt;For generators, try a blank probe and always.Pass detector: &lt;code&gt;python3 -m garak -m mymodule -p test.Blank -d always.Pass&lt;/code&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Get &lt;code&gt;garak&lt;/code&gt; to list all the plugins of the type you're writing, with &lt;code&gt;--list_probes&lt;/code&gt;, &lt;code&gt;--list_detectors&lt;/code&gt;, or &lt;code&gt;--list_generators&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;FAQ&lt;/h2&gt; 
&lt;p&gt;We have an FAQ &lt;a href="https://github.com/NVIDIA/garak/raw/main/FAQ.md"&gt;here&lt;/a&gt;. Reach out if you have any more questions! &lt;a href="mailto:garak@nvidia.com"&gt;garak@nvidia.com&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Code reference documentation is at &lt;a href="https://garak.readthedocs.io/en/latest/"&gt;garak.readthedocs.io&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Citing garak&lt;/h2&gt; 
&lt;p&gt;You can read the &lt;a href="https://raw.githubusercontent.com/NVIDIA/garak/main/garak-paper.pdf"&gt;garak preprint paper&lt;/a&gt;. If you use garak, please cite us.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@article{garak,
  title={{garak: A Framework for Security Probing Large Language Models}},
  author={Leon Derczynski and Erick Galinkin and Jeffrey Martin and Subho Majumdar and Nanna Inie},
  year={2024},
  howpublished={\url{https://garak.ai}}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;em&gt;"Lying is a skill like any other, and if you wish to maintain a level of excellence you have to practice constantly"&lt;/em&gt; - Elim&lt;/p&gt; 
&lt;p&gt;For updates and news see &lt;a href="https://twitter.com/garak_llm"&gt;@garak_llm&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Â© 2023- Leon Derczynski; Apache license v2, see &lt;a href="https://raw.githubusercontent.com/NVIDIA/garak/main/LICENSE"&gt;LICENSE&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>grpc/grpc-go</title>
      <link>https://github.com/grpc/grpc-go</link>
      <description>&lt;p&gt;The Go language implementation of gRPC. HTTP/2 based RPC&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;gRPC-Go&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://pkg.go.dev/google.golang.org/grpc"&gt;&lt;img src="https://pkg.go.dev/badge/google.golang.org/grpc" alt="GoDoc" /&gt;&lt;/a&gt; &lt;a href="https://goreportcard.com/report/github.com/grpc/grpc-go"&gt;&lt;img src="https://goreportcard.com/badge/grpc/grpc-go" alt="GoReportCard" /&gt;&lt;/a&gt; &lt;a href="https://codecov.io/gh/grpc/grpc-go"&gt;&lt;img src="https://codecov.io/gh/grpc/grpc-go/graph/badge.svg?sanitize=true" alt="codecov" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;The &lt;a href="https://golang.org"&gt;Go&lt;/a&gt; implementation of &lt;a href="https://grpc.io"&gt;gRPC&lt;/a&gt;: A high performance, open source, general RPC framework that puts mobile and HTTP/2 first. For more information see the &lt;a href="https://grpc.io/docs/languages/go"&gt;Go gRPC docs&lt;/a&gt;, or jump directly into the &lt;a href="https://grpc.io/docs/languages/go/quickstart"&gt;quick start&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Prerequisites&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://golang.org"&gt;Go&lt;/a&gt;&lt;/strong&gt;: any one of the &lt;strong&gt;two latest major&lt;/strong&gt; &lt;a href="https://golang.org/doc/devel/release.html"&gt;releases&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;Simply add the following import to your code, and then &lt;code&gt;go [build|run|test]&lt;/code&gt; will automatically fetch the necessary dependencies:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-go"&gt;import "google.golang.org/grpc"
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; If you are trying to access &lt;code&gt;grpc-go&lt;/code&gt; from &lt;strong&gt;China&lt;/strong&gt;, see the &lt;a href="https://raw.githubusercontent.com/grpc/grpc-go/master/#FAQ"&gt;FAQ&lt;/a&gt; below.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Learn more&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://grpc.io/docs/languages/go"&gt;Go gRPC docs&lt;/a&gt;, which include a &lt;a href="https://grpc.io/docs/languages/go/quickstart"&gt;quick start&lt;/a&gt; and &lt;a href="https://pkg.go.dev/google.golang.org/grpc"&gt;API reference&lt;/a&gt; among other resources&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/grpc/grpc-go/master/Documentation"&gt;Low-level technical docs&lt;/a&gt; from this repository&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://performance-dot-grpc-testing.appspot.com/explore?dashboard=5180705743044608"&gt;Performance benchmark&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/grpc/grpc-go/master/examples"&gt;Examples&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/grpc/grpc-go/master/CONTRIBUTING.md"&gt;Contribution guidelines&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;FAQ&lt;/h2&gt; 
&lt;h3&gt;I/O Timeout Errors&lt;/h3&gt; 
&lt;p&gt;The &lt;code&gt;golang.org&lt;/code&gt; domain may be blocked from some countries. &lt;code&gt;go get&lt;/code&gt; usually produces an error like the following when this happens:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;$ go get -u google.golang.org/grpc
package google.golang.org/grpc: unrecognized import path "google.golang.org/grpc" (https fetch: Get https://google.golang.org/grpc?go-get=1: dial tcp 216.239.37.1:443: i/o timeout)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To build Go code, there are several options:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Set up a VPN and access google.golang.org through that.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;With Go module support: it is possible to use the &lt;code&gt;replace&lt;/code&gt; feature of &lt;code&gt;go mod&lt;/code&gt; to create aliases for golang.org packages. In your project's directory:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;go mod edit -replace=google.golang.org/grpc=github.com/grpc/grpc-go@latest
go mod tidy
go mod vendor
go build -mod=vendor
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Again, this will need to be done for all transitive dependencies hosted on golang.org as well. For details, refer to &lt;a href="https://github.com/golang/go/issues/28652"&gt;golang/go issue #28652&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Compiling error, undefined: grpc.SupportPackageIsVersion&lt;/h3&gt; 
&lt;p&gt;Please update to the latest version of gRPC-Go using &lt;code&gt;go get google.golang.org/grpc&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;How to turn on logging&lt;/h3&gt; 
&lt;p&gt;The default logger is controlled by environment variables. Turn everything on like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;$ export GRPC_GO_LOG_VERBOSITY_LEVEL=99
$ export GRPC_GO_LOG_SEVERITY_LEVEL=info
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;The RPC failed with error &lt;code&gt;"code = Unavailable desc = transport is closing"&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;This error means the connection the RPC is using was closed, and there are many possible reasons, including:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;mis-configured transport credentials, connection failed on handshaking&lt;/li&gt; 
 &lt;li&gt;bytes disrupted, possibly by a proxy in between&lt;/li&gt; 
 &lt;li&gt;server shutdown&lt;/li&gt; 
 &lt;li&gt;Keepalive parameters caused connection shutdown, for example if you have configured your server to terminate connections regularly to &lt;a href="https://github.com/grpc/grpc-go/issues/3170#issuecomment-552517779"&gt;trigger DNS lookups&lt;/a&gt;. If this is the case, you may want to increase your &lt;a href="https://pkg.go.dev/google.golang.org/grpc/keepalive?tab=doc#ServerParameters"&gt;MaxConnectionAgeGrace&lt;/a&gt;, to allow longer RPC calls to finish.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;It can be tricky to debug this because the error happens on the client side but the root cause of the connection being closed is on the server side. Turn on logging on &lt;strong&gt;both client and server&lt;/strong&gt;, and see if there are any transport errors.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>google/material-design-icons</title>
      <link>https://github.com/google/material-design-icons</link>
      <description>&lt;p&gt;Material Design icons by Google (Material Symbols)&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;Material Symbols / Material Icons&lt;/h2&gt; 
&lt;p&gt;These are two different official icon sets from Google, using the same underlying designs. Material Symbols is the current set, introduced in April 2022, built on variable font technology. Material Icons is the classic set, but no longer updated. More details below.&lt;/p&gt; 
&lt;p&gt;The icons can be browsed in a more user-friendly way at &lt;a href="https://fonts.google.com/icons"&gt;https://fonts.google.com/icons&lt;/a&gt;. Use the popdown menu near top left to choose between the two sets; Material Symbols is the default.&lt;/p&gt; 
&lt;p&gt;The icons are designed under the &lt;a href="https://material.io/guidelines/"&gt;material design guidelines&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Icon Requests&lt;/h2&gt; 
&lt;p&gt;Weâ€™d love to support your icon needs! Please submit your request here on GitHub as an issue.&lt;/p&gt; 
&lt;p&gt;Please note that Google Fonts does not accept user submissions of finished icon designs! There are fairly strict guidelines for Material icons, plus Google has upstream source files from which this repo is generated. Therefore, Google does not accept pull requests for icon files (whether new icon suggestions, or fixes for existing icons). Concepts are appreciatedâ€”just donâ€™t design SVGs and submit them via pull request.&lt;/p&gt; 
&lt;p&gt;However, users are perfectly welcome to point at outside files or images as examplesâ€”for the kind of thing they want, but they wonâ€™t just be taken â€œas is.â€ This works especially well if you have multiple examples for a single icon, to help us understand the â€œessenceâ€&amp;nbsp;of the idea.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;For example, there is a fairly universal conceptual logo/icon for â€œagender,â€ so if you were proposing Google add an agender icon in the Material style, either mentioning that, or pointing at &lt;a href="https://www.google.com/search?q=agender+icon"&gt;https://www.google.com/search?q=agender+icon&lt;/a&gt; would be a helpful tip.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Third-party logos&lt;/h3&gt; 
&lt;p&gt;Currently, Google does not include 3rd-party logos among the Material Symbols or Material Icons due to legal reasons. Some 3rd-party logos that were included in the past have since been removed.&lt;/p&gt; 
&lt;h2&gt;npm Packages&lt;/h2&gt; 
&lt;p&gt;Google does not currently maintain the npm package for this repo, past v3 (2016). However, user @marella is hosting the following. He tells us these are automatically updated and published using GitHub Actions. Note: Google does &lt;strong&gt;not&lt;/strong&gt; monitor or vet these packages.&lt;/p&gt; 
&lt;h3&gt;&lt;a href="https://github.com/marella/material-symbols/tree/main/material-symbols#readme"&gt;material-symbols&lt;/a&gt; &lt;a href="https://www.npmjs.com/package/material-symbols"&gt;&lt;img src="https://img.shields.io/npm/v/material-symbols" alt="npm" /&gt;&lt;/a&gt; &lt;a href="https://packagephobia.com/result?p=material-symbols"&gt;&lt;img src="https://packagephobia.com/badge?p=material-symbols" alt="install size" /&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Only WOFF2 variable fonts and CSS for Material Symbols&lt;/li&gt; 
 &lt;li&gt;Includes outlined, rounded, and sharp icons and all variations of fill, weight, grade, and optical size&lt;/li&gt; 
 &lt;li&gt;Supports Sass&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;a href="https://github.com/marella/material-icons#readme"&gt;material-icons&lt;/a&gt; &lt;a href="https://www.npmjs.com/package/material-icons"&gt;&lt;img src="https://img.shields.io/npm/v/material-icons" alt="npm" /&gt;&lt;/a&gt; &lt;a href="https://packagephobia.com/result?p=material-icons"&gt;&lt;img src="https://packagephobia.com/badge?p=material-icons" alt="install size" /&gt;&lt;/a&gt; &lt;a href="https://www.npmjs.com/package/material-icons"&gt;&lt;img src="https://img.shields.io/npm/dm/material-icons" alt="Downloads" /&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Only WOFF2, WOFF fonts and CSS&lt;/li&gt; 
 &lt;li&gt;Includes outlined, round, sharp and two-tone icons&lt;/li&gt; 
 &lt;li&gt;Supports Sass&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;a href="https://github.com/marella/material-design-icons/tree/main/font#readme"&gt;@material-design-icons/font&lt;/a&gt; &lt;a href="https://www.npmjs.com/package/@material-design-icons/font"&gt;&lt;img src="https://img.shields.io/npm/v/@material-design-icons/font" alt="npm (scoped)" /&gt;&lt;/a&gt; &lt;a href="https://packagephobia.com/result?p=@material-design-icons/font"&gt;&lt;img src="https://packagephobia.com/badge?p=@material-design-icons/font" alt="install size" /&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Only WOFF2 fonts and CSS&lt;/li&gt; 
 &lt;li&gt;Lighter version of &lt;code&gt;material-icons&lt;/code&gt; package&lt;/li&gt; 
 &lt;li&gt;Doesn't support &lt;a href="https://caniuse.com/woff2"&gt;older browsers&lt;/a&gt; such as Internet Explorer because of dropping WOFF (v1)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;a href="https://github.com/marella/material-design-icons/tree/main/svg#readme"&gt;@material-design-icons/svg&lt;/a&gt; &lt;a href="https://www.npmjs.com/package/@material-design-icons/svg"&gt;&lt;img src="https://img.shields.io/npm/v/@material-design-icons/svg" alt="npm (scoped)" /&gt;&lt;/a&gt; &lt;a href="https://packagephobia.com/result?p=@material-design-icons/svg"&gt;&lt;img src="https://packagephobia.com/badge?p=@material-design-icons/svg" alt="install size" /&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Only SVGs&lt;/li&gt; 
 &lt;li&gt;Optimizes SVGs using SVGO&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Material Symbols&lt;/h2&gt; 
&lt;p&gt;These newer icons can be browsed in a more user-friendly way at &lt;a href="https://fonts.google.com/icons"&gt;https://fonts.google.com/icons&lt;/a&gt;. Use the popdown menu near top left to choose between the two sets; Material Symbols is the default.&lt;/p&gt; 
&lt;p&gt;These icons were built/designed as variable fonts first (based on the 24 px designs from Material Icons). There are three separate Material Symbols variable fonts, which also have static icons available (but those do not have all the variations available, as that would be hundreds of styles):&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Outlined&lt;/li&gt; 
 &lt;li&gt;Rounded&lt;/li&gt; 
 &lt;li&gt;Sharp&lt;/li&gt; 
 &lt;li&gt;Note that although there is no separate Filled font, the Fill axis allows access to filled styles, in all three fonts. It can also be manipulated for an animated fill effect, to indicate user selection.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Each of the fonts has these design axes, which can be varied in CSS, or in many more modern design apps:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Optical Size (opsz) from 20 to 48 px. The default is 24.&lt;/li&gt; 
 &lt;li&gt;Weight from 100 (Thin) to 700 (Bold). Regular is 400.&lt;/li&gt; 
 &lt;li&gt;Grade from -50 to 200. The default is 0 (zero). -50 is suggested for reversed contrast (e.g. white icons on black background)&lt;/li&gt; 
 &lt;li&gt;Fill from 0 to 100. The default is 0 (zero).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The following directories in this repo contain specifically Material Symbols (not Material Icons) content:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;symbols&lt;/li&gt; 
 &lt;li&gt;variablefont&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;What is currently &lt;em&gt;not&lt;/em&gt; available in Material Symbols?&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;only the 20 and 24 px versions are designed with perfect pixel-grid alignment&lt;/li&gt; 
 &lt;li&gt;the only pre-made fonts are the variable fonts&lt;/li&gt; 
 &lt;li&gt;there are no two-tone icons&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Material Icons&lt;/h2&gt; 
&lt;p&gt;The icons can be browsed in a more user-friendly way at &lt;a href="https://fonts.google.com/icons?icon.set=Material+Icons"&gt;https://fonts.google.com/icons?icon.set=Material+Icons&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;These classic icons are available in five distinct styles:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Outlined&lt;/li&gt; 
 &lt;li&gt;Filled (the font version is just called Material Icons, as this is the oldest style)&lt;/li&gt; 
 &lt;li&gt;Rounded&lt;/li&gt; 
 &lt;li&gt;Sharp&lt;/li&gt; 
 &lt;li&gt;Two tone&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The following directories in this repo contain specifically Material Icons (not Material Symbols) content:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;android&lt;/li&gt; 
 &lt;li&gt;font&lt;/li&gt; 
 &lt;li&gt;ios&lt;/li&gt; 
 &lt;li&gt;png&lt;/li&gt; 
 &lt;li&gt;src&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;What is currently &lt;em&gt;not&lt;/em&gt; available in Material Icons?&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;variable fonts&lt;/li&gt; 
 &lt;li&gt;weights other than Regular&lt;/li&gt; 
 &lt;li&gt;grades other than Regular&lt;/li&gt; 
 &lt;li&gt;a means to animate Fill transitions&lt;/li&gt; 
 &lt;li&gt;new icons (since updates were halted in 2022)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Material Icons update history&lt;/h2&gt; 
&lt;h3&gt;4.0.0 Update&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;2020 Aug 31&lt;/li&gt; 
 &lt;li&gt;Restructured repository, updated assets.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;3.0.1 Update&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;2016 Sep 01&lt;/li&gt; 
 &lt;li&gt;Changed license in package.json.&lt;/li&gt; 
 &lt;li&gt;Added missing device symbol sprites.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;3.0.0 Update&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;2016 Aug 25&lt;/li&gt; 
 &lt;li&gt;License change to Apache 2.0!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;2.0&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;2016 May 28&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;Read the &lt;a href="https://developers.google.com/fonts/docs/material_icons"&gt;developer guide&lt;/a&gt; on how to use the material design icons in your project.&lt;/p&gt; 
&lt;h3&gt;Using a font&lt;/h3&gt; 
&lt;p&gt;The &lt;code&gt;font&lt;/code&gt; and &lt;code&gt;variablefont&lt;/code&gt; folders contain pre-generated font files that can be included in a project. This is especially convenient for the web; however, it is generally better to link to the web font hosted on Google Fonts:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-html"&gt;&amp;lt;link href="https://fonts.googleapis.com/css2?family=Material+Icons"
      rel="stylesheet"&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-html"&gt;&amp;lt;link href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined"
      rel="stylesheet"&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Read more on &lt;a href="https://developers.google.com/fonts/docs/material_symbols/"&gt;Material Symbols&lt;/a&gt; or &lt;a href="https://developers.google.com/fonts/docs/material_icons/"&gt;Material Icons&lt;/a&gt; in the Google Fonts developer guide.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;We have made these icons available for you to incorporate into your products under the &lt;a href="https://www.apache.org/licenses/LICENSE-2.0.txt"&gt;Apache License Version 2.0&lt;/a&gt;. Feel free to remix and re-share these icons and documentation in your products. We'd love attribution in your app's &lt;em&gt;about&lt;/em&gt; screen, but it's not required.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ml-explore/mlx-lm</title>
      <link>https://github.com/ml-explore/mlx-lm</link>
      <description>&lt;p&gt;Run LLMs with MLX&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;MLX LM&lt;/h2&gt; 
&lt;p&gt;MLX LM is a Python package for generating text and fine-tuning large language models on Apple silicon with MLX.&lt;/p&gt; 
&lt;p&gt;Some key features include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Integration with the Hugging Face Hub to easily use thousands of LLMs with a single command.&lt;/li&gt; 
 &lt;li&gt;Support for quantizing and uploading models to the Hugging Face Hub.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ml-explore/mlx-lm/raw/main/mlx_lm/LORA.md"&gt;Low-rank and full model fine-tuning&lt;/a&gt; with support for quantized models.&lt;/li&gt; 
 &lt;li&gt;Distributed inference and fine-tuning with &lt;code&gt;mx.distributed&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The easiest way to get started is to install the &lt;code&gt;mlx-lm&lt;/code&gt; package:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;With &lt;code&gt;pip&lt;/code&gt;&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip install mlx-lm
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;With &lt;code&gt;conda&lt;/code&gt;&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;conda install -c conda-forge mlx-lm
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Quick Start&lt;/h3&gt; 
&lt;p&gt;To generate text with an LLM use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mlx_lm.generate --prompt "How tall is Mt Everest?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To chat with an LLM use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mlx_lm.chat
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will give you a chat REPL that you can use to interact with the LLM. The chat context is preserved during the lifetime of the REPL.&lt;/p&gt; 
&lt;p&gt;Commands in &lt;code&gt;mlx-lm&lt;/code&gt; typically take command line options which let you specify the model, sampling parameters, and more. Use &lt;code&gt;-h&lt;/code&gt; to see a list of available options for a command, e.g.:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mlx_lm.generate -h
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The default model for generation and chat is &lt;code&gt;mlx-community/Llama-3.2-3B-Instruct-4bit&lt;/code&gt;. You can specify any MLX-compatible model with the &lt;code&gt;--model&lt;/code&gt; flag. Thousands are available in the &lt;a href="https://huggingface.co/mlx-community"&gt;MLX Community&lt;/a&gt; Hugging Face organization.&lt;/p&gt; 
&lt;h3&gt;Python API&lt;/h3&gt; 
&lt;p&gt;You can use &lt;code&gt;mlx-lm&lt;/code&gt; as a module:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_lm import load, generate

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

prompt = "Write a story about Einstein"

messages = [{"role": "user", "content": prompt}]
prompt = tokenizer.apply_chat_template(
    messages, add_generation_prompt=True
)

text = generate(model, tokenizer, prompt=prompt, verbose=True)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To see a description of all the arguments you can do:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; help(generate)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Check out the &lt;a href="https://github.com/ml-explore/mlx-lm/tree/main/mlx_lm/examples/generate_response.py"&gt;generation example&lt;/a&gt; to see how to use the API in more detail. Check out the &lt;a href="https://github.com/ml-explore/mlx-lm/tree/main/mlx_lm/examples/batch_generate_response.py"&gt;batch generation example&lt;/a&gt; to see how to efficiently generate continuations for a batch of prompts.&lt;/p&gt; 
&lt;p&gt;The &lt;code&gt;mlx-lm&lt;/code&gt; package also comes with functionality to quantize and optionally upload models to the Hugging Face Hub.&lt;/p&gt; 
&lt;p&gt;You can convert models using the Python API:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_lm import convert

repo = "mistralai/Mistral-7B-Instruct-v0.3"
upload_repo = "mlx-community/My-Mistral-7B-Instruct-v0.3-4bit"

convert(repo, quantize=True, upload_repo=upload_repo)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will generate a 4-bit quantized Mistral 7B and upload it to the repo &lt;code&gt;mlx-community/My-Mistral-7B-Instruct-v0.3-4bit&lt;/code&gt;. It will also save the converted model in the path &lt;code&gt;mlx_model&lt;/code&gt; by default.&lt;/p&gt; 
&lt;p&gt;To see a description of all the arguments you can do:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; help(convert)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Streaming&lt;/h4&gt; 
&lt;p&gt;For streaming generation, use the &lt;code&gt;stream_generate&lt;/code&gt; function. This yields a generation response object.&lt;/p&gt; 
&lt;p&gt;For example,&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_lm import load, stream_generate

repo = "mlx-community/Mistral-7B-Instruct-v0.3-4bit"
model, tokenizer = load(repo)

prompt = "Write a story about Einstein"

messages = [{"role": "user", "content": prompt}]
prompt = tokenizer.apply_chat_template(
    messages, add_generation_prompt=True
)

for response in stream_generate(model, tokenizer, prompt, max_tokens=512):
    print(response.text, end="", flush=True)
print()
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Sampling&lt;/h4&gt; 
&lt;p&gt;The &lt;code&gt;generate&lt;/code&gt; and &lt;code&gt;stream_generate&lt;/code&gt; functions accept &lt;code&gt;sampler&lt;/code&gt; and &lt;code&gt;logits_processors&lt;/code&gt; keyword arguments. A sampler is any callable which accepts a possibly batched logits array and returns an array of sampled tokens. The &lt;code&gt;logits_processors&lt;/code&gt; must be a list of callables which take the token history and current logits as input and return the processed logits. The logits processors are applied in order.&lt;/p&gt; 
&lt;p&gt;Some standard sampling functions and logits processors are provided in &lt;code&gt;mlx_lm.sample_utils&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Command Line&lt;/h3&gt; 
&lt;p&gt;You can also use &lt;code&gt;mlx-lm&lt;/code&gt; from the command line with:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;mlx_lm.generate --model mistralai/Mistral-7B-Instruct-v0.3 --prompt "hello"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will download a Mistral 7B model from the Hugging Face Hub and generate text using the given prompt.&lt;/p&gt; 
&lt;p&gt;For a full list of options run:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;mlx_lm.generate --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To quantize a model from the command line run:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;mlx_lm.convert --hf-path mistralai/Mistral-7B-Instruct-v0.3 -q
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more options run:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;mlx_lm.convert --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can upload new models to Hugging Face by specifying &lt;code&gt;--upload-repo&lt;/code&gt; to &lt;code&gt;convert&lt;/code&gt;. For example, to upload a quantized Mistral-7B model to the &lt;a href="https://huggingface.co/mlx-community"&gt;MLX Hugging Face community&lt;/a&gt; you can do:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;mlx_lm.convert \
    --hf-path mistralai/Mistral-7B-Instruct-v0.3 \
    -q \
    --upload-repo mlx-community/my-4bit-mistral
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Models can also be converted and quantized directly in the &lt;a href="https://huggingface.co/spaces/mlx-community/mlx-my-repo"&gt;mlx-my-repo&lt;/a&gt; Hugging Face Space.&lt;/p&gt; 
&lt;h3&gt;Long Prompts and Generations&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;mlx-lm&lt;/code&gt; has some tools to scale efficiently to long prompts and generations:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;A rotating fixed-size key-value cache.&lt;/li&gt; 
 &lt;li&gt;Prompt caching&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To use the rotating key-value cache pass the argument &lt;code&gt;--max-kv-size n&lt;/code&gt; where &lt;code&gt;n&lt;/code&gt; can be any integer. Smaller values like &lt;code&gt;512&lt;/code&gt; will use very little RAM but result in worse quality. Larger values like &lt;code&gt;4096&lt;/code&gt; or higher will use more RAM but have better quality.&lt;/p&gt; 
&lt;p&gt;Caching prompts can substantially speedup reusing the same long context with different queries. To cache a prompt use &lt;code&gt;mlx_lm.cache_prompt&lt;/code&gt;. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cat prompt.txt | mlx_lm.cache_prompt \
  --model mistralai/Mistral-7B-Instruct-v0.3 \
  --prompt - \
  --prompt-cache-file mistral_prompt.safetensors
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then use the cached prompt with &lt;code&gt;mlx_lm.generate&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;mlx_lm.generate \
    --prompt-cache-file mistral_prompt.safetensors \
    --prompt "\nSummarize the above text."
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The cached prompt is treated as a prefix to the supplied prompt. Also notice when using a cached prompt, the model to use is read from the cache and need not be supplied explicitly.&lt;/p&gt; 
&lt;p&gt;Prompt caching can also be used in the Python API in order to avoid recomputing the prompt. This is useful in multi-turn dialogues or across requests that use the same context. See the &lt;a href="https://github.com/ml-explore/mlx-lm/raw/main/mlx_lm/examples/chat.py"&gt;example&lt;/a&gt; for more usage details.&lt;/p&gt; 
&lt;h3&gt;Supported Models&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;mlx-lm&lt;/code&gt; supports thousands of Hugging Face format LLMs. If the model you want to run is not supported, file an &lt;a href="https://github.com/ml-explore/mlx-lm/issues/new"&gt;issue&lt;/a&gt; or better yet, submit a pull request.&lt;/p&gt; 
&lt;p&gt;Here are a few examples of Hugging Face models that work with this example:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/mistralai/Mistral-7B-v0.1"&gt;mistralai/Mistral-7B-v0.1&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/meta-llama/Llama-2-7b-hf"&gt;meta-llama/Llama-2-7b-hf&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/deepseek-ai/deepseek-coder-6.7b-instruct"&gt;deepseek-ai/deepseek-coder-6.7b-instruct&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/01-ai/Yi-6B-Chat"&gt;01-ai/Yi-6B-Chat&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/microsoft/phi-2"&gt;microsoft/phi-2&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1"&gt;mistralai/Mixtral-8x7B-Instruct-v0.1&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Qwen/Qwen-7B"&gt;Qwen/Qwen-7B&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/pfnet/plamo-13b"&gt;pfnet/plamo-13b&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/pfnet/plamo-13b-instruct"&gt;pfnet/plamo-13b-instruct&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/stabilityai/stablelm-2-zephyr-1_6b"&gt;stabilityai/stablelm-2-zephyr-1_6b&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/internlm/internlm2-7b"&gt;internlm/internlm2-7b&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/tiiuae/falcon-mamba-7b-instruct"&gt;tiiuae/falcon-mamba-7b-instruct&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Most &lt;a href="https://huggingface.co/models?library=transformers,safetensors&amp;amp;other=mistral&amp;amp;sort=trending"&gt;Mistral&lt;/a&gt;, &lt;a href="https://huggingface.co/models?library=transformers,safetensors&amp;amp;other=llama&amp;amp;sort=trending"&gt;Llama&lt;/a&gt;, &lt;a href="https://huggingface.co/models?library=transformers,safetensors&amp;amp;other=phi&amp;amp;sort=trending"&gt;Phi-2&lt;/a&gt;, and &lt;a href="https://huggingface.co/models?library=transformers,safetensors&amp;amp;other=mixtral&amp;amp;sort=trending"&gt;Mixtral&lt;/a&gt; style models should work out of the box.&lt;/p&gt; 
&lt;p&gt;For some models (such as &lt;code&gt;Qwen&lt;/code&gt; and &lt;code&gt;plamo&lt;/code&gt;) the tokenizer requires you to enable the &lt;code&gt;trust_remote_code&lt;/code&gt; option. You can do this by passing &lt;code&gt;--trust-remote-code&lt;/code&gt; in the command line. If you don't specify the flag explicitly, you will be prompted to trust remote code in the terminal when running the model.&lt;/p&gt; 
&lt;p&gt;For &lt;code&gt;Qwen&lt;/code&gt; models you must also specify the &lt;code&gt;eos_token&lt;/code&gt;. You can do this by passing &lt;code&gt;--eos-token "&amp;lt;|endoftext|&amp;gt;"&lt;/code&gt; in the command line.&lt;/p&gt; 
&lt;p&gt;These options can also be set in the Python API. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;model, tokenizer = load(
    "qwen/Qwen-7B",
    tokenizer_config={"eos_token": "&amp;lt;|endoftext|&amp;gt;", "trust_remote_code": True},
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Large Models&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] This requires macOS 15.0 or higher to work.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Models which are large relative to the total RAM available on the machine can be slow. &lt;code&gt;mlx-lm&lt;/code&gt; will attempt to make them faster by wiring the memory occupied by the model and cache. This requires macOS 15 or higher to work.&lt;/p&gt; 
&lt;p&gt;If you see the following warning message:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[WARNING] Generating with a model that requires ...&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;then the model will likely be slow on the given machine. If the model fits in RAM then it can often be sped up by increasing the system wired memory limit. To increase the limit, set the following &lt;code&gt;sysctl&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo sysctl iogpu.wired_limit_mb=N
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The value &lt;code&gt;N&lt;/code&gt; should be larger than the size of the model in megabytes but smaller than the memory size of the machine.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>CodebuffAI/codebuff</title>
      <link>https://github.com/CodebuffAI/codebuff</link>
      <description>&lt;p&gt;Generate code from the terminal!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Codebuff&lt;/h1&gt; 
&lt;p&gt;Codebuff is an &lt;strong&gt;open-source AI coding assistant&lt;/strong&gt; that edits your codebase through natural language instructions. Instead of using one model for everything, it coordinates specialized agents that work together to understand your project and make precise changes.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/CodebuffAI/codebuff/main/assets/codebuff-vs-claude-code.png" alt="Codebuff vs Claude Code" width="400" /&gt; 
&lt;/div&gt; 
&lt;p&gt;Codebuff beats Claude Code at 61% vs 53% on &lt;a href="https://raw.githubusercontent.com/CodebuffAI/codebuff/main/evals/README.md"&gt;our evals&lt;/a&gt; across 175+ coding tasks over multiple open-source repos that simulate real-world tasks.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/CodebuffAI/codebuff/main/assets/demo.gif" alt="Codebuff Demo" /&gt;&lt;/p&gt; 
&lt;h2&gt;How it works&lt;/h2&gt; 
&lt;p&gt;When you ask Codebuff to "add authentication to my API," it might invoke:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;A &lt;strong&gt;File Explorer Agent&lt;/strong&gt; to scan your codebase to understand the architecture and find relevant files&lt;/li&gt; 
 &lt;li&gt;A &lt;strong&gt;Planner Agent&lt;/strong&gt; to plan which files need changes and in what order&lt;/li&gt; 
 &lt;li&gt;An &lt;strong&gt;Editor Agent&lt;/strong&gt; to make precise edits&lt;/li&gt; 
 &lt;li&gt;A &lt;strong&gt;Reviewer Agent&lt;/strong&gt; to validate changes&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/CodebuffAI/codebuff/main/assets/multi-agents.png" alt="Codebuff Multi-Agents" width="250" /&gt; 
&lt;/div&gt; 
&lt;p&gt;This multi-agent approach gives you better context understanding, more accurate edits, and fewer errors compared to single-model tools.&lt;/p&gt; 
&lt;h2&gt;CLI: Install and start coding&lt;/h2&gt; 
&lt;p&gt;Install:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npm install -g codebuff
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd your-project
codebuff
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then just tell Codebuff what you want and it handles the rest:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;"Fix the SQL injection vulnerability in user registration"&lt;/li&gt; 
 &lt;li&gt;"Add rate limiting to all API endpoints"&lt;/li&gt; 
 &lt;li&gt;"Refactor the database connection code for better performance"&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Codebuff will find the right files, makes changes across your codebase, and runs tests to make sure nothing breaks.&lt;/p&gt; 
&lt;h2&gt;Create custom agents&lt;/h2&gt; 
&lt;p&gt;To get started building your own agents, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;codebuff init-agents
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can write agent definition files that give you maximum control over agent behavior.&lt;/p&gt; 
&lt;p&gt;Implement your workflows by specifying tools, which agents can be spawned, and prompts. We even have TypeScript generators for more programmatic control.&lt;/p&gt; 
&lt;p&gt;For example, here's a &lt;code&gt;git-committer&lt;/code&gt; agent that creates git commits based on the current git state. Notice that it runs &lt;code&gt;git diff&lt;/code&gt; and &lt;code&gt;git log&lt;/code&gt; to analyze changes, but then hands control over to the LLM to craft a meaningful commit message and perform the actual commit.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-typescript"&gt;export default {
  id: 'git-committer',
  displayName: 'Git Committer',
  model: 'openai/gpt-5-nano',
  toolNames: ['read_files', 'run_terminal_command', 'end_turn'],

  instructionsPrompt:
    'You create meaningful git commits by analyzing changes, reading relevant files for context, and crafting clear commit messages that explain the "why" behind changes.',

  async *handleSteps() {
    // Analyze what changed
    yield { tool: 'run_terminal_command', command: 'git diff' }
    yield { tool: 'run_terminal_command', command: 'git log --oneline -5' }

    // Stage files and create commit with good message
    yield 'STEP_ALL'
  },
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;SDK: Run agents in production&lt;/h2&gt; 
&lt;p&gt;Install the &lt;a href="https://www.npmjs.com/package/@codebuff/sdk"&gt;SDK package&lt;/a&gt; -- note this is different than the CLI codebuff package.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npm install @codebuff/sdk
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Import the client and run agents!&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-typescript"&gt;import { CodebuffClient } from '@codebuff/sdk'

// 1. Initialize the client
const client = new CodebuffClient({
  apiKey: 'your-api-key',
  cwd: '/path/to/your/project',
  onError: (error) =&amp;gt; console.error('Codebuff error:', error.message),
})

// 2. Do a coding task...
const result = await client.run({
  agent: 'base', // Codebuff's base coding agent
  prompt: 'Add comprehensive error handling to all API endpoints',
  handleEvent: (event) =&amp;gt; {
    console.log('Progress', event)
  },
})

// 3. Or, run a custom agent!
const myCustomAgent: AgentDefinition = {
  id: 'greeter',
  displayName: 'Greeter',
  model: 'openai/gpt-5',
  instructionsPrompt: 'Say hello!',
}
await client.run({
  agent: 'greeter',
  agentDefinitions: [myCustomAgent],
  prompt: 'My name is Bob.',
  customToolDefinitions: [], // Add custom tools too!
  handleEvent: (event) =&amp;gt; {
    console.log('Progress', event)
  },
})
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Learn more about the SDK &lt;a href="https://www.npmjs.com/package/@codebuff/sdk"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Why choose Codebuff&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Deep customizability&lt;/strong&gt;: Create sophisticated agent workflows with TypeScript generators that mix AI generation with programmatic control. Define custom agents that spawn subagents, implement conditional logic, and orchestrate complex multi-step processes that adapt to your specific use cases.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Any model on OpenRouter&lt;/strong&gt;: Unlike Claude Code which locks you into Anthropic's models, Codebuff supports any model available on &lt;a href="https://openrouter.ai/models"&gt;OpenRouter&lt;/a&gt; - from Claude and GPT to specialized models like Qwen, DeepSeek, and others. Switch models for different tasks or use the latest releases without waiting for platform updates.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Reuse any published agent&lt;/strong&gt;: Compose existing &lt;a href="https://www.codebuff.com/agents"&gt;published agents&lt;/a&gt; to get a leg up. Codebuff agents are the new MCP!&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Fully customizable SDK&lt;/strong&gt;: Build Codebuff's capabilities directly into your applications with a complete TypeScript SDK. Create custom tools, integrate with your CI/CD pipeline, build AI-powered development environments, or embed intelligent coding assistance into your products.&lt;/p&gt; 
&lt;h2&gt;Contributing to Codebuff&lt;/h2&gt; 
&lt;p&gt;We â¤ï¸ contributions from the community - whether you're fixing bugs, tweaking our agents, or improving documentation.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Want to contribute?&lt;/strong&gt; Check out our &lt;a href="https://raw.githubusercontent.com/CodebuffAI/codebuff/main/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt; to get started.&lt;/p&gt; 
&lt;p&gt;Some ways you can help:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ðŸ› &lt;strong&gt;Fix bugs&lt;/strong&gt; or add features&lt;/li&gt; 
 &lt;li&gt;ðŸ¤– &lt;strong&gt;Create specialized agents&lt;/strong&gt; and publish them to the Agent Store&lt;/li&gt; 
 &lt;li&gt;ðŸ“š &lt;strong&gt;Improve documentation&lt;/strong&gt; or write tutorials&lt;/li&gt; 
 &lt;li&gt;ðŸ’¡ &lt;strong&gt;Share ideas&lt;/strong&gt; in our &lt;a href="https://github.com/CodebuffAI/codebuff/issues"&gt;GitHub Issues&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Get started&lt;/h2&gt; 
&lt;h3&gt;Install&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;CLI&lt;/strong&gt;: &lt;code&gt;npm install -g codebuff&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;SDK&lt;/strong&gt;: &lt;code&gt;npm install @codebuff/sdk&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Resources&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Documentation&lt;/strong&gt;: &lt;a href="https://codebuff.com/docs"&gt;codebuff.com/docs&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Community&lt;/strong&gt;: &lt;a href="https://codebuff.com/discord"&gt;Discord&lt;/a&gt; - Join our friendly community&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Issues &amp;amp; Ideas&lt;/strong&gt;: &lt;a href="https://github.com/CodebuffAI/codebuff/issues"&gt;GitHub Issues&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Contributing&lt;/strong&gt;: &lt;a href="https://raw.githubusercontent.com/CodebuffAI/codebuff/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; - Start here to contribute!&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Support&lt;/strong&gt;: &lt;a href="mailto:support@codebuff.com"&gt;support@codebuff.com&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#CodebuffAI/codebuff&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=CodebuffAI/codebuff&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Zie619/n8n-workflows</title>
      <link>https://github.com/Zie619/n8n-workflows</link>
      <description>&lt;p&gt;all of the workflows of n8n i could find (also from the site itself)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;âš¡ N8N Workflow Collection &amp;amp; Documentation&lt;/h1&gt; 
&lt;p&gt;A professionally organized collection of &lt;strong&gt;2,053 n8n workflows&lt;/strong&gt; with a lightning-fast documentation system that provides instant search, analysis, and browsing capabilities.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;âš ï¸ IMPORTANT NOTICE (Aug 14, 2025):&lt;/strong&gt; Repository history has been rewritten due to DMCA compliance. If you have a fork or local clone, please see &lt;a href="https://github.com/Zie619/n8n-workflows/issues/85"&gt;Issue 85&lt;/a&gt; for instructions on syncing your copy.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Support My Work&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.buymeacoffee.com/zie619"&gt;&lt;img src="https://img.shields.io/badge/-Buy%20Me%20a%20Coffee-ffdd00?logo=buy-me-a-coffee&amp;amp;logoColor=black&amp;amp;style=flat" alt="Buy Me a Coffee" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;If you'd like to say thanks, consider buying me a coffeeâ€”your support helps me keep improving this project!&lt;/p&gt; 
&lt;h2&gt;ðŸš€ &lt;strong&gt;NEW: High-Performance Documentation System&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Experience 100x performance improvement over traditional documentation!&lt;/strong&gt;&lt;/p&gt; 
&lt;h3&gt;Quick Start - Fast Documentation System&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install dependencies
pip install -r requirements.txt

# Start the fast API server
python run.py

# Open in browser
http://localhost:8000
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Features:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;âš¡ &lt;strong&gt;Sub-100ms response times&lt;/strong&gt; with SQLite FTS5 search&lt;/li&gt; 
 &lt;li&gt;ðŸ” &lt;strong&gt;Instant full-text search&lt;/strong&gt; with advanced filtering&lt;/li&gt; 
 &lt;li&gt;ðŸ“± &lt;strong&gt;Responsive design&lt;/strong&gt; - works perfectly on mobile&lt;/li&gt; 
 &lt;li&gt;ðŸŒ™ &lt;strong&gt;Dark/light themes&lt;/strong&gt; with system preference detection&lt;/li&gt; 
 &lt;li&gt;ðŸ“Š &lt;strong&gt;Live statistics&lt;/strong&gt; - 365 unique integrations, 29,445 total nodes&lt;/li&gt; 
 &lt;li&gt;ðŸŽ¯ &lt;strong&gt;Smart categorization&lt;/strong&gt; by trigger type and complexity&lt;/li&gt; 
 &lt;li&gt;ðŸŽ¯ &lt;strong&gt;Use case categorization&lt;/strong&gt; by service name mapped to categories&lt;/li&gt; 
 &lt;li&gt;ðŸ“„ &lt;strong&gt;On-demand JSON viewing&lt;/strong&gt; and download&lt;/li&gt; 
 &lt;li&gt;ðŸ”— &lt;strong&gt;Mermaid diagram generation&lt;/strong&gt; for workflow visualization&lt;/li&gt; 
 &lt;li&gt;ðŸ”„ &lt;strong&gt;Real-time workflow naming&lt;/strong&gt; with intelligent formatting&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Performance Comparison&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Metric&lt;/th&gt; 
   &lt;th&gt;Old System&lt;/th&gt; 
   &lt;th&gt;New System&lt;/th&gt; 
   &lt;th&gt;Improvement&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;File Size&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;71MB HTML&lt;/td&gt; 
   &lt;td&gt;&amp;lt;100KB&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;700x smaller&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Load Time&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;10+ seconds&lt;/td&gt; 
   &lt;td&gt;&amp;lt;1 second&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;10x faster&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Search&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Client-side only&lt;/td&gt; 
   &lt;td&gt;Full-text with FTS5&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Instant&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Memory Usage&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;~2GB RAM&lt;/td&gt; 
   &lt;td&gt;&amp;lt;50MB RAM&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;40x less&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Mobile Support&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Poor&lt;/td&gt; 
   &lt;td&gt;Excellent&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Fully responsive&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ðŸ“‚ Repository Organization&lt;/h2&gt; 
&lt;h3&gt;Workflow Collection&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;2,053 workflows&lt;/strong&gt; with meaningful, searchable names&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;365 unique integrations&lt;/strong&gt; across popular platforms&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;29,445 total nodes&lt;/strong&gt; with professional categorization&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Quality assurance&lt;/strong&gt; - All workflows analyzed and categorized&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Advanced Naming System âœ¨&lt;/h3&gt; 
&lt;p&gt;Our intelligent naming system converts technical filenames into readable titles:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Before&lt;/strong&gt;: &lt;code&gt;2051_Telegram_Webhook_Automation_Webhook.json&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;After&lt;/strong&gt;: &lt;code&gt;Telegram Webhook Automation&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;100% meaningful names&lt;/strong&gt; with smart capitalization&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Automatic integration detection&lt;/strong&gt; from node analysis&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Use Case Category âœ¨&lt;/h3&gt; 
&lt;p&gt;The search interface includes a dropdown filter that lets you browse 2,000+ workflows by category.&lt;/p&gt; 
&lt;p&gt;The system includes an automated categorization feature that organizes workflows by service categories to make them easier to discover and filter.&lt;/p&gt; 
&lt;h3&gt;How Categorization Works&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Run the categorization script&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python create_categories.py
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Service Name Recognition&lt;/strong&gt; The script analyzes each workflow JSON filename to identify recognized service names (e.g., "Twilio", "Slack", "Gmail", etc.)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Category Mapping&lt;/strong&gt; Each recognized service name is matched to its corresponding category using the definitions in &lt;code&gt;context/def_categories.json&lt;/code&gt;. For example:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Twilio â†’ Communication &amp;amp; Messaging&lt;/li&gt; 
   &lt;li&gt;Gmail â†’ Communication &amp;amp; Messaging&lt;/li&gt; 
   &lt;li&gt;Airtable â†’ Data Processing &amp;amp; Analysis&lt;/li&gt; 
   &lt;li&gt;Salesforce â†’ CRM &amp;amp; Sales&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Search Categories Generation&lt;/strong&gt; The script produces a &lt;code&gt;search_categories.json&lt;/code&gt; file that contains the categorized workflow data&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Filter Interface&lt;/strong&gt; Users can then filter workflows by category in the search interface, making it easier to find workflows for specific use cases&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Available Categories&lt;/h3&gt; 
&lt;p&gt;The categorization system includes the following main categories:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;AI Agent Development&lt;/li&gt; 
 &lt;li&gt;Business Process Automation&lt;/li&gt; 
 &lt;li&gt;Cloud Storage &amp;amp; File Management&lt;/li&gt; 
 &lt;li&gt;Communication &amp;amp; Messaging&lt;/li&gt; 
 &lt;li&gt;Creative Content &amp;amp; Video Automation&lt;/li&gt; 
 &lt;li&gt;Creative Design Automation&lt;/li&gt; 
 &lt;li&gt;CRM &amp;amp; Sales&lt;/li&gt; 
 &lt;li&gt;Data Processing &amp;amp; Analysis&lt;/li&gt; 
 &lt;li&gt;E-commerce &amp;amp; Retail&lt;/li&gt; 
 &lt;li&gt;Financial &amp;amp; Accounting&lt;/li&gt; 
 &lt;li&gt;Marketing &amp;amp; Advertising Automation&lt;/li&gt; 
 &lt;li&gt;Project Management&lt;/li&gt; 
 &lt;li&gt;Social Media Management&lt;/li&gt; 
 &lt;li&gt;Technical Infrastructure &amp;amp; DevOps&lt;/li&gt; 
 &lt;li&gt;Web Scraping &amp;amp; Data Extraction&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Contribute Categories&lt;/h3&gt; 
&lt;p&gt;You can help expand the categorization by adding more service-to-category mappings (e.g., Twilio â†’ Communication &amp;amp; Messaging) in context/defs_categories.json.&lt;/p&gt; 
&lt;p&gt;Many workflow JSON files are conveniently named with the service name, often separated by underscores (_).&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ðŸ›  Usage Instructions&lt;/h2&gt; 
&lt;h3&gt;Option 1: Modern Fast System (Recommended)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone repository
git clone &amp;lt;repo-url&amp;gt;
cd n8n-workflows

# Install Python dependencies
pip install -r requirements.txt

# Start the documentation server
python run.py

# Browse workflows at http://localhost:8000
# - Instant search across 2,053 workflows
# - Professional responsive interface
# - Real-time workflow statistics
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Option 2: Development Mode&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Start with auto-reload for development
python run.py --dev

# Or specify custom host/port
python run.py --host 0.0.0.0 --port 3000

# Force database reindexing
python run.py --reindex
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Import Workflows into n8n&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Use the Python importer (recommended)
python import_workflows.py

# Or manually import individual workflows:
# 1. Open your n8n Editor UI
# 2. Click menu (â˜°) â†’ Import workflow
# 3. Choose any .json file from the workflows/ folder
# 4. Update credentials/webhook URLs before running
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ðŸ“Š Workflow Statistics&lt;/h2&gt; 
&lt;h3&gt;Current Collection Stats&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Total Workflows&lt;/strong&gt;: 2,053 automation workflows&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Active Workflows&lt;/strong&gt;: 215 (10.5% active rate)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Total Nodes&lt;/strong&gt;: 29,445 (avg 14.3 nodes per workflow)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Unique Integrations&lt;/strong&gt;: 365 different services and APIs&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Database&lt;/strong&gt;: SQLite with FTS5 full-text search&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Trigger Distribution&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Complex&lt;/strong&gt;: 831 workflows (40.5%) - Multi-trigger systems&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Webhook&lt;/strong&gt;: 519 workflows (25.3%) - API-triggered automations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Manual&lt;/strong&gt;: 477 workflows (23.2%) - User-initiated workflows&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Scheduled&lt;/strong&gt;: 226 workflows (11.0%) - Time-based executions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Complexity Analysis&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Low (â‰¤5 nodes)&lt;/strong&gt;: ~35% - Simple automations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Medium (6-15 nodes)&lt;/strong&gt;: ~45% - Standard workflows&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;High (16+ nodes)&lt;/strong&gt;: ~20% - Complex enterprise systems&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Popular Integrations&lt;/h3&gt; 
&lt;p&gt;Top services by usage frequency:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Communication&lt;/strong&gt;: Telegram, Discord, Slack, WhatsApp&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Cloud Storage&lt;/strong&gt;: Google Drive, Google Sheets, Dropbox&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Databases&lt;/strong&gt;: PostgreSQL, MySQL, MongoDB, Airtable&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;AI/ML&lt;/strong&gt;: OpenAI, Anthropic, Hugging Face&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Development&lt;/strong&gt;: HTTP Request, Webhook, GraphQL&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ðŸ” Advanced Search Features&lt;/h2&gt; 
&lt;h3&gt;Smart Search Categories&lt;/h3&gt; 
&lt;p&gt;Our system automatically categorizes workflows into 12 service categories:&lt;/p&gt; 
&lt;h4&gt;Available Categories:&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;messaging&lt;/strong&gt;: Telegram, Discord, Slack, WhatsApp, Teams&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ai_ml&lt;/strong&gt;: OpenAI, Anthropic, Hugging Face&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;database&lt;/strong&gt;: PostgreSQL, MySQL, MongoDB, Redis, Airtable&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;email&lt;/strong&gt;: Gmail, Mailjet, Outlook, SMTP/IMAP&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;cloud_storage&lt;/strong&gt;: Google Drive, Google Docs, Dropbox, OneDrive&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;project_management&lt;/strong&gt;: Jira, GitHub, GitLab, Trello, Asana&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;social_media&lt;/strong&gt;: LinkedIn, Twitter/X, Facebook, Instagram&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ecommerce&lt;/strong&gt;: Shopify, Stripe, PayPal&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;analytics&lt;/strong&gt;: Google Analytics, Mixpanel&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;calendar_tasks&lt;/strong&gt;: Google Calendar, Cal.com, Calendly&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;forms&lt;/strong&gt;: Typeform, Google Forms, Form Triggers&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;development&lt;/strong&gt;: Webhook, HTTP Request, GraphQL, SSE&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;API Usage Examples&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Search workflows by text
curl "http://localhost:8000/api/workflows?q=telegram+automation"

# Filter by trigger type and complexity
curl "http://localhost:8000/api/workflows?trigger=Webhook&amp;amp;complexity=high"

# Find all messaging workflows
curl "http://localhost:8000/api/workflows/category/messaging"

# Get database statistics
curl "http://localhost:8000/api/stats"

# Browse available categories
curl "http://localhost:8000/api/categories"
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ðŸ— Technical Architecture&lt;/h2&gt; 
&lt;h3&gt;Modern Stack&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;SQLite Database&lt;/strong&gt; - FTS5 full-text search with 365 indexed integrations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;FastAPI Backend&lt;/strong&gt; - RESTful API with automatic OpenAPI documentation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Responsive Frontend&lt;/strong&gt; - Modern HTML5 with embedded CSS/JavaScript&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Smart Analysis&lt;/strong&gt; - Automatic workflow categorization and naming&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Key Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Change Detection&lt;/strong&gt; - MD5 hashing for efficient re-indexing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Background Processing&lt;/strong&gt; - Non-blocking workflow analysis&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Compressed Responses&lt;/strong&gt; - Gzip middleware for optimal speed&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Error Handling&lt;/strong&gt; - Graceful degradation and comprehensive logging&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Mobile Optimization&lt;/strong&gt; - Touch-friendly interface design&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Database Performance&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sql"&gt;-- Optimized schema for lightning-fast queries
CREATE TABLE workflows (
    id INTEGER PRIMARY KEY,
    filename TEXT UNIQUE,
    name TEXT,
    active BOOLEAN,
    trigger_type TEXT,
    complexity TEXT,
    node_count INTEGER,
    integrations TEXT,  -- JSON array of 365 unique services
    description TEXT,
    file_hash TEXT,     -- MD5 for change detection
    analyzed_at TIMESTAMP
);

-- Full-text search with ranking
CREATE VIRTUAL TABLE workflows_fts USING fts5(
    filename, name, description, integrations, tags,
    content='workflows', content_rowid='id'
);
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ðŸ”§ Setup &amp;amp; Requirements&lt;/h2&gt; 
&lt;h3&gt;System Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Python 3.7+&lt;/strong&gt; - For running the documentation system&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Modern Browser&lt;/strong&gt; - Chrome, Firefox, Safari, Edge&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;50MB Storage&lt;/strong&gt; - For SQLite database and indexes&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;n8n Instance&lt;/strong&gt; - For importing and running workflows&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone repository
git clone &amp;lt;repo-url&amp;gt;
cd n8n-workflows

# Install dependencies
pip install -r requirements.txt

# Start documentation server
python run.py

# Access at http://localhost:8000
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Development Setup&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Create virtual environment
python3 -m venv .venv
source .venv/bin/activate  # Linux/Mac
# or .venv\Scripts\activate  # Windows

# Install dependencies
pip install -r requirements.txt

# Run with auto-reload for development
python api_server.py --reload

# Force database reindexing
python workflow_db.py --index --force
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ðŸ“‹ Naming Convention&lt;/h2&gt; 
&lt;h3&gt;Intelligent Formatting System&lt;/h3&gt; 
&lt;p&gt;Our system automatically converts technical filenames to user-friendly names:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Automatic transformations:
2051_Telegram_Webhook_Automation_Webhook.json â†’ "Telegram Webhook Automation"
0250_HTTP_Discord_Import_Scheduled.json â†’ "HTTP Discord Import Scheduled"  
0966_OpenAI_Data_Processing_Manual.json â†’ "OpenAI Data Processing Manual"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Technical Format&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;[ID]_[Service1]_[Service2]_[Purpose]_[Trigger].json
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Smart Capitalization Rules&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;HTTP&lt;/strong&gt; â†’ HTTP (not Http)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;API&lt;/strong&gt; â†’ API (not Api)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;webhook&lt;/strong&gt; â†’ Webhook&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;automation&lt;/strong&gt; â†’ Automation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;scheduled&lt;/strong&gt; â†’ Scheduled&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ðŸš€ API Documentation&lt;/h2&gt; 
&lt;h3&gt;Core Endpoints&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;GET /&lt;/code&gt; - Main workflow browser interface&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;GET /api/stats&lt;/code&gt; - Database statistics and metrics&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;GET /api/workflows&lt;/code&gt; - Search with filters and pagination&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;GET /api/workflows/{filename}&lt;/code&gt; - Detailed workflow information&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;GET /api/workflows/{filename}/download&lt;/code&gt; - Download workflow JSON&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;GET /api/workflows/{filename}/diagram&lt;/code&gt; - Generate Mermaid diagram&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Advanced Search&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;GET /api/workflows/category/{category}&lt;/code&gt; - Search by service category&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;GET /api/categories&lt;/code&gt; - List all available categories&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;GET /api/integrations&lt;/code&gt; - Get integration statistics&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;POST /api/reindex&lt;/code&gt; - Trigger background reindexing&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Response Examples&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;// GET /api/stats
{
  "total": 2053,
  "active": 215,
  "inactive": 1838,
  "triggers": {
    "Complex": 831,
    "Webhook": 519,
    "Manual": 477,
    "Scheduled": 226
  },
  "total_nodes": 29445,
  "unique_integrations": 365
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ðŸ¤ Contributing&lt;/h2&gt; 
&lt;h3&gt;Adding New Workflows&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Export workflow&lt;/strong&gt; as JSON from n8n&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Name descriptively&lt;/strong&gt; following the established pattern&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Add to workflows/&lt;/strong&gt; directory&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Remove sensitive data&lt;/strong&gt; (credentials, personal URLs)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Run reindexing&lt;/strong&gt; to update the database&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Quality Standards&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;âœ… Workflow must be functional and tested&lt;/li&gt; 
 &lt;li&gt;âœ… Remove all credentials and sensitive data&lt;/li&gt; 
 &lt;li&gt;âœ… Follow naming convention for consistency&lt;/li&gt; 
 &lt;li&gt;âœ… Verify compatibility with recent n8n versions&lt;/li&gt; 
 &lt;li&gt;âœ… Include meaningful description or comments&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;âš ï¸ Important Notes&lt;/h2&gt; 
&lt;h3&gt;Security &amp;amp; Privacy&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Review before use&lt;/strong&gt; - All workflows shared as-is for educational purposes&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Update credentials&lt;/strong&gt; - Replace API keys, tokens, and webhooks&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Test safely&lt;/strong&gt; - Verify in development environment first&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Check permissions&lt;/strong&gt; - Ensure proper access rights for integrations&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Compatibility&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;n8n Version&lt;/strong&gt; - Compatible with n8n 1.0+ (most workflows)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Community Nodes&lt;/strong&gt; - Some workflows may require additional node installations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;API Changes&lt;/strong&gt; - External services may have updated their APIs since creation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Dependencies&lt;/strong&gt; - Verify required integrations before importing&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ðŸ“š Resources &amp;amp; References&lt;/h2&gt; 
&lt;h3&gt;Workflow Sources&lt;/h3&gt; 
&lt;p&gt;This comprehensive collection includes workflows from:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Official n8n.io&lt;/strong&gt; - Documentation and community examples&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;GitHub repositories&lt;/strong&gt; - Open source community contributions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Blog posts &amp;amp; tutorials&lt;/strong&gt; - Real-world automation patterns&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;User submissions&lt;/strong&gt; - Tested and verified workflows&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Enterprise use cases&lt;/strong&gt; - Business process automations&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Learn More&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.n8n.io/"&gt;n8n Documentation&lt;/a&gt; - Official documentation&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://community.n8n.io/"&gt;n8n Community&lt;/a&gt; - Community forum and support&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://n8n.io/workflows/"&gt;Workflow Templates&lt;/a&gt; - Official template library&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.n8n.io/integrations/"&gt;Integration Docs&lt;/a&gt; - Service-specific guides&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ðŸ† Project Achievements&lt;/h2&gt; 
&lt;h3&gt;Repository Transformation&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;2,053 workflows&lt;/strong&gt; professionally organized and named&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;365 unique integrations&lt;/strong&gt; automatically detected and categorized&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;100% meaningful names&lt;/strong&gt; (improved from basic filename patterns)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Zero data loss&lt;/strong&gt; during intelligent renaming process&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Advanced search&lt;/strong&gt; with 12 service categories&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Performance Revolution&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Sub-100ms search&lt;/strong&gt; with SQLite FTS5 full-text indexing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Instant filtering&lt;/strong&gt; across 29,445 workflow nodes&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Mobile-optimized&lt;/strong&gt; responsive design for all devices&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Real-time statistics&lt;/strong&gt; with live database queries&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Professional interface&lt;/strong&gt; with modern UX principles&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;System Reliability&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Robust error handling&lt;/strong&gt; with graceful degradation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Change detection&lt;/strong&gt; for efficient database updates&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Background processing&lt;/strong&gt; for non-blocking operations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Comprehensive logging&lt;/strong&gt; for debugging and monitoring&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Production-ready&lt;/strong&gt; with proper middleware and security&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;em&gt;This repository represents the most comprehensive and well-organized collection of n8n workflows available, featuring cutting-edge search technology and professional documentation that makes workflow discovery and usage a delightful experience.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;ðŸŽ¯ Perfect for&lt;/strong&gt;: Developers, automation engineers, business analysts, and anyone looking to streamline their workflows with proven n8n automations.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/Zie619/n8n-workflows/main/README_ZH.md"&gt;ä¸­æ–‡&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>microsoft/markitdown</title>
      <link>https://github.com/microsoft/markitdown</link>
      <description>&lt;p&gt;Python tool for converting files and office documents to Markdown.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MarkItDown&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://pypi.org/project/markitdown/"&gt;&lt;img src="https://img.shields.io/pypi/v/markitdown.svg?sanitize=true" alt="PyPI" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/pypi/dd/markitdown" alt="PyPI - Downloads" /&gt; &lt;a href="https://github.com/microsoft/autogen"&gt;&lt;img src="https://img.shields.io/badge/Built%20by-AutoGen%20Team-blue" alt="Built by AutoGen Team" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] MarkItDown now offers an MCP (Model Context Protocol) server for integration with LLM applications like Claude Desktop. See &lt;a href="https://github.com/microsoft/markitdown/tree/main/packages/markitdown-mcp"&gt;markitdown-mcp&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] Breaking changes between 0.0.1 to 0.1.0:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Dependencies are now organized into optional feature-groups (further details below). Use &lt;code&gt;pip install 'markitdown[all]'&lt;/code&gt; to have backward-compatible behavior.&lt;/li&gt; 
  &lt;li&gt;convert_stream() now requires a binary file-like object (e.g., a file opened in binary mode, or an io.BytesIO object). This is a breaking change from the previous version, where it previously also accepted text file-like objects, like io.StringIO.&lt;/li&gt; 
  &lt;li&gt;The DocumentConverter class interface has changed to read from file-like streams rather than file paths. &lt;em&gt;No temporary files are created anymore&lt;/em&gt;. If you are the maintainer of a plugin, or custom DocumentConverter, you likely need to update your code. Otherwise, if only using the MarkItDown class or CLI (as in these examples), you should not need to change anything.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;MarkItDown is a lightweight Python utility for converting various files to Markdown for use with LLMs and related text analysis pipelines. To this end, it is most comparable to &lt;a href="https://github.com/deanmalmgren/textract"&gt;textract&lt;/a&gt;, but with a focus on preserving important document structure and content as Markdown (including: headings, lists, tables, links, etc.) While the output is often reasonably presentable and human-friendly, it is meant to be consumed by text analysis tools -- and may not be the best option for high-fidelity document conversions for human consumption.&lt;/p&gt; 
&lt;p&gt;MarkItDown currently supports the conversion from:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;PDF&lt;/li&gt; 
 &lt;li&gt;PowerPoint&lt;/li&gt; 
 &lt;li&gt;Word&lt;/li&gt; 
 &lt;li&gt;Excel&lt;/li&gt; 
 &lt;li&gt;Images (EXIF metadata and OCR)&lt;/li&gt; 
 &lt;li&gt;Audio (EXIF metadata and speech transcription)&lt;/li&gt; 
 &lt;li&gt;HTML&lt;/li&gt; 
 &lt;li&gt;Text-based formats (CSV, JSON, XML)&lt;/li&gt; 
 &lt;li&gt;ZIP files (iterates over contents)&lt;/li&gt; 
 &lt;li&gt;Youtube URLs&lt;/li&gt; 
 &lt;li&gt;EPubs&lt;/li&gt; 
 &lt;li&gt;... and more!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Why Markdown?&lt;/h2&gt; 
&lt;p&gt;Markdown is extremely close to plain text, with minimal markup or formatting, but still provides a way to represent important document structure. Mainstream LLMs, such as OpenAI's GPT-4o, natively "&lt;em&gt;speak&lt;/em&gt;" Markdown, and often incorporate Markdown into their responses unprompted. This suggests that they have been trained on vast amounts of Markdown-formatted text, and understand it well. As a side benefit, Markdown conventions are also highly token-efficient.&lt;/p&gt; 
&lt;h2&gt;Prerequisites&lt;/h2&gt; 
&lt;p&gt;MarkItDown requires Python 3.10 or higher. It is recommended to use a virtual environment to avoid dependency conflicts.&lt;/p&gt; 
&lt;p&gt;With the standard Python installation, you can create and activate a virtual environment using the following commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m venv .venv
source .venv/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If using &lt;code&gt;uv&lt;/code&gt;, you can create a virtual environment with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv venv --python=3.12 .venv
source .venv/bin/activate
# NOTE: Be sure to use 'uv pip install' rather than just 'pip install' to install packages in this virtual environment
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you are using Anaconda, you can create a virtual environment with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;conda create -n markitdown python=3.12
conda activate markitdown
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;To install MarkItDown, use pip: &lt;code&gt;pip install 'markitdown[all]'&lt;/code&gt;. Alternatively, you can install it from the source:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone git@github.com:microsoft/markitdown.git
cd markitdown
pip install -e 'packages/markitdown[all]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Command-Line&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown path-to-file.pdf &amp;gt; document.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or use &lt;code&gt;-o&lt;/code&gt; to specify the output file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown path-to-file.pdf -o document.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also pipe content:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cat path-to-file.pdf | markitdown
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Optional Dependencies&lt;/h3&gt; 
&lt;p&gt;MarkItDown has optional dependencies for activating various file formats. Earlier in this document, we installed all optional dependencies with the &lt;code&gt;[all]&lt;/code&gt; option. However, you can also install them individually for more control. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install 'markitdown[pdf, docx, pptx]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;will install only the dependencies for PDF, DOCX, and PPTX files.&lt;/p&gt; 
&lt;p&gt;At the moment, the following optional dependencies are available:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;[all]&lt;/code&gt; Installs all optional dependencies&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[pptx]&lt;/code&gt; Installs dependencies for PowerPoint files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[docx]&lt;/code&gt; Installs dependencies for Word files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[xlsx]&lt;/code&gt; Installs dependencies for Excel files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[xls]&lt;/code&gt; Installs dependencies for older Excel files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[pdf]&lt;/code&gt; Installs dependencies for PDF files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[outlook]&lt;/code&gt; Installs dependencies for Outlook messages&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[az-doc-intel]&lt;/code&gt; Installs dependencies for Azure Document Intelligence&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[audio-transcription]&lt;/code&gt; Installs dependencies for audio transcription of wav and mp3 files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[youtube-transcription]&lt;/code&gt; Installs dependencies for fetching YouTube video transcription&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Plugins&lt;/h3&gt; 
&lt;p&gt;MarkItDown also supports 3rd-party plugins. Plugins are disabled by default. To list installed plugins:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown --list-plugins
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To enable plugins use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown --use-plugins path-to-file.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To find available plugins, search GitHub for the hashtag &lt;code&gt;#markitdown-plugin&lt;/code&gt;. To develop a plugin, see &lt;code&gt;packages/markitdown-sample-plugin&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Azure Document Intelligence&lt;/h3&gt; 
&lt;p&gt;To use Microsoft Document Intelligence for conversion:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown path-to-file.pdf -o document.md -d -e "&amp;lt;document_intelligence_endpoint&amp;gt;"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;More information about how to set up an Azure Document Intelligence Resource can be found &lt;a href="https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/how-to-guides/create-document-intelligence-resource?view=doc-intel-4.0.0"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Python API&lt;/h3&gt; 
&lt;p&gt;Basic usage in Python:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from markitdown import MarkItDown

md = MarkItDown(enable_plugins=False) # Set to True to enable plugins
result = md.convert("test.xlsx")
print(result.text_content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Document Intelligence conversion in Python:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from markitdown import MarkItDown

md = MarkItDown(docintel_endpoint="&amp;lt;document_intelligence_endpoint&amp;gt;")
result = md.convert("test.pdf")
print(result.text_content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To use Large Language Models for image descriptions (currently only for pptx and image files), provide &lt;code&gt;llm_client&lt;/code&gt; and &lt;code&gt;llm_model&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from markitdown import MarkItDown
from openai import OpenAI

client = OpenAI()
md = MarkItDown(llm_client=client, llm_model="gpt-4o", llm_prompt="optional custom prompt")
result = md.convert("example.jpg")
print(result.text_content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Docker&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;docker build -t markitdown:latest .
docker run --rm -i markitdown:latest &amp;lt; ~/your-file.pdf &amp;gt; output.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href="https://cla.opensource.microsoft.com"&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; 
&lt;p&gt;This project has adopted the &lt;a href="https://opensource.microsoft.com/codeofconduct/"&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href="https://opensource.microsoft.com/codeofconduct/faq/"&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href="mailto:opencode@microsoft.com"&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; 
&lt;h3&gt;How to Contribute&lt;/h3&gt; 
&lt;p&gt;You can help by looking at issues or helping review PRs. Any issue or PR is welcome, but we have also marked some as 'open for contribution' and 'open for reviewing' to help facilitate community contributions. These are of course just suggestions and you are welcome to contribute in any way you like.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;&lt;/th&gt; 
    &lt;th&gt;All&lt;/th&gt; 
    &lt;th&gt;Especially Needs Help from Community&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Issues&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/markitdown/issues"&gt;All Issues&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/markitdown/issues?q=is%3Aissue+is%3Aopen+label%3A%22open+for+contribution%22"&gt;Issues open for contribution&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;PRs&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/markitdown/pulls"&gt;All PRs&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/markitdown/pulls?q=is%3Apr+is%3Aopen+label%3A%22open+for+reviewing%22"&gt;PRs open for reviewing&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h3&gt;Running Tests and Checks&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Navigate to the MarkItDown package:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;cd packages/markitdown
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install &lt;code&gt;hatch&lt;/code&gt; in your environment and run tests:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;pip install hatch  # Other ways of installing hatch: https://hatch.pypa.io/dev/install/
hatch shell
hatch test
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;(Alternative) Use the Devcontainer which has all the dependencies installed:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;# Reopen the project in Devcontainer and run:
hatch test
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run pre-commit checks before submitting a PR: &lt;code&gt;pre-commit run --all-files&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Contributing 3rd-party Plugins&lt;/h3&gt; 
&lt;p&gt;You can also contribute by creating and sharing 3rd party plugins. See &lt;code&gt;packages/markitdown-sample-plugin&lt;/code&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Trademarks&lt;/h2&gt; 
&lt;p&gt;This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow &lt;a href="https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general"&gt;Microsoft's Trademark &amp;amp; Brand Guidelines&lt;/a&gt;. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>modelcontextprotocol/registry</title>
      <link>https://github.com/modelcontextprotocol/registry</link>
      <description>&lt;p&gt;A community driven registry service for Model Context Protocol (MCP) servers.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MCP Registry&lt;/h1&gt; 
&lt;p&gt;The MCP registry provides MCP clients with a list of MCP servers, like an app store for MCP servers.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/registry/main/docs/guides/publishing/publish-server.md"&gt;&lt;strong&gt;ðŸ“¤ Publish my MCP server&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://registry.modelcontextprotocol.io/docs"&gt;&lt;strong&gt;âš¡ï¸ Live API docs&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/registry/main/docs/explanations/ecosystem-vision.md"&gt;&lt;strong&gt;ðŸ‘€ Ecosystem vision&lt;/strong&gt;&lt;/a&gt; | ðŸ“– &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/registry/main/docs"&gt;Full documentation&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Development Status&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;2025-09-08 update&lt;/strong&gt;: The registry has launched in preview ðŸŽ‰ (&lt;a href="https://blog.modelcontextprotocol.io/posts/2025-09-08-mcp-registry-preview/"&gt;announcement blog post&lt;/a&gt;). While the system is now more stable, this is still a preview release and breaking changes or data resets may occur. A general availability (GA) release will follow later. We'd love your feedback in &lt;a href="https://github.com/modelcontextprotocol/registry/discussions/new?category=ideas"&gt;GitHub discussions&lt;/a&gt; or in the &lt;a href="https://discord.com/channels/1358869848138059966/1369487942862504016"&gt;#registry-dev Discord&lt;/a&gt; (&lt;a href="https://modelcontextprotocol.io/community/communication"&gt;joining details here&lt;/a&gt;).&lt;/p&gt; 
&lt;p&gt;Current key maintainers:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Adam Jones&lt;/strong&gt; (Anthropic) &lt;a href="https://github.com/domdomegg"&gt;@domdomegg&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Tadas Antanavicius&lt;/strong&gt; (PulseMCP) &lt;a href="https://github.com/tadasant"&gt;@tadasant&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Toby Padilla&lt;/strong&gt; (GitHub) &lt;a href="https://github.com/toby"&gt;@toby&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We use multiple channels for collaboration - see &lt;a href="https://modelcontextprotocol.io/community/communication"&gt;modelcontextprotocol.io/community/communication&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Often (but not always) ideas flow through this pipeline:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://modelcontextprotocol.io/community/communication"&gt;Discord&lt;/a&gt;&lt;/strong&gt; - Real-time community discussions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/modelcontextprotocol/registry/discussions"&gt;Discussions&lt;/a&gt;&lt;/strong&gt; - Propose and discuss product/technical requirements&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/modelcontextprotocol/registry/issues"&gt;Issues&lt;/a&gt;&lt;/strong&gt; - Track well-scoped technical work&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/modelcontextprotocol/registry/pulls"&gt;Pull Requests&lt;/a&gt;&lt;/strong&gt; - Contribute work towards issues&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Quick start:&lt;/h3&gt; 
&lt;h4&gt;Pre-requisites&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Docker&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Go 1.24.x&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;golangci-lint v2.4.0&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Running the server&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Start full development environment
make dev-compose
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This starts the registry at &lt;a href="http://localhost:8080"&gt;&lt;code&gt;localhost:8080&lt;/code&gt;&lt;/a&gt; with PostgreSQL and seed data. It can be configured with environment variables in &lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/registry/main/docker-compose.yml"&gt;docker-compose.yml&lt;/a&gt; - see &lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/registry/main/.env.example"&gt;.env.example&lt;/a&gt; for a reference.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Alternative: Local setup without Docker&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;Prerequisites:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;PostgreSQL running locally&lt;/li&gt; 
  &lt;li&gt;Go 1.24.x installed&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Build and run locally
make build
make dev-local
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;The service runs on &lt;a href="http://localhost:8080"&gt;&lt;code&gt;localhost:8080&lt;/code&gt;&lt;/a&gt; by default. This can be configured with environment variables in &lt;code&gt;.env&lt;/code&gt; - see &lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/registry/main/.env.example"&gt;.env.example&lt;/a&gt; for a reference.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Alternative: Running a pre-built Docker image&lt;/summary&gt; 
 &lt;p&gt;Pre-built Docker images are automatically published to GitHub Container Registry:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Run latest stable release
docker run -p 8080:8080 ghcr.io/modelcontextprotocol/registry:latest

# Run latest from main branch (continuous deployment)
docker run -p 8080:8080 ghcr.io/modelcontextprotocol/registry:main

# Run specific release version
docker run -p 8080:8080 ghcr.io/modelcontextprotocol/registry:v1.0.0

# Run development build from main branch
docker run -p 8080:8080 ghcr.io/modelcontextprotocol/registry:main-20250906-abc123d
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Available tags:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Releases&lt;/strong&gt;: &lt;code&gt;latest&lt;/code&gt;, &lt;code&gt;v1.0.0&lt;/code&gt;, &lt;code&gt;v1.1.0&lt;/code&gt;, etc.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Continuous&lt;/strong&gt;: &lt;code&gt;main&lt;/code&gt; (latest main branch build)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Development&lt;/strong&gt;: &lt;code&gt;main-&amp;lt;date&amp;gt;-&amp;lt;sha&amp;gt;&lt;/code&gt; (specific commit builds)&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h4&gt;Publishing a server&lt;/h4&gt; 
&lt;p&gt;To publish a server, we've built a simple CLI. You can use it with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Build the latest CLI
make publisher

# Use it!
./bin/mcp-publisher --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/registry/main/docs/guides/publishing/publish-server.md"&gt;the publisher guide&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h4&gt;Other commands&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run lint, unit tests and integration tests
make check
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;There are also a few more helpful commands for development. Run &lt;code&gt;make help&lt;/code&gt; to learn more, or look in &lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/registry/main/Makefile"&gt;Makefile&lt;/a&gt;.&lt;/p&gt; 
&lt;!--
For Claude and other AI tools: Always prefer make targets over custom commands where possible.
--&gt; 
&lt;h2&gt;Architecture&lt;/h2&gt; 
&lt;h3&gt;Project Structure&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;â”œâ”€â”€ cmd/                     # Application entry points
â”‚   â””â”€â”€ publisher/           # Server publishing tool
â”œâ”€â”€ data/                    # Seed data
â”œâ”€â”€ deploy/                  # Deployment configuration (Pulumi)
â”œâ”€â”€ docs/                    # Documentation
â”œâ”€â”€ internal/                # Private application code
â”‚   â”œâ”€â”€ api/                 # HTTP handlers and routing
â”‚   â”œâ”€â”€ auth/                # Authentication (GitHub OAuth, JWT, namespace blocking)
â”‚   â”œâ”€â”€ config/              # Configuration management
â”‚   â”œâ”€â”€ database/            # Data persistence (PostgreSQL, in-memory)
â”‚   â”œâ”€â”€ service/             # Business logic
â”‚   â”œâ”€â”€ telemetry/           # Metrics and monitoring
â”‚   â””â”€â”€ validators/          # Input validation
â”œâ”€â”€ pkg/                     # Public packages
â”‚   â”œâ”€â”€ api/                 # API types and structures
â”‚   â”‚   â””â”€â”€ v0/              # Version 0 API types
â”‚   â””â”€â”€ model/               # Data models for server.json
â”œâ”€â”€ scripts/                 # Development and testing scripts
â”œâ”€â”€ tests/                   # Integration tests
â””â”€â”€ tools/                   # CLI tools and utilities
    â””â”€â”€ validate-*.sh        # Schema validation tools
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Authentication&lt;/h3&gt; 
&lt;p&gt;Publishing supports multiple authentication methods:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;GitHub OAuth&lt;/strong&gt; - For publishing by logging into GitHub&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;GitHub OIDC&lt;/strong&gt; - For publishing from GitHub Actions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;DNS verification&lt;/strong&gt; - For proving ownership of a domain and its subdomains&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;HTTP verification&lt;/strong&gt; - For proving ownership of a domain&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The registry validates namespace ownership when publishing. E.g. to publish...:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;io.github.domdomegg/my-cool-mcp&lt;/code&gt; you must login to GitHub as &lt;code&gt;domdomegg&lt;/code&gt;, or be in a GitHub Action on domdomegg's repos&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;me.adamjones/my-cool-mcp&lt;/code&gt; you must prove ownership of &lt;code&gt;adamjones.me&lt;/code&gt; via DNS or HTTP challenge&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;More documentation&lt;/h2&gt; 
&lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/registry/main/docs"&gt;documentation&lt;/a&gt; for more details if your question has not been answered here!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>trueadm/ripple</title>
      <link>https://github.com/trueadm/ripple</link>
      <description>&lt;p&gt;the elegant TypeScript UI framework&lt;/p&gt;&lt;hr&gt;&lt;a href="https://ripplejs.com"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="assets/ripple-dark.png" /&gt; 
  &lt;img src="https://raw.githubusercontent.com/trueadm/ripple/main/assets/ripple-light.png" alt="Ripple - the elegant TypeScript UI framework" /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;p&gt;&lt;a href="https://github.com/trueadm/ripple/actions/workflows/ci.yml"&gt;&lt;img src="https://github.com/trueadm/ripple/actions/workflows/ci.yml/badge.svg?sanitize=true" alt="CI" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/JBF2ySrh2W"&gt;&lt;img src="https://img.shields.io/badge/Discord-Join%20Server-7289da?logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://stackblitz.com/github/trueadm/ripple/tree/main/templates/basic"&gt;&lt;img src="https://developer.stackblitz.com/img/open_in_stackblitz_small.svg?sanitize=true" alt="Open in StackBlitz" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;What is Ripple?&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Currently, this project is still in early development, and should not be used in production.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Ripple is a TypeScript UI framework that takes the best parts of React, Solid and Svelte and combines them into one package.&lt;/p&gt; 
&lt;p&gt;I wrote Ripple as a love letter for frontend web â€“ and this is largely a project that I built in less than a week, so it's very raw.&lt;/p&gt; 
&lt;p&gt;Personally, I (&lt;a href="https://github.com/trueadm"&gt;@trueadm&lt;/a&gt;) have been involved in some truly amazing frontend frameworks along their journeys â€“ from &lt;a href="https://github.com/infernojs/inferno"&gt;Inferno&lt;/a&gt;, where it all began, to &lt;a href="https://github.com/facebook/react"&gt;React&lt;/a&gt; and the journey of React Hooks, to creating &lt;a href="https://github.com/facebook/lexical"&gt;Lexical&lt;/a&gt;, to &lt;a href="https://github.com/sveltejs/svelte"&gt;Svelte 5&lt;/a&gt; and its new compiler and signal-based reactivity runtime. Along that journey, I collected ideas, and intriguing thoughts that may or may not pay off. Given my time between roles, I decided it was the best opportunity to try them out, and for open source to see what I was cooking.&lt;/p&gt; 
&lt;p&gt;Ripple was designed to be a JS/TS-first framework, rather than HTML-first. Ripple modules have their own &lt;code&gt;.ripple&lt;/code&gt; extension, and these modules fully support TypeScript. By introducing a new extension, it allows Ripple to invent its own superset language, which plays really nicely with TypeScript and JSX, but with a few interesting touches. In my experience, this has led to better DX not only for humans, but also for LLMs.&lt;/p&gt; 
&lt;p&gt;Right now, there will be plenty of bugs, things just won't work either and you'll find TODOs everywhere. At this stage, Ripple is more of an early alpha version of something that &lt;em&gt;might&lt;/em&gt; be, rather than something you should try and adopt. If anything, maybe some of the ideas can be shared and incubated back into other frameworks. There's also a lot of similarities with Svelte 5, and that's not by accident; that's because of my recent time working on Svelte 5.&lt;/p&gt; 
&lt;p&gt;If you'd like to know more, join the &lt;a href="https://discord.gg/JBF2ySrh2W"&gt;Ripple Discord&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Reactive State Management&lt;/strong&gt;: Built-in reactivity with &lt;code&gt;$&lt;/code&gt; prefixed variables and object properties&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Component-Based Architecture&lt;/strong&gt;: Clean, reusable components with props and children&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;JSX-like Syntax&lt;/strong&gt;: Familiar templating with Ripple-specific enhancements&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Performance&lt;/strong&gt;: Fine-grain rendering, with industry-leading performance and memory usage&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;TypeScript Support&lt;/strong&gt;: Full TypeScript integration with type checking&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;VSCode Integration&lt;/strong&gt;: Rich editor support with diagnostics, syntax highlighting, and IntelliSense&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Prettier Support&lt;/strong&gt;: Full Prettier formatting support for &lt;code&gt;.ripple&lt;/code&gt; modules&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Missing Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;SSR&lt;/strong&gt;: Ripple is currently an SPA only, this is because I haven't gotten around to it&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Types&lt;/strong&gt;: The codebase is very raw with limited types; we're getting around to it&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;h3&gt;Try Ripple&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;We're working hard on getting an online playground available. Watch this space!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;You can try Ripple now by using our basic Vite template either via &lt;a href="https://stackblitz.com/github/trueadm/ripple/tree/main/templates/basic"&gt;StackBlitz&lt;/a&gt;, or by running these commands in your terminal:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npx degit trueadm/ripple/templates/basic my-app
cd my-app
npm i # or yarn or pnpm
npm run dev # or yarn or pnpm
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;VSCode Extension&lt;/h2&gt; 
&lt;p&gt;The &lt;a href="https://marketplace.visualstudio.com/items?itemName=ripplejs.ripple-vscode-plugin"&gt;Ripple VSCode extension&lt;/a&gt; provides:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Syntax Highlighting&lt;/strong&gt; for &lt;code&gt;.ripple&lt;/code&gt; files&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Real-time Diagnostics&lt;/strong&gt; for compilation errors&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;TypeScript Integration&lt;/strong&gt; for type checking&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;IntelliSense&lt;/strong&gt; for autocompletion&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can find the extension on the VS Code Marketplace as &lt;a href="https://marketplace.visualstudio.com/items?itemName=ripplejs.ripple-vscode-plugin"&gt;&lt;code&gt;Ripple for VS Code&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You can also &lt;a href="https://github.com/trueadm/ripple/raw/refs/heads/main/packages/ripple-vscode-plugin/published/ripple-vscode-plugin.vsix"&gt;manually install the extension&lt;/a&gt; &lt;code&gt;.vsix&lt;/code&gt; that have been manually packaged.&lt;/p&gt; 
&lt;h3&gt;Mounting your app&lt;/h3&gt; 
&lt;p&gt;You can use the &lt;code&gt;mount&lt;/code&gt; API from the &lt;code&gt;ripple&lt;/code&gt; package to render your Ripple component, using the &lt;code&gt;target&lt;/code&gt; option to specify what DOM element you want to render the component.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-ts"&gt;// index.ts
import { mount } from 'ripple';
import { App } from '/App.ripple';

mount(App, {
  props: {
    title: 'Hello world!',
  },
  target: document.getElementById('root'),
});
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Key Concepts&lt;/h2&gt; 
&lt;h3&gt;Components&lt;/h3&gt; 
&lt;p&gt;Define reusable components with the &lt;code&gt;component&lt;/code&gt; keyword. These are similar to functions in that they have &lt;code&gt;props&lt;/code&gt;, but crucially, they allow for a JSX-like syntax to be defined alongside standard TypeScript. That means you do not &lt;em&gt;return JSX&lt;/em&gt; like in other frameworks, but you instead use it like a JavaScript statement, as shown:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;component Button(props: { text: string, onClick: () =&amp;gt; void }) {
  &amp;lt;button onClick={props.onClick}&amp;gt;
    {props.text}
  &amp;lt;/button&amp;gt;
}

// Usage
export component App() {
  &amp;lt;Button text="Click me" onClick={() =&amp;gt; console.log("Clicked!")} /&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Ripple's templating language also supports shorthands and object spreads too:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-svelte"&gt;// you can do a normal prop
&amp;lt;div onClick={onClick}&amp;gt;{text}&amp;lt;/div&amp;gt;

// or using the shorthand prop
&amp;lt;div {onClick}&amp;gt;{text}&amp;lt;/div&amp;gt;

// and you can spread props
&amp;lt;div {...properties}&amp;gt;{text}&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Reactive Variables&lt;/h3&gt; 
&lt;p&gt;Variables prefixed with &lt;code&gt;$&lt;/code&gt; are automatically reactive:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-ts"&gt;let $name = 'World';
let $count = 0;

// Updates automatically trigger re-renders
$count++;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Object properties prefixed with &lt;code&gt;$&lt;/code&gt; are also automatically reactive:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-ts"&gt;let counter = { $current: 0 };

// Updates automatically trigger re-renders
counter.$current++;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Derived values are simply &lt;code&gt;$&lt;/code&gt; variables that combined different parts of state:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-ts"&gt;let $count = 0;
let $double = $count * 2;
let $quadruple = $double * 2;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;That means &lt;code&gt;$count&lt;/code&gt; itself might be derived if it were to reference another reactive property. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;component Counter({ $startingCount }) {
  let $count = $startingCount;
  let $double = $count * 2;
  let $quadruple = $double * 2;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Now given &lt;code&gt;$startingCount&lt;/code&gt; is reactive, it would mean that &lt;code&gt;$count&lt;/code&gt; might reset each time an incoming change to &lt;code&gt;$startingCount&lt;/code&gt; occurs. That might not be desirable, so Ripple provides a way to &lt;code&gt;untrack&lt;/code&gt; reactivity in those cases:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;import { untrack } from 'ripple';

component Counter({ $startingCount }) {
  let $count = untrack(() =&amp;gt; $startingCount);
  let $double = $count * 2;
  let $quadruple = $double * 2;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Now &lt;code&gt;$count&lt;/code&gt; will only reactively create its value on initialization.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Note: you cannot define reactive variables in module/global scope, they have to be created on access from an active component&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Transporting Reactivity&lt;/h4&gt; 
&lt;p&gt;Ripple doesn't constrain reactivity to components only. Reactivity can be used inside other functions (and classes in the future) and be composed in a way to improve expressivity and co-location.&lt;/p&gt; 
&lt;p&gt;Ripple provides a very nice way to transport reactivity between boundaries so that it's persisted â€“ using objects and arrays. Here's an example using arrays to transport reactivity:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;import { effect } from 'ripple';

function createDouble([ $count ]) {
  const $double = $count * 2;

  effect(() =&amp;gt; {
    console.log('Count:', $count)
  });

  return [ $double ];
}

export component App() {
  let $count = 0;

  const [ $double ] = createDouble([ $count ]);

  &amp;lt;div&amp;gt;{'Double: ' + $double}&amp;lt;/div&amp;gt;
  &amp;lt;button onClick={() =&amp;gt; { $count++; }}&amp;gt;{'Increment'}&amp;lt;/button&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can do the same with objects too:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;import { effect } from 'ripple';

function createDouble({ $count }) {
  const $double = $count * 2;

  effect(() =&amp;gt; {
    console.log('Count:', $count)
  });

  return { $double };
}

export component App() {
  let $count = 0;
  const { $double } = createDouble({ $count });

  &amp;lt;div&amp;gt;{'Double: ' + $double}&amp;lt;/div&amp;gt;
  &amp;lt;button onClick={() =&amp;gt; { $count++; }}&amp;gt;{'Increment'}&amp;lt;/button&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Just remember, reactive state must be connected to a component and it can't be global or created within the top-level of a module â€“ because then Ripple won't be able to link it to your component tree.&lt;/p&gt; 
&lt;h4&gt;Reactive Arrays&lt;/h4&gt; 
&lt;p&gt;Just like, objects, you can use the &lt;code&gt;$&lt;/code&gt; prefix in an array literal to specify that the field is reactive.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-js"&gt;let $first = 0;
let $second = 0;
const arr = [$first, $second];

const $total = arr.reduce((a, b) =&amp;gt; a + b, 0);
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Like shown in the above example, you can compose normal arrays with reactivity and pass them through props or boundaries.&lt;/p&gt; 
&lt;p&gt;However, if you need the entire array to be fully reactive, including when new elements get added, you should use the reactive array that Ripple provides.&lt;/p&gt; 
&lt;p&gt;You'll need to import the &lt;code&gt;RippleArray&lt;/code&gt; class from Ripple. It extends the standard JS &lt;code&gt;Array&lt;/code&gt; class, and supports all of its methods and properties.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-js"&gt;import { RippleArray } from 'ripple';

// using the new constructor
const arr = new RippleArray(1, 2, 3);

// using static from method
const arr = RippleArray.from([1, 2, 3]);

// using static of method
const arr = RippleArray.of(1, 2, 3);
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The &lt;code&gt;RippleArray&lt;/code&gt; is a reactive array, and that means you can access properties normally using numeric index. However, accessing the &lt;code&gt;length&lt;/code&gt; property of a &lt;code&gt;RippleArray&lt;/code&gt; will be not be reactive, instead you should use &lt;code&gt;$length&lt;/code&gt;.&lt;/p&gt; 
&lt;h4&gt;Reactive Set&lt;/h4&gt; 
&lt;p&gt;The &lt;code&gt;RippleSet&lt;/code&gt; extends the standard JS &lt;code&gt;Set&lt;/code&gt; class, and supports all of its methods and properties. However, accessing the &lt;code&gt;size&lt;/code&gt; property of a &lt;code&gt;RippleSet&lt;/code&gt; will be not be reactive, instead you should use &lt;code&gt;$size&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-js"&gt;import { RippleSet } from 'ripple';

const set = new RippleSet([1, 2, 3]);
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;RippleSet's reactive methods or properties can be used directly or assigned to reactive variables.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;import { RippleSet } from 'ripple';

export component App() {
  const set = new RippleSet([1, 2, 3]);

  // direct usage
  &amp;lt;p&amp;gt;{"Direct usage: set contains 2: "}{set.has(2)}&amp;lt;/p&amp;gt;

  // reactive assignment with prefixed `$`
  let $has = set.has(2);
  &amp;lt;p&amp;gt;{"Assigned usage: set contains 2: "}{$has}&amp;lt;/p&amp;gt;

  &amp;lt;button onClick={() =&amp;gt; set.delete(2)}&amp;gt;{"Delete 2"}&amp;lt;/button&amp;gt;
  &amp;lt;button onClick={() =&amp;gt; set.add(2)}&amp;gt;{"Add 2"}&amp;lt;/button&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Reactive Map&lt;/h4&gt; 
&lt;p&gt;The &lt;code&gt;RippleMap&lt;/code&gt; extends the standard JS &lt;code&gt;Map&lt;/code&gt; class, and supports all of its methods and properties. However, accessing the &lt;code&gt;size&lt;/code&gt; property of a &lt;code&gt;RippleMap&lt;/code&gt; will be not be reactive, instead you should use &lt;code&gt;$size&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-js"&gt;import { RippleMap } from 'ripple';

const map = new RippleMap([[1,1], [2,2], [3,3], [4,4]]);
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;RippleMap's reactive methods or properties can be used directly or assigned to reactive variables.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;import { RippleMap } from 'ripple';

export component App() {
  const map = new RippleMap([[1,1], [2,2], [3,3], [4,4]]);

  // direct usage
  &amp;lt;p&amp;gt;{"Direct usage: map has an item with key 2: "}{map.has(2)}&amp;lt;/p&amp;gt;

  // reactive assignment with prefixed `$`
  let $has = map.has(2);
  &amp;lt;p&amp;gt;{"Assigned usage: map has an item with key 2: "}{$has}&amp;lt;/p&amp;gt;

  &amp;lt;button onClick={() =&amp;gt; map.delete(2)}&amp;gt;{"Delete item with key 2"}&amp;lt;/button&amp;gt;
  &amp;lt;button onClick={() =&amp;gt; map.set(2, 2)}&amp;gt;{"Add key 2 with value 2"}&amp;lt;/button&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Effects&lt;/h3&gt; 
&lt;p&gt;When dealing with reactive state, you might want to be able to create side-effects based upon changes that happen upon updates. To do this, you can use &lt;code&gt;effect&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;import { effect } from 'ripple';

export component App() {
  let $count = 0;

  effect(() =&amp;gt; {
    console.log($count);
  });

  &amp;lt;button onClick={() =&amp;gt; $count++}&amp;gt;{'Increment'}&amp;lt;/button&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Control flow&lt;/h3&gt; 
&lt;p&gt;The JSX-like syntax might take some time to get used to if you're coming from another framework. For one, templating in Ripple can only occur &lt;em&gt;inside&lt;/em&gt; a &lt;code&gt;component&lt;/code&gt; body â€“ you can't create JSX inside functions, or assign it to variables as an expression.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;&amp;lt;div&amp;gt;
  // you can create variables inside the template!
  const str = "hello world";

  console.log(str); // and function calls too!

  debugger; // you can put breakpoints anywhere to help debugging!

  {str}
&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note that strings inside the template need to be inside &lt;code&gt;{"string"}&lt;/code&gt;, you can't do &lt;code&gt;&amp;lt;div&amp;gt;hello&amp;lt;/div&amp;gt;&lt;/code&gt; as Ripple has no idea if &lt;code&gt;hello&lt;/code&gt; is a string or maybe some JavaScript code that needs evaluating, so just ensure you wrap them in curly braces. This shouldn't be an issue in the real-world anyway, as you'll likely use an i18n library that means using JavaScript expressions regardless.&lt;/p&gt; 
&lt;h3&gt;If statements&lt;/h3&gt; 
&lt;p&gt;If blocks work seamlessly with Ripple's templating language, you can put them inside the JSX-like statements, making control-flow far easier to read and reason with.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;component Truthy({ x }) {
  &amp;lt;div&amp;gt;
    if (x) {
      &amp;lt;span&amp;gt;{'x is truthy'}&amp;lt;/span&amp;gt;
    } else {
      &amp;lt;span&amp;gt;{'x is falsy'}&amp;lt;/span&amp;gt;
    }
  &amp;lt;/div&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;For statements&lt;/h3&gt; 
&lt;p&gt;You can render collections using a &lt;code&gt;for...of&lt;/code&gt; block, and you don't need to specify a &lt;code&gt;key&lt;/code&gt; prop like other frameworks.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;component ListView({ title, items }) {
  &amp;lt;h2&amp;gt;{title}&amp;lt;/h2&amp;gt;
  &amp;lt;ul&amp;gt;
    for (const item of items) {
      &amp;lt;li&amp;gt;{item.text}&amp;lt;/li&amp;gt;
    }
  &amp;lt;/ul&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can use Ripple's reactive arrays to easily compose contents of an array.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;import { RippleArray } from 'ripple';

component Numbers() {
  const items = new RippleArray(1, 2, 3);

  for (const item of items) {
    &amp;lt;div&amp;gt;{item}&amp;lt;/div&amp;gt;
  }

  &amp;lt;button onClick={() =&amp;gt; items.push(`Item ${items.$length + 1}`)}&amp;gt;{"Add Item"}&amp;lt;/button&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Clicking the &lt;code&gt;&amp;lt;button&amp;gt;&lt;/code&gt; will create a new item, note that &lt;code&gt;items&lt;/code&gt; is not &lt;code&gt;$&lt;/code&gt; prefixed, because it's not reactive, but rather its properties are instead.&lt;/p&gt; 
&lt;h3&gt;Try statements&lt;/h3&gt; 
&lt;p&gt;Try blocks work to build the foundation for &lt;strong&gt;error boundaries&lt;/strong&gt;, when the runtime encounters an error in the &lt;code&gt;try&lt;/code&gt; block, you can easily render a fallback in the &lt;code&gt;catch&lt;/code&gt; block.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;import { reportError } from 'some-library';

component ErrorBoundary() {
  &amp;lt;div&amp;gt;
    try {
      &amp;lt;ComponentThatFails /&amp;gt;
    } catch (e) {
      reportError(e);

      &amp;lt;div&amp;gt;{'An error occurred! ' + e.message}&amp;lt;/div&amp;gt;
    }
  &amp;lt;/div&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Props&lt;/h3&gt; 
&lt;p&gt;If you want a prop to be reactive, you should also give it a &lt;code&gt;$&lt;/code&gt; prefix.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;component Button(props: { $text: string, onClick: () =&amp;gt; void }) {
  &amp;lt;button onClick={props.onClick}&amp;gt;
    {props.$text}
  &amp;lt;/button&amp;gt;
}

// Usage
&amp;lt;Button $text={some_text} onClick={() =&amp;gt; console.log("Clicked!")} /&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This also applies to DOM elements, if you want an attribute or property to be reactive, it needs to have a &lt;code&gt;$&lt;/code&gt; prefix.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-tsx"&gt;&amp;lt;div $class={props.$someClass} $id={$someId}&amp;gt;
  {$someText}
&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Otherwise changes to the attribute or property will not be reactively updated.&lt;/p&gt; 
&lt;h3&gt;Children&lt;/h3&gt; 
&lt;p&gt;Use &lt;code&gt;$children&lt;/code&gt; prop and then use it in the form of &lt;code&gt;&amp;lt;$children /&amp;gt;&lt;/code&gt; for component composition.&lt;/p&gt; 
&lt;p&gt;When you pass in children to a component, it gets implicitly passed as the &lt;code&gt;$children&lt;/code&gt; prop, in the form of a component.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;import type { Component } from 'ripple';

component Card(props: { $children: Component }) {
  &amp;lt;div class="card"&amp;gt;
    &amp;lt;props.$children /&amp;gt;
  &amp;lt;/div&amp;gt;
}

// Usage
&amp;lt;Card&amp;gt;
  &amp;lt;p&amp;gt;{"Card content here"}&amp;lt;/p&amp;gt;
&amp;lt;/Card&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You could also explicitly write the same code as shown:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;import type { Component } from 'ripple';

component Card(props: { $children: Component }) {
  &amp;lt;div class="card"&amp;gt;
    &amp;lt;props.$children /&amp;gt;
  &amp;lt;/div&amp;gt;
}

// Usage with explicit component
&amp;lt;Card&amp;gt;
  component $children() {
    &amp;lt;p&amp;gt;{"Card content here"}&amp;lt;/p&amp;gt;
  }
&amp;lt;/Card&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Accessor Props&lt;/h3&gt; 
&lt;p&gt;When working with props on composite components (&lt;code&gt;&amp;lt;Foo&amp;gt;&lt;/code&gt; rather than &lt;code&gt;&amp;lt;div&amp;gt;&lt;/code&gt;), it can sometimes be difficult to debug why a certain value is a certain way. JavaScript gives us a way to do this on objects using the &lt;code&gt;get&lt;/code&gt; syntax:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-js"&gt;let name = 'Bob';

const object = {
  get name() {
    // I can easily debug when this property gets
    // access and track it easily
    console.log(name);
    return name;
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;So Ripple provides similar capabilities when working with composite components in a template, specifically using &lt;code&gt;$prop:={}&lt;/code&gt; rather than the typical &lt;code&gt;$prop={}&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;In fact, when you use an accessor, you must pass a function, and the prop must be &lt;code&gt;$&lt;/code&gt; prefixed, as Ripple considers accessor props as reactive:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;let $name = 'Bob';

const getName = () =&amp;gt; {
  // I can easily debug when this property gets
  // access and track it easily
  console.log(name);
  return $name;
};

&amp;lt;Person $name:={getName} /&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also inline the function too:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;let $name = 'Bob';

&amp;lt;Person $name:={() =&amp;gt; {
  // I can easily debug when this property gets
  // access and track it easily
  console.log(name);
  return $name;
}} /&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Furthermore, just like property accessors in JavaScript, Ripple provides a way of capturing the &lt;code&gt;set&lt;/code&gt; too, enabling two-way data-flow on composite component props. You just need to provide a second function after the first, separated using a comma:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;let $name = 'Bob';

const getName = () =&amp;gt; {
  return $name;
}

const setName = (newName) =&amp;gt; {
  $name = newName;
}

&amp;lt;Person $name:={getName, setName} /&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or an inlined version:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;let $name = 'Bob';

&amp;lt;Person $name:={() =&amp;gt; $name, (newName) =&amp;gt; $name = $newName} /&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Now changes in the &lt;code&gt;Person&lt;/code&gt; to its &lt;code&gt;props&lt;/code&gt; will propagate to its parent component:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;component Person(props) {
  const updateName = (newName) =&amp;gt; {
    props.$name = newName;
  }

  &amp;lt;NameInput onChange={updateName}&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Refs&lt;/h3&gt; 
&lt;p&gt;Ripple provides a consistent way to capture the underlying DOM element â€“ refs. Specifically, using the syntax &lt;code&gt;{ref fn}&lt;/code&gt; where &lt;code&gt;fn&lt;/code&gt; is a function that captures the DOM element. If you're familiar with other frameworks, then this is identical to &lt;code&gt;{@attach fn}&lt;/code&gt; in Svelte 5 and somewhat similar to &lt;code&gt;ref&lt;/code&gt; in React. The hook function will receive the reference to the underlying DOM element.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;export component App() {
  let $node;

  const divRef = (node) =&amp;gt; {
    $node = node;
    console.log("mounted", node);

    return () =&amp;gt; {
      $node = undefined;
      console.log("unmounted", node);
    };
  };

  &amp;lt;div {ref divRef}&amp;gt;{"Hello world"}&amp;lt;/div&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also create &lt;code&gt;{ref}&lt;/code&gt; functions inline.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;export component App() {
  let $node;

  &amp;lt;div {ref (node) =&amp;gt; {
    $node = node;
    return () =&amp;gt; $node = null;
  }}&amp;gt;{"Hello world"}&amp;lt;/div&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also use function factories to define properties, these are functions that return functions that do the same thing. However, you can use this pattern to pass reactive properties.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;import { fadeIn } from 'some-library';

export component App({ $ms }) {
  &amp;lt;div {ref fadeIn({ $ms })}&amp;gt;{"Hello world"}&amp;lt;/div&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Lastly, you can use refs on composite components.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;&amp;lt;Image {ref (node) =&amp;gt; console.log(node)} {...props} /&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;When passing refs to composite components (rather than HTML elements) as shown above, they will be passed a &lt;code&gt;Symbol&lt;/code&gt; property, as they are not named. This still means that it can be spread to HTML template elements later on and still work.&lt;/p&gt; 
&lt;h4&gt;createRefKey&lt;/h4&gt; 
&lt;p&gt;Creates a unique object key that will be recognised as a ref when the object is spread onto an element. This allows programmatic assignment of refs without relying directly on the &lt;code&gt;{ref ...}&lt;/code&gt; template syntax.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;import { createRefKey } from 'ripple';

export component App() {
  let $value = '';

  const props = {
    id: "example",
    $value,
    [createRefKey()]: (node) =&amp;gt; {
      const removeListener = node.addEventListener('input', (e) =&amp;gt; $value = e.target.value);

      return () =&amp;gt; {
        removeListener();
      }
    }
  };

  // applied to an element
  &amp;lt;input type="text" {...props} /&amp;gt;

  // with composite component
  &amp;lt;Input {...props} /&amp;gt;
}

component Input({ id, $value, ...rest }) {
  &amp;lt;input type="text" {id} {$value} {...rest} /&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Event Props&lt;/h3&gt; 
&lt;p&gt;Like React, events are props that start with &lt;code&gt;on&lt;/code&gt; and then continue with an uppercase character, such as:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;onClick&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;onPointerMove&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;onPointerDown&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;onKeyDown&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For &lt;code&gt;capture&lt;/code&gt; phase events, just add &lt;code&gt;Capture&lt;/code&gt; to the end of the prop name:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;onClickCapture&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;onPointerMoveCapture&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;onPointerDownCapture&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;onKeyDownCapture&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Note: Some events are automatically delegated where possible by Ripple to improve runtime performance.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Styling&lt;/h3&gt; 
&lt;p&gt;Ripple supports native CSS styling that is localized to the given component using the &lt;code&gt;&amp;lt;style&amp;gt;&lt;/code&gt; element.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;component MyComponent() {
  &amp;lt;div class="container"&amp;gt;&amp;lt;h1&amp;gt;{'Hello World'}&amp;lt;/h1&amp;gt;&amp;lt;/div&amp;gt;

  &amp;lt;style&amp;gt;
    .container {
      background: blue;
      padding: 1rem;
    }

    h1 {
      color: white;
      font-size: 2rem;
    }
  &amp;lt;/style&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Note: the &lt;code&gt;&amp;lt;style&amp;gt;&lt;/code&gt; element must be top-level within a &lt;code&gt;component&lt;/code&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Context&lt;/h3&gt; 
&lt;p&gt;Ripple has the concept of &lt;code&gt;context&lt;/code&gt; where a value or reactive object can be shared through the component tree â€“ like in other frameworks. This all happens from the &lt;code&gt;createContext&lt;/code&gt; function that is imported from &lt;code&gt;ripple&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;When you create a context, you can &lt;code&gt;get&lt;/code&gt; and &lt;code&gt;set&lt;/code&gt; the values, but this must happen within the component. Using them outside will result in an error being thrown.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;import { createContext } from 'ripple';

const MyContext = createContext(null);

component Child() {
  // Context is read in the Child component
  const value = MyContext.get(MyContext);

  // value is "Hello from context!"
  console.log(value);
}

component Parent() {
  const value = MyContext.get(MyContext);

  // Context is read in the Parent component, but hasn't yet
  // been set, so we fallback to the initial context value.
  // So the value is `null`
  console.log(value);

  // Context is set in the Parent component
  MyContext.set("Hello from context!");

  &amp;lt;Child /&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We are happy for your interest in contributing. Please see our &lt;a href="https://raw.githubusercontent.com/trueadm/ripple/main/CONTRIBUTING.md"&gt;contributing guidelines&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/trueadm/ripple/main/LICENSE"&gt;MIT license&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>PowerShell/PowerShell</title>
      <link>https://github.com/PowerShell/PowerShell</link>
      <description>&lt;p&gt;PowerShell for every system!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;img src="https://raw.githubusercontent.com/PowerShell/PowerShell/master/assets/ps_black_64.svg?sanitize=true" alt="logo" /&gt; PowerShell&lt;/h1&gt; 
&lt;p&gt;Welcome to the PowerShell GitHub Community! &lt;a href="https://learn.microsoft.com/powershell/scripting/overview"&gt;PowerShell&lt;/a&gt; is a cross-platform (Windows, Linux, and macOS) automation and configuration tool/framework that works well with your existing tools and is optimized for dealing with structured data (e.g. JSON, CSV, XML, etc.), REST APIs, and object models. It includes a command-line shell, an associated scripting language, and a framework for processing cmdlets.&lt;/p&gt; 
&lt;h2&gt;Windows PowerShell vs. PowerShell 7+&lt;/h2&gt; 
&lt;p&gt;Although this repository started as a fork of the Windows PowerShell codebase, changes made in this repository are not ported back to Windows PowerShell 5.1. This also means that &lt;a href="https://github.com/PowerShell/PowerShell/issues"&gt;issues tracked here&lt;/a&gt; are only for PowerShell 7.x and higher. Windows PowerShell specific issues should be reported with the &lt;a href="https://support.microsoft.com/windows/send-feedback-to-microsoft-with-the-feedback-hub-app-f59187f8-8739-22d6-ba93-f66612949332"&gt;Feedback Hub app&lt;/a&gt;, by choosing "Apps &amp;gt; PowerShell" in the category.&lt;/p&gt; 
&lt;h2&gt;New to PowerShell?&lt;/h2&gt; 
&lt;p&gt;If you are new to PowerShell and want to learn more, we recommend reviewing the &lt;a href="https://learn.microsoft.com/powershell/scripting/learn/more-powershell-learning"&gt;getting started&lt;/a&gt; documentation.&lt;/p&gt; 
&lt;h2&gt;Get PowerShell&lt;/h2&gt; 
&lt;p&gt;PowerShell is supported on Windows, macOS, and a variety of Linux platforms. For more information, see &lt;a href="https://learn.microsoft.com/powershell/scripting/install/installing-powershell"&gt;Installing PowerShell&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Upgrading PowerShell&lt;/h2&gt; 
&lt;p&gt;For best results when upgrading, you should use the same install method you used when you first installed PowerShell. The update method is different for each platform and install method.&lt;/p&gt; 
&lt;h2&gt;Community Dashboard&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://aka.ms/PSPublicDashboard"&gt;Dashboard&lt;/a&gt; with visualizations for community contributions and project status using PowerShell, Azure, and PowerBI.&lt;/p&gt; 
&lt;p&gt;For more information on how and why we built this dashboard, check out this &lt;a href="https://devblogs.microsoft.com/powershell/powershell-open-source-community-dashboard/"&gt;blog post&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Discussions&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://docs.github.com/discussions/quickstart"&gt;GitHub Discussions&lt;/a&gt; is a feature to enable free and open discussions within the community for topics that are not related to code, unlike issues.&lt;/p&gt; 
&lt;p&gt;This is an experiment we are trying in our repositories, to see if it helps move discussions out of issues so that issues remain actionable by the team or members of the community. There should be no expectation that PowerShell team members are regular participants in these discussions. Individual PowerShell team members may choose to participate in discussions, but the expectation is that community members help drive discussions so that team members can focus on issues.&lt;/p&gt; 
&lt;p&gt;Create or join a &lt;a href="https://github.com/PowerShell/PowerShell/discussions"&gt;discussion&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Chat&lt;/h2&gt; 
&lt;p&gt;Want to chat with other members of the PowerShell community?&lt;/p&gt; 
&lt;p&gt;There are dozens of topic-specific channels on our community-driven PowerShell Virtual User Group, which you can join on:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://gitter.im/PowerShell/PowerShell"&gt;Gitter&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://discord.gg/PowerShell"&gt;Discord&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://web.libera.chat/#powershell"&gt;IRC&lt;/a&gt; on Libera.Chat&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aka.ms/psslack"&gt;Slack&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Build status of nightly builds&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Azure CI (Windows)&lt;/th&gt; 
   &lt;th align="left"&gt;Azure CI (Linux)&lt;/th&gt; 
   &lt;th align="left"&gt;Azure CI (macOS)&lt;/th&gt; 
   &lt;th align="left"&gt;CodeFactor Grade&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://powershell.visualstudio.com/PowerShell/_build?definitionId=32"&gt;&lt;img src="https://powershell.visualstudio.com/PowerShell/_apis/build/status/PowerShell-CI-Windows-daily" alt="windows-nightly-image" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://powershell.visualstudio.com/PowerShell/_build?definitionId=23"&gt;&lt;img src="https://powershell.visualstudio.com/PowerShell/_apis/build/status/PowerShell-CI-linux-daily?branchName=master" alt="linux-nightly-image" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://powershell.visualstudio.com/PowerShell/_build?definitionId=24"&gt;&lt;img src="https://powershell.visualstudio.com/PowerShell/_apis/build/status/PowerShell-CI-macos-daily?branchName=master" alt="macOS-nightly-image" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://www.codefactor.io/repository/github/powershell/powershell"&gt;&lt;img src="https://www.codefactor.io/repository/github/powershell/powershell/badge" alt="cf-image" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Developing and Contributing&lt;/h2&gt; 
&lt;p&gt;Want to contribute to PowerShell? Please start with the &lt;a href="https://raw.githubusercontent.com/PowerShell/PowerShell/master/.github/CONTRIBUTING.md"&gt;Contribution Guide&lt;/a&gt; to learn how to develop and contribute.&lt;/p&gt; 
&lt;p&gt;If you are developing .NET Core C# applications targeting PowerShell Core, &lt;a href="https://raw.githubusercontent.com/PowerShell/PowerShell/master/docs/FAQ.md#where-do-i-get-the-powershell-core-sdk-package"&gt;check out our FAQ&lt;/a&gt; to learn more about the PowerShell SDK NuGet package.&lt;/p&gt; 
&lt;p&gt;Also, make sure to check out our &lt;a href="https://github.com/powershell/powershell-rfc"&gt;PowerShell-RFC repository&lt;/a&gt; for request-for-comments (RFC) documents to submit and give comments on proposed and future designs.&lt;/p&gt; 
&lt;h2&gt;Building PowerShell&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Linux&lt;/th&gt; 
   &lt;th&gt;Windows&lt;/th&gt; 
   &lt;th&gt;macOS&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/PowerShell/PowerShell/master/docs/building/linux.md"&gt;Instructions&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/PowerShell/PowerShell/master/docs/building/windows-core.md"&gt;Instructions&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/PowerShell/PowerShell/master/docs/building/macos.md"&gt;Instructions&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;If you have any problems building PowerShell, please start by consulting the developer &lt;a href="https://raw.githubusercontent.com/PowerShell/PowerShell/master/docs/FAQ.md"&gt;FAQ&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Downloading the Source Code&lt;/h2&gt; 
&lt;p&gt;You can clone the repository:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;git clone https://github.com/PowerShell/PowerShell.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more information, see &lt;a href="https://github.com/PowerShell/PowerShell/tree/master/docs/git"&gt;working with the PowerShell repository&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Support&lt;/h2&gt; 
&lt;p&gt;For support, see the &lt;a href="https://github.com/PowerShell/PowerShell/tree/master/.github/SUPPORT.md"&gt;Support Section&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Legal and Licensing&lt;/h2&gt; 
&lt;p&gt;PowerShell is licensed under the &lt;a href="https://github.com/PowerShell/PowerShell/tree/master/LICENSE.txt"&gt;MIT license&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Docker Containers&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Important] The PowerShell container images are now &lt;a href="https://github.com/PowerShell/Announcements/issues/75"&gt;maintained by the .NET team&lt;/a&gt;. The containers at &lt;code&gt;mcr.microsoft.com/powershell&lt;/code&gt; are currently not maintained.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;License: By requesting and using the Container OS Image for Windows containers, you acknowledge, understand, and consent to the Supplemental License Terms available on &lt;a href="https://mcr.microsoft.com/en-us/product/powershell/tags"&gt;Microsoft Artifact Registry&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Telemetry&lt;/h3&gt; 
&lt;p&gt;Please visit our &lt;a href="https://learn.microsoft.com/powershell/module/microsoft.powershell.core/about/about_telemetry"&gt;about_Telemetry&lt;/a&gt; topic to read details about telemetry gathered by PowerShell.&lt;/p&gt; 
&lt;h2&gt;Governance&lt;/h2&gt; 
&lt;p&gt;The governance policy for the PowerShell project is described the &lt;a href="https://github.com/PowerShell/PowerShell/raw/master/docs/community/governance.md"&gt;PowerShell Governance&lt;/a&gt; document.&lt;/p&gt; 
&lt;h2&gt;&lt;a href="https://raw.githubusercontent.com/PowerShell/PowerShell/master/CODE_OF_CONDUCT.md"&gt;Code of Conduct&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;Please see our &lt;a href="https://raw.githubusercontent.com/PowerShell/PowerShell/master/CODE_OF_CONDUCT.md"&gt;Code of Conduct&lt;/a&gt; before participating in this project.&lt;/p&gt; 
&lt;h2&gt;&lt;a href="https://raw.githubusercontent.com/PowerShell/PowerShell/master/.github/SECURITY.md"&gt;Security Policy&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;For any security issues, please see our &lt;a href="https://raw.githubusercontent.com/PowerShell/PowerShell/master/.github/SECURITY.md"&gt;Security Policy&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Physical-Intelligence/openpi</title>
      <link>https://github.com/Physical-Intelligence/openpi</link>
      <description>&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;openpi&lt;/h1&gt; 
&lt;p&gt;openpi holds open-source models and packages for robotics, published by the &lt;a href="https://www.physicalintelligence.company/"&gt;Physical Intelligence team&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Currently, this repo contains three types of models:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;the &lt;a href="https://www.physicalintelligence.company/blog/pi0"&gt;Ï€â‚€ model&lt;/a&gt;, a flow-based vision-language-action model (VLA).&lt;/li&gt; 
 &lt;li&gt;the &lt;a href="https://www.physicalintelligence.company/research/fast"&gt;Ï€â‚€-FAST model&lt;/a&gt;, an autoregressive VLA, based on the FAST action tokenizer.&lt;/li&gt; 
 &lt;li&gt;the &lt;a href="https://www.physicalintelligence.company/blog/pi05"&gt;Ï€â‚€.â‚… model&lt;/a&gt;, an upgraded version of Ï€â‚€ with better open-world generalization trained with &lt;a href="https://www.physicalintelligence.company/research/knowledge_insulation"&gt;knowledge insulation&lt;/a&gt;. Note that, in this repository, we currently only support the flow matching head for both $\pi_{0.5}$ training and inference.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For all models, we provide &lt;em&gt;base model&lt;/em&gt; checkpoints, pre-trained on 10k+ hours of robot data, and examples for using them out of the box or fine-tuning them to your own datasets.&lt;/p&gt; 
&lt;p&gt;This is an experiment: $\pi_0$ was developed for our own robots, which differ from the widely used platforms such as &lt;a href="https://tonyzhaozh.github.io/aloha/"&gt;ALOHA&lt;/a&gt; and &lt;a href="https://droid-dataset.github.io/"&gt;DROID&lt;/a&gt;, and though we are optimistic that researchers and practitioners will be able to run creative new experiments adapting $\pi_0$ to their own platforms, we do not expect every such attempt to be successful. All this is to say: $\pi_0$ may or may not work for you, but you are welcome to try it and see!&lt;/p&gt; 
&lt;h2&gt;Updates&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;[Sept 2025] We released PyTorch support in openpi.&lt;/li&gt; 
 &lt;li&gt;[Sept 2025] We released pi05, an upgraded version of pi0 with better open-world generalization.&lt;/li&gt; 
 &lt;li&gt;[Sept 2025]: We have added an &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/droid/README_train.md#data-filtering"&gt;improved idle filter&lt;/a&gt; for DROID training.&lt;/li&gt; 
 &lt;li&gt;[Jun 2025]: We have added &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/droid/README_train.md"&gt;instructions&lt;/a&gt; for using &lt;code&gt;openpi&lt;/code&gt; to train VLAs on the full &lt;a href="https://droid-dataset.github.io/"&gt;DROID dataset&lt;/a&gt;. This is an approximate open-source implementation of the training pipeline used to train pi0-FAST-DROID.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;p&gt;To run the models in this repository, you will need an NVIDIA GPU with at least the following specifications. These estimations assume a single GPU, but you can also use multiple GPUs with model parallelism to reduce per-GPU memory requirements by configuring &lt;code&gt;fsdp_devices&lt;/code&gt; in the training config. Please also note that the current training script does not yet support multi-node training.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Mode&lt;/th&gt; 
   &lt;th&gt;Memory Required&lt;/th&gt; 
   &lt;th&gt;Example GPU&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;&amp;gt; 8 GB&lt;/td&gt; 
   &lt;td&gt;RTX 4090&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Fine-Tuning (LoRA)&lt;/td&gt; 
   &lt;td&gt;&amp;gt; 22.5 GB&lt;/td&gt; 
   &lt;td&gt;RTX 4090&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Fine-Tuning (Full)&lt;/td&gt; 
   &lt;td&gt;&amp;gt; 70 GB&lt;/td&gt; 
   &lt;td&gt;A100 (80GB) / H100&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;The repo has been tested with Ubuntu 22.04, we do not currently support other operating systems.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;When cloning this repo, make sure to update submodules:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone --recurse-submodules git@github.com:Physical-Intelligence/openpi.git

# Or if you already cloned the repo:
git submodule update --init --recursive
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We use &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt; to manage Python dependencies. See the &lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;uv installation instructions&lt;/a&gt; to set it up. Once uv is installed, run the following to set up the environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;GIT_LFS_SKIP_SMUDGE=1 uv sync
GIT_LFS_SKIP_SMUDGE=1 uv pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;NOTE: &lt;code&gt;GIT_LFS_SKIP_SMUDGE=1&lt;/code&gt; is needed to pull LeRobot as a dependency.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Docker&lt;/strong&gt;: As an alternative to uv installation, we provide instructions for installing openpi using Docker. If you encounter issues with your system setup, consider using Docker to simplify installation. See &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/docs/docker.md"&gt;Docker Setup&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Model Checkpoints&lt;/h2&gt; 
&lt;h3&gt;Base Models&lt;/h3&gt; 
&lt;p&gt;We provide multiple base VLA model checkpoints. These checkpoints have been pre-trained on 10k+ hours of robot data, and can be used for fine-tuning.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Use Case&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Checkpoint Path&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$&lt;/td&gt; 
   &lt;td&gt;Fine-Tuning&lt;/td&gt; 
   &lt;td&gt;Base &lt;a href="https://www.physicalintelligence.company/blog/pi0"&gt;Ï€â‚€ model&lt;/a&gt; for fine-tuning&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_base&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$-FAST&lt;/td&gt; 
   &lt;td&gt;Fine-Tuning&lt;/td&gt; 
   &lt;td&gt;Base autoregressive &lt;a href="https://www.physicalintelligence.company/research/fast"&gt;Ï€â‚€-FAST model&lt;/a&gt; for fine-tuning&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_fast_base&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_{0.5}$&lt;/td&gt; 
   &lt;td&gt;Fine-Tuning&lt;/td&gt; 
   &lt;td&gt;Base &lt;a href="https://www.physicalintelligence.company/blog/pi05"&gt;Ï€â‚€.â‚… model&lt;/a&gt; for fine-tuning&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi05_base&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Fine-Tuned Models&lt;/h3&gt; 
&lt;p&gt;We also provide "expert" checkpoints for various robot platforms and tasks. These models are fine-tuned from the base models above and intended to run directly on the target robot. These may or may not work on your particular robot. Since these checkpoints were fine-tuned on relatively small datasets collected with more widely available robots, such as ALOHA and the DROID Franka setup, they might not generalize to your particular setup, though we found some of these, especially the DROID checkpoint, to generalize quite broadly in practice.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Use Case&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Checkpoint Path&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$-FAST-DROID&lt;/td&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;$\pi_0$-FAST model fine-tuned on the &lt;a href="https://droid-dataset.github.io/"&gt;DROID dataset&lt;/a&gt;: can perform a wide range of simple table-top manipulation tasks 0-shot in new scenes on the DROID robot platform&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_fast_droid&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$-DROID&lt;/td&gt; 
   &lt;td&gt;Fine-Tuning&lt;/td&gt; 
   &lt;td&gt;$\pi_0$ model fine-tuned on the &lt;a href="https://droid-dataset.github.io/"&gt;DROID dataset&lt;/a&gt;: faster inference than $\pi_0$-FAST-DROID, but may not follow language commands as well&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_droid&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$-ALOHA-towel&lt;/td&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;$\pi_0$ model fine-tuned on internal &lt;a href="https://tonyzhaozh.github.io/aloha/"&gt;ALOHA&lt;/a&gt; data: can fold diverse towels 0-shot on ALOHA robot platforms&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_aloha_towel&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$-ALOHA-tupperware&lt;/td&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;$\pi_0$ model fine-tuned on internal &lt;a href="https://tonyzhaozh.github.io/aloha/"&gt;ALOHA&lt;/a&gt; data: can unpack food from a tupperware container&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_aloha_tupperware&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$-ALOHA-pen-uncap&lt;/td&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;$\pi_0$ model fine-tuned on public &lt;a href="https://dit-policy.github.io/"&gt;ALOHA&lt;/a&gt; data: can uncap a pen&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_aloha_pen_uncap&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_{0.5}$-LIBERO&lt;/td&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;$\pi_{0.5}$ model fine-tuned for the &lt;a href="https://libero-project.github.io/datasets"&gt;LIBERO&lt;/a&gt; benchmark: gets state-of-the-art performance (see &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/libero/README.md"&gt;LIBERO README&lt;/a&gt;)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi05_libero&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_{0.5}$-DROID&lt;/td&gt; 
   &lt;td&gt;Inference / Fine-Tuning&lt;/td&gt; 
   &lt;td&gt;$\pi_{0.5}$ model fine-tuned on the &lt;a href="https://droid-dataset.github.io/"&gt;DROID dataset&lt;/a&gt; with &lt;a href="https://www.physicalintelligence.company/research/knowledge_insulation"&gt;knowledge insulation&lt;/a&gt;: fast inference and good language-following&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi05_droid&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;By default, checkpoints are automatically downloaded from &lt;code&gt;gs://openpi-assets&lt;/code&gt; and are cached in &lt;code&gt;~/.cache/openpi&lt;/code&gt; when needed. You can overwrite the download path by setting the &lt;code&gt;OPENPI_DATA_HOME&lt;/code&gt; environment variable.&lt;/p&gt; 
&lt;h2&gt;Running Inference for a Pre-Trained Model&lt;/h2&gt; 
&lt;p&gt;Our pre-trained model checkpoints can be run with a few lines of code (here our $\pi_0$-FAST-DROID model):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openpi.training import config as _config
from openpi.policies import policy_config
from openpi.shared import download

config = _config.get_config("pi05_droid")
checkpoint_dir = download.maybe_download("gs://openpi-assets/checkpoints/pi05_droid")

# Create a trained policy.
policy = policy_config.create_trained_policy(config, checkpoint_dir)

# Run inference on a dummy example.
example = {
    "observation/exterior_image_1_left": ...,
    "observation/wrist_image_left": ...,
    ...
    "prompt": "pick up the fork"
}
action_chunk = policy.infer(example)["actions"]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also test this out in the &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/inference.ipynb"&gt;example notebook&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We provide detailed step-by-step examples for running inference of our pre-trained checkpoints on &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/droid/README.md"&gt;DROID&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/aloha_real/README.md"&gt;ALOHA&lt;/a&gt; robots.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Remote Inference&lt;/strong&gt;: We provide &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/docs/remote_inference.md"&gt;examples and code&lt;/a&gt; for running inference of our models &lt;strong&gt;remotely&lt;/strong&gt;: the model can run on a different server and stream actions to the robot via a websocket connection. This makes it easy to use more powerful GPUs off-robot and keep robot and policy environments separate.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Test inference without a robot&lt;/strong&gt;: We provide a &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/simple_client/README.md"&gt;script&lt;/a&gt; for testing inference without a robot. This script will generate a random observation and run inference with the model. See &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/simple_client/README.md"&gt;here&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Fine-Tuning Base Models on Your Own Data&lt;/h2&gt; 
&lt;p&gt;We will fine-tune the $\pi_{0.5}$ model on the &lt;a href="https://libero-project.github.io/datasets"&gt;LIBERO dataset&lt;/a&gt; as a running example for how to fine-tune a base model on your own data. We will explain three steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Convert your data to a LeRobot dataset (which we use for training)&lt;/li&gt; 
 &lt;li&gt;Defining training configs and running training&lt;/li&gt; 
 &lt;li&gt;Spinning up a policy server and running inference&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;1. Convert your data to a LeRobot dataset&lt;/h3&gt; 
&lt;p&gt;We provide a minimal example script for converting LIBERO data to a LeRobot dataset in &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/libero/convert_libero_data_to_lerobot.py"&gt;&lt;code&gt;examples/libero/convert_libero_data_to_lerobot.py&lt;/code&gt;&lt;/a&gt;. You can easily modify it to convert your own data! You can download the raw LIBERO dataset from &lt;a href="https://huggingface.co/datasets/openvla/modified_libero_rlds"&gt;here&lt;/a&gt;, and run the script with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run examples/libero/convert_libero_data_to_lerobot.py --data_dir /path/to/your/libero/data
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; If you just want to fine-tune on LIBERO, you can skip this step, because our LIBERO fine-tuning configs point to a pre-converted LIBERO dataset. This step is merely an example that you can adapt to your own data.&lt;/p&gt; 
&lt;h3&gt;2. Defining training configs and running training&lt;/h3&gt; 
&lt;p&gt;To fine-tune a base model on your own data, you need to define configs for data processing and training. We provide example configs with detailed comments for LIBERO below, which you can modify for your own dataset:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/src/openpi/policies/libero_policy.py"&gt;&lt;code&gt;LiberoInputs&lt;/code&gt; and &lt;code&gt;LiberoOutputs&lt;/code&gt;&lt;/a&gt;: Defines the data mapping from the LIBERO environment to the model and vice versa. Will be used for both, training and inference.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/src/openpi/training/config.py"&gt;&lt;code&gt;LeRobotLiberoDataConfig&lt;/code&gt;&lt;/a&gt;: Defines how to process raw LIBERO data from LeRobot dataset for training.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/src/openpi/training/config.py"&gt;&lt;code&gt;TrainConfig&lt;/code&gt;&lt;/a&gt;: Defines fine-tuning hyperparameters, data config, and weight loader.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We provide example fine-tuning configs for &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/src/openpi/training/config.py"&gt;Ï€â‚€&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/src/openpi/training/config.py"&gt;Ï€â‚€-FAST&lt;/a&gt;, and &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/src/openpi/training/config.py"&gt;Ï€â‚€.â‚…&lt;/a&gt; on LIBERO data.&lt;/p&gt; 
&lt;p&gt;Before we can run training, we need to compute the normalization statistics for the training data. Run the script below with the name of your training config:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run scripts/compute_norm_stats.py --config-name pi05_libero
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Now we can kick off training with the following command (the &lt;code&gt;--overwrite&lt;/code&gt; flag is used to overwrite existing checkpoints if you rerun fine-tuning with the same config):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;XLA_PYTHON_CLIENT_MEM_FRACTION=0.9 uv run scripts/train.py pi05_libero --exp-name=my_experiment --overwrite
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The command will log training progress to the console and save checkpoints to the &lt;code&gt;checkpoints&lt;/code&gt; directory. You can also monitor training progress on the Weights &amp;amp; Biases dashboard. For maximally using the GPU memory, set &lt;code&gt;XLA_PYTHON_CLIENT_MEM_FRACTION=0.9&lt;/code&gt; before running training -- this enables JAX to use up to 90% of the GPU memory (vs. the default of 75%).&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; We provide functionality for &lt;em&gt;reloading&lt;/em&gt; normalization statistics for state / action normalization from pre-training. This can be beneficial if you are fine-tuning to a new task on a robot that was part of our pre-training mixture. For more details on how to reload normalization statistics, see the &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/docs/norm_stats.md"&gt;norm_stats.md&lt;/a&gt; file.&lt;/p&gt; 
&lt;h3&gt;3. Spinning up a policy server and running inference&lt;/h3&gt; 
&lt;p&gt;Once training is complete, we can run inference by spinning up a policy server and then querying it from a LIBERO evaluation script. Launching a model server is easy (we use the checkpoint for iteration 20,000 for this example, modify as needed):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run scripts/serve_policy.py policy:checkpoint --policy.config=pi05_libero --policy.dir=checkpoints/pi05_libero/my_experiment/20000
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will spin up a server that listens on port 8000 and waits for observations to be sent to it. We can then run an evaluation script (or robot runtime) that queries the server.&lt;/p&gt; 
&lt;p&gt;For running the LIBERO eval in particular, we provide (and recommend using) a Dockerized workflow that handles both the policy server and the evaluation script together. See the &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/libero/README.md"&gt;LIBERO README&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;p&gt;If you want to embed a policy server call in your own robot runtime, we have a minimal example of how to do so in the &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/docs/remote_inference.md"&gt;remote inference docs&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;More Examples&lt;/h3&gt; 
&lt;p&gt;We provide more examples for how to fine-tune and run inference with our models on the ALOHA platform in the following READMEs:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/aloha_sim"&gt;ALOHA Simulator&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/aloha_real"&gt;ALOHA Real&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/ur5"&gt;UR5&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;PyTorch Support&lt;/h2&gt; 
&lt;p&gt;openpi now provides PyTorch implementations of Ï€â‚€ and Ï€â‚€.â‚… models alongside the original JAX versions! The PyTorch implementation has been validated on the LIBERO benchmark (both inference and finetuning). A few features are currently not supported (this may change in the future):&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;The Ï€â‚€-FAST model&lt;/li&gt; 
 &lt;li&gt;Mixed precision training&lt;/li&gt; 
 &lt;li&gt;FSDP (fully-sharded data parallelism) training&lt;/li&gt; 
 &lt;li&gt;LoRA (low-rank adaptation) training&lt;/li&gt; 
 &lt;li&gt;EMA (exponential moving average) weights during training&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Setup&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Make sure that you have the latest version of all dependencies installed: &lt;code&gt;uv sync&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Double check that you have transformers 4.53.2 installed: &lt;code&gt;uv pip show transformers&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Apply the transformers library patches:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cp -r ./src/openpi/models_pytorch/transformers_replace/* .venv/lib/python3.11/site-packages/transformers/
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;This overwrites several files in the transformers library with necessary model changes: 1) supporting AdaRMS, 2) correctly controlling the precision of activations, and 3) allowing the KV cache to be used without being updated.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;WARNING&lt;/strong&gt;: With the default uv link mode (hardlink), this will permanently affect the transformers library in your uv cache, meaning the changes will survive reinstallations of transformers and could even propagate to other projects that use transformers. To fully undo this operation, you must run &lt;code&gt;uv cache clean transformers&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Converting JAX Models to PyTorch&lt;/h3&gt; 
&lt;p&gt;To convert a JAX model checkpoint to PyTorch format:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run examples/convert_jax_model_to_pytorch.py \
    --checkpoint_dir /path/to/jax/checkpoint \
    --config_name &amp;lt;config name&amp;gt; \
    --output_path /path/to/converted/pytorch/checkpoint
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Running Inference with PyTorch&lt;/h3&gt; 
&lt;p&gt;The PyTorch implementation uses the same API as the JAX version - you only need to change the checkpoint path to point to the converted PyTorch model:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openpi.training import config as _config
from openpi.policies import policy_config
from openpi.shared import download

config = _config.get_config("pi05_droid")
checkpoint_dir = "/path/to/converted/pytorch/checkpoint"

# Create a trained policy (automatically detects PyTorch format)
policy = policy_config.create_trained_policy(config, checkpoint_dir)

# Run inference (same API as JAX)
action_chunk = policy.infer(example)["actions"]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Policy Server with PyTorch&lt;/h3&gt; 
&lt;p&gt;The policy server works identically with PyTorch models - just point to the converted checkpoint directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run scripts/serve_policy.py policy:checkpoint \
    --policy.config=pi05_droid \
    --policy.dir=/path/to/converted/pytorch/checkpoint
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Finetuning with PyTorch&lt;/h3&gt; 
&lt;p&gt;To finetune a model in PyTorch:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Convert the JAX base model to PyTorch format:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;uv run examples/convert_jax_model_to_pytorch.py \
    --config_name &amp;lt;config name&amp;gt; \
    --checkpoint_dir /path/to/jax/base/model \
    --output_path /path/to/pytorch/base/model
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Specify the converted PyTorch model path in your config using &lt;code&gt;pytorch_weight_path&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Launch training using one of these modes:&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Single GPU training:
uv run scripts/train_pytorch.py &amp;lt;config_name&amp;gt; --exp_name &amp;lt;run_name&amp;gt; --save_interval &amp;lt;interval&amp;gt;

# Example:
uv run scripts/train_pytorch.py debug --exp_name pytorch_test
uv run scripts/train_pytorch.py debug --exp_name pytorch_test --resume  # Resume from latest checkpoint

# Multi-GPU training (single node):
uv run torchrun --standalone --nnodes=1 --nproc_per_node=&amp;lt;num_gpus&amp;gt; scripts/train_pytorch.py &amp;lt;config_name&amp;gt; --exp_name &amp;lt;run_name&amp;gt;

# Example:
uv run torchrun --standalone --nnodes=1 --nproc_per_node=2 scripts/train_pytorch.py pi0_aloha_sim --exp_name pytorch_ddp_test
uv run torchrun --standalone --nnodes=1 --nproc_per_node=2 scripts/train_pytorch.py pi0_aloha_sim --exp_name pytorch_ddp_test --resume

# Multi-Node Training:
uv run torchrun \
    --nnodes=&amp;lt;num_nodes&amp;gt; \
    --nproc_per_node=&amp;lt;gpus_per_node&amp;gt; \
    --node_rank=&amp;lt;rank_of_node&amp;gt; \
    --master_addr=&amp;lt;master_ip&amp;gt; \
    --master_port=&amp;lt;port&amp;gt; \
    scripts/train_pytorch.py &amp;lt;config_name&amp;gt; --exp_name=&amp;lt;run_name&amp;gt; --save_interval &amp;lt;interval&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Precision Settings&lt;/h3&gt; 
&lt;p&gt;JAX and PyTorch implementations handle precision as follows:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;JAX:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Inference: most weights and computations in bfloat16, with a few computations in float32 for stability&lt;/li&gt; 
 &lt;li&gt;Training: defaults to mixed precision: weights and gradients in float32, (most) activations and computations in bfloat16. You can change to full float32 training by setting &lt;code&gt;dtype&lt;/code&gt; to float32 in the config.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;PyTorch:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Inference: matches JAX -- most weights and computations in bfloat16, with a few weights converted to float32 for stability&lt;/li&gt; 
 &lt;li&gt;Training: supports either full bfloat16 (default) or full float32. You can change it by setting &lt;code&gt;pytorch_training_precision&lt;/code&gt; in the config. bfloat16 uses less memory but exhibits higher losses compared to float32. Mixed precision is not yet supported.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;With torch.compile, inference speed is comparable between JAX and PyTorch.&lt;/p&gt; 
&lt;h2&gt;Troubleshooting&lt;/h2&gt; 
&lt;p&gt;We will collect common issues and their solutions here. If you encounter an issue, please check here first. If you can't find a solution, please file an issue on the repo (see &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/CONTRIBUTING.md"&gt;here&lt;/a&gt; for guidelines).&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Issue&lt;/th&gt; 
   &lt;th&gt;Resolution&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;uv sync&lt;/code&gt; fails with dependency conflicts&lt;/td&gt; 
   &lt;td&gt;Try removing the virtual environment directory (&lt;code&gt;rm -rf .venv&lt;/code&gt;) and running &lt;code&gt;uv sync&lt;/code&gt; again. If issues persist, check that you have the latest version of &lt;code&gt;uv&lt;/code&gt; installed (&lt;code&gt;uv self update&lt;/code&gt;).&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Training runs out of GPU memory&lt;/td&gt; 
   &lt;td&gt;Make sure you set &lt;code&gt;XLA_PYTHON_CLIENT_MEM_FRACTION=0.9&lt;/code&gt; (or higher) before running training to allow JAX to use more GPU memory. You can also use &lt;code&gt;--fsdp-devices &amp;lt;n&amp;gt;&lt;/code&gt; where &lt;code&gt;&amp;lt;n&amp;gt;&lt;/code&gt; is your number of GPUs, to enable &lt;a href="https://engineering.fb.com/2021/07/15/open-source/fsdp/"&gt;fully-sharded data parallelism&lt;/a&gt;, which reduces memory usage in exchange for slower training (the amount of slowdown depends on your particular setup). If you are still running out of memory, you may way to consider disabling EMA.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Policy server connection errors&lt;/td&gt; 
   &lt;td&gt;Check that the server is running and listening on the expected port. Verify network connectivity and firewall settings between client and server.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Missing norm stats error when training&lt;/td&gt; 
   &lt;td&gt;Run &lt;code&gt;scripts/compute_norm_stats.py&lt;/code&gt; with your config name before starting training.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Dataset download fails&lt;/td&gt; 
   &lt;td&gt;Check your internet connection. For HuggingFace datasets, ensure you're logged in (&lt;code&gt;huggingface-cli login&lt;/code&gt;).&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;CUDA/GPU errors&lt;/td&gt; 
   &lt;td&gt;Verify NVIDIA drivers are installed correctly. For Docker, ensure nvidia-container-toolkit is installed. Check GPU compatibility. You do NOT need CUDA libraries installed at a system level --- they will be installed via uv. You may even want to try &lt;em&gt;uninstalling&lt;/em&gt; system CUDA libraries if you run into CUDA issues, since system libraries can sometimes cause conflicts.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Import errors when running examples&lt;/td&gt; 
   &lt;td&gt;Make sure you've installed all dependencies with &lt;code&gt;uv sync&lt;/code&gt;. Some examples may have additional requirements listed in their READMEs.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Action dimensions mismatch&lt;/td&gt; 
   &lt;td&gt;Verify your data processing transforms match the expected input/output dimensions of your robot. Check the action space definitions in your policy classes.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Diverging training loss&lt;/td&gt; 
   &lt;td&gt;Check the &lt;code&gt;q01&lt;/code&gt;, &lt;code&gt;q99&lt;/code&gt;, and &lt;code&gt;std&lt;/code&gt; values in &lt;code&gt;norm_stats.json&lt;/code&gt; for your dataset. Certain dimensions that are rarely used can end up with very small &lt;code&gt;q01&lt;/code&gt;, &lt;code&gt;q99&lt;/code&gt;, or &lt;code&gt;std&lt;/code&gt; values, leading to huge states and actions after normalization. You can manually adjust the norm stats as a workaround.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt;</description>
    </item>
    
    <item>
      <title>ccfos/nightingale</title>
      <link>https://github.com/ccfos/nightingale</link>
      <description>&lt;p&gt;Nightingale for monitoring and alerting, just as Grafana for visualization.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://github.com/ccfos/nightingale"&gt; &lt;img src="https://raw.githubusercontent.com/ccfos/nightingale/main/doc/img/Nightingale_L_V.png" alt="nightingale - cloud native monitoring" width="100" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;b&gt;Open-Source Alerting Expert&lt;/b&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://flashcat.cloud/docs/"&gt; &lt;img alt="Docs" src="https://img.shields.io/badge/docs-get%20started-brightgreen" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/u/flashcatcloud"&gt; &lt;img alt="Docker pulls" src="https://img.shields.io/docker/pulls/flashcatcloud/nightingale" /&gt;&lt;/a&gt; &lt;a href="https://github.com/ccfos/nightingale/graphs/contributors"&gt; &lt;img alt="GitHub contributors" src="https://img.shields.io/github/contributors-anon/ccfos/nightingale" /&gt;&lt;/a&gt; &lt;img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ccfos/nightingale" /&gt; &lt;img alt="GitHub forks" src="https://img.shields.io/github/forks/ccfos/nightingale" /&gt; &lt;br /&gt;&lt;img alt="GitHub Repo issues" src="https://img.shields.io/github/issues/ccfos/nightingale" /&gt; &lt;img alt="GitHub Repo issues closed" src="https://img.shields.io/github/issues-closed/ccfos/nightingale" /&gt; &lt;img alt="GitHub latest release" src="https://img.shields.io/github/v/release/ccfos/nightingale" /&gt; &lt;img alt="License" src="https://img.shields.io/badge/license-Apache--2.0-blue" /&gt; &lt;a href="https://n9e-talk.slack.com/"&gt; &lt;img alt="GitHub contributors" src="https://img.shields.io/badge/join%20slack-%23n9e-brightgreen.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/ccfos/nightingale/main/README.md"&gt;English&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/ccfos/nightingale/main/README_zh.md"&gt;ä¸­æ–‡&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ðŸŽ¯ What is Nightingale&lt;/h2&gt; 
&lt;p&gt;Nightingale is an open-source monitoring project that focuses on alerting. Similar to Grafana, Nightingale also connects with various existing data sources. However, while Grafana emphasizes visualization, Nightingale places greater emphasis on the alerting engine, as well as the processing and distribution of alarms.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;The Nightingale project was initially developed and open-sourced by DiDi.inc. On May 11, 2022, it was donated to the Open Source Development Committee of the China Computer Federation (CCF ODC).&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://n9e.github.io/img/global/arch-bg.png" alt="" /&gt;&lt;/p&gt; 
&lt;h2&gt;ðŸ’¡ How Nightingale Works&lt;/h2&gt; 
&lt;p&gt;Many users have already collected metrics and log data. In this case, you can connect your storage repositories (such as VictoriaMetrics, ElasticSearch, etc.) as data sources in Nightingale. This allows you to configure alerting rules and notification rules within Nightingale, enabling the generation and distribution of alarms.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/ccfos/nightingale/main/doc/img/readme/20240221152601.png" alt="Nightingale Product Architecture" /&gt;&lt;/p&gt; 
&lt;p&gt;Nightingale itself does not provide monitoring data collection capabilities. We recommend using &lt;a href="https://github.com/flashcatcloud/categraf"&gt;Categraf&lt;/a&gt; as the collector, which integrates seamlessly with Nightingale.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/flashcatcloud/categraf"&gt;Categraf&lt;/a&gt; can collect monitoring data from operating systems, network devices, various middleware, and databases. It pushes this data to Nightingale via the &lt;code&gt;Prometheus Remote Write&lt;/code&gt; protocol. Nightingale then stores the monitoring data in a time-series database (such as Prometheus, VictoriaMetrics, etc.) and provides alerting and visualization capabilities.&lt;/p&gt; 
&lt;p&gt;For certain edge data centers with poor network connectivity to the central Nightingale server, we offer a distributed deployment mode for the alerting engine. In this mode, even if the network is disconnected, the alerting functionality remains unaffected.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/ccfos/nightingale/main/doc/img/readme/20240222102119.png" alt="Edge Deployment Mode" /&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;In the above diagram, Data Center A has a good network with the central data center, so it uses the Nightingale process in the central data center as the alerting engine. Data Center B has a poor network with the central data center, so it deploys &lt;code&gt;n9e-edge&lt;/code&gt; as the alerting engine to handle alerting for its own data sources.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;ðŸ”• Alert Noise Reduction, Escalation, and Collaboration&lt;/h2&gt; 
&lt;p&gt;Nightingale focuses on being an alerting engine, responsible for generating alarms and flexibly distributing them based on rules. It supports 20 built-in notification medias (such as phone calls, SMS, email, DingTalk, Slack, etc.).&lt;/p&gt; 
&lt;p&gt;If you have more advanced requirements, such as:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Want to consolidate events from multiple monitoring systems into one platform for unified noise reduction, response handling, and data analysis.&lt;/li&gt; 
 &lt;li&gt;Want to support personnel scheduling, practice on-call culture, and support alert escalation (to avoid missing alerts) and collaborative handling.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Then Nightingale is not suitable. It is recommended that you choose on-call products such as PagerDuty and FlashDuty. These products are simple and easy to use.&lt;/p&gt; 
&lt;h2&gt;ðŸ—¨ï¸ Communication Channels&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Report Bugs:&lt;/strong&gt; It is highly recommended to submit issues via the &lt;a href="https://github.com/ccfos/nightingale/issues/new?assignees=&amp;amp;labels=kind%2Fbug&amp;amp;projects=&amp;amp;template=bug_report.yml"&gt;Nightingale GitHub Issue tracker&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Documentation:&lt;/strong&gt; For more information, we recommend thoroughly browsing the &lt;a href="https://n9e.github.io/"&gt;Nightingale Documentation Site&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ðŸ”‘ Key Features&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/ccfos/nightingale/main/doc/img/readme/2025-05-23_18-43-37.png" alt="Nightingale Alerting rules" /&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Nightingale supports alerting rules, mute rules, subscription rules, and notification rules. It natively supports 20 types of notification media and allows customization of message templates.&lt;/li&gt; 
 &lt;li&gt;It supports event pipelines for Pipeline processing of alarms, facilitating automated integration with in-house systems. For example, it can append metadata to alarms or perform relabeling on events.&lt;/li&gt; 
 &lt;li&gt;It introduces the concept of business groups and a permission system to manage various rules in a categorized manner.&lt;/li&gt; 
 &lt;li&gt;Many databases and middleware come with built-in alert rules that can be directly imported and used. It also supports direct import of Prometheus alerting rules.&lt;/li&gt; 
 &lt;li&gt;It supports alerting self-healing, which automatically triggers a script to execute predefined logic after an alarm is generatedâ€”such as cleaning up disk space or capturing the current system state.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/ccfos/nightingale/main/doc/img/readme/2025-05-30_08-49-28.png" alt="Nightingale Alarm Dashboard" /&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Nightingale archives historical alarms and supports multi-dimensional query and statistics.&lt;/li&gt; 
 &lt;li&gt;It supports flexible aggregation grouping, allowing a clear view of the distribution of alarms across the company.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/ccfos/nightingale/main/doc/img/readme/2025-05-23_18-46-06.png" alt="Nightingale Integration Center" /&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Nightingale has built-in metric descriptions, dashboards, and alerting rules for common operating systems, middleware, and databases, which are contributed by the community with varying quality.&lt;/li&gt; 
 &lt;li&gt;It directly receives data via multiple protocols such as Remote Write, OpenTSDB, Datadog, and Falcon, integrates with various Agents.&lt;/li&gt; 
 &lt;li&gt;It supports data sources like Prometheus, ElasticSearch, Loki, ClickHouse, MySQL, Postgres, allowing alerting based on data from these sources.&lt;/li&gt; 
 &lt;li&gt;Nightingale can be easily embedded into internal enterprise systems (e.g. Grafana, CMDB), and even supports configuring menu visibility for these embedded systems.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/ccfos/nightingale/main/doc/img/readme/2025-05-23_18-49-02.png" alt="Nightingale dashboards" /&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Nightingale supports dashboard functionality, including common chart types, and comes with pre-built dashboards. The image above is a screenshot of one of these dashboards.&lt;/li&gt; 
 &lt;li&gt;If you are already accustomed to Grafana, it is recommended to continue using Grafana for visualization, as Grafana has deeper expertise in this area.&lt;/li&gt; 
 &lt;li&gt;For machine-related monitoring data collected by Categraf, it is advisable to use Nightingale's built-in dashboards for viewing. This is because Categraf's metric naming follows Telegraf's convention, which differs from that of Node Exporter.&lt;/li&gt; 
 &lt;li&gt;Due to Nightingale's concept of business groups (where machines can belong to different groups), there may be scenarios where you only want to view machines within the current business group on the dashboard. Thus, Nightingale's dashboards can be linked with business groups for interactive filtering.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ðŸŒŸ Stargazers over time&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#ccfos/nightingale&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=ccfos/nightingale&amp;amp;type=Date" alt="Stargazers over time" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ðŸ”¥ Users&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/ccfos/nightingale/main/doc/img/readme/logos.png" alt="User Logos" /&gt;&lt;/p&gt; 
&lt;h2&gt;ðŸ¤ Community Co-Building&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;â‡ï¸ Please read the &lt;a href="https://raw.githubusercontent.com/ccfos/nightingale/main/doc/community-governance.md"&gt;Nightingale Open Source Project and Community Governance Draft&lt;/a&gt;. We sincerely welcome every user, developer, company, and organization to use Nightingale, actively report bugs, submit feature requests, share best practices, and help build a professional and active open-source community.&lt;/li&gt; 
 &lt;li&gt;â¤ï¸ Nightingale Contributors&lt;/li&gt; 
&lt;/ul&gt; 
&lt;a href="https://github.com/ccfos/nightingale/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=ccfos/nightingale" /&gt; &lt;/a&gt; 
&lt;h2&gt;ðŸ“œ License&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/didi/nightingale/raw/main/LICENSE"&gt;Apache License V2.0&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>heroui-inc/heroui</title>
      <link>https://github.com/heroui-inc/heroui</link>
      <description>&lt;p&gt;ðŸš€ Beautiful, fast and modern React UI library. (Previously NextUI)&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://heroui.com"&gt; &lt;img width="20%" src="https://raw.githubusercontent.com/heroui-inc/heroui/main/apps/docs/public/isotipo.png" alt="heorui" /&gt; &lt;/a&gt;&lt;/p&gt;
&lt;h1 align="center"&gt;&lt;a href="https://heroui.com"&gt;HeroUI&lt;/a&gt;&lt;/h1&gt;
&lt;a href="https://heroui.com"&gt; &lt;/a&gt; 
&lt;p&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/heroui-inc/heroui/raw/main/LICENSE"&gt; &lt;img src="https://img.shields.io/npm/l/@heroui/react?style=flat" alt="License" /&gt; &lt;/a&gt; &lt;a href="https://codecov.io/gh/jrgarciadev/nextui"&gt; &lt;img src="https://codecov.io/gh/jrgarciadev/nextui/branch/main/graph/badge.svg?token=QJF2QKR5N4" alt="codecov badge" /&gt; &lt;/a&gt; 
 &lt;!-- &lt;a href="https://github.com/heroui-inc/heroui/actions/workflows/main.yaml"&gt;
    &lt;img src="https://github.com/heroui-inc/heroui/actions/workflows/main.yaml/badge.svg" alt="CI/CD heroui"&gt;
  &lt;/a&gt; --&gt; &lt;a href="https://www.npmjs.com/package/@heroui/react"&gt; &lt;img src="https://img.shields.io/npm/dm/@heroui/react.svg?style=flat-round" alt="npm downloads" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;Visit &lt;a aria-label="heroui learn" href="https://heroui.com/learn"&gt;&lt;/a&gt;&lt;a href="https://heroui.com/guide"&gt;https://heroui.com/guide&lt;/a&gt; to get started with HeroUI.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;Visit &lt;a href="https://heroui.com/docs"&gt;https://heroui.com/docs&lt;/a&gt; to view the full documentation.&lt;/p&gt; 
&lt;h2&gt;Storybook&lt;/h2&gt; 
&lt;p&gt;Visit &lt;a href="https://storybook.heroui.com/"&gt;https://storybook.heroui.com&lt;/a&gt; to view the storybook for all components.&lt;/p&gt; 
&lt;h2&gt;Canary Release&lt;/h2&gt; 
&lt;p&gt;Canary versions are available after every merge into &lt;code&gt;canary&lt;/code&gt; branch. You can install the packages with the tag &lt;code&gt;canary&lt;/code&gt; in npm to use the latest changes before the next production release.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://canary.heroui.com/docs"&gt;Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://canary-sb.heroui.com"&gt;Storybook&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;p&gt;We're excited to see the community adopt HeroUI, raise issues, and provide feedback. Whether it's a feature request, bug report, or a project to showcase, please get involved!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://discord.gg/9b6yyZKmH4"&gt;Discord&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://x.com/hero_ui"&gt;X&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/heroui-inc/heroui/discussions"&gt;GitHub Discussions&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions are always welcome!&lt;/p&gt; 
&lt;p&gt;See &lt;a href="https://github.com/heroui-inc/heroui/raw/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for ways to get started.&lt;/p&gt; 
&lt;p&gt;Please adhere to this project's &lt;a href="https://github.com/heroui-inc/heroui/raw/main/CODE_OF_CONDUCT.md"&gt;CODE_OF_CONDUCT&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://choosealicense.com/licenses/mit/"&gt;MIT&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>firebase/genkit</title>
      <link>https://github.com/firebase/genkit</link>
      <description>&lt;p&gt;Open-source framework for building AI-powered apps in JavaScript, Go, and Python, built and used in production by Google&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/firebase/genkit/main/docs/resources/genkit-logo-dark.png#gh-dark-mode-only" alt="Genkit logo" title="Genkit" /&gt; &lt;img src="https://raw.githubusercontent.com/firebase/genkit/main/docs/resources/genkit-logo.png#gh-light-mode-only" alt="Genkit logo" title="Genkit" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://genkit.dev"&gt;Genkit&lt;/a&gt; is an open-source framework for building full-stack AI-powered applications, built and used in production by Google's Firebase. It provides SDKs for multiple programming languages with varying levels of stability:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;JavaScript/TypeScript&lt;/strong&gt;: Production-ready with full feature support&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Go&lt;/strong&gt;: Production-ready with full feature support&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Python (Alpha)&lt;/strong&gt;: Early development with core functionality&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;It offers a unified interface for integrating AI models from providers like &lt;a href="https://genkit.dev/docs/plugins/google-genai"&gt;Google&lt;/a&gt;, &lt;a href="https://genkit.dev/docs/plugins/openai"&gt;OpenAI&lt;/a&gt;, &lt;a href="https://thefireco.github.io/genkit-plugins/docs/plugins/genkitx-anthropic"&gt;Anthropic&lt;/a&gt;, &lt;a href="https://genkit.dev/docs/plugins/ollama/"&gt;Ollama&lt;/a&gt;, and more. Rapidly build and deploy production-ready chatbots, automations, and recommendation systems using streamlined APIs for multimodal content, structured outputs, tool calling, and agentic workflows.&lt;/p&gt; 
&lt;p&gt;Get started with just a few lines of code:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-ts"&gt;import { genkit } from 'genkit';
import { googleAI } from '@genkit-ai/google-genai';

const ai = genkit({ plugins: [googleAI()] });

const { text } = await ai.generate({
    model: googleAI.model('gemini-2.5-flash'),
    prompt: 'Why is Firebase awesome?'
});
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Explore &amp;amp; build with Genkit&lt;/h2&gt; 
&lt;p&gt;Play with AI sample apps, with visualizations of the Genkit code that powers them, at no cost to you.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://examples.genkit.dev"&gt;Explore Genkit by Example&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Key capabilities&lt;/h2&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Broad AI model support&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Use a unified interface to integrate with hundreds of models from providers like &lt;a href="https://genkit.dev/docs/plugins/google-genai"&gt;Google&lt;/a&gt;, &lt;a href="https://genkit.dev/docs/plugins/openai"&gt; OpenAI&lt;/a&gt;, &lt;a href="https://thefireco.github.io/genkit-plugins/docs/plugins/genkitx-anthropic"&gt; Anthropic&lt;/a&gt;, &lt;a href="https://genkit.dev/docs/plugins/ollama"&gt;Ollama&lt;/a&gt;, and more. Explore, compare, and use the best models for your needs.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Simplified AI development&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Use streamlined APIs to build AI features with &lt;a href="https://genkit.dev/docs/models#structured-output"&gt; structured output&lt;/a&gt;, &lt;a href="https://genkit.dev/docs/tool-calling"&gt;agentic tool calling&lt;/a&gt;, &lt;a href="https://genkit.dev/docs/rag"&gt;context-aware generation&lt;/a&gt;, &lt;a href="https://genkit.dev/docs/models#multimodal"&gt;multi-modal input/output&lt;/a&gt;, and more. Genkit handles the complexity of AI development, so you can build and iterate faster.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Web and mobile ready&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Integrate seamlessly with frameworks and platforms including Next.js, React, Angular, iOS, Android, using purpose-built &lt;a href="https://genkit.dev/docs/firebase"&gt;client SDKs&lt;/a&gt; and helpers.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Cross-language support&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Build with the language that best fits your project. Genkit provides SDKs for JavaScript/TypeScript, Go, and Python (Alpha) with consistent APIs and capabilities across all supported languages.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Deploy anywhere&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Deploy AI logic to any environment that supports your chosen programming language, such as &lt;a href="https://genkit.dev/docs/firebase"&gt;Cloud Functions for Firebase&lt;/a&gt;, &lt;a href="https://genkit.dev/docs/cloud-run"&gt;Google Cloud Run&lt;/a&gt;, or &lt;a href="https://genkit.dev/docs/deploy-node"&gt;third-party platforms&lt;/a&gt;, with or without Google services.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Developer tools&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Accelerate AI development with a purpose-built, local &lt;a href="https://genkit.dev/docs/devtools"&gt;CLI and Developer UI&lt;/a&gt;. Test prompts and flows against individual inputs or datasets, compare outputs from different models, debug with detailed execution traces, and use immediate visual feedback to iterate rapidly on prompts.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Production monitoring&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Ship AI features with confidence using comprehensive production monitoring. Track model performance, and request volumes, latency, and error rates in a &lt;a href="https://genkit.dev/docs/observability/getting-started"&gt; purpose-built dashboard&lt;/a&gt;. Identify issues quickly with detailed observability metrics, and ensure your AI features meet quality and performance targets in real-world usage.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;How does it work?&lt;/h2&gt; 
&lt;p&gt;Genkit simplifies AI integration with an open-source SDK and unified APIs that work across various model providers and programming languages. It abstracts away complexity so you can focus on delivering great user experiences.&lt;/p&gt; 
&lt;p&gt;Some key features offered by Genkit include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://genkit.dev/docs/models"&gt;Text and image generation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://genkit.dev/docs/models#structured-output"&gt;Type-safe, structured data generation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://genkit.dev/docs/tool-calling"&gt;Tool calling&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://genkit.dev/docs/dotprompt"&gt;Prompt templating&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://genkit.dev/docs/chat"&gt;Persisted chat interfaces&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://genkit.dev/docs/flows"&gt;AI workflows&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://genkit.dev/docs/rag"&gt;AI-powered data retrieval (RAG)&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Genkit is designed for server-side deployment in multiple language environments, and also provides seamless client-side integration through dedicated helpers and &lt;a href="https://genkit.dev/docs/firebase"&gt;client SDKs&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Implementation path&lt;/h2&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td&gt;&lt;span&gt;1&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;Choose your language and model provider&lt;/td&gt; 
   &lt;td&gt;Select the Genkit SDK for your preferred language (JavaScript/TypeScript, Go, or Python (Alpha)). Choose a model provider like &lt;a href="https://genkit.dev/docs/plugins/google-genai"&gt;Google Gemini&lt;/a&gt; or Anthropic, and get an API key. Some providers, like &lt;a href="https://genkit.dev/docs/plugins/vertex-ai"&gt;Vertex AI&lt;/a&gt;, may rely on a different means of authentication.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;span&gt;2&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;Install the SDK and initialize&lt;/td&gt; 
   &lt;td&gt;Install the Genkit SDK, model-provider package of your choice, and the Genkit CLI. Import the Genkit and provider packages and initialize Genkit with the provider API key.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;span&gt;3&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;Write and test AI features&lt;/td&gt; 
   &lt;td&gt;Use the Genkit SDK to build AI features for your use case, from basic text generation to complex multi-step workflows and agents. Use the CLI and Developer UI to help you rapidly test and iterate.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;span&gt;4&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;Deploy and monitor&lt;/td&gt; 
   &lt;td&gt;Deploy your AI features to Firebase, Google Cloud Run, or any environment that supports your chosen programming language. Integrate them into your app, and monitor them in production in the Firebase console.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Get started&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://genkit.dev/docs/get-started"&gt;JavaScript/TypeScript quickstart&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://genkit.dev/go/docs/get-started-go"&gt;Go quickstart&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://genkit.dev/python/docs/get-started/"&gt;Python quickstart&lt;/a&gt; (Alpha)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Development tools&lt;/h2&gt; 
&lt;p&gt;Genkit provides a CLI and a local UI to streamline your AI development workflow.&lt;/p&gt; 
&lt;h3&gt;CLI&lt;/h3&gt; 
&lt;p&gt;The Genkit CLI includes commands for running and evaluating your Genkit functions (flows) and collecting telemetry and logs.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Install:&lt;/strong&gt; &lt;code&gt;npm install -g genkit-cli&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Run a command, wrapped with telemetry, a interactive developer UI, etc:&lt;/strong&gt; &lt;code&gt;genkit start -- &amp;lt;command to run your code&amp;gt;&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Developer UI&lt;/h3&gt; 
&lt;p&gt;The Genkit developer UI is a local interface for testing, debugging, and iterating on your AI application.&lt;/p&gt; 
&lt;p&gt;Key features:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Run:&lt;/strong&gt; Execute and experiment with Genkit flows, prompts, queries, and more in dedicated playgrounds.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Inspect:&lt;/strong&gt; Analyze detailed traces of past executions, including step-by-step breakdowns of complex flows.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Evaluate:&lt;/strong&gt; Review the results of evaluations run against your flows, including performance metrics and links to relevant traces.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;img src="https://raw.githubusercontent.com/firebase/genkit/main/docs/resources/readme-ui-traces-screenshot.png" width="700" alt="Screenshot of Genkit Developer UI showing traces" /&gt; 
&lt;h2&gt;Try Genkit in Firebase Studio&lt;/h2&gt; 
&lt;p&gt;Want to skip the local setup? Click below to try out Genkit using &lt;a href="https://firebase.studio"&gt;Firebase Studio&lt;/a&gt;, Google's AI-assisted workspace for full-stack app development in the cloud.&lt;/p&gt; 
&lt;a href="https://studio.firebase.google.com/new/genkit"&gt; &lt;img height="32" alt="Open in Firebase Studio" src="https://cdn.firebasestudio.dev/btn/open_bright_32.svg?sanitize=true" /&gt; &lt;/a&gt; 
&lt;h2&gt;Connect with us&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://discord.gg/qXt5zzQKpc"&gt;&lt;strong&gt;Join us on Discord&lt;/strong&gt;&lt;/a&gt; â€“ Get help, share ideas, and chat with other developers.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/firebase/genkit/issues"&gt;&lt;strong&gt;Contribute on GitHub&lt;/strong&gt;&lt;/a&gt; â€“ Report bugs, suggest features, or explore the source code.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/genkit-ai/"&gt;&lt;strong&gt;Contribute to Documentation and Samples&lt;/strong&gt;&lt;/a&gt; â€“ Report issues in Genkit's &lt;a href="https://github.com/genkit-ai/docsite"&gt;documentation&lt;/a&gt;, or contribute to the &lt;a href="https://github.com/genkit-ai/samples"&gt;samples&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions to Genkit are welcome and highly appreciated! See our &lt;a href="https://raw.githubusercontent.com/firebase/genkit/main/CONTRIBUTING.md"&gt;Contribution Guide&lt;/a&gt; to get started.&lt;/p&gt; 
&lt;h2&gt;Authors&lt;/h2&gt; 
&lt;p&gt;Genkit is built by &lt;a href="https://firebase.google.com/"&gt;Firebase&lt;/a&gt; with contributions from the &lt;a href="https://github.com/firebase/genkit/graphs/contributors"&gt;Open Source Community&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>TibixDev/winboat</title>
      <link>https://github.com/TibixDev/winboat</link>
      <description>&lt;p&gt;Run Windows apps on ðŸ§ Linux with âœ¨ seamless integration&lt;/p&gt;&lt;hr&gt;&lt;div align="left"&gt; 
 &lt;table&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td&gt; &lt;img src="https://raw.githubusercontent.com/TibixDev/winboat/main/gh-assets/winboat_logo.png" alt="WinBoat Logo" width="150" /&gt; &lt;/td&gt; 
    &lt;td&gt; &lt;h1 style="color: #7C86FF; margin: 0; font-size: 32px;"&gt;WinBoat&lt;/h1&gt; &lt;p style="color: oklch(90% 0 0); font-size: 14px; margin: 5px 0;"&gt;Windows for Penguins.&lt;br /&gt; Run Windows apps on ðŸ§ Linux with âœ¨ seamless integration&lt;/p&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;Screenshots&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/TibixDev/winboat/main/gh-assets/features/feat_dash.png" alt="WinBoat Dashboard" width="45%" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/TibixDev/winboat/main/gh-assets/features/feat_apps.png" alt="WinBoat Apps" width="45%" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/TibixDev/winboat/main/gh-assets/features/feat_native.png" alt="Native Windows" width="45%" /&gt; 
&lt;/div&gt; 
&lt;h2&gt;âš ï¸ Work in Progress âš ï¸&lt;/h2&gt; 
&lt;p&gt;WinBoat is currently in beta, so expect to occasionally run into hiccups and bugs. You should be comfortable with some level of troubleshooting if you decide to try it, however we encourage you to give it a shot anyway.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ðŸŽ¨ Elegant Interface&lt;/strong&gt;: Sleek and intuitive interface that seamlessly integrates Windows into your Linux desktop environment, making it feel like a native experience&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ðŸ“¦ Automated Installs&lt;/strong&gt;: Simple installation process through our interface - pick your preferences &amp;amp; specs and let us handle the rest&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ðŸš€ Run Any App&lt;/strong&gt;: If it runs on Windows, it can run on WinBoat. Enjoy the full range of Windows applications as native OS-level windows in your Linux environment&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ðŸ–¥ï¸ Full Windows Desktop&lt;/strong&gt;: Access the complete Windows desktop experience when you need it, or run individual apps seamlessly integrated into your Linux workflow&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ðŸ“ Filesystem Integration&lt;/strong&gt;: Your home directory is mounted in Windows, allowing easy file sharing between the two systems without any hassle&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;âœ¨ And many more&lt;/strong&gt;: Smartcard passthrough, resource monitoring, and more features being added regularly&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;How Does It Work?&lt;/h2&gt; 
&lt;p&gt;WinBoat is an Electron app which allows you to run Windows apps on Linux using a containerized approach. Windows runs as a VM inside a Docker container, we communicate with it using the &lt;a href="https://github.com/TibixDev/winboat/tree/main/guest_server"&gt;WinBoat Guest Server&lt;/a&gt; to retrieve data we need from Windows. For compositing applications as native OS-level windows, we use FreeRDP together with Windows's RemoteApp protocol.&lt;/p&gt; 
&lt;h2&gt;Prerequisites&lt;/h2&gt; 
&lt;p&gt;Before running WinBoat, ensure your system meets the following requirements:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;RAM&lt;/strong&gt;: At least 4 GB of RAM&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;CPU&lt;/strong&gt;: At least 2 CPU threads&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Storage&lt;/strong&gt;: At least 32 GB free space in &lt;code&gt;/var&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Virtualization&lt;/strong&gt;: KVM enabled in BIOS/UEFI 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://duckduckgo.com/?t=h_&amp;amp;q=how+to+enable+virtualization+in+%3Cmotherboard+brand%3E+bios&amp;amp;ia=web"&gt;How to enable virtualization&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Docker&lt;/strong&gt;: Required for containerization 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://docs.docker.com/engine/install/"&gt;Installation Guide&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;âš ï¸ NOTE:&lt;/strong&gt; Docker Desktop is &lt;strong&gt;not&lt;/strong&gt; supported, you will run into issues if you use it&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Docker Compose v2&lt;/strong&gt;: Required for compatibility with docker-compose.yml files 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://docs.docker.com/compose/install/#plugin-linux-only"&gt;Installation Guide&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Docker User Group&lt;/strong&gt;: Add your user to the &lt;code&gt;docker&lt;/code&gt; group 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://docs.docker.com/engine/install/linux-postinstall/#manage-docker-as-a-non-root-user"&gt;Setup Instructions&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;FreeRDP&lt;/strong&gt;: Required for remote desktop connection (Please make sure you have &lt;strong&gt;Version 3.x.x&lt;/strong&gt; with sound support included) 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://github.com/FreeRDP/FreeRDP/wiki/PreBuilds"&gt;Installation Guide&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Kernel Modules&lt;/strong&gt;: &lt;code&gt;iptables&lt;/code&gt; and &lt;code&gt;iptable_nat&lt;/code&gt; modules must be loaded 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://rentry.org/rmfq2e5e"&gt;Module loading instructions&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Downloading&lt;/h2&gt; 
&lt;p&gt;You can download the latest Linux builds under the &lt;a href="https://github.com/TibixDev/winboat/releases"&gt;Releases&lt;/a&gt; tab. We currently offer four variants:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;AppImage:&lt;/strong&gt; A popular &amp;amp; portable app format which should run fine on most distributions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Unpacked:&lt;/strong&gt; The raw unpacked files, simply run the executable (&lt;code&gt;linux-unpacked/winboat&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;.deb:&lt;/strong&gt; The intended format for Debian based distributions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;.rpm:&lt;/strong&gt; The intended format for Fedora based distributions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Known Issues About Container Runtimes&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Podman is &lt;strong&gt;unsupported&lt;/strong&gt; for now&lt;/li&gt; 
 &lt;li&gt;Docker Desktop is &lt;strong&gt;unsupported&lt;/strong&gt; for now&lt;/li&gt; 
 &lt;li&gt;Distros that emulate Docker through a Podman socket are &lt;strong&gt;unsupported&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Any rootless containerization solution is currently &lt;strong&gt;unsupported&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Building WinBoat&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;For building you need to have NodeJS and Go installed on your system&lt;/li&gt; 
 &lt;li&gt;Clone the repo (&lt;code&gt;git clone https://github.com/TibixDev/WinBoat&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Install the dependencies (&lt;code&gt;npm i&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Build the app and the guest server using &lt;code&gt;npm run build:linux-gs&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;You can now find the built app under &lt;code&gt;dist&lt;/code&gt; with an AppImage and an Unpacked variant&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Running WinBoat in development mode&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Make sure you meet the &lt;a href="https://raw.githubusercontent.com/TibixDev/winboat/main/#prerequisites"&gt;prerequisites&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Additionally, for development you need to have NodeJS and Go installed on your system&lt;/li&gt; 
 &lt;li&gt;Clone the repo (&lt;code&gt;git clone https://github.com/TibixDev/WinBoat&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Install the dependencies (&lt;code&gt;npm i&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Build the guest server (&lt;code&gt;npm run build-guest-server&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Run the app (&lt;code&gt;npm run dev&lt;/code&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions are welcome! Whether it's bug fixes, feature improvements, or documentation updates, we appreciate your help making WinBoat better.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Please note&lt;/strong&gt;: We maintain a focus on technical contributions only. Pull requests containing political/sexual content, or other sensitive/controversial topics will not be accepted. Let's keep things focused on making great software! ðŸš€&lt;/p&gt; 
&lt;p&gt;Feel free to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Report bugs and issues&lt;/li&gt; 
 &lt;li&gt;Submit feature requests&lt;/li&gt; 
 &lt;li&gt;Contribute code improvements&lt;/li&gt; 
 &lt;li&gt;Help with documentation&lt;/li&gt; 
 &lt;li&gt;Share feedback and suggestions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Check out our issues page to get started, or feel free to open a new issue if you've found something that needs attention.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;WinBoat is licensed under the &lt;a href="https://github.com/TibixDev/winboat/raw/main/LICENSE"&gt;MIT&lt;/a&gt; license&lt;/p&gt; 
&lt;h2&gt;Inspiration / Alternatives&lt;/h2&gt; 
&lt;p&gt;These past few years some cool projects have surfaced with similar concepts, some of which we've also taken inspirations from.&lt;br /&gt; They're awesome and you should check them out:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/winapps-org/winapps"&gt;WinApps&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/casualsnek/cassowary"&gt;Cassowary&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/dockur/windows"&gt;dockur/windows&lt;/a&gt; (ðŸŒŸ Also used in WinBoat)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Socials &amp;amp; Contact&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ðŸŒ &lt;strong&gt;Website&lt;/strong&gt;: &lt;a href="https://www.winboat.app/"&gt;winboat.app&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ðŸ¦ &lt;strong&gt;Twitter/X&lt;/strong&gt;: &lt;a href="https://x.com/winboat_app"&gt;@winboat_app&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ðŸ¦‹ &lt;strong&gt;Bluesky&lt;/strong&gt;: &lt;a href="http://bsky.app/profile/winboat.app"&gt;winboat.app&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ðŸ—¨ï¸ &lt;strong&gt;Discord&lt;/strong&gt;: &lt;a href="http://discord.gg/MEwmpWm4tN"&gt;Join our community&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ðŸ“§ &lt;strong&gt;Email&lt;/strong&gt;: &lt;a href="mailto:staff@winboat.app"&gt;staff@winboat.app&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>TapXWorld/ChinaTextbook</title>
      <link>https://github.com/TapXWorld/ChinaTextbook</link>
      <description>&lt;p&gt;æ‰€æœ‰å°åˆé«˜ã€å¤§å­¦PDFæ•™æã€‚&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;é¡¹ç›®çš„ç”±æ¥&lt;/h2&gt; 
&lt;p&gt;è™½ç„¶å›½å†…æ•™è‚²ç½‘ç«™å·²æä¾›å…è´¹èµ„æºï¼Œä½†å¤§å¤šæ•°æ™®é€šäººèŽ·å–ä¿¡æ¯çš„é€”å¾„ä¾ç„¶å—é™ã€‚æœ‰äº›äººåˆ©ç”¨è¿™ä¸€ç‚¹ï¼Œåœ¨æŸç«™ä¸Šé”€å”®è¿™äº›å¸¦æœ‰ç§äººæ°´å°çš„èµ„æºã€‚ä¸ºäº†åº”å¯¹è¿™ç§æƒ…å†µï¼Œæˆ‘è®¡åˆ’å°†è¿™äº›èµ„æºé›†ä¸­å¹¶å¼€æºï¼Œä»¥ä¿ƒè¿›ä¹‰åŠ¡æ•™è‚²çš„æ™®åŠå’Œæ¶ˆé™¤åœ°åŒºé—´çš„æ•™è‚²è´«å›°ã€‚&lt;/p&gt; 
&lt;p&gt;è¿˜æœ‰ä¸€ä¸ªæœ€é‡è¦çš„åŽŸå› æ˜¯ï¼Œå¸Œæœ›æµ·å¤–åŽäººèƒ½å¤Ÿè®©è‡ªå·±çš„å­©å­ç»§ç»­äº†è§£å›½å†…æ•™è‚²ã€‚&lt;/p&gt; 
&lt;h2&gt;å­¦ä¹ æ•°å­¦&lt;/h2&gt; 
&lt;p&gt;å¸Œæœ›æœªæ¥å‡ºçŽ°æ›´å¤šä¸æ˜¯ä¸ºäº†è€ƒå­¦è€Œè¯»ä¹¦çš„äººã€‚&lt;/p&gt; 
&lt;h3&gt;å°å­¦æ•°å­¦&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/TapXWorld/ChinaTextbook/raw/master/%E5%B0%8F%E5%AD%A6/%E6%95%B0%E5%AD%A6/%E4%BA%BA%E6%95%99%E7%89%88/%E4%B9%89%E5%8A%A1%E6%95%99%E8%82%B2%E6%95%99%E7%A7%91%E4%B9%A6%20%C2%B7%20%E6%95%B0%E5%AD%A6%E4%B8%80%E5%B9%B4%E7%BA%A7%E4%B8%8A%E5%86%8C.pdf"&gt;ä¸€å¹´çº§ä¸Šå†Œ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/TapXWorld/ChinaTextbook/raw/master/%E5%B0%8F%E5%AD%A6/%E6%95%B0%E5%AD%A6/%E4%BA%BA%E6%95%99%E7%89%88/%E4%B9%89%E5%8A%A1%E6%95%99%E8%82%B2%E6%95%99%E7%A7%91%E4%B9%A6%C2%B7%E6%95%B0%E5%AD%A6%E4%B8%80%E5%B9%B4%E7%BA%A7%E4%B8%8B%E5%86%8C.pdf"&gt;ä¸€å¹´çº§ä¸‹å†Œ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/TapXWorld/ChinaTextbook/raw/master/%E5%B0%8F%E5%AD%A6/%E6%95%B0%E5%AD%A6/%E4%BA%BA%E6%95%99%E7%89%88/%E4%B9%89%E5%8A%A1%E6%95%99%E8%82%B2%E6%95%99%E7%A7%91%E4%B9%A6%20%C2%B7%20%E6%95%B0%E5%AD%A6%E4%BA%8C%E5%B9%B4%E7%BA%A7%E4%B8%8A%E5%86%8C.pdf"&gt;äºŒå¹´çº§ä¸Šå†Œ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/TapXWorld/ChinaTextbook/raw/master/%E5%B0%8F%E5%AD%A6/%E6%95%B0%E5%AD%A6/%E4%BA%BA%E6%95%99%E7%89%88/%E4%B9%89%E5%8A%A1%E6%95%99%E8%82%B2%E6%95%99%E7%A7%91%E4%B9%A6%C2%B7%E6%95%B0%E5%AD%A6%E4%BA%8C%E5%B9%B4%E7%BA%A7%E4%B8%8B%E5%86%8C.pdf"&gt;äºŒå¹´çº§ä¸‹å†Œ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/TapXWorld/ChinaTextbook/raw/master/%E5%B0%8F%E5%AD%A6/%E6%95%B0%E5%AD%A6/%E4%BA%BA%E6%95%99%E7%89%88/%E4%B9%89%E5%8A%A1%E6%95%99%E8%82%B2%E6%95%99%E7%A7%91%E4%B9%A6%20%C2%B7%20%E6%95%B0%E5%AD%A6%E4%B8%89%E5%B9%B4%E7%BA%A7%E4%B8%8A%E5%86%8C.pdf"&gt;ä¸‰å¹´çº§ä¸Šå†Œ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/TapXWorld/ChinaTextbook/raw/master/%E5%B0%8F%E5%AD%A6/%E6%95%B0%E5%AD%A6/%E4%BA%BA%E6%95%99%E7%89%88/%E4%B9%89%E5%8A%A1%E6%95%99%E8%82%B2%E6%95%99%E7%A7%91%E4%B9%A6%C2%B7%E6%95%B0%E5%AD%A6%E4%B8%89%E5%B9%B4%E7%BA%A7%E4%B8%8B%E5%86%8C.pdf"&gt;ä¸‰å¹´çº§ä¸‹å†Œ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/TapXWorld/ChinaTextbook/raw/master/%E5%B0%8F%E5%AD%A6/%E6%95%B0%E5%AD%A6/%E4%BA%BA%E6%95%99%E7%89%88/%E4%B9%89%E5%8A%A1%E6%95%99%E8%82%B2%E6%95%99%E7%A7%91%E4%B9%A6%20%C2%B7%20%E6%95%B0%E5%AD%A6%E5%9B%9B%E5%B9%B4%E7%BA%A7%E4%B8%8A%E5%86%8C.pdf"&gt;å››å¹´çº§ä¸Šå†Œ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/TapXWorld/ChinaTextbook/raw/master/%E5%B0%8F%E5%AD%A6/%E6%95%B0%E5%AD%A6/%E4%BA%BA%E6%95%99%E7%89%88/%E4%B9%89%E5%8A%A1%E6%95%99%E8%82%B2%E6%95%99%E7%A7%91%E4%B9%A6%C2%B7%E6%95%B0%E5%AD%A6%E5%9B%9B%E5%B9%B4%E7%BA%A7%E4%B8%8B%E5%86%8C.pdf"&gt;å››å¹´çº§ä¸‹å†Œ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/TapXWorld/ChinaTextbook/raw/master/%E5%B0%8F%E5%AD%A6/%E6%95%B0%E5%AD%A6/%E4%BA%BA%E6%95%99%E7%89%88/%E4%B9%89%E5%8A%A1%E6%95%99%E8%82%B2%E6%95%99%E7%A7%91%E4%B9%A6%20%C2%B7%20%E6%95%B0%E5%AD%A6%E4%BA%94%E5%B9%B4%E7%BA%A7%E4%B8%8A%E5%86%8C.pdf"&gt;äº”å¹´çº§ä¸Šå†Œ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/TapXWorld/ChinaTextbook/raw/master/%E5%B0%8F%E5%AD%A6/%E6%95%B0%E5%AD%A6/%E4%BA%BA%E6%95%99%E7%89%88/%E4%B9%89%E5%8A%A1%E6%95%99%E8%82%B2%E6%95%99%E7%A7%91%E4%B9%A6%C2%B7%E6%95%B0%E5%AD%A6%E4%BA%94%E5%B9%B4%E7%BA%A7%E4%B8%8B%E5%86%8C.pdf"&gt;äº”å¹´çº§ä¸‹å†Œ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/TapXWorld/ChinaTextbook/raw/master/%E5%B0%8F%E5%AD%A6/%E6%95%B0%E5%AD%A6/%E4%BA%BA%E6%95%99%E7%89%88/%E4%B9%89%E5%8A%A1%E6%95%99%E8%82%B2%E6%95%99%E7%A7%91%E4%B9%A6%20%C2%B7%20%E6%95%B0%E5%AD%A6%E5%85%AD%E5%B9%B4%E7%BA%A7%E4%B8%8A%E5%86%8C.pdf"&gt;å…­å¹´çº§ä¸Šå†Œ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/TapXWorld/ChinaTextbook/raw/master/%E5%B0%8F%E5%AD%A6/%E6%95%B0%E5%AD%A6/%E4%BA%BA%E6%95%99%E7%89%88/%E4%B9%89%E5%8A%A1%E6%95%99%E8%82%B2%E6%95%99%E7%A7%91%E4%B9%A6%C2%B7%E6%95%B0%E5%AD%A6%E5%85%AD%E5%B9%B4%E7%BA%A7%E4%B8%8B%E5%86%8C.pdf"&gt;å…­å¹´çº§ä¸‹å†Œ&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;åˆä¸­æ•°å­¦&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/TapXWorld/ChinaTextbook/raw/master/%E5%88%9D%E4%B8%AD/%E6%95%B0%E5%AD%A6/%E4%BA%BA%E6%95%99%E7%89%88-%E4%BA%BA%E6%B0%91%E6%95%99%E8%82%B2%E5%87%BA%E7%89%88%E7%A4%BE/%E4%B8%83%E5%B9%B4%E7%BA%A7/%E4%B9%89%E5%8A%A1%E6%95%99%E8%82%B2%E6%95%99%E7%A7%91%E4%B9%A6%C2%B7%E6%95%B0%E5%AD%A6%E4%B8%83%E5%B9%B4%E7%BA%A7%E4%B8%8A%E5%86%8C.pdf"&gt;åˆä¸€ä¸Šå†Œ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/TapXWorld/ChinaTextbook/raw/master/%E5%88%9D%E4%B8%AD/%E6%95%B0%E5%AD%A6/%E4%BA%BA%E6%95%99%E7%89%88-%E4%BA%BA%E6%B0%91%E6%95%99%E8%82%B2%E5%87%BA%E7%89%88%E7%A4%BE/%E4%B8%83%E5%B9%B4%E7%BA%A7/%E4%B9%89%E5%8A%A1%E6%95%99%E8%82%B2%E6%95%99%E7%A7%91%E4%B9%A6%C2%B7%E6%95%B0%E5%AD%A6%E4%B8%83%E5%B9%B4%E7%BA%A7%E4%B8%8B%E5%86%8C.pdf"&gt;åˆä¸€ä¸‹å†Œ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/TapXWorld/ChinaTextbook/raw/master/%E5%88%9D%E4%B8%AD/%E6%95%B0%E5%AD%A6/%E4%BA%BA%E6%95%99%E7%89%88-%E4%BA%BA%E6%B0%91%E6%95%99%E8%82%B2%E5%87%BA%E7%89%88%E7%A4%BE/%E5%85%AB%E5%B9%B4%E7%BA%A7/%E4%B9%89%E5%8A%A1%E6%95%99%E8%82%B2%E6%95%99%E7%A7%91%E4%B9%A6%C2%B7%E6%95%B0%E5%AD%A6%E5%85%AB%E5%B9%B4%E7%BA%A7%E4%B8%8A%E5%86%8C.pdf"&gt;åˆäºŒä¸Šå†Œ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/TapXWorld/ChinaTextbook/raw/master/%E5%88%9D%E4%B8%AD/%E6%95%B0%E5%AD%A6/%E4%BA%BA%E6%95%99%E7%89%88-%E4%BA%BA%E6%B0%91%E6%95%99%E8%82%B2%E5%87%BA%E7%89%88%E7%A4%BE/%E5%85%AB%E5%B9%B4%E7%BA%A7/%E4%B9%89%E5%8A%A1%E6%95%99%E8%82%B2%E6%95%99%E7%A7%91%E4%B9%A6%C2%B7%E6%95%B0%E5%AD%A6%E5%85%AB%E5%B9%B4%E7%BA%A7%E4%B8%8B%E5%86%8C.pdf"&gt;åˆäºŒä¸‹å†Œ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/TapXWorld/ChinaTextbook/raw/master/%E5%88%9D%E4%B8%AD/%E6%95%B0%E5%AD%A6/%E4%BA%BA%E6%95%99%E7%89%88-%E4%BA%BA%E6%B0%91%E6%95%99%E8%82%B2%E5%87%BA%E7%89%88%E7%A4%BE/%E4%B9%9D%E5%B9%B4%E7%BA%A7/%E4%B9%89%E5%8A%A1%E6%95%99%E8%82%B2%E6%95%99%E7%A7%91%E4%B9%A6%C2%B7%E6%95%B0%E5%AD%A6%E4%B9%9D%E5%B9%B4%E7%BA%A7%E4%B8%8A%E5%86%8C.pdf"&gt;åˆä¸‰ä¸Šå†Œ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/TapXWorld/ChinaTextbook/raw/master/%E5%88%9D%E4%B8%AD/%E6%95%B0%E5%AD%A6/%E4%BA%BA%E6%95%99%E7%89%88-%E4%BA%BA%E6%B0%91%E6%95%99%E8%82%B2%E5%87%BA%E7%89%88%E7%A4%BE/%E4%B9%9D%E5%B9%B4%E7%BA%A7/%E4%B9%89%E5%8A%A1%E6%95%99%E8%82%B2%E6%95%99%E7%A7%91%E4%B9%A6%C2%B7%E6%95%B0%E5%AD%A6%E4%B9%9D%E5%B9%B4%E7%BA%A7%E4%B8%8B%E5%86%8C.pdf"&gt;åˆä¸‰ä¸‹å†Œ&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;é«˜ä¸­æ•°å­¦&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/TapXWorld/ChinaTextbook/tree/master/%E9%AB%98%E4%B8%AD/%E6%95%B0%E5%AD%A6/%E4%BA%BA%E6%95%99%E7%89%88%EF%BC%88A%E7%89%88%EF%BC%89%EF%BC%88%E4%B8%BB%E7%BC%96%EF%BC%9A%E7%AB%A0%E5%BB%BA%E8%B7%83%26%E6%9D%8E%E5%A2%9E%E6%B2%AA%EF%BC%89-%E4%BA%BA%E6%B0%91%E6%95%99%E8%82%B2%E5%87%BA%E7%89%88%E7%A4%BE"&gt;ç›®å½•&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;å¤§å­¦æ•°å­¦&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/TapXWorld/ChinaTextbook/tree/master/%E5%A4%A7%E5%AD%A6/%E9%AB%98%E7%AD%89%E6%95%B0%E5%AD%A6/%E5%90%8C%E6%B5%8E%E5%A4%A7%E5%AD%A6%E9%AB%98%E7%AD%89%E6%95%B0%E5%AD%A6%E7%AC%AC%E4%B8%83%E7%89%88"&gt;é«˜ç­‰æ•°å­¦&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/TapXWorld/ChinaTextbook/tree/master/%E5%A4%A7%E5%AD%A6/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0"&gt;çº¿æ€§ä»£æ•°&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/TapXWorld/ChinaTextbook/tree/master/%E5%A4%A7%E5%AD%A6/%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6"&gt;ç¦»æ•£æ•°å­¦&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/TapXWorld/ChinaTextbook/tree/master/%E5%A4%A7%E5%AD%A6/%E6%A6%82%E7%8E%87%E8%AE%BA"&gt;æ¦‚çŽ‡è®º&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="http://www.dxsx.net/index.php"&gt;æ›´å¤šæ•°å­¦èµ„æ–™-(å¤§å­¦æ•°å­¦ç½‘)&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;/h2&gt; 
&lt;h2&gt;é—®é¢˜ï¼šå¦‚ä½•åˆå¹¶è¢«æ‹†åˆ†çš„æ–‡ä»¶ï¼Ÿ&lt;/h2&gt; 
&lt;p&gt;ç”±äºŽ GitHub å¯¹å•ä¸ªæ–‡ä»¶çš„ä¸Šä¼ æœ‰æœ€å¤§é™åˆ¶ï¼Œè¶…è¿‡ 100MB çš„æ–‡ä»¶ä¼šè¢«æ‹’ç»ä¸Šä¼ ï¼Œè¶…è¿‡ 50MB çš„æ–‡ä»¶ä¸Šä¼ æ—¶ä¼šæ”¶åˆ°è­¦å‘Šã€‚å› æ­¤ï¼Œæ–‡ä»¶å¤§å°è¶…è¿‡ 50MB çš„æ–‡ä»¶ä¼šè¢«æ‹†åˆ†æˆæ¯ä¸ª 35MB çš„å¤šä¸ªæ–‡ä»¶ã€‚&lt;/p&gt; 
&lt;h3&gt;ç¤ºä¾‹&lt;/h3&gt; 
&lt;p&gt;æ–‡ä»¶è¢«æ‹†åˆ†çš„ç¤ºä¾‹ï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ä¹‰åŠ¡æ•™è‚²æ•™ç§‘ä¹¦ Â· æ•°å­¦ä¸€å¹´çº§ä¸Šå†Œ.pdf.1&lt;/li&gt; 
 &lt;li&gt;ä¹‰åŠ¡æ•™è‚²æ•™ç§‘ä¹¦ Â· æ•°å­¦ä¸€å¹´çº§ä¸Šå†Œ.pdf.2&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;è§£å†³åŠžæ³•&lt;/h3&gt; 
&lt;p&gt;è¦åˆå¹¶è¿™äº›è¢«æ‹†åˆ†çš„æ–‡ä»¶ï¼Œæ‚¨åªéœ€æ‰§è¡Œä»¥ä¸‹æ­¥éª¤(å…¶ä»–æ“ä½œç³»ç»ŸåŒç†)ï¼š&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;å°†åˆå¹¶ç¨‹åº &lt;code&gt;mergePDFs-windows-amd64.exe&lt;/code&gt; ä¸‹è½½åˆ°åŒ…å« PDF æ–‡ä»¶çš„æ–‡ä»¶å¤¹ä¸­ã€‚&lt;/li&gt; 
 &lt;li&gt;ç¡®ä¿ &lt;code&gt;mergePDFs-windows-amd64.exe&lt;/code&gt; å’Œè¢«æ‹†åˆ†çš„ PDF æ–‡ä»¶åœ¨åŒä¸€ç›®å½•ä¸‹ã€‚&lt;/li&gt; 
 &lt;li&gt;åŒå‡» &lt;code&gt;mergePDFs-windows-amd64.exe&lt;/code&gt; ç¨‹åºå³å¯è‡ªåŠ¨å®Œæˆæ–‡ä»¶åˆå¹¶ã€‚&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;ä¸‹è½½æ–¹å¼&lt;/h3&gt; 
&lt;p&gt;æ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹é“¾æŽ¥ï¼Œä¸‹è½½æ–‡ä»¶åˆå¹¶ç¨‹åºï¼š&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/TapXWorld/ChinaTextbook-tools/releases"&gt;ä¸‹è½½æ–‡ä»¶åˆå¹¶ç¨‹åº&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;æ–‡ä»¶å’Œç¨‹åºç¤ºä¾‹&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;mergePDFs-windows-amd64.exe&lt;/li&gt; 
 &lt;li&gt;ä¹‰åŠ¡æ•™è‚²æ•™ç§‘ä¹¦ Â· æ•°å­¦ä¸€å¹´çº§ä¸Šå†Œ.pdf.1&lt;/li&gt; 
 &lt;li&gt;ä¹‰åŠ¡æ•™è‚²æ•™ç§‘ä¹¦ Â· æ•°å­¦ä¸€å¹´çº§ä¸Šå†Œ.pdf.2&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;é‡æ–°ä¸‹è½½&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;å¦‚æžœæ‚¨ä½äºŽå†…åœ°ï¼Œå¹¶ä¸”ç½‘ç»œä¸é”™ï¼Œæƒ³é‡æ–°ä¸‹è½½ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ &lt;a href="https://github.com/happycola233/tchMaterial-parser"&gt;tchMaterial-parser&lt;/a&gt; é¡¹ç›®ï¼ˆé¼“åŠ±å¼€æºï¼‰ï¼Œè¿›è¡Œé‡æ–°ä¸‹è½½ã€‚&lt;/li&gt; 
 &lt;li&gt;å¦‚æžœæ‚¨ä½äºŽå›½å¤–ï¼Œå’Œå†…åœ°ç½‘ç»œé€šä¿¡é€Ÿåº¦è¾ƒæ…¢ï¼Œå»ºè®®ä½¿ç”¨æœ¬å­˜å‚¨åº“è¿›è¡Œç­¾å‡ºã€‚&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;æ•™ææçŒ®&lt;/h2&gt; 
&lt;p&gt;å¦‚æžœè¿™ä¸ªé¡¹ç›®å¸®åŠ©æ‚¨å…è´¹èŽ·å–æ•™è‚²èµ„æºï¼Œè¯·è€ƒè™‘æ”¯æŒæˆ‘ä»¬æŽ¨å¹¿å¼€æ”¾æ•™è‚²çš„åŠªåŠ›ï¼æ‚¨çš„æçŒ®å°†å¸®åŠ©æˆ‘ä»¬ç»´æŠ¤å’Œæ‰©å±•è¿™ä¸ªèµ„æºåº“ã€‚&lt;/p&gt; 
&lt;p&gt;åŠ å…¥æˆ‘ä»¬çš„ Telegram ç¤¾åŒºï¼ŒèŽ·å–æœ€æ–°åŠ¨æ€å¹¶åˆ†äº«æ‚¨çš„æƒ³æ³•ï¼š&lt;a href="https://t.me/+1V6WjEq8WEM4MDM1"&gt;https://t.me/+1V6WjEq8WEM4MDM1&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;æ”¯æŒæˆ‘&lt;/h2&gt; 
&lt;p&gt;å¦‚æžœæ‚¨è§‰å¾—è¿™ä¸ªé¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œæ‚¨å¯ä»¥æ‰«æä»¥ä¸‹äºŒç»´ç è¿›è¡Œæèµ ï¼š&lt;/p&gt; 
&lt;p align="left"&gt; &lt;img src="https://raw.githubusercontent.com/TapXWorld/ChinaTextbook/master/.cache/support-alipay.png" width="20%" /&gt; &lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>