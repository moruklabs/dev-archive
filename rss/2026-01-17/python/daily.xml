<rss version="2.0">
  <channel>
    <title>GitHub Python Daily Trending</title>
    <description>Daily Trending of Python in GitHub</description>
    <pubDate>Fri, 16 Jan 2026 01:39:10 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>unslothai/unsloth</title>
      <link>https://github.com/unslothai/unsloth</link>
      <description>&lt;p&gt;Fine-tuning &amp; Reinforcement Learning for LLMs. ü¶• Train OpenAI gpt-oss, DeepSeek, Qwen, Llama, Gemma, TTS 2x faster with 70% less VRAM.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://unsloth.ai/docs"&gt;
   &lt;picture&gt; 
    &lt;source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20white%20text.png" /&gt; 
    &lt;source media="(prefers-color-scheme: light)" srcset="https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png" /&gt; 
    &lt;img alt="unsloth logo" src="https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png" height="110" style="max-width: 100%;" /&gt; 
   &lt;/picture&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-Fine-tuning.ipynb"&gt;&lt;img src="https://raw.githubusercontent.com/unslothai/unsloth/main/images/start free finetune button.png" width="154" /&gt;&lt;/a&gt; &lt;a href="https://discord.com/invite/unsloth"&gt;&lt;img src="https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord button.png" width="165" /&gt;&lt;/a&gt; &lt;a href="https://unsloth.ai/docs"&gt;&lt;img src="https://raw.githubusercontent.com/unslothai/unsloth/refs/heads/main/images/Documentation%20Button.png" width="137" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;h3&gt;Train gpt-oss, DeepSeek, Gemma, Qwen &amp;amp; Llama 2x faster with 70% less VRAM!&lt;/h3&gt; 
 &lt;p&gt;&lt;img src="https://i.ibb.co/sJ7RhGG/image-41.png" alt="" /&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;‚ú® Train for Free&lt;/h2&gt; 
&lt;p&gt;Notebooks are beginner friendly. Read our &lt;a href="https://unsloth.ai/docs/get-started/fine-tuning-llms-guide"&gt;guide&lt;/a&gt;. Add dataset, run, then deploy your trained model.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Free Notebooks&lt;/th&gt; 
   &lt;th&gt;Performance&lt;/th&gt; 
   &lt;th&gt;Memory use&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;gpt-oss (20B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-Fine-tuning.ipynb"&gt;‚ñ∂Ô∏è Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1.5x faster&lt;/td&gt; 
   &lt;td&gt;70% less&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Mistral Ministral 3 (3B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Ministral_3_VL_(3B)_Vision.ipynb"&gt;‚ñ∂Ô∏è Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1.5x faster&lt;/td&gt; 
   &lt;td&gt;60% less&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;gpt-oss (20B): GRPO&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-GRPO.ipynb"&gt;‚ñ∂Ô∏è Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2x faster&lt;/td&gt; 
   &lt;td&gt;80% less&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Qwen3: Advanced GRPO&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(4B)-GRPO.ipynb"&gt;‚ñ∂Ô∏è Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2x faster&lt;/td&gt; 
   &lt;td&gt;50% less&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Qwen3-VL (8B): GSPO&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_VL_(8B)-Vision-GRPO.ipynb"&gt;‚ñ∂Ô∏è Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1.5x faster&lt;/td&gt; 
   &lt;td&gt;80% less&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Gemma 3 (270M)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3_(270M).ipynb"&gt;‚ñ∂Ô∏è Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1.7x faster&lt;/td&gt; 
   &lt;td&gt;60% less&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Gemma 3n (4B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3N_(4B)-Conversational.ipynb"&gt;‚ñ∂Ô∏è Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1.5x faster&lt;/td&gt; 
   &lt;td&gt;50% less&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;DeepSeek-OCR (3B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Deepseek_OCR_(3B).ipynb"&gt;‚ñ∂Ô∏è Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1.5x faster&lt;/td&gt; 
   &lt;td&gt;30% less&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Llama 3.1 (8B) Alpaca&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb"&gt;‚ñ∂Ô∏è Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2x faster&lt;/td&gt; 
   &lt;td&gt;70% less&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Llama 3.2 Conversational&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb"&gt;‚ñ∂Ô∏è Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2x faster&lt;/td&gt; 
   &lt;td&gt;70% less&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Orpheus-TTS (3B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Orpheus_(3B)-TTS.ipynb"&gt;‚ñ∂Ô∏è Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1.5x faster&lt;/td&gt; 
   &lt;td&gt;50% less&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;See all our notebooks for: &lt;a href="https://github.com/unslothai/notebooks?tab=readme-ov-file#-kaggle-notebooks"&gt;Kaggle&lt;/a&gt;, &lt;a href="https://unsloth.ai/docs/get-started/unsloth-notebooks#grpo-reasoning-rl-notebooks"&gt;GRPO&lt;/a&gt;, &lt;a href="https://unsloth.ai/docs/get-started/unsloth-notebooks#text-to-speech-tts-notebooks"&gt;TTS&lt;/a&gt; &amp;amp; &lt;a href="https://unsloth.ai/docs/get-started/unsloth-notebooks#vision-multimodal-notebooks"&gt;Vision&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;See &lt;a href="https://unsloth.ai/docs/get-started/unsloth-model-catalog"&gt;all our models&lt;/a&gt; and &lt;a href="https://unsloth.ai/docs/get-started/unsloth-notebooks"&gt;all our notebooks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;See detailed documentation for Unsloth &lt;a href="https://unsloth.ai/docs"&gt;here&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;‚ö° Quickstart&lt;/h2&gt; 
&lt;h3&gt;Linux or WSL&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install unsloth
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Windows&lt;/h3&gt; 
&lt;p&gt;For Windows, &lt;code&gt;pip install unsloth&lt;/code&gt; works only if you have Pytorch installed. Read our &lt;a href="https://unsloth.ai/docs/get-started/install-and-update/windows-installation"&gt;Windows Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Docker&lt;/h3&gt; 
&lt;p&gt;Use our official &lt;a href="https://hub.docker.com/r/unsloth/unsloth"&gt;Unsloth Docker image&lt;/a&gt; &lt;code&gt;unsloth/unsloth&lt;/code&gt; container. Read our &lt;a href="https://unsloth.ai/docs/get-started/install-and-update/docker"&gt;Docker Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Blackwell &amp;amp; DGX Spark&lt;/h3&gt; 
&lt;p&gt;For RTX 50x, B200, 6000 GPUs: &lt;code&gt;pip install unsloth&lt;/code&gt;. Read our &lt;a href="https://unsloth.ai/docs/basics/fine-tuning-llms-with-blackwell-rtx-50-series-and-unsloth"&gt;Blackwell Guide&lt;/a&gt; and &lt;a href="https://unsloth.ai/docs/basics/fine-tuning-llms-with-nvidia-dgx-spark-and-unsloth"&gt;DGX Spark Guide&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;ü¶• Unsloth News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;New 7x longer context reinforcement learning vs. all other setups, via our new batching algorithms. &lt;a href="https://unsloth.ai/docs/new/grpo-long-context"&gt;Blog&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;New RoPE &amp;amp; MLP &lt;strong&gt;Triton Kernels&lt;/strong&gt; &amp;amp; &lt;strong&gt;Padding Free + Packing&lt;/strong&gt;: 3x faster training &amp;amp; 30% less VRAM. &lt;a href="https://unsloth.ai/docs/new/3x-faster-training-packing"&gt;Blog&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Mistral 3&lt;/strong&gt;: Run Ministral 3 or Devstral 2 and fine-tune with vision/RL sodoku notebooks. &lt;a href="https://unsloth.ai/docs/models/ministral-3"&gt;Guide&lt;/a&gt; ‚Ä¢ &lt;a href="https://unsloth.ai/docs/models/ministral-3#fine-tuning-ministral-3"&gt;Notebooks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;500K Context&lt;/strong&gt;: Training a 20B model with &amp;gt;500K context is now possible on an 80GB GPU. &lt;a href="https://unsloth.ai/docs/new/500k-context-length-fine-tuning"&gt;Blog&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;FP8 Reinforcement Learning&lt;/strong&gt;: You can now do FP8 GRPO on consumer GPUs. &lt;a href="https://unsloth.ai/docs/new/fp8-reinforcement-learning"&gt;Blog&lt;/a&gt; ‚Ä¢ &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_8B_FP8_GRPO.ipynb"&gt;Notebook&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;DeepSeek-OCR&lt;/strong&gt;: Fine-tune to improve language understanding by 89%. &lt;a href="https://unsloth.ai/docs/models/deepseek-ocr-how-to-run-and-fine-tune"&gt;Guide&lt;/a&gt; ‚Ä¢ &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Deepseek_OCR_(3B).ipynb"&gt;Notebook&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Docker&lt;/strong&gt;: Use Unsloth with no setup &amp;amp; environment issues with our new image. &lt;a href="https://unsloth.ai/docs/new/how-to-fine-tune-llms-with-unsloth-and-docker"&gt;Guide&lt;/a&gt; ‚Ä¢ &lt;a href="https://hub.docker.com/r/unsloth/unsloth"&gt;Docker image&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;gpt-oss RL&lt;/strong&gt;: Introducing the fastest possible inference for gpt-oss RL! &lt;a href="https://unsloth.ai/docs/models/gpt-oss-how-to-run-and-fine-tune/gpt-oss-reinforcement-learning"&gt;Read blog&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Vision RL&lt;/strong&gt;: You can now train VLMs with GRPO or GSPO in Unsloth! &lt;a href="https://unsloth.ai/docs/get-started/reinforcement-learning-rl-guide/vision-reinforcement-learning-vlm-rl"&gt;Read guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;gpt-oss&lt;/strong&gt; by OpenAI: Read our &lt;a href="https://unsloth.ai/docs/models/gpt-oss-how-to-run-and-fine-tune/long-context-gpt-oss-training"&gt;Unsloth Flex Attention&lt;/a&gt; blog and &lt;a href="https://unsloth.ai/docs/models/gpt-oss-how-to-run-and-fine-tune"&gt;gpt-oss Guide&lt;/a&gt;. 20B works on 14GB VRAM. 120B on 65GB.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click for more news&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Quantization-Aware Training&lt;/strong&gt;: We collabed with Pytorch, recovering ~70% accuracy. &lt;a href="https://unsloth.ai/docs/basics/quantization-aware-training-qat"&gt;Read blog&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Memory-efficient RL&lt;/strong&gt;: We're introducing even better RL. Our new kernels &amp;amp; algos allows faster RL with 50% less VRAM &amp;amp; 10√ó more context. &lt;a href="https://unsloth.ai/docs/get-started/reinforcement-learning-rl-guide/memory-efficient-rl"&gt;Read blog&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Gemma 3n&lt;/strong&gt; by Google: &lt;a href="https://unsloth.ai/docs/models/gemma-3-how-to-run-and-fine-tune/gemma-3n-how-to-run-and-fine-tune"&gt;Read Blog&lt;/a&gt;. We &lt;a href="https://huggingface.co/collections/unsloth/gemma-3n-685d3874830e49e1c93f9339"&gt;uploaded GGUFs, 4-bit models&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://unsloth.ai/docs/basics/text-to-speech-tts-fine-tuning"&gt;Text-to-Speech (TTS)&lt;/a&gt;&lt;/strong&gt; is now supported, including &lt;code&gt;sesame/csm-1b&lt;/code&gt; and STT &lt;code&gt;openai/whisper-large-v3&lt;/code&gt;.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://unsloth.ai/docs/models/qwen3-how-to-run-and-fine-tune"&gt;Qwen3&lt;/a&gt;&lt;/strong&gt; is now supported. Qwen3-30B-A3B fits on 17.5GB VRAM.&lt;/li&gt; 
  &lt;li&gt;Introducing &lt;strong&gt;&lt;a href="https://unsloth.ai/docs/basics/unsloth-dynamic-2.0-ggufs"&gt;Dynamic 2.0&lt;/a&gt;&lt;/strong&gt; quants that set new benchmarks on 5-shot MMLU &amp;amp; Aider Polyglot.&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://unsloth.ai/blog/gemma3#everything"&gt;&lt;strong&gt;EVERYTHING&lt;/strong&gt; is now supported&lt;/a&gt; - all models (TTS, BERT, Mamba), FFT, etc. &lt;a href="https://unsloth.ai/docs/basics/multi-gpu-training-with-unsloth"&gt;MultiGPU&lt;/a&gt; coming soon. Enable FFT with &lt;code&gt;full_finetuning = True&lt;/code&gt;, 8-bit with &lt;code&gt;load_in_8bit = True&lt;/code&gt;.&lt;/li&gt; 
  &lt;li&gt;üì£ &lt;a href="https://unsloth.ai/blog/deepseek-r1"&gt;DeepSeek-R1&lt;/a&gt; - run or fine-tune them &lt;a href="https://unsloth.ai/blog/deepseek-r1"&gt;with our guide&lt;/a&gt;. All model uploads: &lt;a href="https://huggingface.co/collections/unsloth/deepseek-r1-all-versions-678e1c48f5d2fce87892ace5"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;üì£ Introducing Long-context &lt;a href="https://unsloth.ai/blog/grpo"&gt;Reasoning (GRPO)&lt;/a&gt; in Unsloth. Train your own reasoning model with just 5GB VRAM. Transform Llama, Phi, Mistral etc. into reasoning LLMs!&lt;/li&gt; 
  &lt;li&gt;üì£ Introducing Unsloth &lt;a href="https://unsloth.ai/blog/dynamic-4bit"&gt;Dynamic 4-bit Quantization&lt;/a&gt;! We dynamically opt not to quantize certain parameters and this greatly increases accuracy while only using &amp;lt;10% more VRAM than BnB 4-bit. See our collection on &lt;a href="https://huggingface.co/collections/unsloth/unsloth-4-bit-dynamic-quants-67503bb873f89e15276c44e7"&gt;Hugging Face here.&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;üì£ &lt;strong&gt;&lt;a href="https://unsloth.ai/blog/llama4"&gt;Llama 4&lt;/a&gt;&lt;/strong&gt; by Meta, including Scout &amp;amp; Maverick are now supported.&lt;/li&gt; 
  &lt;li&gt;üì£ &lt;a href="https://unsloth.ai/blog/phi4"&gt;Phi-4&lt;/a&gt; by Microsoft: We also &lt;a href="https://unsloth.ai/blog/phi4"&gt;fixed bugs&lt;/a&gt; in Phi-4 and &lt;a href="https://huggingface.co/collections/unsloth/phi-4-all-versions-677eecf93784e61afe762afa"&gt;uploaded GGUFs, 4-bit&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;üì£ &lt;a href="https://unsloth.ai/blog/vision"&gt;Vision models&lt;/a&gt; now supported! &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb"&gt;Llama 3.2 Vision (11B)&lt;/a&gt;, &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2_VL_(7B)-Vision.ipynb"&gt;Qwen 2.5 VL (7B)&lt;/a&gt; and &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Pixtral_(12B)-Vision.ipynb"&gt;Pixtral (12B) 2409&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;üì£ &lt;a href="https://huggingface.co/collections/unsloth/llama-33-all-versions-67535d7d994794b9d7cf5e9f"&gt;Llama 3.3 (70B)&lt;/a&gt;, Meta's latest model is supported.&lt;/li&gt; 
  &lt;li&gt;üì£ We worked with Apple to add &lt;a href="https://arxiv.org/abs/2411.09009"&gt;Cut Cross Entropy&lt;/a&gt;. Unsloth now supports 89K context for Meta's Llama 3.3 (70B) on a 80GB GPU - 13x longer than HF+FA2. For Llama 3.1 (8B), Unsloth enables 342K context, surpassing its native 128K support.&lt;/li&gt; 
  &lt;li&gt;üì£ We found and helped fix a &lt;a href="https://unsloth.ai/blog/gradient"&gt;gradient accumulation bug&lt;/a&gt;! Please update Unsloth and transformers.&lt;/li&gt; 
  &lt;li&gt;üì£ We cut memory usage by a &lt;a href="https://unsloth.ai/blog/long-context"&gt;further 30%&lt;/a&gt; and now support &lt;a href="https://unsloth.ai/blog/long-context"&gt;4x longer context windows&lt;/a&gt;!&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;üîó Links and Resources&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Links&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img width="15" src="https://redditinc.com/hs-fs/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png" /&gt;&amp;nbsp; &lt;strong&gt;r/unsloth Reddit&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://reddit.com/r/unsloth"&gt;Join Reddit community&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üìö &lt;strong&gt;Documentation &amp;amp; Wiki&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://unsloth.ai/docs"&gt;Read Our Docs&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img width="13" src="https://upload.wikimedia.org/wikipedia/commons/0/09/X_(formerly_Twitter)_logo_late_2025.svg?sanitize=true" /&gt;&amp;nbsp; &lt;strong&gt;Twitter (aka X)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://twitter.com/unslothai"&gt;Follow us on X&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üíæ &lt;strong&gt;Installation&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://unsloth.ai/docs/get-started/install-and-update"&gt;Pip &amp;amp; Docker Install&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üîÆ &lt;strong&gt;Our Models&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://unsloth.ai/docs/get-started/unsloth-model-catalog"&gt;Unsloth Catalog&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;‚úçÔ∏è &lt;strong&gt;Blog&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://unsloth.ai/blog"&gt;Read our Blogs&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;‚≠ê Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Supports &lt;strong&gt;full-finetuning&lt;/strong&gt;, pretraining, 4b-bit, 16-bit and &lt;strong&gt;FP8&lt;/strong&gt; training&lt;/li&gt; 
 &lt;li&gt;Supports &lt;strong&gt;all models&lt;/strong&gt; including &lt;a href="https://unsloth.ai/docs/basics/text-to-speech-tts-fine-tuning"&gt;TTS&lt;/a&gt;, multimodal, &lt;a href="https://unsloth.ai/docs/get-started/unsloth-notebooks#other-important-notebooks"&gt;BERT&lt;/a&gt; and more! Any model that works in transformers, works in Unsloth.&lt;/li&gt; 
 &lt;li&gt;The most efficient library for &lt;a href="https://unsloth.ai/docs/get-started/reinforcement-learning-rl-guide"&gt;Reinforcement Learning (RL)&lt;/a&gt;, using 80% less VRAM. Supports GRPO, GSPO, DrGRPO, DAPO etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;0% loss in accuracy&lt;/strong&gt; - no approximation methods - all exact.&lt;/li&gt; 
 &lt;li&gt;Export and &lt;a href="https://unsloth.ai/docs/basics/inference-and-deployment"&gt;deploy your model&lt;/a&gt; to GGUF, llama.cpp, vLLM, SGLang and Hugging Face.&lt;/li&gt; 
 &lt;li&gt;Supports NVIDIA (since 2018), &lt;a href="https://unsloth.ai/docs/get-started/install-and-update/amd"&gt;AMD&lt;/a&gt; and Intel GPUs. Minimum CUDA Capability 7.0 (V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc)&lt;/li&gt; 
 &lt;li&gt;Works on &lt;strong&gt;Linux&lt;/strong&gt;, WSL and &lt;strong&gt;Windows&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;All kernels written in OpenAI's Triton language. Manual backprop engine.&lt;/li&gt; 
 &lt;li&gt;If you trained a model with ü¶•Unsloth, you can use this cool sticker! &amp;nbsp; &lt;img src="https://raw.githubusercontent.com/unslothai/unsloth/main/images/made with unsloth.png" width="200" align="center" /&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üíæ Install Unsloth&lt;/h2&gt; 
&lt;p&gt;You can also see our docs for more detailed installation and updating instructions &lt;a href="https://unsloth.ai/docs/get-started/install-and-update"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Unsloth supports Python 3.13 or lower.&lt;/p&gt; 
&lt;h3&gt;Pip Installation&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Install with pip (recommended) for Linux devices:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install unsloth
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;To update Unsloth:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install --upgrade --force-reinstall --no-cache-dir unsloth unsloth_zoo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/unslothai/unsloth/main/#advanced-pip-installation"&gt;here&lt;/a&gt; for advanced pip install instructions.&lt;/p&gt; 
&lt;h3&gt;Windows Installation&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install NVIDIA Video Driver:&lt;/strong&gt; You should install the latest driver for your GPU. Download drivers here: &lt;a href="https://www.nvidia.com/Download/index.aspx"&gt;NVIDIA GPU Driver&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install Visual Studio C++:&lt;/strong&gt; You will need Visual Studio, with C++ installed. By default, C++ is not installed with &lt;a href="https://visualstudio.microsoft.com/vs/community/"&gt;Visual Studio&lt;/a&gt;, so make sure you select all of the C++ options. Also select options for Windows 10/11 SDK. For detailed instructions with options, see &lt;a href="https://unsloth.ai/docs/get-started/install-and-update/windows-installation#method-3-windows-directly"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install CUDA Toolkit:&lt;/strong&gt; Follow the instructions to install &lt;a href="https://developer.nvidia.com/cuda-toolkit-archive"&gt;CUDA Toolkit&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install PyTorch:&lt;/strong&gt; You will need the correct version of PyTorch that is compatible with your CUDA drivers, so make sure to select them carefully. &lt;a href="https://pytorch.org/get-started/locally/"&gt;Install PyTorch&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install Unsloth:&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;pip install unsloth
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Advanced/Troubleshooting&lt;/h4&gt; 
&lt;p&gt;For &lt;strong&gt;advanced installation instructions&lt;/strong&gt; or if you see weird errors during installations:&lt;/p&gt; 
&lt;p&gt;First try using an isolated environment via then &lt;code&gt;pip install unsloth&lt;/code&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m venv unsloth
source unsloth/bin/activate
pip install unsloth
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install &lt;code&gt;torch&lt;/code&gt; and &lt;code&gt;triton&lt;/code&gt;. Go to &lt;a href="https://pytorch.org"&gt;https://pytorch.org&lt;/a&gt; to install it. For example &lt;code&gt;pip install torch torchvision torchaudio triton&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Confirm if CUDA is installed correctly. Try &lt;code&gt;nvcc&lt;/code&gt;. If that fails, you need to install &lt;code&gt;cudatoolkit&lt;/code&gt; or CUDA drivers.&lt;/li&gt; 
 &lt;li&gt;Install &lt;code&gt;xformers&lt;/code&gt; manually via:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install ninja
pip install -v --no-build-isolation -U git+https://github.com/facebookresearch/xformers.git@main#egg=xformers
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code&gt;Check if `xformers` succeeded with `python -m xformers.info` Go to https://github.com/facebookresearch/xformers. Another option is to install `flash-attn` for Ampere GPUs and ignore `xformers`
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="5"&gt; 
 &lt;li&gt;For GRPO runs, you can try installing &lt;code&gt;vllm&lt;/code&gt; and seeing if &lt;code&gt;pip install vllm&lt;/code&gt; succeeds.&lt;/li&gt; 
 &lt;li&gt;Double check that your versions of Python, CUDA, CUDNN, &lt;code&gt;torch&lt;/code&gt;, &lt;code&gt;triton&lt;/code&gt;, and &lt;code&gt;xformers&lt;/code&gt; are compatible with one another. The &lt;a href="https://github.com/pytorch/pytorch/raw/main/RELEASE.md#release-compatibility-matrix"&gt;PyTorch Compatibility Matrix&lt;/a&gt; may be useful.&lt;/li&gt; 
 &lt;li&gt;Finally, install &lt;code&gt;bitsandbytes&lt;/code&gt; and check it with &lt;code&gt;python -m bitsandbytes&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Conda Installation (Optional)&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;‚ö†Ô∏èOnly use Conda if you have it. If not, use Pip&lt;/code&gt;. Select either &lt;code&gt;pytorch-cuda=11.8,12.1&lt;/code&gt; for CUDA 11.8 or CUDA 12.1. We support &lt;code&gt;python=3.10,3.11,3.12&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;conda create --name unsloth_env \
    python=3.11 \
    pytorch-cuda=12.1 \
    pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers \
    -y
conda activate unsloth_env

pip install unsloth
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;If you're looking to install Conda in a Linux environment, &lt;a href="https://docs.anaconda.com/miniconda/"&gt;read here&lt;/a&gt;, or run the below üîΩ&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;mkdir -p ~/miniconda3
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh
bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3
rm -rf ~/miniconda3/miniconda.sh
~/miniconda3/bin/conda init bash
~/miniconda3/bin/conda init zsh
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Advanced Pip Installation&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;‚ö†Ô∏èDo **NOT** use this if you have Conda.&lt;/code&gt; Pip is a bit more complex since there are dependency issues. The pip command is different for &lt;code&gt;torch 2.2,2.3,2.4,2.5,2.6,2.7,2.8,2.9&lt;/code&gt; and CUDA versions.&lt;/p&gt; 
&lt;p&gt;For other torch versions, we support &lt;code&gt;torch211&lt;/code&gt;, &lt;code&gt;torch212&lt;/code&gt;, &lt;code&gt;torch220&lt;/code&gt;, &lt;code&gt;torch230&lt;/code&gt;, &lt;code&gt;torch240&lt;/code&gt;, &lt;code&gt;torch250&lt;/code&gt;, &lt;code&gt;torch260&lt;/code&gt;, &lt;code&gt;torch270&lt;/code&gt;, &lt;code&gt;torch280&lt;/code&gt;, &lt;code&gt;torch290&lt;/code&gt; and for CUDA versions, we support &lt;code&gt;cu118&lt;/code&gt; and &lt;code&gt;cu121&lt;/code&gt; and &lt;code&gt;cu124&lt;/code&gt;. For Ampere devices (A100, H100, RTX3090) and above, use &lt;code&gt;cu118-ampere&lt;/code&gt; or &lt;code&gt;cu121-ampere&lt;/code&gt; or &lt;code&gt;cu124-ampere&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;For example, if you have &lt;code&gt;torch 2.4&lt;/code&gt; and &lt;code&gt;CUDA 12.1&lt;/code&gt;, use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install --upgrade pip
pip install "unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Another example, if you have &lt;code&gt;torch 2.9&lt;/code&gt; and &lt;code&gt;CUDA 13.0&lt;/code&gt;, use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install --upgrade pip
pip install "unsloth[cu130-torch290] @ git+https://github.com/unslothai/unsloth.git"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;And other examples:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install "unsloth[cu121-ampere-torch240] @ git+https://github.com/unslothai/unsloth.git"
pip install "unsloth[cu118-ampere-torch240] @ git+https://github.com/unslothai/unsloth.git"
pip install "unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git"
pip install "unsloth[cu118-torch240] @ git+https://github.com/unslothai/unsloth.git"

pip install "unsloth[cu121-torch230] @ git+https://github.com/unslothai/unsloth.git"
pip install "unsloth[cu121-ampere-torch230] @ git+https://github.com/unslothai/unsloth.git"

pip install "unsloth[cu121-torch250] @ git+https://github.com/unslothai/unsloth.git"
pip install "unsloth[cu124-ampere-torch250] @ git+https://github.com/unslothai/unsloth.git"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or, run the below in a terminal to get the &lt;strong&gt;optimal&lt;/strong&gt; pip installation command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;wget -qO- https://raw.githubusercontent.com/unslothai/unsloth/main/unsloth/_auto_install.py | python -
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or, run the below manually in a Python REPL:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;try: import torch
except: raise ImportError('Install torch via `pip install torch`')
from packaging.version import Version as V
import re
v = V(re.match(r"[0-9\.]{3,}", torch.__version__).group(0))
cuda = str(torch.version.cuda)
is_ampere = torch.cuda.get_device_capability()[0] &amp;gt;= 8
USE_ABI = torch._C._GLIBCXX_USE_CXX11_ABI
if cuda not in ("11.8", "12.1", "12.4", "12.6", "12.8", "13.0"): raise RuntimeError(f"CUDA = {cuda} not supported!")
if   v &amp;lt;= V('2.1.0'): raise RuntimeError(f"Torch = {v} too old!")
elif v &amp;lt;= V('2.1.1'): x = 'cu{}{}-torch211'
elif v &amp;lt;= V('2.1.2'): x = 'cu{}{}-torch212'
elif v  &amp;lt; V('2.3.0'): x = 'cu{}{}-torch220'
elif v  &amp;lt; V('2.4.0'): x = 'cu{}{}-torch230'
elif v  &amp;lt; V('2.5.0'): x = 'cu{}{}-torch240'
elif v  &amp;lt; V('2.5.1'): x = 'cu{}{}-torch250'
elif v &amp;lt;= V('2.5.1'): x = 'cu{}{}-torch251'
elif v  &amp;lt; V('2.7.0'): x = 'cu{}{}-torch260'
elif v  &amp;lt; V('2.7.9'): x = 'cu{}{}-torch270'
elif v  &amp;lt; V('2.8.0'): x = 'cu{}{}-torch271'
elif v  &amp;lt; V('2.8.9'): x = 'cu{}{}-torch280'
elif v  &amp;lt; V('2.9.1'): x = 'cu{}{}-torch290'
elif v  &amp;lt; V('2.9.2'): x = 'cu{}{}-torch291'
else: raise RuntimeError(f"Torch = {v} too new!")
if v &amp;gt; V('2.6.9') and cuda not in ("11.8", "12.6", "12.8", "13.0"): raise RuntimeError(f"CUDA = {cuda} not supported!")
x = x.format(cuda.replace(".", ""), "-ampere" if False else "") # is_ampere is broken due to flash-attn
print(f'pip install --upgrade pip &amp;amp;&amp;amp; pip install --no-deps git+https://github.com/unslothai/unsloth-zoo.git &amp;amp;&amp;amp; pip install "unsloth[{x}] @ git+https://github.com/unslothai/unsloth.git" --no-build-isolation')
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Docker Installation&lt;/h3&gt; 
&lt;p&gt;You can use our pre-built Docker container with all dependencies to use Unsloth instantly with no setup required. &lt;a href="https://unsloth.ai/docs/get-started/install-and-update/docker"&gt;Read our guide&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;This container requires installing &lt;a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html"&gt;NVIDIA's Container Toolkit&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -d -e JUPYTER_PASSWORD="mypassword" \
  -p 8888:8888 -p 2222:22 \
  -v $(pwd)/work:/workspace/work \
  --gpus all \
  unsloth/unsloth
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Access Jupyter Lab at &lt;code&gt;http://localhost:8888&lt;/code&gt; and start fine-tuning!&lt;/p&gt; 
&lt;h2&gt;üìú Documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Go to our official &lt;a href="https://unsloth.ai/docs"&gt;Documentation&lt;/a&gt; for &lt;a href="https://unsloth.ai/docs/basics/inference-and-deployment"&gt;running models&lt;/a&gt;, &lt;a href="https://unsloth.ai/docs/basics/inference-and-deployment/saving-to-gguf"&gt;saving to GGUF&lt;/a&gt;, &lt;a href="https://unsloth.ai/docs/basics/finetuning-from-last-checkpoint"&gt;checkpointing&lt;/a&gt;, &lt;a href="https://unsloth.ai/docs/get-started/fine-tuning-llms-guide#evaluation"&gt;evaluation&lt;/a&gt; and more!&lt;/li&gt; 
 &lt;li&gt;Read our Guides for: &lt;a href="https://unsloth.ai/docs/get-started/fine-tuning-llms-guide"&gt;Fine-tuning&lt;/a&gt;, &lt;a href="https://unsloth.ai/docs/get-started/reinforcement-learning-rl-guide"&gt;Reinforcement Learning&lt;/a&gt;, &lt;a href="https://unsloth.ai/docs/basics/text-to-speech-tts-fine-tuning"&gt;Text-to-Speech (TTS)&lt;/a&gt;, &lt;a href="https://unsloth.ai/docs/basics/vision-fine-tuning"&gt;Vision&lt;/a&gt; and &lt;a href="https://unsloth.ai/docs/models/tutorials-how-to-fine-tune-and-run-llms"&gt;any model&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;We support Huggingface's transformers, TRL, Trainer, Seq2SeqTrainer and Pytorch code.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Unsloth example code to fine-tune gpt-oss-20b:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from unsloth import FastLanguageModel, FastModel
import torch
from trl import SFTTrainer, SFTConfig
from datasets import load_dataset
max_seq_length = 2048 # Supports RoPE Scaling internally, so choose any!
# Get LAION dataset
url = "https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl"
dataset = load_dataset("json", data_files = {"train" : url}, split = "train")

# 4bit pre quantized models we support for 4x faster downloading + no OOMs.
fourbit_models = [
    "unsloth/gpt-oss-20b-unsloth-bnb-4bit", #or choose any model

] # More models at https://huggingface.co/unsloth

model, tokenizer = FastModel.from_pretrained(
    model_name = "unsloth/gpt-oss-20b",
    max_seq_length = 2048, # Choose any for long context!
    load_in_4bit = True,  # 4-bit quantization. False = 16-bit LoRA.
    load_in_8bit = False, # 8-bit quantization
    load_in_16bit = False, # 16-bit LoRA
    full_finetuning = False, # Use for full fine-tuning.
    trust_remote_code = False, # Enable to support new models
    # token = "hf_...", # use one if using gated models
)

# Do model patching and add fast LoRA weights
model = FastLanguageModel.get_peft_model(
    model,
    r = 16,
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 16,
    lora_dropout = 0, # Supports any, but = 0 is optimized
    bias = "none",    # Supports any, but = "none" is optimized
    # [NEW] "unsloth" uses 30% less VRAM, fits 2x larger batch sizes!
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
    random_state = 3407,
    max_seq_length = max_seq_length,
    use_rslora = False,  # We support rank stabilized LoRA
    loftq_config = None, # And LoftQ
)

trainer = SFTTrainer(
    model = model,
    train_dataset = dataset,
    tokenizer = tokenizer,
    args = SFTConfig(
        max_seq_length = max_seq_length,
        per_device_train_batch_size = 2,
        gradient_accumulation_steps = 4,
        warmup_steps = 10,
        max_steps = 60,
        logging_steps = 1,
        output_dir = "outputs",
        optim = "adamw_8bit",
        seed = 3407,
    ),
)
trainer.train()

# Go to https://unsloth.ai/docs for advanced tips like
# (1) Saving to GGUF / merging to 16bit for vLLM or SGLang
# (2) Continued training from a saved LoRA adapter
# (3) Adding an evaluation loop / OOMs
# (4) Customized chat templates
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a name="RL"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üí° Reinforcement Learning&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://unsloth.ai/docs/get-started/reinforcement-learning-rl-guide"&gt;RL&lt;/a&gt; including &lt;a href="https://unsloth.ai/docs/get-started/reinforcement-learning-rl-guide#training-with-grpo"&gt;GRPO&lt;/a&gt;, &lt;a href="https://unsloth.ai/docs/get-started/reinforcement-learning-rl-guide/gspo-reinforcement-learning"&gt;GSPO&lt;/a&gt;, &lt;a href="https://unsloth.ai/docs/new/fp8-reinforcement-learning"&gt;&lt;strong&gt;FP8&lt;/strong&gt; training&lt;/a&gt;, DrGRPO, DAPO, PPO, Reward Modelling, Online DPO all work with Unsloth.&lt;/p&gt; 
&lt;p&gt;Read our &lt;a href="https://unsloth.ai/docs/get-started/reinforcement-learning-rl-guide"&gt;Reinforcement Learning Guide&lt;/a&gt; or our &lt;a href="https://unsloth.ai/docs/get-started/reinforcement-learning-rl-guide/advanced-rl-documentation"&gt;advanced RL docs&lt;/a&gt; for batching, generation &amp;amp; training parameters.&lt;/p&gt; 
&lt;p&gt;List of RL notebooks:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;gpt-oss GSPO notebook: &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-GRPO.ipynb"&gt;Link&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;em&gt;&lt;strong&gt;FP8&lt;/strong&gt;&lt;/em&gt; Qwen3-8B GRPO notebook (L4): &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_8B_FP8_GRPO.ipynb"&gt;Link&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Qwen2.3-VL GSPO notebook: &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_VL_(8B)-Vision-GRPO.ipynb"&gt;Link&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Advanced Qwen3 GRPO notebook: &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(4B)-GRPO.ipynb"&gt;Link&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ORPO notebook: &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-ORPO.ipynb"&gt;Link&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;DPO Zephyr notebook: &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Zephyr_(7B)-DPO.ipynb"&gt;Link&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;KTO notebook: &lt;a href="https://colab.research.google.com/drive/1MRgGtLWuZX4ypSfGguFgC-IblTvO2ivM?usp=sharing"&gt;Link&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;SimPO notebook: &lt;a href="https://colab.research.google.com/drive/1Hs5oQDovOay4mFA6Y9lQhVJ8TnbFLFh2?usp=sharing"&gt;Link&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ü•á Performance Benchmarking&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;For our most detailed benchmarks, read our &lt;a href="https://unsloth.ai/blog/llama3-3"&gt;Llama 3.3 Blog&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Benchmarking of Unsloth was also conducted by &lt;a href="https://huggingface.co/blog/unsloth-trl"&gt;ü§óHugging Face&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We tested using the Alpaca Dataset, a batch size of 2, gradient accumulation steps of 4, rank = 32, and applied QLoRA on all linear layers (q, k, v, o, gate, up, down):&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;VRAM&lt;/th&gt; 
   &lt;th&gt;ü¶• Unsloth speed&lt;/th&gt; 
   &lt;th&gt;ü¶• VRAM reduction&lt;/th&gt; 
   &lt;th&gt;ü¶• Longer context&lt;/th&gt; 
   &lt;th&gt;üòä Hugging Face + FA2&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Llama 3.3 (70B)&lt;/td&gt; 
   &lt;td&gt;80GB&lt;/td&gt; 
   &lt;td&gt;2x&lt;/td&gt; 
   &lt;td&gt;&amp;gt;75%&lt;/td&gt; 
   &lt;td&gt;13x longer&lt;/td&gt; 
   &lt;td&gt;1x&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Llama 3.1 (8B)&lt;/td&gt; 
   &lt;td&gt;80GB&lt;/td&gt; 
   &lt;td&gt;2x&lt;/td&gt; 
   &lt;td&gt;&amp;gt;70%&lt;/td&gt; 
   &lt;td&gt;12x longer&lt;/td&gt; 
   &lt;td&gt;1x&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Context length benchmarks&lt;/h3&gt; 
&lt;h4&gt;Llama 3.1 (8B) max. context length&lt;/h4&gt; 
&lt;p&gt;We tested Llama 3.1 (8B) Instruct and did 4bit QLoRA on all linear layers (Q, K, V, O, gate, up and down) with rank = 32 with a batch size of 1. We padded all sequences to a certain maximum sequence length to mimic long context finetuning workloads.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;GPU VRAM&lt;/th&gt; 
   &lt;th&gt;ü¶•Unsloth context length&lt;/th&gt; 
   &lt;th&gt;Hugging Face + FA2&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;8 GB&lt;/td&gt; 
   &lt;td&gt;2,972&lt;/td&gt; 
   &lt;td&gt;OOM&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;12 GB&lt;/td&gt; 
   &lt;td&gt;21,848&lt;/td&gt; 
   &lt;td&gt;932&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;16 GB&lt;/td&gt; 
   &lt;td&gt;40,724&lt;/td&gt; 
   &lt;td&gt;2,551&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;24 GB&lt;/td&gt; 
   &lt;td&gt;78,475&lt;/td&gt; 
   &lt;td&gt;5,789&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;40 GB&lt;/td&gt; 
   &lt;td&gt;153,977&lt;/td&gt; 
   &lt;td&gt;12,264&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;48 GB&lt;/td&gt; 
   &lt;td&gt;191,728&lt;/td&gt; 
   &lt;td&gt;15,502&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;80 GB&lt;/td&gt; 
   &lt;td&gt;342,733&lt;/td&gt; 
   &lt;td&gt;28,454&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h4&gt;Llama 3.3 (70B) max. context length&lt;/h4&gt; 
&lt;p&gt;We tested Llama 3.3 (70B) Instruct on a 80GB A100 and did 4bit QLoRA on all linear layers (Q, K, V, O, gate, up and down) with rank = 32 with a batch size of 1. We padded all sequences to a certain maximum sequence length to mimic long context finetuning workloads.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;GPU VRAM&lt;/th&gt; 
   &lt;th&gt;ü¶•Unsloth context length&lt;/th&gt; 
   &lt;th&gt;Hugging Face + FA2&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;48 GB&lt;/td&gt; 
   &lt;td&gt;12,106&lt;/td&gt; 
   &lt;td&gt;OOM&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;80 GB&lt;/td&gt; 
   &lt;td&gt;89,389&lt;/td&gt; 
   &lt;td&gt;6,916&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://i.ibb.co/sJ7RhGG/image-41.png" alt="" /&gt; &lt;br /&gt;&lt;/p&gt; 
&lt;h3&gt;Citation&lt;/h3&gt; 
&lt;p&gt;You can cite the Unsloth repo as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@software{unsloth,
  author = {Daniel Han, Michael Han and Unsloth team},
  title = {Unsloth},
  url = {http://github.com/unslothai/unsloth},
  year = {2023}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Thank You to&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;The &lt;a href="https://github.com/ggml-org/llama.cpp"&gt;llama.cpp library&lt;/a&gt; that lets users save models with Unsloth&lt;/li&gt; 
 &lt;li&gt;The Hugging Face team and their libraries: &lt;a href="https://github.com/huggingface/transformers"&gt;transformers&lt;/a&gt; and &lt;a href="https://github.com/huggingface/trl"&gt;TRL&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;The Pytorch and &lt;a href="https://github.com/unslothai/unsloth/pull/3391"&gt;Torch AO&lt;/a&gt; team for their contributions&lt;/li&gt; 
 &lt;li&gt;And of course for every single person who has contributed or has used Unsloth!&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>music-assistant/server</title>
      <link>https://github.com/music-assistant/server</link>
      <description>&lt;p&gt;Music Assistant is a free, opensource Media library manager that connects to your streaming services and a wide range of connected speakers. The server is the beating heart, the core of Music Assistant and must run on an always-on device like a Raspberry Pi, a NAS or an Intel NUC or alike.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Music Assistant&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;Music Assistant Server&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Music Assistant is a free, opensource Media library manager that connects to your streaming services and a wide range of connected speakers. The server is the beating heart, the core of Music Assistant and must run on an always-on device like a Raspberry Pi, a NAS or an Intel NUC or alike.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Documentation and support&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Documentation &lt;a href="https://music-assistant.io"&gt;https://music-assistant.io&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Beta Documentation &lt;a href="https://beta.music-assistant.io"&gt;https://beta.music-assistant.io&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;For issues, please go to &lt;a href="https://github.com/music-assistant/support/issues"&gt;the issue tracker&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For feature requests, please see &lt;a href="https://github.com/music-assistant/support/discussions/categories/feature-requests-and-ideas"&gt;feature requests&lt;/a&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Running the server&lt;/h2&gt; 
&lt;p&gt;Music Assistant can be operated as a complete standalone product but it is actually tailored to use side by side with Home Assistant, it is meant with automation in mind, hence our recommended installation method is to run the server as a Home assistant Add-on.&lt;/p&gt; 
&lt;h3&gt;Installation Instructions&lt;/h3&gt; 
&lt;p&gt;See here &lt;a href="https://music-assistant.io/installation/"&gt;https://music-assistant.io/installation/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Note that although Music Assistant's main code is written in python, it has multiple dependencies on external/OS components such as ffmpeg and custom binaries and it is therefore not possible to run it as standalone pypi package. The only available installation method to run the Music Assistant server is by running the Docker container or the Home Assistant add-on.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;a href="https://www.openhomefoundation.org/"&gt;&lt;img src="https://www.openhomefoundation.org/badges/ohf-project.png" alt="A project from the Open Home Foundation" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>OpenPipe/ART</title>
      <link>https://github.com/OpenPipe/ART</link>
      <description>&lt;p&gt;Agent Reinforcement Trainer: train multi-step agents for real-world tasks using GRPO. Give your agents on-the-job training. Reinforcement learning for Qwen2.5, Qwen3, Llama, and more!&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://art.openpipe.ai"&gt;
   &lt;picture&gt; 
    &lt;img alt="ART logo" src="https://github.com/openpipe/art/raw/main/assets/ART_logo.png" width="160px" /&gt; 
   &lt;/picture&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;/p&gt;
 &lt;h1&gt;Agent Reinforcement Trainer&lt;/h1&gt; 
 &lt;p&gt;&lt;/p&gt; 
 &lt;p&gt; Train multi-step agents for real-world tasks using GRPO. &lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/openpipe/art/raw/main/CONTRIBUTING.md"&gt;&lt;img src="https://img.shields.io/badge/PRs-welcome-blue.svg?sanitize=true" alt="PRs-Welcome" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/openpipe-art/"&gt;&lt;img src="https://img.shields.io/pypi/v/openpipe-art?color=364fc7" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/2048/2048.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Train Agent" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://discord.gg/EceeVdhpxD"&gt;&lt;img src="https://img.shields.io/badge/Join%20Discord-5865F2?style=plastic&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Join Discord" /&gt;&lt;/a&gt; &lt;a href="https://art.openpipe.ai"&gt;&lt;img src="https://img.shields.io/badge/Documentation-orange?style=plastic&amp;amp;logo=gitbook&amp;amp;logoColor=white" alt="Documentation" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;üöÄ W&amp;amp;B Training: Serverless RL&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;W&amp;amp;B Training (Serverless RL)&lt;/strong&gt; is the first publicly available service for flexibly training models with reinforcement learning. It manages your training and inference infrastructure automatically, letting you focus on defining your data, environment and reward function‚Äîleading to faster feedback cycles, lower costs, and far less DevOps.&lt;/p&gt; 
&lt;p&gt;‚ú® &lt;strong&gt;Key Benefits:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;40% lower cost&lt;/strong&gt; - Multiplexing on shared production-grade inference cluster&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;28% faster training&lt;/strong&gt; - Scale to 2000+ concurrent requests across many GPUs&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Zero infra headaches&lt;/strong&gt; - Fully managed infrastructure that stays healthy&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Instant deployment&lt;/strong&gt; - Every checkpoint instantly available via W&amp;amp;B Inference&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Before: Hours of GPU setup and infra management
# RuntimeError: CUDA error: out of memory üò¢

# After: Serverless RL with instant feedback
from art.serverless.backend import ServerlessBackend

model = art.TrainableModel(
  project="voice-agent",
  name="agent-001",
  base_model="OpenPipe/Qwen3-14B-Instruct"
)

backend = ServerlessBackend(
    api_key="your_wandb_api_key"
)
model.register(backend)
# Edit and iterate in minutes, not hours!
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://docs.wandb.ai/guides/training"&gt;üìñ Learn more about W&amp;amp;B Training ‚Üí&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ART Overview&lt;/h2&gt; 
&lt;p&gt;ART is an open-source RL framework that improves agent reliability by allowing LLMs to &lt;strong&gt;learn from experience&lt;/strong&gt;. ART provides an ergonomic harness for integrating GRPO into any python application. For a quick hands-on introduction, run one of the notebooks below. When you're ready to learn more, check out the &lt;a href="https://art.openpipe.ai"&gt;docs&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;üìí Notebooks&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Agent Task&lt;/th&gt; 
   &lt;th&gt;Example Notebook&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Comparative Performance&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ART‚Ä¢E [Serverless]&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/art-e.ipynb"&gt;üèãÔ∏è Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen3 14B learns to search emails using RULER&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/openpipe/art/raw/main/assets/benchmarks/email_agent/accuracy-training-progress.svg?sanitize=true" height="72" /&gt; &lt;a href="https://raw.githubusercontent.com/OpenPipe/ART/main/dev/art-e/art_e/evaluate/display_benchmarks.ipynb"&gt;benchmarks&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;2048 [Serverless]&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/2048/2048.ipynb"&gt;üèãÔ∏è Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen3 14B learns to play 2048&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/openpipe/art/raw/main/assets/benchmarks/2048/accuracy-training-progress.svg?sanitize=true" height="72" /&gt; &lt;a href="https://raw.githubusercontent.com/OpenPipe/ART/main/examples/2048/display_benchmarks.ipynb"&gt;benchmarks&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ART‚Ä¢E LangGraph&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/langgraph/art-e-langgraph.ipynb"&gt;üèãÔ∏è Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen 2.5 7B learns to search emails using LangGraph&lt;/td&gt; 
   &lt;td&gt;[Link coming soon]&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;MCP‚Ä¢RL&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/mcp-rl/mcp-rl.ipynb"&gt;üèãÔ∏è Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen 2.5 3B masters the NWS MCP server&lt;/td&gt; 
   &lt;td&gt;[Link coming soon]&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Temporal Clue&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/temporal_clue/temporal-clue.ipynb"&gt;üèãÔ∏è Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen 2.5 7B learns to solve Temporal Clue&lt;/td&gt; 
   &lt;td&gt;[Link coming soon]&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Tic Tac Toe&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/tic_tac_toe/tic-tac-toe.ipynb"&gt;üèãÔ∏è Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen 2.5 3B learns to play Tic Tac Toe&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/openpipe/art/raw/main/assets/benchmarks/tic-tac-toe-local/accuracy-training-progress.svg?sanitize=true" height="72" /&gt; &lt;a href="https://raw.githubusercontent.com/OpenPipe/ART/main/examples/tic_tac_toe/display-benchmarks.ipynb"&gt;benchmarks&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Codenames&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/codenames/Codenames_RL.ipynb"&gt;üèãÔ∏è Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen 2.5 3B learns to play Codenames&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/openpipe/art/raw/main/assets/benchmarks/codenames/win_rate_over_time.png" height="72" /&gt; &lt;a href="https://github.com/OpenPipe/art-notebooks/raw/main/examples/codenames/Codenames_RL.ipynb"&gt;benchmarks&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;AutoRL [RULER]&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/auto_rl.ipynb"&gt;üèãÔ∏è Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Train Qwen 2.5 7B to master any task&lt;/td&gt; 
   &lt;td&gt;[Link coming soon]&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;üì∞ ART News&lt;/h2&gt; 
&lt;p&gt;Explore our latest research and updates on building SOTA agents.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üóûÔ∏è &lt;strong&gt;&lt;a href="https://art.openpipe.ai/integrations/langgraph-integration"&gt;ART now integrates seamlessly with LangGraph&lt;/a&gt;&lt;/strong&gt; - Train your LangGraph agents with reinforcement learning for smarter multi-step reasoning and improved tool usage.&lt;/li&gt; 
 &lt;li&gt;üóûÔ∏è &lt;strong&gt;&lt;a href="https://x.com/corbtt/status/1953171838382817625"&gt;MCP‚Ä¢RL: Teach Your Model to Master Any MCP Server&lt;/a&gt;&lt;/strong&gt; - Automatically train models to effectively use MCP server tools through reinforcement learning.&lt;/li&gt; 
 &lt;li&gt;üóûÔ∏è &lt;strong&gt;&lt;a href="https://x.com/mattshumer_/status/1950572449025650733"&gt;AutoRL: Zero-Data Training for Any Task&lt;/a&gt;&lt;/strong&gt; - Train custom AI models without labeled data using automatic input generation and RULER evaluation.&lt;/li&gt; 
 &lt;li&gt;üóûÔ∏è &lt;strong&gt;&lt;a href="https://openpipe.ai/blog/ruler-easy-mode-for-rl-rewards"&gt;RULER: Easy Mode for RL Rewards&lt;/a&gt;&lt;/strong&gt; is now available for automatic reward generation in reinforcement learning.&lt;/li&gt; 
 &lt;li&gt;üóûÔ∏è &lt;strong&gt;&lt;a href="https://openpipe.ai/blog/art-e-mail-agent"&gt;ART¬∑E: How We Built an Email Research Agent That Beats o3&lt;/a&gt;&lt;/strong&gt; demonstrates a Qwen 2.5 14B email agent outperforming OpenAI's o3.&lt;/li&gt; 
 &lt;li&gt;üóûÔ∏è &lt;strong&gt;&lt;a href="https://openpipe.ai/blog/art-trainer"&gt;ART Trainer: A New RL Trainer for Agents&lt;/a&gt;&lt;/strong&gt; enables easy training of LLM-based agents using GRPO.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://openpipe.ai/blog"&gt;üìñ See all blog posts ‚Üí&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Why ART?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ART provides convenient wrappers for introducing RL training into &lt;strong&gt;existing applications&lt;/strong&gt;. We abstract the training server into a modular service that your code doesn't need to interface with.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Train from anywhere.&lt;/strong&gt; Run the ART client on your laptop and let the ART server kick off an ephemeral GPU-enabled environment, or run on a local GPU.&lt;/li&gt; 
 &lt;li&gt;Integrations with hosted platforms like W&amp;amp;B, Langfuse, and OpenPipe provide flexible observability and &lt;strong&gt;simplify debugging&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;ART is customizable with &lt;strong&gt;intelligent defaults&lt;/strong&gt;. You can configure training parameters and inference engine configurations to meet specific needs, or take advantage of the defaults, which have been optimized for training efficiency and stability.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;ART agents can be trained from any client machine that runs python. To add to an existing project, run this command:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install openpipe-art
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ü§ñ ART‚Ä¢E Agent&lt;/h2&gt; 
&lt;p&gt;Curious about how to use ART for a real-world task? Check out the &lt;a href="https://openpipe.ai/blog/art-e-mail-agent"&gt;ART‚Ä¢E Agent&lt;/a&gt; blog post, where we detail how we trained Qwen 2.5 14B to beat o3 at email retrieval!&lt;/p&gt; 
&lt;img src="https://github.com/openpipe/art/raw/main/assets/ART_E_graphs.png" width="700" /&gt; 
&lt;h2&gt;üîÅ Training Loop Overview&lt;/h2&gt; 
&lt;p&gt;ART's functionality is divided into a &lt;strong&gt;client&lt;/strong&gt; and a &lt;strong&gt;server&lt;/strong&gt;. The OpenAI-compatible client is responsible for interfacing between ART and your codebase. Using the client, you can pass messages and get completions from your LLM as it improves. The server runs independently on any machine with a GPU. It abstracts away the complexity of the inference and training portions of the RL loop while allowing for some custom configuration. An outline of the training loop is shown below:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Inference&lt;/strong&gt;&lt;/p&gt; 
  &lt;ol&gt; 
   &lt;li&gt;Your code uses the ART client to perform an agentic workflow (usually executing several rollouts in parallel to gather data faster).&lt;/li&gt; 
   &lt;li&gt;Completion requests are routed to the ART server, which runs the model's latest LoRA in vLLM.&lt;/li&gt; 
   &lt;li&gt;As the agent executes, each &lt;code&gt;system&lt;/code&gt;, &lt;code&gt;user&lt;/code&gt;, and &lt;code&gt;assistant&lt;/code&gt; message is stored in a Trajectory.&lt;/li&gt; 
   &lt;li&gt;When a rollout finishes, your code assigns a &lt;code&gt;reward&lt;/code&gt; to its Trajectory, indicating the performance of the LLM.&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Training&lt;/strong&gt;&lt;/p&gt; 
  &lt;ol&gt; 
   &lt;li&gt;When each rollout has finished, Trajectories are grouped and sent to the server. Inference is blocked while training executes.&lt;/li&gt; 
   &lt;li&gt;The server trains your model using GRPO, initializing from the latest checkpoint (or an empty LoRA on the first iteration).&lt;/li&gt; 
   &lt;li&gt;The server saves the newly trained LoRA to a local directory and loads it into vLLM.&lt;/li&gt; 
   &lt;li&gt;Inference is unblocked and the loop resumes at step 1.&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;This training loop runs until a specified number of inference and training iterations have completed.&lt;/p&gt; 
&lt;h2&gt;üß© Supported Models&lt;/h2&gt; 
&lt;p&gt;ART should work with most vLLM/HuggingFace-transformers compatible causal language models, or at least the ones supported by &lt;a href="https://docs.unsloth.ai/get-started/all-our-models"&gt;Unsloth&lt;/a&gt;. Gemma 3 does not appear to be supported for the time being. If any other model isn't working for you, please let us know on &lt;a href="https://discord.gg/zbBHRUpwf4"&gt;Discord&lt;/a&gt; or open an issue on &lt;a href="https://github.com/openpipe/art/issues"&gt;GitHub&lt;/a&gt;!&lt;/p&gt; 
&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; 
&lt;p&gt;ART is in active development, and contributions are most welcome! Please see the &lt;a href="https://raw.githubusercontent.com/OpenPipe/ART/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; file for more information.&lt;/p&gt; 
&lt;h2&gt;üìñ Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{hilton2025art,
  author = {Brad Hilton and Kyle Corbitt and David Corbitt and Saumya Gandhi and Angky William and Bohdan Kovalenskyi and Andie Jones},
  title = {ART: Agent Reinforcement Trainer},
  year = {2025},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/openpipe/art}}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;‚öñÔ∏è License&lt;/h2&gt; 
&lt;p&gt;This repository's source code is available under the &lt;a href="https://raw.githubusercontent.com/OpenPipe/ART/main/LICENSE"&gt;Apache-2.0 License&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;üôè Credits&lt;/h2&gt; 
&lt;p&gt;ART stands on the shoulders of giants. While we owe many of the ideas and early experiments that led to ART's development to the open source RL community at large, we're especially grateful to the authors of the following projects:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/unslothai/unsloth"&gt;Unsloth&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vllm-project/vllm"&gt;vLLM&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/huggingface/trl"&gt;trl&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pytorch/torchtune"&gt;torchtune&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/skypilot-org/skypilot"&gt;SkyPilot&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Finally, thank you to our partners who've helped us test ART in the wild! We're excited to see what you all build with it.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ultralytics/ultralytics</title>
      <link>https://github.com/ultralytics/ultralytics</link>
      <description>&lt;p&gt;Ultralytics YOLO üöÄ&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt; &lt;a href="https://www.ultralytics.com/events/yolovision?utm_source=github&amp;amp;utm_medium=org&amp;amp;utm_campaign=yv25_event" target="_blank"&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/banner-yolov8.png" alt="Ultralytics YOLO banner" /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://docs.ultralytics.com/zh/"&gt;‰∏≠Êñá&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/ko/"&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/ja/"&gt;Êó•Êú¨Ë™û&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/ru/"&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/de/"&gt;Deutsch&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/fr/"&gt;Fran√ßais&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/es"&gt;Espa√±ol&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/pt/"&gt;Portugu√™s&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/tr/"&gt;T√ºrk√ße&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/vi/"&gt;Ti·∫øng Vi·ªát&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/ar/"&gt;ÿßŸÑÿπÿ±ÿ®Ÿäÿ©&lt;/a&gt; &lt;br /&gt;&lt;/p&gt; 
 &lt;div&gt; 
  &lt;a href="https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml"&gt;&lt;img src="https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg?sanitize=true" alt="Ultralytics CI" /&gt;&lt;/a&gt; 
  &lt;a href="https://clickpy.clickhouse.com/dashboard/ultralytics"&gt;&lt;img src="https://static.pepy.tech/badge/ultralytics" alt="Ultralytics Downloads" /&gt;&lt;/a&gt; 
  &lt;a href="https://discord.com/invite/ultralytics"&gt;&lt;img alt="Ultralytics Discord" src="https://img.shields.io/discord/1089800235347353640?logo=discord&amp;amp;logoColor=white&amp;amp;label=Discord&amp;amp;color=blue" /&gt;&lt;/a&gt; 
  &lt;a href="https://community.ultralytics.com/"&gt;&lt;img alt="Ultralytics Forums" src="https://img.shields.io/discourse/users?server=https%3A%2F%2Fcommunity.ultralytics.com&amp;amp;logo=discourse&amp;amp;label=Forums&amp;amp;color=blue" /&gt;&lt;/a&gt; 
  &lt;a href="https://www.reddit.com/r/ultralytics/"&gt;&lt;img alt="Ultralytics Reddit" src="https://img.shields.io/reddit/subreddit-subscribers/ultralytics?style=flat&amp;amp;logo=reddit&amp;amp;logoColor=white&amp;amp;label=Reddit&amp;amp;color=blue" /&gt;&lt;/a&gt; 
  &lt;br /&gt; 
  &lt;a href="https://console.paperspace.com/github/ultralytics/ultralytics"&gt;&lt;img src="https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true" alt="Run Ultralytics on Gradient" /&gt;&lt;/a&gt; 
  &lt;a href="https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open Ultralytics In Colab" /&gt;&lt;/a&gt; 
  &lt;a href="https://www.kaggle.com/models/ultralytics/yolo11"&gt;&lt;img src="https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true" alt="Open Ultralytics In Kaggle" /&gt;&lt;/a&gt; 
  &lt;a href="https://mybinder.org/v2/gh/ultralytics/ultralytics/HEAD?labpath=examples%2Ftutorial.ipynb"&gt;&lt;img src="https://mybinder.org/badge_logo.svg?sanitize=true" alt="Open Ultralytics In Binder" /&gt;&lt;/a&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;a href="https://www.ultralytics.com/"&gt;Ultralytics&lt;/a&gt; creates cutting-edge, state-of-the-art (SOTA) &lt;a href="https://www.ultralytics.com/yolo"&gt;YOLO models&lt;/a&gt; built on years of foundational research in computer vision and AI. Constantly updated for performance and flexibility, our models are &lt;strong&gt;fast&lt;/strong&gt;, &lt;strong&gt;accurate&lt;/strong&gt;, and &lt;strong&gt;easy to use&lt;/strong&gt;. They excel at &lt;a href="https://docs.ultralytics.com/tasks/detect/"&gt;object detection&lt;/a&gt;, &lt;a href="https://docs.ultralytics.com/modes/track/"&gt;tracking&lt;/a&gt;, &lt;a href="https://docs.ultralytics.com/tasks/segment/"&gt;instance segmentation&lt;/a&gt;, &lt;a href="https://docs.ultralytics.com/tasks/classify/"&gt;image classification&lt;/a&gt;, and &lt;a href="https://docs.ultralytics.com/tasks/pose/"&gt;pose estimation&lt;/a&gt; tasks.&lt;/p&gt; 
&lt;p&gt;Find detailed documentation in the &lt;a href="https://docs.ultralytics.com/"&gt;Ultralytics Docs&lt;/a&gt;. Get support via &lt;a href="https://github.com/ultralytics/ultralytics/issues/new/choose"&gt;GitHub Issues&lt;/a&gt;. Join discussions on &lt;a href="https://discord.com/invite/ultralytics"&gt;Discord&lt;/a&gt;, &lt;a href="https://www.reddit.com/r/ultralytics/"&gt;Reddit&lt;/a&gt;, and the &lt;a href="https://community.ultralytics.com/"&gt;Ultralytics Community Forums&lt;/a&gt;!&lt;/p&gt; 
&lt;p&gt;Request an Enterprise License for commercial use at &lt;a href="https://www.ultralytics.com/license"&gt;Ultralytics Licensing&lt;/a&gt;.&lt;/p&gt; 
&lt;a href="https://docs.ultralytics.com/models/yolo11/" target="_blank"&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/ultralytics/assets/refs/heads/main/yolo/performance-comparison.png" alt="YOLO11 performance plots" /&gt; &lt;/a&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/ultralytics"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-github.png" width="2%" alt="Ultralytics GitHub" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="2%" alt="space" /&gt; 
 &lt;a href="https://www.linkedin.com/company/ultralytics/"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-linkedin.png" width="2%" alt="Ultralytics LinkedIn" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="2%" alt="space" /&gt; 
 &lt;a href="https://twitter.com/ultralytics"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-twitter.png" width="2%" alt="Ultralytics Twitter" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="2%" alt="space" /&gt; 
 &lt;a href="https://www.youtube.com/ultralytics?sub_confirmation=1"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-youtube.png" width="2%" alt="Ultralytics YouTube" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="2%" alt="space" /&gt; 
 &lt;a href="https://www.tiktok.com/@ultralytics"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-tiktok.png" width="2%" alt="Ultralytics TikTok" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="2%" alt="space" /&gt; 
 &lt;a href="https://ultralytics.com/bilibili"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-bilibili.png" width="2%" alt="Ultralytics BiliBili" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="2%" alt="space" /&gt; 
 &lt;a href="https://discord.com/invite/ultralytics"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-discord.png" width="2%" alt="Ultralytics Discord" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;üìÑ Documentation&lt;/h2&gt; 
&lt;p&gt;See below for quickstart installation and usage examples. For comprehensive guidance on training, validation, prediction, and deployment, refer to our full &lt;a href="https://docs.ultralytics.com/"&gt;Ultralytics Docs&lt;/a&gt;.&lt;/p&gt; 
&lt;details open&gt; 
 &lt;summary&gt;Install&lt;/summary&gt; 
 &lt;p&gt;Install the &lt;code&gt;ultralytics&lt;/code&gt; package, including all &lt;a href="https://github.com/ultralytics/ultralytics/raw/main/pyproject.toml"&gt;requirements&lt;/a&gt;, in a &lt;a href="https://www.python.org/"&gt;&lt;strong&gt;Python&amp;gt;=3.8&lt;/strong&gt;&lt;/a&gt; environment with &lt;a href="https://pytorch.org/get-started/locally/"&gt;&lt;strong&gt;PyTorch&amp;gt;=1.8&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://pypi.org/project/ultralytics/"&gt;&lt;img src="https://img.shields.io/pypi/v/ultralytics?logo=pypi&amp;amp;logoColor=white" alt="PyPI - Version" /&gt;&lt;/a&gt; &lt;a href="https://clickpy.clickhouse.com/dashboard/ultralytics"&gt;&lt;img src="https://static.pepy.tech/badge/ultralytics" alt="Ultralytics Downloads" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/ultralytics/"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/ultralytics?logo=python&amp;amp;logoColor=gold" alt="PyPI - Python Version" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip install ultralytics
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;For alternative installation methods, including &lt;a href="https://anaconda.org/conda-forge/ultralytics"&gt;Conda&lt;/a&gt;, &lt;a href="https://hub.docker.com/r/ultralytics/ultralytics"&gt;Docker&lt;/a&gt;, and building from source via Git, please consult the &lt;a href="https://docs.ultralytics.com/quickstart/"&gt;Quickstart Guide&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://anaconda.org/conda-forge/ultralytics"&gt;&lt;img src="https://img.shields.io/conda/vn/conda-forge/ultralytics?logo=condaforge" alt="Conda Version" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/ultralytics/ultralytics"&gt;&lt;img src="https://img.shields.io/docker/v/ultralytics/ultralytics?sort=semver&amp;amp;logo=docker" alt="Docker Image Version" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/ultralytics/ultralytics"&gt;&lt;img src="https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker" alt="Ultralytics Docker Pulls" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details open&gt; 
 &lt;summary&gt;Usage&lt;/summary&gt; 
 &lt;h3&gt;CLI&lt;/h3&gt; 
 &lt;p&gt;You can use Ultralytics YOLO directly from the Command Line Interface (CLI) with the &lt;code&gt;yolo&lt;/code&gt; command:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Predict using a pretrained YOLO model (e.g., YOLO26n) on an image
yolo predict model=yolo26n.pt source='https://ultralytics.com/images/bus.jpg'
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;The &lt;code&gt;yolo&lt;/code&gt; command supports various tasks and modes, accepting additional arguments like &lt;code&gt;imgsz=640&lt;/code&gt;. Explore the YOLO &lt;a href="https://docs.ultralytics.com/usage/cli/"&gt;CLI Docs&lt;/a&gt; for more examples.&lt;/p&gt; 
 &lt;h3&gt;Python&lt;/h3&gt; 
 &lt;p&gt;Ultralytics YOLO can also be integrated directly into your Python projects. It accepts the same &lt;a href="https://docs.ultralytics.com/usage/cfg/"&gt;configuration arguments&lt;/a&gt; as the CLI:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from ultralytics import YOLO

# Load a pretrained YOLO26n model
model = YOLO("yolo26n.pt")

# Train the model on the COCO8 dataset for 100 epochs
train_results = model.train(
    data="coco8.yaml",  # Path to dataset configuration file
    epochs=100,  # Number of training epochs
    imgsz=640,  # Image size for training
    device="cpu",  # Device to run on (e.g., 'cpu', 0, [0,1,2,3])
)

# Evaluate the model's performance on the validation set
metrics = model.val()

# Perform object detection on an image
results = model("path/to/image.jpg")  # Predict on an image
results[0].show()  # Display results

# Export the model to ONNX format for deployment
path = model.export(format="onnx")  # Returns the path to the exported model
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Discover more examples in the YOLO &lt;a href="https://docs.ultralytics.com/usage/python/"&gt;Python Docs&lt;/a&gt;.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;‚ú® Models&lt;/h2&gt; 
&lt;p&gt;Ultralytics supports a wide range of YOLO models, from early versions like &lt;a href="https://docs.ultralytics.com/models/yolov3/"&gt;YOLOv3&lt;/a&gt; to the latest &lt;a href="https://docs.ultralytics.com/models/yolo26/"&gt;YOLO26&lt;/a&gt;. The tables below showcase YOLO26 models pretrained on the &lt;a href="https://docs.ultralytics.com/datasets/detect/coco/"&gt;COCO&lt;/a&gt; dataset for &lt;a href="https://docs.ultralytics.com/tasks/detect/"&gt;Detection&lt;/a&gt;, &lt;a href="https://docs.ultralytics.com/tasks/segment/"&gt;Segmentation&lt;/a&gt;, and &lt;a href="https://docs.ultralytics.com/tasks/pose/"&gt;Pose Estimation&lt;/a&gt;. Additionally, &lt;a href="https://docs.ultralytics.com/tasks/classify/"&gt;Classification&lt;/a&gt; models pretrained on the &lt;a href="https://docs.ultralytics.com/datasets/classify/imagenet/"&gt;ImageNet&lt;/a&gt; dataset are available. &lt;a href="https://docs.ultralytics.com/modes/track/"&gt;Tracking&lt;/a&gt; mode is compatible with all Detection, Segmentation, and Pose models. All &lt;a href="https://docs.ultralytics.com/models/"&gt;Models&lt;/a&gt; are automatically downloaded from the latest Ultralytics &lt;a href="https://github.com/ultralytics/assets/releases"&gt;release&lt;/a&gt; upon first use.&lt;/p&gt; 
&lt;a href="https://docs.ultralytics.com/tasks/" target="_blank"&gt; &lt;img width="100%" src="https://github.com/ultralytics/docs/releases/download/0/ultralytics-yolov8-tasks-banner.avif" alt="Ultralytics YOLO supported tasks" /&gt; &lt;/a&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;details open&gt;
 &lt;summary&gt;Detection (COCO)&lt;/summary&gt; 
 &lt;p&gt;Explore the &lt;a href="https://docs.ultralytics.com/tasks/detect/"&gt;Detection Docs&lt;/a&gt; for usage examples. These models are trained on the &lt;a href="https://cocodataset.org/"&gt;COCO dataset&lt;/a&gt;, featuring 80 object classes.&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;size&lt;br /&gt;&lt;sup&gt;(pixels)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;mAP&lt;sup&gt;val&lt;br /&gt;50-95&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;mAP&lt;sup&gt;val&lt;br /&gt;50-95(e2e)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Speed&lt;br /&gt;&lt;sup&gt;CPU ONNX&lt;br /&gt;(ms)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Speed&lt;br /&gt;&lt;sup&gt;T4 TensorRT10&lt;br /&gt;(ms)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;params&lt;br /&gt;&lt;sup&gt;(M)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;FLOPs&lt;br /&gt;&lt;sup&gt;(B)&lt;/sup&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26n.pt"&gt;YOLO26n&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;40.9&lt;/td&gt; 
    &lt;td&gt;40.1&lt;/td&gt; 
    &lt;td&gt;38.9 ¬± 0.7&lt;/td&gt; 
    &lt;td&gt;1.7 ¬± 0.0&lt;/td&gt; 
    &lt;td&gt;2.4&lt;/td&gt; 
    &lt;td&gt;5.4&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26s.pt"&gt;YOLO26s&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;48.6&lt;/td&gt; 
    &lt;td&gt;47.8&lt;/td&gt; 
    &lt;td&gt;87.2 ¬± 0.9&lt;/td&gt; 
    &lt;td&gt;2.5 ¬± 0.0&lt;/td&gt; 
    &lt;td&gt;9.5&lt;/td&gt; 
    &lt;td&gt;20.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26m.pt"&gt;YOLO26m&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;53.1&lt;/td&gt; 
    &lt;td&gt;52.5&lt;/td&gt; 
    &lt;td&gt;220.0 ¬± 1.4&lt;/td&gt; 
    &lt;td&gt;4.7 ¬± 0.1&lt;/td&gt; 
    &lt;td&gt;20.4&lt;/td&gt; 
    &lt;td&gt;68.2&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26l.pt"&gt;YOLO26l&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;55.0&lt;/td&gt; 
    &lt;td&gt;54.4&lt;/td&gt; 
    &lt;td&gt;286.2 ¬± 2.0&lt;/td&gt; 
    &lt;td&gt;6.2 ¬± 0.2&lt;/td&gt; 
    &lt;td&gt;24.8&lt;/td&gt; 
    &lt;td&gt;86.4&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26x.pt"&gt;YOLO26x&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;57.5&lt;/td&gt; 
    &lt;td&gt;56.9&lt;/td&gt; 
    &lt;td&gt;525.8 ¬± 4.0&lt;/td&gt; 
    &lt;td&gt;11.8 ¬± 0.2&lt;/td&gt; 
    &lt;td&gt;55.7&lt;/td&gt; 
    &lt;td&gt;193.9&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;mAP&lt;sup&gt;val&lt;/sup&gt;&lt;/strong&gt; values refer to single-model single-scale performance on the &lt;a href="https://cocodataset.org/"&gt;COCO val2017&lt;/a&gt; dataset. See &lt;a href="https://docs.ultralytics.com/guides/yolo-performance-metrics/"&gt;YOLO Performance Metrics&lt;/a&gt; for details. &lt;br /&gt;Reproduce with &lt;code&gt;yolo val detect data=coco.yaml device=0&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Speed&lt;/strong&gt; metrics are averaged over COCO val images using an &lt;a href="https://aws.amazon.com/ec2/instance-types/p4/"&gt;Amazon EC2 P4d&lt;/a&gt; instance. CPU speeds measured with &lt;a href="https://onnx.ai/"&gt;ONNX&lt;/a&gt; export. GPU speeds measured with &lt;a href="https://developer.nvidia.com/tensorrt"&gt;TensorRT&lt;/a&gt; export. &lt;br /&gt;Reproduce with &lt;code&gt;yolo val detect data=coco.yaml batch=1 device=0|cpu&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Segmentation (COCO)&lt;/summary&gt; 
 &lt;p&gt;Refer to the &lt;a href="https://docs.ultralytics.com/tasks/segment/"&gt;Segmentation Docs&lt;/a&gt; for usage examples. These models are trained on &lt;a href="https://docs.ultralytics.com/datasets/segment/coco/"&gt;COCO-Seg&lt;/a&gt;, including 80 classes.&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;size&lt;br /&gt;&lt;sup&gt;(pixels)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;mAP&lt;sup&gt;box&lt;br /&gt;50-95(e2e)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;mAP&lt;sup&gt;mask&lt;br /&gt;50-95(e2e)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Speed&lt;br /&gt;&lt;sup&gt;CPU ONNX&lt;br /&gt;(ms)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Speed&lt;br /&gt;&lt;sup&gt;T4 TensorRT10&lt;br /&gt;(ms)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;params&lt;br /&gt;&lt;sup&gt;(M)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;FLOPs&lt;br /&gt;&lt;sup&gt;(B)&lt;/sup&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26n-seg.pt"&gt;YOLO26n-seg&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;39.6&lt;/td&gt; 
    &lt;td&gt;33.9&lt;/td&gt; 
    &lt;td&gt;53.3 ¬± 0.5&lt;/td&gt; 
    &lt;td&gt;2.1 ¬± 0.0&lt;/td&gt; 
    &lt;td&gt;2.7&lt;/td&gt; 
    &lt;td&gt;9.1&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26s-seg.pt"&gt;YOLO26s-seg&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;47.3&lt;/td&gt; 
    &lt;td&gt;40.0&lt;/td&gt; 
    &lt;td&gt;118.4 ¬± 0.9&lt;/td&gt; 
    &lt;td&gt;3.3 ¬± 0.0&lt;/td&gt; 
    &lt;td&gt;10.4&lt;/td&gt; 
    &lt;td&gt;34.2&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26m-seg.pt"&gt;YOLO26m-seg&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;52.5&lt;/td&gt; 
    &lt;td&gt;44.1&lt;/td&gt; 
    &lt;td&gt;328.2 ¬± 2.4&lt;/td&gt; 
    &lt;td&gt;6.7 ¬± 0.1&lt;/td&gt; 
    &lt;td&gt;23.6&lt;/td&gt; 
    &lt;td&gt;121.5&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26l-seg.pt"&gt;YOLO26l-seg&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;54.4&lt;/td&gt; 
    &lt;td&gt;45.5&lt;/td&gt; 
    &lt;td&gt;387.0 ¬± 3.7&lt;/td&gt; 
    &lt;td&gt;8.0 ¬± 0.1&lt;/td&gt; 
    &lt;td&gt;28.0&lt;/td&gt; 
    &lt;td&gt;139.8&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26x-seg.pt"&gt;YOLO26x-seg&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;56.5&lt;/td&gt; 
    &lt;td&gt;47.0&lt;/td&gt; 
    &lt;td&gt;787.0 ¬± 6.8&lt;/td&gt; 
    &lt;td&gt;16.4 ¬± 0.1&lt;/td&gt; 
    &lt;td&gt;62.8&lt;/td&gt; 
    &lt;td&gt;313.5&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;mAP&lt;sup&gt;val&lt;/sup&gt;&lt;/strong&gt; values are for single-model single-scale on the &lt;a href="https://cocodataset.org/"&gt;COCO val2017&lt;/a&gt; dataset. See &lt;a href="https://docs.ultralytics.com/guides/yolo-performance-metrics/"&gt;YOLO Performance Metrics&lt;/a&gt; for details. &lt;br /&gt;Reproduce with &lt;code&gt;yolo val segment data=coco.yaml device=0&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Speed&lt;/strong&gt; metrics are averaged over COCO val images using an &lt;a href="https://aws.amazon.com/ec2/instance-types/p4/"&gt;Amazon EC2 P4d&lt;/a&gt; instance. CPU speeds measured with &lt;a href="https://onnx.ai/"&gt;ONNX&lt;/a&gt; export. GPU speeds measured with &lt;a href="https://developer.nvidia.com/tensorrt"&gt;TensorRT&lt;/a&gt; export. &lt;br /&gt;Reproduce with &lt;code&gt;yolo val segment data=coco.yaml batch=1 device=0|cpu&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Classification (ImageNet)&lt;/summary&gt; 
 &lt;p&gt;Consult the &lt;a href="https://docs.ultralytics.com/tasks/classify/"&gt;Classification Docs&lt;/a&gt; for usage examples. These models are trained on &lt;a href="https://docs.ultralytics.com/datasets/classify/imagenet/"&gt;ImageNet&lt;/a&gt;, covering 1000 classes.&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;size&lt;br /&gt;&lt;sup&gt;(pixels)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;acc&lt;br /&gt;&lt;sup&gt;top1&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;acc&lt;br /&gt;&lt;sup&gt;top5&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Speed&lt;br /&gt;&lt;sup&gt;CPU ONNX&lt;br /&gt;(ms)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Speed&lt;br /&gt;&lt;sup&gt;T4 TensorRT10&lt;br /&gt;(ms)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;params&lt;br /&gt;&lt;sup&gt;(M)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;FLOPs&lt;br /&gt;&lt;sup&gt;(B) at 224&lt;/sup&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26n-cls.pt"&gt;YOLO26n-cls&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;224&lt;/td&gt; 
    &lt;td&gt;71.4&lt;/td&gt; 
    &lt;td&gt;90.1&lt;/td&gt; 
    &lt;td&gt;5.0 ¬± 0.3&lt;/td&gt; 
    &lt;td&gt;1.1 ¬± 0.0&lt;/td&gt; 
    &lt;td&gt;2.8&lt;/td&gt; 
    &lt;td&gt;0.5&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26s-cls.pt"&gt;YOLO26s-cls&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;224&lt;/td&gt; 
    &lt;td&gt;76.0&lt;/td&gt; 
    &lt;td&gt;92.9&lt;/td&gt; 
    &lt;td&gt;7.9 ¬± 0.2&lt;/td&gt; 
    &lt;td&gt;1.3 ¬± 0.0&lt;/td&gt; 
    &lt;td&gt;6.7&lt;/td&gt; 
    &lt;td&gt;1.6&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26m-cls.pt"&gt;YOLO26m-cls&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;224&lt;/td&gt; 
    &lt;td&gt;78.1&lt;/td&gt; 
    &lt;td&gt;94.2&lt;/td&gt; 
    &lt;td&gt;17.2 ¬± 0.4&lt;/td&gt; 
    &lt;td&gt;2.0 ¬± 0.0&lt;/td&gt; 
    &lt;td&gt;11.6&lt;/td&gt; 
    &lt;td&gt;4.9&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26l-cls.pt"&gt;YOLO26l-cls&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;224&lt;/td&gt; 
    &lt;td&gt;79.0&lt;/td&gt; 
    &lt;td&gt;94.6&lt;/td&gt; 
    &lt;td&gt;23.2 ¬± 0.3&lt;/td&gt; 
    &lt;td&gt;2.8 ¬± 0.0&lt;/td&gt; 
    &lt;td&gt;14.1&lt;/td&gt; 
    &lt;td&gt;6.2&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26x-cls.pt"&gt;YOLO26x-cls&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;224&lt;/td&gt; 
    &lt;td&gt;79.9&lt;/td&gt; 
    &lt;td&gt;95.0&lt;/td&gt; 
    &lt;td&gt;41.4 ¬± 0.9&lt;/td&gt; 
    &lt;td&gt;3.8 ¬± 0.0&lt;/td&gt; 
    &lt;td&gt;29.6&lt;/td&gt; 
    &lt;td&gt;13.6&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;acc&lt;/strong&gt; values represent model accuracy on the &lt;a href="https://www.image-net.org/"&gt;ImageNet&lt;/a&gt; dataset validation set. &lt;br /&gt;Reproduce with &lt;code&gt;yolo val classify data=path/to/ImageNet device=0&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Speed&lt;/strong&gt; metrics are averaged over ImageNet val images using an &lt;a href="https://aws.amazon.com/ec2/instance-types/p4/"&gt;Amazon EC2 P4d&lt;/a&gt; instance. CPU speeds measured with &lt;a href="https://onnx.ai/"&gt;ONNX&lt;/a&gt; export. GPU speeds measured with &lt;a href="https://developer.nvidia.com/tensorrt"&gt;TensorRT&lt;/a&gt; export. &lt;br /&gt;Reproduce with &lt;code&gt;yolo val classify data=path/to/ImageNet batch=1 device=0|cpu&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Pose (COCO)&lt;/summary&gt; 
 &lt;p&gt;See the &lt;a href="https://docs.ultralytics.com/tasks/pose/"&gt;Pose Estimation Docs&lt;/a&gt; for usage examples. These models are trained on &lt;a href="https://docs.ultralytics.com/datasets/pose/coco/"&gt;COCO-Pose&lt;/a&gt;, focusing on the 'person' class.&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;size&lt;br /&gt;&lt;sup&gt;(pixels)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;mAP&lt;sup&gt;pose&lt;br /&gt;50-95(e2e)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;mAP&lt;sup&gt;pose&lt;br /&gt;50(e2e)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Speed&lt;br /&gt;&lt;sup&gt;CPU ONNX&lt;br /&gt;(ms)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Speed&lt;br /&gt;&lt;sup&gt;T4 TensorRT10&lt;br /&gt;(ms)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;params&lt;br /&gt;&lt;sup&gt;(M)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;FLOPs&lt;br /&gt;&lt;sup&gt;(B)&lt;/sup&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26n-pose.pt"&gt;YOLO26n-pose&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;57.2&lt;/td&gt; 
    &lt;td&gt;83.3&lt;/td&gt; 
    &lt;td&gt;40.3 ¬± 0.5&lt;/td&gt; 
    &lt;td&gt;1.8 ¬± 0.0&lt;/td&gt; 
    &lt;td&gt;2.9&lt;/td&gt; 
    &lt;td&gt;7.5&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26s-pose.pt"&gt;YOLO26s-pose&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;63.0&lt;/td&gt; 
    &lt;td&gt;86.6&lt;/td&gt; 
    &lt;td&gt;85.3 ¬± 0.9&lt;/td&gt; 
    &lt;td&gt;2.7 ¬± 0.0&lt;/td&gt; 
    &lt;td&gt;10.4&lt;/td&gt; 
    &lt;td&gt;23.9&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26m-pose.pt"&gt;YOLO26m-pose&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;68.8&lt;/td&gt; 
    &lt;td&gt;89.6&lt;/td&gt; 
    &lt;td&gt;218.0 ¬± 1.5&lt;/td&gt; 
    &lt;td&gt;5.0 ¬± 0.1&lt;/td&gt; 
    &lt;td&gt;21.5&lt;/td&gt; 
    &lt;td&gt;73.1&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26l-pose.pt"&gt;YOLO26l-pose&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;70.4&lt;/td&gt; 
    &lt;td&gt;90.5&lt;/td&gt; 
    &lt;td&gt;275.4 ¬± 2.4&lt;/td&gt; 
    &lt;td&gt;6.5 ¬± 0.1&lt;/td&gt; 
    &lt;td&gt;25.9&lt;/td&gt; 
    &lt;td&gt;91.3&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26x-pose.pt"&gt;YOLO26x-pose&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;71.6&lt;/td&gt; 
    &lt;td&gt;91.6&lt;/td&gt; 
    &lt;td&gt;565.4 ¬± 3.0&lt;/td&gt; 
    &lt;td&gt;12.2 ¬± 0.2&lt;/td&gt; 
    &lt;td&gt;57.6&lt;/td&gt; 
    &lt;td&gt;201.7&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;mAP&lt;sup&gt;val&lt;/sup&gt;&lt;/strong&gt; values are for single-model single-scale on the &lt;a href="https://docs.ultralytics.com/datasets/pose/coco/"&gt;COCO Keypoints val2017&lt;/a&gt; dataset. See &lt;a href="https://docs.ultralytics.com/guides/yolo-performance-metrics/"&gt;YOLO Performance Metrics&lt;/a&gt; for details. &lt;br /&gt;Reproduce with &lt;code&gt;yolo val pose data=coco-pose.yaml device=0&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Speed&lt;/strong&gt; metrics are averaged over COCO val images using an &lt;a href="https://aws.amazon.com/ec2/instance-types/p4/"&gt;Amazon EC2 P4d&lt;/a&gt; instance. CPU speeds measured with &lt;a href="https://onnx.ai/"&gt;ONNX&lt;/a&gt; export. GPU speeds measured with &lt;a href="https://developer.nvidia.com/tensorrt"&gt;TensorRT&lt;/a&gt; export. &lt;br /&gt;Reproduce with &lt;code&gt;yolo val pose data=coco-pose.yaml batch=1 device=0|cpu&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Oriented Bounding Boxes (DOTAv1)&lt;/summary&gt; 
 &lt;p&gt;Check the &lt;a href="https://docs.ultralytics.com/tasks/obb/"&gt;OBB Docs&lt;/a&gt; for usage examples. These models are trained on &lt;a href="https://docs.ultralytics.com/datasets/obb/dota-v2/#dota-v10/"&gt;DOTAv1&lt;/a&gt;, including 15 classes.&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;size&lt;br /&gt;&lt;sup&gt;(pixels)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;mAP&lt;sup&gt;test&lt;br /&gt;50-95(e2e)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;mAP&lt;sup&gt;test&lt;br /&gt;50(e2e)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Speed&lt;br /&gt;&lt;sup&gt;CPU ONNX&lt;br /&gt;(ms)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Speed&lt;br /&gt;&lt;sup&gt;T4 TensorRT10&lt;br /&gt;(ms)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;params&lt;br /&gt;&lt;sup&gt;(M)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;FLOPs&lt;br /&gt;&lt;sup&gt;(B)&lt;/sup&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26n-obb.pt"&gt;YOLO26n-obb&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;1024&lt;/td&gt; 
    &lt;td&gt;52.4&lt;/td&gt; 
    &lt;td&gt;78.9&lt;/td&gt; 
    &lt;td&gt;97.7 ¬± 0.9&lt;/td&gt; 
    &lt;td&gt;2.8 ¬± 0.0&lt;/td&gt; 
    &lt;td&gt;2.5&lt;/td&gt; 
    &lt;td&gt;14.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26s-obb.pt"&gt;YOLO26s-obb&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;1024&lt;/td&gt; 
    &lt;td&gt;54.8&lt;/td&gt; 
    &lt;td&gt;80.9&lt;/td&gt; 
    &lt;td&gt;218.0 ¬± 1.4&lt;/td&gt; 
    &lt;td&gt;4.9 ¬± 0.1&lt;/td&gt; 
    &lt;td&gt;9.8&lt;/td&gt; 
    &lt;td&gt;55.1&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26m-obb.pt"&gt;YOLO26m-obb&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;1024&lt;/td&gt; 
    &lt;td&gt;55.3&lt;/td&gt; 
    &lt;td&gt;81.0&lt;/td&gt; 
    &lt;td&gt;579.2 ¬± 3.8&lt;/td&gt; 
    &lt;td&gt;10.2 ¬± 0.3&lt;/td&gt; 
    &lt;td&gt;21.2&lt;/td&gt; 
    &lt;td&gt;183.3&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26l-obb.pt"&gt;YOLO26l-obb&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;1024&lt;/td&gt; 
    &lt;td&gt;56.2&lt;/td&gt; 
    &lt;td&gt;81.6&lt;/td&gt; 
    &lt;td&gt;735.6 ¬± 3.1&lt;/td&gt; 
    &lt;td&gt;13.0 ¬± 0.2&lt;/td&gt; 
    &lt;td&gt;25.6&lt;/td&gt; 
    &lt;td&gt;230.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26x-obb.pt"&gt;YOLO26x-obb&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;1024&lt;/td&gt; 
    &lt;td&gt;56.7&lt;/td&gt; 
    &lt;td&gt;81.7&lt;/td&gt; 
    &lt;td&gt;1485.7 ¬± 11.5&lt;/td&gt; 
    &lt;td&gt;30.5 ¬± 0.9&lt;/td&gt; 
    &lt;td&gt;57.6&lt;/td&gt; 
    &lt;td&gt;516.5&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;mAP&lt;sup&gt;test&lt;/sup&gt;&lt;/strong&gt; values are for single-model multiscale performance on the &lt;a href="https://captain-whu.github.io/DOTA/dataset.html"&gt;DOTAv1 test set&lt;/a&gt;. &lt;br /&gt;Reproduce by &lt;code&gt;yolo val obb data=DOTAv1.yaml device=0 split=test&lt;/code&gt; and submit merged results to the &lt;a href="https://captain-whu.github.io/DOTA/evaluation.html"&gt;DOTA evaluation server&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Speed&lt;/strong&gt; metrics are averaged over &lt;a href="https://docs.ultralytics.com/datasets/obb/dota-v2/#dota-v10"&gt;DOTAv1 val images&lt;/a&gt; using an &lt;a href="https://aws.amazon.com/ec2/instance-types/p4/"&gt;Amazon EC2 P4d&lt;/a&gt; instance. CPU speeds measured with &lt;a href="https://onnx.ai/"&gt;ONNX&lt;/a&gt; export. GPU speeds measured with &lt;a href="https://developer.nvidia.com/tensorrt"&gt;TensorRT&lt;/a&gt; export. &lt;br /&gt;Reproduce by &lt;code&gt;yolo val obb data=DOTAv1.yaml batch=1 device=0|cpu&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;üß© Integrations&lt;/h2&gt; 
&lt;p&gt;Our key integrations with leading AI platforms extend the functionality of Ultralytics' offerings, enhancing tasks like dataset labeling, training, visualization, and model management. Discover how Ultralytics, in collaboration with partners like &lt;a href="https://docs.ultralytics.com/integrations/weights-biases/"&gt;Weights &amp;amp; Biases&lt;/a&gt;, &lt;a href="https://docs.ultralytics.com/integrations/comet/"&gt;Comet ML&lt;/a&gt;, &lt;a href="https://docs.ultralytics.com/integrations/roboflow/"&gt;Roboflow&lt;/a&gt;, and &lt;a href="https://docs.ultralytics.com/integrations/openvino/"&gt;Intel OpenVINO&lt;/a&gt;, can optimize your AI workflow. Explore more at &lt;a href="https://docs.ultralytics.com/integrations/"&gt;Ultralytics Integrations&lt;/a&gt;.&lt;/p&gt; 
&lt;a href="https://docs.ultralytics.com/integrations/" target="_blank"&gt; &lt;img width="100%" src="https://github.com/ultralytics/assets/raw/main/yolov8/banner-integrations.png" alt="Ultralytics active learning integrations" /&gt; &lt;/a&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://www.ultralytics.com/hub"&gt; &lt;img src="https://github.com/ultralytics/assets/raw/main/partners/logo-ultralytics-hub.png" width="10%" alt="Ultralytics HUB logo" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="15%" height="0" alt="space" /&gt; 
 &lt;a href="https://docs.ultralytics.com/integrations/weights-biases/"&gt; &lt;img src="https://github.com/ultralytics/assets/raw/main/partners/logo-wb.png" width="10%" alt="Weights &amp;amp; Biases logo" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="15%" height="0" alt="space" /&gt; 
 &lt;a href="https://docs.ultralytics.com/integrations/comet/"&gt; &lt;img src="https://github.com/ultralytics/assets/raw/main/partners/logo-comet.png" width="10%" alt="Comet ML logo" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="15%" height="0" alt="space" /&gt; 
 &lt;a href="https://docs.ultralytics.com/integrations/neural-magic/"&gt; &lt;img src="https://github.com/ultralytics/assets/raw/main/partners/logo-neuralmagic.png" width="10%" alt="Neural Magic logo" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Ultralytics HUB üåü&lt;/th&gt; 
   &lt;th align="center"&gt;Weights &amp;amp; Biases&lt;/th&gt; 
   &lt;th align="center"&gt;Comet&lt;/th&gt; 
   &lt;th align="center"&gt;Neural Magic&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Streamline YOLO workflows: Label, train, and deploy effortlessly with &lt;a href="https://hub.ultralytics.com/"&gt;Ultralytics HUB&lt;/a&gt;. Try now!&lt;/td&gt; 
   &lt;td align="center"&gt;Track experiments, hyperparameters, and results with &lt;a href="https://docs.ultralytics.com/integrations/weights-biases/"&gt;Weights &amp;amp; Biases&lt;/a&gt;.&lt;/td&gt; 
   &lt;td align="center"&gt;Free forever, &lt;a href="https://docs.ultralytics.com/integrations/comet/"&gt;Comet ML&lt;/a&gt; lets you save YOLO models, resume training, and interactively visualize predictions.&lt;/td&gt; 
   &lt;td align="center"&gt;Run YOLO inference up to 6x faster with &lt;a href="https://docs.ultralytics.com/integrations/neural-magic/"&gt;Neural Magic DeepSparse&lt;/a&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;ü§ù Contribute&lt;/h2&gt; 
&lt;p&gt;We thrive on community collaboration! Ultralytics YOLO wouldn't be the SOTA framework it is without contributions from developers like you. Please see our &lt;a href="https://docs.ultralytics.com/help/contributing/"&gt;Contributing Guide&lt;/a&gt; to get started. We also welcome your feedback‚Äîshare your experience by completing our &lt;a href="https://www.ultralytics.com/survey?utm_source=github&amp;amp;utm_medium=social&amp;amp;utm_campaign=Survey"&gt;Survey&lt;/a&gt;. A huge &lt;strong&gt;Thank You&lt;/strong&gt; üôè to everyone who contributes!&lt;/p&gt; 
&lt;!-- SVG image from https://opencollective.com/ultralytics/contributors.svg?width=1280 --&gt; 
&lt;p&gt;&lt;a href="https://github.com/ultralytics/ultralytics/graphs/contributors"&gt;&lt;img src="https://raw.githubusercontent.com/ultralytics/assets/main/im/image-contributors.png" alt="Ultralytics open-source contributors" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;We look forward to your contributions to help make the Ultralytics ecosystem even better!&lt;/p&gt; 
&lt;h2&gt;üìú License&lt;/h2&gt; 
&lt;p&gt;Ultralytics offers two licensing options to suit different needs:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;AGPL-3.0 License&lt;/strong&gt;: This &lt;a href="https://opensource.org/license/agpl-v3"&gt;OSI-approved&lt;/a&gt; open-source license is perfect for students, researchers, and enthusiasts. It encourages open collaboration and knowledge sharing. See the &lt;a href="https://github.com/ultralytics/ultralytics/raw/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for full details.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Ultralytics Enterprise License&lt;/strong&gt;: Designed for commercial use, this license allows for the seamless integration of Ultralytics software and AI models into commercial products and services, bypassing the open-source requirements of AGPL-3.0. If your use case involves commercial deployment, please contact us via &lt;a href="https://www.ultralytics.com/license"&gt;Ultralytics Licensing&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìû Contact&lt;/h2&gt; 
&lt;p&gt;For bug reports and feature requests related to Ultralytics software, please visit &lt;a href="https://github.com/ultralytics/ultralytics/issues"&gt;GitHub Issues&lt;/a&gt;. For questions, discussions, and community support, join our active communities on &lt;a href="https://discord.com/invite/ultralytics"&gt;Discord&lt;/a&gt;, &lt;a href="https://www.reddit.com/r/ultralytics/"&gt;Reddit&lt;/a&gt;, and the &lt;a href="https://community.ultralytics.com/"&gt;Ultralytics Community Forums&lt;/a&gt;. We're here to help with all things Ultralytics!&lt;/p&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/ultralytics"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-github.png" width="3%" alt="Ultralytics GitHub" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="3%" alt="space" /&gt; 
 &lt;a href="https://www.linkedin.com/company/ultralytics/"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-linkedin.png" width="3%" alt="Ultralytics LinkedIn" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="3%" alt="space" /&gt; 
 &lt;a href="https://twitter.com/ultralytics"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-twitter.png" width="3%" alt="Ultralytics Twitter" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="3%" alt="space" /&gt; 
 &lt;a href="https://www.youtube.com/ultralytics?sub_confirmation=1"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-youtube.png" width="3%" alt="Ultralytics YouTube" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="3%" alt="space" /&gt; 
 &lt;a href="https://www.tiktok.com/@ultralytics"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-tiktok.png" width="3%" alt="Ultralytics TikTok" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="3%" alt="space" /&gt; 
 &lt;a href="https://ultralytics.com/bilibili"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-bilibili.png" width="3%" alt="Ultralytics BiliBili" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="3%" alt="space" /&gt; 
 &lt;a href="https://discord.com/invite/ultralytics"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-discord.png" width="3%" alt="Ultralytics Discord" /&gt;&lt;/a&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>airbytehq/airbyte</title>
      <link>https://github.com/airbytehq/airbyte</link>
      <description>&lt;p&gt;The leading data integration platform for ETL / ELT data pipelines from APIs, databases &amp; files to data warehouses, data lakes &amp; data lakehouses. Both self-hosted and Cloud-hosted.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://airbyte.com"&gt;&lt;img src="https://assets.website-files.com/605e01bc25f7e19a82e74788/624d9c4a375a55100be6b257_Airbyte_logo_color_dark.svg?sanitize=true" alt="Airbyte" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;em&gt;Data integration platform for ELT pipelines from APIs, databases &amp;amp; files to databases, warehouses &amp;amp; lakes&lt;/em&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/airbytehq/airbyte/stargazers/" target="_blank"&gt; &lt;img src="https://img.shields.io/github/stars/airbytehq/airbyte?style=social&amp;amp;label=Star&amp;amp;maxAge=2592000" alt="Test" /&gt; &lt;/a&gt; &lt;a href="https://github.com/airbytehq/airbyte/releases" target="_blank"&gt; &lt;img src="https://img.shields.io/github/v/release/airbytehq/airbyte?color=white" alt="Release" /&gt; &lt;/a&gt; &lt;a href="https://airbytehq.slack.com/" target="_blank"&gt; &lt;img src="https://img.shields.io/badge/slack-join-white.svg?logo=slack" alt="Slack" /&gt; &lt;/a&gt; &lt;a href="https://www.youtube.com/c/AirbyteHQ/?sub_confirmation=1" target="_blank"&gt; &lt;img alt="YouTube Channel Views" src="https://img.shields.io/youtube/channel/views/UCQ_JWEFzs1_INqdhIO3kmrw?style=social" /&gt; &lt;/a&gt; &lt;a href="https://github.com/airbytehq/airbyte/actions/workflows/gradle.yml" target="_blank"&gt; &lt;img src="https://img.shields.io/github/actions/workflow/status/airbytehq/airbyte/gradle.yml?branch=master" alt="Build" /&gt; &lt;/a&gt; &lt;a href="https://github.com/airbytehq/airbyte/tree/master/docs/project-overview/licenses" target="_blank"&gt; &lt;img src="https://img.shields.io/static/v1?label=license&amp;amp;message=MIT&amp;amp;color=white" alt="License" /&gt; &lt;/a&gt; &lt;a href="https://github.com/airbytehq/airbyte/tree/master/docs/project-overview/licenses" target="_blank"&gt; &lt;img src="https://img.shields.io/static/v1?label=license&amp;amp;message=ELv2&amp;amp;color=white" alt="License" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;We believe that only an &lt;strong&gt;open-source solution to data movement&lt;/strong&gt; can cover the long tail of data sources while empowering data engineers to customize existing connectors. Our ultimate vision is to help you move data from any source to any destination. Airbyte provides a &lt;a href="https://docs.airbyte.com/integrations/"&gt;catalog&lt;/a&gt; of 600+ connectors for APIs, databases, data warehouses, and data lakes.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/airbytehq/airbyte/assets/38087517/35b01d0b-00bf-407b-87e6-a5cd5cd720b5" alt="Airbyte Connections UI" /&gt; &lt;em&gt;Screenshot taken from &lt;a href="https://cloud.airbyte.com/signup"&gt;Airbyte Cloud&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt; 
&lt;h3&gt;Getting Started&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.airbyte.com/quickstart/deploy-airbyte"&gt;Deploy Airbyte Open Source&lt;/a&gt; or set up &lt;a href="https://docs.airbyte.com/cloud/getting-started-with-airbyte-cloud"&gt;Airbyte Cloud&lt;/a&gt; to start centralizing your data.&lt;/li&gt; 
 &lt;li&gt;Create connectors in minutes with our &lt;a href="https://docs.airbyte.com/connector-development/connector-builder-ui/overview"&gt;no-code Connector Builder&lt;/a&gt; or &lt;a href="https://docs.airbyte.com/connector-development/config-based/low-code-cdk-overview"&gt;low-code CDK&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Explore popular use cases in our &lt;a href="https://airbyte.com/tutorials"&gt;tutorials&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Orchestrate Airbyte syncs with &lt;a href="https://docs.airbyte.com/operator-guides/using-the-airflow-airbyte-operator"&gt;Airflow&lt;/a&gt;, &lt;a href="https://docs.airbyte.com/operator-guides/using-prefect-task"&gt;Prefect&lt;/a&gt;, &lt;a href="https://docs.airbyte.com/operator-guides/using-dagster-integration"&gt;Dagster&lt;/a&gt;, &lt;a href="https://docs.airbyte.com/operator-guides/using-kestra-plugin"&gt;Kestra&lt;/a&gt;, or the &lt;a href="https://reference.airbyte.com/"&gt;Airbyte API&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Try it out yourself with our &lt;a href="https://demo.airbyte.io/"&gt;demo app&lt;/a&gt;, visit our &lt;a href="https://docs.airbyte.com/"&gt;full documentation&lt;/a&gt;, and learn more about &lt;a href="https://airbyte.com/blog-categories/company-updates"&gt;recent announcements&lt;/a&gt;. See our &lt;a href="https://connectors.airbyte.com/files/generated_reports/connector_registry_report.html"&gt;registry&lt;/a&gt; for a full list of connectors already available in Airbyte or Airbyte Cloud.&lt;/p&gt; 
&lt;h3&gt;Join the Airbyte Community&lt;/h3&gt; 
&lt;p&gt;The Airbyte community can be found in the &lt;a href="https://airbyte.com/community"&gt;Airbyte Community Slack&lt;/a&gt;, where you can ask questions and voice ideas. You can also ask for help in our &lt;a href="https://github.com/airbytehq/airbyte/discussions"&gt;Airbyte Forum&lt;/a&gt;. Airbyte's roadmap is publicly viewable on &lt;a href="https://github.com/orgs/airbytehq/projects/37/views/1?pane=issue&amp;amp;itemId=26937554"&gt;GitHub&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For videos and blogs on data engineering and building your data stack, check out Airbyte's &lt;a href="https://airbyte.com/content-hub"&gt;Content Hub&lt;/a&gt;, &lt;a href="https://www.youtube.com/c/AirbyteHQ"&gt;YouTube&lt;/a&gt;, and sign up for our &lt;a href="https://airbyte.com/newsletter"&gt;newsletter&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Contributing&lt;/h3&gt; 
&lt;p&gt;If you've found a problem with Airbyte, please open a &lt;a href="https://github.com/airbytehq/airbyte/issues/new/choose"&gt;GitHub issue&lt;/a&gt;. To contribute to Airbyte and see our Code of Conduct, please see the &lt;a href="https://docs.airbyte.com/contributing-to-airbyte/"&gt;contributing guide&lt;/a&gt;. We have a list of &lt;a href="https://github.com/airbytehq/airbyte/labels/contributor-program"&gt;good first issues&lt;/a&gt; that contain bugs that have a relatively limited scope. This is a great place to get started, gain experience, and get familiar with our contribution process.&lt;/p&gt; 
&lt;h4&gt;PR Permission Requirements&lt;/h4&gt; 
&lt;p&gt;When submitting a pull request, please ensure that Airbyte maintainers have write access to your branch. This allows us to apply formatting fixes and dependency updates directly, significantly speeding up the review and approval process.&lt;/p&gt; 
&lt;p&gt;To enable write access on your PR from Airbyte maintainers, please check the "Allow edits from maintainers" box when submitting from your PR. You must also create your PR from a fork in your &lt;strong&gt;personal GitHub account&lt;/strong&gt; rather than an organization account, or else you will not see this option. The requirement to create from your personal fork is based on GitHub's additional security restrictions for PRs created from organization forks. For more information about the GitHub security model, please see the &lt;a href="https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/allowing-changes-to-a-pull-request-branch-created-from-a-fork"&gt;GitHub documentation page regarding PRs from forks&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For more details on contribution requirements, please see our &lt;a href="https://docs.airbyte.com/platform/contributing-to-airbyte#standard-contribution-workflow"&gt;contribution workflow documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Security&lt;/h3&gt; 
&lt;p&gt;Airbyte takes security issues very seriously. &lt;strong&gt;Please do not file GitHub issues or post on our public forum for security vulnerabilities&lt;/strong&gt;. Email &lt;code&gt;security@airbyte.io&lt;/code&gt; if you believe you have uncovered a vulnerability. In the message, try to provide a description of the issue and ideally a way of reproducing it. The security team will get back to you as soon as possible.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://airbyte.com/airbyte-enterprise"&gt;Airbyte Enterprise&lt;/a&gt; also offers additional security features (among others) on top of Airbyte open-source.&lt;/p&gt; 
&lt;h3&gt;License&lt;/h3&gt; 
&lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/airbytehq/airbyte/master/docs/LICENSE"&gt;LICENSE&lt;/a&gt; file for licensing information, and our &lt;a href="https://docs.airbyte.com/platform/developer-guides/licenses/license-faq"&gt;FAQ&lt;/a&gt; for any questions you may have on that topic.&lt;/p&gt; 
&lt;h3&gt;Thank You&lt;/h3&gt; 
&lt;p&gt;Airbyte would not be possible without the support and assistance of other open-source tools and companies! Visit our &lt;a href="https://raw.githubusercontent.com/airbytehq/airbyte/master/THANK-YOU.md"&gt;thank you page&lt;/a&gt; to learn more about how we build Airbyte.&lt;/p&gt; 
&lt;a href="https://github.com/airbytehq/airbyte/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=airbytehq/airbyte" /&gt; &lt;/a&gt;</description>
    </item>
    
    <item>
      <title>chidiwilliams/buzz</title>
      <link>https://github.com/chidiwilliams/buzz</link>
      <description>&lt;p&gt;Buzz transcribes and translates audio offline on your personal computer. Powered by OpenAI's Whisper.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;[&lt;a href="https://raw.githubusercontent.com/chidiwilliams/buzz/main/readme/README.zh_CN.md"&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt;] &amp;lt;- ÁÇπÂáªÊü•Áúã‰∏≠ÊñáÈ°µÈù¢„ÄÇ&lt;/p&gt; 
&lt;h1&gt;Buzz&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://chidiwilliams.github.io/buzz/"&gt;Documentation&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Transcribe and translate audio offline on your personal computer. Powered by OpenAI's &lt;a href="https://github.com/openai/whisper"&gt;Whisper&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://img.shields.io/badge/license-MIT-green" alt="MIT License" /&gt; &lt;a href="https://github.com/chidiwilliams/buzz/actions/workflows/ci.yml"&gt;&lt;img src="https://github.com/chidiwilliams/buzz/actions/workflows/ci.yml/badge.svg?sanitize=true" alt="CI" /&gt;&lt;/a&gt; &lt;a href="https://codecov.io/github/chidiwilliams/buzz"&gt;&lt;img src="https://codecov.io/github/chidiwilliams/buzz/branch/main/graph/badge.svg?token=YJSB8S2VEP" alt="codecov" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/github/v/release/chidiwilliams/buzz" alt="GitHub release (latest by date)" /&gt; &lt;a href="https://GitHub.com/chidiwilliams/buzz/releases/"&gt;&lt;img src="https://img.shields.io/github/downloads/chidiwilliams/buzz/total.svg?sanitize=true" alt="Github all releases" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/chidiwilliams/buzz/main/buzz/assets/buzz-banner.jpg" alt="Buzz" /&gt;&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Transcribe audio and video files or Youtube links&lt;/li&gt; 
 &lt;li&gt;Live realtime audio transcription from microphone 
  &lt;ul&gt; 
   &lt;li&gt;Presentation window for easy accessibility during events and presentations&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Speech separation before transcription for better accuracy on noisy audio&lt;/li&gt; 
 &lt;li&gt;Speaker identification in transcribed media&lt;/li&gt; 
 &lt;li&gt;Multiple whisper backend support 
  &lt;ul&gt; 
   &lt;li&gt;CUDA acceleration support for Nvidia GPUs&lt;/li&gt; 
   &lt;li&gt;Apple Silicon support for Macs&lt;/li&gt; 
   &lt;li&gt;Vulkan acceleration support for Whisper.cpp on most GPUs, including integrated GPUs&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Export transcripts to TXT, SRT, and VTT&lt;/li&gt; 
 &lt;li&gt;Advanced Transcription Viewer with search, playback controls, and speed adjustment&lt;/li&gt; 
 &lt;li&gt;Keyboard shortcuts for efficient navigation&lt;/li&gt; 
 &lt;li&gt;Watch folder for automatic transcription of new files&lt;/li&gt; 
 &lt;li&gt;Command-Line Interface for scripting and automation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;macOS&lt;/h3&gt; 
&lt;p&gt;Download the &lt;code&gt;.dmg&lt;/code&gt; from the &lt;a href="https://sourceforge.net/projects/buzz-captions/files/"&gt;SourceForge&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Windows&lt;/h3&gt; 
&lt;p&gt;Get the installation files from the &lt;a href="https://sourceforge.net/projects/buzz-captions/files/"&gt;SourceForge&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;App is not signed, you will get a warning when you install it. Select &lt;code&gt;More info&lt;/code&gt; -&amp;gt; &lt;code&gt;Run anyway&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Alternatively, install with &lt;a href="https://learn.microsoft.com/en-us/windows/package-manager/winget/"&gt;winget&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;winget install ChidiWilliams.Buzz
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Linux&lt;/h3&gt; 
&lt;p&gt;Buzz is available as a &lt;a href="https://flathub.org/apps/io.github.chidiwilliams.Buzz"&gt;Flatpak&lt;/a&gt; or a &lt;a href="https://snapcraft.io/buzz"&gt;Snap&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To install flatpak, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;flatpak install flathub io.github.chidiwilliams.Buzz
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://flathub.org/en/apps/io.github.chidiwilliams.Buzz"&gt;&lt;img src="https://flathub.org/api/badge?svg&amp;amp;locale=en" alt="Download on Flathub" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;To install snap, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;sudo apt-get install libportaudio2 libcanberra-gtk-module libcanberra-gtk3-module
sudo snap install buzz
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://snapcraft.io/buzz"&gt;&lt;img src="https://snapcraft.io/static/images/badges/en/snap-store-black.svg?sanitize=true" alt="Get it from the Snap Store" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;PyPI&lt;/h3&gt; 
&lt;p&gt;Install &lt;a href="https://www.ffmpeg.org/download.html"&gt;ffmpeg&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Ensure you use Python 3.12 environment.&lt;/p&gt; 
&lt;p&gt;Install Buzz&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install buzz-captions
python -m buzz
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;GPU support for PyPI&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;To have GPU support for Nvidia GPUS on Windows, for PyPI installed version ensure, CUDA support for &lt;a href="https://pytorch.org/get-started/locally/"&gt;torch&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip3 install -U torch==2.8.0+cu129 torchaudio==2.8.0+cu129 --index-url https://download.pytorch.org/whl/cu129
pip3 install nvidia-cublas-cu12==12.9.1.4 nvidia-cuda-cupti-cu12==12.9.79 nvidia-cuda-runtime-cu12==12.9.79 --extra-index-url https://pypi.ngc.nvidia.com
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Latest development version&lt;/h3&gt; 
&lt;p&gt;For info on how to get latest development version with latest features and bug fixes see &lt;a href="https://chidiwilliams.github.io/buzz/docs/faq#9-where-can-i-get-latest-development-version"&gt;FAQ&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Screenshots&lt;/h3&gt; 
&lt;div style="display: flex; flex-wrap: wrap;"&gt; 
 &lt;img alt="File import" src="https://raw.githubusercontent.com/chidiwilliams/buzz/main/share/screenshots/buzz-1-import.png" style="max-width: 18%; margin-right: 1%;" /&gt; 
 &lt;img alt="Main screen" src="https://raw.githubusercontent.com/chidiwilliams/buzz/main/share/screenshots/buzz-2-main_screen.png" style="max-width: 18%; margin-right: 1%; height:auto;" /&gt; 
 &lt;img alt="Preferences" src="https://raw.githubusercontent.com/chidiwilliams/buzz/main/share/screenshots/buzz-3-preferences.png" style="max-width: 18%; margin-right: 1%; height:auto;" /&gt; 
 &lt;img alt="Model preferences" src="https://raw.githubusercontent.com/chidiwilliams/buzz/main/share/screenshots/buzz-3.2-model-preferences.png" style="max-width: 18%; margin-right: 1%; height:auto;" /&gt; 
 &lt;img alt="Transcript" src="https://raw.githubusercontent.com/chidiwilliams/buzz/main/share/screenshots/buzz-4-transcript.png" style="max-width: 18%; margin-right: 1%; height:auto;" /&gt; 
 &lt;img alt="Live recording" src="https://raw.githubusercontent.com/chidiwilliams/buzz/main/share/screenshots/buzz-5-live_recording.png" style="max-width: 18%; margin-right: 1%; height:auto;" /&gt; 
 &lt;img alt="Resize" src="https://raw.githubusercontent.com/chidiwilliams/buzz/main/share/screenshots/buzz-6-resize.png" style="max-width: 18%;" /&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>PriorLabs/TabPFN</title>
      <link>https://github.com/PriorLabs/TabPFN</link>
      <description>&lt;p&gt;‚ö° TabPFN: Foundation Model for Tabular Data ‚ö°&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TabPFN&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://badge.fury.io/py/tabpfn"&gt;&lt;img src="https://badge.fury.io/py/tabpfn.svg?sanitize=true" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/tabpfn"&gt;&lt;img src="https://pepy.tech/badge/tabpfn" alt="Downloads" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/BHnX2Ptf4j"&gt;&lt;img src="https://img.shields.io/discord/1285598202732482621?color=7289da&amp;amp;label=Discord&amp;amp;logo=discord&amp;amp;logoColor=ffffff" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://priorlabs.ai/docs"&gt;&lt;img src="https://img.shields.io/badge/docs-priorlabs.ai-blue" alt="Documentation" /&gt;&lt;/a&gt; &lt;a href="https://colab.research.google.com/github/PriorLabs/TabPFN/blob/main/examples/notebooks/TabPFN_Demo_Local.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="colab" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/tabpfn/"&gt;&lt;img src="https://img.shields.io/badge/python-3.9%20%7C%203.10%20%7C%203.11%20%7C%203.12%20%7C%203.13-blue" alt="Python Versions" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;img src="https://github.com/PriorLabs/tabpfn-extensions/raw/main/tabpfn_summary.webp" width="80%" alt="TabPFN Summary" /&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;h3&gt;Interactive Notebook Tutorial&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP]&lt;/p&gt; 
 &lt;p&gt;Dive right in with our interactive Colab notebook! It's the best way to get a hands-on feel for TabPFN, walking you through installation, classification, and regression examples.&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://colab.research.google.com/github/PriorLabs/TabPFN/blob/main/examples/notebooks/TabPFN_Demo_Local.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;‚ö° &lt;strong&gt;GPU Recommended&lt;/strong&gt;: For optimal performance, use a GPU (even older ones with ~8GB VRAM work well; 16GB needed for some large datasets). On CPU, only small datasets (‚â≤1000 samples) are feasible. No GPU? Use our free hosted inference via &lt;a href="https://github.com/PriorLabs/tabpfn-client"&gt;TabPFN Client&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;p&gt;Official installation (pip)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install tabpfn
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;OR installation from source&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install "tabpfn @ git+https://github.com/PriorLabs/TabPFN.git"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;OR local development installation: First &lt;a href="https://docs.astral.sh/uv/getting-started/installation"&gt;install uv&lt;/a&gt;, which we use for development, then run&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/PriorLabs/TabPFN.git --depth 1
cd TabPFN
uv sync
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Basic Usage&lt;/h3&gt; 
&lt;h4&gt;Classification&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from sklearn.datasets import load_breast_cancer
from sklearn.metrics import accuracy_score, roc_auc_score
from sklearn.model_selection import train_test_split

from tabpfn import TabPFNClassifier
from tabpfn.constants import ModelVersion

# Load data
X, y = load_breast_cancer(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)

# Initialize a classifier
clf = TabPFNClassifier()  # Uses TabPFN 2.5 weights, finetuned on real data.
# To use TabPFN v2:
# clf = TabPFNClassifier.create_default_for_version(ModelVersion.V2)
clf.fit(X_train, y_train)


# Predict probabilities
prediction_probabilities = clf.predict_proba(X_test)
print("ROC AUC:", roc_auc_score(y_test, prediction_probabilities[:, 1]))

# Predict labels
predictions = clf.predict(X_test)
print("Accuracy", accuracy_score(y_test, predictions))
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Regression&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from sklearn.datasets import fetch_openml
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split

from tabpfn import TabPFNRegressor
from tabpfn.constants import ModelVersion

# Load Boston Housing data
df = fetch_openml(data_id=531, as_frame=True)  # Boston Housing dataset
X = df.data
y = df.target.astype(float)  # Ensure target is float for regression

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)

# Initialize the regressor
regressor = TabPFNRegressor()  # Uses TabPFN-2.5 weights, trained on synthetic data only.
# To use TabPFN v2:
# regressor = TabPFNRegressor.create_default_for_version(ModelVersion.V2)
regressor.fit(X_train, y_train)

# Predict on the test set
predictions = regressor.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, predictions)
r2 = r2_score(y_test, predictions)

print("Mean Squared Error (MSE):", mse)
print("R¬≤ Score:", r2)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;TabPFN Ecosystem&lt;/h2&gt; 
&lt;p&gt;Choose the right TabPFN implementation for your needs:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/priorlabs/tabpfn-client"&gt;TabPFN Client&lt;/a&gt;&lt;/strong&gt; Simple API client for using TabPFN via cloud-based inference.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/priorlabs/tabpfn-extensions"&gt;TabPFN Extensions&lt;/a&gt;&lt;/strong&gt; A powerful companion repository packed with advanced utilities, integrations, and features - great place to contribute:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;code&gt;interpretability&lt;/code&gt;&lt;/strong&gt;: Gain insights with SHAP-based explanations, feature importance, and selection tools.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;code&gt;unsupervised&lt;/code&gt;&lt;/strong&gt;: Tools for outlier detection and synthetic tabular data generation.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;code&gt;embeddings&lt;/code&gt;&lt;/strong&gt;: Extract and use TabPFN‚Äôs internal learned embeddings for downstream tasks or analysis.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;code&gt;many_class&lt;/code&gt;&lt;/strong&gt;: Handle multi-class classification problems that exceed TabPFN's built-in class limit.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;code&gt;rf_pfn&lt;/code&gt;&lt;/strong&gt;: Combine TabPFN with traditional models like Random Forests for hybrid approaches.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;code&gt;hpo&lt;/code&gt;&lt;/strong&gt;: Automated hyperparameter optimization tailored to TabPFN.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;code&gt;post_hoc_ensembles&lt;/code&gt;&lt;/strong&gt;: Boost performance by ensembling multiple TabPFN models post-training.&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;To install:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/priorlabs/tabpfn-extensions.git
pip install -e tabpfn-extensions
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/priorlabs/tabpfn"&gt;TabPFN (this repo)&lt;/a&gt;&lt;/strong&gt; Core implementation for fast and local inference with PyTorch and CUDA support.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://ux.priorlabs.ai"&gt;TabPFN UX&lt;/a&gt;&lt;/strong&gt; No-code graphical interface to explore TabPFN capabilities‚Äîideal for business users and prototyping.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;TabPFN Workflow at a Glance&lt;/h2&gt; 
&lt;p&gt;Follow this decision tree to build your model and choose the right extensions from our ecosystem. It walks you through critical questions about your data, hardware, and performance needs, guiding you to the best solution for your specific use case.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-mermaid"&gt;---
config:
  theme: 'default'
  themeVariables:
    edgeLabelBackground: 'white'
---
graph LR
    %% 1. DEFINE COLOR SCHEME &amp;amp; STYLES
    classDef default fill:#fff,stroke:#333,stroke-width:2px,color:#333;
    classDef start_node fill:#e8f5e9,stroke:#43a047,stroke-width:2px,color:#333;
    classDef process_node fill:#e0f2f1,stroke:#00796b,stroke-width:2px,color:#333;
    classDef decision_node fill:#fff8e1,stroke:#ffa000,stroke-width:2px,color:#333;

    style Infrastructure fill:#fff,stroke:#ccc,stroke-width:5px;
    style Unsupervised fill:#fff,stroke:#ccc,stroke-width:5px;
    style Data fill:#fff,stroke:#ccc,stroke-width:5px;
    style Performance fill:#fff,stroke:#ccc,stroke-width:5px;
    style Interpretability fill:#fff,stroke:#ccc,stroke-width:5px;

    %% 2. DEFINE GRAPH STRUCTURE
    subgraph Infrastructure
        start((Start)) --&amp;gt; gpu_check["GPU available?"];
        gpu_check -- Yes --&amp;gt; local_version["Use TabPFN&amp;lt;br/&amp;gt;(local PyTorch)"];
        gpu_check -- No --&amp;gt; api_client["Use TabPFN-Client&amp;lt;br/&amp;gt;(cloud API)"];
        task_type["What is&amp;lt;br/&amp;gt;your task?"]
    end

    local_version --&amp;gt; task_type
    api_client --&amp;gt; task_type

    end_node((Workflow&amp;lt;br/&amp;gt;Complete));

    subgraph Unsupervised
        unsupervised_type["Select&amp;lt;br/&amp;gt;Unsupervised Task"];
        unsupervised_type --&amp;gt; imputation["Imputation"]
        unsupervised_type --&amp;gt; data_gen["Data&amp;lt;br/&amp;gt;Generation"];
        unsupervised_type --&amp;gt; tabebm["Data&amp;lt;br/&amp;gt;Augmentation"];
        unsupervised_type --&amp;gt; density["Outlier&amp;lt;br/&amp;gt;Detection"];
        unsupervised_type --&amp;gt; embedding["Get&amp;lt;br/&amp;gt;Embeddings"];
    end


    subgraph Data
        data_check["Data Checks"];
        model_choice["Samples &amp;gt; 50k or&amp;lt;br/&amp;gt;Classes &amp;gt; 10?"];
        data_check -- "Table Contains Text Data?" --&amp;gt; api_backend_note["Note: API client has&amp;lt;br/&amp;gt;native text support"];
        api_backend_note --&amp;gt; model_choice;
        data_check -- "Time-Series Data?" --&amp;gt; ts_features["Use Time-Series&amp;lt;br/&amp;gt;Features"];
        ts_features --&amp;gt; model_choice;
        data_check -- "Purely Tabular" --&amp;gt; model_choice;
        model_choice -- "No" --&amp;gt; finetune_check;
        model_choice -- "Yes, 50k-100k samples" --&amp;gt; ignore_limits["Set&amp;lt;br/&amp;gt;ignore_pretraining_limits=True"];
        model_choice -- "Yes, &amp;gt;100k samples" --&amp;gt; subsample["Large Datasets Guide&amp;lt;br/&amp;gt;"];
        model_choice -- "Yes, &amp;gt;10 classes" --&amp;gt; many_class["Many-Class&amp;lt;br/&amp;gt;Method"];
    end

    subgraph Performance
        finetune_check["Need&amp;lt;br/&amp;gt;Finetuning?"];
        performance_check["Need Even Better Performance?"];
        speed_check["Need faster inference&amp;lt;br/&amp;gt;at prediction time?"];
        kv_cache["Enable KV Cache&amp;lt;br/&amp;gt;(fit_mode='fit_with_cache')&amp;lt;br/&amp;gt;&amp;lt;small&amp;gt;Faster predict; +Memory ~O(N√óF)&amp;lt;/small&amp;gt;"];
        tuning_complete["Tuning Complete"];

        finetune_check -- Yes --&amp;gt; finetuning["Finetuning"];
        finetune_check -- No --&amp;gt; performance_check;

        finetuning --&amp;gt; performance_check;

        performance_check -- No --&amp;gt; tuning_complete;
        performance_check -- Yes --&amp;gt; hpo["HPO"];
        performance_check -- Yes --&amp;gt; post_hoc["Post-Hoc&amp;lt;br/&amp;gt;Ensembling"];
        performance_check -- Yes --&amp;gt; more_estimators["More&amp;lt;br/&amp;gt;Estimators"];
        performance_check -- Yes --&amp;gt; speed_check;

        speed_check -- Yes --&amp;gt; kv_cache;
        speed_check -- No --&amp;gt; tuning_complete;

        hpo --&amp;gt; tuning_complete;
        post_hoc --&amp;gt; tuning_complete;
        more_estimators --&amp;gt; tuning_complete;
        kv_cache --&amp;gt; tuning_complete;
    end

    subgraph Interpretability

        tuning_complete --&amp;gt; interpretability_check;

        interpretability_check["Need&amp;lt;br/&amp;gt;Interpretability?"];

        interpretability_check --&amp;gt; feature_selection["Feature Selection"];
        interpretability_check --&amp;gt; partial_dependence["Partial Dependence Plots"];
        interpretability_check --&amp;gt; shapley["Explain with&amp;lt;br/&amp;gt;SHAP"];
        interpretability_check --&amp;gt; shap_iq["Explain with&amp;lt;br/&amp;gt;SHAP IQ"];
        interpretability_check -- No --&amp;gt; end_node;

        feature_selection --&amp;gt; end_node;
        partial_dependence --&amp;gt; end_node;
        shapley --&amp;gt; end_node;
        shap_iq --&amp;gt; end_node;

    end

    %% 3. LINK SUBGRAPHS AND PATHS
    task_type -- "Classification or Regression" --&amp;gt; data_check;
    task_type -- "Unsupervised" --&amp;gt; unsupervised_type;

    subsample --&amp;gt; finetune_check;
    ignore_limits --&amp;gt; finetune_check;
    many_class --&amp;gt; finetune_check;

    %% 4. APPLY STYLES
    class start,end_node start_node;
    class local_version,api_client,imputation,data_gen,tabebm,density,embedding,api_backend_note,ts_features,subsample,ignore_limits,many_class,finetuning,feature_selection,partial_dependence,shapley,shap_iq,hpo,post_hoc,more_estimators,kv_cache process_node;
    class gpu_check,task_type,unsupervised_type,data_check,model_choice,finetune_check,interpretability_check,performance_check,speed_check decision_node;
    class tuning_complete process_node;

    %% 5. ADD CLICKABLE LINKS (INCLUDING KV CACHE EXAMPLE)
    click local_version "https://github.com/PriorLabs/TabPFN" "TabPFN Backend Options"
    click api_client "https://github.com/PriorLabs/tabpfn-client" "TabPFN API Client"
    click api_backend_note "https://github.com/PriorLabs/tabpfn-client" "TabPFN API Backend"
    click unsupervised_type "https://github.com/PriorLabs/tabpfn-extensions" "TabPFN Extensions"
    click imputation "https://github.com/PriorLabs/tabpfn-extensions/blob/main/examples/unsupervised/imputation.py" "TabPFN Imputation Example"
    click data_gen "https://github.com/PriorLabs/tabpfn-extensions/blob/main/examples/unsupervised/generate_data.py" "TabPFN Data Generation Example"
    click tabebm "https://github.com/PriorLabs/tabpfn-extensions/blob/main/examples/tabebm/tabebm_augment_real_world_data.ipynb" "TabEBM Data Augmentation Example"
    click density "https://github.com/PriorLabs/tabpfn-extensions/blob/main/examples/unsupervised/density_estimation_outlier_detection.py" "TabPFN Density Estimation/Outlier Detection Example"
    click embedding "https://github.com/PriorLabs/tabpfn-extensions/tree/main/examples/embedding" "TabPFN Embedding Example"
    click ts_features "https://github.com/PriorLabs/tabpfn-time-series" "TabPFN Time-Series Example"
    click many_class "https://github.com/PriorLabs/tabpfn-extensions/blob/main/examples/many_class/many_class_classifier_example.py" "Many Class Example"
    click finetuning "https://github.com/PriorLabs/TabPFN/blob/main/examples/finetune_classifier.py" "Finetuning Example"
    click feature_selection "https://github.com/PriorLabs/tabpfn-extensions/blob/main/examples/interpretability/feature_selection.py" "Feature Selection Example"
    click partial_dependence "https://github.com/PriorLabs/tabpfn-extensions/blob/main/examples/interpretability/pdp_example.py" "Partial Dependence Plots Example"
    click shapley "https://github.com/PriorLabs/tabpfn-extensions/blob/main/examples/interpretability/shap_example.py" "Shapley Values Example"
    click shap_iq "https://github.com/PriorLabs/tabpfn-extensions/blob/main/examples/interpretability/shapiq_example.py" "SHAP IQ Example"
    click post_hoc "https://github.com/PriorLabs/tabpfn-extensions/blob/main/examples/phe/phe_example.py" "Post-Hoc Ensemble Example"
    click hpo "https://github.com/PriorLabs/tabpfn-extensions/blob/main/examples/hpo/tuned_tabpfn.py" "HPO Example"
    click subsample "https://github.com/PriorLabs/tabpfn-extensions/blob/main/examples/large_datasets/large_datasets_example.py" "Large Datasets Example"
    click kv_cache "https://github.com/PriorLabs/TabPFN/blob/main/examples/kv_cache_fast_prediction.py" "KV Cache Fast Prediction Example"

&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;The TabPFN-2.5 model weights are licensed under a &lt;a href="https://huggingface.co/Prior-Labs/tabpfn_2_5/blob/main/LICENSE"&gt;non-commercial license&lt;/a&gt;. These are used by default.&lt;/p&gt; 
&lt;p&gt;The code and TabPFN-2 model weights are licensed under Prior Labs License (Apache 2.0 with additional attribution requirement): &lt;a href="https://raw.githubusercontent.com/PriorLabs/TabPFN/main/LICENSE"&gt;here&lt;/a&gt;. To use the v2 model weights, instantiate your model as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;from tabpfn.constants import ModelVersion
tabpfn_v2 = TabPFNRegressor.create_default_for_version(ModelVersion.V2)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Enterprise &amp;amp; Production&lt;/h2&gt; 
&lt;p&gt;For high-throughput or massive-scale production environments, we offer an &lt;strong&gt;Enterprise Edition&lt;/strong&gt; with the following capabilities:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Fast Inference Mode&lt;/strong&gt;: A proprietary distillation engine that converts TabPFN-2.5 into a compact MLP or tree ensemble, delivering orders-of-magnitude lower latency for real-time applications.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Large Data Mode (Scaling Mode)&lt;/strong&gt;: An advanced operating mode that lifts row constraints to support datasets with up to &lt;strong&gt;10 million rows&lt;/strong&gt;‚Äîa 1,000x increase over the original TabPFNv2.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Commercial Support&lt;/strong&gt;: Includes a Commercial Enterprise License for production use-cases, dedicated integration support, and access to private high-speed inference engines.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;To learn more or request a commercial license, please contact us at &lt;a href="mailto:sales@priorlabs.ai"&gt;sales@priorlabs.ai&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Join Our Community&lt;/h2&gt; 
&lt;p&gt;We're building the future of tabular machine learning and would love your involvement:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Connect &amp;amp; Learn&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Join our &lt;a href="https://discord.gg/VJRuU3bSxt"&gt;Discord Community&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Read our &lt;a href="https://priorlabs.ai/docs"&gt;Documentation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Check out &lt;a href="https://github.com/priorlabs/tabpfn/issues"&gt;GitHub Issues&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Contribute&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Report bugs or request features&lt;/li&gt; 
   &lt;li&gt;Submit pull requests (please make sure to open an issue discussing the feature/bug first if none exists)&lt;/li&gt; 
   &lt;li&gt;Share your research and use cases&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Stay Updated&lt;/strong&gt;: Star the repo and join Discord for the latest updates&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;You can read our paper explaining TabPFNv2 &lt;a href="https://doi.org/10.1038/s41586-024-08328-6"&gt;here&lt;/a&gt;, and the model report of TabPFN-2.5 &lt;a href="https://arxiv.org/abs/2511.08667"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{grinsztajn2025tabpfn,
  title={TabPFN-2.5: Advancing the State of the Art in Tabular Foundation Models},
  author={L√©o Grinsztajn and Klemens Fl√∂ge and Oscar Key and Felix Birkel and Philipp Jund and Brendan Roof and
          Benjamin J√§ger and Dominik Safaric and Simone Alessi and Adrian Hayler and Mihir Manium and Rosen Yu and
          Felix Jablonski and Shi Bin Hoo and Anurag Garg and Jake Robertson and Magnus B√ºhler and Vladyslav Moroshan and
          Lennart Purucker and Clara Cornu and Lilly Charlotte Wehrhahn and Alessandro Bonetto and
          Bernhard Sch√∂lkopf and Sauraj Gambhir and Noah Hollmann and Frank Hutter},
  year={2025},
  eprint={2511.08667},
  archivePrefix={arXiv},
  url={https://arxiv.org/abs/2511.08667},
}

@article{hollmann2025tabpfn,
 title={Accurate predictions on small data with a tabular foundation model},
 author={Hollmann, Noah and M{\"u}ller, Samuel and Purucker, Lennart and
         Krishnakumar, Arjun and K{\"o}rfer, Max and Hoo, Shi Bin and
         Schirrmeister, Robin Tibor and Hutter, Frank},
 journal={Nature},
 year={2025},
 month={01},
 day={09},
 doi={10.1038/s41586-024-08328-6},
 publisher={Springer Nature},
 url={https://www.nature.com/articles/s41586-024-08328-6},
}

@inproceedings{hollmann2023tabpfn,
  title={TabPFN: A transformer that solves small tabular classification problems in a second},
  author={Hollmann, Noah and M{\"u}ller, Samuel and Eggensperger, Katharina and Hutter, Frank},
  booktitle={International Conference on Learning Representations 2023},
  year={2023}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;‚ùì FAQ&lt;/h2&gt; 
&lt;h3&gt;&lt;strong&gt;Usage &amp;amp; Compatibility&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Q: What dataset sizes work best with TabPFN?&lt;/strong&gt; A: TabPFN-2.5 is optimized for &lt;strong&gt;datasets up to 50,000 rows&lt;/strong&gt;. For larger datasets, consider using &lt;strong&gt;Random Forest preprocessing&lt;/strong&gt; or other extensions. See our &lt;a href="https://colab.research.google.com/drive/154SoIzNW1LHBWyrxNwmBqtFAr1uZRZ6a#scrollTo=OwaXfEIWlhC8"&gt;Colab notebook&lt;/a&gt; for strategies.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Q: Why can't I use TabPFN with Python 3.8?&lt;/strong&gt; A: TabPFN requires &lt;strong&gt;Python 3.9+&lt;/strong&gt; due to newer language features. Compatible versions: &lt;strong&gt;3.9, 3.10, 3.11, 3.12, 3.13&lt;/strong&gt;.&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;Installation &amp;amp; Setup&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Q: How do I get access to TabPFN-2.5?&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Visit &lt;a href="https://huggingface.co/Prior-Labs/tabpfn_2_5"&gt;https://huggingface.co/Prior-Labs/tabpfn_2_5&lt;/a&gt; and accept the license terms. If access via huggingface is not an option for you, please contact us at &lt;a href="mailto:sales@priorlabs.ai"&gt;&lt;code&gt;sales@priorlabs.ai&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Downloading the model requires your machine to be logged into Hugging Face. To do so, run &lt;code&gt;hf auth login&lt;/code&gt; in your terminal, see the &lt;a href="https://huggingface.co/docs/huggingface_hub/en/quick-start#authentication"&gt;huggingface documentation&lt;/a&gt; for details..&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Q: How do I use TabPFN without an internet connection?&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;TabPFN automatically downloads model weights when first used. For offline usage:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Using the Provided Download Script&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;If you have the TabPFN repository, you can use the included script to download all models (including ensemble variants):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# After installing TabPFN
python scripts/download_all_models.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This script will download the main classifier and regressor models, as well as all ensemble variant models to your system's default cache directory.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Manual Download&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Download the model files manually from HuggingFace:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Classifier: &lt;a href="https://huggingface.co/Prior-Labs/tabpfn_2_5/blob/main/tabpfn-v2.5-classifier-v2.5_default.ckpt"&gt;tabpfn-v2.5-classifier-v2.5_default.ckpt&lt;/a&gt; (Note: the classifier default uses the model fine-tuned on real data).&lt;/li&gt; 
   &lt;li&gt;Regressor: &lt;a href="https://huggingface.co/Prior-Labs/tabpfn_2_5/blob/main/tabpfn-v2.5-regressor-v2.5_default.ckpt"&gt;tabpfn-v2.5-regressor-v2.5_default.ckpt&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Place the file in one of these locations:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Specify directly: &lt;code&gt;TabPFNClassifier(model_path="/path/to/model.ckpt")&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Set environment variable: &lt;code&gt;export TABPFN_MODEL_CACHE_DIR="/path/to/dir"&lt;/code&gt; (see environment variables FAQ below)&lt;/li&gt; 
   &lt;li&gt;Default OS cache directory: 
    &lt;ul&gt; 
     &lt;li&gt;Windows: &lt;code&gt;%APPDATA%\tabpfn\&lt;/code&gt;&lt;/li&gt; 
     &lt;li&gt;macOS: &lt;code&gt;~/Library/Caches/tabpfn/&lt;/code&gt;&lt;/li&gt; 
     &lt;li&gt;Linux: &lt;code&gt;~/.cache/tabpfn/&lt;/code&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Q: I'm getting a &lt;code&gt;pickle&lt;/code&gt; error when loading the model. What should I do?&lt;/strong&gt; A: Try the following:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Download the newest version of tabpfn &lt;code&gt;pip install tabpfn --upgrade&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Ensure model files downloaded correctly (re-download if needed)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Q: What environment variables can I use to configure TabPFN?&lt;/strong&gt; A: TabPFN uses Pydantic settings for configuration, supporting environment variables and &lt;code&gt;.env&lt;/code&gt; files:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Model Configuration:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;TABPFN_MODEL_CACHE_DIR&lt;/code&gt;: Custom directory for caching downloaded TabPFN models (default: platform-specific user cache directory)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;TABPFN_ALLOW_CPU_LARGE_DATASET&lt;/code&gt;: Allow running TabPFN on CPU with large datasets (&amp;gt;1000 samples). Set to &lt;code&gt;true&lt;/code&gt; to override the CPU limitation. Note: This will be very slow!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;PyTorch Settings:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;PYTORCH_CUDA_ALLOC_CONF&lt;/code&gt;: PyTorch CUDA memory allocation configuration to optimize GPU memory usage (default: &lt;code&gt;max_split_size_mb:512&lt;/code&gt;). See &lt;a href="https://docs.pytorch.org/docs/stable/notes/cuda.html#optimizing-memory-usage-with-pytorch-cuda-alloc-conf"&gt;PyTorch CUDA documentation&lt;/a&gt; for more information.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export TABPFN_MODEL_CACHE_DIR="/path/to/models"
export TABPFN_ALLOW_CPU_LARGE_DATASET=true
export PYTORCH_CUDA_ALLOC_CONF="max_split_size_mb:512"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or simply set them in your &lt;code&gt;.env&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Q: How do I save and load a trained TabPFN model?&lt;/strong&gt; A: Use :func:&lt;code&gt;save_fitted_tabpfn_model&lt;/code&gt; to persist a fitted estimator and reload it later with :func:&lt;code&gt;load_fitted_tabpfn_model&lt;/code&gt; (or the corresponding &lt;code&gt;load_from_fit_state&lt;/code&gt; class methods).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from tabpfn import TabPFNRegressor
from tabpfn.model_loading import (
    load_fitted_tabpfn_model,
    save_fitted_tabpfn_model,
)

# Train the regressor on GPU
reg = TabPFNRegressor(device="cuda")
reg.fit(X_train, y_train)
save_fitted_tabpfn_model(reg, "my_reg.tabpfn_fit")

# Later or on a CPU-only machine
reg_cpu = load_fitted_tabpfn_model("my_reg.tabpfn_fit", device="cpu")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To store just the foundation model weights (without a fitted estimator) use &lt;code&gt;save_tabpfn_model(reg.model_, "my_tabpfn.ckpt")&lt;/code&gt;. This merely saves a checkpoint of the pre-trained weights so you can later create and fit a fresh estimator. Reload the checkpoint with &lt;code&gt;load_model_criterion_config&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;Performance &amp;amp; Limitations&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Q: Can TabPFN handle missing values?&lt;/strong&gt; A: &lt;strong&gt;Yes!&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Q: How can I improve TabPFN‚Äôs performance?&lt;/strong&gt; A: Best practices:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Use &lt;strong&gt;AutoTabPFNClassifier&lt;/strong&gt; from &lt;a href="https://github.com/priorlabs/tabpfn-extensions"&gt;TabPFN Extensions&lt;/a&gt; for post-hoc ensembling&lt;/li&gt; 
 &lt;li&gt;Feature engineering: Add domain-specific features to improve model performance&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Not effective:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Adapt feature scaling&lt;/li&gt; 
 &lt;li&gt;Convert categorical features to numerical values (e.g., one-hot encoding)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Q: What are the different checkpoints on &lt;a href="https://huggingface.co/Prior-Labs/tabpfn_2_5/tree/main"&gt;Hugging-Face&lt;/a&gt;?&lt;/strong&gt; A: Beyond the default checkpoints, the other available checkpoints are experimental and worse on average, and we recommend to always start with the defaults. They can be used as part of an ensembling or hyperparameter optimization system (and are used automatically in &lt;code&gt;AutoTabPFNClassifier&lt;/code&gt;) or tried out manually. Their name suffixes refer to what we expect them to be good at.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;More detail on each TabPFN-2.5 checkpoint&lt;/summary&gt; 
 &lt;p&gt;We add the üåç emoji for checkpoints finetuned on real datasets. See the &lt;a href="https://arxiv.org/abs/2511.08667"&gt;TabPFN-2.5 paper&lt;/a&gt; for the list of 43 datasets.&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;tabpfn-v2.5-classifier-v2.5_default.ckpt&lt;/code&gt; üåç: default classification checkpoint, finetuned on real-data.&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;tabpfn-v2.5-classifier-v2.5_default-2.ckpt&lt;/code&gt;: best classification synthetic checkpoint. Use this to get the default TabPFN-2.5 classification model without real-data finetuning.&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;tabpfn-v2.5-classifier-v2.5_large-features-L.ckpt&lt;/code&gt;: specialized for larger features (up to 500).&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;tabpfn-v2.5-classifier-v2.5_large-features-XL.ckpt&lt;/code&gt;: specialized for larger features (up to 1000, could support &lt;code&gt;max_features_per_estimator=1000&lt;/code&gt;).&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;tabpfn-v2.5-classifier-v2.5_large-samples.ckpt&lt;/code&gt;: specialized for larger sample sizes (larger than 30K)&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;tabpfn-v2.5-classifier-v2.5_real.ckpt&lt;/code&gt; üåç: other real-data finetuned classification checkpoint. Pretty good overall but bad on large features (&amp;gt;100-200).&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;tabpfn-v2.5-classifier-v2.5_real-large-features.ckpt&lt;/code&gt; üåç: other real-data finetuned classification checkpoint, worse on large samples (&amp;gt; 10K)&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;tabpfn-v2.5-classifier-v2.5_real-large-samples-and-features.ckpt&lt;/code&gt; üåç: identical to &lt;code&gt;tabpfn-v2.5-classifier-v2.5_default.ckpt&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;tabpfn-v2.5-classifier-v2.5_variant.ckpt&lt;/code&gt;: pretty good but bad on large features (&amp;gt; 100-200).&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;tabpfn-v2.5-regressor-v2.5_default.ckpt&lt;/code&gt;: default regression checkpoint, trained on synthetic data only.&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;tabpfn-v2.5-regressor-v2.5_low-skew.ckpt&lt;/code&gt;: variant specialized at low target skew data (but quite bad on average).&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;tabpfn-v2.5-regressor-v2.5_quantiles.ckpt&lt;/code&gt;: variant which might be interesting for quantile / distribution estimation, though the default should still be prioritized for this.&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;tabpfn-v2.5-regressor-v2.5_real.ckpt&lt;/code&gt; üåç: finetuned on real-data. Best checkpoint among the checkpoints finetuned on real data. For regression we recommend the synthetic-only checkpoint as a default, but this checkpoint is quite a bit better on some datasets.&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;tabpfn-v2.5-regressor-v2.5_real-variant.ckpt&lt;/code&gt; üåç: other regression variant finetuned on real data.&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;tabpfn-v2.5-regressor-v2.5_small-samples.ckpt&lt;/code&gt;: variant slightly better on small (&amp;lt; 3K) samples.&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;tabpfn-v2.5-regressor-v2.5_variant.ckpt&lt;/code&gt;: other variant, no clear specialty but can be better on a few datasets.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;Development&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Setup environment:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/PriorLabs/TabPFN.git
cd TabPFN
uv sync
source venv/bin/activate  # On Windows: venv\Scripts\activate
pre-commit install
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Before committing:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pre-commit run --all-files
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="4"&gt; 
 &lt;li&gt;Run tests:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pytest tests/
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Anonymized Telemetry&lt;/h2&gt; 
&lt;p&gt;This project collects fully anonymous usage telemetry with an option to opt-out of any telemetry or opt-in to extended telemetry.&lt;/p&gt; 
&lt;p&gt;The data is used exclusively to help us provide stability to the relevant products and compute environments and guide future improvements.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;No personal data is collected&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;No code, model inputs, or outputs are ever sent&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Data is strictly anonymous and cannot be linked to individuals&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For details on telemetry, please see our &lt;a href="https://github.com/PriorLabs/TabPFN/raw/main/TELEMETRY.md"&gt;Telemetry Reference&lt;/a&gt; and our &lt;a href="https://priorlabs.ai/privacy_policy/"&gt;Privacy Policy&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;To opt out&lt;/strong&gt;, set the following environment variable:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export TABPFN_DISABLE_TELEMETRY=1
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;p&gt;Built with ‚ù§Ô∏è by &lt;a href="https://priorlabs.ai"&gt;Prior Labs&lt;/a&gt; - Copyright (c) 2025 Prior Labs GmbH&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>skypilot-org/skypilot</title>
      <link>https://github.com/skypilot-org/skypilot</link>
      <description>&lt;p&gt;Run, manage, and scale AI workloads on any AI infrastructure. Use one system to access &amp; manage all AI compute (Kubernetes, 20+ clouds, or on-prem).&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/skypilot-org/skypilot/master/docs/source/images/skypilot-wide-dark-1k.png" /&gt; 
  &lt;img alt="SkyPilot" src="https://raw.githubusercontent.com/skypilot-org/skypilot/master/docs/source/images/skypilot-wide-light-1k.png" width="55%" /&gt; 
 &lt;/picture&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://docs.skypilot.co/"&gt; &lt;img alt="Documentation" src="https://img.shields.io/badge/docs-gray?logo=readthedocs&amp;amp;logoColor=f5f5f5" /&gt; &lt;/a&gt; &lt;a href="https://github.com/skypilot-org/skypilot/releases"&gt; &lt;img alt="GitHub Release" src="https://img.shields.io/github/release/skypilot-org/skypilot.svg?sanitize=true" /&gt; &lt;/a&gt; &lt;a href="http://slack.skypilot.co"&gt; &lt;img alt="Join Slack" src="https://img.shields.io/badge/SkyPilot-Join%20Slack-blue?logo=slack" /&gt; &lt;/a&gt; &lt;a href="https://github.com/skypilot-org/skypilot/releases"&gt; &lt;img alt="Downloads" src="https://img.shields.io/pypi/dm/skypilot" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h3 align="center"&gt; Run AI on Any Infrastructure &lt;/h3&gt; 
&lt;div align="center"&gt; 
 &lt;h4&gt;&lt;a href="https://demo.skypilot.co/dashboard/"&gt;üåü &lt;strong&gt;SkyPilot Demo&lt;/strong&gt; üåü: Click to see a 1-minute tour&lt;/a&gt;&lt;/h4&gt; 
&lt;/div&gt; 
&lt;p&gt;SkyPilot is a system to run, manage, and scale AI workloads on any AI infrastructure.&lt;/p&gt; 
&lt;p&gt;SkyPilot gives &lt;strong&gt;AI teams&lt;/strong&gt; a simple interface to run jobs on any infra. &lt;strong&gt;Infra teams&lt;/strong&gt; get a unified control plane to manage any AI compute ‚Äî with advanced scheduling, scaling, and orchestration.&lt;/p&gt; 
&lt;picture&gt; 
 &lt;source media="(prefers-color-scheme: dark)" srcset="./docs/source/images/skypilot-abstractions-long-2-dark.png" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/skypilot-org/skypilot/master/docs/source/images/skypilot-abstractions-long-2.png" alt="SkyPilot Abstractions" /&gt; 
&lt;/picture&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;span&gt;üî•&lt;/span&gt; &lt;em&gt;News&lt;/em&gt; &lt;span&gt;üî•&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;[Dec 2025] &lt;strong&gt;SkyPilot v0.11&lt;/strong&gt; released: Multi-Cloud Pools, Fast Managed Jobs, Enterprise-Readiness at Large Scale, Programmability. &lt;a href="https://github.com/skypilot-org/skypilot/releases/tag/v0.11.0"&gt;&lt;strong&gt;Release notes&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[Dec 2025] &lt;strong&gt;SkyPilot Pools&lt;/strong&gt; released: Run batch inference and other jobs on a managed pool of warm workers (across clouds or clusters). &lt;a href="https://blog.skypilot.co/skypilot-pools-deepseek-ocr/"&gt;&lt;strong&gt;blog&lt;/strong&gt;&lt;/a&gt;, &lt;a href="https://docs.skypilot.co/en/latest/examples/pools.html"&gt;&lt;strong&gt;docs&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[Nov 2025] Serve &lt;strong&gt;Kimi K2 Thinking&lt;/strong&gt; with reasoning capabilities on your Kubernetes or clouds: &lt;a href="https://raw.githubusercontent.com/skypilot-org/skypilot/master/llm/kimi-k2-thinking/"&gt;&lt;strong&gt;example&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[Oct 2025] Run &lt;strong&gt;RL training for LLMs&lt;/strong&gt; with SkyRL on your Kubernetes or clouds: &lt;a href="https://raw.githubusercontent.com/skypilot-org/skypilot/master/llm/skyrl/"&gt;&lt;strong&gt;example&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[Oct 2025] Train and serve &lt;a href="https://x.com/karpathy/status/1977755427569111362"&gt;Andrej Karpathy's&lt;/a&gt; &lt;strong&gt;nanochat&lt;/strong&gt; - the best ChatGPT that $100 can buy: &lt;a href="https://raw.githubusercontent.com/skypilot-org/skypilot/master/llm/nanochat"&gt;&lt;strong&gt;example&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[Oct 2025] Run large-scale &lt;strong&gt;LLM training with TorchTitan&lt;/strong&gt; on any AI infra: &lt;a href="https://raw.githubusercontent.com/skypilot-org/skypilot/master/examples/training/torchtitan"&gt;&lt;strong&gt;example&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[Sep 2025] Scaling AI infrastructure at Abridge - &lt;strong&gt;10x faster development&lt;/strong&gt; with SkyPilot: &lt;a href="https://blog.skypilot.co/abridge/"&gt;&lt;strong&gt;blog&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[Sep 2025] Network and Storage Benchmarks for LLM training on the cloud: &lt;a href="https://maknee.github.io/blog/2025/Network-And-Storage-Training-Skypilot/"&gt;&lt;strong&gt;blog&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[Aug 2025] Serve and finetune &lt;strong&gt;OpenAI GPT-OSS models&lt;/strong&gt; (gpt-oss-120b, gpt-oss-20b) with one command on any infra: &lt;a href="https://raw.githubusercontent.com/skypilot-org/skypilot/master/llm/gpt-oss/"&gt;&lt;strong&gt;serve&lt;/strong&gt;&lt;/a&gt; + &lt;a href="https://raw.githubusercontent.com/skypilot-org/skypilot/master/llm/gpt-oss-finetuning/"&gt;&lt;strong&gt;LoRA and full finetuning&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[Jul 2025] Run distributed &lt;strong&gt;RL training for LLMs&lt;/strong&gt; with Verl (PPO, GRPO) on any cloud: &lt;a href="https://raw.githubusercontent.com/skypilot-org/skypilot/master/llm/verl/"&gt;&lt;strong&gt;example&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Overview&lt;/h2&gt; 
&lt;p&gt;SkyPilot &lt;strong&gt;is easy to use for AI teams&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Quickly spin up compute on your own infra&lt;/li&gt; 
 &lt;li&gt;Environment and job as code ‚Äî simple and portable&lt;/li&gt; 
 &lt;li&gt;Easy job management: queue, run, and auto-recover many jobs&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;SkyPilot &lt;strong&gt;makes Kubernetes easy for AI &amp;amp; Infra teams&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Slurm-like ease of use, cloud-native robustness&lt;/li&gt; 
 &lt;li&gt;Local dev experience on K8s: SSH into pods, sync code, or connect IDE&lt;/li&gt; 
 &lt;li&gt;Turbocharge your clusters: gang scheduling, multi-cluster, and scaling&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;SkyPilot &lt;strong&gt;unifies multiple clusters, clouds, and hardware&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;One interface to use reserved GPUs, Kubernetes clusters, Slurm clusters, or 20+ clouds&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.skypilot.co/en/latest/examples/auto-failover.html"&gt;Flexible provisioning&lt;/a&gt; of GPUs, TPUs, CPUs, with auto-retry&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.skypilot.co/en/latest/reference/api-server/api-server.html"&gt;Team deployment&lt;/a&gt; and resource sharing&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;SkyPilot &lt;strong&gt;cuts your cloud costs &amp;amp; maximizes GPU availability&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Autostop: automatic cleanup of idle resources&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.skypilot.co/en/latest/examples/managed-jobs.html#running-on-spot-instances"&gt;Spot instance support&lt;/a&gt;: 3-6x cost savings, with preemption auto-recovery&lt;/li&gt; 
 &lt;li&gt;Intelligent scheduling: automatically run on the cheapest &amp;amp; most available infra&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;SkyPilot supports your existing GPU, TPU, and CPU workloads, with no code changes.&lt;/p&gt; 
&lt;p&gt;Install with pip:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Choose your clouds:
pip install -U "skypilot[kubernetes,aws,gcp,azure,oci,nebius,lambda,runpod,fluidstack,paperspace,cudo,ibm,scp,seeweb,shadeform]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To get the latest features and fixes, use the nightly build or &lt;a href="https://docs.skypilot.co/en/latest/getting-started/installation.html"&gt;install from source&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Choose your clouds:
pip install "skypilot-nightly[kubernetes,aws,gcp,azure,oci,nebius,lambda,runpod,fluidstack,paperspace,cudo,ibm,scp,seeweb,shadeform]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/skypilot-org/skypilot/master/docs/source/_static/intro.gif" alt="SkyPilot" /&gt; &lt;/p&gt; 
&lt;p&gt;Current supported infra: Kubernetes, Slurm, AWS, GCP, Azure, OCI, CoreWeave, Nebius, Lambda Cloud, RunPod, Fluidstack, Cudo, Digital Ocean, Paperspace, Cloudflare, Samsung, IBM, Vast.ai, VMware vSphere, Seeweb, Prime Intellect, Shadeform.&lt;/p&gt; 
&lt;p align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/skypilot-org/skypilot/master/docs/source/images/cloud-logos-dark.png" /&gt; 
  &lt;img alt="SkyPilot" src="https://raw.githubusercontent.com/skypilot-org/skypilot/master/docs/source/images/cloud-logos-light.png" width="85%" /&gt; 
 &lt;/picture&gt; &lt;/p&gt; 
&lt;!-- source xcf file: https://drive.google.com/drive/folders/1S_acjRsAD3T14qMeEnf6FFrIwHu_Gs_f?usp=drive_link --&gt; 
&lt;h2&gt;Getting started&lt;/h2&gt; 
&lt;p&gt;You can find our documentation &lt;a href="https://docs.skypilot.co/"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.skypilot.co/en/latest/getting-started/installation.html"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.skypilot.co/en/latest/getting-started/quickstart.html"&gt;Quickstart&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.skypilot.co/en/latest/reference/cli.html"&gt;CLI reference&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;SkyPilot in 1 minute&lt;/h2&gt; 
&lt;p&gt;A SkyPilot task specifies: resource requirements, data to be synced, setup commands, and the task commands.&lt;/p&gt; 
&lt;p&gt;Once written in this &lt;a href="https://docs.skypilot.co/en/latest/reference/yaml-spec.html"&gt;&lt;strong&gt;unified interface&lt;/strong&gt;&lt;/a&gt; (YAML or Python API), the task can be launched on any available infra (Kubernetes, Slurm, cloud, etc.). This avoids vendor lock-in, and allows easily moving jobs to a different provider.&lt;/p&gt; 
&lt;p&gt;Paste the following into a file &lt;code&gt;my_task.yaml&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;resources:
  accelerators: A100:8  # 8x NVIDIA A100 GPU

num_nodes: 1  # Number of VMs to launch

# Working directory (optional) containing the project codebase.
# Its contents are synced to ~/sky_workdir/ on the cluster.
workdir: ~/torch_examples

# Commands to be run before executing the job.
# Typical use: pip install -r requirements.txt, git clone, etc.
setup: |
  cd mnist
  pip install -r requirements.txt

# Commands to run as a job.
# Typical use: launch the main program.
run: |
  cd mnist
  python main.py --epochs 1
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Prepare the workdir by cloning:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/pytorch/examples.git ~/torch_examples
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Launch with &lt;code&gt;sky launch&lt;/code&gt; (note: &lt;a href="https://docs.skypilot.co/en/latest/cloud-setup/quota.html"&gt;access to GPU instances&lt;/a&gt; is needed for this example):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sky launch my_task.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;SkyPilot then performs the heavy-lifting for you, including:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Find the cheapest &amp;amp; available infra across your clusters or clouds&lt;/li&gt; 
 &lt;li&gt;Provision the GPUs (pods or VMs), with auto-failover if the infra returned capacity errors&lt;/li&gt; 
 &lt;li&gt;Sync your local &lt;code&gt;workdir&lt;/code&gt; to the provisioned cluster&lt;/li&gt; 
 &lt;li&gt;Auto-install dependencies by running the task's &lt;code&gt;setup&lt;/code&gt; commands&lt;/li&gt; 
 &lt;li&gt;Run the task's &lt;code&gt;run&lt;/code&gt; commands, and stream logs&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;See &lt;a href="https://docs.skypilot.co/en/latest/getting-started/quickstart.html"&gt;Quickstart&lt;/a&gt; to get started with SkyPilot.&lt;/p&gt; 
&lt;h2&gt;Runnable examples&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://docs.skypilot.co/en/docs-examples/examples/index.html"&gt;&lt;strong&gt;SkyPilot examples&lt;/strong&gt;&lt;/a&gt; that cover: development, training, serving, LLM models, AI apps, and common frameworks.&lt;/p&gt; 
&lt;p&gt;Latest featured examples:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Task&lt;/th&gt; 
   &lt;th&gt;Examples&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Training&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.skypilot.co/en/latest/examples/training/verl.html"&gt;Verl&lt;/a&gt;, &lt;a href="https://docs.skypilot.co/en/latest/examples/training/llama-4-finetuning.html"&gt;Finetune Llama 4&lt;/a&gt;, &lt;a href="https://docs.skypilot.co/en/latest/examples/training/torchtitan.html"&gt;TorchTitan&lt;/a&gt;, &lt;a href="https://docs.skypilot.co/en/latest/getting-started/tutorial.html"&gt;PyTorch&lt;/a&gt;, &lt;a href="https://docs.skypilot.co/en/latest/examples/training/deepspeed.html"&gt;DeepSpeed&lt;/a&gt;, &lt;a href="https://docs.skypilot.co/en/latest/examples/training/nemo.html"&gt;NeMo&lt;/a&gt;, &lt;a href="https://docs.skypilot.co/en/latest/examples/training/ray.html"&gt;Ray&lt;/a&gt;, &lt;a href="https://docs.skypilot.co/en/latest/examples/training/unsloth.html"&gt;Unsloth&lt;/a&gt;, &lt;a href="https://docs.skypilot.co/en/latest/examples/training/tpu.html"&gt;Jax/TPU&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Serving&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.skypilot.co/en/latest/examples/serving/vllm.html"&gt;vLLM&lt;/a&gt;, &lt;a href="https://docs.skypilot.co/en/latest/examples/serving/sglang.html"&gt;SGLang&lt;/a&gt;, &lt;a href="https://docs.skypilot.co/en/latest/examples/serving/ollama.html"&gt;Ollama&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Models&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.skypilot.co/en/latest/examples/models/deepseek-r1.html"&gt;DeepSeek-R1&lt;/a&gt;, &lt;a href="https://docs.skypilot.co/en/latest/examples/models/llama-4.html"&gt;Llama 4&lt;/a&gt;, &lt;a href="https://docs.skypilot.co/en/latest/examples/models/llama-3.html"&gt;Llama 3&lt;/a&gt;, &lt;a href="https://docs.skypilot.co/en/latest/examples/models/codellama.html"&gt;CodeLlama&lt;/a&gt;, &lt;a href="https://docs.skypilot.co/en/latest/examples/models/qwen.html"&gt;Qwen&lt;/a&gt;, &lt;a href="https://docs.skypilot.co/en/latest/examples/models/kimi-k2.html"&gt;Kimi-K2&lt;/a&gt;, &lt;a href="https://docs.skypilot.co/en/latest/examples/models/kimi-k2-thinking.html"&gt;Kimi-K2-Thinking&lt;/a&gt;, &lt;a href="https://docs.skypilot.co/en/latest/examples/models/mixtral.html"&gt;Mixtral&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;AI apps&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.skypilot.co/en/latest/examples/applications/rag.html"&gt;RAG&lt;/a&gt;, &lt;a href="https://docs.skypilot.co/en/latest/examples/applications/vector_database.html"&gt;vector databases&lt;/a&gt; (ChromaDB, CLIP)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Common frameworks&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.skypilot.co/en/latest/examples/frameworks/airflow.html"&gt;Airflow&lt;/a&gt;, &lt;a href="https://docs.skypilot.co/en/latest/examples/frameworks/jupyter.html"&gt;Jupyter&lt;/a&gt;, &lt;a href="https://docs.skypilot.co/en/latest/examples/frameworks/marimo.html"&gt;marimo&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Source files can be found in &lt;a href="https://github.com/skypilot-org/skypilot/tree/master/llm"&gt;&lt;code&gt;llm/&lt;/code&gt;&lt;/a&gt; and &lt;a href="https://github.com/skypilot-org/skypilot/tree/master/examples"&gt;&lt;code&gt;examples/&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;More information&lt;/h2&gt; 
&lt;p&gt;To learn more, see &lt;a href="https://docs.skypilot.co/en/latest/overview.html"&gt;SkyPilot Overview&lt;/a&gt;, &lt;a href="https://docs.skypilot.co/en/latest/"&gt;SkyPilot docs&lt;/a&gt;, and &lt;a href="https://blog.skypilot.co/"&gt;SkyPilot blog&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;SkyPilot adopters: &lt;a href="https://blog.skypilot.co/case-studies/"&gt;Testimonials and Case Studies&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Partners and integrations: &lt;a href="https://blog.skypilot.co/community/"&gt;Community Spotlights&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Follow updates:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="http://slack.skypilot.co"&gt;Slack&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://twitter.com/skypilot_org"&gt;X / Twitter&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.linkedin.com/company/skypilot-oss/"&gt;LinkedIn&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://blog.skypilot.co/"&gt;SkyPilot Blog&lt;/a&gt; (&lt;a href="https://blog.skypilot.co/introducing-skypilot/"&gt;Introductory blog post&lt;/a&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Read the research:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.usenix.org/system/files/nsdi23-yang-zongheng.pdf"&gt;SkyPilot paper&lt;/a&gt; and &lt;a href="https://www.usenix.org/conference/nsdi23/presentation/yang-zongheng"&gt;talk&lt;/a&gt; (NSDI 2023)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2205.07147"&gt;Sky Computing whitepaper&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://sigops.org/s/conferences/hotos/2021/papers/hotos21-s02-stoica.pdf"&gt;Sky Computing vision paper&lt;/a&gt; (HotOS 2021)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/pdf/2411.01438"&gt;SkyServe: AI serving across regions and clouds&lt;/a&gt; (EuroSys 2025)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.usenix.org/conference/nsdi24/presentation/wu-zhanghao"&gt;Managed jobs spot instance policy&lt;/a&gt; (NSDI 2024)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;SkyPilot was initially started at the &lt;a href="https://sky.cs.berkeley.edu"&gt;Sky Computing Lab&lt;/a&gt; at UC Berkeley and has since gained many industry contributors. To read about the project's origin and vision, see &lt;a href="https://docs.skypilot.co/en/latest/sky-computing.html"&gt;Concept: Sky Computing&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Questions and feedback&lt;/h2&gt; 
&lt;p&gt;We are excited to hear your feedback:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;For issues and feature requests, please &lt;a href="https://github.com/skypilot-org/skypilot/issues/new"&gt;open a GitHub issue&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;For questions, please use &lt;a href="https://github.com/skypilot-org/skypilot/discussions"&gt;GitHub Discussions&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For general discussions, join us on the &lt;a href="http://slack.skypilot.co"&gt;SkyPilot Slack&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome all contributions to the project! See &lt;a href="https://raw.githubusercontent.com/skypilot-org/skypilot/master/CONTRIBUTING.md"&gt;CONTRIBUTING&lt;/a&gt; for how to get involved.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>inventree/InvenTree</title>
      <link>https://github.com/inventree/InvenTree</link>
      <description>&lt;p&gt;Open Source Inventory Management System&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/inventree/InvenTree/master/assets/images/logo/inventree.png" alt="InvenTree logo" width="200" height="auto" /&gt; 
 &lt;h1&gt;InvenTree&lt;/h1&gt; 
 &lt;p&gt;Open Source Inventory Management System &lt;/p&gt; 
 &lt;!-- Badges --&gt; 
 &lt;p&gt;&lt;a href="https://opensource.org/license/MIT"&gt;&lt;img src="https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true" alt="License: MIT" /&gt;&lt;/a&gt;&lt;img src="https://img.shields.io/github/v/tag/inventree/inventree" alt="GitHub tag (latest SemVer)" /&gt; &lt;img src="https://github.com/inventree/inventree/actions/workflows/qc_checks.yaml/badge.svg?sanitize=true" alt="CI" /&gt; &lt;a href="https://inventree.readthedocs.io/en/latest/?badge=latest"&gt;&lt;img src="https://readthedocs.org/projects/inventree/badge/?version=latest" alt="Documentation Status" /&gt;&lt;/a&gt; &lt;img src="https://github.com/inventree/inventree/actions/workflows/docker.yaml/badge.svg?sanitize=true" alt="Docker Build" /&gt; &lt;a href="https://app.netlify.com/sites/inventree/deploys"&gt;&lt;img src="https://api.netlify.com/api/v1/badges/9bbb2101-0a4d-41e7-ad56-b63fb6053094/deploy-status" alt="Netlify Status" /&gt;&lt;/a&gt; &lt;a href="https://dev.azure.com/InvenTree/InvenTree%20test%20statistics/_build/latest?definitionId=3&amp;amp;branchName=testing"&gt;&lt;img src="https://dev.azure.com/InvenTree/InvenTree%20test%20statistics/_apis/build/status%2Fmatmair.InvenTree?branchName=testing" alt="Performance Testing" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://bestpractices.coreinfrastructure.org/projects/7179"&gt;&lt;img src="https://bestpractices.coreinfrastructure.org/projects/7179/badge" alt="OpenSSF Best Practices" /&gt;&lt;/a&gt; &lt;a href="https://securityscorecards.dev/viewer/?uri=github.com/inventree/InvenTree"&gt;&lt;img src="https://api.securityscorecards.dev/projects/github.com/inventree/InvenTree/badge" alt="OpenSSF Scorecard" /&gt;&lt;/a&gt; &lt;a href="https://sonarcloud.io/summary/new_code?id=inventree_InvenTree"&gt;&lt;img src="https://sonarcloud.io/api/project_badges/measure?project=inventree_InvenTree&amp;amp;metric=sqale_rating" alt="Maintainability Rating" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://codecov.io/gh/inventree/InvenTree"&gt;&lt;img src="https://codecov.io/gh/inventree/InvenTree/graph/badge.svg?token=9DZRGUUV7B" alt="codecov" /&gt;&lt;/a&gt; &lt;a href="https://crowdin.com/project/inventree"&gt;&lt;img src="https://badges.crowdin.net/inventree/localized.svg?sanitize=true" alt="Crowdin" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/github/commit-activity/m/inventree/inventree" alt="GitHub commit activity" /&gt; &lt;a href="https://hub.docker.com/r/inventree/inventree"&gt;&lt;img src="https://img.shields.io/docker/pulls/inventree/inventree" alt="Docker Pulls" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/inventree/InvenTree/"&gt;&lt;img src="https://img.shields.io/github/stars/inventree?style=social" alt="GitHub Org's stars" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/inventreedb"&gt;&lt;img src="https://img.shields.io/twitter/follow/inventreedb?style=social" alt="Twitter Follow" /&gt;&lt;/a&gt; &lt;a href="https://www.reddit.com/r/InvenTree/"&gt;&lt;img src="https://img.shields.io/reddit/subreddit-subscribers/inventree?style=social" alt="Subreddit subscribers" /&gt;&lt;/a&gt; &lt;a href="https://chaos.social/@InvenTree"&gt;&lt;img src="https://img.shields.io/badge/dynamic/json?label=Mastodon&amp;amp;query=followers_count&amp;amp;url=https%3A%2F%2Fchaos.social%2Fapi%2Fv1%2Faccounts%2Flookup%3Facct=InvenTree&amp;amp;logo=mastodon&amp;amp;style=social" alt="Mastdon" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;h4&gt; &lt;a href="https://demo.inventree.org/"&gt;View Demo&lt;/a&gt; &lt;span&gt; ¬∑ &lt;/span&gt; &lt;a href="https://docs.inventree.org/en/latest/"&gt;Documentation&lt;/a&gt; &lt;span&gt; ¬∑ &lt;/span&gt; &lt;a href="https://github.com/inventree/InvenTree/issues/new?template=bug_report.md&amp;amp;title=%5BBUG%5D"&gt;Report Bug&lt;/a&gt; &lt;span&gt; ¬∑ &lt;/span&gt; &lt;a href="https://github.com/inventree/InvenTree/issues/new?template=feature_request.md&amp;amp;title=%5BFR%5D"&gt;Request Feature&lt;/a&gt; &lt;/h4&gt; 
&lt;/div&gt; 
&lt;!-- About the Project --&gt; 
&lt;h2&gt;&lt;span&gt;üåü&lt;/span&gt; About the Project&lt;/h2&gt; 
&lt;p&gt;InvenTree is an open-source Inventory Management System which provides powerful low-level stock control and part tracking. The core of the InvenTree system is a Python/Django database backend which provides an admin interface (web-based) and a REST API for interaction with external interfaces and applications. A powerful plugin system provides support for custom applications and extensions.&lt;/p&gt; 
&lt;p&gt;Check out &lt;a href="https://inventree.org"&gt;our website&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;!-- Roadmap --&gt; 
&lt;h3&gt;&lt;span&gt;üß≠&lt;/span&gt; Roadmap&lt;/h3&gt; 
&lt;p&gt;Want to see what we are working on? Check out the &lt;a href="https://github.com/inventree/InvenTree/issues?q=is%3Aopen+is%3Aissue+label%3Aroadmap"&gt;roadmap tag&lt;/a&gt; and &lt;a href="https://github.com/inventree/InvenTree/milestone/42"&gt;horizon milestone&lt;/a&gt;.&lt;/p&gt; 
&lt;!-- Integration --&gt; 
&lt;h3&gt;&lt;span&gt;üõ†&lt;/span&gt; Integration&lt;/h3&gt; 
&lt;p&gt;InvenTree is designed to be &lt;strong&gt;extensible&lt;/strong&gt;, and provides multiple options for &lt;strong&gt;integration&lt;/strong&gt; with external applications or addition of custom plugins:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.inventree.org/en/latest/api/"&gt;InvenTree API&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.inventree.org/en/latest/api/python/"&gt;Python module&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.inventree.org/en/latest/plugins/"&gt;Plugin interface&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://inventree.org/extend/integrate/"&gt;Third party tools&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- TechStack --&gt; 
&lt;h3&gt;&lt;span&gt;üëæ&lt;/span&gt; Tech Stack&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;Server&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://www.python.org/"&gt;Python&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://www.djangoproject.com/"&gt;Django&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://www.django-rest-framework.org/"&gt;DRF&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://django-q.readthedocs.io/"&gt;Django Q&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://docs.allauth.org/"&gt;Django-Allauth&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Database&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://www.postgresql.org/"&gt;PostgreSQL&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://www.mysql.com/"&gt;MySQL&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://www.sqlite.org/"&gt;SQLite&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://redis.io/"&gt;Redis&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Client&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://react.dev/"&gt;React&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://lingui.dev/"&gt;Lingui&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://reactrouter.com/"&gt;React Router&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://tanstack.com/query/"&gt;TanStack Query&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/pmndrs/zustand"&gt;Zustand&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://mantine.dev/"&gt;Mantine&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://icflorescu.github.io/mantine-datatable/"&gt;Mantine Data Table&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://codemirror.net/"&gt;CodeMirror&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;DevOps&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://hub.docker.com/r/inventree/inventree"&gt;Docker&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://crowdin.com/project/inventree"&gt;Crowdin&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://app.codecov.io/gh/inventree/InvenTree"&gt;Codecov&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://sonarcloud.io/project/overview?id=inventree_InvenTree"&gt;SonarCloud&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://packager.io/gh/inventree/InvenTree"&gt;Packager.io&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;!-- Getting Started --&gt; 
&lt;h2&gt;&lt;span&gt;üß∞&lt;/span&gt; Deployment / Getting Started&lt;/h2&gt; 
&lt;p&gt;There are several options to deploy InvenTree.&lt;/p&gt; 
&lt;div align="center"&gt;
 &lt;h4&gt; &lt;a href="https://docs.inventree.org/en/latest/start/docker/"&gt;Docker&lt;/a&gt; &lt;span&gt; ¬∑ &lt;/span&gt; &lt;a href="https://inventree.org/digitalocean"&gt;&lt;img src="https://www.deploytodo.com/do-btn-blue-ghost.svg?sanitize=true" alt="Deploy to DO" width="auto" height="40" /&gt;&lt;/a&gt; &lt;span&gt; ¬∑ &lt;/span&gt; &lt;a href="https://docs.inventree.org/en/latest/start/install/"&gt;Bare Metal&lt;/a&gt; &lt;/h4&gt;
&lt;/div&gt; 
&lt;p&gt;Single line install - read &lt;a href="https://docs.inventree.org/en/latest/start/installer/"&gt;the docs&lt;/a&gt; for supported distros and details about the function:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;wget -qO install.sh https://get.inventree.org &amp;amp;&amp;amp; bash install.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Refer to the &lt;a href="https://docs.inventree.org/en/latest/start/install/"&gt;getting started guide&lt;/a&gt; for a full set of installation and setup instructions.&lt;/p&gt; 
&lt;!-- Mobile App --&gt; 
&lt;h2&gt;&lt;span&gt;üì±&lt;/span&gt; Mobile App&lt;/h2&gt; 
&lt;p&gt;InvenTree is supported by a &lt;a href="https://docs.inventree.org/en/latest/app/"&gt;companion mobile app&lt;/a&gt; which allows users access to stock control information and functionality.&lt;/p&gt; 
&lt;div align="center"&gt;
 &lt;h4&gt; &lt;a href="https://play.google.com/store/apps/details?id=inventree.inventree_app"&gt;Android Play Store&lt;/a&gt; &lt;span&gt; ¬∑ &lt;/span&gt; &lt;a href="https://apps.apple.com/au/app/inventree/id1581731101#?platform=iphone"&gt;Apple App Store&lt;/a&gt; &lt;/h4&gt;
&lt;/div&gt; 
&lt;!-- Security --&gt; 
&lt;h2&gt;&lt;span&gt;üîí&lt;/span&gt; Code of Conduct &amp;amp; Security Policy&lt;/h2&gt; 
&lt;p&gt;The InvenTree project team is committed to providing a safe and welcoming environment for all users. Please read our &lt;a href="https://raw.githubusercontent.com/inventree/InvenTree/master/CODE_OF_CONDUCT.md"&gt;Code of Conduct&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;p&gt;InvenTree is following industry best practices for security. Our security policy is included &lt;a href="https://raw.githubusercontent.com/inventree/InvenTree/master/SECURITY.md"&gt;in this repo&lt;/a&gt;. We provide dedicated security pages on &lt;a href="https://docs.inventree.org/en/latest/security/"&gt;our documentation site&lt;/a&gt;.&lt;/p&gt; 
&lt;!-- Contributing --&gt; 
&lt;h2&gt;&lt;span&gt;üëã&lt;/span&gt; Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions are welcomed and encouraged. Please help to make this project even better! Refer to the &lt;a href="https://docs.inventree.org/en/latest/develop/contributing/"&gt;contribution page&lt;/a&gt;.&lt;/p&gt; 
&lt;!-- Translation --&gt; 
&lt;h2&gt;&lt;span&gt;üìú&lt;/span&gt; Translation&lt;/h2&gt; 
&lt;p&gt;Native language translation of the InvenTree web application is &lt;a href="https://crowdin.com/project/inventree"&gt;community contributed via crowdin&lt;/a&gt;. &lt;strong&gt;Contributions are welcomed and encouraged&lt;/strong&gt;.&lt;/p&gt; 
&lt;!-- Sponsor --&gt; 
&lt;h2&gt;&lt;span&gt;üí∏&lt;/span&gt; Sponsor&lt;/h2&gt; 
&lt;p&gt;If you use InvenTree and find it to be useful, please consider &lt;a href="https://github.com/sponsors/inventree"&gt;sponsoring the project&lt;/a&gt;.&lt;/p&gt; 
&lt;!-- Acknowledgments --&gt; 
&lt;h2&gt;&lt;span&gt;üíé&lt;/span&gt; Acknowledgements&lt;/h2&gt; 
&lt;p&gt;We want to acknowledge &lt;a href="https://github.com/partkeepr/PartKeepr"&gt;PartKeepr&lt;/a&gt; as a valuable predecessor and inspiration. Find a full list of used third-party libraries in the license information dialog of your instance.&lt;/p&gt; 
&lt;h2&gt;&lt;span&gt;‚ù§Ô∏è&lt;/span&gt; Support&lt;/h2&gt; 
&lt;p&gt;This project is supported by the following sponsors:&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/MartinLoeper"&gt;&lt;img src="https://github.com/MartinLoeper.png" width="60px" alt="Martin L√∂per" /&gt;&lt;/a&gt; &lt;a href="https://github.com/lippoliv"&gt;&lt;img src="https://github.com/lippoliv.png" width="60px" alt="Oliver Lippert" /&gt;&lt;/a&gt; &lt;a href="https://github.com/lfg-seth"&gt;&lt;img src="https://github.com/lfg-seth.png" width="60px" alt="Seth Smith" /&gt;&lt;/a&gt; &lt;a href="https://github.com/snorkrat"&gt;&lt;img src="https://github.com/snorkrat.png" width="60px" alt="" /&gt;&lt;/a&gt; &lt;a href="https://github.com/spacequest-ltd"&gt;&lt;img src="https://github.com/spacequest-ltd.png" width="60px" alt="SpaceQuest Ltd" /&gt;&lt;/a&gt; &lt;a href="https://github.com/appwrite"&gt;&lt;img src="https://github.com/appwrite.png" width="60px" alt="Appwrite" /&gt;&lt;/a&gt; &lt;a href="https://github.com/PricelessToolkit"&gt;&lt;img src="https://github.com/PricelessToolkit.png" width="60px" alt="" /&gt;&lt;/a&gt; &lt;a href="https://github.com/cabottech"&gt;&lt;img src="https://github.com/cabottech.png" width="60px" alt="Cabot Technologies" /&gt;&lt;/a&gt; &lt;a href="https://github.com/markus-k"&gt;&lt;img src="https://github.com/markus-k.png" width="60px" alt="Markus Kasten" /&gt;&lt;/a&gt; &lt;a href="https://github.com/jefffhaynes"&gt;&lt;img src="https://github.com/jefffhaynes.png" width="60px" alt="Jeff Haynes" /&gt;&lt;/a&gt; &lt;a href="https://github.com/dnviti"&gt;&lt;img src="https://github.com/dnviti.png" width="60px" alt="Daniele Viti" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Islendur"&gt;&lt;img src="https://github.com/Islendur.png" width="60px" alt="Islendur" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Gibeon-NL"&gt;&lt;img src="https://github.com/Gibeon-NL.png" width="60px" alt="Gibeon-NL" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Motrac-Research-Engineering"&gt;&lt;img src="https://github.com/Motrac-Research-Engineering.png" width="60px" alt="Motrac Research" /&gt;&lt;/a&gt; &lt;a href="https://github.com/trytuna"&gt;&lt;img src="https://github.com/trytuna.png" width="60px" alt="Timo Scrappe" /&gt;&lt;/a&gt; &lt;a href="https://github.com/ATLAS2246"&gt;&lt;img src="https://github.com/ATLAS2246.png" width="60px" alt="ATLAS2246" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Kedarius"&gt;&lt;img src="https://github.com/Kedarius.png" width="60px" alt="Radek Hladik" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;With ongoing resources provided by:&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://depot.dev?utm_source=inventree"&gt;&lt;img src="https://depot.dev/badges/built-with-depot.svg?sanitize=true" alt="Built with Depot" /&gt;&lt;/a&gt; &lt;a href="https://inventree.org/digitalocean"&gt; &lt;img src="https://opensource.nyc3.cdn.digitaloceanspaces.com/attribution/assets/SVG/DO_Logo_horizontal_blue.svg?sanitize=true" width="201px" alt="Servers by Digital Ocean" /&gt; &lt;/a&gt; &lt;a href="https://www.netlify.com"&gt; &lt;img src="https://www.netlify.com/v3/img/components/netlify-color-bg.svg?sanitize=true" alt="Deploys by Netlify" /&gt; &lt;/a&gt; &lt;a href="https://crowdin.com"&gt; &lt;img src="https://crowdin.com/images/crowdin-logo.svg?sanitize=true" alt="Translation by Crowdin" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;a href="https://codspeed.io/inventree/InvenTree?utm_source=badge"&gt;&lt;img src="https://img.shields.io/endpoint?url=https://codspeed.io/badge.json" alt="CodSpeed Badge" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;!-- License --&gt; 
&lt;h2&gt;&lt;span&gt;‚ö†&lt;/span&gt; License&lt;/h2&gt; 
&lt;p&gt;Distributed under the &lt;a href="https://choosealicense.com/licenses/mit/"&gt;MIT&lt;/a&gt; License. See &lt;a href="https://github.com/inventree/InvenTree/raw/master/LICENSE"&gt;LICENSE.txt&lt;/a&gt; for more information.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>intuitem/ciso-assistant-community</title>
      <link>https://github.com/intuitem/ciso-assistant-community</link>
      <description>&lt;p&gt;CISO Assistant is a one-stop-shop GRC platform for Risk Management, AppSec, Compliance &amp; Audit, TPRM, Privacy, and Reporting. It supports 100+ global frameworks with automatic control mapping, including ISO 27001, NIST CSF, SOC 2, CIS, PCI DSS, NIS2, DORA, GDPR, HIPAA, CMMC, and more.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; Star the project üåü to get releases notification and help growing the community! &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/9343" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/9343" alt="intuitem%2Fciso-assistant-community | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;a href="https://intuitem.com"&gt;intuitem.com&lt;/a&gt; ¬∑ &lt;a href="https://intuitem.com/trial"&gt;SaaS Free trial&lt;/a&gt; ¬∑ &lt;a href="https://intuitem.releasedhub.com/ciso-assistant-public/roadmap/d738f2fd"&gt;Roadmap&lt;/a&gt; ¬∑ &lt;a href="https://intuitem.gitbook.io/ciso-assistant" target="_blank"&gt;Docs&lt;/a&gt; ¬∑ &lt;a href="https://raw.githubusercontent.com/intuitem/ciso-assistant-community/main/#supported-languages-"&gt;Languages&lt;/a&gt; ¬∑ &lt;a href="https://discord.gg/qvkaMdQ8da"&gt;Discord&lt;/a&gt; ¬∑ &lt;a href="https://raw.githubusercontent.com/intuitem/ciso-assistant-community/main/#supported-frameworks-"&gt;Frameworks&lt;/a&gt; &lt;br /&gt; &lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/intuitem/ciso-assistant-community/main/gh_banner.png" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://img.shields.io/github/v/release/intuitem/ciso-assistant-community?style=for-the-badge" alt="GitHub Release" /&gt; &lt;img src="https://img.shields.io/github/contributors-anon/intuitem/ciso-assistant-community?style=for-the-badge&amp;amp;color=%235D4596" alt="GitHub contributors" /&gt; &lt;img src="https://img.shields.io/github/stars/intuitem/ciso-assistant-community?style=for-the-badge" alt="GitHub Repo stars" /&gt; &lt;img src="https://img.shields.io/github/forks/intuitem/ciso-assistant-community?style=for-the-badge&amp;amp;color=%235D4596" alt="GitHub forks" /&gt; &lt;img src="https://img.shields.io/discord/1155083727932764190?style=for-the-badge&amp;amp;label=Discord" alt="Discord" /&gt; &lt;a href="https://intuitem.gitbook.io/ciso-assistant"&gt;&lt;img src="https://img.shields.io/static/v1?message=Documentation&amp;amp;logo=gitbook&amp;amp;logoColor=ffffff&amp;amp;label=%20&amp;amp;labelColor=5c5c5c&amp;amp;color=F4E28D&amp;amp;style=for-the-badge" /&gt;&lt;/a&gt; &lt;a href="https://ca-api-doc.pages.dev/"&gt;&lt;img src="https://img.shields.io/static/v1?message=API&amp;amp;logo=swagger&amp;amp;label=%20&amp;amp;style=for-the-badge" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;CISO Assistant offers a fresh perspective on Cybersecurity Management and &lt;strong&gt;GRC&lt;/strong&gt; (Governance, Risk, and Compliance) practices:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Designed as a central hub to connect multiple cybersecurity concepts with smart linking between objects,&lt;/li&gt; 
 &lt;li&gt;Built as a &lt;strong&gt;multi-paradigm&lt;/strong&gt; tool that adapts to different backgrounds, methodologies, and expectations,&lt;/li&gt; 
 &lt;li&gt;Explicitly &lt;strong&gt;decouples&lt;/strong&gt; compliance from cybersecurity controls, enabling reusability across the platform,&lt;/li&gt; 
 &lt;li&gt;Promotes &lt;strong&gt;reusability&lt;/strong&gt; and interlinking instead of redundant work,&lt;/li&gt; 
 &lt;li&gt;Developed with an &lt;strong&gt;API-first&lt;/strong&gt; approach to support both UI interaction and external &lt;strong&gt;automation&lt;/strong&gt;,&lt;/li&gt; 
 &lt;li&gt;Comes packed with a wide range of built-in standards, security controls, and threat libraries,&lt;/li&gt; 
 &lt;li&gt;Offers an &lt;strong&gt;open format&lt;/strong&gt; to customize and reuse your own objects and frameworks,&lt;/li&gt; 
 &lt;li&gt;Includes built-in &lt;strong&gt;risk assessment&lt;/strong&gt; and &lt;strong&gt;remediation tracking&lt;/strong&gt; workflows,&lt;/li&gt; 
 &lt;li&gt;Supports custom frameworks via a simple syntax and flexible tooling,&lt;/li&gt; 
 &lt;li&gt;Provides rich &lt;strong&gt;import/export&lt;/strong&gt; capabilities across various channels and formats (UI, CLI, Kafka, reports, etc.).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/intuitem/ciso-assistant-community/main/single_hub.png" alt="Single Hub" /&gt;&lt;/p&gt; 
&lt;p&gt;Our vision is to create a &lt;strong&gt;one-stop-shop&lt;/strong&gt; for cybersecurity management‚Äîmodernizing GRC through &lt;strong&gt;simplification&lt;/strong&gt; and &lt;strong&gt;interoperability&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;As practitioners working with cybersecurity and IT professionals, we've faced the same issues: tool fragmentation, data duplication, and a lack of intuitive, integrated solutions. CISO Assistant was born from those lessons, and we're building a community around &lt;strong&gt;pragmatic&lt;/strong&gt;, &lt;strong&gt;common-sense&lt;/strong&gt; principles.&lt;/p&gt; 
&lt;p&gt;We‚Äôre constantly evolving with input from users and customers. Like an octopus üêô, CISO Assistant keeps growing extra arms‚Äîbringing clarity, automation, and productivity to cybersecurity teams while reducing the effort of data input and output.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.codefactor.io/repository/github/intuitem/ciso-assistant-community"&gt;&lt;img src="https://www.codefactor.io/repository/github/intuitem/ciso-assistant-community/badge" alt="CodeFactor" /&gt;&lt;/a&gt; &lt;a href="https://github.com/intuitem/ciso-assistant-community/actions/workflows/backend-api-tests.yml"&gt;&lt;img src="https://github.com/intuitem/ciso-assistant-community/actions/workflows/backend-api-tests.yml/badge.svg?sanitize=true" alt="API Tests" /&gt;&lt;/a&gt; &lt;a href="https://github.com/intuitem/ciso-assistant-community/actions/workflows/functional-tests.yml"&gt;&lt;img src="https://github.com/intuitem/ciso-assistant-community/actions/workflows/functional-tests.yml/badge.svg?branch=main" alt="Functional Tests" /&gt;&lt;/a&gt; &lt;a href="https://app.fossa.com/projects/git%2Bgithub.com%2Fab-smith%2Fciso-assistant-community?ref=badge_small"&gt;&lt;img src="https://app.fossa.com/api/projects/git%2Bgithub.com%2Fab-smith%2Fciso-assistant-community.svg?type=small" alt="FOSSA Status" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Quick Start üöÄ&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] The easiest way to get started is through the &lt;a href="https://intuitem.com/trial"&gt;free trial of cloud instance available here&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Alternatively, once you have &lt;em&gt;Docker&lt;/em&gt; and &lt;em&gt;Docker-compose&lt;/em&gt; installed, on your workstation or server:&lt;/p&gt; 
&lt;p&gt;clone the repo:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;git clone --single-branch -b main https://github.com/intuitem/ciso-assistant-community.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;and run the starter script&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;./docker-compose.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you are looking for other installation options for self-hosting, check the &lt;a href="https://raw.githubusercontent.com/intuitem/ciso-assistant-community/main/config/"&gt;config builder&lt;/a&gt; and the &lt;a href="https://intuitem.gitbook.io/ciso-assistant"&gt;docs&lt;/a&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] The docker-compose script uses prebuilt Docker images supporting most of the standard hardware architecture. If you're using &lt;strong&gt;Windows&lt;/strong&gt;, Make sure to have &lt;a href="https://learn.microsoft.com/en-us/windows/wsl/install"&gt;WSL&lt;/a&gt; installed and trigger the script within a WSL command line. It will feed Docker Desktop on your behalf.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;The docker compose file can be adjusted to pass extra parameters to suit your setup (e.g. Mailer settings).&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING] If you're getting warnings or errors about image's platform not matching host platform, raise an issue with the details and we'll add it shortly after. You can also use &lt;code&gt;docker-compose-build.sh&lt;/code&gt; instead (see below) to build for your specific architecture.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!CAUTION] Don't use the &lt;code&gt;main&lt;/code&gt; branch code directly for production as it's the merge upstream and can have breaking changes during our development. Either use the &lt;code&gt;tags&lt;/code&gt; for stable versions or prebuilt images.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/intuitem/ciso-assistant-community/main/features.png" alt="Current features" /&gt;&lt;/p&gt; 
&lt;p&gt;Upcoming features are listed on the roadmap.&lt;/p&gt; 
&lt;p&gt;CISO Assistant is developed and maintained by &lt;a href="https://intuitem.com/"&gt;Intuitem&lt;/a&gt;, a company specialized in Cybersecurity, Cloud, and Data/AI.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Core Concepts&lt;/h2&gt; 
&lt;p&gt;Here‚Äôs an extract of some of the building blocks in CISO Assistant to illustrate the decoupling concept that encourages reusability:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/intuitem/ciso-assistant-community/main/core_objects.png" alt="Core Objects" /&gt;&lt;/p&gt; 
&lt;p&gt;For full details, check the &lt;a href="https://raw.githubusercontent.com/intuitem/ciso-assistant-community/main/documentation/architecture/data-model.md"&gt;data model documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Decoupling Concept&lt;/h2&gt; 
&lt;p&gt;At the heart of CISO Assistant lies the &lt;strong&gt;decoupling principle&lt;/strong&gt;, which enables powerful use cases and major time savings:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Reuse past assessments across scopes or frameworks,&lt;/li&gt; 
 &lt;li&gt;Evaluate a single scope against multiple frameworks simultaneously,&lt;/li&gt; 
 &lt;li&gt;Let CISO Assistant handle reporting and consistency checks so you can focus on remediation,&lt;/li&gt; 
 &lt;li&gt;Separate control implementation from compliance tracking.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Here is an illustration of the &lt;strong&gt;decoupling&lt;/strong&gt; principle and its advantages:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/87bd4497-5cc2-4221-aeff-396f6b6ebe62"&gt;https://github.com/user-attachments/assets/87bd4497-5cc2-4221-aeff-396f6b6ebe62&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;System architecture&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/intuitem/ciso-assistant-community/main/documentation/system-architecture.png" alt="" /&gt;&lt;/p&gt; 
&lt;h2&gt;End-user Documentation&lt;/h2&gt; 
&lt;p&gt;Check out the online documentation on &lt;a href="https://intuitem.gitbook.io/ciso-assistant"&gt;https://intuitem.gitbook.io/ciso-assistant&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Supported frameworks üêô&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;ISO 27001:2013 &amp;amp; 27001:2022 üåê&lt;/li&gt; 
 &lt;li&gt;NIST Cyber Security Framework (CSF) v1.1 üá∫üá∏&lt;/li&gt; 
 &lt;li&gt;NIST Cyber Security Framework (CSF) v2.0 üá∫üá∏&lt;/li&gt; 
 &lt;li&gt;NIS2 üá™üá∫&lt;/li&gt; 
 &lt;li&gt;SOC2 üá∫üá∏&lt;/li&gt; 
 &lt;li&gt;PCI DSS 4.0 üí≥&lt;/li&gt; 
 &lt;li&gt;CMMC v2 üá∫üá∏&lt;/li&gt; 
 &lt;li&gt;PSPF üá¶üá∫&lt;/li&gt; 
 &lt;li&gt;General Data Protection Regulation (GDPR): Full text and checklist from GDPR.EU üá™üá∫&lt;/li&gt; 
 &lt;li&gt;Essential Eight üá¶üá∫&lt;/li&gt; 
 &lt;li&gt;NYDFS 500 with 2023-11 amendments üá∫üá∏&lt;/li&gt; 
 &lt;li&gt;DORA (Act, RTS, ITS and GL) üá™üá∫&lt;/li&gt; 
 &lt;li&gt;NIST AI Risk Management Framework üá∫üá∏ü§ñ&lt;/li&gt; 
 &lt;li&gt;NIST SP 800-53 rev5 üá∫üá∏&lt;/li&gt; 
 &lt;li&gt;France LPM/OIV rules üá´üá∑&lt;/li&gt; 
 &lt;li&gt;CCB CyberFundamentals Framework üáßüá™&lt;/li&gt; 
 &lt;li&gt;NIST SP-800-66 (HIPAA) üè•&lt;/li&gt; 
 &lt;li&gt;HDS/HDH üá´üá∑&lt;/li&gt; 
 &lt;li&gt;OWASP Application Security Verification Standard (ASVS) 4 üêùüñ•Ô∏è&lt;/li&gt; 
 &lt;li&gt;RGS v2.0 üá´üá∑&lt;/li&gt; 
 &lt;li&gt;AirCyber ‚úàÔ∏èüåê&lt;/li&gt; 
 &lt;li&gt;Cyber Resilience Act (CRA) üá™üá∫&lt;/li&gt; 
 &lt;li&gt;TIBER-EU üá™üá∫&lt;/li&gt; 
 &lt;li&gt;NIST Privacy Framework üá∫üá∏&lt;/li&gt; 
 &lt;li&gt;TISAX (VDA ISA) v5.1 and v6.0 üöò&lt;/li&gt; 
 &lt;li&gt;ANSSI hygiene guide üá´üá∑&lt;/li&gt; 
 &lt;li&gt;Essential Cybersecurity Controls (ECC) üá∏üá¶&lt;/li&gt; 
 &lt;li&gt;CIS Controls v8* üåê&lt;/li&gt; 
 &lt;li&gt;CSA CCM (Cloud Controls Matrix)* ‚òÅÔ∏è&lt;/li&gt; 
 &lt;li&gt;FADP (Federal Act on Data Protection) üá®üá≠&lt;/li&gt; 
 &lt;li&gt;NIST SP 800-171 rev2 (2021) üá∫üá∏&lt;/li&gt; 
 &lt;li&gt;ANSSI : recommandations de s√©curit√© pour un syst√®me d'IA g√©n√©rative üá´üá∑ü§ñ&lt;/li&gt; 
 &lt;li&gt;NIST SP 800-218: Secure Software Development Framework (SSDF) üñ•Ô∏è&lt;/li&gt; 
 &lt;li&gt;GSA FedRAMP rev5 ‚òÅÔ∏èüá∫üá∏&lt;/li&gt; 
 &lt;li&gt;Cadre Conformit√© Cyber France (3CF) v1 (2021) ‚úàÔ∏èüá´üá∑&lt;/li&gt; 
 &lt;li&gt;ANSSI : SecNumCloud ‚òÅÔ∏èüá´üá∑&lt;/li&gt; 
 &lt;li&gt;Cadre Conformit√© Cyber France (3CF) v2 (2024) ‚úàÔ∏èüá´üá∑&lt;/li&gt; 
 &lt;li&gt;ANSSI : outil d‚Äôauto√©valuation de gestion de crise cyber üí•üá´üá∑&lt;/li&gt; 
 &lt;li&gt;BSI: IT-Grundschutz-Kompendium üá©üá™&lt;/li&gt; 
 &lt;li&gt;NIST SP 800-171 rev3 (2024) üá∫üá∏&lt;/li&gt; 
 &lt;li&gt;ENISA: 5G Security Controls Matrix üá™üá∫&lt;/li&gt; 
 &lt;li&gt;OWASP Mobile Application Security Verification Standard (MASVS) üêùüì±&lt;/li&gt; 
 &lt;li&gt;Agile Security Framework (ASF) - baseline - by intuitem ü§ó&lt;/li&gt; 
 &lt;li&gt;ISO 27001:2013 üåê (For legacy and migration)&lt;/li&gt; 
 &lt;li&gt;EU AI Act üá™üá∫ü§ñ&lt;/li&gt; 
 &lt;li&gt;FBI CJIS üá∫üá∏üëÆ&lt;/li&gt; 
 &lt;li&gt;Operational Technology Cybersecurity Controls (OTCC) üá∏üá¶&lt;/li&gt; 
 &lt;li&gt;Secure Controls Framework (SCF) üá∫üá∏üåê&lt;/li&gt; 
 &lt;li&gt;NCSC Cyber Assessment Framework (CAF) üá¨üáß&lt;/li&gt; 
 &lt;li&gt;California Consumer Privacy Act (CCPA) üá∫üá∏&lt;/li&gt; 
 &lt;li&gt;California Consumer Privacy Act Regulations üá∫üá∏&lt;/li&gt; 
 &lt;li&gt;NCSC Cyber Essentials üá¨üáß&lt;/li&gt; 
 &lt;li&gt;Directive Nationale de la S√©curit√© des Syst√®mes d'Information (DNSSI) Maroc üá≤üá¶&lt;/li&gt; 
 &lt;li&gt;Part-IS ‚úàÔ∏èüá™üá∫&lt;/li&gt; 
 &lt;li&gt;ENS Esquema Nacional de seguridad üá™üá∏&lt;/li&gt; 
 &lt;li&gt;Korea ISA ISMS-P üá∞üá∑&lt;/li&gt; 
 &lt;li&gt;Swiss ICT minimum standard üá®üá≠&lt;/li&gt; 
 &lt;li&gt;Adobe Common Controls Framework (CCF) v5 üåê&lt;/li&gt; 
 &lt;li&gt;BSI Cloud Computing Compliance Criteria Catalogue (C5) üá©üá™&lt;/li&gt; 
 &lt;li&gt;R√©f√©rentiel d‚ÄôAudit de la S√©curit√© des Syst√®mes d‚ÄôInformation, ANCS Tunisie üáπüá≥&lt;/li&gt; 
 &lt;li&gt;ECB Cyber resilience oversight expectations for financial market infrastructures üá™üá∫&lt;/li&gt; 
 &lt;li&gt;Mindeststandard-des-BSI-zur-Nutzung-externer-Cloud-Dienste (Version 2.1) üá©üá™&lt;/li&gt; 
 &lt;li&gt;Formulaire d'√©valuation de la maturit√© - niveau fondamental (DGA) üá´üá∑&lt;/li&gt; 
 &lt;li&gt;NIS2 technical and methodological requirements 2024/2690 üá™üá∫&lt;/li&gt; 
 &lt;li&gt;Saudi Arabian Monetary Authority (SAMA) Cybersecurity Framework üá∏üá¶&lt;/li&gt; 
 &lt;li&gt;Guide de s√©curit√© des donn√©es (CNIL) üá´üá∑&lt;/li&gt; 
 &lt;li&gt;International Traffic in Arms Regulations (ITAR) üá∫üá∏&lt;/li&gt; 
 &lt;li&gt;Federal Trade Commission (FTC) Standards for Safeguarding Customer Information üá∫üá∏&lt;/li&gt; 
 &lt;li&gt;OWASP's checklist for LLM governance and security üåê&lt;/li&gt; 
 &lt;li&gt;Recommandations pour les architectures des syst√®mes d‚Äôinformation sensibles ou √† diffusion restreinte (ANSSI) üá´üá∑&lt;/li&gt; 
 &lt;li&gt;CIS benchmark for Kubernetes v1.10 üåê&lt;/li&gt; 
 &lt;li&gt;De tekniske minimumskrav for statslige myndigheder üá©üá∞&lt;/li&gt; 
 &lt;li&gt;Google SAIF framework ü§ñ&lt;/li&gt; 
 &lt;li&gt;Recommandations relatives √† l'administration s√©curis√©e des SI (ANSSI) üá´üá∑&lt;/li&gt; 
 &lt;li&gt;Prudential Standard CPS 230 - Operational Risk Management (APRA) üá¶üá∫&lt;/li&gt; 
 &lt;li&gt;Prudential Standard CPS 234 - Information Security (APRA) üá¶üá∫&lt;/li&gt; 
 &lt;li&gt;Vehicle Cyber Security Audit (VCSA) v1.1 üöò&lt;/li&gt; 
 &lt;li&gt;Cisco Cloud Controls Framework (CCF) v3.0 ‚òÅÔ∏èüåê&lt;/li&gt; 
 &lt;li&gt;FINMA - Circular 2023/01 - Operational risks and resilience - Banks üá®üá≠&lt;/li&gt; 
 &lt;li&gt;Post-Quantum Cryptography (PQC) Migration Roadmap (May 2025) üîê&lt;/li&gt; 
 &lt;li&gt;Cloud Sovereignty Framework - 1.2.1 - Oct 2025 üá™üá∫&lt;/li&gt; 
 &lt;li&gt;ISO 22301:2019 outline - Business continuity management systems üåê&lt;/li&gt; 
 &lt;li&gt;Prestataires de d√©tection des incidents de s√©curit√© (PDIS) - R√©f√©rentiel d‚Äôexigences üá´üá∑&lt;/li&gt; 
 &lt;li&gt;Vendor Due Diligence - simple baseline - intuitem üåê&lt;/li&gt; 
 &lt;li&gt;Points de contr√¥le Active Directory (AD) - ANSSI üá´üá∑&lt;/li&gt; 
 &lt;li&gt;ISO 42001:2023 outline - Artificial Intelligence Management System, including Annex A ü§ñüåê&lt;/li&gt; 
 &lt;li&gt;India's Digital Personal Data Protection Act (DPDPA) - 2023 üáÆüá≥&lt;/li&gt; 
 &lt;li&gt;E-ITS (Estonia's national cyber security standard) - 2024 üá™üá™&lt;/li&gt; 
 &lt;li&gt;Microsoft cloud security benchmark v1 - ‚òÅÔ∏èüåê&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Community contributions&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;PGSSI-S (Politique G√©n√©rale de S√©curit√© des Syst√®mes d'Information de Sant√©) üá´üá∑&lt;/li&gt; 
 &lt;li&gt;ANSSI : Recommandations de configuration d'un syst√®me GNU/Linux üá´üá∑&lt;/li&gt; 
 &lt;li&gt;PSSI-MCAS (Politique de s√©curit√© des syst√®mes d‚Äôinformation pour les minist√®res charg√©s des affaires sociales) üá´üá∑&lt;/li&gt; 
 &lt;li&gt;ANSSI : Recommandations pour la protection des syst√®mes d'information essentiels üá´üá∑&lt;/li&gt; 
 &lt;li&gt;ANSSI : Recommandations de s√©curit√© pour l'architecture d'un syst√®me de journalisation üá´üá∑&lt;/li&gt; 
 &lt;li&gt;ANSSI : Recommandations de s√©curit√© relatives √† TLS üá´üá∑&lt;/li&gt; 
 &lt;li&gt;New Zealand Information Security Manual (NZISM) üá≥üáø&lt;/li&gt; 
 &lt;li&gt;Clausier de s√©curit√© num√©rique du Club RSSI Sant√© üá´üá∑&lt;/li&gt; 
 &lt;li&gt;R√©f√©rentiel National de S√©curit√© de l‚ÄôInformation (RNSI), MPT Alg√©rie üá©üáø&lt;/li&gt; 
 &lt;li&gt;Misure minime di sicurezza ICT per le pubbliche amministrazioni, AGID Italia üáÆüáπ&lt;/li&gt; 
 &lt;li&gt;Framework Nazionale CyberSecurity v2, FNCS Italia üáÆüáπ&lt;/li&gt; 
 &lt;li&gt;Framework Nazionale per la Cybersecurity e la Data Protection, ACN Italia üáÆüáπ&lt;/li&gt; 
 &lt;li&gt;PSSIE du B√©nin, ANSSI B√©nin üáßüáØ&lt;/li&gt; 
 &lt;li&gt;IGI 1300 / II 901 - Liste des exigences pour la mise en oeuvre d'un SI classifi√© (ANSSI) üá´üá∑&lt;/li&gt; 
 &lt;li&gt;R√©f√©rentiel G√©n√©ral de S√©curit√© 2.0 - Annexe B2 üá´üá∑&lt;/li&gt; 
 &lt;li&gt;Recommandations sur la s√©curisation des syst√®mes de contr√¥le d'acc√®s physique et de vid√©oprotection üá´üá∑&lt;/li&gt; 
 &lt;li&gt;Recommandations pour un usage s√©curis√© d‚Äô(Open)SSH üá´üá∑&lt;/li&gt; 
 &lt;li&gt;Recommandations de s√©curit√© relatives √† IPsec pour la protection des flux r√©seau üá´üá∑&lt;/li&gt; 
 &lt;li&gt;Recommandations relatives √† l'interconnexion d'un syst√®me d'information √† internet üá´üá∑&lt;/li&gt; 
 &lt;li&gt;Guides des m√©canismes cryptographiques üá´üá∑&lt;/li&gt; 
 &lt;li&gt;Swift Customer Security Controls Framework (CSCF) v2025 üè¶üåê&lt;/li&gt; 
 &lt;li&gt;OWASP Application Security Verification Standard (ASVS) 5 üêùüñ•Ô∏è&lt;/li&gt; 
 &lt;li&gt;NIST 800-82 (OT) - appendix üè≠ü§ñ&lt;/li&gt; 
&lt;/ol&gt; 
&lt;br /&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Frameworks with &lt;code&gt;*&lt;/code&gt; require an extra manual step of getting the latest Excel sheet through their website as their license prevent direct usage.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;br /&gt; 
&lt;p&gt;Checkout the &lt;a href="https://raw.githubusercontent.com/intuitem/ciso-assistant-community/main/backend/library/libraries/"&gt;library&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/intuitem/ciso-assistant-community/main/tools/"&gt;tools&lt;/a&gt; for the Domain Specific Language used and how you can define your own.&lt;/p&gt; 
&lt;h3&gt;Coming soon&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Indonesia PDP üáÆüá©&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;OWASP SAMM&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;COBAC R-2024/01&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ICO Data protection self-assessment üá¨üáß&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ASD ISM üá¶üá∫&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Baseline informatiebeveiliging Overheid (BIO) üá≥üá±&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;and much more: just ask on &lt;a href="https://discord.gg/qvkaMdQ8da"&gt;Discord&lt;/a&gt;. If it's an open standard, we'll do it for you, &lt;em&gt;free of charge&lt;/em&gt; üòâ&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Add your own library&lt;/h2&gt; 
&lt;p&gt;A library can be a framework, a catalog of threats or reference controls, and even a custom risk matrix.&lt;/p&gt; 
&lt;p&gt;Take a look at the &lt;code&gt;tools&lt;/code&gt; directory and its &lt;a href="https://raw.githubusercontent.com/intuitem/ciso-assistant-community/main/tools/README.md"&gt;dedicated README&lt;/a&gt;. The &lt;code&gt;convert_library_v2.py&lt;/code&gt; script will help you create your library from a simple Excel file. Once you have structured your items in that format, just run the script and use the resulting YAML file.&lt;/p&gt; 
&lt;p&gt;You can also find some specific converters in the tools directory (e.g. for CIS or CCM Controls).&lt;/p&gt; 
&lt;p&gt;There is also a tool to facilitate the creation of mappings, called &lt;code&gt;prepare_mapping_v2.py&lt;/code&gt; that will create an Excel file based on two framework libraries in YAML. Once properly filled, this Excel file can be processed by the &lt;code&gt;convert_library_v2.py&lt;/code&gt; tool to get the resulting mapping library.&lt;/p&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;p&gt;Join our &lt;a href="https://discord.gg/qvkaMdQ8da"&gt;open Discord community&lt;/a&gt; to interact with the team and other GRC experts.&lt;/p&gt; 
&lt;h2&gt;Testing the cloud version&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;The fastest and easiest way to get started is through the &lt;a href="https://intuitem.com/trial"&gt;free trial of cloud instance available here&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Testing locally üöÄ&lt;/h2&gt; 
&lt;p&gt;To run CISO Assistant locally in a straightforward way, you can use Docker compose.&lt;/p&gt; 
&lt;ol start="0"&gt; 
 &lt;li&gt;Update docker&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Make sure you have a recent version of docker (&amp;gt;= 27.0).&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Clone the repository&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;git clone --single-branch -b main https://github.com/intuitem/ciso-assistant-community.git
cd ciso-assistant-community
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Launch docker-compose script for prebuilt images:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;./docker-compose.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Alternatively&lt;/em&gt;, you can use this variant to build the docker images for your specific architecture:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;./docker-compose-build.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;When asked for, enter your email and password for your superuser.&lt;/p&gt; 
&lt;p&gt;You can then reach CISO Assistant using your web browser at &lt;a href="https://localhost:8443/"&gt;https://localhost:8443/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;For the following executions, use "docker compose up" directly.&lt;/p&gt; 
&lt;h2&gt;Setting up CISO Assistant for development&lt;/h2&gt; 
&lt;h3&gt;Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.12+&lt;/li&gt; 
 &lt;li&gt;pip 20.3+&lt;/li&gt; 
 &lt;li&gt;poetry 2.0+&lt;/li&gt; 
 &lt;li&gt;node 22+&lt;/li&gt; 
 &lt;li&gt;npm 10.2+&lt;/li&gt; 
 &lt;li&gt;pnpm 9.0+&lt;/li&gt; 
 &lt;li&gt;yaml-cpp (brew install yaml-cpp libyaml or apt install libyaml-cpp-dev)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Running the backend&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Clone the repository.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;git clone git@github.com:intuitem/ciso-assistant-community.git
cd ciso-assistant-community
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Create a file in the parent folder (e.g. ../myvars) and store your environment variables within it by copying and modifying the following code and replace &lt;code&gt;"&amp;lt;XXX&amp;gt;"&lt;/code&gt; by your private values. Take care not to commit this file in your git repo.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Mandatory variables&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;All variables in the backend have handy default values.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Recommended variables&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;export DJANGO_DEBUG=True

# Default url is set to http://localhost:5173 but you can change it, e.g. to use https with a caddy proxy
export CISO_ASSISTANT_URL=https://localhost:8443

# Setup a development mailer with Mailhog for example
export EMAIL_HOST_USER=''
export EMAIL_HOST_PASSWORD=''
export DEFAULT_FROM_EMAIL=ciso-assistant@ciso-assistantcloud.com
export EMAIL_HOST=localhost
export EMAIL_PORT=1025
export EMAIL_USE_TLS=True
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Other variables&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# CISO Assistant will use SQLite by default, but you can setup PostgreSQL by declaring these variables
export POSTGRES_NAME=ciso-assistant
export POSTGRES_USER=ciso-assistantuser
export POSTGRES_PASSWORD=&amp;lt;XXX&amp;gt;
export POSTGRES_PASSWORD_FILE=&amp;lt;XXX&amp;gt;  # alternative way to specify password
export DB_HOST=localhost
export DB_PORT=5432  # optional, default value is 5432

# CISO Assistant will use filesystem storage backend by default.
# You can use a S3 Bucket by declaring these variables
# The S3 bucket must be created before starting CISO Assistant
export USE_S3=True
export AWS_ACCESS_KEY_ID=&amp;lt;XXX&amp;gt;
export AWS_SECRET_ACCESS_KEY=&amp;lt;XXX&amp;gt;
export AWS_STORAGE_BUCKET_NAME=&amp;lt;your-bucket-name&amp;gt;
export AWS_S3_ENDPOINT_URL=&amp;lt;your-bucket-endpoint&amp;gt;

# Add a second backup mailer (will be deprecated, not recommended anymore)
export EMAIL_HOST_RESCUE=&amp;lt;XXX&amp;gt;
export EMAIL_PORT_RESCUE=587
export EMAIL_HOST_USER_RESCUE=&amp;lt;XXX&amp;gt;
export EMAIL_HOST_PASSWORD_RESCUE=&amp;lt;XXX&amp;gt;
export EMAIL_USE_TLS_RESCUE=True

# You can define the email of the first superuser, useful for automation. A mail is sent to the superuser for password initialization
export CISO_SUPERUSER_EMAIL=&amp;lt;XXX&amp;gt;

# By default, Django secret key is generated randomly at each start of CISO Assistant. This is convenient for quick test,
# but not recommended for production, as it can break the sessions (see
# this [topic](https://stackoverflow.com/questions/15170637/effects-of-changing-djangos-secret-key) for more information).
# To set a fixed secret key, use the environment variable DJANGO_SECRET_KEY.
export DJANGO_SECRET_KEY=...

# Logging configuration
export LOG_LEVEL=INFO # optional, default value is INFO. Available options: DEBUG, INFO, WARNING, ERROR, CRITICAL
export LOG_FORMAT=plain # optional, default value is plain. Available options: json, plain

# Authentication options
export AUTH_TOKEN_TTL=3600 # optional, default value is 3600 seconds (60 minutes). It defines the time to live of the authentication token
export AUTH_TOKEN_AUTO_REFRESH=True # optional, default value is True. It defines if the token TTL should be refreshed automatically after each request authenticated with the token
export AUTH_TOKEN_AUTO_REFRESH_TTL=36000 # optional, default value is 36000 seconds (10 hours). It defines the time to live of the authentication token after auto refresh. You can disable it by setting it to 0.
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Install poetry&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Visit the poetry website for instructions: &lt;a href="https://python-poetry.org/docs/#installation"&gt;https://python-poetry.org/docs/#installation&lt;/a&gt;&lt;/p&gt; 
&lt;ol start="4"&gt; 
 &lt;li&gt;Install required dependencies.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;poetry install
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="5"&gt; 
 &lt;li&gt;Recommended: Install the pre-commit hooks.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pre-commit install
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="6"&gt; 
 &lt;li&gt;If you want to setup Postgres:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;ul&gt; 
 &lt;li&gt;Launch one of these commands to enter in Postgres: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;psql as superadmin&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;sudo su postgres&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;psql&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Create the database "ciso-assistant" 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;create database ciso-assistant;&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Create user "ciso-assistantuser" and grant it access 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;create user ciso-assistantuser with password '&amp;lt;POSTGRES_PASSWORD&amp;gt;';&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;grant all privileges on database ciso-assistant to ciso-assistantuser;&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;ol start="7"&gt; 
 &lt;li&gt;If you want to setup s3 bucket:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;ul&gt; 
 &lt;li&gt;Choose your s3 provider or try s3 feature with miniO with this command: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;docker run -p 9000:9000 -p 9001:9001 -e "MINIO_ROOT_USER=XXX" -e "MINIO_ROOT_PASSWORD=XXX" quay.io/minio/minio server /data --console-address ":9001"&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;You can now check your bucket on &lt;a href="http://localhost:9001"&gt;http://localhost:9001&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fill the login with the credentials you filled on the docker run env variables&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Export in the backend directory all the env variables asked about S3 
  &lt;ul&gt; 
   &lt;li&gt;You can see the list above in the recommanded variables&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;ol start="8"&gt; 
 &lt;li&gt;Apply migrations.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;poetry run python manage.py migrate
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="9"&gt; 
 &lt;li&gt;Create a Django superuser, that will be CISO Assistant administrator.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;If you have set a mailer and CISO_SUPERUSER_EMAIL variable, there's no need to create a Django superuser with &lt;code&gt;createsuperuser&lt;/code&gt;, as it will be created automatically on first start. You should receive an email with a link to setup your password.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;poetry run python manage.py createsuperuser
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="10"&gt; 
 &lt;li&gt;Run development server.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;poetry run python manage.py runserver
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="11"&gt; 
 &lt;li&gt;for Huey (tasks runner)&lt;/li&gt; 
&lt;/ol&gt; 
&lt;ul&gt; 
 &lt;li&gt;prepare a mailer for testing.&lt;/li&gt; 
 &lt;li&gt;run &lt;code&gt;python manage.py run_huey -w 2 -k process&lt;/code&gt; or equivalent in a separate shell.&lt;/li&gt; 
 &lt;li&gt;you can use &lt;code&gt;MAIL_DEBUG&lt;/code&gt; to have mail on the console for easier debug&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Running the frontend&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;cd into the frontend directory&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;cd frontend
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Install dependencies&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npm install -g pnpm
pnpm install
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Start a development server (make sure that the django app is running)&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pnpm run dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="4"&gt; 
 &lt;li&gt;Reach the frontend on &lt;a href="http://localhost:5173"&gt;http://localhost:5173&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Safari will not properly work in this setup, as it requires https for secure cookies. The simplest solution is to use Chrome or Firefox. An alternative is to use a caddy proxy. Please see the &lt;a href="https://raw.githubusercontent.com/intuitem/ciso-assistant-community/main/frontend/README.md"&gt;readme file&lt;/a&gt; in frontend directory for more information on this.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ol start="5"&gt; 
 &lt;li&gt;Environment variables&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;All variables in the frontend have handy default values.&lt;/p&gt; 
&lt;p&gt;If you move the frontend on another host, you should set the following variable: PUBLIC_BACKEND_API_URL. Its default value is &lt;a href="http://localhost:8000/api"&gt;http://localhost:8000/api&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The PUBLIC_BACKEND_API_EXPOSED_URL is necessary for proper functioning of the SSO. It points to the URL of the API as seen from the browser. It should be equal to the concatenation of CISO_ASSISTANT_URL (in the backend) with "/api".&lt;/p&gt; 
&lt;p&gt;When you launch "node server" instead of "pnpm run dev", you need to set the ORIGIN variable to the same value as CISO_ASSISTANT_URL in the backend (e.g. &lt;a href="http://localhost:3000"&gt;http://localhost:3000&lt;/a&gt;).&lt;/p&gt; 
&lt;h3&gt;Managing migrations&lt;/h3&gt; 
&lt;p&gt;The migrations are tracked by version control, &lt;a href="https://docs.djangoproject.com/en/4.2/topics/migrations/#version-control"&gt;https://docs.djangoproject.com/en/4.2/topics/migrations/#version-control&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;For the first version of the product, it is recommended to start from a clean migration.&lt;/p&gt; 
&lt;p&gt;Note: to clean existing migrations, type:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;find . -path "*/migrations/*.py" -not -name "__init__.py" -delete
find . -path "*/migrations/*.pyc"  -delete
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After a change (or a clean), it is necessary to re-generate migration files:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;poetry run python manage.py makemigrations
poetry run python manage.py migrate
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;These migration files should be tracked by version control.&lt;/p&gt; 
&lt;h3&gt;Test suite&lt;/h3&gt; 
&lt;p&gt;To run API tests on the backend, simply type "poetry run pytest" in a shell in the backend folder.&lt;/p&gt; 
&lt;p&gt;To run functional tests on the frontend, do the following actions:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;in the frontend folder, launch the following command:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;tests/e2e-tests.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The goal of the test harness is to prevent any regression, i.e. all the tests shall be successful, both for backend and frontend.&lt;/p&gt; 
&lt;h2&gt;API and Swagger&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;The interactive API documentation (Swagger UI) is available only in development mode. To enable it, set &lt;code&gt;export DJANGO_DEBUG=True&lt;/code&gt; before starting the backend.&lt;/li&gt; 
 &lt;li&gt;Once the server is running, the documentation will be accessible at &lt;code&gt;&amp;lt;backend_endpoint&amp;gt;/api/schema/swagger/&lt;/code&gt;, for example: &lt;a href="http://127.0.0.1:8000/api/schema/swagger/"&gt;http://127.0.0.1:8000/api/schema/swagger/&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To interact with the API via Swagger or directly with HTTP calls:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Authenticate by sending a POST request to &lt;code&gt;/api/iam/login/&lt;/code&gt; with your credentials in the request body. The response will include an authentication token.&lt;/li&gt; 
 &lt;li&gt;Include this token in the header of subsequent requests as: &lt;code&gt;Authorization: Token &amp;lt;token&amp;gt;&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;‚ö†Ô∏è Note: use &lt;code&gt;Token&lt;/code&gt;, &lt;strong&gt;not&lt;/strong&gt; &lt;code&gt;Bearer&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;When using the interactive Swagger UI, simply log in, the token will be automatically handled for subsequent requests.&lt;/p&gt; 
&lt;h2&gt;Setting CISO Assistant for production&lt;/h2&gt; 
&lt;p&gt;The docker-compose-prod.yml highlights a relevant configuration with a Caddy proxy in front of the frontend. It exposes API calls only for SSO. Note that docker-compose.yml exposes the full API, which is not yet recommended for production.&lt;/p&gt; 
&lt;p&gt;Set DJANGO_DEBUG=False for security reason.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] The frontend cannot infer the host automatically, so you need to either set the ORIGIN variable, or the HOST_HEADER and PROTOCOL_HEADER variables. Please see &lt;a href="https://kit.svelte.dev/docs/adapter-node#environment-variables-origin-protocolheader-hostheader-and-port-header"&gt;the sveltekit doc&lt;/a&gt; on this tricky issue. Beware that this approach does not work with "pnpm run dev", which should not be a worry for production.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Caddy needs to receive a SNI header. Therefore, for your public URL (the one declared in CISO_ASSISTANT_URL), you need to use a FQDN, not an IP address, as the SNI is not transmitted by a browser if the host is an IP address. Another tricky issue!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Supported languages üåê&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;FR: French&lt;/li&gt; 
 &lt;li&gt;EN: English&lt;/li&gt; 
 &lt;li&gt;AR: Arabic&lt;/li&gt; 
 &lt;li&gt;PT: Portuguese&lt;/li&gt; 
 &lt;li&gt;ES: Spanish&lt;/li&gt; 
 &lt;li&gt;DE: German&lt;/li&gt; 
 &lt;li&gt;NL: Dutch&lt;/li&gt; 
 &lt;li&gt;IT: Italian&lt;/li&gt; 
 &lt;li&gt;PL: Polish&lt;/li&gt; 
 &lt;li&gt;RO: Romanian&lt;/li&gt; 
 &lt;li&gt;HI: Hindi&lt;/li&gt; 
 &lt;li&gt;UR: Urdu&lt;/li&gt; 
 &lt;li&gt;CS: Czech&lt;/li&gt; 
 &lt;li&gt;SV: Swedish&lt;/li&gt; 
 &lt;li&gt;ID: Indonesian&lt;/li&gt; 
 &lt;li&gt;DA: Danish&lt;/li&gt; 
 &lt;li&gt;HU: Hungarian&lt;/li&gt; 
 &lt;li&gt;UK: Ukrainian&lt;/li&gt; 
 &lt;li&gt;EL: Greek&lt;/li&gt; 
 &lt;li&gt;TR: Turkish&lt;/li&gt; 
 &lt;li&gt;HR: Croatian&lt;/li&gt; 
 &lt;li&gt;ZH: Chinese (Simplified)&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Contributors ü§ù&lt;/h2&gt; 
&lt;a href="https://github.com/intuitem/ciso-assistant-community/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=intuitem/ciso-assistant-community&amp;amp;columns=9" /&gt; &lt;/a&gt; 
&lt;h2&gt;Built With üíú&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.djangoproject.com/"&gt;Django&lt;/a&gt; - Python Web Development Framework&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://kit.svelte.dev/"&gt;SvelteKit&lt;/a&gt; - Frontend Framework&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://echarts.apache.org"&gt;eCharts&lt;/a&gt; - Charting library&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://unovis.dev"&gt;unovis&lt;/a&gt; - Complementary charting library&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://gunicorn.org/"&gt;Gunicorn&lt;/a&gt; - Python WSGI HTTP Server for UNIX&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://caddyserver.com"&gt;Caddy&lt;/a&gt; - The coolest reverse Proxy&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.gitbook.com"&gt;Gitbook&lt;/a&gt; - Documentation platform&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.postgresql.org/"&gt;PostgreSQL&lt;/a&gt; - Open Source RDBMS&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.sqlite.org/index.html"&gt;SQLite&lt;/a&gt; - Open Source RDBMS&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.docker.com/"&gt;Docker&lt;/a&gt; - Container Engine&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://inlang.com/"&gt;inlang&lt;/a&gt; - The ecosystem to globalize your software&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huey.readthedocs.io/en/latest/"&gt;Huey&lt;/a&gt; - A lightweight task queue&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Security&lt;/h2&gt; 
&lt;p&gt;Great care has been taken to follow security best practices. Please report any issue to &lt;a href="mailto:security@intuitem.com"&gt;security@intuitem.com&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This repository contains the source code for both the Open Source edition of CISO Assistant (Community Edition), released under the AGPL v3, as well as the commercial edition of CISO Assistant (Pro and Enterprise Editions), released under the intuitem Commercial Software License. This mono-repository approach is adopted for simplicity.&lt;/p&gt; 
&lt;p&gt;All the files within the top-level "enterprise" directory are released under the intuitem Commercial Software License.&lt;/p&gt; 
&lt;p&gt;All the files outside the top-level "enterprise" directory are released under the &lt;a href="https://choosealicense.com/licenses/agpl-3.0/"&gt;AGPLv3&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/intuitem/ciso-assistant-community/main/LICENSE.md"&gt;LICENSE.md&lt;/a&gt; for details. For more details about the commercial editions, you can reach us on &lt;a href="mailto:contact@intuitem.com"&gt;contact@intuitem.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Unless otherwise noted, all files are ¬© intuitem.&lt;/p&gt; 
&lt;h2&gt;Activity&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://repobeats.axiom.co/api/embed/02f80d1b099ffd1ae66d9cfdc3a0e13234606f35.svg?sanitize=true" alt="Alt" title="Repobeats analytics image" /&gt;&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>