<rss version="2.0">
  <channel>
    <title>GitHub Python Daily Trending</title>
    <description>Daily Trending of Python in GitHub</description>
    <pubDate>Sat, 13 Sep 2025 01:36:17 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>rwv/chinese-dos-games</title>
      <link>https://github.com/rwv/chinese-dos-games</link>
      <description>&lt;p&gt;🎮 Chinese DOS games collections.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;🎮 中文 DOS 游戏&lt;/h1&gt; 
&lt;p&gt;网址： &lt;a href="https://dos.lol"&gt;https://dos.lol&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;中文 DOS 游戏合集，目前共有 1898 款游戏。&lt;/p&gt; 
&lt;h2&gt;下载游戏文件&lt;/h2&gt; 
&lt;p&gt;在根目录下运行 Python 3 脚本&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;python download_data.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;若下载出错请参见 &lt;a href="https://github.com/rwv/chinese-dos-games/issues/26"&gt;Issue #26&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;游戏列表&lt;/h2&gt; 
&lt;p&gt;参见 &lt;a href="https://dos.lol/games"&gt;https://dos.lol/games&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;IPFS&lt;/h2&gt; 
&lt;p&gt;IPNS Hash: &lt;a href="https://ipfs.io/ipns/k2k4r8oyknzob8jjqpj6toer4dw3jc6srsbqlbsalktnw1fopb7iyqd2"&gt;&lt;code&gt;k2k4r8oyknzob8jjqpj6toer4dw3jc6srsbqlbsalktnw1fopb7iyqd2&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;网站源代码&lt;/h2&gt; 
&lt;p&gt;请参见 &lt;a href="https://github.com/rwv/chinese-dos-games-web"&gt;rwv/chinese-dos-games-web: 🌐 Source code of https://dos.zczc.cz&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;版权问题&lt;/h2&gt; 
&lt;p&gt;本人明白此项目存在版权上的侵权，如版权方介意的话，请联系 &lt;a href="mailto:chinese.dos.games@outlook.com"&gt;chinese.dos.games@outlook.com&lt;/a&gt;，本人将立刻删除有关文件。&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;欢迎提 &lt;a href="https://github.com/rwv/chinese-dos-games/issues"&gt;Issue&lt;/a&gt; 和 &lt;a href="https://github.com/rwv/chinese-dos-games/pulls"&gt;Pull request&lt;/a&gt; 来增加新的游戏!&lt;/p&gt; 
&lt;p&gt;PR 具体参见 &lt;a href="https://github.com/rwv/chinese-dos-games/raw/master/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Credits&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/dreamlayers/em-dosbox"&gt;dreamlayers/em-dosbox: An Emscripten port of DOSBox&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/db48x/emularity"&gt;db48x/emularity: easily embed emulators&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://tieba.baidu.com/p/3962261741"&gt;衡兰若芷制作的DOS游戏合集&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>1Panel-dev/MaxKB</title>
      <link>https://github.com/1Panel-dev/MaxKB</link>
      <description>&lt;p&gt;🔥 MaxKB is an open-source platform for building enterprise-grade agents. MaxKB 是强大易用的开源企业级智能体平台。&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt;&lt;img src="https://github.com/1Panel-dev/maxkb/assets/52996290/c0694996-0eed-40d8-b369-322bf2a380bf" alt="MaxKB" width="300" /&gt;&lt;/p&gt; 
&lt;h3 align="center"&gt;Open-source platform for building enterprise-grade agents&lt;/h3&gt; 
&lt;h3 align="center"&gt;强大易用的企业级智能体平台&lt;/h3&gt; 
&lt;p align="center"&gt;&lt;a href="https://trendshift.io/repositories/9113" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/9113" alt="1Panel-dev%2FMaxKB | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://www.gnu.org/licenses/gpl-3.0.html#license-text"&gt;&lt;img src="https://img.shields.io/github/license/1Panel-dev/maxkb?color=%231890FF" alt="License: GPL v3" /&gt;&lt;/a&gt; &lt;a href="https://github.com/1Panel-dev/maxkb/releases/latest"&gt;&lt;img src="https://img.shields.io/github/v/release/1Panel-dev/maxkb" alt="Latest release" /&gt;&lt;/a&gt; &lt;a href="https://github.com/1Panel-dev/maxkb"&gt;&lt;img src="https://img.shields.io/github/stars/1Panel-dev/maxkb?color=%231890FF&amp;amp;style=flat-square" alt="Stars" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/1panel/maxkb"&gt;&lt;img src="https://img.shields.io/docker/pulls/1panel/maxkb?label=downloads" alt="Download" /&gt;&lt;/a&gt;&lt;br /&gt; [&lt;a href="https://raw.githubusercontent.com/1Panel-dev/MaxKB/v2/README_CN.md"&gt;中文(简体)&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/1Panel-dev/MaxKB/v2/README.md"&gt;English&lt;/a&gt;] &lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;MaxKB = Max Knowledge Brain, it is an open-source platform for building enterprise-grade agents. MaxKB integrates Retrieval-Augmented Generation (RAG) pipelines, supports robust workflows, and provides advanced MCP tool-use capabilities. MaxKB is widely applied in scenarios such as intelligent customer service, corporate internal knowledge bases, academic research, and education.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;RAG Pipeline&lt;/strong&gt;: Supports direct uploading of documents / automatic crawling of online documents, with features for automatic text splitting, vectorization. This effectively reduces hallucinations in large models, providing a superior smart Q&amp;amp;A interaction experience.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Agentic Workflow&lt;/strong&gt;: Equipped with a powerful workflow engine, function library and MCP tool-use, enabling the orchestration of AI processes to meet the needs of complex business scenarios.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Seamless Integration&lt;/strong&gt;: Facilitates zero-coding rapid integration into third-party business systems, quickly equipping existing systems with intelligent Q&amp;amp;A capabilities to enhance user satisfaction.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Model-Agnostic&lt;/strong&gt;: Supports various large models, including private models (such as DeepSeek, Llama, Qwen, etc.) and public models (like OpenAI, Claude, Gemini, etc.).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi Modal&lt;/strong&gt;: Native support for input and output text, image, audio and video.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quick start&lt;/h2&gt; 
&lt;p&gt;Execute the script below to start a MaxKB container using Docker:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -d --name=maxkb --restart=always -p 8080:8080 -v ~/.maxkb:/opt/maxkb 1panel/maxkb
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Access MaxKB web interface at &lt;code&gt;http://your_server_ip:8080&lt;/code&gt; with default admin credentials:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;username: admin&lt;/li&gt; 
 &lt;li&gt;password: MaxKB@123..&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;中国用户如遇到 Docker 镜像 Pull 失败问题，请参照该 &lt;a href="https://maxkb.cn/docs/v2/installation/offline_installtion/"&gt;离线安装文档&lt;/a&gt; 进行安装。&lt;/p&gt; 
&lt;h2&gt;Screenshots&lt;/h2&gt; 
&lt;table style="border-collapse: collapse; border: 1px solid black;"&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td style="padding: 5px;background-color:#fff;"&gt;&lt;img src="https://github.com/user-attachments/assets/eb285512-a66a-4752-8941-c65ed1592238" alt="MaxKB Demo1" /&gt;&lt;/td&gt; 
   &lt;td style="padding: 5px;background-color:#fff;"&gt;&lt;img src="https://github.com/user-attachments/assets/f732f1f5-472c-4fd2-93c1-a277eda83d04" alt="MaxKB Demo2" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td style="padding: 5px;background-color:#fff;"&gt;&lt;img src="https://github.com/user-attachments/assets/c927474a-9a23-4830-822f-5db26025c9b2" alt="MaxKB Demo3" /&gt;&lt;/td&gt; 
   &lt;td style="padding: 5px;background-color:#fff;"&gt;&lt;img src="https://github.com/user-attachments/assets/e6268996-a46d-4e58-9f30-31139df78ad2" alt="MaxKB Demo4" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Technical stack&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Frontend：&lt;a href="https://vuejs.org/"&gt;Vue.js&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Backend：&lt;a href="https://www.djangoproject.com/"&gt;Python / Django&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;LLM Framework：&lt;a href="https://www.langchain.com/"&gt;LangChain&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Database：&lt;a href="https://www.postgresql.org/"&gt;PostgreSQL + pgvector&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#1Panel-dev/MaxKB&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=1Panel-dev/MaxKB&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Licensed under The GNU General Public License version 3 (GPLv3) (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.gnu.org/licenses/gpl-3.0.html"&gt;https://www.gnu.org/licenses/gpl-3.0.html&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>LMCache/LMCache</title>
      <link>https://github.com/LMCache/LMCache</link>
      <description>&lt;p&gt;Supercharge Your LLM with the Fastest KV Cache Layer&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/LMCache/LMCache/dev/asset/logo.png" width="720" alt="lmcache logo" /&gt; &lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://docs.lmcache.ai/"&gt;&lt;img src="https://img.shields.io/badge/docs-live-brightgreen" alt="Docs" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/lmcache/"&gt;&lt;img src="https://img.shields.io/pypi/v/lmcache" alt="PyPI" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/lmcache/"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/lmcache" alt="PyPI - Python Version" /&gt;&lt;/a&gt; &lt;a href="https://buildkite.com/lmcache/lmcache-unittests"&gt;&lt;img src="https://badge.buildkite.com/ce25f1819a274b7966273bfa54f0e02f092c3de0d7563c5c9d.svg?sanitize=true" alt="Unit Tests" /&gt;&lt;/a&gt; &lt;a href="https://github.com/LMCache/LMCache/actions/workflows/code_quality_checks.yml"&gt;&lt;img src="https://github.com/lmcache/lmcache/actions/workflows/code_quality_checks.yml/badge.svg?branch=dev&amp;amp;label=tests" alt="Code Quality" /&gt;&lt;/a&gt; &lt;a href="https://buildkite.com/lmcache/lmcache-vllm-integration-tests"&gt;&lt;img src="https://badge.buildkite.com/108ddd4ab482a2480999dec8c62a640a3315ed4e6c4e86798e.svg?sanitize=true" alt="Integration Tests" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://www.bestpractices.dev/projects/10841"&gt;&lt;img src="https://www.bestpractices.dev/projects/10841/badge" alt="OpenSSF Best Practices" /&gt;&lt;/a&gt; &lt;a href="https://scorecard.dev/viewer/?uri=github.com/LMCache/LMCache"&gt;&lt;img src="https://api.scorecard.dev/projects/github.com/LMCache/LMCache/badge" alt="OpenSSF Scorecard" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/LMCache/LMCache/"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt; &lt;a href="https://github.com/LMCache/LMCache/graphs/commit-activity"&gt;&lt;img src="https://img.shields.io/github/commit-activity/w/LMCache/LMCache" alt="GitHub commit activity" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/lmcache/"&gt;&lt;img src="https://img.shields.io/pypi/dm/lmcache" alt="PyPI - Downloads" /&gt;&lt;/a&gt; &lt;a href="https://www.youtube.com/channel/UC58zMz55n70rtf1Ak2PULJA"&gt;&lt;img src="https://img.shields.io/youtube/channel/views/UC58zMz55n70rtf1Ak2PULJA" alt="YouTube Channel Views" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;p&gt;| &lt;a href="https://blog.lmcache.ai/"&gt;&lt;strong&gt;Blog&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://docs.lmcache.ai/"&gt;&lt;strong&gt;Documentation&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://join.slack.com/t/lmcacheworkspace/shared_invite/zt-3bgx768yd-H8WkOTmPtbxVYJ5nuZ4dmA"&gt;&lt;strong&gt;Join Slack&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://forms.gle/MHwLiYDU6kcW3dLj7"&gt;&lt;strong&gt;Interest Form&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://github.com/LMCache/LMCache/issues/1253"&gt;&lt;strong&gt;Roadmap&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;🔥 &lt;strong&gt;NEW: For enterprise-scale deployment of LMCache and vLLM, please check out vLLM &lt;a href="https://github.com/vllm-project/production-stack"&gt;Production Stack&lt;/a&gt;. LMCache is also officially supported in &lt;a href="https://github.com/llm-d/llm-d/"&gt;llm-d&lt;/a&gt; and &lt;a href="https://github.com/kserve/kserve"&gt;KServe&lt;/a&gt;!&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Summary&lt;/h2&gt; 
&lt;p&gt;LMCache is an &lt;strong&gt;LLM&lt;/strong&gt; serving engine extension to &lt;strong&gt;reduce TTFT&lt;/strong&gt; and &lt;strong&gt;increase throughput&lt;/strong&gt;, especially under long-context scenarios. By storing the KV caches of reusable texts across various locations, including (GPU, CPU DRAM, Local Disk), LMCache reuses the KV caches of &lt;strong&gt;&lt;em&gt;any&lt;/em&gt;&lt;/strong&gt; reused text (not necessarily prefix) in &lt;strong&gt;&lt;em&gt;any&lt;/em&gt;&lt;/strong&gt; serving engine instance. Thus, LMCache saves precious GPU cycles and reduces user response delay.&lt;/p&gt; 
&lt;p&gt;By combining LMCache with vLLM, developers achieve 3-10x delay savings and GPU cycle reduction in many LLM use cases, including multi-round QA and RAG.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/86137f17-f216-41a0-96a7-e537764f7a4c" alt="performance" /&gt;&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; 🔥 Integration with vLLM v1 with the following features: 
  &lt;ul&gt; 
   &lt;li&gt;High performance CPU KVCache offloading&lt;/li&gt; 
   &lt;li&gt;Disaggregated prefill&lt;/li&gt; 
   &lt;li&gt;P2P KVCache sharing&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; LMCache is supported in the &lt;a href="https://github.com/vllm-project/production-stack/"&gt;vLLM production stack&lt;/a&gt;, &lt;a href="https://github.com/llm-d/llm-d/"&gt;llm-d&lt;/a&gt;, and &lt;a href="https://github.com/kserve/kserve"&gt;KServe&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Stable support for non-prefix KV caches&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Storage support as follows: 
  &lt;ul&gt; 
   &lt;li&gt;CPU&lt;/li&gt; 
   &lt;li&gt;Disk&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/ai-dynamo/nixl"&gt;NIXL&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Installation support through pip and latest vLLM&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;To use LMCache, simply install &lt;code&gt;lmcache&lt;/code&gt; from your package manager, e.g. pip:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install lmcache
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Works on Linux NVIDIA GPU platform.&lt;/p&gt; 
&lt;p&gt;More &lt;a href="https://docs.lmcache.ai/getting_started/installation"&gt;detailed installation instructions&lt;/a&gt; are available in the docs, particularly if you are not using the latest stable version of vllm or using another serving engine with different dependencies. Any "undefined symbol" or torch mismatch versions can be resolved in the documentation.&lt;/p&gt; 
&lt;h2&gt;Getting started&lt;/h2&gt; 
&lt;p&gt;The best way to get started is to checkout the &lt;a href="https://docs.lmcache.ai/getting_started/quickstart/"&gt;Quickstart Examples&lt;/a&gt; in the docs.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;Check out the LMCache &lt;a href="https://docs.lmcache.ai/"&gt;documentation&lt;/a&gt; which is available online.&lt;/p&gt; 
&lt;p&gt;We also post regularly in &lt;a href="https://blog.lmcache.ai/"&gt;LMCache blogs&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;p&gt;Go hands-on with our &lt;a href="https://github.com/LMCache/LMCache/tree/dev/examples"&gt;examples&lt;/a&gt;, demonstrating how to address different use cases with LMCache.&lt;/p&gt; 
&lt;h2&gt;Interested in Connecting?&lt;/h2&gt; 
&lt;p&gt;Fill out the &lt;a href="https://forms.gle/mQfQDUXbKfp2St1z7"&gt;interest form&lt;/a&gt;, &lt;a href="https://mailchi.mp/tensormesh/lmcache-sign-up-newsletter"&gt;sign up for our newsletter&lt;/a&gt;, &lt;a href="https://join.slack.com/t/lmcacheworkspace/shared_invite/zt-2viziwhue-5Amprc9k5hcIdXT7XevTaQ"&gt;join LMCache slack&lt;/a&gt;, &lt;a href="https://lmcache.ai/"&gt;check out LMCache website&lt;/a&gt;, or &lt;a href="mailto:contact@lmcache.ai"&gt;drop an email&lt;/a&gt;, and our team will reach out to you!&lt;/p&gt; 
&lt;h2&gt;Community meeting&lt;/h2&gt; 
&lt;p&gt;The &lt;a href="https://uchicago.zoom.us/j/6603596916?pwd=Z1E5MDRWUSt2am5XbEt4dTFkNGx6QT09"&gt;community meeting&lt;/a&gt; for LMCache is hosted bi-weekly. All are welcome to join!&lt;/p&gt; 
&lt;p&gt;Meetings are held bi-weekly on: Tuesdays at 9:00 AM PT – &lt;a href="https://drive.usercontent.google.com/u/0/uc?id=1f5EXbooGcwNwzIpTgn5u4PHqXgfypMtu&amp;amp;export=download"&gt;Add to Calendar&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;We keep notes from each meeting on this &lt;a href="https://docs.google.com/document/d/1_Fl3vLtERFa3vTH00cezri78NihNBtSClK-_1tSrcow"&gt;document&lt;/a&gt; for summaries of standups, discussion, and action items.&lt;/p&gt; 
&lt;p&gt;Recordings of meetings are available on the &lt;a href="https://www.youtube.com/channel/UC58zMz55n70rtf1Ak2PULJA"&gt;YouTube LMCache channel&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome and value all contributions and collaborations. Please check out &lt;a href="https://raw.githubusercontent.com/LMCache/LMCache/dev/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt; on how to contribute.&lt;/p&gt; 
&lt;p&gt;We continually update &lt;a href="https://github.com/LMCache/LMCache/issues/627"&gt;[Onboarding] Welcoming contributors with good first issues!&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you use LMCache for your research, please cite our papers:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@inproceedings{liu2024cachegen,
  title={Cachegen: Kv cache compression and streaming for fast large language model serving},
  author={Liu, Yuhan and Li, Hanchen and Cheng, Yihua and Ray, Siddhant and Huang, Yuyang and Zhang, Qizheng and Du, Kuntai and Yao, Jiayi and Lu, Shan and Ananthanarayanan, Ganesh and others},
  booktitle={Proceedings of the ACM SIGCOMM 2024 Conference},
  pages={38--56},
  year={2024}
}

@article{cheng2024large,
  title={Do Large Language Models Need a Content Delivery Network?},
  author={Cheng, Yihua and Du, Kuntai and Yao, Jiayi and Jiang, Junchen},
  journal={arXiv preprint arXiv:2409.13761},
  year={2024}
}

@inproceedings{10.1145/3689031.3696098,
  author = {Yao, Jiayi and Li, Hanchen and Liu, Yuhan and Ray, Siddhant and Cheng, Yihua and Zhang, Qizheng and Du, Kuntai and Lu, Shan and Jiang, Junchen},
  title = {CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion},
  year = {2025},
  url = {https://doi.org/10.1145/3689031.3696098},
  doi = {10.1145/3689031.3696098},
  booktitle = {Proceedings of the Twentieth European Conference on Computer Systems},
  pages = {94–109},
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Socials&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.linkedin.com/company/lmcache-lab/?viewAsMember=true"&gt;Linkedin&lt;/a&gt; | &lt;a href="https://x.com/lmcache"&gt;Twitter&lt;/a&gt; | &lt;a href="https://www.youtube.com/@LMCacheTeam"&gt;Youtube&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;The LMCache codebase is licensed under Apache License 2.0. See the &lt;a href="https://raw.githubusercontent.com/LMCache/LMCache/dev/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>sentient-agi/ROMA</title>
      <link>https://github.com/sentient-agi/ROMA</link>
      <description>&lt;p&gt;Recursive-Open-Meta-Agent v0.1 (Beta). A meta-agent framework to build high-performance multi-agent systems.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;div align="center"&gt; 
  &lt;img src="https://raw.githubusercontent.com/sentient-agi/ROMA/main/assets/sentient-logo.png" alt="alt text" width="60%" /&gt; 
 &lt;/div&gt; 
 &lt;h1&gt;ROMA: Recursive Open Meta-Agents&lt;/h1&gt; 
 &lt;p align="center"&gt; &lt;strong&gt;Building hierarchical high-performance multi-agent systems made easy! (Beta) &lt;/strong&gt; &lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/14848" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/14848" alt="sentient-agi%2FROMA | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;a href="https://sentient.xyz/" target="_blank" style="margin: 2px;"&gt; &lt;img alt="Homepage" src="https://img.shields.io/badge/Sentient-Homepage-%23EAEAEA?logo=data%3Aimage%2Fsvg%2Bxml%3Bbase64%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%2BPC9zdmc%2B&amp;amp;link=https%3A%2F%2Fhuggingface.co%2FSentientagi" style="display: inline-block; vertical-align: middle;" /&gt; &lt;/a&gt; &lt;a href="https://github.com/sentient-agi" target="_blank" style="margin: 2px;"&gt; &lt;img alt="GitHub" src="https://img.shields.io/badge/Github-sentient_agi-181717?logo=github" style="display: inline-block; vertical-align: middle;" /&gt; &lt;/a&gt; &lt;a href="https://huggingface.co/Sentientagi" target="_blank" style="margin: 2px;"&gt; &lt;img alt="Hugging Face" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-SentientAGI-ffc107?color=ffc107&amp;amp;logoColor=white" style="display: inline-block; vertical-align: middle;" /&gt; &lt;/a&gt; &lt;/p&gt;
&lt;/div&gt; 
&lt;div align="center" style="line-height: 1;"&gt; 
 &lt;a href="https://discord.gg/sentientfoundation" target="_blank" style="margin: 2px;"&gt; &lt;img alt="Discord" src="https://img.shields.io/badge/Discord-SentientAGI-7289da?logo=discord&amp;amp;logoColor=white&amp;amp;color=7289da" style="display: inline-block; vertical-align: middle;" /&gt; &lt;/a&gt; 
 &lt;a href="https://x.com/SentientAGI" target="_blank" style="margin: 2px;"&gt; &lt;img alt="Twitter Follow" src="https://img.shields.io/badge/-SentientAGI-grey?logo=x&amp;amp;link=https%3A%2F%2Fx.com%2FSentientAGI%2F" style="display: inline-block; vertical-align: middle;" /&gt; &lt;/a&gt; 
 &lt;p&gt;&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;a href="https://www.sentient.xyz/blog/recursive-open-meta-agent"&gt;Technical Blog&lt;/a&gt; • &lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/docs/"&gt;Paper (Coming soon)&lt;/a&gt; • &lt;a href="https://www.sentient.xyz/"&gt;Build Agents for $$$&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt;  
&lt;h2&gt;📖 Documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/docs/INTRODUCTION.md"&gt;🚀 Introduction&lt;/a&gt;&lt;/strong&gt; - Understand the vision and architecture behind ROMA&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/docs/SETUP.md"&gt;📦 Setup&lt;/a&gt;&lt;/strong&gt; - Detailed configuration options and environment setup&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/docs/AGENTS_GUIDE.md"&gt;🤖 Agents Guide&lt;/a&gt;&lt;/strong&gt; - Learn how to create and customize your own agents&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/docs/CONFIGURATION.md"&gt;⚙️ Configuration&lt;/a&gt;&lt;/strong&gt; - Detailed configuration options and environment setup&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/docs/ROADMAP.md"&gt;🗺️ Roadmap&lt;/a&gt;&lt;/strong&gt; - See what's coming next for ROMA&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🎯 What is ROMA?&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/sentient-agi/ROMA/main/assets/roma_run.gif" alt="alt text" width="80%" /&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;strong&gt;ROMA&lt;/strong&gt; is a &lt;strong&gt;meta-agent framework&lt;/strong&gt; that uses recursive hierarchical structures to solve complex problems. By breaking down tasks into parallelizable components, ROMA enables agents to tackle sophisticated reasoning challenges while maintaining transparency that makes context-engineering and iteration straightforward. The framework offers &lt;strong&gt;parallel problem solving&lt;/strong&gt; where agents work simultaneously on different parts of complex tasks, &lt;strong&gt;transparent development&lt;/strong&gt; with a clear structure for easy debugging, and &lt;strong&gt;proven performance&lt;/strong&gt; demonstrated through our search agent's strong benchmark results. We've shown the framework's effectiveness, but this is just the beginning. As an &lt;strong&gt;open-source and extensible&lt;/strong&gt; platform, ROMA is designed for community-driven development, allowing you to build and customize agents for your specific needs while benefiting from the collective improvements of the community.&lt;/p&gt; 
&lt;h2&gt;🏗️ How It Works&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;ROMA&lt;/strong&gt; framework processes tasks through a recursive &lt;strong&gt;plan–execute loop&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;def solve(task):
    if is_atomic(task):                 # Step 1: Atomizer
        return execute(task)            # Step 2: Executor
    else:
        subtasks = plan(task)           # Step 2: Planner
        results = []
        for subtask in subtasks:
            results.append(solve(subtask))  # Recursive call
        return aggregate(results)       # Step 3: Aggregator

# Entry point:
answer = solve(initial_request)
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Atomizer&lt;/strong&gt; – Decides whether a request is &lt;strong&gt;atomic&lt;/strong&gt; (directly executable) or requires &lt;strong&gt;planning&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Planner&lt;/strong&gt; – If planning is needed, the task is broken into smaller &lt;strong&gt;subtasks&lt;/strong&gt;. Each subtask is fed back into the &lt;strong&gt;Atomizer&lt;/strong&gt;, making the process recursive.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Executors&lt;/strong&gt; – Handle atomic tasks. Executors can be &lt;strong&gt;LLMs, APIs, or even other agents&lt;/strong&gt; — as long as they implement an &lt;code&gt;agent.execute()&lt;/code&gt; interface.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Aggregator&lt;/strong&gt; – Collects and integrates results from subtasks. Importantly, the Aggregator produces the &lt;strong&gt;answer to the original parent task&lt;/strong&gt;, not just raw child outputs.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;📐 Information Flow&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Top-down:&lt;/strong&gt; Tasks are decomposed into subtasks recursively.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Bottom-up:&lt;/strong&gt; Subtask results are aggregated upwards into solutions for parent tasks.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Left-to-right:&lt;/strong&gt; If a subtask depends on the output of a previous one, it waits until that subtask completes before execution.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This structure makes the system flexible, recursive, and dependency-aware — capable of decomposing complex problems into smaller steps while ensuring results are integrated coherently.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click to view the system flow diagram&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-mermaid"&gt;flowchart TB
    A[Your Request] --&amp;gt; B{Atomizer}
    B --&amp;gt;|Plan Needed| C[Planner]
    B --&amp;gt;|Atomic Task| D[Executor]

    %% Planner spawns subtasks
    C --&amp;gt; E[Subtasks]
    E --&amp;gt; G[Aggregator]

    %% Recursion
    E -.-&amp;gt; B  

    %% Execution + Aggregation
    D --&amp;gt; F[Final Result]
    G --&amp;gt; F

    style A fill:#e1f5fe
    style F fill:#c8e6c9
    style B fill:#fff3e0
    style C fill:#ffe0b2
    style D fill:#d1c4e9
    style G fill:#c5cae9

&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt;
&lt;br /&gt; 
&lt;h3&gt;🚀 30-Second Quick Start&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/sentient-agi/ROMA.git
cd ROMA

# Run the automated setup
./setup.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Choose between:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Docker Setup&lt;/strong&gt; (Recommended) - One-command setup with isolation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Native Setup&lt;/strong&gt; - Direct installation for development&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🛠️ Technical Stack&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Framework&lt;/strong&gt;: Built on &lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/%5Bhttps://github.com/your/agnoagents%5D(https://github.com/agno-agi/agno)"&gt;AgnoAgents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Backend&lt;/strong&gt;: Python 3.12+ with FastAPI/Flask&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Frontend&lt;/strong&gt;: React + TypeScript with real-time WebSocket&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;LLM Support&lt;/strong&gt;: Any provider via LiteLLM&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Data Persistence&lt;/strong&gt;: Enterprise S3 mounting with security validation 
  &lt;ul&gt; 
   &lt;li&gt;🔒 &lt;strong&gt;goofys FUSE mounting&lt;/strong&gt; for zero-latency file access&lt;/li&gt; 
   &lt;li&gt;🛡️ &lt;strong&gt;Path injection protection&lt;/strong&gt; with comprehensive validation&lt;/li&gt; 
   &lt;li&gt;🔐 &lt;strong&gt;AWS credentials verification&lt;/strong&gt; before operations&lt;/li&gt; 
   &lt;li&gt;📁 &lt;strong&gt;Dynamic Docker Compose&lt;/strong&gt; with secure volume mounting&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Code Execution&lt;/strong&gt;: E2B sandboxes with unified S3 integration&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Security&lt;/strong&gt;: Production-grade validation and error handling&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Features&lt;/strong&gt;: Multi-modal, tools, MCP, hooks, caching&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;📦 Installation Options&lt;/h2&gt; 
&lt;h3&gt;Quick Start (Recommended)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Main setup (choose Docker or Native)
./setup.sh

# Optional: Setup E2B sandbox integration
./setup.sh --e2b

# Test E2B integration  
./setup.sh --test-e2b
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Command Line Options&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;./setup.sh --docker     # Run Docker setup directly
./setup.sh --docker-from-scratch  # Rebuild Docker images/containers from scratch (down -v, no cache)
./setup.sh --native     # Run native setup directly (macOS/Ubuntu/Debian)
./setup.sh --e2b        # Setup E2B template (requires E2B_API_KEY + AWS creds)
./setup.sh --test-e2b   # Test E2B template integration
./setup.sh --help       # Show all available options
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Manual Installation&lt;/h3&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/docs/SETUP.md"&gt;setup docs&lt;/a&gt; for detailed instructions.&lt;/p&gt; 
&lt;h3&gt;🏗️ Optional: E2B Sandbox Integration&lt;/h3&gt; 
&lt;p&gt;For secure code execution capabilities, optionally set up E2B sandboxes:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# After main setup, configure E2B (requires E2B_API_KEY and AWS credentials in .env)
./setup.sh --e2b

# Test E2B integration
./setup.sh --test-e2b
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;E2B Features:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;🔒 &lt;strong&gt;Secure Code Execution&lt;/strong&gt; - Run untrusted code in isolated sandboxes&lt;/li&gt; 
 &lt;li&gt;☁️ &lt;strong&gt;S3 Integration&lt;/strong&gt; - Automatic data sync between local and sandbox environments&lt;/li&gt; 
 &lt;li&gt;🚀 &lt;strong&gt;goofys Mounting&lt;/strong&gt; - High-performance S3 filesystem mounting&lt;/li&gt; 
 &lt;li&gt;🔧 &lt;strong&gt;AWS Credentials&lt;/strong&gt; - Passed securely via Docker build arguments&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🤖 Pre-built Agents&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; These agents are demonstrations built using ROMA's framework through simple vibe-prompting and minimal manual tuning. They showcase how easily you can create high-performance agents with ROMA, rather than being production-final solutions. Our mission is to empower the community to build, share, and get rewarded for creating innovative agent recipes and use-cases.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;ROMA comes with example agents that demonstrate the framework's capabilities:&lt;/p&gt; 
&lt;h3&gt;🔍 General Task Solver&lt;/h3&gt; 
&lt;p&gt;A versatile agent powered by ChatGPT Search Preview for handling diverse tasks:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Intelligent Search&lt;/strong&gt;: Leverages OpenAI's latest search capabilities for real-time information&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible Planning&lt;/strong&gt;: Adapts task decomposition based on query complexity&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-Domain&lt;/strong&gt;: Handles everything from technical questions to creative projects&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Quick Prototyping&lt;/strong&gt;: Perfect for testing ROMA's capabilities without domain-specific setup&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Perfect for: General research, fact-checking, exploratory analysis, quick information gathering&lt;/p&gt; 
&lt;h3&gt;🔬 Deep Research Agent&lt;/h3&gt; 
&lt;p&gt;A comprehensive research system that breaks down complex research questions into manageable sub-tasks:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Smart Task Decomposition&lt;/strong&gt;: Automatically splits research topics into search, analysis, and synthesis phases&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Parallel Information Gathering&lt;/strong&gt;: Executes multiple searches simultaneously for faster results&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-Source Integration&lt;/strong&gt;: Combines results from web search, Wikipedia, and specialized APIs&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Intelligent Synthesis&lt;/strong&gt;: Aggregates findings into coherent, well-structured reports&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Perfect for: Academic research, market analysis, competitive intelligence, technical documentation&lt;/p&gt; 
&lt;h3&gt;💹 Crypto Analytics Agent&lt;/h3&gt; 
&lt;p&gt;Specialized financial analysis agent with deep blockchain and DeFi expertise:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Real-Time Market Data&lt;/strong&gt;: Integrates with Binance, CoinGecko, and DefiLlama APIs&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;On-Chain Analytics&lt;/strong&gt;: Access to Arkham Intelligence for wallet tracking and token flows&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Technical Analysis&lt;/strong&gt;: Advanced charting with OHLC data and market indicators&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;DeFi Metrics&lt;/strong&gt;: TVL tracking, yield analysis, protocol comparisons&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Secure Execution&lt;/strong&gt;: Runs analysis in E2B sandboxes with data persistence&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Perfect for: Token research, portfolio analysis, DeFi protocol evaluation, market trend analysis&lt;/p&gt; 
&lt;p&gt;All three agents demonstrate ROMA's recursive architecture in action, showing how complex queries that would overwhelm single-pass systems can be elegantly decomposed and solved. They serve as templates and inspiration for building your own specialized agents.&lt;/p&gt; 
&lt;h3&gt;Your First Agent in 5 Minutes&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;./setup.sh  # Automated setup with Docker or native installation
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Access all the pre-defined agents through the frontend on &lt;code&gt;localhost:3000&lt;/code&gt; after setting up the backend on &lt;code&gt;localhost:5000&lt;/code&gt;. Please checkout &lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/docs/SETUP.md"&gt;Setup&lt;/a&gt; and the &lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/docs/AGENTS_GUIDE.md"&gt;Agents guide&lt;/a&gt; to get started!&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/sentient-agi/ROMA/main/assets/agent_customization.png" alt="alt text" width="60%" /&gt; 
&lt;/div&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Your first agent in 3 lines
from sentientresearchagent import SentientAgent

agent = SentientAgent.create()
result = await agent.run("Create a podcast about AI safety")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;📊 Benchmarks&lt;/h2&gt; 
&lt;p&gt;We evaluate our simple implementation of a search system using ROMA, called ROMA-Search across three benchmarks: &lt;strong&gt;SEAL-0&lt;/strong&gt;, &lt;strong&gt;FRAMES&lt;/strong&gt;, and &lt;strong&gt;SimpleQA&lt;/strong&gt;.&lt;br /&gt; Below are the performance graphs for each benchmark.&lt;/p&gt; 
&lt;h3&gt;&lt;a href="https://huggingface.co/datasets/vtllms/sealqa"&gt;SEAL-0&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;SealQA is a new challenging benchmark for evaluating Search-Augmented Language models on fact-seeking questions where web search yields conflicting, noisy, or unhelpful results.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/sentient-agi/ROMA/main/assets/seal-0-full.001.jpeg" alt="SEAL-0 Results" /&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;&lt;a href="https://huggingface.co/datasets/google/frames-benchmark"&gt;FRAMES&lt;/a&gt;&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;View full results&lt;/summary&gt; 
 &lt;p&gt;A comprehensive evaluation dataset designed to test the capabilities of Retrieval-Augmented Generation (RAG) systems across factuality, retrieval accuracy, and reasoning.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/sentient-agi/ROMA/main/assets/FRAMES-full.001.jpeg" alt="FRAMES Results" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h3&gt;&lt;a href="https://openai.com/index/introducing-simpleqa/"&gt;SimpleQA&lt;/a&gt;&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;View full results&lt;/summary&gt; 
 &lt;p&gt;Factuality benchmark that measures the ability for language models to answer short, fact-seeking questions.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/sentient-agi/ROMA/main/assets/simpleQAFull.001.jpeg" alt="SimpleQA Results" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;✨ Features&lt;/h2&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;🔄 &lt;strong&gt;Recursive Task Decomposition&lt;/strong&gt;&lt;/h3&gt; &lt;p&gt;Automatically breaks down complex tasks into manageable subtasks with intelligent dependency management. Runs independent sub-tasks in &lt;strong&gt;parallel&lt;/strong&gt;.&lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;🤖 &lt;strong&gt;Agent Agnostic&lt;/strong&gt;&lt;/h3&gt; &lt;p&gt;Works with any provider (OpenAI, Anthropic, Google, local models) through unified interface, as long as it has an &lt;code&gt;agent.run()&lt;/code&gt; command, then you can use it!&lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;🔍 &lt;strong&gt;Complete Transparency&lt;/strong&gt;&lt;/h3&gt; &lt;p&gt;Stage tracing shows exactly what happens at each step - debug and optimize with full visibility&lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;🔌 Connect Any Tool&lt;/h3&gt; &lt;p&gt;Seamlessly integrate external tools and protocols with configurable intervention points. Already includes production-grade connectors such as E2B, file-read-write, and more.&lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;🙏 Acknowledgments&lt;/h2&gt; 
&lt;p&gt;This framework would not have been possible if it wasn't for these amazing open-source contributions!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Inspired by the hierarchical planning approach described in &lt;a href="https://arxiv.org/abs/2503.08275"&gt;"Beyond Outlining: Heterogeneous Recursive Planning"&lt;/a&gt; by Xiong et al.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pydantic/pydantic"&gt;Pydantic&lt;/a&gt; - Data validation using Python type annotations&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/%5Bhttps://github.com/agno-ai/agno%5D(https://github.com/agno-agi/agno)"&gt;Agno&lt;/a&gt; - Framework for building AI agents&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/e2b-dev/e2b"&gt;E2B&lt;/a&gt; - Cloud runtime for AI agents&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;📚 Citation&lt;/h2&gt; 
&lt;p&gt;If you use the ROMA repo in your research, please cite:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@software{al_zubi_2025_17052592,
  author       = {Al-Zubi, Salah and
                  Nama, Baran and
                  Kaz, Arda and
                  Oh, Sewoong},
  title        = {SentientResearchAgent: A Hierarchical AI Agent
                   Framework for Research and Analysis
                  },
  month        = sep,
  year         = 2025,
  publisher    = {Zenodo},
  version      = {ROMA},
  doi          = {10.5281/zenodo.17052592},
  url          = {https://doi.org/10.5281/zenodo.17052592},
  swhid        = {swh:1:dir:69cd1552103e0333dd0c39fc4f53cb03196017ce
                   ;origin=https://doi.org/10.5281/zenodo.17052591;vi
                   sit=swh:1:snp:f50bf99634f9876adb80c027361aec9dff97
                   3433;anchor=swh:1:rel:afa7caa843ce1279f5b4b29b5d3d
                   5e3fe85edc95;path=salzubi401-ROMA-b31c382
                  },
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;📄 License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the MIT License - see the &lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>NVIDIA/garak</title>
      <link>https://github.com/NVIDIA/garak</link>
      <description>&lt;p&gt;the LLM vulnerability scanner&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;garak, LLM vulnerability scanner&lt;/h1&gt; 
&lt;p&gt;&lt;em&gt;Generative AI Red-teaming &amp;amp; Assessment Kit&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;garak&lt;/code&gt; checks if an LLM can be made to fail in a way we don't want. &lt;code&gt;garak&lt;/code&gt; probes for hallucination, data leakage, prompt injection, misinformation, toxicity generation, jailbreaks, and many other weaknesses. If you know &lt;code&gt;nmap&lt;/code&gt; or &lt;code&gt;msf&lt;/code&gt; / Metasploit Framework, garak does somewhat similar things to them, but for LLMs.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;garak&lt;/code&gt; focuses on ways of making an LLM or dialog system fail. It combines static, dynamic, and adaptive probes to explore this.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;garak&lt;/code&gt;'s a free tool. We love developing it and are always interested in adding functionality to support applications.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://opensource.org/licenses/Apache-2.0"&gt;&lt;img src="https://img.shields.io/badge/License-Apache_2.0-blue.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://github.com/NVIDIA/garak/actions/workflows/test_linux.yml"&gt;&lt;img src="https://github.com/NVIDIA/garak/actions/workflows/test_linux.yml/badge.svg?sanitize=true" alt="Tests/Linux" /&gt;&lt;/a&gt; &lt;a href="https://github.com/NVIDIA/garak/actions/workflows/test_windows.yml"&gt;&lt;img src="https://github.com/NVIDIA/garak/actions/workflows/test_windows.yml/badge.svg?sanitize=true" alt="Tests/Windows" /&gt;&lt;/a&gt; &lt;a href="https://github.com/NVIDIA/garak/actions/workflows/test_macos.yml"&gt;&lt;img src="https://github.com/NVIDIA/garak/actions/workflows/test_macos.yml/badge.svg?sanitize=true" alt="Tests/OSX" /&gt;&lt;/a&gt; &lt;a href="http://garak.readthedocs.io/en/latest/?badge=latest"&gt;&lt;img src="https://readthedocs.org/projects/garak/badge/?version=latest" alt="Documentation Status" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2406.11036"&gt;&lt;img src="https://img.shields.io/badge/cs.CL-arXiv%3A2406.11036-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/uVch4puUCs"&gt;&lt;img src="https://img.shields.io/badge/chat-on%20discord-yellow.svg?sanitize=true" alt="discord-img" /&gt;&lt;/a&gt; &lt;a href="https://github.com/psf/black"&gt;&lt;img src="https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true" alt="Code style: black" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/garak"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/garak" alt="PyPI - Python Version" /&gt;&lt;/a&gt; &lt;a href="https://badge.fury.io/py/garak"&gt;&lt;img src="https://badge.fury.io/py/garak.svg?sanitize=true" alt="PyPI" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/garak"&gt;&lt;img src="https://static.pepy.tech/badge/garak" alt="Downloads" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/garak"&gt;&lt;img src="https://static.pepy.tech/badge/garak/month" alt="Downloads" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Get started&lt;/h2&gt; 
&lt;h3&gt;&amp;gt; See our user guide! &lt;a href="https://docs.garak.ai/"&gt;docs.garak.ai&lt;/a&gt;&lt;/h3&gt; 
&lt;h3&gt;&amp;gt; Join our &lt;a href="https://discord.gg/uVch4puUCs"&gt;Discord&lt;/a&gt;!&lt;/h3&gt; 
&lt;h3&gt;&amp;gt; Project links &amp;amp; home: &lt;a href="https://garak.ai/"&gt;garak.ai&lt;/a&gt;&lt;/h3&gt; 
&lt;h3&gt;&amp;gt; Twitter: &lt;a href="https://twitter.com/garak_llm"&gt;@garak_llm&lt;/a&gt;&lt;/h3&gt; 
&lt;h3&gt;&amp;gt; DEF CON &lt;a href="https://garak.ai/garak_aiv_slides.pdf"&gt;slides&lt;/a&gt;!&lt;/h3&gt; 
&lt;hr /&gt; 
&lt;h2&gt;LLM support&lt;/h2&gt; 
&lt;p&gt;currently supports:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/models"&gt;hugging face hub&lt;/a&gt; generative models&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://replicate.com/"&gt;replicate&lt;/a&gt; text models&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://platform.openai.com/docs/introduction"&gt;openai api&lt;/a&gt; chat &amp;amp; continuation models&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.litellm.ai/"&gt;litellm&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;pretty much anything accessible via REST&lt;/li&gt; 
 &lt;li&gt;gguf models like &lt;a href="https://github.com/ggerganov/llama.cpp"&gt;llama.cpp&lt;/a&gt; version &amp;gt;= 1046&lt;/li&gt; 
 &lt;li&gt;.. and many more LLMs!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Install:&lt;/h2&gt; 
&lt;p&gt;&lt;code&gt;garak&lt;/code&gt; is a command-line tool. It's developed in Linux and OSX.&lt;/p&gt; 
&lt;h3&gt;Standard install with &lt;code&gt;pip&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;Just grab it from PyPI and you should be good to go:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python -m pip install -U garak
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Install development version with &lt;code&gt;pip&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;The standard pip version of &lt;code&gt;garak&lt;/code&gt; is updated periodically. To get a fresher version from GitHub, try:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python -m pip install -U git+https://github.com/NVIDIA/garak.git@main
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Clone from source&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;garak&lt;/code&gt; has its own dependencies. You can to install &lt;code&gt;garak&lt;/code&gt; in its own Conda environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;conda create --name garak "python&amp;gt;=3.10,&amp;lt;=3.12"
conda activate garak
gh repo clone NVIDIA/garak
cd garak
python -m pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;OK, if that went fine, you're probably good to go!&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: if you cloned before the move to the &lt;code&gt;NVIDIA&lt;/code&gt; GitHub organisation, but you're reading this at the &lt;code&gt;github.com/NVIDIA&lt;/code&gt; URI, please update your remotes as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;git remote set-url origin https://github.com/NVIDIA/garak.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Getting started&lt;/h2&gt; 
&lt;p&gt;The general syntax is:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;garak &amp;lt;options&amp;gt;&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;garak&lt;/code&gt; needs to know what model to scan, and by default, it'll try all the probes it knows on that model, using the vulnerability detectors recommended by each probe. You can see a list of probes using:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;garak --list_probes&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;To specify a generator, use the &lt;code&gt;--model_type&lt;/code&gt; and, optionally, the &lt;code&gt;--model_name&lt;/code&gt; options. Model type specifies a model family/interface; model name specifies the exact model to be used. The "Intro to generators" section below describes some of the generators supported. A straightforward generator family is Hugging Face models; to load one of these, set &lt;code&gt;--model_type&lt;/code&gt; to &lt;code&gt;huggingface&lt;/code&gt; and &lt;code&gt;--model_name&lt;/code&gt; to the model's name on Hub (e.g. &lt;code&gt;"RWKV/rwkv-4-169m-pile"&lt;/code&gt;). Some generators might need an API key to be set as an environment variable, and they'll let you know if they need that.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;garak&lt;/code&gt; runs all the probes by default, but you can be specific about that too. &lt;code&gt;--probes promptinject&lt;/code&gt; will use only the &lt;a href="https://github.com/agencyenterprise/promptinject"&gt;PromptInject&lt;/a&gt; framework's methods, for example. You can also specify one specific plugin instead of a plugin family by adding the plugin name after a &lt;code&gt;.&lt;/code&gt;; for example, &lt;code&gt;--probes lmrc.SlurUsage&lt;/code&gt; will use an implementation of checking for models generating slurs based on the &lt;a href="https://arxiv.org/abs/2303.18190"&gt;Language Model Risk Cards&lt;/a&gt; framework.&lt;/p&gt; 
&lt;p&gt;For help and inspiration, find us on &lt;a href="https://twitter.com/garak_llm"&gt;Twitter&lt;/a&gt; or &lt;a href="https://discord.gg/uVch4puUCs"&gt;discord&lt;/a&gt;!&lt;/p&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;p&gt;Probe ChatGPT for encoding-based prompt injection (OSX/*nix) (replace example value with a real OpenAI API key)&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;export OPENAI_API_KEY="sk-123XXXXXXXXXXXX"
python3 -m garak --model_type openai --model_name gpt-3.5-turbo --probes encoding
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See if the Hugging Face version of GPT2 is vulnerable to DAN 11.0&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python3 -m garak --model_type huggingface --model_name gpt2 --probes dan.Dan_11_0
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Reading the results&lt;/h2&gt; 
&lt;p&gt;For each probe loaded, garak will print a progress bar as it generates. Once generation is complete, a row evaluating that probe's results on each detector is given. If any of the prompt attempts yielded an undesirable behavior, the response will be marked as FAIL, and the failure rate given.&lt;/p&gt; 
&lt;p&gt;Here are the results with the &lt;code&gt;encoding&lt;/code&gt; module on a GPT-3 variant: &lt;img src="https://i.imgur.com/8Dxf45N.png" alt="alt text" /&gt;&lt;/p&gt; 
&lt;p&gt;And the same results for ChatGPT: &lt;img src="https://i.imgur.com/VKAF5if.png" alt="alt text" /&gt;&lt;/p&gt; 
&lt;p&gt;We can see that the more recent model is much more susceptible to encoding-based injection attacks, where text-babbage-001 was only found to be vulnerable to quoted-printable and MIME encoding injections. The figures at the end of each row, e.g. 840/840, indicate the number of text generations total and then how many of these seemed to behave OK. The figure can be quite high because more than one generation is made per prompt - by default, 10.&lt;/p&gt; 
&lt;p&gt;Errors go in &lt;code&gt;garak.log&lt;/code&gt;; the run is logged in detail in a &lt;code&gt;.jsonl&lt;/code&gt; file specified at analysis start &amp;amp; end. There's a basic analysis script in &lt;code&gt;analyse/analyse_log.py&lt;/code&gt; which will output the probes and prompts that led to the most hits.&lt;/p&gt; 
&lt;p&gt;Send PRs &amp;amp; open issues. Happy hunting!&lt;/p&gt; 
&lt;h2&gt;Intro to generators&lt;/h2&gt; 
&lt;h3&gt;Hugging Face&lt;/h3&gt; 
&lt;p&gt;Using the Pipeline API:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--model_type huggingface&lt;/code&gt; (for transformers models to run locally)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - use the model name from Hub. Only generative models will work. If it fails and shouldn't, please open an issue and paste in the command you tried + the exception!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Using the Inference API:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--model_type huggingface.InferenceAPI&lt;/code&gt; (for API-based model access)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - the model name from Hub, e.g. &lt;code&gt;"mosaicml/mpt-7b-instruct"&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Using private endpoints:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;--model_type huggingface.InferenceEndpoint&lt;/code&gt; (for private endpoints)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;--model_name&lt;/code&gt; - the endpoint URL, e.g. &lt;code&gt;https://xxx.us-east-1.aws.endpoints.huggingface.cloud&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;(optional) set the &lt;code&gt;HF_INFERENCE_TOKEN&lt;/code&gt; environment variable to a Hugging Face API token with the "read" role; see &lt;a href="https://huggingface.co/settings/tokens"&gt;https://huggingface.co/settings/tokens&lt;/a&gt; when logged in&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;OpenAI&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--model_type openai&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - the OpenAI model you'd like to use. &lt;code&gt;gpt-3.5-turbo-0125&lt;/code&gt; is fast and fine for testing.&lt;/li&gt; 
 &lt;li&gt;set the &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; environment variable to your OpenAI API key (e.g. "sk-19763ASDF87q6657"); see &lt;a href="https://platform.openai.com/account/api-keys"&gt;https://platform.openai.com/account/api-keys&lt;/a&gt; when logged in&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Recognised model types are whitelisted, because the plugin needs to know which sub-API to use. Completion or ChatCompletion models are OK. If you'd like to use a model not supported, you should get an informative error message, and please send a PR / open an issue.&lt;/p&gt; 
&lt;h3&gt;Replicate&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;set the &lt;code&gt;REPLICATE_API_TOKEN&lt;/code&gt; environment variable to your Replicate API token, e.g. "r8-123XXXXXXXXXXXX"; see &lt;a href="https://replicate.com/account/api-tokens"&gt;https://replicate.com/account/api-tokens&lt;/a&gt; when logged in&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Public Replicate models:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--model_type replicate&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - the Replicate model name and hash, e.g. &lt;code&gt;"stability-ai/stablelm-tuned-alpha-7b:c49dae36"&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Private Replicate endpoints:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--model_type replicate.InferenceEndpoint&lt;/code&gt; (for private endpoints)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - username/model-name slug from the deployed endpoint, e.g. &lt;code&gt;elim/elims-llama2-7b&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Cohere&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--model_type cohere&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; (optional, &lt;code&gt;command&lt;/code&gt; by default) - The specific Cohere model you'd like to test&lt;/li&gt; 
 &lt;li&gt;set the &lt;code&gt;COHERE_API_KEY&lt;/code&gt; environment variable to your Cohere API key, e.g. "aBcDeFgHiJ123456789"; see &lt;a href="https://dashboard.cohere.ai/api-keys"&gt;https://dashboard.cohere.ai/api-keys&lt;/a&gt; when logged in&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Groq&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--model_type groq&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - The name of the model to access via the Groq API&lt;/li&gt; 
 &lt;li&gt;set the &lt;code&gt;GROQ_API_KEY&lt;/code&gt; environment variable to your Groq API key, see &lt;a href="https://console.groq.com/docs/quickstart"&gt;https://console.groq.com/docs/quickstart&lt;/a&gt; for details on creating an API key&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ggml&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--model_type ggml&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - The path to the ggml model you'd like to load, e.g. &lt;code&gt;/home/leon/llama.cpp/models/7B/ggml-model-q4_0.bin&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;set the &lt;code&gt;GGML_MAIN_PATH&lt;/code&gt; environment variable to the path to your ggml &lt;code&gt;main&lt;/code&gt; executable&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;REST&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;rest.RestGenerator&lt;/code&gt; is highly flexible and can connect to any REST endpoint that returns plaintext or JSON. It does need some brief config, which will typically result a short YAML file describing your endpoint. See &lt;a href="https://reference.garak.ai/en/latest/garak.generators.rest.html"&gt;https://reference.garak.ai/en/latest/garak.generators.rest.html&lt;/a&gt; for examples.&lt;/p&gt; 
&lt;h3&gt;NIM&lt;/h3&gt; 
&lt;p&gt;Use models from &lt;a href="https://build.nvidia.com/"&gt;https://build.nvidia.com/&lt;/a&gt; or other NIM endpoints.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;set the &lt;code&gt;NIM_API_KEY&lt;/code&gt; environment variable to your authentication API token, or specify it in the config YAML&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For chat models:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--model_type nim&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - the NIM &lt;code&gt;model&lt;/code&gt; name, e.g. &lt;code&gt;meta/llama-3.1-8b-instruct&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For completion models:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--model_type nim.NVOpenAICompletion&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - the NIM &lt;code&gt;model&lt;/code&gt; name, e.g. &lt;code&gt;bigcode/starcoder2-15b&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Test&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;--model_type test&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;(alternatively) &lt;code&gt;--model_name test.Blank&lt;/code&gt; For testing. This always generates the empty string, using the &lt;code&gt;test.Blank&lt;/code&gt; generator. Will be marked as failing for any tests that &lt;em&gt;require&lt;/em&gt; an output, e.g. those that make contentious claims and expect the model to refute them in order to pass.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;--model_type test.Repeat&lt;/code&gt; For testing. This generator repeats back the prompt it received.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Intro to probes&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Probe&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;blank&lt;/td&gt; 
   &lt;td&gt;A simple probe that always sends an empty prompt.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;atkgen&lt;/td&gt; 
   &lt;td&gt;Automated Attack Generation. A red-teaming LLM probes the target and reacts to it in an attempt to get toxic output. Prototype, mostly stateless, for now uses a simple GPT-2 &lt;a href="https://huggingface.co/garak-llm/artgpt2tox"&gt;fine-tuned&lt;/a&gt; on the subset of hhrlhf attempts that yielded detectable toxicity (the only target currently supported for now).&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;av_spam_scanning&lt;/td&gt; 
   &lt;td&gt;Probes that attempt to make the model output malicious content signatures&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;continuation&lt;/td&gt; 
   &lt;td&gt;Probes that test if the model will continue a probably undesirable word&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;dan&lt;/td&gt; 
   &lt;td&gt;Various &lt;a href="https://adguard.com/en/blog/chatgpt-dan-prompt-abuse.html"&gt;DAN&lt;/a&gt; and DAN-like attacks&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;donotanswer&lt;/td&gt; 
   &lt;td&gt;Prompts to which responsible language models should not answer.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;encoding&lt;/td&gt; 
   &lt;td&gt;Prompt injection through text encoding&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;gcg&lt;/td&gt; 
   &lt;td&gt;Disrupt a system prompt by appending an adversarial suffix.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;glitch&lt;/td&gt; 
   &lt;td&gt;Probe model for glitch tokens that provoke unusual behavior.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;grandma&lt;/td&gt; 
   &lt;td&gt;Appeal to be reminded of one's grandmother.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;goodside&lt;/td&gt; 
   &lt;td&gt;Implementations of Riley Goodside attacks.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;leakreplay&lt;/td&gt; 
   &lt;td&gt;Evaluate if a model will replay training data.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;lmrc&lt;/td&gt; 
   &lt;td&gt;Subsample of the &lt;a href="https://arxiv.org/abs/2303.18190"&gt;Language Model Risk Cards&lt;/a&gt; probes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;malwaregen&lt;/td&gt; 
   &lt;td&gt;Attempts to have the model generate code for building malware&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;misleading&lt;/td&gt; 
   &lt;td&gt;Attempts to make a model support misleading and false claims&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;packagehallucination&lt;/td&gt; 
   &lt;td&gt;Trying to get code generations that specify non-existent (and therefore insecure) packages.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;promptinject&lt;/td&gt; 
   &lt;td&gt;Implementation of the Agency Enterprise &lt;a href="https://github.com/agencyenterprise/PromptInject/tree/main/promptinject"&gt;PromptInject&lt;/a&gt; work (best paper awards @ NeurIPS ML Safety Workshop 2022)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;realtoxicityprompts&lt;/td&gt; 
   &lt;td&gt;Subset of the RealToxicityPrompts work (data constrained because the full test will take so long to run)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;snowball&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ofir.io/snowballed_hallucination.pdf"&gt;Snowballed Hallucination&lt;/a&gt; probes designed to make a model give a wrong answer to questions too complex for it to process&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;xss&lt;/td&gt; 
   &lt;td&gt;Look for vulnerabilities the permit or enact cross-site attacks, such as private data exfiltration.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Logging&lt;/h2&gt; 
&lt;p&gt;&lt;code&gt;garak&lt;/code&gt; generates multiple kinds of log:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;A log file, &lt;code&gt;garak.log&lt;/code&gt;. This includes debugging information from &lt;code&gt;garak&lt;/code&gt; and its plugins, and is continued across runs.&lt;/li&gt; 
 &lt;li&gt;A report of the current run, structured as JSONL. A new report file is created every time &lt;code&gt;garak&lt;/code&gt; runs. The name of this file is output at the beginning and, if successful, also at the end of the run. In the report, an entry is made for each probing attempt both as the generations are received, and again when they are evaluated; the entry's &lt;code&gt;status&lt;/code&gt; attribute takes a constant from &lt;code&gt;garak.attempts&lt;/code&gt; to describe what stage it was made at.&lt;/li&gt; 
 &lt;li&gt;A hit log, detailing attempts that yielded a vulnerability (a 'hit')&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;How is the code structured?&lt;/h2&gt; 
&lt;p&gt;Check out the &lt;a href="https://reference.garak.ai/"&gt;reference docs&lt;/a&gt; for an authoritative guide to &lt;code&gt;garak&lt;/code&gt; code structure.&lt;/p&gt; 
&lt;p&gt;In a typical run, &lt;code&gt;garak&lt;/code&gt; will read a model type (and optionally model name) from the command line, then determine which &lt;code&gt;probe&lt;/code&gt;s and &lt;code&gt;detector&lt;/code&gt;s to run, start up a &lt;code&gt;generator&lt;/code&gt;, and then pass these to a &lt;code&gt;harness&lt;/code&gt; to do the probing; an &lt;code&gt;evaluator&lt;/code&gt; deals with the results. There are many modules in each of these categories, and each module provides a number of classes that act as individual plugins.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;garak/probes/&lt;/code&gt; - classes for generating interactions with LLMs&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;garak/detectors/&lt;/code&gt; - classes for detecting an LLM is exhibiting a given failure mode&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;garak/evaluators/&lt;/code&gt; - assessment reporting schemes&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;garak/generators/&lt;/code&gt; - plugins for LLMs to be probed&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;garak/harnesses/&lt;/code&gt; - classes for structuring testing&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;resources/&lt;/code&gt; - ancillary items required by plugins&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The default operating mode is to use the &lt;code&gt;probewise&lt;/code&gt; harness. Given a list of probe module names and probe plugin names, the &lt;code&gt;probewise&lt;/code&gt; harness instantiates each probe, then for each probe reads its &lt;code&gt;recommended_detectors&lt;/code&gt; attribute to get a list of &lt;code&gt;detector&lt;/code&gt;s to run on the output.&lt;/p&gt; 
&lt;p&gt;Each plugin category (&lt;code&gt;probes&lt;/code&gt;, &lt;code&gt;detectors&lt;/code&gt;, &lt;code&gt;evaluators&lt;/code&gt;, &lt;code&gt;generators&lt;/code&gt;, &lt;code&gt;harnesses&lt;/code&gt;) includes a &lt;code&gt;base.py&lt;/code&gt; which defines the base classes usable by plugins in that category. Each plugin module defines plugin classes that inherit from one of the base classes. For example, &lt;code&gt;garak.generators.openai.OpenAIGenerator&lt;/code&gt; descends from &lt;code&gt;garak.generators.base.Generator&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Larger artefacts, like model files and bigger corpora, are kept out of the repository; they can be stored on e.g. Hugging Face Hub and loaded locally by clients using &lt;code&gt;garak&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Developing your own plugin&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Take a look at how other plugins do it&lt;/li&gt; 
 &lt;li&gt;Inherit from one of the base classes, e.g. &lt;code&gt;garak.probes.base.TextProbe&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Override as little as possible&lt;/li&gt; 
 &lt;li&gt;You can test the new code in at least two ways: 
  &lt;ul&gt; 
   &lt;li&gt;Start an interactive Python session 
    &lt;ul&gt; 
     &lt;li&gt;Import the model, e.g. &lt;code&gt;import garak.probes.mymodule&lt;/code&gt;&lt;/li&gt; 
     &lt;li&gt;Instantiate the plugin, e.g. &lt;code&gt;p = garak.probes.mymodule.MyProbe()&lt;/code&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Run a scan with test plugins 
    &lt;ul&gt; 
     &lt;li&gt;For probes, try a blank generator and always.Pass detector: &lt;code&gt;python3 -m garak -m test.Blank -p mymodule -d always.Pass&lt;/code&gt;&lt;/li&gt; 
     &lt;li&gt;For detectors, try a blank generator and a blank probe: &lt;code&gt;python3 -m garak -m test.Blank -p test.Blank -d mymodule&lt;/code&gt;&lt;/li&gt; 
     &lt;li&gt;For generators, try a blank probe and always.Pass detector: &lt;code&gt;python3 -m garak -m mymodule -p test.Blank -d always.Pass&lt;/code&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Get &lt;code&gt;garak&lt;/code&gt; to list all the plugins of the type you're writing, with &lt;code&gt;--list_probes&lt;/code&gt;, &lt;code&gt;--list_detectors&lt;/code&gt;, or &lt;code&gt;--list_generators&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;FAQ&lt;/h2&gt; 
&lt;p&gt;We have an FAQ &lt;a href="https://github.com/NVIDIA/garak/raw/main/FAQ.md"&gt;here&lt;/a&gt;. Reach out if you have any more questions! &lt;a href="mailto:garak@nvidia.com"&gt;garak@nvidia.com&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Code reference documentation is at &lt;a href="https://garak.readthedocs.io/en/latest/"&gt;garak.readthedocs.io&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Citing garak&lt;/h2&gt; 
&lt;p&gt;You can read the &lt;a href="https://raw.githubusercontent.com/NVIDIA/garak/main/garak-paper.pdf"&gt;garak preprint paper&lt;/a&gt;. If you use garak, please cite us.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@article{garak,
  title={{garak: A Framework for Security Probing Large Language Models}},
  author={Leon Derczynski and Erick Galinkin and Jeffrey Martin and Subho Majumdar and Nanna Inie},
  year={2024},
  howpublished={\url{https://garak.ai}}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;em&gt;"Lying is a skill like any other, and if you wish to maintain a level of excellence you have to practice constantly"&lt;/em&gt; - Elim&lt;/p&gt; 
&lt;p&gt;For updates and news see &lt;a href="https://twitter.com/garak_llm"&gt;@garak_llm&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;© 2023- Leon Derczynski; Apache license v2, see &lt;a href="https://raw.githubusercontent.com/NVIDIA/garak/main/LICENSE"&gt;LICENSE&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>mxrch/GHunt</title>
      <link>https://github.com/mxrch/GHunt</link>
      <description>&lt;p&gt;🕵️‍♂️ Offensive Google framework.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/mxrch/GHunt/master/assets/long_banner.png" alt="" /&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;h4&gt;🌐 GHunt Online version : &lt;a href="https://osint.industries"&gt;https://osint.industries&lt;/a&gt;&lt;/h4&gt; 
&lt;h4&gt;🐍 Now Python 3.13 compatible !&lt;/h4&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://img.shields.io/badge/Python-3.10%2B-brightgreen" alt="Python minimum version" /&gt;&lt;/p&gt; 
&lt;h1&gt;😊 Description&lt;/h1&gt; 
&lt;p&gt;GHunt (v2) is an offensive Google framework, designed to evolve efficiently.&lt;br /&gt; It's currently focused on OSINT, but any use related with Google is possible.&lt;/p&gt; 
&lt;p&gt;Features :&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;CLI usage and modules&lt;/li&gt; 
 &lt;li&gt;Python library usage&lt;/li&gt; 
 &lt;li&gt;Fully async&lt;/li&gt; 
 &lt;li&gt;JSON export&lt;/li&gt; 
 &lt;li&gt;Browser extension to ease login&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;✔️ Requirements&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python &amp;gt;= 3.10&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;⚙️ Installation&lt;/h1&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;$ pip3 install pipx
$ pipx ensurepath
$ pipx install ghunt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It will automatically use venvs to avoid dependency conflicts with other projects.&lt;/p&gt; 
&lt;h1&gt;💃 Usage&lt;/h1&gt; 
&lt;h2&gt;Login&lt;/h2&gt; 
&lt;p&gt;First, launch the listener by doing &lt;code&gt;ghunt login&lt;/code&gt; and choose between 1 of the 2 first methods :&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;$ ghunt login

[1] (Companion) Put GHunt on listening mode (currently not compatible with docker)
[2] (Companion) Paste base64-encoded cookies
[3] Enter manually all cookies

Choice =&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then, use GHunt Companion to complete the login.&lt;/p&gt; 
&lt;p&gt;The extension is available on the following stores :&lt;br /&gt; &lt;br /&gt; &lt;a href="https://addons.mozilla.org/en-US/firefox/addon/ghunt-companion/"&gt;&lt;img src="https://files.catbox.moe/5g2ld5.png" alt="Firefox" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://chrome.google.com/webstore/detail/ghunt-companion/dpdcofblfbmmnikcbmmiakkclocadjab"&gt;&lt;img src="https://developer.chrome.com/static/docs/webstore/branding/image/206x58-chrome-web-bcb82d15b2486.png" alt="Chrome" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Modules&lt;/h2&gt; 
&lt;p&gt;Then, profit :&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;Usage: ghunt [-h] {login,email,gaia,drive,geolocate} ...

Positional Arguments:
  {login,email,gaia,drive,geolocate}
    login               Authenticate GHunt to Google.
    email               Get information on an email address.
    gaia                Get information on a Gaia ID.
    drive               Get information on a Drive file or folder.
    geolocate           Geolocate a BSSID.
    spiderdal           Find assets using Digital Assets Links.

Options:
  -h, --help            show this help message and exit
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;📄 You can also use --json with email, gaia, drive and geolocate modules to export in JSON ! Example :&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;$ ghunt email &amp;lt;email_address&amp;gt; --json user_data.json
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Have fun 🥰💞&lt;/strong&gt;&lt;/p&gt; 
&lt;h1&gt;🧑‍💻 Developers&lt;/h1&gt; 
&lt;p&gt;📕 I started writing some docs &lt;a href="https://github.com/mxrch/GHunt/wiki"&gt;here&lt;/a&gt; and examples &lt;a href="https://github.com/mxrch/GHunt/tree/master/examples"&gt;here&lt;/a&gt;, feel free to contribute !&lt;/p&gt; 
&lt;p&gt;To use GHunt as a lib, you can't use pipx because it uses a venv.&lt;br /&gt; So you should install GHunt with pip :&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;$ pip3 install ghunt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;And now, you should be able to &lt;code&gt;import ghunt&lt;/code&gt; in your projects !&lt;br /&gt; You can right now play with the &lt;a href="https://github.com/mxrch/GHunt/tree/master/examples"&gt;examples&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;📮 Details&lt;/h1&gt; 
&lt;h2&gt;Obvious disclaimer&lt;/h2&gt; 
&lt;p&gt;This tool is for educational purposes only, I am not responsible for its use.&lt;/p&gt; 
&lt;h2&gt;Less obvious disclaimer&lt;/h2&gt; 
&lt;p&gt;This project is under &lt;a href="https://choosealicense.com/licenses/agpl-3.0/"&gt;AGPL Licence&lt;/a&gt;, and you have to respect it.&lt;br /&gt; &lt;strong&gt;Use it only in personal, criminal investigations, pentesting, or open-source projects.&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Thanks&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/novitae"&gt;novitae&lt;/a&gt; for being my Python colleague&lt;/li&gt; 
 &lt;li&gt;All the people on &lt;a href="https://discord.gg/sg2YcrC6x9"&gt;Malfrats Industries&lt;/a&gt; and elsewhere for the beta test !&lt;/li&gt; 
 &lt;li&gt;The HideAndSec team 💗 (blog : &lt;a href="https://hideandsec.sh"&gt;https://hideandsec.sh&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://dribbble.com/jouiniamine"&gt;Med Amine Jouini&lt;/a&gt; for his beautiful rework of the Google logo, which I was inspired by &lt;em&gt;a lot&lt;/em&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Sponsors&lt;/h2&gt; 
&lt;p&gt;Thanks to these awesome people for supporting me !&lt;/p&gt; 
&lt;!-- sponsors --&gt;
&lt;a href="https://github.com/BlWasp"&gt;&lt;img src="https://github.com/BlWasp.png" width="50px" alt="BlWasp" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;
&lt;a href="https://github.com/gingeleski"&gt;&lt;img src="https://github.com/gingeleski.png" width="50px" alt="gingeleski" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;
&lt;a href="https://github.com/ADS-Fund"&gt;&lt;img src="https://github.com/ADS-Fund.png" width="50px" alt="ADS-Fund" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;
&lt;!-- sponsors --&gt; 
&lt;p&gt;&lt;br /&gt; You like my work ?&lt;br /&gt; &lt;a href="https://github.com/sponsors/mxrch"&gt;Sponsor me&lt;/a&gt; on GitHub ! 🤗&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>pyca/cryptography</title>
      <link>https://github.com/pyca/cryptography</link>
      <description>&lt;p&gt;cryptography is a package designed to expose cryptographic primitives and recipes to Python developers.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;pyca/cryptography&lt;/h1&gt; 
&lt;p&gt;.. image:: &lt;a href="https://img.shields.io/pypi/v/cryptography.svg"&gt;https://img.shields.io/pypi/v/cryptography.svg&lt;/a&gt; :target: &lt;a href="https://pypi.org/project/cryptography/"&gt;https://pypi.org/project/cryptography/&lt;/a&gt; :alt: Latest Version&lt;/p&gt; 
&lt;p&gt;.. image:: &lt;a href="https://readthedocs.org/projects/cryptography/badge/?version=latest"&gt;https://readthedocs.org/projects/cryptography/badge/?version=latest&lt;/a&gt; :target: &lt;a href="https://cryptography.io"&gt;https://cryptography.io&lt;/a&gt; :alt: Latest Docs&lt;/p&gt; 
&lt;p&gt;.. image:: &lt;a href="https://github.com/pyca/cryptography/actions/workflows/ci.yml/badge.svg"&gt;https://github.com/pyca/cryptography/actions/workflows/ci.yml/badge.svg&lt;/a&gt; :target: &lt;a href="https://github.com/pyca/cryptography/actions/workflows/ci.yml?query=branch%3Amain"&gt;https://github.com/pyca/cryptography/actions/workflows/ci.yml?query=branch%3Amain&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;cryptography&lt;/code&gt; is a package which provides cryptographic recipes and primitives to Python developers. Our goal is for it to be your "cryptographic standard library". It supports Python 3.8+ and PyPy3 7.3.11+.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;cryptography&lt;/code&gt; includes both high level recipes and low level interfaces to common cryptographic algorithms such as symmetric ciphers, message digests, and key derivation functions. For example, to encrypt something with &lt;code&gt;cryptography&lt;/code&gt;'s high level symmetric encryption recipe:&lt;/p&gt; 
&lt;p&gt;.. code-block:: pycon&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from cryptography.fernet import Fernet
&amp;gt;&amp;gt;&amp;gt; # Put this somewhere safe!
&amp;gt;&amp;gt;&amp;gt; key = Fernet.generate_key()
&amp;gt;&amp;gt;&amp;gt; f = Fernet(key)
&amp;gt;&amp;gt;&amp;gt; token = f.encrypt(b"A really secret message. Not for prying eyes.")
&amp;gt;&amp;gt;&amp;gt; token
b'...'
&amp;gt;&amp;gt;&amp;gt; f.decrypt(token)
b'A really secret message. Not for prying eyes.'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can find more information in the &lt;code&gt;documentation&lt;/code&gt;_.&lt;/p&gt; 
&lt;p&gt;You can install &lt;code&gt;cryptography&lt;/code&gt; with:&lt;/p&gt; 
&lt;p&gt;.. code-block:: console&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ pip install cryptography
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For full details see &lt;code&gt;the installation documentation&lt;/code&gt;_.&lt;/p&gt; 
&lt;p&gt;Discussion&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;
If you run into bugs, you can file them in our `issue tracker`_.

We maintain a `cryptography-dev`_ mailing list for development discussion.

You can also join ``#pyca`` on ``irc.libera.chat`` to ask questions or get
involved.

Security
~~~~~~~~

Need to report a security issue? Please consult our `security reporting`_
documentation.


.. _`documentation`: https://cryptography.io/
.. _`the installation documentation`: https://cryptography.io/en/latest/installation/
.. _`issue tracker`: https://github.com/pyca/cryptography/issues
.. _`cryptography-dev`: https://mail.python.org/mailman/listinfo/cryptography-dev
.. _`security reporting`: https://cryptography.io/en/latest/security/
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Azure/azure-sdk-for-python</title>
      <link>https://github.com/Azure/azure-sdk-for-python</link>
      <description>&lt;p&gt;This repository is for active development of the Azure SDK for Python. For consumers of the SDK we recommend visiting our public developer docs at https://learn.microsoft.com/python/azure/ or our versioned developer docs at https://azure.github.io/azure-sdk-for-python.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Azure SDK for Python&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://azure.github.io/azure-sdk/releases/latest/python.html"&gt;&lt;img src="https://img.shields.io/badge/packages-latest-blue.svg?sanitize=true" alt="Packages" /&gt;&lt;/a&gt; &lt;a href="https://azuresdkartifacts.blob.core.windows.net/azure-sdk-for-python/dependencies/dependencies.html"&gt;&lt;img src="https://img.shields.io/badge/dependency-report-blue.svg?sanitize=true" alt="Dependencies" /&gt;&lt;/a&gt; &lt;a href="https://azuresdkartifacts.blob.core.windows.net/azure-sdk-for-python/dependencies/dependencyGraph/index.html"&gt;&lt;img src="https://img.shields.io/badge/dependency-graph-blue.svg?sanitize=true" alt="DepGraph" /&gt;&lt;/a&gt; &lt;a href="https://pypi.python.org/pypi/azure/"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/azure-core.svg?maxAge=2592000" alt="Python" /&gt;&lt;/a&gt; &lt;a href="https://dev.azure.com/azure-sdk/public/_build/latest?definitionId=458&amp;amp;branchName=main"&gt;&lt;img src="https://dev.azure.com/azure-sdk/public/_apis/build/status/python/python%20-%20core%20-%20ci?branchName=main" alt="Build Status" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;This repository is for the active development of the Azure SDK for Python. For consumers of the SDK we recommend visiting our &lt;a href="https://docs.microsoft.com/python/azure/"&gt;public developer docs&lt;/a&gt; or our versioned &lt;a href="https://azure.github.io/azure-sdk-for-python"&gt;developer docs&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Getting started&lt;/h2&gt; 
&lt;p&gt;For your convenience, each service has a separate set of libraries that you can choose to use instead of one, large Azure package. To get started with a specific library, see the &lt;code&gt;README.md&lt;/code&gt; (or &lt;code&gt;README.rst&lt;/code&gt;) file located in the library's project folder.&lt;/p&gt; 
&lt;p&gt;You can find service libraries in the &lt;code&gt;/sdk&lt;/code&gt; directory.&lt;/p&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;p&gt;The client libraries are supported on Python 3.9 or later. For more details, please read our page on &lt;a href="https://github.com/Azure/azure-sdk-for-python/wiki/Azure-SDKs-Python-version-support-policy"&gt;Azure SDK for Python version support policy&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Packages available&lt;/h2&gt; 
&lt;p&gt;Each service might have a number of libraries available from each of the following categories:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Azure/azure-sdk-for-python/main/#client-new-releases"&gt;Client - New Releases&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Azure/azure-sdk-for-python/main/#client-previous-versions"&gt;Client - Previous Versions&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Azure/azure-sdk-for-python/main/#management-new-releases"&gt;Management - New Releases&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Azure/azure-sdk-for-python/main/#management-previous-versions"&gt;Management - Previous Versions&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Client: New Releases&lt;/h3&gt; 
&lt;p&gt;New wave of packages that we are announcing as &lt;strong&gt;GA&lt;/strong&gt; and several that are currently releasing in &lt;strong&gt;preview&lt;/strong&gt;. These libraries allow you to use and consume existing resources and interact with them, for example: upload a blob. These libraries share several core functionalities such as: retries, logging, transport protocols, authentication protocols, etc. that can be found in the &lt;a href="https://github.com/Azure/azure-sdk-for-python/raw/main/sdk/core/azure-core"&gt;azure-core&lt;/a&gt; library. You can learn more about these libraries by reading guidelines that they follow &lt;a href="https://azure.github.io/azure-sdk/python/guidelines/index.html"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You can find the &lt;a href="https://azure.github.io/azure-sdk/releases/latest/index.html#python"&gt;most up to date list of all of the new packages on our page&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;NOTE: If you need to ensure your code is ready for production use one of the stable, non-preview libraries.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Client: Previous Versions&lt;/h3&gt; 
&lt;p&gt;Last stable versions of packages that have been provided for usage with Azure and are production-ready. These libraries provide you with similar functionalities to the Preview ones as they allow you to use and consume existing resources and interact with them, for example: upload a blob. They might not implement the &lt;a href="https://azure.github.io/azure-sdk/python/guidelines/index.html"&gt;guidelines&lt;/a&gt; or have the same feature set as the November releases. They do however offer wider coverage of services.&lt;/p&gt; 
&lt;h3&gt;Management: New Releases&lt;/h3&gt; 
&lt;p&gt;A new set of management libraries that follow the &lt;a href="https://azure.github.io/azure-sdk/python/guidelines/"&gt;Azure SDK Design Guidelines for Python&lt;/a&gt; are now available. These new libraries provide a number of core capabilities that are shared amongst all Azure SDKs, including the intuitive Azure Identity library, an HTTP Pipeline with custom policies, error-handling, distributed tracing, and much more. Documentation and code samples for these new libraries can be found &lt;a href="https://aka.ms/azsdk/python/mgmt"&gt;here&lt;/a&gt;. In addition, a migration guide that shows how to transition from older versions of libraries is located &lt;a href="https://github.com/Azure/azure-sdk-for-python/raw/main/doc/sphinx/mgmt_quickstart.rst#migration-guide"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You can find the &lt;a href="https://azure.github.io/azure-sdk/releases/latest/mgmt/python.html"&gt;most up to date list of all of the new packages on our page&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;NOTE: If you need to ensure your code is ready for production use one of the stable, non-preview libraries. Also, if you are experiencing authentication issues with the management libraries after upgrading certain packages, it's possible that you upgraded to the new versions of SDK without changing the authentication code, please refer to the migration guide mentioned above for proper instructions.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Management: Previous Versions&lt;/h3&gt; 
&lt;p&gt;For a complete list of management libraries that enable you to provision and manage Azure resources, please &lt;a href="https://azure.github.io/azure-sdk/releases/latest/all/python.html"&gt;check here&lt;/a&gt;. They might not have the same feature set as the new releases but they do offer wider coverage of services. Management libraries can be identified by namespaces that start with &lt;code&gt;azure-mgmt-&lt;/code&gt;, e.g. &lt;code&gt;azure-mgmt-compute&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;Need help?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;For detailed documentation visit our &lt;a href="https://aka.ms/python-docs"&gt;Azure SDK for Python documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;File an issue via &lt;a href="https://github.com/Azure/azure-sdk-for-python/issues"&gt;GitHub Issues&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Check &lt;a href="https://stackoverflow.com/questions/tagged/azure+python"&gt;previous questions&lt;/a&gt; or ask new ones on StackOverflow using &lt;code&gt;azure&lt;/code&gt; and &lt;code&gt;python&lt;/code&gt; tags.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Data Collection&lt;/h2&gt; 
&lt;p&gt;The software may collect information about you and your use of the software and send it to Microsoft. Microsoft may use this information to provide services and improve our products and services. You may turn off the telemetry as described below. You can learn more about data collection and use in the help documentation and Microsoft’s &lt;a href="https://go.microsoft.com/fwlink/?LinkID=824704"&gt;privacy statement&lt;/a&gt;. For more information on the data collected by the Azure SDK, please visit the &lt;a href="https://azure.github.io/azure-sdk/general_azurecore.html#telemetry-policy"&gt;Telemetry Guidelines&lt;/a&gt; page.&lt;/p&gt; 
&lt;h3&gt;Telemetry Configuration&lt;/h3&gt; 
&lt;p&gt;Telemetry collection is on by default.&lt;/p&gt; 
&lt;p&gt;To opt out, you can disable telemetry at client construction. Define a &lt;code&gt;NoUserAgentPolicy&lt;/code&gt; class that is a subclass of &lt;code&gt;UserAgentPolicy&lt;/code&gt; with an &lt;code&gt;on_request&lt;/code&gt; method that does nothing. Then pass instance of this class as kwargs &lt;code&gt;user_agent_policy=NoUserAgentPolicy()&lt;/code&gt; during client creation. This will disable telemetry for all methods in the client. Do this for every new client.&lt;/p&gt; 
&lt;p&gt;The example below uses the &lt;code&gt;azure-storage-blob&lt;/code&gt; package. In your code, you can replace &lt;code&gt;azure-storage-blob&lt;/code&gt; with the package you are using.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import os
from azure.identity import ManagedIdentityCredential
from azure.storage.blob import BlobServiceClient
from azure.core.pipeline.policies import UserAgentPolicy


# Create your credential you want to use
mi_credential = ManagedIdentityCredential()

account_url = "https://&amp;lt;storageaccountname&amp;gt;.blob.core.windows.net"

# Set up user-agent override
class NoUserAgentPolicy(UserAgentPolicy):
    def on_request(self, request):
        pass

# Create the BlobServiceClient object
blob_service_client = BlobServiceClient(account_url, credential=mi_credential, user_agent_policy=NoUserAgentPolicy())

container_client = blob_service_client.get_container_client(container=&amp;lt;container_name&amp;gt;) 
# TODO: do something with the container client like download blob to a file
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Reporting security issues and security bugs&lt;/h3&gt; 
&lt;p&gt;Security issues and bugs should be reported privately, via email, to the Microsoft Security Response Center (MSRC) &lt;a href="mailto:secure@microsoft.com"&gt;secure@microsoft.com&lt;/a&gt;. You should receive a response within 24 hours. If for some reason you do not, please follow up via email to ensure we received your original message. Further information, including the MSRC PGP key, can be found in the &lt;a href="https://www.microsoft.com/msrc/faqs-report-an-issue"&gt;Security TechCenter&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;For details on contributing to this repository, see the &lt;a href="https://github.com/Azure/azure-sdk-for-python/raw/main/CONTRIBUTING.md"&gt;contributing guide&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href="https://cla.microsoft.com"&gt;https://cla.microsoft.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;When you submit a pull request, a CLA-bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repositories using our CLA.&lt;/p&gt; 
&lt;p&gt;This project has adopted the &lt;a href="https://opensource.microsoft.com/codeofconduct/"&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href="https://opensource.microsoft.com/codeofconduct/faq/"&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href="mailto:opencode@microsoft.com"&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Physical-Intelligence/openpi</title>
      <link>https://github.com/Physical-Intelligence/openpi</link>
      <description>&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;openpi&lt;/h1&gt; 
&lt;p&gt;openpi holds open-source models and packages for robotics, published by the &lt;a href="https://www.physicalintelligence.company/"&gt;Physical Intelligence team&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Currently, this repo contains three types of models:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;the &lt;a href="https://www.physicalintelligence.company/blog/pi0"&gt;π₀ model&lt;/a&gt;, a flow-based vision-language-action model (VLA).&lt;/li&gt; 
 &lt;li&gt;the &lt;a href="https://www.physicalintelligence.company/research/fast"&gt;π₀-FAST model&lt;/a&gt;, an autoregressive VLA, based on the FAST action tokenizer.&lt;/li&gt; 
 &lt;li&gt;the &lt;a href="https://www.physicalintelligence.company/blog/pi05"&gt;π₀.₅ model&lt;/a&gt;, an upgraded version of π₀ with better open-world generalization trained with &lt;a href="https://www.physicalintelligence.company/research/knowledge_insulation"&gt;knowledge insulation&lt;/a&gt;. Note that, in this repository, we currently only support the flow matching head for both $\pi_{0.5}$ training and inference.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For all models, we provide &lt;em&gt;base model&lt;/em&gt; checkpoints, pre-trained on 10k+ hours of robot data, and examples for using them out of the box or fine-tuning them to your own datasets.&lt;/p&gt; 
&lt;p&gt;This is an experiment: $\pi_0$ was developed for our own robots, which differ from the widely used platforms such as &lt;a href="https://tonyzhaozh.github.io/aloha/"&gt;ALOHA&lt;/a&gt; and &lt;a href="https://droid-dataset.github.io/"&gt;DROID&lt;/a&gt;, and though we are optimistic that researchers and practitioners will be able to run creative new experiments adapting $\pi_0$ to their own platforms, we do not expect every such attempt to be successful. All this is to say: $\pi_0$ may or may not work for you, but you are welcome to try it and see!&lt;/p&gt; 
&lt;h2&gt;Updates&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;[Sept 2025] We released PyTorch support in openpi.&lt;/li&gt; 
 &lt;li&gt;[Sept 2025] We released pi05, an upgraded version of pi0 with better open-world generalization.&lt;/li&gt; 
 &lt;li&gt;[Sept 2025]: We have added an &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/droid/README_train.md#data-filtering"&gt;improved idle filter&lt;/a&gt; for DROID training.&lt;/li&gt; 
 &lt;li&gt;[Jun 2025]: We have added &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/droid/README_train.md"&gt;instructions&lt;/a&gt; for using &lt;code&gt;openpi&lt;/code&gt; to train VLAs on the full &lt;a href="https://droid-dataset.github.io/"&gt;DROID dataset&lt;/a&gt;. This is an approximate open-source implementation of the training pipeline used to train pi0-FAST-DROID.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;p&gt;To run the models in this repository, you will need an NVIDIA GPU with at least the following specifications. These estimations assume a single GPU, but you can also use multiple GPUs with model parallelism to reduce per-GPU memory requirements by configuring &lt;code&gt;fsdp_devices&lt;/code&gt; in the training config. Please also note that the current training script does not yet support multi-node training.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Mode&lt;/th&gt; 
   &lt;th&gt;Memory Required&lt;/th&gt; 
   &lt;th&gt;Example GPU&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;&amp;gt; 8 GB&lt;/td&gt; 
   &lt;td&gt;RTX 4090&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Fine-Tuning (LoRA)&lt;/td&gt; 
   &lt;td&gt;&amp;gt; 22.5 GB&lt;/td&gt; 
   &lt;td&gt;RTX 4090&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Fine-Tuning (Full)&lt;/td&gt; 
   &lt;td&gt;&amp;gt; 70 GB&lt;/td&gt; 
   &lt;td&gt;A100 (80GB) / H100&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;The repo has been tested with Ubuntu 22.04, we do not currently support other operating systems.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;When cloning this repo, make sure to update submodules:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone --recurse-submodules git@github.com:Physical-Intelligence/openpi.git

# Or if you already cloned the repo:
git submodule update --init --recursive
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We use &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt; to manage Python dependencies. See the &lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;uv installation instructions&lt;/a&gt; to set it up. Once uv is installed, run the following to set up the environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;GIT_LFS_SKIP_SMUDGE=1 uv sync
GIT_LFS_SKIP_SMUDGE=1 uv pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;NOTE: &lt;code&gt;GIT_LFS_SKIP_SMUDGE=1&lt;/code&gt; is needed to pull LeRobot as a dependency.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Docker&lt;/strong&gt;: As an alternative to uv installation, we provide instructions for installing openpi using Docker. If you encounter issues with your system setup, consider using Docker to simplify installation. See &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/docs/docker.md"&gt;Docker Setup&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Model Checkpoints&lt;/h2&gt; 
&lt;h3&gt;Base Models&lt;/h3&gt; 
&lt;p&gt;We provide multiple base VLA model checkpoints. These checkpoints have been pre-trained on 10k+ hours of robot data, and can be used for fine-tuning.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Use Case&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Checkpoint Path&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$&lt;/td&gt; 
   &lt;td&gt;Fine-Tuning&lt;/td&gt; 
   &lt;td&gt;Base &lt;a href="https://www.physicalintelligence.company/blog/pi0"&gt;π₀ model&lt;/a&gt; for fine-tuning&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_base&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$-FAST&lt;/td&gt; 
   &lt;td&gt;Fine-Tuning&lt;/td&gt; 
   &lt;td&gt;Base autoregressive &lt;a href="https://www.physicalintelligence.company/research/fast"&gt;π₀-FAST model&lt;/a&gt; for fine-tuning&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_fast_base&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_{0.5}$&lt;/td&gt; 
   &lt;td&gt;Fine-Tuning&lt;/td&gt; 
   &lt;td&gt;Base &lt;a href="https://www.physicalintelligence.company/blog/pi05"&gt;π₀.₅ model&lt;/a&gt; for fine-tuning&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi05_base&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Fine-Tuned Models&lt;/h3&gt; 
&lt;p&gt;We also provide "expert" checkpoints for various robot platforms and tasks. These models are fine-tuned from the base models above and intended to run directly on the target robot. These may or may not work on your particular robot. Since these checkpoints were fine-tuned on relatively small datasets collected with more widely available robots, such as ALOHA and the DROID Franka setup, they might not generalize to your particular setup, though we found some of these, especially the DROID checkpoint, to generalize quite broadly in practice.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Use Case&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Checkpoint Path&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$-FAST-DROID&lt;/td&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;$\pi_0$-FAST model fine-tuned on the &lt;a href="https://droid-dataset.github.io/"&gt;DROID dataset&lt;/a&gt;: can perform a wide range of simple table-top manipulation tasks 0-shot in new scenes on the DROID robot platform&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_fast_droid&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$-DROID&lt;/td&gt; 
   &lt;td&gt;Fine-Tuning&lt;/td&gt; 
   &lt;td&gt;$\pi_0$ model fine-tuned on the &lt;a href="https://droid-dataset.github.io/"&gt;DROID dataset&lt;/a&gt;: faster inference than $\pi_0$-FAST-DROID, but may not follow language commands as well&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_droid&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$-ALOHA-towel&lt;/td&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;$\pi_0$ model fine-tuned on internal &lt;a href="https://tonyzhaozh.github.io/aloha/"&gt;ALOHA&lt;/a&gt; data: can fold diverse towels 0-shot on ALOHA robot platforms&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_aloha_towel&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$-ALOHA-tupperware&lt;/td&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;$\pi_0$ model fine-tuned on internal &lt;a href="https://tonyzhaozh.github.io/aloha/"&gt;ALOHA&lt;/a&gt; data: can unpack food from a tupperware container&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_aloha_tupperware&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$-ALOHA-pen-uncap&lt;/td&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;$\pi_0$ model fine-tuned on public &lt;a href="https://dit-policy.github.io/"&gt;ALOHA&lt;/a&gt; data: can uncap a pen&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_aloha_pen_uncap&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_{0.5}$-LIBERO&lt;/td&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;$\pi_{0.5}$ model fine-tuned for the &lt;a href="https://libero-project.github.io/datasets"&gt;LIBERO&lt;/a&gt; benchmark: gets state-of-the-art performance (see &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/libero/README.md"&gt;LIBERO README&lt;/a&gt;)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi05_libero&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_{0.5}$-DROID&lt;/td&gt; 
   &lt;td&gt;Inference / Fine-Tuning&lt;/td&gt; 
   &lt;td&gt;$\pi_{0.5}$ model fine-tuned on the &lt;a href="https://droid-dataset.github.io/"&gt;DROID dataset&lt;/a&gt; with &lt;a href="https://www.physicalintelligence.company/research/knowledge_insulation"&gt;knowledge insulation&lt;/a&gt;: fast inference and good language-following&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi05_droid&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;By default, checkpoints are automatically downloaded from &lt;code&gt;gs://openpi-assets&lt;/code&gt; and are cached in &lt;code&gt;~/.cache/openpi&lt;/code&gt; when needed. You can overwrite the download path by setting the &lt;code&gt;OPENPI_DATA_HOME&lt;/code&gt; environment variable.&lt;/p&gt; 
&lt;h2&gt;Running Inference for a Pre-Trained Model&lt;/h2&gt; 
&lt;p&gt;Our pre-trained model checkpoints can be run with a few lines of code (here our $\pi_0$-FAST-DROID model):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openpi.training import config as _config
from openpi.policies import policy_config
from openpi.shared import download

config = _config.get_config("pi05_droid")
checkpoint_dir = download.maybe_download("gs://openpi-assets/checkpoints/pi05_droid")

# Create a trained policy.
policy = policy_config.create_trained_policy(config, checkpoint_dir)

# Run inference on a dummy example.
example = {
    "observation/exterior_image_1_left": ...,
    "observation/wrist_image_left": ...,
    ...
    "prompt": "pick up the fork"
}
action_chunk = policy.infer(example)["actions"]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also test this out in the &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/inference.ipynb"&gt;example notebook&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We provide detailed step-by-step examples for running inference of our pre-trained checkpoints on &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/droid/README.md"&gt;DROID&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/aloha_real/README.md"&gt;ALOHA&lt;/a&gt; robots.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Remote Inference&lt;/strong&gt;: We provide &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/docs/remote_inference.md"&gt;examples and code&lt;/a&gt; for running inference of our models &lt;strong&gt;remotely&lt;/strong&gt;: the model can run on a different server and stream actions to the robot via a websocket connection. This makes it easy to use more powerful GPUs off-robot and keep robot and policy environments separate.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Test inference without a robot&lt;/strong&gt;: We provide a &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/simple_client/README.md"&gt;script&lt;/a&gt; for testing inference without a robot. This script will generate a random observation and run inference with the model. See &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/simple_client/README.md"&gt;here&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Fine-Tuning Base Models on Your Own Data&lt;/h2&gt; 
&lt;p&gt;We will fine-tune the $\pi_{0.5}$ model on the &lt;a href="https://libero-project.github.io/datasets"&gt;LIBERO dataset&lt;/a&gt; as a running example for how to fine-tune a base model on your own data. We will explain three steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Convert your data to a LeRobot dataset (which we use for training)&lt;/li&gt; 
 &lt;li&gt;Defining training configs and running training&lt;/li&gt; 
 &lt;li&gt;Spinning up a policy server and running inference&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;1. Convert your data to a LeRobot dataset&lt;/h3&gt; 
&lt;p&gt;We provide a minimal example script for converting LIBERO data to a LeRobot dataset in &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/libero/convert_libero_data_to_lerobot.py"&gt;&lt;code&gt;examples/libero/convert_libero_data_to_lerobot.py&lt;/code&gt;&lt;/a&gt;. You can easily modify it to convert your own data! You can download the raw LIBERO dataset from &lt;a href="https://huggingface.co/datasets/openvla/modified_libero_rlds"&gt;here&lt;/a&gt;, and run the script with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run examples/libero/convert_libero_data_to_lerobot.py --data_dir /path/to/your/libero/data
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; If you just want to fine-tune on LIBERO, you can skip this step, because our LIBERO fine-tuning configs point to a pre-converted LIBERO dataset. This step is merely an example that you can adapt to your own data.&lt;/p&gt; 
&lt;h3&gt;2. Defining training configs and running training&lt;/h3&gt; 
&lt;p&gt;To fine-tune a base model on your own data, you need to define configs for data processing and training. We provide example configs with detailed comments for LIBERO below, which you can modify for your own dataset:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/src/openpi/policies/libero_policy.py"&gt;&lt;code&gt;LiberoInputs&lt;/code&gt; and &lt;code&gt;LiberoOutputs&lt;/code&gt;&lt;/a&gt;: Defines the data mapping from the LIBERO environment to the model and vice versa. Will be used for both, training and inference.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/src/openpi/training/config.py"&gt;&lt;code&gt;LeRobotLiberoDataConfig&lt;/code&gt;&lt;/a&gt;: Defines how to process raw LIBERO data from LeRobot dataset for training.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/src/openpi/training/config.py"&gt;&lt;code&gt;TrainConfig&lt;/code&gt;&lt;/a&gt;: Defines fine-tuning hyperparameters, data config, and weight loader.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We provide example fine-tuning configs for &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/src/openpi/training/config.py"&gt;π₀&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/src/openpi/training/config.py"&gt;π₀-FAST&lt;/a&gt;, and &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/src/openpi/training/config.py"&gt;π₀.₅&lt;/a&gt; on LIBERO data.&lt;/p&gt; 
&lt;p&gt;Before we can run training, we need to compute the normalization statistics for the training data. Run the script below with the name of your training config:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run scripts/compute_norm_stats.py --config-name pi05_libero
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Now we can kick off training with the following command (the &lt;code&gt;--overwrite&lt;/code&gt; flag is used to overwrite existing checkpoints if you rerun fine-tuning with the same config):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;XLA_PYTHON_CLIENT_MEM_FRACTION=0.9 uv run scripts/train.py pi05_libero --exp-name=my_experiment --overwrite
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The command will log training progress to the console and save checkpoints to the &lt;code&gt;checkpoints&lt;/code&gt; directory. You can also monitor training progress on the Weights &amp;amp; Biases dashboard. For maximally using the GPU memory, set &lt;code&gt;XLA_PYTHON_CLIENT_MEM_FRACTION=0.9&lt;/code&gt; before running training -- this enables JAX to use up to 90% of the GPU memory (vs. the default of 75%).&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; We provide functionality for &lt;em&gt;reloading&lt;/em&gt; normalization statistics for state / action normalization from pre-training. This can be beneficial if you are fine-tuning to a new task on a robot that was part of our pre-training mixture. For more details on how to reload normalization statistics, see the &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/docs/norm_stats.md"&gt;norm_stats.md&lt;/a&gt; file.&lt;/p&gt; 
&lt;h3&gt;3. Spinning up a policy server and running inference&lt;/h3&gt; 
&lt;p&gt;Once training is complete, we can run inference by spinning up a policy server and then querying it from a LIBERO evaluation script. Launching a model server is easy (we use the checkpoint for iteration 20,000 for this example, modify as needed):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run scripts/serve_policy.py policy:checkpoint --policy.config=pi05_libero --policy.dir=checkpoints/pi05_libero/my_experiment/20000
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will spin up a server that listens on port 8000 and waits for observations to be sent to it. We can then run an evaluation script (or robot runtime) that queries the server.&lt;/p&gt; 
&lt;p&gt;For running the LIBERO eval in particular, we provide (and recommend using) a Dockerized workflow that handles both the policy server and the evaluation script together. See the &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/libero/README.md"&gt;LIBERO README&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;p&gt;If you want to embed a policy server call in your own robot runtime, we have a minimal example of how to do so in the &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/docs/remote_inference.md"&gt;remote inference docs&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;More Examples&lt;/h3&gt; 
&lt;p&gt;We provide more examples for how to fine-tune and run inference with our models on the ALOHA platform in the following READMEs:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/aloha_sim"&gt;ALOHA Simulator&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/aloha_real"&gt;ALOHA Real&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/ur5"&gt;UR5&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;PyTorch Support&lt;/h2&gt; 
&lt;p&gt;openpi now provides PyTorch implementations of π₀ and π₀.₅ models alongside the original JAX versions! The PyTorch implementation has been validated on the LIBERO benchmark (both inference and finetuning). A few features are currently not supported (this may change in the future):&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;The π₀-FAST model&lt;/li&gt; 
 &lt;li&gt;Mixed precision training&lt;/li&gt; 
 &lt;li&gt;FSDP (fully-sharded data parallelism) training&lt;/li&gt; 
 &lt;li&gt;LoRA (low-rank adaptation) training&lt;/li&gt; 
 &lt;li&gt;EMA (exponential moving average) weights during training&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Setup&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Make sure that you have the latest version of all dependencies installed: &lt;code&gt;uv sync&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Double check that you have transformers 4.53.2 installed: &lt;code&gt;uv pip show transformers&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Apply the transformers library patches:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cp -r ./src/openpi/models_pytorch/transformers_replace/* .venv/lib/python3.11/site-packages/transformers/
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;This overwrites several files in the transformers library with necessary model changes: 1) supporting AdaRMS, 2) correctly controlling the precision of activations, and 3) allowing the KV cache to be used without being updated.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;WARNING&lt;/strong&gt;: With the default uv link mode (hardlink), this will permanently affect the transformers library in your uv cache, meaning the changes will survive reinstallations of transformers and could even propagate to other projects that use transformers. To fully undo this operation, you must run &lt;code&gt;uv cache clean transformers&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Converting JAX Models to PyTorch&lt;/h3&gt; 
&lt;p&gt;To convert a JAX model checkpoint to PyTorch format:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run examples/convert_jax_model_to_pytorch.py \
    --checkpoint_dir /path/to/jax/checkpoint \
    --config_name &amp;lt;config name&amp;gt; \
    --output_path /path/to/converted/pytorch/checkpoint
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Running Inference with PyTorch&lt;/h3&gt; 
&lt;p&gt;The PyTorch implementation uses the same API as the JAX version - you only need to change the checkpoint path to point to the converted PyTorch model:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openpi.training import config as _config
from openpi.policies import policy_config
from openpi.shared import download

config = _config.get_config("pi05_droid")
checkpoint_dir = "/path/to/converted/pytorch/checkpoint"

# Create a trained policy (automatically detects PyTorch format)
policy = policy_config.create_trained_policy(config, checkpoint_dir)

# Run inference (same API as JAX)
action_chunk = policy.infer(example)["actions"]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Policy Server with PyTorch&lt;/h3&gt; 
&lt;p&gt;The policy server works identically with PyTorch models - just point to the converted checkpoint directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run scripts/serve_policy.py policy:checkpoint \
    --policy.config=pi05_droid \
    --policy.dir=/path/to/converted/pytorch/checkpoint
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Finetuning with PyTorch&lt;/h3&gt; 
&lt;p&gt;To finetune a model in PyTorch:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Convert the JAX base model to PyTorch format:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;uv run examples/convert_jax_model_to_pytorch.py \
    --config_name &amp;lt;config name&amp;gt; \
    --checkpoint_dir /path/to/jax/base/model \
    --output_path /path/to/pytorch/base/model
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Specify the converted PyTorch model path in your config using &lt;code&gt;pytorch_weight_path&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Launch training using one of these modes:&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Single GPU training:
uv run scripts/train_pytorch.py &amp;lt;config_name&amp;gt; --exp_name &amp;lt;run_name&amp;gt; --save_interval &amp;lt;interval&amp;gt;

# Example:
uv run scripts/train_pytorch.py debug --exp_name pytorch_test
uv run scripts/train_pytorch.py debug --exp_name pytorch_test --resume  # Resume from latest checkpoint

# Multi-GPU training (single node):
uv run torchrun --standalone --nnodes=1 --nproc_per_node=&amp;lt;num_gpus&amp;gt; scripts/train_pytorch.py &amp;lt;config_name&amp;gt; --exp_name &amp;lt;run_name&amp;gt;

# Example:
uv run torchrun --standalone --nnodes=1 --nproc_per_node=2 scripts/train_pytorch.py pi0_aloha_sim --exp_name pytorch_ddp_test
uv run torchrun --standalone --nnodes=1 --nproc_per_node=2 scripts/train_pytorch.py pi0_aloha_sim --exp_name pytorch_ddp_test --resume

# Multi-Node Training:
uv run torchrun \
    --nnodes=&amp;lt;num_nodes&amp;gt; \
    --nproc_per_node=&amp;lt;gpus_per_node&amp;gt; \
    --node_rank=&amp;lt;rank_of_node&amp;gt; \
    --master_addr=&amp;lt;master_ip&amp;gt; \
    --master_port=&amp;lt;port&amp;gt; \
    scripts/train_pytorch.py &amp;lt;config_name&amp;gt; --exp_name=&amp;lt;run_name&amp;gt; --save_interval &amp;lt;interval&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Precision Settings&lt;/h3&gt; 
&lt;p&gt;JAX and PyTorch implementations handle precision as follows:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;JAX:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Inference: most weights and computations in bfloat16, with a few computations in float32 for stability&lt;/li&gt; 
 &lt;li&gt;Training: defaults to mixed precision: weights and gradients in float32, (most) activations and computations in bfloat16. You can change to full float32 training by setting &lt;code&gt;dtype&lt;/code&gt; to float32 in the config.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;PyTorch:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Inference: matches JAX -- most weights and computations in bfloat16, with a few weights converted to float32 for stability&lt;/li&gt; 
 &lt;li&gt;Training: supports either full bfloat16 (default) or full float32. You can change it by setting &lt;code&gt;pytorch_training_precision&lt;/code&gt; in the config. bfloat16 uses less memory but exhibits higher losses compared to float32. Mixed precision is not yet supported.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;With torch.compile, inference speed is comparable between JAX and PyTorch.&lt;/p&gt; 
&lt;h2&gt;Troubleshooting&lt;/h2&gt; 
&lt;p&gt;We will collect common issues and their solutions here. If you encounter an issue, please check here first. If you can't find a solution, please file an issue on the repo (see &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/CONTRIBUTING.md"&gt;here&lt;/a&gt; for guidelines).&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Issue&lt;/th&gt; 
   &lt;th&gt;Resolution&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;uv sync&lt;/code&gt; fails with dependency conflicts&lt;/td&gt; 
   &lt;td&gt;Try removing the virtual environment directory (&lt;code&gt;rm -rf .venv&lt;/code&gt;) and running &lt;code&gt;uv sync&lt;/code&gt; again. If issues persist, check that you have the latest version of &lt;code&gt;uv&lt;/code&gt; installed (&lt;code&gt;uv self update&lt;/code&gt;).&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Training runs out of GPU memory&lt;/td&gt; 
   &lt;td&gt;Make sure you set &lt;code&gt;XLA_PYTHON_CLIENT_MEM_FRACTION=0.9&lt;/code&gt; (or higher) before running training to allow JAX to use more GPU memory. You can also use &lt;code&gt;--fsdp-devices &amp;lt;n&amp;gt;&lt;/code&gt; where &lt;code&gt;&amp;lt;n&amp;gt;&lt;/code&gt; is your number of GPUs, to enable &lt;a href="https://engineering.fb.com/2021/07/15/open-source/fsdp/"&gt;fully-sharded data parallelism&lt;/a&gt;, which reduces memory usage in exchange for slower training (the amount of slowdown depends on your particular setup). If you are still running out of memory, you may way to consider disabling EMA.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Policy server connection errors&lt;/td&gt; 
   &lt;td&gt;Check that the server is running and listening on the expected port. Verify network connectivity and firewall settings between client and server.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Missing norm stats error when training&lt;/td&gt; 
   &lt;td&gt;Run &lt;code&gt;scripts/compute_norm_stats.py&lt;/code&gt; with your config name before starting training.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Dataset download fails&lt;/td&gt; 
   &lt;td&gt;Check your internet connection. For HuggingFace datasets, ensure you're logged in (&lt;code&gt;huggingface-cli login&lt;/code&gt;).&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;CUDA/GPU errors&lt;/td&gt; 
   &lt;td&gt;Verify NVIDIA drivers are installed correctly. For Docker, ensure nvidia-container-toolkit is installed. Check GPU compatibility. You do NOT need CUDA libraries installed at a system level --- they will be installed via uv. You may even want to try &lt;em&gt;uninstalling&lt;/em&gt; system CUDA libraries if you run into CUDA issues, since system libraries can sometimes cause conflicts.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Import errors when running examples&lt;/td&gt; 
   &lt;td&gt;Make sure you've installed all dependencies with &lt;code&gt;uv sync&lt;/code&gt;. Some examples may have additional requirements listed in their READMEs.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Action dimensions mismatch&lt;/td&gt; 
   &lt;td&gt;Verify your data processing transforms match the expected input/output dimensions of your robot. Check the action space definitions in your policy classes.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Diverging training loss&lt;/td&gt; 
   &lt;td&gt;Check the &lt;code&gt;q01&lt;/code&gt;, &lt;code&gt;q99&lt;/code&gt;, and &lt;code&gt;std&lt;/code&gt; values in &lt;code&gt;norm_stats.json&lt;/code&gt; for your dataset. Certain dimensions that are rarely used can end up with very small &lt;code&gt;q01&lt;/code&gt;, &lt;code&gt;q99&lt;/code&gt;, or &lt;code&gt;std&lt;/code&gt; values, leading to huge states and actions after normalization. You can manually adjust the norm stats as a workaround.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt;</description>
    </item>
    
    <item>
      <title>sinaptik-ai/pandas-ai</title>
      <link>https://github.com/sinaptik-ai/pandas-ai</link>
      <description>&lt;p&gt;Chat with your database or your datalake (SQL, CSV, parquet). PandasAI makes data analysis conversational using LLMs and RAG.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;img src="https://raw.githubusercontent.com/sinaptik-ai/pandas-ai/main/assets/logo.png" alt="PandasAI" /&gt;&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://pypi.org/project/pandasai/"&gt;&lt;img src="https://img.shields.io/pypi/v/pandasai?label=Release&amp;amp;style=flat-square" alt="Release" /&gt;&lt;/a&gt; &lt;a href="https://github.com/sinaptik-ai/pandas-ai/actions/workflows/ci-core.yml/badge.svg"&gt;&lt;img src="https://github.com/sinaptik-ai/pandas-ai/actions/workflows/ci-core.yml/badge.svg?sanitize=true" alt="CI" /&gt;&lt;/a&gt; &lt;a href="https://github.com/sinaptik-ai/pandas-ai/actions/workflows/cd.yml/badge.svg"&gt;&lt;img src="https://github.com/sinaptik-ai/pandas-ai/actions/workflows/cd.yml/badge.svg?sanitize=true" alt="CD" /&gt;&lt;/a&gt; &lt;a href="https://codecov.io/gh/sinaptik-ai/pandas-ai"&gt;&lt;img src="https://codecov.io/gh/sinaptik-ai/pandas-ai/branch/main/graph/badge.svg?sanitize=true" alt="Coverage" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/KYKj9F2FRH"&gt;&lt;img src="https://dcbadge.vercel.app/api/server/kF7FqH2FwS?style=flat&amp;amp;compact=true" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/pandasai"&gt;&lt;img src="https://static.pepy.tech/badge/pandasai" alt="Downloads" /&gt;&lt;/a&gt; &lt;a href="https://opensource.org/licenses/MIT"&gt;&lt;img src="https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true" alt="License: MIT" /&gt;&lt;/a&gt; &lt;a href="https://colab.research.google.com/drive/1ZnO-njhL7TBOYPZaqvMvGtsjckZKrv2E?usp=sharing"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open in Colab" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;PandasAI is a Python platform that makes it easy to ask questions to your data in natural language. It helps non-technical users to interact with their data in a more natural way, and it helps technical users to save time, and effort when working with data.&lt;/p&gt; 
&lt;h1&gt;🔧 Getting started&lt;/h1&gt; 
&lt;p&gt;You can find the full documentation for PandasAI &lt;a href="https://pandas-ai.readthedocs.io/en/latest/"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You can either decide to use PandasAI in your Jupyter notebooks, Streamlit apps, or use the client and server architecture from the repo.&lt;/p&gt; 
&lt;h2&gt;📚 Using the library&lt;/h2&gt; 
&lt;h3&gt;Python Requirements&lt;/h3&gt; 
&lt;p&gt;Python version &lt;code&gt;3.8+ &amp;lt;3.12&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;📦 Installation&lt;/h3&gt; 
&lt;p&gt;You can install the PandasAI library using pip or poetry.&lt;/p&gt; 
&lt;p&gt;With pip:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install "pandasai&amp;gt;=3.0.0b2"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;With poetry:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry add "pandasai&amp;gt;=3.0.0b2"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;💻 Usage&lt;/h3&gt; 
&lt;h4&gt;Ask questions&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import pandasai as pai
from pandasai_openai.openai import OpenAI

llm = OpenAI("OPEN_AI_API_KEY")

pai.config.set({
    "llm": llm
})

# Sample DataFrame
df = pai.DataFrame({
    "country": ["United States", "United Kingdom", "France", "Germany", "Italy", "Spain", "Canada", "Australia", "Japan", "China"],
    "revenue": [5000, 3200, 2900, 4100, 2300, 2100, 2500, 2600, 4500, 7000]
})

df.chat('Which are the top 5 countries by sales?')
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code&gt;China, United States, Japan, Germany, Australia
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;p&gt;Or you can ask more complex questions:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;df.chat(
    "What is the total sales for the top 3 countries by sales?"
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code&gt;The total sales for the top 3 countries by sales is 16500.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Visualize charts&lt;/h4&gt; 
&lt;p&gt;You can also ask PandasAI to generate charts for you:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;df.chat(
    "Plot the histogram of countries showing for each one the gd. Use different colors for each bar",
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/sinaptik-ai/pandas-ai/main/assets/histogram-chart.png?raw=true" alt="Chart" /&gt;&lt;/p&gt; 
&lt;h4&gt;Multiple DataFrames&lt;/h4&gt; 
&lt;p&gt;You can also pass in multiple dataframes to PandasAI and ask questions relating them.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import pandasai as pai
from pandasai_openai.openai import OpenAI

employees_data = {
    'EmployeeID': [1, 2, 3, 4, 5],
    'Name': ['John', 'Emma', 'Liam', 'Olivia', 'William'],
    'Department': ['HR', 'Sales', 'IT', 'Marketing', 'Finance']
}

salaries_data = {
    'EmployeeID': [1, 2, 3, 4, 5],
    'Salary': [5000, 6000, 4500, 7000, 5500]
}

llm = OpenAI("OPEN_AI_API_KEY")

pai.config.set({
    "llm": llm
})

employees_df = pai.DataFrame(employees_data)
salaries_df = pai.DataFrame(salaries_data)


pai.chat("Who gets paid the most?", employees_df, salaries_df)
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code&gt;Olivia gets paid the most.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Docker Sandbox&lt;/h4&gt; 
&lt;p&gt;You can run PandasAI in a Docker sandbox, providing a secure, isolated environment to execute code safely and mitigate the risk of malicious attacks.&lt;/p&gt; 
&lt;h5&gt;Python Requirements&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install "pandasai-docker"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;Usage&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import pandasai as pai
from pandasai_docker import DockerSandbox
from pandasai_openai.openai import OpenAI

# Initialize the sandbox
sandbox = DockerSandbox()
sandbox.start()

employees_data = {
    'EmployeeID': [1, 2, 3, 4, 5],
    'Name': ['John', 'Emma', 'Liam', 'Olivia', 'William'],
    'Department': ['HR', 'Sales', 'IT', 'Marketing', 'Finance']
}

salaries_data = {
    'EmployeeID': [1, 2, 3, 4, 5],
    'Salary': [5000, 6000, 4500, 7000, 5500]
}

llm = OpenAI("OPEN_AI_API_KEY")

pai.config.set({
    "llm": llm
})

employees_df = pai.DataFrame(employees_data)
salaries_df = pai.DataFrame(salaries_data)

pai.chat("Who gets paid the most?", employees_df, salaries_df, sandbox=sandbox)

# Don't forget to stop the sandbox when done
sandbox.stop()
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code&gt;Olivia gets paid the most.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can find more examples in the &lt;a href="https://raw.githubusercontent.com/sinaptik-ai/pandas-ai/main/examples"&gt;examples&lt;/a&gt; directory.&lt;/p&gt; 
&lt;h2&gt;📜 License&lt;/h2&gt; 
&lt;p&gt;PandasAI is available under the MIT expat license, except for the &lt;code&gt;pandasai/ee&lt;/code&gt; directory of this repository, which has its &lt;a href="https://github.com/sinaptik-ai/pandas-ai/raw/main/ee/LICENSE"&gt;license here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you are interested in managed PandasAI Cloud or self-hosted Enterprise Offering, &lt;a href="https://getpanda.ai/pricing"&gt;contact us&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Resources&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Beta Notice&lt;/strong&gt;&lt;br /&gt; Release v3 is currently in beta. The following documentation and examples reflect the features and functionality in progress and may change before the final release.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://pandas-ai.readthedocs.io/en/latest/"&gt;Docs&lt;/a&gt; for comprehensive documentation&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/sinaptik-ai/pandas-ai/main/examples"&gt;Examples&lt;/a&gt; for example notebooks&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://discord.gg/KYKj9F2FRH"&gt;Discord&lt;/a&gt; for discussion with the community and PandasAI team&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🤝 Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions are welcome! Please check the outstanding issues and feel free to open a pull request. For more information, please check out the &lt;a href="https://raw.githubusercontent.com/sinaptik-ai/pandas-ai/main/CONTRIBUTING.md"&gt;contributing guidelines&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Thank you!&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/sinaptik-ai/pandas-ai/graphs/contributors"&gt;&lt;img src="https://contrib.rocks/image?repo=sinaptik-ai/pandas-ai" alt="Contributors" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>HKUDS/AutoAgent</title>
      <link>https://github.com/HKUDS/AutoAgent</link>
      <description>&lt;p&gt;"AutoAgent: Fully-Automated and Zero-Code LLM Agent Framework"&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a name="readme-top"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/AutoAgent_logo.svg?sanitize=true" alt="Logo" width="200" /&gt; 
 &lt;h1 align="center"&gt;AutoAgent: Fully-Automated &amp;amp; Zero-Code&lt;br /&gt; LLM Agent Framework &lt;/h1&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://autoagent-ai.github.io"&gt;&lt;img src="https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&amp;amp;color=FFE165&amp;amp;logo=homepage&amp;amp;logoColor=white" alt="Credits" /&gt;&lt;/a&gt; 
 &lt;a href="https://join.slack.com/t/metachain-workspace/shared_invite/zt-2zibtmutw-v7xOJObBf9jE2w3x7nctFQ"&gt;&lt;img src="https://img.shields.io/badge/Slack-Join%20Us-red?logo=slack&amp;amp;logoColor=white&amp;amp;style=for-the-badge" alt="Join our Slack community" /&gt;&lt;/a&gt; 
 &lt;a href="https://discord.gg/jQJdXyDB"&gt;&lt;img src="https://img.shields.io/badge/Discord-Join%20Us-purple?logo=discord&amp;amp;logoColor=white&amp;amp;style=for-the-badge" alt="Join our Discord community" /&gt;&lt;/a&gt; 
 &lt;a href="https://github.com/HKUDS/AutoAgent/raw/main/assets/autoagent-wechat.jpg"&gt;&lt;img src="https://img.shields.io/badge/Wechat-Join%20Us-green?logo=wechat&amp;amp;logoColor=white&amp;amp;style=for-the-badge" alt="Join our Wechat community" /&gt;&lt;/a&gt; 
 &lt;br /&gt; 
 &lt;a href="https://autoagent-ai.github.io/docs"&gt;&lt;img src="https://img.shields.io/badge/Documentation-000?logo=googledocs&amp;amp;logoColor=FFE165&amp;amp;style=for-the-badge" alt="Check out the documentation" /&gt;&lt;/a&gt; 
 &lt;a href="https://arxiv.org/abs/2502.05957"&gt;&lt;img src="https://img.shields.io/badge/Paper%20on%20Arxiv-000?logoColor=FFE165&amp;amp;logo=arxiv&amp;amp;style=for-the-badge" alt="Paper" /&gt;&lt;/a&gt; 
 &lt;a href="https://gaia-benchmark-leaderboard.hf.space/"&gt;&lt;img src="https://img.shields.io/badge/GAIA%20Benchmark-000?logoColor=FFE165&amp;amp;logo=huggingface&amp;amp;style=for-the-badge" alt="Evaluation Benchmark Score" /&gt;&lt;/a&gt; 
 &lt;hr /&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://trendshift.io/repositories/13954" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/13954" alt="HKUDS%2FAutoAgent | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;p&gt;Welcome to AutoAgent! AutoAgent is a &lt;strong&gt;Fully-Automated&lt;/strong&gt; and highly &lt;strong&gt;Self-Developing&lt;/strong&gt; framework that enables users to create and deploy LLM agents through &lt;strong&gt;Natural Language Alone&lt;/strong&gt;.&lt;/p&gt; 
&lt;h2&gt;✨Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;🏆 Top Performers on the GAIA Benchmark &lt;br /&gt;AutoAgent has delivering comparable performance to many &lt;strong&gt;Deep Research Agents&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;✨ Agent and Workflow Create with Ease &lt;br /&gt;AutoAgent leverages natural language to effortlessly build ready-to-use &lt;strong&gt;tools&lt;/strong&gt;, &lt;strong&gt;agents&lt;/strong&gt; and &lt;strong&gt;workflows&lt;/strong&gt; - no coding required.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;📚 Agentic-RAG with Native Self-Managing Vector Database &lt;br /&gt;AutoAgent equipped with a native self-managing vector database, outperforms industry-leading solutions like &lt;strong&gt;LangChain&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;🌐 Universal LLM Support &lt;br /&gt;AutoAgent seamlessly integrates with &lt;strong&gt;A Wide Range&lt;/strong&gt; of LLMs (e.g., OpenAI, Anthropic, Deepseek, vLLM, Grok, Huggingface ...)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;🔀 Flexible Interaction &lt;br /&gt;Benefit from support for both &lt;strong&gt;function-calling&lt;/strong&gt; and &lt;strong&gt;ReAct&lt;/strong&gt; interaction modes.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;🤖 Dynamic, Extensible, Lightweight &lt;br /&gt;AutoAgent is your &lt;strong&gt;Personal AI Assistant&lt;/strong&gt;, designed to be dynamic, extensible, customized, and lightweight.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;🚀 Unlock the Future of LLM Agents. Try 🔥AutoAgent🔥 Now!&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;!-- &lt;img src="./assets/AutoAgentnew-intro.pdf" alt="Logo" width="100%"&gt; --&gt; 
 &lt;figure&gt; 
  &lt;img src="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/autoagent-intro.svg?sanitize=true" alt="Logo" style="max-width: 100%; height: auto;" /&gt; 
  &lt;figcaption&gt;
   &lt;em&gt;Quick Overview of AutoAgent.&lt;/em&gt;
  &lt;/figcaption&gt; 
 &lt;/figure&gt; 
&lt;/div&gt; 
&lt;h2&gt;🔥 News&lt;/h2&gt; 
&lt;div class="scrollable"&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;[2025, Feb 17]&lt;/strong&gt;: &amp;nbsp;🎉🎉We've updated and released AutoAgent v0.2.0 (formerly known as MetaChain). Detailed changes include: 1) fix the bug of different LLM providers from issues; 2) add automatic installation of AutoAgent in the container environment according to issues; 3) add more easy-to-use commands for the CLI mode. 4) Rename the project to AutoAgent for better understanding.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;[2025, Feb 10]&lt;/strong&gt;: &amp;nbsp;🎉🎉We've released &lt;b&gt;MetaChain!&lt;/b&gt;, including framework, evaluation codes and CLI mode! Check our &lt;a href="https://arxiv.org/abs/2502.05957"&gt;paper&lt;/a&gt; for more details.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/div&gt; 
&lt;span id="table-of-contents"&gt;&lt;/span&gt; 
&lt;h2&gt;📑 Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#features"&gt;✨ Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#news"&gt;🔥 News&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#how-to-use"&gt;🔍 How to Use AutoAgent&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#user-mode"&gt;1. &lt;code&gt;user mode&lt;/code&gt; (SOTA 🏆 Open Deep Research)&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#agent-editor"&gt;2. &lt;code&gt;agent editor&lt;/code&gt; (Agent Creation without Workflow)&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#workflow-editor"&gt;3. &lt;code&gt;workflow editor&lt;/code&gt; (Agent Creation with Workflow)&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#quick-start"&gt;⚡ Quick Start&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#api-keys-setup"&gt;API Keys Setup&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#start-with-cli-mode"&gt;Start with CLI Mode&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#todo"&gt;☑️ Todo List&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#reproduce"&gt;🔬 How To Reproduce the Results in the Paper&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#documentation"&gt;📖 Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#community"&gt;🤝 Join the Community&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#acknowledgements"&gt;🙏 Acknowledgements&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#cite"&gt;🌟 Cite&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id="how-to-use"&gt;&lt;/span&gt; 
&lt;h2&gt;🔍 How to Use AutoAgent&lt;/h2&gt; 
&lt;span id="user-mode"&gt;&lt;/span&gt; 
&lt;h3&gt;1. &lt;code&gt;user mode&lt;/code&gt; (SOTA 🏆 Open Deep Research)&lt;/h3&gt; 
&lt;p&gt;AutoAgent have an out-of-the-box multi-agent system, which you could choose &lt;code&gt;user mode&lt;/code&gt; in the start page to use it. This multi-agent system is a general AI assistant, having the same functionality with &lt;strong&gt;OpenAI's Deep Research&lt;/strong&gt; and the comparable performance with it in &lt;a href="https://gaia-benchmark-leaderboard.hf.space/"&gt;GAIA&lt;/a&gt; benchmark.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;🚀 &lt;strong&gt;High Performance&lt;/strong&gt;: Matches Deep Research using Claude 3.5 rather than OpenAI's o3 model.&lt;/li&gt; 
 &lt;li&gt;🔄 &lt;strong&gt;Model Flexibility&lt;/strong&gt;: Compatible with any LLM (including Deepseek-R1, Grok, Gemini, etc.)&lt;/li&gt; 
 &lt;li&gt;💰 &lt;strong&gt;Cost-Effective&lt;/strong&gt;: Open-source alternative to Deep Research's $200/month subscription&lt;/li&gt; 
 &lt;li&gt;🎯 &lt;strong&gt;User-Friendly&lt;/strong&gt;: Easy-to-deploy CLI interface for seamless interaction&lt;/li&gt; 
 &lt;li&gt;📁 &lt;strong&gt;File Support&lt;/strong&gt;: Handles file uploads for enhanced data interaction&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center"&gt; 
 &lt;video width="80%" controls&gt; 
  &lt;source src="./assets/video_v1_compressed.mp4" type="video/mp4" /&gt; 
 &lt;/video&gt; 
 &lt;p&gt;&lt;em&gt;🎥 Deep Research (aka User Mode)&lt;/em&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;span id="agent-editor"&gt;&lt;/span&gt; 
&lt;h3&gt;2. &lt;code&gt;agent editor&lt;/code&gt; (Agent Creation without Workflow)&lt;/h3&gt; 
&lt;p&gt;The most distinctive feature of AutoAgent is its natural language customization capability. Unlike other agent frameworks, AutoAgent allows you to create tools, agents, and workflows using natural language alone. Simply choose &lt;code&gt;agent editor&lt;/code&gt; or &lt;code&gt;workflow editor&lt;/code&gt; mode to start your journey of building agents through conversations.&lt;/p&gt; 
&lt;p&gt;You can use &lt;code&gt;agent editor&lt;/code&gt; as shown in the following figure.&lt;/p&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr align="center"&gt; 
   &lt;td width="33%"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/agent_editor/1-requirement.png" alt="requirement" width="100%" /&gt; &lt;br /&gt; &lt;em&gt;Input what kind of agent you want to create.&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="33%"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/agent_editor/2-profiling.png" alt="profiling" width="100%" /&gt; &lt;br /&gt; &lt;em&gt;Automated agent profiling.&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="33%"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/agent_editor/3-profiles.png" alt="profiles" width="100%" /&gt; &lt;br /&gt; &lt;em&gt;Output the agent profiles.&lt;/em&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr align="center"&gt; 
   &lt;td width="33%"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/agent_editor/4-tools.png" alt="tools" width="100%" /&gt; &lt;br /&gt; &lt;em&gt;Create the desired tools.&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="33%"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/agent_editor/5-task.png" alt="task" width="100%" /&gt; &lt;br /&gt; &lt;em&gt;Input what do you want to complete with the agent. (Optional)&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="33%"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/agent_editor/6-output-next.png" alt="output" width="100%" /&gt; &lt;br /&gt; &lt;em&gt;Create the desired agent(s) and go to the next step.&lt;/em&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;span id="workflow-editor"&gt;&lt;/span&gt; 
&lt;h3&gt;3. &lt;code&gt;workflow editor&lt;/code&gt; (Agent Creation with Workflow)&lt;/h3&gt; 
&lt;p&gt;You can also create the agent workflows using natural language description with the &lt;code&gt;workflow editor&lt;/code&gt; mode, as shown in the following figure. (Tips: this mode does not support tool creation temporarily.)&lt;/p&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr align="center"&gt; 
   &lt;td width="33%"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/workflow_editor/1-requirement.png" alt="requirement" width="100%" /&gt; &lt;br /&gt; &lt;em&gt;Input what kind of workflow you want to create.&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="33%"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/workflow_editor/2-profiling.png" alt="profiling" width="100%" /&gt; &lt;br /&gt; &lt;em&gt;Automated workflow profiling.&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="33%"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/workflow_editor/3-profiles.png" alt="profiles" width="100%" /&gt; &lt;br /&gt; &lt;em&gt;Output the workflow profiles.&lt;/em&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr align="center"&gt; 
   &lt;td width="33%"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/workflow_editor/4-task.png" alt="task" width="66%" /&gt; &lt;br /&gt; &lt;em&gt;Input what do you want to complete with the workflow. (Optional)&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="33%"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/workflow_editor/5-output-next.png" alt="output" width="66%" /&gt; &lt;br /&gt; &lt;em&gt;Create the desired workflow(s) and go to the next step.&lt;/em&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;span id="quick-start"&gt;&lt;/span&gt; 
&lt;h2&gt;⚡ Quick Start&lt;/h2&gt; 
&lt;span id="installation"&gt;&lt;/span&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;h4&gt;AutoAgent Installation&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/HKUDS/AutoAgent.git
cd AutoAgent
pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Docker Installation&lt;/h4&gt; 
&lt;p&gt;We use Docker to containerize the agent-interactive environment. So please install &lt;a href="https://www.docker.com/"&gt;Docker&lt;/a&gt; first. You don't need to manually pull the pre-built image, because we have let Auto-Deep-Research &lt;strong&gt;automatically pull the pre-built image based on your architecture of your machine&lt;/strong&gt;.&lt;/p&gt; 
&lt;span id="api-keys-setup"&gt;&lt;/span&gt; 
&lt;h3&gt;API Keys Setup&lt;/h3&gt; 
&lt;p&gt;Create an environment variable file, just like &lt;code&gt;.env.template&lt;/code&gt;, and set the API keys for the LLMs you want to use. Not every LLM API Key is required, use what you need.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Required Github Tokens of your own
GITHUB_AI_TOKEN=

# Optional API Keys
OPENAI_API_KEY=
DEEPSEEK_API_KEY=
ANTHROPIC_API_KEY=
GEMINI_API_KEY=
HUGGINGFACE_API_KEY=
GROQ_API_KEY=
XAI_API_KEY=
&lt;/code&gt;&lt;/pre&gt; 
&lt;span id="start-with-cli-mode"&gt;&lt;/span&gt; 
&lt;h3&gt;Start with CLI Mode&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[🚨 &lt;strong&gt;News&lt;/strong&gt;: ] We have updated a more easy-to-use command to start the CLI mode and fix the bug of different LLM providers from issues. You can follow the following steps to start the CLI mode with different LLM providers with much less configuration.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Command Options:&lt;/h4&gt; 
&lt;p&gt;You can run &lt;code&gt;auto main&lt;/code&gt; to start full part of AutoAgent, including &lt;code&gt;user mode&lt;/code&gt;, &lt;code&gt;agent editor&lt;/code&gt; and &lt;code&gt;workflow editor&lt;/code&gt;. Btw, you can also run &lt;code&gt;auto deep-research&lt;/code&gt; to start more lightweight &lt;code&gt;user mode&lt;/code&gt;, just like the &lt;a href="https://github.com/HKUDS/Auto-Deep-Research"&gt;Auto-Deep-Research&lt;/a&gt; project. Some configuration of this command is shown below.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--container_name&lt;/code&gt;: Name of the Docker container (default: 'deepresearch')&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--port&lt;/code&gt;: Port for the container (default: 12346)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;COMPLETION_MODEL&lt;/code&gt;: Specify the LLM model to use, you should follow the name of &lt;a href="https://github.com/BerriAI/litellm"&gt;Litellm&lt;/a&gt; to set the model name. (Default: &lt;code&gt;claude-3-5-sonnet-20241022&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;DEBUG&lt;/code&gt;: Enable debug mode for detailed logs (default: False)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;API_BASE_URL&lt;/code&gt;: The base URL for the LLM provider (default: None)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;FN_CALL&lt;/code&gt;: Enable function calling (default: None). Most of time, you could ignore this option because we have already set the default value based on the model name.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;git_clone&lt;/code&gt;: Clone the AutoAgent repository to the local environment (only support with the &lt;code&gt;auto main&lt;/code&gt; command, default: True)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;test_pull_name&lt;/code&gt;: The name of the test pull. (only support with the &lt;code&gt;auto main&lt;/code&gt; command, default: 'autoagent_mirror')&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;More details about &lt;code&gt;git_clone&lt;/code&gt; and &lt;code&gt;test_pull_name&lt;/code&gt;]&lt;/h4&gt; 
&lt;p&gt;In the &lt;code&gt;agent editor&lt;/code&gt; and &lt;code&gt;workflow editor&lt;/code&gt; mode, we should clone a mirror of the AutoAgent repository to the local agent-interactive environment and let our &lt;strong&gt;AutoAgent&lt;/strong&gt; automatically update the AutoAgent itself, such as creating new tools, agents and workflows. So if you want to use the &lt;code&gt;agent editor&lt;/code&gt; and &lt;code&gt;workflow editor&lt;/code&gt; mode, you should set the &lt;code&gt;git_clone&lt;/code&gt; to True and set the &lt;code&gt;test_pull_name&lt;/code&gt; to 'autoagent_mirror' or other branches.&lt;/p&gt; 
&lt;h4&gt;&lt;code&gt;auto main&lt;/code&gt; with different LLM Providers&lt;/h4&gt; 
&lt;p&gt;Then I will show you how to use the full part of AutoAgent with the &lt;code&gt;auto main&lt;/code&gt; command and different LLM providers. If you want to use the &lt;code&gt;auto deep-research&lt;/code&gt; command, you can refer to the &lt;a href="https://github.com/HKUDS/Auto-Deep-Research"&gt;Auto-Deep-Research&lt;/a&gt; project for more details.&lt;/p&gt; 
&lt;h5&gt;Anthropic&lt;/h5&gt; 
&lt;ul&gt; 
 &lt;li&gt;set the &lt;code&gt;ANTHROPIC_API_KEY&lt;/code&gt; in the &lt;code&gt;.env&lt;/code&gt; file.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;ANTHROPIC_API_KEY=your_anthropic_api_key
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;run the following command to start Auto-Deep-Research.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;auto main # default model is claude-3-5-sonnet-20241022
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;OpenAI&lt;/h5&gt; 
&lt;ul&gt; 
 &lt;li&gt;set the &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; in the &lt;code&gt;.env&lt;/code&gt; file.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;OPENAI_API_KEY=your_openai_api_key
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;run the following command to start Auto-Deep-Research.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;COMPLETION_MODEL=gpt-4o auto main
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;Mistral&lt;/h5&gt; 
&lt;ul&gt; 
 &lt;li&gt;set the &lt;code&gt;MISTRAL_API_KEY&lt;/code&gt; in the &lt;code&gt;.env&lt;/code&gt; file.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;MISTRAL_API_KEY=your_mistral_api_key
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;run the following command to start Auto-Deep-Research.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;COMPLETION_MODEL=mistral/mistral-large-2407 auto main
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;Gemini - Google AI Studio&lt;/h5&gt; 
&lt;ul&gt; 
 &lt;li&gt;set the &lt;code&gt;GEMINI_API_KEY&lt;/code&gt; in the &lt;code&gt;.env&lt;/code&gt; file.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;GEMINI_API_KEY=your_gemini_api_key
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;run the following command to start Auto-Deep-Research.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;COMPLETION_MODEL=gemini/gemini-2.0-flash auto main
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;Huggingface&lt;/h5&gt; 
&lt;ul&gt; 
 &lt;li&gt;set the &lt;code&gt;HUGGINGFACE_API_KEY&lt;/code&gt; in the &lt;code&gt;.env&lt;/code&gt; file.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;HUGGINGFACE_API_KEY=your_huggingface_api_key
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;run the following command to start Auto-Deep-Research.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;COMPLETION_MODEL=huggingface/meta-llama/Llama-3.3-70B-Instruct auto main
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;Groq&lt;/h5&gt; 
&lt;ul&gt; 
 &lt;li&gt;set the &lt;code&gt;GROQ_API_KEY&lt;/code&gt; in the &lt;code&gt;.env&lt;/code&gt; file.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;GROQ_API_KEY=your_groq_api_key
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;run the following command to start Auto-Deep-Research.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;COMPLETION_MODEL=groq/deepseek-r1-distill-llama-70b auto main
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;OpenAI-Compatible Endpoints (e.g., Grok)&lt;/h5&gt; 
&lt;ul&gt; 
 &lt;li&gt;set the &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; in the &lt;code&gt;.env&lt;/code&gt; file.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;OPENAI_API_KEY=your_api_key_for_openai_compatible_endpoints
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;run the following command to start Auto-Deep-Research.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;COMPLETION_MODEL=openai/grok-2-latest API_BASE_URL=https://api.x.ai/v1 auto main
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;OpenRouter (e.g., DeepSeek-R1)&lt;/h5&gt; 
&lt;p&gt;We recommend using OpenRouter as LLM provider of DeepSeek-R1 temporarily. Because official API of DeepSeek-R1 can not be used efficiently.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;set the &lt;code&gt;OPENROUTER_API_KEY&lt;/code&gt; in the &lt;code&gt;.env&lt;/code&gt; file.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;OPENROUTER_API_KEY=your_openrouter_api_key
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;run the following command to start Auto-Deep-Research.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;COMPLETION_MODEL=openrouter/deepseek/deepseek-r1 auto main
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;DeepSeek&lt;/h5&gt; 
&lt;ul&gt; 
 &lt;li&gt;set the &lt;code&gt;DEEPSEEK_API_KEY&lt;/code&gt; in the &lt;code&gt;.env&lt;/code&gt; file.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;DEEPSEEK_API_KEY=your_deepseek_api_key
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;run the following command to start Auto-Deep-Research.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;COMPLETION_MODEL=deepseek/deepseek-chat auto main
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After the CLI mode is started, you can see the start page of AutoAgent:&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;!-- &lt;img src="./assets/AutoAgentnew-intro.pdf" alt="Logo" width="100%"&gt; --&gt; 
 &lt;figure&gt; 
  &lt;img src="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/cover.png" alt="Logo" style="max-width: 100%; height: auto;" /&gt; 
  &lt;figcaption&gt;
   &lt;em&gt;Start Page of AutoAgent.&lt;/em&gt;
  &lt;/figcaption&gt; 
 &lt;/figure&gt; 
&lt;/div&gt; 
&lt;h3&gt;Tips&lt;/h3&gt; 
&lt;h4&gt;Import browser cookies to browser environment&lt;/h4&gt; 
&lt;p&gt;You can import the browser cookies to the browser environment to let the agent better access some specific websites. For more details, please refer to the &lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/AutoAgent/environment/cookie_json/README.md"&gt;cookies&lt;/a&gt; folder.&lt;/p&gt; 
&lt;h4&gt;Add your own API keys for third-party Tool Platforms&lt;/h4&gt; 
&lt;p&gt;If you want to create tools from the third-party tool platforms, such as RapidAPI, you should subscribe tools from the platform and add your own API keys by running &lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/process_tool_docs.py"&gt;process_tool_docs.py&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python process_tool_docs.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;More features coming soon! 🚀 &lt;strong&gt;Web GUI interface&lt;/strong&gt; under development.&lt;/p&gt; 
&lt;span id="todo"&gt;&lt;/span&gt; 
&lt;h2&gt;☑️ Todo List&lt;/h2&gt; 
&lt;p&gt;AutoAgent is continuously evolving! Here's what's coming:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;📊 &lt;strong&gt;More Benchmarks&lt;/strong&gt;: Expanding evaluations to &lt;strong&gt;SWE-bench&lt;/strong&gt;, &lt;strong&gt;WebArena&lt;/strong&gt;, and more&lt;/li&gt; 
 &lt;li&gt;🖥️ &lt;strong&gt;GUI Agent&lt;/strong&gt;: Supporting &lt;em&gt;Computer-Use&lt;/em&gt; agents with GUI interaction&lt;/li&gt; 
 &lt;li&gt;🔧 &lt;strong&gt;Tool Platforms&lt;/strong&gt;: Integration with more platforms like &lt;strong&gt;Composio&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;🏗️ &lt;strong&gt;Code Sandboxes&lt;/strong&gt;: Supporting additional environments like &lt;strong&gt;E2B&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;🎨 &lt;strong&gt;Web Interface&lt;/strong&gt;: Developing comprehensive GUI for better user experience&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Have ideas or suggestions? Feel free to open an issue! Stay tuned for more exciting updates! 🚀&lt;/p&gt; 
&lt;span id="reproduce"&gt;&lt;/span&gt; 
&lt;h2&gt;🔬 How To Reproduce the Results in the Paper&lt;/h2&gt; 
&lt;h3&gt;GAIA Benchmark&lt;/h3&gt; 
&lt;p&gt;For the GAIA benchmark, you can run the following command to run the inference.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd path/to/AutoAgent &amp;amp;&amp;amp; sh evaluation/gaia/scripts/run_infer.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For the evaluation, you can run the following command.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd path/to/AutoAgent &amp;amp;&amp;amp; python evaluation/gaia/get_score.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Agentic-RAG&lt;/h3&gt; 
&lt;p&gt;For the Agentic-RAG task, you can run the following command to run the inference.&lt;/p&gt; 
&lt;p&gt;Step1. Turn to &lt;a href="https://huggingface.co/datasets/yixuantt/MultiHopRAG"&gt;this page&lt;/a&gt; and download it. Save them to your datapath.&lt;/p&gt; 
&lt;p&gt;Step2. Run the following command to run the inference.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd path/to/AutoAgent &amp;amp;&amp;amp; sh evaluation/multihoprag/scripts/run_rag.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Step3. The result will be saved in the &lt;code&gt;evaluation/multihoprag/result.json&lt;/code&gt;.&lt;/p&gt; 
&lt;span id="documentation"&gt;&lt;/span&gt; 
&lt;h2&gt;📖 Documentation&lt;/h2&gt; 
&lt;p&gt;A more detailed documentation is coming soon 🚀, and we will update in the &lt;a href="https://AutoAgent-ai.github.io/docs"&gt;Documentation&lt;/a&gt; page.&lt;/p&gt; 
&lt;span id="community"&gt;&lt;/span&gt; 
&lt;h2&gt;🤝 Join the Community&lt;/h2&gt; 
&lt;p&gt;We want to build a community for AutoAgent, and we welcome everyone to join us. You can join our community by:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://join.slack.com/t/AutoAgent-workspace/shared_invite/zt-2zibtmutw-v7xOJObBf9jE2w3x7nctFQ"&gt;Join our Slack workspace&lt;/a&gt; - Here we talk about research, architecture, and future development.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://discord.gg/z68KRvwB"&gt;Join our Discord server&lt;/a&gt; - This is a community-run server for general discussion, questions, and feedback.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/HKUDS/AutoAgent/issues"&gt;Read or post Github Issues&lt;/a&gt; - Check out the issues we're working on, or add your own ideas.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id="acknowledgements"&gt;&lt;/span&gt; 
&lt;h2&gt;Misc&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/HKUDS/AutoAgent/stargazers"&gt;&lt;img src="https://reporoster.com/stars/HKUDS/AutoAgent" alt="Stargazers repo roster for @HKUDS/AutoAgent" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/HKUDS/AutoAgent/network/members"&gt;&lt;img src="https://reporoster.com/forks/HKUDS/AutoAgent" alt="Forkers repo roster for @HKUDS/AutoAgent" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://star-history.com/#HKUDS/AutoAgent&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=HKUDS/AutoAgent&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;🙏 Acknowledgements&lt;/h2&gt; 
&lt;p&gt;Rome wasn't built in a day. AutoAgent stands on the shoulders of giants, and we are deeply grateful for the outstanding work that came before us. Our framework architecture draws inspiration from &lt;a href="https://github.com/openai/swarm"&gt;OpenAI Swarm&lt;/a&gt;, while our user mode's three-agent design benefits from &lt;a href="https://github.com/microsoft/autogen/tree/main/python/packages/autogen-magentic-one"&gt;Magentic-one&lt;/a&gt;'s insights. We've also learned from &lt;a href="https://github.com/All-Hands-AI/OpenHands"&gt;OpenHands&lt;/a&gt; for documentation structure and many other excellent projects for agent-environment interaction design, among others. We express our sincere gratitude and respect to all these pioneering works that have been instrumental in shaping AutoAgent.&lt;/p&gt; 
&lt;span id="cite"&gt;&lt;/span&gt; 
&lt;h2&gt;🌟 Cite&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-tex"&gt;@misc{AutoAgent,
      title={{AutoAgent: A Fully-Automated and Zero-Code Framework for LLM Agents}},
      author={Jiabin Tang, Tianyu Fan, Chao Huang},
      year={2025},
      eprint={202502.05957},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2502.05957},
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>DepthAnything/Depth-Anything-V2</title>
      <link>https://github.com/DepthAnything/Depth-Anything-V2</link>
      <description>&lt;p&gt;[NeurIPS 2024] Depth Anything V2. A More Capable Foundation Model for Monocular Depth Estimation&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h1&gt;Depth Anything V2&lt;/h1&gt; 
 &lt;p&gt;&lt;a href="https://liheyoung.github.io/"&gt;&lt;strong&gt;Lihe Yang&lt;/strong&gt;&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt; · &lt;a href="https://bingykang.github.io/"&gt;&lt;strong&gt;Bingyi Kang&lt;/strong&gt;&lt;/a&gt;&lt;sup&gt;2†&lt;/sup&gt; · &lt;a href="http://speedinghzl.github.io/"&gt;&lt;strong&gt;Zilong Huang&lt;/strong&gt;&lt;/a&gt;&lt;sup&gt;2&lt;/sup&gt; &lt;br /&gt; &lt;a href="http://zhaozhen.me/"&gt;&lt;strong&gt;Zhen Zhao&lt;/strong&gt;&lt;/a&gt; · &lt;a href="https://xiaogang00.github.io/"&gt;&lt;strong&gt;Xiaogang Xu&lt;/strong&gt;&lt;/a&gt; · &lt;a href="https://sites.google.com/site/jshfeng/"&gt;&lt;strong&gt;Jiashi Feng&lt;/strong&gt;&lt;/a&gt;&lt;sup&gt;2&lt;/sup&gt; · &lt;a href="https://hszhao.github.io/"&gt;&lt;strong&gt;Hengshuang Zhao&lt;/strong&gt;&lt;/a&gt;&lt;sup&gt;1*&lt;/sup&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;HKU   &lt;sup&gt;2&lt;/sup&gt;TikTok &lt;br /&gt; †project lead *corresponding author&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://arxiv.org/abs/2406.09414"&gt;&lt;img src="https://img.shields.io/badge/arXiv-Depth Anything V2-red" alt="Paper PDF" /&gt;&lt;/a&gt; &lt;a href="https://depth-anything-v2.github.io"&gt;&lt;img src="https://img.shields.io/badge/Project_Page-Depth Anything V2-green" alt="Project Page" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/spaces/depth-anything/Depth-Anything-V2"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Demo-blue" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/datasets/depth-anything/DA-2K"&gt;&lt;img src="https://img.shields.io/badge/Benchmark-DA--2K-yellow" alt="Benchmark" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;This work presents Depth Anything V2. It significantly outperforms &lt;a href="https://github.com/LiheYoung/Depth-Anything"&gt;V1&lt;/a&gt; in fine-grained details and robustness. Compared with SD-based models, it enjoys faster inference speed, fewer parameters, and higher depth accuracy.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/DepthAnything/Depth-Anything-V2/main/assets/teaser.png" alt="teaser" /&gt;&lt;/p&gt; 
&lt;h2&gt;News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;2025-01-22:&lt;/strong&gt; &lt;a href="https://videodepthanything.github.io"&gt;Video Depth Anything&lt;/a&gt; has been released. It generates consistent depth maps for super-long videos (e.g., over 5 minutes).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;2024-12-22:&lt;/strong&gt; &lt;a href="https://promptda.github.io/"&gt;Prompt Depth Anything&lt;/a&gt; has been released. It supports 4K resolution metric depth estimation when low-res LiDAR is used to prompt the DA models.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;2024-07-06:&lt;/strong&gt; Depth Anything V2 is supported in &lt;a href="https://github.com/huggingface/transformers/"&gt;Transformers&lt;/a&gt;. See the &lt;a href="https://huggingface.co/docs/transformers/main/en/model_doc/depth_anything_v2"&gt;instructions&lt;/a&gt; for convenient usage.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;2024-06-25:&lt;/strong&gt; Depth Anything is integrated into &lt;a href="https://developer.apple.com/machine-learning/models/"&gt;Apple Core ML Models&lt;/a&gt;. See the instructions (&lt;a href="https://huggingface.co/apple/coreml-depth-anything-small"&gt;V1&lt;/a&gt;, &lt;a href="https://huggingface.co/apple/coreml-depth-anything-v2-small"&gt;V2&lt;/a&gt;) for usage.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;2024-06-22:&lt;/strong&gt; We release &lt;a href="https://github.com/DepthAnything/Depth-Anything-V2/tree/main/metric_depth#pre-trained-models"&gt;smaller metric depth models&lt;/a&gt; based on Depth-Anything-V2-Small and Base.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;2024-06-20:&lt;/strong&gt; Our repository and project page are flagged by GitHub and removed from the public for 6 days. Sorry for the inconvenience.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;2024-06-14:&lt;/strong&gt; Paper, project page, code, models, demo, and benchmark are all released.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Pre-trained Models&lt;/h2&gt; 
&lt;p&gt;We provide &lt;strong&gt;four models&lt;/strong&gt; of varying scales for robust relative depth estimation:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Model&lt;/th&gt; 
   &lt;th align="right"&gt;Params&lt;/th&gt; 
   &lt;th align="center"&gt;Checkpoint&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;Depth-Anything-V2-Small&lt;/td&gt; 
   &lt;td align="right"&gt;24.8M&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/depth-anything/Depth-Anything-V2-Small/resolve/main/depth_anything_v2_vits.pth?download=true"&gt;Download&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;Depth-Anything-V2-Base&lt;/td&gt; 
   &lt;td align="right"&gt;97.5M&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/depth-anything/Depth-Anything-V2-Base/resolve/main/depth_anything_v2_vitb.pth?download=true"&gt;Download&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;Depth-Anything-V2-Large&lt;/td&gt; 
   &lt;td align="right"&gt;335.3M&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/depth-anything/Depth-Anything-V2-Large/resolve/main/depth_anything_v2_vitl.pth?download=true"&gt;Download&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;Depth-Anything-V2-Giant&lt;/td&gt; 
   &lt;td align="right"&gt;1.3B&lt;/td&gt; 
   &lt;td align="center"&gt;Coming soon&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Prepraration&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/DepthAnything/Depth-Anything-V2
cd Depth-Anything-V2
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Download the checkpoints listed &lt;a href="https://raw.githubusercontent.com/DepthAnything/Depth-Anything-V2/main/#pre-trained-models"&gt;here&lt;/a&gt; and put them under the &lt;code&gt;checkpoints&lt;/code&gt; directory.&lt;/p&gt; 
&lt;h3&gt;Use our models&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import cv2
import torch

from depth_anything_v2.dpt import DepthAnythingV2

DEVICE = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'

model_configs = {
    'vits': {'encoder': 'vits', 'features': 64, 'out_channels': [48, 96, 192, 384]},
    'vitb': {'encoder': 'vitb', 'features': 128, 'out_channels': [96, 192, 384, 768]},
    'vitl': {'encoder': 'vitl', 'features': 256, 'out_channels': [256, 512, 1024, 1024]},
    'vitg': {'encoder': 'vitg', 'features': 384, 'out_channels': [1536, 1536, 1536, 1536]}
}

encoder = 'vitl' # or 'vits', 'vitb', 'vitg'

model = DepthAnythingV2(**model_configs[encoder])
model.load_state_dict(torch.load(f'checkpoints/depth_anything_v2_{encoder}.pth', map_location='cpu'))
model = model.to(DEVICE).eval()

raw_img = cv2.imread('your/image/path')
depth = model.infer_image(raw_img) # HxW raw depth map in numpy
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you do not want to clone this repository, you can also load our models through &lt;a href="https://github.com/huggingface/transformers/"&gt;Transformers&lt;/a&gt;. Below is a simple code snippet. Please refer to the &lt;a href="https://huggingface.co/docs/transformers/main/en/model_doc/depth_anything_v2"&gt;official page&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Note 1: Make sure you can connect to Hugging Face and have installed the latest Transformers.&lt;/li&gt; 
 &lt;li&gt;Note 2: Due to the &lt;a href="https://github.com/huggingface/transformers/pull/31522#issuecomment-2184123463"&gt;upsampling difference&lt;/a&gt; between OpenCV (we used) and Pillow (HF used), predictions may differ slightly. So you are more recommended to use our models through the way introduced above.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from transformers import pipeline
from PIL import Image

pipe = pipeline(task="depth-estimation", model="depth-anything/Depth-Anything-V2-Small-hf")
image = Image.open('your/image/path')
depth = pipe(image)["depth"]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Running script on &lt;em&gt;images&lt;/em&gt;&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python run.py \
  --encoder &amp;lt;vits | vitb | vitl | vitg&amp;gt; \
  --img-path &amp;lt;path&amp;gt; --outdir &amp;lt;outdir&amp;gt; \
  [--input-size &amp;lt;size&amp;gt;] [--pred-only] [--grayscale]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Options:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--img-path&lt;/code&gt;: You can either 1) point it to an image directory storing all interested images, 2) point it to a single image, or 3) point it to a text file storing all image paths.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--input-size&lt;/code&gt; (optional): By default, we use input size &lt;code&gt;518&lt;/code&gt; for model inference. &lt;em&gt;&lt;strong&gt;You can increase the size for even more fine-grained results.&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--pred-only&lt;/code&gt; (optional): Only save the predicted depth map, without raw image.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--grayscale&lt;/code&gt; (optional): Save the grayscale depth map, without applying color palette.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python run.py --encoder vitl --img-path assets/examples --outdir depth_vis
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Running script on &lt;em&gt;videos&lt;/em&gt;&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python run_video.py \
  --encoder &amp;lt;vits | vitb | vitl | vitg&amp;gt; \
  --video-path assets/examples_video --outdir video_depth_vis \
  [--input-size &amp;lt;size&amp;gt;] [--pred-only] [--grayscale]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Our larger model has better temporal consistency on videos.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;h3&gt;Gradio demo&lt;/h3&gt; 
&lt;p&gt;To use our gradio demo locally:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python app.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also try our &lt;a href="https://huggingface.co/spaces/Depth-Anything/Depth-Anything-V2"&gt;online demo&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Note: Compared to V1, we have made a minor modification to the DINOv2-DPT architecture (originating from this &lt;a href="https://github.com/LiheYoung/Depth-Anything/issues/81"&gt;issue&lt;/a&gt;).&lt;/strong&gt;&lt;/em&gt; In V1, we &lt;em&gt;unintentionally&lt;/em&gt; used features from the last four layers of DINOv2 for decoding. In V2, we use &lt;a href="https://github.com/DepthAnything/Depth-Anything-V2/raw/2cbc36a8ce2cec41d38ee51153f112e87c8e42d8/depth_anything_v2/dpt.py#L164-L169"&gt;intermediate features&lt;/a&gt; instead. Although this modification did not improve details or accuracy, we decided to follow this common practice.&lt;/p&gt; 
&lt;h2&gt;Fine-tuned to Metric Depth Estimation&lt;/h2&gt; 
&lt;p&gt;Please refer to &lt;a href="https://raw.githubusercontent.com/DepthAnything/Depth-Anything-V2/main/metric_depth"&gt;metric depth estimation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;DA-2K Evaluation Benchmark&lt;/h2&gt; 
&lt;p&gt;Please refer to &lt;a href="https://raw.githubusercontent.com/DepthAnything/Depth-Anything-V2/main/DA-2K.md"&gt;DA-2K benchmark&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Community Support&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;We sincerely appreciate all the community support for our Depth Anything series. Thank you a lot!&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Apple Core ML: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://developer.apple.com/machine-learning/models"&gt;https://developer.apple.com/machine-learning/models&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://huggingface.co/apple/coreml-depth-anything-v2-small"&gt;https://huggingface.co/apple/coreml-depth-anything-v2-small&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://huggingface.co/apple/coreml-depth-anything-small"&gt;https://huggingface.co/apple/coreml-depth-anything-small&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Transformers: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://huggingface.co/docs/transformers/main/en/model_doc/depth_anything_v2"&gt;https://huggingface.co/docs/transformers/main/en/model_doc/depth_anything_v2&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://huggingface.co/docs/transformers/main/en/model_doc/depth_anything"&gt;https://huggingface.co/docs/transformers/main/en/model_doc/depth_anything&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;TensorRT: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://github.com/spacewalk01/depth-anything-tensorrt"&gt;https://github.com/spacewalk01/depth-anything-tensorrt&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/zhujiajian98/Depth-Anythingv2-TensorRT-python"&gt;https://github.com/zhujiajian98/Depth-Anythingv2-TensorRT-python&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;ONNX: &lt;a href="https://github.com/fabio-sim/Depth-Anything-ONNX"&gt;https://github.com/fabio-sim/Depth-Anything-ONNX&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ComfyUI: &lt;a href="https://github.com/kijai/ComfyUI-DepthAnythingV2"&gt;https://github.com/kijai/ComfyUI-DepthAnythingV2&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Transformers.js (real-time depth in web): &lt;a href="https://huggingface.co/spaces/Xenova/webgpu-realtime-depth-estimation"&gt;https://huggingface.co/spaces/Xenova/webgpu-realtime-depth-estimation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Android: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://github.com/shubham0204/Depth-Anything-Android"&gt;https://github.com/shubham0204/Depth-Anything-Android&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/FeiGeChuanShu/ncnn-android-depth_anything"&gt;https://github.com/FeiGeChuanShu/ncnn-android-depth_anything&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Acknowledgement&lt;/h2&gt; 
&lt;p&gt;We are sincerely grateful to the awesome Hugging Face team (&lt;a href="https://huggingface.co/pcuenq"&gt;@Pedro Cuenca&lt;/a&gt;, &lt;a href="https://huggingface.co/nielsr"&gt;@Niels Rogge&lt;/a&gt;, &lt;a href="https://huggingface.co/merve"&gt;@Merve Noyan&lt;/a&gt;, &lt;a href="https://huggingface.co/amyeroberts"&gt;@Amy Roberts&lt;/a&gt;, et al.) for their huge efforts in supporting our models in Transformers and Apple Core ML.&lt;/p&gt; 
&lt;p&gt;We also thank the &lt;a href="https://github.com/facebookresearch/dinov2"&gt;DINOv2&lt;/a&gt; team for contributing such impressive models to our community.&lt;/p&gt; 
&lt;h2&gt;LICENSE&lt;/h2&gt; 
&lt;p&gt;Depth-Anything-V2-Small model is under the Apache-2.0 license. Depth-Anything-V2-Base/Large/Giant models are under the CC-BY-NC-4.0 license.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you find this project useful, please consider citing:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@article{depth_anything_v2,
  title={Depth Anything V2},
  author={Yang, Lihe and Kang, Bingyi and Huang, Zilong and Zhao, Zhen and Xu, Xiaogang and Feng, Jiashi and Zhao, Hengshuang},
  journal={arXiv:2406.09414},
  year={2024}
}

@inproceedings{depth_anything_v1,
  title={Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data}, 
  author={Yang, Lihe and Kang, Bingyi and Huang, Zilong and Xu, Xiaogang and Feng, Jiashi and Zhao, Hengshuang},
  booktitle={CVPR},
  year={2024}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>apecloud/ApeRAG</title>
      <link>https://github.com/apecloud/ApeRAG</link>
      <description>&lt;p&gt;ApeRAG: Production-ready GraphRAG with multi-modal indexing, AI agents, MCP support, and scalable K8s deployment&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ApeRAG&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://archestra.ai/mcp-catalog/apecloud__aperag"&gt;&lt;img src="https://archestra.ai/mcp-catalog/api/badge/quality/apecloud/ApeRAG" alt="Trust Score" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;🚀 &lt;a href="https://rag.apecloud.com/"&gt;Try ApeRAG Live Demo&lt;/a&gt;&lt;/strong&gt; - Experience the full platform capabilities with our hosted demo&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/apecloud/ApeRAG/main/docs%2Fimages%2FHarryPotterKG2.png" alt="HarryPotterKG2.png" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/apecloud/ApeRAG/main/docs%2Fimages%2Fchat2.png" alt="chat2.png" /&gt;&lt;/p&gt; 
&lt;p&gt;ApeRAG is a production-ready RAG (Retrieval-Augmented Generation) platform that combines Graph RAG, vector search, and full-text search with advanced AI agents. Build sophisticated AI applications with hybrid retrieval, multimodal document processing, intelligent agents, and enterprise-grade management features.&lt;/p&gt; 
&lt;p&gt;ApeRAG is the best choice for building your own Knowledge Graph, Context Engineering, and deploying intelligent AI agents that can autonomously search and reason across your knowledge base.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/apecloud/ApeRAG/main/README-zh.md"&gt;阅读中文文档&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apecloud/ApeRAG/main/#quick-start"&gt;Quick Start&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apecloud/ApeRAG/main/#key-features"&gt;Key Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apecloud/ApeRAG/main/#kubernetes-deployment-recommended-for-production"&gt;Kubernetes Deployment (Recommended for Production)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apecloud/ApeRAG/main/docs/development-guide.md"&gt;Development&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apecloud/ApeRAG/main/docs/build-docker-image.md"&gt;Build Docker Image&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apecloud/ApeRAG/main/#acknowledgments"&gt;Acknowledgments&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apecloud/ApeRAG/main/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Before installing ApeRAG, make sure your machine meets the following minimum system requirements:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;CPU &amp;gt;= 2 Core&lt;/li&gt; 
  &lt;li&gt;RAM &amp;gt;= 4 GiB&lt;/li&gt; 
  &lt;li&gt;Docker &amp;amp; Docker Compose&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;The easiest way to start ApeRAG is through Docker Compose. Before running the following commands, make sure that &lt;a href="https://docs.docker.com/get-docker/"&gt;Docker&lt;/a&gt; and &lt;a href="https://docs.docker.com/compose/install/"&gt;Docker Compose&lt;/a&gt; are installed on your machine:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/apecloud/ApeRAG.git
cd ApeRAG
cp envs/env.template .env
docker-compose up -d --pull always
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After running, you can access ApeRAG in your browser at:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Web Interface&lt;/strong&gt;: &lt;a href="http://localhost:3000/web/"&gt;http://localhost:3000/web/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;API Documentation&lt;/strong&gt;: &lt;a href="http://localhost:8000/docs"&gt;http://localhost:8000/docs&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;MCP (Model Context Protocol) Support&lt;/h4&gt; 
&lt;p&gt;ApeRAG supports &lt;a href="https://modelcontextprotocol.io/"&gt;MCP (Model Context Protocol)&lt;/a&gt; integration, allowing AI assistants to interact with your knowledge base directly. After starting the services, configure your MCP client with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "aperag-mcp": {
      "url": "https://rag.apecloud.com/mcp",
      "headers": {
        "Authorization": "Bearer your-api-key-here"
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: Replace &lt;code&gt;http://localhost:8000&lt;/code&gt; with your actual ApeRAG API URL and &lt;code&gt;your-api-key-here&lt;/code&gt; with a valid API key from your ApeRAG settings.&lt;/p&gt; 
&lt;p&gt;The MCP server provides:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Collection browsing&lt;/strong&gt;: List and explore your knowledge collections&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Hybrid search&lt;/strong&gt;: Search using vector, full-text, and graph methods&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Intelligent querying&lt;/strong&gt;: Ask natural language questions about your documents&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Enhanced Document Parsing&lt;/h4&gt; 
&lt;p&gt;For enhanced document parsing capabilities, ApeRAG supports an &lt;strong&gt;advanced document parsing service&lt;/strong&gt; powered by MinerU, which provides superior parsing for complex documents, tables, and formulas.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Enhanced Document Parsing Commands&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Enable advanced document parsing service
DOCRAY_HOST=http://aperag-docray:8639 docker compose --profile docray up -d

# Enable advanced parsing with GPU acceleration 
DOCRAY_HOST=http://aperag-docray-gpu:8639 docker compose --profile docray-gpu up -d
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Or use the Makefile shortcuts (requires &lt;a href="https://www.gnu.org/software/make/"&gt;GNU Make&lt;/a&gt;):&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Enable advanced document parsing service
make compose-up WITH_DOCRAY=1

# Enable advanced parsing with GPU acceleration (recommended)
make compose-up WITH_DOCRAY=1 WITH_GPU=1
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h4&gt;Development &amp;amp; Contributing&lt;/h4&gt; 
&lt;p&gt;For developers interested in source code development, advanced configurations, or contributing to ApeRAG, please refer to our &lt;a href="https://raw.githubusercontent.com/apecloud/ApeRAG/main/docs/development-guide.md"&gt;Development Guide&lt;/a&gt; for detailed setup instructions.&lt;/p&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;1. Advanced Index Types&lt;/strong&gt;: Five comprehensive index types for optimal retrieval: &lt;strong&gt;Vector&lt;/strong&gt;, &lt;strong&gt;Full-text&lt;/strong&gt;, &lt;strong&gt;Graph&lt;/strong&gt;, &lt;strong&gt;Summary&lt;/strong&gt;, and &lt;strong&gt;Vision&lt;/strong&gt; - providing multi-dimensional document understanding and search capabilities.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;2. Intelligent AI Agents&lt;/strong&gt;: Built-in AI agents with MCP (Model Context Protocol) tool support that can automatically identify relevant collections, search content intelligently, and provide web search capabilities for comprehensive question answering.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;3. Enhanced Graph RAG with Entity Normalization&lt;/strong&gt;: Deeply modified LightRAG implementation with advanced entity normalization (entity merging) for cleaner knowledge graphs and improved relational understanding.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;4. Multimodal Processing &amp;amp; Vision Support&lt;/strong&gt;: Complete multimodal document processing including vision capabilities for images, charts, and visual content analysis alongside traditional text processing.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;5. Hybrid Retrieval Engine&lt;/strong&gt;: Sophisticated retrieval system combining Graph RAG, vector search, full-text search, summary-based retrieval, and vision-based search for comprehensive document understanding.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;6. MinerU Integration&lt;/strong&gt;: Advanced document parsing service powered by MinerU technology, providing superior parsing for complex documents, tables, formulas, and scientific content with optional GPU acceleration.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;7. Production-Grade Deployment&lt;/strong&gt;: Full Kubernetes support with Helm charts and KubeBlocks integration for simplified deployment of production-grade databases (PostgreSQL, Redis, Qdrant, Elasticsearch, Neo4j).&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;8. Enterprise Management&lt;/strong&gt;: Built-in audit logging, LLM model management, graph visualization, comprehensive document management interface, and agent workflow management.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;9. MCP Integration&lt;/strong&gt;: Full support for Model Context Protocol (MCP), enabling seamless integration with AI assistants and tools for direct knowledge base access and intelligent querying.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;10. Developer Friendly&lt;/strong&gt;: FastAPI backend, React frontend, async task processing with Celery, extensive testing, comprehensive development guides, and agent development framework for easy contribution and customization.&lt;/p&gt; 
&lt;h2&gt;Kubernetes Deployment (Recommended for Production)&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Enterprise-grade deployment with high availability and scalability&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Deploy ApeRAG to Kubernetes using our provided Helm chart. This approach offers high availability, scalability, and production-grade management capabilities.&lt;/p&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://kubernetes.io/docs/setup/"&gt;Kubernetes cluster&lt;/a&gt; (v1.20+)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://kubernetes.io/docs/tasks/tools/"&gt;&lt;code&gt;kubectl&lt;/code&gt;&lt;/a&gt; configured and connected to your cluster&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://helm.sh/docs/intro/install/"&gt;Helm v3+&lt;/a&gt; installed&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Clone the Repository&lt;/h3&gt; 
&lt;p&gt;First, clone the ApeRAG repository to get the deployment files:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/apecloud/ApeRAG.git
cd ApeRAG
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Step 1: Deploy Database Services&lt;/h3&gt; 
&lt;p&gt;ApeRAG requires PostgreSQL, Redis, Qdrant, and Elasticsearch. You have two options:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Option A: Use existing databases&lt;/strong&gt; - If you already have these databases running in your cluster, edit &lt;code&gt;deploy/aperag/values.yaml&lt;/code&gt; to configure your database connection details, then skip to Step 2.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Option B: Deploy databases with KubeBlocks&lt;/strong&gt; - Use our automated database deployment (database connections are pre-configured):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Navigate to database deployment scripts
cd deploy/databases/

# (Optional) Review configuration - defaults work for most cases
# edit 00-config.sh

# Install KubeBlocks and deploy databases
bash ./01-prepare.sh          # Installs KubeBlocks
bash ./02-install-database.sh # Deploys PostgreSQL, Redis, Qdrant, Elasticsearch

# Monitor database deployment
kubectl get pods -n default

# Return to project root for Step 2
cd ../../
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Wait for all database pods to be in &lt;code&gt;Running&lt;/code&gt; status before proceeding.&lt;/p&gt; 
&lt;h3&gt;Step 2: Deploy ApeRAG Application&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# If you deployed databases with KubeBlocks in Step 1, database connections are pre-configured
# If you're using existing databases, edit deploy/aperag/values.yaml with your connection details

# Deploy ApeRAG
helm install aperag ./deploy/aperag --namespace default --create-namespace

# Monitor ApeRAG deployment
kubectl get pods -n default -l app.kubernetes.io/instance=aperag
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Configuration Options&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Resource Requirements&lt;/strong&gt;: By default, includes &lt;a href="https://github.com/apecloud/doc-ray"&gt;&lt;code&gt;doc-ray&lt;/code&gt;&lt;/a&gt; service (requires 4+ CPU cores, 8GB+ RAM). To disable: set &lt;code&gt;docray.enabled: false&lt;/code&gt; in &lt;code&gt;values.yaml&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Advanced Settings&lt;/strong&gt;: Review &lt;code&gt;values.yaml&lt;/code&gt; for additional configuration options including images, resources, and Ingress settings.&lt;/p&gt; 
&lt;h3&gt;Access Your Deployment&lt;/h3&gt; 
&lt;p&gt;Once deployed, access ApeRAG using port forwarding:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Forward ports for quick access
kubectl port-forward svc/aperag-frontend 3000:3000 -n default
kubectl port-forward svc/aperag-api 8000:8000 -n default

# Access in browser
# Web Interface: http://localhost:3000
# API Documentation: http://localhost:8000/docs
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For production environments, configure Ingress in &lt;code&gt;values.yaml&lt;/code&gt; for external access.&lt;/p&gt; 
&lt;h3&gt;Troubleshooting&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Database Issues&lt;/strong&gt;: See &lt;code&gt;deploy/databases/README.md&lt;/code&gt; for KubeBlocks management, credentials, and uninstall procedures.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Pod Status&lt;/strong&gt;: Check pod logs for any deployment issues:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;kubectl logs -f deployment/aperag-api -n default
kubectl logs -f deployment/aperag-frontend -n default
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Acknowledgments&lt;/h2&gt; 
&lt;p&gt;ApeRAG integrates and builds upon several excellent open-source projects:&lt;/p&gt; 
&lt;h3&gt;LightRAG&lt;/h3&gt; 
&lt;p&gt;The graph-based knowledge retrieval capabilities in ApeRAG are powered by a deeply modified version of &lt;a href="https://github.com/HKUDS/LightRAG"&gt;LightRAG&lt;/a&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Paper&lt;/strong&gt;: "LightRAG: Simple and Fast Retrieval-Augmented Generation" (&lt;a href="https://arxiv.org/abs/2410.05779"&gt;arXiv:2410.05779&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Authors&lt;/strong&gt;: Zirui Guo, Lianghao Xia, Yanhua Yu, Tu Ao, Chao Huang&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;License&lt;/strong&gt;: MIT License&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We have extensively modified LightRAG to support production-grade concurrent processing, distributed task queues (Celery/Prefect), and stateless operations. See our &lt;a href="https://raw.githubusercontent.com/apecloud/ApeRAG/main/aperag/graph/changelog.md"&gt;LightRAG modifications changelog&lt;/a&gt; for details.&lt;/p&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://discord.gg/FsKpXukFuB"&gt;Discord&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apecloud/ApeRAG/main/docs%2Fimages%2Ffeishu-qr-code.png"&gt;Feishu&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;img src="https://raw.githubusercontent.com/apecloud/ApeRAG/main/docs/images/feishu-qr-code.png" alt="Feishu" width="150" /&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/apecloud/ApeRAG/main/docs%2Fimages%2Fstar-history-202595.png" alt="star-history-202595.png" /&gt;&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;ApeRAG is licensed under the Apache License 2.0. See the &lt;a href="https://raw.githubusercontent.com/apecloud/ApeRAG/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>wagtail/wagtail</title>
      <link>https://github.com/wagtail/wagtail</link>
      <description>&lt;p&gt;A Django content management system focused on flexibility and user experience&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset=".github/wagtail.svg" /&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset=".github/wagtail-inverse.svg" /&gt; 
  &lt;img width="343" src="https://raw.githubusercontent.com/wagtail/wagtail/main/.github/wagtail.svg?sanitize=true" alt="Wagtail" /&gt; 
 &lt;/picture&gt; &lt;/h1&gt; 
&lt;p align="center"&gt; &lt;br /&gt; &lt;a href="https://github.com/wagtail/wagtail/actions"&gt; &lt;img src="https://github.com/wagtail/wagtail/workflows/Wagtail%20CI/badge.svg?sanitize=true" alt="Build Status" /&gt; &lt;/a&gt; &lt;a href="https://opensource.org/licenses/BSD-3-Clause"&gt; &lt;img src="https://img.shields.io/badge/license-BSD-blue.svg?sanitize=true" alt="License" /&gt; &lt;/a&gt; &lt;a href="https://pypi.python.org/pypi/wagtail/"&gt; &lt;img src="https://img.shields.io/pypi/v/wagtail.svg?sanitize=true" alt="Version" /&gt; &lt;/a&gt; &lt;a href="https://pypi.python.org/pypi/wagtail/"&gt; &lt;img src="https://img.shields.io/pypi/dm/wagtail?logo=Downloads" alt="Monthly downloads" /&gt; &lt;/a&gt; &lt;a href="https://fosstodon.org/@wagtail"&gt; &lt;img src="https://img.shields.io/mastodon/follow/109308882653647818?domain=https%3A%2F%2Ffosstodon.org&amp;amp;style=social" alt="Follow @wagtail@fosstodon.org" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;Wagtail is an open source content management system built on Django, with a strong community and commercial support. It's focused on user experience, and offers precise control for designers and developers.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://cdn.jsdelivr.net/gh/wagtail/wagtail@main/.github/wagtail-screenshot-with-browser.png" alt="Wagtail screenshot" /&gt;&lt;/p&gt; 
&lt;h3&gt;🔥 Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;A fast, attractive interface for authors&lt;/li&gt; 
 &lt;li&gt;Complete control over front-end design and structure&lt;/li&gt; 
 &lt;li&gt;Scales to millions of pages and thousands of editors&lt;/li&gt; 
 &lt;li&gt;Fast out of the box, cache-friendly when you need it&lt;/li&gt; 
 &lt;li&gt;Content API for 'headless' sites with decoupled front-end&lt;/li&gt; 
 &lt;li&gt;Runs on a Raspberry Pi or a multi-datacenter cloud platform&lt;/li&gt; 
 &lt;li&gt;StreamField encourages flexible content without compromising structure&lt;/li&gt; 
 &lt;li&gt;Powerful, integrated search, using Elasticsearch or PostgreSQL&lt;/li&gt; 
 &lt;li&gt;Excellent support for images and embedded content&lt;/li&gt; 
 &lt;li&gt;Multi-site and multi-language ready&lt;/li&gt; 
 &lt;li&gt;Embraces and extends Django&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Find out more at &lt;a href="https://wagtail.org/"&gt;wagtail.org&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;👉 Getting started&lt;/h3&gt; 
&lt;p&gt;Wagtail works with &lt;a href="https://www.python.org/downloads/"&gt;Python 3&lt;/a&gt;, on any platform.&lt;/p&gt; 
&lt;p&gt;To get started with using Wagtail, run the following in a &lt;a href="https://docs.python.org/3/tutorial/venv.html"&gt;virtual environment&lt;/a&gt;:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/wagtail/wagtail/main/.github/install-animation.gif" alt="Installing Wagtail" /&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip install wagtail
wagtail start mysite
cd mysite
pip install -r requirements.txt
python manage.py migrate
python manage.py createsuperuser
python manage.py runserver
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For detailed installation and setup docs, see &lt;a href="https://docs.wagtail.org/en/stable/getting_started/tutorial.html"&gt;the getting started tutorial&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;👨‍👩‍👧‍👦 Who’s using it?&lt;/h3&gt; 
&lt;p&gt;Wagtail is used by &lt;a href="https://www.nasa.gov/"&gt;NASA&lt;/a&gt;, &lt;a href="https://www.google.com/"&gt;Google&lt;/a&gt;, &lt;a href="https://www.oxfam.org/en"&gt;Oxfam&lt;/a&gt;, the &lt;a href="https://www.nhs.uk/"&gt;NHS&lt;/a&gt;, &lt;a href="https://www.mozilla.org/en-US/"&gt;Mozilla&lt;/a&gt;, &lt;a href="https://www.mit.edu/"&gt;MIT&lt;/a&gt;, the &lt;a href="https://www.icrc.org/en"&gt;Red Cross&lt;/a&gt;, &lt;a href="https://www.salesforce.com/"&gt;Salesforce&lt;/a&gt;, &lt;a href="https://www.nbc.com/"&gt;NBC&lt;/a&gt;, &lt;a href="https://www.bmw.com/en/index.html"&gt;BMW&lt;/a&gt;, and the US and UK governments. Add your own Wagtail site to &lt;a href="https://madewithwagtail.org"&gt;madewithwagtail.org&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;📖 Documentation&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://docs.wagtail.org/"&gt;docs.wagtail.org&lt;/a&gt; is the full reference for Wagtail, and includes guides for developers, designers and editors, alongside &lt;a href="https://docs.wagtail.org/en/stable/releases/"&gt;release notes&lt;/a&gt; and our &lt;a href="https://wagtail.org/roadmap/"&gt;roadmap&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For those who are &lt;strong&gt;new to Wagtail&lt;/strong&gt;, the &lt;a href="https://docs.wagtail.org/en/stable/getting_started/the_zen_of_wagtail.html"&gt;Zen of Wagtail&lt;/a&gt; will help you understand what Wagtail is, and what Wagtail is &lt;em&gt;not&lt;/em&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;For developers&lt;/strong&gt; who are ready to jump in to their first Wagtail website the &lt;a href="https://docs.wagtail.org/en/stable/getting_started/tutorial.html"&gt;Getting Started Tutorial&lt;/a&gt; will guide you through creating and editing your first page.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Do you have an existing Django project?&lt;/strong&gt; The &lt;a href="https://docs.wagtail.org/en/stable/getting_started/integrating_into_django.html"&gt;Wagtail Integration documentation&lt;/a&gt; is the best place to start.&lt;/p&gt; 
&lt;h3&gt;📌 Compatibility&lt;/h3&gt; 
&lt;p&gt;&lt;em&gt;(If you are reading this on GitHub, the details here may not be indicative of the current released version - please see &lt;a href="https://docs.wagtail.org/en/stable/releases/upgrading.html#compatible-django-python-versions"&gt;Compatible Django / Python versions&lt;/a&gt; in the Wagtail documentation.)&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;Wagtail supports:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Django 4.2.x and 5.1.x&lt;/li&gt; 
 &lt;li&gt;Python 3.10, 3.11, 3.12 and 3.13&lt;/li&gt; 
 &lt;li&gt;PostgreSQL, MySQL, MariaDB and SQLite (with JSON1) as database backends&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://docs.wagtail.org/en/stable/releases/upgrading.html#compatible-django-python-versions"&gt;Previous versions of Wagtail&lt;/a&gt; additionally supported Python 2.7, 3.8 and earlier Django versions.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;📢 Community Support&lt;/h3&gt; 
&lt;p&gt;There is an active community of Wagtail users and developers responding to questions on &lt;a href="https://stackoverflow.com/questions/tagged/wagtail"&gt;Stack Overflow&lt;/a&gt;. When posting questions, please read Stack Overflow's advice on &lt;a href="https://stackoverflow.com/help/how-to-ask"&gt;how to ask questions&lt;/a&gt; and remember to tag your question "wagtail".&lt;/p&gt; 
&lt;p&gt;For topics and discussions that do not fit Stack Overflow's question and answer format we have a &lt;a href="https://github.com/wagtail/wagtail/wiki/Slack"&gt;Slack workspace&lt;/a&gt;. Please respect the time and effort of volunteers by not asking the same question in multiple places.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/wagtail/wagtail/wiki/Slack"&gt;&lt;img src="https://raw.githubusercontent.com/wagtail/wagtail/main/.github/join-slack-community.png" alt="Join slack community" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Our &lt;a href="https://github.com/wagtail/wagtail/discussions"&gt;GitHub discussion boards&lt;/a&gt; are open for sharing ideas and plans for the Wagtail project.&lt;/p&gt; 
&lt;p&gt;We maintain a curated list of third party packages, articles and other resources at &lt;a href="https://github.com/springload/awesome-wagtail"&gt;Awesome Wagtail&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;🧑‍💼 Commercial Support&lt;/h3&gt; 
&lt;p&gt;Wagtail is sponsored by &lt;a href="https://torchbox.com/"&gt;Torchbox&lt;/a&gt;. If you need help implementing or hosting Wagtail, please contact us: &lt;a href="mailto:hello@torchbox.com"&gt;hello@torchbox.com&lt;/a&gt;. See also &lt;a href="https://madewithwagtail.org/developers/"&gt;madewithwagtail.org/developers/&lt;/a&gt; for expert Wagtail developers around the world.&lt;/p&gt; 
&lt;h3&gt;🔐 Security&lt;/h3&gt; 
&lt;p&gt;We take the security of Wagtail, and related packages we maintain, seriously. If you have found a security issue with any of our projects please email us at &lt;a href="mailto:security@wagtail.org"&gt;security@wagtail.org&lt;/a&gt; so we can work together to find and patch the issue. We appreciate responsible disclosure with any security related issues, so please contact us first before creating a GitHub issue.&lt;/p&gt; 
&lt;p&gt;If you want to send an encrypted email (optional), the public key ID for &lt;a href="mailto:security@wagtail.org"&gt;security@wagtail.org&lt;/a&gt; is 0xbed227b4daf93ff9, and this public key is available from most commonly-used keyservers.&lt;/p&gt; 
&lt;h3&gt;🕒 Release schedule&lt;/h3&gt; 
&lt;p&gt;Feature releases of Wagtail are released every three months. Selected releases are designated as Long Term Support (LTS) releases, and will receive maintenance updates for an extended period to address any security and data-loss related issues. For dates of past and upcoming releases and support periods, see &lt;a href="https://github.com/wagtail/wagtail/wiki/Release-schedule"&gt;Release Schedule&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;🕛 Nightly releases&lt;/h4&gt; 
&lt;p&gt;To try out the latest features before a release, we also create builds from &lt;code&gt;main&lt;/code&gt; every night. You can find instructions on how to install the latest nightly release at &lt;a href="https://releases.wagtail.org/nightly/index.html"&gt;https://releases.wagtail.org/nightly/index.html&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;🙋🏽 Contributing&lt;/h3&gt; 
&lt;p&gt;If you're a Python or Django developer, fork the repo and get stuck in! We have several developer focused channels on the &lt;a href="https://github.com/wagtail/wagtail/wiki/Slack"&gt;Slack workspace&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You might like to start by reviewing the &lt;a href="https://docs.wagtail.org/en/latest/contributing/index.html"&gt;contributing guidelines&lt;/a&gt; and checking issues with the &lt;a href="https://github.com/wagtail/wagtail/labels/good%20first%20issue"&gt;good first issue&lt;/a&gt; label.&lt;/p&gt; 
&lt;p&gt;We also welcome translations for Wagtail's interface. Translation work should be submitted through &lt;a href="https://explore.transifex.com/torchbox/wagtail/"&gt;Transifex&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;🔓 License&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/wagtail/wagtail/raw/main/LICENSE"&gt;BSD&lt;/a&gt; - Free to use and modify for any purpose, including both open and closed-source code.&lt;/p&gt; 
&lt;h3&gt;👏 Thanks&lt;/h3&gt; 
&lt;p&gt;We thank the following organisations for their services used in Wagtail's development:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.browserstack.com/"&gt;&lt;img src="https://cdn.jsdelivr.net/gh/wagtail/wagtail@main/.github/browserstack-logo.svg?sanitize=true" alt="Browserstack" /&gt;&lt;/a&gt;&lt;br /&gt; &lt;a href="https://www.browserstack.com/"&gt;BrowserStack&lt;/a&gt; provides the project with free access to their live web-based browser testing tool, and automated Selenium cloud testing.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://assistivlabs.com/"&gt;&lt;img src="https://cdn.jsdelivr.net/gh/wagtail/wagtail@main/.github/assistivlabs-logo.png" alt="Assistiv Labs" /&gt;&lt;/a&gt;&lt;br /&gt; &lt;a href="https://assistivlabs.com/"&gt;Assistiv Labs&lt;/a&gt; provides the project with unlimited access to their remote testing with assistive technologies.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>