<rss version="2.0">
  <channel>
    <title>GitHub Python Monthly Trending</title>
    <description>Monthly Trending of Python in GitHub</description>
    <pubDate>Sat, 13 Sep 2025 01:53:08 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>HunxByts/GhostTrack</title>
      <link>https://github.com/HunxByts/GhostTrack</link>
      <description>&lt;p&gt;Useful tool to track location or mobile number&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;GhostTrack&lt;/h1&gt; 
&lt;p&gt;Useful tool to track location or mobile number, so this tool can be called osint or also information gathering&lt;/p&gt; 
&lt;img src="https://github.com/HunxByts/GhostTrack/raw/main/asset/bn.png" /&gt; 
&lt;p&gt;New update : &lt;code&gt;Version 2.2&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Instalation on Linux (deb)&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;sudo apt-get install git
sudo apt-get install python3
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Instalation on Termux&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;pkg install git
pkg install python3
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Usage Tool&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;git clone https://github.com/HunxByts/GhostTrack.git
cd GhostTrack
pip3 install -r requirements.txt
python3 GhostTR.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Display on the menu &lt;code&gt;IP Tracker&lt;/code&gt;&lt;/p&gt; 
&lt;img src="https://github.com/HunxByts/GhostTrack/blob/main/asset/ip.png " /&gt; 
&lt;p&gt;on the IP Track menu, you can combo with the seeker tool to get the target IP&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;span&gt;âš¡&lt;/span&gt; Install Seeker :&lt;/summary&gt; - 
 &lt;strong&gt;&lt;a href="https://github.com/thewhiteh4t/seeker"&gt;Get Seeker&lt;/a&gt;&lt;/strong&gt; 
&lt;/details&gt; 
&lt;p&gt;Display on the menu &lt;code&gt;Phone Tracker&lt;/code&gt;&lt;/p&gt; 
&lt;img src="https://github.com/HunxByts/GhostTrack/raw/main/asset/phone.png" /&gt; 
&lt;p&gt;on this menu you can search for information from the target phone number&lt;/p&gt; 
&lt;p&gt;Display on the menu &lt;code&gt;Username Tracker&lt;/code&gt;&lt;/p&gt; 
&lt;img src="https://github.com/HunxByts/GhostTrack/raw/main/asset/User.png" /&gt; on this menu you can search for information from the target username on social media 
&lt;details&gt; 
 &lt;summary&gt;&lt;span&gt;âš¡&lt;/span&gt; Author :&lt;/summary&gt; - 
 &lt;strong&gt;&lt;a href="https://github.com/HunxByts"&gt;HunxByts&lt;/a&gt;&lt;/strong&gt; 
&lt;/details&gt;</description>
    </item>
    
    <item>
      <title>Shubhamsaboo/awesome-llm-apps</title>
      <link>https://github.com/Shubhamsaboo/awesome-llm-apps</link>
      <description>&lt;p&gt;Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="http://www.theunwindai.com"&gt; &lt;img src="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/unwind_black.png" width="900px" alt="Unwind AI" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://www.linkedin.com/in/shubhamsaboo/"&gt; &lt;img src="https://img.shields.io/badge/-Follow%20Shubham%20Saboo-blue?logo=linkedin&amp;amp;style=flat-square" alt="LinkedIn" /&gt; &lt;/a&gt; &lt;a href="https://twitter.com/Saboo_Shubham_"&gt; &lt;img src="https://img.shields.io/twitter/follow/Shubham_Saboo" alt="Twitter" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; 
 &lt;!-- Keep these links. Translations will automatically update with the README. --&gt; &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=de"&gt;Deutsch&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=es"&gt;EspaÃ±ol&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=fr"&gt;franÃ§ais&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ja"&gt;æ—¥æœ¬èª&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ko"&gt;í•œêµ­ì–´&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=pt"&gt;PortuguÃªs&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ru"&gt;Ğ ÑƒÑÑĞºĞ¸Ğ¹&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=zh"&gt;ä¸­æ–‡&lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h1&gt;ğŸŒŸ Awesome LLM Apps&lt;/h1&gt; 
&lt;p&gt;A curated collection of &lt;strong&gt;Awesome LLM apps built with RAG, AI Agents, Multi-agent Teams, MCP, Voice Agents, and more.&lt;/strong&gt; This repository features LLM apps that use models from &lt;img src="https://cdn.simpleicons.org/openai" alt="openai logo" width="25" height="15" /&gt;&lt;strong&gt;OpenAI&lt;/strong&gt; , &lt;img src="https://cdn.simpleicons.org/anthropic" alt="anthropic logo" width="25" height="15" /&gt;&lt;strong&gt;Anthropic&lt;/strong&gt;, &lt;img src="https://cdn.simpleicons.org/googlegemini" alt="google logo" width="25" height="18" /&gt;&lt;strong&gt;Google&lt;/strong&gt;, &lt;img src="https://cdn.simpleicons.org/x" alt="X logo" width="25" height="15" /&gt;&lt;strong&gt;xAI&lt;/strong&gt; and open-source models like &lt;img src="https://cdn.simpleicons.org/alibabacloud" alt="alibaba logo" width="25" height="15" /&gt;&lt;strong&gt;Qwen&lt;/strong&gt; or &lt;img src="https://cdn.simpleicons.org/meta" alt="meta logo" width="25" height="15" /&gt;&lt;strong&gt;Llama&lt;/strong&gt; that you can run locally on your computer.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/9876" target="_blank"&gt; &lt;img src="https://trendshift.io/api/badge/repositories/9876" alt="Shubhamsaboo%2Fawesome-llm-apps | Trendshift" style="width: 250px; height: 55px;" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;ğŸ¤” Why Awesome LLM Apps?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ’¡ Discover practical and creative ways LLMs can be applied across different domains, from code repositories to email inboxes and more.&lt;/li&gt; 
 &lt;li&gt;ğŸ”¥ Explore apps that combine LLMs from OpenAI, Anthropic, Gemini, and open-source alternatives with AI Agents, Agent Teams, MCP &amp;amp; RAG.&lt;/li&gt; 
 &lt;li&gt;ğŸ“ Learn from well-documented projects and contribute to the growing open-source ecosystem of LLM-powered applications.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ™ Thanks to our sponsors&lt;/h2&gt; 
&lt;table align="center" cellpadding="16" cellspacing="12"&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td align="center"&gt; &lt;a href="https://getunblocked.com/unblocked-mcp/?utm_source=oss&amp;amp;utm_medium=sponsorship&amp;amp;utm_campaign=awesome-llm-apps" target="_blank" rel="noopener" title="Unblocked"&gt; &lt;img src="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/sponsors/unblocked.png" alt="Unblocked" width="6000" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;a href="https://getunblocked.com/unblocked-mcp/?utm_source=oss&amp;amp;utm_medium=sponsorship&amp;amp;utm_campaign=awesome-llm-apps" target="_blank" rel="noopener" style="text-decoration: none; color: #333; font-weight: bold; font-size: 18px;"&gt; Unblocked &lt;/a&gt; &lt;/td&gt; 
   &lt;td align="center"&gt; &lt;a href="https://sponsorunwindai.com/" title="Sponsor Awesome LLM Apps Repo"&gt; &lt;img src="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/sponsor_awesome_llm_apps.png" alt="Sponsor Awesome LLM Apps Repo" width="6000" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;a href="https://sponsorunwindai.com/" style="text-decoration: none; color: #333; font-weight: bold; font-size: 18px;"&gt; Become a Sponsor &lt;/a&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;ğŸ“‚ Featured AI Projects&lt;/h2&gt; 
&lt;h3&gt;AI Agents&lt;/h3&gt; 
&lt;h3&gt;ğŸŒ± Starter AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_blog_to_podcast_agent/"&gt;ğŸ™ï¸ AI Blog to Podcast Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_breakup_recovery_agent/"&gt;â¤ï¸â€ğŸ©¹ AI Breakup Recovery Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_data_analysis_agent/"&gt;ğŸ“Š AI Data Analysis Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_medical_imaging_agent/"&gt;ğŸ©» AI Medical Imaging Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_meme_generator_agent_browseruse/"&gt;ğŸ˜‚ AI Meme Generator Agent (Browser)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_music_generator_agent/"&gt;ğŸµ AI Music Generator Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_travel_agent/"&gt;ğŸ›« AI Travel Agent (Local &amp;amp; Cloud)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/gemini_multimodal_agent_demo/"&gt;âœ¨ Gemini Multimodal Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/local_news_agent_openai_swarm/"&gt;ğŸŒ Local News Agent (OpenAI Swarm)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/mixture_of_agents/"&gt;ğŸ”„ Mixture of Agents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/xai_finance_agent/"&gt;ğŸ“Š xAI Finance Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/opeani_research_agent/"&gt;ğŸ” OpenAI Research Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/web_scrapping_ai_agent/"&gt;ğŸ•¸ï¸ Web Scrapping AI Agent (Local &amp;amp; Cloud)&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸš€ Advanced AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_deep_research_agent/"&gt;ğŸ” AI Deep Research Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_consultant_agent"&gt;ğŸ¤ AI Consultant Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_system_architect_r1/"&gt;ğŸ—ï¸ AI System Architect Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_lead_generation_agent/"&gt;ğŸ¯ AI Lead Generation Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_financial_coach_agent/"&gt;ğŸ’° AI Financial Coach Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_movie_production_agent/"&gt;ğŸ¬ AI Movie Production Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_investment_agent/"&gt;ğŸ“ˆ AI Investment Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_health_fitness_agent/"&gt;ğŸ‹ï¸â€â™‚ï¸ AI Health &amp;amp; Fitness Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/product_launch_intelligence_agent"&gt;ğŸš€ AI Product Launch Intelligence Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_journalist_agent/"&gt;ğŸ—ï¸ AI Journalist Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_mental_wellbeing_agent/"&gt;ğŸ§  AI Mental Wellbeing Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_meeting_agent/"&gt;ğŸ“‘ AI Meeting Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_Self-Evolving_agent/"&gt;ğŸ§¬ AI Self-Evolving Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_news_and_podcast_agents/"&gt;ğŸ§ AI Social Media News and Podcast Agent&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ® Autonomous Game Playing Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/autonomous_game_playing_agent_apps/ai_3dpygame_r1/"&gt;ğŸ® AI 3D Pygame Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/autonomous_game_playing_agent_apps/ai_chess_agent/"&gt;â™œ AI Chess Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/autonomous_game_playing_agent_apps/ai_tic_tac_toe_agent/"&gt;ğŸ² AI Tic-Tac-Toe Agent&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ¤ Multi-agent Teams&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_competitor_intelligence_agent_team/"&gt;ğŸ§² AI Competitor Intelligence Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_finance_agent_team/"&gt;ğŸ’² AI Finance Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_game_design_agent_team/"&gt;ğŸ¨ AI Game Design Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_legal_agent_team/"&gt;ğŸ‘¨â€âš–ï¸ AI Legal Agent Team (Cloud &amp;amp; Local)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_recruitment_agent_team/"&gt;ğŸ’¼ AI Recruitment Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_real_estate_agent_team"&gt;ğŸ  AI Real Estate Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_services_agency/"&gt;ğŸ‘¨â€ğŸ’¼ AI Services Agency (CrewAI)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_teaching_agent_team/"&gt;ğŸ‘¨â€ğŸ« AI Teaching Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_coding_agent_team/"&gt;ğŸ’» Multimodal Coding Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_design_agent_team/"&gt;âœ¨ Multimodal Design Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_travel_planner_agent_team/"&gt;ğŸŒ AI Travel Planner Agent Team&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ—£ï¸ Voice AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/voice_ai_agents/ai_audio_tour_agent/"&gt;ğŸ—£ï¸ AI Audio Tour Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/voice_ai_agents/customer_support_voice_agent/"&gt;ğŸ“ Customer Support Voice Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/voice_ai_agents/voice_rag_openaisdk/"&gt;ğŸ”Š Voice RAG Agent (OpenAI SDK)&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;img src="https://cdn.simpleicons.org/modelcontextprotocol" alt="mcp logo" width="25" height="20" /&gt; MCP AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/mcp_ai_agents/browser_mcp_agent/"&gt;â™¾ï¸ Browser MCP Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/mcp_ai_agents/github_mcp_agent/"&gt;ğŸ™ GitHub MCP Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/mcp_ai_agents/notion_mcp_agent"&gt;ğŸ“‘ Notion MCP Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/mcp_ai_agents/ai_travel_planner_mcp_agent_team"&gt;ğŸŒ AI Travel Planner MCP Agent&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ“€ RAG (Retrieval Augmented Generation)&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/agentic_rag_embedding_gemma"&gt;ğŸ”¥ Agentic RAG with Embedding Gemma&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/agentic_rag_with_reasoning/"&gt;ğŸ§ Agentic RAG with Reasoning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/ai_blog_search/"&gt;ğŸ“° AI Blog Search (RAG)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/autonomous_rag/"&gt;ğŸ” Autonomous RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/contextualai_rag_agent/"&gt;ğŸ”„ Contextual AI RAG Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/corrective_rag/"&gt;ğŸ”„ Corrective RAG (CRAG)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/deepseek_local_rag_agent/"&gt;ğŸ‹ Deepseek Local RAG Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/gemini_agentic_rag/"&gt;ğŸ¤” Gemini Agentic RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/hybrid_search_rag/"&gt;ğŸ‘€ Hybrid Search RAG (Cloud)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/llama3.1_local_rag/"&gt;ğŸ”„ Llama 3.1 Local RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/local_hybrid_search_rag/"&gt;ğŸ–¥ï¸ Local Hybrid Search RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/local_rag_agent/"&gt;ğŸ¦™ Local RAG Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/rag-as-a-service/"&gt;ğŸ§© RAG-as-a-Service&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/rag_agent_cohere/"&gt;âœ¨ RAG Agent with Cohere&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/rag_chain/"&gt;â›“ï¸ Basic RAG Chain&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/rag_database_routing/"&gt;ğŸ“  RAG with Database Routing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/vision_rag/"&gt;ğŸ–¼ï¸ Vision RAG&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ’¾ LLM Apps with Memory Tutorials&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/ai_arxiv_agent_memory/"&gt;ğŸ’¾ AI ArXiv Agent with Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/ai_travel_agent_memory/"&gt;ğŸ›©ï¸ AI Travel Agent with Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/llama3_stateful_chat/"&gt;ğŸ’¬ Llama3 Stateful Chat&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/llm_app_personalized_memory/"&gt;ğŸ“ LLM App with Personalized Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/local_chatgpt_with_memory/"&gt;ğŸ—„ï¸ Local ChatGPT Clone with Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/multi_llm_memory/"&gt;ğŸ§  Multi-LLM Application with Shared Memory&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ’¬ Chat with X Tutorials&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_github/"&gt;ğŸ’¬ Chat with GitHub (GPT &amp;amp; Llama3)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_gmail/"&gt;ğŸ“¨ Chat with Gmail&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_pdf/"&gt;ğŸ“„ Chat with PDF (GPT &amp;amp; Llama3)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_research_papers/"&gt;ğŸ“š Chat with Research Papers (ArXiv) (GPT &amp;amp; Llama3)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_substack/"&gt;ğŸ“ Chat with Substack&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_youtube_videos/"&gt;ğŸ“½ï¸ Chat with YouTube Videos&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ”§ LLM Fine-tuning Tutorials&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;img src="https://cdn.simpleicons.org/google" alt="google logo" width="20" height="15" /&gt; &lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_finetuning_tutorials/gemma3_finetuning/"&gt;Gemma 3 Fine-tuning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;img src="https://cdn.simpleicons.org/meta" alt="meta logo" width="25" height="15" /&gt; &lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_finetuning_tutorials/llama3.2_finetuning/"&gt;Llama 3.2 Fine-tuning&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ§‘â€ğŸ« AI Agent Framework Crash Course&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://cdn.simpleicons.org/google" alt="google logo" width="25" height="15" /&gt; &lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/ai_agent_framework_crash_course/google_adk_crash_course/"&gt;Google ADK Crash Course&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Starter agent; modelâ€‘agnostic (OpenAI, Claude)&lt;/li&gt; 
 &lt;li&gt;Structured outputs (Pydantic)&lt;/li&gt; 
 &lt;li&gt;Tools: builtâ€‘in, function, thirdâ€‘party, MCP tools&lt;/li&gt; 
 &lt;li&gt;Memory; callbacks; Plugins&lt;/li&gt; 
 &lt;li&gt;Simple multiâ€‘agent; Multiâ€‘agent patterns&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://cdn.simpleicons.org/openai" alt="openai logo" width="25" height="15" /&gt; &lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/ai_agent_framework_crash_course/openai_sdk_crash_course/"&gt;OpenAI Agents SDK Crash Course&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Starter agent; function calling; structured outputs&lt;/li&gt; 
 &lt;li&gt;Tools: builtâ€‘in, function, thirdâ€‘party integrations&lt;/li&gt; 
 &lt;li&gt;Memory; callbacks; evaluation&lt;/li&gt; 
 &lt;li&gt;Multiâ€‘agent patterns; agent handoffs&lt;/li&gt; 
 &lt;li&gt;Swarm orchestration; routing logic&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸš€ Getting Started&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Clone the repository&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/Shubhamsaboo/awesome-llm-apps.git 
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Navigate to the desired project directory&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cd awesome-llm-apps/starter_ai_agents/ai_travel_agent
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install the required dependencies&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Follow the project-specific instructions&lt;/strong&gt; in each project's &lt;code&gt;README.md&lt;/code&gt; file to set up and run the app.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;ğŸ¤ Contributing to Open Source&lt;/h2&gt; 
&lt;p&gt;Contributions are welcome! If you have any ideas, improvements, or new apps to add, please create a new &lt;a href="https://github.com/Shubhamsaboo/awesome-llm-apps/issues"&gt;GitHub Issue&lt;/a&gt; or submit a pull request. Make sure to follow the existing project structure and include a detailed &lt;code&gt;README.md&lt;/code&gt; for each new app.&lt;/p&gt; 
&lt;h3&gt;&lt;img src="https://cdn.simpleicons.org/github" alt="github logo" width="25" height="20" /&gt; Thank You, Community, for the Support! ğŸ™&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#Shubhamsaboo/awesome-llm-apps&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=Shubhamsaboo/awesome-llm-apps&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;ğŸŒŸ &lt;strong&gt;Donâ€™t miss out on future updates! Star the repo now and be the first to know about new and exciting LLM apps with RAG and AI Agents.&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>willccbb/verifiers</title>
      <link>https://github.com/willccbb/verifiers</link>
      <description>&lt;p&gt;Verifiers for LLM Reinforcement Learning&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p align="center"&gt; &lt;/p&gt;
 &lt;h1&gt;Verifiers&lt;/h1&gt; 
 &lt;p&gt;&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;a href="https://github.com/willccbb/verifiers/actions/workflows/style.yml"&gt; &lt;img src="https://github.com/willccbb/verifiers/actions/workflows/style.yml/badge.svg?sanitize=true" alt="Style" /&gt; &lt;/a&gt; &lt;a href="https://github.com/willccbb/verifiers/actions/workflows/test.yml"&gt; &lt;img src="https://github.com/willccbb/verifiers/actions/workflows/test.yml/badge.svg?sanitize=true" alt="Test" /&gt; &lt;/a&gt; &lt;a href="https://github.com/willccbb/verifiers/actions/workflows/publish-environments.yml"&gt; &lt;img src="https://github.com/willccbb/verifiers/actions/workflows/publish-environments.yml/badge.svg?sanitize=true" alt="Envs" /&gt; &lt;/a&gt; &lt;/p&gt; 
 &lt;p&gt; Environments for LLM Reinforcement Learning &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;Overview&lt;/h2&gt; 
&lt;p&gt;Verifiers is a library of modular components for creating RL environments and training LLM agents. Environments built with Verifiers can be used directly as LLM evaluations, synthetic data pipelines, or agent harnesses for any OpenAI-compatible model endpoint, in addition to RL training. Verifiers includes an async GRPO implementation built around the &lt;code&gt;transformers&lt;/code&gt; Trainer, is supported by &lt;code&gt;prime-rl&lt;/code&gt; for large-scale FSDP training, and can easily be integrated into any RL framework which exposes an OpenAI-compatible inference client.&lt;/p&gt; 
&lt;p&gt;Full documentation is available &lt;a href="https://verifiers.readthedocs.io/en/latest/"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Setup&lt;/h2&gt; 
&lt;p&gt;We recommend using &lt;code&gt;verifiers&lt;/code&gt; with along &lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;uv&lt;/a&gt; for dependency management in your own project:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# install uv (first time only)
curl -LsSf https://astral.sh/uv/install.sh | sh
# create a fresh project -- 3.11 + 3.12 supported
uv init &amp;amp;&amp;amp; uv venv --python 3.12 
source .venv/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For local (CPU) development and evaluation with API models, do:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv add verifiers # uv add 'verifiers[dev]' for Jupyter + testing support
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For training on GPUs with &lt;code&gt;vf.GRPOTrainer&lt;/code&gt;, do:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv add 'verifiers[all]' &amp;amp;&amp;amp; uv pip install flash-attn --no-build-isolation
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To use the latest &lt;code&gt;main&lt;/code&gt; branch, do:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv add verifiers @ git+https://github.com/willccbb/verifiers.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To use with &lt;code&gt;prime-rl&lt;/code&gt;, see &lt;a href="https://github.com/PrimeIntellect-ai/prime-rl"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To install &lt;code&gt;verifiers&lt;/code&gt; from source for core library development, do:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/willccbb/verifiers.git
cd verifiers

# for CPU-only dev:
uv sync --extra dev

# or, for trainer dev:
uv sync --all-extras &amp;amp;&amp;amp; uv pip install flash-attn --no-build-isolation

# install pre-commit hooks
uv run pre-commit install
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In general, we recommend that you build and train Environments &lt;em&gt;with&lt;/em&gt; &lt;code&gt;verifiers&lt;/code&gt;, not &lt;em&gt;in&lt;/em&gt; &lt;code&gt;verifiers&lt;/code&gt;. If you find yourself needing to clone and modify the core library in order to implement key functionality for your project, we'd love for you to open an issue so that we can try and streamline the development experience. Our aim is for &lt;code&gt;verifiers&lt;/code&gt; to be a reliable toolkit to build on top of, and to minimize the "fork proliferation" which often pervades the RL infrastructure ecosystem.&lt;/p&gt; 
&lt;h2&gt;Environments&lt;/h2&gt; 
&lt;p&gt;Environments in Verifiers are installable Python modules which can specify dependencies in a &lt;code&gt;pyproject.toml&lt;/code&gt;, and which expose a &lt;code&gt;load_environment&lt;/code&gt; function for instantiation by downstream applications (e.g. trainers). See &lt;code&gt;environments/&lt;/code&gt; for examples.&lt;/p&gt; 
&lt;p&gt;To initialize a blank Environment module template, do:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;vf-init vf-environment-name # -p /path/to/environments (defaults to "./environments")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To an install an Environment module into your project, do:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;vf-install vf-environment-name # -p /path/to/environments (defaults to "./environments") 
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To install an Environment module from this repo's &lt;code&gt;environments&lt;/code&gt; folder, do:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;vf-install vf-math-python --from-repo # -b branch_or_commit (defaults to "main")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Once an Environment module is installed, you can create an instance of the Environment using &lt;code&gt;load_environment&lt;/code&gt;, passing any necessary args:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import verifiers as vf
vf_env = vf.load_environment("vf-environment-name", **env_args)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To run a quick evaluation of your Environment with an API-based model, do:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;vf-eval vf-environment-name -s # run and save eval results locally
# vf-eval -h for config options; defaults to gpt-4.1-mini, 5 prompts, 3 rollouts for each
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The core elements of Environments are:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Datasets: a Hugging Face &lt;code&gt;Dataset&lt;/code&gt; with a &lt;code&gt;prompt&lt;/code&gt; column for inputs, and optionally &lt;code&gt;answer (str)&lt;/code&gt; or &lt;code&gt;info (dict)&lt;/code&gt; columns for evaluation (both can be omitted for environments that evaluate based solely on completion quality)&lt;/li&gt; 
 &lt;li&gt;Rollout logic: interactions between models and the environment (e.g. &lt;code&gt;env_response&lt;/code&gt; + &lt;code&gt;is_completed&lt;/code&gt; for any &lt;code&gt;MultiTurnEnv&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Rubrics: an encapsulation for one or more reward functions&lt;/li&gt; 
 &lt;li&gt;Parsers: optional; an encapsulation for reusable parsing logic&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We support both &lt;code&gt;/v1/chat/completions&lt;/code&gt;-style and &lt;code&gt;/v1/completions&lt;/code&gt;-style inference via OpenAI clients, though we generally recommend &lt;code&gt;/v1/chat/completions&lt;/code&gt;-style inference for the vast majority of applications. Both the included &lt;code&gt;GRPOTrainer&lt;/code&gt; as well as &lt;code&gt;prime-rl&lt;/code&gt; support the full set of &lt;a href="https://docs.vllm.ai/en/v0.6.0/dev/sampling_params.html"&gt;SamplingParams&lt;/a&gt; exposed by vLLM (via their OpenAI-compatible &lt;a href="https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html"&gt;server&lt;/a&gt; interface), and leveraging this will often be the appropriate way to implement rollout strategies requiring finer-grained control, such as interrupting and resuming generations for interleaved tool use, or enforcing reasoning budgets.&lt;/p&gt; 
&lt;p&gt;The primary constraint we impose on rollout logic is that token sequences must be &lt;em&gt;increasing&lt;/em&gt;, i.e. once a token has been added to a model's context in a rollout, it must remain as the rollout progresses. Note that this causes issues with some popular reasoning models such as the Qwen3 and DeepSeek-R1-Distill series; see &lt;a href="https://raw.githubusercontent.com/willccbb/verifiers/main/#footguns"&gt;Footguns&lt;/a&gt; for guidance on adapting these models to support multi-turn rollouts.&lt;/p&gt; 
&lt;h3&gt;SingleTurnEnv&lt;/h3&gt; 
&lt;p&gt;For tasks requiring only a single response from a model for each prompt, you can use &lt;code&gt;SingleTurnEnv&lt;/code&gt; directly by specifying a Dataset and a Rubric. Rubrics are sets of reward functions, which can be either sync or async.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from datasets import load_dataset
import verifiers as vf

dataset = load_dataset("my-account/my-dataset", split="train")

def reward_A(prompt, completion, info) -&amp;gt; float:
	# reward fn, e.g. correctness
	...

def reward_B(parser, completion) -&amp;gt; float:
	# auxiliary reward fn, e.g. format
	...

async def metric(completion) -&amp;gt; float:
	# non-reward metric, e.g. proper noun count
	...

rubric = vf.Rubric(funcs=[reward_A, reward_B, metric], weights=[1.0, 0.5, 0.0])

vf_env = SingleTurnEnv(
	dataset=dataset,
	rubric=rubric
)
results = vf_env.evaluate(client=OpenAI(), model="gpt-4.1-mini", num_examples=100, rollouts_per_example=1)
vf_env.make_dataset(results) # HF dataset format
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Datasets should be formatted with columns for:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;'prompt' (List[ChatMessage])&lt;/code&gt; OR &lt;code&gt;'question' (str)&lt;/code&gt; fields 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;ChatMessage&lt;/code&gt; = e.g. &lt;code&gt;{'role': 'user', 'content': '...'}&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;if &lt;code&gt;question&lt;/code&gt; is set instead of &lt;code&gt;prompt&lt;/code&gt;, you can also pass &lt;code&gt;system_prompt (str)&lt;/code&gt; and/or &lt;code&gt;few_shot (List[ChatMessage])&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;answer (str)&lt;/code&gt; AND/OR &lt;code&gt;info (dict)&lt;/code&gt; (both optional, can be omitted entirely)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;task (str)&lt;/code&gt;: optional, used by &lt;code&gt;EnvGroup&lt;/code&gt; and &lt;code&gt;RubricGroup&lt;/code&gt; for orchestrating composition of Environments and Rubrics&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The following named attributes available for use by reward functions in your Rubric:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;prompt&lt;/code&gt;: sequence of input messages&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;completion&lt;/code&gt;: sequence of messages generated during rollout by model and Environment&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;answer&lt;/code&gt;: primary answer column, optional (defaults to empty string if omitted)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;state&lt;/code&gt;: can be modified during rollout to accumulate any metadata (&lt;code&gt;state['responses']&lt;/code&gt; includes full OpenAI response objects by default)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;info&lt;/code&gt;: auxiliary info needed for reward computation (e.g. test cases), optional (defaults to empty dict if omitted)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;task&lt;/code&gt;: tag for task type (used by &lt;code&gt;EnvGroup&lt;/code&gt; and &lt;code&gt;RubricGroup&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;parser&lt;/code&gt;: the parser object declared. Note: &lt;code&gt;vf.Parser().get_format_reward_func()&lt;/code&gt; is a no-op (always 1.0); use &lt;code&gt;vf.ThinkParser&lt;/code&gt; or a custom parser if you want a real format adherence reward.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Some environments can fully evaluate using only &lt;code&gt;prompt&lt;/code&gt;, &lt;code&gt;completion&lt;/code&gt;, and &lt;code&gt;state&lt;/code&gt; without requiring ground truth &lt;code&gt;answer&lt;/code&gt; or &lt;code&gt;info&lt;/code&gt; data. Examples include format compliance checking, completion quality assessment, or length-based rewards.&lt;/p&gt; 
&lt;p&gt;For tasks involving LLM judges, you may wish to use &lt;code&gt;vf.JudgeRubric()&lt;/code&gt; for managing requests to auxiliary models.&lt;/p&gt; 
&lt;p&gt;Note on concurrency: environment APIs accept &lt;code&gt;max_concurrent&lt;/code&gt; to control parallel rollouts. The &lt;code&gt;vf-eval&lt;/code&gt; CLI currently exposes &lt;code&gt;--max-concurrent-requests&lt;/code&gt;; ensure this maps to your environmentâ€™s concurrency as expected.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;vf-eval&lt;/code&gt; also supports specifying &lt;code&gt;sampling_args&lt;/code&gt; as a JSON object, which is sent to the vLLM inference engine:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;vf-eval vf-environment-name --sampling-args '{"reasoning_effort": "low"}'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Use &lt;code&gt;vf-eval -s&lt;/code&gt; to save outputs as dataset-formatted JSON, and view all locally-saved eval results with &lt;code&gt;vf-tui&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;ToolEnv&lt;/h3&gt; 
&lt;p&gt;For many applications involving tool use, you can use &lt;code&gt;ToolEnv&lt;/code&gt; to leverage models' native tool/function-calling capabilities in an agentic loop. Tools can be specified as generic Python functions (with type hints and docstrings), which will then be passed in JSON schema form to each inference request.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import verifiers as vf
vf_env = vf.ToolEnv(
	dataset= ... # HF Dataset with 'prompt'/'question' and optionally 'answer'/'info' columns
	rubric= ... # Rubric object; vf.ToolRubric() can be optionally used for counting tool invocations in each rollout
	tools=[search_tool, read_article_tool, python_tool], # python functions with type hints + docstrings
	max_turns=10
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In cases where your tools require heavy computational resources, we recommend hosting your tools as standalone servers (e.g. MCP servers) and creating lightweight wrapper functions to pass to &lt;code&gt;ToolEnv&lt;/code&gt;. Parallel tool call support is enabled by default.&lt;/p&gt; 
&lt;p&gt;For training, or self-hosted endpoints, you'll want to enable auto tool choice in &lt;a href="https://docs.vllm.ai/en/stable/features/tool_calling.html#automatic-function-calling"&gt;vLLM&lt;/a&gt; with the appropriate parser. If your model does not support native tool calling, you may find the &lt;code&gt;XMLParser&lt;/code&gt; abstraction useful for rolling your own tool call parsing on top of &lt;code&gt;MultiTurnEnv&lt;/code&gt;; see &lt;code&gt;environments/xml_tool_env&lt;/code&gt; for an example.&lt;/p&gt; 
&lt;h3&gt;MultiTurnEnv&lt;/h3&gt; 
&lt;p&gt;Both &lt;code&gt;SingleTurnEnv&lt;/code&gt; and &lt;code&gt;ToolEnv&lt;/code&gt; are instances of &lt;code&gt;MultiTurnEnv&lt;/code&gt;, which exposes an interface for writing custom Environment interaction protocols. The two methods you must override are&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from typing import Tuple
import verifiers as vf
from verifiers.types import Messages, State
class YourMultiTurnEnv(vf.MultiTurnEnv):
    def __init__(self,
                 dataset: Dataset,
                 rubric: Rubric,
				 max_turns: int,
                 **kwargs):
	
  async def is_completed(self, messages: Messages, state: State, **kwargs) -&amp;gt; bool:
    # return whether or not a rollout is completed

  async def env_response(self, messages: Messages, state: State, **kwargs) -&amp;gt; Tuple[Messages, State]:
    # return new environment message(s) + updated state
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If your application requires more fine-grained control than is allowed by &lt;code&gt;MultiTurnEnv&lt;/code&gt;, you may want to inherit from the base &lt;code&gt;Environment&lt;/code&gt; functionality directly and override the &lt;code&gt;rollout&lt;/code&gt; method.&lt;/p&gt; 
&lt;h2&gt;Training&lt;/h2&gt; 
&lt;h3&gt;GRPOTrainer&lt;/h3&gt; 
&lt;p&gt;The included trainer (&lt;code&gt;vf.GRPOTrainer&lt;/code&gt;) supports running GRPO-style RL training via Accelerate/DeepSpeed, and uses vLLM for inference. It supports both full-parameter finetuning, and is optimized for efficiently training dense transformer models on 2-16 GPUs.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# install environment
vf-install vf-wordle (-p /path/to/environments | --from-repo)

# quick eval
vf-eval vf-wordle -m (model_name in configs/endpoints.py) -n NUM_EXAMPLES -r ROLLOUTS_PER_EXAMPLE

# inference (shell 0)
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 vf-vllm --model willcb/Qwen3-1.7B-Wordle \
    --data-parallel-size 7 --enforce-eager --disable-log-requests

# training (shell 1)
CUDA_VISIBLE_DEVICES=6,7 accelerate launch --num-processes 2 \
    --config-file configs/zero3.yaml examples/grpo/train_wordle.py --size 1.7B
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, you can train environments with the external &lt;code&gt;prime-rl&lt;/code&gt; project (FSDP-first orchestration). See the &lt;code&gt;prime-rl&lt;/code&gt; README for installation and examples. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-toml"&gt;# orchestrator config (prime-rl)
[environment]
id = "vf-math-python"  # or your environment ID
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# run (prime-rl)
uv run rl \
  --trainer @ configs/your_exp/train.toml \
  --orchestrator @ configs/your_exp/orch.toml \
  --inference @ configs/your_exp/infer.toml
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Troubleshooting&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Ensure your &lt;code&gt;wandb&lt;/code&gt; and &lt;code&gt;huggingface-cli&lt;/code&gt; logins are set up (or set &lt;code&gt;report_to=None&lt;/code&gt; in &lt;code&gt;training_args&lt;/code&gt;). You should also have something set as your &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; in your environment (can be a dummy key for vLLM).&lt;/li&gt; 
 &lt;li&gt;If using high max concurrency, increase the number of allowed open sockets (e.g. &lt;code&gt;ulimit -n 4096&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;On some setups, inter-GPU communication can &lt;a href="https://github.com/huggingface/trl/issues/2923"&gt;hang&lt;/a&gt; or crash during vLLM weight syncing. This can usually be alleviated by setting (or unsetting) &lt;code&gt;NCCL_P2P_DISABLE=1&lt;/code&gt; in your environment (or potentially &lt;code&gt;NCCL_CUMEM_ENABLE=1&lt;/code&gt;). Try this as your first step if you experience NCCL-related issues.&lt;/li&gt; 
 &lt;li&gt;If problems persist, please open an &lt;a href="https://github.com/willccbb/verifiers/issues"&gt;issue&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Resource Requirements&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;GRPOTrainer&lt;/code&gt; is optimized for setups with at least 2 GPUs, scaling up to multiple nodes. 2-GPU setups with sufficient memory to enable small-scale experimentation can be &lt;a href="https://app.primeintellect.ai/dashboard/create-cluster?image=ubuntu_22_cuda_12"&gt;rented&lt;/a&gt; for &amp;lt;$1/hr.&lt;/p&gt; 
&lt;h3&gt;PRIME-RL&lt;/h3&gt; 
&lt;p&gt;If you do not require LoRA support, you may want to use the &lt;code&gt;prime-rl&lt;/code&gt; trainer, which natively supports Environments created using &lt;code&gt;verifiers&lt;/code&gt;, is more optimized for performance and scalability via FSDP, includes a broader set of configuration options and user experience features, and has more battle-tested defaults. Both trainers support asynchronous rollouts, and use a one-step off-policy delay by default for overlapping training and inference. See the &lt;code&gt;prime-rl&lt;/code&gt; &lt;a href="https://github.com/PrimeIntellect-ai/prime-rl"&gt;docs&lt;/a&gt; for usage instructions.&lt;/p&gt; 
&lt;h2&gt;Further Documentation&lt;/h2&gt; 
&lt;p&gt;See the full &lt;a href="https://verifiers.readthedocs.io/en/latest/"&gt;docs&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;Contributions&lt;/h2&gt; 
&lt;p&gt;Verifiers warmly welcomes community contributions! Please open an issue or PR if you encounter bugs or other pain points during your development, or start a discussion for more open-ended questions.&lt;/p&gt; 
&lt;p&gt;Please note that the core &lt;code&gt;verifiers/&lt;/code&gt; library is intended to be a relatively lightweight set of reusable components rather than an exhaustive catalog of RL environments. For &lt;em&gt;applications&lt;/em&gt; of &lt;code&gt;verifiers&lt;/code&gt; (e.g. "an Environment for XYZ task"), you are welcome to submit a PR for a self-contained module that lives within &lt;code&gt;environments/&lt;/code&gt; if it serves as a canonical example of a new pattern. Stay tuned for more info shortly about our plans for supporting community Environment contributions ğŸ™‚&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you use this code in your research, please cite:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{brown_verifiers_2025,
  author       = {William&amp;nbsp;Brown},
  title        = {{Verifiers}: Reinforcement Learning with LLMs in Verifiable Environments},
  howpublished = {\url{https://github.com/willccbb/verifiers}},
  note         = {Commit abcdefg â€¢ accessed DDâ€¯Monâ€¯YYYY},
  year         = {2025}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Roadmap&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Default patterns for hosted resources such as code sandboxes, auxiliary models, and MCP servers&lt;/li&gt; 
 &lt;li&gt;Multimodal input support&lt;/li&gt; 
 &lt;li&gt;Non-increasing token sequences via REINFORCE&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>coleam00/Archon</title>
      <link>https://github.com/coleam00/Archon</link>
      <description>&lt;p&gt;Beta release of Archon OS - the knowledge and task management backbone for AI coding assistants.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/coleam00/Archon/main/archon-ui-main/public/archon-main-graphic.png" alt="Archon Main Graphic" width="853" height="422" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;em&gt;Power up your AI coding assistants with your own custom knowledge base and task management as an MCP server&lt;/em&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://raw.githubusercontent.com/coleam00/Archon/main/#quick-start"&gt;Quick Start&lt;/a&gt; â€¢ &lt;a href="https://raw.githubusercontent.com/coleam00/Archon/main/#upgrading"&gt;Upgrading&lt;/a&gt; â€¢ &lt;a href="https://raw.githubusercontent.com/coleam00/Archon/main/#whats-included"&gt;What's Included&lt;/a&gt; â€¢ &lt;a href="https://raw.githubusercontent.com/coleam00/Archon/main/#architecture"&gt;Architecture&lt;/a&gt; â€¢ &lt;a href="https://raw.githubusercontent.com/coleam00/Archon/main/#troubleshooting"&gt;Troubleshooting&lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ¯ What is Archon?&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Archon is currently in beta! Expect things to not work 100%, and please feel free to share any feedback and contribute with fixes/new features! Thank you to everyone for all the excitement we have for Archon already, as well as the bug reports, PRs, and discussions. It's a lot for our small team to get through but we're committed to addressing everything and making Archon into the best tool it possibly can be!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Archon is the &lt;strong&gt;command center&lt;/strong&gt; for AI coding assistants. For you, it's a sleek interface to manage knowledge, context, and tasks for your projects. For the AI coding assistant(s), it's a &lt;strong&gt;Model Context Protocol (MCP) server&lt;/strong&gt; to collaborate on and leverage the same knowledge, context, and tasks. Connect Claude Code, Kiro, Cursor, Windsurf, etc. to give your AI agents access to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Your documentation&lt;/strong&gt; (crawled websites, uploaded PDFs/docs)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Smart search capabilities&lt;/strong&gt; with advanced RAG strategies&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Task management&lt;/strong&gt; integrated with your knowledge base&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Real-time updates&lt;/strong&gt; as you add new content and collaborate with your coding assistant on tasks&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Much more&lt;/strong&gt; coming soon to build Archon into an integrated environment for all context engineering&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This new vision for Archon replaces the old one (the agenteer). Archon used to be the AI agent that builds other agents, and now you can use Archon to do that and more.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;It doesn't matter what you're building or if it's a new/existing codebase - Archon's knowledge and task management capabilities will improve the output of &lt;strong&gt;any&lt;/strong&gt; AI driven coding.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;ğŸ”— Important Links&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/coleam00/Archon/discussions"&gt;GitHub Discussions&lt;/a&gt;&lt;/strong&gt; - Join the conversation and share ideas about Archon&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/coleam00/Archon/main/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt;&lt;/strong&gt; - How to get involved and contribute to Archon&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://youtu.be/8pRc_s2VQIo"&gt;Introduction Video&lt;/a&gt;&lt;/strong&gt; - Getting started guide and vision for Archon&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/users/coleam00/projects/1"&gt;Archon Kanban Board&lt;/a&gt;&lt;/strong&gt; - Where maintainers are managing issues/features&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://dynamous.ai"&gt;Dynamous AI Mastery&lt;/a&gt;&lt;/strong&gt; - The birthplace of Archon - come join a vibrant community of other early AI adopters all helping each other transform their careers and businesses!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.docker.com/products/docker-desktop/"&gt;Docker Desktop&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nodejs.org/"&gt;Node.js 18+&lt;/a&gt; (for hybrid development mode)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://supabase.com/"&gt;Supabase&lt;/a&gt; account (free tier or local Supabase both work)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://platform.openai.com/api-keys"&gt;OpenAI API key&lt;/a&gt; (Gemini and Ollama are supported too!)&lt;/li&gt; 
 &lt;li&gt;(OPTIONAL) &lt;a href="https://www.gnu.org/software/make/"&gt;Make&lt;/a&gt; (see &lt;a href="https://raw.githubusercontent.com/coleam00/Archon/main/#installing-make"&gt;Installing Make&lt;/a&gt; below)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Setup Instructions&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Clone Repository&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/coleam00/archon.git
&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cd archon
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Environment Configuration&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cp .env.example .env
# Edit .env and add your Supabase credentials:
# SUPABASE_URL=https://your-project.supabase.co
# SUPABASE_SERVICE_KEY=your-service-key-here
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;IMPORTANT NOTES:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;For cloud Supabase: they recently introduced a new type of service role key but use the legacy one (the longer one).&lt;/li&gt; 
   &lt;li&gt;For local Supabase: set SUPABASE_URL to &lt;a href="http://host.docker.internal:8000"&gt;http://host.docker.internal:8000&lt;/a&gt; (unless you have an IP address set up).&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Database Setup&lt;/strong&gt;: In your &lt;a href="https://supabase.com/dashboard"&gt;Supabase project&lt;/a&gt; SQL Editor, copy, paste, and execute the contents of &lt;code&gt;migration/complete_setup.sql&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Start Services&lt;/strong&gt; (choose one):&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Full Docker Mode (Recommended for Normal Archon Usage)&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;docker compose up --build -d
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This starts all core microservices in Docker:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Server&lt;/strong&gt;: Core API and business logic (Port: 8181)&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;MCP Server&lt;/strong&gt;: Protocol interface for AI clients (Port: 8051)&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;UI&lt;/strong&gt;: Web interface (Port: 3737)&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;Ports are configurable in your .env as well!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Configure API Keys&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Open &lt;a href="http://localhost:3737"&gt;http://localhost:3737&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;You'll automatically be brought through an onboarding flow to set your API key (OpenAI is default)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;âš¡ Quick Test&lt;/h2&gt; 
&lt;p&gt;Once everything is running:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Test Web Crawling&lt;/strong&gt;: Go to &lt;a href="http://localhost:3737"&gt;http://localhost:3737&lt;/a&gt; â†’ Knowledge Base â†’ "Crawl Website" â†’ Enter a doc URL (such as &lt;a href="https://ai.pydantic.dev/llms-full.txt"&gt;https://ai.pydantic.dev/llms-full.txt&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Test Document Upload&lt;/strong&gt;: Knowledge Base â†’ Upload a PDF&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Test Projects&lt;/strong&gt;: Projects â†’ Create a new project and add tasks&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Integrate with your AI coding assistant&lt;/strong&gt;: MCP Dashboard â†’ Copy connection config for your AI coding assistant&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Installing Make&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ› ï¸ Make installation (OPTIONAL - For Dev Workflows)&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h3&gt;Windows&lt;/h3&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Option 1: Using Chocolatey
choco install make

# Option 2: Using Scoop
scoop install make

# Option 3: Using WSL2
wsl --install
# Then in WSL: sudo apt-get install make
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;macOS&lt;/h3&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Make comes pre-installed on macOS
# If needed: brew install make
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;Linux&lt;/h3&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Debian/Ubuntu
sudo apt-get install make

# RHEL/CentOS/Fedora
sudo yum install make
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸš€ Quick Command Reference for Make&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Command&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;make dev&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Start hybrid dev (backend in Docker, frontend local) â­&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;make dev-docker&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Everything in Docker&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;make stop&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Stop all services&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;make test&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Run all tests&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;make lint&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Run linters&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;make install&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Install dependencies&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;make check&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Check environment setup&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;make clean&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Remove containers and volumes (with confirmation)&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h2&gt;ğŸ”„ Database Reset (Start Fresh if Needed)&lt;/h2&gt; 
&lt;p&gt;If you need to completely reset your database and start fresh:&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;âš ï¸ &lt;strong&gt;Reset Database - This will delete ALL data for Archon!&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Run Reset Script&lt;/strong&gt;: In your Supabase SQL Editor, run the contents of &lt;code&gt;migration/RESET_DB.sql&lt;/code&gt;&lt;/p&gt; &lt;p&gt;âš ï¸ WARNING: This will delete all Archon specific tables and data! Nothing else will be touched in your DB though.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Rebuild Database&lt;/strong&gt;: After reset, run &lt;code&gt;migration/complete_setup.sql&lt;/code&gt; to create all the tables again.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Restart Services&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;docker compose --profile full up -d
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Reconfigure&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Select your LLM/embedding provider and set the API key again&lt;/li&gt; 
    &lt;li&gt;Re-upload any documents or re-crawl websites&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;The reset script safely removes all tables, functions, triggers, and policies with proper dependency handling.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;ğŸ“š Documentation&lt;/h2&gt; 
&lt;h3&gt;Core Services&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Service&lt;/th&gt; 
   &lt;th&gt;Container Name&lt;/th&gt; 
   &lt;th&gt;Default URL&lt;/th&gt; 
   &lt;th&gt;Purpose&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Web Interface&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;archon-ui&lt;/td&gt; 
   &lt;td&gt;&lt;a href="http://localhost:3737"&gt;http://localhost:3737&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Main dashboard and controls&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;API Service&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;archon-server&lt;/td&gt; 
   &lt;td&gt;&lt;a href="http://localhost:8181"&gt;http://localhost:8181&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Web crawling, document processing&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;MCP Server&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;archon-mcp&lt;/td&gt; 
   &lt;td&gt;&lt;a href="http://localhost:8051"&gt;http://localhost:8051&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Model Context Protocol interface&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Agents Service&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;archon-agents&lt;/td&gt; 
   &lt;td&gt;&lt;a href="http://localhost:8052"&gt;http://localhost:8052&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;AI/ML operations, reranking&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Upgrading&lt;/h2&gt; 
&lt;p&gt;To upgrade Archon to the latest version:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Pull latest changes&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git pull
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Check for migrations&lt;/strong&gt;: Look in the &lt;code&gt;migration/&lt;/code&gt; folder for any SQL files newer than your last update. Check the file created dates to determine if you need to run them. You can run these in the SQL editor just like you did when you first set up Archon. We are also working on a way to make handling these migrations automatic!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Rebuild and restart&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;docker compose up -d --build
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;This is the same command used for initial setup - it rebuilds containers with the latest code and restarts services.&lt;/p&gt; 
&lt;h2&gt;What's Included&lt;/h2&gt; 
&lt;h3&gt;ğŸ§  Knowledge Management&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Smart Web Crawling&lt;/strong&gt;: Automatically detects and crawls entire documentation sites, sitemaps, and individual pages&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Document Processing&lt;/strong&gt;: Upload and process PDFs, Word docs, markdown files, and text documents with intelligent chunking&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Code Example Extraction&lt;/strong&gt;: Automatically identifies and indexes code examples from documentation for enhanced search&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Vector Search&lt;/strong&gt;: Advanced semantic search with contextual embeddings for precise knowledge retrieval&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Source Management&lt;/strong&gt;: Organize knowledge by source, type, and tags for easy filtering&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ¤– AI Integration&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Model Context Protocol (MCP)&lt;/strong&gt;: Connect any MCP-compatible client (Claude Code, Cursor, even non-AI coding assistants like Claude Desktop)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MCP Tools&lt;/strong&gt;: Comprehensive yet simple set of tools for RAG queries, task management, and project operations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-LLM Support&lt;/strong&gt;: Works with OpenAI, Ollama, and Google Gemini models&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;RAG Strategies&lt;/strong&gt;: Hybrid search, contextual embeddings, and result reranking for optimal AI responses&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Real-time Streaming&lt;/strong&gt;: Live responses from AI agents with progress tracking&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ“‹ Project &amp;amp; Task Management&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Hierarchical Projects&lt;/strong&gt;: Organize work with projects, features, and tasks in a structured workflow&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;AI-Assisted Creation&lt;/strong&gt;: Generate project requirements and tasks using integrated AI agents&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Document Management&lt;/strong&gt;: Version-controlled documents with collaborative editing capabilities&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Progress Tracking&lt;/strong&gt;: Real-time updates and status management across all project activities&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ”„ Real-time Collaboration&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;WebSocket Updates&lt;/strong&gt;: Live progress tracking for crawling, processing, and AI operations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-user Support&lt;/strong&gt;: Collaborative knowledge building and project management&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Background Processing&lt;/strong&gt;: Asynchronous operations that don't block the user interface&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Health Monitoring&lt;/strong&gt;: Built-in service health checks and automatic reconnection&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Architecture&lt;/h2&gt; 
&lt;h3&gt;Microservices Structure&lt;/h3&gt; 
&lt;p&gt;Archon uses true microservices architecture with clear separation of concerns:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend UI   â”‚    â”‚  Server (API)   â”‚    â”‚   MCP Server    â”‚    â”‚ Agents Service  â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚  React + Vite   â”‚â—„â”€â”€â–ºâ”‚    FastAPI +    â”‚â—„â”€â”€â–ºâ”‚    Lightweight  â”‚â—„â”€â”€â–ºâ”‚   PydanticAI    â”‚
â”‚  Port 3737      â”‚    â”‚    SocketIO     â”‚    â”‚    HTTP Wrapper â”‚    â”‚   Port 8052     â”‚
â”‚                 â”‚    â”‚    Port 8181    â”‚    â”‚    Port 8051    â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                        â”‚                        â”‚                        â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚                        â”‚
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
                         â”‚    Database     â”‚               â”‚
                         â”‚                 â”‚               â”‚
                         â”‚    Supabase     â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚    PostgreSQL   â”‚
                         â”‚    PGVector     â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Service Responsibilities&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Service&lt;/th&gt; 
   &lt;th&gt;Location&lt;/th&gt; 
   &lt;th&gt;Purpose&lt;/th&gt; 
   &lt;th&gt;Key Features&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Frontend&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;archon-ui-main/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Web interface and dashboard&lt;/td&gt; 
   &lt;td&gt;React, TypeScript, TailwindCSS, Socket.IO client&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Server&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;python/src/server/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Core business logic and APIs&lt;/td&gt; 
   &lt;td&gt;FastAPI, service layer, Socket.IO broadcasts, all ML/AI operations&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;MCP Server&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;python/src/mcp/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;MCP protocol interface&lt;/td&gt; 
   &lt;td&gt;Lightweight HTTP wrapper, MCP tools, session management&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Agents&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;python/src/agents/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;PydanticAI agent hosting&lt;/td&gt; 
   &lt;td&gt;Document and RAG agents, streaming responses&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Communication Patterns&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;HTTP-based&lt;/strong&gt;: All inter-service communication uses HTTP APIs&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Socket.IO&lt;/strong&gt;: Real-time updates from Server to Frontend&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MCP Protocol&lt;/strong&gt;: AI clients connect to MCP Server via SSE or stdio&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;No Direct Imports&lt;/strong&gt;: Services are truly independent with no shared code dependencies&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Key Architectural Benefits&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Lightweight Containers&lt;/strong&gt;: Each service contains only required dependencies&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Independent Scaling&lt;/strong&gt;: Services can be scaled independently based on load&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Development Flexibility&lt;/strong&gt;: Teams can work on different services without conflicts&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Technology Diversity&lt;/strong&gt;: Each service uses the best tools for its specific purpose&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ”§ Configuring Custom Ports &amp;amp; Hostname&lt;/h2&gt; 
&lt;p&gt;By default, Archon services run on the following ports:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;archon-ui&lt;/strong&gt;: 3737&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;archon-server&lt;/strong&gt;: 8181&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;archon-mcp&lt;/strong&gt;: 8051&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;archon-agents&lt;/strong&gt;: 8052&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;archon-docs&lt;/strong&gt;: 3838 (optional)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Changing Ports&lt;/h3&gt; 
&lt;p&gt;To use custom ports, add these variables to your &lt;code&gt;.env&lt;/code&gt; file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Service Ports Configuration
ARCHON_UI_PORT=3737
ARCHON_SERVER_PORT=8181
ARCHON_MCP_PORT=8051
ARCHON_AGENTS_PORT=8052
ARCHON_DOCS_PORT=3838
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Example: Running on different ports:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;ARCHON_SERVER_PORT=8282
ARCHON_MCP_PORT=8151
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Configuring Hostname&lt;/h3&gt; 
&lt;p&gt;By default, Archon uses &lt;code&gt;localhost&lt;/code&gt; as the hostname. You can configure a custom hostname or IP address by setting the &lt;code&gt;HOST&lt;/code&gt; variable in your &lt;code&gt;.env&lt;/code&gt; file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Hostname Configuration
HOST=localhost  # Default

# Examples of custom hostnames:
HOST=192.168.1.100     # Use specific IP address
HOST=archon.local      # Use custom domain
HOST=myserver.com      # Use public domain
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This is useful when:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Running Archon on a different machine and accessing it remotely&lt;/li&gt; 
 &lt;li&gt;Using a custom domain name for your installation&lt;/li&gt; 
 &lt;li&gt;Deploying in a network environment where &lt;code&gt;localhost&lt;/code&gt; isn't accessible&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;After changing hostname or ports:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Restart Docker containers: &lt;code&gt;docker compose down &amp;amp;&amp;amp; docker compose --profile full up -d&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Access the UI at: &lt;code&gt;http://${HOST}:${ARCHON_UI_PORT}&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Update your AI client configuration with the new hostname and MCP port&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;ğŸ”§ Development&lt;/h2&gt; 
&lt;h3&gt;Quick Start&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install dependencies
make install

# Start development (recommended)
make dev        # Backend in Docker, frontend local with hot reload

# Alternative: Everything in Docker
make dev-docker # All services in Docker

# Stop everything (local FE needs to be stopped manually)
make stop
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Development Modes&lt;/h3&gt; 
&lt;h4&gt;Hybrid Mode (Recommended) - &lt;code&gt;make dev&lt;/code&gt;&lt;/h4&gt; 
&lt;p&gt;Best for active development with instant frontend updates:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Backend services run in Docker (isolated, consistent)&lt;/li&gt; 
 &lt;li&gt;Frontend runs locally with hot module replacement&lt;/li&gt; 
 &lt;li&gt;Instant UI updates without Docker rebuilds&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Full Docker Mode - &lt;code&gt;make dev-docker&lt;/code&gt;&lt;/h4&gt; 
&lt;p&gt;For all services in Docker environment:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;All services run in Docker containers&lt;/li&gt; 
 &lt;li&gt;Better for integration testing&lt;/li&gt; 
 &lt;li&gt;Slower frontend updates&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Testing &amp;amp; Code Quality&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run tests
make test       # Run all tests
make test-fe    # Run frontend tests
make test-be    # Run backend tests

# Run linters
make lint       # Lint all code
make lint-fe    # Lint frontend code
make lint-be    # Lint backend code

# Check environment
make check      # Verify environment setup

# Clean up
make clean      # Remove containers and volumes (asks for confirmation)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Viewing Logs&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# View logs using Docker Compose directly
docker compose logs -f              # All services
docker compose logs -f archon-server # API server
docker compose logs -f archon-mcp    # MCP server
docker compose logs -f archon-ui     # Frontend
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The backend services are configured with &lt;code&gt;--reload&lt;/code&gt; flag in their uvicorn commands and have source code mounted as volumes for automatic hot reloading when you make changes.&lt;/p&gt; 
&lt;h2&gt;Troubleshooting&lt;/h2&gt; 
&lt;h3&gt;Common Issues and Solutions&lt;/h3&gt; 
&lt;h4&gt;Port Conflicts&lt;/h4&gt; 
&lt;p&gt;If you see "Port already in use" errors:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Check what's using a port (e.g., 3737)
lsof -i :3737

# Stop all containers and local services
make stop

# Change the port in .env
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Docker Permission Issues (Linux)&lt;/h4&gt; 
&lt;p&gt;If you encounter permission errors with Docker:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Add your user to the docker group
sudo usermod -aG docker $USER

# Log out and back in, or run
newgrp docker
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Windows-Specific Issues&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Make not found&lt;/strong&gt;: Install Make via Chocolatey, Scoop, or WSL2 (see &lt;a href="https://raw.githubusercontent.com/coleam00/Archon/main/#installing-make"&gt;Installing Make&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Line ending issues&lt;/strong&gt;: Configure Git to use LF endings: &lt;pre&gt;&lt;code class="language-bash"&gt;git config --global core.autocrlf false
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Frontend Can't Connect to Backend&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Check backend is running: &lt;code&gt;curl http://localhost:8181/health&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Verify port configuration in &lt;code&gt;.env&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;For custom ports, ensure both &lt;code&gt;ARCHON_SERVER_PORT&lt;/code&gt; and &lt;code&gt;VITE_ARCHON_SERVER_PORT&lt;/code&gt; are set&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Docker Compose Hangs&lt;/h4&gt; 
&lt;p&gt;If &lt;code&gt;docker compose&lt;/code&gt; commands hang:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Reset Docker Compose
docker compose down --remove-orphans
docker system prune -f

# Restart Docker Desktop (if applicable)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Hot Reload Not Working&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Frontend&lt;/strong&gt;: Ensure you're running in hybrid mode (&lt;code&gt;make dev&lt;/code&gt;) for best HMR experience&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Backend&lt;/strong&gt;: Check that volumes are mounted correctly in &lt;code&gt;docker-compose.yml&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;File permissions&lt;/strong&gt;: On some systems, mounted volumes may have permission issues&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ“ˆ Progress&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;a href="https://star-history.com/#coleam00/Archon&amp;amp;Date"&gt; &lt;img src="https://api.star-history.com/svg?repos=coleam00/Archon&amp;amp;type=Date" width="500" alt="Star History Chart" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;ğŸ“„ License&lt;/h2&gt; 
&lt;p&gt;Archon Community License (ACL) v1.2 - see &lt;a href="https://raw.githubusercontent.com/coleam00/Archon/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: Archon is free, open, and hackable. Run it, fork it, share it - just don't sell it as-a-service without permission.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>aliasrobotics/cai</title>
      <link>https://github.com/aliasrobotics/cai</link>
      <description>&lt;p&gt;Cybersecurity AI (CAI), the framework for AI Security&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Cybersecurity AI (&lt;code&gt;CAI&lt;/code&gt;)&lt;/h1&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;a align="center" href="" target="https://github.com/aliasrobotics/CAI"&gt; &lt;img width="100%" src="https://github.com/aliasrobotics/cai/raw/main/media/cai.png" /&gt; &lt;/a&gt; &lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://badge.fury.io/py/cai-framework"&gt;&lt;img src="https://badge.fury.io/py/cai-framework.svg?sanitize=true" alt="version" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/projects/cai-framework"&gt;&lt;img src="https://static.pepy.tech/badge/cai-framework" alt="downloads" /&gt;&lt;/a&gt; &lt;a href="https://github.com/aliasrobotics/cai"&gt;&lt;img src="https://img.shields.io/badge/Linux-Supported-brightgreen?logo=linux&amp;amp;logoColor=white" alt="Linux" /&gt;&lt;/a&gt; &lt;a href="https://github.com/aliasrobotics/cai"&gt;&lt;img src="https://img.shields.io/badge/OS%20X-Supported-brightgreen?logo=apple&amp;amp;logoColor=white" alt="OS X" /&gt;&lt;/a&gt; &lt;a href="https://github.com/aliasrobotics/cai"&gt;&lt;img src="https://img.shields.io/badge/Windows-Supported-brightgreen?logo=windows&amp;amp;logoColor=white" alt="Windows" /&gt;&lt;/a&gt; &lt;a href="https://github.com/aliasrobotics/cai"&gt;&lt;img src="https://img.shields.io/badge/Android-Supported-brightgreen?logo=android&amp;amp;logoColor=white" alt="Android" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/fnUFcTaQAC"&gt;&lt;img src="https://img.shields.io/badge/Discord-7289DA?logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/pdf/2504.06017"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2504.06017-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2506.23592"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2506.23592-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2508.13588"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2508.13588-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2508.21669"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2508.21669-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;Cybersecurity AI (CAI) is a lightweight, open-source framework that empowers security professionals to build and deploy AI-powered offensive and defensive automation. CAI is the &lt;em&gt;de facto&lt;/em&gt; framework for AI Security, already used by thousands of individual users and hundreds of organizations. Whether you're a security researcher, ethical hacker, IT professional, or organization looking to enhance your security posture, CAI provides the building blocks to create specialized AI agents that can assist with mitigation, vulnerability discovery, exploitation, and security assessment.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ¤– &lt;strong&gt;300+ AI Models&lt;/strong&gt;: Support for OpenAI, Anthropic, DeepSeek, Ollama, and more&lt;/li&gt; 
 &lt;li&gt;ğŸ”§ &lt;strong&gt;Built-in Security Tools&lt;/strong&gt;: Ready-to-use tools for reconnaissance, exploitation, and privilege escalation&lt;/li&gt; 
 &lt;li&gt;ğŸ† &lt;strong&gt;Battle-tested&lt;/strong&gt;: Proven in HackTheBox CTFs, bug bounties, and real-world security &lt;a href="https://aliasrobotics.com/case-studies-robot-cybersecurity.php"&gt;case studies&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ¯ &lt;strong&gt;Agent-based Architecture&lt;/strong&gt;: Modular framework design to build specialized agents for different security tasks&lt;/li&gt; 
 &lt;li&gt;ğŸ›¡ï¸ &lt;strong&gt;Guardrails Protection&lt;/strong&gt;: Built-in defenses against prompt injection and dangerous command execution&lt;/li&gt; 
 &lt;li&gt;ğŸ“š &lt;strong&gt;Research-oriented&lt;/strong&gt;: Research foundation to democratize cybersecurity AI for the community&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Read the technical report: &lt;a href="https://arxiv.org/pdf/2504.06017"&gt;CAI: An Open, Bug Bounty-Ready Cybersecurity AI&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;For further readings, refer to our &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-impact"&gt;impact&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#citation"&gt;CAI citation&lt;/a&gt; sections.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;a href="https://aliasrobotics.com/case-study-ecoforest.php"&gt;&lt;code&gt;OT&lt;/code&gt; - CAI and alias0 on: Ecoforest Heat Pumps&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://aliasrobotics.com/case-study-cai-mir.php"&gt;&lt;code&gt;Robotics&lt;/code&gt; - CAI and alias0 on: Mobile Industrial Robots (MiR)&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;CAI discovers critical vulnerability in Ecoforest heat pumps allowing unauthorized remote access and potential catastrophic failures. AI-powered security testing reveals exposed credentials and DES encryption weaknesses affecting all of their deployed units across Europe.&lt;/td&gt; 
   &lt;td&gt;CAI-powered security testing of MiR (Mobile Industrial Robot) platform through automated ROS message injection attacks. This study demonstrates how AI-driven vulnerability discovery can expose unauthorized access to robot control systems and alarm triggers.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://aliasrobotics.com/case-study-ecoforest.php"&gt;&lt;img src="https://aliasrobotics.com/img/case-study-portada-ecoforest.png" alt="" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://aliasrobotics.com/case-study-cai-mir.php"&gt;&lt;img src="https://aliasrobotics.com/img/case-study-portada-mir-cai.png" alt="" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;a href="https://aliasrobotics.com/case-study-mercado-libre.php"&gt;&lt;code&gt;IT&lt;/code&gt; (Web) - CAI and alias0 on: Mercado Libre's e-commerce&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://aliasrobotics.com/case-study-cai-mqtt-broker.php"&gt;&lt;code&gt;OT&lt;/code&gt; - CAI and alias0 on: MQTT broker&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;CAI-powered API vulnerability discovery at Mercado Libre through automated enumeration attacks. This study demonstrates how AI-driven security testing can expose user data exposure risks in e-commerce platforms at scale.&lt;/td&gt; 
   &lt;td&gt;CAI-powered testing exposed critical flaws in an MQTT broker within a Dockerized OT network. Without authentication, CAI subscribed to temperature and humidity topics and injected false values, corrupting data shown in Grafana dashboards.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://aliasrobotics.com/case-study-mercado-libre.php"&gt;&lt;img src="https://aliasrobotics.com/img/case-study-portada-mercado-libre.png" alt="" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://aliasrobotics.com/case-study-cai-mqtt-broker.php"&gt;&lt;img src="https://aliasrobotics.com/img/case-study-portada-mqtt-broker-cai.png" alt="" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING] &lt;span&gt;âš &lt;/span&gt; CAI is in active development, so don't expect it to work flawlessly. Instead, contribute by raising an issue or &lt;a href="https://github.com/aliasrobotics/cai/pulls"&gt;sending a PR&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;Access to this library and the use of information, materials (or portions thereof), is &lt;strong&gt;&lt;u&gt;not intended&lt;/u&gt;, and is &lt;u&gt;prohibited&lt;/u&gt;, where such access or use violates applicable laws or regulations&lt;/strong&gt;. By no means the authors encourage or promote the unauthorized tampering with running systems. This can cause serious human harm and material damages.&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;By no means the authors of CAI encourage or promote the unauthorized tampering with compute systems. Please don't use the source code in here for cybercrime. &lt;u&gt;Pentest for good instead&lt;/u&gt;&lt;/em&gt;. By downloading, using, or modifying this source code, you agree to the terms of the &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/LICENSE"&gt;&lt;code&gt;LICENSE&lt;/code&gt;&lt;/a&gt; and the limitations outlined in the &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/DISCLAIMER"&gt;&lt;code&gt;DISCLAIMER&lt;/code&gt;&lt;/a&gt; file.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;&lt;span&gt;ğŸ”–&lt;/span&gt; Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#cybersecurity-ai-cai"&gt;Cybersecurity AI (&lt;code&gt;CAI&lt;/code&gt;)&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#bookmark-table-of-contents"&gt;&lt;span&gt;ğŸ”–&lt;/span&gt; Table of Contents&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-impact"&gt;ğŸ¯ Impact&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-competitions-and-challenges"&gt;ğŸ† Competitions and challenges&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-research-impact"&gt;ğŸ“Š Research Impact&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-research-products-cybersecurity-ai"&gt;ğŸ“š Research products: &lt;code&gt;Cybersecurity AI&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#pocs"&gt;PoCs&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#motivation"&gt;Motivation&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#bust_in_silhouette-why-cai"&gt;&lt;span&gt;ğŸ‘¤&lt;/span&gt; Why CAI?&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#ethical-principles-behind-cai"&gt;Ethical principles behind CAI&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#closed-source-alternatives"&gt;Closed-source alternatives&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#learn---cai-fluency"&gt;Learn - &lt;code&gt;CAI&lt;/code&gt; Fluency&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#nut_and_bolt-install"&gt;&lt;span&gt;ğŸ”©&lt;/span&gt; Install&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#os-x"&gt;OS X&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#ubuntu-2404"&gt;Ubuntu 24.04&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#ubuntu-2004"&gt;Ubuntu 20.04&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#windows-wsl"&gt;Windows WSL&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#android"&gt;Android&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#nut_and_bolt-setup-env-file"&gt;&lt;span&gt;ğŸ”©&lt;/span&gt; Setup &lt;code&gt;.env&lt;/code&gt; file&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-custom-openai-base-url-support"&gt;ğŸ”¹ Custom OpenAI Base URL Support&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#triangular_ruler-architecture"&gt;&lt;span&gt;ğŸ“&lt;/span&gt; Architecture:&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-agent"&gt;ğŸ”¹ Agent&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-tools"&gt;ğŸ”¹ Tools&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-handoffs"&gt;ğŸ”¹ Handoffs&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-patterns"&gt;ğŸ”¹ Patterns&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-turns-and-interactions"&gt;ğŸ”¹ Turns and Interactions&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-tracing"&gt;ğŸ”¹ Tracing&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-guardrails"&gt;ğŸ”¹ Guardrails&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-human-in-the-loop-hitl"&gt;ğŸ”¹ Human-In-The-Loop (HITL)&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#rocket-quickstart"&gt;&lt;span&gt;ğŸš€&lt;/span&gt; Quickstart&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#environment-variables"&gt;Environment Variables&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#openrouter-integration"&gt;OpenRouter Integration&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#mcp"&gt;MCP&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#development"&gt;Development&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#contributions"&gt;Contributions&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#optional-requirements-caiextensions"&gt;Optional Requirements: caiextensions&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#information_source-usage-data-collection"&gt;&lt;span&gt;â„¹&lt;/span&gt; Usage Data Collection&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#reproduce-ci-setup-locally"&gt;Reproduce CI-Setup locally&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#faq"&gt;FAQ&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#citation"&gt;Citation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#acknowledgements"&gt;Acknowledgements&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#academic-collaborations"&gt;Academic Collaborations&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ¯ Impact&lt;/h2&gt; 
&lt;h3&gt;ğŸ† Competitions and challenges&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://app.hackthebox.com/users/2268644"&gt;&lt;img src="https://img.shields.io/badge/HTB_ranking-top_90_Spain_(5_days)-red.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://app.hackthebox.com/users/2268644"&gt;&lt;img src="https://img.shields.io/badge/HTB_ranking-top_50_Spain_(6_days)-red.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://app.hackthebox.com/users/2268644"&gt;&lt;img src="https://img.shields.io/badge/HTB_ranking-top_30_Spain_(7_days)-red.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://app.hackthebox.com/users/2268644"&gt;&lt;img src="https://img.shields.io/badge/HTB_ranking-top_500_World_(7_days)-red.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://ctf.hackthebox.com/event/2000/scoreboard"&gt;&lt;img src="https://img.shields.io/badge/HTB_%22Human_vs_AI%22_CTF-top_1_(AIs)_world-red.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://ctf.hackthebox.com/event/2000/scoreboard"&gt;&lt;img src="https://img.shields.io/badge/HTB_%22Human_vs_AI%22_CTF-top_1_Spain-red.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://ctf.hackthebox.com/event/2000/scoreboard"&gt;&lt;img src="https://img.shields.io/badge/HTB_%22Human_vs_AI%22_CTF-top_20_World-red.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://ctf.hackthebox.com/event/2000/scoreboard"&gt;&lt;img src="https://img.shields.io/badge/HTB_%22Human_vs_AI%22_CTF-750_$-yellow.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://lu.ma/roboticshack?tk=RuryKF"&gt;&lt;img src="https://img.shields.io/badge/Mistral_AI_Robotics_Hackathon-2500_$-yellow.svg?sanitize=true" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;ğŸ“Š Research Impact&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Pioneered LLM-powered AI Security with PentestGPT, establishing the foundation for the &lt;code&gt;Cybersecurity AI&lt;/code&gt; research domain &lt;a href="https://arxiv.org/pdf/2308.06782"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2308.06782-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Established the &lt;code&gt;Cybersecurity AI&lt;/code&gt; research line with &lt;strong&gt;4 papers and technical reports&lt;/strong&gt; and active research collaborations &lt;a href="https://arxiv.org/pdf/2504.06017"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2504.06017-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2506.23592"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2506.23592-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2508.13588"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2508.13588-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2508.21669"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2508.21669-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Demonstrated &lt;strong&gt;3,600Ã— performance improvement&lt;/strong&gt; over human penetration testers in standardized CTF benchmark evaluations &lt;a href="https://arxiv.org/pdf/2504.06017"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2504.06017-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Identified &lt;strong&gt;CVSS 4.3-7.5 severity vulnerabilities&lt;/strong&gt; in production systems through automated security assessment &lt;a href="https://arxiv.org/pdf/2504.06017"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2504.06017-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Democratization of AI-empowered vulnerability research&lt;/strong&gt;: CAI enables both non-security domain experts and experienced researchers to conduct more efficient vulnerability discovery, expanding the security research community while empowering small and medium enterprises to conduct autonomous security assessments &lt;a href="https://arxiv.org/pdf/2504.06017"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2504.06017-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Systematic evaluation of large language models&lt;/strong&gt; across both proprietary and open-weight architectures, revealing &lt;u&gt;substantial gaps&lt;/u&gt; between vendor-reported capabilities and empirical cybersecurity performance metrics &lt;a href="https://arxiv.org/pdf/2504.06017"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2504.06017-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Established the &lt;strong&gt;autonomy levels in cybersecurity&lt;/strong&gt; and argued about autonomy vs automation in the field &lt;a href="https://arxiv.org/abs/2506.23592"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2506.23592-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Collaborative research initiatives&lt;/strong&gt; with international academic institutions focused on developing cybersecurity education curricula and training methodologies &lt;a href="https://arxiv.org/abs/2508.13588"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2508.13588-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Contributed a comprehensive defense framework against prompt injection in AI security agents&lt;/strong&gt;: developed and empirically validated a multi-layered defense system that addresses the identified prompt injection issues &lt;a href="https://arxiv.org/abs/2508.21669"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2508.21669-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ“š Research products: &lt;code&gt;Cybersecurity AI&lt;/code&gt;&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;CAI, An Open, Bug Bounty-Ready Cybersecurity AI&lt;/th&gt; 
   &lt;th&gt;The Dangerous Gap Between Automation and Autonomy&lt;/th&gt; 
   &lt;th&gt;CAI Fluency, A Framework for Cybersecurity AI Fluency&lt;/th&gt; 
   &lt;th&gt;Hacking the AI Hackers via Prompt Injection&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/2508.13588"&gt;&lt;img src="https://aliasrobotics.com/img/paper-cai.png" width="350" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.arxiv.org/pdf/2506.23592"&gt;&lt;img src="https://aliasrobotics.com/img/cai_automation_vs_autonomy.png" width="350" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/2504.06017"&gt;&lt;img src="https://aliasrobotics.com/img/cai_fluency_cover.png" width="350" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/2508.21669"&gt;&lt;img src="https://aliasrobotics.com/img/aihackers.jpeg" width="500" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;PoCs&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;CAI with &lt;code&gt;alias0&lt;/code&gt; on ROS message injection attacks in MiR-100 robot&lt;/th&gt; 
   &lt;th&gt;CAI with &lt;code&gt;alias0&lt;/code&gt; on API vulnerability discovery at Mercado Libre&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://asciinema.org/a/dNv705hZel2Rzrw0cju9HBGPh"&gt;&lt;img src="https://asciinema.org/a/dNv705hZel2Rzrw0cju9HBGPh.svg?sanitize=true" alt="asciicast" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://asciinema.org/a/9Hc9z1uFcdNjqP3bY5y7wO1Ww"&gt;&lt;img src="https://asciinema.org/a/9Hc9z1uFcdNjqP3bY5y7wO1Ww.svg?sanitize=true" alt="asciicast" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;CAI on JWT@PortSwigger CTF â€” Cybersecurity AI&lt;/th&gt; 
   &lt;th&gt;CAI on HackableII Boot2Root CTF â€” Cybersecurity AI&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://asciinema.org/a/713487"&gt;&lt;img src="https://asciinema.org/a/713487.svg?sanitize=true" alt="asciicast" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://asciinema.org/a/713485"&gt;&lt;img src="https://asciinema.org/a/713485.svg?sanitize=true" alt="asciicast" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;More case studies and PoCs are available at &lt;a href="https://aliasrobotics.com/case-studies-robot-cybersecurity.php"&gt;https://aliasrobotics.com/case-studies-robot-cybersecurity.php&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Motivation&lt;/h2&gt; 
&lt;h3&gt;&lt;span&gt;ğŸ‘¤&lt;/span&gt; Why CAI?&lt;/h3&gt; 
&lt;p&gt;The cybersecurity landscape is undergoing a dramatic transformation as AI becomes increasingly integrated into security operations. &lt;strong&gt;We predict that by 2028, AI-powered security testing tools will outnumber human pentesters&lt;/strong&gt;. This shift represents a fundamental change in how we approach cybersecurity challenges. &lt;em&gt;AI is not just another tool - it's becoming essential for addressing complex security vulnerabilities and staying ahead of sophisticated threats. As organizations face more advanced cyber attacks, AI-enhanced security testing will be crucial for maintaining robust defenses.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;This work builds upon prior efforts[^4] and similarly, we believe that democratizing access to advanced cybersecurity AI tools is vital for the entire security community. That's why we're releasing Cybersecurity AI (&lt;code&gt;CAI&lt;/code&gt;) as an open source framework. Our goal is to empower security researchers, ethical hackers, and organizations to build and deploy powerful AI-driven security tools. By making these capabilities openly available, we aim to level the playing field and ensure that cutting-edge security AI technology isn't limited to well-funded private companies or state actors.&lt;/p&gt; 
&lt;p&gt;Bug Bounty programs have become a cornerstone of modern cybersecurity, providing a crucial mechanism for organizations to identify and fix vulnerabilities in their systems before they can be exploited. These programs have proven highly effective at securing both public and private infrastructure, with researchers discovering critical vulnerabilities that might have otherwise gone unnoticed. CAI is specifically designed to enhance these efforts by providing a lightweight, ergonomic framework for building specialized AI agents that can assist in various aspects of Bug Bounty hunting - from initial reconnaissance to vulnerability validation and reporting. Our framework aims to augment human expertise with AI capabilities, helping researchers work more efficiently and thoroughly in their quest to make digital systems more secure.&lt;/p&gt; 
&lt;h3&gt;Ethical principles behind CAI&lt;/h3&gt; 
&lt;p&gt;You might be wondering if releasing CAI &lt;em&gt;in-the-wild&lt;/em&gt; given its capabilities and security implications is ethical. Our decision to open-source this framework is guided by two core ethical principles:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Democratizing Cybersecurity AI&lt;/strong&gt;: We believe that advanced cybersecurity AI tools should be accessible to the entire security community, not just well-funded private companies or state actors. By releasing CAI as an open source framework, we aim to empower security researchers, ethical hackers, and organizations to build and deploy powerful AI-driven security tools, leveling the playing field in cybersecurity.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Transparency in AI Security Capabilities&lt;/strong&gt;: Based on our research results, understanding of the technology, and dissection of top technical reports, we argue that current LLM vendors are undermining their cybersecurity capabilities. This is extremely dangerous and misleading. By developing CAI openly, we provide a transparent benchmark of what AI systems can actually do in cybersecurity contexts, enabling more informed decisions about security postures.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;CAI is built on the following core principles:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Cybersecurity oriented AI framework&lt;/strong&gt;: CAI is specifically designed for cybersecurity use cases, aiming at semi- and fully-automating offensive and defensive security tasks.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Open source, free for research&lt;/strong&gt;: CAI is open source and free for research purposes. We aim at democratizing access to AI and Cybersecurity. For professional or commercial use, including on-premise deployments, dedicated technical support and custom extensions &lt;a href="mailto:research@aliasrobotics.com"&gt;reach out&lt;/a&gt; to obtain a license.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Lightweight&lt;/strong&gt;: CAI is designed to be fast, and easy to use.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Modular and agent-centric design&lt;/strong&gt;: CAI operates on the basis of agents and agentic patterns, which allows flexibility and scalability. You can easily add the most suitable agents and pattern for your cybersecuritytarget case.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Tool-integration&lt;/strong&gt;: CAI integrates already built-in tools, and allows the user to integrate their own tools with their own logic easily.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Logging and tracing integrated&lt;/strong&gt;: using &lt;a href="https://github.com/Arize-ai/phoenix"&gt;&lt;code&gt;phoenix&lt;/code&gt;&lt;/a&gt;, the open source tracing and logging tool for LLMs. This provides the user with a detailed traceability of the agents and their execution.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-Model Support&lt;/strong&gt;: more than 300 supported and empowered by &lt;a href="https://github.com/BerriAI/litellm"&gt;LiteLLM&lt;/a&gt;. The most popular providers: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Anthropic&lt;/strong&gt;: &lt;code&gt;Claude 3.7&lt;/code&gt;, &lt;code&gt;Claude 3.5&lt;/code&gt;, &lt;code&gt;Claude 3&lt;/code&gt;, &lt;code&gt;Claude 3 Opus&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;OpenAI&lt;/strong&gt;: &lt;code&gt;O1&lt;/code&gt;, &lt;code&gt;O1 Mini&lt;/code&gt;, &lt;code&gt;O3 Mini&lt;/code&gt;, &lt;code&gt;GPT-4o&lt;/code&gt;, &lt;code&gt;GPT-4.5 Preview&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;DeepSeek&lt;/strong&gt;: &lt;code&gt;DeepSeek V3&lt;/code&gt;, &lt;code&gt;DeepSeek R1&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Ollama&lt;/strong&gt;: &lt;code&gt;Qwen2.5 72B&lt;/code&gt;, &lt;code&gt;Qwen2.5 14B&lt;/code&gt;, etc&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Closed-source alternatives&lt;/h3&gt; 
&lt;p&gt;Cybersecurity AI is a critical field, yet many groups are misguidedly pursuing it through closed-source methods for pure economic return, leveraging similar techniques and building upon existing closed-source (&lt;em&gt;often third-party owned&lt;/em&gt;) models. This approach not only squanders valuable engineering resources but also represents an economic waste and results in redundant efforts, as they often end up reinventing the wheel. Here are some of the closed-source initiatives we keep track of and attempting to leverage genAI and agentic frameworks in cybersecurity AI:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.acyber.co/"&gt;Autonomous Cyber&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://cracken.ai/"&gt;CrackenAGI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ethiack.com/"&gt;ETHIACK&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://horizon3.ai/"&gt;Horizon3&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.kindo.ai/"&gt;Kindo&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://lakera.ai"&gt;Lakera&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/www.mindfort.ai"&gt;Mindfort&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mindgard.ai/"&gt;Mindgard&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ndaysecurity.com/"&gt;NDAY Security&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.runsybil.com"&gt;Runsybil&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.selfhack.fi"&gt;Selfhack&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://sola.security/"&gt;Sola Security&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://squr.ai/"&gt;SQUR&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://staris.tech/"&gt;Staris&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.sxipher.com/"&gt;Sxipher&lt;/a&gt; (seems discontinued)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.terra.security"&gt;Terra Security&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://xint.io/"&gt;Xint&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.xbow.com"&gt;XBOW&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.zeropath.com"&gt;ZeroPath&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.zynap.com"&gt;Zynap&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://7ai.com"&gt;7ai&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Learn - &lt;code&gt;CAI&lt;/code&gt; Fluency&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;a align="center" href="" target="https://github.com/aliasrobotics/CAI"&gt; &lt;img width="100%" src="https://github.com/aliasrobotics/cai/raw/main/media/caiedu.PNG" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;CAI Fluency technical report (&lt;a href="https://arxiv.org/pdf/2508.13588"&gt;arXiv:2508.13588&lt;/a&gt;) establishes formal educational frameworks for cybersecurity AI literacy.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;English&lt;/th&gt; 
   &lt;th&gt;Spanish&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Episode 0&lt;/strong&gt;: What is CAI?&lt;/td&gt; 
   &lt;td&gt;Cybersecurity AI (&lt;code&gt;CAI&lt;/code&gt;) explained&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=nBdTxbKM4oo"&gt;&lt;img src="https://img.youtube.com/vi/nBdTxbKM4oo/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=FaUL9HXrQ5k"&gt;&lt;img src="https://img.youtube.com/vi/FaUL9HXrQ5k/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Episode 1&lt;/strong&gt;: The &lt;code&gt;CAI&lt;/code&gt; Framework&lt;/td&gt; 
   &lt;td&gt;Vision &amp;amp; Ethics - Explore the core motivation behind CAI and delve into the crucial ethical principles guiding its development. Understand the motivation behind CAI and how you can actively contribute to the future of cybersecurity and the CAI framework.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=QEiGdsMf29M&amp;amp;list=PLLc16OUiZWd4RuFdN5_Wx9xwjCVVbopzr&amp;amp;index=3"&gt;&lt;img src="https://img.youtube.com/vi/QEiGdsMf29M/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Episode 2&lt;/strong&gt;: From Zero to Cyber Hero&lt;/td&gt; 
   &lt;td&gt;Breaking into Cybersecurity with AI - A comprehensive guide for complete beginners to become cybersecurity practitioners using CAI and AI tools. Learn how to leverage artificial intelligence to accelerate your cybersecurity learning journey, from understanding basic security concepts to performing real-world security assessments, all without requiring prior cybersecurity experience.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=hSTLHOOcQoY&amp;amp;list=PLLc16OUiZWd4RuFdN5_Wx9xwjCVVbopzr&amp;amp;index=14"&gt;&lt;img src="https://img.youtube.com/vi/hSTLHOOcQoY/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Episode 3&lt;/strong&gt;: Vibe-Hacking Tutorial&lt;/td&gt; 
   &lt;td&gt;"My first Hack" - A Vibe-Hacking guide for newbies. We demonstrate a simple web security hack using a default agent and show how to leverage tools and interpret CIA output with the help of the CAI Python API. You'll also learn to compare different LLM models to find the best fit for your hacking endeavors.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=9vZ_Iyex7uI&amp;amp;list=PLLc16OUiZWd4RuFdN5_Wx9xwjCVVbopzr&amp;amp;index=1"&gt;&lt;img src="https://img.youtube.com/vi/9vZ_Iyex7uI/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=iAOMaI1ftiA&amp;amp;list=PLLc16OUiZWd4RuFdN5_Wx9xwjCVVbopzr&amp;amp;index=2"&gt;&lt;img src="https://img.youtube.com/vi/iAOMaI1ftiA/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Episode 4&lt;/strong&gt;: Intro ReAct&lt;/td&gt; 
   &lt;td&gt;The Evolution of LLMs - Learn how LLMs evolved from basic language models to advanced multiagency AI systems. From basic LLMs to Chain-of-Thought and Reasoning LLMs towards ReAct and Multi-Agent Architectures. Get to know the basic terms&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=tLdFO1flj_o&amp;amp;list=PLLc16OUiZWd4RuFdN5_Wx9xwjCVVbopzr&amp;amp;index=13"&gt;&lt;img src="https://img.youtube.com/vi/tLdFO1flj_o/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Episode 5&lt;/strong&gt;: CAI on CTF challenges&lt;/td&gt; 
   &lt;td&gt;Dive into Capture The Flag (CTF) competitions using CAI. Learn how to leverage AI agents to solve various cybersecurity challenges including web exploitation, cryptography, reverse engineering, and forensics. Discover how to configure CAI for competitive hacking scenarios and maximize your CTF performance with intelligent automation.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=MrXTQ0e2to4&amp;amp;list=PLLc16OUiZWd4RuFdN5_Wx9xwjCVVbopzr&amp;amp;index=13"&gt;&lt;img src="https://img.youtube.com/vi/MrXTQ0e2to4/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=r9US_JZa9_c&amp;amp;list=PLLc16OUiZWd4RuFdN5_Wx9xwjCVVbopzr&amp;amp;index=12"&gt;&lt;img src="https://img.youtube.com/vi/r9US_JZa9_c/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Annex 1&lt;/strong&gt;: &lt;code&gt;CAI&lt;/code&gt; 0.5.x release&lt;/td&gt; 
   &lt;td&gt;Introduce version 0.5 of &lt;code&gt;CAI&lt;/code&gt; including new multi-agent functionality, new commands such as &lt;code&gt;/history&lt;/code&gt;, &lt;code&gt;/compact&lt;/code&gt;, &lt;code&gt;/graph&lt;/code&gt; or &lt;code&gt;/memory&lt;/code&gt; and a case study showing how &lt;code&gt;CAI&lt;/code&gt; found a critical security flaw in OT heap pumps spread around the world.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=OPFH0ANUMMw"&gt;&lt;img src="https://img.youtube.com/vi/OPFH0ANUMMw/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=Q8AI4E4gH8k"&gt;&lt;img src="https://img.youtube.com/vi/Q8AI4E4gH8k/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Annex 2&lt;/strong&gt;: &lt;code&gt;CAI&lt;/code&gt; 0.4.x release and &lt;code&gt;alias0&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Introducing version 0.4 of &lt;code&gt;CAI&lt;/code&gt; with &lt;em&gt;streaming&lt;/em&gt; and improved MCP support. We also introduce &lt;code&gt;alias0&lt;/code&gt;, the Privacy-First Cybersecurity AI, a Model-of-Models Intelligence that implements a Privacy-by-Design architecture and obtains state-of-the-art results in cybersecurity benchmarks.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=NZjzfnvAZcc"&gt;&lt;img src="https://img.youtube.com/vi/NZjzfnvAZcc/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Annex 3&lt;/strong&gt;: Cybersecurity AI Community Meeting #1&lt;/td&gt; 
   &lt;td&gt;First Cybersecurity AI (&lt;code&gt;CAI&lt;/code&gt;) community meeting, over 40 participants from academia, industry, and defense gathered to discuss the open-source scaffolding behind CAI â€” a project designed to build agentic AI systems for cybersecurity that are open, modular, and Bug Bounty-ready.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=4JqaTiVlgsw"&gt;&lt;img src="https://img.youtube.com/vi/4JqaTiVlgsw/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;&lt;span&gt;ğŸ”©&lt;/span&gt; Install&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install cai-framework
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Always create a new virtual environment to ensure proper dependency installation when updating CAI.&lt;/p&gt; 
&lt;p&gt;The following subsections provide a more detailed walkthrough on selected popular Operating Systems. Refer to the &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#development"&gt;Development&lt;/a&gt; section for developer-related install instructions.&lt;/p&gt; 
&lt;h3&gt;OS X&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;brew update &amp;amp;&amp;amp; \
    brew install git python@3.12

# Create virtual environment
python3.12 -m venv cai_env

# Install the package from the local directory
source cai_env/bin/activate &amp;amp;&amp;amp; pip install cai-framework

# Generate a .env file and set up with defaults
echo -e 'OPENAI_API_KEY="sk-1234"\nANTHROPIC_API_KEY=""\nOLLAMA=""\nPROMPT_TOOLKIT_NO_CPR=1\nCAI_STREAM=false' &amp;gt; .env

# Launch CAI
cai  # first launch it can take up to 30 seconds
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Ubuntu 24.04&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt-get update &amp;amp;&amp;amp; \
    sudo apt-get install -y git python3-pip python3.12-venv

# Create the virtual environment
python3.12 -m venv cai_env

# Install the package from the local directory
source cai_env/bin/activate &amp;amp;&amp;amp; pip install cai-framework

# Generate a .env file and set up with defaults
echo -e 'OPENAI_API_KEY="sk-1234"\nANTHROPIC_API_KEY=""\nOLLAMA=""\nPROMPT_TOOLKIT_NO_CPR=1\nCAI_STREAM=false' &amp;gt; .env

# Launch CAI
cai  # first launch it can take up to 30 seconds
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Ubuntu 20.04&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt-get update &amp;amp;&amp;amp; \
    sudo apt-get install -y software-properties-common

# Fetch Python 3.12
sudo add-apt-repository ppa:deadsnakes/ppa &amp;amp;&amp;amp; sudo apt update
sudo apt install python3.12 python3.12-venv python3.12-dev -y

# Create the virtual environment
python3.12 -m venv cai_env

# Install the package from the local directory
source cai_env/bin/activate &amp;amp;&amp;amp; pip install cai-framework

# Generate a .env file and set up with defaults
echo -e 'OPENAI_API_KEY="sk-1234"\nANTHROPIC_API_KEY=""\nOLLAMA=""\nPROMPT_TOOLKIT_NO_CPR=1\nCAI_STREAM=false' &amp;gt; .env

# Launch CAI
cai  # first launch it can take up to 30 seconds
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Windows WSL&lt;/h3&gt; 
&lt;p&gt;Go to the Microsoft page: &lt;a href="https://learn.microsoft.com/en-us/windows/wsl/install"&gt;https://learn.microsoft.com/en-us/windows/wsl/install&lt;/a&gt;. Here you will find all the instructions to install WSL&lt;/p&gt; 
&lt;p&gt;From Powershell write: wsl --install&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;
sudo apt-get update &amp;amp;&amp;amp; \
    sudo apt-get install -y git python3-pip python3-venv

# Create the virtual environment
python3 -m venv cai_env

# Install the package from the local directory
source cai_env/bin/activate &amp;amp;&amp;amp; pip install cai-framework

# Generate a .env file and set up with defaults
echo -e 'OPENAI_API_KEY="sk-1234"\nANTHROPIC_API_KEY=""\nOLLAMA=""\nPROMPT_TOOLKIT_NO_CPR=1\nCAI_STREAM=false' &amp;gt; .env

# Launch CAI
cai  # first launch it can take up to 30 seconds
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Android&lt;/h3&gt; 
&lt;p&gt;We recommend having at least 8 GB of RAM:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;First of all, install userland &lt;a href="https://play.google.com/store/apps/details?id=tech.ula&amp;amp;hl=es"&gt;https://play.google.com/store/apps/details?id=tech.ula&amp;amp;hl=es&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install Kali minimal in basic options (for free). [Or any other kali option if preferred]&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Update apt keys like in this example: &lt;a href="https://superuser.com/questions/1644520/apt-get-update-issue-in-kali"&gt;https://superuser.com/questions/1644520/apt-get-update-issue-in-kali&lt;/a&gt;, inside UserLand's Kali terminal execute&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Get new apt keys
wget http://http.kali.org/kali/pool/main/k/kali-archive-keyring/kali-archive-keyring_2024.1_all.deb

# Install new apt keys
sudo dpkg -i kali-archive-keyring_2024.1_all.deb &amp;amp;&amp;amp; rm kali-archive-keyring_2024.1_all.deb

# Update APT repository
sudo apt-get update

# CAI requieres python 3.12, lets install it (CAI for kali in Android)
sudo apt-get update &amp;amp;&amp;amp; sudo apt-get install -y git python3-pip build-essential zlib1g-dev libncurses5-dev libgdbm-dev libnss3-dev libssl-dev libreadline-dev libffi-dev libsqlite3-dev wget libbz2-dev pkg-config
wget https://www.python.org/ftp/python/3.12.4/Python-3.12.4.tar.xz
tar xf Python-3.12.4.tar.xz
cd ./configure --enable-optimizations
sudo make altinstall # This command takes long to execute

# Clone CAI's source code
git clone https://github.com/aliasrobotics/cai &amp;amp;&amp;amp; cd cai

# Create virtual environment
python3.12 -m venv cai_env

# Install the package from the local directory
source cai_env/bin/activate &amp;amp;&amp;amp; pip3 install -e .

# Generate a .env file and set up
cp .env.example .env  # edit here your keys/models

# Launch CAI
cai
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;&lt;span&gt;ğŸ”©&lt;/span&gt; Setup &lt;code&gt;.env&lt;/code&gt; file&lt;/h3&gt; 
&lt;p&gt;CAI leverages the &lt;code&gt;.env&lt;/code&gt; file to load configuration at launch. To facilitate the setup, the repo provides an exemplary &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/.env.example"&gt;&lt;code&gt;.env.example&lt;/code&gt;&lt;/a&gt; file provides a template for configuring CAI's setup and your LLM API keys to work with desired LLM models.&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;âš &lt;/span&gt; Important:&lt;/p&gt; 
&lt;p&gt;CAI does NOT provide API keys for any model by default. Don't ask us to provide keys, use your own or host your own models.&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;âš &lt;/span&gt; Note:&lt;/p&gt; 
&lt;p&gt;The OPENAI_API_KEY must not be left blank. It should contain either "sk-123" (as a placeholder) or your actual API key. See &lt;a href="https://github.com/aliasrobotics/cai/issues/27"&gt;https://github.com/aliasrobotics/cai/issues/27&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;âš &lt;/span&gt; Note:&lt;/p&gt; 
&lt;p&gt;If you are using alias0 model, make sure that CAI is &amp;gt;0.4.0 version and here you have an .env example to be able to use it.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;OPENAI_API_KEY="sk-1234"
OLLAMA=""
ALIAS_API_KEY="&amp;lt;sk-your-key&amp;gt;"  # note, add yours
CAI_STEAM=False
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;ğŸ”¹ Custom OpenAI Base URL Support&lt;/h3&gt; 
&lt;p&gt;CAI supports configuring a custom OpenAI API base URL via the &lt;code&gt;OPENAI_BASE_URL&lt;/code&gt; environment variable. This allows users to redirect API calls to a custom endpoint, such as a proxy or self-hosted OpenAI-compatible service.&lt;/p&gt; 
&lt;p&gt;Example &lt;code&gt;.env&lt;/code&gt; entry configuration:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;OLLAMA_API_BASE="https://custom-openai-proxy.com/v1"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or directly from the command line:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;OLLAMA_API_BASE="https://custom-openai-proxy.com/v1" cai
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;&lt;span&gt;ğŸ“&lt;/span&gt; Architecture:&lt;/h2&gt; 
&lt;p&gt;CAI focuses on making cybersecurity agent &lt;strong&gt;coordination&lt;/strong&gt; and &lt;strong&gt;execution&lt;/strong&gt; lightweight, highly controllable, and useful for humans. To do so it builds upon 8 pillars: &lt;code&gt;Agent&lt;/code&gt;s, &lt;code&gt;Tools&lt;/code&gt;, &lt;code&gt;Handoffs&lt;/code&gt;, &lt;code&gt;Patterns&lt;/code&gt;, &lt;code&gt;Turns&lt;/code&gt;, &lt;code&gt;Tracing&lt;/code&gt;, &lt;code&gt;Guardrails&lt;/code&gt; and &lt;code&gt;HITL&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚      HITL     â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   Turns   â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Patterns â”‚â—€â”€â”€â”€â”€â”€â–¶â”‚  Handoffs â”‚â—€â”€â”€â”€â”€â–¶ â”‚   Agents  â”‚â—€â”€â”€â”€â”€â–¶â”‚    LLMs   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚                   â”‚
                          â”‚                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Extensions â”‚â—€â”€â”€â”€â”€â”€â–¶â”‚  Tracing  â”‚       â”‚   Tools   â”‚â—€â”€â”€â”€â–¶â”‚ Guardrails â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â–¼             â–¼          â–¼             â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ LinuxCmd  â”‚â”‚ WebSearch â”‚â”‚    Code    â”‚â”‚ SSHTunnel â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you want to dive deeper into the code, check the following files as a start point for using CAI:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/raw/main/src/cai/__init__.py"&gt;&lt;strong&gt;init&lt;/strong&gt;.py&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/raw/main/src/cai/cli.py"&gt;cli.py&lt;/a&gt; - entrypoint for command line interface&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/raw/main/src/cai/util.py"&gt;util.py&lt;/a&gt; - utility functions&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/raw/main/src/cai/agents"&gt;agents&lt;/a&gt; - Agent implementations&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/raw/main/src/cai/internal"&gt;internal&lt;/a&gt; - CAI internal functions (endpoints, metrics, logging, etc.)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/raw/main/src/cai/prompts"&gt;prompts&lt;/a&gt; - Agent Prompt Database&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/raw/main/src/cai/repl"&gt;repl&lt;/a&gt; - CLI aesthetics and commands&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/raw/main/src/cai/sdk"&gt;sdk&lt;/a&gt; - CAI command sdk&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/tree/main/src/cai/tools"&gt;tools&lt;/a&gt; - agent tools&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ”¹ Agent&lt;/h3&gt; 
&lt;p&gt;At its core, CAI abstracts its cybersecurity behavior via &lt;code&gt;Agents&lt;/code&gt; and agentic &lt;code&gt;Patterns&lt;/code&gt;. An Agent in &lt;em&gt;an intelligent system that interacts with some environment&lt;/em&gt;. More technically, within CAI we embrace a robotics-centric definition wherein an agent is anything that can be viewed as a system perceiving its environment through sensors, reasoning about its goals and and acting accordingly upon that environment through actuators (&lt;em&gt;adapted&lt;/em&gt; from Russel &amp;amp; Norvig, AI: A Modern Approach). In cybersecurity, an &lt;code&gt;Agent&lt;/code&gt; interacts with systems and networks, using peripherals and network interfaces as sensors, reasons accordingly and then executes network actions as if actuators. Correspondingly, in CAI, &lt;code&gt;Agent&lt;/code&gt;s implement the &lt;code&gt;ReACT&lt;/code&gt; (Reasoning and Action) agent model[^3]. For more information, see the &lt;a href="https://github.com/aliasrobotics/cai/raw/main/examples/basic/hello_world.py"&gt;example here&lt;/a&gt; for the full execution code, and refer to this &lt;a href="https://github.com/aliasrobotics/cai/raw/main/fluency/my-first-hack/my_first_hack.ipynb"&gt;jupyter notebook&lt;/a&gt; for a tutorial on how to use it.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from cai.sdk.agents import Agent, Runner, OpenAIChatCompletionsModel

import os
from openai import AsyncOpenAI
from dotenv import load_dotenv
load_dotenv()

agent = Agent(
      name="Custom Agent",
      instructions="""You are a Cybersecurity expert Leader""",
      model=OpenAIChatCompletionsModel(
          model=os.getenv('CAI_MODEL', "openai/gpt-4o"),
          openai_client=AsyncOpenAI(),
          )
      )

message = "Tell me about recursion in programming."
result = await Runner.run(agent, message)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;ğŸ”¹ Tools&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;Tools&lt;/code&gt; let cybersecurity agents take actions by providing interfaces to execute system commands, run security scans, analyze vulnerabilities, and interact with target systems and APIs - they are the core capabilities that enable CAI agents to perform security tasks effectively; in CAI, tools include built-in cybersecurity utilities (like LinuxCmd for command execution, WebSearch for OSINT gathering, Code for dynamic script execution, and SSHTunnel for secure remote access), function calling mechanisms that allow integration of any Python function as a security tool, and agent-as-tool functionality that enables specialized security agents (such as reconnaissance or exploit agents) to be used by other agents, creating powerful collaborative security workflows without requiring formal handoffs between agents. For more information, please refer to the &lt;a href="https://github.com/aliasrobotics/cai/raw/main/examples/basic/tools.py"&gt;example here&lt;/a&gt; for the complete configuration of custom functions.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from cai.sdk.agents import Agent, Runner, OpenAIChatCompletionsModel
from cai.tools.reconnaissance.exec_code import execute_code
from cai.tools.reconnaissance.generic_linux_command import generic_linux_command

import os
from openai import AsyncOpenAI
from dotenv import load_dotenv
load_dotenv()

agent = Agent(
      name="Custom Agent",
      instructions="""You are a Cybersecurity expert Leader""",
      tools= [
        generic_linux_command,
        execute_code
      ],
      model=OpenAIChatCompletionsModel(
          model=os.getenv('CAI_MODEL', "openai/gpt-4o"),
          openai_client=AsyncOpenAI(),
          )
      )

message = "Tell me about recursion in programming."
result = await Runner.run(agent, message)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You may find different &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/cai/tools"&gt;tools&lt;/a&gt;. They are grouped in 6 major categories inspired by the security kill chain [^2]:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Reconnaissance and weaponization - &lt;em&gt;reconnaissance&lt;/em&gt; (crypto, listing, etc)&lt;/li&gt; 
 &lt;li&gt;Exploitation - &lt;em&gt;exploitation&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;Privilege escalation - &lt;em&gt;escalation&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;Lateral movement - &lt;em&gt;lateral&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;Data exfiltration - &lt;em&gt;exfiltration&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;Command and control - &lt;em&gt;control&lt;/em&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;ğŸ”¹ Handoffs&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;Handoffs&lt;/code&gt; allow an &lt;code&gt;Agent&lt;/code&gt; to delegate tasks to another agent, which is crucial in cybersecurity operations where specialized expertise is needed for different phases of an engagement. In our framework, &lt;code&gt;Handoffs&lt;/code&gt; are implemented as tools for the LLM, where a &lt;strong&gt;handoff/transfer function&lt;/strong&gt; like &lt;code&gt;transfer_to_flag_discriminator&lt;/code&gt; enables the &lt;code&gt;ctf_agent&lt;/code&gt; to pass control to the &lt;code&gt;flag_discriminator_agent&lt;/code&gt; once it believes it has found the flag. This creates a security validation chain where the first agent handles exploitation and flag discovery, while the second agent specializes in flag verification, ensuring proper segregation of duties and leveraging specialized capabilities of different models for distinct security tasks. For more information, please refer to the &lt;a href="https://github.com/aliasrobotics/cai/raw/main/examples/cai/agent_patterns/handoffs.py"&gt;example here&lt;/a&gt; for the full execution code.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from cai.sdk.agents import function_tool
from cai.tools.common import run_command
from cai.sdk.agents import Agent, OpenAIChatCompletionsModel, HandoffInputData, Runner, function_tool, handoff, trace
from cai.sdk.agents.extensions import handoff_filters

import os
from openai import AsyncOpenAI
from dotenv import load_dotenv
load_dotenv()

@function_tool
def execute_cli_command(command: str) -&amp;gt; str:
    return run_command(command)

flag_discriminator = Agent(
    name="Flag discriminator",
    description="Agent focused on extracting the flag from the output",
    instructions="You are an agent tailored to extract the flag from a given output.",
    model=OpenAIChatCompletionsModel(
        model=os.getenv('CAI_MODEL', "qwen2.5:14b"),
        openai_client=AsyncOpenAI(),
    ) 
)

ctf_agent = Agent(
    name="CTF agent",
    description="Agent focused on conquering security challenges",
    instructions="You are a Cybersecurity expert Leader facing a CTF",
    tools=[
        execute_cli_command,
    ],
    model=OpenAIChatCompletionsModel(
        model= os.getenv('CAI_MODEL', "qwen2.5:14b"),
        openai_client=AsyncOpenAI(),
    ), 
    handoffs = [flag_discriminator]
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;ğŸ”¹ Patterns&lt;/h3&gt; 
&lt;p&gt;An agentic &lt;code&gt;Pattern&lt;/code&gt; is a &lt;em&gt;structured design paradigm&lt;/em&gt; in artificial intelligence systems where autonomous or semi-autonomous agents operate within a defined &lt;em&gt;interaction framework&lt;/em&gt; (the pattern) to achieve a goal. These &lt;code&gt;Patterns&lt;/code&gt; specify the organization, coordination, and communication methods among agents, guiding decision-making, task execution, and delegation.&lt;/p&gt; 
&lt;p&gt;An agentic pattern (&lt;code&gt;AP&lt;/code&gt;) can be formally defined as a tuple:&lt;/p&gt; 
&lt;p&gt;\[ AP = (A, H, D, C, E) \]&lt;/p&gt; 
&lt;p&gt;wherein:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;\(A\) (Agents):&lt;/strong&gt; A set of autonomous entities, \( A = \{a_1, a_2, ..., a_n\} \), each with defined roles, capabilities, and internal states.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;\(H\) (Handoffs):&lt;/strong&gt; A function \( H: A \times T \to A \) that governs how tasks \( T \) are transferred between agents based on predefined logic (e.g., rules, negotiation, bidding).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;\(D\) (Decision Mechanism):&lt;/strong&gt; A decision function \( D: S \to A \) where \( S \) represents system states, and \( D \) determines which agent takes action at any given time.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;\(C\) (Communication Protocol):&lt;/strong&gt; A messaging function \( C: A \times A \to M \), where \( M \) is a message space, defining how agents share information.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;\(E\) (Execution Model):&lt;/strong&gt; A function \( E: A \times I \to O \) where \( I \) is the input space and \( O \) is the output space, defining how agents perform tasks.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;When building &lt;code&gt;Patterns&lt;/code&gt;, we generall y classify them among one of the following categories, though others exist:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;strong&gt;Agentic&lt;/strong&gt; &lt;code&gt;Pattern&lt;/code&gt; &lt;strong&gt;categories&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Swarm&lt;/code&gt; (Decentralized)&lt;/td&gt; 
   &lt;td&gt;Agents share tasks and self-assign responsibilities without a central orchestrator. Handoffs occur dynamically. &lt;em&gt;An example of a peer-to-peer agentic pattern is the &lt;code&gt;CTF Agentic Pattern&lt;/code&gt;, which involves a team of agents working together to solve a CTF challenge with dynamic handoffs.&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Hierarchical&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;A top-level agent (e.g., "PlannerAgent") assigns tasks via structured handoffs to specialized sub-agents. Alternatively, the structure of the agents is harcoded into the agentic pattern with pre-defined handoffs.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Chain-of-Thought&lt;/code&gt; (Sequential Workflow)&lt;/td&gt; 
   &lt;td&gt;A structured pipeline where Agent A produces an output, hands it to Agent B for reuse or refinement, and so on. Handoffs follow a linear sequence. &lt;em&gt;An example of a chain-of-thought agentic pattern is the &lt;code&gt;ReasonerAgent&lt;/code&gt;, which involves a Reasoning-type LLM that provides context to the main agent to solve a CTF challenge with a linear sequence.&lt;/em&gt;[^1]&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Auction-Based&lt;/code&gt; (Competitive Allocation)&lt;/td&gt; 
   &lt;td&gt;Agents "bid" on tasks based on priority, capability, or cost. A decision agent evaluates bids and hands off tasks to the best-fit agent.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Recursive&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;A single agent continuously refines its own output, treating itself as both executor and evaluator, with handoffs (internal or external) to itself. &lt;em&gt;An example of a recursive agentic pattern is the &lt;code&gt;CodeAgent&lt;/code&gt; (when used as a recursive agent), which continuously refines its own output by executing code and updating its own instructions.&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;For more information and examples of common agentic patterns, see the &lt;a href="https://github.com/aliasrobotics/cai/raw/main/examples/agent_patterns/README.md"&gt;examples folder&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;ğŸ”¹ Turns and Interactions&lt;/h3&gt; 
&lt;p&gt;During the agentic flow (conversation), we distinguish between &lt;strong&gt;interactions&lt;/strong&gt; and &lt;strong&gt;turns&lt;/strong&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Interactions&lt;/strong&gt; are sequential exchanges between one or multiple agents. Each agent executing its logic corresponds with one &lt;em&gt;interaction&lt;/em&gt;. Since an &lt;code&gt;Agent&lt;/code&gt; in CAI generally implements the &lt;code&gt;ReACT&lt;/code&gt; agent model[^3], each &lt;em&gt;interaction&lt;/em&gt; consists of 1) a reasoning step via an LLM inference and 2) act by calling zero-to-n &lt;code&gt;Tools&lt;/code&gt;. This is defined in&lt;code&gt;process_interaction()&lt;/code&gt; in &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/cai/core.py"&gt;core.py&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Turns&lt;/strong&gt;: A turn represents a cycle of one ore more &lt;strong&gt;interactions&lt;/strong&gt; which finishes when the &lt;code&gt;Agent&lt;/code&gt; (or &lt;code&gt;Pattern&lt;/code&gt;) executing returns &lt;code&gt;None&lt;/code&gt;, judging there're no further actions to undertake. This is defined in &lt;code&gt;run()&lt;/code&gt;, see &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/cai/core.py"&gt;core.py&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] CAI Agents are not related to Assistants in the Assistants API. They are named similarly for convenience, but are otherwise completely unrelated. CAI is entirely powered by the Chat Completions API and is hence stateless between calls.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;ğŸ”¹ Tracing&lt;/h3&gt; 
&lt;p&gt;CAI implements AI observability by adopting the OpenTelemetry standard and to do so, it leverages &lt;a href="https://github.com/Arize-ai/phoenix"&gt;Phoenix&lt;/a&gt; which provides comprehensive tracing capabilities through OpenTelemetry-based instrumentation, allowing you to monitor and analyze your security operations in real-time. This integration enables detailed visibility into agent interactions, tool usage, and attack vectors throughout penetration testing workflows, making it easier to debug complex exploitation chains, track vulnerability discovery processes, and optimize agent performance for more effective security assessments.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/media/tracing.png" alt="" /&gt;&lt;/p&gt; 
&lt;h3&gt;ğŸ”¹ Guardrails&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;Guardrails&lt;/code&gt; provide a critical security layer for CAI agents, protecting against prompt injection attacks and preventing execution of dangerous commands. These guardrails run in parallel to agents, validating both input and output to ensure safe operation. The framework includes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Input Guardrails&lt;/strong&gt;: Detect and block prompt injection attempts before they reach agents, using pattern matching, Unicode homograph detection, and AI-powered analysis&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Output Guardrails&lt;/strong&gt;: Validate agent outputs before execution, preventing dangerous commands like reverse shells, fork bombs, or data exfiltration&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-layered Defense&lt;/strong&gt;: Protection at input, processing, and execution stages with tool-level validation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Base64/Base32 Aware&lt;/strong&gt;: Automatically decodes and analyzes encoded payloads to detect hidden malicious commands&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Configurable&lt;/strong&gt;: Can be enabled/disabled via &lt;code&gt;CAI_GUARDRAILS&lt;/code&gt; environment variable&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For detailed implementation, see &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/docs/guardrails.md"&gt;docs/guardrails.md&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/docs/cai_prompt_injection.md"&gt;docs/cai_prompt_injection.md&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;ğŸ”¹ Human-In-The-Loop (HITL)&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚                                 â”‚
                      â”‚      Cybersecurity AI (CAI)     â”‚
                      â”‚                                 â”‚
                      â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
                      â”‚       â”‚  Autonomous AI  â”‚       â”‚
                      â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
                      â”‚                â”‚                â”‚
                      â”‚                â”‚                â”‚
                      â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
                      â”‚       â”‚ HITL Interaction â”‚      â”‚
                      â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
                      â”‚                â”‚                â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                                       â”‚ Ctrl+C (cli.py)
                                       â”‚
                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                           â”‚   Human Operator(s)   â”‚
                           â”‚  Expertise | Judgment â”‚
                           â”‚    Teleoperation      â”‚
                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;CAI delivers a framework for building Cybersecurity AIs with a strong emphasis on &lt;em&gt;semi-autonomous&lt;/em&gt; operation, as the reality is that &lt;strong&gt;fully-autonomous&lt;/strong&gt; cybersecurity systems remain premature and face significant challenges when tackling complex tasks. While CAI explores autonomous capabilities, we recognize that effective security operations still require human teleoperation providing expertise, judgment, and oversight in the security process.&lt;/p&gt; 
&lt;p&gt;Accordingly, the Human-In-The-Loop (&lt;code&gt;HITL&lt;/code&gt;) module is a core design principle of CAI, acknowledging that human intervention and teleoperation are essential components of responsible security testing. Through the &lt;code&gt;cli.py&lt;/code&gt; interface, users can seamlessly interact with agents at any point during execution by simply pressing &lt;code&gt;Ctrl+C&lt;/code&gt;. This is implemented across &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/cai/core.py"&gt;core.py&lt;/a&gt; and also in the REPL abstractions &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/cai/repl"&gt;REPL&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;&lt;span&gt;ğŸš€&lt;/span&gt; Quickstart&lt;/h2&gt; 
&lt;p&gt;To start CAI after installing it, just type &lt;code&gt;cai&lt;/code&gt; in the CLI:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;â””â”€# cai

          CCCCCCCCCCCCC      ++++++++   ++++++++      IIIIIIIIII
       CCC::::::::::::C  ++++++++++       ++++++++++  I::::::::I
     CC:::::::::::::::C ++++++++++         ++++++++++ I::::::::I
    C:::::CCCCCCCC::::C +++++++++    ++     +++++++++ II::::::II
   C:::::C       CCCCCC +++++++     +++++     +++++++   I::::I
  C:::::C                +++++     +++++++     +++++    I::::I
  C:::::C                ++++                   ++++    I::::I
  C:::::C                 ++                     ++     I::::I
  C:::::C                  +   +++++++++++++++   +      I::::I
  C:::::C                    +++++++++++++++++++        I::::I
  C:::::C                     +++++++++++++++++         I::::I
   C:::::C       CCCCCC        +++++++++++++++          I::::I
    C:::::CCCCCCCC::::C         +++++++++++++         II::::::II
     CC:::::::::::::::C           +++++++++           I::::::::I
       CCC::::::::::::C             +++++             I::::::::I
          CCCCCCCCCCCCC               ++              IIIIIIIIII

                      Cybersecurity AI (CAI), vX.Y.Z
                          Bug bounty-ready AI

CAI&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;That should initialize CAI and provide a prompt to execute any security task you want to perform. The navigation bar at the bottom displays important system information. This information helps you understand your environment while working with CAI.&lt;/p&gt; 
&lt;p&gt;Here's a quick &lt;a href="https://asciinema.org/a/zm7wS5DA2o0S9pu1Tb44pnlvy"&gt;demo video&lt;/a&gt; to help you get started with CAI. We'll walk through the basic steps â€” from launching the tool to running your first AI-powered task in the terminal. Whether you're a beginner or just curious, this guide will show you how easy it is to begin using CAI.&lt;/p&gt; 
&lt;p&gt;From here on, type on &lt;code&gt;CAI&lt;/code&gt; and start your security exercise. Best way to learn is by example:&lt;/p&gt; 
&lt;h3&gt;Environment Variables&lt;/h3&gt; 
&lt;p&gt;For using private models, you are given a &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/.env.example"&gt;&lt;code&gt;.env.example&lt;/code&gt;&lt;/a&gt; file. Copy it and rename it as &lt;code&gt;.env&lt;/code&gt;. Fill in your corresponding API keys, and you are ready to use CAI.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;List of Environment Variables&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Variable&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CTF_NAME&lt;/td&gt; 
    &lt;td&gt;Name of the CTF challenge to run (e.g. "picoctf_static_flag")&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CTF_CHALLENGE&lt;/td&gt; 
    &lt;td&gt;Specific challenge name within the CTF to test&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CTF_SUBNET&lt;/td&gt; 
    &lt;td&gt;Network subnet for the CTF container&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CTF_IP&lt;/td&gt; 
    &lt;td&gt;IP address for the CTF container&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CTF_INSIDE&lt;/td&gt; 
    &lt;td&gt;Whether to conquer the CTF from within container&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_MODEL&lt;/td&gt; 
    &lt;td&gt;Model to use for agents&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_DEBUG&lt;/td&gt; 
    &lt;td&gt;Set debug output level (0: Only tool outputs, 1: Verbose debug output, 2: CLI debug output)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_BRIEF&lt;/td&gt; 
    &lt;td&gt;Enable/disable brief output mode&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_MAX_TURNS&lt;/td&gt; 
    &lt;td&gt;Maximum number of turns for agent interactions&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_TRACING&lt;/td&gt; 
    &lt;td&gt;Enable/disable OpenTelemetry tracing&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_AGENT_TYPE&lt;/td&gt; 
    &lt;td&gt;Specify the agents to use (boot2root, one_tool...)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_STATE&lt;/td&gt; 
    &lt;td&gt;Enable/disable stateful mode&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_MEMORY&lt;/td&gt; 
    &lt;td&gt;Enable/disable memory mode (episodic, semantic, all)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_MEMORY_ONLINE&lt;/td&gt; 
    &lt;td&gt;Enable/disable online memory mode&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_MEMORY_OFFLINE&lt;/td&gt; 
    &lt;td&gt;Enable/disable offline memory&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_ENV_CONTEXT&lt;/td&gt; 
    &lt;td&gt;Add dirs and current env to llm context&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_MEMORY_ONLINE_INTERVAL&lt;/td&gt; 
    &lt;td&gt;Number of turns between online memory updates&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_PRICE_LIMIT&lt;/td&gt; 
    &lt;td&gt;Price limit for the conversation in dollars&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_REPORT&lt;/td&gt; 
    &lt;td&gt;Enable/disable reporter mode (ctf, nis2, pentesting)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_SUPPORT_MODEL&lt;/td&gt; 
    &lt;td&gt;Model to use for the support agent&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_SUPPORT_INTERVAL&lt;/td&gt; 
    &lt;td&gt;Number of turns between support agent executions&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_WORKSPACE&lt;/td&gt; 
    &lt;td&gt;Defines the name of the workspace&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_WORKSPACE_DIR&lt;/td&gt; 
    &lt;td&gt;Specifies the directory path where the workspace is located&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_GUARDRAILS&lt;/td&gt; 
    &lt;td&gt;Enable/disable guardrails for prompt injection protection (default: true)&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h3&gt;OpenRouter Integration&lt;/h3&gt; 
&lt;p&gt;The Cybersecurity AI (CAI) platform offers seamless integration with OpenRouter, a unified interface for Large Language Models (LLMs). This integration is crucial for users who wish to leverage advanced AI capabilities in their cybersecurity tasks. OpenRouter acts as a bridge, allowing CAI to communicate with various LLMs, thereby enhancing the flexibility and power of the AI agents used within CAI.&lt;/p&gt; 
&lt;p&gt;To enable OpenRouter support in CAI, you need to configure your environment by adding specific entries to your &lt;code&gt;.env&lt;/code&gt; file. This setup ensures that CAI can interact with the OpenRouter API, facilitating the use of sophisticated models like Meta-LLaMA. Hereâ€™s how you can configure it:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CAI_AGENT_TYPE=redteam_agent
CAI_MODEL=openrouter/meta-llama/llama-4-maverick
OPENROUTER_API_KEY=&amp;lt;sk-your-key&amp;gt;  # note, add yours
OPENROUTER_API_BASE=https://openrouter.ai/api/v1
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;MCP&lt;/h3&gt; 
&lt;p&gt;CAI supports the Model Context Protocol (MCP) for integrating external tools and services with AI agents. MCP is supported via two transport mechanisms:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;SSE (Server-Sent Events)&lt;/strong&gt; - For web-based servers that push updates over HTTP connections:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CAI&amp;gt;/mcp load http://localhost:9876/sse burp
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;&lt;strong&gt;STDIO (Standard Input/Output)&lt;/strong&gt; - For local inter-process communication:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CAI&amp;gt;/mcp load stdio myserver python mcp_server.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Once connected, you can add the MCP tools to any agent:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CAI&amp;gt;/mcp add burp redteam_agent
Adding tools from MCP server 'burp' to agent 'Red Team Agent'...
                                 Adding tools to Red Team Agent
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Tool                              â”ƒ Status â”ƒ Details                                         â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ send_http_request                 â”‚ Added  â”‚ Available as: send_http_request                 â”‚
â”‚ create_repeater_tab               â”‚ Added  â”‚ Available as: create_repeater_tab               â”‚
â”‚ send_to_intruder                  â”‚ Added  â”‚ Available as: send_to_intruder                  â”‚
â”‚ url_encode                        â”‚ Added  â”‚ Available as: url_encode                        â”‚
â”‚ url_decode                        â”‚ Added  â”‚ Available as: url_decode                        â”‚
â”‚ base64encode                      â”‚ Added  â”‚ Available as: base64encode                      â”‚
â”‚ base64decode                      â”‚ Added  â”‚ Available as: base64decode                      â”‚
â”‚ generate_random_string            â”‚ Added  â”‚ Available as: generate_random_string            â”‚
â”‚ output_project_options            â”‚ Added  â”‚ Available as: output_project_options            â”‚
â”‚ output_user_options               â”‚ Added  â”‚ Available as: output_user_options               â”‚
â”‚ set_project_options               â”‚ Added  â”‚ Available as: set_project_options               â”‚
â”‚ set_user_options                  â”‚ Added  â”‚ Available as: set_user_options                  â”‚
â”‚ get_proxy_http_history            â”‚ Added  â”‚ Available as: get_proxy_http_history            â”‚
â”‚ get_proxy_http_history_regex      â”‚ Added  â”‚ Available as: get_proxy_http_history_regex      â”‚
â”‚ get_proxy_websocket_history       â”‚ Added  â”‚ Available as: get_proxy_websocket_history       â”‚
â”‚ get_proxy_websocket_history_regex â”‚ Added  â”‚ Available as: get_proxy_websocket_history_regex â”‚
â”‚ set_task_execution_engine_state   â”‚ Added  â”‚ Available as: set_task_execution_engine_state   â”‚
â”‚ set_proxy_intercept_state         â”‚ Added  â”‚ Available as: set_proxy_intercept_state         â”‚
â”‚ get_active_editor_contents        â”‚ Added  â”‚ Available as: get_active_editor_contents        â”‚
â”‚ set_active_editor_contents        â”‚ Added  â”‚ Available as: set_active_editor_contents        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Added 20 tools from server 'burp' to agent 'Red Team Agent'.
CAI&amp;gt;/agent 13
CAI&amp;gt;Create a repeater tab
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can list all active MCP connections and their transport types:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CAI&amp;gt;/mcp list
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/386a1fd3-3469-4f84-9396-2a5236febe1f"&gt;https://github.com/user-attachments/assets/386a1fd3-3469-4f84-9396-2a5236febe1f&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Development&lt;/h2&gt; 
&lt;p&gt;Development is facilitated via VS Code dev. environments. To try out our development environment, clone the repository, open VS Code and enter de dev. container mode:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/media/cai_devenv.gif" alt="CAI Development Environment" /&gt;&lt;/p&gt; 
&lt;h3&gt;Contributions&lt;/h3&gt; 
&lt;p&gt;If you want to contribute to this project, use &lt;a href="https://pre-commit.com/"&gt;&lt;strong&gt;Pre-commit&lt;/strong&gt;&lt;/a&gt; before your MR&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install pre-commit
pre-commit # files staged
pre-commit run --all-files # all files
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Optional Requirements: caiextensions&lt;/h3&gt; 
&lt;p&gt;Currently, the extensions are not publicly available as the engineering endeavour to maintain them is significant. Instead, we're making selected custom caiextensions available for partner companies across collaborations.&lt;/p&gt; 
&lt;h3&gt;&lt;span&gt;â„¹&lt;/span&gt; Usage Data Collection&lt;/h3&gt; 
&lt;p&gt;CAI is provided free of charge for researchers. To improve CAIâ€™s detection accuracy and publish open security research, instead of payment for research use cases, we ask you to contribute to the CAI community by allowing usage data collection. This data helps us identify areas for improvement, understand how the framework is being used, and prioritize new features. Legal basis of data collection is under Art. 6 (1)(f) GDPR â€” CAIâ€™s legitimate interest in maintaining and improving security tooling, with Art. 89 safeguards for research. The collected data includes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Basic system information (OS type, Python version)&lt;/li&gt; 
 &lt;li&gt;Username and IP information&lt;/li&gt; 
 &lt;li&gt;Tool usage patterns and performance metrics&lt;/li&gt; 
 &lt;li&gt;Model interactions and token usage statistics&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We take your privacy seriously and only collect what's needed to make CAI better. For further info, reach out to researchï¼ aliasrobotics.com. You can disable some of the data collection features via the &lt;code&gt;CAI_TELEMETRY&lt;/code&gt; environment variable but we encourage you to keep it enabled and contribute back to research:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CAI_TELEMETRY=False cai
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Reproduce CI-Setup locally&lt;/h3&gt; 
&lt;p&gt;To simulate the CI/CD pipeline, you can run the following in the Gitlab runner machines:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run --rm -it \
  --privileged \
  --network=exploitflow_net \
  --add-host="host.docker.internal:host-gateway" \
  -v /cache:/cache \
  -v /var/run/docker.sock:/var/run/docker.sock:rw \
  registry.gitlab.com/aliasrobotics/alias_research/cai:latest bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;FAQ&lt;/h2&gt; 
&lt;details&gt;
 &lt;summary&gt;OLLAMA is giving me 404 errors&lt;/summary&gt; 
 &lt;p&gt;Ollama's API in OpenAI mode uses &lt;code&gt;/v1/chat/completions&lt;/code&gt; whereas the &lt;code&gt;openai&lt;/code&gt; library uses &lt;code&gt;base_url&lt;/code&gt; + &lt;code&gt;/chat/completions&lt;/code&gt;.&lt;/p&gt; 
 &lt;p&gt;We adopt the latter for overall alignment with the gen AI community and empower the former by allowing users to add the &lt;code&gt;v1&lt;/code&gt; themselves via:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;OLLAMA_API_BASE=http://IP:PORT/v1
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;See the following issues that treat this topic in more detail:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/issues/76"&gt;https://github.com/aliasrobotics/cai/issues/76&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/issues/83"&gt;https://github.com/aliasrobotics/cai/issues/83&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/issues/82"&gt;https://github.com/aliasrobotics/cai/issues/82&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Where are all the caiextensions?&lt;/summary&gt; 
 &lt;p&gt;See &lt;a href="https://gitlab.com/aliasrobotics/alias_research/caiextensions"&gt;all caiextensions&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;How do I install the report caiextension?&lt;/summary&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#optional-requirements-caiextensions"&gt;See here&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;How do I set up SSH access for Gitlab?&lt;/summary&gt; 
 &lt;p&gt;Generate a new SSH key&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;ssh-keygen -t ed25519
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Add the key to the SSH agent&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;ssh-add ~/.ssh/id_ed25519
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Add the public key to Gitlab Copy the key and add it to Gitlab under &lt;a href="https://gitlab.com/-/user_settings/ssh_keys"&gt;https://gitlab.com/-/user_settings/ssh_keys&lt;/a&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;cat ~/.ssh/id_ed25519.pub
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;To verify it:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;ssh -T git@gitlab.com
Welcome to GitLab, @vmayoral!
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;How do I clear Python cache?&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;find . -name "*.pyc" -delete &amp;amp;&amp;amp; find . -name "__pycache__" -delete
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;If host networking is not working with ollama check whether it has been disabled in Docker because you are not signed in&lt;/summary&gt; 
 &lt;p&gt;Docker in OS X behaves funny sometimes. Check if the following message has shown up:&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Host networking has been disabled because you are not signed in. Please sign in to enable it&lt;/em&gt;.&lt;/p&gt; 
 &lt;p&gt;Make sure this has been addressed and also that the Dev Container is not forwarding the 8000 port (click on x, if necessary in the ports section).&lt;/p&gt; 
 &lt;p&gt;To verify connection, from within the VSCode devcontainer:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;curl -v http://host.docker.internal:8000/api/version
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Run CAI against any target&lt;/summary&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/imgs/readme_imgs/cai-004-first-message.png" alt="cai-004-first-message" /&gt;&lt;/p&gt; 
 &lt;p&gt;The starting user prompt in this case is: &lt;code&gt;Target IP: 192.168.3.10, perform a full network scan&lt;/code&gt;.&lt;/p&gt; 
 &lt;p&gt;The agent started performing a nmap scan. You could either interact with the agent and give it more instructions, or let it run to see what it explores next.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;How do I interact with the agent? Type twice CTRL + C &lt;/summary&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/imgs/readme_imgs/cai-005-ctrl-c.png" alt="cai-005-ctrl-c" /&gt;&lt;/p&gt; 
 &lt;p&gt;If you want to use the HITL mode, you can do it by presssing twice &lt;code&gt;Ctrl + C&lt;/code&gt;. This will allow you to interact (prompt) with the agent whenever you want. The agent will not lose the previous context, as it is stored in the &lt;code&gt;history&lt;/code&gt; variable, which is passed to it and any agent that is called. This enables any agent to use the previous information and be more accurate and efficient.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; Can I change the model while CAI is running? /model &lt;/summary&gt; 
 &lt;p&gt;Use &lt;code&gt;/model&lt;/code&gt; to change the model.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/imgs/readme_imgs/cai-007-model-change.png" alt="cai-007-model-change" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;How can I list all the agents available? /agent &lt;/summary&gt; 
 &lt;p&gt;Use &lt;code&gt;/agent&lt;/code&gt; to list all the agents available.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/imgs/readme_imgs/cai-010-agents-menu.png" alt="cai-010-agents-menu" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; Where can I list all the environment variables? /config &lt;/summary&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/imgs/readme_imgs/cai-008-config.png" alt="cai-008-config" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; How to know more about the CLI? /help &lt;/summary&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/imgs/readme_imgs/cai-006-help.png" alt="cai-006-help" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;How can I trace the whole execution?&lt;/summary&gt; The environment variable `CAI_TRACING` allows the user to set it to `CAI_TRACING=true` to enable tracing, or `CAI_TRACING=false` to disable it. When CAI is prompted by the first time, the user is provided with two paths, the execution log, and the tracing log. 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/imgs/readme_imgs/cai-009-logs.png" alt="cai-009-logs" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Can I expand CAI capabilities using previous run logs?&lt;/summary&gt; 
 &lt;p&gt;Yes. Today CAI performs best by relying on Inâ€‘Context Learning (ICL). Rather than building longâ€‘term stores, the recommended workflow is to load relevant prior logs directly into the current session so the model can reason with them in context.&lt;/p&gt; 
 &lt;p&gt;Use the &lt;code&gt;/load&lt;/code&gt; command to bring JSONL logs into CAIâ€™s context (this replaces the legacy memory-loading tool):&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;CAI&amp;gt;/load logs/cai_20250408_111856.jsonl         # Load into current agent
CAI&amp;gt;/load &amp;lt;file&amp;gt; agent &amp;lt;name&amp;gt;                    # Load into a specific agent
CAI&amp;gt;/load &amp;lt;file&amp;gt; all                             # Distribute across all agents
CAI&amp;gt;/load &amp;lt;file&amp;gt; parallel                        # Match to configured parallel agents
# Tip: if you omit &amp;lt;file&amp;gt;, /load uses `logs/last`. Alias: /l
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;CAI prints the path to the current runâ€™s JSONL log at startup (highlighted in orange), which you can pass to &lt;code&gt;/load&lt;/code&gt;:&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/imgs/readme_imgs/cai-009-logs.png" alt="cai-009-logs" /&gt;&lt;/p&gt; 
 &lt;p&gt;Legacy notes: earlier â€œmemory extensionâ€ mechanisms (episodic/semantic stores and offline ingestion) are retained for reference only. See &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/src/cai/agents/memory.py"&gt;src/cai/agents/memory.py&lt;/a&gt; for background and legacy details. Our current direction prioritizes ICL over persistent memory.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Can I expand CAI capabilities using scripts or extra information?&lt;/summary&gt; 
 &lt;p&gt;Currently, CAI supports text based information. You can add any extra information on the target you are facing by copy-pasting it directly into the system or user prompt.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;How?&lt;/strong&gt; By adding it to the system (&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/cai/repl/templates/system_master_template.md"&gt;&lt;code&gt;system_master_template.md&lt;/code&gt;&lt;/a&gt;) or the user prompt (&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/cai/repl/templates/user_master_template.md"&gt;&lt;code&gt;user_master_template.md&lt;/code&gt;&lt;/a&gt;). You can always directly prompt the path to the model, and it will &lt;code&gt;cat&lt;/code&gt; it.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;How CAI licence works?&lt;/summary&gt; 
 &lt;p&gt;CAIâ€™s current license does not restrict usage for research purposes. You are free to use CAI for security assessments (pentests), to develop additional features, and to integrate it into your research activities, as long as you comply with local laws.&lt;/p&gt; 
 &lt;p&gt;If you or your organization start benefiting commercially from CAI (e.g., offering pentesting services powered by CAI), then a commercial license will be required to help sustain the project.&lt;/p&gt; 
 &lt;p&gt;CAI itself is not a profit-seeking initiative. Our goal is to build a sustainable open-source project. We simply ask that those who profit from CAI contribute back and support our ongoing development.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you want to cite our work, please use the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{mayoralvilches2025caiopenbugbountyready,
      title={CAI: An Open, Bug Bounty-Ready Cybersecurity AI},
      author={VÃ­ctor Mayoral-Vilches and Luis Javier Navarrete-Lozano and MarÃ­a Sanz-GÃ³mez and Lidia Salas Espejo and MartiÃ±o Crespo-Ãlvarez and Francisco Oca-Gonzalez and Francesco Balassone and Alfonso Glera-PicÃ³n and Unai Ayucar-Carbajo and Jon Ander Ruiz-Alcalde and Stefan Rass and Martin Pinzger and Endika Gil-Uriarte},
      year={2025},
      eprint={2504.06017},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2504.06017},
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{mayoralvilches2025cybersecurityaidangerousgap,
      title={Cybersecurity AI: The Dangerous Gap Between Automation and Autonomy}, 
      author={VÃ­ctor Mayoral-Vilches},
      year={2025},
      eprint={2506.23592},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2506.23592}, 
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{mayoralvilches2025caifluencyframeworkcybersecurity,
      title={CAI Fluency: A Framework for Cybersecurity AI Fluency}, 
      author={VÃ­ctor Mayoral-Vilches and Jasmin Wachter and CristÃ³bal R. J. Veas Chavez and Cathrin Schachner and Luis Javier Navarrete-Lozano and MarÃ­a Sanz-GÃ³mez},
      year={2025},
      eprint={2508.13588},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2508.13588}, 
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{mayoralvilches2025cybersecurityaihackingai,
      title={Cybersecurity AI: Hacking the AI Hackers via Prompt Injection}, 
      author={VÃ­ctor Mayoral-Vilches and Per Mannermaa Rynning},
      year={2025},
      eprint={2508.21669},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2508.21669}, 
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;p&gt;CAI was initially developed by &lt;a href="https://aliasrobotics.com"&gt;Alias Robotics&lt;/a&gt; and co-funded by the European EIC accelerator project RIS (GA 101161136) - HORIZON-EIC-2023-ACCELERATOR-01 call. The original agentic principles are inspired from OpenAI's &lt;a href="https://github.com/openai/swarm"&gt;&lt;code&gt;swarm&lt;/code&gt;&lt;/a&gt; library and translated into newer prototypes. This project also makes use of other relevant open source building blocks including &lt;a href="https://github.com/BerriAI/litellm"&gt;&lt;code&gt;LiteLLM&lt;/code&gt;&lt;/a&gt;, and &lt;a href="https://github.com/Arize-ai/phoenix"&gt;&lt;code&gt;phoenix&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Academic Collaborations&lt;/h3&gt; 
&lt;p&gt;CAI benefits from ongoing research collaborations with academic institutions. Researchers interested in collaborative projects, dataset access, or academic licenses should contact &lt;a href="mailto:research@aliasrobotics.com"&gt;research@aliasrobotics.com&lt;/a&gt;. We provide special support for:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;PhD research projects&lt;/li&gt; 
 &lt;li&gt;Academic benchmarking studies&lt;/li&gt; 
 &lt;li&gt;Security education initiatives&lt;/li&gt; 
 &lt;li&gt;Open-source contributions from research labs&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- Footnotes --&gt; 
&lt;p&gt;[^1]: Arguably, the Chain-of-Thought agentic pattern is a special case of the Hierarchical agentic pattern. [^2]: Kamhoua, C. A., Leslie, N. O., &amp;amp; Weisman, M. J. (2018). Game theoretic modeling of advanced persistent threat in internet of things. Journal of Cyber Security and Information Systems. [^3]: Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., &amp;amp; Cao, Y. (2023, January). React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR). [^4]: Deng, G., Liu, Y., Mayoral-Vilches, V., Liu, P., Li, Y., Xu, Y., ... &amp;amp; Rass, S. (2024). {PentestGPT}: Evaluating and harnessing large language models for automated penetration testing. In 33rd USENIX Security Symposium (USENIX Security 24) (pp. 847-864).&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>OpenBMB/MiniCPM-V</title>
      <link>https://github.com/OpenBMB/MiniCPM-V</link>
      <description>&lt;p&gt;MiniCPM-V 4.5: A GPT-4o Level MLLM for Single Image, Multi Image and High-FPS Video Understanding on Your Phone&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpm_v_and_minicpm_o_title.png" width="500em" /&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;A GPT-4o Level MLLM for Single Image, Multi Image and High-FPS Video Understanding on Your Phone&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/README_zh.md"&gt;ä¸­æ–‡&lt;/a&gt; | English&lt;/strong&gt;&lt;/p&gt; 
 &lt;span style="display: inline-flex; align-items: center; margin-right: 2px;"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/wechat.png" alt="WeChat" style="margin-right: 4px;" /&gt; &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/wechat.md" target="_blank"&gt; WeChat&lt;/a&gt; &amp;nbsp;| &lt;/span&gt; &amp;nbsp; 
 &lt;span style="display: inline-flex; align-items: center; margin-left: -8px;"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/discord.png" alt="Discord" style="margin-right: 4px;" /&gt; &lt;a href="https://discord.gg/rftuRMbqzf" target="_blank"&gt; Discord&lt;/a&gt; &amp;nbsp; &lt;/span&gt; 
 &lt;p align="center"&gt; MiniCPM-V 4.5 &lt;a href="https://huggingface.co/openbmb/MiniCPM-V-4_5"&gt;ğŸ¤—&lt;/a&gt; &lt;a href="http://101.126.42.235:30910/"&gt;ğŸ¤–&lt;/a&gt; | MiniCPM-o 2.6 &lt;a href="https://huggingface.co/openbmb/MiniCPM-o-2_6"&gt;ğŸ¤—&lt;/a&gt; &lt;a href="https://minicpm-omni-webdemo-us.modelbest.cn/"&gt; ğŸ¤–&lt;/a&gt; | &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-Cookbook"&gt;ğŸ³ Cookbook&lt;/a&gt; | ğŸ“„ Technical Report (Coming Soon) &lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;MiniCPM-V&lt;/strong&gt; is a series of efficient end-side multimodal LLMs (MLLMs), which accept images, videos and text as inputs and deliver high-quality text outputs. &lt;strong&gt;MiniCPM-o&lt;/strong&gt; additionally takes audio as inputs and provides high-quality speech outputs in an end-to-end fashion. Since February 2024, we have released 7 versions of the model, aiming to achieve &lt;strong&gt;strong performance and efficient deployment&lt;/strong&gt;. The most notable models in the series currently include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;MiniCPM-V 4.5&lt;/strong&gt;: ğŸ”¥ğŸ”¥ğŸ”¥ The latest and most capable model in the MiniCPM-V series. With a total of 8B parameters, this model &lt;strong&gt;outperforms GPT-4o-latest, Gemini-2.0 Pro, and Qwen2.5-VL 72B&lt;/strong&gt; in vision-language capabilities, making it the most performant on-device multimodal model in the open-source community. This version brings &lt;strong&gt;new features including efficient high-FPS and long video understanding (up to 96x compression rate for video tokens), controllable hybrid fast/deep thinking, strong handwritten OCR and complex table/document parsing&lt;/strong&gt;. It also advances MiniCPM-V's popular features such as trustworthy behavior, multilingual support and end-side deployability.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;MiniCPM-o 2.6&lt;/strong&gt;: â­ï¸â­ï¸â­ï¸ The most capable model in the MiniCPM-o series. With a total of 8B parameters, this end-to-end model &lt;strong&gt;achieves comparable performance to GPT-4o-202405 in vision, speech, and multimodal live streaming&lt;/strong&gt;, making it one of the most versatile and performant models in the open-source community. For the new voice mode, MiniCPM-o 2.6 &lt;strong&gt;supports bilingual real-time speech conversation with configurable voices&lt;/strong&gt;, and also allows for fun capabilities such as emotion/speed/style control, end-to-end voice cloning, role play, etc. Due to its superior token density, MiniCPM-o 2.6 can for the first time &lt;strong&gt;support multimodal live streaming on end-side devices&lt;/strong&gt; such as iPad.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;News 
 &lt;!-- omit in toc --&gt;&lt;/h2&gt; 
&lt;h4&gt;ğŸ“Œ Pinned&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;[2025.09.01] â­ï¸â­ï¸â­ï¸ MiniCPM-V 4.5 has been officially supported by &lt;a href="https://github.com/ggml-org/llama.cpp/pull/15575"&gt;llama.cpp&lt;/a&gt;, &lt;a href="https://github.com/vllm-project/vllm/pull/23586"&gt;vLLM&lt;/a&gt;, and &lt;a href="https://github.com/hiyouga/LLaMA-Factory/pull/9022"&gt;LLaMA-Factory&lt;/a&gt;. You are welcome to use it directly through these official channels! Support for additional frameworks such as &lt;a href="https://github.com/ollama/ollama/pull/12078"&gt;Ollama&lt;/a&gt; and &lt;a href="https://github.com/sgl-project/sglang/pull/9610"&gt;SGLang&lt;/a&gt; is actively in progress.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2025.08.26] ğŸ”¥ğŸ”¥ğŸ”¥ We open-source MiniCPM-V 4.5, which outperforms GPT-4o-latest, Gemini-2.0 Pro, and Qwen2.5-VL 72B. It advances popular capabilities of MiniCPM-V, and brings useful new features. Try it now!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2025.08.01] â­ï¸â­ï¸â­ï¸ We open-sourced the &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook"&gt;MiniCPM-V &amp;amp; o Cookbook&lt;/a&gt;! It provides comprehensive guides for diverse user scenarios, paired with our new &lt;a href="https://minicpm-o.readthedocs.io/en/latest/index.html"&gt;Docs Site&lt;/a&gt; for smoother onboarding.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2025.06.20] â­ï¸â­ï¸â­ï¸ Our official &lt;a href="https://ollama.com/openbmb"&gt;Ollama repository&lt;/a&gt; is released. Try our latest models with &lt;a href="https://ollama.com/openbmb/minicpm-o2.6"&gt;one click&lt;/a&gt;ï¼&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2025.03.01] ğŸš€ğŸš€ğŸš€ RLAIF-V, the alignment technique of MiniCPM-o, is accepted by CVPR 2025 Highlightsï¼The &lt;a href="https://github.com/RLHF-V/RLAIF-V"&gt;code&lt;/a&gt;, &lt;a href="https://huggingface.co/datasets/openbmb/RLAIF-V-Dataset"&gt;dataset&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/2405.17220"&gt;paper&lt;/a&gt; are open-sourced!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2025.01.24] ğŸ“¢ğŸ“¢ğŸ“¢ MiniCPM-o 2.6 technical report is released! See &lt;a href="https://openbmb.notion.site/MiniCPM-o-2-6-A-GPT-4o-Level-MLLM-for-Vision-Speech-and-Multimodal-Live-Streaming-on-Your-Phone-185ede1b7a558042b5d5e45e6b237da9"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2025.01.19] ğŸ“¢ &lt;strong&gt;ATTENTION!&lt;/strong&gt; We are currently working on merging MiniCPM-o 2.6 into the official repositories of llama.cpp, Ollama, and vllm. Until the merge is complete, please USE OUR LOCAL FORKS of &lt;a href="https://github.com/OpenBMB/llama.cpp/raw/minicpm-omni/examples/llava/README-minicpmo2.6.md"&gt;llama.cpp&lt;/a&gt;, &lt;a href="https://github.com/OpenBMB/ollama/raw/minicpm-v2.6/examples/minicpm-v2.6/README.md"&gt;Ollama&lt;/a&gt;, and &lt;a href="https://github.com/OpenBMB/MiniCPM-o?tab=readme-ov-file#efficient-inference-with-llamacpp-ollama-vllm"&gt;vllm&lt;/a&gt;. &lt;strong&gt;Using the official repositories before the merge may lead to unexpected issues&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2025.01.19] â­ï¸â­ï¸â­ï¸ MiniCPM-o tops GitHub Trending and reaches top-2 on Hugging Face Trending!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2025.01.17] We have updated the usage of MiniCPM-o 2.6 int4 quantization version and resolved the model initialization error. Click &lt;a href="https://huggingface.co/openbmb/MiniCPM-o-2_6-int4"&gt;here&lt;/a&gt; and try it now!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2025.01.13] ğŸ”¥ğŸ”¥ğŸ”¥ We open-source MiniCPM-o 2.6, which matches GPT-4o-202405 on vision, speech and multimodal live streaming. It advances popular capabilities of MiniCPM-V 2.6, and supports various new fun features. Try it now!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2024.08.17] ğŸš€ğŸš€ğŸš€ MiniCPM-V 2.6 is now fully supported by &lt;a href="https://github.com/ggerganov/llama.cpp"&gt;official&lt;/a&gt; llama.cpp! GGUF models of various sizes are available &lt;a href="https://huggingface.co/openbmb/MiniCPM-V-2_6-gguf"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2024.08.06] ğŸ”¥ğŸ”¥ğŸ”¥ We open-source MiniCPM-V 2.6, which outperforms GPT-4V on single image, multi-image and video understanding. It advances popular features of MiniCPM-Llama3-V 2.5, and can support real-time video understanding on iPad. Try it now!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2024.08.03] MiniCPM-Llama3-V 2.5 technical report is released! See &lt;a href="https://arxiv.org/abs/2408.01800"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2024.05.23] ğŸ”¥ğŸ”¥ğŸ”¥ MiniCPM-V tops GitHub Trending and Hugging Face Trending! Our demo, recommended by Hugging Face Gradioâ€™s official account, is available &lt;a href="https://huggingface.co/spaces/openbmb/MiniCPM-Llama3-V-2_5"&gt;here&lt;/a&gt;. Come and try it out!&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click to view more news.&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;[2025.08.02] ğŸš€ğŸš€ğŸš€ We open-source MiniCPM-V 4.0, which outperforms GPT-4.1-mini-20250414 in image understanding. It advances popular features of MiniCPM-V 2.6, and largely improves the efficiency. We also open-source the iOS App on iPhone and iPad. Try it now!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2025.01.23] ğŸ’¡ğŸ’¡ğŸ’¡ MiniCPM-o 2.6 is now supported by &lt;a href="https://github.com/PKU-Alignment/align-anything"&gt;Align-Anything&lt;/a&gt;, a framework by PKU-Alignment Team for aligning any-to-any modality large models with human intentions. It supports DPO and SFT fine-tuning on both vision and audio. Try it now!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.08.15] We now also support multi-image SFT. For more details, please refer to the &lt;a href="https://github.com/OpenBMB/MiniCPM-V/tree/main/finetune"&gt;document&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.08.14] MiniCPM-V 2.6 now also supports &lt;a href="https://github.com/modelscope/ms-swift/issues/1613"&gt;fine-tuning&lt;/a&gt; with the SWIFT framework!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.08.10] ğŸš€ğŸš€ğŸš€ MiniCPM-Llama3-V 2.5 is now fully supported by &lt;a href="https://github.com/ggerganov/llama.cpp"&gt;official&lt;/a&gt; llama.cpp! GGUF models of various sizes are available &lt;a href="https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-gguf"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.07.19] MiniCPM-Llama3-V 2.5 supports vLLM now! See &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#inference-with-vllm"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.06.03] Now, you can run MiniCPM-Llama3-V 2.5 on multiple low VRAM GPUs(12 GB or 16 GB) by distributing the model's layers across multiple GPUs. For more details, check this &lt;a href="https://github.com/OpenBMB/MiniCPM-V/raw/main/docs/inference_on_multiple_gpus.md"&gt;link&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.05.28] ğŸš€ğŸš€ğŸš€ MiniCPM-Llama3-V 2.5 now fully supports its feature in llama.cpp and Ollama! Please pull the latest code &lt;strong&gt;of our provided forks&lt;/strong&gt; (&lt;a href="https://github.com/OpenBMB/llama.cpp/raw/minicpm-v2.5/examples/minicpmv/README.md"&gt;llama.cpp&lt;/a&gt;, &lt;a href="https://github.com/OpenBMB/ollama/tree/minicpm-v2.5/examples/minicpm-v2.5"&gt;Ollama&lt;/a&gt;). GGUF models in various sizes are available &lt;a href="https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-gguf/tree/main"&gt;here&lt;/a&gt;. MiniCPM-Llama3-V 2.5 series is &lt;strong&gt;not supported by the official repositories yet&lt;/strong&gt;, and we are working hard to merge PRs. Please stay tuned!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.05.28] ğŸ’« We now support LoRA fine-tuning for MiniCPM-Llama3-V 2.5, using only 2 V100 GPUs! See more statistics &lt;a href="https://github.com/OpenBMB/MiniCPM-V/tree/main/finetune#model-fine-tuning-memory-usage-statistics"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.05.25] MiniCPM-Llama3-V 2.5 now supports streaming outputs and customized system prompts. Try it &lt;a href="https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5#usage"&gt;here&lt;/a&gt;!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.05.24] We release the MiniCPM-Llama3-V 2.5 &lt;a href="https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-gguf"&gt;gguf&lt;/a&gt;, which supports &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#inference-with-llamacpp"&gt;llama.cpp&lt;/a&gt; inference and provides a 6~8 token/s smooth decoding on mobile phones. Try it now!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.05.23] ğŸ” We've released a comprehensive comparison between Phi-3-vision-128k-instruct and MiniCPM-Llama3-V 2.5, including benchmark evaluations, multilingual capabilities, and inference efficiency ğŸŒŸğŸ“ŠğŸŒğŸš€. Click &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/compare_with_phi-3_vision.md"&gt;here&lt;/a&gt; to view more details.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.05.20] We open-soure MiniCPM-Llama3-V 2.5, it has improved OCR capability and supports 30+ languages, representing the first end-side MLLM achieving GPT-4V level performance! We provide &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#deployment-on-mobile-phone"&gt;efficient inference&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/finetune/readme.md"&gt;simple fine-tuning&lt;/a&gt;. Try it now!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.04.23] MiniCPM-V-2.0 supports vLLM now! Click &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#inference-with-vllm"&gt;here&lt;/a&gt; to view more details.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.04.18] We create a HuggingFace Space to host the demo of MiniCPM-V 2.0 at &lt;a href="https://huggingface.co/spaces/openbmb/MiniCPM-V-2"&gt;here&lt;/a&gt;!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.04.17] MiniCPM-V-2.0 supports deploying &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#webui-demo"&gt;WebUI Demo&lt;/a&gt; now!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.04.15] MiniCPM-V-2.0 now also supports &lt;a href="https://github.com/modelscope/swift/raw/main/docs/source/Multi-Modal/minicpm-v-2%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5.md"&gt;fine-tuning&lt;/a&gt; with the SWIFT framework!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.04.12] We open-source MiniCPM-V 2.0, which achieves comparable performance with Gemini Pro in understanding scene text and outperforms strong Qwen-VL-Chat 9.6B and Yi-VL 34B on &lt;a href="https://rank.opencompass.org.cn/leaderboard-multimodal"&gt;OpenCompass&lt;/a&gt;, a comprehensive evaluation over 11 popular benchmarks. Click &lt;a href="https://openbmb.vercel.app/minicpm-v-2"&gt;here&lt;/a&gt; to view the MiniCPM-V 2.0 technical blog.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.03.14] MiniCPM-V now supports &lt;a href="https://github.com/modelscope/swift/raw/main/docs/source/Multi-Modal/minicpm-v%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5.md"&gt;fine-tuning&lt;/a&gt; with the SWIFT framework. Thanks to &lt;a href="https://github.com/Jintao-Huang"&gt;Jintao&lt;/a&gt; for the contributionï¼&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.03.01] MiniCPM-V can now be deployed on Mac!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.02.01] We open-source MiniCPM-V and OmniLMM-12B, which support efficient end-side deployment and powerful multimodal capabilities correspondingly.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;Contents 
 &lt;!-- omit in toc --&gt;&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#minicpm-v-45"&gt;MiniCPM-V 4.5&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#inference-efficiency"&gt;Inference Efficiency&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#minicpm-o-26"&gt;MiniCPM-o 2.6&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#minicpm-v--o-cookbook"&gt;MiniCPM-V &amp;amp; o Cookbook&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#chat-with-our-demo-on-gradio-"&gt;Chat with Our Demo on Gradio ğŸ¤—&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#inference"&gt;Inference&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#model-zoo"&gt;Model Zoo&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#multi-turn-conversation"&gt;Multi-turn Conversation&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#chat-with-multiple-images"&gt;Chat with Multiple Images&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#in-context-few-shot-learning"&gt;In-context Few-shot Learning&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#chat-with-video"&gt;Chat with Video&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#speech-and-audio-mode"&gt;Speech and Audio Mode&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#multimodal-live-streaming"&gt;Multimodal Live Streaming&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#inference-on-multiple-gpus"&gt;Inference on Multiple GPUs&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#inference-on-mac"&gt;Inference on Mac&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#efficient-inference-with-llamacpp-ollama-vllm"&gt;Efficient Inference with llama.cpp, Ollama, vLLM&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#fine-tuning"&gt;Fine-tuning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#awesome-work-using-minicpm-v--minicpm-o"&gt;Awesome work using MiniCPM-V &amp;amp; MiniCPM-o&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#faqs"&gt;FAQs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#limitations"&gt;Limitations&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;MiniCPM-V 4.5&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;MiniCPM-V 4.5&lt;/strong&gt; is the latest and most capable model in the MiniCPM-V series. The model is built on Qwen3-8B and SigLIP2-400M with a total of 8B parameters. It exhibits a significant performance improvement over previous MiniCPM-V and MiniCPM-o models, and introduces new useful features. Notable features of MiniCPM-V 4.5 include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;ğŸ”¥ &lt;strong&gt;State-of-the-art Vision-Language Capability.&lt;/strong&gt; MiniCPM-V 4.5 achieves an average score of 77.0 on OpenCompass, a comprehensive evaluation of 8 popular benchmarks. &lt;strong&gt;With only 8B parameters, it surpasses widely used proprietary models like GPT-4o-latest, Gemini-2.0 Pro, and strong open-source models like Qwen2.5-VL 72B&lt;/strong&gt; for vision-language capabilities, making it the most performant MLLM under 30B parameters.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ğŸ¬ &lt;strong&gt;Efficient High-FPS and Long Video Understanding.&lt;/strong&gt; Powered by a new unified 3D-Resampler over images and videos, MiniCPM-V 4.5 can now achieve 96x compression rate for video tokens, where 6 448x448 video frames can be jointly compressed into 64 video tokens (normally 1,536 tokens for most MLLMs). This means that the model can perceive significantly more video frames without increasing the LLM inference cost. This brings state-of-the-art high-FPS (up to 10FPS) video understanding and long video understanding capabilities on Video-MME, LVBench, MLVU, MotionBench, FavorBench, etc., efficiently.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;âš™ï¸ &lt;strong&gt;Controllable Hybrid Fast/Deep Thinking.&lt;/strong&gt; MiniCPM-V 4.5 supports both fast thinking for efficient frequent usage with competitive performance, and deep thinking for more complex problem solving. To cover efficiency and performance trade-offs in different user scenarios, this fast/deep thinking mode can be switched in a highly controlled fashion.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ğŸ’ª &lt;strong&gt;Strong OCR, Document Parsing and Others.&lt;/strong&gt; Based on &lt;a href="https://arxiv.org/pdf/2403.11703"&gt;LLaVA-UHD&lt;/a&gt; architecture, MiniCPM-V 4.5 can process high-resolution images with any aspect ratio and up to 1.8 million pixels (e.g., 1344x1344), using 4x fewer visual tokens than most MLLMs. The model achieves &lt;strong&gt;leading performance on OCRBench, surpassing proprietary models such as GPT-4o-latest and Gemini 2.5&lt;/strong&gt;. It also achieves state-of-the-art performance for PDF document parsing capability on OmniDocBench among general MLLMs. Based on the latest &lt;a href="https://github.com/RLHF-V/RLAIF-V/"&gt;RLAIF-V&lt;/a&gt; and &lt;a href="https://github.com/OpenBMB/VisCPM"&gt;VisCPM&lt;/a&gt; techniques, it features &lt;strong&gt;trustworthy behaviors&lt;/strong&gt;, outperforming GPT-4o-latest on MMHal-Bench, and supports &lt;strong&gt;multilingual capabilities&lt;/strong&gt; in more than 30 languages.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ğŸ’« &lt;strong&gt;Easy Usage.&lt;/strong&gt; MiniCPM-V 4.5 can be easily used in various ways: (1) &lt;a href="https://github.com/tc-mb/llama.cpp/raw/Support-MiniCPM-V-4.5/docs/multimodal/minicpmv4.5.md"&gt;llama.cpp&lt;/a&gt; and &lt;a href="https://github.com/tc-mb/ollama/tree/MIniCPM-V"&gt;ollama&lt;/a&gt; support for efficient CPU inference on local devices, (2) &lt;a href="https://huggingface.co/openbmb/MiniCPM-V-4_5-int4"&gt;int4&lt;/a&gt;, &lt;a href="https://huggingface.co/openbmb/MiniCPM-V-4_5-gguf"&gt;GGUF&lt;/a&gt; and &lt;a href="https://github.com/tc-mb/AutoAWQ"&gt;AWQ&lt;/a&gt; format quantized models in 16 sizes, (3) &lt;a href="https://github.com/tc-mb/sglang/tree/main"&gt;SGLang&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#efficient-inference-with-llamacpp-ollama-vllm"&gt;vLLM&lt;/a&gt; support for high-throughput and memory-efficient inference, (4) fine-tuning on new domains and tasks with &lt;a href="https://github.com/tc-mb/transformers/tree/main"&gt;Transformers&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/llamafactory_train_and_infer.md"&gt;LLaMA-Factory&lt;/a&gt;, (5) quick &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#chat-with-our-demo-on-gradio"&gt;local WebUI demo&lt;/a&gt;, (6) optimized &lt;a href="https://github.com/tc-mb/MiniCPM-o-demo-iOS"&gt;local iOS app&lt;/a&gt; on iPhone and iPad, and (7) online web demo on &lt;a href="http://101.126.42.235:30910/"&gt;server&lt;/a&gt;. See our &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook"&gt;Cookbook&lt;/a&gt; for full usage!&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Key Techniques 
 &lt;!-- omit in toc --&gt;&lt;/h3&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpm-v-4dot5-framework.png" , width="100%" /&gt; 
&lt;/div&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Architechture: Unified 3D-Resampler for High-density Video Compression.&lt;/strong&gt; MiniCPM-V 4.5 introduces a 3D-Resampler that overcomes the performance-efficiency trade-off in video understanding. By grouping and jointly compressing up to 6 consecutive video frames into just 64 tokens (the same token count used for a single image in MiniCPM-V series), MiniCPM-V 4.5 achieves a 96Ã— compression rate for video tokens. This allows the model to process more video frames without additional LLM computational cost, enabling high-FPS video and long video understanding. The architecture supports unified encoding for images, multi-image inputs, and videos, ensuring seamless capability and knowledge transfer.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Pre-training: Unified Learning for OCR and Knowledge from Documents.&lt;/strong&gt; Existing MLLMs learn OCR capability and knowledge from documents in isolated training approaches. We observe that the essential difference between these two training approaches is the visibility of the text in images. By dynamically corrupting text regions in documents with varying noise levels and asking the model to reconstruct the text, the model learns to adaptively and properly switch between accurate text recognition (when text is visible) and multimodal context-based knowledge reasoning (when text is heavily obscured). This eliminates reliance on error-prone document parsers in knowledge learning from documents, and prevents hallucinations from over-augmented OCR data, resulting in top-tier OCR and multimodal knowledge performance with minimal engineering overhead.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Post-training: Hybrid Fast/Deep Thinking with Multimodal RL.&lt;/strong&gt; MiniCPM-V 4.5 offers a balanced reasoning experience through two switchable modes: fast thinking for efficient daily use and deep thinking for complex tasks. Using a new hybrid reinforcement learning method, the model jointly optimizes both modes, significantly enhancing fast-mode performance without compromising deep-mode capability. Incorporated with &lt;a href="https://github.com/OpenBMB/RLPR"&gt;RLPR&lt;/a&gt; and &lt;a href="https://github.com/RLHF-V/RLAIF-V"&gt;RLAIF-V&lt;/a&gt;, it generalizes robust reasoning skills from broad multimodal data while effectively reducing hallucinations.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Evaluation 
 &lt;!-- omit in toc --&gt;&lt;/h3&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/radar_minicpm_v45.png" , width="60%" /&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv_4_5_evaluation_result.png" , width="80%" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;Inference Efficiency&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;OpenCompass&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="left"&gt; 
 &lt;table style="margin: 0px auto;"&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="left"&gt;Model&lt;/th&gt; 
    &lt;th&gt;Size&lt;/th&gt; 
    &lt;th&gt;Avg Score â†‘&lt;/th&gt; 
    &lt;th&gt;Total Inference Time â†“&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody align="center"&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;GLM-4.1V-9B-Thinking&lt;/td&gt; 
    &lt;td&gt;10.3B&lt;/td&gt; 
    &lt;td&gt;76.6&lt;/td&gt; 
    &lt;td&gt;17.5h&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;MiMo-VL-7B-RL&lt;/td&gt; 
    &lt;td&gt;8.3B&lt;/td&gt; 
    &lt;td&gt;76.4&lt;/td&gt; 
    &lt;td&gt;11h&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;MiniCPM-V 4.5&lt;/td&gt; 
    &lt;td&gt;8.7B&lt;/td&gt; 
    &lt;td&gt;&lt;b&gt;77.0&lt;/b&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;b&gt;7.5h&lt;/b&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Video-MME&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="left"&gt; 
 &lt;table style="margin: 0px auto;"&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="left"&gt;Model&lt;/th&gt; 
    &lt;th&gt;Size&lt;/th&gt; 
    &lt;th&gt;Avg Score â†‘&lt;/th&gt; 
    &lt;th&gt;Total Inference Time â†“&lt;/th&gt; 
    &lt;th&gt;GPU Mem â†“&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody align="center"&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;Qwen2.5-VL-7B-Instruct&lt;/td&gt; 
    &lt;td&gt;8.3B&lt;/td&gt; 
    &lt;td&gt;71.6&lt;/td&gt; 
    &lt;td&gt;3h&lt;/td&gt; 
    &lt;td&gt;60G&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;GLM-4.1V-9B-Thinking&lt;/td&gt; 
    &lt;td&gt;10.3B&lt;/td&gt; 
    &lt;td&gt;&lt;b&gt;73.6&lt;/b&gt;&lt;/td&gt; 
    &lt;td&gt;2.63h&lt;/td&gt; 
    &lt;td&gt;32G&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;MiniCPM-V 4.5&lt;/td&gt; 
    &lt;td&gt;8.7B&lt;/td&gt; 
    &lt;td&gt;73.5&lt;/td&gt; 
    &lt;td&gt;&lt;b&gt;0.26h&lt;/b&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;b&gt;28G&lt;/b&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;p&gt;Both Video-MME and OpenCompass were evaluated using 8Ã—A100 GPUs for inference. The reported inference time of Video-MME includes full model-side computation, and excludes the external cost of video frame extraction (dependent on specific frame extraction tools) for fair comparison.&lt;/p&gt; 
&lt;h3&gt;Examples 
 &lt;!-- omit in toc --&gt;&lt;/h3&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://www.youtube.com/watch?v=Cn23FujYMMU"&gt;&lt;img src="./assets/minicpmv4_5/MiniCPM-V 4.5-8.26_img.jpeg" , width="70%" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;div style="display: flex; flex-direction: column; align-items: center;"&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv4_5/en_case1.png" alt="en_case1" style="margin-bottom: 5px;" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv4_5/en_case2.png" alt="en_case2" style="margin-bottom: 5px;" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv4_5/en_case3.jpeg" alt="en_case3" style="margin-bottom: 5px;" /&gt; 
&lt;/div&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click to view more cases.&lt;/summary&gt; 
 &lt;div style="display: flex; flex-direction: column; align-items: center;"&gt; 
  &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv4_5/zh_extra.jpeg" alt="zh_extra" style="margin-bottom: 5px;" /&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;p&gt;We deploy MiniCPM-V 4.5 on iPad M4 with &lt;a href="https://github.com/tc-mb/MiniCPM-o-demo-iOS"&gt;iOS demo&lt;/a&gt;. The demo video is the raw screen recording without edition.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv4_5/v45_en_handwriting.gif" width="45%/" /&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv4_5/v45_en_cot.gif" width="45%/" /&gt; &lt;/p&gt;
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv4_5/v45_cn_handwriting.gif" width="45%/" /&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv4_5/v45_cn_travel.gif" width="45%/" /&gt; &lt;/p&gt;
&lt;table align="center"&gt;   
&lt;/table&gt; 
&lt;h2&gt;MiniCPM-o 2.6&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;MiniCPM-o 2.6&lt;/strong&gt; is the latest and most capable model in the MiniCPM-o series. The model is built in an end-to-end fashion based on SigLip-400M, Whisper-medium-300M, ChatTTS-200M, and Qwen2.5-7B with a total of 8B parameters. It exhibits a significant performance improvement over MiniCPM-V 2.6, and introduces new features for real-time speech conversation and multimodal live streaming. Notable features of MiniCPM-o 2.6 include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;ğŸ”¥ &lt;strong&gt;Leading Visual Capability.&lt;/strong&gt; MiniCPM-o 2.6 achieves an average score of 70.2 on OpenCompass, a comprehensive evaluation of 8 popular benchmarks. &lt;strong&gt;With only 8B parameters, it surpasses widely used proprietary models like GPT-4o-202405, Gemini 1.5 Pro, and Claude 3.5 Sonnet&lt;/strong&gt; for single image understanding. It also &lt;strong&gt;outperforms GPT-4V and Claude 3.5 Sonnet&lt;/strong&gt; in multi-image and video understanding, and shows promising in-context learning capability.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ğŸ™ &lt;strong&gt;State-of-the-art Speech Capability.&lt;/strong&gt; MiniCPM-o 2.6 supports &lt;strong&gt;bilingual real-time speech conversation with configurable voices&lt;/strong&gt; in English and Chinese. It &lt;strong&gt;outperforms GPT-4o-realtime on audio understanding tasks&lt;/strong&gt; such as ASR and STT translation, and shows &lt;strong&gt;state-of-the-art performance on speech conversation in both semantic and acoustic evaluations in the open-source community&lt;/strong&gt;. It also allows for fun features such as emotion/speed/style control, end-to-end voice cloning, role play, etc.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ğŸ¬ &lt;strong&gt;Strong Multimodal Live Streaming Capability.&lt;/strong&gt; As a new feature, MiniCPM-o 2.6 can &lt;strong&gt;accept continuous video and audio streams independent of user queries, and support real-time speech interaction&lt;/strong&gt;. It &lt;strong&gt;outperforms GPT-4o-202408 and Claude 3.5 Sonnet and shows state-of-the-art performance in the open-source community on StreamingBench&lt;/strong&gt;, a comprehensive benchmark for real-time video understanding, omni-source (video &amp;amp; audio) understanding, and multimodal contextual understanding.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ğŸ’ª &lt;strong&gt;Strong OCR Capability and Others.&lt;/strong&gt; Advancing popular visual capabilities from MiniCPM-V series, MiniCPM-o 2.6 can process images with any aspect ratio and up to 1.8 million pixels (e.g., 1344x1344). It achieves &lt;strong&gt;state-of-the-art performance on OCRBench for models under 25B, surpassing proprietary models such as GPT-4o-202405&lt;/strong&gt;. Based on the latest &lt;a href="https://github.com/RLHF-V/RLAIF-V/"&gt;RLAIF-V&lt;/a&gt; and &lt;a href="https://github.com/OpenBMB/VisCPM"&gt;VisCPM&lt;/a&gt; techniques, it features &lt;strong&gt;trustworthy behaviors&lt;/strong&gt;, outperforming GPT-4o and Claude 3.5 Sonnet on MMHal-Bench, and supports &lt;strong&gt;multilingual capabilities&lt;/strong&gt; on more than 30 languages.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ğŸš€ &lt;strong&gt;Superior Efficiency.&lt;/strong&gt; In addition to its friendly size, MiniCPM-o 2.6 also shows &lt;strong&gt;state-of-the-art token density&lt;/strong&gt; (i.e., the number of pixels encoded into each visual token). &lt;strong&gt;It produces only 640 tokens when processing a 1.8M pixel image, which is 75% fewer than most models&lt;/strong&gt;. This directly improves the inference speed, first-token latency, memory usage, and power consumption. As a result, MiniCPM-o 2.6 can efficiently support &lt;strong&gt;multimodal live streaming&lt;/strong&gt; on end-side devices such as iPads.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ğŸ’« &lt;strong&gt;Easy Usage.&lt;/strong&gt; MiniCPM-o 2.6 can be easily used in various ways: (1) &lt;a href="https://github.com/OpenBMB/llama.cpp/raw/minicpm-omni/examples/llava/README-minicpmo2.6.md"&gt;llama.cpp&lt;/a&gt; support for efficient CPU inference on local devices, (2) &lt;a href="https://huggingface.co/openbmb/MiniCPM-o-2_6-int4"&gt;int4&lt;/a&gt; and &lt;a href="https://huggingface.co/openbmb/MiniCPM-o-2_6-gguf"&gt;GGUF&lt;/a&gt; format quantized models in 16 sizes, (3) &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#efficient-inference-with-llamacpp-ollama-vllm"&gt;vLLM&lt;/a&gt; support for high-throughput and memory-efficient inference, (4) fine-tuning on new domains and tasks with &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/llamafactory_train_and_infer.md"&gt;LLaMA-Factory&lt;/a&gt;, (5) quick &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#chat-with-our-demo-on-gradio"&gt;local WebUI demo&lt;/a&gt;, and (6) online web demo on &lt;a href="https://minicpm-omni-webdemo-us.modelbest.cn/"&gt;server&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Model Architecture.&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;End-to-end Omni-modal Architecture.&lt;/strong&gt; Different modality encoders/decoders are connected and trained in an &lt;strong&gt;end-to-end&lt;/strong&gt; fashion to fully exploit rich multimodal knowledge. The model is trained in a fully end-to-end manner with only CE loss.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Omni-modal Live Streaming Mechanism.&lt;/strong&gt; (1) We change the offline modality encoder/decoders into online ones for &lt;strong&gt;streaming inputs/outputs.&lt;/strong&gt; (2) We devise a &lt;strong&gt;time-division multiplexing (TDM) mechanism&lt;/strong&gt; for omni-modality streaming processing in the LLM backbone. It divides parallel omni-modality streams into sequential info within small periodic time slices.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Configurable Speech Modeling Design.&lt;/strong&gt; We devise a multimodal system prompt, including traditional text system prompt, and &lt;strong&gt;a new audio system prompt to determine the assistant voice&lt;/strong&gt;. This enables flexible voice configurations in inference time, and also facilitates end-to-end voice cloning and description-based voice creation.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpm-o-26-framework-v2.png" , width="80%" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;Evaluation 
 &lt;!-- omit in toc --&gt;&lt;/h3&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/radar.jpg" , width="80%" /&gt; 
&lt;/div&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click to view visual understanding results.&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;Image Understanding&lt;/strong&gt;&lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;table style="margin: 0px auto;"&gt; 
   &lt;thead&gt; 
    &lt;tr&gt; 
     &lt;th align="left"&gt;Model&lt;/th&gt; 
     &lt;th&gt;Size&lt;/th&gt; 
     &lt;th&gt;Token Density&lt;sup&gt;+&lt;/sup&gt;&lt;/th&gt; 
     &lt;th&gt;OpenCompass&lt;/th&gt; 
     &lt;th&gt;OCRBench&lt;/th&gt; 
     &lt;th&gt;MathVista mini&lt;/th&gt; 
     &lt;th&gt;ChartQA&lt;/th&gt; 
     &lt;th&gt;MMVet&lt;/th&gt; 
     &lt;th&gt;MMStar&lt;/th&gt; 
     &lt;th&gt;MME&lt;/th&gt; 
     &lt;th&gt;MMB1.1 test&lt;/th&gt; 
     &lt;th&gt;AI2D&lt;/th&gt; 
     &lt;th&gt;MMMU val&lt;/th&gt; 
     &lt;th&gt;HallusionBench&lt;/th&gt; 
     &lt;th&gt;TextVQA val&lt;/th&gt; 
     &lt;th&gt;DocVQA test&lt;/th&gt; 
     &lt;th&gt;MathVerse mini&lt;/th&gt; 
     &lt;th&gt;MathVision&lt;/th&gt; 
     &lt;th&gt;MMHal Score&lt;/th&gt; 
    &lt;/tr&gt; 
   &lt;/thead&gt; 
   &lt;tbody align="center"&gt; 
    &lt;tr&gt; 
     &lt;td colspan="19" align="left"&gt;&lt;strong&gt;Proprietary&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;GPT-4o-20240513&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;1088&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;69.9&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;736&lt;/td&gt; 
     &lt;td&gt;61.3&lt;/td&gt; 
     &lt;td&gt;85.7&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;69.1&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;63.9&lt;/td&gt; 
     &lt;td&gt;2328.7&lt;/td&gt; 
     &lt;td&gt;82.2&lt;/td&gt; 
     &lt;td&gt;84.6&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;69.2&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;55.0&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;92.8&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;50.2&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;30.4&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;3.6&lt;/u&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Claude3.5-Sonnet&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;750&lt;/td&gt; 
     &lt;td&gt;67.9&lt;/td&gt; 
     &lt;td&gt;788&lt;/td&gt; 
     &lt;td&gt;61.6&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;90.8&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;66.0&lt;/td&gt; 
     &lt;td&gt;62.2&lt;/td&gt; 
     &lt;td&gt;1920.0&lt;/td&gt; 
     &lt;td&gt;78.5&lt;/td&gt; 
     &lt;td&gt;80.2&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;65.9&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;49.9&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;95.2&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;3.4&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Gemini 1.5 Pro&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;64.4&lt;/td&gt; 
     &lt;td&gt;754&lt;/td&gt; 
     &lt;td&gt;57.7&lt;/td&gt; 
     &lt;td&gt;81.3&lt;/td&gt; 
     &lt;td&gt;64.0&lt;/td&gt; 
     &lt;td&gt;59.1&lt;/td&gt; 
     &lt;td&gt;2110.6&lt;/td&gt; 
     &lt;td&gt;73.9&lt;/td&gt; 
     &lt;td&gt;79.1&lt;/td&gt; 
     &lt;td&gt;60.6&lt;/td&gt; 
     &lt;td&gt;45.6&lt;/td&gt; 
     &lt;td&gt;73.5&lt;/td&gt; 
     &lt;td&gt;86.5&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;19.2&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;GPT-4o-mini-20240718&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;1088&lt;/td&gt; 
     &lt;td&gt;64.1&lt;/td&gt; 
     &lt;td&gt;785&lt;/td&gt; 
     &lt;td&gt;52.4&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;66.9&lt;/td&gt; 
     &lt;td&gt;54.8&lt;/td&gt; 
     &lt;td&gt;2003.4&lt;/td&gt; 
     &lt;td&gt;76.0&lt;/td&gt; 
     &lt;td&gt;77.8&lt;/td&gt; 
     &lt;td&gt;60.0&lt;/td&gt; 
     &lt;td&gt;46.1&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;3.3&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td colspan="19" align="left"&gt;&lt;strong&gt;Open Source&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Cambrian-34B&lt;/td&gt; 
     &lt;td&gt;34B&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;1820&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;58.3&lt;/td&gt; 
     &lt;td&gt;591&lt;/td&gt; 
     &lt;td&gt;50.3&lt;/td&gt; 
     &lt;td&gt;75.6&lt;/td&gt; 
     &lt;td&gt;53.2&lt;/td&gt; 
     &lt;td&gt;54.2&lt;/td&gt; 
     &lt;td&gt;2049.9&lt;/td&gt; 
     &lt;td&gt;77.8&lt;/td&gt; 
     &lt;td&gt;79.5&lt;/td&gt; 
     &lt;td&gt;50.4&lt;/td&gt; 
     &lt;td&gt;41.6&lt;/td&gt; 
     &lt;td&gt;76.7&lt;/td&gt; 
     &lt;td&gt;75.5&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;GLM-4V-9B&lt;/td&gt; 
     &lt;td&gt;13B&lt;/td&gt; 
     &lt;td&gt;784&lt;/td&gt; 
     &lt;td&gt;59.1&lt;/td&gt; 
     &lt;td&gt;776&lt;/td&gt; 
     &lt;td&gt;51.1&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;58.0&lt;/td&gt; 
     &lt;td&gt;54.8&lt;/td&gt; 
     &lt;td&gt;2018.8&lt;/td&gt; 
     &lt;td&gt;67.9&lt;/td&gt; 
     &lt;td&gt;71.2&lt;/td&gt; 
     &lt;td&gt;46.9&lt;/td&gt; 
     &lt;td&gt;45.0&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Pixtral-12B&lt;/td&gt; 
     &lt;td&gt;12B&lt;/td&gt; 
     &lt;td&gt;256&lt;/td&gt; 
     &lt;td&gt;61.0&lt;/td&gt; 
     &lt;td&gt;685&lt;/td&gt; 
     &lt;td&gt;56.9&lt;/td&gt; 
     &lt;td&gt;81.8&lt;/td&gt; 
     &lt;td&gt;58.5&lt;/td&gt; 
     &lt;td&gt;54.5&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;72.7&lt;/td&gt; 
     &lt;td&gt;79.0&lt;/td&gt; 
     &lt;td&gt;51.1&lt;/td&gt; 
     &lt;td&gt;47.0&lt;/td&gt; 
     &lt;td&gt;75.7&lt;/td&gt; 
     &lt;td&gt;90.7&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;VITA-1.5&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;784&lt;/td&gt; 
     &lt;td&gt;63.3&lt;/td&gt; 
     &lt;td&gt;741&lt;/td&gt; 
     &lt;td&gt;66.2&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;52.7&lt;/td&gt; 
     &lt;td&gt;60.2&lt;/td&gt; 
     &lt;td&gt;2328.1&lt;/td&gt; 
     &lt;td&gt;76.8&lt;/td&gt; 
     &lt;td&gt;79.2&lt;/td&gt; 
     &lt;td&gt;52.6&lt;/td&gt; 
     &lt;td&gt;44.6&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;DeepSeek-VL2-27B (4B)&lt;/td&gt; 
     &lt;td&gt;27B&lt;/td&gt; 
     &lt;td&gt;672&lt;/td&gt; 
     &lt;td&gt;66.4&lt;/td&gt; 
     &lt;td&gt;809&lt;/td&gt; 
     &lt;td&gt;63.9&lt;/td&gt; 
     &lt;td&gt;86.0&lt;/td&gt; 
     &lt;td&gt;60.0&lt;/td&gt; 
     &lt;td&gt;61.9&lt;/td&gt; 
     &lt;td&gt;2253.0&lt;/td&gt; 
     &lt;td&gt;81.2&lt;/td&gt; 
     &lt;td&gt;83.8&lt;/td&gt; 
     &lt;td&gt;54.0&lt;/td&gt; 
     &lt;td&gt;45.3&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;84.2&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;93.3&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;3.0&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Qwen2-VL-7B&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;784&lt;/td&gt; 
     &lt;td&gt;67.1&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;866&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;58.2&lt;/td&gt; 
     &lt;td&gt;83.0&lt;/td&gt; 
     &lt;td&gt;62.0&lt;/td&gt; 
     &lt;td&gt;60.7&lt;/td&gt; 
     &lt;td&gt;2326.0&lt;/td&gt; 
     &lt;td&gt;81.8&lt;/td&gt; 
     &lt;td&gt;83.0&lt;/td&gt; 
     &lt;td&gt;54.1&lt;/td&gt; 
     &lt;td&gt;50.6&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;84.3&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;94.5&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;31.9&lt;/td&gt; 
     &lt;td&gt;16.3&lt;/td&gt; 
     &lt;td&gt;3.2&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;LLaVA-OneVision-72B&lt;/td&gt; 
     &lt;td&gt;72B&lt;/td&gt; 
     &lt;td&gt;182&lt;/td&gt; 
     &lt;td&gt;68.1&lt;/td&gt; 
     &lt;td&gt;741&lt;/td&gt; 
     &lt;td&gt;67.5&lt;/td&gt; 
     &lt;td&gt;83.7&lt;/td&gt; 
     &lt;td&gt;60.6&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;65.8&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;2261.0&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;85.0&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;85.6&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;56.8&lt;/td&gt; 
     &lt;td&gt;49.0&lt;/td&gt; 
     &lt;td&gt;80.5&lt;/td&gt; 
     &lt;td&gt;91.3&lt;/td&gt; 
     &lt;td&gt;39.1&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;3.5&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;InternVL2.5-8B&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;706&lt;/td&gt; 
     &lt;td&gt;68.3&lt;/td&gt; 
     &lt;td&gt;822&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;64.4&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;84.8&lt;/td&gt; 
     &lt;td&gt;62.8&lt;/td&gt; 
     &lt;td&gt;62.8&lt;/td&gt; 
     &lt;td&gt;2344.0&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;83.6&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;84.5&lt;/td&gt; 
     &lt;td&gt;56.0&lt;/td&gt; 
     &lt;td&gt;50.1&lt;/td&gt; 
     &lt;td&gt;79.1&lt;/td&gt; 
     &lt;td&gt;93.0&lt;/td&gt; 
     &lt;td&gt;39.5&lt;/td&gt; 
     &lt;td&gt;19.7&lt;/td&gt; 
     &lt;td&gt;3.4&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;MiniCPM-V 2.6&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;2822&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;65.2&lt;/td&gt; 
     &lt;td&gt;852*&lt;/td&gt; 
     &lt;td&gt;60.6&lt;/td&gt; 
     &lt;td&gt;79.4&lt;/td&gt; 
     &lt;td&gt;60.0&lt;/td&gt; 
     &lt;td&gt;57.5&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;2348.4*&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;78.0&lt;/td&gt; 
     &lt;td&gt;82.1&lt;/td&gt; 
     &lt;td&gt;49.8*&lt;/td&gt; 
     &lt;td&gt;48.1*&lt;/td&gt; 
     &lt;td&gt;80.1&lt;/td&gt; 
     &lt;td&gt;90.8&lt;/td&gt; 
     &lt;td&gt;25.7&lt;/td&gt; 
     &lt;td&gt;18.3&lt;/td&gt; 
     &lt;td&gt;3.6&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;MiniCPM-o 2.6&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;2822&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;70.2&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;897*&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;71.9*&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;86.9*&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;67.5&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;64.0&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;2372.0*&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;80.5&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;85.8&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;50.4*&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;51.9&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;82.0&lt;/td&gt; 
     &lt;td&gt;93.5&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;41.4*&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;23.1*&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;3.8&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt; 
  &lt;/table&gt; 
 &lt;/div&gt; * We evaluate this benchmark using chain-of-thought prompting. Specifically, for MME, we used this technique only for the Cognition set. 
 &lt;p&gt;&lt;sup&gt;+&lt;/sup&gt; Token Density: number of pixels encoded into each visual token at maximum resolution, i.e., # pixels at maximum resolution / # visual tokens.&lt;/p&gt; 
 &lt;p&gt;Note: For proprietary models, we calculate token density based on the image encoding charging strategy defined in the official API documentation, which provides an upper-bound estimation.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Multi-image and Video Understanding&lt;/strong&gt;&lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;table style="margin: 0px auto;"&gt; 
   &lt;thead&gt; 
    &lt;tr&gt; 
     &lt;th align="left"&gt;Model&lt;/th&gt; 
     &lt;th&gt;Size&lt;/th&gt; 
     &lt;th&gt;BLINK val&lt;/th&gt; 
     &lt;th&gt;Mantis Eval&lt;/th&gt; 
     &lt;th&gt;MIRB&lt;/th&gt; 
     &lt;th&gt;Video-MME (wo / w subs)&lt;/th&gt; 
    &lt;/tr&gt; 
   &lt;/thead&gt; 
   &lt;tbody align="center"&gt; 
    &lt;tr&gt; 
     &lt;td colspan="6" align="left"&gt;&lt;strong&gt;Proprietary&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;GPT-4o-20240513&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;68.0&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;71.9/77.2&lt;strong&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;GPT4V&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;54.6&lt;/td&gt; 
     &lt;td&gt;62.7&lt;/td&gt; 
     &lt;td&gt;53.1&lt;/td&gt; 
     &lt;td&gt;59.9/63.3&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td colspan="6" align="left"&gt;&lt;strong&gt;Open-source&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;VITA-1.5&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;45.0&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;56.1/58.7&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;LLaVA-NeXT-Interleave 14B&lt;/td&gt; 
     &lt;td&gt;14B&lt;/td&gt; 
     &lt;td&gt;52.6&lt;/td&gt; 
     &lt;td&gt;66.4&lt;/td&gt; 
     &lt;td&gt;30.2&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;LLaVA-OneVision-72B&lt;/td&gt; 
     &lt;td&gt;72B&lt;/td&gt; 
     &lt;td&gt;55.4&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;77.6&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;66.2/69.5&lt;/u&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;MANTIS 8B&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;49.1&lt;/td&gt; 
     &lt;td&gt;59.5&lt;/td&gt; 
     &lt;td&gt;34.8&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Qwen2-VL-7B&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;53.2&lt;/td&gt; 
     &lt;td&gt;69.6*&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;67.6*&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;63.3/69.0&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;InternVL2.5-8B&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;54.8&lt;/td&gt; 
     &lt;td&gt;67.7&lt;/td&gt; 
     &lt;td&gt;52.5&lt;/td&gt; 
     &lt;td&gt;64.2/66.9&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;MiniCPM-V 2.6&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;53.0&lt;/td&gt; 
     &lt;td&gt;69.1&lt;/td&gt; 
     &lt;td&gt;53.8&lt;/td&gt; 
     &lt;td&gt;60.9/63.6&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;MiniCPM-o 2.6&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;56.7&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;71.9&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;58.6&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;63.9/67.9&lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt; 
  &lt;/table&gt; 
 &lt;/div&gt; * We evaluate officially released checkpoints by ourselves. 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click to view audio understanding and speech conversation results.&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;Audio Understanding&lt;/strong&gt;&lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;table style="margin: 0px auto;"&gt; 
   &lt;thead&gt; 
    &lt;tr&gt; 
     &lt;th align="left"&gt;Task&lt;/th&gt; 
     &lt;th&gt;Size&lt;/th&gt; 
     &lt;th colspan="3"&gt;ASR (zh)&lt;/th&gt; 
     &lt;th colspan="3"&gt;ASR (en)&lt;/th&gt; 
     &lt;th colspan="2"&gt;AST&lt;/th&gt; 
     &lt;th&gt;Emotion&lt;/th&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;th align="left"&gt;Metric&lt;/th&gt; 
     &lt;td&gt;&lt;/td&gt; 
     &lt;th colspan="3"&gt;CERâ†“&lt;/th&gt; 
     &lt;th colspan="3"&gt;WERâ†“&lt;/th&gt; 
     &lt;th colspan="2"&gt;BLEUâ†‘&lt;/th&gt; 
     &lt;th&gt;ACCâ†‘&lt;/th&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;th align="left"&gt;Dataset&lt;/th&gt; 
     &lt;td&gt;&lt;/td&gt; 
     &lt;th&gt;AISHELL-1&lt;/th&gt; 
     &lt;th&gt;Fleurs zh&lt;/th&gt; 
     &lt;th&gt;WenetSpeech test-net&lt;/th&gt; 
     &lt;th&gt;LibriSpeech test-clean&lt;/th&gt; 
     &lt;th&gt;GigaSpeech&lt;/th&gt; 
     &lt;th&gt;TED-LIUM&lt;/th&gt; 
     &lt;th&gt;CoVoST en2zh&lt;/th&gt; 
     &lt;th&gt;CoVoST zh2en&lt;/th&gt; 
     &lt;th&gt;MELD emotion&lt;/th&gt; 
    &lt;/tr&gt; 
   &lt;/thead&gt; 
   &lt;tbody align="center"&gt; 
    &lt;tr&gt; 
     &lt;td colspan="11" align="left"&gt;&lt;strong&gt;Proprietary&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;GPT-4o-Realtime&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;7.3*&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;5.4*&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;28.9*&lt;/td&gt; 
     &lt;td&gt;2.6*&lt;/td&gt; 
     &lt;td&gt;12.9*&lt;/td&gt; 
     &lt;td&gt;4.8*&lt;/td&gt; 
     &lt;td&gt;37.1*&lt;/td&gt; 
     &lt;td&gt;15.7*&lt;/td&gt; 
     &lt;td&gt;33.2*&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Gemini 1.5 Pro&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;4.5*&lt;/td&gt; 
     &lt;td&gt;5.9*&lt;/td&gt; 
     &lt;td&gt;14.3*&lt;/td&gt; 
     &lt;td&gt;2.9*&lt;/td&gt; 
     &lt;td&gt;10.6*&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;3.0*&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;47.3*&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;22.6*&lt;/td&gt; 
     &lt;td&gt;48.4*&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td colspan="11" align="left"&gt;&lt;strong&gt;Open-Source&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Qwen2-Audio-7B&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;7.5&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;1.6&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;45.2&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;24.4&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;55.3&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Qwen2-Audio-7B-Instruct&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;2.6*&lt;/td&gt; 
     &lt;td&gt;6.9*&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;10.3*&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;3.1*&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;9.7&lt;/u&gt;*&lt;/td&gt; 
     &lt;td&gt;5.9*&lt;/td&gt; 
     &lt;td&gt;39.5*&lt;/td&gt; 
     &lt;td&gt;22.9*&lt;/td&gt; 
     &lt;td&gt;17.4*&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;VITA-1.5&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;2.16&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;8.4&lt;/td&gt; 
     &lt;td&gt;3.4&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;GLM-4-Voice-Base&lt;/td&gt; 
     &lt;td&gt;9B&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;2.5&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;2.8&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;MiniCPM-o 2.6&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;1.6&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;4.4&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;6.9&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;1.7&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;8.7&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;3.0&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;48.2&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;27.2&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;52.4&lt;/u&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt; 
  &lt;/table&gt; 
 &lt;/div&gt; * We evaluate officially released checkpoints by ourselves.
 &lt;br /&gt;
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;Speech Generation&lt;/strong&gt;&lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;table style="margin: 0px auto;"&gt; 
   &lt;thead&gt; 
    &lt;tr&gt; 
     &lt;th align="left"&gt;Task&lt;/th&gt; 
     &lt;th&gt;Size&lt;/th&gt; 
     &lt;th colspan="9"&gt;SpeechQA&lt;/th&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;th align="left"&gt;Metric&lt;/th&gt; 
     &lt;th&gt;&lt;/th&gt; 
     &lt;th colspan="3"&gt;ACCâ†‘&lt;/th&gt; 
     &lt;th&gt;G-Eval (10 point)â†‘&lt;/th&gt; 
     &lt;th&gt;Semantic ELO scoreâ†‘&lt;/th&gt; 
     &lt;th&gt;Acoustic ELO scoreâ†‘&lt;/th&gt; 
     &lt;th&gt;Overall ELO scoreâ†‘&lt;/th&gt; 
     &lt;th&gt;UTMOSâ†‘&lt;/th&gt; 
     &lt;th&gt;ASR-WERâ†“&lt;/th&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;th align="left"&gt;Dataset&lt;/th&gt; 
     &lt;th&gt;&lt;/th&gt; 
     &lt;th&gt;Speech Llama Q.&lt;/th&gt; 
     &lt;th&gt;Speech Web Q.&lt;/th&gt; 
     &lt;th&gt;Speech Trivia QA&lt;/th&gt; 
     &lt;th&gt;Speech AlpacaEval&lt;/th&gt; 
     &lt;th colspan="5"&gt;AudioArena&lt;/th&gt; 
    &lt;/tr&gt; 
   &lt;/thead&gt; 
   &lt;tbody align="center"&gt; 
    &lt;tr&gt; 
     &lt;td colspan="11" align="left"&gt;&lt;strong&gt;Proprietary&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;GPT-4o-Realtime&lt;/td&gt; 
     &lt;td&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;71.7&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;51.6&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;69.7&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;7.4&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;1157&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;1203&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;1200&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;4.2&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;2.3&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td colspan="11" align="left"&gt;&lt;strong&gt;Open-Source&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;GLM-4-Voice&lt;/td&gt; 
     &lt;td&gt;9B&lt;/td&gt; 
     &lt;td&gt;50.0&lt;/td&gt; 
     &lt;td&gt;32.0&lt;/td&gt; 
     &lt;td&gt;36.4&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;5.1&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;999&lt;/td&gt; 
     &lt;td&gt;1147&lt;/td&gt; 
     &lt;td&gt;1035&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;4.1&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;11.7&lt;/u&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Llama-Omni&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;45.3&lt;/td&gt; 
     &lt;td&gt;22.9&lt;/td&gt; 
     &lt;td&gt;10.7&lt;/td&gt; 
     &lt;td&gt;3.9&lt;/td&gt; 
     &lt;td&gt;960&lt;/td&gt; 
     &lt;td&gt;878&lt;/td&gt; 
     &lt;td&gt;897&lt;/td&gt; 
     &lt;td&gt;3.2&lt;/td&gt; 
     &lt;td&gt;24.3&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;VITA-1.5&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;46.7&lt;/td&gt; 
     &lt;td&gt;28.1&lt;/td&gt; 
     &lt;td&gt;23.3&lt;/td&gt; 
     &lt;td&gt;2.0&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Moshi&lt;/td&gt; 
     &lt;td&gt;7B&lt;/td&gt; 
     &lt;td&gt;43.7&lt;/td&gt; 
     &lt;td&gt;23.8&lt;/td&gt; 
     &lt;td&gt;16.7&lt;/td&gt; 
     &lt;td&gt;2.4&lt;/td&gt; 
     &lt;td&gt;871&lt;/td&gt; 
     &lt;td&gt;808&lt;/td&gt; 
     &lt;td&gt;875&lt;/td&gt; 
     &lt;td&gt;2.8&lt;/td&gt; 
     &lt;td&gt;8.2&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Mini-Omni&lt;/td&gt; 
     &lt;td&gt;1B&lt;/td&gt; 
     &lt;td&gt;22.0&lt;/td&gt; 
     &lt;td&gt;12.8&lt;/td&gt; 
     &lt;td&gt;6.9&lt;/td&gt; 
     &lt;td&gt;2.5&lt;/td&gt; 
     &lt;td&gt;926&lt;/td&gt; 
     &lt;td&gt;803&lt;/td&gt; 
     &lt;td&gt;865&lt;/td&gt; 
     &lt;td&gt;3.4&lt;/td&gt; 
     &lt;td&gt;10.0&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;MiniCPM-o 2.6&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;61.0&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;40.0&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;40.2&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;5.1&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;1088&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;1163&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;1131&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;4.2&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;9.8&lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt; 
  &lt;/table&gt; 
 &lt;/div&gt; All results are from AudioEvals, and the evaluation methods along with further details can be found in 
 &lt;a href="https://github.com/OpenBMB/UltraEval-Audio" target="_blank"&gt;AudioEvals&lt;/a&gt;.
 &lt;br /&gt;
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;End-to-end Voice Cloning&lt;/strong&gt;&lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;table style="margin: 0px auto;"&gt; 
   &lt;thead&gt; 
    &lt;tr&gt; 
     &lt;th align="left"&gt;Task&lt;/th&gt; 
     &lt;th colspan="2"&gt;Voice cloning&lt;/th&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;th align="left"&gt;Metric&lt;/th&gt; 
     &lt;th&gt;SIMOâ†‘&lt;/th&gt; 
     &lt;th&gt;SIMOâ†‘&lt;/th&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;th align="left"&gt;Dataset&lt;/th&gt; 
     &lt;th&gt;Seed-TTS test-zh&lt;/th&gt; 
     &lt;th&gt;Seed-TTS test-en&lt;/th&gt; 
    &lt;/tr&gt; 
   &lt;/thead&gt; 
   &lt;tbody align="center"&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;F5-TTS&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;76&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;67&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;CosyVoice&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;75&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;64&lt;/u&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;FireRedTTS&lt;/td&gt; 
     &lt;td&gt;63&lt;/td&gt; 
     &lt;td&gt;46&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;MiniCPM-o 2.6&lt;/td&gt; 
     &lt;td&gt;57&lt;/td&gt; 
     &lt;td&gt;47&lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt; 
  &lt;/table&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click to view multimodal live streaming results.&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;Multimodal Live Streaming&lt;/strong&gt;: results on StreamingBench&lt;/p&gt; 
 &lt;table style="margin: 0px auto;"&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="left"&gt;Model&lt;/th&gt; 
    &lt;th&gt;Size&lt;/th&gt; 
    &lt;th&gt;Real-Time Video Understanding&lt;/th&gt; 
    &lt;th&gt;Omni-Source Understanding&lt;/th&gt; 
    &lt;th&gt;Contextual Understanding&lt;/th&gt; 
    &lt;th&gt;Overall&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody align="center"&gt; 
   &lt;tr&gt; 
    &lt;td colspan="7" align="left"&gt;&lt;strong&gt;Proprietary&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;Gemini 1.5 Pro&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;&lt;u&gt;77.4&lt;/u&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;67.8&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;51.1&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;70.3&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;GPT-4o-202408&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;74.5&lt;/td&gt; 
    &lt;td&gt;51.0&lt;/td&gt; 
    &lt;td&gt;&lt;u&gt;48.0&lt;/u&gt;&lt;/td&gt; 
    &lt;td&gt;64.1&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;Claude-3.5-Sonnet&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;74.0&lt;/td&gt; 
    &lt;td&gt;41.4&lt;/td&gt; 
    &lt;td&gt;37.8&lt;/td&gt; 
    &lt;td&gt;59.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td colspan="9" align="left"&gt;&lt;strong&gt;Open-source&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;VILA-1.5&lt;/td&gt; 
    &lt;td&gt;8B&lt;/td&gt; 
    &lt;td&gt;61.5&lt;/td&gt; 
    &lt;td&gt;37.5&lt;/td&gt; 
    &lt;td&gt;26.7&lt;/td&gt; 
    &lt;td&gt;49.5&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;LongVA&lt;/td&gt; 
    &lt;td&gt;7B&lt;/td&gt; 
    &lt;td&gt;63.1&lt;/td&gt; 
    &lt;td&gt;35.9&lt;/td&gt; 
    &lt;td&gt;30.2&lt;/td&gt; 
    &lt;td&gt;50.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;LLaVA-Next-Video-34B&lt;/td&gt; 
    &lt;td&gt;34B&lt;/td&gt; 
    &lt;td&gt;69.8&lt;/td&gt; 
    &lt;td&gt;41.7&lt;/td&gt; 
    &lt;td&gt;34.3&lt;/td&gt; 
    &lt;td&gt;56.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;Qwen2-VL-7B&lt;/td&gt; 
    &lt;td&gt;8B&lt;/td&gt; 
    &lt;td&gt;71.2&lt;/td&gt; 
    &lt;td&gt;40.7&lt;/td&gt; 
    &lt;td&gt;33.1&lt;/td&gt; 
    &lt;td&gt;57.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;InternVL2-8B&lt;/td&gt; 
    &lt;td&gt;8B&lt;/td&gt; 
    &lt;td&gt;70.1&lt;/td&gt; 
    &lt;td&gt;42.7&lt;/td&gt; 
    &lt;td&gt;34.1&lt;/td&gt; 
    &lt;td&gt;57.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;VITA-1.5&lt;/td&gt; 
    &lt;td&gt;8B&lt;/td&gt; 
    &lt;td&gt;70.9&lt;/td&gt; 
    &lt;td&gt;40.8&lt;/td&gt; 
    &lt;td&gt;35.8&lt;/td&gt; 
    &lt;td&gt;57.4&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;LLaVA-OneVision-7B&lt;/td&gt; 
    &lt;td&gt;8B&lt;/td&gt; 
    &lt;td&gt;74.3&lt;/td&gt; 
    &lt;td&gt;40.8&lt;/td&gt; 
    &lt;td&gt;31.0&lt;/td&gt; 
    &lt;td&gt;58.4&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;InternLM-XC2.5-OL-7B&lt;/td&gt; 
    &lt;td&gt;8B&lt;/td&gt; 
    &lt;td&gt;75.4&lt;/td&gt; 
    &lt;td&gt;46.2&lt;/td&gt; 
    &lt;td&gt;33.6&lt;/td&gt; 
    &lt;td&gt;60.8&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;MiniCPM-V 2.6&lt;/td&gt; 
    &lt;td&gt;8B&lt;/td&gt; 
    &lt;td&gt;72.4&lt;/td&gt; 
    &lt;td&gt;40.2&lt;/td&gt; 
    &lt;td&gt;33.4&lt;/td&gt; 
    &lt;td&gt;57.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;MiniCPM-o 2.6&lt;/td&gt; 
    &lt;td&gt;8B&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;79.9&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;u&gt;53.4&lt;/u&gt;&lt;/td&gt; 
    &lt;td&gt;38.5&lt;/td&gt; 
    &lt;td&gt;&lt;u&gt;66.0&lt;/u&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h3&gt;Examples 
 &lt;!-- omit in toc --&gt;&lt;/h3&gt; 
&lt;p&gt;We deploy MiniCPM-o 2.6 on end devices. The demo video is the raw-speed recording on an iPad Pro and a Web demo.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://www.youtube.com/watch?v=vRIMbxJzStY&amp;amp;t=2s"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmo2_6/2dot6_o_demo_video_img.png" , width="70%" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;div style="display: flex; flex-direction: column; align-items: center;"&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmo2_6/minicpmo2_6_math_intersect.png" alt="math" style="margin-bottom: 5px;" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmo2_6/minicpmo2_6_diagram_train_NN.png" alt="diagram" style="margin-bottom: 5px;" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmo2_6/minicpmo2_6_multi-image_bike.png" alt="bike" style="margin-bottom: 5px;" /&gt; 
&lt;/div&gt; 
&lt;h2&gt;Legacy Models 
 &lt;!-- omit in toc --&gt;&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Model&lt;/th&gt; 
   &lt;th align="center"&gt;Introduction and Guidance&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-V 4.0&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/minicpm_v4_en.md"&gt;Document&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-V 2.6&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/minicpm_v2dot6_en.md"&gt;Document&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-Llama3-V 2.5&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/minicpm_llama3_v2dot5.md"&gt;Document&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-V 2.0&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/minicpm_v2.md"&gt;Document&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-V 1.0&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/minicpm_v1.md"&gt;Document&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;OmniLMM-12B&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/omnilmm_en.md"&gt;Document&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;MiniCPM-V &amp;amp; o Cookbook&lt;/h2&gt; 
&lt;p&gt;Discover comprehensive, ready-to-deploy solutions for the MiniCPM-V and MiniCPM-o model series in our structured &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook"&gt;cookbook&lt;/a&gt;, which empowers developers to rapidly implement multimodal AI applications with integrated vision, speech, and live-streaming capabilities. Key features include:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Easy Usage Documentation&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Our comprehensive &lt;a href="https://minicpm-o.readthedocs.io/en/latest/index.html"&gt;documentation website&lt;/a&gt; presents every recipe in a clear, well-organized manner. All features are displayed at a glance, making it easy for you to quickly find exactly what you need.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Broad User Spectrum&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;We support a wide range of users, from individuals to enterprises and researchers.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Individuals&lt;/strong&gt;: Enjoy effortless inference using &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/deployment/ollama/minicpm-v4_ollama.md"&gt;Ollama&lt;/a&gt; and &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/deployment/llama.cpp/minicpm-v4_llamacpp.md"&gt;Llama.cpp&lt;/a&gt; with minimal setup.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Enterprises&lt;/strong&gt;: Achieve high-throughput, scalable performance with &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/deployment/vllm/minicpm-v4_vllm.md"&gt;vLLM&lt;/a&gt; and &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/deployment/sglang/MiniCPM-v4_sglang.md"&gt;SGLang&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Researchers&lt;/strong&gt;: Leverage advanced frameworks including &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/finetune/finetune_full.md"&gt;Transformers&lt;/a&gt;, &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/finetune/finetune_llamafactory.md"&gt;LLaMA-Factory&lt;/a&gt;, &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/finetune/swift.md"&gt;SWIFT&lt;/a&gt;, and &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/finetune/align_anything.md"&gt;Align-anything&lt;/a&gt; to enable flexible model development and cutting-edge experimentation.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Versatile Deployment Scenarios&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Our ecosystem delivers optimal solution for a variety of hardware environments and deployment demands.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Web demo&lt;/strong&gt;: Launch interactive multimodal AI web demo with &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/demo/README.md"&gt;FastAPI&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Quantized deployment&lt;/strong&gt;: Maximize efficiency and minimize resource consumption using &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/quantization/gguf/minicpm-v4_gguf_quantize.md"&gt;GGUF&lt;/a&gt; and &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/quantization/bnb/minicpm-v4_bnb_quantize.md"&gt;BNB&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;End devices&lt;/strong&gt;: Bring powerful AI experiences to &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/demo/ios_demo/ios.md"&gt;iPhone and iPad&lt;/a&gt;, supporting offline and privacy-sensitive applications.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Chat with Our Demo on Gradio ğŸ¤—&lt;/h2&gt; 
&lt;p&gt;We provide online and local demos powered by Hugging Face Gradio &lt;a href="https://github.com/gradio-app/gradio"&gt;&lt;img src="https://img.shields.io/github/stars/gradio-app/gradio" /&gt;&lt;/a&gt;, the most popular model deployment framework nowadays. It supports streaming outputs, progress bars, queuing, alerts, and other useful features.&lt;/p&gt; 
&lt;h3&gt;Online Demo 
 &lt;!-- omit in toc --&gt;&lt;/h3&gt; 
&lt;p&gt;Click here to try out the online demo of &lt;a href="https://minicpm-omni-webdemo-us.modelbest.cn/"&gt;MiniCPM-o 2.6&lt;/a&gt; | &lt;a href="http://120.92.209.146:8887/"&gt;MiniCPM-V 2.6&lt;/a&gt; | &lt;a href="https://huggingface.co/spaces/openbmb/MiniCPM-Llama3-V-2_5"&gt;MiniCPM-Llama3-V 2.5&lt;/a&gt; | &lt;a href="https://huggingface.co/spaces/openbmb/MiniCPM-V-2"&gt;MiniCPM-V 2.0&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Local WebUI Demo 
 &lt;!-- omit in toc --&gt;&lt;/h3&gt; 
&lt;p&gt;You can easily build your own local WebUI demo using the following commands.&lt;/p&gt; 
&lt;p&gt;Please ensure that &lt;code&gt;transformers==4.44.2&lt;/code&gt; is installed, as other versions may have compatibility issues.&lt;/p&gt; 
&lt;p&gt;If you are using an older version of PyTorch, you might encounter this issue &lt;code&gt;"weight_norm_fwd_first_dim_kernel" not implemented for 'BFloat16'&lt;/code&gt;, Please add &lt;code&gt;self.minicpmo_model.tts.float()&lt;/code&gt; during the model initialization.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;For real-time voice/video call demo:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;launch model server:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install -r requirements_o2.6.txt

python web_demos/minicpm-o_2.6/model_server.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;launch web server:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# Make sure Node and PNPM is installed.
sudo apt-get update
sudo apt-get install nodejs npm
npm install -g pnpm


cd web_demos/minicpm-o_2.6/web_server
# create ssl cert for https, https is required to request camera and microphone permissions.
bash ./make_ssl_cert.sh  # output key.pem and cert.pem

pnpm install  # install requirements
pnpm run dev  # start server
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Open &lt;code&gt;https://localhost:8088/&lt;/code&gt; in browser and enjoy the real-time voice/video call.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;For chatbot demo:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install -r requirements_o2.6.txt

python web_demos/minicpm-o_2.6/chatbot_web_demo_o2.6.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Open &lt;code&gt;http://localhost:8000/&lt;/code&gt; in browser and enjoy the vision mode chatbot.&lt;/p&gt; 
&lt;h2&gt;Inference&lt;/h2&gt; 
&lt;h3&gt;Model Zoo&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Model&lt;/th&gt; 
   &lt;th align="center"&gt;Device&lt;/th&gt; 
   &lt;th align="center"&gt;Memory&lt;/th&gt; 
   &lt;th align="left"&gt;â€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒ Description&lt;/th&gt; 
   &lt;th align="center"&gt;Download&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-V 4.5&lt;/td&gt; 
   &lt;td align="center"&gt;GPU&lt;/td&gt; 
   &lt;td align="center"&gt;18 GB&lt;/td&gt; 
   &lt;td align="left"&gt;The latest version, strong end-side multimodal performance for single image, multi-image and video understanding.&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/openbmb/MiniCPM-V-4_5"&gt;ğŸ¤—&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href="https://modelscope.cn/models/OpenBMB/MiniCPM-V-4_5"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelscope_logo.png" width="20px" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-V 4.5 gguf&lt;/td&gt; 
   &lt;td align="center"&gt;CPU&lt;/td&gt; 
   &lt;td align="center"&gt;8 GB&lt;/td&gt; 
   &lt;td align="left"&gt;The gguf version, lower memory usage and faster inference.&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/openbmb/MiniCPM-V-4_5-gguf"&gt;ğŸ¤—&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href="https://modelscope.cn/models/OpenBMB/MiniCPM-V-4_5-gguf"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelscope_logo.png" width="20px" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-V 4.5 int4&lt;/td&gt; 
   &lt;td align="center"&gt;GPU&lt;/td&gt; 
   &lt;td align="center"&gt;9 GB&lt;/td&gt; 
   &lt;td align="left"&gt;The int4 quantized version, lower GPU memory usage.&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/openbmb/MiniCPM-V-4_5-int4"&gt;ğŸ¤—&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href="https://modelscope.cn/models/OpenBMB/MiniCPM-V-4_5-int4"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelscope_logo.png" width="20px" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-V 4.5 AWQ&lt;/td&gt; 
   &lt;td align="center"&gt;GPU&lt;/td&gt; 
   &lt;td align="center"&gt;9 GB&lt;/td&gt; 
   &lt;td align="left"&gt;The int4 quantized version, lower GPU memory usage.&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/openbmb/MiniCPM-V-4_5-AWQ"&gt;ğŸ¤—&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href="https://modelscope.cn/models/OpenBMB/MiniCPM-V-4_5-AWQ"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelscope_logo.png" width="20px" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-o 2.6&lt;/td&gt; 
   &lt;td align="center"&gt;GPU&lt;/td&gt; 
   &lt;td align="center"&gt;18 GB&lt;/td&gt; 
   &lt;td align="left"&gt;The latest version, achieving GPT-4o level performance for vision, speech and multimodal live streaming on end-side devices.&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/openbmb/MiniCPM-o-2_6"&gt;ğŸ¤—&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href="https://modelscope.cn/models/OpenBMB/MiniCPM-o-2_6"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelscope_logo.png" width="20px" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-o 2.6 gguf&lt;/td&gt; 
   &lt;td align="center"&gt;CPU&lt;/td&gt; 
   &lt;td align="center"&gt;8 GB&lt;/td&gt; 
   &lt;td align="left"&gt;The gguf version, lower memory usage and faster inference.&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/openbmb/MiniCPM-o-2_6-gguf"&gt;ğŸ¤—&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href="https://modelscope.cn/models/OpenBMB/MiniCPM-o-2_6-gguf"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelscope_logo.png" width="20px" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-o 2.6 int4&lt;/td&gt; 
   &lt;td align="center"&gt;GPU&lt;/td&gt; 
   &lt;td align="center"&gt;9 GB&lt;/td&gt; 
   &lt;td align="left"&gt;The int4 quantized version, lower GPU memory usage.&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/openbmb/MiniCPM-o-2_6-int4"&gt;ğŸ¤—&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href="https://modelscope.cn/models/OpenBMB/MiniCPM-o-2_6-int4"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelscope_logo.png" width="20px" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Multi-turn Conversation&lt;/h3&gt; 
&lt;p&gt;If you wish to enable long-thinking mode, provide the argument &lt;code&gt;enable_thinking=True&lt;/code&gt; to the chat function.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install -r requirements_o2.6.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Please refer to the following codes to run.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmo2_6/show_demo.jpg" width="500px" /&gt; 
&lt;/div&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import torch
from PIL import Image
from transformers import AutoModel, AutoTokenizer

torch.manual_seed(100)

model = AutoModel.from_pretrained('openbmb/MiniCPM-V-4_5', trust_remote_code=True, # or openbmb/MiniCPM-o-2_6
    attn_implementation='sdpa', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager
model = model.eval().cuda()
tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-4_5', trust_remote_code=True) # or openbmb/MiniCPM-o-2_6

image = Image.open('./assets/minicpmo2_6/show_demo.jpg').convert('RGB')

enable_thinking=False # If `enable_thinking=True`, the long-thinking mode is enabled.

# First round chat 
question = "What is the landform in the picture?"
msgs = [{'role': 'user', 'content': [image, question]}]

answer = model.chat(
    msgs=msgs,
    tokenizer=tokenizer,
    enable_thinking=enable_thinking
)
print(answer)

# Second round chat, pass history context of multi-turn conversation
msgs.append({"role": "assistant", "content": [answer]})
msgs.append({"role": "user", "content": ["What should I pay attention to when traveling here?"]})

answer = model.chat(
    msgs=msgs,
    tokenizer=tokenizer
)
print(answer)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You will get the following output:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# round1
The landform in the picture is karst topography. Karst landscapes are characterized by distinctive, jagged limestone hills or mountains with steep, irregular peaks and deep valleysâ€”exactly what you see here These unique formations result from the dissolution of soluble rocks like limestone over millions of years through water erosion.

This scene closely resembles the famous karst landscape of Guilin and Yangshuo in Chinaâ€™s Guangxi Province. The area features dramatic, pointed limestone peaks rising dramatically above serene rivers and lush green forests, creating a breathtaking and iconic natural beauty that attracts millions of visitors each year for its picturesque views.

# round2
When traveling to a karst landscape like this, here are some important tips:

1. Wear comfortable shoes: The terrain can be uneven and hilly.
2. Bring water and snacks for energy during hikes or boat rides.
3. Protect yourself from the sun with sunscreen, hats, and sunglassesâ€”especially since youâ€™ll likely spend time outdoors exploring scenic spots.
4. Respect local customs and nature regulations by not littering or disturbing wildlife.

By following these guidelines, you'll have a safe and enjoyable trip while appreciating the stunning natural beauty of places such as Guilinâ€™s karst mountains.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Chat with Multiple Images&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt; Click to view Python code running MiniCPM-V-4_5 with multiple images input. &lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import torch
from PIL import Image
from transformers import AutoModel, AutoTokenizer

model = AutoModel.from_pretrained('openbmb/MiniCPM-V-4_5', trust_remote_code=True,  # or openbmb/MiniCPM-o-2_6
    attn_implementation='sdpa', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager
model = model.eval().cuda()
tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-4_5', trust_remote_code=True)  # or openbmb/MiniCPM-o-2_6

image1 = Image.open('image1.jpg').convert('RGB')
image2 = Image.open('image2.jpg').convert('RGB')
question = 'Compare image 1 and image 2, tell me about the differences between image 1 and image 2.'

msgs = [{'role': 'user', 'content': [image1, image2, question]}]

answer = model.chat(
    msgs=msgs,
    tokenizer=tokenizer
)
print(answer)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h4&gt;In-context Few-shot Learning&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt; Click to view Python code running MiniCPM-V-4_5 with few-shot input. &lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import torch
from PIL import Image
from transformers import AutoModel, AutoTokenizer

model = AutoModel.from_pretrained('openbmb/MiniCPM-V-4_5', trust_remote_code=True,  # or openbmb/MiniCPM-o-2_6
    attn_implementation='sdpa', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager
model = model.eval().cuda()
tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-4_5', trust_remote_code=True)  # or openbmb/MiniCPM-o-2_6

question = "production date" 
image1 = Image.open('example1.jpg').convert('RGB')
answer1 = "2023.08.04"
image2 = Image.open('example2.jpg').convert('RGB')
answer2 = "2007.04.24"
image_test = Image.open('test.jpg').convert('RGB')

msgs = [
    {'role': 'user', 'content': [image1, question]}, {'role': 'assistant', 'content': [answer1]},
    {'role': 'user', 'content': [image2, question]}, {'role': 'assistant', 'content': [answer2]},
    {'role': 'user', 'content': [image_test, question]}
]

answer = model.chat(
    msgs=msgs,
    tokenizer=tokenizer
)
print(answer)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h4&gt;Chat with Video&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt; Click to view Python code running MiniCPM-V-4_5 by with video input and 3D-Resampler. &lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;## The 3d-resampler compresses multiple frames into 64 tokens by introducing temporal_ids. 
# To achieve this, you need to organize your video data into two corresponding sequences: 
#   frames: List[Image]
#   temporal_ids: List[List[Int]].

import torch
from PIL import Image
from transformers import AutoModel, AutoTokenizer
from decord import VideoReader, cpu    # pip install decord
from scipy.spatial import cKDTree
import numpy as np
import math

model = AutoModel.from_pretrained('openbmb/MiniCPM-V-4_5', trust_remote_code=True,  # or openbmb/MiniCPM-o-2_6
    attn_implementation='sdpa', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager
model = model.eval().cuda()
tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-4_5', trust_remote_code=True)  # or openbmb/MiniCPM-o-2_6

MAX_NUM_FRAMES=180 # Indicates the maximum number of frames received after the videos are packed. The actual maximum number of valid frames is MAX_NUM_FRAMES * MAX_NUM_PACKING.
MAX_NUM_PACKING=3  # indicates the maximum packing number of video frames. valid range: 1-6
TIME_SCALE = 0.1 

def map_to_nearest_scale(values, scale):
    tree = cKDTree(np.asarray(scale)[:, None])
    _, indices = tree.query(np.asarray(values)[:, None])
    return np.asarray(scale)[indices]


def group_array(arr, size):
    return [arr[i:i+size] for i in range(0, len(arr), size)]

def encode_video(video_path, choose_fps=3, force_packing=None):
    def uniform_sample(l, n):
        gap = len(l) / n
        idxs = [int(i * gap + gap / 2) for i in range(n)]
        return [l[i] for i in idxs]
    vr = VideoReader(video_path, ctx=cpu(0))
    fps = vr.get_avg_fps()
    video_duration = len(vr) / fps
        
    if choose_fps * int(video_duration) &amp;lt;= MAX_NUM_FRAMES:
        packing_nums = 1
        choose_frames = round(min(choose_fps, round(fps)) * min(MAX_NUM_FRAMES, video_duration))
        
    else:
        packing_nums = math.ceil(video_duration * choose_fps / MAX_NUM_FRAMES)
        if packing_nums &amp;lt;= MAX_NUM_PACKING:
            choose_frames = round(video_duration * choose_fps)
        else:
            choose_frames = round(MAX_NUM_FRAMES * MAX_NUM_PACKING)
            packing_nums = MAX_NUM_PACKING

    frame_idx = [i for i in range(0, len(vr))]      
    frame_idx =  np.array(uniform_sample(frame_idx, choose_frames))

    if force_packing:
        packing_nums = min(force_packing, MAX_NUM_PACKING)
    
    print(video_path, ' duration:', video_duration)
    print(f'get video frames={len(frame_idx)}, packing_nums={packing_nums}')
    
    frames = vr.get_batch(frame_idx).asnumpy()

    frame_idx_ts = frame_idx / fps
    scale = np.arange(0, video_duration, TIME_SCALE)

    frame_ts_id = map_to_nearest_scale(frame_idx_ts, scale) / TIME_SCALE
    frame_ts_id = frame_ts_id.astype(np.int32)

    assert len(frames) == len(frame_ts_id)

    frames = [Image.fromarray(v.astype('uint8')).convert('RGB') for v in frames]
    frame_ts_id_group = group_array(frame_ts_id, packing_nums)
    
    return frames, frame_ts_id_group


video_path="video_test.mp4"
fps = 5 # fps for video
force_packing = None # You can set force_packing to ensure that 3D packing is forcibly enabled; otherwise, encode_video will dynamically set the packing quantity based on the duration.
frames, frame_ts_id_group = encode_video(video_path, fps, force_packing=force_packing)

question = "Describe the video"
msgs = [
    {'role': 'user', 'content': frames + [question]}, 
]


answer = model.chat(
    msgs=msgs,
    tokenizer=tokenizer,
    use_image_id=False,
    max_slice_nums=1,
    temporal_ids=frame_ts_id_group
)
print(answer)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h4&gt;Speech and Audio Mode&lt;/h4&gt; 
&lt;p&gt;Model initialization&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import torch
import librosa
from transformers import AutoModel, AutoTokenizer

model = AutoModel.from_pretrained('openbmb/MiniCPM-o-2_6', trust_remote_code=True,
    attn_implementation='sdpa', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager
model = model.eval().cuda()
tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-o-2_6', trust_remote_code=True)

model.init_tts()
model.tts.float()
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h5&gt;Mimick 
 &lt;!-- omit in toc --&gt;&lt;/h5&gt; 
&lt;p&gt;&lt;code&gt;Mimick&lt;/code&gt; task reflects a model's end-to-end speech modeling capability. The model takes audio input, and outputs an ASR transcription and subsequently reconstructs the original audio with high similarity. The higher the similarity between the reconstructed audio and the original audio, the stronger the model's foundational capability in end-to-end speech modeling.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;mimick_prompt = "Please repeat each user's speech, including voice style and speech content."
audio_input, _ = librosa.load('./assets/input_examples/Trump_WEF_2018_10s.mp3', sr=16000, mono=True) # load the audio to be mimicked

# `./assets/input_examples/fast-pace.wav`, 
# `./assets/input_examples/chi-english-1.wav` 
# `./assets/input_examples/exciting-emotion.wav` 
# for different aspects of speech-centric features.

msgs = [{'role': 'user', 'content': [mimick_prompt, audio_input]}]
res = model.chat(
    msgs=msgs,
    tokenizer=tokenizer,
    sampling=True,
    max_new_tokens=128,
    use_tts_template=True,
    temperature=0.3,
    generate_audio=True,
    output_audio_path='output_mimick.wav', # save the tts result to output_audio_path
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h5&gt;General Speech Conversation with Configurable Voices 
 &lt;!-- omit in toc --&gt;&lt;/h5&gt; 
&lt;p&gt;A general usage scenario of &lt;code&gt;MiniCPM-o-2.6&lt;/code&gt; is role-playing a specific character based on the audio prompt. It will mimic the voice of the character to some extent and act like the character in text, including language style. In this mode, &lt;code&gt;MiniCPM-o-2.6&lt;/code&gt; sounds &lt;strong&gt;more natural and human-like&lt;/strong&gt;. Self-defined audio prompts can be used to customize the voice of the character in an end-to-end manner.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;ref_audio, _ = librosa.load('./assets/input_examples/icl_20.wav', sr=16000, mono=True) # load the reference audio
sys_prompt = model.get_sys_prompt(ref_audio=ref_audio, mode='audio_roleplay', language='en')

# round one
user_question = {'role': 'user', 'content': [librosa.load('xxx.wav', sr=16000, mono=True)[0]]}
msgs = [sys_prompt, user_question]
res = model.chat(
    msgs=msgs,
    tokenizer=tokenizer,
    sampling=True,
    max_new_tokens=128,
    use_tts_template=True,
    generate_audio=True,
    temperature=0.3,
    output_audio_path='result_roleplay_round_1.wav',
)

# round two
history = msgs.append({'role': 'assistant', 'content': res})
user_question = {'role': 'user', 'content': [librosa.load('xxx.wav', sr=16000, mono=True)[0]]}
msgs = history.append(user_question)
res = model.chat(
    msgs=msgs,
    tokenizer=tokenizer,
    sampling=True,
    max_new_tokens=128,
    use_tts_template=True,
    generate_audio=True,
    temperature=0.3,
    output_audio_path='result_roleplay_round_2.wav',
)
print(res)
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h5&gt;Speech Conversation as an AI Assistant 
 &lt;!-- omit in toc --&gt;&lt;/h5&gt; 
&lt;p&gt;An enhanced feature of &lt;code&gt;MiniCPM-o-2.6&lt;/code&gt; is to act as an AI assistant, but only with limited choice of voices. In this mode, &lt;code&gt;MiniCPM-o-2.6&lt;/code&gt; is &lt;strong&gt;less human-like and more like a voice assistant&lt;/strong&gt;. In this mode, the model is more instruction-following. For demo, you are suggested to use &lt;code&gt;assistant_female_voice&lt;/code&gt;, &lt;code&gt;assistant_male_voice&lt;/code&gt;, and &lt;code&gt;assistant_default_female_voice&lt;/code&gt;. Other voices may work but not as stable as the default voices.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Please note that, &lt;code&gt;assistant_female_voice&lt;/code&gt; and &lt;code&gt;assistant_male_voice&lt;/code&gt; are more stable but sounds like robots, while &lt;code&gt;assistant_default_female_voice&lt;/code&gt; is more human-alike but not stable, its voice often changes in multiple turns. We suggest you to try stable voices &lt;code&gt;assistant_female_voice&lt;/code&gt; and &lt;code&gt;assistant_male_voice&lt;/code&gt;.&lt;/em&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;ref_audio, _ = librosa.load('./assets/input_examples/assistant_female_voice.wav', sr=16000, mono=True) # or use `./assets/input_examples/assistant_male_voice.wav`
sys_prompt = model.get_sys_prompt(ref_audio=ref_audio, mode='audio_assistant', language='en') 
user_question = {'role': 'user', 'content': [librosa.load('xxx.wav', sr=16000, mono=True)[0]]} # load the user's audio question

# round one
msgs = [sys_prompt, user_question]
res = model.chat(
    msgs=msgs,
    tokenizer=tokenizer,
    sampling=True,
    max_new_tokens=128,
    use_tts_template=True,
    generate_audio=True,
    temperature=0.3,
    output_audio_path='result_assistant_round_1.wav',
)

# round two
history = msgs.append({'role': 'assistant', 'content': res})
user_question = {'role': 'user', 'content': [librosa.load('xxx.wav', sr=16000, mono=True)[0]]}
msgs = history.append(user_question)
res = model.chat(
    msgs=msgs,
    tokenizer=tokenizer,
    sampling=True,
    max_new_tokens=128,
    use_tts_template=True,
    generate_audio=True,
    temperature=0.3,
    output_audio_path='result_assistant_round_2.wav',
)
print(res)
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h5&gt;Instruction-to-Speech 
 &lt;!-- omit in toc --&gt;&lt;/h5&gt; 
&lt;p&gt;&lt;code&gt;MiniCPM-o-2.6&lt;/code&gt; can also do Instruction-to-Speech, aka &lt;strong&gt;Voice Creation&lt;/strong&gt;. You can describe a voice in detail, and the model will generate a voice that matches the description. For more Instruction-to-Speech sample instructions, you can refer to &lt;a href="https://voxinstruct.github.io/VoxInstruct/"&gt;https://voxinstruct.github.io/VoxInstruct/&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;instruction = 'Speak like a male charming superstar, radiating confidence and style in every word.'

msgs = [{'role': 'user', 'content': [instruction]}]

res = model.chat(
    msgs=msgs,
    tokenizer=tokenizer,
    sampling=True,
    max_new_tokens=128,
    use_tts_template=True,
    generate_audio=True,
    temperature=0.3,
    output_audio_path='result_voice_creation.wav',
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h5&gt;Voice Cloning 
 &lt;!-- omit in toc --&gt;&lt;/h5&gt; 
&lt;p&gt;&lt;code&gt;MiniCPM-o-2.6&lt;/code&gt; can also do zero-shot text-to-speech, aka &lt;strong&gt;Voice Cloning&lt;/strong&gt;. With this mode, model will act like a TTS model.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;ref_audio, _ = librosa.load('./assets/input_examples/icl_20.wav', sr=16000, mono=True) # load the reference audio
sys_prompt = model.get_sys_prompt(ref_audio=ref_audio, mode='voice_cloning', language='en')
text_prompt = f"Please read the text below."
user_question = {'role': 'user', 'content': [text_prompt, "content that you want to read"]}

msgs = [sys_prompt, user_question]
res = model.chat(
    msgs=msgs,
    tokenizer=tokenizer,
    sampling=True,
    max_new_tokens=128,
    use_tts_template=True,
    generate_audio=True,
    temperature=0.3,
    output_audio_path='result_voice_cloning.wav',
)

&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h5&gt;Addressing Various Audio Understanding Tasks 
 &lt;!-- omit in toc --&gt;&lt;/h5&gt; 
&lt;p&gt;&lt;code&gt;MiniCPM-o-2.6&lt;/code&gt; can also be used to address various audio understanding tasks, such as ASR, speaker analysis, general audio captioning, and sound scene tagging.&lt;/p&gt; 
&lt;p&gt;For audio-to-text tasks, you can use the following prompts:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ASR with ZH(same as AST en2zh): &lt;code&gt;è¯·ä»”ç»†å¬è¿™æ®µéŸ³é¢‘ç‰‡æ®µï¼Œå¹¶å°†å…¶å†…å®¹é€å­—è®°å½•ã€‚&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;ASR with EN(same as AST zh2en): &lt;code&gt;Please listen to the audio snippet carefully and transcribe the content.&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Speaker Analysis: &lt;code&gt;Based on the speaker's content, speculate on their gender, condition, age range, and health status.&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;General Audio Caption: &lt;code&gt;Summarize the main content of the audio.&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;General Sound Scene Tagging: &lt;code&gt;Utilize one keyword to convey the audio's content or the associated scene.&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;task_prompt = "Please listen to the audio snippet carefully and transcribe the content." + "\n" # can change to other prompts.
audio_input, _ = librosa.load('./assets/input_examples/audio_understanding.mp3', sr=16000, mono=True) # load the audio to be captioned

msgs = [{'role': 'user', 'content': [task_prompt, audio_input]}]

res = model.chat(
    msgs=msgs,
    tokenizer=tokenizer,
    sampling=True,
    max_new_tokens=128,
    use_tts_template=True,
    generate_audio=True,
    temperature=0.3,
    output_audio_path='result_audio_understanding.wav',
)
print(res)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Multimodal Live Streaming&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt; Click to view Python code running MiniCPM-o 2.6 with chat inference. &lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import math
import numpy as np
from PIL import Image
from moviepy.editor import VideoFileClip
import tempfile
import librosa
import soundfile as sf
import torch
from transformers import AutoModel, AutoTokenizer

def get_video_chunk_content(video_path, flatten=True):
    video = VideoFileClip(video_path)
    print('video_duration:', video.duration)
    
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=True) as temp_audio_file:
        temp_audio_file_path = temp_audio_file.name
        video.audio.write_audiofile(temp_audio_file_path, codec="pcm_s16le", fps=16000)
        audio_np, sr = librosa.load(temp_audio_file_path, sr=16000, mono=True)
    num_units = math.ceil(video.duration)
    
    # 1 frame + 1s audio chunk
    contents= []
    for i in range(num_units):
        frame = video.get_frame(i+1)
        image = Image.fromarray((frame).astype(np.uint8))
        audio = audio_np[sr*i:sr*(i+1)]
        if flatten:
            contents.extend(["&amp;lt;unit&amp;gt;", image, audio])
        else:
            contents.append(["&amp;lt;unit&amp;gt;", image, audio])
    
    return contents


model = AutoModel.from_pretrained('openbmb/MiniCPM-o-2_6', trust_remote_code=True,
    attn_implementation='sdpa', torch_dtype=torch.bfloat16)
model = model.eval().cuda()
tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-o-2_6', trust_remote_code=True)

model.init_tts()

# If you are using an older version of PyTorch, you might encounter this issue "weight_norm_fwd_first_dim_kernel" not implemented for 'BFloat16', Please convert the TTS to float32 type.
# model.tts.float()

# https://huggingface.co/openbmb/MiniCPM-o-2_6/blob/main/assets/Skiing.mp4
video_path="assets/Skiing.mp4"
sys_msg = model.get_sys_prompt(mode='omni', language='en')
# if use voice clone prompt, please set ref_audio
# ref_audio_path = '/path/to/ref_audio'
# ref_audio, _ = librosa.load(ref_audio_path, sr=16000, mono=True)
# sys_msg = model.get_sys_prompt(ref_audio=ref_audio, mode='omni', language='en')

contents = get_video_chunk_content(video_path)
msg = {"role":"user", "content": contents}
msgs = [sys_msg, msg]

# please set generate_audio=True and output_audio_path to save the tts result
generate_audio = True
output_audio_path = 'output.wav'

res = model.chat(
    msgs=msgs,
    tokenizer=tokenizer,
    sampling=True,
    temperature=0.5,
    max_new_tokens=4096,
    omni_input=True, # please set omni_input=True when omni inference
    use_tts_template=True,
    generate_audio=generate_audio,
    output_audio_path=output_audio_path,
    max_slice_nums=1,
    use_image_id=False,
    return_dict=True
)
print(res)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; Click to view Python code running MiniCPM-o 2.6 with streaming inference. &lt;/summary&gt; 
 &lt;p&gt;Note: The streaming inference has a slight performance degradation because the audio encoding is not global.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# a new conversation need reset session first, it will reset the kv-cache
model.reset_session()

contents = get_video_chunk_content(video_path, flatten=False)
session_id = '123'
generate_audio = True

# 1. prefill system prompt
res = model.streaming_prefill(
    session_id=session_id,
    msgs=[sys_msg], 
    tokenizer=tokenizer
)

# 2. prefill video/audio chunks
for content in contents:
    msgs = [{"role":"user", "content": content}]
    res = model.streaming_prefill(
        session_id=session_id,
        msgs=msgs, 
        tokenizer=tokenizer
    )

# 3. generate
res = model.streaming_generate(
    session_id=session_id,
    tokenizer=tokenizer,
    temperature=0.5,
    generate_audio=generate_audio
)

audios = []
text = ""

if generate_audio:
    for r in res:
        audio_wav = r.audio_wav
        sampling_rate = r.sampling_rate
        txt = r.text

        audios.append(audio_wav)
        text += txt
        
    res = np.concatenate(audios)
    sf.write("output.wav", res, samplerate=sampling_rate)
    print("text:", text)
    print("audio saved to output.wav")
else:
    for r in res:
        text += r['text']
    print("text:", text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Inference on Multiple GPUs&lt;/h3&gt; 
&lt;p&gt;You can run MiniCPM-Llama3-V 2.5 on multiple low VRAM GPUs (12 GB or 16 GB) by distributing the model's layers across multiple GPUs. Please refer to this &lt;a href="https://github.com/OpenBMB/MiniCPM-V/raw/main/docs/inference_on_multiple_gpus.md"&gt;tutorial&lt;/a&gt; for detailed instructions on how to load the model and inference using multiple low VRAM GPUs.&lt;/p&gt; 
&lt;h3&gt;Inference on Mac&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click to view an example, to run MiniCPM-Llama3-V 2.5 on ğŸ’» Mac with MPS (Apple silicon or AMD GPUs). &lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# test.py  Need more than 16GB memory.
import torch
from PIL import Image
from transformers import AutoModel, AutoTokenizer

model = AutoModel.from_pretrained('openbmb/MiniCPM-Llama3-V-2_5', trust_remote_code=True, low_cpu_mem_usage=True)
model = model.to(device='mps')

tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-Llama3-V-2_5', trust_remote_code=True)
model.eval()

image = Image.open('./assets/hk_OCR.jpg').convert('RGB')
question = 'Where is this photo taken?'
msgs = [{'role': 'user', 'content': question}]

answer, context, _ = model.chat(
    image=image,
    msgs=msgs,
    context=None,
    tokenizer=tokenizer,
    sampling=True
)
print(answer)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Run with command:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;PYTORCH_ENABLE_MPS_FALLBACK=1 python test.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Efficient Inference with llama.cpp, Ollama, vLLM&lt;/h3&gt; 
&lt;p&gt;See &lt;a href="https://github.com/OpenBMB/llama.cpp/tree/minicpmv-main/examples/llava/README-minicpmv2.6.md"&gt;our fork of llama.cpp&lt;/a&gt; for more detail. This implementation supports smooth inference of 16~18 token/s on iPad (test environmentï¼šiPad Pro + M4).&lt;/p&gt; 
&lt;p&gt;See &lt;a href="https://github.com/OpenBMB/ollama/raw/minicpm-v2.6/examples/minicpm-v2.6/README.md"&gt;our fork of Ollama&lt;/a&gt; for more detail. This implementation supports smooth inference of 16~18 token/s on iPad (test environmentï¼šiPad Pro + M4).&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt; vLLM now officially supports MiniCPM-V 2.6, MiniCPM-Llama3-V 2.5 and MiniCPM-V 2.0. And you can use our fork to run MiniCPM-o 2.6 for now. Click to see. &lt;/summary&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Install vLLM(&amp;gt;=0.7.1):&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;pip install vllm
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="2"&gt; 
  &lt;li&gt;Run Example:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://docs.vllm.ai/en/latest/getting_started/examples/vision_language.html"&gt;Vision Language&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://docs.vllm.ai/en/latest/getting_started/examples/audio_language.html"&gt;Audio Language&lt;/a&gt; &lt;/li&gt;
 &lt;/ul&gt;
&lt;/details&gt;   
&lt;h2&gt;Fine-tuning&lt;/h2&gt; 
&lt;h3&gt;Simple Fine-tuning 
 &lt;!-- omit in toc --&gt;&lt;/h3&gt; 
&lt;p&gt;We support simple fine-tuning with Hugging Face for MiniCPM-o 2.6, MiniCPM-V 2.6, MiniCPM-Llama3-V 2.5 and MiniCPM-V 2.0.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/finetune/readme.md"&gt;Reference Document&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;With Align-Anything 
 &lt;!-- omit in toc --&gt;&lt;/h3&gt; 
&lt;p&gt;We support fine-tuning MiniCPM-o 2.6 by PKU-Alignment Team (both vision and audio, SFT and DPO) with the &lt;a href="https://github.com/PKU-Alignment/align-anything"&gt;Align-Anything framework&lt;/a&gt;. Align-Anything is a scalable framework that aims to align any-modality large models with human intentions, open-sourcing the &lt;a href="https://huggingface.co/datasets/PKU-Alignment/align-anything"&gt;datasets, models and benchmarks&lt;/a&gt;. Benefiting from its concise and modular design, it supports 30+ open-source benchmarks, 40+ models and algorithms including SFT, SimPO, RLHF, &lt;em&gt;etc&lt;/em&gt;. It also provides 30+ directly runnable scripts, making it suitable for beginners to quickly get started.&lt;/p&gt; 
&lt;p&gt;Best Practices: &lt;a href="https://github.com/PKU-Alignment/align-anything/tree/main/scripts"&gt;MiniCPM-o 2.6&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;With LLaMA-Factory 
 &lt;!-- omit in toc --&gt;&lt;/h3&gt; 
&lt;p&gt;We support fine-tuning MiniCPM-o 2.6 and MiniCPM-V 2.6 with the LLaMA-Factory framework. LLaMA-Factory provides a solution for flexibly customizing the fine-tuning (Lora/Full/Qlora) of 200+ LLMs without the need for coding through the built-in web UI LLaMABoard. It supports various training methods like sft/ppo/dpo/kto and advanced algorithms like Galore/BAdam/LLaMA-Pro/Pissa/LongLoRA.&lt;/p&gt; 
&lt;p&gt;Best Practices: &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/llamafactory_train_and_infer.md"&gt;MiniCPM-o 2.6 | MiniCPM-V 2.6&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;With the SWIFT Framework 
 &lt;!-- omit in toc --&gt;&lt;/h3&gt; 
&lt;p&gt;We now support MiniCPM-V series fine-tuning with the SWIFT framework. SWIFT supports training, inference, evaluation and deployment of nearly 200 LLMs and MLLMs . It supports the lightweight training solutions provided by PEFT and a complete Adapters Library including techniques such as NEFTune, LoRA+ and LLaMA-PRO.&lt;/p&gt; 
&lt;p&gt;Best Practicesï¼š&lt;a href="https://github.com/modelscope/swift/raw/main/docs/source/Multi-Modal/minicpm-v%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5.md"&gt;MiniCPM-V 1.0&lt;/a&gt;, &lt;a href="https://github.com/modelscope/swift/raw/main/docs/source/Multi-Modal/minicpm-v-2%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5.md"&gt;MiniCPM-V 2.0&lt;/a&gt;, &lt;a href="https://github.com/modelscope/ms-swift/issues/1613"&gt;MiniCPM-V 2.6&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Awesome work using MiniCPM-V &amp;amp; MiniCPM-o&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/CatchTheTornado/text-extract-api"&gt;text-extract-api&lt;/a&gt;: Document extraction API using OCRs and Ollama supported models &lt;img src="https://img.shields.io/github/stars/CatchTheTornado/text-extract-api" alt="GitHub Repo stars" /&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/heshengtao/comfyui_LLM_party"&gt;comfyui_LLM_party&lt;/a&gt;: Build LLM workflows and integrate into existing image workflows &lt;img src="https://img.shields.io/github/stars/heshengtao/comfyui_LLM_party" alt="GitHub Repo stars" /&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/imanoop7/Ollama-OCR"&gt;Ollama-OCR&lt;/a&gt;: OCR package uses vlms through Ollama to extract text from images and PDF &lt;img src="https://img.shields.io/github/stars/imanoop7/Ollama-OCR" alt="GitHub Repo stars" /&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/MixLabPro/comfyui-mixlab-nodes"&gt;comfyui-mixlab-nodes&lt;/a&gt;: ComfyUI node suite supports Workflow-to-APPã€GPT&amp;amp;3D and more &lt;img src="https://img.shields.io/github/stars/MixLabPro/comfyui-mixlab-nodes" alt="GitHub Repo stars" /&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/HumanAIGC-Engineering/OpenAvatarChat"&gt;OpenAvatarChat&lt;/a&gt;: Interactive digital human conversation implementation on single PC &lt;img src="https://img.shields.io/github/stars/HumanAIGC-Engineering/OpenAvatarChat" alt="GitHub Repo stars" /&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/arkohut/pensieve"&gt;pensieve&lt;/a&gt;: A privacy-focused passive recording project by recording screen content &lt;img src="https://img.shields.io/github/stars/arkohut/pensieve" alt="GitHub Repo stars" /&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/icereed/paperless-gpt"&gt;paperless-gpt&lt;/a&gt;: Use LLMs to handle paperless-ngx, AI-powered titles, tags and OCR &lt;img src="https://img.shields.io/github/stars/icereed/paperless-gpt" alt="GitHub Repo stars" /&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/kimjammer/Neuro"&gt;Neuro&lt;/a&gt;: A recreation of Neuro-Sama, but running on local models on consumer hardware &lt;img src="https://img.shields.io/github/stars/kimjammer/Neuro" alt="GitHub Repo stars" /&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;FAQs&lt;/h2&gt; 
&lt;p&gt;Click here to view the &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/faqs.md"&gt;FAQs&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Limitations&lt;/h2&gt; 
&lt;p&gt;As an experimental trial, we find MiniCPM-o 2.6 has notable limitations worth further investigation and improvement.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Unstable speech output.&lt;/strong&gt; The speech generation can be flawed with noisy backgrounds and unmeaningful sounds.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Repeated response.&lt;/strong&gt; The model tends to repeat its response when encountering similar consecutive user queries.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;High-latency on Web Demo.&lt;/strong&gt; Users may experience unusual high-latency when using web demo hosted on overseas servers. We recommend deploying the demo locally or with good network connections.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Model License 
 &lt;!-- omit in toc --&gt;&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;This repository is released under the &lt;a href="https://github.com/OpenBMB/MiniCPM/raw/main/LICENSE"&gt;Apache-2.0&lt;/a&gt; License.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;The usage of MiniCPM-o/V model weights must strictly follow &lt;a href="https://github.com/OpenBMB/MiniCPM/raw/main/MiniCPM%20Model%20License.md"&gt;MiniCPM Model License.md&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;The models and weights of MiniCPM are completely free for academic research. after filling out a &lt;a href="https://modelbest.feishu.cn/share/base/form/shrcnpV5ZT9EJ6xYjh3Kx0J6v8g"&gt;"questionnaire"&lt;/a&gt; for registration, are also available for free commercial use.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Statement 
 &lt;!-- omit in toc --&gt;&lt;/h2&gt; 
&lt;p&gt;As MLLMs, MiniCPM-o/V models generate content by learning a large number of multimodal corpora, but they cannot comprehend, express personal opinions, or make value judgements. Anything generated by MiniCPM-o/V models does not represent the views and positions of the model developers&lt;/p&gt; 
&lt;p&gt;We will not be liable for any problems arising from the use of MiniCPM-o/V models, including but not limited to data security issues, risk of public opinion, or any risks and problems arising from the misdirection, misuse, dissemination, or misuse of the model.&lt;/p&gt; 
&lt;h2&gt;Institutions 
 &lt;!-- omit in toc --&gt;&lt;/h2&gt; 
&lt;p&gt;This project is developed by the following institutions:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/thunlp.png" width="28px" /&gt; &lt;a href="https://nlp.csai.tsinghua.edu.cn/"&gt;THUNLP&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelbest.png" width="28px" /&gt; &lt;a href="https://modelbest.cn/"&gt;ModelBest&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸŒŸ Star History 
 &lt;!-- omit in toc --&gt;&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/star-history-25-09-02.png" /&gt; &lt;/p&gt;
&lt;table align="center"&gt;  
&lt;/table&gt; 
&lt;!-- &lt;picture&gt;
  &lt;source
    media="(prefers-color-scheme: dark)"
    srcset="
      https://api.star-history.com/svg?repos=OpenBMB/MiniCPM-o&amp;type=Date&amp;theme=dark
    "
  /&gt;
  &lt;source
    media="(prefers-color-scheme: light)"
    srcset="
      https://api.star-history.com/svg?repos=OpenBMB/MiniCPM-o&amp;type=Date
    "
  /&gt;
  &lt;img
    alt="Star History Chart"
    src="https://api.star-history.com/svg?repos=OpenBMB/MiniCPM-o&amp;type=Date"
  /&gt;
&lt;/picture&gt; --&gt; 
&lt;h2&gt;Key Techniques and Other Multimodal Projects 
 &lt;!-- omit in toc --&gt;&lt;/h2&gt; 
&lt;p&gt;ğŸ‘ Welcome to explore key techniques of MiniCPM-o/V and other multimodal projects of our team:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/OpenBMB/VisCPM/tree/main"&gt;VisCPM&lt;/a&gt; | &lt;a href="https://github.com/OpenBMB/RLPR"&gt;RLPR&lt;/a&gt; | &lt;a href="https://github.com/RLHF-V/RLHF-V"&gt;RLHF-V&lt;/a&gt; | &lt;a href="https://github.com/thunlp/LLaVA-UHD"&gt;LLaVA-UHD&lt;/a&gt; | &lt;a href="https://github.com/RLHF-V/RLAIF-V"&gt;RLAIF-V&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Citation 
 &lt;!-- omit in toc --&gt;&lt;/h2&gt; 
&lt;p&gt;If you find our model/code/paper helpful, please consider citing our papers ğŸ“ and staring us â­ï¸ï¼&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bib"&gt;@article{yao2024minicpm,
  title={MiniCPM-V: A GPT-4V Level MLLM on Your Phone},
  author={Yao, Yuan and Yu, Tianyu and Zhang, Ao and Wang, Chongyi and Cui, Junbo and Zhu, Hongji and Cai, Tianchi and Li, Haoyu and Zhao, Weilin and He, Zhihui and others},
  journal={arXiv preprint arXiv:2408.01800},
  year={2024}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>laramies/theHarvester</title>
      <link>https://github.com/laramies/theHarvester</link>
      <description>&lt;p&gt;E-mails, subdomains and names Harvester - OSINT&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://github.com/laramies/theHarvester/raw/master/theHarvester-logo.webp" alt="theHarvester" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/laramies/theHarvester/workflows/TheHarvester%20Python%20CI/badge.svg?sanitize=true" alt="TheHarvester CI" /&gt; &lt;img src="https://github.com/laramies/theHarvester/workflows/TheHarvester%20Docker%20Image%20CI/badge.svg?sanitize=true" alt="TheHarvester Docker Image CI" /&gt; &lt;a href="https://inventory.raw.pm/"&gt;&lt;img src="https://inventory.raw.pm/img/badges/Rawsec-inventoried-FF5050_flat_without_logo.svg?sanitize=true" alt="Rawsec's CyberSecurity Inventory" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;About&lt;/h2&gt; 
&lt;p&gt;theHarvester is a simple to use, yet powerful tool designed to be used during the reconnaissance stage of a red team assessment or penetration test. It performs open source intelligence (OSINT) gathering to help determine a domain's external threat landscape. The tool gathers names, emails, IPs, subdomains, and URLs by using multiple public resources that include:&lt;/p&gt; 
&lt;h2&gt;Install and dependencies&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.12 or higher.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/laramies/theHarvester/wiki/Installation"&gt;https://github.com/laramies/theHarvester/wiki/Installation&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Install uv:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -LsSf https://astral.sh/uv/install.sh | sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Clone the repository:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/laramies/theHarvester
cd theHarvester
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Install dependencies and create a virtual environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv sync
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run theHarvester:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run theHarvester
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Development&lt;/h2&gt; 
&lt;p&gt;To install development dependencies:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv sync --extra dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To run tests:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run pytest
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To run linting and formatting:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run ruff check
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run ruff format
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Passive modules&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;baidu: Baidu search engine (&lt;a href="https://www.baidu.com"&gt;https://www.baidu.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;bevigil: CloudSEK BeVigil scans mobile application for OSINT assets (&lt;a href="https://bevigil.com/osint-api"&gt;https://bevigil.com/osint-api&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;brave: Brave search engine - now uses official Brave Search API (&lt;a href="https://api-dashboard.search.brave.com"&gt;https://api-dashboard.search.brave.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;bufferoverun: Fast domain name lookups for TLS certificates in IPv4 space (&lt;a href="https://tls.bufferover.run"&gt;https://tls.bufferover.run&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;builtwith: Find out what websites are built with (&lt;a href="https://builtwith.com"&gt;https://builtwith.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;censys: Uses certificates searches to enumerate subdomains and gather emails (&lt;a href="https://censys.io"&gt;https://censys.io&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;certspotter: Cert Spotter monitors Certificate Transparency logs (&lt;a href="https://sslmate.com/certspotter"&gt;https://sslmate.com/certspotter&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;criminalip: Specialized Cyber Threat Intelligence (CTI) search engine (&lt;a href="https://www.criminalip.io"&gt;https://www.criminalip.io&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;crtsh: Comodo Certificate search (&lt;a href="https://crt.sh"&gt;https://crt.sh&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;dehashed: Take your data security to the next level is (&lt;a href="https://dehashed.com"&gt;https://dehashed.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;dnsdumpster: Domain research tool that can discover hosts related to a domain (&lt;a href="https://dnsdumpster.com"&gt;https://dnsdumpster.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;duckduckgo: DuckDuckGo search engine (&lt;a href="https://duckduckgo.com"&gt;https://duckduckgo.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;fullhunt: Next-generation attack surface security platform (&lt;a href="https://fullhunt.io"&gt;https://fullhunt.io&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;github-code: GitHub code search engine (&lt;a href="https://www.github.com"&gt;https://www.github.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;hackertarget: Online vulnerability scanners and network intelligence to help organizations (&lt;a href="https://hackertarget.com"&gt;https://hackertarget.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;haveibeenpwned: Check if your email address is in a data breach (&lt;a href="https://haveibeenpwned.com"&gt;https://haveibeenpwned.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;hunter: Hunter search engine (&lt;a href="https://hunter.io"&gt;https://hunter.io&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;hunterhow: Internet search engines for security researchers (&lt;a href="https://hunter.how"&gt;https://hunter.how&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;intelx: Intelx search engine (&lt;a href="https://intelx.io"&gt;https://intelx.io&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;leaklookup: Data breach search engine (&lt;a href="https://leak-lookup.com"&gt;https://leak-lookup.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;netlas: A Shodan or Censys competitor (&lt;a href="https://app.netlas.io"&gt;https://app.netlas.io&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;onyphe: Cyber defense search engine (&lt;a href="https://www.onyphe.io"&gt;https://www.onyphe.io&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;otx: AlienVault open threat exchange (&lt;a href="https://otx.alienvault.com"&gt;https://otx.alienvault.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;pentesttools: Cloud-based toolkit for offensive security testing, focused on web applications and network penetration testing (&lt;a href="https://pentest-tools.com"&gt;https://pentest-tools.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;projecdiscovery: Actively collects and maintains internet-wide assets data, to enhance research and analyse changes around DNS for better insights (&lt;a href="https://chaos.projectdiscovery.io"&gt;https://chaos.projectdiscovery.io&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;rapiddns: DNS query tool which make querying subdomains or sites of a same IP easy (&lt;a href="https://rapiddns.io"&gt;https://rapiddns.io&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;rocketreach: Access real-time verified personal/professional emails, phone numbers, and social media links (&lt;a href="https://rocketreach.co"&gt;https://rocketreach.co&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;securityscorecard: helps TPRM and SOC teams detect, prioritize, and remediate vendor risk across their entire supplier ecosystem at scale (&lt;a href="https://securityscorecard.com"&gt;https://securityscorecard.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;securityTrails: Security Trails search engine, the world's largest repository of historical DNS data (&lt;a href="https://securitytrails.com"&gt;https://securitytrails.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;-s, --shodan: Shodan search engine will search for ports and banners from discovered hosts (&lt;a href="https://shodan.io"&gt;https://shodan.io&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;subdomaincenter: A subdomain finder tool used to find subdomains of a given domain (&lt;a href="https://www.subdomain.center"&gt;https://www.subdomain.center&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;subdomainfinderc99: A subdomain finder is a tool used to find the subdomains of a given domain (&lt;a href="https://subdomainfinder.c99.nl"&gt;https://subdomainfinder.c99.nl&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;threatminer: Data mining for threat intelligence (&lt;a href="https://www.threatminer.org"&gt;https://www.threatminer.org&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;tomba: Tomba search engine (&lt;a href="https://tomba.io"&gt;https://tomba.io&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;urlscan: A sandbox for the web that is a URL and website scanner (&lt;a href="https://urlscan.io"&gt;https://urlscan.io&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;venacus: Venacus search engine (&lt;a href="https://venacus.com"&gt;https://venacus.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;virustotal: Domain search (&lt;a href="https://www.virustotal.com"&gt;https://www.virustotal.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;whoisxml: Subdomain search (&lt;a href="https://subdomains.whoisxmlapi.com/api/pricing"&gt;https://subdomains.whoisxmlapi.com/api/pricing&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;yahoo: Yahoo search engine (&lt;a href="https://www.yahoo.com"&gt;https://www.yahoo.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;zoomeye: China's version of Shodan (&lt;a href="https://www.zoomeye.org"&gt;https://www.zoomeye.org&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Active modules&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;DNS brute force: dictionary brute force enumeration&lt;/li&gt; 
 &lt;li&gt;Screenshots: Take screenshots of subdomains that were found&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Modules that require an API key&lt;/h2&gt; 
&lt;p&gt;Documentation to setup API keys can be found at - &lt;a href="https://github.com/laramies/theHarvester/wiki/Installation#api-keys"&gt;https://github.com/laramies/theHarvester/wiki/Installation#api-keys&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;bevigil - 50 free queries/month, 1k queries/month $50&lt;/li&gt; 
 &lt;li&gt;brave - Free plan available, Pro plans for higher limits&lt;/li&gt; 
 &lt;li&gt;bufferoverun - 100 free queries/month, 10k/month $25&lt;/li&gt; 
 &lt;li&gt;builtwith - 50 free queries ever, $2950/yr&lt;/li&gt; 
 &lt;li&gt;censys - 500 credits $100&lt;/li&gt; 
 &lt;li&gt;criminalip - 100 free queries/month, 700k/month $59&lt;/li&gt; 
 &lt;li&gt;dehashed - 500 credts $15, 5k credits $150&lt;/li&gt; 
 &lt;li&gt;dnsdumpster - 50 free querries/day, $49&lt;/li&gt; 
 &lt;li&gt;fullhunt - 50 free queries, 200 queries $29/month, 500 queries $59/month&lt;/li&gt; 
 &lt;li&gt;github-code&lt;/li&gt; 
 &lt;li&gt;haveibeenpwned - 10 email searches/min $4.50, 50 email searches/min $22&lt;/li&gt; 
 &lt;li&gt;hunter - 50 credits/month free, 12k credits/yr $34&lt;/li&gt; 
 &lt;li&gt;hunterhow - 10k free API results per 30 days, 50k API results per 30 days $10&lt;/li&gt; 
 &lt;li&gt;intelx&lt;/li&gt; 
 &lt;li&gt;leaklookup - 20 credits $10, 50 credits $20, 140 credits $50, 300 credits $100&lt;/li&gt; 
 &lt;li&gt;netlas - 50 free requests/day, 1k requests $49, 10k requests $249&lt;/li&gt; 
 &lt;li&gt;onyphe - 10M results/month $587&lt;/li&gt; 
 &lt;li&gt;pentesttools - 5 assets netsec $95/month, 5 assets webnetsec $140/month&lt;/li&gt; 
 &lt;li&gt;projecdiscovery - requires work email. Free monthly discovery and vulnerability scans on sign-up email domain, enterprise $&lt;/li&gt; 
 &lt;li&gt;rocketreach - 100 email lookups/month $48, 250 email lookups/month $108&lt;/li&gt; 
 &lt;li&gt;securityscorecard&lt;/li&gt; 
 &lt;li&gt;securityTrails - 50 free queries/month, 20k queries/month $500&lt;/li&gt; 
 &lt;li&gt;shodan - Freelancer $69 month, Small Business $359 month&lt;/li&gt; 
 &lt;li&gt;tomba - 25 searches/month free, 1k searches/month $39, 5k searches/month $89&lt;/li&gt; 
 &lt;li&gt;venacus - 1 search/day free, 10 searches/day $12, 30 searches/day $36&lt;/li&gt; 
 &lt;li&gt;whoisxml - 2k queries $50, 5k queries $105&lt;/li&gt; 
 &lt;li&gt;zoomeye - 5 results/day free, 30/results/day $190/yr&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Package versions&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://repology.org/project/theharvester/versions"&gt;&lt;img src="https://repology.org/badge/vertical-allrepos/theharvester.svg?sanitize=true" alt="Packaging status" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Comments, bugs, and requests&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://twitter.com/laramies"&gt;&lt;img src="https://img.shields.io/twitter/follow/laramies.svg?style=social&amp;amp;label=Follow" alt="Twitter Follow" /&gt;&lt;/a&gt; Christian Martorella @laramies &lt;a href="mailto:cmartorella@edge-security.com"&gt;cmartorella@edge-security.com&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://twitter.com/NotoriousRebel1"&gt;&lt;img src="https://img.shields.io/twitter/follow/NotoriousRebel1.svg?style=social&amp;amp;label=Follow" alt="Twitter Follow" /&gt;&lt;/a&gt; Matthew Brown @NotoriousRebel1&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://twitter.com/jay_townsend1"&gt;&lt;img src="https://img.shields.io/twitter/follow/jay_townsend1.svg?style=social&amp;amp;label=Follow" alt="Twitter Follow" /&gt;&lt;/a&gt; Jay "L1ghtn1ng" Townsend @jay_townsend1&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Main contributors&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://twitter.com/NotoriousRebel1"&gt;&lt;img src="https://img.shields.io/twitter/follow/NotoriousRebel1.svg?style=social&amp;amp;label=Follow" alt="Twitter Follow" /&gt;&lt;/a&gt; Matthew Brown @NotoriousRebel1&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://twitter.com/jay_townsend1"&gt;&lt;img src="https://img.shields.io/twitter/follow/jay_townsend1.svg?style=social&amp;amp;label=Follow" alt="Twitter Follow" /&gt;&lt;/a&gt; Jay "L1ghtn1ng" Townsend @jay_townsend1&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://twitter.com/discoverscripts"&gt;&lt;img src="https://img.shields.io/twitter/follow/discoverscripts.svg?style=social&amp;amp;label=Follow" alt="Twitter Follow" /&gt;&lt;/a&gt; Lee Baird @discoverscripts&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Thanks&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;John Matherly - Shodan project&lt;/li&gt; 
 &lt;li&gt;Ahmed Aboul Ela - subdomain names dictionaries (big and small)&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>IBM/mcp-context-forge</title>
      <link>https://github.com/IBM/mcp-context-forge</link>
      <description>&lt;p&gt;A Model Context Protocol (MCP) Gateway &amp; Registry. Serves as a central management point for tools, resources, and prompts that can be accessed by MCP-compatible LLM applications. Converts REST API endpoints to MCP, composes virtual MCP servers with added security and observability, and converts between protocols (stdio, SSE, Streamable HTTP).&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MCP Gateway&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Model Context Protocol gateway &amp;amp; proxy - unify REST, MCP, and A2A with federation, virtual servers, retries, security, and an optional admin UI.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/docs/docs/images/contextforge-banner.png" alt="" /&gt;&lt;/p&gt; 
&lt;!-- === CI / Security / Build Badges === --&gt; 
&lt;p&gt;&lt;a href="https://github.com/IBM/mcp-context-forge/actions/workflows/python-package.yml"&gt;&lt;img src="https://github.com/IBM/mcp-context-forge/actions/workflows/python-package.yml/badge.svg?sanitize=true" alt="Build Python Package" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/IBM/mcp-context-forge/actions/workflows/codeql.yml"&gt;&lt;img src="https://github.com/IBM/mcp-context-forge/actions/workflows/codeql.yml/badge.svg?sanitize=true" alt="CodeQL" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/IBM/mcp-context-forge/actions/workflows/bandit.yml"&gt;&lt;img src="https://github.com/IBM/mcp-context-forge/actions/workflows/bandit.yml/badge.svg?sanitize=true" alt="Bandit Security" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/IBM/mcp-context-forge/actions/workflows/dependency-review.yml"&gt;&lt;img src="https://github.com/IBM/mcp-context-forge/actions/workflows/dependency-review.yml/badge.svg?sanitize=true" alt="Dependency Review" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/IBM/mcp-context-forge/actions/workflows/pytest.yml"&gt;&lt;img src="https://github.com/IBM/mcp-context-forge/actions/workflows/pytest.yml/badge.svg?sanitize=true" alt="Tests &amp;amp; Coverage" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/IBM/mcp-context-forge/actions/workflows/lint.yml"&gt;&lt;img src="https://github.com/IBM/mcp-context-forge/actions/workflows/lint.yml/badge.svg?sanitize=true" alt="Lint &amp;amp; Static Analysis" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;!-- === Container Build &amp; Deploy === --&gt; 
&lt;p&gt;&lt;a href="https://github.com/IBM/mcp-context-forge/actions/workflows/docker-image.yml"&gt;&lt;img src="https://github.com/IBM/mcp-context-forge/actions/workflows/docker-image.yml/badge.svg?sanitize=true" alt="Secure Docker Build" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/IBM/mcp-context-forge/actions/workflows/ibm-cloud-code-engine.yml"&gt;&lt;img src="https://github.com/IBM/mcp-context-forge/actions/workflows/ibm-cloud-code-engine.yml/badge.svg?sanitize=true" alt="Deploy to IBM Code Engine" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;!-- === Package / Container === --&gt; 
&lt;p&gt;&lt;a href="https://docs.python.org/3/library/asyncio.html"&gt;&lt;img src="https://img.shields.io/badge/async-await-green.svg?sanitize=true" alt="Async" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/ibm/mcp-context-forge" alt="License" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://pypi.org/project/mcp-contextforge-gateway/"&gt;&lt;img src="https://img.shields.io/pypi/v/mcp-contextforge-gateway" alt="PyPI" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/ibm/mcp-context-forge/pkgs/container/mcp-context-forge"&gt;&lt;img src="https://img.shields.io/badge/docker-ghcr.io%2Fibm%2Fmcp--context--forge-blue" alt="Docker Image" /&gt;&lt;/a&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;ContextForge MCP Gateway is a feature-rich gateway, proxy and MCP Registry that federates MCP and REST services - unifying discovery, auth, rate-limiting, observability, virtual servers, multi-transport protocols, and an optional Admin UI into one clean endpoint for your AI clients. It runs as a fully compliant MCP server, deployable via PyPI or Docker, and scales to multi-cluster environments on Kubernetes with Redis-backed federation and caching.&lt;/p&gt; 
&lt;h2&gt;&lt;img src="https://ibm.github.io/mcp-context-forge/images/mcpgateway.gif" alt="MCP Gateway" /&gt;&lt;/h2&gt; 
&lt;!-- vscode-markdown-toc --&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; 
  &lt;ol&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#table-of-contents"&gt;Table of Contents&lt;/a&gt;&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;ol start="2"&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#-overview--goals"&gt;ğŸš€ Overview &amp;amp; Goals&lt;/a&gt;&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;ol start="3"&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#quick-start---pypi"&gt;Quick Start - PyPI&lt;/a&gt;&lt;/li&gt; 
  &lt;/ol&gt; 
  &lt;ul&gt; 
   &lt;li&gt;3.1. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#1---install--run-copy-paste-friendly"&gt;1 - Install &amp;amp; run (copy-paste friendly)&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;ol start="4"&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#quick-start---containers"&gt;Quick Start - Containers&lt;/a&gt;&lt;/li&gt; 
  &lt;/ol&gt; 
  &lt;ul&gt; 
   &lt;li&gt;4.1. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#-docker"&gt;ğŸ³ Docker&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;4.1.1. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#1---minimum-viable-run"&gt;1 - Minimum viable run&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;4.1.2. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#2---persist-the-sqlite-database"&gt;2 - Persist the SQLite database&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;4.1.3. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#3---local-tool-discovery-host-network"&gt;3 - Local tool discovery (host network)&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;4.2. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#-podman-rootless-friendly"&gt;ğŸ¦­ Podman (rootless-friendly)&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;4.2.1. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#1---basic-run"&gt;1 - Basic run&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;4.2.2. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#2---persist-sqlite"&gt;2 - Persist SQLite&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;4.2.3. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#3---host-networking-rootless"&gt;3 - Host networking (rootless)&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;ol start="5"&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#testing-mcpgatewaywrapper-by-hand"&gt;Testing &lt;code&gt;mcpgateway.wrapper&lt;/code&gt; by hand&lt;/a&gt;&lt;/li&gt; 
  &lt;/ol&gt; 
  &lt;ul&gt; 
   &lt;li&gt;5.1. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#-running-from-an-mcp-client-mcpgatewaywrapper"&gt;ğŸ§© Running from an MCP Client (&lt;code&gt;mcpgateway.wrapper&lt;/code&gt;)&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;5.1.1. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#1---install-uv-uvx-is-an-alias-it-provides"&gt;1 - Install &lt;code&gt;uv&lt;/code&gt; (&lt;code&gt;uvx&lt;/code&gt; is an alias it provides)&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;5.1.2. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#2---create-an-on-the-spot-venv--run-the-wrapper"&gt;2 - Create an on-the-spot venv &amp;amp; run the wrapper&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;5.1.3. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#claude-desktop-json-runs-through-uvx"&gt;Claude Desktop JSON (runs through &lt;strong&gt;uvx&lt;/strong&gt;)&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;5.2. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#-using-with-claude-desktop-or-any-gui-mcp-client"&gt;ğŸš€ Using with Claude Desktop (or any GUI MCP client)&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;ol start="6"&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#-quick-start-vs-code-dev-container"&gt;ğŸš€ Quick Start: VS Code Dev Container&lt;/a&gt;&lt;/li&gt; 
  &lt;/ol&gt; 
  &lt;ul&gt; 
   &lt;li&gt;6.1. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#1---clone--open"&gt;1 - Clone &amp;amp; Open&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;6.2. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#2---first-time-build-automatic"&gt;2 - First-Time Build (Automatic)&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;ol start="7"&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#quick-start-manual-install"&gt;Quick Start (manual install)&lt;/a&gt;&lt;/li&gt; 
  &lt;/ol&gt; 
  &lt;ul&gt; 
   &lt;li&gt;7.1. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#prerequisites"&gt;Prerequisites&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;7.2. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#one-liner-dev"&gt;One-liner (dev)&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;7.3. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#containerized-self-signed-tls"&gt;Containerized (self-signed TLS)&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;7.4. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#smoke-test-the-api"&gt;Smoke-test the API&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;ol start="8"&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
  &lt;/ol&gt; 
  &lt;ul&gt; 
   &lt;li&gt;8.1. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#via-make"&gt;Via Make&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;8.2. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#uv-alternative"&gt;UV (alternative)&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;8.3. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#pip-alternative"&gt;pip (alternative)&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;8.4. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#optional-postgresql-adapter"&gt;Optional (PostgreSQL adapter)&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;8.4.1. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#quick-postgres-container"&gt;Quick Postgres container&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;ol start="9"&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#-upgrading-to-v070"&gt;ğŸ”„ Upgrading to v0.7.0&lt;/a&gt;&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;ol start="10"&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#configuration-env-or-env-vars"&gt;Configuration (&lt;code&gt;.env&lt;/code&gt; or env vars)&lt;/a&gt;&lt;/li&gt; 
  &lt;/ol&gt; 
  &lt;ul&gt; 
   &lt;li&gt;10.1. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#basic"&gt;Basic&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;10.2. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#authentication"&gt;Authentication&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;10.3. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#ui-features"&gt;UI Features&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;10.4. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#security"&gt;Security&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;10.5. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#logging"&gt;Logging&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;10.6. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#transport"&gt;Transport&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;10.7. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#federation"&gt;Federation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;10.8. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#resources"&gt;Resources&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;10.9. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#tools"&gt;Tools&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;10.10. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#prompts"&gt;Prompts&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;10.11. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#health-checks"&gt;Health Checks&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;10.12. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#database"&gt;Database&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;10.13. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#cache-backend"&gt;Cache Backend&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;10.14. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#development"&gt;Development&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;ol start="11"&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#running"&gt;Running&lt;/a&gt;&lt;/li&gt; 
  &lt;/ol&gt; 
  &lt;ul&gt; 
   &lt;li&gt;11.1. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#makefile"&gt;Makefile&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;11.2. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#script-helper"&gt;Script helper&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;11.3. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#manual-uvicorn"&gt;Manual (Uvicorn)&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;ol start="12"&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#authentication-examples"&gt;Authentication examples&lt;/a&gt;&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;ol start="13"&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#%EF%B8%8F-aws--azure--openshift"&gt;â˜ï¸ AWS / Azure / OpenShift&lt;/a&gt;&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;ol start="14"&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#%EF%B8%8F-ibm-cloud-code-engine-deployment"&gt;â˜ï¸ IBM Cloud Code Engine Deployment&lt;/a&gt;&lt;/li&gt; 
  &lt;/ol&gt; 
  &lt;ul&gt; 
   &lt;li&gt;14.1. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#-prerequisites-1"&gt;ğŸ”§ Prerequisites&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;14.2. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#-environment-variables"&gt;ğŸ“¦ Environment Variables&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;14.3. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#-make-targets"&gt;ğŸš€ Make Targets&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;14.4. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#-example-workflow"&gt;ğŸ“ Example Workflow&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;ol start="15"&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#api-endpoints"&gt;API Endpoints&lt;/a&gt;&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;ol start="16"&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#testing"&gt;Testing&lt;/a&gt;&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;ol start="17"&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#project-structure"&gt;Project Structure&lt;/a&gt;&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;ol start="18"&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#api-documentation"&gt;API Documentation&lt;/a&gt;&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;ol start="19"&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#makefile-targets"&gt;Makefile targets&lt;/a&gt;&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;ol start="20"&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#-troubleshooting"&gt;ğŸ” Troubleshooting&lt;/a&gt;&lt;/li&gt; 
  &lt;/ol&gt; 
  &lt;ul&gt; 
   &lt;li&gt;20.1. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#diagnose-the-listener"&gt;Diagnose the listener&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;20.2. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#why-localhost-fails-on-windows"&gt;Why localhost fails on Windows&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;20.2.1. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#fix-podman-rootless"&gt;Fix (Podman rootless)&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;20.2.2. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#fix-docker-desktop--419"&gt;Fix (Docker Desktop &amp;gt; 4.19)&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;ol start="21"&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;ol start="22"&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#changelog"&gt;Changelog&lt;/a&gt;&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;ol start="23"&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;ol start="24"&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#core-authors-and-maintainers"&gt;Core Authors and Maintainers&lt;/a&gt;&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;ol start="25"&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#star-history-and-project-activity"&gt;Star History and Project Activity&lt;/a&gt;&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- vscode-markdown-toc-config
    numbering=true
    autoSave=true
    /vscode-markdown-toc-config --&gt; 
&lt;!-- /vscode-markdown-toc --&gt; 
&lt;h2&gt;ğŸš€ Overview &amp;amp; Goals&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;ContextForge MCP Gateway&lt;/strong&gt; is a gateway, registry, and proxy that sits in front of any &lt;a href="https://modelcontextprotocol.io"&gt;Model Context Protocol&lt;/a&gt; (MCP) server or REST API-exposing a unified endpoint for all your AI clients.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;âš ï¸ Caution&lt;/strong&gt;: The current release (0.6.0) is considered alpha / early beta. It is not production-ready and should only be used for local development, testing, or experimentation. Features, APIs, and behaviors are subject to change without notice. &lt;strong&gt;Do not&lt;/strong&gt; deploy in production environments without thorough security review, validation and additional security mechanisms. Many of the features required for secure, large-scale, or multi-tenant production deployments are still on the &lt;a href="https://ibm.github.io/mcp-context-forge/architecture/roadmap/"&gt;project roadmap&lt;/a&gt; - which is itself evolving.&lt;/p&gt; 
&lt;p&gt;It currently supports:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Federation across multiple MCP and REST services&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;A2A (Agent-to-Agent) integration&lt;/strong&gt; for external AI agents (OpenAI, Anthropic, custom)&lt;/li&gt; 
 &lt;li&gt;Virtualization of legacy APIs as MCP-compliant tools and servers&lt;/li&gt; 
 &lt;li&gt;Transport over HTTP, JSON-RPC, WebSocket, SSE (with configurable keepalive), stdio and streamable-HTTP&lt;/li&gt; 
 &lt;li&gt;An Admin UI for real-time management, configuration, and log monitoring&lt;/li&gt; 
 &lt;li&gt;Built-in auth, retries, and rate-limiting&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;OpenTelemetry observability&lt;/strong&gt; with Phoenix, Jaeger, Zipkin, and other OTLP backends&lt;/li&gt; 
 &lt;li&gt;Scalable deployments via Docker or PyPI, Redis-backed caching, and multi-cluster federation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://ibm.github.io/mcp-context-forge/images/mcpgateway.svg?sanitize=true" alt="MCP Gateway Architecture" /&gt;&lt;/p&gt; 
&lt;p&gt;For a list of upcoming features, check out the &lt;a href="https://ibm.github.io/mcp-context-forge/architecture/roadmap/"&gt;ContextForge MCP Gateway Roadmap&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;âš ï¸ Important&lt;/strong&gt;: MCP Gateway is not a standalone product - it is an open source component with &lt;strong&gt;NO OFFICIAL SUPPORT&lt;/strong&gt; from IBM or its affiliates that can be integrated into your own solution architecture. If you choose to use it, you are responsible for evaluating its fit, securing the deployment, and managing its lifecycle. See &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/SECURITY.md"&gt;SECURITY.md&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ”Œ Gateway Layer with Protocol Flexibility&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Sits in front of any MCP server or REST API&lt;/li&gt; 
  &lt;li&gt;Lets you choose your MCP protocol version (e.g., &lt;code&gt;2025-03-26&lt;/code&gt;)&lt;/li&gt; 
  &lt;li&gt;Exposes a single, unified interface for diverse backends&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸŒ Federation of Peer Gateways (MCP Registry)&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Auto-discovers or configures peer gateways (via mDNS or manual)&lt;/li&gt; 
  &lt;li&gt;Performs health checks and merges remote registries transparently&lt;/li&gt; 
  &lt;li&gt;Supports Redis-backed syncing and fail-over&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ§© Virtualization of REST/gRPC Services&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Wraps non-MCP services as virtual MCP servers&lt;/li&gt; 
  &lt;li&gt;Registers tools, prompts, and resources with minimal configuration&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ” REST-to-MCP Tool Adapter&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;Adapts REST APIs into tools with:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Automatic JSON Schema extraction&lt;/li&gt; 
    &lt;li&gt;Support for headers, tokens, and custom auth&lt;/li&gt; 
    &lt;li&gt;Retry, timeout, and rate-limit policies&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ§  Unified Registries&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Prompts&lt;/strong&gt;: Jinja2 templates, multimodal support, rollback/versioning&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Resources&lt;/strong&gt;: URI-based access, MIME detection, caching, SSE updates&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Native or adapted, with input validation and concurrency controls&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ“ˆ Admin UI, Observability &amp;amp; Dev Experience&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Admin UI built with HTMX + Alpine.js&lt;/li&gt; 
  &lt;li&gt;Real-time log viewer with filtering, search, and export capabilities&lt;/li&gt; 
  &lt;li&gt;Auth: Basic, JWT, or custom schemes&lt;/li&gt; 
  &lt;li&gt;Structured logs, health endpoints, metrics&lt;/li&gt; 
  &lt;li&gt;400+ tests, Makefile targets, live reload, pre-commit hooks&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ” OpenTelemetry Observability&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Vendor-agnostic tracing&lt;/strong&gt; with OpenTelemetry (OTLP) protocol support&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Multiple backend support&lt;/strong&gt;: Phoenix (LLM-focused), Jaeger, Zipkin, Tempo, DataDog, New Relic&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Distributed tracing&lt;/strong&gt; across federated gateways and services&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Automatic instrumentation&lt;/strong&gt; of tools, prompts, resources, and gateway operations&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;LLM-specific metrics&lt;/strong&gt;: Token usage, costs, model performance&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Zero-overhead when disabled&lt;/strong&gt; with graceful degradation&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Easy configuration&lt;/strong&gt; via environment variables&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;Quick start with Phoenix (LLM observability):&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Start Phoenix
docker run -p 6006:6006 -p 4317:4317 arizephoenix/phoenix:latest

# Configure gateway
export OTEL_ENABLE_OBSERVABILITY=true
export OTEL_TRACES_EXPORTER=otlp
export OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317

# Run gateway - traces automatically sent to Phoenix
mcpgateway
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;See &lt;a href="https://ibm.github.io/mcp-context-forge/manage/observability/"&gt;Observability Documentation&lt;/a&gt; for detailed setup with other backends.&lt;/p&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Quick Start - PyPI&lt;/h2&gt; 
&lt;p&gt;MCP Gateway is published on &lt;a href="https://pypi.org/project/mcp-contextforge-gateway/"&gt;PyPI&lt;/a&gt; as &lt;code&gt;mcp-contextforge-gateway&lt;/code&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;TLDR;&lt;/strong&gt;: (single command using &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt;)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;BASIC_AUTH_PASSWORD=pass \
MCPGATEWAY_UI_ENABLED=true \
MCPGATEWAY_ADMIN_API_ENABLED=true \
uvx --from mcp-contextforge-gateway mcpgateway --host 0.0.0.0 --port 4444
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ“‹ Prerequisites&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Python â‰¥ 3.10&lt;/strong&gt; (3.11 recommended)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;curl + jq&lt;/strong&gt; - only for the last smoke-test step&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;1 - Install &amp;amp; run (copy-paste friendly)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 1ï¸âƒ£  Isolated env + install from pypi
mkdir mcpgateway &amp;amp;&amp;amp; cd mcpgateway
python3 -m venv .venv &amp;amp;&amp;amp; source .venv/bin/activate
pip install --upgrade pip
pip install mcp-contextforge-gateway

# 2ï¸âƒ£  Launch on all interfaces with custom creds &amp;amp; secret key
# Enable the Admin API endpoints (true/false) - disabled by default
export MCPGATEWAY_UI_ENABLED=true
export MCPGATEWAY_ADMIN_API_ENABLED=true

BASIC_AUTH_PASSWORD=pass JWT_SECRET_KEY=my-test-key \
  mcpgateway --host 0.0.0.0 --port 4444 &amp;amp;   # admin/pass

# 3ï¸âƒ£  Generate a bearer token &amp;amp; smoke-test the API
export MCPGATEWAY_BEARER_TOKEN=$(python3 -m mcpgateway.utils.create_jwt_token \
    --username admin@example.com --exp 10080 --secret my-test-key)

curl -s -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     http://127.0.0.1:4444/version | jq
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Windows (PowerShell) quick-start&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-powershell"&gt;# 1ï¸âƒ£  Isolated env + install from PyPI
mkdir mcpgateway ; cd mcpgateway
python3 -m venv .venv ; .\.venv\Scripts\Activate.ps1
pip install --upgrade pip
pip install mcp-contextforge-gateway

# 2ï¸âƒ£  Environment variables (session-only)
$Env:MCPGATEWAY_UI_ENABLED        = "true"
$Env:MCPGATEWAY_ADMIN_API_ENABLED = "true"
$Env:BASIC_AUTH_PASSWORD          = "changeme"      # admin/changeme
$Env:JWT_SECRET_KEY               = "my-test-key"

# 3ï¸âƒ£  Launch the gateway
mcpgateway.exe --host 0.0.0.0 --port 4444

#   Optional: background it
# Start-Process -FilePath "mcpgateway.exe" -ArgumentList "--host 0.0.0.0 --port 4444"

# 4ï¸âƒ£  Bearer token and smoke-test
$Env:MCPGATEWAY_BEARER_TOKEN = python3 -m mcpgateway.utils.create_jwt_token `
    --username admin@example.com --exp 10080 --secret my-test-key

curl -s -H "Authorization: Bearer $Env:MCPGATEWAY_BEARER_TOKEN" `
     http://127.0.0.1:4444/version | jq
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;More configuration&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Copy &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/.env.example"&gt;.env.example&lt;/a&gt; to &lt;code&gt;.env&lt;/code&gt; and tweak any of the settings (or use them as env variables).&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸš€ End-to-end demo (register a local MCP server)&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# 1ï¸âƒ£  Spin up the sample GO MCP time server using mcpgateway.translate &amp;amp; docker
python3 -m mcpgateway.translate \
     --stdio "docker run --rm -i -p 8888:8080 ghcr.io/ibm/fast-time-server:latest -transport=stdio" \
     --expose-sse \
     --port 8003

# Or using the official mcp-server-git using uvx:
pip install uv # to install uvx, if not already installed
python3 -m mcpgateway.translate --stdio "uvx mcp-server-git" --expose-sse --port 9000

# Alternative: running the local binary
# cd mcp-servers/go/fast-time-server; make build
# python3 -m mcpgateway.translate --stdio "./dist/fast-time-server -transport=stdio" --expose-sse --port 8002

# NEW: Expose via multiple protocols simultaneously!
python3 -m mcpgateway.translate \
     --stdio "uvx mcp-server-git" \
     --expose-sse \
     --expose-streamable-http \
     --port 9000
# Now accessible via both /sse (SSE) and /mcp (streamable HTTP) endpoints

# 2ï¸âƒ£  Register it with the gateway
curl -s -X POST -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{"name":"fast_time","url":"http://localhost:9000/sse"}' \
     http://localhost:4444/gateways

# 3ï¸âƒ£  Verify tool catalog
curl -s -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" http://localhost:4444/tools | jq

# 4ï¸âƒ£  Create a *virtual server* bundling those tools. Use the ID of tools from the tool catalog (Step #3) and pass them in the associatedTools list.
curl -s -X POST -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{"name":"time_server","description":"Fast time tools","associatedTools":[&amp;lt;ID_OF_TOOLS&amp;gt;]}' \
     http://localhost:4444/servers | jq

# Example curl
curl -s -X POST -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN"
     -H "Content-Type: application/json"
     -d '{"name":"time_server","description":"Fast time tools","associatedTools":["6018ca46d32a4ac6b4c054c13a1726a2"]}' \
     http://localhost:4444/servers | jq

# 5ï¸âƒ£  List servers (should now include the UUID of the newly created virtual server)
curl -s -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" http://localhost:4444/servers | jq

# 6ï¸âƒ£  Client SSE endpoint. Inspect it interactively with the MCP Inspector CLI (or use any MCP client)
npx -y @modelcontextprotocol/inspector
# Transport Type: SSE, URL: http://localhost:4444/servers/UUID_OF_SERVER_1/sse,  Header Name: "Authorization", Bearer Token
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ–§ Using the stdio wrapper (mcpgateway-wrapper)&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;export MCP_AUTH=$MCPGATEWAY_BEARER_TOKEN
export MCP_SERVER_URL=http://localhost:4444/servers/UUID_OF_SERVER_1/mcp
python3 -m mcpgateway.wrapper  # Ctrl-C to exit
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;You can also run it with &lt;code&gt;uv&lt;/code&gt; or inside Docker/Podman - see the &lt;em&gt;Containers&lt;/em&gt; section above.&lt;/p&gt; 
 &lt;p&gt;In MCP Inspector, define &lt;code&gt;MCP_AUTH&lt;/code&gt; and &lt;code&gt;MCP_SERVER_URL&lt;/code&gt; env variables, and select &lt;code&gt;python3&lt;/code&gt; as the Command, and &lt;code&gt;-m mcpgateway.wrapper&lt;/code&gt; as Arguments.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;echo $PWD/.venv/bin/python3 # Using the Python3 full path ensures you have a working venv
export MCP_SERVER_URL='http://localhost:4444/servers/UUID_OF_SERVER_1/mcp'
export MCP_AUTH=${MCPGATEWAY_BEARER_TOKEN}
npx -y @modelcontextprotocol/inspector
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;or&lt;/p&gt; 
 &lt;p&gt;Pass the url and auth as arguments (no need to set environment variables)&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;npx -y @modelcontextprotocol/inspector
command as `python`
Arguments as `-m mcpgateway.wrapper --url "http://localhost:4444/servers/UUID_OF_SERVER_1/mcp" --auth "Bearer &amp;lt;your token&amp;gt;"`
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;When using a MCP Client such as Claude with stdio:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "mcpgateway-wrapper": {
      "command": "python",
      "args": ["-m", "mcpgateway.wrapper"],
      "env": {
        "MCP_AUTH": "your-token-here",
        "MCP_SERVER_URL": "http://localhost:4444/servers/UUID_OF_SERVER_1",
        "MCP_TOOL_CALL_TIMEOUT": "120"
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Quick Start - Containers&lt;/h2&gt; 
&lt;p&gt;Use the official OCI image from GHCR with &lt;strong&gt;Docker&lt;/strong&gt; &lt;em&gt;or&lt;/em&gt; &lt;strong&gt;Podman&lt;/strong&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;ğŸ³ Docker&lt;/h3&gt; 
&lt;h4&gt;1 - Minimum viable run&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -d --name mcpgateway \
  -p 4444:4444 \
  -e MCPGATEWAY_UI_ENABLED=true \
  -e MCPGATEWAY_ADMIN_API_ENABLED=true \
  -e HOST=0.0.0.0 \
  -e JWT_SECRET_KEY=my-test-key \
  -e BASIC_AUTH_USER=admin \
  -e BASIC_AUTH_PASSWORD=changeme \
  -e AUTH_REQUIRED=true \
  -e DATABASE_URL=sqlite:///./mcp.db \
  ghcr.io/ibm/mcp-context-forge:0.6.0

# Tail logs (Ctrl+C to quit)
docker logs -f mcpgateway

# Generating an API key
docker run --rm -it ghcr.io/ibm/mcp-context-forge:0.6.0 \
  python3 -m mcpgateway.utils.create_jwt_token --username admin@example.com --exp 0 --secret my-test-key
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Browse to &lt;strong&gt;&lt;a href="http://localhost:4444/admin"&gt;http://localhost:4444/admin&lt;/a&gt;&lt;/strong&gt; (user &lt;code&gt;admin&lt;/code&gt; / pass &lt;code&gt;changeme&lt;/code&gt;).&lt;/p&gt; 
&lt;h4&gt;2 - Persist the SQLite database&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mkdir -p $(pwd)/data

touch $(pwd)/data/mcp.db

sudo chown -R :docker $(pwd)/data

chmod 777 $(pwd)/data

docker run -d --name mcpgateway \
  --restart unless-stopped \
  -p 4444:4444 \
  -v $(pwd)/data:/data \
  -e MCPGATEWAY_UI_ENABLED=true \
  -e MCPGATEWAY_ADMIN_API_ENABLED=true \
  -e DATABASE_URL=sqlite:////data/mcp.db \
  -e HOST=0.0.0.0 \
  -e JWT_SECRET_KEY=my-test-key \
  -e BASIC_AUTH_USER=admin \
  -e BASIC_AUTH_PASSWORD=changeme \
  ghcr.io/ibm/mcp-context-forge:0.6.0
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;SQLite now lives on the host at &lt;code&gt;./data/mcp.db&lt;/code&gt;.&lt;/p&gt; 
&lt;h4&gt;3 - Local tool discovery (host network)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mkdir -p $(pwd)/data

touch $(pwd)/data/mcp.db

sudo chown -R :docker $(pwd)/data

chmod 777 $(pwd)/data

docker run -d --name mcpgateway \
  --network=host \
  -e MCPGATEWAY_UI_ENABLED=true \
  -e MCPGATEWAY_ADMIN_API_ENABLED=true \
  -e HOST=0.0.0.0 \
  -e PORT=4444 \
  -e DATABASE_URL=sqlite:////data/mcp.db \
  -v $(pwd)/data:/data \
  ghcr.io/ibm/mcp-context-forge:0.6.0
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Using &lt;code&gt;--network=host&lt;/code&gt; allows Docker to access the local network, allowing you to add MCP servers running on your host. See &lt;a href="https://docs.docker.com/engine/network/drivers/host/"&gt;Docker Host network driver documentation&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;ğŸ¦­ Podman (rootless-friendly)&lt;/h3&gt; 
&lt;h4&gt;1 - Basic run&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;podman run -d --name mcpgateway \
  -p 4444:4444 \
  -e HOST=0.0.0.0 \
  -e DATABASE_URL=sqlite:///./mcp.db \
  ghcr.io/ibm/mcp-context-forge:0.6.0
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;2 - Persist SQLite&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mkdir -p $(pwd)/data

touch $(pwd)/data/mcp.db

sudo chown -R :docker $(pwd)/data

chmod 777 $(pwd)/data

podman run -d --name mcpgateway \
  --restart=on-failure \
  -p 4444:4444 \
  -v $(pwd)/data:/data \
  -e DATABASE_URL=sqlite:////data/mcp.db \
  ghcr.io/ibm/mcp-context-forge:0.6.0
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;3 - Host networking (rootless)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mkdir -p $(pwd)/data

touch $(pwd)/data/mcp.db

sudo chown -R :docker $(pwd)/data

chmod 777 $(pwd)/data

podman run -d --name mcpgateway \
  --network=host \
  -v $(pwd)/data:/data \
  -e DATABASE_URL=sqlite:////data/mcp.db \
  ghcr.io/ibm/mcp-context-forge:0.6.0
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;âœï¸ Docker/Podman tips&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;.env files&lt;/strong&gt; - Put all the &lt;code&gt;-e FOO=&lt;/code&gt; lines into a file and replace them with &lt;code&gt;--env-file .env&lt;/code&gt;. See the provided &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/.env.example"&gt;.env.example&lt;/a&gt; for reference.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Pinned tags&lt;/strong&gt; - Use an explicit version (e.g. &lt;code&gt;v0.6.0&lt;/code&gt;) instead of &lt;code&gt;latest&lt;/code&gt; for reproducible builds.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;JWT tokens&lt;/strong&gt; - Generate one in the running container:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;docker exec mcpgateway python3 -m mcpgateway.utils.create_jwt_token -u admin@example.com -e 10080 --secret my-test-key
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Upgrades&lt;/strong&gt; - Stop, remove, and rerun with the same &lt;code&gt;-v $(pwd)/data:/data&lt;/code&gt; mount; your DB and config stay intact.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸš‘ Smoke-test the running container&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;curl -s -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     http://localhost:4444/health | jq
curl -s -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     http://localhost:4444/tools | jq
curl -s -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     http://localhost:4444/version | jq
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ–§ Running the MCP Gateway stdio wrapper&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;The &lt;code&gt;mcpgateway.wrapper&lt;/code&gt; lets you connect to the gateway over &lt;strong&gt;stdio&lt;/strong&gt; while keeping JWT authentication. You should run this from the MCP Client. The example below is just for testing.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Set environment variables
export MCPGATEWAY_BEARER_TOKEN=$(python3 -m mcpgateway.utils.create_jwt_token --username admin@example.com --exp 10080 --secret my-test-key)
export MCP_AUTH=${MCPGATEWAY_BEARER_TOKEN}
export MCP_SERVER_URL='http://localhost:4444/servers/UUID_OF_SERVER_1/mcp'
export MCP_TOOL_CALL_TIMEOUT=120
export MCP_WRAPPER_LOG_LEVEL=DEBUG  # or OFF to disable logging

docker run --rm -i \
  -e MCP_AUTH=$MCPGATEWAY_BEARER_TOKEN \
  -e MCP_SERVER_URL=http://host.docker.internal:4444/servers/UUID_OF_SERVER_1/mcp \
  -e MCP_TOOL_CALL_TIMEOUT=120 \
  -e MCP_WRAPPER_LOG_LEVEL=DEBUG \
  ghcr.io/ibm/mcp-context-forge:0.6.0 \
  python3 -m mcpgateway.wrapper
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Testing &lt;code&gt;mcpgateway.wrapper&lt;/code&gt; by hand:&lt;/h2&gt; 
&lt;p&gt;Because the wrapper speaks JSON-RPC over stdin/stdout, you can interact with it using nothing more than a terminal or pipes.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Start the MCP Gateway Wrapper
export MCP_AUTH=${MCPGATEWAY_BEARER_TOKEN}
export MCP_SERVER_URL=http://localhost:4444/servers/YOUR_SERVER_UUID
python3 -m mcpgateway.wrapper
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Initialize the protocol&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;# Initialize the protocol
{"jsonrpc":"2.0","id":1,"method":"initialize","params":{"protocolVersion":"2025-03-26","capabilities":{},"clientInfo":{"name":"demo","version":"0.0.1"}}}

# Then after the reply:
{"jsonrpc":"2.0","method":"notifications/initialized","params":{}}

# Get prompts
{"jsonrpc":"2.0","id":4,"method":"prompts/list"}
{"jsonrpc":"2.0","id":5,"method":"prompts/get","params":{"name":"greeting","arguments":{"user":"Bob"}}}

# Get resources
{"jsonrpc":"2.0","id":6,"method":"resources/list"}
{"jsonrpc":"2.0","id":7,"method":"resources/read","params":{"uri":"https://example.com/some.txt"}}

# Get / call tools
{"jsonrpc":"2.0","id":2,"method":"tools/list"}
{"jsonrpc":"2.0","id":3,"method":"tools/call","params":{"name":"get_system_time","arguments":{"timezone":"Europe/Dublin"}}}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Expected responses from mcpgateway.wrapper&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{"jsonrpc":"2.0","id":1,"result":{"protocolVersion":"2025-03-26","capabilities":{"experimental":{},"prompts":{"listChanged":false},"resources":{"subscribe":false,"listChanged":false},"tools":{"listChanged":false}},"serverInfo":{"name":"mcpgateway-wrapper","version":"0.6.0"}}}

# When there's no tools
{"jsonrpc":"2.0","id":2,"result":{"tools":[]}}

# After you add some tools and create a virtual server
{"jsonrpc":"2.0","id":2,"result":{"tools":[{"annotations":{"readOnlyHint":false,"destructiveHint":true,"idempotentHint":false,"openWorldHint":true},"description":"Convert time between different timezones","inputSchema":{"properties":{"source_timezone":{"description":"Source IANA timezone name","type":"string"},"target_timezone":{"description":"Target IANA timezone name","type":"string"},"time":{"description":"Time to convert in RFC3339 format or common formats like '2006-01-02 15:04:05'","type":"string"}},"required":["time","source_timezone","target_timezone"],"type":"object"},"name":"convert_time"},{"annotations":{"readOnlyHint":false,"destructiveHint":true,"idempotentHint":false,"openWorldHint":true},"description":"Get current system time in specified timezone","inputSchema":{"properties":{"timezone":{"description":"IANA timezone name (e.g., 'America/New_York', 'Europe/London'). Defaults to UTC","type":"string"}},"type":"object"},"name":"get_system_time"}]}}

# Running the time tool:
{"jsonrpc":"2.0","id":3,"result":{"content":[{"type":"text","text":"2025-07-09T00:09:45+01:00"}]}}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;ğŸ§© Running from an MCP Client (&lt;code&gt;mcpgateway.wrapper&lt;/code&gt;)&lt;/h3&gt; 
&lt;p&gt;The &lt;code&gt;mcpgateway.wrapper&lt;/code&gt; exposes everything your Gateway knows about over &lt;strong&gt;stdio&lt;/strong&gt;, so any MCP client that &lt;em&gt;can't&lt;/em&gt; (or &lt;em&gt;shouldn't&lt;/em&gt;) open an authenticated SSE stream still gets full tool-calling power.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Remember&lt;/strong&gt; to substitute your real Gateway URL (and server ID) for &lt;code&gt;http://localhost:4444/servers/UUID_OF_SERVER_1/mcp&lt;/code&gt;. When inside Docker/Podman, that often becomes &lt;code&gt;http://host.docker.internal:4444/servers/UUID_OF_SERVER_1/mcp&lt;/code&gt; (macOS/Windows) or the gateway container's hostname (Linux).&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ³ Docker / Podman&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;docker run -i --rm \
  --network=host \
  -e MCP_SERVER_URL=http://localhost:4444/servers/UUID_OF_SERVER_1/mcp \
  -e MCP_AUTH=${MCPGATEWAY_BEARER_TOKEN} \
  -e MCP_TOOL_CALL_TIMEOUT=120 \
  ghcr.io/ibm/mcp-context-forge:0.6.0 \
  python3 -m mcpgateway.wrapper
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ“¦ pipx (one-liner install &amp;amp; run)&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Install gateway package in its own isolated venv
pipx install --include-deps mcp-contextforge-gateway

# Run the stdio wrapper
MCP_AUTH=${MCPGATEWAY_BEARER_TOKEN} \
MCP_SERVER_URL=http://localhost:4444/servers/UUID_OF_SERVER_1/mcp \
python3 -m mcpgateway.wrapper
# Alternatively with uv
uv run --directory . -m mcpgateway.wrapper
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Claude Desktop JSON&lt;/strong&gt; (uses the host Python that pipx injected):&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "mcpgateway-wrapper": {
      "command": "python3",
      "args": ["-m", "mcpgateway.wrapper"],
      "env": {
        "MCP_AUTH": "&amp;lt;your-token&amp;gt;",
        "MCP_SERVER_URL": "http://localhost:4444/servers/UUID_OF_SERVER_1/mcp",
        "MCP_TOOL_CALL_TIMEOUT": "120"
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;âš¡ uv / uvx (light-speed venvs)&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h4&gt;1 - Install &lt;code&gt;uv&lt;/code&gt; (&lt;code&gt;uvx&lt;/code&gt; is an alias it provides)&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# (a) official one-liner
curl -Ls https://astral.sh/uv/install.sh | sh

# (b) or via pipx
pipx install uv
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;2 - Create an on-the-spot venv &amp;amp; run the wrapper&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Create venv in ~/.venv/mcpgateway (or current dir if you prefer)
uv venv ~/.venv/mcpgateway
source ~/.venv/mcpgateway/bin/activate

# Install the gateway package using uv
uv pip install mcp-contextforge-gateway

# Launch wrapper
MCP_AUTH=${MCPGATEWAY_BEARER_TOKEN} \
MCP_SERVER_URL=http://localhost:4444/servers/UUID_OF_SERVER_1/mcp \
uv run --directory . -m mcpgateway.wrapper # Use this just for testing, as the Client will run the uv command
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Claude Desktop JSON (runs through &lt;strong&gt;uvx&lt;/strong&gt;)&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "mcpgateway-wrapper": {
      "command": "uvx",
      "args": [
        "run",
        "--",
        "python",
        "-m",
        "mcpgateway.wrapper"
      ],
      "env": {
        "MCP_AUTH": "&amp;lt;your-token&amp;gt;",
        "MCP_SERVER_URL": "http://localhost:4444/servers/UUID_OF_SERVER_1/mcp"
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h3&gt;ğŸš€ Using with Claude Desktop (or any GUI MCP client)&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Edit Config&lt;/strong&gt; â†’ &lt;code&gt;File â–¸ Settings â–¸ Developer â–¸ Edit Config&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Paste one of the JSON blocks above (Docker / pipx / uvx).&lt;/li&gt; 
 &lt;li&gt;Restart the app so the new stdio server is spawned.&lt;/li&gt; 
 &lt;li&gt;Open logs in the same menu to verify &lt;code&gt;mcpgateway-wrapper&lt;/code&gt; started and listed your tools.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Need help? See:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;MCP Debugging Guide&lt;/strong&gt; - &lt;a href="https://modelcontextprotocol.io/docs/tools/debugging"&gt;https://modelcontextprotocol.io/docs/tools/debugging&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸš€ Quick Start: VS Code Dev Container&lt;/h2&gt; 
&lt;p&gt;Spin up a fully-loaded dev environment (Python 3.11, Docker/Podman CLI, all project dependencies) in just two clicks.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ“‹ Prerequisites&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;VS Code&lt;/strong&gt; with the &lt;a href="https://code.visualstudio.com/docs/devcontainers/containers"&gt;Dev Containers extension&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Docker&lt;/strong&gt; or &lt;strong&gt;Podman&lt;/strong&gt; installed and running locally&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ§° Setup Instructions&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h3&gt;1 - Clone &amp;amp; Open&lt;/h3&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/ibm/mcp-context-forge.git
cd mcp-context-forge
code .
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;VS Code will detect the &lt;code&gt;.devcontainer&lt;/code&gt; and prompt: &lt;strong&gt;"Reopen in Container"&lt;/strong&gt; &lt;em&gt;or&lt;/em&gt; manually run: &lt;kbd&gt;Ctrl/Cmd â‡§ P&lt;/kbd&gt; â†’ &lt;strong&gt;Dev Containers: Reopen in Container&lt;/strong&gt;&lt;/p&gt; 
 &lt;hr /&gt; 
 &lt;h3&gt;2 - First-Time Build (Automatic)&lt;/h3&gt; 
 &lt;p&gt;The container build will:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Install system packages &amp;amp; Python 3.11&lt;/li&gt; 
  &lt;li&gt;Run &lt;code&gt;make install-dev&lt;/code&gt; to pull all dependencies&lt;/li&gt; 
  &lt;li&gt;Execute tests to verify the toolchain&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;You'll land in &lt;code&gt;/workspace&lt;/code&gt; ready to develop.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ› ï¸ Daily Developer Workflow&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Common tasks inside the container:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Start dev server (hot reload)
make dev            # http://localhost:4444

# Run tests &amp;amp; linters
make test
make lint
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Optional:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;make bash&lt;/code&gt; - drop into an interactive shell&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;make clean&lt;/code&gt; - clear build artefacts &amp;amp; caches&lt;/li&gt; 
  &lt;li&gt;Port forwarding is automatic (customize via &lt;code&gt;.devcontainer/devcontainer.json&lt;/code&gt;)&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;â˜ï¸ GitHub Codespaces: 1-Click Cloud IDE&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;No local Docker? Use Codespaces:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Go to the repo â†’ &lt;strong&gt;Code â–¸ Codespaces â–¸ Create codespace on main&lt;/strong&gt;&lt;/li&gt; 
  &lt;li&gt;Wait for the container image to build in the cloud&lt;/li&gt; 
  &lt;li&gt;Develop using the same workflow above&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Quick Start (manual install)&lt;/h2&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Python â‰¥ 3.10&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;GNU Make&lt;/strong&gt; (optional, but all common workflows are available as Make targets)&lt;/li&gt; 
 &lt;li&gt;Optional: &lt;strong&gt;Docker / Podman&lt;/strong&gt; for containerized runs&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;One-liner (dev)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;make venv install serve
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;What it does:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Creates / activates a &lt;code&gt;.venv&lt;/code&gt; in your home folder &lt;code&gt;~/.venv/mcpgateway&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Installs the gateway and necessary dependencies&lt;/li&gt; 
 &lt;li&gt;Launches &lt;strong&gt;Gunicorn&lt;/strong&gt; (Uvicorn workers) on &lt;a href="http://localhost:4444"&gt;http://localhost:4444&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;For development, you can use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;make install-dev # Install development dependencies, ex: linters and test harness
make lint          # optional: run style checks (ruff, mypy, etc.)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Containerized (self-signed TLS)&lt;/h3&gt; 
&lt;h2&gt;Container Runtime Support&lt;/h2&gt; 
&lt;p&gt;This project supports both Docker and Podman. The Makefile automatically detects which runtime is available and handles image naming differences.&lt;/p&gt; 
&lt;h3&gt;Auto-detection&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;make container-build  # Uses podman if available, otherwise docker

&amp;gt; You can use docker or podman, ex:

```bash
make podman            # build production image
make podman-run-ssl    # run at https://localhost:4444
# or listen on port 4444 on your host directly, adds --network=host to podman
make podman-run-ssl-host
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Smoke-test the API&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -k -sX GET \
     -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     https://localhost:4444/tools | jq
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You should receive &lt;code&gt;[]&lt;/code&gt; until you register a tool.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;Via Make&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;make venv install          # create .venv + install deps
make serve                 # gunicorn on :4444
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;UV (alternative)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv venv &amp;amp;&amp;amp; source .venv/bin/activate
uv pip install -e '.[dev]' # IMPORTANT: in zsh, quote to disable glob expansion!
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;pip (alternative)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python3 -m venv .venv &amp;amp;&amp;amp; source .venv/bin/activate
pip install -e ".[dev]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Optional (PostgreSQL adapter)&lt;/h3&gt; 
&lt;p&gt;You can configure the gateway with SQLite, PostgreSQL (or any other compatible database) in .env.&lt;/p&gt; 
&lt;p&gt;When using PostgreSQL, you need to install &lt;code&gt;psycopg2&lt;/code&gt; driver.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv pip install psycopg2-binary   # dev convenience
# or
uv pip install psycopg2          # production build
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Quick Postgres container&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run --name mcp-postgres \
  -e POSTGRES_USER=postgres \
  -e POSTGRES_PASSWORD=mysecretpassword \
  -e POSTGRES_DB=mcp \
  -p 5432:5432 -d postgres
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;A &lt;code&gt;make compose-up&lt;/code&gt; target is provided along with a &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/docker-compose.yml"&gt;docker-compose.yml&lt;/a&gt; file to make this process simpler.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ”„ Upgrading to v0.7.0&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;âš ï¸ CRITICAL&lt;/strong&gt;: Version 0.7.0 introduces comprehensive multi-tenancy and requires database migration.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Backup Your Data First&lt;/h3&gt; 
&lt;p&gt;Before upgrading to v0.7.0, &lt;strong&gt;always&lt;/strong&gt; backup your database, environment configuration, and export your settings:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Backup database (SQLite example)
cp mcp.db mcp.db.backup.$(date +%Y%m%d_%H%M%S)

# Backup existing .env file
cp .env .env.bak

# Export configuration via Admin UI or API
curl -u admin:changeme "http://localhost:4444/admin/export/configuration" \
     -o config_backup_$(date +%Y%m%d_%H%M%S).json
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Migration Process&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Update &lt;code&gt;.env&lt;/code&gt;&lt;/strong&gt; - Copy new settings: &lt;code&gt;cp .env.example .env&lt;/code&gt; then configure &lt;code&gt;PLATFORM_ADMIN_EMAIL&lt;/code&gt; and other required multi-tenancy settings&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Run migration&lt;/strong&gt; - Database schema updates automatically: &lt;code&gt;python3 -m mcpgateway.bootstrap_db&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Verify migration&lt;/strong&gt; - Use verification script: &lt;code&gt;python3 scripts/verify_multitenancy_0_7_0_migration.py&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;If Migration Fails&lt;/h3&gt; 
&lt;p&gt;If the database migration fails or you encounter issues:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Restore database backup&lt;/strong&gt;: &lt;code&gt;cp mcp.db.backup.YYYYMMDD_HHMMSS mcp.db&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Restore .env backup&lt;/strong&gt;: &lt;code&gt;cp .env.bak .env&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Delete corrupted database&lt;/strong&gt;: &lt;code&gt;rm mcp.db&lt;/code&gt; (if migration partially completed)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Restore configuration&lt;/strong&gt;: Import your exported configuration via Admin UI&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Complete Migration Guide&lt;/h3&gt; 
&lt;p&gt;For detailed upgrade instructions, troubleshooting, and rollback procedures, see:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ“– &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/MIGRATION-0.7.0.md"&gt;MIGRATION-0.7.0.md&lt;/a&gt;&lt;/strong&gt; - Complete step-by-step upgrade guide&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ—ï¸ &lt;a href="https://ibm.github.io/mcp-context-forge/architecture/multitenancy/"&gt;Multi-tenancy Architecture&lt;/a&gt;&lt;/strong&gt; - Understanding the new system&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Configuration (&lt;code&gt;.env&lt;/code&gt; or env vars)&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;âš ï¸ If any required &lt;code&gt;.env&lt;/code&gt; variable is missing or invalid, the gateway will fail fast at startup with a validation error via Pydantic.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;You can get started by copying the provided &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/.env.example"&gt;.env.example&lt;/a&gt; to &lt;code&gt;.env&lt;/code&gt; and making the necessary edits to fit your environment.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ”§ Environment Configuration Variables&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h3&gt;Basic&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Setting&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;Default&lt;/th&gt; 
    &lt;th&gt;Options&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;APP_NAME&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Gateway / OpenAPI title&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;MCP Gateway&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;string&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;HOST&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Bind address for the app&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;127.0.0.1&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;IPv4/IPv6&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;PORT&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Port the server listens on&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;4444&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;1-65535&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;DATABASE_URL&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;SQLAlchemy connection URL&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;sqlite:///./mcp.db&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;any SQLAlchemy dialect&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;APP_ROOT_PATH&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Subpath prefix for app (e.g. &lt;code&gt;/gateway&lt;/code&gt;)&lt;/td&gt; 
    &lt;td&gt;(empty)&lt;/td&gt; 
    &lt;td&gt;string&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;TEMPLATES_DIR&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Path to Jinja2 templates&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;mcpgateway/templates&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;path&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;STATIC_DIR&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Path to static files&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;mcpgateway/static&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;path&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;PROTOCOL_VERSION&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;MCP protocol version supported&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;2025-03-26&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;string&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;ğŸ’¡ Use &lt;code&gt;APP_ROOT_PATH=/foo&lt;/code&gt; if reverse-proxying under a subpath like &lt;code&gt;https://host.com/foo/&lt;/code&gt;.&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;h3&gt;Authentication&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Setting&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;Default&lt;/th&gt; 
    &lt;th&gt;Options&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;BASIC_AUTH_USER&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Username for Admin UI login and HTTP Basic authentication&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;admin&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;string&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;BASIC_AUTH_PASSWORD&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Password for Admin UI login and HTTP Basic authentication&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;changeme&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;string&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;PLATFORM_ADMIN_EMAIL&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Email for bootstrap platform admin user (auto-created with admin privileges)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;admin@example.com&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;string&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;AUTH_REQUIRED&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Require authentication for all API routes&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;JWT_ALGORITHM&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Algorithm used to sign the JWTs (&lt;code&gt;HS256&lt;/code&gt; is default, HMAC-based)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;HS256&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;PyJWT algs&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;JWT_SECRET_KEY&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Secret key used to &lt;strong&gt;sign JWT tokens&lt;/strong&gt; for API access&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;my-test-key&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;string&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;JWT_PUBLIC_KEY_PATH&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;If an asymmetric algorithm is used, a public key is required&lt;/td&gt; 
    &lt;td&gt;(empty)&lt;/td&gt; 
    &lt;td&gt;path to pem&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;JWT_PRIVATE_KEY_PATH&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;If an asymmetric algorithm is used, a private key is required&lt;/td&gt; 
    &lt;td&gt;(empty)&lt;/td&gt; 
    &lt;td&gt;path to pem&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;JWT_AUDIENCE&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;JWT audience claim for token validation&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;mcpgateway-api&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;string&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;JWT_AUDIENCE_VERIFICATION&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Disables jwt audience verification (useful for DCR)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;boolean&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;JWT_ISSUER&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;JWT issuer claim for token validation&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;mcpgateway&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;string&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;TOKEN_EXPIRY&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Expiry of generated JWTs in minutes&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;10080&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;REQUIRE_TOKEN_EXPIRATION&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Require all JWT tokens to have expiration claims&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;AUTH_ENCRYPTION_SECRET&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Passphrase used to derive AES key for encrypting tool auth headers&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;my-test-salt&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;string&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;OAUTH_REQUEST_TIMEOUT&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;OAuth request timeout in seconds&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;30&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;OAUTH_MAX_RETRIES&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Maximum retries for OAuth token requests&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;3&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;ğŸ” &lt;code&gt;BASIC_AUTH_USER&lt;/code&gt;/&lt;code&gt;PASSWORD&lt;/code&gt; are used for:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Logging into the web-based Admin UI&lt;/li&gt; 
   &lt;li&gt;Accessing APIs via Basic Auth (&lt;code&gt;curl -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN"&lt;/code&gt;)&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;p&gt;ğŸ”‘ &lt;code&gt;JWT_SECRET_KEY&lt;/code&gt; is used to:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;Sign JSON Web Tokens (&lt;code&gt;Authorization: Bearer &amp;lt;token&amp;gt;&lt;/code&gt;)&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Generate tokens via:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;export MCPGATEWAY_BEARER_TOKEN=$(python3 -m mcpgateway.utils.create_jwt_token --username admin@example.com --exp 0 --secret my-test-key)
echo $MCPGATEWAY_BEARER_TOKEN
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Tokens allow non-interactive API clients to authenticate securely.&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;p&gt;ğŸ§ª Set &lt;code&gt;AUTH_REQUIRED=false&lt;/code&gt; during development if you want to disable all authentication (e.g. for local testing or open APIs) or clients that don't support SSE authentication. In production, you should use the SSE to stdio &lt;code&gt;mcpgateway-wrapper&lt;/code&gt; for such tools that don't support authenticated SSE, while still ensuring the gateway uses authentication.&lt;/p&gt; 
  &lt;p&gt;ğŸ” &lt;code&gt;AUTH_ENCRYPTION_SECRET&lt;/code&gt; is used to encrypt and decrypt tool authentication credentials (&lt;code&gt;auth_value&lt;/code&gt;). You must set the same value across environments to decode previously stored encrypted auth values. Recommended: use a long, random string.&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;h3&gt;UI Features&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Setting&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;Default&lt;/th&gt; 
    &lt;th&gt;Options&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;MCPGATEWAY_UI_ENABLED&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Enable the interactive Admin dashboard&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;MCPGATEWAY_ADMIN_API_ENABLED&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Enable API endpoints for admin ops&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;MCPGATEWAY_BULK_IMPORT_ENABLED&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Enable bulk import endpoint for tools&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;MCPGATEWAY_UI_TOOL_TEST_TIMEOUT&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Tool test timeout in milliseconds for the admin UI&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;60000&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;ğŸ–¥ï¸ Set both UI and Admin API to &lt;code&gt;false&lt;/code&gt; to disable management UI and APIs in production. ğŸ“¥ The bulk import endpoint allows importing up to 200 tools in a single request via &lt;code&gt;/admin/tools/import&lt;/code&gt;. â±ï¸ Increase &lt;code&gt;MCPGATEWAY_UI_TOOL_TEST_TIMEOUT&lt;/code&gt; if your tools make multiple API calls or operate in high-latency environments.&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;h3&gt;A2A (Agent-to-Agent) Features&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Setting&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;Default&lt;/th&gt; 
    &lt;th&gt;Options&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;MCPGATEWAY_A2A_ENABLED&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Enable A2A agent features&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;MCPGATEWAY_A2A_MAX_AGENTS&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Maximum number of A2A agents allowed&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;100&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;MCPGATEWAY_A2A_DEFAULT_TIMEOUT&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Default timeout for A2A HTTP requests (seconds)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;30&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;MCPGATEWAY_A2A_MAX_RETRIES&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Maximum retry attempts for A2A calls&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;3&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;MCPGATEWAY_A2A_METRICS_ENABLED&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Enable A2A agent metrics collection&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;ğŸ¤– &lt;strong&gt;A2A Integration&lt;/strong&gt;: Register external AI agents (OpenAI, Anthropic, custom) and expose them as MCP tools ğŸ“Š &lt;strong&gt;Metrics&lt;/strong&gt;: Track agent performance, success rates, and response times ğŸ”’ &lt;strong&gt;Security&lt;/strong&gt;: Encrypted credential storage and configurable authentication ğŸ›ï¸ &lt;strong&gt;Admin UI&lt;/strong&gt;: Dedicated tab for agent management with test functionality&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;A2A Configuration Effects:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;MCPGATEWAY_A2A_ENABLED=false&lt;/code&gt;: Completely disables A2A features (API endpoints return 404, admin tab hidden)&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;MCPGATEWAY_A2A_METRICS_ENABLED=false&lt;/code&gt;: Disables metrics collection while keeping functionality&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;Email-Based Authentication &amp;amp; User Management&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Setting&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;Default&lt;/th&gt; 
    &lt;th&gt;Options&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;EMAIL_AUTH_ENABLED&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Enable email-based authentication system&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;PLATFORM_ADMIN_EMAIL&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Email for bootstrap platform admin user&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;admin@example.com&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;string&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;PLATFORM_ADMIN_PASSWORD&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Password for bootstrap platform admin user&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;changeme&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;string&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;PLATFORM_ADMIN_FULL_NAME&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Full name for bootstrap platform admin user&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;Platform Administrator&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;string&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;ARGON2ID_TIME_COST&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Argon2id time cost (iterations)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;3&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;ARGON2ID_MEMORY_COST&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Argon2id memory cost in KiB&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;65536&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;ARGON2ID_PARALLELISM&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Argon2id parallelism (threads)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;1&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;PASSWORD_MIN_LENGTH&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Minimum password length&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;8&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;PASSWORD_REQUIRE_UPPERCASE&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Require uppercase letters in passwords&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;PASSWORD_REQUIRE_LOWERCASE&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Require lowercase letters in passwords&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;PASSWORD_REQUIRE_NUMBERS&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Require numbers in passwords&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;PASSWORD_REQUIRE_SPECIAL&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Require special characters in passwords&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;MAX_FAILED_LOGIN_ATTEMPTS&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Maximum failed login attempts before lockout&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;5&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;ACCOUNT_LOCKOUT_DURATION_MINUTES&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Account lockout duration in minutes&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;30&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h3&gt;MCP Client Authentication&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Setting&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;Default&lt;/th&gt; 
    &lt;th&gt;Options&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;MCP_CLIENT_AUTH_ENABLED&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Enable JWT authentication for MCP client operations&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;TRUST_PROXY_AUTH&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Trust proxy authentication headers&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;PROXY_USER_HEADER&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Header containing authenticated username from proxy&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;X-Authenticated-User&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;string&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;ğŸ” &lt;strong&gt;MCP Client Auth&lt;/strong&gt;: When &lt;code&gt;MCP_CLIENT_AUTH_ENABLED=false&lt;/code&gt;, you must set &lt;code&gt;TRUST_PROXY_AUTH=true&lt;/code&gt; if using a trusted authentication proxy. This is a security-sensitive setting.&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;h3&gt;SSO (Single Sign-On) Configuration&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Setting&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;Default&lt;/th&gt; 
    &lt;th&gt;Options&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;SSO_ENABLED&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Master switch for Single Sign-On authentication&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;SSO_AUTO_CREATE_USERS&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Automatically create users from SSO providers&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;SSO_TRUSTED_DOMAINS&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Trusted email domains (JSON array)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;JSON array&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;SSO_PRESERVE_ADMIN_AUTH&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Preserve local admin authentication when SSO enabled&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;SSO_REQUIRE_ADMIN_APPROVAL&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Require admin approval for new SSO registrations&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p&gt;&lt;strong&gt;GitHub OAuth:&lt;/strong&gt;&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Setting&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;Default&lt;/th&gt; 
    &lt;th&gt;Options&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;SSO_GITHUB_ENABLED&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Enable GitHub OAuth authentication&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;SSO_GITHUB_CLIENT_ID&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;GitHub OAuth client ID&lt;/td&gt; 
    &lt;td&gt;(none)&lt;/td&gt; 
    &lt;td&gt;string&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;SSO_GITHUB_CLIENT_SECRET&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;GitHub OAuth client secret&lt;/td&gt; 
    &lt;td&gt;(none)&lt;/td&gt; 
    &lt;td&gt;string&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;SSO_GITHUB_ADMIN_ORGS&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;GitHub orgs granting admin privileges (JSON)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;JSON array&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p&gt;&lt;strong&gt;Google OAuth:&lt;/strong&gt;&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Setting&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;Default&lt;/th&gt; 
    &lt;th&gt;Options&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;SSO_GOOGLE_ENABLED&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Enable Google OAuth authentication&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;SSO_GOOGLE_CLIENT_ID&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Google OAuth client ID&lt;/td&gt; 
    &lt;td&gt;(none)&lt;/td&gt; 
    &lt;td&gt;string&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;SSO_GOOGLE_CLIENT_SECRET&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Google OAuth client secret&lt;/td&gt; 
    &lt;td&gt;(none)&lt;/td&gt; 
    &lt;td&gt;string&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;SSO_GOOGLE_ADMIN_DOMAINS&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Google admin domains (JSON)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;JSON array&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p&gt;&lt;strong&gt;IBM Security Verify OIDC:&lt;/strong&gt;&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Setting&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;Default&lt;/th&gt; 
    &lt;th&gt;Options&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;SSO_IBM_VERIFY_ENABLED&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Enable IBM Security Verify OIDC authentication&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;SSO_IBM_VERIFY_CLIENT_ID&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;IBM Security Verify client ID&lt;/td&gt; 
    &lt;td&gt;(none)&lt;/td&gt; 
    &lt;td&gt;string&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;SSO_IBM_VERIFY_CLIENT_SECRET&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;IBM Security Verify client secret&lt;/td&gt; 
    &lt;td&gt;(none)&lt;/td&gt; 
    &lt;td&gt;string&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;SSO_IBM_VERIFY_ISSUER&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;IBM Security Verify OIDC issuer URL&lt;/td&gt; 
    &lt;td&gt;(none)&lt;/td&gt; 
    &lt;td&gt;string&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p&gt;&lt;strong&gt;Okta OIDC:&lt;/strong&gt;&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Setting&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;Default&lt;/th&gt; 
    &lt;th&gt;Options&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;SSO_OKTA_ENABLED&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Enable Okta OIDC authentication&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;SSO_OKTA_CLIENT_ID&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Okta client ID&lt;/td&gt; 
    &lt;td&gt;(none)&lt;/td&gt; 
    &lt;td&gt;string&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;SSO_OKTA_CLIENT_SECRET&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Okta client secret&lt;/td&gt; 
    &lt;td&gt;(none)&lt;/td&gt; 
    &lt;td&gt;string&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;SSO_OKTA_ISSUER&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Okta issuer URL&lt;/td&gt; 
    &lt;td&gt;(none)&lt;/td&gt; 
    &lt;td&gt;string&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p&gt;&lt;strong&gt;SSO Admin Assignment:&lt;/strong&gt;&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Setting&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;Default&lt;/th&gt; 
    &lt;th&gt;Options&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;SSO_AUTO_ADMIN_DOMAINS&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Email domains that automatically get admin privileges&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;JSON array&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h3&gt;Personal Teams Configuration&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Setting&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;Default&lt;/th&gt; 
    &lt;th&gt;Options&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;AUTO_CREATE_PERSONAL_TEAMS&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Enable automatic personal team creation for new users&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;PERSONAL_TEAM_PREFIX&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Personal team naming prefix&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;personal&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;string&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;MAX_TEAMS_PER_USER&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Maximum number of teams a user can belong to&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;50&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;MAX_MEMBERS_PER_TEAM&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Maximum number of members per team&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;100&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;INVITATION_EXPIRY_DAYS&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Number of days before team invitations expire&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;7&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;REQUIRE_EMAIL_VERIFICATION_FOR_INVITES&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Require email verification for team invitations&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h3&gt;Security&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Setting&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;Default&lt;/th&gt; 
    &lt;th&gt;Options&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;SKIP_SSL_VERIFY&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Skip upstream TLS verification&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;ENVIRONMENT&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Deployment environment (affects security defaults)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;development&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;development&lt;/code&gt;/&lt;code&gt;production&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;APP_DOMAIN&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Domain for production CORS origins&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;localhost&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;string&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;ALLOWED_ORIGINS&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;CORS allow-list&lt;/td&gt; 
    &lt;td&gt;Auto-configured by environment&lt;/td&gt; 
    &lt;td&gt;JSON array&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;CORS_ENABLED&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Enable CORS&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;CORS_ALLOW_CREDENTIALS&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Allow credentials in CORS&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;SECURE_COOKIES&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Force secure cookie flags&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;COOKIE_SAMESITE&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Cookie SameSite attribute&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;lax&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;strict&lt;/code&gt;/&lt;code&gt;lax&lt;/code&gt;/&lt;code&gt;none&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;SECURITY_HEADERS_ENABLED&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Enable security headers middleware&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;X_FRAME_OPTIONS&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;X-Frame-Options header value&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;DENY&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;DENY&lt;/code&gt;/&lt;code&gt;SAMEORIGIN&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;HSTS_ENABLED&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Enable HSTS header&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;HSTS_MAX_AGE&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;HSTS max age in seconds&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;31536000&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;REMOVE_SERVER_HEADERS&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Remove server identification&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;DOCS_ALLOW_BASIC_AUTH&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Allow Basic Auth for docs (in addition to JWT)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;strong&gt;CORS Configuration&lt;/strong&gt;: When &lt;code&gt;ENVIRONMENT=development&lt;/code&gt;, CORS origins are automatically configured for common development ports (3000, 8080, gateway port). In production, origins are constructed from &lt;code&gt;APP_DOMAIN&lt;/code&gt; (e.g., &lt;code&gt;https://yourdomain.com&lt;/code&gt;, &lt;code&gt;https://app.yourdomain.com&lt;/code&gt;). You can override this by explicitly setting &lt;code&gt;ALLOWED_ORIGINS&lt;/code&gt;.&lt;/p&gt; 
  &lt;p&gt;&lt;strong&gt;Security Headers&lt;/strong&gt;: The gateway automatically adds configurable security headers to all responses including CSP, X-Frame-Options, X-Content-Type-Options, X-Download-Options, and HSTS (on HTTPS). All headers can be individually enabled/disabled. Sensitive server headers are removed.&lt;/p&gt; 
  &lt;p&gt;&lt;strong&gt;iframe Embedding&lt;/strong&gt;: By default, &lt;code&gt;X-Frame-Options: DENY&lt;/code&gt; prevents iframe embedding for security. To allow embedding, set &lt;code&gt;X_FRAME_OPTIONS=SAMEORIGIN&lt;/code&gt; (same domain) or disable with &lt;code&gt;X_FRAME_OPTIONS=""&lt;/code&gt;. Also update CSP &lt;code&gt;frame-ancestors&lt;/code&gt; directive if needed.&lt;/p&gt; 
  &lt;p&gt;&lt;strong&gt;Cookie Security&lt;/strong&gt;: Authentication cookies are automatically configured with HttpOnly, Secure (in production), and SameSite attributes for CSRF protection.&lt;/p&gt; 
  &lt;p&gt;Note: do not quote the ALLOWED_ORIGINS values, this needs to be valid JSON, such as: ALLOWED_ORIGINS=["http://localhost", "http://localhost:4444"]&lt;/p&gt; 
  &lt;p&gt;Documentation endpoints (&lt;code&gt;/docs&lt;/code&gt;, &lt;code&gt;/redoc&lt;/code&gt;, &lt;code&gt;/openapi.json&lt;/code&gt;) are always protected by authentication. By default, they require Bearer token authentication. Setting &lt;code&gt;DOCS_ALLOW_BASIC_AUTH=true&lt;/code&gt; enables HTTP Basic Authentication as an additional method using the same credentials as &lt;code&gt;BASIC_AUTH_USER&lt;/code&gt; and &lt;code&gt;BASIC_AUTH_PASSWORD&lt;/code&gt;.&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;h3&gt;Logging&lt;/h3&gt; 
 &lt;p&gt;MCP Gateway provides flexible logging with &lt;strong&gt;stdout/stderr output by default&lt;/strong&gt; and &lt;strong&gt;optional file-based logging&lt;/strong&gt;. When file logging is enabled, it provides JSON formatting for structured logs and text formatting for console output.&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Setting&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;Default&lt;/th&gt; 
    &lt;th&gt;Options&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;LOG_LEVEL&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Minimum log level&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;INFO&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;DEBUG&lt;/code&gt;...&lt;code&gt;CRITICAL&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;LOG_FORMAT&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Console log format&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;json&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;json&lt;/code&gt;, &lt;code&gt;text&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;LOG_TO_FILE&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;Enable file logging&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;&lt;code&gt;false&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;LOG_FILE&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Log filename (when enabled)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;null&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;mcpgateway.log&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;LOG_FOLDER&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Directory for log files&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;null&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;logs&lt;/code&gt;, &lt;code&gt;/var/log/gateway&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;LOG_FILEMODE&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;File write mode&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;a+&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;a+&lt;/code&gt; (append), &lt;code&gt;w&lt;/code&gt; (overwrite)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;LOG_ROTATION_ENABLED&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;Enable log file rotation&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;&lt;code&gt;false&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;LOG_MAX_SIZE_MB&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Max file size before rotation (MB)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;1&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Any positive integer&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;LOG_BACKUP_COUNT&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Number of backup files to keep&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;5&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Any non-negative integer&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;LOG_BUFFER_SIZE_MB&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Size of in-memory log buffer (MB)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;1.0&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;float &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p&gt;&lt;strong&gt;Logging Behavior:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Default&lt;/strong&gt;: Logs only to &lt;strong&gt;stdout/stderr&lt;/strong&gt; with human-readable text format&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;File Logging&lt;/strong&gt;: When &lt;code&gt;LOG_TO_FILE=true&lt;/code&gt;, logs to &lt;strong&gt;both&lt;/strong&gt; file (JSON format) and console (text format)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Log Rotation&lt;/strong&gt;: When &lt;code&gt;LOG_ROTATION_ENABLED=true&lt;/code&gt;, files rotate at &lt;code&gt;LOG_MAX_SIZE_MB&lt;/code&gt; with &lt;code&gt;LOG_BACKUP_COUNT&lt;/code&gt; backup files (e.g., &lt;code&gt;.log.1&lt;/code&gt;, &lt;code&gt;.log.2&lt;/code&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Directory Creation&lt;/strong&gt;: Log folder is automatically created if it doesn't exist&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Centralized Service&lt;/strong&gt;: All modules use the unified &lt;code&gt;LoggingService&lt;/code&gt; for consistent formatting&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;Example Configurations:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Default: stdout/stderr only (recommended for containers)
LOG_LEVEL=INFO
# No additional config needed - logs to stdout/stderr

# Optional: Enable file logging (no rotation)
LOG_TO_FILE=true
LOG_FOLDER=/var/log/mcpgateway
LOG_FILE=gateway.log
LOG_FILEMODE=a+

# Optional: Enable file logging with rotation
LOG_TO_FILE=true
LOG_ROTATION_ENABLED=true
LOG_MAX_SIZE_MB=10
LOG_BACKUP_COUNT=3
LOG_FOLDER=/var/log/mcpgateway
LOG_FILE=gateway.log
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Default Behavior:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Logs are written &lt;strong&gt;only to stdout/stderr&lt;/strong&gt; in human-readable text format&lt;/li&gt; 
  &lt;li&gt;File logging is &lt;strong&gt;disabled by default&lt;/strong&gt; (no files created)&lt;/li&gt; 
  &lt;li&gt;Set &lt;code&gt;LOG_TO_FILE=true&lt;/code&gt; to enable optional file logging with JSON format&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;Observability (OpenTelemetry)&lt;/h3&gt; 
 &lt;p&gt;MCP Gateway includes &lt;strong&gt;vendor-agnostic OpenTelemetry support&lt;/strong&gt; for distributed tracing. Works with Phoenix, Jaeger, Zipkin, Tempo, DataDog, New Relic, and any OTLP-compatible backend.&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Setting&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;Default&lt;/th&gt; 
    &lt;th&gt;Options&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;OTEL_ENABLE_OBSERVABILITY&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Master switch for observability&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;OTEL_SERVICE_NAME&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Service identifier in traces&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;mcp-gateway&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;string&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;OTEL_SERVICE_VERSION&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Service version in traces&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;0.6.0&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;string&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;OTEL_DEPLOYMENT_ENVIRONMENT&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Environment tag (dev/staging/prod)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;development&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;string&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;OTEL_TRACES_EXPORTER&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Trace exporter backend&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;otlp&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;otlp&lt;/code&gt;, &lt;code&gt;jaeger&lt;/code&gt;, &lt;code&gt;zipkin&lt;/code&gt;, &lt;code&gt;console&lt;/code&gt;, &lt;code&gt;none&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;OTEL_RESOURCE_ATTRIBUTES&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Custom resource attributes&lt;/td&gt; 
    &lt;td&gt;(empty)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;key=value,key2=value2&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p&gt;&lt;strong&gt;OTLP Configuration&lt;/strong&gt; (for Phoenix, Tempo, DataDog, etc.):&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Setting&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;Default&lt;/th&gt; 
    &lt;th&gt;Options&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;OTEL_EXPORTER_OTLP_ENDPOINT&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;OTLP collector endpoint&lt;/td&gt; 
    &lt;td&gt;(none)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;http://localhost:4317&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;OTEL_EXPORTER_OTLP_PROTOCOL&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;OTLP protocol&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;grpc&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;grpc&lt;/code&gt;, &lt;code&gt;http/protobuf&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;OTEL_EXPORTER_OTLP_HEADERS&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Authentication headers&lt;/td&gt; 
    &lt;td&gt;(empty)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;api-key=secret,x-auth=token&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;OTEL_EXPORTER_OTLP_INSECURE&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Skip TLS verification&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p&gt;&lt;strong&gt;Alternative Backends&lt;/strong&gt; (optional):&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Setting&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;Default&lt;/th&gt; 
    &lt;th&gt;Options&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;OTEL_EXPORTER_JAEGER_ENDPOINT&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Jaeger collector endpoint&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;http://localhost:14268/api/traces&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;URL&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;OTEL_EXPORTER_ZIPKIN_ENDPOINT&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Zipkin collector endpoint&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;http://localhost:9411/api/v2/spans&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;URL&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p&gt;&lt;strong&gt;Performance Tuning&lt;/strong&gt;:&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Setting&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;Default&lt;/th&gt; 
    &lt;th&gt;Options&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;OTEL_TRACES_SAMPLER&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Sampling strategy&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;parentbased_traceidratio&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;always_on&lt;/code&gt;, &lt;code&gt;always_off&lt;/code&gt;, &lt;code&gt;traceidratio&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;OTEL_TRACES_SAMPLER_ARG&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Sample rate (0.0-1.0)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;0.1&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;float (0.1 = 10% sampling)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;OTEL_BSP_MAX_QUEUE_SIZE&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Max queued spans&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;2048&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;OTEL_BSP_MAX_EXPORT_BATCH_SIZE&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Max batch size for export&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;512&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;OTEL_BSP_SCHEDULE_DELAY&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Export interval (ms)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;5000&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p&gt;&lt;strong&gt;Quick Start with Phoenix&lt;/strong&gt;:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Start Phoenix for LLM observability
docker run -p 6006:6006 -p 4317:4317 arizephoenix/phoenix:latest

# Configure gateway
export OTEL_ENABLE_OBSERVABILITY=true
export OTEL_TRACES_EXPORTER=otlp
export OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317

# Run gateway - traces automatically sent to Phoenix
mcpgateway
&lt;/code&gt;&lt;/pre&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;ğŸ” &lt;strong&gt;What Gets Traced&lt;/strong&gt;: Tool invocations, prompt rendering, resource fetching, gateway federation, health checks, plugin execution (if enabled)&lt;/p&gt; 
  &lt;p&gt;ğŸš€ &lt;strong&gt;Zero Overhead&lt;/strong&gt;: When &lt;code&gt;OTEL_ENABLE_OBSERVABILITY=false&lt;/code&gt;, all tracing is disabled with no performance impact&lt;/p&gt; 
  &lt;p&gt;ğŸ“Š &lt;strong&gt;View Traces&lt;/strong&gt;: Phoenix UI at &lt;code&gt;http://localhost:6006&lt;/code&gt;, Jaeger at &lt;code&gt;http://localhost:16686&lt;/code&gt;, or your configured backend&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;h3&gt;Transport&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Setting&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;Default&lt;/th&gt; 
    &lt;th&gt;Options&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;TRANSPORT_TYPE&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Enabled transports&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;all&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;http&lt;/code&gt;,&lt;code&gt;ws&lt;/code&gt;,&lt;code&gt;sse&lt;/code&gt;,&lt;code&gt;stdio&lt;/code&gt;,&lt;code&gt;all&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;WEBSOCKET_PING_INTERVAL&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;WebSocket ping (secs)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;30&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;SSE_RETRY_TIMEOUT&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;SSE retry timeout (ms)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;5000&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;SSE_KEEPALIVE_ENABLED&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Enable SSE keepalive events&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;SSE_KEEPALIVE_INTERVAL&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;SSE keepalive interval (secs)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;30&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;USE_STATEFUL_SESSIONS&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;streamable http config&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;JSON_RESPONSE_ENABLED&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;json/sse streams (streamable http)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;strong&gt;ğŸ’¡ SSE Keepalive Events&lt;/strong&gt;: The gateway sends periodic keepalive events to prevent connection timeouts with proxies and load balancers. Disable with &lt;code&gt;SSE_KEEPALIVE_ENABLED=false&lt;/code&gt; if your client doesn't handle unknown event types. Common intervals: 30s (default), 60s (AWS ALB), 240s (Azure).&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;h3&gt;Federation&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Setting&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;Default&lt;/th&gt; 
    &lt;th&gt;Options&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;FEDERATION_ENABLED&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Enable federation&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;FEDERATION_DISCOVERY&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Auto-discover peers&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;FEDERATION_PEERS&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Comma-sep peer URLs&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;JSON array&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;FEDERATION_TIMEOUT&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Gateway timeout (secs)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;30&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;FEDERATION_SYNC_INTERVAL&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Sync interval (secs)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;300&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h3&gt;Resources&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Setting&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;Default&lt;/th&gt; 
    &lt;th&gt;Options&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;RESOURCE_CACHE_SIZE&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;LRU cache size&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;1000&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;RESOURCE_CACHE_TTL&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Cache TTL (seconds)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;3600&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;MAX_RESOURCE_SIZE&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Max resource bytes&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;10485760&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;ALLOWED_MIME_TYPES&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Acceptable MIME types&lt;/td&gt; 
    &lt;td&gt;see code&lt;/td&gt; 
    &lt;td&gt;JSON array&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h3&gt;Tools&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Setting&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;Default&lt;/th&gt; 
    &lt;th&gt;Options&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;TOOL_TIMEOUT&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Tool invocation timeout (secs)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;60&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;MAX_TOOL_RETRIES&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Max retry attempts&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;3&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int â‰¥ 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;TOOL_RATE_LIMIT&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Tool calls per minute&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;100&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;TOOL_CONCURRENT_LIMIT&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Concurrent tool invocations&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;10&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;GATEWAY_TOOL_NAME_SEPARATOR&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Tool name separator for gateway routing&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;-&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;-&lt;/code&gt;, &lt;code&gt;--&lt;/code&gt;, &lt;code&gt;_&lt;/code&gt;, &lt;code&gt;.&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h3&gt;Prompts&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Setting&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;Default&lt;/th&gt; 
    &lt;th&gt;Options&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;PROMPT_CACHE_SIZE&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Cached prompt templates&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;100&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;MAX_PROMPT_SIZE&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Max prompt template size (bytes)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;102400&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;PROMPT_RENDER_TIMEOUT&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Jinja render timeout (secs)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;10&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h3&gt;Health Checks&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Setting&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;Default&lt;/th&gt; 
    &lt;th&gt;Options&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;HEALTH_CHECK_INTERVAL&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Health poll interval (secs)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;60&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;HEALTH_CHECK_TIMEOUT&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Health request timeout (secs)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;10&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;UNHEALTHY_THRESHOLD&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Fail-count before peer deactivation,&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;3&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;Set to -1 if deactivation is not needed.&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;GATEWAY_VALIDATION_TIMEOUT&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Gateway URL validation timeout (secs)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;5&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h3&gt;Database&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Setting&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;Default&lt;/th&gt; 
    &lt;th&gt;Options&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;DB_POOL_SIZE&lt;/code&gt; .&lt;/td&gt; 
    &lt;td&gt;SQLAlchemy connection pool size&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;200&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;DB_MAX_OVERFLOW&lt;/code&gt;.&lt;/td&gt; 
    &lt;td&gt;Extra connections beyond pool&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;10&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int â‰¥ 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;DB_POOL_TIMEOUT&lt;/code&gt;.&lt;/td&gt; 
    &lt;td&gt;Wait for connection (secs)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;30&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;DB_POOL_RECYCLE&lt;/code&gt;.&lt;/td&gt; 
    &lt;td&gt;Recycle connections (secs)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;3600&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;DB_MAX_RETRIES&lt;/code&gt; .&lt;/td&gt; 
    &lt;td&gt;Max Retry Attempts&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;3&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;DB_RETRY_INTERVAL_MS&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Retry Interval (ms)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;2000&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h3&gt;Cache Backend&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Setting&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;Default&lt;/th&gt; 
    &lt;th&gt;Options&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;CACHE_TYPE&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Backend type&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;database&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;none&lt;/code&gt;, &lt;code&gt;memory&lt;/code&gt;, &lt;code&gt;database&lt;/code&gt;, &lt;code&gt;redis&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;REDIS_URL&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Redis connection URL&lt;/td&gt; 
    &lt;td&gt;(none)&lt;/td&gt; 
    &lt;td&gt;string or empty&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;CACHE_PREFIX&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Key prefix&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;mcpgw:&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;string&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;REDIS_MAX_RETRIES&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Max Retry Attempts&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;3&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;REDIS_RETRY_INTERVAL_MS&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Retry Interval (ms)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;2000&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;ğŸ§  &lt;code&gt;none&lt;/code&gt; disables caching entirely. Use &lt;code&gt;memory&lt;/code&gt; for dev, &lt;code&gt;database&lt;/code&gt; for local persistence, or &lt;code&gt;redis&lt;/code&gt; for distributed caching across multiple instances.&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;h3&gt;Database Management&lt;/h3&gt; 
 &lt;p&gt;MCP Gateway uses Alembic for database migrations. Common commands:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;make db-current&lt;/code&gt; - Show current database version&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;make db-upgrade&lt;/code&gt; - Apply pending migrations&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;make db-migrate&lt;/code&gt; - Create new migration&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;make db-history&lt;/code&gt; - Show migration history&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;make db-status&lt;/code&gt; - Detailed migration status&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h4&gt;Troubleshooting&lt;/h4&gt; 
 &lt;p&gt;&lt;strong&gt;Common Issues:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;"No 'script_location' key found"&lt;/strong&gt;: Ensure you're running from the project root directory.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;"Unknown SSE event: keepalive" warnings&lt;/strong&gt;: Some MCP clients don't recognize keepalive events. These warnings are harmless and don't affect functionality. To disable: &lt;code&gt;SSE_KEEPALIVE_ENABLED=false&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Connection timeouts with proxies/load balancers&lt;/strong&gt;: If experiencing timeouts, adjust keepalive interval to match your infrastructure: &lt;code&gt;SSE_KEEPALIVE_INTERVAL=60&lt;/code&gt; (AWS ALB) or &lt;code&gt;240&lt;/code&gt; (Azure).&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;Development&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Setting&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;Default&lt;/th&gt; 
    &lt;th&gt;Options&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;DEV_MODE&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Enable dev mode&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;RELOAD&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Auto-reload on changes&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;DEBUG&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Debug logging&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h3&gt;Well-Known URI Configuration&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Setting&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;Default&lt;/th&gt; 
    &lt;th&gt;Options&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;WELL_KNOWN_ENABLED&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Enable well-known URI endpoints (/.well-known/*)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;WELL_KNOWN_ROBOTS_TXT&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;robots.txt content&lt;/td&gt; 
    &lt;td&gt;(blocks crawlers)&lt;/td&gt; 
    &lt;td&gt;string&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;WELL_KNOWN_SECURITY_TXT&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;security.txt content (RFC 9116)&lt;/td&gt; 
    &lt;td&gt;(empty)&lt;/td&gt; 
    &lt;td&gt;string&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;WELL_KNOWN_CUSTOM_FILES&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Additional custom well-known files (JSON)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;JSON object&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;WELL_KNOWN_CACHE_MAX_AGE&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Cache control for well-known files (seconds)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;3600&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;ğŸ” &lt;strong&gt;robots.txt&lt;/strong&gt;: By default, blocks all crawlers for security. Customize for your needs.&lt;/p&gt; 
  &lt;p&gt;ğŸ” &lt;strong&gt;security.txt&lt;/strong&gt;: Define security contact information per RFC 9116. Leave empty to disable.&lt;/p&gt; 
  &lt;p&gt;ğŸ“„ &lt;strong&gt;Custom Files&lt;/strong&gt;: Add arbitrary well-known files like &lt;code&gt;ai.txt&lt;/code&gt;, &lt;code&gt;dnt-policy.txt&lt;/code&gt;, etc.&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;h3&gt;Header Passthrough Configuration&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Setting&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;Default&lt;/th&gt; 
    &lt;th&gt;Options&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;ENABLE_HEADER_PASSTHROUGH&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Enable HTTP header passthrough feature (âš ï¸ Security implications)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;DEFAULT_PASSTHROUGH_HEADERS&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Default headers to pass through (JSON array)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;["X-Tenant-Id", "X-Trace-Id"]&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;JSON array&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;âš ï¸ &lt;strong&gt;Security Warning&lt;/strong&gt;: Header passthrough is disabled by default for security. Only enable if you understand the implications and have reviewed which headers should be passed through to backing MCP servers. Authorization headers are not included in defaults.&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;h3&gt;Plugin Configuration&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Setting&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;Default&lt;/th&gt; 
    &lt;th&gt;Options&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;PLUGINS_ENABLED&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Enable the plugin framework&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;PLUGIN_CONFIG_FILE&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Path to main plugin configuration file&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;plugins/config.yaml&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;string&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;PLUGINS_CLI_COMPLETION&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Enable auto-completion for plugins CLI&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;PLUGINS_CLI_MARKUP_MODE&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Set markup mode for plugins CLI&lt;/td&gt; 
    &lt;td&gt;(none)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;rich&lt;/code&gt;, &lt;code&gt;markdown&lt;/code&gt;, &lt;code&gt;disabled&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h3&gt;HTTP Retry Configuration&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Setting&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;Default&lt;/th&gt; 
    &lt;th&gt;Options&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;RETRY_MAX_ATTEMPTS&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Maximum retry attempts for HTTP requests&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;3&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;RETRY_BASE_DELAY&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Base delay between retries (seconds)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;1.0&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;float &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;RETRY_MAX_DELAY&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Maximum delay between retries (seconds)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;60&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;RETRY_JITTER_MAX&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Maximum jitter fraction of base delay&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;0.5&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;float 0-1&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Running&lt;/h2&gt; 
&lt;h3&gt;Makefile&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt; make serve               # Run production Gunicorn server on
 make serve-ssl           # Run Gunicorn behind HTTPS on :4444 (uses ./certs)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Script helper&lt;/h3&gt; 
&lt;p&gt;To run the development (uvicorn) server:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;make dev
# or
./run.sh --reload --log debug --workers 2
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;code&gt;run.sh&lt;/code&gt; is a wrapper around &lt;code&gt;uvicorn&lt;/code&gt; that loads &lt;code&gt;.env&lt;/code&gt;, supports reload, and passes arguments to the server.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Key flags:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Flag&lt;/th&gt; 
   &lt;th&gt;Purpose&lt;/th&gt; 
   &lt;th&gt;Example&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-e, --env FILE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;load env-file&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;--env prod.env&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-H, --host&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;bind address&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;--host 127.0.0.1&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-p, --port&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;listen port&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;--port 8080&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-w, --workers&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;gunicorn workers&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;--workers 4&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-r, --reload&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;auto-reload&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;--reload&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Manual (Uvicorn)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uvicorn mcpgateway.main:app --host 0.0.0.0 --port 4444 --workers 4
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Authentication examples&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Generate a JWT token using JWT_SECRET_KEY and export it as MCPGATEWAY_BEARER_TOKEN
# Note that the module needs to be installed. If running locally use:
export MCPGATEWAY_BEARER_TOKEN=$(JWT_SECRET_KEY=my-test-key python3 -m mcpgateway.utils.create_jwt_token)

# Use the JWT token in an API call
curl -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" http://localhost:4444/tools
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;â˜ï¸ AWS / Azure / OpenShift&lt;/h2&gt; 
&lt;p&gt;Deployment details can be found in the GitHub Pages.&lt;/p&gt; 
&lt;h2&gt;â˜ï¸ IBM Cloud Code Engine Deployment&lt;/h2&gt; 
&lt;p&gt;This project supports deployment to &lt;a href="https://cloud.ibm.com/codeengine"&gt;IBM Cloud Code Engine&lt;/a&gt; using the &lt;strong&gt;ibmcloud&lt;/strong&gt; CLI and the IBM Container Registry.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;â˜ï¸ IBM Cloud Code Engine Deployment&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h3&gt;ğŸ”§ Prerequisites&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Podman &lt;strong&gt;or&lt;/strong&gt; Docker installed locally&lt;/li&gt; 
  &lt;li&gt;IBM Cloud CLI (use &lt;code&gt;make ibmcloud-cli-install&lt;/code&gt; to install)&lt;/li&gt; 
  &lt;li&gt;An &lt;a href="https://cloud.ibm.com/iam/apikeys"&gt;IBM Cloud API key&lt;/a&gt; with access to Code Engine &amp;amp; Container Registry&lt;/li&gt; 
  &lt;li&gt;Code Engine and Container Registry services &lt;strong&gt;enabled&lt;/strong&gt; in your IBM Cloud account&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;hr /&gt; 
 &lt;h3&gt;ğŸ“¦ Environment Variables&lt;/h3&gt; 
 &lt;p&gt;Create a &lt;strong&gt;&lt;code&gt;.env&lt;/code&gt;&lt;/strong&gt; file (or export the variables in your shell). The first block is &lt;strong&gt;required&lt;/strong&gt;; the second provides &lt;strong&gt;tunable defaults&lt;/strong&gt; you can override:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# â”€â”€ Required â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
IBMCLOUD_REGION=us-south
IBMCLOUD_RESOURCE_GROUP=default
IBMCLOUD_PROJECT=my-codeengine-project
IBMCLOUD_CODE_ENGINE_APP=mcpgateway
IBMCLOUD_IMAGE_NAME=us.icr.io/myspace/mcpgateway:latest
IBMCLOUD_IMG_PROD=mcpgateway/mcpgateway
IBMCLOUD_API_KEY=your_api_key_here   # Optional - omit to use interactive `ibmcloud login --sso`

# â”€â”€ Optional overrides (sensible defaults provided) â”€â”€â”€â”€â”€â”€
IBMCLOUD_CPU=1                       # vCPUs for the app
IBMCLOUD_MEMORY=4G                   # Memory allocation
IBMCLOUD_REGISTRY_SECRET=my-regcred  # Name of the Container Registry secret
&lt;/code&gt;&lt;/pre&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;âœ… &lt;strong&gt;Quick check:&lt;/strong&gt; &lt;code&gt;make ibmcloud-check-env&lt;/code&gt;&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;hr /&gt; 
 &lt;h3&gt;ğŸš€ Make Targets&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Target&lt;/th&gt; 
    &lt;th&gt;Purpose&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;make ibmcloud-cli-install&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Install IBM Cloud CLI and required plugins&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;make ibmcloud-login&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Log in to IBM Cloud (API key or SSO)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;make ibmcloud-ce-login&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Select the Code Engine project &amp;amp; region&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;make ibmcloud-tag&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Tag the local container image&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;make ibmcloud-push&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Push the image to IBM Container Registry&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;make ibmcloud-deploy&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;Create or update&lt;/strong&gt; the Code Engine application (uses CPU/memory/secret)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;make ibmcloud-ce-status&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Show current deployment status&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;make ibmcloud-ce-logs&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Stream logs from the running app&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;make ibmcloud-ce-rm&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Delete the Code Engine application&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;hr /&gt; 
 &lt;h3&gt;ğŸ“ Example Workflow&lt;/h3&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;make ibmcloud-check-env
make ibmcloud-cli-install
make ibmcloud-login
make ibmcloud-ce-login
make ibmcloud-tag
make ibmcloud-push
make ibmcloud-deploy
make ibmcloud-ce-status
make ibmcloud-ce-logs
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h2&gt;API Endpoints&lt;/h2&gt; 
&lt;p&gt;You can test the API endpoints through curl, or Swagger UI, and check detailed documentation on ReDoc:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Swagger UI&lt;/strong&gt; â†’ &lt;a href="http://localhost:4444/docs"&gt;http://localhost:4444/docs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ReDoc&lt;/strong&gt; â†’ &lt;a href="http://localhost:4444/redoc"&gt;http://localhost:4444/redoc&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Generate an API Bearer token, and test the various API endpoints.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ” Authentication &amp;amp; Health Checks&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Generate a bearer token using the configured secret key (use the same as your .env)
export MCPGATEWAY_BEARER_TOKEN=$(python3 -m mcpgateway.utils.create_jwt_token -u admin@example.com --secret my-test-key)
echo ${MCPGATEWAY_BEARER_TOKEN}

# Quickly confirm that authentication works and the gateway is healthy
curl -s -k -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" https://localhost:4444/health
# {"status":"healthy"}

# Quickly confirm the gateway version &amp;amp; DB connectivity
curl -s -k -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" https://localhost:4444/version | jq
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ§± Protocol APIs (MCP) /protocol&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Initialize MCP session
curl -X POST -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{
           "protocol_version":"2025-03-26",
           "capabilities":{},
           "client_info":{"name":"MyClient","version":"1.0.0"}
         }' \
     http://localhost:4444/protocol/initialize

# Ping (JSON-RPC style)
curl -X POST -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{"jsonrpc":"2.0","id":1,"method":"ping"}' \
     http://localhost:4444/protocol/ping

# Completion for prompt/resource arguments (not implemented)
curl -X POST -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{
           "ref":{"type":"ref/prompt","name":"example_prompt"},
           "argument":{"name":"topic","value":"py"}
         }' \
     http://localhost:4444/protocol/completion/complete

# Sampling (streaming) (not implemented)
curl -N -X POST -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{
           "messages":[{"role":"user","content":{"type":"text","text":"Hello"}}],
           "maxTokens":16
         }' \
     http://localhost:4444/protocol/sampling/createMessage
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ§  JSON-RPC Utility /rpc&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Generic JSON-RPC calls (tools, gateways, roots, etc.)
curl -X POST -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{"jsonrpc":"2.0","id":1,"method":"list_tools"}' \
     http://localhost:4444/rpc
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Handles any method name: &lt;code&gt;list_tools&lt;/code&gt;, &lt;code&gt;list_gateways&lt;/code&gt;, &lt;code&gt;prompts/get&lt;/code&gt;, or invokes a tool if method matches a registered tool name .&lt;/p&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ”§ Tool Management /tools&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Register a new tool
curl -X POST -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{
           "name":"clock_tool",
           "url":"http://localhost:9000/rpc",
           "description":"Returns current time",
           "input_schema":{
             "type":"object",
             "properties":{"timezone":{"type":"string"}},
             "required":[]
           }
         }' \
     http://localhost:4444/tools

# List tools
curl -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" http://localhost:4444/tools

# Get tool by ID
curl -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" http://localhost:4444/tools/1

# Update tool
curl -X PUT -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{ "description":"Updated desc" }' \
     http://localhost:4444/tools/1

# Toggle active status
curl -X POST -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     http://localhost:4444/tools/1/toggle?activate=false
curl -X POST -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     http://localhost:4444/tools/1/toggle?activate=true

# Delete tool
curl -X DELETE -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" http://localhost:4444/tools/1
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ¤– A2A Agent Management /a2a&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Register a new A2A agent
curl -X POST -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{
           "name":"hello_world_agent",
           "endpoint_url":"http://localhost:9999/",
           "agent_type":"jsonrpc",
           "description":"External AI agent for hello world functionality",
           "auth_type":"api_key",
           "auth_value":"your-api-key",
           "tags":["ai", "hello-world"]
         }' \
     http://localhost:4444/a2a

# List A2A agents
curl -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" http://localhost:4444/a2a

# Get agent by ID
curl -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" http://localhost:4444/a2a/agent-id

# Update agent
curl -X PUT -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{ "description":"Updated description" }' \
     http://localhost:4444/a2a/agent-id

# Test agent (direct invocation)
curl -X POST -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{
           "parameters": {
             "method": "message/send",
             "params": {
               "message": {
                 "messageId": "test-123",
                 "role": "user",
                 "parts": [{"type": "text", "text": "Hello!"}]
               }
             }
           },
           "interaction_type": "test"
         }' \
     http://localhost:4444/a2a/agent-name/invoke

# Toggle agent status
curl -X POST -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     http://localhost:4444/a2a/agent-id/toggle?activate=false

# Delete agent
curl -X DELETE -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     http://localhost:4444/a2a/agent-id

# Associate agent with virtual server (agents become available as MCP tools)
curl -X POST -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{
           "name":"AI Assistant Server",
           "description":"Virtual server with AI agents",
           "associated_a2a_agents":["agent-id"]
         }' \
     http://localhost:4444/servers
&lt;/code&gt;&lt;/pre&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;ğŸ¤– &lt;strong&gt;A2A Integration&lt;/strong&gt;: A2A agents are external AI agents that can be registered and exposed as MCP tools ğŸ”„ &lt;strong&gt;Protocol Detection&lt;/strong&gt;: Gateway automatically detects JSONRPC vs custom A2A protocols ğŸ“Š &lt;strong&gt;Testing&lt;/strong&gt;: Built-in test functionality via Admin UI or &lt;code&gt;/a2a/{agent_id}/test&lt;/code&gt; endpoint ğŸ›ï¸ &lt;strong&gt;Virtual Servers&lt;/strong&gt;: Associate agents with servers to expose them as standard MCP tools&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸŒ Gateway Management /gateways&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Register an MCP server as a new gateway provider
curl -X POST -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{"name":"peer_gateway","url":"http://peer:4444"}' \
     http://localhost:4444/gateways

# List gateways
curl -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" http://localhost:4444/gateways

# Get gateway by ID
curl -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" http://localhost:4444/gateways/1

# Update gateway
curl -X PUT -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{"description":"New description"}' \
     http://localhost:4444/gateways/1

# Toggle active status
curl -X POST -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     http://localhost:4444/gateways/1/toggle?activate=false

# Delete gateway
curl -X DELETE -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" http://localhost:4444/gateways/1
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ“ Resource Management /resources&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Register resource
curl -X POST -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{
           "uri":"config://app/settings",
           "name":"App Settings",
           "content":"key=value"
         }' \
     http://localhost:4444/resources

# List resources
curl -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" http://localhost:4444/resources

# Read a resource
curl -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" http://localhost:4444/resources/config://app/settings

# Update resource
curl -X PUT -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{"content":"new=value"}' \
     http://localhost:4444/resources/config://app/settings

# Delete resource
curl -X DELETE -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" http://localhost:4444/resources/config://app/settings

# Subscribe to updates (SSE)
curl -N -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" http://localhost:4444/resources/subscribe/config://app/settings
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ“ Prompt Management /prompts&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Create prompt template
curl -X POST -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{
           "name":"greet",
           "template":"Hello, {{ user }}!",
           "argument_schema":{
             "type":"object",
             "properties":{"user":{"type":"string"}},
             "required":["user"]
           }
         }' \
     http://localhost:4444/prompts

# List prompts
curl -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" http://localhost:4444/prompts

# Get prompt (with args)
curl -X POST -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{"user":"Alice"}' \
     http://localhost:4444/prompts/greet

# Get prompt (no args)
curl -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" http://localhost:4444/prompts/greet

# Update prompt
curl -X PUT -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{"template":"Hi, {{ user }}!"}' \
     http://localhost:4444/prompts/greet

# Toggle active
curl -X POST -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     http://localhost:4444/prompts/5/toggle?activate=false

# Delete prompt
curl -X DELETE -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" http://localhost:4444/prompts/greet
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸŒ² Root Management /roots&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# List roots
curl -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" http://localhost:4444/roots

# Add root
curl -X POST -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{"uri":"/data","name":"Data Root"}' \
     http://localhost:4444/roots

# Remove root
curl -X DELETE -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" http://localhost:4444/roots/%2Fdata

# Subscribe to root changes (SSE)
curl -N -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" http://localhost:4444/roots/changes
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ–¥ï¸ Server Management /servers&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# List servers
curl -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" http://localhost:4444/servers

# Get server
curl -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" http://localhost:4444/servers/UUID_OF_SERVER_1

# Create server
curl -X POST -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{"name":"db","description":"Database","associatedTools": ["1","2","3"]}' \
     http://localhost:4444/servers

# Update server
curl -X PUT -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{"description":"Updated"}' \
     http://localhost:4444/servers/UUID_OF_SERVER_1

# Toggle active
curl -X POST -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     http://localhost:4444/servers/UUID_OF_SERVER_1/toggle?activate=false
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ“Š Metrics /metrics&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Get aggregated metrics
curl -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" http://localhost:4444/metrics

# Reset metrics (all or per-entity)
curl -X POST -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" http://localhost:4444/metrics/reset
curl -X POST -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" http://localhost:4444/metrics/reset?entity=tool&amp;amp;id=1
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ“¡ Events &amp;amp; Health&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# SSE: all events
curl -N -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" http://localhost:4444/events

# WebSocket
wscat -c ws://localhost:4444/ws \
      -H "Authorization: Basic $(echo -n admin:changeme|base64)"

# Health check
curl http://localhost:4444/health
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Full Swagger UI at &lt;code&gt;/docs&lt;/code&gt;.&lt;/p&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ› ï¸ Sample Tool&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;uvicorn sample_tool.clock_tool:app --host 0.0.0.0 --port 9000
&lt;/code&gt;&lt;/pre&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;curl -X POST -H "Content-Type: application/json" \
     -d '{"jsonrpc":"2.0","id":1,"method":"get_time","params":{"timezone":"UTC"}}' \
     http://localhost:9000/rpc
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Testing&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;make test            # Run unit tests
make lint            # Run lint tools
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Doctest Coverage&lt;/h2&gt; 
&lt;p&gt;MCP Context Forge implements comprehensive doctest coverage to ensure all code examples in documentation are tested and verified:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;make doctest         # Run all doctests
make doctest-verbose # Run with detailed output
make doctest-coverage # Generate coverage report
make doctest-check   # Check coverage percentage
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Coverage Status:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;âœ… &lt;strong&gt;Transport Modules&lt;/strong&gt;: 100% (base, stdio, SSE, WebSocket, streamable HTTP)&lt;/li&gt; 
 &lt;li&gt;âœ… &lt;strong&gt;Utility Functions&lt;/strong&gt;: 100% (slug generation, JWT tokens, validation)&lt;/li&gt; 
 &lt;li&gt;âœ… &lt;strong&gt;Configuration&lt;/strong&gt;: 100% (settings, environment variables)&lt;/li&gt; 
 &lt;li&gt;ğŸ”„ &lt;strong&gt;Service Classes&lt;/strong&gt;: ~60% (in progress)&lt;/li&gt; 
 &lt;li&gt;ğŸ”„ &lt;strong&gt;Complex Classes&lt;/strong&gt;: ~40% (in progress)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Benefits:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;All documented examples are automatically tested&lt;/li&gt; 
 &lt;li&gt;Documentation stays accurate and up-to-date&lt;/li&gt; 
 &lt;li&gt;Developers can run examples directly from docstrings&lt;/li&gt; 
 &lt;li&gt;Regression prevention through automated verification&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For detailed information, see the &lt;a href="https://ibm.github.io/mcp-context-forge/development/doctest-coverage/"&gt;Doctest Coverage Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Project Structure&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ“ Directory and file structure for mcpgateway&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CI / Quality &amp;amp; Meta-files â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”œâ”€â”€ .bumpversion.cfg                # Automated semantic-version bumps
â”œâ”€â”€ .coveragerc                     # Coverage.py settings
â”œâ”€â”€ .darglint                       # Doc-string linter rules
â”œâ”€â”€ .dockerignore                   # Context exclusions for image builds
â”œâ”€â”€ .editorconfig                   # Consistent IDE / editor behaviour
â”œâ”€â”€ .env                            # Local runtime variables (git-ignored)
â”œâ”€â”€ .env.ce                         # IBM Code Engine runtime env (ignored)
â”œâ”€â”€ .env.ce.example                 # Sample env for IBM Code Engine
â”œâ”€â”€ .env.example                    # Generic sample env file
â”œâ”€â”€ .env.gcr                        # Google Cloud Run runtime env (ignored)
â”œâ”€â”€ .eslintrc.json                  # ESLint rules for JS / TS assets
â”œâ”€â”€ .flake8                         # Flake-8 configuration
â”œâ”€â”€ .gitattributes                  # Git attributes (e.g. EOL normalisation)
â”œâ”€â”€ .github                         # GitHub settings, CI/CD workflows &amp;amp; templates
â”‚   â”œâ”€â”€ CODEOWNERS                  # Default reviewers
â”‚   â””â”€â”€ workflows/                  # Bandit, Docker, CodeQL, Python Package, Container Deployment, etc.
â”œâ”€â”€ .gitignore                      # Git exclusion rules
â”œâ”€â”€ .hadolint.yaml                  # Hadolint rules for Dockerfiles
â”œâ”€â”€ .htmlhintrc                     # HTMLHint rules
â”œâ”€â”€ .markdownlint.json              # Markdown-lint rules
â”œâ”€â”€ .pre-commit-config.yaml         # Pre-commit hooks (ruff, black, mypy, ...)
â”œâ”€â”€ .pycodestyle                    # PEP-8 checker settings
â”œâ”€â”€ .pylintrc                       # Pylint configuration
â”œâ”€â”€ .pyspelling.yml                 # Spell-checker dictionary &amp;amp; filters
â”œâ”€â”€ .ruff.toml                      # Ruff linter / formatter settings
â”œâ”€â”€ .spellcheck-en.txt              # Extra dictionary entries
â”œâ”€â”€ .stylelintrc.json               # Stylelint rules for CSS
â”œâ”€â”€ .travis.yml                     # Legacy Travis CI config (reference)
â”œâ”€â”€ .whitesource                    # WhiteSource security-scanning config
â”œâ”€â”€ .yamllint                       # yamllint ruleset

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Documentation &amp;amp; Guidance â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”œâ”€â”€ CHANGELOG.md                    # Version-by-version change log
â”œâ”€â”€ CODE_OF_CONDUCT.md              # Community behaviour guidelines
â”œâ”€â”€ CONTRIBUTING.md                 # How to file issues &amp;amp; send PRs
â”œâ”€â”€ DEVELOPING.md                   # Contributor workflows &amp;amp; style guide
â”œâ”€â”€ LICENSE                         # Apache License 2.0
â”œâ”€â”€ README.md                       # Project overview &amp;amp; quick-start
â”œâ”€â”€ SECURITY.md                     # Security policy &amp;amp; CVE disclosure process
â”œâ”€â”€ TESTING.md                      # Testing strategy, fixtures &amp;amp; guidelines

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Containerisation &amp;amp; Runtime â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”œâ”€â”€ Containerfile                   # OCI image build (Docker / Podman)
â”œâ”€â”€ Containerfile.lite              # FROM scratch UBI-Micro production build
â”œâ”€â”€ docker-compose.yml              # Local multi-service stack
â”œâ”€â”€ podman-compose-sonarqube.yaml   # One-liner SonarQube stack
â”œâ”€â”€ run-gunicorn.sh                 # Opinionated Gunicorn startup script
â”œâ”€â”€ run.sh                          # Uvicorn shortcut with arg parsing

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Build / Packaging / Tooling â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”œâ”€â”€ MANIFEST.in                     # sdist inclusion rules
â”œâ”€â”€ Makefile                        # Dev &amp;amp; deployment targets
â”œâ”€â”€ package-lock.json               # Deterministic npm lock-file
â”œâ”€â”€ package.json                    # Front-end / docs tooling deps
â”œâ”€â”€ pyproject.toml                  # Poetry / PDM config &amp;amp; lint rules
â”œâ”€â”€ sonar-code.properties           # SonarQube analysis settings
â”œâ”€â”€ uv.lock                         # UV resolver lock-file

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Kubernetes &amp;amp; Helm Assets â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”œâ”€â”€ charts                          # Helm chart(s) for K8s / OpenShift
â”‚   â”œâ”€â”€ mcp-stack                   # Umbrella chart
â”‚   â”‚   â”œâ”€â”€ Chart.yaml              # Chart metadata
â”‚   â”‚   â”œâ”€â”€ templates/...             # Manifest templates
â”‚   â”‚   â””â”€â”€ values.yaml             # Default values
â”‚   â””â”€â”€ README.md                   # Install / upgrade guide
â”œâ”€â”€ k8s                             # Raw (non-Helm) K8s manifests
â”‚   â””â”€â”€ *.yaml                      # Deployment, Service, PVC resources

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Documentation Source â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”œâ”€â”€ docs                            # MkDocs site source
â”‚   â”œâ”€â”€ base.yml                    # MkDocs "base" configuration snippet (do not modify)
â”‚   â”œâ”€â”€ mkdocs.yml                  # Site configuration (requires base.yml)
â”‚   â”œâ”€â”€ requirements.txt            # Python dependencies for the MkDocs site
â”‚   â”œâ”€â”€ Makefile                    # Make targets for building/serving the docs
â”‚   â””â”€â”€ theme                       # Custom MkDocs theme assets
â”‚       â””â”€â”€ logo.png                # Logo for the documentation theme
â”‚   â””â”€â”€ docs                        # Markdown documentation
â”‚       â”œâ”€â”€ architecture/           # ADRs for the project
â”‚       â”œâ”€â”€ articles/               # Long-form writeups
â”‚       â”œâ”€â”€ blog/                   # Blog posts
â”‚       â”œâ”€â”€ deployment/             # Deployment guides (AWS, Azure, etc.)
â”‚       â”œâ”€â”€ development/            # Development workflows &amp;amp; CI docs
â”‚       â”œâ”€â”€ images/                 # Diagrams &amp;amp; screenshots
â”‚       â”œâ”€â”€ index.md                # Top-level docs landing page
â”‚       â”œâ”€â”€ manage/                 # Management topics (backup, logging, tuning, upgrade)
â”‚       â”œâ”€â”€ overview/               # Feature overviews &amp;amp; UI documentation
â”‚       â”œâ”€â”€ security/               # Security guidance &amp;amp; policies
â”‚       â”œâ”€â”€ testing/                # Testing strategy &amp;amp; fixtures
â”‚       â””â”€â”€ using/                  # User-facing usage guides (agents, clients, etc.)
â”‚       â”œâ”€â”€ media/                  # Social media, press coverage, videos &amp;amp; testimonials
â”‚       â”‚   â”œâ”€â”€ press/              # Press articles and blog posts
â”‚       â”‚   â”œâ”€â”€ social/             # Tweets, LinkedIn posts, YouTube embeds
â”‚       â”‚   â”œâ”€â”€ testimonials/       # Customer quotes &amp;amp; community feedback
â”‚       â”‚   â””â”€â”€ kit/                # Media kit &amp;amp; logos for bloggers &amp;amp; press
â”œâ”€â”€ dictionary.dic                  # Custom dictionary for spell-checker (make spellcheck)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Application &amp;amp; Libraries â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”œâ”€â”€ agent_runtimes                  # Configurable agentic frameworks converted to MCP Servers
â”œâ”€â”€ mcpgateway                      # â† main application package
â”‚   â”œâ”€â”€ __init__.py                 # Package metadata &amp;amp; version constant
â”‚   â”œâ”€â”€ admin.py                    # FastAPI routers for Admin UI
â”‚   â”œâ”€â”€ cache
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ resource_cache.py       # LRU+TTL cache implementation
â”‚   â”‚   â””â”€â”€ session_registry.py     # Session â†” cache mapping
â”‚   â”œâ”€â”€ config.py                   # Pydantic settings loader
â”‚   â”œâ”€â”€ db.py                       # SQLAlchemy models &amp;amp; engine setup
â”‚   â”œâ”€â”€ federation
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ discovery.py            # Peer-gateway discovery
â”‚   â”‚   â”œâ”€â”€ forward.py              # RPC forwarding
â”‚   â”œâ”€â”€ handlers
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ sampling.py             # Streaming sampling handler
â”‚   â”œâ”€â”€ main.py                     # FastAPI app factory &amp;amp; startup events
â”‚   â”œâ”€â”€ mcp.db                      # SQLite fixture for tests
â”‚   â”œâ”€â”€ py.typed                    # PEP 561 marker (ships type hints)
â”‚   â”œâ”€â”€ schemas.py                  # Shared Pydantic DTOs
â”‚   â”œâ”€â”€ services
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ completion_service.py   # Prompt / argument completion
â”‚   â”‚   â”œâ”€â”€ gateway_service.py      # Peer-gateway registry
â”‚   â”‚   â”œâ”€â”€ logging_service.py      # Central logging helpers
â”‚   â”‚   â”œâ”€â”€ prompt_service.py       # Prompt CRUD &amp;amp; rendering
â”‚   â”‚   â”œâ”€â”€ resource_service.py     # Resource registration &amp;amp; retrieval
â”‚   â”‚   â”œâ”€â”€ root_service.py         # File-system root registry
â”‚   â”‚   â”œâ”€â”€ server_service.py       # Server registry &amp;amp; monitoring
â”‚   â”‚   â””â”€â”€ tool_service.py         # Tool registry &amp;amp; invocation
â”‚   â”œâ”€â”€ static
â”‚   â”‚   â”œâ”€â”€ admin.css               # Styles for Admin UI
â”‚   â”‚   â””â”€â”€ admin.js                # Behaviour for Admin UI
â”‚   â”œâ”€â”€ templates
â”‚   â”‚   â””â”€â”€ admin.html              # HTMX/Alpine Admin UI template
â”‚   â”œâ”€â”€ transports
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py                 # Abstract transport interface
â”‚   â”‚   â”œâ”€â”€ sse_transport.py        # Server-Sent Events transport
â”‚   â”‚   â”œâ”€â”€ stdio_transport.py      # stdio transport for embedding
â”‚   â”‚   â””â”€â”€ websocket_transport.py  # WS transport with ping/pong
â”‚   â”œâ”€â”€ models.py                   # Core enums / type aliases
â”‚   â”œâ”€â”€ utils
â”‚   â”‚   â”œâ”€â”€ create_jwt_token.py     # CLI &amp;amp; library for JWT generation
â”‚   â”‚   â”œâ”€â”€ services_auth.py        # Service-to-service auth dependency
â”‚   â”‚   â””â”€â”€ verify_credentials.py   # Basic / JWT auth helpers
â”‚   â”œâ”€â”€ validation
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ jsonrpc.py              # JSON-RPC 2.0 validation
â”‚   â””â”€â”€ version.py                  # Library version helper
â”œâ”€â”€ mcpgateway-wrapper              # Stdio client wrapper (PyPI)
â”‚   â”œâ”€â”€ pyproject.toml
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ src/mcpgateway_wrapper/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ server.py               # Wrapper entry-point
â”œâ”€â”€ mcp-servers                     # Sample downstream MCP servers
â”œâ”€â”€ mcp.db                          # Default SQLite DB (auto-created)
â”œâ”€â”€ mcpgrid                         # Experimental grid client / PoC
â”œâ”€â”€ os_deps.sh                      # Installs system-level deps for CI

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Tests &amp;amp; QA Assets â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”œâ”€â”€ test_readme.py                  # Guard: README stays in sync
â”œâ”€â”€ tests
â”‚   â”œâ”€â”€ conftest.py                 # Shared fixtures
â”‚   â”œâ”€â”€ e2e/...                       # End-to-end scenarios
â”‚   â”œâ”€â”€ hey/...                       # Load-test logs &amp;amp; helper script
â”‚   â”œâ”€â”€ integration/...               # API-level integration tests
â”‚   â””â”€â”€ unit/...                      # Pure unit tests for business logic
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h2&gt;API Documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Swagger UI&lt;/strong&gt; â†’ &lt;a href="http://localhost:4444/docs"&gt;http://localhost:4444/docs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ReDoc&lt;/strong&gt; â†’ &lt;a href="http://localhost:4444/redoc"&gt;http://localhost:4444/redoc&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Admin Panel&lt;/strong&gt; â†’ &lt;a href="http://localhost:4444/admin"&gt;http://localhost:4444/admin&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Makefile targets&lt;/h2&gt; 
&lt;p&gt;This project offer the following Makefile targets. Type &lt;code&gt;make&lt;/code&gt; in the project root to show all targets.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ”§ Available Makefile targets&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;ğŸ MCP CONTEXT FORGE  (An enterprise-ready Model Context Protocol Gateway)
ğŸ”§ SYSTEM-LEVEL DEPENDENCIES (DEV BUILD ONLY)
os-deps              - Install Graphviz, Pandoc, Trivy, SCC used for dev docs generation and security scan
ğŸŒ± VIRTUAL ENVIRONMENT &amp;amp; INSTALLATION
venv                 - Create a fresh virtual environment with uv &amp;amp; friends
activate             - Activate the virtual environment in the current shell
install              - Install project into the venv
install-dev          - Install project (incl. dev deps) into the venv
install-db           - Install project (incl. postgres and redis) into venv
update               - Update all installed deps inside the venv
check-env            - Verify all required env vars in .env are present
â–¶ï¸ SERVE &amp;amp; TESTING
serve                - Run production Gunicorn server on :4444
certs                - Generate self-signed TLS cert &amp;amp; key in ./certs (won't overwrite)
serve-ssl            - Run Gunicorn behind HTTPS on :4444 (uses ./certs)
dev                  - Run fast-reload dev server (uvicorn)
run                  - Execute helper script ./run.sh
test                 - Run unit tests with pytest
test-curl            - Smoke-test API endpoints with curl script
pytest-examples      - Run README / examples through pytest-examples
clean                - Remove caches, build artefacts, virtualenv, docs, certs, coverage, SBOM, etc.
ğŸ“Š COVERAGE &amp;amp; METRICS
coverage             - Run tests with coverage, emit md/HTML/XML + badge
pip-licenses         - Produce dependency license inventory (markdown)
scc                  - Quick LoC/complexity snapshot with scc
scc-report           - Generate HTML LoC &amp;amp; per-file metrics with scc
ğŸ“š DOCUMENTATION &amp;amp; SBOM
docs                 - Build docs (graphviz + handsdown + images + SBOM)
images               - Generate architecture &amp;amp; dependency diagrams
ğŸ” LINTING &amp;amp; STATIC ANALYSIS
lint                 - Run the full linting suite (see targets below)
black                - Reformat code with black
autoflake            - Remove unused imports / variables with autoflake
isort                - Organise &amp;amp; sort imports with isort
flake8               - PEP-8 style &amp;amp; logical errors
pylint               - Pylint static analysis
markdownlint         - Lint Markdown files with markdownlint (requires markdownlint-cli)
mypy                 - Static type-checking with mypy
bandit               - Security scan with bandit
pydocstyle           - Docstring style checker
pycodestyle          - Simple PEP-8 checker
pre-commit           - Run all configured pre-commit hooks
ruff                 - Ruff linter + formatter
ty                   - Ty type checker from astral
pyright              - Static type-checking with Pyright
radon                - Code complexity &amp;amp; maintainability metrics
pyroma               - Validate packaging metadata
importchecker        - Detect orphaned imports
spellcheck           - Spell-check the codebase
fawltydeps           - Detect undeclared / unused deps
wily                 - Maintainability report
pyre                 - Static analysis with Facebook Pyre
depend               - List dependencies in â‰ˆrequirements format
snakeviz             - Profile &amp;amp; visualise with snakeviz
pstats               - Generate PNG call-graph from cProfile stats
spellcheck-sort      - Sort local spellcheck dictionary
tox                  - Run tox across multi-Python versions
sbom                 - Produce a CycloneDX SBOM and vulnerability scan
pytype               - Flow-sensitive type checker
check-manifest       - Verify sdist/wheel completeness
yamllint            - Lint YAML files (uses .yamllint)
jsonlint            - Validate every *.json file with jq (--exit-status)
tomllint            - Validate *.toml files with tomlcheck
ğŸ•¸ï¸  WEBPAGE LINTERS &amp;amp; STATIC ANALYSIS (HTML/CSS/JS lint + security scans + formatting)
install-web-linters  - Install HTMLHint, Stylelint, ESLint, Retire.js &amp;amp; Prettier via npm
lint-web             - Run HTMLHint, Stylelint, ESLint, Retire.js and npm audit
format-web           - Format HTML, CSS &amp;amp; JS files with Prettier
osv-install          - Install/upgrade osv-scanner (Go)
osv-scan-source      - Scan source &amp;amp; lockfiles for CVEs
osv-scan-image       - Scan the built container image for CVEs
osv-scan             - Run all osv-scanner checks (source, image, licence)
ğŸ“¡ SONARQUBE ANALYSIS
sonar-deps-podman    - Install podman-compose + supporting tools
sonar-deps-docker    - Install docker-compose + supporting tools
sonar-up-podman      - Launch SonarQube with podman-compose
sonar-up-docker      - Launch SonarQube with docker-compose
sonar-submit-docker  - Run containerized Sonar Scanner CLI with Docker
sonar-submit-podman  - Run containerized Sonar Scanner CLI with Podman
pysonar-scanner      - Run scan with Python wrapper (pysonar-scanner)
sonar-info           - How to create a token &amp;amp; which env vars to export
ğŸ›¡ï¸ SECURITY &amp;amp; PACKAGE SCANNING
trivy                - Scan container image for CVEs (HIGH/CRIT). Needs podman socket enabled
grype-scan           - Scan container for security audit and vulnerability scanning
dockle               - Lint the built container image via tarball (no daemon/socket needed)
hadolint             - Lint Containerfile/Dockerfile(s) with hadolint
pip-audit            - Audit Python dependencies for published CVEs
ğŸ“¦ DEPENDENCY MANAGEMENT
deps-update          - Run update-deps.py to update all dependencies in pyproject.toml and docs/requirements.txt
containerfile-update - Update base image in Containerfile to latest tag
ğŸ“¦ PACKAGING &amp;amp; PUBLISHING
dist                 - Clean-build wheel *and* sdist into ./dist
wheel                - Build wheel only
sdist                - Build source distribution only
verify               - Build + twine + check-manifest + pyroma (no upload)
publish              - Verify, then upload to PyPI (needs TWINE_* creds)
ğŸ¦­ PODMAN CONTAINER BUILD &amp;amp; RUN
podman-dev           - Build development container image
podman               - Build container image
podman-prod          - Build production container image (using ubi-micro â†’ scratch). Not supported on macOS.
podman-run           - Run the container on HTTP  (port 4444)
podman-run-shell     - Run the container on HTTP  (port 4444) and start a shell
podman-run-ssl       - Run the container on HTTPS (port 4444, self-signed)
podman-run-ssl-host  - Run the container on HTTPS with --network=host (port 4444, self-signed)
podman-stop          - Stop &amp;amp; remove the container
podman-test          - Quick curl smoke-test against the container
podman-logs          - Follow container logs (âŒƒC to quit)
podman-stats         - Show container resource stats (if supported)
podman-top           - Show live top-level process info in container
podman-shell         - Open an interactive shell inside the Podman container
ğŸ‹ DOCKER BUILD &amp;amp; RUN
docker-dev           - Build development Docker image
docker               - Build production Docker image
docker-prod          - Build production container image (using ubi-micro â†’ scratch). Not supported on macOS.
docker-run           - Run the container on HTTP  (port 4444)
docker-run-ssl       - Run the container on HTTPS (port 4444, self-signed)
docker-stop          - Stop &amp;amp; remove the container
docker-test          - Quick curl smoke-test against the container
docker-logs          - Follow container logs (âŒƒC to quit)
docker-stats         - Show container resource usage stats (non-streaming)
docker-top           - Show top-level process info in Docker container
docker-shell         - Open an interactive shell inside the Docker container
ğŸ› ï¸ COMPOSE STACK     - Build / start / stop the multi-service stack
compose-up           - Bring the whole stack up (detached)
compose-restart      - Recreate changed containers, pulling / building as needed
compose-build        - Build (or rebuild) images defined in the compose file
compose-pull         - Pull the latest images only
compose-logs         - Tail logs from all services (Ctrl-C to exit)
compose-ps           - Show container status table
compose-shell        - Open an interactive shell in the "gateway" container
compose-stop         - Gracefully stop the stack (keep containers)
compose-down         - Stop &amp;amp; remove containers (keep named volumes)
compose-rm           - Remove *stopped* containers
compose-clean        - âœ¨ Down **and** delete named volumes (data-loss âš )
â˜ï¸ IBM CLOUD CODE ENGINE
ibmcloud-check-env          - Verify all required IBM Cloud env vars are set
ibmcloud-cli-install        - Auto-install IBM Cloud CLI + required plugins (OS auto-detected)
ibmcloud-login              - Login to IBM Cloud CLI using IBMCLOUD_API_KEY (--sso)
ibmcloud-ce-login           - Set Code Engine target project and region
ibmcloud-list-containers    - List deployed Code Engine apps
ibmcloud-tag                - Tag container image for IBM Container Registry
ibmcloud-push               - Push image to IBM Container Registry
ibmcloud-deploy             - Deploy (or update) container image in Code Engine
ibmcloud-ce-logs            - Stream logs for the deployed application
ibmcloud-ce-status          - Get deployment status
ibmcloud-ce-rm              - Delete the Code Engine application
ğŸ§ª MINIKUBE LOCAL CLUSTER
minikube-install      - Install Minikube (macOS, Linux, or Windows via choco)
helm-install          - Install Helm CLI (macOS, Linux, or Windows)
minikube-start        - Start local Minikube cluster with Ingress + DNS + metrics-server
minikube-stop         - Stop the Minikube cluster
minikube-delete       - Delete the Minikube cluster
minikube-image-load   - Build and load ghcr.io/ibm/mcp-context-forge:latest into Minikube
minikube-k8s-apply    - Apply Kubernetes manifests from deployment/k8s/
minikube-status       - Show status of Minikube and ingress pods
ğŸ› ï¸ HELM CHART TASKS
helm-lint            - Lint the Helm chart (static analysis)
helm-package         - Package the chart into dist/ as mcp-stack-&amp;lt;ver&amp;gt;.tgz
helm-deploy          - Upgrade/Install chart into Minikube (profile mcpgw)
helm-delete          - Uninstall the chart release from Minikube
ğŸ  LOCAL PYPI SERVER
local-pypi-install   - Install pypiserver for local testing
local-pypi-start     - Start local PyPI server on :8084 (no auth)
local-pypi-start-auth - Start local PyPI server with basic auth (admin/admin)
local-pypi-stop      - Stop local PyPI server
local-pypi-upload    - Upload existing package to local PyPI (no auth)
local-pypi-upload-auth - Upload existing package to local PyPI (with auth)
local-pypi-test      - Install package from local PyPI
local-pypi-clean     - Full cycle: build â†’ upload â†’ install locally
ğŸ  LOCAL DEVPI SERVER
devpi-install        - Install devpi server and client
devpi-init           - Initialize devpi server (first time only)
devpi-start          - Start devpi server
devpi-stop           - Stop devpi server
devpi-setup-user     - Create user and dev index
devpi-upload         - Upload existing package to devpi
devpi-test           - Install package from devpi
devpi-clean          - Full cycle: build â†’ upload â†’ install locally
devpi-status         - Show devpi server status
devpi-web            - Open devpi web interface
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;ğŸ” Troubleshooting&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;macOS: SQLite "disk I/O error" when running make serve&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;If the gateway fails on macOS with &lt;code&gt;sqlite3.OperationalError: disk I/O error&lt;/code&gt; (works on Linux/Docker), it's usually a filesystem/locking quirk rather than a schema bug.&lt;/p&gt; 
 &lt;p&gt;Quick placement guidance (macOS):&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;Avoid cloning/running the repo under &lt;code&gt;~/Documents&lt;/code&gt; or &lt;code&gt;~/Desktop&lt;/code&gt; if iCloud "Desktop &amp;amp; Documents" sync is enabled.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;A simple, safe choice is a project folder directly under your home directory:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;code&gt;mkdir -p "$HOME/mcp-context-forge" &amp;amp;&amp;amp; cd "$HOME/mcp-context-forge"&lt;/code&gt;&lt;/li&gt; 
    &lt;li&gt;If you keep the DB inside the repo, use a subfolder like &lt;code&gt;data/&lt;/code&gt; and an absolute path in &lt;code&gt;.env&lt;/code&gt;: 
     &lt;ul&gt; 
      &lt;li&gt;&lt;code&gt;mkdir -p "$HOME/mcp-context-forge/data"&lt;/code&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;code&gt;DATABASE_URL=sqlite:////Users/$USER/mcp-context-forge/data/mcp.db&lt;/code&gt;&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Use a safe, local APFS path for SQLite (avoid iCloud/Dropbox/OneDrive/Google Drive, network shares, or external exFAT/NAS):&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Option A (system location): point the DB to Application Support (note spaces): 
     &lt;ul&gt; 
      &lt;li&gt;&lt;code&gt;mkdir -p "$HOME/Library/Application Support/mcpgateway"&lt;/code&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;code&gt;export DATABASE_URL="sqlite:////Users/$USER/Library/Application Support/mcpgateway/mcp.db"&lt;/code&gt;&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
    &lt;li&gt;Option B (project-local): keep the DB under &lt;code&gt;~/mcp-context-forge/data&lt;/code&gt;: 
     &lt;ul&gt; 
      &lt;li&gt;&lt;code&gt;mkdir -p "$HOME/mcp-context-forge/data"&lt;/code&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;code&gt;export DATABASE_URL="sqlite:////Users/$USER/mcp-context-forge/data/mcp.db"&lt;/code&gt;&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Clean stale SQLite artifacts after any crash:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;code&gt;pkill -f mcpgateway || true &amp;amp;&amp;amp; rm -f mcp.db-wal mcp.db-shm mcp.db-journal&lt;/code&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Reduce startup concurrency to rule out multi-process contention:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;code&gt;GUNICORN_WORKERS=1 make serve&lt;/code&gt; (or use &lt;code&gt;make dev&lt;/code&gt; which runs single-process)&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Run the diagnostic helper to verify the environment:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;code&gt;python3 scripts/test_sqlite.py --verbose&lt;/code&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;While debugging, consider lowering pool pressure and retry:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;code&gt;DB_POOL_SIZE=10 DB_MAX_OVERFLOW=0 DB_POOL_TIMEOUT=60 DB_MAX_RETRIES=10 DB_RETRY_INTERVAL_MS=5000&lt;/code&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Optional: temporarily disable the file-lock leader path by using the in-process mode:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;code&gt;export CACHE_TYPE=none&lt;/code&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;If the error persists, update SQLite and ensure Python links against it:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;brew install sqlite3 &amp;amp;&amp;amp; brew link --force sqlite3&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;brew install python3 &amp;amp;&amp;amp; /opt/homebrew/bin/python3 -c 'import sqlite3; print(sqlite3.sqlite_version)'&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;See the full migration guide's "SQLite Troubleshooting Guide" for deeper steps (WAL cleanup, integrity check, recovery): &lt;code&gt;MIGRATION-0.7.0.md&lt;/code&gt;.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Port publishing on WSL2 (rootless Podman &amp;amp; Docker Desktop)&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h3&gt;Diagnose the listener&lt;/h3&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Inside your WSL distro
ss -tlnp | grep 4444        # Use ss
netstat -anp | grep 4444    # or netstat
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;em&gt;Seeing &lt;code&gt;:::4444 LISTEN rootlessport&lt;/code&gt; is normal&lt;/em&gt; - the IPv6 wildcard socket (&lt;code&gt;::&lt;/code&gt;) also accepts IPv4 traffic &lt;strong&gt;when&lt;/strong&gt; &lt;code&gt;net.ipv6.bindv6only = 0&lt;/code&gt; (default on Linux).&lt;/p&gt; 
 &lt;h3&gt;Why localhost fails on Windows&lt;/h3&gt; 
 &lt;p&gt;WSL 2's NAT layer rewrites only the &lt;em&gt;IPv6&lt;/em&gt; side of the dual-stack listener. From Windows, &lt;code&gt;http://127.0.0.1:4444&lt;/code&gt; (or Docker Desktop's "localhost") therefore times-out.&lt;/p&gt; 
 &lt;h4&gt;Fix (Podman rootless)&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Inside the WSL distro
echo "wsl" | sudo tee /etc/containers/podman-machine
systemctl --user restart podman.socket
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;code&gt;ss&lt;/code&gt; should now show &lt;code&gt;0.0.0.0:4444&lt;/code&gt; instead of &lt;code&gt;:::4444&lt;/code&gt;, and the service becomes reachable from Windows &lt;em&gt;and&lt;/em&gt; the LAN.&lt;/p&gt; 
 &lt;h4&gt;Fix (Docker Desktop &amp;gt; 4.19)&lt;/h4&gt; 
 &lt;p&gt;Docker Desktop adds a "WSL integration" switch per-distro. Turn it &lt;strong&gt;on&lt;/strong&gt; for your distro, restart Docker Desktop, then restart the container:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;docker restart mcpgateway
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Gateway starts but immediately exits ("Failed to read DATABASE_URL")&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Copy &lt;code&gt;.env.example&lt;/code&gt; to &lt;code&gt;.env&lt;/code&gt; first:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;cp .env.example .env
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Then edit &lt;code&gt;DATABASE_URL&lt;/code&gt;, &lt;code&gt;JWT_SECRET_KEY&lt;/code&gt;, &lt;code&gt;BASIC_AUTH_PASSWORD&lt;/code&gt;, etc. Missing or empty required vars cause a fast-fail at startup.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Fork the repo, create a feature branch.&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;make lint&lt;/code&gt; and fix any issues.&lt;/li&gt; 
 &lt;li&gt;Keep &lt;code&gt;make test&lt;/code&gt; green and 100% coverage.&lt;/li&gt; 
 &lt;li&gt;Open a PR - describe your changes clearly.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;See &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for more details.&lt;/h2&gt; 
&lt;h2&gt;Changelog&lt;/h2&gt; 
&lt;p&gt;A complete changelog can be found here: &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/CHANGELOG.md"&gt;CHANGELOG.md&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Licensed under the &lt;strong&gt;Apache License 2.0&lt;/strong&gt; - see &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/LICENSE"&gt;LICENSE&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Core Authors and Maintainers&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.linkedin.com/in/crivetimihai"&gt;Mihai Criveti&lt;/a&gt; - Distinguished Engineer, Agentic AI&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Special thanks to our contributors for helping us improve ContextForge MCP Gateway:&lt;/p&gt; 
&lt;a href="https://github.com/ibm/mcp-context-forge/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=ibm/mcp-context-forge&amp;amp;max=100&amp;amp;anon=0&amp;amp;columns=10" /&gt; &lt;/a&gt; 
&lt;h2&gt;Star History and Project Activity&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#ibm/mcp-context-forge&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=ibm/mcp-context-forge&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;!-- === Usage Stats === --&gt; 
&lt;p&gt;&lt;a href="https://pepy.tech/project/mcp-contextforge-gateway"&gt;&lt;img src="https://static.pepy.tech/badge/mcp-contextforge-gateway/month" alt="PyPi Downloads" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/ibm/mcp-context-forge/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/ibm/mcp-context-forge?style=social" alt="Stars" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/ibm/mcp-context-forge/network/members"&gt;&lt;img src="https://img.shields.io/github/forks/ibm/mcp-context-forge?style=social" alt="Forks" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/ibm/mcp-context-forge/graphs/contributors"&gt;&lt;img src="https://img.shields.io/github/contributors/ibm/mcp-context-forge" alt="Contributors" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/ibm/mcp-context-forge/commits"&gt;&lt;img src="https://img.shields.io/github/last-commit/ibm/mcp-context-forge" alt="Last Commit" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/ibm/mcp-context-forge/issues"&gt;&lt;img src="https://img.shields.io/github/issues/ibm/mcp-context-forge" alt="Open Issues" /&gt;&lt;/a&gt;&amp;nbsp;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>resemble-ai/chatterbox</title>
      <link>https://github.com/resemble-ai/chatterbox</link>
      <description>&lt;p&gt;SoTA open-source TTS&lt;/p&gt;&lt;hr&gt;&lt;img width="1200" height="600" alt="Chatterbox-Multilingual" src="https://www.resemble.ai/wp-content/uploads/2025/09/Chatterbox-Multilingual-1.png" /&gt; 
&lt;h1&gt;Chatterbox TTS&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://resemble-ai.github.io/chatterbox_demopage/"&gt;&lt;img src="https://img.shields.io/badge/listen-demo_samples-blue" alt="Alt Text" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/spaces/ResembleAI/Chatterbox"&gt;&lt;img src="https://huggingface.co/datasets/huggingface/badges/resolve/main/open-in-hf-spaces-sm.svg?sanitize=true" alt="Alt Text" /&gt;&lt;/a&gt; &lt;a href="https://podonos.com/resembleai/chatterbox"&gt;&lt;img src="https://static-public.podonos.com/badges/insight-on-pdns-sm-dark.svg?sanitize=true" alt="Alt Text" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/rJq9cRJBJ6"&gt;&lt;img src="https://img.shields.io/discord/1377773249798344776?label=join%20discord&amp;amp;logo=discord&amp;amp;style=flat" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;_Made with â™¥ï¸ by &lt;a href="https://resemble.ai" target="_blank"&gt;&lt;img width="100" alt="resemble-logo-horizontal" src="https://github.com/user-attachments/assets/35cf756b-3506-4943-9c72-c05ddfa4e525" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;We're excited to introduce &lt;strong&gt;Chatterbox Multilingual&lt;/strong&gt;, &lt;a href="https://resemble.ai"&gt;Resemble AI's&lt;/a&gt; first production-grade open source TTS model supporting &lt;strong&gt;23 languages&lt;/strong&gt; out of the box. Licensed under MIT, Chatterbox has been benchmarked against leading closed-source systems like ElevenLabs, and is consistently preferred in side-by-side evaluations.&lt;/p&gt; 
&lt;p&gt;Whether you're working on memes, videos, games, or AI agents, Chatterbox brings your content to life across languages. It's also the first open source TTS model to support &lt;strong&gt;emotion exaggeration control&lt;/strong&gt; with robust &lt;strong&gt;multilingual zero-shot voice cloning&lt;/strong&gt;. Try the english only version now on our &lt;a href="https://huggingface.co/spaces/ResembleAI/Chatterbox"&gt;English Hugging Face Gradio app.&lt;/a&gt;. Or try the multilingual version on our &lt;a href="https://huggingface.co/spaces/ResembleAI/Chatterbox-Multilingual-TTS"&gt;Multilingual Hugging Face Gradio app.&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you like the model but need to scale or tune it for higher accuracy, check out our competitively priced TTS service (&lt;a href="https://resemble.ai"&gt;link&lt;/a&gt;). It delivers reliable performance with ultra-low latency of sub 200msâ€”ideal for production use in agents, applications, or interactive media.&lt;/p&gt; 
&lt;h1&gt;Key Details&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;Multilingual, zero-shot TTS supporting 23 languages&lt;/li&gt; 
 &lt;li&gt;SoTA zeroshot English TTS&lt;/li&gt; 
 &lt;li&gt;0.5B Llama backbone&lt;/li&gt; 
 &lt;li&gt;Unique exaggeration/intensity control&lt;/li&gt; 
 &lt;li&gt;Ultra-stable with alignment-informed inference&lt;/li&gt; 
 &lt;li&gt;Trained on 0.5M hours of cleaned data&lt;/li&gt; 
 &lt;li&gt;Watermarked outputs&lt;/li&gt; 
 &lt;li&gt;Easy voice conversion script&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://podonos.com/resembleai/chatterbox"&gt;Outperforms ElevenLabs&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Supported Languages&lt;/h1&gt; 
&lt;p&gt;Arabic (ar) â€¢ Danish (da) â€¢ German (de) â€¢ Greek (el) â€¢ English (en) â€¢ Spanish (es) â€¢ Finnish (fi) â€¢ French (fr) â€¢ Hebrew (he) â€¢ Hindi (hi) â€¢ Italian (it) â€¢ Japanese (ja) â€¢ Korean (ko) â€¢ Malay (ms) â€¢ Dutch (nl) â€¢ Norwegian (no) â€¢ Polish (pl) â€¢ Portuguese (pt) â€¢ Russian (ru) â€¢ Swedish (sv) â€¢ Swahili (sw) â€¢ Turkish (tr) â€¢ Chinese (zh)&lt;/p&gt; 
&lt;h1&gt;Tips&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;General Use (TTS and Voice Agents):&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Ensure that the reference clip matches the specified language tag. Otherwise, language transfer outputs may inherit the accent of the reference clipâ€™s language. To mitigate this, set &lt;code&gt;cfg_weight&lt;/code&gt; to &lt;code&gt;0&lt;/code&gt;.&lt;/li&gt; 
   &lt;li&gt;The default settings (&lt;code&gt;exaggeration=0.5&lt;/code&gt;, &lt;code&gt;cfg_weight=0.5&lt;/code&gt;) work well for most prompts across all languages.&lt;/li&gt; 
   &lt;li&gt;If the reference speaker has a fast speaking style, lowering &lt;code&gt;cfg_weight&lt;/code&gt; to around &lt;code&gt;0.3&lt;/code&gt; can improve pacing.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Expressive or Dramatic Speech:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Try lower &lt;code&gt;cfg_weight&lt;/code&gt; values (e.g. &lt;code&gt;~0.3&lt;/code&gt;) and increase &lt;code&gt;exaggeration&lt;/code&gt; to around &lt;code&gt;0.7&lt;/code&gt; or higher.&lt;/li&gt; 
   &lt;li&gt;Higher &lt;code&gt;exaggeration&lt;/code&gt; tends to speed up speech; reducing &lt;code&gt;cfg_weight&lt;/code&gt; helps compensate with slower, more deliberate pacing.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Installation&lt;/h1&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install chatterbox-tts
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, you can install from source:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# conda create -yn chatterbox python=3.11
# conda activate chatterbox

git clone https://github.com/resemble-ai/chatterbox.git
cd chatterbox
pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We developed and tested Chatterbox on Python 3.11 on Debian 11 OS; the versions of the dependencies are pinned in &lt;code&gt;pyproject.toml&lt;/code&gt; to ensure consistency. You can modify the code or dependencies in this installation mode.&lt;/p&gt; 
&lt;h1&gt;Usage&lt;/h1&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import torchaudio as ta
from chatterbox.tts import ChatterboxTTS
from chatterbox.mtl_tts import ChatterboxMultilingualTTS

# English example
model = ChatterboxTTS.from_pretrained(device="cuda")

text = "Ezreal and Jinx teamed up with Ahri, Yasuo, and Teemo to take down the enemy's Nexus in an epic late-game pentakill."
wav = model.generate(text)
ta.save("test-english.wav", wav, model.sr)

# Multilingual examples
multilingual_model = ChatterboxMultilingualTTS.from_pretrained(device=device)

french_text = "Bonjour, comment Ã§a va? Ceci est le modÃ¨le de synthÃ¨se vocale multilingue Chatterbox, il prend en charge 23 langues."
wav_french = multilingual_model.generate(spanish_text, language_id="fr")
ta.save("test-french.wav", wav_french, model.sr)

chinese_text = "ä½ å¥½ï¼Œä»Šå¤©å¤©æ°”çœŸä¸é”™ï¼Œå¸Œæœ›ä½ æœ‰ä¸€ä¸ªæ„‰å¿«çš„å‘¨æœ«ã€‚"
wav_chinese = multilingual_model.generate(chinese_text, language_id="zh")
ta.save("test-chinese.wav", wav_chinese, model.sr)

# If you want to synthesize with a different voice, specify the audio prompt
AUDIO_PROMPT_PATH = "YOUR_FILE.wav"
wav = model.generate(text, audio_prompt_path=AUDIO_PROMPT_PATH)
ta.save("test-2.wav", wav, model.sr)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;code&gt;example_tts.py&lt;/code&gt; and &lt;code&gt;example_vc.py&lt;/code&gt; for more examples.&lt;/p&gt; 
&lt;h1&gt;Acknowledgements&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/FunAudioLLM/CosyVoice"&gt;Cosyvoice&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning"&gt;Real-Time-Voice-Cloning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/yl4579/HiFTNet"&gt;HiFT-GAN&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/meta-llama/llama3"&gt;Llama 3&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/xingchensong/S3Tokenizer"&gt;S3Tokenizer&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Built-in PerTh Watermarking for Responsible AI&lt;/h1&gt; 
&lt;p&gt;Every audio file generated by Chatterbox includes &lt;a href="https://github.com/resemble-ai/perth"&gt;Resemble AI's Perth (Perceptual Threshold) Watermarker&lt;/a&gt; - imperceptible neural watermarks that survive MP3 compression, audio editing, and common manipulations while maintaining nearly 100% detection accuracy.&lt;/p&gt; 
&lt;h2&gt;Watermark extraction&lt;/h2&gt; 
&lt;p&gt;You can look for the watermark using the following script.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import perth
import librosa

AUDIO_PATH = "YOUR_FILE.wav"

# Load the watermarked audio
watermarked_audio, sr = librosa.load(AUDIO_PATH, sr=None)

# Initialize watermarker (same as used for embedding)
watermarker = perth.PerthImplicitWatermarker()

# Extract watermark
watermark = watermarker.get_watermark(watermarked_audio, sample_rate=sr)
print(f"Extracted watermark: {watermark}")
# Output: 0.0 (no watermark) or 1.0 (watermarked)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Official Discord&lt;/h1&gt; 
&lt;p&gt;ğŸ‘‹ Join us on &lt;a href="https://discord.gg/rJq9cRJBJ6"&gt;Discord&lt;/a&gt; and let's build something awesome together!&lt;/p&gt; 
&lt;h1&gt;Citation&lt;/h1&gt; 
&lt;p&gt;If you find this model useful, please consider citing.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@misc{chatterboxtts2025,
  author       = {{Resemble AI}},
  title        = {{Chatterbox-TTS}},
  year         = {2025},
  howpublished = {\url{https://github.com/resemble-ai/chatterbox}},
  note         = {GitHub repository}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Disclaimer&lt;/h1&gt; 
&lt;p&gt;Don't use this model to do bad things. Prompts are sourced from freely available data on the internet.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>comfyanonymous/ComfyUI</title>
      <link>https://github.com/comfyanonymous/ComfyUI</link>
      <description>&lt;p&gt;The most powerful and modular diffusion model GUI, api and backend with a graph/nodes interface.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h1&gt;ComfyUI&lt;/h1&gt; 
 &lt;p&gt;&lt;strong&gt;The most powerful and modular visual AI engine and application.&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://www.comfy.org/"&gt;&lt;img src="https://img.shields.io/badge/ComfyOrg-4285F4?style=flat" alt="Website" /&gt;&lt;/a&gt; &lt;a href="https://www.comfy.org/discord"&gt;&lt;img src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fdiscord.com%2Fapi%2Finvites%2Fcomfyorg%3Fwith_counts%3Dtrue&amp;amp;query=%24.approximate_member_count&amp;amp;logo=discord&amp;amp;logoColor=white&amp;amp;label=Discord&amp;amp;color=green&amp;amp;suffix=%20total" alt="Dynamic JSON Badge" /&gt;&lt;/a&gt; &lt;a href="https://x.com/ComfyUI"&gt;&lt;img src="https://img.shields.io/twitter/follow/ComfyUI" alt="Twitter" /&gt;&lt;/a&gt; &lt;a href="https://app.element.io/#/room/%23comfyui_space%3Amatrix.org"&gt;&lt;img src="https://img.shields.io/badge/Matrix-000000?style=flat&amp;amp;logo=matrix&amp;amp;logoColor=white" alt="Matrix" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;a href="https://github.com/comfyanonymous/ComfyUI/releases"&gt;&lt;img src="https://img.shields.io/github/v/release/comfyanonymous/ComfyUI?style=flat&amp;amp;sort=semver" alt="" /&gt;&lt;/a&gt; &lt;a href="https://github.com/comfyanonymous/ComfyUI/releases"&gt;&lt;img src="https://img.shields.io/github/release-date/comfyanonymous/ComfyUI?style=flat" alt="" /&gt;&lt;/a&gt; &lt;a href="https://github.com/comfyanonymous/ComfyUI/releases"&gt;&lt;img src="https://img.shields.io/github/downloads/comfyanonymous/ComfyUI/total?style=flat" alt="" /&gt;&lt;/a&gt; &lt;a href="https://github.com/comfyanonymous/ComfyUI/releases"&gt;&lt;img src="https://img.shields.io/github/downloads/comfyanonymous/ComfyUI/latest/total?style=flat&amp;amp;label=downloads%40latest" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;!-- Workaround to display total user from https://github.com/badges/shields/issues/4500#issuecomment-2060079995 --&gt; 
 &lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/7ccaf2c1-9b72-41ae-9a89-5688c94b7abe" alt="ComfyUI Screenshot" /&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;ComfyUI lets you design and execute advanced stable diffusion pipelines using a graph/nodes/flowchart based interface. Available on Windows, Linux, and macOS.&lt;/p&gt; 
&lt;h2&gt;Get Started&lt;/h2&gt; 
&lt;h4&gt;&lt;a href="https://www.comfy.org/download"&gt;Desktop Application&lt;/a&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;The easiest way to get started.&lt;/li&gt; 
 &lt;li&gt;Available on Windows &amp;amp; macOS.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;&lt;a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/#installing"&gt;Windows Portable Package&lt;/a&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Get the latest commits and completely portable.&lt;/li&gt; 
 &lt;li&gt;Available on Windows.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;&lt;a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/#manual-install-windows-linux"&gt;Manual Install&lt;/a&gt;&lt;/h4&gt; 
&lt;p&gt;Supports all operating systems and GPU types (NVIDIA, AMD, Intel, Apple Silicon, Ascend).&lt;/p&gt; 
&lt;h2&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/"&gt;Examples&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;See what ComfyUI can do with the &lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/"&gt;example workflows&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Nodes/graph/flowchart interface to experiment and create complex Stable Diffusion workflows without needing to code anything.&lt;/li&gt; 
 &lt;li&gt;Image Models 
  &lt;ul&gt; 
   &lt;li&gt;SD1.x, SD2.x (&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/unclip/"&gt;unCLIP&lt;/a&gt;)&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/sdxl/"&gt;SDXL&lt;/a&gt;, &lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/sdturbo/"&gt;SDXL Turbo&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/stable_cascade/"&gt;Stable Cascade&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/sd3/"&gt;SD3 and SD3.5&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Pixart Alpha and Sigma&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/aura_flow/"&gt;AuraFlow&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/hunyuan_dit/"&gt;HunyuanDiT&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/flux/"&gt;Flux&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/lumina2/"&gt;Lumina Image 2.0&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/hidream/"&gt;HiDream&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/qwen_image/"&gt;Qwen Image&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Image Editing Models 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/omnigen/"&gt;Omnigen 2&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/flux/#flux-kontext-image-editing-model"&gt;Flux Kontext&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/hidream/#hidream-e11"&gt;HiDream E1.1&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/qwen_image/#edit-model"&gt;Qwen Image Edit&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Video Models 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/video/"&gt;Stable Video Diffusion&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/mochi/"&gt;Mochi&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/ltxv/"&gt;LTX-Video&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/hunyuan_video/"&gt;Hunyuan Video&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/wan/"&gt;Wan 2.1&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/wan22/"&gt;Wan 2.2&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Audio Models 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/audio/"&gt;Stable Audio&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/audio/"&gt;ACE Step&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;3D Models 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://docs.comfy.org/tutorials/3d/hunyuan3D-2"&gt;Hunyuan3D 2.0&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Asynchronous Queue system&lt;/li&gt; 
 &lt;li&gt;Many optimizations: Only re-executes the parts of the workflow that changes between executions.&lt;/li&gt; 
 &lt;li&gt;Smart memory management: can automatically run large models on GPUs with as low as 1GB vram with smart offloading.&lt;/li&gt; 
 &lt;li&gt;Works even if you don't have a GPU with: &lt;code&gt;--cpu&lt;/code&gt; (slow)&lt;/li&gt; 
 &lt;li&gt;Can load ckpt and safetensors: All in one checkpoints or standalone diffusion models, VAEs and CLIP models.&lt;/li&gt; 
 &lt;li&gt;Safe loading of ckpt, pt, pth, etc.. files.&lt;/li&gt; 
 &lt;li&gt;Embeddings/Textual inversion&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/lora/"&gt;Loras (regular, locon and loha)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/hypernetworks/"&gt;Hypernetworks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Loading full workflows (with seeds) from generated PNG, WebP and FLAC files.&lt;/li&gt; 
 &lt;li&gt;Saving/Loading workflows as Json files.&lt;/li&gt; 
 &lt;li&gt;Nodes interface can be used to create complex workflows like one for &lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/2_pass_txt2img/"&gt;Hires fix&lt;/a&gt; or much more advanced ones.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/area_composition/"&gt;Area Composition&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/inpaint/"&gt;Inpainting&lt;/a&gt; with both regular and inpainting models.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/controlnet/"&gt;ControlNet and T2I-Adapter&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/upscale_models/"&gt;Upscale Models (ESRGAN, ESRGAN variants, SwinIR, Swin2SR, etc...)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/gligen/"&gt;GLIGEN&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/model_merging/"&gt;Model Merging&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/lcm/"&gt;LCM models and Loras&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Latent previews with &lt;a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/#how-to-show-high-quality-previews"&gt;TAESD&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Works fully offline: core will never download anything unless you want to.&lt;/li&gt; 
 &lt;li&gt;Optional API nodes to use paid models from external providers through the online &lt;a href="https://docs.comfy.org/tutorials/api-nodes/overview"&gt;Comfy API&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/extra_model_paths.yaml.example"&gt;Config file&lt;/a&gt; to set the search paths for models.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Workflow examples can be found on the &lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/"&gt;Examples page&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Release Process&lt;/h2&gt; 
&lt;p&gt;ComfyUI follows a weekly release cycle targeting Friday but this regularly changes because of model releases or large changes to the codebase. There are three interconnected repositories:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/comfyanonymous/ComfyUI"&gt;ComfyUI Core&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Releases a new stable version (e.g., v0.7.0)&lt;/li&gt; 
   &lt;li&gt;Serves as the foundation for the desktop release&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/Comfy-Org/desktop"&gt;ComfyUI Desktop&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Builds a new release using the latest stable core version&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/Comfy-Org/ComfyUI_frontend"&gt;ComfyUI Frontend&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Weekly frontend updates are merged into the core repository&lt;/li&gt; 
   &lt;li&gt;Features are frozen for the upcoming core release&lt;/li&gt; 
   &lt;li&gt;Development continues for the next release cycle&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Shortcuts&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Keybind&lt;/th&gt; 
   &lt;th&gt;Explanation&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;Enter&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Queue up current graph for generation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;Shift&lt;/code&gt; + &lt;code&gt;Enter&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Queue up current graph as first for generation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;Alt&lt;/code&gt; + &lt;code&gt;Enter&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Cancel current generation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;Z&lt;/code&gt;/&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;Y&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Undo/Redo&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;S&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Save workflow&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;O&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Load workflow&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;A&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Select all nodes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Alt &lt;/code&gt;+ &lt;code&gt;C&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Collapse/uncollapse selected nodes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;M&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Mute/unmute selected nodes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;B&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Bypass selected nodes (acts like the node was removed from the graph and the wires reconnected through)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Delete&lt;/code&gt;/&lt;code&gt;Backspace&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Delete selected nodes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;Backspace&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Delete the current graph&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Space&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Move the canvas around when held and moving the cursor&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt;/&lt;code&gt;Shift&lt;/code&gt; + &lt;code&gt;Click&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Add clicked node to selection&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;C&lt;/code&gt;/&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;V&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Copy and paste selected nodes (without maintaining connections to outputs of unselected nodes)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;C&lt;/code&gt;/&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;Shift&lt;/code&gt; + &lt;code&gt;V&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Copy and paste selected nodes (maintaining connections from outputs of unselected nodes to inputs of pasted nodes)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Shift&lt;/code&gt; + &lt;code&gt;Drag&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Move multiple selected nodes at the same time&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;D&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Load default graph&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Alt&lt;/code&gt; + &lt;code&gt;+&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Canvas Zoom in&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Alt&lt;/code&gt; + &lt;code&gt;-&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Canvas Zoom out&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;Shift&lt;/code&gt; + LMB + Vertical drag&lt;/td&gt; 
   &lt;td&gt;Canvas Zoom in/out&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;P&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Pin/Unpin selected nodes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;G&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Group selected nodes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Q&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Toggle visibility of the queue&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;H&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Toggle visibility of history&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;R&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Refresh graph&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;F&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Show/Hide menu&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;.&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Fit view to selection (Whole graph when nothing is selected)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Double-Click LMB&lt;/td&gt; 
   &lt;td&gt;Open node quick search palette&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Shift&lt;/code&gt; + Drag&lt;/td&gt; 
   &lt;td&gt;Move multiple wires at once&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;Alt&lt;/code&gt; + LMB&lt;/td&gt; 
   &lt;td&gt;Disconnect all wires from clicked slot&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;code&gt;Ctrl&lt;/code&gt; can also be replaced with &lt;code&gt;Cmd&lt;/code&gt; instead for macOS users&lt;/p&gt; 
&lt;h1&gt;Installing&lt;/h1&gt; 
&lt;h2&gt;Windows Portable&lt;/h2&gt; 
&lt;p&gt;There is a portable standalone build for Windows that should work for running on Nvidia GPUs or for running on your CPU only on the &lt;a href="https://github.com/comfyanonymous/ComfyUI/releases"&gt;releases page&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;&lt;a href="https://github.com/comfyanonymous/ComfyUI/releases/latest/download/ComfyUI_windows_portable_nvidia.7z"&gt;Direct link to download&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Simply download, extract with &lt;a href="https://7-zip.org"&gt;7-Zip&lt;/a&gt; and run. Make sure you put your Stable Diffusion checkpoints/models (the huge ckpt/safetensors files) in: ComfyUI\models\checkpoints&lt;/p&gt; 
&lt;p&gt;If you have trouble extracting it, right click the file -&amp;gt; properties -&amp;gt; unblock&lt;/p&gt; 
&lt;h4&gt;How do I share models between another UI and ComfyUI?&lt;/h4&gt; 
&lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/extra_model_paths.yaml.example"&gt;Config file&lt;/a&gt; to set the search paths for models. In the standalone windows build you can find this file in the ComfyUI directory. Rename this file to extra_model_paths.yaml and edit it with your favorite text editor.&lt;/p&gt; 
&lt;h2&gt;&lt;a href="https://docs.comfy.org/comfy-cli/getting-started"&gt;comfy-cli&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;You can install and start ComfyUI using comfy-cli:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install comfy-cli
comfy install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Manual Install (Windows, Linux)&lt;/h2&gt; 
&lt;p&gt;Python 3.13 is very well supported. If you have trouble with some custom node dependencies you can try 3.12&lt;/p&gt; 
&lt;p&gt;Git clone this repo.&lt;/p&gt; 
&lt;p&gt;Put your SD checkpoints (the huge ckpt/safetensors files) in: models/checkpoints&lt;/p&gt; 
&lt;p&gt;Put your VAE in: models/vae&lt;/p&gt; 
&lt;h3&gt;AMD GPUs (Linux only)&lt;/h3&gt; 
&lt;p&gt;AMD users can install rocm and pytorch with pip if you don't have it already installed, this is the command to install the stable version:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.4&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;This is the command to install the nightly with ROCm 6.4 which might have some performance improvements:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm6.4&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Intel GPUs (Windows and Linux)&lt;/h3&gt; 
&lt;p&gt;(Option 1) Intel Arc GPU users can install native PyTorch with torch.xpu support using pip. More information can be found &lt;a href="https://pytorch.org/docs/main/notes/get_start_xpu.html"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;To install PyTorch xpu, use the following command:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;code&gt;pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/xpu&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;This is the command to install the Pytorch xpu nightly which might have some performance improvements:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/xpu&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;(Option 2) Alternatively, Intel GPUs supported by Intel Extension for PyTorch (IPEX) can leverage IPEX for improved performance.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;visit &lt;a href="https://intel.github.io/intel-extension-for-pytorch/index.html#installation?platform=gpu"&gt;Installation&lt;/a&gt; for more information.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;NVIDIA&lt;/h3&gt; 
&lt;p&gt;Nvidia users should install stable pytorch using this command:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu129&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;This is the command to install pytorch nightly instead which might have performance improvements.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu129&lt;/code&gt;&lt;/p&gt; 
&lt;h4&gt;Troubleshooting&lt;/h4&gt; 
&lt;p&gt;If you get the "Torch not compiled with CUDA enabled" error, uninstall torch with:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip uninstall torch&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;And install it again with the command above.&lt;/p&gt; 
&lt;h3&gt;Dependencies&lt;/h3&gt; 
&lt;p&gt;Install the dependencies by opening your terminal inside the ComfyUI folder and:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;After this you should have everything installed and can proceed to running ComfyUI.&lt;/p&gt; 
&lt;h3&gt;Others:&lt;/h3&gt; 
&lt;h4&gt;Apple Mac silicon&lt;/h4&gt; 
&lt;p&gt;You can install ComfyUI in Apple Mac silicon (M1 or M2) with any recent macOS version.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install pytorch nightly. For instructions, read the &lt;a href="https://developer.apple.com/metal/pytorch/"&gt;Accelerated PyTorch training on Mac&lt;/a&gt; Apple Developer guide (make sure to install the latest pytorch nightly).&lt;/li&gt; 
 &lt;li&gt;Follow the &lt;a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/#manual-install-windows-linux"&gt;ComfyUI manual installation&lt;/a&gt; instructions for Windows and Linux.&lt;/li&gt; 
 &lt;li&gt;Install the ComfyUI &lt;a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/#dependencies"&gt;dependencies&lt;/a&gt;. If you have another Stable Diffusion UI &lt;a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/#i-already-have-another-ui-for-stable-diffusion-installed-do-i-really-have-to-install-all-of-these-dependencies"&gt;you might be able to reuse the dependencies&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Launch ComfyUI by running &lt;code&gt;python main.py&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Remember to add your models, VAE, LoRAs etc. to the corresponding Comfy folders, as discussed in &lt;a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/#manual-install-windows-linux"&gt;ComfyUI manual installation&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;DirectML (AMD Cards on Windows)&lt;/h4&gt; 
&lt;p&gt;This is very badly supported and is not recommended. There are some unofficial builds of pytorch ROCm on windows that exist that will give you a much better experience than this. This readme will be updated once official pytorch ROCm builds for windows come out.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip install torch-directml&lt;/code&gt; Then you can launch ComfyUI with: &lt;code&gt;python main.py --directml&lt;/code&gt;&lt;/p&gt; 
&lt;h4&gt;Ascend NPUs&lt;/h4&gt; 
&lt;p&gt;For models compatible with Ascend Extension for PyTorch (torch_npu). To get started, ensure your environment meets the prerequisites outlined on the &lt;a href="https://ascend.github.io/docs/sources/ascend/quick_install.html"&gt;installation&lt;/a&gt; page. Here's a step-by-step guide tailored to your platform and installation method:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Begin by installing the recommended or newer kernel version for Linux as specified in the Installation page of torch-npu, if necessary.&lt;/li&gt; 
 &lt;li&gt;Proceed with the installation of Ascend Basekit, which includes the driver, firmware, and CANN, following the instructions provided for your specific platform.&lt;/li&gt; 
 &lt;li&gt;Next, install the necessary packages for torch-npu by adhering to the platform-specific instructions on the &lt;a href="https://ascend.github.io/docs/sources/pytorch/install.html#pytorch"&gt;Installation&lt;/a&gt; page.&lt;/li&gt; 
 &lt;li&gt;Finally, adhere to the &lt;a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/#manual-install-windows-linux"&gt;ComfyUI manual installation&lt;/a&gt; guide for Linux. Once all components are installed, you can run ComfyUI as described earlier.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Cambricon MLUs&lt;/h4&gt; 
&lt;p&gt;For models compatible with Cambricon Extension for PyTorch (torch_mlu). Here's a step-by-step guide tailored to your platform and installation method:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install the Cambricon CNToolkit by adhering to the platform-specific instructions on the &lt;a href="https://www.cambricon.com/docs/sdk_1.15.0/cntoolkit_3.7.2/cntoolkit_install_3.7.2/index.html"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Next, install the PyTorch(torch_mlu) following the instructions on the &lt;a href="https://www.cambricon.com/docs/sdk_1.15.0/cambricon_pytorch_1.17.0/user_guide_1.9/index.html"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Launch ComfyUI by running &lt;code&gt;python main.py&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Iluvatar Corex&lt;/h4&gt; 
&lt;p&gt;For models compatible with Iluvatar Extension for PyTorch. Here's a step-by-step guide tailored to your platform and installation method:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install the Iluvatar Corex Toolkit by adhering to the platform-specific instructions on the &lt;a href="https://support.iluvatar.com/#/DocumentCentre?id=1&amp;amp;nameCenter=2&amp;amp;productId=520117912052801536"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Launch ComfyUI by running &lt;code&gt;python main.py&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h1&gt;Running&lt;/h1&gt; 
&lt;p&gt;&lt;code&gt;python main.py&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;For AMD cards not officially supported by ROCm&lt;/h3&gt; 
&lt;p&gt;Try running it with this command if you have issues:&lt;/p&gt; 
&lt;p&gt;For 6700, 6600 and maybe other RDNA2 or older: &lt;code&gt;HSA_OVERRIDE_GFX_VERSION=10.3.0 python main.py&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;For AMD 7600 and maybe other RDNA3 cards: &lt;code&gt;HSA_OVERRIDE_GFX_VERSION=11.0.0 python main.py&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;AMD ROCm Tips&lt;/h3&gt; 
&lt;p&gt;You can enable experimental memory efficient attention on recent pytorch in ComfyUI on some AMD GPUs using this command, it should already be enabled by default on RDNA3. If this improves speed for you on latest pytorch on your GPU please report it so that I can enable it by default.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL=1 python main.py --use-pytorch-cross-attention&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;You can also try setting this env variable &lt;code&gt;PYTORCH_TUNABLEOP_ENABLED=1&lt;/code&gt; which might speed things up at the cost of a very slow initial run.&lt;/p&gt; 
&lt;h1&gt;Notes&lt;/h1&gt; 
&lt;p&gt;Only parts of the graph that have an output with all the correct inputs will be executed.&lt;/p&gt; 
&lt;p&gt;Only parts of the graph that change from each execution to the next will be executed, if you submit the same graph twice only the first will be executed. If you change the last part of the graph only the part you changed and the part that depends on it will be executed.&lt;/p&gt; 
&lt;p&gt;Dragging a generated png on the webpage or loading one will give you the full workflow including seeds that were used to create it.&lt;/p&gt; 
&lt;p&gt;You can use () to change emphasis of a word or phrase like: (good code:1.2) or (bad code:0.8). The default emphasis for () is 1.1. To use () characters in your actual prompt escape them like \( or \).&lt;/p&gt; 
&lt;p&gt;You can use {day|night}, for wildcard/dynamic prompts. With this syntax "{wild|card|test}" will be randomly replaced by either "wild", "card" or "test" by the frontend every time you queue the prompt. To use {} characters in your actual prompt escape them like: \{ or \}.&lt;/p&gt; 
&lt;p&gt;Dynamic prompts also support C-style comments, like &lt;code&gt;// comment&lt;/code&gt; or &lt;code&gt;/* comment */&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;To use a textual inversion concepts/embeddings in a text prompt put them in the models/embeddings directory and use them in the CLIPTextEncode node like this (you can omit the .pt extension):&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;embedding:embedding_filename.pt&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;How to show high-quality previews?&lt;/h2&gt; 
&lt;p&gt;Use &lt;code&gt;--preview-method auto&lt;/code&gt; to enable previews.&lt;/p&gt; 
&lt;p&gt;The default installation includes a fast latent preview method that's low-resolution. To enable higher-quality previews with &lt;a href="https://github.com/madebyollin/taesd"&gt;TAESD&lt;/a&gt;, download the &lt;a href="https://github.com/madebyollin/taesd/"&gt;taesd_decoder.pth, taesdxl_decoder.pth, taesd3_decoder.pth and taef1_decoder.pth&lt;/a&gt; and place them in the &lt;code&gt;models/vae_approx&lt;/code&gt; folder. Once they're installed, restart ComfyUI and launch it with &lt;code&gt;--preview-method taesd&lt;/code&gt; to enable high-quality previews.&lt;/p&gt; 
&lt;h2&gt;How to use TLS/SSL?&lt;/h2&gt; 
&lt;p&gt;Generate a self-signed certificate (not appropriate for shared/production use) and key by running the command: &lt;code&gt;openssl req -x509 -newkey rsa:4096 -keyout key.pem -out cert.pem -sha256 -days 3650 -nodes -subj "/C=XX/ST=StateName/L=CityName/O=CompanyName/OU=CompanySectionName/CN=CommonNameOrHostname"&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Use &lt;code&gt;--tls-keyfile key.pem --tls-certfile cert.pem&lt;/code&gt; to enable TLS/SSL, the app will now be accessible with &lt;code&gt;https://...&lt;/code&gt; instead of &lt;code&gt;http://...&lt;/code&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Note: Windows users can use &lt;a href="https://github.com/alexisrolland/docker-openssl"&gt;alexisrolland/docker-openssl&lt;/a&gt; or one of the &lt;a href="https://wiki.openssl.org/index.php/Binaries"&gt;3rd party binary distributions&lt;/a&gt; to run the command example above. &lt;br /&gt;&lt;br /&gt;If you use a container, note that the volume mount &lt;code&gt;-v&lt;/code&gt; can be a relative path so &lt;code&gt;... -v ".\:/openssl-certs" ...&lt;/code&gt; would create the key &amp;amp; cert files in the current directory of your command prompt or powershell terminal.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Support and dev channel&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://comfy.org/discord"&gt;Discord&lt;/a&gt;: Try the #help or #feedback channels.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://app.element.io/#/room/%23comfyui_space%3Amatrix.org"&gt;Matrix space: #comfyui_space:matrix.org&lt;/a&gt; (it's like discord but open source).&lt;/p&gt; 
&lt;p&gt;See also: &lt;a href="https://www.comfy.org/"&gt;https://www.comfy.org/&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Frontend Development&lt;/h2&gt; 
&lt;p&gt;As of August 15, 2024, we have transitioned to a new frontend, which is now hosted in a separate repository: &lt;a href="https://github.com/Comfy-Org/ComfyUI_frontend"&gt;ComfyUI Frontend&lt;/a&gt;. This repository now hosts the compiled JS (from TS/Vue) under the &lt;code&gt;web/&lt;/code&gt; directory.&lt;/p&gt; 
&lt;h3&gt;Reporting Issues and Requesting Features&lt;/h3&gt; 
&lt;p&gt;For any bugs, issues, or feature requests related to the frontend, please use the &lt;a href="https://github.com/Comfy-Org/ComfyUI_frontend"&gt;ComfyUI Frontend repository&lt;/a&gt;. This will help us manage and address frontend-specific concerns more efficiently.&lt;/p&gt; 
&lt;h3&gt;Using the Latest Frontend&lt;/h3&gt; 
&lt;p&gt;The new frontend is now the default for ComfyUI. However, please note:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;The frontend in the main ComfyUI repository is updated fortnightly.&lt;/li&gt; 
 &lt;li&gt;Daily releases are available in the separate frontend repository.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;To use the most up-to-date frontend version:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;For the latest daily release, launch ComfyUI with this command line argument:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;--front-end-version Comfy-Org/ComfyUI_frontend@latest
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;For a specific version, replace &lt;code&gt;latest&lt;/code&gt; with the desired version number:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;--front-end-version Comfy-Org/ComfyUI_frontend@1.2.2
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;This approach allows you to easily switch between the stable fortnightly release and the cutting-edge daily updates, or even specific versions for testing purposes.&lt;/p&gt; 
&lt;h3&gt;Accessing the Legacy Frontend&lt;/h3&gt; 
&lt;p&gt;If you need to use the legacy frontend for any reason, you can access it using the following command line argument:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;--front-end-version Comfy-Org/ComfyUI_legacy_frontend@latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will use a snapshot of the legacy frontend preserved in the &lt;a href="https://github.com/Comfy-Org/ComfyUI_legacy_frontend"&gt;ComfyUI Legacy Frontend repository&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;QA&lt;/h1&gt; 
&lt;h3&gt;Which GPU should I buy for this?&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/comfyanonymous/ComfyUI/wiki/Which-GPU-should-I-buy-for-ComfyUI"&gt;See this page for some recommendations&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>HKUDS/RAG-Anything</title>
      <link>https://github.com/HKUDS/RAG-Anything</link>
      <description>&lt;p&gt;"RAG-Anything: All-in-One RAG System"&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;div style="margin: 20px 0;"&gt; 
  &lt;img src="https://raw.githubusercontent.com/HKUDS/RAG-Anything/main/assets/logo.png" width="120" height="120" alt="RAG-Anything Logo" style="border-radius: 20px; box-shadow: 0 8px 32px rgba(0, 217, 255, 0.3);" /&gt; 
 &lt;/div&gt; 
 &lt;h1&gt;ğŸš€ RAG-Anything: All-in-One RAG System&lt;/h1&gt; 
 &lt;div align="center"&gt; 
  &lt;img src="https://readme-typing-svg.herokuapp.com?font=Orbitron&amp;amp;size=24&amp;amp;duration=3000&amp;amp;pause=1000&amp;amp;color=00D9FF&amp;amp;center=true&amp;amp;vCenter=true&amp;amp;width=600&amp;amp;lines=Welcome+to+RAG-Anything;Next-Gen+Multimodal+RAG+System;Powered+by+Advanced+AI+Technology" alt="Typing Animation" /&gt; 
 &lt;/div&gt; 
 &lt;div align="center"&gt; 
  &lt;div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; padding: 25px; text-align: center;"&gt; 
   &lt;p&gt; &lt;a href="https://github.com/HKUDS/RAG-Anything"&gt;&lt;img src="https://img.shields.io/badge/ğŸ”¥Project-Page-00d9ff?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2410.05779"&gt;&lt;img src="https://img.shields.io/badge/ğŸ“„arXiv-2410.05779-ff6b6b?style=for-the-badge&amp;amp;logo=arxiv&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;a href="https://github.com/HKUDS/LightRAG"&gt;&lt;img src="https://img.shields.io/badge/âš¡Based%20on-LightRAG-4ecdc4?style=for-the-badge&amp;amp;logo=lightning&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;/p&gt; 
   &lt;p&gt; &lt;a href="https://github.com/HKUDS/RAG-Anything/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/HKUDS/RAG-Anything?color=00d9ff&amp;amp;style=for-the-badge&amp;amp;logo=star&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/badge/ğŸPython-3.10-4ecdc4?style=for-the-badge&amp;amp;logo=python&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt; &lt;a href="https://pypi.org/project/raganything/"&gt;&lt;img src="https://img.shields.io/pypi/v/raganything.svg?style=for-the-badge&amp;amp;logo=pypi&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e&amp;amp;color=ff6b6b" /&gt;&lt;/a&gt; &lt;/p&gt; 
   &lt;p&gt; &lt;a href="https://discord.gg/yF2MmDJyGJ"&gt;&lt;img src="https://img.shields.io/badge/ğŸ’¬Discord-Community-7289da?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;a href="https://github.com/HKUDS/RAG-Anything/issues/7"&gt;&lt;img src="https://img.shields.io/badge/ğŸ’¬WeChat-Group-07c160?style=for-the-badge&amp;amp;logo=wechat&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;/p&gt; 
   &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/RAG-Anything/main/README_zh.md"&gt;&lt;img src="https://img.shields.io/badge/ğŸ‡¨ğŸ‡³ä¸­æ–‡ç‰ˆ-1a1a2e?style=for-the-badge" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/RAG-Anything/main/README.md"&gt;&lt;img src="https://img.shields.io/badge/ğŸ‡ºğŸ‡¸English-1a1a2e?style=for-the-badge" /&gt;&lt;/a&gt; &lt;/p&gt; 
  &lt;/div&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;div style="width: 100%; height: 2px; margin: 20px 0; background: linear-gradient(90deg, transparent, #00d9ff, transparent);"&gt;&lt;/div&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://raw.githubusercontent.com/HKUDS/RAG-Anything/main/#-quick-start" style="text-decoration: none;"&gt; &lt;img src="https://img.shields.io/badge/Quick%20Start-Get%20Started%20Now-00d9ff?style=for-the-badge&amp;amp;logo=rocket&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ‰ News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; [2025.08.12]ğŸ¯ğŸ“¢ ğŸ” RAG-Anything now features &lt;strong&gt;VLM-Enhanced Query&lt;/strong&gt; mode! When documents include images, the system seamlessly integrates them into VLM for advanced multimodal analysis, combining visual and textual context for deeper insights.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; [2025.07.05]ğŸ¯ğŸ“¢ RAG-Anything now features a &lt;a href="https://raw.githubusercontent.com/HKUDS/RAG-Anything/main/docs/context_aware_processing.md"&gt;context configuration module&lt;/a&gt;, enabling intelligent integration of relevant contextual information to enhance multimodal content processing.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; [2025.07.04]ğŸ¯ğŸ“¢ ğŸš€ RAG-Anything now supports multimodal query capabilities, enabling enhanced RAG with seamless processing of text, images, tables, and equations.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; [2025.07.03]ğŸ¯ğŸ“¢ ğŸ‰ RAG-Anything has reached 1kğŸŒŸ stars on GitHub! Thank you for your incredible support and valuable contributions to the project.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸŒŸ System Overview&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;Next-Generation Multimodal Intelligence&lt;/em&gt;&lt;/p&gt; 
&lt;div style="background: linear-gradient(135deg, #1a1a2e 0%, #16213e 50%, #0f3460 100%); border-radius: 15px; padding: 25px; margin: 20px 0; border: 2px solid #00d9ff; box-shadow: 0 0 30px rgba(0, 217, 255, 0.3);"&gt; 
 &lt;p&gt;Modern documents increasingly contain diverse multimodal contentâ€”text, images, tables, equations, charts, and multimediaâ€”that traditional text-focused RAG systems cannot effectively process. &lt;strong&gt;RAG-Anything&lt;/strong&gt; addresses this challenge as a comprehensive &lt;strong&gt;All-in-One Multimodal Document Processing RAG system&lt;/strong&gt; built on &lt;a href="https://github.com/HKUDS/LightRAG"&gt;LightRAG&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;As a unified solution, RAG-Anything &lt;strong&gt;eliminates the need for multiple specialized tools&lt;/strong&gt;. It provides &lt;strong&gt;seamless processing and querying across all content modalities&lt;/strong&gt; within a single integrated framework. Unlike conventional RAG approaches that struggle with non-textual elements, our all-in-one system delivers &lt;strong&gt;comprehensive multimodal retrieval capabilities&lt;/strong&gt;.&lt;/p&gt; 
 &lt;p&gt;Users can query documents containing &lt;strong&gt;interleaved text&lt;/strong&gt;, &lt;strong&gt;visual diagrams&lt;/strong&gt;, &lt;strong&gt;structured tables&lt;/strong&gt;, and &lt;strong&gt;mathematical formulations&lt;/strong&gt; through &lt;strong&gt;one cohesive interface&lt;/strong&gt;. This consolidated approach makes RAG-Anything particularly valuable for academic research, technical documentation, financial reports, and enterprise knowledge management where rich, mixed-content documents demand a &lt;strong&gt;unified processing framework&lt;/strong&gt;.&lt;/p&gt; 
 &lt;img src="https://raw.githubusercontent.com/HKUDS/RAG-Anything/main/assets/rag_anything_framework.png" alt="RAG-Anything" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;ğŸ¯ Key Features&lt;/h3&gt; 
&lt;div style="background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%); border-radius: 15px; padding: 25px; margin: 20px 0;"&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;ğŸ”„ End-to-End Multimodal Pipeline&lt;/strong&gt; - Complete workflow from document ingestion and parsing to intelligent multimodal query answering&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;ğŸ“„ Universal Document Support&lt;/strong&gt; - Seamless processing of PDFs, Office documents, images, and diverse file formats&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;ğŸ§  Specialized Content Analysis&lt;/strong&gt; - Dedicated processors for images, tables, mathematical equations, and heterogeneous content types&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;ğŸ”— Multimodal Knowledge Graph&lt;/strong&gt; - Automatic entity extraction and cross-modal relationship discovery for enhanced understanding&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;âš¡ Adaptive Processing Modes&lt;/strong&gt; - Flexible MinerU-based parsing or direct multimodal content injection workflows&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;ğŸ“‹ Direct Content List Insertion&lt;/strong&gt; - Bypass document parsing by directly inserting pre-parsed content lists from external sources&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;ğŸ¯ Hybrid Intelligent Retrieval&lt;/strong&gt; - Advanced search capabilities spanning textual and multimodal content with contextual understanding&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ—ï¸ Algorithm &amp;amp; Architecture&lt;/h2&gt; 
&lt;div style="background: linear-gradient(135deg, #0f0f23 0%, #1a1a2e 100%); border-radius: 15px; padding: 25px; margin: 20px 0; border-left: 5px solid #00d9ff;"&gt; 
 &lt;h3&gt;Core Algorithm&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;RAG-Anything&lt;/strong&gt; implements an effective &lt;strong&gt;multi-stage multimodal pipeline&lt;/strong&gt; that fundamentally extends traditional RAG architectures to seamlessly handle diverse content modalities through intelligent orchestration and cross-modal understanding.&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;div style="width: 100%; max-width: 600px; margin: 20px auto; padding: 20px; background: linear-gradient(135deg, rgba(0, 217, 255, 0.1) 0%, rgba(0, 217, 255, 0.05) 100%); border-radius: 15px; border: 1px solid rgba(0, 217, 255, 0.2);"&gt; 
  &lt;div style="display: flex; justify-content: space-around; align-items: center; flex-wrap: wrap; gap: 20px;"&gt; 
   &lt;div style="text-align: center;"&gt; 
    &lt;div style="font-size: 24px; margin-bottom: 10px;"&gt;
     ğŸ“„
    &lt;/div&gt; 
    &lt;div style="font-size: 14px; color: #00d9ff;"&gt;
     Document Parsing
    &lt;/div&gt; 
   &lt;/div&gt; 
   &lt;div style="font-size: 20px; color: #00d9ff;"&gt;
    â†’
   &lt;/div&gt; 
   &lt;div style="text-align: center;"&gt; 
    &lt;div style="font-size: 24px; margin-bottom: 10px;"&gt;
     ğŸ§ 
    &lt;/div&gt; 
    &lt;div style="font-size: 14px; color: #00d9ff;"&gt;
     Content Analysis
    &lt;/div&gt; 
   &lt;/div&gt; 
   &lt;div style="font-size: 20px; color: #00d9ff;"&gt;
    â†’
   &lt;/div&gt; 
   &lt;div style="text-align: center;"&gt; 
    &lt;div style="font-size: 24px; margin-bottom: 10px;"&gt;
     ğŸ”
    &lt;/div&gt; 
    &lt;div style="font-size: 14px; color: #00d9ff;"&gt;
     Knowledge Graph
    &lt;/div&gt; 
   &lt;/div&gt; 
   &lt;div style="font-size: 20px; color: #00d9ff;"&gt;
    â†’
   &lt;/div&gt; 
   &lt;div style="text-align: center;"&gt; 
    &lt;div style="font-size: 24px; margin-bottom: 10px;"&gt;
     ğŸ¯
    &lt;/div&gt; 
    &lt;div style="font-size: 14px; color: #00d9ff;"&gt;
     Intelligent Retrieval
    &lt;/div&gt; 
   &lt;/div&gt; 
  &lt;/div&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;h3&gt;1. Document Parsing Stage&lt;/h3&gt; 
&lt;div style="background: linear-gradient(90deg, #1a1a2e 0%, #16213e 100%); border-radius: 10px; padding: 20px; margin: 15px 0; border-left: 4px solid #4ecdc4;"&gt; 
 &lt;p&gt;The system provides high-fidelity document extraction through adaptive content decomposition. It intelligently segments heterogeneous elements while preserving contextual relationships. Universal format compatibility is achieved via specialized optimized parsers.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Key Components:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;âš™ï¸ MinerU Integration&lt;/strong&gt;: Leverages &lt;a href="https://github.com/opendatalab/MinerU"&gt;MinerU&lt;/a&gt; for high-fidelity document structure extraction and semantic preservation across complex layouts.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ§© Adaptive Content Decomposition&lt;/strong&gt;: Automatically segments documents into coherent text blocks, visual elements, structured tables, mathematical equations, and specialized content types while preserving contextual relationships.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ“ Universal Format Support&lt;/strong&gt;: Provides comprehensive handling of PDFs, Office documents (DOC/DOCX/PPT/PPTX/XLS/XLSX), images, and emerging formats through specialized parsers with format-specific optimization.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/div&gt; 
&lt;h3&gt;2. Multi-Modal Content Understanding &amp;amp; Processing&lt;/h3&gt; 
&lt;div style="background: linear-gradient(90deg, #16213e 0%, #0f3460 100%); border-radius: 10px; padding: 20px; margin: 15px 0; border-left: 4px solid #ff6b6b;"&gt; 
 &lt;p&gt;The system automatically categorizes and routes content through optimized channels. It uses concurrent pipelines for parallel text and multimodal processing. Document hierarchy and relationships are preserved during transformation.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Key Components:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ¯ Autonomous Content Categorization and Routing&lt;/strong&gt;: Automatically identify, categorize, and route different content types through optimized execution channels.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;âš¡ Concurrent Multi-Pipeline Architecture&lt;/strong&gt;: Implements concurrent execution of textual and multimodal content through dedicated processing pipelines. This approach maximizes throughput efficiency while preserving content integrity.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ—ï¸ Document Hierarchy Extraction&lt;/strong&gt;: Extracts and preserves original document hierarchy and inter-element relationships during content transformation.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/div&gt; 
&lt;h3&gt;3. Multimodal Analysis Engine&lt;/h3&gt; 
&lt;div style="background: linear-gradient(90deg, #0f3460 0%, #1a1a2e 100%); border-radius: 10px; padding: 20px; margin: 15px 0; border-left: 4px solid #00d9ff;"&gt; 
 &lt;p&gt;The system deploys modality-aware processing units for heterogeneous data modalities:&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Specialized Analyzers:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ” Visual Content Analyzer&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Integrate vision model for image analysis.&lt;/li&gt; 
    &lt;li&gt;Generates context-aware descriptive captions based on visual semantics.&lt;/li&gt; 
    &lt;li&gt;Extracts spatial relationships and hierarchical structures between visual elements.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ“Š Structured Data Interpreter&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Performs systematic interpretation of tabular and structured data formats.&lt;/li&gt; 
    &lt;li&gt;Implements statistical pattern recognition algorithms for data trend analysis.&lt;/li&gt; 
    &lt;li&gt;Identifies semantic relationships and dependencies across multiple tabular datasets.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ“ Mathematical Expression Parser&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Parses complex mathematical expressions and formulas with high accuracy.&lt;/li&gt; 
    &lt;li&gt;Provides native LaTeX format support for seamless integration with academic workflows.&lt;/li&gt; 
    &lt;li&gt;Establishes conceptual mappings between mathematical equations and domain-specific knowledge bases.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ”§ Extensible Modality Handler&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Provides configurable processing framework for custom and emerging content types.&lt;/li&gt; 
    &lt;li&gt;Enables dynamic integration of new modality processors through plugin architecture.&lt;/li&gt; 
    &lt;li&gt;Supports runtime configuration of processing pipelines for specialized use cases.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/div&gt; 
&lt;h3&gt;4. Multimodal Knowledge Graph Index&lt;/h3&gt; 
&lt;div style="background: linear-gradient(90deg, #1a1a2e 0%, #16213e 100%); border-radius: 10px; padding: 20px; margin: 15px 0; border-left: 4px solid #4ecdc4;"&gt; 
 &lt;p&gt;The multi-modal knowledge graph construction module transforms document content into structured semantic representations. It extracts multimodal entities, establishes cross-modal relationships, and preserves hierarchical organization. The system applies weighted relevance scoring for optimized knowledge retrieval.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Core Functions:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ” Multi-Modal Entity Extraction&lt;/strong&gt;: Transforms significant multimodal elements into structured knowledge graph entities. The process includes semantic annotations and metadata preservation.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ”— Cross-Modal Relationship Mapping&lt;/strong&gt;: Establishes semantic connections and dependencies between textual entities and multimodal components. This is achieved through automated relationship inference algorithms.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ—ï¸ Hierarchical Structure Preservation&lt;/strong&gt;: Maintains original document organization through "belongs_to" relationship chains. These chains preserve logical content hierarchy and sectional dependencies.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;âš–ï¸ Weighted Relationship Scoring&lt;/strong&gt;: Assigns quantitative relevance scores to relationship types. Scoring is based on semantic proximity and contextual significance within the document structure.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/div&gt; 
&lt;h3&gt;5. Modality-Aware Retrieval&lt;/h3&gt; 
&lt;div style="background: linear-gradient(90deg, #16213e 0%, #0f3460 100%); border-radius: 10px; padding: 20px; margin: 15px 0; border-left: 4px solid #ff6b6b;"&gt; 
 &lt;p&gt;The hybrid retrieval system combines vector similarity search with graph traversal algorithms for comprehensive content retrieval. It implements modality-aware ranking mechanisms and maintains relational coherence between retrieved elements to ensure contextually integrated information delivery.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Retrieval Mechanisms:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ”€ Vector-Graph Fusion&lt;/strong&gt;: Integrates vector similarity search with graph traversal algorithms. This approach leverages both semantic embeddings and structural relationships for comprehensive content retrieval.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ“Š Modality-Aware Ranking&lt;/strong&gt;: Implements adaptive scoring mechanisms that weight retrieval results based on content type relevance. The system adjusts rankings according to query-specific modality preferences.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ”— Relational Coherence Maintenance&lt;/strong&gt;: Maintains semantic and structural relationships between retrieved elements. This ensures coherent information delivery and contextual integrity.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸš€ Quick Start&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;Initialize Your AI Journey&lt;/em&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://user-images.githubusercontent.com/74038190/212284158-e840e285-664b-44d7-b79b-e264b5e54825.gif" width="400" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;h4&gt;Option 1: Install from PyPI (Recommended)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Basic installation
pip install raganything

# With optional dependencies for extended format support:
pip install 'raganything[all]'              # All optional features
pip install 'raganything[image]'            # Image format conversion (BMP, TIFF, GIF, WebP)
pip install 'raganything[text]'             # Text file processing (TXT, MD)
pip install 'raganything[image,text]'       # Multiple features
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Option 2: Install from Source&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/HKUDS/RAG-Anything.git
cd RAG-Anything
pip install -e .

# With optional dependencies
pip install -e '.[all]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Optional Dependencies&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;[image]&lt;/code&gt;&lt;/strong&gt; - Enables processing of BMP, TIFF, GIF, WebP image formats (requires Pillow)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;[text]&lt;/code&gt;&lt;/strong&gt; - Enables processing of TXT and MD files (requires ReportLab)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;[all]&lt;/code&gt;&lt;/strong&gt; - Includes all Python optional dependencies&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;âš ï¸ Office Document Processing Requirements:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Office documents (.doc, .docx, .ppt, .pptx, .xls, .xlsx) require &lt;strong&gt;LibreOffice&lt;/strong&gt; installation&lt;/li&gt; 
  &lt;li&gt;Download from &lt;a href="https://www.libreoffice.org/download/download/"&gt;LibreOffice official website&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt;: Download installer from official website&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;macOS&lt;/strong&gt;: &lt;code&gt;brew install --cask libreoffice&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Ubuntu/Debian&lt;/strong&gt;: &lt;code&gt;sudo apt-get install libreoffice&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;CentOS/RHEL&lt;/strong&gt;: &lt;code&gt;sudo yum install libreoffice&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Check MinerU installation:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Verify installation
mineru --version

# Check if properly configured
python -c "from raganything import RAGAnything; rag = RAGAnything(); print('âœ… MinerU installed properly' if rag.check_mineru_installation() else 'âŒ MinerU installation issue')"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Models are downloaded automatically on first use. For manual download, refer to &lt;a href="https://github.com/opendatalab/MinerU/raw/master/README.md#22-model-source-configuration"&gt;MinerU Model Source Configuration&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Usage Examples&lt;/h3&gt; 
&lt;h4&gt;1. End-to-End Document Processing&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import asyncio
from raganything import RAGAnything, RAGAnythingConfig
from lightrag.llm.openai import openai_complete_if_cache, openai_embed
from lightrag.utils import EmbeddingFunc

async def main():
    # Set up API configuration
    api_key = "your-api-key"
    base_url = "your-base-url"  # Optional

    # Create RAGAnything configuration
    config = RAGAnythingConfig(
        working_dir="./rag_storage",
        parser="mineru",  # Parser selection: mineru or docling
        parse_method="auto",  # Parse method: auto, ocr, or txt
        enable_image_processing=True,
        enable_table_processing=True,
        enable_equation_processing=True,
    )

    # Define LLM model function
    def llm_model_func(prompt, system_prompt=None, history_messages=[], **kwargs):
        return openai_complete_if_cache(
            "gpt-4o-mini",
            prompt,
            system_prompt=system_prompt,
            history_messages=history_messages,
            api_key=api_key,
            base_url=base_url,
            **kwargs,
        )

    # Define vision model function for image processing
    def vision_model_func(
        prompt, system_prompt=None, history_messages=[], image_data=None, messages=None, **kwargs
    ):
        # If messages format is provided (for multimodal VLM enhanced query), use it directly
        if messages:
            return openai_complete_if_cache(
                "gpt-4o",
                "",
                system_prompt=None,
                history_messages=[],
                messages=messages,
                api_key=api_key,
                base_url=base_url,
                **kwargs,
            )
        # Traditional single image format
        elif image_data:
            return openai_complete_if_cache(
                "gpt-4o",
                "",
                system_prompt=None,
                history_messages=[],
                messages=[
                    {"role": "system", "content": system_prompt}
                    if system_prompt
                    else None,
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{image_data}"
                                },
                            },
                        ],
                    }
                    if image_data
                    else {"role": "user", "content": prompt},
                ],
                api_key=api_key,
                base_url=base_url,
                **kwargs,
            )
        # Pure text format
        else:
            return llm_model_func(prompt, system_prompt, history_messages, **kwargs)

    # Define embedding function
    embedding_func = EmbeddingFunc(
        embedding_dim=3072,
        max_token_size=8192,
        func=lambda texts: openai_embed(
            texts,
            model="text-embedding-3-large",
            api_key=api_key,
            base_url=base_url,
        ),
    )

    # Initialize RAGAnything
    rag = RAGAnything(
        config=config,
        llm_model_func=llm_model_func,
        vision_model_func=vision_model_func,
        embedding_func=embedding_func,
    )

    # Process a document
    await rag.process_document_complete(
        file_path="path/to/your/document.pdf",
        output_dir="./output",
        parse_method="auto"
    )

    # Query the processed content
    # Pure text query - for basic knowledge base search
    text_result = await rag.aquery(
        "What are the main findings shown in the figures and tables?",
        mode="hybrid"
    )
    print("Text query result:", text_result)

    # Multimodal query with specific multimodal content
    multimodal_result = await rag.aquery_with_multimodal(
    "Explain this formula and its relevance to the document content",
    multimodal_content=[{
        "type": "equation",
        "latex": "P(d|q) = \\frac{P(q|d) \\cdot P(d)}{P(q)}",
        "equation_caption": "Document relevance probability"
    }],
    mode="hybrid"
)
    print("Multimodal query result:", multimodal_result)

if __name__ == "__main__":
    asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;2. Direct Multimodal Content Processing&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import asyncio
from lightrag import LightRAG
from lightrag.llm.openai import openai_complete_if_cache, openai_embed
from lightrag.utils import EmbeddingFunc
from raganything.modalprocessors import ImageModalProcessor, TableModalProcessor

async def process_multimodal_content():
    # Set up API configuration
    api_key = "your-api-key"
    base_url = "your-base-url"  # Optional

    # Initialize LightRAG
    rag = LightRAG(
        working_dir="./rag_storage",
        llm_model_func=lambda prompt, system_prompt=None, history_messages=[], **kwargs: openai_complete_if_cache(
            "gpt-4o-mini",
            prompt,
            system_prompt=system_prompt,
            history_messages=history_messages,
            api_key=api_key,
            base_url=base_url,
            **kwargs,
        ),
        embedding_func=EmbeddingFunc(
            embedding_dim=3072,
            max_token_size=8192,
            func=lambda texts: openai_embed(
                texts,
                model="text-embedding-3-large",
                api_key=api_key,
                base_url=base_url,
            ),
        )
    )
    await rag.initialize_storages()

    # Process an image
    image_processor = ImageModalProcessor(
        lightrag=rag,
        modal_caption_func=lambda prompt, system_prompt=None, history_messages=[], image_data=None, **kwargs: openai_complete_if_cache(
            "gpt-4o",
            "",
            system_prompt=None,
            history_messages=[],
            messages=[
                {"role": "system", "content": system_prompt} if system_prompt else None,
                {"role": "user", "content": [
                    {"type": "text", "text": prompt},
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}}
                ]} if image_data else {"role": "user", "content": prompt}
            ],
            api_key=api_key,
            base_url=base_url,
            **kwargs,
        ) if image_data else openai_complete_if_cache(
            "gpt-4o-mini",
            prompt,
            system_prompt=system_prompt,
            history_messages=history_messages,
            api_key=api_key,
            base_url=base_url,
            **kwargs,
        )
    )

    image_content = {
        "img_path": "path/to/image.jpg",
        "img_caption": ["Figure 1: Experimental results"],
        "img_footnote": ["Data collected in 2024"]
    }

    description, entity_info = await image_processor.process_multimodal_content(
        modal_content=image_content,
        content_type="image",
        file_path="research_paper.pdf",
        entity_name="Experimental Results Figure"
    )

    # Process a table
    table_processor = TableModalProcessor(
        lightrag=rag,
        modal_caption_func=lambda prompt, system_prompt=None, history_messages=[], **kwargs: openai_complete_if_cache(
            "gpt-4o-mini",
            prompt,
            system_prompt=system_prompt,
            history_messages=history_messages,
            api_key=api_key,
            base_url=base_url,
            **kwargs,
        )
    )

    table_content = {
        "table_body": """
        | Method | Accuracy | F1-Score |
        |--------|----------|----------|
        | RAGAnything | 95.2% | 0.94 |
        | Baseline | 87.3% | 0.85 |
        """,
        "table_caption": ["Performance Comparison"],
        "table_footnote": ["Results on test dataset"]
    }

    description, entity_info = await table_processor.process_multimodal_content(
        modal_content=table_content,
        content_type="table",
        file_path="research_paper.pdf",
        entity_name="Performance Results Table"
    )

if __name__ == "__main__":
    asyncio.run(process_multimodal_content())
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;3. Batch Processing&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Process multiple documents
await rag.process_folder_complete(
    folder_path="./documents",
    output_dir="./output",
    file_extensions=[".pdf", ".docx", ".pptx"],
    recursive=True,
    max_workers=4
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;4. Custom Modal Processors&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from raganything.modalprocessors import GenericModalProcessor

class CustomModalProcessor(GenericModalProcessor):
    async def process_multimodal_content(self, modal_content, content_type, file_path, entity_name):
        # Your custom processing logic
        enhanced_description = await self.analyze_custom_content(modal_content)
        entity_info = self.create_custom_entity(enhanced_description, entity_name)
        return await self._create_entity_and_chunk(enhanced_description, entity_info, file_path)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;5. Query Options&lt;/h4&gt; 
&lt;p&gt;RAG-Anything provides three types of query methods:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Pure Text Queries&lt;/strong&gt; - Direct knowledge base search using LightRAG:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Different query modes for text queries
text_result_hybrid = await rag.aquery("Your question", mode="hybrid")
text_result_local = await rag.aquery("Your question", mode="local")
text_result_global = await rag.aquery("Your question", mode="global")
text_result_naive = await rag.aquery("Your question", mode="naive")

# Synchronous version
sync_text_result = rag.query("Your question", mode="hybrid")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;VLM Enhanced Queries&lt;/strong&gt; - Automatically analyze images in retrieved context using VLM:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# VLM enhanced query (automatically enabled when vision_model_func is provided)
vlm_result = await rag.aquery(
    "Analyze the charts and figures in the document",
    mode="hybrid"
    # vlm_enhanced=True is automatically set when vision_model_func is available
)

# Manually control VLM enhancement
vlm_enabled = await rag.aquery(
    "What do the images show in this document?",
    mode="hybrid",
    vlm_enhanced=True  # Force enable VLM enhancement
)

vlm_disabled = await rag.aquery(
    "What do the images show in this document?",
    mode="hybrid",
    vlm_enhanced=False  # Force disable VLM enhancement
)

# When documents contain images, VLM can see and analyze them directly
# The system will automatically:
# 1. Retrieve relevant context containing image paths
# 2. Load and encode images as base64
# 3. Send both text context and images to VLM for comprehensive analysis
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Multimodal Queries&lt;/strong&gt; - Enhanced queries with specific multimodal content analysis:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Query with table data
table_result = await rag.aquery_with_multimodal(
    "Compare these performance metrics with the document content",
    multimodal_content=[{
        "type": "table",
        "table_data": """Method,Accuracy,Speed
                        RAGAnything,95.2%,120ms
                        Traditional,87.3%,180ms""",
        "table_caption": "Performance comparison"
    }],
    mode="hybrid"
)

# Query with equation content
equation_result = await rag.aquery_with_multimodal(
    "Explain this formula and its relevance to the document content",
    multimodal_content=[{
        "type": "equation",
        "latex": "P(d|q) = \\frac{P(q|d) \\cdot P(d)}{P(q)}",
        "equation_caption": "Document relevance probability"
    }],
    mode="hybrid"
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;6. Loading Existing LightRAG Instance&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import asyncio
from raganything import RAGAnything, RAGAnythingConfig
from lightrag import LightRAG
from lightrag.llm.openai import openai_complete_if_cache, openai_embed
from lightrag.kg.shared_storage import initialize_pipeline_status
from lightrag.utils import EmbeddingFunc
import os

async def load_existing_lightrag():
    # Set up API configuration
    api_key = "your-api-key"
    base_url = "your-base-url"  # Optional

    # First, create or load existing LightRAG instance
    lightrag_working_dir = "./existing_lightrag_storage"

    # Check if previous LightRAG instance exists
    if os.path.exists(lightrag_working_dir) and os.listdir(lightrag_working_dir):
        print("âœ… Found existing LightRAG instance, loading...")
    else:
        print("âŒ No existing LightRAG instance found, will create new one")

    # Create/load LightRAG instance with your configuration
    lightrag_instance = LightRAG(
        working_dir=lightrag_working_dir,
        llm_model_func=lambda prompt, system_prompt=None, history_messages=[], **kwargs: openai_complete_if_cache(
            "gpt-4o-mini",
            prompt,
            system_prompt=system_prompt,
            history_messages=history_messages,
            api_key=api_key,
            base_url=base_url,
            **kwargs,
        ),
        embedding_func=EmbeddingFunc(
            embedding_dim=3072,
            max_token_size=8192,
            func=lambda texts: openai_embed(
                texts,
                model="text-embedding-3-large",
                api_key=api_key,
                base_url=base_url,
            ),
        )
    )

    # Initialize storage (this will load existing data if available)
    await lightrag_instance.initialize_storages()
    await initialize_pipeline_status()

    # Define vision model function for image processing
    def vision_model_func(
        prompt, system_prompt=None, history_messages=[], image_data=None, messages=None, **kwargs
    ):
        # If messages format is provided (for multimodal VLM enhanced query), use it directly
        if messages:
            return openai_complete_if_cache(
                "gpt-4o",
                "",
                system_prompt=None,
                history_messages=[],
                messages=messages,
                api_key=api_key,
                base_url=base_url,
                **kwargs,
            )
        # Traditional single image format
        elif image_data:
            return openai_complete_if_cache(
                "gpt-4o",
                "",
                system_prompt=None,
                history_messages=[],
                messages=[
                    {"role": "system", "content": system_prompt}
                    if system_prompt
                    else None,
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{image_data}"
                                },
                            },
                        ],
                    }
                    if image_data
                    else {"role": "user", "content": prompt},
                ],
                api_key=api_key,
                base_url=base_url,
                **kwargs,
            )
        # Pure text format
        else:
            return lightrag_instance.llm_model_func(prompt, system_prompt, history_messages, **kwargs)

    # Now use existing LightRAG instance to initialize RAGAnything
    rag = RAGAnything(
        lightrag=lightrag_instance,  # Pass existing LightRAG instance
        vision_model_func=vision_model_func,
        # Note: working_dir, llm_model_func, embedding_func, etc. are inherited from lightrag_instance
    )

    # Query existing knowledge base
    result = await rag.aquery(
        "What data has been processed in this LightRAG instance?",
        mode="hybrid"
    )
    print("Query result:", result)

    # Add new multimodal document to existing LightRAG instance
    await rag.process_document_complete(
        file_path="path/to/new/multimodal_document.pdf",
        output_dir="./output"
    )

if __name__ == "__main__":
    asyncio.run(load_existing_lightrag())
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;7. Direct Content List Insertion&lt;/h4&gt; 
&lt;p&gt;For scenarios where you already have a pre-parsed content list (e.g., from external parsers or previous processing), you can directly insert it into RAGAnything without document parsing:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import asyncio
from raganything import RAGAnything, RAGAnythingConfig
from lightrag.llm.openai import openai_complete_if_cache, openai_embed
from lightrag.utils import EmbeddingFunc

async def insert_content_list_example():
    # Set up API configuration
    api_key = "your-api-key"
    base_url = "your-base-url"  # Optional

    # Create RAGAnything configuration
    config = RAGAnythingConfig(
        working_dir="./rag_storage",
        enable_image_processing=True,
        enable_table_processing=True,
        enable_equation_processing=True,
    )

    # Define model functions
    def llm_model_func(prompt, system_prompt=None, history_messages=[], **kwargs):
        return openai_complete_if_cache(
            "gpt-4o-mini",
            prompt,
            system_prompt=system_prompt,
            history_messages=history_messages,
            api_key=api_key,
            base_url=base_url,
            **kwargs,
        )

    def vision_model_func(prompt, system_prompt=None, history_messages=[], image_data=None, messages=None, **kwargs):
        # If messages format is provided (for multimodal VLM enhanced query), use it directly
        if messages:
            return openai_complete_if_cache(
                "gpt-4o",
                "",
                system_prompt=None,
                history_messages=[],
                messages=messages,
                api_key=api_key,
                base_url=base_url,
                **kwargs,
            )
        # Traditional single image format
        elif image_data:
            return openai_complete_if_cache(
                "gpt-4o",
                "",
                system_prompt=None,
                history_messages=[],
                messages=[
                    {"role": "system", "content": system_prompt} if system_prompt else None,
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}}
                        ],
                    } if image_data else {"role": "user", "content": prompt},
                ],
                api_key=api_key,
                base_url=base_url,
                **kwargs,
            )
        # Pure text format
        else:
            return llm_model_func(prompt, system_prompt, history_messages, **kwargs)

    embedding_func = EmbeddingFunc(
        embedding_dim=3072,
        max_token_size=8192,
        func=lambda texts: openai_embed(
            texts,
            model="text-embedding-3-large",
            api_key=api_key,
            base_url=base_url,
        ),
    )

    # Initialize RAGAnything
    rag = RAGAnything(
        config=config,
        llm_model_func=llm_model_func,
        vision_model_func=vision_model_func,
        embedding_func=embedding_func,
    )

    # Example: Pre-parsed content list from external source
    content_list = [
        {
            "type": "text",
            "text": "This is the introduction section of our research paper.",
            "page_idx": 0  # Page number where this content appears
        },
        {
            "type": "image",
            "img_path": "/absolute/path/to/figure1.jpg",  # IMPORTANT: Use absolute path
            "img_caption": ["Figure 1: System Architecture"],
            "img_footnote": ["Source: Authors' original design"],
            "page_idx": 1  # Page number where this image appears
        },
        {
            "type": "table",
            "table_body": "| Method | Accuracy | F1-Score |\n|--------|----------|----------|\n| Ours | 95.2% | 0.94 |\n| Baseline | 87.3% | 0.85 |",
            "table_caption": ["Table 1: Performance Comparison"],
            "table_footnote": ["Results on test dataset"],
            "page_idx": 2  # Page number where this table appears
        },
        {
            "type": "equation",
            "latex": "P(d|q) = \\frac{P(q|d) \\cdot P(d)}{P(q)}",
            "text": "Document relevance probability formula",
            "page_idx": 3  # Page number where this equation appears
        },
        {
            "type": "text",
            "text": "In conclusion, our method demonstrates superior performance across all metrics.",
            "page_idx": 4  # Page number where this content appears
        }
    ]

    # Insert the content list directly
    await rag.insert_content_list(
        content_list=content_list,
        file_path="research_paper.pdf",  # Reference file name for citation
        split_by_character=None,         # Optional text splitting
        split_by_character_only=False,   # Optional text splitting mode
        doc_id=None,                     # Optional custom document ID (will be auto-generated if not provided)
        display_stats=True               # Show content statistics
    )

    # Query the inserted content
    result = await rag.aquery(
        "What are the key findings and performance metrics mentioned in the research?",
        mode="hybrid"
    )
    print("Query result:", result)

    # You can also insert multiple content lists with different document IDs
    another_content_list = [
        {
            "type": "text",
            "text": "This is content from another document.",
            "page_idx": 0  # Page number where this content appears
        },
        {
            "type": "table",
            "table_body": "| Feature | Value |\n|---------|-------|\n| Speed | Fast |\n| Accuracy | High |",
            "table_caption": ["Feature Comparison"],
            "page_idx": 1  # Page number where this table appears
        }
    ]

    await rag.insert_content_list(
        content_list=another_content_list,
        file_path="another_document.pdf",
        doc_id="custom-doc-id-123"  # Custom document ID
    )

if __name__ == "__main__":
    asyncio.run(insert_content_list_example())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Content List Format:&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;The &lt;code&gt;content_list&lt;/code&gt; should follow the standard format with each item being a dictionary containing:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Text content&lt;/strong&gt;: &lt;code&gt;{"type": "text", "text": "content text", "page_idx": 0}&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Image content&lt;/strong&gt;: &lt;code&gt;{"type": "image", "img_path": "/absolute/path/to/image.jpg", "img_caption": ["caption"], "img_footnote": ["note"], "page_idx": 1}&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Table content&lt;/strong&gt;: &lt;code&gt;{"type": "table", "table_body": "markdown table", "table_caption": ["caption"], "table_footnote": ["note"], "page_idx": 2}&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Equation content&lt;/strong&gt;: &lt;code&gt;{"type": "equation", "latex": "LaTeX formula", "text": "description", "page_idx": 3}&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Generic content&lt;/strong&gt;: &lt;code&gt;{"type": "custom_type", "content": "any content", "page_idx": 4}&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Important Notes:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;img_path&lt;/code&gt;&lt;/strong&gt;: Must be an absolute path to the image file (e.g., &lt;code&gt;/home/user/images/chart.jpg&lt;/code&gt; or &lt;code&gt;C:\Users\user\images\chart.jpg&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;page_idx&lt;/code&gt;&lt;/strong&gt;: Represents the page number where the content appears in the original document (0-based indexing)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Content ordering&lt;/strong&gt;: Items are processed in the order they appear in the list&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This method is particularly useful when:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;You have content from external parsers (non-MinerU/Docling)&lt;/li&gt; 
 &lt;li&gt;You want to process programmatically generated content&lt;/li&gt; 
 &lt;li&gt;You need to insert content from multiple sources into a single knowledge base&lt;/li&gt; 
 &lt;li&gt;You have cached parsing results that you want to reuse&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ› ï¸ Examples&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;Practical Implementation Demos&lt;/em&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://user-images.githubusercontent.com/74038190/212257455-13e3e01e-d6a6-45dc-bb92-3ab87b12dfc1.gif" width="300" /&gt; 
&lt;/div&gt; 
&lt;p&gt;The &lt;code&gt;examples/&lt;/code&gt; directory contains comprehensive usage examples:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;raganything_example.py&lt;/code&gt;&lt;/strong&gt;: End-to-end document processing with MinerU&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;modalprocessors_example.py&lt;/code&gt;&lt;/strong&gt;: Direct multimodal content processing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;office_document_test.py&lt;/code&gt;&lt;/strong&gt;: Office document parsing test with MinerU (no API key required)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;image_format_test.py&lt;/code&gt;&lt;/strong&gt;: Image format parsing test with MinerU (no API key required)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;text_format_test.py&lt;/code&gt;&lt;/strong&gt;: Text format parsing test with MinerU (no API key required)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Run examples:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# End-to-end processing with parser selection
python examples/raganything_example.py path/to/document.pdf --api-key YOUR_API_KEY --parser mineru

# Direct modal processing
python examples/modalprocessors_example.py --api-key YOUR_API_KEY

# Office document parsing test (MinerU only)
python examples/office_document_test.py --file path/to/document.docx

# Image format parsing test (MinerU only)
python examples/image_format_test.py --file path/to/image.bmp

# Text format parsing test (MinerU only)
python examples/text_format_test.py --file path/to/document.md

# Check LibreOffice installation
python examples/office_document_test.py --check-libreoffice --file dummy

# Check PIL/Pillow installation
python examples/image_format_test.py --check-pillow --file dummy

# Check ReportLab installation
python examples/text_format_test.py --check-reportlab --file dummy
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ”§ Configuration&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;System Optimization Parameters&lt;/em&gt;&lt;/p&gt; 
&lt;h3&gt;Environment Variables&lt;/h3&gt; 
&lt;p&gt;Create a &lt;code&gt;.env&lt;/code&gt; file (refer to &lt;code&gt;.env.example&lt;/code&gt;):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;OPENAI_API_KEY=your_openai_api_key
OPENAI_BASE_URL=your_base_url  # Optional
OUTPUT_DIR=./output             # Default output directory for parsed documents
PARSER=mineru                   # Parser selection: mineru or docling
PARSE_METHOD=auto              # Parse method: auto, ocr, or txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; For backward compatibility, legacy environment variable names are still supported:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;MINERU_PARSE_METHOD&lt;/code&gt; is deprecated, please use &lt;code&gt;PARSE_METHOD&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: API keys are only required for full RAG processing with LLM integration. The parsing test files (&lt;code&gt;office_document_test.py&lt;/code&gt; and &lt;code&gt;image_format_test.py&lt;/code&gt;) only test parser functionality and do not require API keys.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Parser Configuration&lt;/h3&gt; 
&lt;p&gt;RAGAnything now supports multiple parsers, each with specific advantages:&lt;/p&gt; 
&lt;h4&gt;MinerU Parser&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Supports PDF, images, Office documents, and more formats&lt;/li&gt; 
 &lt;li&gt;Powerful OCR and table extraction capabilities&lt;/li&gt; 
 &lt;li&gt;GPU acceleration support&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Docling Parser&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Optimized for Office documents and HTML files&lt;/li&gt; 
 &lt;li&gt;Better document structure preservation&lt;/li&gt; 
 &lt;li&gt;Native support for multiple Office formats&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;MinerU Configuration&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# MinerU 2.0 uses command-line parameters instead of config files
# Check available options:
mineru --help

# Common configurations:
mineru -p input.pdf -o output_dir -m auto    # Automatic parsing mode
mineru -p input.pdf -o output_dir -m ocr     # OCR-focused parsing
mineru -p input.pdf -o output_dir -b pipeline --device cuda  # GPU acceleration
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also configure parsing through RAGAnything parameters:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Basic parsing configuration with parser selection
await rag.process_document_complete(
    file_path="document.pdf",
    output_dir="./output/",
    parse_method="auto",          # or "ocr", "txt"
    parser="mineru"               # Optional: "mineru" or "docling"
)

# Advanced parsing configuration with special parameters
await rag.process_document_complete(
    file_path="document.pdf",
    output_dir="./output/",
    parse_method="auto",          # Parsing method: "auto", "ocr", "txt"
    parser="mineru",              # Parser selection: "mineru" or "docling"

    # MinerU special parameters - all supported kwargs:
    lang="ch",                   # Document language for OCR optimization (e.g., "ch", "en", "ja")
    device="cuda:0",             # Inference device: "cpu", "cuda", "cuda:0", "npu", "mps"
    start_page=0,                # Starting page number (0-based, for PDF)
    end_page=10,                 # Ending page number (0-based, for PDF)
    formula=True,                # Enable formula parsing
    table=True,                  # Enable table parsing
    backend="pipeline",          # Parsing backend: pipeline|vlm-transformers|vlm-sglang-engine|vlm-sglang-client.
    source="huggingface",        # Model source: "huggingface", "modelscope", "local"
    # vlm_url="http://127.0.0.1:3000" # Service address when using backend=vlm-sglang-client

    # Standard RAGAnything parameters
    display_stats=True,          # Display content statistics
    split_by_character=None,     # Optional character to split text by
    doc_id=None                  # Optional document ID
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: MinerU 2.0 no longer uses the &lt;code&gt;magic-pdf.json&lt;/code&gt; configuration file. All settings are now passed as command-line parameters or function arguments. RAG-Anything now supports multiple document parsers - you can choose between MinerU and Docling based on your needs.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Processing Requirements&lt;/h3&gt; 
&lt;p&gt;Different content types require specific optional dependencies:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Office Documents&lt;/strong&gt; (.doc, .docx, .ppt, .pptx, .xls, .xlsx): Install &lt;a href="https://www.libreoffice.org/download/download/"&gt;LibreOffice&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Extended Image Formats&lt;/strong&gt; (.bmp, .tiff, .gif, .webp): Install with &lt;code&gt;pip install raganything[image]&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Text Files&lt;/strong&gt; (.txt, .md): Install with &lt;code&gt;pip install raganything[text]&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;ğŸ“‹ Quick Install&lt;/strong&gt;: Use &lt;code&gt;pip install raganything[all]&lt;/code&gt; to enable all format support (Python dependencies only - LibreOffice still needs separate installation)&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ§ª Supported Content Types&lt;/h2&gt; 
&lt;h3&gt;Document Formats&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;PDFs&lt;/strong&gt; - Research papers, reports, presentations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Office Documents&lt;/strong&gt; - DOC, DOCX, PPT, PPTX, XLS, XLSX&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Images&lt;/strong&gt; - JPG, PNG, BMP, TIFF, GIF, WebP&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Text Files&lt;/strong&gt; - TXT, MD&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Multimodal Elements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Images&lt;/strong&gt; - Photographs, diagrams, charts, screenshots&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Tables&lt;/strong&gt; - Data tables, comparison charts, statistical summaries&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Equations&lt;/strong&gt; - Mathematical formulas in LaTeX format&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Generic Content&lt;/strong&gt; - Custom content types via extensible processors&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;em&gt;For installation of format-specific dependencies, see the &lt;a href="https://raw.githubusercontent.com/HKUDS/RAG-Anything/main/#-configuration"&gt;Configuration&lt;/a&gt; section.&lt;/em&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ“– Citation&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;Academic Reference&lt;/em&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;div style="width: 60px; height: 60px; margin: 20px auto; position: relative;"&gt; 
  &lt;div style="width: 100%; height: 100%; border: 2px solid #00d9ff; border-radius: 50%; position: relative;"&gt; 
   &lt;div style="position: absolute; top: 50%; left: 50%; transform: translate(-50%, -50%); font-size: 24px; color: #00d9ff;"&gt;
    ğŸ“–
   &lt;/div&gt; 
  &lt;/div&gt; 
  &lt;div style="position: absolute; bottom: -5px; left: 50%; transform: translateX(-50%); width: 20px; height: 20px; background: white; border-right: 2px solid #00d9ff; border-bottom: 2px solid #00d9ff; transform: rotate(45deg);"&gt;&lt;/div&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;p&gt;If you find RAG-Anything useful in your research, please cite our paper:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@article{guo2024lightrag,
  title={LightRAG: Simple and Fast Retrieval-Augmented Generation},
  author={Zirui Guo and Lianghao Xia and Yanhua Yu and Tu Ao and Chao Huang},
  year={2024},
  eprint={2410.05779},
  archivePrefix={arXiv},
  primaryClass={cs.IR}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ”— Related Projects&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;Ecosystem &amp;amp; Extensions&lt;/em&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td align="center"&gt; &lt;a href="https://github.com/HKUDS/LightRAG"&gt; 
      &lt;div style="width: 100px; height: 100px; background: linear-gradient(135deg, rgba(0, 217, 255, 0.1) 0%, rgba(0, 217, 255, 0.05) 100%); border-radius: 15px; border: 1px solid rgba(0, 217, 255, 0.2); display: flex; align-items: center; justify-content: center; margin-bottom: 10px;"&gt; 
       &lt;span style="font-size: 32px;"&gt;âš¡&lt;/span&gt; 
      &lt;/div&gt; &lt;b&gt;LightRAG&lt;/b&gt;&lt;br /&gt; &lt;sub&gt;Simple and Fast RAG&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt; 
    &lt;td align="center"&gt; &lt;a href="https://github.com/HKUDS/VideoRAG"&gt; 
      &lt;div style="width: 100px; height: 100px; background: linear-gradient(135deg, rgba(0, 217, 255, 0.1) 0%, rgba(0, 217, 255, 0.05) 100%); border-radius: 15px; border: 1px solid rgba(0, 217, 255, 0.2); display: flex; align-items: center; justify-content: center; margin-bottom: 10px;"&gt; 
       &lt;span style="font-size: 32px;"&gt;ğŸ¥&lt;/span&gt; 
      &lt;/div&gt; &lt;b&gt;VideoRAG&lt;/b&gt;&lt;br /&gt; &lt;sub&gt;Extreme Long-Context Video RAG&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt; 
    &lt;td align="center"&gt; &lt;a href="https://github.com/HKUDS/MiniRAG"&gt; 
      &lt;div style="width: 100px; height: 100px; background: linear-gradient(135deg, rgba(0, 217, 255, 0.1) 0%, rgba(0, 217, 255, 0.05) 100%); border-radius: 15px; border: 1px solid rgba(0, 217, 255, 0.2); display: flex; align-items: center; justify-content: center; margin-bottom: 10px;"&gt; 
       &lt;span style="font-size: 32px;"&gt;âœ¨&lt;/span&gt; 
      &lt;/div&gt; &lt;b&gt;MiniRAG&lt;/b&gt;&lt;br /&gt; &lt;sub&gt;Extremely Simple RAG&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;â­ Star History&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;Community Growth Trajectory&lt;/em&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://star-history.com/#HKUDS/RAG-Anything&amp;amp;Date"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=HKUDS/RAG-Anything&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
   &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=HKUDS/RAG-Anything&amp;amp;type=Date" /&gt; 
   &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=HKUDS/RAG-Anything&amp;amp;type=Date" style="border-radius: 15px; box-shadow: 0 0 30px rgba(0, 217, 255, 0.3);" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ¤ Contribution&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;Join the Innovation&lt;/em&gt;&lt;/p&gt; 
&lt;div align="center"&gt;
  We thank all our contributors for their valuable contributions. 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/HKUDS/RAG-Anything/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=HKUDS/RAG-Anything" style="border-radius: 15px; box-shadow: 0 0 20px rgba(0, 217, 255, 0.3);" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;div align="center" style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; padding: 30px; margin: 30px 0;"&gt; 
 &lt;div&gt; 
  &lt;img src="https://user-images.githubusercontent.com/74038190/212284100-561aa473-3905-4a80-b561-0d28506553ee.gif" width="500" /&gt; 
 &lt;/div&gt; 
 &lt;div style="margin-top: 20px;"&gt; 
  &lt;a href="https://github.com/HKUDS/RAG-Anything" style="text-decoration: none;"&gt; &lt;img src="https://img.shields.io/badge/â­%20Star%20us%20on%20GitHub-1a1a2e?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white" /&gt; &lt;/a&gt; 
  &lt;a href="https://github.com/HKUDS/RAG-Anything/issues" style="text-decoration: none;"&gt; &lt;img src="https://img.shields.io/badge/ğŸ›%20Report%20Issues-ff6b6b?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white" /&gt; &lt;/a&gt; 
  &lt;a href="https://github.com/HKUDS/RAG-Anything/discussions" style="text-decoration: none;"&gt; &lt;img src="https://img.shields.io/badge/ğŸ’¬%20Discussions-4ecdc4?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white" /&gt; &lt;/a&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;div style="width: 100%; max-width: 600px; margin: 20px auto; padding: 20px; background: linear-gradient(135deg, rgba(0, 217, 255, 0.1) 0%, rgba(0, 217, 255, 0.05) 100%); border-radius: 15px; border: 1px solid rgba(0, 217, 255, 0.2);"&gt; 
  &lt;div style="display: flex; justify-content: center; align-items: center; gap: 15px;"&gt; 
   &lt;span style="font-size: 24px;"&gt;â­&lt;/span&gt; 
   &lt;span style="color: #00d9ff; font-size: 18px;"&gt;Thank you for visiting RAG-Anything!&lt;/span&gt; 
   &lt;span style="font-size: 24px;"&gt;â­&lt;/span&gt; 
  &lt;/div&gt; 
  &lt;div style="margin-top: 10px; color: #00d9ff; font-size: 16px;"&gt;
   Building the Future of Multimodal AI
  &lt;/div&gt; 
 &lt;/div&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Physical-Intelligence/openpi</title>
      <link>https://github.com/Physical-Intelligence/openpi</link>
      <description>&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;openpi&lt;/h1&gt; 
&lt;p&gt;openpi holds open-source models and packages for robotics, published by the &lt;a href="https://www.physicalintelligence.company/"&gt;Physical Intelligence team&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Currently, this repo contains three types of models:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;the &lt;a href="https://www.physicalintelligence.company/blog/pi0"&gt;Ï€â‚€ model&lt;/a&gt;, a flow-based vision-language-action model (VLA).&lt;/li&gt; 
 &lt;li&gt;the &lt;a href="https://www.physicalintelligence.company/research/fast"&gt;Ï€â‚€-FAST model&lt;/a&gt;, an autoregressive VLA, based on the FAST action tokenizer.&lt;/li&gt; 
 &lt;li&gt;the &lt;a href="https://www.physicalintelligence.company/blog/pi05"&gt;Ï€â‚€.â‚… model&lt;/a&gt;, an upgraded version of Ï€â‚€ with better open-world generalization trained with &lt;a href="https://www.physicalintelligence.company/research/knowledge_insulation"&gt;knowledge insulation&lt;/a&gt;. Note that, in this repository, we currently only support the flow matching head for both $\pi_{0.5}$ training and inference.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For all models, we provide &lt;em&gt;base model&lt;/em&gt; checkpoints, pre-trained on 10k+ hours of robot data, and examples for using them out of the box or fine-tuning them to your own datasets.&lt;/p&gt; 
&lt;p&gt;This is an experiment: $\pi_0$ was developed for our own robots, which differ from the widely used platforms such as &lt;a href="https://tonyzhaozh.github.io/aloha/"&gt;ALOHA&lt;/a&gt; and &lt;a href="https://droid-dataset.github.io/"&gt;DROID&lt;/a&gt;, and though we are optimistic that researchers and practitioners will be able to run creative new experiments adapting $\pi_0$ to their own platforms, we do not expect every such attempt to be successful. All this is to say: $\pi_0$ may or may not work for you, but you are welcome to try it and see!&lt;/p&gt; 
&lt;h2&gt;Updates&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;[Sept 2025] We released PyTorch support in openpi.&lt;/li&gt; 
 &lt;li&gt;[Sept 2025] We released pi05, an upgraded version of pi0 with better open-world generalization.&lt;/li&gt; 
 &lt;li&gt;[Sept 2025]: We have added an &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/droid/README_train.md#data-filtering"&gt;improved idle filter&lt;/a&gt; for DROID training.&lt;/li&gt; 
 &lt;li&gt;[Jun 2025]: We have added &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/droid/README_train.md"&gt;instructions&lt;/a&gt; for using &lt;code&gt;openpi&lt;/code&gt; to train VLAs on the full &lt;a href="https://droid-dataset.github.io/"&gt;DROID dataset&lt;/a&gt;. This is an approximate open-source implementation of the training pipeline used to train pi0-FAST-DROID.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;p&gt;To run the models in this repository, you will need an NVIDIA GPU with at least the following specifications. These estimations assume a single GPU, but you can also use multiple GPUs with model parallelism to reduce per-GPU memory requirements by configuring &lt;code&gt;fsdp_devices&lt;/code&gt; in the training config. Please also note that the current training script does not yet support multi-node training.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Mode&lt;/th&gt; 
   &lt;th&gt;Memory Required&lt;/th&gt; 
   &lt;th&gt;Example GPU&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;&amp;gt; 8 GB&lt;/td&gt; 
   &lt;td&gt;RTX 4090&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Fine-Tuning (LoRA)&lt;/td&gt; 
   &lt;td&gt;&amp;gt; 22.5 GB&lt;/td&gt; 
   &lt;td&gt;RTX 4090&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Fine-Tuning (Full)&lt;/td&gt; 
   &lt;td&gt;&amp;gt; 70 GB&lt;/td&gt; 
   &lt;td&gt;A100 (80GB) / H100&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;The repo has been tested with Ubuntu 22.04, we do not currently support other operating systems.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;When cloning this repo, make sure to update submodules:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone --recurse-submodules git@github.com:Physical-Intelligence/openpi.git

# Or if you already cloned the repo:
git submodule update --init --recursive
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We use &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt; to manage Python dependencies. See the &lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;uv installation instructions&lt;/a&gt; to set it up. Once uv is installed, run the following to set up the environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;GIT_LFS_SKIP_SMUDGE=1 uv sync
GIT_LFS_SKIP_SMUDGE=1 uv pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;NOTE: &lt;code&gt;GIT_LFS_SKIP_SMUDGE=1&lt;/code&gt; is needed to pull LeRobot as a dependency.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Docker&lt;/strong&gt;: As an alternative to uv installation, we provide instructions for installing openpi using Docker. If you encounter issues with your system setup, consider using Docker to simplify installation. See &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/docs/docker.md"&gt;Docker Setup&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Model Checkpoints&lt;/h2&gt; 
&lt;h3&gt;Base Models&lt;/h3&gt; 
&lt;p&gt;We provide multiple base VLA model checkpoints. These checkpoints have been pre-trained on 10k+ hours of robot data, and can be used for fine-tuning.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Use Case&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Checkpoint Path&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$&lt;/td&gt; 
   &lt;td&gt;Fine-Tuning&lt;/td&gt; 
   &lt;td&gt;Base &lt;a href="https://www.physicalintelligence.company/blog/pi0"&gt;Ï€â‚€ model&lt;/a&gt; for fine-tuning&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_base&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$-FAST&lt;/td&gt; 
   &lt;td&gt;Fine-Tuning&lt;/td&gt; 
   &lt;td&gt;Base autoregressive &lt;a href="https://www.physicalintelligence.company/research/fast"&gt;Ï€â‚€-FAST model&lt;/a&gt; for fine-tuning&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_fast_base&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_{0.5}$&lt;/td&gt; 
   &lt;td&gt;Fine-Tuning&lt;/td&gt; 
   &lt;td&gt;Base &lt;a href="https://www.physicalintelligence.company/blog/pi05"&gt;Ï€â‚€.â‚… model&lt;/a&gt; for fine-tuning&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi05_base&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Fine-Tuned Models&lt;/h3&gt; 
&lt;p&gt;We also provide "expert" checkpoints for various robot platforms and tasks. These models are fine-tuned from the base models above and intended to run directly on the target robot. These may or may not work on your particular robot. Since these checkpoints were fine-tuned on relatively small datasets collected with more widely available robots, such as ALOHA and the DROID Franka setup, they might not generalize to your particular setup, though we found some of these, especially the DROID checkpoint, to generalize quite broadly in practice.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Use Case&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Checkpoint Path&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$-FAST-DROID&lt;/td&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;$\pi_0$-FAST model fine-tuned on the &lt;a href="https://droid-dataset.github.io/"&gt;DROID dataset&lt;/a&gt;: can perform a wide range of simple table-top manipulation tasks 0-shot in new scenes on the DROID robot platform&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_fast_droid&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$-DROID&lt;/td&gt; 
   &lt;td&gt;Fine-Tuning&lt;/td&gt; 
   &lt;td&gt;$\pi_0$ model fine-tuned on the &lt;a href="https://droid-dataset.github.io/"&gt;DROID dataset&lt;/a&gt;: faster inference than $\pi_0$-FAST-DROID, but may not follow language commands as well&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_droid&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$-ALOHA-towel&lt;/td&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;$\pi_0$ model fine-tuned on internal &lt;a href="https://tonyzhaozh.github.io/aloha/"&gt;ALOHA&lt;/a&gt; data: can fold diverse towels 0-shot on ALOHA robot platforms&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_aloha_towel&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$-ALOHA-tupperware&lt;/td&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;$\pi_0$ model fine-tuned on internal &lt;a href="https://tonyzhaozh.github.io/aloha/"&gt;ALOHA&lt;/a&gt; data: can unpack food from a tupperware container&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_aloha_tupperware&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$-ALOHA-pen-uncap&lt;/td&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;$\pi_0$ model fine-tuned on public &lt;a href="https://dit-policy.github.io/"&gt;ALOHA&lt;/a&gt; data: can uncap a pen&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_aloha_pen_uncap&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_{0.5}$-LIBERO&lt;/td&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;$\pi_{0.5}$ model fine-tuned for the &lt;a href="https://libero-project.github.io/datasets"&gt;LIBERO&lt;/a&gt; benchmark: gets state-of-the-art performance (see &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/libero/README.md"&gt;LIBERO README&lt;/a&gt;)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi05_libero&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_{0.5}$-DROID&lt;/td&gt; 
   &lt;td&gt;Inference / Fine-Tuning&lt;/td&gt; 
   &lt;td&gt;$\pi_{0.5}$ model fine-tuned on the &lt;a href="https://droid-dataset.github.io/"&gt;DROID dataset&lt;/a&gt; with &lt;a href="https://www.physicalintelligence.company/research/knowledge_insulation"&gt;knowledge insulation&lt;/a&gt;: fast inference and good language-following&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi05_droid&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;By default, checkpoints are automatically downloaded from &lt;code&gt;gs://openpi-assets&lt;/code&gt; and are cached in &lt;code&gt;~/.cache/openpi&lt;/code&gt; when needed. You can overwrite the download path by setting the &lt;code&gt;OPENPI_DATA_HOME&lt;/code&gt; environment variable.&lt;/p&gt; 
&lt;h2&gt;Running Inference for a Pre-Trained Model&lt;/h2&gt; 
&lt;p&gt;Our pre-trained model checkpoints can be run with a few lines of code (here our $\pi_0$-FAST-DROID model):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openpi.training import config as _config
from openpi.policies import policy_config
from openpi.shared import download

config = _config.get_config("pi05_droid")
checkpoint_dir = download.maybe_download("gs://openpi-assets/checkpoints/pi05_droid")

# Create a trained policy.
policy = policy_config.create_trained_policy(config, checkpoint_dir)

# Run inference on a dummy example.
example = {
    "observation/exterior_image_1_left": ...,
    "observation/wrist_image_left": ...,
    ...
    "prompt": "pick up the fork"
}
action_chunk = policy.infer(example)["actions"]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also test this out in the &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/inference.ipynb"&gt;example notebook&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We provide detailed step-by-step examples for running inference of our pre-trained checkpoints on &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/droid/README.md"&gt;DROID&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/aloha_real/README.md"&gt;ALOHA&lt;/a&gt; robots.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Remote Inference&lt;/strong&gt;: We provide &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/docs/remote_inference.md"&gt;examples and code&lt;/a&gt; for running inference of our models &lt;strong&gt;remotely&lt;/strong&gt;: the model can run on a different server and stream actions to the robot via a websocket connection. This makes it easy to use more powerful GPUs off-robot and keep robot and policy environments separate.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Test inference without a robot&lt;/strong&gt;: We provide a &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/simple_client/README.md"&gt;script&lt;/a&gt; for testing inference without a robot. This script will generate a random observation and run inference with the model. See &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/simple_client/README.md"&gt;here&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Fine-Tuning Base Models on Your Own Data&lt;/h2&gt; 
&lt;p&gt;We will fine-tune the $\pi_{0.5}$ model on the &lt;a href="https://libero-project.github.io/datasets"&gt;LIBERO dataset&lt;/a&gt; as a running example for how to fine-tune a base model on your own data. We will explain three steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Convert your data to a LeRobot dataset (which we use for training)&lt;/li&gt; 
 &lt;li&gt;Defining training configs and running training&lt;/li&gt; 
 &lt;li&gt;Spinning up a policy server and running inference&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;1. Convert your data to a LeRobot dataset&lt;/h3&gt; 
&lt;p&gt;We provide a minimal example script for converting LIBERO data to a LeRobot dataset in &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/libero/convert_libero_data_to_lerobot.py"&gt;&lt;code&gt;examples/libero/convert_libero_data_to_lerobot.py&lt;/code&gt;&lt;/a&gt;. You can easily modify it to convert your own data! You can download the raw LIBERO dataset from &lt;a href="https://huggingface.co/datasets/openvla/modified_libero_rlds"&gt;here&lt;/a&gt;, and run the script with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run examples/libero/convert_libero_data_to_lerobot.py --data_dir /path/to/your/libero/data
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; If you just want to fine-tune on LIBERO, you can skip this step, because our LIBERO fine-tuning configs point to a pre-converted LIBERO dataset. This step is merely an example that you can adapt to your own data.&lt;/p&gt; 
&lt;h3&gt;2. Defining training configs and running training&lt;/h3&gt; 
&lt;p&gt;To fine-tune a base model on your own data, you need to define configs for data processing and training. We provide example configs with detailed comments for LIBERO below, which you can modify for your own dataset:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/src/openpi/policies/libero_policy.py"&gt;&lt;code&gt;LiberoInputs&lt;/code&gt; and &lt;code&gt;LiberoOutputs&lt;/code&gt;&lt;/a&gt;: Defines the data mapping from the LIBERO environment to the model and vice versa. Will be used for both, training and inference.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/src/openpi/training/config.py"&gt;&lt;code&gt;LeRobotLiberoDataConfig&lt;/code&gt;&lt;/a&gt;: Defines how to process raw LIBERO data from LeRobot dataset for training.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/src/openpi/training/config.py"&gt;&lt;code&gt;TrainConfig&lt;/code&gt;&lt;/a&gt;: Defines fine-tuning hyperparameters, data config, and weight loader.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We provide example fine-tuning configs for &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/src/openpi/training/config.py"&gt;Ï€â‚€&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/src/openpi/training/config.py"&gt;Ï€â‚€-FAST&lt;/a&gt;, and &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/src/openpi/training/config.py"&gt;Ï€â‚€.â‚…&lt;/a&gt; on LIBERO data.&lt;/p&gt; 
&lt;p&gt;Before we can run training, we need to compute the normalization statistics for the training data. Run the script below with the name of your training config:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run scripts/compute_norm_stats.py --config-name pi05_libero
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Now we can kick off training with the following command (the &lt;code&gt;--overwrite&lt;/code&gt; flag is used to overwrite existing checkpoints if you rerun fine-tuning with the same config):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;XLA_PYTHON_CLIENT_MEM_FRACTION=0.9 uv run scripts/train.py pi05_libero --exp-name=my_experiment --overwrite
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The command will log training progress to the console and save checkpoints to the &lt;code&gt;checkpoints&lt;/code&gt; directory. You can also monitor training progress on the Weights &amp;amp; Biases dashboard. For maximally using the GPU memory, set &lt;code&gt;XLA_PYTHON_CLIENT_MEM_FRACTION=0.9&lt;/code&gt; before running training -- this enables JAX to use up to 90% of the GPU memory (vs. the default of 75%).&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; We provide functionality for &lt;em&gt;reloading&lt;/em&gt; normalization statistics for state / action normalization from pre-training. This can be beneficial if you are fine-tuning to a new task on a robot that was part of our pre-training mixture. For more details on how to reload normalization statistics, see the &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/docs/norm_stats.md"&gt;norm_stats.md&lt;/a&gt; file.&lt;/p&gt; 
&lt;h3&gt;3. Spinning up a policy server and running inference&lt;/h3&gt; 
&lt;p&gt;Once training is complete, we can run inference by spinning up a policy server and then querying it from a LIBERO evaluation script. Launching a model server is easy (we use the checkpoint for iteration 20,000 for this example, modify as needed):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run scripts/serve_policy.py policy:checkpoint --policy.config=pi05_libero --policy.dir=checkpoints/pi05_libero/my_experiment/20000
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will spin up a server that listens on port 8000 and waits for observations to be sent to it. We can then run an evaluation script (or robot runtime) that queries the server.&lt;/p&gt; 
&lt;p&gt;For running the LIBERO eval in particular, we provide (and recommend using) a Dockerized workflow that handles both the policy server and the evaluation script together. See the &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/libero/README.md"&gt;LIBERO README&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;p&gt;If you want to embed a policy server call in your own robot runtime, we have a minimal example of how to do so in the &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/docs/remote_inference.md"&gt;remote inference docs&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;More Examples&lt;/h3&gt; 
&lt;p&gt;We provide more examples for how to fine-tune and run inference with our models on the ALOHA platform in the following READMEs:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/aloha_sim"&gt;ALOHA Simulator&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/aloha_real"&gt;ALOHA Real&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/ur5"&gt;UR5&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;PyTorch Support&lt;/h2&gt; 
&lt;p&gt;openpi now provides PyTorch implementations of Ï€â‚€ and Ï€â‚€.â‚… models alongside the original JAX versions! The PyTorch implementation has been validated on the LIBERO benchmark (both inference and finetuning). A few features are currently not supported (this may change in the future):&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;The Ï€â‚€-FAST model&lt;/li&gt; 
 &lt;li&gt;Mixed precision training&lt;/li&gt; 
 &lt;li&gt;FSDP (fully-sharded data parallelism) training&lt;/li&gt; 
 &lt;li&gt;LoRA (low-rank adaptation) training&lt;/li&gt; 
 &lt;li&gt;EMA (exponential moving average) weights during training&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Setup&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Make sure that you have the latest version of all dependencies installed: &lt;code&gt;uv sync&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Double check that you have transformers 4.53.2 installed: &lt;code&gt;uv pip show transformers&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Apply the transformers library patches:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cp -r ./src/openpi/models_pytorch/transformers_replace/* .venv/lib/python3.11/site-packages/transformers/
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;This overwrites several files in the transformers library with necessary model changes: 1) supporting AdaRMS, 2) correctly controlling the precision of activations, and 3) allowing the KV cache to be used without being updated.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;WARNING&lt;/strong&gt;: With the default uv link mode (hardlink), this will permanently affect the transformers library in your uv cache, meaning the changes will survive reinstallations of transformers and could even propagate to other projects that use transformers. To fully undo this operation, you must run &lt;code&gt;uv cache clean transformers&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Converting JAX Models to PyTorch&lt;/h3&gt; 
&lt;p&gt;To convert a JAX model checkpoint to PyTorch format:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run examples/convert_jax_model_to_pytorch.py \
    --checkpoint_dir /path/to/jax/checkpoint \
    --config_name &amp;lt;config name&amp;gt; \
    --output_path /path/to/converted/pytorch/checkpoint
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Running Inference with PyTorch&lt;/h3&gt; 
&lt;p&gt;The PyTorch implementation uses the same API as the JAX version - you only need to change the checkpoint path to point to the converted PyTorch model:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openpi.training import config as _config
from openpi.policies import policy_config
from openpi.shared import download

config = _config.get_config("pi05_droid")
checkpoint_dir = "/path/to/converted/pytorch/checkpoint"

# Create a trained policy (automatically detects PyTorch format)
policy = policy_config.create_trained_policy(config, checkpoint_dir)

# Run inference (same API as JAX)
action_chunk = policy.infer(example)["actions"]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Policy Server with PyTorch&lt;/h3&gt; 
&lt;p&gt;The policy server works identically with PyTorch models - just point to the converted checkpoint directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run scripts/serve_policy.py policy:checkpoint \
    --policy.config=pi05_droid \
    --policy.dir=/path/to/converted/pytorch/checkpoint
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Finetuning with PyTorch&lt;/h3&gt; 
&lt;p&gt;To finetune a model in PyTorch:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Convert the JAX base model to PyTorch format:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;uv run examples/convert_jax_model_to_pytorch.py \
    --config_name &amp;lt;config name&amp;gt; \
    --checkpoint_dir /path/to/jax/base/model \
    --output_path /path/to/pytorch/base/model
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Specify the converted PyTorch model path in your config using &lt;code&gt;pytorch_weight_path&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Launch training using one of these modes:&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Single GPU training:
uv run scripts/train_pytorch.py &amp;lt;config_name&amp;gt; --exp_name &amp;lt;run_name&amp;gt; --save_interval &amp;lt;interval&amp;gt;

# Example:
uv run scripts/train_pytorch.py debug --exp_name pytorch_test
uv run scripts/train_pytorch.py debug --exp_name pytorch_test --resume  # Resume from latest checkpoint

# Multi-GPU training (single node):
uv run torchrun --standalone --nnodes=1 --nproc_per_node=&amp;lt;num_gpus&amp;gt; scripts/train_pytorch.py &amp;lt;config_name&amp;gt; --exp_name &amp;lt;run_name&amp;gt;

# Example:
uv run torchrun --standalone --nnodes=1 --nproc_per_node=2 scripts/train_pytorch.py pi0_aloha_sim --exp_name pytorch_ddp_test
uv run torchrun --standalone --nnodes=1 --nproc_per_node=2 scripts/train_pytorch.py pi0_aloha_sim --exp_name pytorch_ddp_test --resume

# Multi-Node Training:
uv run torchrun \
    --nnodes=&amp;lt;num_nodes&amp;gt; \
    --nproc_per_node=&amp;lt;gpus_per_node&amp;gt; \
    --node_rank=&amp;lt;rank_of_node&amp;gt; \
    --master_addr=&amp;lt;master_ip&amp;gt; \
    --master_port=&amp;lt;port&amp;gt; \
    scripts/train_pytorch.py &amp;lt;config_name&amp;gt; --exp_name=&amp;lt;run_name&amp;gt; --save_interval &amp;lt;interval&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Precision Settings&lt;/h3&gt; 
&lt;p&gt;JAX and PyTorch implementations handle precision as follows:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;JAX:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Inference: most weights and computations in bfloat16, with a few computations in float32 for stability&lt;/li&gt; 
 &lt;li&gt;Training: defaults to mixed precision: weights and gradients in float32, (most) activations and computations in bfloat16. You can change to full float32 training by setting &lt;code&gt;dtype&lt;/code&gt; to float32 in the config.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;PyTorch:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Inference: matches JAX -- most weights and computations in bfloat16, with a few weights converted to float32 for stability&lt;/li&gt; 
 &lt;li&gt;Training: supports either full bfloat16 (default) or full float32. You can change it by setting &lt;code&gt;pytorch_training_precision&lt;/code&gt; in the config. bfloat16 uses less memory but exhibits higher losses compared to float32. Mixed precision is not yet supported.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;With torch.compile, inference speed is comparable between JAX and PyTorch.&lt;/p&gt; 
&lt;h2&gt;Troubleshooting&lt;/h2&gt; 
&lt;p&gt;We will collect common issues and their solutions here. If you encounter an issue, please check here first. If you can't find a solution, please file an issue on the repo (see &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/CONTRIBUTING.md"&gt;here&lt;/a&gt; for guidelines).&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Issue&lt;/th&gt; 
   &lt;th&gt;Resolution&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;uv sync&lt;/code&gt; fails with dependency conflicts&lt;/td&gt; 
   &lt;td&gt;Try removing the virtual environment directory (&lt;code&gt;rm -rf .venv&lt;/code&gt;) and running &lt;code&gt;uv sync&lt;/code&gt; again. If issues persist, check that you have the latest version of &lt;code&gt;uv&lt;/code&gt; installed (&lt;code&gt;uv self update&lt;/code&gt;).&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Training runs out of GPU memory&lt;/td&gt; 
   &lt;td&gt;Make sure you set &lt;code&gt;XLA_PYTHON_CLIENT_MEM_FRACTION=0.9&lt;/code&gt; (or higher) before running training to allow JAX to use more GPU memory. You can also use &lt;code&gt;--fsdp-devices &amp;lt;n&amp;gt;&lt;/code&gt; where &lt;code&gt;&amp;lt;n&amp;gt;&lt;/code&gt; is your number of GPUs, to enable &lt;a href="https://engineering.fb.com/2021/07/15/open-source/fsdp/"&gt;fully-sharded data parallelism&lt;/a&gt;, which reduces memory usage in exchange for slower training (the amount of slowdown depends on your particular setup). If you are still running out of memory, you may way to consider disabling EMA.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Policy server connection errors&lt;/td&gt; 
   &lt;td&gt;Check that the server is running and listening on the expected port. Verify network connectivity and firewall settings between client and server.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Missing norm stats error when training&lt;/td&gt; 
   &lt;td&gt;Run &lt;code&gt;scripts/compute_norm_stats.py&lt;/code&gt; with your config name before starting training.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Dataset download fails&lt;/td&gt; 
   &lt;td&gt;Check your internet connection. For HuggingFace datasets, ensure you're logged in (&lt;code&gt;huggingface-cli login&lt;/code&gt;).&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;CUDA/GPU errors&lt;/td&gt; 
   &lt;td&gt;Verify NVIDIA drivers are installed correctly. For Docker, ensure nvidia-container-toolkit is installed. Check GPU compatibility. You do NOT need CUDA libraries installed at a system level --- they will be installed via uv. You may even want to try &lt;em&gt;uninstalling&lt;/em&gt; system CUDA libraries if you run into CUDA issues, since system libraries can sometimes cause conflicts.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Import errors when running examples&lt;/td&gt; 
   &lt;td&gt;Make sure you've installed all dependencies with &lt;code&gt;uv sync&lt;/code&gt;. Some examples may have additional requirements listed in their READMEs.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Action dimensions mismatch&lt;/td&gt; 
   &lt;td&gt;Verify your data processing transforms match the expected input/output dimensions of your robot. Check the action space definitions in your policy classes.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Diverging training loss&lt;/td&gt; 
   &lt;td&gt;Check the &lt;code&gt;q01&lt;/code&gt;, &lt;code&gt;q99&lt;/code&gt;, and &lt;code&gt;std&lt;/code&gt; values in &lt;code&gt;norm_stats.json&lt;/code&gt; for your dataset. Certain dimensions that are rarely used can end up with very small &lt;code&gt;q01&lt;/code&gt;, &lt;code&gt;q99&lt;/code&gt;, or &lt;code&gt;std&lt;/code&gt; values, leading to huge states and actions after normalization. You can manually adjust the norm stats as a workaround.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt;</description>
    </item>
    
    <item>
      <title>PostHog/posthog</title>
      <link>https://github.com/PostHog/posthog</link>
      <description>&lt;p&gt;ğŸ¦” PostHog provides open-source web &amp; product analytics, session recording, feature flagging and A/B testing that you can self-host. Get started - free.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img alt="posthoglogo" src="https://user-images.githubusercontent.com/65415371/205059737-c8a4f836-4889-4654-902e-f302b187b6a0.png" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://posthog.com/contributors"&gt;&lt;img alt="GitHub contributors" src="https://img.shields.io/github/contributors/posthog/posthog" /&gt;&lt;/a&gt; &lt;a href="http://makeapullrequest.com"&gt;&lt;img alt="PRs Welcome" src="https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=shields" /&gt;&lt;/a&gt; &lt;img alt="Docker Pulls" src="https://img.shields.io/docker/pulls/posthog/posthog" /&gt; &lt;a href="https://github.com/PostHog/posthog/commits/master"&gt;&lt;img alt="GitHub commit activity" src="https://img.shields.io/github/commit-activity/m/posthog/posthog" /&gt; &lt;/a&gt; &lt;a href="https://github.com/PostHog/posthog/issues?q=is%3Aissue%20state%3Aclosed"&gt;&lt;img alt="GitHub closed issues" src="https://img.shields.io/github/issues-closed/posthog/posthog" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://posthog.com/docs"&gt;Docs&lt;/a&gt; - &lt;a href="https://posthog.com/community"&gt;Community&lt;/a&gt; - &lt;a href="https://posthog.com/roadmap"&gt;Roadmap&lt;/a&gt; - &lt;a href="https://posthog.com/why"&gt;Why PostHog?&lt;/a&gt; - &lt;a href="https://posthog.com/changelog"&gt;Changelog&lt;/a&gt; - &lt;a href="https://github.com/PostHog/posthog/issues/new?assignees=&amp;amp;labels=bug&amp;amp;template=bug_report.md"&gt;Bug reports&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://www.youtube.com/watch?v=2jQco8hEvTI"&gt; &lt;img src="https://res.cloudinary.com/dmukukwp6/image/upload/demo_thumb_68d0d8d56d" alt="PostHog Demonstration" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;PostHog is an all-in-one, open source platform for building successful products&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://posthog.com/"&gt;PostHog&lt;/a&gt; provides every tool you need to build a successful product including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://posthog.com/product-analytics"&gt;Product analytics&lt;/a&gt;: Autocapture or manually instrument event-based analytics to understand user behavior and analyze data with visualization or SQL.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://posthog.com/web-analytics"&gt;Web analytics&lt;/a&gt;: Monitor web traffic and user sessions with a GA-like dashboard. Easily monitor conversion, web vitals, and revenue.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://posthog.com/session-replay"&gt;Session replays&lt;/a&gt;: Watch real user sessions of interactions with your website or mobile app to diagnose issues and understand user behavior.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://posthog.com/feature-flags"&gt;Feature flags&lt;/a&gt;: Safely roll out features to select users or cohorts with feature flags.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://posthog.com/experiments"&gt;Experiments&lt;/a&gt;: Test changes and measure their statistical impact on goal metrics. Set up experiments with no-code too.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://posthog.com/error-tracking"&gt;Error tracking&lt;/a&gt;: Track errors, get alerts, and resolve issues to improve your product.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://posthog.com/surveys"&gt;Surveys&lt;/a&gt;: Ask anything with our collection of no-code survey templates, or build custom surveys with our survey builder.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://posthog.com/data-warehouse"&gt;Data warehouse&lt;/a&gt;: Sync data from external tools like Stripe, Hubspot, your data warehouse, and more. Query it alongside your product data.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://posthog.com/cdp"&gt;Data pipelines&lt;/a&gt;: Run custom filters and transformations on your incoming data. Send it to 25+ tools or any webhook in real time or batch export large amounts to your warehouse.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://posthog.com/docs/llm-analytics"&gt;LLM analytics&lt;/a&gt;: Capture traces, generations, latency, and cost for your LLM-powered app.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Best of all, all of this is free to use with a &lt;a href="https://posthog.com/pricing"&gt;generous monthly free tier&lt;/a&gt; for each product. Get started by signing up for &lt;a href="https://us.posthog.com/signup"&gt;PostHog Cloud US&lt;/a&gt; or &lt;a href="https://eu.posthog.com/signup"&gt;PostHog Cloud EU&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PostHog/posthog/master/#posthog-is-an-all-in-one-open-source-platform-for-building-successful-products"&gt;PostHog is an all-in-one, open source platform for building successful products&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PostHog/posthog/master/#table-of-contents"&gt;Table of Contents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PostHog/posthog/master/#getting-started-with-posthog"&gt;Getting started with PostHog&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PostHog/posthog/master/#posthog-cloud-recommended"&gt;PostHog Cloud (Recommended)&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PostHog/posthog/master/#self-hosting-the-open-source-hobby-deploy-advanced"&gt;Self-hosting the open-source hobby deploy (Advanced)&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PostHog/posthog/master/#setting-up-posthog"&gt;Setting up PostHog&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PostHog/posthog/master/#learning-more-about-posthog"&gt;Learning more about PostHog&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PostHog/posthog/master/#contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PostHog/posthog/master/#open-source-vs-paid"&gt;Open-source vs. paid&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PostHog/posthog/master/#were-hiring"&gt;Weâ€™re hiring!&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Getting started with PostHog&lt;/h2&gt; 
&lt;h3&gt;PostHog Cloud (Recommended)&lt;/h3&gt; 
&lt;p&gt;The fastest and most reliable way to get started with PostHog is signing up for free to&amp;nbsp;&lt;a href="https://us.posthog.com/signup"&gt;PostHog Cloud&lt;/a&gt; or &lt;a href="https://eu.posthog.com/signup"&gt;PostHog Cloud EU&lt;/a&gt;. Your first 1 million events, 5k recordings, 1M flag requests, 100k exceptions, and 1500 survey responses are free every month, after which you pay based on usage.&lt;/p&gt; 
&lt;h3&gt;Self-hosting the open-source hobby deploy (Advanced)&lt;/h3&gt; 
&lt;p&gt;If you want to self-host PostHog, you can deploy a hobby instance in one line on Linux with Docker (recommended 4GB memory):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/posthog/posthog/HEAD/bin/deploy-hobby)"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Open source deployments should scale to approximately 100k events per month, after which we recommend &lt;a href="https://posthog.com/docs/migrate/migrate-to-cloud"&gt;migrating to a PostHog Cloud&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We &lt;em&gt;do not&lt;/em&gt; provide customer support or offer guarantees for open source deployments. See our &lt;a href="https://posthog.com/docs/self-host"&gt;self-hosting docs&lt;/a&gt;, &lt;a href="https://posthog.com/docs/self-host/deploy/troubleshooting"&gt;troubleshooting guide&lt;/a&gt;, and &lt;a href="https://posthog.com/docs/self-host/open-source/disclaimer"&gt;disclaimer&lt;/a&gt; for more info.&lt;/p&gt; 
&lt;h2&gt;Setting up PostHog&lt;/h2&gt; 
&lt;p&gt;Once you've got a PostHog instance, you can set it up by installing our &lt;a href="https://posthog.com/docs/getting-started/install?tab=snippet"&gt;JavaScript web snippet&lt;/a&gt;, one of &lt;a href="https://posthog.com/docs/getting-started/install?tab=sdks"&gt;our SDKs&lt;/a&gt;, or by &lt;a href="https://posthog.com/docs/getting-started/install?tab=api"&gt;using our API&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We have SDKs and libraries for popular languages and frameworks like:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Frontend&lt;/th&gt; 
   &lt;th&gt;Mobile&lt;/th&gt; 
   &lt;th&gt;Backend&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://posthog.com/docs/libraries/js"&gt;JavaScript&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://posthog.com/docs/libraries/react-native"&gt;React Native&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://posthog.com/docs/libraries/python"&gt;Python&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://posthog.com/docs/libraries/next-js"&gt;Next.js&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://posthog.com/docs/libraries/android"&gt;Android&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://posthog.com/docs/libraries/node"&gt;Node&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://posthog.com/docs/libraries/react"&gt;React&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://posthog.com/docs/libraries/ios"&gt;iOS&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://posthog.com/docs/libraries/php"&gt;PHP&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://posthog.com/docs/libraries/vue-js"&gt;Vue&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://posthog.com/docs/libraries/flutter"&gt;Flutter&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://posthog.com/docs/libraries/ruby"&gt;Ruby&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Beyond this, we have docs and guides for &lt;a href="https://posthog.com/docs/libraries/go"&gt;Go&lt;/a&gt;, &lt;a href="https://posthog.com/docs/libraries/dotnet"&gt;.NET/C#&lt;/a&gt;, &lt;a href="https://posthog.com/docs/libraries/django"&gt;Django&lt;/a&gt;, &lt;a href="https://posthog.com/docs/libraries/angular"&gt;Angular&lt;/a&gt;, &lt;a href="https://posthog.com/docs/libraries/wordpress"&gt;WordPress&lt;/a&gt;, &lt;a href="https://posthog.com/docs/libraries/webflow"&gt;Webflow&lt;/a&gt;, and more.&lt;/p&gt; 
&lt;p&gt;Once you've installed PostHog, see our &lt;a href="https://posthog.com/docs/product-os"&gt;product docs&lt;/a&gt; for more information on how to set up &lt;a href="https://posthog.com/docs/product-analytics/capture-events"&gt;product analytics&lt;/a&gt;, &lt;a href="https://posthog.com/docs/web-analytics/getting-started"&gt;web analytics&lt;/a&gt;, &lt;a href="https://posthog.com/docs/session-replay/how-to-watch-recordings"&gt;session replays&lt;/a&gt;, &lt;a href="https://posthog.com/docs/feature-flags/creating-feature-flags"&gt;feature flags&lt;/a&gt;, &lt;a href="https://posthog.com/docs/experiments/creating-an-experiment"&gt;experiments&lt;/a&gt;, &lt;a href="https://posthog.com/docs/error-tracking/installation#setting-up-exception-autocapture"&gt;error tracking&lt;/a&gt;, &lt;a href="https://posthog.com/docs/surveys/installation"&gt;surveys&lt;/a&gt;, &lt;a href="https://posthog.com/docs/cdp/sources"&gt;data warehouse&lt;/a&gt;, and more.&lt;/p&gt; 
&lt;h2&gt;Learning more about PostHog&lt;/h2&gt; 
&lt;p&gt;Our code isn't the only thing that's open source ğŸ˜³. We also open source our &lt;a href="https://posthog.com/handbook"&gt;company handbook&lt;/a&gt; which details our &lt;a href="https://posthog.com/handbook/why-does-posthog-exist"&gt;strategy&lt;/a&gt;, &lt;a href="https://posthog.com/handbook/company/culture"&gt;ways of working&lt;/a&gt;, and &lt;a href="https://posthog.com/handbook/team-structure"&gt;processes&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Curious about how to make the most of PostHog? We wrote a guide to &lt;a href="https://posthog.com/docs/new-to-posthog/getting-hogpilled"&gt;winning with PostHog&lt;/a&gt; which walks you through the basics of &lt;a href="https://posthog.com/docs/new-to-posthog/activation"&gt;measuring activation&lt;/a&gt;, &lt;a href="https://posthog.com/docs/new-to-posthog/retention"&gt;tracking retention&lt;/a&gt;, and &lt;a href="https://posthog.com/docs/new-to-posthog/revenue"&gt;capturing revenue&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We &amp;lt;3 contributions big and small:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Vote on features or get early access to beta functionality in our &lt;a href="https://posthog.com/roadmap"&gt;roadmap&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Open a PR (see our instructions on &lt;a href="https://posthog.com/handbook/engineering/developing-locally"&gt;developing PostHog locally&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;Submit a &lt;a href="https://github.com/PostHog/posthog/issues/new?assignees=&amp;amp;labels=enhancement%2C+feature&amp;amp;template=feature_request.md"&gt;feature request&lt;/a&gt; or &lt;a href="https://github.com/PostHog/posthog/issues/new?assignees=&amp;amp;labels=bug&amp;amp;template=bug_report.md"&gt;bug report&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Open-source vs. paid&lt;/h2&gt; 
&lt;p&gt;This repo is available under the &lt;a href="https://github.com/PostHog/posthog/raw/master/LICENSE"&gt;MIT expat license&lt;/a&gt;, except for the &lt;code&gt;ee&lt;/code&gt; directory (which has its &lt;a href="https://github.com/PostHog/posthog/raw/master/ee/LICENSE"&gt;license here&lt;/a&gt;) if applicable.&lt;/p&gt; 
&lt;p&gt;Need &lt;em&gt;absolutely ğŸ’¯% FOSS&lt;/em&gt;? Check out our &lt;a href="https://github.com/PostHog/posthog-foss"&gt;posthog-foss&lt;/a&gt; repository, which is purged of all proprietary code and features.&lt;/p&gt; 
&lt;p&gt;The pricing for our paid plan is completely transparent and available on &lt;a href="https://posthog.com/pricing"&gt;our pricing page&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Weâ€™re hiring!&lt;/h2&gt; 
&lt;img src="https://res.cloudinary.com/dmukukwp6/image/upload/v1/posthog.com/src/components/Home/images/mission-control-hog" alt="Hedgehog working on a Mission Control Center" width="350px" /&gt; 
&lt;p&gt;Hey! If you're reading this, you've proven yourself as a dedicated README reader.&lt;/p&gt; 
&lt;p&gt;You might also make a great addition to our team. We're growing fast &lt;a href="https://posthog.com/careers"&gt;and would love for you to join us&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>QuentinFuxa/WhisperLiveKit</title>
      <link>https://github.com/QuentinFuxa/WhisperLiveKit</link>
      <description>&lt;p&gt;Real-time &amp; local speech-to-text, translation, and speaker diarization. With server &amp; web UI.&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt;WhisperLiveKit&lt;/h1&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/QuentinFuxa/WhisperLiveKit/refs/heads/main/demo.png" alt="WhisperLiveKit Demo" width="730" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt;&lt;b&gt;Real-time, Fully Local Speech-to-Text with Speaker Identification&lt;/b&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://pypi.org/project/whisperlivekit/"&gt;&lt;img alt="PyPI Version" src="https://img.shields.io/pypi/v/whisperlivekit?color=g" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/whisperlivekit"&gt;&lt;img alt="PyPI Downloads" src="https://static.pepy.tech/personalized-badge/whisperlivekit?period=total&amp;amp;units=international_system&amp;amp;left_color=grey&amp;amp;right_color=brightgreen&amp;amp;left_text=installations" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/whisperlivekit/"&gt;&lt;img alt="Python Versions" src="https://img.shields.io/badge/python-3.9--3.15-dark_green" /&gt;&lt;/a&gt; &lt;a href="https://github.com/QuentinFuxa/WhisperLiveKit/raw/main/LICENSE"&gt;&lt;img alt="License" src="https://img.shields.io/badge/License-MIT/Dual Licensed-dark_green" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;Real-time speech transcription directly to your browser, with a ready-to-use backend+server and a simple frontend. âœ¨&lt;/p&gt; 
&lt;h4&gt;Powered by Leading Research:&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ufal/SimulStreaming"&gt;SimulStreaming&lt;/a&gt; (SOTA 2025) - Ultra-low latency transcription with AlignAtt policy&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2207.04672"&gt;NLLB&lt;/a&gt;, (&lt;a href="https://huggingface.co/entai2965/nllb-200-distilled-600M-ctranslate2"&gt;distilled&lt;/a&gt;) (2024) - Translation to more than 100 languages.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ufal/whisper_streaming"&gt;WhisperStreaming&lt;/a&gt; (SOTA 2023) - Low latency transcription with LocalAgreement policy&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2507.18446"&gt;Streaming Sortformer&lt;/a&gt; (SOTA 2025) - Advanced real-time speaker diarization&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/juanmc2005/diart"&gt;Diart&lt;/a&gt; (SOTA 2021) - Real-time speaker diarization&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/snakers4/silero-vad"&gt;Silero VAD&lt;/a&gt; (2024) - Enterprise-grade Voice Activity Detection&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Why not just run a simple Whisper model on every audio batch?&lt;/strong&gt; Whisper is designed for complete utterances, not real-time chunks. Processing small segments loses context, cuts off words mid-syllable, and produces poor transcription. WhisperLiveKit uses state-of-the-art simultaneous speech research for intelligent buffering and incremental processing.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Architecture&lt;/h3&gt; 
&lt;img alt="Architecture" src="https://raw.githubusercontent.com/QuentinFuxa/WhisperLiveKit/refs/heads/main/architecture.png" /&gt; 
&lt;p&gt;&lt;em&gt;The backend supports multiple concurrent users. Voice Activity Detection reduces overhead when no voice is detected.&lt;/em&gt;&lt;/p&gt; 
&lt;h3&gt;Installation &amp;amp; Quick Start&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install whisperlivekit
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;You can also clone the repo and &lt;code&gt;pip install -e .&lt;/code&gt; for the latest version.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;FFmpeg is required&lt;/strong&gt; and must be installed before using WhisperLiveKit&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;OS&lt;/th&gt; 
    &lt;th&gt;How to install&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Ubuntu/Debian&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;sudo apt install ffmpeg&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MacOS&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;brew install ffmpeg&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Windows&lt;/td&gt; 
    &lt;td&gt;Download .exe from &lt;a href="https://ffmpeg.org/download.html"&gt;https://ffmpeg.org/download.html&lt;/a&gt; and add to PATH&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Quick Start&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Start the transcription server:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;whisperlivekit-server --model base --language en
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Open your browser&lt;/strong&gt; and navigate to &lt;code&gt;http://localhost:8000&lt;/code&gt;. Start speaking and watch your words appear in real-time!&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;ul&gt; 
  &lt;li&gt;See &lt;a href="https://github.com/QuentinFuxa/WhisperLiveKit/raw/main/whisperlivekit/simul_whisper/whisper/tokenizer.py"&gt;tokenizer.py&lt;/a&gt; for the list of all available languages.&lt;/li&gt; 
  &lt;li&gt;For HTTPS requirements, see the &lt;strong&gt;Parameters&lt;/strong&gt; section for SSL configuration options.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Optional Dependencies&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Optional&lt;/th&gt; 
   &lt;th&gt;&lt;code&gt;pip install&lt;/code&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Speaker diarization with Sortformer&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;git+https://github.com/NVIDIA/NeMo.git@main#egg=nemo_toolkit[asr]&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Apple Silicon optimized backend&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;mlx-whisper&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;NLLB Translation&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;huggingface_hub&lt;/code&gt; &amp;amp; &lt;code&gt;transformers&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;em&gt;[Not recommanded]&lt;/em&gt; Speaker diarization with Diart&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;diart&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;em&gt;[Not recommanded]&lt;/em&gt; Original Whisper backend&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;whisper&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;em&gt;[Not recommanded]&lt;/em&gt; Improved timestamps backend&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;whisper-timestamped&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenAI API backend&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;openai&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;See &lt;strong&gt;Parameters &amp;amp; Configuration&lt;/strong&gt; below on how to use them.&lt;/p&gt; 
&lt;h3&gt;Usage Examples&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Command-line Interface&lt;/strong&gt;: Start the transcription server with various options:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Use better model than default (small)
whisperlivekit-server --model large-v3

# Advanced configuration with diarization and language
whisperlivekit-server --host 0.0.0.0 --port 8000 --model medium --diarization --language fr
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Python API Integration&lt;/strong&gt;: Check &lt;a href="https://github.com/QuentinFuxa/WhisperLiveKit/raw/main/whisperlivekit/basic_server.py"&gt;basic_server&lt;/a&gt; for a more complete example of how to use the functions and classes.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from whisperlivekit import TranscriptionEngine, AudioProcessor, parse_args
from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.responses import HTMLResponse
from contextlib import asynccontextmanager
import asyncio

transcription_engine = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    global transcription_engine
    transcription_engine = TranscriptionEngine(model="medium", diarization=True, lan="en")
    yield

app = FastAPI(lifespan=lifespan)

async def handle_websocket_results(websocket: WebSocket, results_generator):
    async for response in results_generator:
        await websocket.send_json(response)
    await websocket.send_json({"type": "ready_to_stop"})

@app.websocket("/asr")
async def websocket_endpoint(websocket: WebSocket):
    global transcription_engine

    # Create a new AudioProcessor for each connection, passing the shared engine
    audio_processor = AudioProcessor(transcription_engine=transcription_engine)    
    results_generator = await audio_processor.create_tasks()
    results_task = asyncio.create_task(handle_websocket_results(websocket, results_generator))
    await websocket.accept()
    while True:
        message = await websocket.receive_bytes()
        await audio_processor.process_audio(message)        
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Frontend Implementation&lt;/strong&gt;: The package includes an HTML/JavaScript implementation &lt;a href="https://github.com/QuentinFuxa/WhisperLiveKit/raw/main/whisperlivekit/web/live_transcription.html"&gt;here&lt;/a&gt;. You can also import it using &lt;code&gt;from whisperlivekit import get_inline_ui_html&lt;/code&gt; &amp;amp; &lt;code&gt;page = get_inline_ui_html()&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;Parameters &amp;amp; Configuration&lt;/h2&gt; 
&lt;p&gt;An important list of parameters can be changed. But what &lt;em&gt;should&lt;/em&gt; you change?&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;the &lt;code&gt;--model&lt;/code&gt; size. List and recommandations &lt;a href="https://github.com/QuentinFuxa/WhisperLiveKit/raw/main/available_models.md"&gt;here&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;the &lt;code&gt;--language&lt;/code&gt;. List &lt;a href="https://github.com/QuentinFuxa/WhisperLiveKit/raw/main/whisperlivekit/simul_whisper/whisper/tokenizer.py"&gt;here&lt;/a&gt;. If you use &lt;code&gt;auto&lt;/code&gt;, the model attempts to detect the language automatically, but it tends to bias towards English.&lt;/li&gt; 
 &lt;li&gt;the &lt;code&gt;--backend&lt;/code&gt; ? you can switch to &lt;code&gt;--backend faster-whisper&lt;/code&gt; if &lt;code&gt;simulstreaming&lt;/code&gt; does not work correctly or if you prefer to avoid the dual-license requirements.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--warmup-file&lt;/code&gt;, if you have one&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--task translate&lt;/code&gt;, to translate in english&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--host&lt;/code&gt;, &lt;code&gt;--port&lt;/code&gt;, &lt;code&gt;--ssl-certfile&lt;/code&gt;, &lt;code&gt;--ssl-keyfile&lt;/code&gt;, if you set up a server&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--diarization&lt;/code&gt;, if you want to use it.&lt;/li&gt; 
 &lt;li&gt;[BETA] &lt;code&gt;--target-language&lt;/code&gt;, to translate using NLLB. &lt;a href="https://github.com/QuentinFuxa/WhisperLiveKit/raw/main/whisperlivekit/translation/mapping_languages.py"&gt;118 languages available&lt;/a&gt;. If you want to translate to english, you should rather use &lt;code&gt;--task translate&lt;/code&gt;, since Whisper can do it directly.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Full list of parameters :&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Parameter&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Default&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--model&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Whisper model size.&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;small&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--language&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Source language code or &lt;code&gt;auto&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;auto&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--task&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Set to &lt;code&gt;translate&lt;/code&gt; to translate to english&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;transcribe&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--target-language&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;[BETA] Translation language target. Ex: &lt;code&gt;fr&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--backend&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Processing backend&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;simulstreaming&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--min-chunk-size&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Minimum audio chunk size (seconds)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;1.0&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--no-vac&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Disable Voice Activity Controller&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--no-vad&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Disable Voice Activity Detection&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--warmup-file&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Audio file path for model warmup&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;jfk.wav&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--host&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Server host address&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;localhost&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--port&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Server port&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;8000&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--ssl-certfile&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Path to the SSL certificate file (for HTTPS support)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--ssl-keyfile&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Path to the SSL private key file (for HTTPS support)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--pcm-input&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;raw PCM (s16le) data is expected as input and FFmpeg will be bypassed.&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;SimulStreaming backend options&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Default&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--disable-fast-encoder&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Disable Faster Whisper or MLX Whisper backends for the encoder (if installed). Inference can be slower but helpful when GPU memory is limited&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--frame-threshold&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;AlignAtt frame threshold (lower = faster, higher = more accurate)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;25&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--beams&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Number of beams for beam search (1 = greedy decoding)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;1&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--decoder&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Force decoder type (&lt;code&gt;beam&lt;/code&gt; or &lt;code&gt;greedy&lt;/code&gt;)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;auto&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--audio-max-len&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Maximum audio buffer length (seconds)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;30.0&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--audio-min-len&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Minimum audio length to process (seconds)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;0.0&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--cif-ckpt-path&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Path to CIF model for word boundary detection&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--never-fire&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Never truncate incomplete words&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--init-prompt&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Initial prompt for the model&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--static-init-prompt&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Static prompt that doesn't scroll&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--max-context-tokens&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Maximum context tokens&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--model-path&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Direct path to .pt model file. Download it if not found&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;./base.pt&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--preload-model-count&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Optional. Number of models to preload in memory to speed up loading (set up to the expected number of concurrent users)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;1&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;WhisperStreaming backend options&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Default&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--confidence-validation&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Use confidence scores for faster validation&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--buffer_trimming&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Buffer trimming strategy (&lt;code&gt;sentence&lt;/code&gt; or &lt;code&gt;segment&lt;/code&gt;)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;segment&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Diarization options&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Default&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--diarization&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Enable speaker identification&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--diarization-backend&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;diart&lt;/code&gt; or &lt;code&gt;sortformer&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;sortformer&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--disable-punctuation-split&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Disable punctuation based splits. See #214&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--segmentation-model&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Hugging Face model ID for Diart segmentation model. &lt;a href="https://github.com/juanmc2005/diart/tree/main?tab=readme-ov-file#pre-trained-models"&gt;Available models&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pyannote/segmentation-3.0&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--embedding-model&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Hugging Face model ID for Diart embedding model. &lt;a href="https://github.com/juanmc2005/diart/tree/main?tab=readme-ov-file#pre-trained-models"&gt;Available models&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;speechbrain/spkrec-ecapa-voxceleb&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;For diarization using Diart, you need access to pyannote.audio models:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/pyannote/segmentation"&gt;Accept user conditions&lt;/a&gt; for the &lt;code&gt;pyannote/segmentation&lt;/code&gt; model&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/pyannote/segmentation-3.0"&gt;Accept user conditions&lt;/a&gt; for the &lt;code&gt;pyannote/segmentation-3.0&lt;/code&gt; model&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/pyannote/embedding"&gt;Accept user conditions&lt;/a&gt; for the &lt;code&gt;pyannote/embedding&lt;/code&gt; model&lt;/li&gt; 
  &lt;li&gt;Login with HuggingFace: &lt;code&gt;huggingface-cli login&lt;/code&gt;&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;ğŸš€ Deployment Guide&lt;/h3&gt; 
&lt;p&gt;To deploy WhisperLiveKit in production:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Server Setup&lt;/strong&gt;: Install production ASGI server &amp;amp; launch with multiple workers&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install uvicorn gunicorn
gunicorn -k uvicorn.workers.UvicornWorker -w 4 your_app:app
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Frontend&lt;/strong&gt;: Host your customized version of the &lt;code&gt;html&lt;/code&gt; example &amp;amp; ensure WebSocket connection points correctly&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Nginx Configuration&lt;/strong&gt; (recommended for production):&lt;/p&gt; &lt;pre&gt;&lt;code class="language-nginx"&gt;server {
   listen 80;
   server_name your-domain.com;
    location / {
        proxy_pass http://localhost:8000;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        proxy_set_header Host $host;
}}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;HTTPS Support&lt;/strong&gt;: For secure deployments, use "wss://" instead of "ws://" in WebSocket URL&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;ğŸ‹ Docker&lt;/h2&gt; 
&lt;p&gt;Deploy the application easily using Docker with GPU or CPU support.&lt;/p&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Docker installed on your system&lt;/li&gt; 
 &lt;li&gt;For GPU support: NVIDIA Docker runtime installed&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Quick Start&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;With GPU acceleration (recommended):&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker build -t wlk .
docker run --gpus all -p 8000:8000 --name wlk wlk
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;CPU only:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker build -f Dockerfile.cpu -t wlk .
docker run -p 8000:8000 --name wlk wlk
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Advanced Usage&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Custom configuration:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Example with custom model and language
docker run --gpus all -p 8000:8000 --name wlk wlk --model large-v3 --language fr
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Memory Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Large models&lt;/strong&gt;: Ensure your Docker runtime has sufficient memory allocated&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Customization&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--build-arg&lt;/code&gt; Options: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;EXTRAS="whisper-timestamped"&lt;/code&gt; - Add extras to the image's installation (no spaces). Remember to set necessary container options!&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;HF_PRECACHE_DIR="./.cache/"&lt;/code&gt; - Pre-load a model cache for faster first-time start&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;HF_TKN_FILE="./token"&lt;/code&gt; - Add your Hugging Face Hub access token to download gated models&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ”® Use Cases&lt;/h2&gt; 
&lt;p&gt;Capture discussions in real-time for meeting transcription, help hearing-impaired users follow conversations through accessibility tools, transcribe podcasts or videos automatically for content creation, transcribe support calls with speaker identification for customer service...&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>microsoft/qlib</title>
      <link>https://github.com/microsoft/qlib</link>
      <description>&lt;p&gt;Qlib is an AI-oriented Quant investment platform that aims to use AI tech to empower Quant Research, from exploring ideas to implementing productions. Qlib supports diverse ML modeling paradigms, including supervised learning, market dynamics modeling, and RL, and is now equipped with https://github.com/microsoft/RD-Agent to automate R&amp;D process.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="https://pypi.org/project/pyqlib/#files"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/pyqlib.svg?logo=python&amp;amp;logoColor=white" alt="Python Versions" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/pyqlib/#files"&gt;&lt;img src="https://img.shields.io/badge/platform-linux%20%7C%20windows%20%7C%20macos-lightgrey" alt="Platform" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/pyqlib/#history"&gt;&lt;img src="https://img.shields.io/pypi/v/pyqlib" alt="PypI Versions" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/pyqlib/"&gt;&lt;img src="https://github.com/microsoft/qlib/workflows/Upload%20Python%20Package/badge.svg?sanitize=true" alt="Upload Python Package" /&gt;&lt;/a&gt; &lt;a href="https://github.com/microsoft/qlib/actions"&gt;&lt;img src="https://github.com/microsoft/qlib/workflows/Test/badge.svg?branch=main" alt="Github Actions Test Status" /&gt;&lt;/a&gt; &lt;a href="https://qlib.readthedocs.io/en/latest/?badge=latest"&gt;&lt;img src="https://readthedocs.org/projects/qlib/badge/?version=latest" alt="Documentation Status" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/LICENSE"&gt;&lt;img src="https://img.shields.io/pypi/l/pyqlib" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://gitter.im/Microsoft/qlib?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge"&gt;&lt;img src="https://badges.gitter.im/Microsoft/qlib.svg?sanitize=true" alt="Join the chat at https://gitter.im/Microsoft/qlib" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;span&gt;ğŸ“°&lt;/span&gt; &lt;strong&gt;What's NEW!&lt;/strong&gt; &amp;nbsp; &lt;span&gt;ğŸ’–&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;Recent released features&lt;/p&gt; 
&lt;h3&gt;Introducing &lt;a href="https://github.com/microsoft/RD-Agent"&gt;&lt;img src="https://raw.githubusercontent.com/microsoft/qlib/main/docs/_static/img/rdagent_logo.png" alt="RD_Agent" style="height: 2em" /&gt;&lt;/a&gt;: LLM-Based Autonomous Evolving Agents for Industrial Data-Driven R&amp;amp;D&lt;/h3&gt; 
&lt;p&gt;We are excited to announce the release of &lt;strong&gt;RD-Agent&lt;/strong&gt;ğŸ“¢, a powerful tool that supports automated factor mining and model optimization in quant investment R&amp;amp;D.&lt;/p&gt; 
&lt;p&gt;RD-Agent is now available on &lt;a href="https://github.com/microsoft/RD-Agent"&gt;GitHub&lt;/a&gt;, and we welcome your starğŸŒŸ!&lt;/p&gt; 
&lt;p&gt;To learn more, please visit our &lt;a href="https://rdagent.azurewebsites.net/"&gt;â™¾ï¸Demo page&lt;/a&gt;. Here, you will find demo videos in both English and Chinese to help you better understand the scenario and usage of RD-Agent.&lt;/p&gt; 
&lt;p&gt;We have prepared several demo videos for you:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Scenario&lt;/th&gt; 
   &lt;th&gt;Demo video (English)&lt;/th&gt; 
   &lt;th&gt;Demo video (ä¸­æ–‡)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Quant Factor Mining&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://rdagent.azurewebsites.net/factor_loop?lang=en"&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://rdagent.azurewebsites.net/factor_loop?lang=zh"&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Quant Factor Mining from reports&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://rdagent.azurewebsites.net/report_factor?lang=en"&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://rdagent.azurewebsites.net/report_factor?lang=zh"&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Quant Model Optimization&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://rdagent.azurewebsites.net/model_loop?lang=en"&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://rdagent.azurewebsites.net/model_loop?lang=zh"&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ƒ&lt;strong&gt;Paper&lt;/strong&gt;: &lt;a href="https://arxiv.org/abs/2505.15155"&gt;R&amp;amp;D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ‘¾&lt;strong&gt;Code&lt;/strong&gt;: &lt;a href="https://github.com/microsoft/RD-Agent/"&gt;https://github.com/microsoft/RD-Agent/&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-BibTeX"&gt;@misc{li2025rdagentquant,
    title={R\&amp;amp;D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization},
    author={Yuante Li and Xu Yang and Xiao Yang and Minrui Xu and Xisen Wang and Weiqing Liu and Jiang Bian},
    year={2025},
    eprint={2505.15155},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/3198bc10-47ba-4ee0-8a8e-46d5ce44f45d" alt="image" /&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Feature&lt;/th&gt; 
   &lt;th&gt;Status&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/abs/2505.15155"&gt;R&amp;amp;D-Agent-Quant&lt;/a&gt; Published&lt;/td&gt; 
   &lt;td&gt;Apply R&amp;amp;D-Agent to Qlib for quant trading&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BPQP for End-to-end learning&lt;/td&gt; 
   &lt;td&gt;ğŸ“ˆComing soon!(&lt;a href="https://github.com/microsoft/qlib/pull/1863"&gt;Under review&lt;/a&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ğŸ”¥LLM-driven Auto Quant FactoryğŸ”¥&lt;/td&gt; 
   &lt;td&gt;ğŸš€ Released in &lt;a href="https://github.com/microsoft/RD-Agent"&gt;â™¾ï¸RD-Agent&lt;/a&gt; on Aug 8, 2024&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;KRNN and Sandwich models&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;ğŸ“ˆ&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/1414/"&gt;Released&lt;/a&gt; on May 26, 2023&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Release Qlib v0.9.0&lt;/td&gt; 
   &lt;td&gt;&lt;img alt="octocat" src="https://github.githubassets.com/images/icons/emoji/octocat.png?v8" /&gt;) &lt;a href="https://github.com/microsoft/qlib/releases/tag/v0.9.0"&gt;Released&lt;/a&gt; on Dec 9, 2022&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;RL Learning Framework&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;ğŸ”¨&lt;/span&gt; &lt;span&gt;ğŸ“ˆ&lt;/span&gt; Released on Nov 10, 2022. &lt;a href="https://github.com/microsoft/qlib/pull/1332"&gt;#1332&lt;/a&gt;, &lt;a href="https://github.com/microsoft/qlib/pull/1322"&gt;#1322&lt;/a&gt;, &lt;a href="https://github.com/microsoft/qlib/pull/1316"&gt;#1316&lt;/a&gt;,&lt;a href="https://github.com/microsoft/qlib/pull/1299"&gt;#1299&lt;/a&gt;,&lt;a href="https://github.com/microsoft/qlib/pull/1263"&gt;#1263&lt;/a&gt;, &lt;a href="https://github.com/microsoft/qlib/pull/1244"&gt;#1244&lt;/a&gt;, &lt;a href="https://github.com/microsoft/qlib/pull/1169"&gt;#1169&lt;/a&gt;, &lt;a href="https://github.com/microsoft/qlib/pull/1125"&gt;#1125&lt;/a&gt;, &lt;a href="https://github.com/microsoft/qlib/pull/1076"&gt;#1076&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;HIST and IGMTF models&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;ğŸ“ˆ&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/1040"&gt;Released&lt;/a&gt; on Apr 10, 2022&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qlib &lt;a href="https://github.com/microsoft/qlib/tree/main/examples/tutorial"&gt;notebook tutorial&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ğŸ“– &lt;a href="https://github.com/microsoft/qlib/pull/1037"&gt;Released&lt;/a&gt; on Apr 7, 2022&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ibovespa index data&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;ğŸš&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/990"&gt;Released&lt;/a&gt; on Apr 6, 2022&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Point-in-Time database&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;ğŸ”¨&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/343"&gt;Released&lt;/a&gt; on Mar 10, 2022&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Arctic Provider Backend &amp;amp; Orderbook data example&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;ğŸ”¨&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/744"&gt;Released&lt;/a&gt; on Jan 17, 2022&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Meta-Learning-based framework &amp;amp; DDG-DA&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;ğŸ“ˆ&lt;/span&gt; &lt;span&gt;ğŸ”¨&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/743"&gt;Released&lt;/a&gt; on Jan 10, 2022&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Planning-based portfolio optimization&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;ğŸ”¨&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/754"&gt;Released&lt;/a&gt; on Dec 28, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Release Qlib v0.8.0&lt;/td&gt; 
   &lt;td&gt;&lt;img alt="octocat" src="https://github.githubassets.com/images/icons/emoji/octocat.png?v8" /&gt;) &lt;a href="https://github.com/microsoft/qlib/releases/tag/v0.8.0"&gt;Released&lt;/a&gt; on Dec 8, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ADD model&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;ğŸ“ˆ&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/704"&gt;Released&lt;/a&gt; on Nov 22, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ADARNN model&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;ğŸ“ˆ&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/689"&gt;Released&lt;/a&gt; on Nov 14, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;TCN model&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;ğŸ“ˆ&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/668"&gt;Released&lt;/a&gt; on Nov 4, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Nested Decision Framework&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;ğŸ”¨&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/438"&gt;Released&lt;/a&gt; on Oct 1, 2021. &lt;a href="https://github.com/microsoft/qlib/raw/main/examples/nested_decision_execution/workflow.py"&gt;Example&lt;/a&gt; and &lt;a href="https://qlib.readthedocs.io/en/latest/component/highfreq.html"&gt;Doc&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Temporal Routing Adaptor (TRA)&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;ğŸ“ˆ&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/531"&gt;Released&lt;/a&gt; on July 30, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Transformer &amp;amp; Localformer&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;ğŸ“ˆ&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/508"&gt;Released&lt;/a&gt; on July 22, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Release Qlib v0.7.0&lt;/td&gt; 
   &lt;td&gt;&lt;img alt="octocat" src="https://github.githubassets.com/images/icons/emoji/octocat.png?v8" /&gt;) &lt;a href="https://github.com/microsoft/qlib/releases/tag/v0.7.0"&gt;Released&lt;/a&gt; on July 12, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;TCTS Model&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;ğŸ“ˆ&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/491"&gt;Released&lt;/a&gt; on July 1, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Online serving and automatic model rolling&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;ğŸ”¨&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/290"&gt;Released&lt;/a&gt; on May 17, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;DoubleEnsemble Model&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;ğŸ“ˆ&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/286"&gt;Released&lt;/a&gt; on Mar 2, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;High-frequency data processing example&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;ğŸ”¨&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/257"&gt;Released&lt;/a&gt; on Feb 5, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;High-frequency trading example&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;ğŸ“ˆ&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/227"&gt;Part of code released&lt;/a&gt; on Jan 28, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;High-frequency data(1min)&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;ğŸš&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/221"&gt;Released&lt;/a&gt; on Jan 27, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Tabnet Model&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;ğŸ“ˆ&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/205"&gt;Released&lt;/a&gt; on Jan 22, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Features released before 2021 are not listed here.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/qlib/main/docs/_static/img/logo/1.png" /&gt; &lt;/p&gt; 
&lt;p&gt;Qlib is an open-source, AI-oriented quantitative investment platform that aims to realize the potential, empower research, and create value using AI technologies in quantitative investment, from exploring ideas to implementing productions. Qlib supports diverse machine learning modeling paradigms, including supervised learning, market dynamics modeling, and reinforcement learning.&lt;/p&gt; 
&lt;p&gt;An increasing number of SOTA Quant research works/papers in diverse paradigms are being released in Qlib to collaboratively solve key challenges in quantitative investment. For example, 1) using supervised learning to mine the market's complex non-linear patterns from rich and heterogeneous financial data, 2) modeling the dynamic nature of the financial market using adaptive concept drift technology, and 3) using reinforcement learning to model continuous investment decisions and assist investors in optimizing their trading strategies.&lt;/p&gt; 
&lt;p&gt;It contains the full ML pipeline of data processing, model training, back-testing; and covers the entire chain of quantitative investment: alpha seeking, risk modeling, portfolio optimization, and order execution. For more details, please refer to our paper &lt;a href="https://arxiv.org/abs/2009.11189"&gt;"Qlib: An AI-oriented Quantitative Investment Platform"&lt;/a&gt;.&lt;/p&gt; 
&lt;table&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Frameworks, Tutorial, Data &amp;amp; DevOps&lt;/th&gt; 
   &lt;th&gt;Main Challenges &amp;amp; Solutions in Quant Research&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt; &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#plans"&gt;&lt;strong&gt;Plans&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#framework-of-qlib"&gt;Framework of Qlib&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#quick-start"&gt;Quick Start&lt;/a&gt;&lt;/li&gt; 
    &lt;ul dir="auto"&gt; 
     &lt;li type="circle"&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#installation"&gt;Installation&lt;/a&gt; &lt;/li&gt; 
     &lt;li type="circle"&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#data-preparation"&gt;Data Preparation&lt;/a&gt;&lt;/li&gt; 
     &lt;li type="circle"&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#auto-quant-research-workflow"&gt;Auto Quant Research Workflow&lt;/a&gt;&lt;/li&gt; 
     &lt;li type="circle"&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#building-customized-quant-research-workflow-by-code"&gt;Building Customized Quant Research Workflow by Code&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt; &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#quant-dataset-zoo"&gt;&lt;strong&gt;Quant Dataset Zoo&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#learning-framework"&gt;Learning Framework&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#more-about-qlib"&gt;More About Qlib&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#offline-mode-and-online-mode"&gt;Offline Mode and Online Mode&lt;/a&gt; 
     &lt;ul&gt; 
      &lt;li type="circle"&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#performance-of-qlib-data-server"&gt;Performance of Qlib Data Server&lt;/a&gt;&lt;/li&gt;
     &lt;/ul&gt; &lt;/li&gt;&lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#related-reports"&gt;Related Reports&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#contact-us"&gt;Contact Us&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; &lt;/td&gt; 
   &lt;td valign="baseline"&gt; &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#main-challenges--solutions-in-quant-research"&gt;Main Challenges &amp;amp; Solutions in Quant Research&lt;/a&gt; 
     &lt;ul&gt; 
      &lt;li type="circle"&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#forecasting-finding-valuable-signalspatterns"&gt;Forecasting: Finding Valuable Signals/Patterns&lt;/a&gt; 
       &lt;ul&gt; 
        &lt;li type="disc"&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#quant-model-paper-zoo"&gt;&lt;strong&gt;Quant Model (Paper) Zoo&lt;/strong&gt;&lt;/a&gt; 
         &lt;ul&gt; 
          &lt;li type="circle"&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#run-a-single-model"&gt;Run a Single Model&lt;/a&gt;&lt;/li&gt; 
          &lt;li type="circle"&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#run-multiple-models"&gt;Run Multiple Models&lt;/a&gt;&lt;/li&gt; 
         &lt;/ul&gt; &lt;/li&gt; 
       &lt;/ul&gt; &lt;/li&gt; 
      &lt;li type="circle"&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#adapting-to-market-dynamics"&gt;Adapting to Market Dynamics&lt;/a&gt;&lt;/li&gt; 
      &lt;li type="circle"&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#reinforcement-learning-modeling-continuous-decisions"&gt;Reinforcement Learning: modeling continuous decisions&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;Plans&lt;/h1&gt; 
&lt;p&gt;New features under development(order by estimated release time). Your feedbacks about the features are very important.&lt;/p&gt; 
&lt;!-- | Feature                        | Status      | --&gt; 
&lt;!-- | --                      | ------    | --&gt; 
&lt;h1&gt;Framework of Qlib&lt;/h1&gt; 
&lt;div style="align: center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/microsoft/qlib/main/docs/_static/img/framework-abstract.jpg" /&gt; 
&lt;/div&gt; 
&lt;p&gt;The high-level framework of Qlib can be found above(users can find the &lt;a href="https://qlib.readthedocs.io/en/latest/introduction/introduction.html#framework"&gt;detailed framework&lt;/a&gt; of Qlib's design when getting into nitty gritty). The components are designed as loose-coupled modules, and each component could be used stand-alone.&lt;/p&gt; 
&lt;p&gt;Qlib provides a strong infrastructure to support Quant research. &lt;a href="https://qlib.readthedocs.io/en/latest/component/data.html"&gt;Data&lt;/a&gt; is always an important part. A strong learning framework is designed to support diverse learning paradigms (e.g. &lt;a href="https://qlib.readthedocs.io/en/latest/component/rl.html"&gt;reinforcement learning&lt;/a&gt;, &lt;a href="https://qlib.readthedocs.io/en/latest/component/workflow.html#model-section"&gt;supervised learning&lt;/a&gt;) and patterns at different levels(e.g. &lt;a href="https://qlib.readthedocs.io/en/latest/component/meta.html"&gt;market dynamic modeling&lt;/a&gt;). By modeling the market, &lt;a href="https://qlib.readthedocs.io/en/latest/component/strategy.html"&gt;trading strategies&lt;/a&gt; will generate trade decisions that will be executed. Multiple trading strategies and executors in different levels or granularities can be &lt;a href="https://qlib.readthedocs.io/en/latest/component/highfreq.html"&gt;nested to be optimized and run together&lt;/a&gt;. At last, a comprehensive &lt;a href="https://qlib.readthedocs.io/en/latest/component/report.html"&gt;analysis&lt;/a&gt; will be provided and the model can be &lt;a href="https://qlib.readthedocs.io/en/latest/component/online.html"&gt;served online&lt;/a&gt; in a low cost.&lt;/p&gt; 
&lt;h1&gt;Quick Start&lt;/h1&gt; 
&lt;p&gt;This quick start guide tries to demonstrate&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;It's very easy to build a complete Quant research workflow and try your ideas with &lt;em&gt;Qlib&lt;/em&gt;.&lt;/li&gt; 
 &lt;li&gt;Though with &lt;em&gt;public data&lt;/em&gt; and &lt;em&gt;simple models&lt;/em&gt;, machine learning technologies &lt;strong&gt;work very well&lt;/strong&gt; in practical Quant investment.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Here is a quick &lt;strong&gt;&lt;a href="https://terminalizer.com/view/3f24561a4470"&gt;demo&lt;/a&gt;&lt;/strong&gt; shows how to install &lt;code&gt;Qlib&lt;/code&gt;, and run LightGBM with &lt;code&gt;qrun&lt;/code&gt;. &lt;strong&gt;But&lt;/strong&gt;, please make sure you have already prepared the data following the &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#data-preparation"&gt;instruction&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;This table demonstrates the supported Python version of &lt;code&gt;Qlib&lt;/code&gt;:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;install with pip&lt;/th&gt; 
   &lt;th align="center"&gt;install from source&lt;/th&gt; 
   &lt;th align="center"&gt;plot&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Python 3.8&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;âœ”&lt;/span&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;âœ”&lt;/span&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;âœ”&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Python 3.9&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;âœ”&lt;/span&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;âœ”&lt;/span&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;âœ”&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Python 3.10&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;âœ”&lt;/span&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;âœ”&lt;/span&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;âœ”&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Python 3.11&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;âœ”&lt;/span&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;âœ”&lt;/span&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;âœ”&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Python 3.12&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;âœ”&lt;/span&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;âœ”&lt;/span&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;âœ”&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Conda&lt;/strong&gt; is suggested for managing your Python environment. In some cases, using Python outside of a &lt;code&gt;conda&lt;/code&gt; environment may result in missing header files, causing the installation failure of certain packages.&lt;/li&gt; 
 &lt;li&gt;Please pay attention that installing cython in Python 3.6 will raise some error when installing &lt;code&gt;Qlib&lt;/code&gt; from source. If users use Python 3.6 on their machines, it is recommended to &lt;em&gt;upgrade&lt;/em&gt; Python to version 3.8 or higher, or use &lt;code&gt;conda&lt;/code&gt;'s Python to install &lt;code&gt;Qlib&lt;/code&gt; from source.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Install with pip&lt;/h3&gt; 
&lt;p&gt;Users can easily install &lt;code&gt;Qlib&lt;/code&gt; by pip according to the following command.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;  pip install pyqlib
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: pip will install the latest stable qlib. However, the main branch of qlib is in active development. If you want to test the latest scripts or functions in the main branch. Please install qlib with the methods below.&lt;/p&gt; 
&lt;h3&gt;Install from source&lt;/h3&gt; 
&lt;p&gt;Also, users can install the latest dev version &lt;code&gt;Qlib&lt;/code&gt; by the source code according to the following steps:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Before installing &lt;code&gt;Qlib&lt;/code&gt; from source, users need to install some dependencies:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install numpy
pip install --upgrade cython
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Clone the repository and install &lt;code&gt;Qlib&lt;/code&gt; as follows.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/microsoft/qlib.git &amp;amp;&amp;amp; cd qlib
pip install .  # `pip install -e .[dev]` is recommended for development. check details in docs/developer/code_standard_and_dev_guide.rst
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Tips&lt;/strong&gt;: If you fail to install &lt;code&gt;Qlib&lt;/code&gt; or run the examples in your environment, comparing your steps and the &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/.github/workflows/test_qlib_from_source.yml"&gt;CI workflow&lt;/a&gt; may help you find the problem.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Tips for Mac&lt;/strong&gt;: If you are using Mac with M1, you might encounter issues in building the wheel for LightGBM, which is due to missing dependencies from OpenMP. To solve the problem, install openmp first with &lt;code&gt;brew install libomp&lt;/code&gt; and then run &lt;code&gt;pip install .&lt;/code&gt; to build it successfully.&lt;/p&gt; 
&lt;h2&gt;Data Preparation&lt;/h2&gt; 
&lt;p&gt;â— Due to more restrict data security policy. The official dataset is disabled temporarily. You can try &lt;a href="https://github.com/chenditc/investment_data/releases"&gt;this data source&lt;/a&gt; contributed by the community. Here is an example to download the latest data.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;wget https://github.com/chenditc/investment_data/releases/latest/download/qlib_bin.tar.gz
mkdir -p ~/.qlib/qlib_data/cn_data
tar -zxvf qlib_bin.tar.gz -C ~/.qlib/qlib_data/cn_data --strip-components=1
rm -f qlib_bin.tar.gz
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The official dataset below will resume in short future.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;Load and prepare data by running the following code:&lt;/p&gt; 
&lt;h3&gt;Get with module&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# get 1d data
python -m qlib.cli.data qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn

# get 1min data
python -m qlib.cli.data qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min

&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Get from source&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# get 1d data
python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn

# get 1min data
python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This dataset is created by public data collected by &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/scripts/data_collector/"&gt;crawler scripts&lt;/a&gt;, which have been released in the same repository. Users could create the same dataset with it. &lt;a href="https://github.com/microsoft/qlib/tree/main/scripts/data_collector#description-of-dataset"&gt;Description of dataset&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Please pay &lt;strong&gt;ATTENTION&lt;/strong&gt; that the data is collected from &lt;a href="https://finance.yahoo.com/lookup"&gt;Yahoo Finance&lt;/a&gt;, and the data might not be perfect. We recommend users to prepare their own data if they have a high-quality dataset. For more information, users can refer to the &lt;a href="https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format"&gt;related document&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt; 
&lt;h3&gt;Automatic update of daily frequency data (from yahoo finance)&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;This step is &lt;em&gt;Optional&lt;/em&gt; if users only want to try their models and strategies on history data.&lt;/p&gt; 
 &lt;p&gt;It is recommended that users update the data manually once (--trading_date 2021-05-25) and then set it to update automatically.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: Users can't incrementally update data based on the offline data provided by Qlib(some fields are removed to reduce the data size). Users should use &lt;a href="https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance"&gt;yahoo collector&lt;/a&gt; to download Yahoo data from scratch and then incrementally update it.&lt;/p&gt; 
 &lt;p&gt;For more information, please refer to: &lt;a href="https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance"&gt;yahoo collector&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Automatic update of data to the "qlib" directory each trading day(Linux)&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;use &lt;em&gt;crontab&lt;/em&gt;: &lt;code&gt;crontab -e&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;set up timed tasks:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;* * * * 1-5 python &amp;lt;script path&amp;gt; update_data_to_bin --qlib_data_1d_dir &amp;lt;user data dir&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;script path&lt;/strong&gt;: &lt;em&gt;scripts/data_collector/yahoo/collector.py&lt;/em&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Manual update of data&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir &amp;lt;user data dir&amp;gt; --trading_date &amp;lt;start date&amp;gt; --end_date &amp;lt;end date&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;em&gt;trading_date&lt;/em&gt;: start of trading day&lt;/li&gt; 
   &lt;li&gt;&lt;em&gt;end_date&lt;/em&gt;: end of trading day(not included)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Checking the health of the data&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;We provide a script to check the health of the data, you can run the following commands to check whether the data is healthy or not. &lt;pre&gt;&lt;code&gt;python scripts/check_data_health.py check_data --qlib_dir ~/.qlib/qlib_data/cn_data
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Of course, you can also add some parameters to adjust the test results, such as this. &lt;pre&gt;&lt;code&gt;python scripts/check_data_health.py check_data --qlib_dir ~/.qlib/qlib_data/cn_data --missing_data_num 30055 --large_step_threshold_volume 94485 --large_step_threshold_price 20
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;If you want more information about &lt;code&gt;check_data_health&lt;/code&gt;, please refer to the &lt;a href="https://qlib.readthedocs.io/en/latest/component/data.html#checking-the-health-of-the-data"&gt;documentation&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- 
- Run the initialization code and get stock data:

  ```python
  import qlib
  from qlib.data import D
  from qlib.constant import REG_CN

  # Initialization
  mount_path = "~/.qlib/qlib_data/cn_data"  # target_dir
  qlib.init(mount_path=mount_path, region=REG_CN)

  # Get stock data by Qlib
  # Load trading calendar with the given time range and frequency
  print(D.calendar(start_time='2010-01-01', end_time='2017-12-31', freq='day')[:2])

  # Parse a given market name into a stockpool config
  instruments = D.instruments('csi500')
  print(D.list_instruments(instruments=instruments, start_time='2010-01-01', end_time='2017-12-31', as_list=True)[:6])

  # Load features of certain instruments in given time range
  instruments = ['SH600000']
  fields = ['$close', '$volume', 'Ref($close, 1)', 'Mean($close, 3)', '$high-$low']
  print(D.features(instruments, fields, start_time='2010-01-01', end_time='2017-12-31', freq='day').head())
  ```
 --&gt; 
&lt;h2&gt;Docker images&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Pulling a docker image from a docker hub repository &lt;pre&gt;&lt;code class="language-bash"&gt;docker pull pyqlib/qlib_image_stable:stable
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Start a new Docker container &lt;pre&gt;&lt;code class="language-bash"&gt;docker run -it --name &amp;lt;container name&amp;gt; -v &amp;lt;Mounted local directory&amp;gt;:/app qlib_image_stable
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;At this point you are in the docker environment and can run the qlib scripts. An example: &lt;pre&gt;&lt;code class="language-bash"&gt;&amp;gt;&amp;gt;&amp;gt; python scripts/get_data.py qlib_data --name qlib_data_simple --target_dir ~/.qlib/qlib_data/cn_data --interval 1d --region cn
&amp;gt;&amp;gt;&amp;gt; python qlib/cli/run.py examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Exit the container &lt;pre&gt;&lt;code class="language-bash"&gt;&amp;gt;&amp;gt;&amp;gt; exit
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Restart the container &lt;pre&gt;&lt;code class="language-bash"&gt;docker start -i -a &amp;lt;container name&amp;gt;
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Stop the container &lt;pre&gt;&lt;code class="language-bash"&gt;docker stop &amp;lt;container name&amp;gt;
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Delete the container &lt;pre&gt;&lt;code class="language-bash"&gt;docker rm &amp;lt;container name&amp;gt;
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;If you want to know more information, please refer to the &lt;a href="https://qlib.readthedocs.io/en/latest/developer/how_to_build_image.html"&gt;documentation&lt;/a&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Auto Quant Research Workflow&lt;/h2&gt; 
&lt;p&gt;Qlib provides a tool named &lt;code&gt;qrun&lt;/code&gt; to run the whole workflow automatically (including building dataset, training models, backtest and evaluation). You can start an auto quant research workflow and have a graphical reports analysis according to the following steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Quant Research Workflow: Run &lt;code&gt;qrun&lt;/code&gt; with lightgbm workflow config (&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml"&gt;workflow_config_lightgbm_Alpha158.yaml&lt;/a&gt; as following.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;  cd examples  # Avoid running program under the directory contains `qlib`
  qrun benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If users want to use &lt;code&gt;qrun&lt;/code&gt; under debug mode, please use the following command:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;python -m pdb qlib/cli/run.py examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The result of &lt;code&gt;qrun&lt;/code&gt; is as follows, please refer to &lt;a href="https://qlib.readthedocs.io/en/latest/component/strategy.html#result"&gt;docs&lt;/a&gt; for more explanations about the result.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;
'The following are analysis results of the excess return without cost.'
                       risk
mean               0.000708
std                0.005626
annualized_return  0.178316
information_ratio  1.996555
max_drawdown      -0.081806
'The following are analysis results of the excess return with cost.'
                       risk
mean               0.000512
std                0.005626
annualized_return  0.128982
information_ratio  1.444287
max_drawdown      -0.091078
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here are detailed documents for &lt;code&gt;qrun&lt;/code&gt; and &lt;a href="https://qlib.readthedocs.io/en/latest/component/workflow.html"&gt;workflow&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Graphical Reports Analysis: First, run &lt;code&gt;python -m pip install .[analysis]&lt;/code&gt; to install the required dependencies. Then run &lt;code&gt;examples/workflow_by_code.ipynb&lt;/code&gt; with &lt;code&gt;jupyter notebook&lt;/code&gt; to get graphical reports.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;Forecasting signal (model prediction) analysis&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Cumulative Return of groups &lt;img src="https://github.com/microsoft/qlib/raw/main/docs/_static/img/analysis/analysis_model_cumulative_return.png" alt="Cumulative Return" /&gt;&lt;/li&gt; 
     &lt;li&gt;Return distribution &lt;img src="https://github.com/microsoft/qlib/raw/main/docs/_static/img/analysis/analysis_model_long_short.png" alt="long_short" /&gt;&lt;/li&gt; 
     &lt;li&gt;Information Coefficient (IC) &lt;img src="https://github.com/microsoft/qlib/raw/main/docs/_static/img/analysis/analysis_model_IC.png" alt="Information Coefficient" /&gt; &lt;img src="https://github.com/microsoft/qlib/raw/main/docs/_static/img/analysis/analysis_model_monthly_IC.png" alt="Monthly IC" /&gt; &lt;img src="https://github.com/microsoft/qlib/raw/main/docs/_static/img/analysis/analysis_model_NDQ.png" alt="IC" /&gt;&lt;/li&gt; 
     &lt;li&gt;Auto Correlation of forecasting signal (model prediction) &lt;img src="https://github.com/microsoft/qlib/raw/main/docs/_static/img/analysis/analysis_model_auto_correlation.png" alt="Auto Correlation" /&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Portfolio analysis&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Backtest return &lt;img src="https://github.com/microsoft/qlib/raw/main/docs/_static/img/analysis/report.png" alt="Report" /&gt;&lt;/li&gt; 
    &lt;/ul&gt; 
    &lt;!-- 
- Score IC
![Score IC](docs/_static/img/score_ic.png)
- Cumulative Return
![Cumulative Return](docs/_static/img/cumulative_return.png)
- Risk Analysis
![Risk Analysis](docs/_static/img/risk_analysis.png)
- Rank Label
![Rank Label](docs/_static/img/rank_label.png)
--&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;a href="https://qlib.readthedocs.io/en/latest/component/report.html"&gt;Explanation&lt;/a&gt; of above results&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Building Customized Quant Research Workflow by Code&lt;/h2&gt; 
&lt;p&gt;The automatic workflow may not suit the research workflow of all Quant researchers. To support a flexible Quant research workflow, Qlib also provides a modularized interface to allow researchers to build their own workflow by code. &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/workflow_by_code.ipynb"&gt;Here&lt;/a&gt; is a demo for customized Quant research workflow by code.&lt;/p&gt; 
&lt;h1&gt;Main Challenges &amp;amp; Solutions in Quant Research&lt;/h1&gt; 
&lt;p&gt;Quant investment is a very unique scenario with lots of key challenges to be solved. Currently, Qlib provides some solutions for several of them.&lt;/p&gt; 
&lt;h2&gt;Forecasting: Finding Valuable Signals/Patterns&lt;/h2&gt; 
&lt;p&gt;Accurate forecasting of the stock price trend is a very important part to construct profitable portfolios. However, huge amount of data with various formats in the financial market which make it challenging to build forecasting models.&lt;/p&gt; 
&lt;p&gt;An increasing number of SOTA Quant research works/papers, which focus on building forecasting models to mine valuable signals/patterns in complex financial data, are released in &lt;code&gt;Qlib&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks"&gt;Quant Model (Paper) Zoo&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Here is a list of models built on &lt;code&gt;Qlib&lt;/code&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/XGBoost/"&gt;GBDT based on XGBoost (Tianqi Chen, et al. KDD 2016)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/LightGBM/"&gt;GBDT based on LightGBM (Guolin Ke, et al. NIPS 2017)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/CatBoost/"&gt;GBDT based on Catboost (Liudmila Prokhorenkova, et al. NIPS 2018)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/MLP/"&gt;MLP based on pytorch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/LSTM/"&gt;LSTM based on pytorch (Sepp Hochreiter, et al. Neural computation 1997)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/GRU/"&gt;GRU based on pytorch (Kyunghyun Cho, et al. 2014)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/ALSTM"&gt;ALSTM based on pytorch (Yao Qin, et al. IJCAI 2017)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/GATs/"&gt;GATs based on pytorch (Petar Velickovic, et al. 2017)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/SFM/"&gt;SFM based on pytorch (Liheng Zhang, et al. KDD 2017)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/TFT/"&gt;TFT based on tensorflow (Bryan Lim, et al. International Journal of Forecasting 2019)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/TabNet/"&gt;TabNet based on pytorch (Sercan O. Arik, et al. AAAI 2019)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/DoubleEnsemble/"&gt;DoubleEnsemble based on LightGBM (Chuheng Zhang, et al. ICDM 2020)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/TCTS/"&gt;TCTS based on pytorch (Xueqing Wu, et al. ICML 2021)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/Transformer/"&gt;Transformer based on pytorch (Ashish Vaswani, et al. NeurIPS 2017)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/Localformer/"&gt;Localformer based on pytorch (Juyong Jiang, et al.)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/TRA/"&gt;TRA based on pytorch (Hengxu, Dong, et al. KDD 2021)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/TCN/"&gt;TCN based on pytorch (Shaojie Bai, et al. 2018)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/ADARNN/"&gt;ADARNN based on pytorch (YunTao Du, et al. 2021)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/ADD/"&gt;ADD based on pytorch (Hongshun Tang, et al.2020)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/IGMTF/"&gt;IGMTF based on pytorch (Wentao Xu, et al.2021)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/HIST/"&gt;HIST based on pytorch (Wentao Xu, et al.2021)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/KRNN/"&gt;KRNN based on pytorch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/Sandwich/"&gt;Sandwich based on pytorch&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Your PR of new Quant models is highly welcomed.&lt;/p&gt; 
&lt;p&gt;The performance of each model on the &lt;code&gt;Alpha158&lt;/code&gt; and &lt;code&gt;Alpha360&lt;/code&gt; datasets can be found &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/README.md"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Run a single model&lt;/h3&gt; 
&lt;p&gt;All the models listed above are runnable with &lt;code&gt;Qlib&lt;/code&gt;. Users can find the config files we provide and some details about the model through the &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks"&gt;benchmarks&lt;/a&gt; folder. More information can be retrieved at the model files listed above.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;Qlib&lt;/code&gt; provides three different ways to run a single model, users can pick the one that fits their cases best:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Users can use the tool &lt;code&gt;qrun&lt;/code&gt; mentioned above to run a model's workflow based from a config file.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Users can create a &lt;code&gt;workflow_by_code&lt;/code&gt; python script based on the &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/workflow_by_code.py"&gt;one&lt;/a&gt; listed in the &lt;code&gt;examples&lt;/code&gt; folder.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Users can use the script &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/run_all_model.py"&gt;&lt;code&gt;run_all_model.py&lt;/code&gt;&lt;/a&gt; listed in the &lt;code&gt;examples&lt;/code&gt; folder to run a model. Here is an example of the specific shell command to be used: &lt;code&gt;python run_all_model.py run --models=lightgbm&lt;/code&gt;, where the &lt;code&gt;--models&lt;/code&gt; arguments can take any number of models listed above(the available models can be found in &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/"&gt;benchmarks&lt;/a&gt;). For more use cases, please refer to the file's &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/run_all_model.py"&gt;docstrings&lt;/a&gt;.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: Each baseline has different environment dependencies, please make sure that your python version aligns with the requirements(e.g. TFT only supports Python 3.6~3.7 due to the limitation of &lt;code&gt;tensorflow==1.15.0&lt;/code&gt;)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Run multiple models&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;Qlib&lt;/code&gt; also provides a script &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/run_all_model.py"&gt;&lt;code&gt;run_all_model.py&lt;/code&gt;&lt;/a&gt; which can run multiple models for several iterations. (&lt;strong&gt;Note&lt;/strong&gt;: the script only support &lt;em&gt;Linux&lt;/em&gt; for now. Other OS will be supported in the future. Besides, it doesn't support parallel running the same model for multiple times as well, and this will be fixed in the future development too.)&lt;/p&gt; 
&lt;p&gt;The script will create a unique virtual environment for each model, and delete the environments after training. Thus, only experiment results such as &lt;code&gt;IC&lt;/code&gt; and &lt;code&gt;backtest&lt;/code&gt; results will be generated and stored.&lt;/p&gt; 
&lt;p&gt;Here is an example of running all the models for 10 iterations:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;python run_all_model.py run 10
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It also provides the API to run specific models at once. For more use cases, please refer to the file's &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/run_all_model.py"&gt;docstrings&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Break change&lt;/h3&gt; 
&lt;p&gt;In &lt;code&gt;pandas&lt;/code&gt;, &lt;code&gt;group_key&lt;/code&gt; is one of the parameters of the &lt;code&gt;groupby&lt;/code&gt; method. From version 1.5 to 2.0 of &lt;code&gt;pandas&lt;/code&gt;, the default value of &lt;code&gt;group_key&lt;/code&gt; has been changed from &lt;code&gt;no default&lt;/code&gt; to &lt;code&gt;True&lt;/code&gt;, which will cause qlib to report an error during operation. So we set &lt;code&gt;group_key=False&lt;/code&gt;, but it doesn't guarantee that some programmes will run correctly, including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;qlib\examples\rl_order_execution\scripts\gen_training_orders.py&lt;/li&gt; 
 &lt;li&gt;qlib\examples\benchmarks\TRA\src\dataset.MTSDatasetH.py&lt;/li&gt; 
 &lt;li&gt;qlib\examples\benchmarks\TFT\tft.py&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks_dynamic"&gt;Adapting to Market Dynamics&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;Due to the non-stationary nature of the environment of the financial market, the data distribution may change in different periods, which makes the performance of models build on training data decays in the future test data. So adapting the forecasting models/strategies to market dynamics is very important to the model/strategies' performance.&lt;/p&gt; 
&lt;p&gt;Here is a list of solutions built on &lt;code&gt;Qlib&lt;/code&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks_dynamic/baseline/"&gt;Rolling Retraining&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks_dynamic/DDG-DA/"&gt;DDG-DA on pytorch (Wendi, et al. AAAI 2022)&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Reinforcement Learning: modeling continuous decisions&lt;/h2&gt; 
&lt;p&gt;Qlib now supports reinforcement learning, a feature designed to model continuous investment decisions. This functionality assists investors in optimizing their trading strategies by learning from interactions with the environment to maximize some notion of cumulative reward.&lt;/p&gt; 
&lt;p&gt;Here is a list of solutions built on &lt;code&gt;Qlib&lt;/code&gt; categorized by scenarios.&lt;/p&gt; 
&lt;h3&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/rl_order_execution"&gt;RL for order execution&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://qlib.readthedocs.io/en/latest/component/rl/overall.html#order-execution"&gt;Here&lt;/a&gt; is the introduction of this scenario. All the methods below are compared &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/rl_order_execution"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/rl_order_execution/exp_configs/backtest_twap.yml"&gt;TWAP&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/rl_order_execution/exp_configs/backtest_ppo.yml"&gt;PPO: "An End-to-End Optimal Trade Execution Framework based on Proximal Policy Optimization", IJCAL 2020&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/rl_order_execution/exp_configs/backtest_opds.yml"&gt;OPDS: "Universal Trading for Order Execution with Oracle Policy Distillation", AAAI 2021&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Quant Dataset Zoo&lt;/h1&gt; 
&lt;p&gt;Dataset plays a very important role in Quant. Here is a list of the datasets built on &lt;code&gt;Qlib&lt;/code&gt;:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Dataset&lt;/th&gt; 
   &lt;th&gt;US Market&lt;/th&gt; 
   &lt;th&gt;China Market&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/qlib/contrib/data/handler.py"&gt;Alpha360&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âˆš&lt;/td&gt; 
   &lt;td&gt;âˆš&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/qlib/contrib/data/handler.py"&gt;Alpha158&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âˆš&lt;/td&gt; 
   &lt;td&gt;âˆš&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;a href="https://qlib.readthedocs.io/en/latest/advanced/alpha.html"&gt;Here&lt;/a&gt; is a tutorial to build dataset with &lt;code&gt;Qlib&lt;/code&gt;. Your PR to build new Quant dataset is highly welcomed.&lt;/p&gt; 
&lt;h1&gt;Learning Framework&lt;/h1&gt; 
&lt;p&gt;Qlib is high customizable and a lot of its components are learnable. The learnable components are instances of &lt;code&gt;Forecast Model&lt;/code&gt; and &lt;code&gt;Trading Agent&lt;/code&gt;. They are learned based on the &lt;code&gt;Learning Framework&lt;/code&gt; layer and then applied to multiple scenarios in &lt;code&gt;Workflow&lt;/code&gt; layer. The learning framework leverages the &lt;code&gt;Workflow&lt;/code&gt; layer as well(e.g. sharing &lt;code&gt;Information Extractor&lt;/code&gt;, creating environments based on &lt;code&gt;Execution Env&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;Based on learning paradigms, they can be categorized into reinforcement learning and supervised learning.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;For supervised learning, the detailed docs can be found &lt;a href="https://qlib.readthedocs.io/en/latest/component/model.html"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;For reinforcement learning, the detailed docs can be found &lt;a href="https://qlib.readthedocs.io/en/latest/component/rl.html"&gt;here&lt;/a&gt;. Qlib's RL learning framework leverages &lt;code&gt;Execution Env&lt;/code&gt; in &lt;code&gt;Workflow&lt;/code&gt; layer to create environments. It's worth noting that &lt;code&gt;NestedExecutor&lt;/code&gt; is supported as well. This empowers users to optimize different level of strategies/models/agents together (e.g. optimizing an order execution strategy for a specific portfolio management strategy).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;More About Qlib&lt;/h1&gt; 
&lt;p&gt;If you want to have a quick glance at the most frequently used components of qlib, you can try notebooks &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/tutorial/"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The detailed documents are organized in &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/docs/"&gt;docs&lt;/a&gt;. &lt;a href="http://www.sphinx-doc.org"&gt;Sphinx&lt;/a&gt; and the readthedocs theme is required to build the documentation in html formats.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd docs/
conda install sphinx sphinx_rtd_theme -y
# Otherwise, you can install them with pip
# pip install sphinx sphinx_rtd_theme
make html
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also view the &lt;a href="http://qlib.readthedocs.io/"&gt;latest document&lt;/a&gt; online directly.&lt;/p&gt; 
&lt;p&gt;Qlib is in active and continuing development. Our plan is in the roadmap, which is managed as a &lt;a href="https://github.com/microsoft/qlib/projects/1"&gt;github project&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Offline Mode and Online Mode&lt;/h1&gt; 
&lt;p&gt;The data server of Qlib can either deployed as &lt;code&gt;Offline&lt;/code&gt; mode or &lt;code&gt;Online&lt;/code&gt; mode. The default mode is offline mode.&lt;/p&gt; 
&lt;p&gt;Under &lt;code&gt;Offline&lt;/code&gt; mode, the data will be deployed locally.&lt;/p&gt; 
&lt;p&gt;Under &lt;code&gt;Online&lt;/code&gt; mode, the data will be deployed as a shared data service. The data and their cache will be shared by all the clients. The data retrieval performance is expected to be improved due to a higher rate of cache hits. It will consume less disk space, too. The documents of the online mode can be found in &lt;a href="https://qlib-server.readthedocs.io/"&gt;Qlib-Server&lt;/a&gt;. The online mode can be deployed automatically with &lt;a href="https://qlib-server.readthedocs.io/en/latest/build.html#one-click-deployment-in-azure"&gt;Azure CLI based scripts&lt;/a&gt;. The source code of online data server can be found in &lt;a href="https://github.com/microsoft/qlib-server"&gt;Qlib-Server repository&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Performance of Qlib Data Server&lt;/h2&gt; 
&lt;p&gt;The performance of data processing is important to data-driven methods like AI technologies. As an AI-oriented platform, Qlib provides a solution for data storage and data processing. To demonstrate the performance of Qlib data server, we compare it with several other data storage solutions.&lt;/p&gt; 
&lt;p&gt;We evaluate the performance of several storage solutions by finishing the same task, which creates a dataset (14 features/factors) from the basic OHLCV daily data of a stock market (800 stocks each day from 2007 to 2020). The task involves data queries and processing.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;HDF5&lt;/th&gt; 
   &lt;th&gt;MySQL&lt;/th&gt; 
   &lt;th&gt;MongoDB&lt;/th&gt; 
   &lt;th&gt;InfluxDB&lt;/th&gt; 
   &lt;th&gt;Qlib -E -D&lt;/th&gt; 
   &lt;th&gt;Qlib +E -D&lt;/th&gt; 
   &lt;th&gt;Qlib +E +D&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Total (1CPU) (seconds)&lt;/td&gt; 
   &lt;td&gt;184.4Â±3.7&lt;/td&gt; 
   &lt;td&gt;365.3Â±7.5&lt;/td&gt; 
   &lt;td&gt;253.6Â±6.7&lt;/td&gt; 
   &lt;td&gt;368.2Â±3.6&lt;/td&gt; 
   &lt;td&gt;147.0Â±8.8&lt;/td&gt; 
   &lt;td&gt;47.6Â±1.0&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;7.4Â±0.3&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Total (64CPU) (seconds)&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;8.8Â±0.6&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;4.2Â±0.2&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;+(-)E&lt;/code&gt; indicates with (out) &lt;code&gt;ExpressionCache&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;+(-)D&lt;/code&gt; indicates with (out) &lt;code&gt;DatasetCache&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Most general-purpose databases take too much time to load data. After looking into the underlying implementation, we find that data go through too many layers of interfaces and unnecessary format transformations in general-purpose database solutions. Such overheads greatly slow down the data loading process. Qlib data are stored in a compact format, which is efficient to be combined into arrays for scientific computation.&lt;/p&gt; 
&lt;h1&gt;Related Reports&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://analyticsindiamag.com/qlib/"&gt;Guide To Qlib: Microsoftâ€™s AI Investment Platform&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mp.weixin.qq.com/s/47bP5YwxfTp2uTHjUBzJQQ"&gt;å¾®è½¯ä¹ŸæAIé‡åŒ–å¹³å°ï¼Ÿè¿˜æ˜¯å¼€æºçš„ï¼&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mp.weixin.qq.com/s/vsJv7lsgjEi-ALYUz4CvtQ"&gt;å¾®çŸ¿Qlibï¼šä¸šå†…é¦–ä¸ªAIé‡åŒ–æŠ•èµ„å¼€æºå¹³å°&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Contact Us&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;If you have any issues, please create issue &lt;a href="https://github.com/microsoft/qlib/issues/new/choose"&gt;here&lt;/a&gt; or send messages in &lt;a href="https://gitter.im/Microsoft/qlib"&gt;gitter&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;If you want to make contributions to &lt;code&gt;Qlib&lt;/code&gt;, please &lt;a href="https://github.com/microsoft/qlib/compare"&gt;create pull requests&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;For other reasons, you are welcome to contact us by email(&lt;a href="mailto:qlib@microsoft.com"&gt;qlib@microsoft.com&lt;/a&gt;). 
  &lt;ul&gt; 
   &lt;li&gt;We are recruiting new members(both FTEs and interns), your resumes are welcome!&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Join IM discussion groups:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;a href="https://gitter.im/Microsoft/qlib"&gt;Gitter&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src="https://github.com/microsoft/qlib/raw/main/docs/_static/img/qrcode/gitter_qr.png" alt="image" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;Contributing&lt;/h1&gt; 
&lt;p&gt;We appreciate all contributions and thank all the contributors! &lt;a href="https://github.com/microsoft/qlib/graphs/contributors"&gt;&lt;img src="https://contrib.rocks/image?repo=microsoft/qlib" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Before we released Qlib as an open-source project on Github in Sep 2020, Qlib is an internal project in our group. Unfortunately, the internal commit history is not kept. A lot of members in our group have also contributed a lot to Qlib, which includes Ruihua Wang, Yinda Zhang, Haisu Yu, Shuyu Wang, Bochen Pang, and &lt;a href="https://github.com/evanzd/evanzd"&gt;Dong Zhou&lt;/a&gt;. Especially thanks to &lt;a href="https://github.com/evanzd/evanzd"&gt;Dong Zhou&lt;/a&gt; due to his initial version of Qlib.&lt;/p&gt; 
&lt;h2&gt;Guidance&lt;/h2&gt; 
&lt;p&gt;This project welcomes contributions and suggestions.&lt;br /&gt; &lt;strong&gt;Here are some &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/docs/developer/code_standard_and_dev_guide.rst"&gt;code standards and development guidance&lt;/a&gt; for submiting a pull request.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Making contributions is not a hard thing. Solving an issue(maybe just answering a question raised in &lt;a href="https://github.com/microsoft/qlib/issues"&gt;issues list&lt;/a&gt; or &lt;a href="https://gitter.im/Microsoft/qlib"&gt;gitter&lt;/a&gt;), fixing/issuing a bug, improving the documents and even fixing a typo are important contributions to Qlib.&lt;/p&gt; 
&lt;p&gt;For example, if you want to contribute to Qlib's document/code, you can follow the steps in the figure below.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://github.com/demon143/qlib/raw/main/docs/_static/img/change%20doc.gif" /&gt; &lt;/p&gt; 
&lt;p&gt;If you don't know how to start to contribute, you can refer to the following examples.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Examples&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Solving issues&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/qlib/issues/749"&gt;Answer a question&lt;/a&gt;; &lt;a href="https://github.com/microsoft/qlib/issues/765"&gt;issuing&lt;/a&gt; or &lt;a href="https://github.com/microsoft/qlib/pull/792"&gt;fixing&lt;/a&gt; a bug&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Docs&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/qlib/pull/797/files"&gt;Improve docs quality&lt;/a&gt; ; &lt;a href="https://github.com/microsoft/qlib/pull/774"&gt;Fix a typo&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Feature&lt;/td&gt; 
   &lt;td&gt;Implement a &lt;a href="https://github.com/microsoft/qlib/projects"&gt;requested feature&lt;/a&gt; like &lt;a href="https://github.com/microsoft/qlib/pull/754"&gt;this&lt;/a&gt;; &lt;a href="https://github.com/microsoft/qlib/pull/539/files"&gt;Refactor interfaces&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Dataset&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/qlib/pull/733"&gt;Add a dataset&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Models&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/qlib/pull/689"&gt;Implement a new model&lt;/a&gt;, &lt;a href="https://github.com/microsoft/qlib/tree/main/examples/benchmarks#contributing"&gt;some instructions to contribute models&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;a href="https://github.com/microsoft/qlib/labels/good%20first%20issue"&gt;Good first issues&lt;/a&gt; are labelled to indicate that they are easy to start your contributions.&lt;/p&gt; 
&lt;p&gt;You can find some impefect implementation in Qlib by &lt;code&gt;rg 'TODO|FIXME' qlib&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;If you would like to become one of Qlib's maintainers to contribute more (e.g. help merge PR, triage issues), please contact us by email(&lt;a href="mailto:qlib@microsoft.com"&gt;qlib@microsoft.com&lt;/a&gt;). We are glad to help to upgrade your permission.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the right to use your contribution. For details, visit &lt;a href="https://cla.opensource.microsoft.com"&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; 
&lt;p&gt;This project has adopted the &lt;a href="https://opensource.microsoft.com/codeofconduct/"&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href="https://opensource.microsoft.com/codeofconduct/faq/"&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href="mailto:opencode@microsoft.com"&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>