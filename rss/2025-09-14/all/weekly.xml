<rss version="2.0">
  <channel>
    <title>GitHub All Languages Weekly Trending</title>
    <description>Weekly Trending of All Languages in GitHub</description>
    <pubDate>Sat, 13 Sep 2025 01:38:37 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>11cafe/jaaz</title>
      <link>https://github.com/11cafe/jaaz</link>
      <description>&lt;p&gt;The world's first open-source multimodal creative assistant This is a substitute for Canva and Manus that prioritizes privacy and is usable locally.&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt; &lt;a href="https://jaaz.app" target="_blank"&gt; Jaaz.app&lt;/a&gt; &lt;p align="center"&gt;Open source Canva AI alternative&lt;/p&gt; &lt;p align="center"&gt; &lt;a href="https://jaaz.app"&gt; &lt;img src="https://github.com/user-attachments/assets/e0cffb94-8c6f-4867-800a-c144aceb6d54" alt="Jaaz Logo" /&gt; &lt;/a&gt; &lt;/p&gt; &lt;/h1&gt; 
&lt;p align="center"&gt;The world's first open-source multimodal canvas creative agent&lt;/p&gt; 
&lt;p align="center"&gt;This is a substitute for Canva and Manus that prioritizes privacy and is usable locally.&lt;/p&gt; 
&lt;p&gt; &lt;b&gt;ğŸ“£ [New!] Enterprise Cloud â€œFullâ€ Edition&lt;/b&gt; â€” Private/on-prem deployment &amp;amp; commercial licensing (Docker image or full source). Includes all jaaz.app online features. &lt;b&gt;30% OFF&lt;/b&gt; through &lt;b&gt;Sep 15, 2025&lt;/b&gt;. &lt;a href="mailto:info@jaaz.app"&gt;Contact us â†’&lt;/a&gt; info@jaaz.app &lt;br /&gt; &lt;br /&gt; &lt;b&gt;ğŸ“£ [New!] ä¼ä¸šäº‘ç«¯å®Œæ•´ç‰ˆ&lt;/b&gt; â€” æ”¯æŒ&lt;span&gt;ç§æœ‰åŒ–éƒ¨ç½²&lt;/span&gt;ä¸&lt;span&gt;å•†ä¸šæˆæƒ&lt;/span&gt;ï¼ˆDocker é•œåƒæˆ–æºç äº¤ä»˜ï¼‰ï¼ŒåŒ…å« jaaz.app å…¨é‡çº¿ä¸ŠåŠŸèƒ½ã€‚é™æ—¶ &lt;b&gt;30% OFF&lt;/b&gt;ï¼Œæˆªæ­¢ &lt;b&gt;2025-09-15&lt;/b&gt;ã€‚ &lt;a href="mailto:info@jaaz.app"&gt;äº†è§£/æ´½è°ˆ â†’&lt;/a&gt; info@jaaz.app &lt;/p&gt; 
&lt;br /&gt;
&lt;br /&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/11cafe/jaaz/raw/main/README_zh.md"&gt;ä¸­æ–‡ç‰ˆ&lt;/a&gt;| &lt;a href="https://mxnpt25l6k.feishu.cn/docx/LvcTdlVbFoRAZWxnhBYcqVydnpc"&gt;æ–°æ‰‹æŒ‡å—&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://discord.gg/https://discord.gg/SMRe5n3m"&gt; &lt;img src="https://img.shields.io/badge/Discord-5865F2?logo=discord&amp;amp;logoColor=white&amp;amp;style=for-the-badge" alt="Discord" /&gt; &lt;/a&gt; &lt;a href="https://github.com/11cafe/jaaz/stargazers"&gt; &lt;img src="https://img.shields.io/github/stars/11cafe/jaaz?style=for-the-badge&amp;amp;logo=github" alt="GitHub Stars" /&gt; &lt;/a&gt; 
 &lt;!-- Download for Mac --&gt; &lt;a href="https://jaaz.app/api/downloads/mac-latest"&gt; &lt;img src="https://img.shields.io/badge/For%20Mac-000000?logo=apple&amp;amp;logoColor=white&amp;amp;style=for-the-badge" alt="Download for Mac" /&gt; &lt;/a&gt; 
 &lt;!-- Download for Windows --&gt; &lt;a href="https://jaaz.app/api/downloads/windows-latest"&gt; &lt;img src="https://img.shields.io/badge/For%20Windows-0078D6?logo=laptop&amp;amp;logoColor=white&amp;amp;style=for-the-badge" alt="Download for Windows" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; Magic Canva! &lt;/p&gt;
&lt;p&gt;"Build" your ideas like playing with LEGOâ€”paint directly, point with arrows, and the AI instantly understands and generates results. &lt;img width="900" alt="Screenshot 2025-06-02 at 3 03 49 PM" src="https://github.com/user-attachments/assets/543b170c-14f7-4a73-96bd-909662138592" /&gt; &lt;img width="900" alt="Screenshot 2025-06-02 at 3 03 49 PM" src="https://github.com/user-attachments/assets/7dd9af32-cc60-4145-9b30-7db96d8fa09a" /&gt;&lt;/p&gt; 
&lt;p&gt;Magic video!&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/b7abf987-c65d-49b1-8178-82770873c583"&gt;https://github.com/user-attachments/assets/b7abf987-c65d-49b1-8178-82770873c583&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Create Viral Shorts with a Single Sentence 
 &lt;video src="https://github.com/user-attachments/assets/1c15e792-098a-4557-b310-d9c223f73442" controls width="100%"&gt;&lt;/video&gt;&lt;/p&gt; 
&lt;h2&gt;âœ¨ Getting started &amp;amp; staying tuned with us.&lt;/h2&gt; 
&lt;p&gt;Star us, and you will receive all release notifications from GitHub without any delay! &lt;img width="900" alt="Screenshot 2025-06-02 at 3 03 49 PM" src="https://github.com/user-attachments/assets/1c9a3661-80a4-4fba-a30f-f469898b0aec" /&gt;&lt;/p&gt; 
&lt;h2&gt;âœ¨ Key Features&lt;/h2&gt; 
&lt;p&gt;ğŸ¬ One-Prompt Image &amp;amp; Video Generation Turn one prompt into complete images or videos in seconds.&lt;/p&gt; 
&lt;p&gt;-Supports GPT-4o, Midjourney, VEO3, Kling,veo3,seedance etc.&lt;/p&gt; 
&lt;p&gt;-Auto-optimized prompts &amp;amp; multi-turn refinement&lt;/p&gt; 
&lt;p&gt;ğŸ§™ Magic Canvas&amp;amp;Magic Video Prompt-free creation â€” build like Lego.&lt;/p&gt; 
&lt;p&gt;-Simple sketching and free combination â€” AI instantly understands and generates.&lt;/p&gt; 
&lt;p&gt;-AI understands and generates instantly&lt;/p&gt; 
&lt;p&gt;-No prompt writing needed&lt;/p&gt; 
&lt;p&gt;-Describe steps simply on the video, and AI will generate following them.&lt;/p&gt; 
&lt;p&gt;ğŸ–¼ï¸ Infinite Canvas &amp;amp; Visual Storyboarding Plan scenes with an unlimited canvas&lt;/p&gt; 
&lt;p&gt;-Link layouts, manage media visually&lt;/p&gt; 
&lt;p&gt;-Real-time collaboration supported&lt;/p&gt; 
&lt;p&gt;ğŸ¤– Smart AI Agent System -Chat to insert objects, transfer styles, control logic&lt;/p&gt; 
&lt;p&gt;-Works with local (ComfyUI) &amp;amp; cloud models&lt;/p&gt; 
&lt;p&gt;-Maintains multi-character coherence&lt;/p&gt; 
&lt;p&gt;âš™ï¸ Flexible Deployment &amp;amp; Local Assets -Fully offline or hybrid setup (Ollama + APIs)&lt;/p&gt; 
&lt;p&gt;-Built-in library for media &amp;amp; prompts&lt;/p&gt; 
&lt;p&gt;-Cross-platform: Windows &amp;amp; macOS&lt;/p&gt; 
&lt;p&gt;ğŸ” Privacy &amp;amp; Security -Local-first, no data leaves your device&lt;/p&gt; 
&lt;p&gt;-Open-source, no tracking&lt;/p&gt; 
&lt;p&gt;-Safe for commercial use â€” you own your data&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;p&gt;Download here: &lt;a href="https://jaaz.app/"&gt;https://jaaz.app/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Click the "Log In" button at the top right of the homepage to access API models. With a low-cost plan, you can seamlessly use a variety of powerful APIs.&lt;/p&gt; 
&lt;img width="400" alt="Screenshot 2025-06-02 at 3 08 51 PM" src="https://github.com/user-attachments/assets/0055557d-c247-4801-ac3f-01ed4fa775ae" /&gt; 
&lt;p&gt;Start chatting with agent to generate stories or storyboards!&lt;/p&gt; 
&lt;h2&gt;Cases&lt;/h2&gt; 
&lt;img width="889" height="1103" alt="Frame 122" src="https://github.com/user-attachments/assets/90503110-0f5c-4297-bbfe-6d35e3f54d4c" /&gt; 
&lt;ul&gt; 
 &lt;li&gt;Prompt: Help me place this character in six different scenes, all in front of landmark buildings from around the world. The lighting is harmonious. He takes photos from all over the world, realistic, with warm light, high picture quality, and a picture ratio of 9:16&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/4e2634b3-9068-47cd-a18f-ddde8f218d25" alt="814c563b08f6ef44de0c2c31f0fdd00b-min" /&gt;&lt;/p&gt; 
&lt;img width="1000" alt="Screenshot 2025-06-02 at 3 51 56 AM" src="https://github.com/user-attachments/assets/5d8efe74-99b0-41bc-aa3e-6f7b92b69c36" /&gt; 
&lt;img width="900" alt="Screenshot 2025-06-02 at 3 51 56 AM" src="https://github.com/user-attachments/assets/186982a9-5e4e-4ac1-a42c-c840092fd616" /&gt; 
&lt;img width="900" alt="Screenshot 2025-06-02 at 3 03 49 PM" src="https://github.com/user-attachments/assets/b8508efd-def8-40ed-8ab5-62ed3c26de67" /&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/2065cabd-af32-43b6-bc01-59a935d9a287" alt="image26" /&gt;&lt;/p&gt; 
&lt;h2&gt;Team and Enterprise Support:&lt;/h2&gt; 
&lt;p&gt;Support for multi-user private deployment of enterprise teams, ensuring privacy and security.&lt;/p&gt; 
&lt;p&gt;Please contact via email: &lt;a href="mailto:aifoxdw@gmail.com"&gt;aifoxdw@gmail.com&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;WeChat: aifox1 &lt;img width="500" alt="Screenshot 2025-06-02 at 3 51 56 AM" src="https://github.com/user-attachments/assets/d5c54eda-120b-4fc2-a571-68fcab440868" /&gt;&lt;/p&gt; 
&lt;h2&gt;Manual Install (For Linux or local builds)&lt;/h2&gt; 
&lt;p&gt;ğŸŸ  &lt;strong&gt;Need Python version &amp;gt;=3.12&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;First git clone this repo:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;git clone https://github.com/11cafe/localart&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;cd react&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;npm install --force&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;npx vite build&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;cd ../server&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;python main.py&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;Development&lt;/h2&gt; 
&lt;p&gt;ğŸŸ  &lt;strong&gt;Need Python version &amp;gt;=3.12&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;VSCode/Cursor Install Extensionsï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Black Formatter by ms-python (ms-python.black-formatter)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;code&gt;cd react&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;npm install --force &amp;amp;&amp;amp; npm run dev&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;cd server&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;python main.py&lt;/code&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>heroui-inc/heroui</title>
      <link>https://github.com/heroui-inc/heroui</link>
      <description>&lt;p&gt;ğŸš€ Beautiful, fast and modern React UI library. (Previously NextUI)&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://heroui.com"&gt; &lt;img width="20%" src="https://raw.githubusercontent.com/heroui-inc/heroui/main/apps/docs/public/isotipo.png" alt="heorui" /&gt; &lt;/a&gt;&lt;/p&gt;
&lt;h1 align="center"&gt;&lt;a href="https://heroui.com"&gt;HeroUI&lt;/a&gt;&lt;/h1&gt;
&lt;a href="https://heroui.com"&gt; &lt;/a&gt; 
&lt;p&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/heroui-inc/heroui/raw/main/LICENSE"&gt; &lt;img src="https://img.shields.io/npm/l/@heroui/react?style=flat" alt="License" /&gt; &lt;/a&gt; &lt;a href="https://codecov.io/gh/jrgarciadev/nextui"&gt; &lt;img src="https://codecov.io/gh/jrgarciadev/nextui/branch/main/graph/badge.svg?token=QJF2QKR5N4" alt="codecov badge" /&gt; &lt;/a&gt; 
 &lt;!-- &lt;a href="https://github.com/heroui-inc/heroui/actions/workflows/main.yaml"&gt;
    &lt;img src="https://github.com/heroui-inc/heroui/actions/workflows/main.yaml/badge.svg" alt="CI/CD heroui"&gt;
  &lt;/a&gt; --&gt; &lt;a href="https://www.npmjs.com/package/@heroui/react"&gt; &lt;img src="https://img.shields.io/npm/dm/@heroui/react.svg?style=flat-round" alt="npm downloads" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;Visit &lt;a aria-label="heroui learn" href="https://heroui.com/learn"&gt;&lt;/a&gt;&lt;a href="https://heroui.com/guide"&gt;https://heroui.com/guide&lt;/a&gt; to get started with HeroUI.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;Visit &lt;a href="https://heroui.com/docs"&gt;https://heroui.com/docs&lt;/a&gt; to view the full documentation.&lt;/p&gt; 
&lt;h2&gt;Storybook&lt;/h2&gt; 
&lt;p&gt;Visit &lt;a href="https://storybook.heroui.com/"&gt;https://storybook.heroui.com&lt;/a&gt; to view the storybook for all components.&lt;/p&gt; 
&lt;h2&gt;Canary Release&lt;/h2&gt; 
&lt;p&gt;Canary versions are available after every merge into &lt;code&gt;canary&lt;/code&gt; branch. You can install the packages with the tag &lt;code&gt;canary&lt;/code&gt; in npm to use the latest changes before the next production release.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://canary.heroui.com/docs"&gt;Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://canary-sb.heroui.com"&gt;Storybook&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;p&gt;We're excited to see the community adopt HeroUI, raise issues, and provide feedback. Whether it's a feature request, bug report, or a project to showcase, please get involved!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://discord.gg/9b6yyZKmH4"&gt;Discord&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://x.com/hero_ui"&gt;X&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/heroui-inc/heroui/discussions"&gt;GitHub Discussions&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions are always welcome!&lt;/p&gt; 
&lt;p&gt;See &lt;a href="https://github.com/heroui-inc/heroui/raw/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for ways to get started.&lt;/p&gt; 
&lt;p&gt;Please adhere to this project's &lt;a href="https://github.com/heroui-inc/heroui/raw/main/CODE_OF_CONDUCT.md"&gt;CODE_OF_CONDUCT&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://choosealicense.com/licenses/mit/"&gt;MIT&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>emcie-co/parlant</title>
      <link>https://github.com/emcie-co/parlant</link>
      <description>&lt;p&gt;LLM agents built for control. Designed for real-world use. Deployed in minutes.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://github.com/emcie-co/parlant/blob/develop/docs/LogoTransparentLight.png?raw=true" /&gt; 
  &lt;img alt="Parlant - AI Agent Framework" src="https://github.com/emcie-co/parlant/raw/develop/docs/LogoTransparentDark.png?raw=true" width="400" /&gt; 
 &lt;/picture&gt; 
 &lt;h3&gt;Finally, LLM agents that actually follow instructions&lt;/h3&gt; 
 &lt;p&gt; &lt;a href="https://www.parlant.io/" target="_blank"&gt;ğŸŒ Website&lt;/a&gt; â€¢ &lt;a href="https://www.parlant.io/docs/quickstart/installation" target="_blank"&gt;âš¡ Quick Start&lt;/a&gt; â€¢ &lt;a href="https://discord.gg/duxWqxKk6J" target="_blank"&gt;ğŸ’¬ Discord&lt;/a&gt; â€¢ &lt;a href="https://www.parlant.io/docs/quickstart/examples" target="_blank"&gt;ğŸ“– Examples&lt;/a&gt; &lt;/p&gt; 
 &lt;p&gt; 
  &lt;!-- Keep these links. Translations will automatically update with the README. --&gt; &lt;a href="https://zdoc.app/de/emcie-co/parlant"&gt;Deutsch&lt;/a&gt; | &lt;a href="https://zdoc.app/es/emcie-co/parlant"&gt;EspaÃ±ol&lt;/a&gt; | &lt;a href="https://zdoc.app/fr/emcie-co/parlant"&gt;franÃ§ais&lt;/a&gt; | &lt;a href="https://zdoc.app/ja/emcie-co/parlant"&gt;æ—¥æœ¬èª&lt;/a&gt; | &lt;a href="https://zdoc.app/ko/emcie-co/parlant"&gt;í•œêµ­ì–´&lt;/a&gt; | &lt;a href="https://zdoc.app/pt/emcie-co/parlant"&gt;PortuguÃªs&lt;/a&gt; | &lt;a href="https://zdoc.app/ru/emcie-co/parlant"&gt;Ğ ÑƒÑÑĞºĞ¸Ğ¹&lt;/a&gt; | &lt;a href="https://zdoc.app/zh/emcie-co/parlant"&gt;ä¸­æ–‡&lt;/a&gt; &lt;/p&gt; 
 &lt;p&gt; &lt;a href="https://pypi.org/project/parlant/"&gt;&lt;img alt="PyPI" src="https://img.shields.io/pypi/v/parlant?color=blue" /&gt;&lt;/a&gt; &lt;img alt="Python 3.10+" src="https://img.shields.io/badge/python-3.10+-blue" /&gt; &lt;a href="https://opensource.org/licenses/Apache-2.0"&gt;&lt;img alt="License" src="https://img.shields.io/badge/license-Apache%202.0-green" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/duxWqxKk6J"&gt;&lt;img alt="Discord" src="https://img.shields.io/discord/1312378700993663007?color=7289da&amp;amp;logo=discord&amp;amp;logoColor=white" /&gt;&lt;/a&gt; &lt;img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/emcie-co/parlant?style=social" /&gt; &lt;/p&gt; 
 &lt;a href="https://trendshift.io/repositories/12768" target="_blank"&gt; &lt;img src="https://trendshift.io/api/badge/repositories/12768" alt="Trending on TrendShift" style="width: 250px; height: 55px;" width="250" height="55" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸ¯ The Problem Every AI Developer Faces&lt;/h2&gt; 
&lt;p&gt;You build an AI agent. It works great in testing. Then real users start talking to it and...&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;âŒ It ignores your carefully crafted system prompts&lt;/li&gt; 
 &lt;li&gt;âŒ It hallucinates responses in critical moments&lt;/li&gt; 
 &lt;li&gt;âŒ It can't handle edge cases consistently&lt;/li&gt; 
 &lt;li&gt;âŒ Each conversation feels like a roll of the dice&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Sound familiar?&lt;/strong&gt; You're not alone. This is the #1 pain point for developers building production AI agents.&lt;/p&gt; 
&lt;h2&gt;âš¡ The Solution: Stop Fighting Prompts, Teach Principles&lt;/h2&gt; 
&lt;p&gt;Parlant flips the script on AI agent development. Instead of hoping your LLM will follow instructions, &lt;strong&gt;Parlant ensures it&lt;/strong&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Traditional approach: Cross your fingers ğŸ¤
system_prompt = "You are a helpful assistant. Please follow these 47 rules..."

# Parlant approach: Ensured compliance âœ…
await agent.create_guideline(
    condition="Customer asks about refunds",
    action="Check order status first to see if eligible",
    tools=[check_order_status],
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://www.parlant.io/blog/how-parlant-guarantees-compliance"&gt;âœ… Blog: How Parlant Ensures Agent Compliance&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;Parlant gives you all the structure you need to build customer-facing agents that behave exactly as your business requires:&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://parlant.io/docs/concepts/customization/journeys"&gt;Journeys&lt;/a&gt;&lt;/strong&gt;: Define clear customer journeys and how your agent should respond at each step.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://parlant.io/docs/concepts/customization/guidelines"&gt;Behavioral Guidelines&lt;/a&gt;&lt;/strong&gt;: Easily craft agent behavior; Parlant will match the relevant elements contextually.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://parlant.io/docs/concepts/customization/tools"&gt;Tool Use&lt;/a&gt;&lt;/strong&gt;: Attach external APIs, data fetchers, or backend services to specific interaction events.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://parlant.io/docs/concepts/customization/glossary"&gt;Domain Adaptation&lt;/a&gt;&lt;/strong&gt;: Teach your agent domain-specific terminology and craft personalized responses.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://parlant.io/docs/concepts/customization/canned-responses"&gt;Canned Responses&lt;/a&gt;&lt;/strong&gt;: Use response templates to eliminate hallucinations and guarantee style consistency.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://parlant.io/docs/advanced/explainability"&gt;Explainability&lt;/a&gt;&lt;/strong&gt;: Understand why and when each guideline was matched and followed.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center"&gt; 
 &lt;h2&gt;ğŸš€ Get Your Agent Running in 60 Seconds&lt;/h2&gt; 
&lt;/div&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install parlant
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import parlant.sdk as p

@p.tool
async def get_weather(context: p.ToolContext, city: str) -&amp;gt; p.ToolResult:
    # Your weather API logic here
    return p.ToolResult(f"Sunny, 72Â°F in {city}")

@p.tool
async def get_datetime(context: p.ToolContext) -&amp;gt; p.ToolResult:
    from datetime import datetime
    return p.ToolResult(datetime.now())

async def main():
    async with p.Server() as server:
        agent = await server.create_agent(
            name="WeatherBot",
            description="Helpful weather assistant"
        )

        # Have the agent's context be updated on every response (though
        # update interval is customizable) using a context variable.
        await agent.create_variable(name="current-datetime", tool=get_datetime)

        # Control and guide agent behavior with natural language
        await agent.create_guideline(
            condition="User asks about weather",
            action="Get current weather and provide a friendly response with suggestions",
            tools=[get_weather]
        )

        # Add other (reliably enforced) behavioral modeling elements
        # ...

        # ğŸ‰ Test playground ready at http://localhost:8800
        # Integrate the official React widget into your app,
        # or follow the tutorial to build your own frontend!

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;That's it!&lt;/strong&gt; Your agent is running with ensured rule-following behavior.&lt;/p&gt; 
&lt;h2&gt;ğŸ¬ See It In Action&lt;/h2&gt; 
&lt;img alt="Parlant Demo" src="https://github.com/emcie-co/parlant/raw/develop/docs/demo.gif?raw=true" width="100%" /&gt; 
&lt;h2&gt;ğŸ”¥ Why Developers Are Switching to Parlant&lt;/h2&gt; 
&lt;table width="100%"&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;ğŸ—ï¸ &lt;strong&gt;Traditional AI Frameworks&lt;/strong&gt;&lt;/h3&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;âš¡ &lt;strong&gt;Parlant&lt;/strong&gt;&lt;/h3&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%"&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Write complex system prompts&lt;/li&gt; 
     &lt;li&gt;Hope the LLM follows them&lt;/li&gt; 
     &lt;li&gt;Debug unpredictable behaviors&lt;/li&gt; 
     &lt;li&gt;Scale by prompt engineering&lt;/li&gt; 
     &lt;li&gt;Cross fingers for reliability&lt;/li&gt; 
    &lt;/ul&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Define rules in natural language&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Ensured&lt;/strong&gt; rule compliance&lt;/li&gt; 
     &lt;li&gt;Predictable, consistent behavior&lt;/li&gt; 
     &lt;li&gt;Scale by adding guidelines&lt;/li&gt; 
     &lt;li&gt;Production-ready from day one&lt;/li&gt; 
    &lt;/ul&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;ğŸ¯ Perfect For Your Use Case&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;Financial Services&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;Healthcare&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;E-commerce&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;Legal Tech&lt;/strong&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;Compliance-first design&lt;/td&gt; 
    &lt;td align="center"&gt;HIPAA-ready agents&lt;/td&gt; 
    &lt;td align="center"&gt;Customer service at scale&lt;/td&gt; 
    &lt;td align="center"&gt;Precise legal guidance&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;Built-in risk management&lt;/td&gt; 
    &lt;td align="center"&gt;Patient data protection&lt;/td&gt; 
    &lt;td align="center"&gt;Order processing automation&lt;/td&gt; 
    &lt;td align="center"&gt;Document review assistance&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸ› ï¸ Enterprise-Grade Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ§­ Conversational Journeys&lt;/strong&gt; - Lead the customer step-by-step to a goal&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ¯ Dynamic Guideline Matching&lt;/strong&gt; - Context-aware rule application&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ”§ Reliable Tool Integration&lt;/strong&gt; - APIs, databases, external services&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ“Š Conversation Analytics&lt;/strong&gt; - Deep insights into agent behavior&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ”„ Iterative Refinement&lt;/strong&gt; - Continuously improve agent responses&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ›¡ï¸ Built-in Guardrails&lt;/strong&gt; - Prevent hallucination and off-topic responses&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ“± React Widget&lt;/strong&gt; - &lt;a href="https://github.com/emcie-co/parlant-chat-react"&gt;Drop-in chat UI for any web app&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ” Full Explainability&lt;/strong&gt; - Understand every decision your agent makes&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ“ˆ Join 10,000+ Developers Building Better AI&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;strong&gt;Companies using Parlant:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Financial institutions â€¢ Healthcare providers â€¢ Legal firms â€¢ E-commerce platforms&lt;/em&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://star-history.com/#emcie-co/parlant&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=emcie-co/parlant&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸŒŸ What Developers Are Saying&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;"By far the most elegant conversational AI framework that I've come across! Developing with Parlant is pure joy."&lt;/em&gt; &lt;strong&gt;â€” Vishal Ahuja, Senior Lead, Customer-Facing Conversational AI @ JPMorgan Chase&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;ğŸƒâ€â™‚ï¸ Quick Start Paths&lt;/h2&gt; 
&lt;table border="0"&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸ¯ I want to test it myself&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.parlant.io/docs/quickstart/installation"&gt;â†’ 5-minute quickstart&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸ› ï¸ I want to see an example&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.parlant.io/docs/quickstart/examples"&gt;â†’ Healthcare agent example&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸš€ I want to get involved&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://discord.gg/duxWqxKk6J"&gt;â†’ Join our Discord community&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;ğŸ¤ Community &amp;amp; Support&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ’¬ &lt;strong&gt;&lt;a href="https://discord.gg/duxWqxKk6J"&gt;Discord Community&lt;/a&gt;&lt;/strong&gt; - Get help from the team and community&lt;/li&gt; 
 &lt;li&gt;ğŸ“– &lt;strong&gt;&lt;a href="https://parlant.io/docs/quickstart/installation"&gt;Documentation&lt;/a&gt;&lt;/strong&gt; - Comprehensive guides and examples&lt;/li&gt; 
 &lt;li&gt;ğŸ› &lt;strong&gt;&lt;a href="https://github.com/emcie-co/parlant/issues"&gt;GitHub Issues&lt;/a&gt;&lt;/strong&gt; - Bug reports and feature requests&lt;/li&gt; 
 &lt;li&gt;ğŸ“§ &lt;strong&gt;&lt;a href="https://parlant.io/contact"&gt;Direct Support&lt;/a&gt;&lt;/strong&gt; - Direct line to our engineering team&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ“„ License&lt;/h2&gt; 
&lt;p&gt;Apache 2.0 - Use it anywhere, including commercial projects.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;strong&gt;Ready to build AI agents that actually work?&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;â­ &lt;strong&gt;Star this repo&lt;/strong&gt; â€¢ ğŸš€ &lt;strong&gt;&lt;a href="https://parlant.io/"&gt;Try Parlant now&lt;/a&gt;&lt;/strong&gt; â€¢ ğŸ’¬ &lt;strong&gt;&lt;a href="https://discord.gg/duxWqxKk6J"&gt;Join Discord&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Built with â¤ï¸ by the team at &lt;a href="https://emcie.co"&gt;Emcie&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>modelcontextprotocol/registry</title>
      <link>https://github.com/modelcontextprotocol/registry</link>
      <description>&lt;p&gt;A community driven registry service for Model Context Protocol (MCP) servers.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MCP Registry&lt;/h1&gt; 
&lt;p&gt;The MCP registry provides MCP clients with a list of MCP servers, like an app store for MCP servers.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/registry/main/docs/guides/publishing/publish-server.md"&gt;&lt;strong&gt;ğŸ“¤ Publish my MCP server&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://registry.modelcontextprotocol.io/docs"&gt;&lt;strong&gt;âš¡ï¸ Live API docs&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/registry/main/docs/explanations/ecosystem-vision.md"&gt;&lt;strong&gt;ğŸ‘€ Ecosystem vision&lt;/strong&gt;&lt;/a&gt; | ğŸ“– &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/registry/main/docs"&gt;Full documentation&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Development Status&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;2025-09-08 update&lt;/strong&gt;: The registry has launched in preview ğŸ‰ (&lt;a href="https://blog.modelcontextprotocol.io/posts/2025-09-08-mcp-registry-preview/"&gt;announcement blog post&lt;/a&gt;). While the system is now more stable, this is still a preview release and breaking changes or data resets may occur. A general availability (GA) release will follow later. We'd love your feedback in &lt;a href="https://github.com/modelcontextprotocol/registry/discussions/new?category=ideas"&gt;GitHub discussions&lt;/a&gt; or in the &lt;a href="https://discord.com/channels/1358869848138059966/1369487942862504016"&gt;#registry-dev Discord&lt;/a&gt; (&lt;a href="https://modelcontextprotocol.io/community/communication"&gt;joining details here&lt;/a&gt;).&lt;/p&gt; 
&lt;p&gt;Current key maintainers:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Adam Jones&lt;/strong&gt; (Anthropic) &lt;a href="https://github.com/domdomegg"&gt;@domdomegg&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Tadas Antanavicius&lt;/strong&gt; (PulseMCP) &lt;a href="https://github.com/tadasant"&gt;@tadasant&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Toby Padilla&lt;/strong&gt; (GitHub) &lt;a href="https://github.com/toby"&gt;@toby&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We use multiple channels for collaboration - see &lt;a href="https://modelcontextprotocol.io/community/communication"&gt;modelcontextprotocol.io/community/communication&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Often (but not always) ideas flow through this pipeline:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://modelcontextprotocol.io/community/communication"&gt;Discord&lt;/a&gt;&lt;/strong&gt; - Real-time community discussions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/modelcontextprotocol/registry/discussions"&gt;Discussions&lt;/a&gt;&lt;/strong&gt; - Propose and discuss product/technical requirements&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/modelcontextprotocol/registry/issues"&gt;Issues&lt;/a&gt;&lt;/strong&gt; - Track well-scoped technical work&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/modelcontextprotocol/registry/pulls"&gt;Pull Requests&lt;/a&gt;&lt;/strong&gt; - Contribute work towards issues&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Quick start:&lt;/h3&gt; 
&lt;h4&gt;Pre-requisites&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Docker&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Go 1.24.x&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;golangci-lint v2.4.0&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Running the server&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Start full development environment
make dev-compose
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This starts the registry at &lt;a href="http://localhost:8080"&gt;&lt;code&gt;localhost:8080&lt;/code&gt;&lt;/a&gt; with PostgreSQL and seed data. It can be configured with environment variables in &lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/registry/main/docker-compose.yml"&gt;docker-compose.yml&lt;/a&gt; - see &lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/registry/main/.env.example"&gt;.env.example&lt;/a&gt; for a reference.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Alternative: Local setup without Docker&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;Prerequisites:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;PostgreSQL running locally&lt;/li&gt; 
  &lt;li&gt;Go 1.24.x installed&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Build and run locally
make build
make dev-local
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;The service runs on &lt;a href="http://localhost:8080"&gt;&lt;code&gt;localhost:8080&lt;/code&gt;&lt;/a&gt; by default. This can be configured with environment variables in &lt;code&gt;.env&lt;/code&gt; - see &lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/registry/main/.env.example"&gt;.env.example&lt;/a&gt; for a reference.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Alternative: Running a pre-built Docker image&lt;/summary&gt; 
 &lt;p&gt;Pre-built Docker images are automatically published to GitHub Container Registry:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Run latest stable release
docker run -p 8080:8080 ghcr.io/modelcontextprotocol/registry:latest

# Run latest from main branch (continuous deployment)
docker run -p 8080:8080 ghcr.io/modelcontextprotocol/registry:main

# Run specific release version
docker run -p 8080:8080 ghcr.io/modelcontextprotocol/registry:v1.0.0

# Run development build from main branch
docker run -p 8080:8080 ghcr.io/modelcontextprotocol/registry:main-20250906-abc123d
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Available tags:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Releases&lt;/strong&gt;: &lt;code&gt;latest&lt;/code&gt;, &lt;code&gt;v1.0.0&lt;/code&gt;, &lt;code&gt;v1.1.0&lt;/code&gt;, etc.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Continuous&lt;/strong&gt;: &lt;code&gt;main&lt;/code&gt; (latest main branch build)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Development&lt;/strong&gt;: &lt;code&gt;main-&amp;lt;date&amp;gt;-&amp;lt;sha&amp;gt;&lt;/code&gt; (specific commit builds)&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h4&gt;Publishing a server&lt;/h4&gt; 
&lt;p&gt;To publish a server, we've built a simple CLI. You can use it with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Build the latest CLI
make publisher

# Use it!
./bin/mcp-publisher --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/registry/main/docs/guides/publishing/publish-server.md"&gt;the publisher guide&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h4&gt;Other commands&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run lint, unit tests and integration tests
make check
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;There are also a few more helpful commands for development. Run &lt;code&gt;make help&lt;/code&gt; to learn more, or look in &lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/registry/main/Makefile"&gt;Makefile&lt;/a&gt;.&lt;/p&gt; 
&lt;!--
For Claude and other AI tools: Always prefer make targets over custom commands where possible.
--&gt; 
&lt;h2&gt;Architecture&lt;/h2&gt; 
&lt;h3&gt;Project Structure&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;â”œâ”€â”€ cmd/                     # Application entry points
â”‚   â””â”€â”€ publisher/           # Server publishing tool
â”œâ”€â”€ data/                    # Seed data
â”œâ”€â”€ deploy/                  # Deployment configuration (Pulumi)
â”œâ”€â”€ docs/                    # Documentation
â”œâ”€â”€ internal/                # Private application code
â”‚   â”œâ”€â”€ api/                 # HTTP handlers and routing
â”‚   â”œâ”€â”€ auth/                # Authentication (GitHub OAuth, JWT, namespace blocking)
â”‚   â”œâ”€â”€ config/              # Configuration management
â”‚   â”œâ”€â”€ database/            # Data persistence (PostgreSQL, in-memory)
â”‚   â”œâ”€â”€ service/             # Business logic
â”‚   â”œâ”€â”€ telemetry/           # Metrics and monitoring
â”‚   â””â”€â”€ validators/          # Input validation
â”œâ”€â”€ pkg/                     # Public packages
â”‚   â”œâ”€â”€ api/                 # API types and structures
â”‚   â”‚   â””â”€â”€ v0/              # Version 0 API types
â”‚   â””â”€â”€ model/               # Data models for server.json
â”œâ”€â”€ scripts/                 # Development and testing scripts
â”œâ”€â”€ tests/                   # Integration tests
â””â”€â”€ tools/                   # CLI tools and utilities
    â””â”€â”€ validate-*.sh        # Schema validation tools
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Authentication&lt;/h3&gt; 
&lt;p&gt;Publishing supports multiple authentication methods:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;GitHub OAuth&lt;/strong&gt; - For publishing by logging into GitHub&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;GitHub OIDC&lt;/strong&gt; - For publishing from GitHub Actions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;DNS verification&lt;/strong&gt; - For proving ownership of a domain and its subdomains&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;HTTP verification&lt;/strong&gt; - For proving ownership of a domain&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The registry validates namespace ownership when publishing. E.g. to publish...:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;io.github.domdomegg/my-cool-mcp&lt;/code&gt; you must login to GitHub as &lt;code&gt;domdomegg&lt;/code&gt;, or be in a GitHub Action on domdomegg's repos&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;me.adamjones/my-cool-mcp&lt;/code&gt; you must prove ownership of &lt;code&gt;adamjones.me&lt;/code&gt; via DNS or HTTP challenge&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;More documentation&lt;/h2&gt; 
&lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/registry/main/docs"&gt;documentation&lt;/a&gt; for more details if your question has not been answered here!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>resemble-ai/chatterbox</title>
      <link>https://github.com/resemble-ai/chatterbox</link>
      <description>&lt;p&gt;SoTA open-source TTS&lt;/p&gt;&lt;hr&gt;&lt;img width="1200" height="600" alt="Chatterbox-Multilingual" src="https://www.resemble.ai/wp-content/uploads/2025/09/Chatterbox-Multilingual-1.png" /&gt; 
&lt;h1&gt;Chatterbox TTS&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://resemble-ai.github.io/chatterbox_demopage/"&gt;&lt;img src="https://img.shields.io/badge/listen-demo_samples-blue" alt="Alt Text" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/spaces/ResembleAI/Chatterbox"&gt;&lt;img src="https://huggingface.co/datasets/huggingface/badges/resolve/main/open-in-hf-spaces-sm.svg?sanitize=true" alt="Alt Text" /&gt;&lt;/a&gt; &lt;a href="https://podonos.com/resembleai/chatterbox"&gt;&lt;img src="https://static-public.podonos.com/badges/insight-on-pdns-sm-dark.svg?sanitize=true" alt="Alt Text" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/rJq9cRJBJ6"&gt;&lt;img src="https://img.shields.io/discord/1377773249798344776?label=join%20discord&amp;amp;logo=discord&amp;amp;style=flat" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;_Made with â™¥ï¸ by &lt;a href="https://resemble.ai" target="_blank"&gt;&lt;img width="100" alt="resemble-logo-horizontal" src="https://github.com/user-attachments/assets/35cf756b-3506-4943-9c72-c05ddfa4e525" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;We're excited to introduce &lt;strong&gt;Chatterbox Multilingual&lt;/strong&gt;, &lt;a href="https://resemble.ai"&gt;Resemble AI's&lt;/a&gt; first production-grade open source TTS model supporting &lt;strong&gt;23 languages&lt;/strong&gt; out of the box. Licensed under MIT, Chatterbox has been benchmarked against leading closed-source systems like ElevenLabs, and is consistently preferred in side-by-side evaluations.&lt;/p&gt; 
&lt;p&gt;Whether you're working on memes, videos, games, or AI agents, Chatterbox brings your content to life across languages. It's also the first open source TTS model to support &lt;strong&gt;emotion exaggeration control&lt;/strong&gt; with robust &lt;strong&gt;multilingual zero-shot voice cloning&lt;/strong&gt;. Try the english only version now on our &lt;a href="https://huggingface.co/spaces/ResembleAI/Chatterbox"&gt;English Hugging Face Gradio app.&lt;/a&gt;. Or try the multilingual version on our &lt;a href="https://huggingface.co/spaces/ResembleAI/Chatterbox-Multilingual-TTS"&gt;Multilingual Hugging Face Gradio app.&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you like the model but need to scale or tune it for higher accuracy, check out our competitively priced TTS service (&lt;a href="https://resemble.ai"&gt;link&lt;/a&gt;). It delivers reliable performance with ultra-low latency of sub 200msâ€”ideal for production use in agents, applications, or interactive media.&lt;/p&gt; 
&lt;h1&gt;Key Details&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;Multilingual, zero-shot TTS supporting 23 languages&lt;/li&gt; 
 &lt;li&gt;SoTA zeroshot English TTS&lt;/li&gt; 
 &lt;li&gt;0.5B Llama backbone&lt;/li&gt; 
 &lt;li&gt;Unique exaggeration/intensity control&lt;/li&gt; 
 &lt;li&gt;Ultra-stable with alignment-informed inference&lt;/li&gt; 
 &lt;li&gt;Trained on 0.5M hours of cleaned data&lt;/li&gt; 
 &lt;li&gt;Watermarked outputs&lt;/li&gt; 
 &lt;li&gt;Easy voice conversion script&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://podonos.com/resembleai/chatterbox"&gt;Outperforms ElevenLabs&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Supported Languages&lt;/h1&gt; 
&lt;p&gt;Arabic (ar) â€¢ Danish (da) â€¢ German (de) â€¢ Greek (el) â€¢ English (en) â€¢ Spanish (es) â€¢ Finnish (fi) â€¢ French (fr) â€¢ Hebrew (he) â€¢ Hindi (hi) â€¢ Italian (it) â€¢ Japanese (ja) â€¢ Korean (ko) â€¢ Malay (ms) â€¢ Dutch (nl) â€¢ Norwegian (no) â€¢ Polish (pl) â€¢ Portuguese (pt) â€¢ Russian (ru) â€¢ Swedish (sv) â€¢ Swahili (sw) â€¢ Turkish (tr) â€¢ Chinese (zh)&lt;/p&gt; 
&lt;h1&gt;Tips&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;General Use (TTS and Voice Agents):&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Ensure that the reference clip matches the specified language tag. Otherwise, language transfer outputs may inherit the accent of the reference clipâ€™s language. To mitigate this, set &lt;code&gt;cfg_weight&lt;/code&gt; to &lt;code&gt;0&lt;/code&gt;.&lt;/li&gt; 
   &lt;li&gt;The default settings (&lt;code&gt;exaggeration=0.5&lt;/code&gt;, &lt;code&gt;cfg_weight=0.5&lt;/code&gt;) work well for most prompts across all languages.&lt;/li&gt; 
   &lt;li&gt;If the reference speaker has a fast speaking style, lowering &lt;code&gt;cfg_weight&lt;/code&gt; to around &lt;code&gt;0.3&lt;/code&gt; can improve pacing.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Expressive or Dramatic Speech:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Try lower &lt;code&gt;cfg_weight&lt;/code&gt; values (e.g. &lt;code&gt;~0.3&lt;/code&gt;) and increase &lt;code&gt;exaggeration&lt;/code&gt; to around &lt;code&gt;0.7&lt;/code&gt; or higher.&lt;/li&gt; 
   &lt;li&gt;Higher &lt;code&gt;exaggeration&lt;/code&gt; tends to speed up speech; reducing &lt;code&gt;cfg_weight&lt;/code&gt; helps compensate with slower, more deliberate pacing.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Installation&lt;/h1&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install chatterbox-tts
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, you can install from source:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# conda create -yn chatterbox python=3.11
# conda activate chatterbox

git clone https://github.com/resemble-ai/chatterbox.git
cd chatterbox
pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We developed and tested Chatterbox on Python 3.11 on Debian 11 OS; the versions of the dependencies are pinned in &lt;code&gt;pyproject.toml&lt;/code&gt; to ensure consistency. You can modify the code or dependencies in this installation mode.&lt;/p&gt; 
&lt;h1&gt;Usage&lt;/h1&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import torchaudio as ta
from chatterbox.tts import ChatterboxTTS
from chatterbox.mtl_tts import ChatterboxMultilingualTTS

# English example
model = ChatterboxTTS.from_pretrained(device="cuda")

text = "Ezreal and Jinx teamed up with Ahri, Yasuo, and Teemo to take down the enemy's Nexus in an epic late-game pentakill."
wav = model.generate(text)
ta.save("test-english.wav", wav, model.sr)

# Multilingual examples
multilingual_model = ChatterboxMultilingualTTS.from_pretrained(device=device)

french_text = "Bonjour, comment Ã§a va? Ceci est le modÃ¨le de synthÃ¨se vocale multilingue Chatterbox, il prend en charge 23 langues."
wav_french = multilingual_model.generate(spanish_text, language_id="fr")
ta.save("test-french.wav", wav_french, model.sr)

chinese_text = "ä½ å¥½ï¼Œä»Šå¤©å¤©æ°”çœŸä¸é”™ï¼Œå¸Œæœ›ä½ æœ‰ä¸€ä¸ªæ„‰å¿«çš„å‘¨æœ«ã€‚"
wav_chinese = multilingual_model.generate(chinese_text, language_id="zh")
ta.save("test-chinese.wav", wav_chinese, model.sr)

# If you want to synthesize with a different voice, specify the audio prompt
AUDIO_PROMPT_PATH = "YOUR_FILE.wav"
wav = model.generate(text, audio_prompt_path=AUDIO_PROMPT_PATH)
ta.save("test-2.wav", wav, model.sr)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;code&gt;example_tts.py&lt;/code&gt; and &lt;code&gt;example_vc.py&lt;/code&gt; for more examples.&lt;/p&gt; 
&lt;h1&gt;Acknowledgements&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/FunAudioLLM/CosyVoice"&gt;Cosyvoice&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning"&gt;Real-Time-Voice-Cloning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/yl4579/HiFTNet"&gt;HiFT-GAN&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/meta-llama/llama3"&gt;Llama 3&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/xingchensong/S3Tokenizer"&gt;S3Tokenizer&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Built-in PerTh Watermarking for Responsible AI&lt;/h1&gt; 
&lt;p&gt;Every audio file generated by Chatterbox includes &lt;a href="https://github.com/resemble-ai/perth"&gt;Resemble AI's Perth (Perceptual Threshold) Watermarker&lt;/a&gt; - imperceptible neural watermarks that survive MP3 compression, audio editing, and common manipulations while maintaining nearly 100% detection accuracy.&lt;/p&gt; 
&lt;h2&gt;Watermark extraction&lt;/h2&gt; 
&lt;p&gt;You can look for the watermark using the following script.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import perth
import librosa

AUDIO_PATH = "YOUR_FILE.wav"

# Load the watermarked audio
watermarked_audio, sr = librosa.load(AUDIO_PATH, sr=None)

# Initialize watermarker (same as used for embedding)
watermarker = perth.PerthImplicitWatermarker()

# Extract watermark
watermark = watermarker.get_watermark(watermarked_audio, sample_rate=sr)
print(f"Extracted watermark: {watermark}")
# Output: 0.0 (no watermark) or 1.0 (watermarked)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Official Discord&lt;/h1&gt; 
&lt;p&gt;ğŸ‘‹ Join us on &lt;a href="https://discord.gg/rJq9cRJBJ6"&gt;Discord&lt;/a&gt; and let's build something awesome together!&lt;/p&gt; 
&lt;h1&gt;Citation&lt;/h1&gt; 
&lt;p&gt;If you find this model useful, please consider citing.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@misc{chatterboxtts2025,
  author       = {{Resemble AI}},
  title        = {{Chatterbox-TTS}},
  year         = {2025},
  howpublished = {\url{https://github.com/resemble-ai/chatterbox}},
  note         = {GitHub repository}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Disclaimer&lt;/h1&gt; 
&lt;p&gt;Don't use this model to do bad things. Prompts are sourced from freely available data on the internet.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Stirling-Tools/Stirling-PDF</title>
      <link>https://github.com/Stirling-Tools/Stirling-PDF</link>
      <description>&lt;p&gt;#1 Locally hosted web application that allows you to perform various operations on PDF files&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Stirling-Tools/Stirling-PDF/main/docs/stirling.png" width="80" /&gt;&lt;/p&gt; 
&lt;h1 align="center"&gt;Stirling-PDF&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://hub.docker.com/r/frooodle/s-pdf"&gt;&lt;img src="https://img.shields.io/docker/pulls/frooodle/s-pdf" alt="Docker Pulls" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/HYmhKj45pU"&gt;&lt;img src="https://img.shields.io/discord/1068636748814483718?label=Discord" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://scorecard.dev/viewer/?uri=github.com/Stirling-Tools/Stirling-PDF"&gt;&lt;img src="https://api.scorecard.dev/projects/github.com/Stirling-Tools/Stirling-PDF/badge" alt="OpenSSF Scorecard" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Stirling-Tools/stirling-pdf"&gt;&lt;img src="https://img.shields.io/github/stars/stirling-tools/stirling-pdf?style=social" alt="GitHub Repo stars" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.producthunt.com/posts/stirling-pdf?embed=true&amp;amp;utm_source=badge-featured&amp;amp;utm_medium=badge&amp;amp;utm_souce=badge-stirling-pdf" target="_blank"&gt;&lt;img src="https://api.producthunt.com/widgets/embed-image/v1/featured.svg?post_id=641239&amp;amp;theme=light" alt="Stirling PDF - Open source locally hosted web PDF editor | Product Hunt" style="width: 250px; height: 54px;" width="250" height="54" /&gt;&lt;/a&gt; &lt;a href="https://cloud.digitalocean.com/apps/new?repo=https://github.com/Stirling-Tools/Stirling-PDF/tree/digitalOcean&amp;amp;refcode=c3210994b1af"&gt;&lt;img src="https://www.deploytodo.com/do-btn-blue.svg?sanitize=true" alt="Deploy to DO" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.stirlingpdf.com"&gt;Stirling-PDF&lt;/a&gt; is a robust, locally hosted web-based PDF manipulation tool using Docker. It enables you to carry out various operations on PDF files, including splitting, merging, converting, reorganizing, adding images, rotating, compressing, and more. This locally hosted web application has evolved to encompass a comprehensive set of features, addressing all your PDF requirements.&lt;/p&gt; 
&lt;p&gt;All files and PDFs exist either exclusively on the client side, reside in server memory only during task execution, or temporarily reside in a file solely for the execution of the task. Any file downloaded by the user will have been deleted from the server by that point.&lt;/p&gt; 
&lt;p&gt;Homepage: &lt;a href="https://stirlingpdf.com"&gt;https://stirlingpdf.com&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;All documentation available at &lt;a href="https://docs.stirlingpdf.com/"&gt;https://docs.stirlingpdf.com/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Stirling-Tools/Stirling-PDF/main/images/stirling-home.jpg" alt="stirling-home" /&gt;&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;50+ PDF Operations&lt;/li&gt; 
 &lt;li&gt;Parallel file processing and downloads&lt;/li&gt; 
 &lt;li&gt;Dark mode support&lt;/li&gt; 
 &lt;li&gt;Custom download options&lt;/li&gt; 
 &lt;li&gt;Custom 'Pipelines' to run multiple features in a automated queue&lt;/li&gt; 
 &lt;li&gt;API for integration with external scripts&lt;/li&gt; 
 &lt;li&gt;Optional Login and Authentication support (see &lt;a href="https://docs.stirlingpdf.com/Advanced%20Configuration/System%20and%20Security"&gt;here&lt;/a&gt; for documentation)&lt;/li&gt; 
 &lt;li&gt;Database Backup and Import (see &lt;a href="https://docs.stirlingpdf.com/Advanced%20Configuration/DATABASE"&gt;here&lt;/a&gt; for documentation)&lt;/li&gt; 
 &lt;li&gt;Enterprise features like SSO (see &lt;a href="https://docs.stirlingpdf.com/Advanced%20Configuration/Single%20Sign-On%20Configuration"&gt;here&lt;/a&gt; for documentation)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;PDF Features&lt;/h2&gt; 
&lt;h3&gt;Page Operations&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;View and modify PDFs - View multi-page PDFs with custom viewing, sorting, and searching. Plus, on-page edit features like annotating, drawing, and adding text and images. (Using PDF.js with Joxit and Liberation fonts)&lt;/li&gt; 
 &lt;li&gt;Full interactive GUI for merging/splitting/rotating/moving PDFs and their pages&lt;/li&gt; 
 &lt;li&gt;Merge multiple PDFs into a single resultant file&lt;/li&gt; 
 &lt;li&gt;Split PDFs into multiple files at specified page numbers or extract all pages as individual files&lt;/li&gt; 
 &lt;li&gt;Reorganize PDF pages into different orders&lt;/li&gt; 
 &lt;li&gt;Rotate PDFs in 90-degree increments&lt;/li&gt; 
 &lt;li&gt;Remove pages&lt;/li&gt; 
 &lt;li&gt;Multi-page layout (format PDFs into a multi-paged page)&lt;/li&gt; 
 &lt;li&gt;Scale page contents size by set percentage&lt;/li&gt; 
 &lt;li&gt;Adjust contrast&lt;/li&gt; 
 &lt;li&gt;Crop PDF&lt;/li&gt; 
 &lt;li&gt;Auto-split PDF (with physically scanned page dividers)&lt;/li&gt; 
 &lt;li&gt;Extract page(s)&lt;/li&gt; 
 &lt;li&gt;Convert PDF to a single page&lt;/li&gt; 
 &lt;li&gt;Overlay PDFs on top of each other&lt;/li&gt; 
 &lt;li&gt;PDF to a single page&lt;/li&gt; 
 &lt;li&gt;Split PDF by sections&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Conversion Operations&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Convert PDFs to and from images&lt;/li&gt; 
 &lt;li&gt;Convert any common file to PDF (using LibreOffice)&lt;/li&gt; 
 &lt;li&gt;Convert PDF to Word/PowerPoint/others (using LibreOffice)&lt;/li&gt; 
 &lt;li&gt;Convert HTML to PDF&lt;/li&gt; 
 &lt;li&gt;Convert PDF to XML&lt;/li&gt; 
 &lt;li&gt;Convert PDF to CSV&lt;/li&gt; 
 &lt;li&gt;URL to PDF&lt;/li&gt; 
 &lt;li&gt;Markdown to PDF&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Security &amp;amp; Permissions&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Add and remove passwords&lt;/li&gt; 
 &lt;li&gt;Change/set PDF permissions&lt;/li&gt; 
 &lt;li&gt;Add watermark(s)&lt;/li&gt; 
 &lt;li&gt;Certify/sign PDFs&lt;/li&gt; 
 &lt;li&gt;Sanitize PDFs&lt;/li&gt; 
 &lt;li&gt;Auto-redact text&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Other Operations&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Add/generate/write signatures&lt;/li&gt; 
 &lt;li&gt;Split by Size or PDF&lt;/li&gt; 
 &lt;li&gt;Repair PDFs&lt;/li&gt; 
 &lt;li&gt;Detect and remove blank pages&lt;/li&gt; 
 &lt;li&gt;Compare two PDFs and show differences in text&lt;/li&gt; 
 &lt;li&gt;Add images to PDFs&lt;/li&gt; 
 &lt;li&gt;Compress PDFs to decrease their filesize (using qpdf)&lt;/li&gt; 
 &lt;li&gt;Extract images from PDF&lt;/li&gt; 
 &lt;li&gt;Remove images from PDF&lt;/li&gt; 
 &lt;li&gt;Extract images from scans&lt;/li&gt; 
 &lt;li&gt;Remove annotations&lt;/li&gt; 
 &lt;li&gt;Add page numbers&lt;/li&gt; 
 &lt;li&gt;Auto-rename files by detecting PDF header text&lt;/li&gt; 
 &lt;li&gt;OCR on PDF (using Tesseract OCR)&lt;/li&gt; 
 &lt;li&gt;PDF/A conversion (using LibreOffice)&lt;/li&gt; 
 &lt;li&gt;Edit metadata&lt;/li&gt; 
 &lt;li&gt;Flatten PDFs&lt;/li&gt; 
 &lt;li&gt;Get all information on a PDF to view or export as JSON&lt;/li&gt; 
 &lt;li&gt;Show/detect embedded JavaScript&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;ğŸ“– Get Started&lt;/h1&gt; 
&lt;p&gt;Visit our comprehensive documentation at &lt;a href="https://docs.stirlingpdf.com"&gt;docs.stirlingpdf.com&lt;/a&gt; for:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Installation guides for all platforms&lt;/li&gt; 
 &lt;li&gt;Configuration options&lt;/li&gt; 
 &lt;li&gt;Feature documentation&lt;/li&gt; 
 &lt;li&gt;API reference&lt;/li&gt; 
 &lt;li&gt;Security setup&lt;/li&gt; 
 &lt;li&gt;Enterprise features&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Supported Languages&lt;/h2&gt; 
&lt;p&gt;Stirling-PDF currently supports 40 languages!&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Language&lt;/th&gt; 
   &lt;th&gt;Progress&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Arabic (Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©) (ar_AR)&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://geps.dev/progress/61" alt="61%" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Azerbaijani (AzÉ™rbaycan Dili) (az_AZ)&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://geps.dev/progress/62" alt="62%" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Basque (Euskara) (eu_ES)&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://geps.dev/progress/36" alt="36%" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Bulgarian (Ğ‘ÑŠĞ»Ğ³Ğ°Ñ€ÑĞºĞ¸) (bg_BG)&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://geps.dev/progress/68" alt="68%" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Catalan (CatalÃ ) (ca_CA)&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://geps.dev/progress/67" alt="67%" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Croatian (Hrvatski) (hr_HR)&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://geps.dev/progress/60" alt="60%" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Czech (ÄŒesky) (cs_CZ)&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://geps.dev/progress/69" alt="69%" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Danish (Dansk) (da_DK)&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://geps.dev/progress/61" alt="61%" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Dutch (Nederlands) (nl_NL)&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://geps.dev/progress/60" alt="60%" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;English (English) (en_GB)&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://geps.dev/progress/100" alt="100%" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;English (US) (en_US)&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://geps.dev/progress/100" alt="100%" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;French (FranÃ§ais) (fr_FR)&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://geps.dev/progress/88" alt="88%" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;German (Deutsch) (de_DE)&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://geps.dev/progress/97" alt="97%" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Greek (Î•Î»Î»Î·Î½Î¹ÎºÎ¬) (el_GR)&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://geps.dev/progress/67" alt="67%" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Hindi (à¤¹à¤¿à¤‚à¤¦à¥€) (hi_IN)&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://geps.dev/progress/67" alt="67%" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Hungarian (Magyar) (hu_HU)&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://geps.dev/progress/99" alt="99%" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Indonesian (Bahasa Indonesia) (id_ID)&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://geps.dev/progress/62" alt="62%" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Irish (Gaeilge) (ga_IE)&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://geps.dev/progress/68" alt="68%" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Italian (Italiano) (it_IT)&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://geps.dev/progress/98" alt="98%" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Japanese (æ—¥æœ¬èª) (ja_JP)&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://geps.dev/progress/92" alt="92%" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Korean (í•œêµ­ì–´) (ko_KR)&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://geps.dev/progress/67" alt="67%" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Norwegian (Norsk) (no_NB)&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://geps.dev/progress/66" alt="66%" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Persian (ÙØ§Ø±Ø³ÛŒ) (fa_IR)&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://geps.dev/progress/64" alt="64%" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Polish (Polski) (pl_PL)&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://geps.dev/progress/72" alt="72%" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Portuguese (PortuguÃªs) (pt_PT)&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://geps.dev/progress/68" alt="68%" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Portuguese Brazilian (PortuguÃªs) (pt_BR)&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://geps.dev/progress/76" alt="76%" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Romanian (RomÃ¢nÄƒ) (ro_RO)&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://geps.dev/progress/57" alt="57%" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Russian (Ğ ÑƒÑÑĞºĞ¸Ğ¹) (ru_RU)&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://geps.dev/progress/88" alt="88%" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Serbian Latin alphabet (Srpski) (sr_LATN_RS)&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://geps.dev/progress/94" alt="94%" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Simplified Chinese (ç®€ä½“ä¸­æ–‡) (zh_CN)&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://geps.dev/progress/93" alt="93%" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Slovakian (Slovensky) (sk_SK)&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://geps.dev/progress/51" alt="51%" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Slovenian (SlovenÅ¡Äina) (sl_SI)&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://geps.dev/progress/71" alt="71%" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Spanish (EspaÃ±ol) (es_ES)&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://geps.dev/progress/74" alt="74%" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Swedish (Svenska) (sv_SE)&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://geps.dev/progress/65" alt="65%" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Thai (à¹„à¸—à¸¢) (th_TH)&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://geps.dev/progress/59" alt="59%" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Tibetan (à½–à½¼à½‘à¼‹à½¡à½²à½‚à¼‹) (bo_CN)&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://geps.dev/progress/65" alt="65%" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Traditional Chinese (ç¹é«”ä¸­æ–‡) (zh_TW)&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://geps.dev/progress/99" alt="99%" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Turkish (TÃ¼rkÃ§e) (tr_TR)&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://geps.dev/progress/99" alt="99%" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ukrainian (Ğ£ĞºÑ€Ğ°Ñ—Ğ½ÑÑŒĞºĞ°) (uk_UA)&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://geps.dev/progress/70" alt="70%" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Vietnamese (Tiáº¿ng Viá»‡t) (vi_VN)&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://geps.dev/progress/57" alt="57%" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Malayalam (à´®à´²à´¯à´¾à´³à´‚) (ml_IN)&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://geps.dev/progress/73" alt="73%" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Stirling PDF Enterprise&lt;/h2&gt; 
&lt;p&gt;Stirling PDF offers an Enterprise edition of its software. This is the same great software but with added features, support and comforts. Check out our &lt;a href="https://docs.stirlingpdf.com/Pro"&gt;Enterprise docs&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ğŸ¤ Looking to contribute?&lt;/h2&gt; 
&lt;p&gt;Join our community:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Stirling-Tools/Stirling-PDF/main/CONTRIBUTING.md"&gt;Contribution Guidelines&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Stirling-Tools/Stirling-PDF/main/devGuide/HowToAddNewLanguage.md"&gt;Translation Guide (How to add custom languages)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Stirling-Tools/Stirling-PDF/main/devGuide/DeveloperGuide.md"&gt;Developer Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Stirling-Tools/Stirling-PDF/issues"&gt;Issue Tracker&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://discord.gg/HYmhKj45pU"&gt;Discord Community&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Cinnamon/kotaemon</title>
      <link>https://github.com/Cinnamon/kotaemon</link>
      <description>&lt;p&gt;An open-source RAG-based tool for chatting with your documents.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h1&gt;kotaemon&lt;/h1&gt; 
 &lt;p&gt;An open-source clean &amp;amp; customizable RAG UI for chatting with your documents. Built with both end users and developers in mind.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Cinnamon/kotaemon/main/docs/images/preview-graph.png" alt="Preview" /&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/11607" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/11607" alt="Cinnamon%2Fkotaemon | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://huggingface.co/spaces/cin-model/kotaemon"&gt;Live Demo #1&lt;/a&gt; | &lt;a href="https://huggingface.co/spaces/cin-model/kotaemon-demo"&gt;Live Demo #2&lt;/a&gt; | &lt;a href="https://cinnamon.github.io/kotaemon/online_install/"&gt;Online Install&lt;/a&gt; | &lt;a href="https://colab.research.google.com/drive/1eTfieec_UOowNizTJA1NjawBJH9y_1nn"&gt;Colab Notebook (Local RAG)&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://cinnamon.github.io/kotaemon/"&gt;User Guide&lt;/a&gt; | &lt;a href="https://cinnamon.github.io/kotaemon/development/"&gt;Developer Guide&lt;/a&gt; | &lt;a href="https://github.com/Cinnamon/kotaemon/issues"&gt;Feedback&lt;/a&gt; | &lt;a href="mailto:kotaemon.support@cinnamon.is"&gt;Contact&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://www.python.org/downloads/release/python-31013/"&gt;&lt;img src="https://img.shields.io/badge/python-3.10+-blue.svg?sanitize=true" alt="Python 3.10+" /&gt;&lt;/a&gt; &lt;a href="https://github.com/psf/black"&gt;&lt;img src="https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true" alt="Code style: black" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Cinnamon/kotaemon/pkgs/container/kotaemon" target="_blank"&gt; &lt;img src="https://img.shields.io/badge/docker_pull-kotaemon:latest-brightgreen" alt="docker pull ghcr.io/cinnamon/kotaemon:latest" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/github/downloads/Cinnamon/kotaemon/total.svg?label=downloads&amp;amp;color=blue" alt="download" /&gt; &lt;a href="https://huggingface.co/spaces/cin-model/kotaemon-demo"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue" /&gt;&lt;/a&gt; &lt;a href="https://hellogithub.com/en/repository/d3141471a0244d5798bc654982b263eb" target="_blank"&gt;&lt;img src="https://abroad.hellogithub.com/v1/widgets/recommend.svg?rid=d3141471a0244d5798bc654982b263eb&amp;amp;claim_uid=RLiD9UZ1rEHNaMf&amp;amp;theme=small" alt="Featuredï½œHelloGitHub" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;!-- start-intro --&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;This project serves as a functional RAG UI for both end users who want to do QA on their documents and developers who want to build their own RAG pipeline. &lt;br /&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yml"&gt;+----------------------------------------------------------------------------+
| End users: Those who use apps built with `kotaemon`.                       |
| (You use an app like the one in the demo above)                            |
|     +----------------------------------------------------------------+     |
|     | Developers: Those who built with `kotaemon`.                   |     |
|     | (You have `import kotaemon` somewhere in your project)         |     |
|     |     +----------------------------------------------------+     |     |
|     |     | Contributors: Those who make `kotaemon` better.    |     |     |
|     |     | (You make PR to this repo)                         |     |     |
|     |     +----------------------------------------------------+     |     |
|     +----------------------------------------------------------------+     |
+----------------------------------------------------------------------------+
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;For end users&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Clean &amp;amp; Minimalistic UI&lt;/strong&gt;: A user-friendly interface for RAG-based QA.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Support for Various LLMs&lt;/strong&gt;: Compatible with LLM API providers (OpenAI, AzureOpenAI, Cohere, etc.) and local LLMs (via &lt;code&gt;ollama&lt;/code&gt; and &lt;code&gt;llama-cpp-python&lt;/code&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Easy Installation&lt;/strong&gt;: Simple scripts to get you started quickly.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;For developers&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Framework for RAG Pipelines&lt;/strong&gt;: Tools to build your own RAG-based document QA pipeline.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Customizable UI&lt;/strong&gt;: See your RAG pipeline in action with the provided UI, built with &lt;a href="https://github.com/gradio-app/gradio"&gt;Gradio &lt;img src="https://img.shields.io/github/stars/gradio-app/gradio" /&gt;&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Gradio Theme&lt;/strong&gt;: If you use Gradio for development, check out our theme here: &lt;a href="https://github.com/lone17/kotaemon-gradio-theme"&gt;kotaemon-gradio-theme&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Host your own document QA (RAG) web-UI&lt;/strong&gt;: Support multi-user login, organize your files in private/public collections, collaborate and share your favorite chat with others.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Organize your LLM &amp;amp; Embedding models&lt;/strong&gt;: Support both local LLMs &amp;amp; popular API providers (OpenAI, Azure, Ollama, Groq).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Hybrid RAG pipeline&lt;/strong&gt;: Sane default RAG pipeline with hybrid (full-text &amp;amp; vector) retriever and re-ranking to ensure best retrieval quality.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Multi-modal QA support&lt;/strong&gt;: Perform Question Answering on multiple documents with figures and tables support. Support multi-modal document parsing (selectable options on UI).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Advanced citations with document preview&lt;/strong&gt;: By default the system will provide detailed citations to ensure the correctness of LLM answers. View your citations (incl. relevant score) directly in the &lt;em&gt;in-browser PDF viewer&lt;/em&gt; with highlights. Warning when retrieval pipeline return low relevant articles.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Support complex reasoning methods&lt;/strong&gt;: Use question decomposition to answer your complex/multi-hop question. Support agent-based reasoning with &lt;code&gt;ReAct&lt;/code&gt;, &lt;code&gt;ReWOO&lt;/code&gt; and other agents.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Configurable settings UI&lt;/strong&gt;: You can adjust most important aspects of retrieval &amp;amp; generation process on the UI (incl. prompts).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Extensible&lt;/strong&gt;: Being built on Gradio, you are free to customize or add any UI elements as you like. Also, we aim to support multiple strategies for document indexing &amp;amp; retrieval. &lt;code&gt;GraphRAG&lt;/code&gt; indexing pipeline is provided as an example.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Cinnamon/kotaemon/main/docs/images/preview.png" alt="Preview" /&gt;&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;If you are not a developer and just want to use the app, please check out our easy-to-follow &lt;a href="https://cinnamon.github.io/kotaemon/"&gt;User Guide&lt;/a&gt;. Download the &lt;code&gt;.zip&lt;/code&gt; file from the &lt;a href="https://github.com/Cinnamon/kotaemon/releases/latest"&gt;latest release&lt;/a&gt; to get all the newest features and bug fixes.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;System requirements&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://www.python.org/downloads/"&gt;Python&lt;/a&gt; &amp;gt;= 3.10&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.docker.com/"&gt;Docker&lt;/a&gt;: optional, if you &lt;a href="https://raw.githubusercontent.com/Cinnamon/kotaemon/main/#with-docker-recommended"&gt;install with Docker&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.unstructured.io/open-source/installation/full-installation#full-installation"&gt;Unstructured&lt;/a&gt; if you want to process files other than &lt;code&gt;.pdf&lt;/code&gt;, &lt;code&gt;.html&lt;/code&gt;, &lt;code&gt;.mhtml&lt;/code&gt;, and &lt;code&gt;.xlsx&lt;/code&gt; documents. Installation steps differ depending on your operating system. Please visit the link and follow the specific instructions provided there.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;With Docker (recommended)&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;We support both &lt;code&gt;lite&lt;/code&gt; &amp;amp; &lt;code&gt;full&lt;/code&gt; version of Docker images. With &lt;code&gt;full&lt;/code&gt; version, the extra packages of &lt;code&gt;unstructured&lt;/code&gt; will be installed, which can support additional file types (&lt;code&gt;.doc&lt;/code&gt;, &lt;code&gt;.docx&lt;/code&gt;, ...) but the cost is larger docker image size. For most users, the &lt;code&gt;lite&lt;/code&gt; image should work well in most cases.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;To use the &lt;code&gt;full&lt;/code&gt; version.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;docker run \
-e GRADIO_SERVER_NAME=0.0.0.0 \
-e GRADIO_SERVER_PORT=7860 \
-v ./ktem_app_data:/app/ktem_app_data \
-p 7860:7860 -it --rm \
ghcr.io/cinnamon/kotaemon:main-full
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;To use the &lt;code&gt;full&lt;/code&gt; version with bundled &lt;strong&gt;Ollama&lt;/strong&gt; for &lt;em&gt;local / private RAG&lt;/em&gt;.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# change image name to
docker run &amp;lt;...&amp;gt; ghcr.io/cinnamon/kotaemon:main-ollama
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;To use the &lt;code&gt;lite&lt;/code&gt; version.&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;pre&gt;&lt;code class="language-bash"&gt; # change image name to
 docker run &amp;lt;...&amp;gt; ghcr.io/cinnamon/kotaemon:main-lite
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;We currently support and test two platforms: &lt;code&gt;linux/amd64&lt;/code&gt; and &lt;code&gt;linux/arm64&lt;/code&gt; (for newer Mac). You can specify the platform by passing &lt;code&gt;--platform&lt;/code&gt; in the &lt;code&gt;docker run&lt;/code&gt; command. For example:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# To run docker with platform linux/arm64
docker run \
-e GRADIO_SERVER_NAME=0.0.0.0 \
-e GRADIO_SERVER_PORT=7860 \
-v ./ktem_app_data:/app/ktem_app_data \
-p 7860:7860 -it --rm \
--platform linux/arm64 \
ghcr.io/cinnamon/kotaemon:main-lite
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Once everything is set up correctly, you can go to &lt;code&gt;http://localhost:7860/&lt;/code&gt; to access the WebUI.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;We use &lt;a href="https://docs.github.com/en/packages/working-with-a-github-packages-registry/working-with-the-container-registry"&gt;GHCR&lt;/a&gt; to store docker images, all images can be found &lt;a href="https://github.com/Cinnamon/kotaemon/pkgs/container/kotaemon"&gt;here.&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Without Docker&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Clone and install required packages on a fresh python environment.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;# optional (setup env)
conda create -n kotaemon python=3.10
conda activate kotaemon

# clone this repo
git clone https://github.com/Cinnamon/kotaemon
cd kotaemon

pip install -e "libs/kotaemon[all]"
pip install -e "libs/ktem"
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Create a &lt;code&gt;.env&lt;/code&gt; file in the root of this project. Use &lt;code&gt;.env.example&lt;/code&gt; as a template&lt;/p&gt; &lt;p&gt;The &lt;code&gt;.env&lt;/code&gt; file is there to serve use cases where users want to pre-config the models before starting up the app (e.g. deploy the app on HF hub). The file will only be used to populate the db once upon the first run, it will no longer be used in consequent runs.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;(Optional) To enable in-browser &lt;code&gt;PDF_JS&lt;/code&gt; viewer, download &lt;a href="https://github.com/mozilla/pdf.js/releases/download/v4.0.379/pdfjs-4.0.379-dist.zip"&gt;PDF_JS_DIST&lt;/a&gt; then extract it to &lt;code&gt;libs/ktem/ktem/assets/prebuilt&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;img src="https://raw.githubusercontent.com/Cinnamon/kotaemon/main/docs/images/pdf-viewer-setup.png" alt="pdf-setup" width="300" /&gt; 
&lt;ol start="4"&gt; 
 &lt;li&gt; &lt;p&gt;Start the web server:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;python app.py
&lt;/code&gt;&lt;/pre&gt; 
  &lt;ul&gt; 
   &lt;li&gt;The app will be automatically launched in your browser.&lt;/li&gt; 
   &lt;li&gt;Default username and password are both &lt;code&gt;admin&lt;/code&gt;. You can set up additional users directly through the UI.&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Cinnamon/kotaemon/main/docs/images/chat-tab.png" alt="Chat tab" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Check the &lt;code&gt;Resources&lt;/code&gt; tab and &lt;code&gt;LLMs and Embeddings&lt;/code&gt; and ensure that your &lt;code&gt;api_key&lt;/code&gt; value is set correctly from your &lt;code&gt;.env&lt;/code&gt; file. If it is not set, you can set it there.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Setup GraphRAG&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Official MS GraphRAG indexing only works with OpenAI or Ollama API. We recommend most users to use NanoGraphRAG implementation for straightforward integration with Kotaemon.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;details&gt; 
 &lt;summary&gt;Setup Nano GRAPHRAG&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Install nano-GraphRAG: &lt;code&gt;pip install nano-graphrag&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;nano-graphrag&lt;/code&gt; install might introduce version conflicts, see &lt;a href="https://github.com/Cinnamon/kotaemon/issues/440"&gt;this issue&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;To quickly fix: &lt;code&gt;pip uninstall hnswlib chroma-hnswlib &amp;amp;&amp;amp; pip install chroma-hnswlib&lt;/code&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;Launch Kotaemon with &lt;code&gt;USE_NANO_GRAPHRAG=true&lt;/code&gt; environment variable.&lt;/li&gt; 
  &lt;li&gt;Set your default LLM &amp;amp; Embedding models in Resources setting and it will be recognized automatically from NanoGraphRAG.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Setup LIGHTRAG&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Install LightRAG: &lt;code&gt;pip install git+https://github.com/HKUDS/LightRAG.git&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;LightRAG&lt;/code&gt; install might introduce version conflicts, see &lt;a href="https://github.com/Cinnamon/kotaemon/issues/440"&gt;this issue&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;To quickly fix: &lt;code&gt;pip uninstall hnswlib chroma-hnswlib &amp;amp;&amp;amp; pip install chroma-hnswlib&lt;/code&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;Launch Kotaemon with &lt;code&gt;USE_LIGHTRAG=true&lt;/code&gt; environment variable.&lt;/li&gt; 
  &lt;li&gt;Set your default LLM &amp;amp; Embedding models in Resources setting and it will be recognized automatically from LightRAG.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Setup MS GRAPHRAG&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Non-Docker Installation&lt;/strong&gt;: If you are not using Docker, install GraphRAG with the following command:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;pip install "graphrag&amp;lt;=0.3.6" future
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Setting Up API KEY&lt;/strong&gt;: To use the GraphRAG retriever feature, ensure you set the &lt;code&gt;GRAPHRAG_API_KEY&lt;/code&gt; environment variable. You can do this directly in your environment or by adding it to a &lt;code&gt;.env&lt;/code&gt; file.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Using Local Models and Custom Settings&lt;/strong&gt;: If you want to use GraphRAG with local models (like &lt;code&gt;Ollama&lt;/code&gt;) or customize the default LLM and other configurations, set the &lt;code&gt;USE_CUSTOMIZED_GRAPHRAG_SETTING&lt;/code&gt; environment variable to true. Then, adjust your settings in the &lt;code&gt;settings.yaml.example&lt;/code&gt; file.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;Setup Local Models (for local/private RAG)&lt;/h3&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/Cinnamon/kotaemon/main/docs/local_model.md"&gt;Local model setup&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Setup multimodal document parsing (OCR, table parsing, figure extraction)&lt;/h3&gt; 
&lt;p&gt;These options are available:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://azure.microsoft.com/en-us/products/ai-services/ai-document-intelligence"&gt;Azure Document Intelligence (API)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://developer.adobe.com/document-services/docs/overview/pdf-extract-api/"&gt;Adobe PDF Extract (API)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/DS4SD/docling"&gt;Docling (local, open-source)&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;To use Docling, first install required dependencies: &lt;code&gt;pip install docling&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Select corresponding loaders in &lt;code&gt;Settings -&amp;gt; Retrieval Settings -&amp;gt; File loader&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Customize your application&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;By default, all application data is stored in the &lt;code&gt;./ktem_app_data&lt;/code&gt; folder. You can back up or copy this folder to transfer your installation to a new machine.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;For advanced users or specific use cases, you can customize these files:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;flowsettings.py&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;.env&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;&lt;code&gt;flowsettings.py&lt;/code&gt;&lt;/h4&gt; 
&lt;p&gt;This file contains the configuration of your application. You can use the example &lt;a href="https://raw.githubusercontent.com/Cinnamon/kotaemon/main/flowsettings.py"&gt;here&lt;/a&gt; as the starting point.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Notable settings&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# setup your preferred document store (with full-text search capabilities)
KH_DOCSTORE=(Elasticsearch | LanceDB | SimpleFileDocumentStore)

# setup your preferred vectorstore (for vector-based search)
KH_VECTORSTORE=(ChromaDB | LanceDB | InMemory | Milvus | Qdrant)

# Enable / disable multimodal QA
KH_REASONINGS_USE_MULTIMODAL=True

# Setup your new reasoning pipeline or modify existing one.
KH_REASONINGS = [
    "ktem.reasoning.simple.FullQAPipeline",
    "ktem.reasoning.simple.FullDecomposeQAPipeline",
    "ktem.reasoning.react.ReactAgentPipeline",
    "ktem.reasoning.rewoo.RewooAgentPipeline",
]
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h4&gt;&lt;code&gt;.env&lt;/code&gt;&lt;/h4&gt; 
&lt;p&gt;This file provides another way to configure your models and credentials.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Configure model via the .env file&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;Alternatively, you can configure the models via the &lt;code&gt;.env&lt;/code&gt; file with the information needed to connect to the LLMs. This file is located in the folder of the application. If you don't see it, you can create one.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Currently, the following providers are supported:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt; &lt;p&gt;&lt;strong&gt;OpenAI&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In the &lt;code&gt;.env&lt;/code&gt; file, set the &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; variable with your OpenAI API key in order to enable access to OpenAI's models. There are other variables that can be modified, please feel free to edit them to fit your case. Otherwise, the default parameter should work for most people.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;OPENAI_API_BASE=https://api.openai.com/v1
OPENAI_API_KEY=&amp;lt;your OpenAI API key here&amp;gt;
OPENAI_CHAT_MODEL=gpt-3.5-turbo
OPENAI_EMBEDDINGS_MODEL=text-embedding-ada-002
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Azure OpenAI&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;For OpenAI models via Azure platform, you need to provide your Azure endpoint and API key. Your might also need to provide your developments' name for the chat model and the embedding model depending on how you set up Azure development.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;AZURE_OPENAI_ENDPOINT=
AZURE_OPENAI_API_KEY=
OPENAI_API_VERSION=2024-02-15-preview
AZURE_OPENAI_CHAT_DEPLOYMENT=gpt-35-turbo
AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT=text-embedding-ada-002
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Local Models&lt;/strong&gt;&lt;/p&gt; 
     &lt;ul&gt; 
      &lt;li&gt; &lt;p&gt;Using &lt;code&gt;ollama&lt;/code&gt; OpenAI compatible server:&lt;/p&gt; 
       &lt;ul&gt; 
        &lt;li&gt; &lt;p&gt;Install &lt;a href="https://github.com/ollama/ollama"&gt;ollama&lt;/a&gt; and start the application.&lt;/p&gt; &lt;/li&gt; 
        &lt;li&gt; &lt;p&gt;Pull your model, for example:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;ollama pull llama3.1:8b
ollama pull nomic-embed-text
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
        &lt;li&gt; &lt;p&gt;Set the model names on web UI and make it as default:&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Cinnamon/kotaemon/main/docs/images/models.png" alt="Models" /&gt;&lt;/p&gt; &lt;/li&gt; 
       &lt;/ul&gt; &lt;/li&gt; 
      &lt;li&gt; &lt;p&gt;Using &lt;code&gt;GGUF&lt;/code&gt; with &lt;code&gt;llama-cpp-python&lt;/code&gt;&lt;/p&gt; &lt;p&gt;You can search and download a LLM to be ran locally from the &lt;a href="https://huggingface.co/models"&gt;Hugging Face Hub&lt;/a&gt;. Currently, these model formats are supported:&lt;/p&gt; 
       &lt;ul&gt; 
        &lt;li&gt; &lt;p&gt;GGUF&lt;/p&gt; &lt;p&gt;You should choose a model whose size is less than your device's memory and should leave about 2 GB. For example, if you have 16 GB of RAM in total, of which 12 GB is available, then you should choose a model that takes up at most 10 GB of RAM. Bigger models tend to give better generation but also take more processing time.&lt;/p&gt; &lt;p&gt;Here are some recommendations and their size in memory:&lt;/p&gt; &lt;/li&gt; 
        &lt;li&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen1.5-1.8B-Chat-GGUF/resolve/main/qwen1_5-1_8b-chat-q8_0.gguf?download=true"&gt;Qwen1.5-1.8B-Chat-GGUF&lt;/a&gt;: around 2 GB&lt;/p&gt; &lt;p&gt;Add a new LlamaCpp model with the provided model name on the web UI.&lt;/p&gt; &lt;/li&gt; 
       &lt;/ul&gt; &lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt;
 &lt;/ul&gt;
&lt;/details&gt;   
&lt;h3&gt;Adding your own RAG pipeline&lt;/h3&gt; 
&lt;h4&gt;Custom Reasoning Pipeline&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Check the default pipeline implementation in &lt;a href="https://raw.githubusercontent.com/Cinnamon/kotaemon/main/libs/ktem/ktem/reasoning/simple.py"&gt;here&lt;/a&gt;. You can make quick adjustment to how the default QA pipeline work.&lt;/li&gt; 
 &lt;li&gt;Add new &lt;code&gt;.py&lt;/code&gt; implementation in &lt;code&gt;libs/ktem/ktem/reasoning/&lt;/code&gt; and later include it in &lt;code&gt;flowssettings&lt;/code&gt; to enable it on the UI.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Custom Indexing Pipeline&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Check sample implementation in &lt;code&gt;libs/ktem/ktem/index/file/graph&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;(more instruction WIP).&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;!-- end-intro --&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;Please cite this project as&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-BibTeX"&gt;@misc{kotaemon2024,
    title = {Kotaemon - An open-source RAG-based tool for chatting with any content.},
    author = {The Kotaemon Team},
    year = {2024},
    howpublished = {\url{https://github.com/Cinnamon/kotaemon}},
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;a href="https://star-history.com/#Cinnamon/kotaemon&amp;amp;Date"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=Cinnamon/kotaemon&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=Cinnamon/kotaemon&amp;amp;type=Date" /&gt; 
  &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=Cinnamon/kotaemon&amp;amp;type=Date" /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;h2&gt;Contribution&lt;/h2&gt; 
&lt;p&gt;Since our project is actively being developed, we greatly value your feedback and contributions. Please see our &lt;a href="https://github.com/Cinnamon/kotaemon/raw/main/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt; to get started. Thank you to all our contributors!&lt;/p&gt; 
&lt;a href="https://github.com/Cinnamon/kotaemon/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=Cinnamon/kotaemon" /&gt; &lt;/a&gt;</description>
    </item>
    
    <item>
      <title>Physical-Intelligence/openpi</title>
      <link>https://github.com/Physical-Intelligence/openpi</link>
      <description>&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;openpi&lt;/h1&gt; 
&lt;p&gt;openpi holds open-source models and packages for robotics, published by the &lt;a href="https://www.physicalintelligence.company/"&gt;Physical Intelligence team&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Currently, this repo contains three types of models:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;the &lt;a href="https://www.physicalintelligence.company/blog/pi0"&gt;Ï€â‚€ model&lt;/a&gt;, a flow-based vision-language-action model (VLA).&lt;/li&gt; 
 &lt;li&gt;the &lt;a href="https://www.physicalintelligence.company/research/fast"&gt;Ï€â‚€-FAST model&lt;/a&gt;, an autoregressive VLA, based on the FAST action tokenizer.&lt;/li&gt; 
 &lt;li&gt;the &lt;a href="https://www.physicalintelligence.company/blog/pi05"&gt;Ï€â‚€.â‚… model&lt;/a&gt;, an upgraded version of Ï€â‚€ with better open-world generalization trained with &lt;a href="https://www.physicalintelligence.company/research/knowledge_insulation"&gt;knowledge insulation&lt;/a&gt;. Note that, in this repository, we currently only support the flow matching head for both $\pi_{0.5}$ training and inference.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For all models, we provide &lt;em&gt;base model&lt;/em&gt; checkpoints, pre-trained on 10k+ hours of robot data, and examples for using them out of the box or fine-tuning them to your own datasets.&lt;/p&gt; 
&lt;p&gt;This is an experiment: $\pi_0$ was developed for our own robots, which differ from the widely used platforms such as &lt;a href="https://tonyzhaozh.github.io/aloha/"&gt;ALOHA&lt;/a&gt; and &lt;a href="https://droid-dataset.github.io/"&gt;DROID&lt;/a&gt;, and though we are optimistic that researchers and practitioners will be able to run creative new experiments adapting $\pi_0$ to their own platforms, we do not expect every such attempt to be successful. All this is to say: $\pi_0$ may or may not work for you, but you are welcome to try it and see!&lt;/p&gt; 
&lt;h2&gt;Updates&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;[Sept 2025] We released PyTorch support in openpi.&lt;/li&gt; 
 &lt;li&gt;[Sept 2025] We released pi05, an upgraded version of pi0 with better open-world generalization.&lt;/li&gt; 
 &lt;li&gt;[Sept 2025]: We have added an &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/droid/README_train.md#data-filtering"&gt;improved idle filter&lt;/a&gt; for DROID training.&lt;/li&gt; 
 &lt;li&gt;[Jun 2025]: We have added &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/droid/README_train.md"&gt;instructions&lt;/a&gt; for using &lt;code&gt;openpi&lt;/code&gt; to train VLAs on the full &lt;a href="https://droid-dataset.github.io/"&gt;DROID dataset&lt;/a&gt;. This is an approximate open-source implementation of the training pipeline used to train pi0-FAST-DROID.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;p&gt;To run the models in this repository, you will need an NVIDIA GPU with at least the following specifications. These estimations assume a single GPU, but you can also use multiple GPUs with model parallelism to reduce per-GPU memory requirements by configuring &lt;code&gt;fsdp_devices&lt;/code&gt; in the training config. Please also note that the current training script does not yet support multi-node training.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Mode&lt;/th&gt; 
   &lt;th&gt;Memory Required&lt;/th&gt; 
   &lt;th&gt;Example GPU&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;&amp;gt; 8 GB&lt;/td&gt; 
   &lt;td&gt;RTX 4090&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Fine-Tuning (LoRA)&lt;/td&gt; 
   &lt;td&gt;&amp;gt; 22.5 GB&lt;/td&gt; 
   &lt;td&gt;RTX 4090&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Fine-Tuning (Full)&lt;/td&gt; 
   &lt;td&gt;&amp;gt; 70 GB&lt;/td&gt; 
   &lt;td&gt;A100 (80GB) / H100&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;The repo has been tested with Ubuntu 22.04, we do not currently support other operating systems.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;When cloning this repo, make sure to update submodules:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone --recurse-submodules git@github.com:Physical-Intelligence/openpi.git

# Or if you already cloned the repo:
git submodule update --init --recursive
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We use &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt; to manage Python dependencies. See the &lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;uv installation instructions&lt;/a&gt; to set it up. Once uv is installed, run the following to set up the environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;GIT_LFS_SKIP_SMUDGE=1 uv sync
GIT_LFS_SKIP_SMUDGE=1 uv pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;NOTE: &lt;code&gt;GIT_LFS_SKIP_SMUDGE=1&lt;/code&gt; is needed to pull LeRobot as a dependency.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Docker&lt;/strong&gt;: As an alternative to uv installation, we provide instructions for installing openpi using Docker. If you encounter issues with your system setup, consider using Docker to simplify installation. See &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/docs/docker.md"&gt;Docker Setup&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Model Checkpoints&lt;/h2&gt; 
&lt;h3&gt;Base Models&lt;/h3&gt; 
&lt;p&gt;We provide multiple base VLA model checkpoints. These checkpoints have been pre-trained on 10k+ hours of robot data, and can be used for fine-tuning.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Use Case&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Checkpoint Path&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$&lt;/td&gt; 
   &lt;td&gt;Fine-Tuning&lt;/td&gt; 
   &lt;td&gt;Base &lt;a href="https://www.physicalintelligence.company/blog/pi0"&gt;Ï€â‚€ model&lt;/a&gt; for fine-tuning&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_base&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$-FAST&lt;/td&gt; 
   &lt;td&gt;Fine-Tuning&lt;/td&gt; 
   &lt;td&gt;Base autoregressive &lt;a href="https://www.physicalintelligence.company/research/fast"&gt;Ï€â‚€-FAST model&lt;/a&gt; for fine-tuning&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_fast_base&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_{0.5}$&lt;/td&gt; 
   &lt;td&gt;Fine-Tuning&lt;/td&gt; 
   &lt;td&gt;Base &lt;a href="https://www.physicalintelligence.company/blog/pi05"&gt;Ï€â‚€.â‚… model&lt;/a&gt; for fine-tuning&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi05_base&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Fine-Tuned Models&lt;/h3&gt; 
&lt;p&gt;We also provide "expert" checkpoints for various robot platforms and tasks. These models are fine-tuned from the base models above and intended to run directly on the target robot. These may or may not work on your particular robot. Since these checkpoints were fine-tuned on relatively small datasets collected with more widely available robots, such as ALOHA and the DROID Franka setup, they might not generalize to your particular setup, though we found some of these, especially the DROID checkpoint, to generalize quite broadly in practice.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Use Case&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Checkpoint Path&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$-FAST-DROID&lt;/td&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;$\pi_0$-FAST model fine-tuned on the &lt;a href="https://droid-dataset.github.io/"&gt;DROID dataset&lt;/a&gt;: can perform a wide range of simple table-top manipulation tasks 0-shot in new scenes on the DROID robot platform&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_fast_droid&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$-DROID&lt;/td&gt; 
   &lt;td&gt;Fine-Tuning&lt;/td&gt; 
   &lt;td&gt;$\pi_0$ model fine-tuned on the &lt;a href="https://droid-dataset.github.io/"&gt;DROID dataset&lt;/a&gt;: faster inference than $\pi_0$-FAST-DROID, but may not follow language commands as well&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_droid&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$-ALOHA-towel&lt;/td&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;$\pi_0$ model fine-tuned on internal &lt;a href="https://tonyzhaozh.github.io/aloha/"&gt;ALOHA&lt;/a&gt; data: can fold diverse towels 0-shot on ALOHA robot platforms&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_aloha_towel&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$-ALOHA-tupperware&lt;/td&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;$\pi_0$ model fine-tuned on internal &lt;a href="https://tonyzhaozh.github.io/aloha/"&gt;ALOHA&lt;/a&gt; data: can unpack food from a tupperware container&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_aloha_tupperware&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$-ALOHA-pen-uncap&lt;/td&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;$\pi_0$ model fine-tuned on public &lt;a href="https://dit-policy.github.io/"&gt;ALOHA&lt;/a&gt; data: can uncap a pen&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_aloha_pen_uncap&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_{0.5}$-LIBERO&lt;/td&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;$\pi_{0.5}$ model fine-tuned for the &lt;a href="https://libero-project.github.io/datasets"&gt;LIBERO&lt;/a&gt; benchmark: gets state-of-the-art performance (see &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/libero/README.md"&gt;LIBERO README&lt;/a&gt;)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi05_libero&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_{0.5}$-DROID&lt;/td&gt; 
   &lt;td&gt;Inference / Fine-Tuning&lt;/td&gt; 
   &lt;td&gt;$\pi_{0.5}$ model fine-tuned on the &lt;a href="https://droid-dataset.github.io/"&gt;DROID dataset&lt;/a&gt; with &lt;a href="https://www.physicalintelligence.company/research/knowledge_insulation"&gt;knowledge insulation&lt;/a&gt;: fast inference and good language-following&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi05_droid&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;By default, checkpoints are automatically downloaded from &lt;code&gt;gs://openpi-assets&lt;/code&gt; and are cached in &lt;code&gt;~/.cache/openpi&lt;/code&gt; when needed. You can overwrite the download path by setting the &lt;code&gt;OPENPI_DATA_HOME&lt;/code&gt; environment variable.&lt;/p&gt; 
&lt;h2&gt;Running Inference for a Pre-Trained Model&lt;/h2&gt; 
&lt;p&gt;Our pre-trained model checkpoints can be run with a few lines of code (here our $\pi_0$-FAST-DROID model):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openpi.training import config as _config
from openpi.policies import policy_config
from openpi.shared import download

config = _config.get_config("pi05_droid")
checkpoint_dir = download.maybe_download("gs://openpi-assets/checkpoints/pi05_droid")

# Create a trained policy.
policy = policy_config.create_trained_policy(config, checkpoint_dir)

# Run inference on a dummy example.
example = {
    "observation/exterior_image_1_left": ...,
    "observation/wrist_image_left": ...,
    ...
    "prompt": "pick up the fork"
}
action_chunk = policy.infer(example)["actions"]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also test this out in the &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/inference.ipynb"&gt;example notebook&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We provide detailed step-by-step examples for running inference of our pre-trained checkpoints on &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/droid/README.md"&gt;DROID&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/aloha_real/README.md"&gt;ALOHA&lt;/a&gt; robots.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Remote Inference&lt;/strong&gt;: We provide &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/docs/remote_inference.md"&gt;examples and code&lt;/a&gt; for running inference of our models &lt;strong&gt;remotely&lt;/strong&gt;: the model can run on a different server and stream actions to the robot via a websocket connection. This makes it easy to use more powerful GPUs off-robot and keep robot and policy environments separate.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Test inference without a robot&lt;/strong&gt;: We provide a &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/simple_client/README.md"&gt;script&lt;/a&gt; for testing inference without a robot. This script will generate a random observation and run inference with the model. See &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/simple_client/README.md"&gt;here&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Fine-Tuning Base Models on Your Own Data&lt;/h2&gt; 
&lt;p&gt;We will fine-tune the $\pi_{0.5}$ model on the &lt;a href="https://libero-project.github.io/datasets"&gt;LIBERO dataset&lt;/a&gt; as a running example for how to fine-tune a base model on your own data. We will explain three steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Convert your data to a LeRobot dataset (which we use for training)&lt;/li&gt; 
 &lt;li&gt;Defining training configs and running training&lt;/li&gt; 
 &lt;li&gt;Spinning up a policy server and running inference&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;1. Convert your data to a LeRobot dataset&lt;/h3&gt; 
&lt;p&gt;We provide a minimal example script for converting LIBERO data to a LeRobot dataset in &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/libero/convert_libero_data_to_lerobot.py"&gt;&lt;code&gt;examples/libero/convert_libero_data_to_lerobot.py&lt;/code&gt;&lt;/a&gt;. You can easily modify it to convert your own data! You can download the raw LIBERO dataset from &lt;a href="https://huggingface.co/datasets/openvla/modified_libero_rlds"&gt;here&lt;/a&gt;, and run the script with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run examples/libero/convert_libero_data_to_lerobot.py --data_dir /path/to/your/libero/data
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; If you just want to fine-tune on LIBERO, you can skip this step, because our LIBERO fine-tuning configs point to a pre-converted LIBERO dataset. This step is merely an example that you can adapt to your own data.&lt;/p&gt; 
&lt;h3&gt;2. Defining training configs and running training&lt;/h3&gt; 
&lt;p&gt;To fine-tune a base model on your own data, you need to define configs for data processing and training. We provide example configs with detailed comments for LIBERO below, which you can modify for your own dataset:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/src/openpi/policies/libero_policy.py"&gt;&lt;code&gt;LiberoInputs&lt;/code&gt; and &lt;code&gt;LiberoOutputs&lt;/code&gt;&lt;/a&gt;: Defines the data mapping from the LIBERO environment to the model and vice versa. Will be used for both, training and inference.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/src/openpi/training/config.py"&gt;&lt;code&gt;LeRobotLiberoDataConfig&lt;/code&gt;&lt;/a&gt;: Defines how to process raw LIBERO data from LeRobot dataset for training.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/src/openpi/training/config.py"&gt;&lt;code&gt;TrainConfig&lt;/code&gt;&lt;/a&gt;: Defines fine-tuning hyperparameters, data config, and weight loader.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We provide example fine-tuning configs for &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/src/openpi/training/config.py"&gt;Ï€â‚€&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/src/openpi/training/config.py"&gt;Ï€â‚€-FAST&lt;/a&gt;, and &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/src/openpi/training/config.py"&gt;Ï€â‚€.â‚…&lt;/a&gt; on LIBERO data.&lt;/p&gt; 
&lt;p&gt;Before we can run training, we need to compute the normalization statistics for the training data. Run the script below with the name of your training config:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run scripts/compute_norm_stats.py --config-name pi05_libero
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Now we can kick off training with the following command (the &lt;code&gt;--overwrite&lt;/code&gt; flag is used to overwrite existing checkpoints if you rerun fine-tuning with the same config):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;XLA_PYTHON_CLIENT_MEM_FRACTION=0.9 uv run scripts/train.py pi05_libero --exp-name=my_experiment --overwrite
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The command will log training progress to the console and save checkpoints to the &lt;code&gt;checkpoints&lt;/code&gt; directory. You can also monitor training progress on the Weights &amp;amp; Biases dashboard. For maximally using the GPU memory, set &lt;code&gt;XLA_PYTHON_CLIENT_MEM_FRACTION=0.9&lt;/code&gt; before running training -- this enables JAX to use up to 90% of the GPU memory (vs. the default of 75%).&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; We provide functionality for &lt;em&gt;reloading&lt;/em&gt; normalization statistics for state / action normalization from pre-training. This can be beneficial if you are fine-tuning to a new task on a robot that was part of our pre-training mixture. For more details on how to reload normalization statistics, see the &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/docs/norm_stats.md"&gt;norm_stats.md&lt;/a&gt; file.&lt;/p&gt; 
&lt;h3&gt;3. Spinning up a policy server and running inference&lt;/h3&gt; 
&lt;p&gt;Once training is complete, we can run inference by spinning up a policy server and then querying it from a LIBERO evaluation script. Launching a model server is easy (we use the checkpoint for iteration 20,000 for this example, modify as needed):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run scripts/serve_policy.py policy:checkpoint --policy.config=pi05_libero --policy.dir=checkpoints/pi05_libero/my_experiment/20000
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will spin up a server that listens on port 8000 and waits for observations to be sent to it. We can then run an evaluation script (or robot runtime) that queries the server.&lt;/p&gt; 
&lt;p&gt;For running the LIBERO eval in particular, we provide (and recommend using) a Dockerized workflow that handles both the policy server and the evaluation script together. See the &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/libero/README.md"&gt;LIBERO README&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;p&gt;If you want to embed a policy server call in your own robot runtime, we have a minimal example of how to do so in the &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/docs/remote_inference.md"&gt;remote inference docs&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;More Examples&lt;/h3&gt; 
&lt;p&gt;We provide more examples for how to fine-tune and run inference with our models on the ALOHA platform in the following READMEs:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/aloha_sim"&gt;ALOHA Simulator&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/aloha_real"&gt;ALOHA Real&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/ur5"&gt;UR5&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;PyTorch Support&lt;/h2&gt; 
&lt;p&gt;openpi now provides PyTorch implementations of Ï€â‚€ and Ï€â‚€.â‚… models alongside the original JAX versions! The PyTorch implementation has been validated on the LIBERO benchmark (both inference and finetuning). A few features are currently not supported (this may change in the future):&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;The Ï€â‚€-FAST model&lt;/li&gt; 
 &lt;li&gt;Mixed precision training&lt;/li&gt; 
 &lt;li&gt;FSDP (fully-sharded data parallelism) training&lt;/li&gt; 
 &lt;li&gt;LoRA (low-rank adaptation) training&lt;/li&gt; 
 &lt;li&gt;EMA (exponential moving average) weights during training&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Setup&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Make sure that you have the latest version of all dependencies installed: &lt;code&gt;uv sync&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Double check that you have transformers 4.53.2 installed: &lt;code&gt;uv pip show transformers&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Apply the transformers library patches:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cp -r ./src/openpi/models_pytorch/transformers_replace/* .venv/lib/python3.11/site-packages/transformers/
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;This overwrites several files in the transformers library with necessary model changes: 1) supporting AdaRMS, 2) correctly controlling the precision of activations, and 3) allowing the KV cache to be used without being updated.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;WARNING&lt;/strong&gt;: With the default uv link mode (hardlink), this will permanently affect the transformers library in your uv cache, meaning the changes will survive reinstallations of transformers and could even propagate to other projects that use transformers. To fully undo this operation, you must run &lt;code&gt;uv cache clean transformers&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Converting JAX Models to PyTorch&lt;/h3&gt; 
&lt;p&gt;To convert a JAX model checkpoint to PyTorch format:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run examples/convert_jax_model_to_pytorch.py \
    --checkpoint_dir /path/to/jax/checkpoint \
    --config_name &amp;lt;config name&amp;gt; \
    --output_path /path/to/converted/pytorch/checkpoint
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Running Inference with PyTorch&lt;/h3&gt; 
&lt;p&gt;The PyTorch implementation uses the same API as the JAX version - you only need to change the checkpoint path to point to the converted PyTorch model:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openpi.training import config as _config
from openpi.policies import policy_config
from openpi.shared import download

config = _config.get_config("pi05_droid")
checkpoint_dir = "/path/to/converted/pytorch/checkpoint"

# Create a trained policy (automatically detects PyTorch format)
policy = policy_config.create_trained_policy(config, checkpoint_dir)

# Run inference (same API as JAX)
action_chunk = policy.infer(example)["actions"]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Policy Server with PyTorch&lt;/h3&gt; 
&lt;p&gt;The policy server works identically with PyTorch models - just point to the converted checkpoint directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run scripts/serve_policy.py policy:checkpoint \
    --policy.config=pi05_droid \
    --policy.dir=/path/to/converted/pytorch/checkpoint
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Finetuning with PyTorch&lt;/h3&gt; 
&lt;p&gt;To finetune a model in PyTorch:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Convert the JAX base model to PyTorch format:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;uv run examples/convert_jax_model_to_pytorch.py \
    --config_name &amp;lt;config name&amp;gt; \
    --checkpoint_dir /path/to/jax/base/model \
    --output_path /path/to/pytorch/base/model
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Specify the converted PyTorch model path in your config using &lt;code&gt;pytorch_weight_path&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Launch training using one of these modes:&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Single GPU training:
uv run scripts/train_pytorch.py &amp;lt;config_name&amp;gt; --exp_name &amp;lt;run_name&amp;gt; --save_interval &amp;lt;interval&amp;gt;

# Example:
uv run scripts/train_pytorch.py debug --exp_name pytorch_test
uv run scripts/train_pytorch.py debug --exp_name pytorch_test --resume  # Resume from latest checkpoint

# Multi-GPU training (single node):
uv run torchrun --standalone --nnodes=1 --nproc_per_node=&amp;lt;num_gpus&amp;gt; scripts/train_pytorch.py &amp;lt;config_name&amp;gt; --exp_name &amp;lt;run_name&amp;gt;

# Example:
uv run torchrun --standalone --nnodes=1 --nproc_per_node=2 scripts/train_pytorch.py pi0_aloha_sim --exp_name pytorch_ddp_test
uv run torchrun --standalone --nnodes=1 --nproc_per_node=2 scripts/train_pytorch.py pi0_aloha_sim --exp_name pytorch_ddp_test --resume

# Multi-Node Training:
uv run torchrun \
    --nnodes=&amp;lt;num_nodes&amp;gt; \
    --nproc_per_node=&amp;lt;gpus_per_node&amp;gt; \
    --node_rank=&amp;lt;rank_of_node&amp;gt; \
    --master_addr=&amp;lt;master_ip&amp;gt; \
    --master_port=&amp;lt;port&amp;gt; \
    scripts/train_pytorch.py &amp;lt;config_name&amp;gt; --exp_name=&amp;lt;run_name&amp;gt; --save_interval &amp;lt;interval&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Precision Settings&lt;/h3&gt; 
&lt;p&gt;JAX and PyTorch implementations handle precision as follows:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;JAX:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Inference: most weights and computations in bfloat16, with a few computations in float32 for stability&lt;/li&gt; 
 &lt;li&gt;Training: defaults to mixed precision: weights and gradients in float32, (most) activations and computations in bfloat16. You can change to full float32 training by setting &lt;code&gt;dtype&lt;/code&gt; to float32 in the config.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;PyTorch:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Inference: matches JAX -- most weights and computations in bfloat16, with a few weights converted to float32 for stability&lt;/li&gt; 
 &lt;li&gt;Training: supports either full bfloat16 (default) or full float32. You can change it by setting &lt;code&gt;pytorch_training_precision&lt;/code&gt; in the config. bfloat16 uses less memory but exhibits higher losses compared to float32. Mixed precision is not yet supported.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;With torch.compile, inference speed is comparable between JAX and PyTorch.&lt;/p&gt; 
&lt;h2&gt;Troubleshooting&lt;/h2&gt; 
&lt;p&gt;We will collect common issues and their solutions here. If you encounter an issue, please check here first. If you can't find a solution, please file an issue on the repo (see &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/CONTRIBUTING.md"&gt;here&lt;/a&gt; for guidelines).&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Issue&lt;/th&gt; 
   &lt;th&gt;Resolution&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;uv sync&lt;/code&gt; fails with dependency conflicts&lt;/td&gt; 
   &lt;td&gt;Try removing the virtual environment directory (&lt;code&gt;rm -rf .venv&lt;/code&gt;) and running &lt;code&gt;uv sync&lt;/code&gt; again. If issues persist, check that you have the latest version of &lt;code&gt;uv&lt;/code&gt; installed (&lt;code&gt;uv self update&lt;/code&gt;).&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Training runs out of GPU memory&lt;/td&gt; 
   &lt;td&gt;Make sure you set &lt;code&gt;XLA_PYTHON_CLIENT_MEM_FRACTION=0.9&lt;/code&gt; (or higher) before running training to allow JAX to use more GPU memory. You can also use &lt;code&gt;--fsdp-devices &amp;lt;n&amp;gt;&lt;/code&gt; where &lt;code&gt;&amp;lt;n&amp;gt;&lt;/code&gt; is your number of GPUs, to enable &lt;a href="https://engineering.fb.com/2021/07/15/open-source/fsdp/"&gt;fully-sharded data parallelism&lt;/a&gt;, which reduces memory usage in exchange for slower training (the amount of slowdown depends on your particular setup). If you are still running out of memory, you may way to consider disabling EMA.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Policy server connection errors&lt;/td&gt; 
   &lt;td&gt;Check that the server is running and listening on the expected port. Verify network connectivity and firewall settings between client and server.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Missing norm stats error when training&lt;/td&gt; 
   &lt;td&gt;Run &lt;code&gt;scripts/compute_norm_stats.py&lt;/code&gt; with your config name before starting training.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Dataset download fails&lt;/td&gt; 
   &lt;td&gt;Check your internet connection. For HuggingFace datasets, ensure you're logged in (&lt;code&gt;huggingface-cli login&lt;/code&gt;).&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;CUDA/GPU errors&lt;/td&gt; 
   &lt;td&gt;Verify NVIDIA drivers are installed correctly. For Docker, ensure nvidia-container-toolkit is installed. Check GPU compatibility. You do NOT need CUDA libraries installed at a system level --- they will be installed via uv. You may even want to try &lt;em&gt;uninstalling&lt;/em&gt; system CUDA libraries if you run into CUDA issues, since system libraries can sometimes cause conflicts.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Import errors when running examples&lt;/td&gt; 
   &lt;td&gt;Make sure you've installed all dependencies with &lt;code&gt;uv sync&lt;/code&gt;. Some examples may have additional requirements listed in their READMEs.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Action dimensions mismatch&lt;/td&gt; 
   &lt;td&gt;Verify your data processing transforms match the expected input/output dimensions of your robot. Check the action space definitions in your policy classes.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Diverging training loss&lt;/td&gt; 
   &lt;td&gt;Check the &lt;code&gt;q01&lt;/code&gt;, &lt;code&gt;q99&lt;/code&gt;, and &lt;code&gt;std&lt;/code&gt; values in &lt;code&gt;norm_stats.json&lt;/code&gt; for your dataset. Certain dimensions that are rarely used can end up with very small &lt;code&gt;q01&lt;/code&gt;, &lt;code&gt;q99&lt;/code&gt;, or &lt;code&gt;std&lt;/code&gt; values, leading to huge states and actions after normalization. You can manually adjust the norm stats as a workaround.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt;</description>
    </item>
    
    <item>
      <title>uutils/coreutils</title>
      <link>https://github.com/uutils/coreutils</link>
      <description>&lt;p&gt;Cross-platform Rust rewrite of the GNU coreutils&lt;/p&gt;&lt;hr&gt;&lt;div class="oranda-hide"&gt; 
 &lt;div align="center"&gt; 
  &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/uutils/coreutils/main/docs/src/logo.svg?sanitize=true" alt="uutils logo" /&gt;&lt;/p&gt; 
  &lt;h1&gt;uutils coreutils&lt;/h1&gt; 
  &lt;p&gt;&lt;a href="https://crates.io/crates/coreutils"&gt;&lt;img src="https://img.shields.io/crates/v/coreutils.svg?sanitize=true" alt="Crates.io" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/wQVJbvJ"&gt;&lt;img src="https://img.shields.io/badge/discord-join-7289DA.svg?logo=discord&amp;amp;longCache=true&amp;amp;style=flat" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://github.com/uutils/coreutils/raw/main/LICENSE"&gt;&lt;img src="http://img.shields.io/badge/license-MIT-blue.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://deps.rs/repo/github/uutils/coreutils"&gt;&lt;img src="https://deps.rs/repo/github/uutils/coreutils/status.svg?sanitize=true" alt="dependency status" /&gt;&lt;/a&gt;&lt;/p&gt; 
  &lt;p&gt;&lt;a href="https://codecov.io/gh/uutils/coreutils"&gt;&lt;img src="https://codecov.io/gh/uutils/coreutils/branch/master/graph/badge.svg?sanitize=true" alt="CodeCov" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/badge/MSRV-1.85.0-brightgreen" alt="MSRV" /&gt; &lt;a href="https://hosted.weblate.org/projects/rust-coreutils/"&gt;&lt;img src="https://hosted.weblate.org/widget/rust-coreutils/svg-badge.svg?sanitize=true" alt="Weblate" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;hr /&gt; 
&lt;/div&gt; 
&lt;p&gt;uutils coreutils is a cross-platform reimplementation of the GNU coreutils in &lt;a href="http://www.rust-lang.org"&gt;Rust&lt;/a&gt;. While all programs have been implemented, some options might be missing or different behavior might be experienced.&lt;/p&gt; 
&lt;div class="oranda-hide"&gt; 
 &lt;p&gt;To install it:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;cargo install coreutils
~/.cargo/bin/coreutils
&lt;/code&gt;&lt;/pre&gt; 
&lt;/div&gt; 
&lt;!-- markdownlint-disable-next-line MD026 --&gt; 
&lt;h2&gt;Goals&lt;/h2&gt; 
&lt;p&gt;uutils coreutils aims to be a drop-in replacement for the GNU utils. Differences with GNU are treated as bugs.&lt;/p&gt; 
&lt;p&gt;Our key objectives include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Matching GNU's output (stdout and error code) exactly&lt;/li&gt; 
 &lt;li&gt;Better error messages&lt;/li&gt; 
 &lt;li&gt;Providing comprehensive internationalization support (UTF-8)&lt;/li&gt; 
 &lt;li&gt;Improved performances&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/uutils/coreutils/main/docs/src/extensions.md"&gt;Extensions&lt;/a&gt; when relevant (example: --progress)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;uutils aims to work on as many platforms as possible, to be able to use the same utils on Linux, macOS, Windows and other platforms. This ensures, for example, that scripts can be easily transferred between platforms.&lt;/p&gt; 
&lt;div class="oranda-hide"&gt; 
 &lt;h2&gt;Documentation&lt;/h2&gt; 
 &lt;p&gt;uutils has both user and developer documentation available:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://uutils.github.io/coreutils/docs/"&gt;User Manual&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://docs.rs/crate/coreutils/"&gt;Developer Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;Both can also be generated locally, the instructions for that can be found in the &lt;a href="https://github.com/uutils/uutils.github.io"&gt;coreutils docs&lt;/a&gt; repository.&lt;/p&gt; 
 &lt;p&gt;Use &lt;a href="https://hosted.weblate.org/projects/rust-coreutils/"&gt;weblate/rust-coreutils&lt;/a&gt; to translate the Rust coreutils into your language.&lt;/p&gt; 
 &lt;!-- ANCHOR: build (this mark is needed for mdbook) --&gt; 
 &lt;h2&gt;Requirements&lt;/h2&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Rust (&lt;code&gt;cargo&lt;/code&gt;, &lt;code&gt;rustc&lt;/code&gt;)&lt;/li&gt; 
  &lt;li&gt;GNU Make (optional)&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;Rust Version&lt;/h3&gt; 
 &lt;p&gt;uutils follows Rust's release channels and is tested against stable, beta and nightly. The current Minimum Supported Rust Version (MSRV) is &lt;code&gt;1.85.0&lt;/code&gt;.&lt;/p&gt; 
 &lt;h2&gt;Building&lt;/h2&gt; 
 &lt;p&gt;There are currently two methods to build the uutils binaries: either Cargo or GNU Make.&lt;/p&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;Building the full package, including all documentation, requires both Cargo and GNU Make on a Unix platform.&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;p&gt;For either method, we first need to fetch the repository:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;git clone https://github.com/uutils/coreutils
cd coreutils
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;Cargo&lt;/h3&gt; 
 &lt;p&gt;Building uutils using Cargo is easy because the process is the same as for every other Rust program:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;cargo build --release
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;This command builds the most portable common core set of uutils into a multicall (BusyBox-type) binary, named 'coreutils', on most Rust-supported platforms.&lt;/p&gt; 
 &lt;p&gt;Additional platform-specific uutils are often available. Building these expanded sets of uutils for a platform (on that platform) is as simple as specifying it as a feature:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;cargo build --release --features macos
# or ...
cargo build --release --features windows
# or ...
cargo build --release --features unix
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;If you don't want to build every utility available on your platform into the final binary, you can also specify which ones you want to build manually. For example:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;cargo build --features "base32 cat echo rm" --no-default-features
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;If you don't want to build the multicall binary and would prefer to build the utilities as individual binaries, that is also possible. Each utility is contained in its own package within the main repository, named "uu_UTILNAME". To build individual utilities, use cargo to build just the specific packages (using the &lt;code&gt;--package&lt;/code&gt; [aka &lt;code&gt;-p&lt;/code&gt;] option). For example:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;cargo build -p uu_base32 -p uu_cat -p uu_echo -p uu_rm
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;GNU Make&lt;/h3&gt; 
 &lt;p&gt;Building using &lt;code&gt;make&lt;/code&gt; is a simple process as well.&lt;/p&gt; 
 &lt;p&gt;To simply build all available utilities:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;make
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;In release mode:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;make PROFILE=release
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;To build all but a few of the available utilities:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;make SKIP_UTILS='UTILITY_1 UTILITY_2'
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;To build only a few of the available utilities:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;make UTILS='UTILITY_1 UTILITY_2'
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h2&gt;Installation&lt;/h2&gt; 
 &lt;h3&gt;Install with Cargo&lt;/h3&gt; 
 &lt;p&gt;Likewise, installing can simply be done using:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;cargo install --path . --locked
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;This command will install uutils into Cargo's &lt;em&gt;bin&lt;/em&gt; folder (&lt;em&gt;e.g.&lt;/em&gt; &lt;code&gt;$HOME/.cargo/bin&lt;/code&gt;).&lt;/p&gt; 
 &lt;p&gt;This does not install files necessary for shell completion or manpages. For manpages or shell completion to work, use &lt;code&gt;GNU Make&lt;/code&gt; or see &lt;code&gt;Manually install shell completions&lt;/code&gt;/&lt;code&gt;Manually install manpages&lt;/code&gt;.&lt;/p&gt; 
 &lt;h3&gt;Install with GNU Make&lt;/h3&gt; 
 &lt;p&gt;To install all available utilities:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;make install
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;To install using &lt;code&gt;sudo&lt;/code&gt; switch &lt;code&gt;-E&lt;/code&gt; must be used:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;sudo -E make install
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;To install all but a few of the available utilities:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;make SKIP_UTILS='UTILITY_1 UTILITY_2' install
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;To install only a few of the available utilities:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;make UTILS='UTILITY_1 UTILITY_2' install
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;To install every program with a prefix (e.g. uu-echo uu-cat):&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;make PROG_PREFIX=PREFIX_GOES_HERE install
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;To install the multicall binary:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;make MULTICALL=y install
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Set install parent directory (default value is /usr/local):&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;# DESTDIR is also supported
make PREFIX=/my/path install
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Installing with &lt;code&gt;make&lt;/code&gt; installs shell completions for all installed utilities for &lt;code&gt;bash&lt;/code&gt;, &lt;code&gt;fish&lt;/code&gt; and &lt;code&gt;zsh&lt;/code&gt;. Completions for &lt;code&gt;elvish&lt;/code&gt; and &lt;code&gt;powershell&lt;/code&gt; can also be generated; See &lt;code&gt;Manually install shell completions&lt;/code&gt;.&lt;/p&gt; 
 &lt;p&gt;To skip installation of completions and manpages:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;make COMPLETIONS=n MANPAGES=n install
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;Manually install shell completions&lt;/h3&gt; 
 &lt;p&gt;The &lt;code&gt;coreutils&lt;/code&gt; binary can generate completions for the &lt;code&gt;bash&lt;/code&gt;, &lt;code&gt;elvish&lt;/code&gt;, &lt;code&gt;fish&lt;/code&gt;, &lt;code&gt;powershell&lt;/code&gt; and &lt;code&gt;zsh&lt;/code&gt; shells. It prints the result to stdout.&lt;/p&gt; 
 &lt;p&gt;The syntax is:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;cargo run completion &amp;lt;utility&amp;gt; &amp;lt;shell&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;So, to install completions for &lt;code&gt;ls&lt;/code&gt; on &lt;code&gt;bash&lt;/code&gt; to &lt;code&gt;/usr/local/share/bash-completion/completions/ls&lt;/code&gt;, run:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;cargo run completion ls bash &amp;gt; /usr/local/share/bash-completion/completions/ls
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;Manually install manpages&lt;/h3&gt; 
 &lt;p&gt;To generate manpages, the syntax is:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;cargo run manpage &amp;lt;utility&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;So, to install the manpage for &lt;code&gt;ls&lt;/code&gt; to &lt;code&gt;/usr/local/share/man/man1/ls.1&lt;/code&gt; run:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;cargo run manpage ls &amp;gt; /usr/local/share/man/man1/ls.1
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h2&gt;Un-installation&lt;/h2&gt; 
 &lt;p&gt;Un-installation differs depending on how you have installed uutils. If you used Cargo to install, use Cargo to uninstall. If you used GNU Make to install, use Make to uninstall.&lt;/p&gt; 
 &lt;h3&gt;Uninstall with Cargo&lt;/h3&gt; 
 &lt;p&gt;To uninstall uutils:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;cargo uninstall coreutils
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;Uninstall with GNU Make&lt;/h3&gt; 
 &lt;p&gt;To uninstall all utilities:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;make uninstall
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;To uninstall every program with a set prefix:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;make PROG_PREFIX=PREFIX_GOES_HERE uninstall
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;To uninstall the multicall binary:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;make MULTICALL=y uninstall
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;To uninstall from a custom parent directory:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;# DESTDIR is also supported
make PREFIX=/my/path uninstall
&lt;/code&gt;&lt;/pre&gt; 
 &lt;!-- ANCHOR_END: build (this mark is needed for mdbook) --&gt; 
 &lt;h2&gt;GNU test suite compatibility&lt;/h2&gt; 
 &lt;p&gt;Below is the evolution of how many GNU tests uutils passes. A more detailed breakdown of the GNU test results of the main branch can be found &lt;a href="https://uutils.github.io/coreutils/docs/test_coverage.html"&gt;in the user manual&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;See &lt;a href="https://github.com/orgs/uutils/projects/1"&gt;https://github.com/orgs/uutils/projects/1&lt;/a&gt; for the main meta bugs (many are missing).&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://github.com/uutils/coreutils-tracking/raw/main/gnu-results.svg?raw=true" alt="Evolution over time" /&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;!-- close oranda-hide div --&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;To contribute to uutils, please see &lt;a href="https://raw.githubusercontent.com/uutils/coreutils/main/CONTRIBUTING.md"&gt;CONTRIBUTING&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;uutils is licensed under the MIT License - see the &lt;code&gt;LICENSE&lt;/code&gt; file for details&lt;/p&gt; 
&lt;p&gt;GNU Coreutils is licensed under the GPL 3.0 or later.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Eventual-Inc/Daft</title>
      <link>https://github.com/Eventual-Inc/Daft</link>
      <description>&lt;p&gt;Distributed query engine providing simple and reliable data processing for any modality and scale&lt;/p&gt;&lt;hr&gt;&lt;p&gt;|Banner|&lt;/p&gt; 
&lt;p&gt;|CI| |PyPI| |Latest Tag| |Coverage| |Slack|&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;Website &amp;lt;https://www.daft.ai&amp;gt;&lt;/code&gt;_ â€¢ &lt;code&gt;Docs &amp;lt;https://docs.daft.ai&amp;gt;&lt;/code&gt;_ â€¢ &lt;code&gt;Installation &amp;lt;https://docs.daft.ai/en/stable/install/&amp;gt;&lt;/code&gt;_ â€¢ &lt;code&gt;Daft Quickstart &amp;lt;https://docs.daft.ai/en/stable/quickstart/&amp;gt;&lt;/code&gt;_ â€¢ &lt;code&gt;Community and Support &amp;lt;https://github.com/Eventual-Inc/Daft/discussions&amp;gt;&lt;/code&gt;_&lt;/p&gt; 
&lt;h1&gt;Daft: Unified Engine for Data Analytics, Engineering &amp;amp; ML/AI&lt;/h1&gt; 
&lt;p&gt;|TrendShift|&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;Daft &amp;lt;https://www.daft.ai&amp;gt;&lt;/code&gt;_ is a distributed query engine for large-scale data processing using Python or SQL, implemented in Rust.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Familiar interactive API:&lt;/strong&gt; Lazy Python Dataframe for rapid and interactive iteration, or SQL for analytical queries&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Focus on the what:&lt;/strong&gt; Powerful Query Optimizer that rewrites queries to be as efficient as possible&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Data Catalog integrations:&lt;/strong&gt; Integration with data catalogs (AWS Glue, Unity Catalog) and table formats like Apache Iceberg&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Rich multimodal type-system:&lt;/strong&gt; Supports multimodal types such as Images, URLs, Tensors and more&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Seamless Interchange&lt;/strong&gt;: Built on the &lt;code&gt;Apache Arrow &amp;lt;https://arrow.apache.org/docs/index.html&amp;gt;&lt;/code&gt;_ In-Memory Format&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Built for the cloud:&lt;/strong&gt; &lt;code&gt;Record-setting &amp;lt;https://www.daft.ai/blog/announcing-daft-02&amp;gt;&lt;/code&gt;_ I/O performance for integrations with S3 cloud storage&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;About Daft&lt;/code&gt;_&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Getting Started&lt;/code&gt;_&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Benchmarks&lt;/code&gt;_&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Contributing&lt;/code&gt;_&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Telemetry&lt;/code&gt;_&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Related Projects&lt;/code&gt;_&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;License&lt;/code&gt;_&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;About Daft&lt;/h2&gt; 
&lt;p&gt;Daft was designed with the following principles in mind:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Any Data&lt;/strong&gt;: Beyond the usual strings/numbers/dates, Daft columns can also hold complex or nested multimodal data such as Images, Embeddings and Python objects efficiently with its Arrow based memory representation. Ingestion and basic transformations of multimodal data is extremely easy and performant in Daft.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Interactive Computing&lt;/strong&gt;: Daft is built for the interactive developer experience through notebooks or REPLs - intelligent caching/query optimizations accelerates your experimentation and data exploration.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Distributed Computing&lt;/strong&gt;: Some workloads can quickly outgrow your local laptop's computational resources - Daft integrates natively with &lt;code&gt;Ray &amp;lt;https://www.ray.io&amp;gt;&lt;/code&gt;_ for running dataframes on large clusters of machines with thousands of CPUs/GPUs.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;Installation ^^^^^^^^^^^^&lt;/p&gt; 
&lt;p&gt;Install Daft with &lt;code&gt;pip install daft&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;For more advanced installations (e.g. installing from source or with extra dependencies such as Ray and AWS utilities), please see our &lt;code&gt;Installation Guide &amp;lt;https://docs.daft.ai/en/stable/install/&amp;gt;&lt;/code&gt;_&lt;/p&gt; 
&lt;p&gt;Quickstart ^^^^^^^^^^&lt;/p&gt; 
&lt;p&gt;Check out our &lt;code&gt;quickstart &amp;lt;https://docs.daft.ai/en/stable/quickstart/&amp;gt;&lt;/code&gt;_!&lt;/p&gt; 
&lt;p&gt;In this example, we load images from an AWS S3 bucket's URLs and resize each image in the dataframe:&lt;/p&gt; 
&lt;p&gt;.. code:: python&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;import daft

# Load a dataframe from filepaths in an S3 bucket
df = daft.from_glob_path("s3://daft-public-data/laion-sample-images/*")

# 1. Download column of image URLs as a column of bytes
# 2. Decode the column of bytes into a column of images
df = df.with_column("image", df["path"].url.download().image.decode())

# Resize each image into 32x32
df = df.with_column("resized", df["image"].image.resize(32, 32))

df.show(3)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;|Quickstart Image|&lt;/p&gt; 
&lt;h2&gt;Benchmarks&lt;/h2&gt; 
&lt;p&gt;|Benchmark Image|&lt;/p&gt; 
&lt;p&gt;To see the full benchmarks, detailed setup, and logs, check out our &lt;code&gt;benchmarking page. &amp;lt;https://docs.daft.ai/en/stable/resources/benchmarks/tpch/&amp;gt;&lt;/code&gt;_&lt;/p&gt; 
&lt;p&gt;More Resources ^^^^^^^^^^^^^^&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;Daft Quickstart &amp;lt;https://docs.daft.ai/en/stable/quickstart/&amp;gt;&lt;/code&gt;_ - learn more about Daft's full range of capabilities including dataloading from URLs, joins, user-defined functions (UDF), groupby, aggregations and more.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;User Guide &amp;lt;https://docs.daft.ai/en/stable/&amp;gt;&lt;/code&gt;_ - take a deep-dive into each topic within Daft&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;API Reference &amp;lt;https://docs.daft.ai/en/stable/api/&amp;gt;&lt;/code&gt;_ - API reference for public classes/functions of Daft&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;SQL Reference &amp;lt;https://docs.daft.ai/en/stable/sql/&amp;gt;&lt;/code&gt;_ - Daft SQL reference&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We &amp;lt;3 developers! To start contributing to Daft, please read &lt;code&gt;CONTRIBUTING.md &amp;lt;https://github.com/Eventual-Inc/Daft/blob/main/CONTRIBUTING.md&amp;gt;&lt;/code&gt;_. This document describes the development lifecycle and toolchain for working on Daft. It also details how to add new functionality to the core engine and expose it through a Python API.&lt;/p&gt; 
&lt;p&gt;Here's a list of &lt;code&gt;good first issues &amp;lt;https://github.com/Eventual-Inc/Daft/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22&amp;gt;&lt;/code&gt;_ to get yourself warmed up with Daft. Comment in the issue to pick it up, and feel free to ask any questions!&lt;/p&gt; 
&lt;h2&gt;Telemetry&lt;/h2&gt; 
&lt;p&gt;To help improve Daft, we collect non-identifiable data via Scarf (&lt;a href="https://scarf.sh"&gt;https://scarf.sh&lt;/a&gt;).&lt;/p&gt; 
&lt;p&gt;To disable this behavior, set the environment variable &lt;code&gt;DO_NOT_TRACK=true&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;The data that we collect is:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Non-identifiable:&lt;/strong&gt; Events are keyed by a session ID which is generated on import of Daft&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Metadata-only:&lt;/strong&gt; We do not collect any of our usersâ€™ proprietary code or data&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;For development only:&lt;/strong&gt; We do not buy or sell any user data&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Please see our &lt;code&gt;documentation &amp;lt;https://docs.daft.ai/en/stable/resources/telemetry/&amp;gt;&lt;/code&gt;_ for more details.&lt;/p&gt; 
&lt;p&gt;.. image:: &lt;a href="https://static.scarf.sh/a.png?x-pxid=31f8d5ba-7e09-4d75-8895-5252bbf06cf6"&gt;https://static.scarf.sh/a.png?x-pxid=31f8d5ba-7e09-4d75-8895-5252bbf06cf6&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Related Projects&lt;/h2&gt; 
&lt;p&gt;+---------------------------------------------------+-----------------+---------------+-------------+-----------------+-----------------------------+-------------+ | Engine | Query Optimizer | Multimodal | Distributed | Arrow Backed | Vectorized Execution Engine | Out-of-core | +===================================================+=================+===============+=============+=================+=============================+=============+ | Daft | Yes | Yes | Yes | Yes | Yes | Yes | +---------------------------------------------------+-----------------+---------------+-------------+-----------------+-----------------------------+-------------+ | &lt;code&gt;Pandas &amp;lt;https://github.com/pandas-dev/pandas&amp;gt;&lt;/code&gt;_ | No | Python object | No | optional &amp;gt;= 2.0 | Some(Numpy) | No | +---------------------------------------------------+-----------------+---------------+-------------+-----------------+-----------------------------+-------------+ | &lt;code&gt;Polars &amp;lt;https://github.com/pola-rs/polars&amp;gt;&lt;/code&gt;_ | Yes | Python object | No | Yes | Yes | Yes | +---------------------------------------------------+-----------------+---------------+-------------+-----------------+-----------------------------+-------------+ | &lt;code&gt;Modin &amp;lt;https://github.com/modin-project/modin&amp;gt;&lt;/code&gt;_ | Yes | Python object | Yes | No | Some(Pandas) | Yes | +---------------------------------------------------+-----------------+---------------+-------------+-----------------+-----------------------------+-------------+ | &lt;code&gt;PySpark &amp;lt;https://github.com/apache/spark&amp;gt;&lt;/code&gt;_ | Yes | No | Yes | Pandas UDF/IO | Pandas UDF | Yes | +---------------------------------------------------+-----------------+---------------+-------------+-----------------+-----------------------------+-------------+ | &lt;code&gt;Dask DF &amp;lt;https://github.com/dask/dask&amp;gt;&lt;/code&gt;_ | No | Python object | Yes | No | Some(Pandas) | Yes | +---------------------------------------------------+-----------------+---------------+-------------+-----------------+-----------------------------+-------------+&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Daft has an Apache 2.0 license - please see the LICENSE file.&lt;/p&gt; 
&lt;p&gt;.. |Quickstart Image| image:: &lt;a href="https://github.com/Eventual-Inc/Daft/assets/17691182/dea2f515-9739-4f3e-ac58-cd96d51e44a8"&gt;https://github.com/Eventual-Inc/Daft/assets/17691182/dea2f515-9739-4f3e-ac58-cd96d51e44a8&lt;/a&gt; :alt: Dataframe code to load a folder of images from AWS S3 and create thumbnails :height: 256&lt;/p&gt; 
&lt;p&gt;.. |Benchmark Image| image:: &lt;a href="https://github-production-user-asset-6210df.s3.amazonaws.com/2550285/243524430-338e427d-f049-40b3-b555-4059d6be7bfd.png"&gt;https://github-production-user-asset-6210df.s3.amazonaws.com/2550285/243524430-338e427d-f049-40b3-b555-4059d6be7bfd.png&lt;/a&gt; :alt: Benchmarks for SF100 TPCH&lt;/p&gt; 
&lt;p&gt;.. |Banner| image:: &lt;a href="https://daft.ai/images/diagram.png"&gt;https://daft.ai/images/diagram.png&lt;/a&gt; :target: &lt;a href="https://www.daft.ai"&gt;https://www.daft.ai&lt;/a&gt; :alt: Daft dataframes can load any data such as PDF documents, images, protobufs, csv, parquet and audio files into a table dataframe structure for easy querying&lt;/p&gt; 
&lt;p&gt;.. |CI| image:: &lt;a href="https://github.com/Eventual-Inc/Daft/actions/workflows/pr-test-suite.yml/badge.svg"&gt;https://github.com/Eventual-Inc/Daft/actions/workflows/pr-test-suite.yml/badge.svg&lt;/a&gt; :target: &lt;a href="https://github.com/Eventual-Inc/Daft/actions/workflows/pr-test-suite.yml?query=branch:main"&gt;https://github.com/Eventual-Inc/Daft/actions/workflows/pr-test-suite.yml?query=branch:main&lt;/a&gt; :alt: GitHub Actions tests&lt;/p&gt; 
&lt;p&gt;.. |PyPI| image:: &lt;a href="https://img.shields.io/pypi/v/daft.svg?label=pip&amp;amp;logo=PyPI&amp;amp;logoColor=white"&gt;https://img.shields.io/pypi/v/daft.svg?label=pip&amp;amp;logo=PyPI&amp;amp;logoColor=white&lt;/a&gt; :target: &lt;a href="https://pypi.org/project/daft"&gt;https://pypi.org/project/daft&lt;/a&gt; :alt: PyPI&lt;/p&gt; 
&lt;p&gt;.. |Latest Tag| image:: &lt;a href="https://img.shields.io/github/v/tag/Eventual-Inc/Daft?label=latest&amp;amp;logo=GitHub"&gt;https://img.shields.io/github/v/tag/Eventual-Inc/Daft?label=latest&amp;amp;logo=GitHub&lt;/a&gt; :target: &lt;a href="https://github.com/Eventual-Inc/Daft/tags"&gt;https://github.com/Eventual-Inc/Daft/tags&lt;/a&gt; :alt: latest tag&lt;/p&gt; 
&lt;p&gt;.. |Coverage| image:: &lt;a href="https://codecov.io/gh/Eventual-Inc/Daft/branch/main/graph/badge.svg?token=J430QVFE89"&gt;https://codecov.io/gh/Eventual-Inc/Daft/branch/main/graph/badge.svg?token=J430QVFE89&lt;/a&gt; :target: &lt;a href="https://codecov.io/gh/Eventual-Inc/Daft"&gt;https://codecov.io/gh/Eventual-Inc/Daft&lt;/a&gt; :alt: Coverage&lt;/p&gt; 
&lt;p&gt;.. |Slack| image:: &lt;a href="https://img.shields.io/badge/slack-@distdata-purple.svg?logo=slack"&gt;https://img.shields.io/badge/slack-@distdata-purple.svg?logo=slack&lt;/a&gt; :target: &lt;a href="https://join.slack.com/t/dist-data/shared_invite/zt-2e77olvxw-uyZcPPV1SRchhi8ah6ZCtg"&gt;https://join.slack.com/t/dist-data/shared_invite/zt-2e77olvxw-uyZcPPV1SRchhi8ah6ZCtg&lt;/a&gt; :alt: slack community&lt;/p&gt; 
&lt;p&gt;.. |TrendShift| image:: &lt;a href="https://trendshift.io/api/badge/repositories/8239"&gt;https://trendshift.io/api/badge/repositories/8239&lt;/a&gt; :target: &lt;a href="https://trendshift.io/repositories/8239"&gt;https://trendshift.io/repositories/8239&lt;/a&gt; :alt: Eventual-Inc/Daft | Trendshift :width: 250px :height: 55px&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>vercel/examples</title>
      <link>https://github.com/vercel/examples</link>
      <description>&lt;p&gt;Enjoy our curated collection of examples and solutions. Use these patterns to build your own robust and scalable applications.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://vercel.com"&gt; &lt;img src="https://assets.vercel.com/image/upload/v1588805858/repositories/vercel/logo.png" height="96" /&gt; &lt;/a&gt;&lt;/p&gt;
&lt;h3 align="center"&gt;&lt;a href="https://vercel.com"&gt;Vercel Examples&lt;/a&gt;&lt;/h3&gt;
&lt;a href="https://vercel.com"&gt; &lt;/a&gt; 
&lt;p&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/vercel/examples/main/solutions"&gt;Solutions&lt;/a&gt; â€“ Demos, reference architecture, and best practices&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/vercel/examples/main/starter"&gt;Starter&lt;/a&gt; â€“ Functional applications which can act as a starting point&lt;/li&gt; 
 &lt;li&gt;And more!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Vercel Templates&lt;/h2&gt; 
&lt;p&gt;Multiple examples are being featured in &lt;a href="https://vercel.com/templates"&gt;Vercel's Templates&lt;/a&gt;, visit that page for more advanced filtering options.&lt;/p&gt; 
&lt;h3&gt;For Vercelians&lt;/h3&gt; 
&lt;p&gt;Examples that have front matter metadata will create a new Draft template in &lt;a href="https://app.contentful.com"&gt;Contentful&lt;/a&gt;, for more steps on how to publish a template, read &lt;a href="https://raw.githubusercontent.com/vercel/examples/main/internal/publishing-templates.md"&gt;Publishing Templates&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Adding a new example&lt;/h2&gt; 
&lt;p&gt;To quickly start contributing with a new example, run the following commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pnpm i
pnpm new-example
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If the script above isn't used, make sure the example complies with the following:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;It must have a &lt;code&gt;.gitignore&lt;/code&gt; similar to &lt;a href="https://raw.githubusercontent.com/vercel/examples/main/plop-templates/example/.gitignore"&gt;plop-templates/example/.gitignore&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;It must have a &lt;code&gt;package.json&lt;/code&gt; similar to &lt;a href="https://raw.githubusercontent.com/vercel/examples/main/plop-templates/example/package.json"&gt;plop-templates/example/package.json&lt;/a&gt; (usage of Next.js is optional). The license should be &lt;code&gt;MIT&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;It must have a &lt;code&gt;README.md&lt;/code&gt; similar to &lt;a href="https://raw.githubusercontent.com/vercel/examples/main/plop-templates/example/README.md"&gt;plop-templates/example/README.md&lt;/a&gt;. The example has to be able to include a demo URL (the Vercel team will deploy it!) and if it requires environment variables, it must have a &lt;code&gt;.env.example&lt;/code&gt; file and instructions on how to set them up. Take &lt;a href="https://raw.githubusercontent.com/vercel/examples/main/edge-middleware/bot-protection-datadome/README.md"&gt;bot-protection-datadome&lt;/a&gt; as an example. 
  &lt;ul&gt; 
   &lt;li&gt;To customize the Vercel Deploy Button take a look at the &lt;a href="https://vercel.com/docs/deploy-button"&gt;docs&lt;/a&gt;, useful if the deployment has required environment variables.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;If using Next.js, it must have a &lt;code&gt;.eslintrc.json&lt;/code&gt; similar to &lt;a href="https://raw.githubusercontent.com/vercel/examples/main/plop-templates/example/.eslintrc.json"&gt;plop-templates/example/.eslintrc.json&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;All Next.js examples should be using the same styling and layout provided by &lt;code&gt;@vercel/examples-ui&lt;/code&gt;, its usage can be seen in the &lt;a href="https://raw.githubusercontent.com/vercel/examples/main/plop-templates/example"&gt;plop template&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Adding a template&lt;/h3&gt; 
&lt;p&gt;If you would like the example to be featured in &lt;a href="https://vercel.com/templates"&gt;vercel.com/templates&lt;/a&gt; then also add the front matter metadata to the top of the readme, like in &lt;a href="https://raw.githubusercontent.com/vercel/examples/main/edge-middleware/bot-protection-datadome/README.md"&gt;bot-protection-datadome&lt;/a&gt;. To know all the possible values for each metadata take a look at &lt;a href="https://raw.githubusercontent.com/vercel/examples/main/internal/fields.json"&gt;&lt;code&gt;internal/fields.json&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you want to add related templates to your template, copy the &lt;code&gt;slug&lt;/code&gt; from the other template into the &lt;code&gt;relatedTemplates&lt;/code&gt; field, for example for &lt;a href="https://vercel.com/templates/next.js/monorepo-turborepo"&gt;vercel.com/templates/next.js/monorepo-turborepo&lt;/a&gt; the slug is &lt;code&gt;monorepo-turborepo&lt;/code&gt;, as written in &lt;a href="https://raw.githubusercontent.com/vercel/examples/main/solutions/monorepo/README.md"&gt;solutions/monorepo/README.md&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;The pre-commit hook&lt;/h3&gt; 
&lt;p&gt;We use &lt;a href="https://typicode.github.io/husky/#/"&gt;Husky&lt;/a&gt; to manage the pre-commit &lt;a href="https://git-scm.com/docs/githooks"&gt;Git hook&lt;/a&gt; in this repo. Husky configures hooks automatically during install, so you don't need to do anything special to get them working, but if it fails to install, you can run the following command to install it manually:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pnpm run prepare
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Code changes automatically go through Prettier and ESLint when you make a commit, &lt;strong&gt;please do not skip these steps&lt;/strong&gt; unless they're broken and in that case let us known by creating an issue.&lt;/p&gt; 
&lt;h2&gt;Read the Docs&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://vercel.com/docs"&gt;Vercel Docs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nextjs.org/docs"&gt;Next.js Docs&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If you have any questions or suggestions about the docs, feel free to &lt;a href="https://github.com/vercel/examples/discussions"&gt;open a discussion&lt;/a&gt;, or &lt;a href="https://github.com/vercel/examples/pulls"&gt;submit a PR&lt;/a&gt; with your suggestions!&lt;/p&gt; 
&lt;h2&gt;Provide Feedback&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vercel/examples/discussions"&gt;Start a Discussion&lt;/a&gt; with a question, piece of feedback, or idea you want to share with the team.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vercel/examples/issues"&gt;Open an Issue&lt;/a&gt; if you believe you've encountered a bug that you want to flag for the team.&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>awslabs/agent-squad</title>
      <link>https://github.com/awslabs/agent-squad</link>
      <description>&lt;p&gt;Flexible and powerful framework for managing multiple AI agents and handling complex conversations&lt;/p&gt;&lt;hr&gt;&lt;h2 align="center"&gt;Agent Squad&lt;/h2&gt; 
&lt;p align="center"&gt;Flexible, lightweight open-source framework for orchestrating multiple AI agents to handle complex conversations.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p align="center"&gt; &lt;strong&gt;ğŸ“¢ New Name Alert:&lt;/strong&gt; Multi-Agent Orchestrator is now &lt;strong&gt;Agent Squad!&lt;/strong&gt; ğŸ‰&lt;br /&gt; Same powerful functionalities, new catchy name. Embrace the squad! &lt;/p&gt; 
&lt;hr /&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/awslabs/agent-squad"&gt;&lt;img alt="GitHub Repo" src="https://img.shields.io/badge/GitHub-Repo-green.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://www.npmjs.com/package/agent-squad"&gt;&lt;img alt="npm" src="https://img.shields.io/npm/v/agent-squad.svg?style=flat-square" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/agent-squad/"&gt;&lt;img alt="PyPI" src="https://img.shields.io/pypi/v/agent-squad.svg?style=flat-square" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; 
 &lt;!-- GitHub Stats --&gt; &lt;img src="https://img.shields.io/github/stars/awslabs/agent-squad?style=social" alt="GitHub stars" /&gt; &lt;img src="https://img.shields.io/github/forks/awslabs/agent-squad?style=social" alt="GitHub forks" /&gt; &lt;img src="https://img.shields.io/github/watchers/awslabs/agent-squad?style=social" alt="GitHub watchers" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; 
 &lt;!-- Repository Info --&gt; &lt;img src="https://img.shields.io/github/last-commit/awslabs/agent-squad" alt="Last Commit" /&gt; &lt;img src="https://img.shields.io/github/issues/awslabs/agent-squad" alt="Issues" /&gt; &lt;img src="https://img.shields.io/github/issues-pr/awslabs/agent-squad" alt="Pull Requests" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://awslabs.github.io/agent-squad/" style="display: inline-block; background-color: #0066cc; color: white; padding: 10px 20px; text-decoration: none; border-radius: 5px; font-weight: bold; font-size: 15px; transition: background-color 0.3s;"&gt; ğŸ“š Explore Full Documentation &lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;ğŸ”– Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ§  &lt;strong&gt;Intelligent intent classification&lt;/strong&gt; â€” Dynamically route queries to the most suitable agent based on context and content.&lt;/li&gt; 
 &lt;li&gt;ğŸ”¤ &lt;strong&gt;Dual language support&lt;/strong&gt; â€” Fully implemented in both &lt;strong&gt;Python&lt;/strong&gt; and &lt;strong&gt;TypeScript&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;ğŸŒŠ &lt;strong&gt;Flexible agent responses&lt;/strong&gt; â€” Support for both streaming and non-streaming responses from different agents.&lt;/li&gt; 
 &lt;li&gt;ğŸ“š &lt;strong&gt;Context management&lt;/strong&gt; â€” Maintain and utilize conversation context across multiple agents for coherent interactions.&lt;/li&gt; 
 &lt;li&gt;ğŸ”§ &lt;strong&gt;Extensible architecture&lt;/strong&gt; â€” Easily integrate new agents or customize existing ones to fit your specific needs.&lt;/li&gt; 
 &lt;li&gt;ğŸŒ &lt;strong&gt;Universal deployment&lt;/strong&gt; â€” Run anywhere - from AWS Lambda to your local environment or any cloud platform.&lt;/li&gt; 
 &lt;li&gt;ğŸ“¦ &lt;strong&gt;Pre-built agents and classifiers&lt;/strong&gt; â€” A variety of ready-to-use agents and multiple classifier implementations available.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;What's the Agent Squad â“&lt;/h2&gt; 
&lt;p&gt;The Agent Squad is a flexible framework for managing multiple AI agents and handling complex conversations. It intelligently routes queries and maintains context across interactions.&lt;/p&gt; 
&lt;p&gt;The system offers pre-built components for quick deployment, while also allowing easy integration of custom agents and conversation messages storage solutions.&lt;/p&gt; 
&lt;p&gt;This adaptability makes it suitable for a wide range of applications, from simple chatbots to sophisticated AI systems, accommodating diverse requirements and scaling efficiently.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ—ï¸ High-level architecture flow diagram&lt;/h2&gt; 
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/awslabs/agent-squad/main/img/flow.jpg" alt="High-level architecture flow diagram" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;The process begins with user input, which is analyzed by a Classifier.&lt;/li&gt; 
 &lt;li&gt;The Classifier leverages both Agents' Characteristics and Agents' Conversation history to select the most appropriate agent for the task.&lt;/li&gt; 
 &lt;li&gt;Once an agent is selected, it processes the user input.&lt;/li&gt; 
 &lt;li&gt;The orchestrator then saves the conversation, updating the Agents' Conversation history, before delivering the response back to the user.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;&lt;img src="https://raw.githubusercontent.com/awslabs/agent-squad/main/img/new.png" alt="" /&gt; Introducing SupervisorAgent: Agents Coordination&lt;/h2&gt; 
&lt;p&gt;The Agent Squad now includes a powerful new SupervisorAgent that enables sophisticated team coordination between multiple specialized agents. This new component implements a "agent-as-tools" architecture, allowing a lead agent to coordinate a team of specialized agents in parallel, maintaining context and delivering coherent responses.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/awslabs/agent-squad/main/img/flow-supervisor.jpg" alt="SupervisorAgent flow diagram" /&gt;&lt;/p&gt; 
&lt;p&gt;Key capabilities:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ¤ &lt;strong&gt;Team Coordination&lt;/strong&gt; - Coordinate multiple specialized agents working together on complex tasks&lt;/li&gt; 
 &lt;li&gt;âš¡ &lt;strong&gt;Parallel Processing&lt;/strong&gt; - Execute multiple agent queries simultaneously&lt;/li&gt; 
 &lt;li&gt;ğŸ§  &lt;strong&gt;Smart Context Management&lt;/strong&gt; - Maintain conversation history across all team members&lt;/li&gt; 
 &lt;li&gt;ğŸ”„ &lt;strong&gt;Dynamic Delegation&lt;/strong&gt; - Intelligently distribute subtasks to appropriate team members&lt;/li&gt; 
 &lt;li&gt;ğŸ¤– &lt;strong&gt;Agent Compatibility&lt;/strong&gt; - Works with all agent types (Bedrock, Anthropic, Lex, etc.)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The SupervisorAgent can be used in two powerful ways:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Direct Usage&lt;/strong&gt; - Call it directly when you need dedicated team coordination for specific tasks&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Classifier Integration&lt;/strong&gt; - Add it as an agent within the classifier to build complex hierarchical systems with multiple specialized teams&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Here are just a few examples where this agent can be used:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Customer Support Teams with specialized sub-teams&lt;/li&gt; 
 &lt;li&gt;AI Movie Production Studios&lt;/li&gt; 
 &lt;li&gt;Travel Planning Services&lt;/li&gt; 
 &lt;li&gt;Product Development Teams&lt;/li&gt; 
 &lt;li&gt;Healthcare Coordination Systems&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://awslabs.github.io/agent-squad/agents/built-in/supervisor-agent"&gt;Learn more about SupervisorAgent â†’&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ğŸ’¬ Demo App&lt;/h2&gt; 
&lt;p&gt;In the screen recording below, we demonstrate an extended version of the demo app that uses 6 specialized agents:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Travel Agent&lt;/strong&gt;: Powered by an Amazon Lex Bot&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Weather Agent&lt;/strong&gt;: Utilizes a Bedrock LLM Agent with a tool to query the open-meteo API&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Restaurant Agent&lt;/strong&gt;: Implemented as an Amazon Bedrock Agent&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Math Agent&lt;/strong&gt;: Utilizes a Bedrock LLM Agent with two tools for executing mathematical operations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Tech Agent&lt;/strong&gt;: A Bedrock LLM Agent designed to answer questions on technical topics&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Health Agent&lt;/strong&gt;: A Bedrock LLM Agent focused on addressing health-related queries&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Watch as the system seamlessly switches context between diverse topics, from booking flights to checking weather, solving math problems, and providing health information. Notice how the appropriate agent is selected for each query, maintaining coherence even with brief follow-up inputs.&lt;/p&gt; 
&lt;p&gt;The demo highlights the system's ability to handle complex, multi-turn conversations while preserving context and leveraging specialized agents across various domains.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/awslabs/agent-squad/main/img/demo-app.gif?raw=true" alt="" /&gt;&lt;/p&gt; 
&lt;h2&gt;ğŸ¯ Examples &amp;amp; Quick Start&lt;/h2&gt; 
&lt;p&gt;Get hands-on experience with the Agent Squad through our diverse set of examples:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Demo Applications&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://github.com/awslabs/agent-squad/tree/main/examples/python"&gt;Streamlit Global Demo&lt;/a&gt;: A single Streamlit application showcasing multiple demos, including: 
    &lt;ul&gt; 
     &lt;li&gt;AI Movie Production Studio&lt;/li&gt; 
     &lt;li&gt;AI Travel Planner&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://awslabs.github.io/agent-squad/cookbook/examples/chat-demo-app/"&gt;Chat Demo App&lt;/a&gt;: 
    &lt;ul&gt; 
     &lt;li&gt;Explore multiple specialized agents handling various domains like travel, weather, math, and health&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://awslabs.github.io/agent-squad/cookbook/examples/ecommerce-support-simulator/"&gt;E-commerce Support Simulator&lt;/a&gt;: Experience AI-powered customer support with: 
    &lt;ul&gt; 
     &lt;li&gt;Automated response generation for common queries&lt;/li&gt; 
     &lt;li&gt;Intelligent routing of complex issues to human support&lt;/li&gt; 
     &lt;li&gt;Real-time chat and email-style communication&lt;/li&gt; 
     &lt;li&gt;Human-in-the-loop interactions for complex cases&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Sample Projects&lt;/strong&gt;: Explore our example implementations in the &lt;code&gt;examples&lt;/code&gt; folder: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://github.com/awslabs/agent-squad/tree/main/examples/chat-demo-app"&gt;&lt;code&gt;chat-demo-app&lt;/code&gt;&lt;/a&gt;: Web-based chat interface with multiple specialized agents&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/awslabs/agent-squad/tree/main/examples/ecommerce-support-simulator"&gt;&lt;code&gt;ecommerce-support-simulator&lt;/code&gt;&lt;/a&gt;: AI-powered customer support system&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/awslabs/agent-squad/tree/main/examples/chat-chainlit-app"&gt;&lt;code&gt;chat-chainlit-app&lt;/code&gt;&lt;/a&gt;: Chat application built with Chainlit&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/awslabs/agent-squad/tree/main/examples/fast-api-streaming"&gt;&lt;code&gt;fast-api-streaming&lt;/code&gt;&lt;/a&gt;: FastAPI implementation with streaming support&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/awslabs/agent-squad/tree/main/examples/text-2-structured-output"&gt;&lt;code&gt;text-2-structured-output&lt;/code&gt;&lt;/a&gt;: Natural Language to Structured Data&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/awslabs/agent-squad/tree/main/examples/bedrock-inline-agents"&gt;&lt;code&gt;bedrock-inline-agents&lt;/code&gt;&lt;/a&gt;: Bedrock Inline Agents sample&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/awslabs/agent-squad/tree/main/examples/bedrock-prompt-routing"&gt;&lt;code&gt;bedrock-prompt-routing&lt;/code&gt;&lt;/a&gt;: Bedrock Prompt Routing sample code&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Examples are available in both Python and TypeScript. Check out our &lt;a href="https://awslabs.github.io/agent-squad/"&gt;documentation&lt;/a&gt; for comprehensive guides on setting up and using the Agent Squad framework!&lt;/p&gt; 
&lt;h2&gt;ğŸ“š Deep Dives: Stories, Blogs &amp;amp; Podcasts&lt;/h2&gt; 
&lt;p&gt;Discover creative implementations and diverse applications of the Agent Squad:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://community.aws/content/2lCi8jEKydhDm8eE8QFIQ5K23pF/from-bonjour-to-boarding-pass-multilingual-ai-chatbot-for-flight-reservations"&gt;From 'Bonjour' to 'Boarding Pass': Multilingual AI Chatbot for Flight Reservations&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This article demonstrates how to build a multilingual chatbot using the Agent Squad framework. The article explains how to use an &lt;strong&gt;Amazon Lex&lt;/strong&gt; bot as an agent, along with 2 other new agents to make it work in many languages with just a few lines of code.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://community.aws/content/2lq6cYYwTYGc7S3Zmz28xZoQNQj/beyond-auto-replies-building-an-ai-powered-e-commerce-support-system"&gt;Beyond Auto-Replies: Building an AI-Powered E-commerce Support system&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This article demonstrates how to build an AI-driven multi-agent system for automated e-commerce customer email support. It covers the architecture and setup of specialized AI agents using the Agent Squad framework, integrating automated processing with human-in-the-loop oversight. The guide explores email ingestion, intelligent routing, automated response generation, and human verification, providing a comprehensive approach to balancing AI efficiency with human expertise in customer support.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://community.aws/content/2mt7CFG7xg4yw6GRHwH9akhg0oD/speak-up-ai-voicing-your-agents-with-amazon-connect-lex-and-bedrock"&gt;Speak Up, AI: Voicing Your Agents with Amazon Connect, Lex, and Bedrock&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This article demonstrates how to build an AI customer call center. It covers the architecture and setup of specialized AI agents using the Agent Squad framework interacting with voice via &lt;strong&gt;Amazon Connect&lt;/strong&gt; and &lt;strong&gt;Amazon Lex&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://community.aws/content/2pTsHrYPqvAbJBl9ht1XxPOSPjR/unlock-bedrock-invokeinlineagent-api-s-hidden-potential-with-agent-squad"&gt;Unlock Bedrock InvokeInlineAgent API's Hidden Potential&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Learn how to scale &lt;strong&gt;Amazon Bedrock Agents&lt;/strong&gt; beyond knowledge base limitations using the Agent Squad framework and &lt;strong&gt;InvokeInlineAgent API&lt;/strong&gt;. This article demonstrates dynamic agent creation and knowledge base selection for enterprise-scale AI applications.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://community.aws/content/2phMjQ0bqWMg4PBwejBs1uf4YQE/supercharging-amazon-bedrock-flows-with-aws-agent-squad"&gt;Supercharging Amazon Bedrock Flows&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Learn how to enhance &lt;strong&gt;Amazon Bedrock Flows&lt;/strong&gt; with conversation memory and multi-flow orchestration using the Agent Squad framework. This guide shows how to overcome Bedrock Flows' limitations to build more sophisticated AI workflows with persistent memory and intelligent routing between flows.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ™ï¸ Podcast Discussions&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ‡«ğŸ‡· Podcast (French)&lt;/strong&gt;: L'orchestrateur multi-agents : Un orchestrateur open source pour vos agents IA&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Platforms&lt;/strong&gt;: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://podcasts.apple.com/be/podcast/lorchestrateur-multi-agents/id1452118442?i=1000684332612"&gt;Apple Podcasts&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://open.spotify.com/episode/4RdMazSRhZUyW2pniG91Vf"&gt;Spotify&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ‡¬ğŸ‡§ Podcast (English)&lt;/strong&gt;: An Orchestrator for Your AI Agents&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Platforms&lt;/strong&gt;: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://podcasts.apple.com/us/podcast/an-orchestrator-for-your-ai-agents/id1574162669?i=1000677039579"&gt;Apple Podcasts&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://open.spotify.com/episode/2a9DBGZn2lVqVMBLWGipHU"&gt;Spotify&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;TypeScript Version&lt;/h3&gt; 
&lt;h4&gt;Installation&lt;/h4&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ğŸ”„ &lt;code&gt;multi-agent-orchestrator&lt;/code&gt; becomes &lt;code&gt;agent-squad&lt;/code&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npm install agent-squad
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Usage&lt;/h4&gt; 
&lt;p&gt;The following example demonstrates how to use the Agent Squad with two different types of agents: a Bedrock LLM Agent with Converse API support and a Lex Bot Agent. This showcases the flexibility of the system in integrating various AI services.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-typescript"&gt;import { AgentSquad, BedrockLLMAgent, LexBotAgent } from "agent-squad";

const orchestrator = new AgentSquad();

// Add a Bedrock LLM Agent with Converse API support
orchestrator.addAgent(
  new BedrockLLMAgent({
      name: "Tech Agent",
      description:
        "Specializes in technology areas including software development, hardware, AI, cybersecurity, blockchain, cloud computing, emerging tech innovations, and pricing/costs related to technology products and services.",
      streaming: true
  })
);

// Add a Lex Bot Agent for handling travel-related queries
orchestrator.addAgent(
  new LexBotAgent({
    name: "Travel Agent",
    description: "Helps users book and manage their flight reservations",
    botId: process.env.LEX_BOT_ID,
    botAliasId: process.env.LEX_BOT_ALIAS_ID,
    localeId: "en_US",
  })
);

// Example usage
const response = await orchestrator.routeRequest(
  "I want to book a flight",
  'user123',
  'session456'
);

// Handle the response (streaming or non-streaming)
if (response.streaming == true) {
    console.log("\n** RESPONSE STREAMING ** \n");
    // Send metadata immediately
    console.log(`&amp;gt; Agent ID: ${response.metadata.agentId}`);
    console.log(`&amp;gt; Agent Name: ${response.metadata.agentName}`);
    console.log(`&amp;gt; User Input: ${response.metadata.userInput}`);
    console.log(`&amp;gt; User ID: ${response.metadata.userId}`);
    console.log(`&amp;gt; Session ID: ${response.metadata.sessionId}`);
    console.log(
      `&amp;gt; Additional Parameters:`,
      response.metadata.additionalParams
    );
    console.log(`\n&amp;gt; Response: `);

    // Stream the content
    for await (const chunk of response.output) {
      if (typeof chunk === "string") {
        process.stdout.write(chunk);
      } else {
        console.error("Received unexpected chunk type:", typeof chunk);
      }
    }

} else {
    // Handle non-streaming response (AgentProcessingResult)
    console.log("\n** RESPONSE ** \n");
    console.log(`&amp;gt; Agent ID: ${response.metadata.agentId}`);
    console.log(`&amp;gt; Agent Name: ${response.metadata.agentName}`);
    console.log(`&amp;gt; User Input: ${response.metadata.userInput}`);
    console.log(`&amp;gt; User ID: ${response.metadata.userId}`);
    console.log(`&amp;gt; Session ID: ${response.metadata.sessionId}`);
    console.log(
      `&amp;gt; Additional Parameters:`,
      response.metadata.additionalParams
    );
    console.log(`\n&amp;gt; Response: ${response.output}`);
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Python Version&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ğŸ”„ &lt;code&gt;multi-agent-orchestrator&lt;/code&gt; becomes &lt;code&gt;agent-squad&lt;/code&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Optional: Set up a virtual environment
python -m venv venv
source venv/bin/activate  # On Windows use `venv\Scripts\activate`
pip install agent-squad[aws]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Default Usage&lt;/h4&gt; 
&lt;p&gt;Here's an equivalent Python example demonstrating the use of the Agent Squad with a Bedrock LLM Agent and a Lex Bot Agent:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import sys
import asyncio
from agent_squad.orchestrator import AgentSquad
from agent_squad.agents import BedrockLLMAgent, BedrockLLMAgentOptions, AgentStreamResponse

orchestrator = AgentSquad()

tech_agent = BedrockLLMAgent(BedrockLLMAgentOptions(
  name="Tech Agent",
  streaming=True,
  description="Specializes in technology areas including software development, hardware, AI, \
  cybersecurity, blockchain, cloud computing, emerging tech innovations, and pricing/costs \
  related to technology products and services.",
  model_id="anthropic.claude-3-sonnet-20240229-v1:0",
))
orchestrator.add_agent(tech_agent)


health_agent = BedrockLLMAgent(BedrockLLMAgentOptions(
  name="Health Agent",
  streaming=True,
  description="Specializes in health and well being",
))
orchestrator.add_agent(health_agent)

async def main():
    # Example usage
    response = await orchestrator.route_request(
        "What is AWS Lambda?",
        'user123',
        'session456',
        {},
        True
    )

    # Handle the response (streaming or non-streaming)
    if response.streaming:
        print("\n** RESPONSE STREAMING ** \n")
        # Send metadata immediately
        print(f"&amp;gt; Agent ID: {response.metadata.agent_id}")
        print(f"&amp;gt; Agent Name: {response.metadata.agent_name}")
        print(f"&amp;gt; User Input: {response.metadata.user_input}")
        print(f"&amp;gt; User ID: {response.metadata.user_id}")
        print(f"&amp;gt; Session ID: {response.metadata.session_id}")
        print(f"&amp;gt; Additional Parameters: {response.metadata.additional_params}")
        print("\n&amp;gt; Response: ")

        # Stream the content
        async for chunk in response.output:
            async for chunk in response.output:
              if isinstance(chunk, AgentStreamResponse):
                  print(chunk.text, end='', flush=True)
              else:
                  print(f"Received unexpected chunk type: {type(chunk)}", file=sys.stderr)

    else:
        # Handle non-streaming response (AgentProcessingResult)
        print("\n** RESPONSE ** \n")
        print(f"&amp;gt; Agent ID: {response.metadata.agent_id}")
        print(f"&amp;gt; Agent Name: {response.metadata.agent_name}")
        print(f"&amp;gt; User Input: {response.metadata.user_input}")
        print(f"&amp;gt; User ID: {response.metadata.user_id}")
        print(f"&amp;gt; Session ID: {response.metadata.session_id}")
        print(f"&amp;gt; Additional Parameters: {response.metadata.additional_params}")
        print(f"\n&amp;gt; Response: {response.output.content}")

if __name__ == "__main__":
  asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;These examples showcase:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;The use of a Bedrock LLM Agent with Converse API support, allowing for multi-turn conversations.&lt;/li&gt; 
 &lt;li&gt;Integration of a Lex Bot Agent for specialized tasks (in this case, travel-related queries).&lt;/li&gt; 
 &lt;li&gt;The orchestrator's ability to route requests to the most appropriate agent based on the input.&lt;/li&gt; 
 &lt;li&gt;Handling of both streaming and non-streaming responses from different types of agents.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Modular Installation Options&lt;/h3&gt; 
&lt;p&gt;The Agent Squad is designed with a modular architecture, allowing you to install only the components you need while ensuring you always get the core functionality.&lt;/p&gt; 
&lt;h4&gt;Installation Options&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;1. AWS Integration&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt; pip install "agent-squad[aws]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Includes core orchestration functionality with comprehensive AWS service integrations (&lt;code&gt;BedrockLLMAgent&lt;/code&gt;, &lt;code&gt;AmazonBedrockAgent&lt;/code&gt;, &lt;code&gt;LambdaAgent&lt;/code&gt;, etc.)&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;2. Anthropic Integration&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install "agent-squad[anthropic]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;3. OpenAI Integration&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install "agent-squad[openai]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Adds OpenAI's GPT models for agents and classification, along with core packages.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;4. Full Installation&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install "agent-squad[all]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Includes all optional dependencies for maximum flexibility.&lt;/p&gt; 
&lt;h3&gt;ğŸ™Œ &lt;strong&gt;We Want to Hear From You!&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Have something to share, discuss, or brainstorm? Weâ€™d love to connect with you and hear about your journey with the &lt;strong&gt;Agent Squad framework&lt;/strong&gt;. Hereâ€™s how you can get involved:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ™Œ Show &amp;amp; Tell&lt;/strong&gt;: Got a success story, cool project, or creative implementation? Share it with us in the &lt;a href="https://github.com/awslabs/agent-squad/discussions/categories/show-and-tell"&gt;&lt;strong&gt;Show and Tell&lt;/strong&gt;&lt;/a&gt; section. Your work might inspire the entire community! ğŸ‰&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ’¬ General Discussion&lt;/strong&gt;: Have questions, feedback, or suggestions? Join the conversation in our &lt;a href="https://github.com/awslabs/agent-squad/discussions/categories/general"&gt;&lt;strong&gt;General Discussions&lt;/strong&gt;&lt;/a&gt; section. Itâ€™s the perfect place to connect with other users and contributors.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ’¡ Ideas&lt;/strong&gt;: Thinking of a new feature or improvement? Share your thoughts in the &lt;a href="https://github.com/awslabs/agent-squad/discussions/categories/ideas"&gt;&lt;strong&gt;Ideas&lt;/strong&gt;&lt;/a&gt; section. Weâ€™re always open to exploring innovative ways to make the orchestrator even better!&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Letâ€™s collaborate, learn from each other, and build something incredible together! ğŸš€&lt;/p&gt; 
&lt;h2&gt;ğŸ“ Pull Request Guidelines&lt;/h2&gt; 
&lt;h3&gt;Issue-First Policy&lt;/h3&gt; 
&lt;p&gt;This repository follows an &lt;strong&gt;Issue-First&lt;/strong&gt; policy:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Every pull request must be linked to an existing issue&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;If there isn't an issue for the changes you want to make, please create one first&lt;/li&gt; 
 &lt;li&gt;Use the issue to discuss proposed changes before investing time in implementation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;How to Link Pull Requests to Issues&lt;/h3&gt; 
&lt;p&gt;When creating a pull request, you must link it to an issue using one of these methods:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Include a reference in the PR description using keywords:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;Fixes #123&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;Resolves #123&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;Closes #123&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Manually link the PR to an issue through GitHub's UI:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;On the right sidebar of your PR, click "Development" and then "Link an issue"&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Automated Enforcement&lt;/h3&gt; 
&lt;p&gt;We use GitHub Actions to automatically verify that each PR is linked to an issue. PRs without linked issues will not pass required checks and cannot be merged.&lt;/p&gt; 
&lt;p&gt;This policy helps us:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Maintain clear documentation of changes and their purposes&lt;/li&gt; 
 &lt;li&gt;Ensure community discussion before implementation&lt;/li&gt; 
 &lt;li&gt;Keep a structured development process&lt;/li&gt; 
 &lt;li&gt;Make project history more traceable and understandable&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ¤ Contributing&lt;/h2&gt; 
&lt;p&gt;âš ï¸ Note: Our project has been renamed from &lt;strong&gt;Multi-Agent Orchestrator&lt;/strong&gt; to &lt;strong&gt;Agent Squad&lt;/strong&gt;. Please use the new name in your contributions and discussions.&lt;/p&gt; 
&lt;p&gt;âš ï¸ We value your contributions! Before submitting changes, please start a discussion by opening an issue to share your proposal.&lt;/p&gt; 
&lt;p&gt;Once your proposal is approved, here are the next steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;ğŸ“š Review our &lt;a href="https://raw.githubusercontent.com/awslabs/agent-squad/main/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ’¡ Create a &lt;a href="https://github.com/awslabs/agent-squad/issues"&gt;GitHub Issue&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ”¨ Submit a pull request&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;âœ… Follow existing project structure and include documentation for new features.&lt;/p&gt; 
&lt;p&gt;ğŸŒŸ &lt;strong&gt;Stay Updated&lt;/strong&gt;: Star the repository to be notified about new features, improvements, and exciting developments in the Agent Squad framework!&lt;/p&gt; 
&lt;h1&gt;Authors&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.linkedin.com/in/corneliucroitoru/"&gt;Corneliu Croitoru&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.linkedin.com/in/anthonybernabeu/"&gt;Anthony Bernabeu&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;ğŸ‘¥ Contributors&lt;/h1&gt; 
&lt;p&gt;Big shout out to our awesome contributors! Thank you for making this project better! ğŸŒŸ â­ ğŸš€&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/awslabs/agent-squad/graphs/contributors"&gt;&lt;img src="https://contrib.rocks/image?repo=awslabs/agent-squad&amp;amp;max=2000" alt="contributors" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Please see our &lt;a href="https://raw.githubusercontent.com/awslabs/agent-squad/main/CONTRIBUTING.md"&gt;contributing guide&lt;/a&gt; for guidelines on how to propose bugfixes and improvements.&lt;/p&gt; 
&lt;h2&gt;ğŸ“„ LICENSE&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the Apache 2.0 licence - see the &lt;a href="https://raw.githubusercontent.com/awslabs/agent-squad/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h2&gt;ğŸ“„ Font License&lt;/h2&gt; 
&lt;p&gt;This project uses the JetBrainsMono NF font, licensed under the SIL Open Font License 1.1. For full license details, see &lt;a href="https://github.com/JetBrains/JetBrainsMono/raw/master/OFL.txt"&gt;FONT-LICENSE.md&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>bytebot-ai/bytebot</title>
      <link>https://github.com/bytebot-ai/bytebot</link>
      <description>&lt;p&gt;Bytebot is a self-hosted AI desktop agent that automates computer tasks through natural language commands, operating within a containerized Linux desktop environment.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/bytebot-ai/bytebot/main/docs/images/bytebot-logo.png" width="500" alt="Bytebot Logo" /&gt; 
 &lt;h1&gt;Bytebot: Open-Source AI Desktop Agent&lt;/h1&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/14624" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/14624" alt="bytebot-ai%2Fbytebot | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;An AI that has its own computer to complete tasks for you&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://railway.com/deploy/bytebot?referralCode=L9lKXQ"&gt;&lt;img src="https://railway.com/button.svg?sanitize=true" alt="Deploy on Railway" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/bytebot-ai/bytebot/tree/main/docker"&gt;&lt;img src="https://img.shields.io/badge/docker-ready-blue.svg?sanitize=true" alt="Docker" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/bytebot-ai/bytebot/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-Apache%202.0-green.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://discord.com/invite/d9ewZkWPTP"&gt;&lt;img src="https://img.shields.io/discord/1232768900274585720?color=7289da&amp;amp;label=discord" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://bytebot.ai"&gt;ğŸŒ Website&lt;/a&gt; â€¢ &lt;a href="https://docs.bytebot.ai"&gt;ğŸ“š Documentation&lt;/a&gt; â€¢ &lt;a href="https://discord.com/invite/d9ewZkWPTP"&gt;ğŸ’¬ Discord&lt;/a&gt; â€¢ &lt;a href="https://x.com/bytebot_ai"&gt;ğ• Twitter&lt;/a&gt;&lt;/p&gt; 
 &lt;!-- Keep these links. Translations will automatically update with the README. --&gt; 
 &lt;p&gt;&lt;a href="https://zdoc.app/de/bytebot-ai/bytebot"&gt;Deutsch&lt;/a&gt; | &lt;a href="https://zdoc.app/es/bytebot-ai/bytebot"&gt;EspaÃ±ol&lt;/a&gt; | &lt;a href="https://zdoc.app/fr/bytebot-ai/bytebot"&gt;franÃ§ais&lt;/a&gt; | &lt;a href="https://zdoc.app/ja/bytebot-ai/bytebot"&gt;æ—¥æœ¬èª&lt;/a&gt; | &lt;a href="https://zdoc.app/ko/bytebot-ai/bytebot"&gt;í•œêµ­ì–´&lt;/a&gt; | &lt;a href="https://zdoc.app/pt/bytebot-ai/bytebot"&gt;PortuguÃªs&lt;/a&gt; | &lt;a href="https://zdoc.app/ru/bytebot-ai/bytebot"&gt;Ğ ÑƒÑÑĞºĞ¸Ğ¹&lt;/a&gt; | &lt;a href="https://zdoc.app/zh/bytebot-ai/bytebot"&gt;ä¸­æ–‡&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/f271282a-27a3-43f3-9b99-b34007fdd169"&gt;https://github.com/user-attachments/assets/f271282a-27a3-43f3-9b99-b34007fdd169&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/72a43cf2-bd87-44c5-a582-e7cbe176f37f"&gt;https://github.com/user-attachments/assets/72a43cf2-bd87-44c5-a582-e7cbe176f37f&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;What is a Desktop Agent?&lt;/h2&gt; 
&lt;p&gt;A desktop agent is an AI that has its own computer. Unlike browser-only agents or traditional RPA tools, Bytebot comes with a full virtual desktop where it can:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Use any application (browsers, email clients, office tools, IDEs)&lt;/li&gt; 
 &lt;li&gt;Download and organize files with its own file system&lt;/li&gt; 
 &lt;li&gt;Log into websites and applications using password managers&lt;/li&gt; 
 &lt;li&gt;Read and process documents, PDFs, and spreadsheets&lt;/li&gt; 
 &lt;li&gt;Complete complex multi-step workflows across different programs&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Think of it as a virtual employee with their own computer who can see the screen, move the mouse, type on the keyboard, and complete tasks just like a human would.&lt;/p&gt; 
&lt;h2&gt;Why Give AI Its Own Computer?&lt;/h2&gt; 
&lt;p&gt;When AI has access to a complete desktop environment, it unlocks capabilities that aren't possible with browser-only agents or API integrations:&lt;/p&gt; 
&lt;h3&gt;Complete Task Autonomy&lt;/h3&gt; 
&lt;p&gt;Give Bytebot a task like "Download all invoices from our vendor portals and organize them into a folder" and it will:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Open the browser&lt;/li&gt; 
 &lt;li&gt;Navigate to each portal&lt;/li&gt; 
 &lt;li&gt;Handle authentication (including 2FA via password managers)&lt;/li&gt; 
 &lt;li&gt;Download the files to its local file system&lt;/li&gt; 
 &lt;li&gt;Organize them into a folder&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Process Documents&lt;/h3&gt; 
&lt;p&gt;Upload files directly to Bytebot's desktop and it can:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Read entire PDFs into its context&lt;/li&gt; 
 &lt;li&gt;Extract data from complex documents&lt;/li&gt; 
 &lt;li&gt;Cross-reference information across multiple files&lt;/li&gt; 
 &lt;li&gt;Create new documents based on analysis&lt;/li&gt; 
 &lt;li&gt;Handle formats that APIs can't access&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Use Real Applications&lt;/h3&gt; 
&lt;p&gt;Bytebot isn't limited to web interfaces. It can:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Use desktop applications like text editors, VS Code, or email clients&lt;/li&gt; 
 &lt;li&gt;Run scripts and command-line tools&lt;/li&gt; 
 &lt;li&gt;Install new software as needed&lt;/li&gt; 
 &lt;li&gt;Configure applications for specific workflows&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;h3&gt;Deploy in 2 Minutes&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Option 1: Railway (Easiest)&lt;/strong&gt; &lt;a href="https://railway.com/deploy/bytebot?referralCode=L9lKXQ"&gt;&lt;img src="https://railway.com/button.svg?sanitize=true" alt="Deploy on Railway" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Just click and add your AI provider API key.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Option 2: Docker Compose&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/bytebot-ai/bytebot.git
cd bytebot

# Add your AI provider key (choose one)
echo "ANTHROPIC_API_KEY=sk-ant-..." &amp;gt; docker/.env
# Or: echo "OPENAI_API_KEY=sk-..." &amp;gt; docker/.env
# Or: echo "GEMINI_API_KEY=..." &amp;gt; docker/.env

docker-compose -f docker/docker-compose.yml up -d

# Open http://localhost:9992
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://docs.bytebot.ai/quickstart"&gt;Full deployment guide â†’&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;How It Works&lt;/h2&gt; 
&lt;p&gt;Bytebot consists of four integrated components:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Virtual Desktop&lt;/strong&gt;: A complete Ubuntu Linux environment with pre-installed applications&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;AI Agent&lt;/strong&gt;: Understands your tasks and controls the desktop to complete them&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Task Interface&lt;/strong&gt;: Web UI where you create tasks and watch Bytebot work&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;APIs&lt;/strong&gt;: REST endpoints for programmatic task creation and desktop control&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Key Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Natural Language Tasks&lt;/strong&gt;: Just describe what you need done&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;File Uploads&lt;/strong&gt;: Drop files onto tasks for Bytebot to process&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Live Desktop View&lt;/strong&gt;: Watch Bytebot work in real-time&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Takeover Mode&lt;/strong&gt;: Take control when you need to help or configure something&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Password Manager Support&lt;/strong&gt;: Install 1Password, Bitwarden, etc. for automatic authentication&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Persistent Environment&lt;/strong&gt;: Install programs and they stay available for future tasks&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Example Tasks&lt;/h2&gt; 
&lt;h3&gt;Basic Examples&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;"Go to Wikipedia and create a summary of quantum computing"
"Research flights from NYC to London and create a comparison document"
"Take screenshots of the top 5 news websites"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Document Processing&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;"Read the uploaded contracts.pdf and extract all payment terms and deadlines"
"Process these 5 invoice PDFs and create a summary report"
"Download and analyze the latest financial report and answer: What were the key risks mentioned?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Multi-Application Workflows&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;"Download last month's bank statements from our three banks and consolidate them"
"Check all our vendor portals for new invoices and create a summary report"
"Log into our CRM, export the customer list, and update records in the ERP system"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Programmatic Control&lt;/h2&gt; 
&lt;h3&gt;Create Tasks via API&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import requests

# Simple task
response = requests.post('http://localhost:9991/tasks', json={
    'description': 'Download the latest sales report and create a summary'
})

# Task with file upload
files = {'files': open('contracts.pdf', 'rb')}
response = requests.post('http://localhost:9991/tasks',
    data={'description': 'Review these contracts for important dates'},
    files=files
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Direct Desktop Control&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Take a screenshot
curl -X POST http://localhost:9990/computer-use \
  -H "Content-Type: application/json" \
  -d '{"action": "screenshot"}'

# Click at specific coordinates
curl -X POST http://localhost:9990/computer-use \
  -H "Content-Type: application/json" \
  -d '{"action": "click_mouse", "coordinate": [500, 300]}'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://docs.bytebot.ai/api-reference/introduction"&gt;Full API documentation â†’&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Setting Up Your Desktop Agent&lt;/h2&gt; 
&lt;h3&gt;1. Deploy Bytebot&lt;/h3&gt; 
&lt;p&gt;Use one of the deployment methods above to get Bytebot running.&lt;/p&gt; 
&lt;h3&gt;2. Configure the Desktop&lt;/h3&gt; 
&lt;p&gt;Use the Desktop tab in the UI to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Install additional programs you need&lt;/li&gt; 
 &lt;li&gt;Set up password managers for authentication&lt;/li&gt; 
 &lt;li&gt;Configure applications with your preferences&lt;/li&gt; 
 &lt;li&gt;Log into websites you want Bytebot to access&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;3. Start Giving Tasks&lt;/h3&gt; 
&lt;p&gt;Create tasks in natural language and watch Bytebot complete them using the configured desktop.&lt;/p&gt; 
&lt;h2&gt;Use Cases&lt;/h2&gt; 
&lt;h3&gt;Business Process Automation&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Invoice processing and data extraction&lt;/li&gt; 
 &lt;li&gt;Multi-system data synchronization&lt;/li&gt; 
 &lt;li&gt;Report generation from multiple sources&lt;/li&gt; 
 &lt;li&gt;Compliance checking across platforms&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Development &amp;amp; Testing&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Automated UI testing&lt;/li&gt; 
 &lt;li&gt;Cross-browser compatibility checks&lt;/li&gt; 
 &lt;li&gt;Documentation generation with screenshots&lt;/li&gt; 
 &lt;li&gt;Code deployment verification&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Research &amp;amp; Analysis&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Competitive analysis across websites&lt;/li&gt; 
 &lt;li&gt;Data gathering from multiple sources&lt;/li&gt; 
 &lt;li&gt;Document analysis and summarization&lt;/li&gt; 
 &lt;li&gt;Market research compilation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Architecture&lt;/h2&gt; 
&lt;p&gt;Bytebot is built with:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Desktop&lt;/strong&gt;: Ubuntu 22.04 with XFCE, Firefox, VS Code, and other tools&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Agent&lt;/strong&gt;: NestJS service that coordinates AI and desktop actions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;UI&lt;/strong&gt;: Next.js application for task management&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;AI Support&lt;/strong&gt;: Works with Anthropic Claude, OpenAI GPT, Google Gemini&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Deployment&lt;/strong&gt;: Docker containers for easy self-hosting&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Why Self-Host?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Data Privacy&lt;/strong&gt;: Everything runs on your infrastructure&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Full Control&lt;/strong&gt;: Customize the desktop environment as needed&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;No Limits&lt;/strong&gt;: Use your own AI API keys without platform restrictions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexibility&lt;/strong&gt;: Install any software, access any systems&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Advanced Features&lt;/h2&gt; 
&lt;h3&gt;Multiple AI Providers&lt;/h3&gt; 
&lt;p&gt;Use any AI provider through our &lt;a href="https://docs.bytebot.ai/deployment/litellm"&gt;LiteLLM integration&lt;/a&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Azure OpenAI&lt;/li&gt; 
 &lt;li&gt;AWS Bedrock&lt;/li&gt; 
 &lt;li&gt;Local models via Ollama&lt;/li&gt; 
 &lt;li&gt;100+ other providers&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Enterprise Deployment&lt;/h3&gt; 
&lt;p&gt;Deploy on Kubernetes with Helm:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone the repository
git clone https://github.com/bytebot-ai/bytebot.git
cd bytebot

# Install with Helm
helm install bytebot ./helm \
  --set agent.env.ANTHROPIC_API_KEY=sk-ant-...
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://docs.bytebot.ai/deployment/helm"&gt;Enterprise deployment guide â†’&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Community &amp;amp; Support&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Discord&lt;/strong&gt;: &lt;a href="https://discord.com/invite/d9ewZkWPTP"&gt;Join our community&lt;/a&gt; for help and discussions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Documentation&lt;/strong&gt;: Comprehensive guides at &lt;a href="https://docs.bytebot.ai"&gt;docs.bytebot.ai&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;GitHub Issues&lt;/strong&gt;: Report bugs and request features&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions! Whether it's:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ› Bug fixes&lt;/li&gt; 
 &lt;li&gt;âœ¨ New features&lt;/li&gt; 
 &lt;li&gt;ğŸ“š Documentation improvements&lt;/li&gt; 
 &lt;li&gt;ğŸŒ Translations&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Please:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Check existing &lt;a href="https://github.com/bytebot-ai/bytebot/issues"&gt;issues&lt;/a&gt; first&lt;/li&gt; 
 &lt;li&gt;Open an issue to discuss major changes&lt;/li&gt; 
 &lt;li&gt;Submit PRs with clear descriptions&lt;/li&gt; 
 &lt;li&gt;Join our &lt;a href="https://discord.com/invite/d9ewZkWPTP"&gt;Discord&lt;/a&gt; to discuss ideas&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Bytebot is open source under the Apache 2.0 license.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;strong&gt;Give your AI its own computer. See what it can do.&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://railway.com/deploy/bytebot?referralCode=L9lKXQ"&gt;&lt;img src="https://railway.com/button.svg?sanitize=true" alt="Deploy on Railway" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;sub&gt;Built by &lt;a href="https://tantl.com"&gt;Tantl Labs&lt;/a&gt; and the open source community&lt;/sub&gt;&lt;/p&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>zama-ai/fhevm</title>
      <link>https://github.com/zama-ai/fhevm</link>
      <description>&lt;p&gt;FHEVM, a full-stack framework for integrating Fully Homomorphic Encryption (FHE) with blockchain applications&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="docs/.gitbook/assets/fhevm-header-dark.png" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="docs/.gitbook/assets/fhevm-header-light.png" /&gt; 
  &lt;img width="500" alt="fhevm" /&gt; 
 &lt;/picture&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;p align="center"&gt; &lt;a href="https://raw.githubusercontent.com/zama-ai/fhevm/main/fhevm-whitepaper.pdf"&gt; ğŸ“ƒ Read white paper&lt;/a&gt; |&lt;a href="https://docs.zama.ai/protocol"&gt; ğŸ“’ Documentation&lt;/a&gt; | &lt;a href="https://zama.ai/community"&gt; ğŸ’› Community support&lt;/a&gt; | &lt;a href="https://github.com/zama-ai/awesome-zama"&gt; ğŸ“š FHE resources by Zama&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/zama-ai/fhevm/releases"&gt; &lt;img src="https://img.shields.io/github/v/release/zama-ai/fhevm?style=flat-square" /&gt;&lt;/a&gt; &lt;a href="https://github.com/zama-ai/fhevm/raw/main/LICENSE"&gt; 
  &lt;!-- markdown-link-check-disable-next-line --&gt; &lt;img src="https://img.shields.io/badge/License-BSD--3--Clause--Clear-%23ffb243?style=flat-square" /&gt;&lt;/a&gt; &lt;a href="https://github.com/zama-ai/bounty-program"&gt; 
  &lt;!-- markdown-link-check-disable-next-line --&gt; &lt;img src="https://img.shields.io/badge/Contribute-Zama%20Bounty%20Program-%23ffd208?style=flat-square" /&gt;&lt;/a&gt; &lt;a href="https://slsa.dev"&gt;&lt;img alt="SLSA 3" src="https://slsa.dev/images/gh-badge-level3.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;About&lt;/h2&gt; 
&lt;h3&gt;What is FHEVM?&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;FHEVM&lt;/strong&gt; is the core framework of the &lt;em&gt;Zama Confidential Blockchain Protocol&lt;/em&gt;. It enables confidential smart contracts on EVM-compatible blockchains by leveraging Fully Homomorphic Encryption (FHE), allowing encrypted data to be processed directly onchain.&lt;/p&gt; 
&lt;p&gt;FHEVM ensures both confidentiality and composability, with the following guarantees:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;End-to-end encryption of transactions and state:&lt;/strong&gt; Data included in transactions is encrypted and never visible to anyone.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Composability and data availability on-chain:&lt;/strong&gt; States are updated while remaining encrypted at all times.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;No impact on existing dApps and state:&lt;/strong&gt; Encrypted state co-exists alongside public one, and doesn't impact existing dApps. &lt;br /&gt;&lt;br /&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Table of contents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zama-ai/fhevm/main/#about"&gt;About&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zama-ai/fhevm/main/#what-is-fhevm"&gt;What is FHEVM?&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zama-ai/fhevm/main/#project-structure"&gt;Project structure&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zama-ai/fhevm/main/#main-features"&gt;Main features&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zama-ai/fhevm/main/#use-cases"&gt;Use cases&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zama-ai/fhevm/main/#resources"&gt;Resources&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zama-ai/fhevm/main/#working-with-fhevm"&gt;Working with FHEVM&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zama-ai/fhevm/main/#citations"&gt;Citations&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zama-ai/fhevm/main/#contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zama-ai/fhevm/main/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zama-ai/fhevm/main/#faq"&gt;FAQ&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zama-ai/fhevm/main/#support"&gt;Support&lt;/a&gt; &lt;br /&gt;&lt;br /&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Project structure&lt;/h3&gt; 
&lt;p&gt;The directories of this repository are organized in the following way:&lt;/p&gt; 
&lt;h6&gt;FHEVM Contracts&lt;/h6&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;gateway-contracts/&lt;/code&gt;&lt;/strong&gt;: Smart contracts managing the gateway between on-chain and off-chain components.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;host-contracts/&lt;/code&gt;&lt;/strong&gt;: Smart Contracts deployed on the host chain for orchestrating FHE workflows.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h6&gt;FHEVM Compute Engines&lt;/h6&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;coprocessor/&lt;/code&gt;&lt;/strong&gt;: Rust-based coprocessor implementation for FHE operations.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;kms-connector/&lt;/code&gt;&lt;/strong&gt;: Interface for integrating with Key Management Services (KMS) to handle encryption keys securely.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h6&gt;FHEVM Utilities&lt;/h6&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;charts/&lt;/code&gt;&lt;/strong&gt;: Helm charts and deployment configurations for the stack.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;golden-container-images/&lt;/code&gt;&lt;/strong&gt;: Docker golden images for Node.js and Rust environments used as base images by the stack.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;test-suite/&lt;/code&gt;&lt;/strong&gt;: Integration with docker-compose and tests covering end-to-end FHEVM stack behavior.&lt;/p&gt; &lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Main features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Privacy by design:&lt;/strong&gt; Building decentralized apps with full privacy and confidentiality on Ethereum, leveraging FHE.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Solidity integration:&lt;/strong&gt; Write FHEVM contracts like any standard Solidity contract using Solidity. Compatible with existing toolchains â€” such as Hardhat and Foundry (&lt;em&gt;coming soon&lt;/em&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Programmable privacy:&lt;/strong&gt; Define exactly what data is encrypted and write the access control logic directly in your smart contracts.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;High precision encrypted integers :&lt;/strong&gt; Up to 256 bits of precision for integers.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Full range of operators:&lt;/strong&gt; All typical operators are available: &lt;code&gt;+&lt;/code&gt;, &lt;code&gt;-&lt;/code&gt;, &lt;code&gt;*&lt;/code&gt;, &lt;code&gt;/&lt;/code&gt;, &lt;code&gt;&amp;lt;&lt;/code&gt;, &lt;code&gt;&amp;gt;&lt;/code&gt;, &lt;code&gt;==&lt;/code&gt;, ternary-if, boolean operationsâ€¦. Consecutive FHE operations are not limited.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Security:&lt;/strong&gt; The underlying FHE crypto-scheme of FHEVM is quantum-resistant. Decryption is managed via a key management system (KMS) using multi-party computation (MPC), ensuring security even if some parties are compromised or misbehaving.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Symbolic execution of FHE computations:&lt;/strong&gt; All FHE operations are executed symbolically on the host chain, significantly reducing execution time. The actual computations on encrypted data are offloaded asynchronously to our coprocessor, allowing for faster, efficient, and scalable processing.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;em&gt;Learn more about FHEVM features in the &lt;a href="https://docs.zama.ai/protocol"&gt;documentation&lt;/a&gt; and in our &lt;a href="https://github.com/zama-ai/fhevm/raw/main/fhevm-whitepaper.pdf"&gt;whitepaper&lt;/a&gt;.&lt;/em&gt; &lt;br /&gt;&lt;br /&gt;&lt;/p&gt; 
&lt;h3&gt;Use cases&lt;/h3&gt; 
&lt;p&gt;FHEVM is built for developers to write confidential smart contracts without the need to learn cryptography. Leveraging FHEVM, you can unlock a myriad of new use cases such as DeFi, gaming, and more. For instance:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Confidential transfers&lt;/strong&gt;: Keep balances and amounts private, without using mixers.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Tokenization&lt;/strong&gt;: Swap tokens and RWAs on-chain without others seeing the amounts.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Blind auctions&lt;/strong&gt;: Bid on items without revealing the amount or the winner.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;On-chain games&lt;/strong&gt;: Keep moves, selections, cards, or items hidden until ready to reveal.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Confidential voting&lt;/strong&gt;: Prevents bribery and blackmailing by keeping votes private.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Encrypted DIDs&lt;/strong&gt;: Store identities on-chain and generate attestations without ZK.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;em&gt;Learn more use cases in the &lt;a href="https://docs.zama.ai/protocol/examples"&gt;list of examples&lt;/a&gt;.&lt;/em&gt; &lt;br /&gt;&lt;br /&gt;&lt;/p&gt; 
&lt;h2&gt;Resources&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.zama.ai/protocol"&gt;Documentation&lt;/a&gt; â€” Official documentation of FHEVM.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zama-ai/fhevm/main/fhevm-whitepaper.pdf"&gt;Whitepaper&lt;/a&gt; â€” Technical overview of FHEVM's cryptographic design.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.zama.ai/protocol/examples"&gt;Examples&lt;/a&gt; â€” Examples of building confidential smart contracts.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/zama-ai/awesome-zama?tab=readme-ov-file#fhevm"&gt;Awesome Zama â€“ FHEVM&lt;/a&gt; â€” Curated articles, talks, and ecosystem projects.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="right"&gt; &lt;a href="https://raw.githubusercontent.com/zama-ai/fhevm/main/#about"&gt; â†‘ Back to top &lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;Working with FHEVM&lt;/h2&gt; 
&lt;h3&gt;Citations&lt;/h3&gt; 
&lt;p&gt;To cite FHEVM or the whitepaper in academic papers, please use the following entries:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;@Misc{FHEVM,
title={{FHEVM: A full-stack framework for integrating Fully Homomorphic Encryption (FHE) with blockchain applications},
author={Zama},
year={2025},
note={\url{https://github.com/zama-ai/fhevm}},
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Contributing&lt;/h3&gt; 
&lt;p&gt;There are two ways to contribute to FHEVM:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/zama-ai/fhevm/issues/new/choose"&gt;Open issues&lt;/a&gt; to report bugs and typos, or to suggest new ideas&lt;/li&gt; 
 &lt;li&gt;Request to become an official contributor by emailing &lt;a href="mailto:hello@zama.ai"&gt;hello@zama.ai&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Becoming an approved contributor involves signing our Contributor License Agreement (CLA). Only approved contributors can send pull requests, so please make sure to get in touch before you do! &lt;br /&gt;&lt;br /&gt;&lt;/p&gt; 
&lt;h3&gt;License&lt;/h3&gt; 
&lt;p&gt;This software is distributed under the &lt;strong&gt;BSD-3-Clause-Clear&lt;/strong&gt; license. Read &lt;a href="https://raw.githubusercontent.com/zama-ai/fhevm/main/LICENSE"&gt;this&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h3&gt;FAQ&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Is Zamaâ€™s technology free to use?&lt;/strong&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Zamaâ€™s libraries are free to use under the BSD 3-Clause Clear license only for development, research, prototyping, and experimentation purposes. However, for any commercial use of Zama's open source code, companies must purchase Zamaâ€™s commercial patent license.&lt;/p&gt; 
 &lt;p&gt;Everything we do is open source, and we are very transparent on what it means for our users, you can read more about how we monetize our open source products at Zama in &lt;a href="https://www.zama.ai/post/open-source"&gt;this blog post&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;What do I need to do if I want to use Zamaâ€™s technology for commercial purposes?&lt;/strong&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;To commercially use Zamaâ€™s technology you need to be granted Zamaâ€™s patent license. Please contact us at &lt;a href="mailto:hello@zama.ai"&gt;hello@zama.ai&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Do you file IP on your technology?&lt;/strong&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Yes, all Zamaâ€™s technologies are patented.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Can you customize a solution for my specific use case?&lt;/strong&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;We are open to collaborating and advancing the FHE space with our partners. If you have specific needs, please email us at &lt;a href="mailto:hello@zama.ai"&gt;hello@zama.ai&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Support&lt;/h2&gt; 
&lt;a target="_blank" href="https://community.zama.ai"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="docs/.gitbook/assets/support-banner-dark.png" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="docs/.gitbook/assets/support-banner-light.png" /&gt; 
  &lt;img alt="Support" /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;p&gt;ğŸŒŸ If you find this project helpful or interesting, please consider giving it a star on GitHub! Your support helps to grow the community and motivates further development.&lt;/p&gt; 
&lt;p align="right"&gt; &lt;a href="https://raw.githubusercontent.com/zama-ai/fhevm/main/#about"&gt; â†‘ Back to top &lt;/a&gt; &lt;/p&gt;</description>
    </item>
    
    <item>
      <title>trufflesecurity/trufflehog</title>
      <link>https://github.com/trufflesecurity/trufflehog</link>
      <description>&lt;p&gt;Find, verify, and analyze leaked credentials&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img alt="GoReleaser Logo" src="https://storage.googleapis.com/trufflehog-static-sources/pixel_pig.png" height="140" /&gt; &lt;/p&gt;
&lt;h2 align="center"&gt;TruffleHog&lt;/h2&gt; 
&lt;p align="center"&gt;Find leaked credentials.&lt;/p&gt; 
&lt;p&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://goreportcard.com/report/github.com/trufflesecurity/trufflehog/v3"&gt;&lt;img src="https://goreportcard.com/badge/github.com/trufflesecurity/trufflehog/v3" alt="Go Report Card" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/trufflesecurity/trufflehog/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-AGPL--3.0-brightgreen" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/trufflesecurity/trufflehog/main/pkg/detectors"&gt;&lt;img src="https://img.shields.io/github/directory-file-count/trufflesecurity/truffleHog/pkg/detectors?label=Total%20Detectors&amp;amp;type=dir" alt="Total Detectors" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h1&gt;&lt;span&gt;ğŸ”&lt;/span&gt; &lt;em&gt;Now Scanning&lt;/em&gt;&lt;/h1&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/trufflesecurity/trufflehog/main/assets/scanning_logos.svg?sanitize=true" /&gt; 
 &lt;p&gt;&lt;strong&gt;...and more&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;To learn more about TruffleHog and its features and capabilities, visit our &lt;a href="https://trufflesecurity.com/trufflehog?gclid=CjwKCAjwouexBhAuEiwAtW_Zx5IW87JNj97Ci7heFnA5ar6-DuNzT2Y5nIl9DuZ-FOUqx0Qg3vb9nxoClcEQAvD_BwE"&gt;product page&lt;/a&gt;.&lt;/p&gt; 
&lt;/div&gt; 
&lt;h1&gt;&lt;span&gt;ğŸŒ&lt;/span&gt; TruffleHog Enterprise&lt;/h1&gt; 
&lt;p&gt;Are you interested in continuously monitoring &lt;strong&gt;Git, Jira, Slack, Confluence, Microsoft Teams, Sharepoint, and more..&lt;/strong&gt; for credentials? We have an enterprise product that can help! Learn more at &lt;a href="https://trufflesecurity.com/trufflehog-enterprise"&gt;https://trufflesecurity.com/trufflehog-enterprise&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We take the revenue from the enterprise product to fund more awesome open source projects that the whole community can benefit from.&lt;/p&gt;  
&lt;h1&gt;What is TruffleHog ğŸ½&lt;/h1&gt; 
&lt;p&gt;TruffleHog is the most powerful secrets &lt;strong&gt;Discovery, Classification, Validation,&lt;/strong&gt; and &lt;strong&gt;Analysis&lt;/strong&gt; tool. In this context, secret refers to a credential a machine uses to authenticate itself to another machine. This includes API keys, database passwords, private encryption keys, and more...&lt;/p&gt; 
&lt;h2&gt;Discovery ğŸ”&lt;/h2&gt; 
&lt;p&gt;TruffleHog can look for secrets in many places including Git, chats, wikis, logs, API testing platforms, object stores, filesystems and more&lt;/p&gt; 
&lt;h2&gt;Classification ğŸ“&lt;/h2&gt; 
&lt;p&gt;TruffleHog classifies over 800 secret types, mapping them back to the specific identity they belong to. Is it an AWS secret? Stripe secret? Cloudflare secret? Postgres password? SSL Private key? Sometimes it's hard to tell looking at it, so TruffleHog classifies everything it finds.&lt;/p&gt; 
&lt;h2&gt;Validation âœ…&lt;/h2&gt; 
&lt;p&gt;For every secret TruffleHog can classify, it can also log in to confirm if that secret is live or not. This step is critical to know if thereâ€™s an active present danger or not.&lt;/p&gt; 
&lt;h2&gt;Analysis ğŸ”¬&lt;/h2&gt; 
&lt;p&gt;For the 20 some of the most commonly leaked out credential types, instead of sending one request to check if the secret can log in, TruffleHog can send many requests to learn everything there is to know about the secret. Who created it? What resources can it access? What permissions does it have on those resources?&lt;/p&gt; 
&lt;h1&gt;&lt;span&gt;ğŸ“¢&lt;/span&gt; Join Our Community&lt;/h1&gt; 
&lt;p&gt;Have questions? Feedback? Jump into Slack or Discord and hang out with us.&lt;/p&gt; 
&lt;p&gt;Join our &lt;a href="https://join.slack.com/t/trufflehog-community/shared_invite/zt-pw2qbi43-Aa86hkiimstfdKH9UCpPzQ"&gt;Slack Community&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Join the &lt;a href="https://discord.gg/8Hzbrnkr7E"&gt;Secret Scanning Discord&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;&lt;span&gt;ğŸ“º&lt;/span&gt; Demo&lt;/h1&gt; 
&lt;p&gt;&lt;img src="https://storage.googleapis.com/truffle-demos/non-interactive.svg?sanitize=true" alt="GitHub scanning demo" /&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run --rm -it -v "$PWD:/pwd" trufflesecurity/trufflehog:latest github --org=trufflesecurity
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;&lt;span&gt;ğŸ’¾&lt;/span&gt; Installation&lt;/h1&gt; 
&lt;p&gt;Several options are available for you:&lt;/p&gt; 
&lt;h3&gt;MacOS users&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;brew install trufflehog
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Docker:&lt;/h3&gt; 
&lt;p&gt;&lt;sub&gt;&lt;i&gt;&lt;em&gt;Ensure Docker engine is running before executing the following commands:&lt;/em&gt;&lt;/i&gt;&lt;/sub&gt;&lt;/p&gt; 
&lt;h4&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Unix&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run --rm -it -v "$PWD:/pwd" trufflesecurity/trufflehog:latest github --repo https://github.com/trufflesecurity/test_keys
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Windows Command Prompt&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run --rm -it -v "%cd:/=\%:/pwd" trufflesecurity/trufflehog:latest github --repo https://github.com/trufflesecurity/test_keys
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Windows PowerShell&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run --rm -it -v "${PWD}:/pwd" trufflesecurity/trufflehog github --repo https://github.com/trufflesecurity/test_keys
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;M1 and M2 Mac&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run --platform linux/arm64 --rm -it -v "$PWD:/pwd" trufflesecurity/trufflehog:latest github --repo https://github.com/trufflesecurity/test_keys
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Binary releases&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;Download and unpack from https://github.com/trufflesecurity/trufflehog/releases
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Compile from source&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/trufflesecurity/trufflehog.git
cd trufflehog; go install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Using installation script&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -sSfL https://raw.githubusercontent.com/trufflesecurity/trufflehog/main/scripts/install.sh | sh -s -- -b /usr/local/bin
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Using installation script, verify checksum signature (requires cosign to be installed)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -sSfL https://raw.githubusercontent.com/trufflesecurity/trufflehog/main/scripts/install.sh | sh -s -- -v -b /usr/local/bin
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Using installation script to install a specific version&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -sSfL https://raw.githubusercontent.com/trufflesecurity/trufflehog/main/scripts/install.sh | sh -s -- -b /usr/local/bin &amp;lt;ReleaseTag like v3.56.0&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;&lt;span&gt;ğŸ”&lt;/span&gt; Verifying the artifacts&lt;/h1&gt; 
&lt;p&gt;Checksums are applied to all artifacts, and the resulting checksum file is signed using cosign.&lt;/p&gt; 
&lt;p&gt;You need the following tool to verify signature:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.sigstore.dev/cosign/system_config/installation/"&gt;Cosign&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Verification steps are as follows:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Download the artifact files you want, and the following files from the &lt;a href="https://github.com/trufflesecurity/trufflehog/releases"&gt;releases&lt;/a&gt; page.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;trufflehog_{version}_checksums.txt&lt;/li&gt; 
   &lt;li&gt;trufflehog_{version}_checksums.txt.pem&lt;/li&gt; 
   &lt;li&gt;trufflehog_{version}_checksums.txt.sig&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Verify the signature:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;cosign verify-blob &amp;lt;path to trufflehog_{version}_checksums.txt&amp;gt; \
--certificate &amp;lt;path to trufflehog_{version}_checksums.txt.pem&amp;gt; \
--signature &amp;lt;path to trufflehog_{version}_checksums.txt.sig&amp;gt; \
--certificate-identity-regexp 'https://github\.com/trufflesecurity/trufflehog/\.github/workflows/.+' \
--certificate-oidc-issuer "https://token.actions.githubusercontent.com"
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Once the signature is confirmed as valid, you can proceed to validate that the SHA256 sums align with the downloaded artifact:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;sha256sum --ignore-missing -c trufflehog_{version}_checksums.txt
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Replace &lt;code&gt;{version}&lt;/code&gt; with the downloaded files version&lt;/p&gt; 
&lt;p&gt;Alternatively, if you are using the installation script, pass &lt;code&gt;-v&lt;/code&gt; option to perform signature verification. This requires Cosign binary to be installed prior to running the installation script.&lt;/p&gt; 
&lt;h1&gt;&lt;span&gt;ğŸš€&lt;/span&gt; Quick Start&lt;/h1&gt; 
&lt;h2&gt;1: Scan a repo for only verified secrets&lt;/h2&gt; 
&lt;p&gt;Command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;trufflehog git https://github.com/trufflesecurity/test_keys --results=verified,unknown
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Expected output:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;ğŸ·ğŸ”‘ğŸ·  TruffleHog. Unearth your secrets. ğŸ·ğŸ”‘ğŸ·

Found verified result ğŸ·ğŸ”‘
Detector Type: AWS
Decoder Type: PLAIN
Raw result: AKIAYVP4CIPPERUVIFXG
Line: 4
Commit: fbc14303ffbf8fb1c2c1914e8dda7d0121633aca
File: keys
Email: counter &amp;lt;counter@counters-MacBook-Air.local&amp;gt;
Repository: https://github.com/trufflesecurity/test_keys
Timestamp: 2022-06-16 10:17:40 -0700 PDT
...
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;2: Scan a GitHub Org for only verified secrets&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;trufflehog github --org=trufflesecurity --results=verified,unknown
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;3: Scan a GitHub Repo for only verified keys and get JSON output&lt;/h2&gt; 
&lt;p&gt;Command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;trufflehog git https://github.com/trufflesecurity/test_keys --results=verified,unknown --json
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Expected output:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;{"SourceMetadata":{"Data":{"Git":{"commit":"fbc14303ffbf8fb1c2c1914e8dda7d0121633aca","file":"keys","email":"counter \u003ccounter@counters-MacBook-Air.local\u003e","repository":"https://github.com/trufflesecurity/test_keys","timestamp":"2022-06-16 10:17:40 -0700 PDT","line":4}}},"SourceID":0,"SourceType":16,"SourceName":"trufflehog - git","DetectorType":2,"DetectorName":"AWS","DecoderName":"PLAIN","Verified":true,"Raw":"AKIAYVP4CIPPERUVIFXG","Redacted":"AKIAYVP4CIPPERUVIFXG","ExtraData":{"account":"595918472158","arn":"arn:aws:iam::595918472158:user/canarytokens.com@@mirux23ppyky6hx3l6vclmhnj","user_id":"AIDAYVP4CIPPJ5M54LRCY"},"StructuredData":null}
...
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;4: Scan a GitHub Repo + its Issues and Pull Requests&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;trufflehog github --repo=https://github.com/trufflesecurity/test_keys --issue-comments --pr-comments
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;5: Scan an S3 bucket for verified keys&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;trufflehog s3 --bucket=&amp;lt;bucket name&amp;gt; --results=verified,unknown
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;6: Scan S3 buckets using IAM Roles&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;trufflehog s3 --role-arn=&amp;lt;iam role arn&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;7: Scan a Github Repo using SSH authentication in Docker&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run --rm -v "$HOME/.ssh:/root/.ssh:ro" trufflesecurity/trufflehog:latest git ssh://github.com/trufflesecurity/test_keys
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;8: Scan individual files or directories&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;trufflehog filesystem path/to/file1.txt path/to/file2.txt path/to/dir
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;9: Scan a local git repo&lt;/h2&gt; 
&lt;p&gt;Clone the git repo. For example &lt;a href="https://raw.githubusercontent.com/trufflesecurity/trufflehog/main/git@github.com:trufflesecurity/test_keys.git"&gt;test keys&lt;/a&gt; repo.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;$ git clone git@github.com:trufflesecurity/test_keys.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run trufflehog from the parent directory (outside the git repo).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;$ trufflehog git file://test_keys --results=verified,unknown
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;10: Scan GCS buckets for verified secrets&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;trufflehog gcs --project-id=&amp;lt;project-ID&amp;gt; --cloud-environment --results=verified,unknown
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;11: Scan a Docker image for verified secrets&lt;/h2&gt; 
&lt;p&gt;Use the &lt;code&gt;--image&lt;/code&gt; flag multiple times to scan multiple images.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# to scan from a remote registry
trufflehog docker --image trufflesecurity/secrets --results=verified,unknown

# to scan from the local docker daemon
trufflehog docker --image docker://new_image:tag --results=verified,unknown

# to scan from an image saved as a tarball
trufflehog docker --image file://path_to_image.tar --results=verified,unknown
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;12: Scan in CI&lt;/h2&gt; 
&lt;p&gt;Set the &lt;code&gt;--since-commit&lt;/code&gt; flag to your default branch that people merge into (ex: "main"). Set the &lt;code&gt;--branch&lt;/code&gt; flag to your PR's branch name (ex: "feature-1"). Depending on the CI/CD platform you use, this value can be pulled in dynamically (ex: &lt;a href="https://circleci.com/docs/variables/"&gt;CIRCLE_BRANCH in Circle CI&lt;/a&gt; and &lt;a href="https://docs.travis-ci.com/user/environment-variables/"&gt;TRAVIS_PULL_REQUEST_BRANCH in Travis CI&lt;/a&gt;). If the repo is cloned and the target branch is already checked out during the CI/CD workflow, then &lt;code&gt;--branch HEAD&lt;/code&gt; should be sufficient. The &lt;code&gt;--fail&lt;/code&gt; flag will return an 183 error code if valid credentials are found.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;trufflehog git file://. --since-commit main --branch feature-1 --results=verified,unknown --fail
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;13: Scan a Postman workspace&lt;/h2&gt; 
&lt;p&gt;Use the &lt;code&gt;--workspace-id&lt;/code&gt;, &lt;code&gt;--collection-id&lt;/code&gt;, &lt;code&gt;--environment&lt;/code&gt; flags multiple times to scan multiple targets.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;trufflehog postman --token=&amp;lt;postman api token&amp;gt; --workspace-id=&amp;lt;workspace id&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;14: Scan a Jenkins server&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;trufflehog jenkins --url https://jenkins.example.com --username admin --password admin
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;15: Scan an Elasticsearch server&lt;/h2&gt; 
&lt;h3&gt;Scan a Local Cluster&lt;/h3&gt; 
&lt;p&gt;There are two ways to authenticate to a local cluster with TruffleHog: (1) username and password, (2) service token.&lt;/p&gt; 
&lt;h4&gt;Connect to a local cluster with username and password&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;trufflehog elasticsearch --nodes 192.168.14.3 192.168.14.4 --username truffle --password hog
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Connect to a local cluster with a service token&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;trufflehog elasticsearch --nodes 192.168.14.3 192.168.14.4 --service-token â€˜AAEWVaWM...Rva2VuaSDZâ€™
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Scan an Elastic Cloud Cluster&lt;/h3&gt; 
&lt;p&gt;To scan a cluster on Elastic Cloud, youâ€™ll need a Cloud ID and API key.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;trufflehog elasticsearch \
  --cloud-id 'search-prod:dXMtY2Vx...YjM1ODNlOWFiZGRlNjI0NA==' \
  --api-key 'MlVtVjBZ...ZSYlduYnF1djh3NG5FQQ=='
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;16. Scan a GitHub Repository for Cross Fork Object References and Deleted Commits&lt;/h2&gt; 
&lt;p&gt;The following command will enumerate deleted and hidden commits on a GitHub repository and then scan them for secrets. This is an alpha release feature.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;trufflehog github-experimental --repo https://github.com/&amp;lt;USER&amp;gt;/&amp;lt;REPO&amp;gt;.git --object-discovery
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In addition to the normal TruffleHog output, the &lt;code&gt;--object-discovery&lt;/code&gt; flag creates two files in a new &lt;code&gt;$HOME/.trufflehog&lt;/code&gt; directory: &lt;code&gt;valid_hidden.txt&lt;/code&gt; and &lt;code&gt;invalid.txt&lt;/code&gt;. These are used to track state during commit enumeration, as well as to provide users with a complete list of all hidden and deleted commits (&lt;code&gt;valid_hidden.txt&lt;/code&gt;). If you'd like to automatically remove these files after scanning, please add the flag &lt;code&gt;--delete-cached-data&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Enumerating all valid commits on a repository using this method takes between 20 minutes and a few hours, depending on the size of your repository. We added a progress bar to keep you updated on how long the enumeration will take. The actual secret scanning runs extremely fast.&lt;/p&gt; 
&lt;p&gt;For more information on Cross Fork Object References, please &lt;a href="https://trufflesecurity.com/blog/anyone-can-access-deleted-and-private-repo-data-github"&gt;read our blog post&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;17. Scan Hugging Face&lt;/h2&gt; 
&lt;h3&gt;Scan a Hugging Face Model, Dataset or Space&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;trufflehog huggingface --model &amp;lt;model_id&amp;gt; --space &amp;lt;space_id&amp;gt; --dataset &amp;lt;dataset_id&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Scan all Models, Datasets and Spaces belonging to a Hugging Face Organization or User&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;trufflehog huggingface --org &amp;lt;orgname&amp;gt; --user &amp;lt;username&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;(Optionally) When scanning an organization or user, you can skip an entire class of resources with &lt;code&gt;--skip-models&lt;/code&gt;, &lt;code&gt;--skip-datasets&lt;/code&gt;, &lt;code&gt;--skip-spaces&lt;/code&gt; OR a particular resource with &lt;code&gt;--ignore-models &amp;lt;model_id&amp;gt;&lt;/code&gt;, &lt;code&gt;--ignore-datasets &amp;lt;dataset_id&amp;gt;&lt;/code&gt;, &lt;code&gt;--ignore-spaces &amp;lt;space_id&amp;gt;&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Scan Discussion and PR Comments&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;trufflehog huggingface --model &amp;lt;model_id&amp;gt; --include-discussions --include-prs
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;18. Scan stdin Input&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;aws s3 cp s3://example/gzipped/data.gz - | gunzip -c | trufflehog stdin
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;&lt;span&gt;â“&lt;/span&gt; FAQ&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;All I see is &lt;code&gt;ğŸ·ğŸ”‘ğŸ· TruffleHog. Unearth your secrets. ğŸ·ğŸ”‘ğŸ·&lt;/code&gt; and the program exits, what gives? 
  &lt;ul&gt; 
   &lt;li&gt;That means no secrets were detected&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Why is the scan taking a long time when I scan a GitHub org 
  &lt;ul&gt; 
   &lt;li&gt;Unauthenticated GitHub scans have rate limits. To improve your rate limits, include the &lt;code&gt;--token&lt;/code&gt; flag with a personal access token&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;It says a private key was verified, what does that mean? 
  &lt;ul&gt; 
   &lt;li&gt;Check out our Driftwood blog post to learn how to do this, in short we've confirmed the key can be used live for SSH or SSL &lt;a href="https://trufflesecurity.com/blog/driftwood-know-if-private-keys-are-sensitive/"&gt;Blog post&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Is there an easy way to ignore specific secrets? 
  &lt;ul&gt; 
   &lt;li&gt;If the scanned source &lt;a href="https://github.com/trufflesecurity/trufflehog/raw/d6375ba92172fd830abb4247cca15e3176448c5d/pkg/engine/engine.go#L358-L365"&gt;supports line numbers&lt;/a&gt;, then you can add a &lt;code&gt;trufflehog:ignore&lt;/code&gt; comment on the line containing the secret to ignore that secrets.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;&lt;span&gt;ğŸ“°&lt;/span&gt; What's new in v3?&lt;/h1&gt; 
&lt;p&gt;TruffleHog v3 is a complete rewrite in Go with many new powerful features.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;We've &lt;strong&gt;added over 700 credential detectors that support active verification against their respective APIs&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;We've also added native &lt;strong&gt;support for scanning GitHub, GitLab, Docker, filesystems, S3, GCS, Circle CI and Travis CI&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Instantly verify private keys&lt;/strong&gt; against millions of github users and &lt;strong&gt;billions&lt;/strong&gt; of TLS certificates using our &lt;a href="https://trufflesecurity.com/blog/driftwood"&gt;Driftwood&lt;/a&gt; technology.&lt;/li&gt; 
 &lt;li&gt;Scan binaries, documents, and other file formats&lt;/li&gt; 
 &lt;li&gt;Available as a GitHub Action and a pre-commit hook&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;What is credential verification?&lt;/h2&gt; 
&lt;p&gt;For every potential credential that is detected, we've painstakingly implemented programmatic verification against the API that we think it belongs to. Verification eliminates false positives. For example, the &lt;a href="https://raw.githubusercontent.com/trufflesecurity/trufflehog/main/pkg/detectors/aws/aws.go"&gt;AWS credential detector&lt;/a&gt; performs a &lt;code&gt;GetCallerIdentity&lt;/code&gt; API call against the AWS API to verify if an AWS credential is active.&lt;/p&gt; 
&lt;h1&gt;&lt;span&gt;ğŸ“&lt;/span&gt; Usage&lt;/h1&gt; 
&lt;p&gt;TruffleHog has a sub-command for each source of data that you may want to scan:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;git&lt;/li&gt; 
 &lt;li&gt;github&lt;/li&gt; 
 &lt;li&gt;gitlab&lt;/li&gt; 
 &lt;li&gt;docker&lt;/li&gt; 
 &lt;li&gt;s3&lt;/li&gt; 
 &lt;li&gt;filesystem (files and directories)&lt;/li&gt; 
 &lt;li&gt;syslog&lt;/li&gt; 
 &lt;li&gt;circleci&lt;/li&gt; 
 &lt;li&gt;travisci&lt;/li&gt; 
 &lt;li&gt;gcs (Google Cloud Storage)&lt;/li&gt; 
 &lt;li&gt;postman&lt;/li&gt; 
 &lt;li&gt;jenkins&lt;/li&gt; 
 &lt;li&gt;elasticsearch&lt;/li&gt; 
 &lt;li&gt;stdin&lt;/li&gt; 
 &lt;li&gt;multi-scan&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Each subcommand can have options that you can see with the &lt;code&gt;--help&lt;/code&gt; flag provided to the sub command:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ trufflehog git --help
usage: TruffleHog git [&amp;lt;flags&amp;gt;] &amp;lt;uri&amp;gt;

Find credentials in git repositories.

Flags:
  -h, --help                Show context-sensitive help (also try --help-long and --help-man).
      --log-level=0         Logging verbosity on a scale of 0 (info) to 5 (trace). Can be disabled with "-1".
      --profile             Enables profiling and sets a pprof and fgprof server on :18066.
  -j, --json                Output in JSON format.
      --json-legacy         Use the pre-v3.0 JSON format. Only works with git, gitlab, and github sources.
      --github-actions      Output in GitHub Actions format.
      --concurrency=20           Number of concurrent workers.
      --no-verification     Don't verify the results.
      --results=RESULTS          Specifies which type(s) of results to output: verified, unknown, unverified, filtered_unverified. Defaults to all types.
      --allow-verification-overlap
                                 Allow verification of similar credentials across detectors
      --filter-unverified   Only output first unverified result per chunk per detector if there are more than one results.
      --filter-entropy=FILTER-ENTROPY
                                 Filter unverified results with Shannon entropy. Start with 3.0.
      --config=CONFIG            Path to configuration file.
      --print-avg-detector-time
                                 Print the average time spent on each detector.
      --no-update           Don't check for updates.
      --fail                Exit with code 183 if results are found.
      --verifier=VERIFIER ...    Set custom verification endpoints.
      --custom-verifiers-only   Only use custom verification endpoints.
      --archive-max-size=ARCHIVE-MAX-SIZE
                                 Maximum size of archive to scan. (Byte units eg. 512B, 2KB, 4MB)
      --archive-max-depth=ARCHIVE-MAX-DEPTH
                                 Maximum depth of archive to scan.
      --archive-timeout=ARCHIVE-TIMEOUT
                                 Maximum time to spend extracting an archive.
      --include-detectors="all"  Comma separated list of detector types to include. Protobuf name or IDs may be used, as well as ranges.
      --exclude-detectors=EXCLUDE-DETECTORS
                                 Comma separated list of detector types to exclude. Protobuf name or IDs may be used, as well as ranges. IDs defined here take precedence over the include list.
      --version             Show application version.
  -i, --include-paths=INCLUDE-PATHS
                                 Path to file with newline separated regexes for files to include in scan.
  -x, --exclude-paths=EXCLUDE-PATHS
                                 Path to file with newline separated regexes for files to exclude in scan.
      --exclude-globs=EXCLUDE-GLOBS
                                 Comma separated list of globs to exclude in scan. This option filters at the `git log` level, resulting in faster scans.
      --since-commit=SINCE-COMMIT
                                 Commit to start scan from.
      --branch=BRANCH            Branch to scan.
      --max-depth=MAX-DEPTH      Maximum depth of commits to scan.
      --bare                Scan bare repository (e.g. useful while using in pre-receive hooks)

Args:
  &amp;lt;uri&amp;gt;  Git repository URL. https://, file://, or ssh:// schema expected.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For example, to scan a &lt;code&gt;git&lt;/code&gt; repository, start with&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;trufflehog git https://github.com/trufflesecurity/trufflehog.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Configuration&lt;/h2&gt; 
&lt;p&gt;TruffleHog supports defining &lt;a href="https://raw.githubusercontent.com/trufflesecurity/trufflehog/main/#regex-detector-alpha"&gt;custom regex detectors&lt;/a&gt; and multiple sources in a configuration file provided via the &lt;code&gt;--config&lt;/code&gt; flag. The regex detectors can be used with any subcommand, while the sources defined in configuration are only for the &lt;code&gt;multi-scan&lt;/code&gt; subcommand.&lt;/p&gt; 
&lt;p&gt;The configuration format for sources can be found on Truffle Security's &lt;a href="https://docs.trufflesecurity.com/scan-data-for-secrets"&gt;source configuration documentation page&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Example GitHub source configuration and &lt;a href="https://docs.trufflesecurity.com/github#Fvm1I"&gt;options reference&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;sources:
- connection:
    '@type': type.googleapis.com/sources.GitHub
    repositories:
    - https://github.com/trufflesecurity/test_keys.git
    unauthenticated: {}
  name: example config scan
  type: SOURCE_TYPE_GITHUB
  verify: true
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You may define multiple connections under the &lt;code&gt;sources&lt;/code&gt; key (see above), and TruffleHog will scan all of the sources concurrently.&lt;/p&gt; 
&lt;h2&gt;S3&lt;/h2&gt; 
&lt;p&gt;The S3 source supports assuming IAM roles for scanning in addition to IAM users. This makes it easier for users to scan multiple AWS accounts without needing to rely on hardcoded credentials for each account.&lt;/p&gt; 
&lt;p&gt;The IAM identity that TruffleHog uses initially will need to have &lt;code&gt;AssumeRole&lt;/code&gt; privileges as a principal in the &lt;a href="https://aws.amazon.com/blogs/security/how-to-use-trust-policies-with-iam-roles/"&gt;trust policy&lt;/a&gt; of each IAM role to assume.&lt;/p&gt; 
&lt;p&gt;To scan a specific bucket using locally set credentials or instance metadata if on an EC2 instance:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;trufflehog s3 --bucket=&amp;lt;bucket-name&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To scan a specific bucket using an assumed role:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;trufflehog s3 --bucket=&amp;lt;bucket-name&amp;gt; --role-arn=&amp;lt;iam-role-arn&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Multiple roles can be passed as separate arguments. The following command will attempt to scan every bucket each role has permissions to list in the S3 API:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;trufflehog s3 --role-arn=&amp;lt;iam-role-arn-1&amp;gt; --role-arn=&amp;lt;iam-role-arn-2&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Exit Codes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;0: No errors and no results were found.&lt;/li&gt; 
 &lt;li&gt;1: An error was encountered. Sources may not have completed scans.&lt;/li&gt; 
 &lt;li&gt;183: No errors were encountered, but results were found. Will only be returned if &lt;code&gt;--fail&lt;/code&gt; flag is used.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;img alt="octocat" src="https://github.githubassets.com/images/icons/emoji/octocat.png?v8" /&gt;) TruffleHog Github Action&lt;/h2&gt; 
&lt;h3&gt;General Usage&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;on:
  push:
    branches:
      - main
  pull_request:

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
    - name: Secret Scanning
      uses: trufflesecurity/trufflehog@main
      with:
        extra_args: --results=verified,unknown
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In the example config above, we're scanning for live secrets in all PRs and Pushes to &lt;code&gt;main&lt;/code&gt;. Only code changes in the referenced commits are scanned. If you'd like to scan an entire branch, please see the "Advanced Usage" section below.&lt;/p&gt; 
&lt;h3&gt;Shallow Cloning&lt;/h3&gt; 
&lt;p&gt;If you're incorporating TruffleHog into a standalone workflow and aren't running any other CI/CD tooling alongside TruffleHog, then we recommend using &lt;a href="https://git-scm.com/docs/git-clone#Documentation/git-clone.txt---depthltdepthgt"&gt;Shallow Cloning&lt;/a&gt; to speed up your workflow. Here's an example of how to do it:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;...
      - shell: bash
        run: |
          if [ "${{ github.event_name }}" == "push" ]; then
            echo "depth=$(($(jq length &amp;lt;&amp;lt;&amp;lt; '${{ toJson(github.event.commits) }}') + 2))" &amp;gt;&amp;gt; $GITHUB_ENV
            echo "branch=${{ github.ref_name }}" &amp;gt;&amp;gt; $GITHUB_ENV
          fi
          if [ "${{ github.event_name }}" == "pull_request" ]; then
            echo "depth=$((${{ github.event.pull_request.commits }}+2))" &amp;gt;&amp;gt; $GITHUB_ENV
            echo "branch=${{ github.event.pull_request.head.ref }}" &amp;gt;&amp;gt; $GITHUB_ENV
          fi
      - uses: actions/checkout@v3
        with:
          ref: ${{env.branch}}
          fetch-depth: ${{env.depth}}
      - uses: trufflesecurity/trufflehog@main
        with:
          extra_args: --results=verified,unknown
...
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Depending on the event type (push or PR), we calculate the number of commits present. Then we add 2, so that we can reference a base commit before our code changes. We pass that integer value to the &lt;code&gt;fetch-depth&lt;/code&gt; flag in the checkout action in addition to the relevant branch. Now our checkout process should be much shorter.&lt;/p&gt; 
&lt;h3&gt;Canary detection&lt;/h3&gt; 
&lt;p&gt;TruffleHog statically detects &lt;a href="https://canarytokens.org/"&gt;https://canarytokens.org/&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/trufflesecurity/trufflehog/assets/52866392/74ace530-08c5-4eaf-a169-84a73e328f6f" alt="image" /&gt;&lt;/p&gt; 
&lt;h3&gt;Advanced Usage&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;- name: TruffleHog
  uses: trufflesecurity/trufflehog@main
  with:
    # Repository path
    path:
    # Start scanning from here (usually main branch).
    base:
    # Scan commits until here (usually dev branch).
    head: # optional
    # Extra args to be passed to the trufflehog cli.
    extra_args: --log-level=2 --results=verified,unknown
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you'd like to specify specific &lt;code&gt;base&lt;/code&gt; and &lt;code&gt;head&lt;/code&gt; refs, you can use the &lt;code&gt;base&lt;/code&gt; argument (&lt;code&gt;--since-commit&lt;/code&gt; flag in TruffleHog CLI) and the &lt;code&gt;head&lt;/code&gt; argument (&lt;code&gt;--branch&lt;/code&gt; flag in the TruffleHog CLI). We only recommend using these arguments for very specific use cases, where the default behavior does not work.&lt;/p&gt; 
&lt;h4&gt;Advanced Usage: Scan entire branch&lt;/h4&gt; 
&lt;pre&gt;&lt;code&gt;- name: scan-push
        uses: trufflesecurity/trufflehog@main
        with:
          base: ""
          head: ${{ github.ref_name }}
          extra_args: --results=verified,unknown
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;TruffleHog GitLab CI&lt;/h2&gt; 
&lt;h3&gt;Example Usage&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;stages:
  - security

security-secrets:
  stage: security
  allow_failure: false
  image: alpine:latest
  variables:
    SCAN_PATH: "." # Set the relative path in the repo to scan
  before_script:
    - apk add --no-cache git curl jq
    - curl -sSfL https://raw.githubusercontent.com/trufflesecurity/trufflehog/main/scripts/install.sh | sh -s -- -b /usr/local/bin
  script:
    - trufflehog filesystem "$SCAN_PATH" --results=verified,unknown --fail --json | jq
  rules:
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event"'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In the example pipeline above, we're scanning for live secrets in all repository directories and files. This job runs only when the pipeline source is a merge request event, meaning it's triggered when a new merge request is created.&lt;/p&gt; 
&lt;h2&gt;Pre-commit Hook&lt;/h2&gt; 
&lt;p&gt;TruffleHog can be used in a pre-commit hook to prevent credentials from leaking before they ever leave your computer.&lt;/p&gt; 
&lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/trufflesecurity/trufflehog/main/PreCommit.md"&gt;pre-commit hook documentation&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;Regex Detector (alpha)&lt;/h2&gt; 
&lt;p&gt;TruffleHog supports detection and verification of custom regular expressions. For detection, at least one &lt;strong&gt;regular expression&lt;/strong&gt; and &lt;strong&gt;keyword&lt;/strong&gt; is required. A &lt;strong&gt;keyword&lt;/strong&gt; is a fixed literal string identifier that appears in or around the regex to be detected. To allow maximum flexibility for verification, a webhook is used containing the regular expression matches.&lt;/p&gt; 
&lt;p&gt;TruffleHog will send a JSON POST request containing the regex matches to a configured webhook endpoint. If the endpoint responds with a &lt;code&gt;200 OK&lt;/code&gt; response status code, the secret is considered verified.&lt;/p&gt; 
&lt;p&gt;Custom Detectors support a few different filtering mechanisms: entropy, regex targeting the entire match, regex targeting the captured secret, and excluded word lists checked against the secret (captured group if present, entire match if capture group is not present). Note that if your custom detector has multiple &lt;code&gt;regex&lt;/code&gt; set (in this example &lt;code&gt;hogID&lt;/code&gt;, and &lt;code&gt;hogToken&lt;/code&gt;), then the filters get applied to each regex. &lt;a href="https://raw.githubusercontent.com/trufflesecurity/trufflehog/main/examples/generic_with_filters.yml"&gt;Here&lt;/a&gt; is an example of a custom detector using these filters.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;NB:&lt;/strong&gt; This feature is alpha and subject to change.&lt;/p&gt; 
&lt;h3&gt;Regex Detector Example&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/trufflesecurity/trufflehog/main/pkg/custom_detectors/CUSTOM_DETECTORS.md"&gt;Here&lt;/a&gt; is how to setup a custom regex detector with verification server.&lt;/p&gt; 
&lt;h2&gt;&lt;span&gt;ğŸ”&lt;/span&gt; Analyze&lt;/h2&gt; 
&lt;p&gt;TruffleHog supports running a deeper analysis of a credential to view its permissions and the resources it has access to.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;trufflehog analyze
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;&lt;span&gt;â¤ï¸&lt;/span&gt; Contributors&lt;/h1&gt; 
&lt;p&gt;This project exists thanks to all the people who contribute. [&lt;a href="https://raw.githubusercontent.com/trufflesecurity/trufflehog/main/CONTRIBUTING.md"&gt;Contribute&lt;/a&gt;].&lt;/p&gt; 
&lt;a href="https://github.com/trufflesecurity/trufflehog/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=trufflesecurity/trufflehog" /&gt; &lt;/a&gt; 
&lt;h1&gt;&lt;span&gt;ğŸ’»&lt;/span&gt; Contributing&lt;/h1&gt; 
&lt;p&gt;Contributions are very welcome! Please see our &lt;a href="https://raw.githubusercontent.com/trufflesecurity/trufflehog/main/CONTRIBUTING.md"&gt;contribution guidelines first&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We no longer accept contributions to TruffleHog v2, but that code is available in the &lt;code&gt;v2&lt;/code&gt; branch.&lt;/p&gt; 
&lt;h2&gt;Adding new secret detectors&lt;/h2&gt; 
&lt;p&gt;We have published some &lt;a href="https://raw.githubusercontent.com/trufflesecurity/trufflehog/main/hack/docs/Adding_Detectors_external.md"&gt;documentation and tooling to get started on adding new secret detectors&lt;/a&gt;. Let's improve detection together!&lt;/p&gt; 
&lt;h1&gt;Use as a library&lt;/h1&gt; 
&lt;p&gt;Currently, trufflehog is in heavy development and no guarantees can be made on the stability of the public APIs at this time.&lt;/p&gt; 
&lt;h1&gt;License Change&lt;/h1&gt; 
&lt;p&gt;Since v3.0, TruffleHog is released under a AGPL 3 license, included in &lt;a href="https://raw.githubusercontent.com/trufflesecurity/trufflehog/main/LICENSE"&gt;&lt;code&gt;LICENSE&lt;/code&gt;&lt;/a&gt;. TruffleHog v3.0 uses none of the previous codebase, but care was taken to preserve backwards compatibility on the command line interface. The work previous to this release is still available licensed under GPL 2.0 in the history of this repository and the previous package releases and tags. A completed CLA is required for us to accept contributions going forward.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>microsoft/ai-agents-for-beginners</title>
      <link>https://github.com/microsoft/ai-agents-for-beginners</link>
      <description>&lt;p&gt;12 Lessons to Get Started Building AI Agents&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AI Agents for Beginners - A Course&lt;/h1&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/images/repo-thumbnailv2.png" alt="Generative AI For Beginners" /&gt;&lt;/p&gt; 
&lt;h2&gt;A course teaching everything you need to know to start building AI Agents&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/microsoft/ai-agents-for-beginners/raw/master/LICENSE?WT.mc_id=academic-105485-koreyst"&gt;&lt;img src="https://img.shields.io/github/license/microsoft/ai-agents-for-beginners.svg?sanitize=true" alt="GitHub license" /&gt;&lt;/a&gt; &lt;a href="https://GitHub.com/microsoft/ai-agents-for-beginners/graphs/contributors/?WT.mc_id=academic-105485-koreyst"&gt;&lt;img src="https://img.shields.io/github/contributors/microsoft/ai-agents-for-beginners.svg?sanitize=true" alt="GitHub contributors" /&gt;&lt;/a&gt; &lt;a href="https://GitHub.com/microsoft/ai-agents-for-beginners/issues/?WT.mc_id=academic-105485-koreyst"&gt;&lt;img src="https://img.shields.io/github/issues/microsoft/ai-agents-for-beginners.svg?sanitize=true" alt="GitHub issues" /&gt;&lt;/a&gt; &lt;a href="https://GitHub.com/microsoft/ai-agents-for-beginners/pulls/?WT.mc_id=academic-105485-koreyst"&gt;&lt;img src="https://img.shields.io/github/issues-pr/microsoft/ai-agents-for-beginners.svg?sanitize=true" alt="GitHub pull-requests" /&gt;&lt;/a&gt; &lt;a href="http://makeapullrequest.com?WT.mc_id=academic-105485-koreyst"&gt;&lt;img src="https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square" alt="PRs Welcome" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;ğŸŒ Multi-Language Support&lt;/h3&gt; 
&lt;h4&gt;Supported via GitHub Action (Automated &amp;amp; Always Up-to-Date)&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/fr/README.md"&gt;French&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/es/README.md"&gt;Spanish&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/de/README.md"&gt;German&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/ru/README.md"&gt;Russian&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/ar/README.md"&gt;Arabic&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/fa/README.md"&gt;Persian (Farsi)&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/ur/README.md"&gt;Urdu&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/zh/README.md"&gt;Chinese (Simplified)&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/mo/README.md"&gt;Chinese (Traditional, Macau)&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/hk/README.md"&gt;Chinese (Traditional, Hong Kong)&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/tw/README.md"&gt;Chinese (Traditional, Taiwan)&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/ja/README.md"&gt;Japanese&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/ko/README.md"&gt;Korean&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/hi/README.md"&gt;Hindi&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/bn/README.md"&gt;Bengali&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/mr/README.md"&gt;Marathi&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/ne/README.md"&gt;Nepali&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/pa/README.md"&gt;Punjabi (Gurmukhi)&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/pt/README.md"&gt;Portuguese (Portugal)&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/br/README.md"&gt;Portuguese (Brazil)&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/it/README.md"&gt;Italian&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/pl/README.md"&gt;Polish&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/tr/README.md"&gt;Turkish&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/el/README.md"&gt;Greek&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/th/README.md"&gt;Thai&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/sv/README.md"&gt;Swedish&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/da/README.md"&gt;Danish&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/no/README.md"&gt;Norwegian&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/fi/README.md"&gt;Finnish&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/nl/README.md"&gt;Dutch&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/he/README.md"&gt;Hebrew&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/vi/README.md"&gt;Vietnamese&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/id/README.md"&gt;Indonesian&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/ms/README.md"&gt;Malay&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/tl/README.md"&gt;Tagalog (Filipino)&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/sw/README.md"&gt;Swahili&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/hu/README.md"&gt;Hungarian&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/cs/README.md"&gt;Czech&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/sk/README.md"&gt;Slovak&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/ro/README.md"&gt;Romanian&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/bg/README.md"&gt;Bulgarian&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/sr/README.md"&gt;Serbian (Cyrillic)&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/hr/README.md"&gt;Croatian&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/sl/README.md"&gt;Slovenian&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/uk/README.md"&gt;Ukrainian&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/my/README.md"&gt;Burmese (Myanmar)&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;If you wish to have additional translations languages supported are listed &lt;a href="https://github.com/Azure/co-op-translator/raw/main/getting_started/supported-languages.md"&gt;here&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://GitHub.com/microsoft/ai-agents-for-beginners/watchers/?WT.mc_id=academic-105485-koreyst"&gt;&lt;img src="https://img.shields.io/github/watchers/microsoft/ai-agents-for-beginners.svg?style=social&amp;amp;label=Watch" alt="GitHub watchers" /&gt;&lt;/a&gt; &lt;a href="https://GitHub.com/microsoft/ai-agents-for-beginners/network/?WT.mc_id=academic-105485-koreyst"&gt;&lt;img src="https://img.shields.io/github/forks/microsoft/ai-agents-for-beginners.svg?style=social&amp;amp;label=Fork" alt="GitHub forks" /&gt;&lt;/a&gt; &lt;a href="https://GitHub.com/microsoft/ai-agents-for-beginners/stargazers/?WT.mc_id=academic-105485-koreyst"&gt;&lt;img src="https://img.shields.io/github/stars/microsoft/ai-agents-for-beginners.svg?style=social&amp;amp;label=Star" alt="GitHub stars" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://discord.gg/kzRShWzttr"&gt;&lt;img src="https://dcbadge.limes.pink/api/server/kzRShWzttr" alt="Azure AI Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ğŸŒ± Getting Started&lt;/h2&gt; 
&lt;p&gt;This course has lessons covering the fundamentals of building AI Agents. Each lesson covers its own topic so start wherever you like!&lt;/p&gt; 
&lt;p&gt;There is multi-language support for this course. Go to our &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/#-multi-language-support"&gt;available languages here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If this is your first time building with Generative AI models, check out our &lt;a href="https://aka.ms/genai-beginners"&gt;Generative AI For Beginners&lt;/a&gt; course, which includes 21 lessons on building with GenAI.&lt;/p&gt; 
&lt;p&gt;Don't forget to &lt;a href="https://docs.github.com/en/get-started/exploring-projects-on-github/saving-repositories-with-stars?WT.mc_id=academic-105485-koreyst"&gt;star (ğŸŒŸ) this repo&lt;/a&gt; and &lt;a href="https://github.com/microsoft/ai-agents-for-beginners/fork"&gt;fork this repo&lt;/a&gt; to run the code.&lt;/p&gt; 
&lt;h3&gt;Meet Other Learners, Get Your Questions Answered&lt;/h3&gt; 
&lt;p&gt;If you get stuck or have any questions about building AI Agents, join our dedicated Discord Channel in the &lt;a href="https://aka.ms/ai-agents/discord"&gt;Azure AI Foundry Community Discord&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;What You Need&lt;/h3&gt; 
&lt;p&gt;Each lesson in this course includes code examples, which can be found in the code_samples folder. You can &lt;a href="https://github.com/microsoft/ai-agents-for-beginners/fork"&gt;fork this repo&lt;/a&gt; to create your own copy.&lt;/p&gt; 
&lt;p&gt;The code example in these exercises, utilize Azure AI Foundry and GitHub Model Catalogs for interacting with Language Models:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://aka.ms/ai-agents-beginners/github-models"&gt;Github Models&lt;/a&gt; - Free / Limited&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aka.ms/ai-agents-beginners/ai-foundry"&gt;Azure AI Foundry&lt;/a&gt; - Azure Account Required&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This course also uses the following AI Agent frameworks and services from Microsoft:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://aka.ms/ai-agents-beginners/ai-agent-service"&gt;Azure AI Agent Service&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aka.ms/ai-agents-beginners/semantic-kernel"&gt;Semantic Kernel&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aka.ms/ai-agents/autogen"&gt;AutoGen&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For more information on running the code for this course, go to the &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/00-course-setup/README.md"&gt;Course Setup&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ğŸ™ Want to help?&lt;/h2&gt; 
&lt;p&gt;Do you have suggestions or found spelling or code errors? &lt;a href="https://github.com/microsoft/ai-agents-for-beginners/issues?WT.mc_id=academic-105485-koreyst"&gt;Raise an issue&lt;/a&gt; or &lt;a href="https://github.com/microsoft/ai-agents-for-beginners/pulls?WT.mc_id=academic-105485-koreyst"&gt;Create a pull request&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ğŸ“‚ Each lesson includes&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;A written lesson located in the README and a short video&lt;/li&gt; 
 &lt;li&gt;Python code samples supporting Azure AI Foundry and Github Models (Free)&lt;/li&gt; 
 &lt;li&gt;Links to extra resources to continue your learning&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ—ƒï¸ Lessons&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;strong&gt;Lesson&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Text &amp;amp; Code&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Video&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Extra Learning&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Intro to AI Agents and Agent Use Cases&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/01-intro-to-ai-agents/README.md"&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://youtu.be/3zgm60bXmQk?si=z8QygFvYQv-9WtO1"&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst"&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Exploring AI Agentic Frameworks&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/02-explore-agentic-frameworks/README.md"&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://youtu.be/ODwF-EZo_O8?si=Vawth4hzVaHv-u0H"&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst"&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Understanding AI Agentic Design Patterns&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/03-agentic-design-patterns/README.md"&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://youtu.be/m9lM8qqoOEA?si=BIzHwzstTPL8o9GF"&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst"&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Tool Use Design Pattern&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/04-tool-use/README.md"&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://youtu.be/vieRiPRx-gI?si=2z6O2Xu2cu_Jz46N"&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst"&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Agentic RAG&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/05-agentic-rag/README.md"&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://youtu.be/WcjAARvdL7I?si=gKPWsQpKiIlDH9A3"&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst"&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Building Trustworthy AI Agents&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/06-building-trustworthy-agents/README.md"&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://youtu.be/iZKkMEGBCUQ?si=jZjpiMnGFOE9L8OK"&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst"&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Planning Design Pattern&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/07-planning-design/README.md"&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://youtu.be/kPfJ2BrBCMY?si=6SC_iv_E5-mzucnC"&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst"&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Multi-Agent Design Pattern&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/08-multi-agent/README.md"&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://youtu.be/V6HpE9hZEx0?si=rMgDhEu7wXo2uo6g"&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst"&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Metacognition Design Pattern&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/09-metacognition/README.md"&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://youtu.be/His9R6gw6Ec?si=8gck6vvdSNCt6OcF"&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst"&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;AI Agents in Production&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/10-ai-agents-production/README.md"&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://youtu.be/l4TP6IyJxmQ?si=31dnhexRo6yLRJDl"&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst"&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Using Agentic Protocols (MCP, A2A and NLWeb)&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/11-agentic-protocols/README.md"&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://youtu.be/X-Dh9R3Opn8"&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst"&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Context Engineering for AI Agents&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/12-context-engineering/README.md"&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://youtu.be/F5zqRV7gEag"&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst"&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Managing Agentic Memory&lt;/td&gt; 
   &lt;td&gt;Coming - September 11th&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Evaluating AI Agents&lt;/td&gt; 
   &lt;td&gt;Coming - September 18th&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Building Computer Use Agents (CUA)&lt;/td&gt; 
   &lt;td&gt;Coming - September 25th&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Deploying Scalable Agents&lt;/td&gt; 
   &lt;td&gt;Coming - September 25th&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Creating Local AI Agents&lt;/td&gt; 
   &lt;td&gt;Coming - October 3rd&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Securing AI Agents&lt;/td&gt; 
   &lt;td&gt;Coming - October 10th&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;ğŸ’ Other Courses&lt;/h2&gt; 
&lt;p&gt;Our team produces other courses! Check out:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/microsoft/mcp-for-beginners?WT.mc_id=academic-105485-koreyst"&gt;&lt;strong&gt;NEW&lt;/strong&gt; Model Context Protocol (MCP) For Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/microsoft/Generative-AI-for-beginners-dotnet?WT.mc_id=academic-105485-koreyst"&gt;Generative AI for Beginners using .NET&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/microsoft/generative-ai-for-beginners?WT.mc_id=academic-105485-koreyst"&gt;Generative AI for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/microsoft/generative-ai-for-beginners-java?WT.mc_id=academic-105485-koreyst"&gt;Generative AI for Beginners using Java&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aka.ms/ml-beginners?WT.mc_id=academic-105485-koreyst"&gt;ML for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aka.ms/datascience-beginners?WT.mc_id=academic-105485-koreyst"&gt;Data Science for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aka.ms/ai-beginners?WT.mc_id=academic-105485-koreyst"&gt;AI for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/microsoft/Security-101??WT.mc_id=academic-96948-sayoung"&gt;Cybersecurity for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aka.ms/webdev-beginners?WT.mc_id=academic-105485-koreyst"&gt;Web Dev for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aka.ms/iot-beginners?WT.mc_id=academic-105485-koreyst"&gt;IoT for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/microsoft/xr-development-for-beginners?WT.mc_id=academic-105485-koreyst"&gt;XR Development for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aka.ms/GitHubCopilotAI?WT.mc_id=academic-105485-koreyst"&gt;Mastering GitHub Copilot for AI Paired Programming&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/microsoft/mastering-github-copilot-for-dotnet-csharp-developers?WT.mc_id=academic-105485-koreyst"&gt;Mastering GitHub Copilot for C#/.NET Developers&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/microsoft/CopilotAdventures?WT.mc_id=academic-105485-koreyst"&gt;Choose Your Own Copilot Adventure&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸŒŸ Community Thanks&lt;/h2&gt; 
&lt;p&gt;Thanks to &lt;a href="https://www.linkedin.com/in/shivam2003/"&gt;Shivam Goyal&lt;/a&gt; for contributing important code samples demonstrating Agentic RAG.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href="https://cla.opensource.microsoft.com"&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; 
&lt;p&gt;This project has adopted the &lt;a href="https://opensource.microsoft.com/codeofconduct/"&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href="https://opensource.microsoft.com/codeofconduct/faq/"&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href="mailto:opencode@microsoft.com"&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; 
&lt;h2&gt;Trademarks&lt;/h2&gt; 
&lt;p&gt;This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow &lt;a href="https://www.microsoft.com/legal/intellectualproperty/trademarks/usage/general"&gt;Microsoft's Trademark &amp;amp; Brand Guidelines&lt;/a&gt;. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos is subject to those third-parties' policies.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>