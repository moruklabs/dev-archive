<rss version="2.0">
  <channel>
    <title>GitHub Python Monthly Trending</title>
    <description>Monthly Trending of Python in GitHub</description>
    <pubDate>Wed, 03 Sep 2025 01:54:35 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>vllm-project/vllm</title>
      <link>https://github.com/vllm-project/vllm</link>
      <description>&lt;p&gt;A high-throughput and memory-efficient inference and serving engine for LLMs&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/vllm-project/vllm/main/docs/assets/logos/vllm-logo-text-dark.png" /&gt; 
  &lt;img alt="vLLM" src="https://raw.githubusercontent.com/vllm-project/vllm/main/docs/assets/logos/vllm-logo-text-light.png" width="55%" /&gt; 
 &lt;/picture&gt; &lt;/p&gt; 
&lt;h3 align="center"&gt; Easy, fast, and cheap LLM serving for everyone &lt;/h3&gt; 
&lt;p align="center"&gt; | &lt;a href="https://docs.vllm.ai"&gt;&lt;b&gt;Documentation&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://blog.vllm.ai/"&gt;&lt;b&gt;Blog&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://arxiv.org/abs/2309.06180"&gt;&lt;b&gt;Paper&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://x.com/vllm_project"&gt;&lt;b&gt;Twitter/X&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://discuss.vllm.ai"&gt;&lt;b&gt;User Forum&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://slack.vllm.ai"&gt;&lt;b&gt;Developer Slack&lt;/b&gt;&lt;/a&gt; | &lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;em&gt;Latest News&lt;/em&gt; 🔥&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;[2025/08] We hosted &lt;a href="https://mp.weixin.qq.com/s/pDmAXHcN7Iqc8sUKgJgGtg"&gt;vLLM Shanghai Meetup&lt;/a&gt; focusing on building, developing, and integrating with vLLM! Please find the meetup slides &lt;a href="https://drive.google.com/drive/folders/1OvLx39wnCGy_WKq8SiVKf7YcxxYI3WCH"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[2025/08] We hosted &lt;a href="https://luma.com/cgcgprmh"&gt;vLLM Korea Meetup&lt;/a&gt; with Red Hat and Rebellions! We shared the latest advancements in vLLM along with project spotlights from the vLLM Korea community. Please find the meetup slides &lt;a href="https://drive.google.com/file/d/1bcrrAE1rxUgx0mjIeOWT6hNe2RefC5Hm/view"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[2025/08] We hosted &lt;a href="https://mp.weixin.qq.com/s/dgkWg1WFpWGO2jCdTqQHxA"&gt;vLLM Beijing Meetup&lt;/a&gt; focusing on large-scale LLM deployment! Please find the meetup slides &lt;a href="https://drive.google.com/drive/folders/1Pid6NSFLU43DZRi0EaTcPgXsAzDvbBqF"&gt;here&lt;/a&gt; and the recording &lt;a href="https://www.chaspark.com/#/live/1166916873711665152"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[2025/05] vLLM is now a hosted project under PyTorch Foundation! Please find the announcement &lt;a href="https://pytorch.org/blog/pytorch-foundation-welcomes-vllm/"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[2025/01] We are excited to announce the alpha release of vLLM V1: A major architectural upgrade with 1.7x speedup! Clean code, optimized execution loop, zero-overhead prefix caching, enhanced multimodal support, and more. Please check out our blog post &lt;a href="https://blog.vllm.ai/2025/01/27/v1-alpha-release.html"&gt;here&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;Previous News&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;[2025/05] We hosted &lt;a href="https://lu.ma/c1rqyf1f"&gt;NYC vLLM Meetup&lt;/a&gt;! Please find the meetup slides &lt;a href="https://docs.google.com/presentation/d/1_q_aW_ioMJWUImf1s1YM-ZhjXz8cUeL0IJvaquOYBeA/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2025/04] We hosted &lt;a href="https://www.sginnovate.com/event/limited-availability-morning-evening-slots-remaining-inaugural-vllm-asia-developer-day"&gt;Asia Developer Day&lt;/a&gt;! Please find the meetup slides from the vLLM team &lt;a href="https://docs.google.com/presentation/d/19cp6Qu8u48ihB91A064XfaXruNYiBOUKrBxAmDOllOo/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2025/03] We hosted &lt;a href="https://lu.ma/vllm-ollama"&gt;vLLM x Ollama Inference Night&lt;/a&gt;! Please find the meetup slides from the vLLM team &lt;a href="https://docs.google.com/presentation/d/16T2PDD1YwRnZ4Tu8Q5r6n53c5Lr5c73UV9Vd2_eBo4U/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2025/03] We hosted &lt;a href="https://mp.weixin.qq.com/s/n77GibL2corAtQHtVEAzfg"&gt;the first vLLM China Meetup&lt;/a&gt;! Please find the meetup slides from vLLM team &lt;a href="https://docs.google.com/presentation/d/1REHvfQMKGnvz6p3Fd23HhSO4c8j5WPGZV0bKYLwnHyQ/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2025/03] We hosted &lt;a href="https://lu.ma/7mu4k4xx"&gt;the East Coast vLLM Meetup&lt;/a&gt;! Please find the meetup slides &lt;a href="https://docs.google.com/presentation/d/1NHiv8EUFF1NLd3fEYODm56nDmL26lEeXCaDgyDlTsRs/edit#slide=id.g31441846c39_0_0"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2025/02] We hosted &lt;a href="https://lu.ma/h7g3kuj9"&gt;the ninth vLLM meetup&lt;/a&gt; with Meta! Please find the meetup slides from vLLM team &lt;a href="https://docs.google.com/presentation/d/1jzC_PZVXrVNSFVCW-V4cFXb6pn7zZ2CyP_Flwo05aqg/edit?usp=sharing"&gt;here&lt;/a&gt; and AMD &lt;a href="https://drive.google.com/file/d/1Zk5qEJIkTmlQ2eQcXQZlljAx3m9s7nwn/view?usp=sharing"&gt;here&lt;/a&gt;. The slides from Meta will not be posted.&lt;/li&gt; 
  &lt;li&gt;[2025/01] We hosted &lt;a href="https://lu.ma/zep56hui"&gt;the eighth vLLM meetup&lt;/a&gt; with Google Cloud! Please find the meetup slides from vLLM team &lt;a href="https://docs.google.com/presentation/d/1epVkt4Zu8Jz_S5OhEHPc798emsYh2BwYfRuDDVEF7u4/edit?usp=sharing"&gt;here&lt;/a&gt;, and Google Cloud team &lt;a href="https://drive.google.com/file/d/1h24pHewANyRL11xy5dXUbvRC9F9Kkjix/view?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2024/12] vLLM joins &lt;a href="https://pytorch.org/blog/vllm-joins-pytorch"&gt;pytorch ecosystem&lt;/a&gt;! Easy, Fast, and Cheap LLM Serving for Everyone!&lt;/li&gt; 
  &lt;li&gt;[2024/11] We hosted &lt;a href="https://lu.ma/h0qvrajz"&gt;the seventh vLLM meetup&lt;/a&gt; with Snowflake! Please find the meetup slides from vLLM team &lt;a href="https://docs.google.com/presentation/d/1e3CxQBV3JsfGp30SwyvS3eM_tW-ghOhJ9PAJGK6KR54/edit?usp=sharing"&gt;here&lt;/a&gt;, and Snowflake team &lt;a href="https://docs.google.com/presentation/d/1qF3RkDAbOULwz9WK5TOltt2fE9t6uIc_hVNLFAaQX6A/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2024/10] We have just created a developer slack (&lt;a href="https://slack.vllm.ai"&gt;slack.vllm.ai&lt;/a&gt;) focusing on coordinating contributions and discussing features. Please feel free to join us there!&lt;/li&gt; 
  &lt;li&gt;[2024/10] Ray Summit 2024 held a special track for vLLM! Please find the opening talk slides from the vLLM team &lt;a href="https://docs.google.com/presentation/d/1B_KQxpHBTRa_mDF-tR6i8rWdOU5QoTZNcEg2MKZxEHM/edit?usp=sharing"&gt;here&lt;/a&gt;. Learn more from the &lt;a href="https://www.youtube.com/playlist?list=PLzTswPQNepXl6AQwifuwUImLPFRVpksjR"&gt;talks&lt;/a&gt; from other vLLM contributors and users!&lt;/li&gt; 
  &lt;li&gt;[2024/09] We hosted &lt;a href="https://lu.ma/87q3nvnh"&gt;the sixth vLLM meetup&lt;/a&gt; with NVIDIA! Please find the meetup slides &lt;a href="https://docs.google.com/presentation/d/1wrLGwytQfaOTd5wCGSPNhoaW3nq0E-9wqyP7ny93xRs/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2024/07] We hosted &lt;a href="https://lu.ma/lp0gyjqr"&gt;the fifth vLLM meetup&lt;/a&gt; with AWS! Please find the meetup slides &lt;a href="https://docs.google.com/presentation/d/1RgUD8aCfcHocghoP3zmXzck9vX3RCI9yfUAB2Bbcl4Y/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2024/07] In partnership with Meta, vLLM officially supports Llama 3.1 with FP8 quantization and pipeline parallelism! Please check out our blog post &lt;a href="https://blog.vllm.ai/2024/07/23/llama31.html"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2024/06] We hosted &lt;a href="https://lu.ma/agivllm"&gt;the fourth vLLM meetup&lt;/a&gt; with Cloudflare and BentoML! Please find the meetup slides &lt;a href="https://docs.google.com/presentation/d/1iJ8o7V2bQEi0BFEljLTwc5G1S10_Rhv3beed5oB0NJ4/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2024/04] We hosted &lt;a href="https://robloxandvllmmeetup2024.splashthat.com/"&gt;the third vLLM meetup&lt;/a&gt; with Roblox! Please find the meetup slides &lt;a href="https://docs.google.com/presentation/d/1A--47JAK4BJ39t954HyTkvtfwn0fkqtsL8NGFuslReM/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2024/01] We hosted &lt;a href="https://lu.ma/ygxbpzhl"&gt;the second vLLM meetup&lt;/a&gt; with IBM! Please find the meetup slides &lt;a href="https://docs.google.com/presentation/d/12mI2sKABnUw5RBWXDYY-HtHth4iMSNcEoQ10jDQbxgA/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2023/10] We hosted &lt;a href="https://lu.ma/first-vllm-meetup"&gt;the first vLLM meetup&lt;/a&gt; with a16z! Please find the meetup slides &lt;a href="https://docs.google.com/presentation/d/1QL-XPFXiFpDBh86DbEegFXBXFXjix4v032GhShbKf3s/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2023/08] We would like to express our sincere gratitude to &lt;a href="https://a16z.com/2023/08/30/supporting-the-open-source-ai-community/"&gt;Andreessen Horowitz&lt;/a&gt; (a16z) for providing a generous grant to support the open-source development and research of vLLM.&lt;/li&gt; 
  &lt;li&gt;[2023/06] We officially released vLLM! FastChat-vLLM integration has powered &lt;a href="https://chat.lmsys.org"&gt;LMSYS Vicuna and Chatbot Arena&lt;/a&gt; since mid-April. Check out our &lt;a href="https://vllm.ai"&gt;blog post&lt;/a&gt;.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h2&gt;About&lt;/h2&gt; 
&lt;p&gt;vLLM is a fast and easy-to-use library for LLM inference and serving.&lt;/p&gt; 
&lt;p&gt;Originally developed in the &lt;a href="https://sky.cs.berkeley.edu"&gt;Sky Computing Lab&lt;/a&gt; at UC Berkeley, vLLM has evolved into a community-driven project with contributions from both academia and industry.&lt;/p&gt; 
&lt;p&gt;vLLM is fast with:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;State-of-the-art serving throughput&lt;/li&gt; 
 &lt;li&gt;Efficient management of attention key and value memory with &lt;a href="https://blog.vllm.ai/2023/06/20/vllm.html"&gt;&lt;strong&gt;PagedAttention&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Continuous batching of incoming requests&lt;/li&gt; 
 &lt;li&gt;Fast model execution with CUDA/HIP graph&lt;/li&gt; 
 &lt;li&gt;Quantizations: &lt;a href="https://arxiv.org/abs/2210.17323"&gt;GPTQ&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/2306.00978"&gt;AWQ&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/2309.05516"&gt;AutoRound&lt;/a&gt;, INT4, INT8, and FP8&lt;/li&gt; 
 &lt;li&gt;Optimized CUDA kernels, including integration with FlashAttention and FlashInfer&lt;/li&gt; 
 &lt;li&gt;Speculative decoding&lt;/li&gt; 
 &lt;li&gt;Chunked prefill&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;vLLM is flexible and easy to use with:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Seamless integration with popular Hugging Face models&lt;/li&gt; 
 &lt;li&gt;High-throughput serving with various decoding algorithms, including &lt;em&gt;parallel sampling&lt;/em&gt;, &lt;em&gt;beam search&lt;/em&gt;, and more&lt;/li&gt; 
 &lt;li&gt;Tensor, pipeline, data and expert parallelism support for distributed inference&lt;/li&gt; 
 &lt;li&gt;Streaming outputs&lt;/li&gt; 
 &lt;li&gt;OpenAI-compatible API server&lt;/li&gt; 
 &lt;li&gt;Support NVIDIA GPUs, AMD CPUs and GPUs, Intel CPUs and GPUs, PowerPC CPUs, TPU, and AWS Neuron&lt;/li&gt; 
 &lt;li&gt;Prefix caching support&lt;/li&gt; 
 &lt;li&gt;Multi-LoRA support&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;vLLM seamlessly supports most popular open-source models on HuggingFace, including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Transformer-like LLMs (e.g., Llama)&lt;/li&gt; 
 &lt;li&gt;Mixture-of-Expert LLMs (e.g., Mixtral, Deepseek-V2 and V3)&lt;/li&gt; 
 &lt;li&gt;Embedding Models (e.g., E5-Mistral)&lt;/li&gt; 
 &lt;li&gt;Multi-modal LLMs (e.g., LLaVA)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Find the full list of supported models &lt;a href="https://docs.vllm.ai/en/latest/models/supported_models.html"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;Install vLLM with &lt;code&gt;pip&lt;/code&gt; or &lt;a href="https://docs.vllm.ai/en/latest/getting_started/installation/gpu/index.html#build-wheel-from-source"&gt;from source&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install vllm
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Visit our &lt;a href="https://docs.vllm.ai/en/latest/"&gt;documentation&lt;/a&gt; to learn more.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.vllm.ai/en/latest/getting_started/installation.html"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.vllm.ai/en/latest/getting_started/quickstart.html"&gt;Quickstart&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.vllm.ai/en/latest/models/supported_models.html"&gt;List of Supported Models&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome and value any contributions and collaborations. Please check out &lt;a href="https://docs.vllm.ai/en/latest/contributing/index.html"&gt;Contributing to vLLM&lt;/a&gt; for how to get involved.&lt;/p&gt; 
&lt;h2&gt;Sponsors&lt;/h2&gt; 
&lt;p&gt;vLLM is a community project. Our compute resources for development and testing are supported by the following organizations. Thank you for your support!&lt;/p&gt; 
&lt;!-- Note: Please sort them in alphabetical order. --&gt; 
&lt;!-- Note: Please keep these consistent with docs/community/sponsors.md --&gt; 
&lt;p&gt;Cash Donations:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;a16z&lt;/li&gt; 
 &lt;li&gt;Dropbox&lt;/li&gt; 
 &lt;li&gt;Sequoia Capital&lt;/li&gt; 
 &lt;li&gt;Skywork AI&lt;/li&gt; 
 &lt;li&gt;ZhenFund&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Compute Resources:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Alibaba Cloud&lt;/li&gt; 
 &lt;li&gt;AMD&lt;/li&gt; 
 &lt;li&gt;Anyscale&lt;/li&gt; 
 &lt;li&gt;AWS&lt;/li&gt; 
 &lt;li&gt;Crusoe Cloud&lt;/li&gt; 
 &lt;li&gt;Databricks&lt;/li&gt; 
 &lt;li&gt;DeepInfra&lt;/li&gt; 
 &lt;li&gt;Google Cloud&lt;/li&gt; 
 &lt;li&gt;Intel&lt;/li&gt; 
 &lt;li&gt;Lambda Lab&lt;/li&gt; 
 &lt;li&gt;Nebius&lt;/li&gt; 
 &lt;li&gt;Novita AI&lt;/li&gt; 
 &lt;li&gt;NVIDIA&lt;/li&gt; 
 &lt;li&gt;Replicate&lt;/li&gt; 
 &lt;li&gt;Roblox&lt;/li&gt; 
 &lt;li&gt;RunPod&lt;/li&gt; 
 &lt;li&gt;Trainy&lt;/li&gt; 
 &lt;li&gt;UC Berkeley&lt;/li&gt; 
 &lt;li&gt;UC San Diego&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Slack Sponsor: Anyscale&lt;/p&gt; 
&lt;p&gt;We also have an official fundraising venue through &lt;a href="https://opencollective.com/vllm"&gt;OpenCollective&lt;/a&gt;. We plan to use the fund to support the development, maintenance, and adoption of vLLM.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you use vLLM for your research, please cite our &lt;a href="https://arxiv.org/abs/2309.06180"&gt;paper&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@inproceedings{kwon2023efficient,
  title={Efficient Memory Management for Large Language Model Serving with PagedAttention},
  author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},
  booktitle={Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles},
  year={2023}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contact Us&lt;/h2&gt; 
&lt;!-- --8&lt;-- [start:contact-us] --&gt; 
&lt;ul&gt; 
 &lt;li&gt;For technical questions and feature requests, please use GitHub &lt;a href="https://github.com/vllm-project/vllm/issues"&gt;Issues&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;For discussing with fellow users, please use the &lt;a href="https://discuss.vllm.ai"&gt;vLLM Forum&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;For coordinating contributions and development, please use &lt;a href="https://slack.vllm.ai"&gt;Slack&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;For security disclosures, please use GitHub's &lt;a href="https://github.com/vllm-project/vllm/security/advisories"&gt;Security Advisories&lt;/a&gt; feature&lt;/li&gt; 
 &lt;li&gt;For collaborations and partnerships, please contact us at &lt;a href="mailto:vllm-questions@lists.berkeley.edu"&gt;vllm-questions@lists.berkeley.edu&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- --8&lt;-- [end:contact-us] --&gt; 
&lt;h2&gt;Media Kit&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;If you wish to use vLLM's logo, please refer to &lt;a href="https://github.com/vllm-project/media-kit"&gt;our media kit repo&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>9001/copyparty</title>
      <link>https://github.com/9001/copyparty</link>
      <description>&lt;p&gt;Portable file server with accelerated resumable uploads, dedup, WebDAV, FTP, TFTP, zeroconf, media indexer, thumbnails++ all in one file, no deps&lt;/p&gt;&lt;hr&gt;&lt;img src="https://github.com/9001/copyparty/raw/hovudstraum/docs/logo.svg?sanitize=true" width="250" align="right" /&gt; 
&lt;h3&gt;💾🎉 copyparty&lt;/h3&gt; 
&lt;p&gt;turn almost any device into a file server with resumable uploads/downloads using &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#browser-support"&gt;&lt;em&gt;any&lt;/em&gt;&lt;/a&gt; web browser&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;server only needs Python (2 or 3), all dependencies optional&lt;/li&gt; 
 &lt;li&gt;🔌 protocols: &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#the-browser"&gt;http&lt;/a&gt; // &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#webdav-server"&gt;webdav&lt;/a&gt; // &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#ftp-server"&gt;ftp&lt;/a&gt; // &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#tftp-server"&gt;tftp&lt;/a&gt; // &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#smb-server"&gt;smb/cifs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;📱 &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#android-app"&gt;android app&lt;/a&gt; // &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#ios-shortcuts"&gt;iPhone shortcuts&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;👉 &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#quickstart"&gt;Get started&lt;/a&gt;!&lt;/strong&gt; or visit the &lt;strong&gt;&lt;a href="https://a.ocv.me/pub/demo/"&gt;read-only demo server&lt;/a&gt;&lt;/strong&gt; 👀 running on a nuc in my basement&lt;/p&gt; 
&lt;p&gt;📷 &lt;strong&gt;screenshots:&lt;/strong&gt; &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#the-browser"&gt;browser&lt;/a&gt; // &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#uploading"&gt;upload&lt;/a&gt; // &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#unpost"&gt;unpost&lt;/a&gt; // &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#thumbnails"&gt;thumbnails&lt;/a&gt; // &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#searching"&gt;search&lt;/a&gt; // &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#file-search"&gt;fsearch&lt;/a&gt; // &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#zip-downloads"&gt;zip-DL&lt;/a&gt; // &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#markdown-viewer"&gt;md-viewer&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;🎬 &lt;strong&gt;videos:&lt;/strong&gt; &lt;a href="https://a.ocv.me/pub/demo/pics-vids/up2k.webm"&gt;upload&lt;/a&gt; // &lt;a href="https://a.ocv.me/pub/demo/pics-vids/u2cli.webm"&gt;cli-upload&lt;/a&gt; // &lt;a href="https://a.ocv.me/pub/g/nerd-stuff/cpp/2024-0418-race-the-beam.webm"&gt;race-the-beam&lt;/a&gt; // 👉 &lt;strong&gt;&lt;a href="https://a.ocv.me/pub/demo/showcase-hq.webm"&gt;feature-showcase&lt;/a&gt;&lt;/strong&gt; (&lt;a href="https://www.youtube.com/watch?v=15_-hgsX2V0"&gt;youtube&lt;/a&gt;)&lt;/p&gt; 
&lt;p&gt;made in Norway 🇳🇴&lt;/p&gt; 
&lt;h2&gt;readme toc&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;top 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#quickstart"&gt;quickstart&lt;/a&gt; - just run &lt;strong&gt;&lt;a href="https://github.com/9001/copyparty/releases/latest/download/copyparty-sfx.py"&gt;copyparty-sfx.py&lt;/a&gt;&lt;/strong&gt; -- that's it! 🎉 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#at-home"&gt;at home&lt;/a&gt; - make it accessible over the internet&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#on-servers"&gt;on servers&lt;/a&gt; - you may also want these, especially on servers&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#features"&gt;features&lt;/a&gt; - also see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/versus.md"&gt;comparison to similar software&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#testimonials"&gt;testimonials&lt;/a&gt; - small collection of user feedback&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#motivations"&gt;motivations&lt;/a&gt; - project goals / philosophy 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#notes"&gt;notes&lt;/a&gt; - general notes&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#bugs"&gt;bugs&lt;/a&gt; - roughly sorted by chance of encounter 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#not-my-bugs"&gt;not my bugs&lt;/a&gt; - same order here too&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#breaking-changes"&gt;breaking changes&lt;/a&gt; - upgrade notes&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#FAQ"&gt;FAQ&lt;/a&gt; - "frequently" asked questions&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#accounts-and-volumes"&gt;accounts and volumes&lt;/a&gt; - per-folder, per-user permissions 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#shadowing"&gt;shadowing&lt;/a&gt; - hiding specific subfolders&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#dotfiles"&gt;dotfiles&lt;/a&gt; - unix-style hidden files/folders&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#the-browser"&gt;the browser&lt;/a&gt; - accessing a copyparty server using a web-browser 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#tabs"&gt;tabs&lt;/a&gt; - the main tabs in the ui&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#hotkeys"&gt;hotkeys&lt;/a&gt; - the browser has the following hotkeys&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#navpane"&gt;navpane&lt;/a&gt; - switching between breadcrumbs or navpane&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#thumbnails"&gt;thumbnails&lt;/a&gt; - press &lt;code&gt;g&lt;/code&gt; or &lt;code&gt;田&lt;/code&gt; to toggle grid-view instead of the file listing&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#zip-downloads"&gt;zip downloads&lt;/a&gt; - download folders (or file selections) as &lt;code&gt;zip&lt;/code&gt; or &lt;code&gt;tar&lt;/code&gt; files&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#uploading"&gt;uploading&lt;/a&gt; - drag files/folders into the web-browser to upload 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#file-search"&gt;file-search&lt;/a&gt; - dropping files into the browser also lets you see if they exist on the server&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#unpost"&gt;unpost&lt;/a&gt; - undo/delete accidental uploads&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#self-destruct"&gt;self-destruct&lt;/a&gt; - uploads can be given a lifetime&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#race-the-beam"&gt;race the beam&lt;/a&gt; - download files while they're still uploading (&lt;a href="http://a.ocv.me/pub/g/nerd-stuff/cpp/2024-0418-race-the-beam.webm"&gt;demo video&lt;/a&gt;)&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#incoming-files"&gt;incoming files&lt;/a&gt; - the control-panel shows the ETA for all incoming files&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#file-manager"&gt;file manager&lt;/a&gt; - cut/paste, rename, and delete files/folders (if you have permission)&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#shares"&gt;shares&lt;/a&gt; - share a file or folder by creating a temporary link&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#batch-rename"&gt;batch rename&lt;/a&gt; - select some files and press &lt;code&gt;F2&lt;/code&gt; to bring up the rename UI&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#rss-feeds"&gt;rss feeds&lt;/a&gt; - monitor a folder with your RSS reader&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#recent-uploads"&gt;recent uploads&lt;/a&gt; - list all recent uploads&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#media-player"&gt;media player&lt;/a&gt; - plays almost every audio format there is 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#playlists"&gt;playlists&lt;/a&gt; - create and play &lt;a href="https://en.wikipedia.org/wiki/M3U"&gt;m3u8&lt;/a&gt; playlists&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#creating-a-playlist"&gt;creating a playlist&lt;/a&gt; - with a standalone mediaplayer or copyparty&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#audio-equalizer"&gt;audio equalizer&lt;/a&gt; - and &lt;a href="https://en.wikipedia.org/wiki/Dynamic_range_compression"&gt;dynamic range compressor&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#fix-unreliable-playback-on-android"&gt;fix unreliable playback on android&lt;/a&gt; - due to phone / app settings&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#textfile-viewer"&gt;textfile viewer&lt;/a&gt; - with realtime streaming of logfiles and such (&lt;a href="https://a.ocv.me/pub/demo/logtail/"&gt;demo&lt;/a&gt;)&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#markdown-viewer"&gt;markdown viewer&lt;/a&gt; - and there are &lt;em&gt;two&lt;/em&gt; editors 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#markdown-vars"&gt;markdown vars&lt;/a&gt; - dynamic docs with serverside variable expansion&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#other-tricks"&gt;other tricks&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#searching"&gt;searching&lt;/a&gt; - search by size, date, path/name, mp3-tags, ...&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#server-config"&gt;server config&lt;/a&gt; - using arguments or config files, or a mix of both 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#zeroconf"&gt;zeroconf&lt;/a&gt; - announce enabled services on the LAN (&lt;a href="https://user-images.githubusercontent.com/241032/215344737-0eae8d98-9496-4256-9aa8-cd2f6971810d.png"&gt;pic&lt;/a&gt;) 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#mdns"&gt;mdns&lt;/a&gt; - LAN domain-name and feature announcer&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#ssdp"&gt;ssdp&lt;/a&gt; - windows-explorer announcer&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#qr-code"&gt;qr-code&lt;/a&gt; - print a qr-code &lt;a href="https://user-images.githubusercontent.com/241032/194728533-6f00849b-c6ac-43c6-9359-83e454d11e00.png"&gt;(screenshot)&lt;/a&gt; for quick access&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#ftp-server"&gt;ftp server&lt;/a&gt; - an FTP server can be started using &lt;code&gt;--ftp 3921&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#webdav-server"&gt;webdav server&lt;/a&gt; - with read-write support 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#connecting-to-webdav-from-windows"&gt;connecting to webdav from windows&lt;/a&gt; - using the GUI&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#tftp-server"&gt;tftp server&lt;/a&gt; - a TFTP server (read/write) can be started using &lt;code&gt;--tftp 3969&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#smb-server"&gt;smb server&lt;/a&gt; - unsafe, slow, not recommended for wan&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#browser-ux"&gt;browser ux&lt;/a&gt; - tweaking the ui&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#opengraph"&gt;opengraph&lt;/a&gt; - discord and social-media embeds&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#file-deduplication"&gt;file deduplication&lt;/a&gt; - enable symlink-based upload deduplication&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#file-indexing"&gt;file indexing&lt;/a&gt; - enable music search, upload-undo, and better dedup 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#exclude-patterns"&gt;exclude-patterns&lt;/a&gt; - to save some time&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#filesystem-guards"&gt;filesystem guards&lt;/a&gt; - avoid traversing into other filesystems&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#periodic-rescan"&gt;periodic rescan&lt;/a&gt; - filesystem monitoring&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#upload-rules"&gt;upload rules&lt;/a&gt; - set upload rules using volflags&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#compress-uploads"&gt;compress uploads&lt;/a&gt; - files can be autocompressed on upload&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#chmod-and-chown"&gt;chmod and chown&lt;/a&gt; - per-volume filesystem-permissions and ownership&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#other-flags"&gt;other flags&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#database-location"&gt;database location&lt;/a&gt; - in-volume (&lt;code&gt;.hist/up2k.db&lt;/code&gt;, default) or somewhere else&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#metadata-from-audio-files"&gt;metadata from audio files&lt;/a&gt; - set &lt;code&gt;-e2t&lt;/code&gt; to index tags on upload&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#file-parser-plugins"&gt;file parser plugins&lt;/a&gt; - provide custom parsers to index additional tags&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#event-hooks"&gt;event hooks&lt;/a&gt; - trigger a program on uploads, renames etc (&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/bin/hooks/"&gt;examples&lt;/a&gt;) 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#zeromq"&gt;zeromq&lt;/a&gt; - event-hooks can send zeromq messages&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#upload-events"&gt;upload events&lt;/a&gt; - the older, more powerful approach (&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/bin/mtag/"&gt;examples&lt;/a&gt;)&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#handlers"&gt;handlers&lt;/a&gt; - redefine behavior with plugins (&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/bin/handlers/"&gt;examples&lt;/a&gt;)&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#ip-auth"&gt;ip auth&lt;/a&gt; - autologin based on IP range (CIDR) 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#restrict-to-ip"&gt;restrict to ip&lt;/a&gt; - limit a user to certain IP ranges (CIDR)&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#identity-providers"&gt;identity providers&lt;/a&gt; - replace copyparty passwords with oauth and such 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#generic-header-auth"&gt;generic header auth&lt;/a&gt; - other ways to auth by header&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#user-changeable-passwords"&gt;user-changeable passwords&lt;/a&gt; - if permitted, users can change their own passwords&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#using-the-cloud-as-storage"&gt;using the cloud as storage&lt;/a&gt; - connecting to an aws s3 bucket and similar&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#hiding-from-google"&gt;hiding from google&lt;/a&gt; - tell search engines you don't wanna be indexed&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#themes"&gt;themes&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#complete-examples"&gt;complete examples&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#listen-on-port-80-and-443"&gt;listen on port 80 and 443&lt;/a&gt; - become a &lt;em&gt;real&lt;/em&gt; webserver&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#reverse-proxy"&gt;reverse-proxy&lt;/a&gt; - running copyparty next to other websites 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#real-ip"&gt;real-ip&lt;/a&gt; - teaching copyparty how to see client IPs&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#reverse-proxy-performance"&gt;reverse-proxy performance&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#permanent-cloudflare-tunnel"&gt;permanent cloudflare tunnel&lt;/a&gt; - if you have a domain and want to get your copyparty online real quick&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#prometheus"&gt;prometheus&lt;/a&gt; - metrics/stats can be enabled&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#other-extremely-specific-features"&gt;other extremely specific features&lt;/a&gt; - you'll never find a use for these 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#custom-mimetypes"&gt;custom mimetypes&lt;/a&gt; - change the association of a file extension&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#GDPR-compliance"&gt;GDPR compliance&lt;/a&gt; - imagine using copyparty professionally...&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#feature-chickenbits"&gt;feature chickenbits&lt;/a&gt; - buggy feature? rip it out&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#feature-beefybits"&gt;feature beefybits&lt;/a&gt; - force-enable features with known issues on your OS/env&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#packages"&gt;packages&lt;/a&gt; - the party might be closer than you think 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#arch-package"&gt;arch package&lt;/a&gt; - &lt;code&gt;pacman -S copyparty&lt;/code&gt; (in &lt;a href="https://archlinux.org/packages/extra/any/copyparty/"&gt;arch linux extra&lt;/a&gt;)&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#fedora-package"&gt;fedora package&lt;/a&gt; - does not exist yet&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#homebrew-formulae"&gt;homebrew formulae&lt;/a&gt; - &lt;code&gt;brew install copyparty ffmpeg&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#nix-package"&gt;nix package&lt;/a&gt; - &lt;code&gt;nix profile install github:9001/copyparty&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#nixos-module"&gt;nixos module&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#browser-support"&gt;browser support&lt;/a&gt; - TLDR: yes&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#client-examples"&gt;client examples&lt;/a&gt; - interact with copyparty using non-browser clients 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#folder-sync"&gt;folder sync&lt;/a&gt; - sync folders to/from copyparty&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#mount-as-drive"&gt;mount as drive&lt;/a&gt; - a remote copyparty server as a local filesystem&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#android-app"&gt;android app&lt;/a&gt; - upload to copyparty with one tap&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#iOS-shortcuts"&gt;iOS shortcuts&lt;/a&gt; - there is no iPhone app, but&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#performance"&gt;performance&lt;/a&gt; - defaults are usually fine - expect &lt;code&gt;8 GiB/s&lt;/code&gt; download, &lt;code&gt;1 GiB/s&lt;/code&gt; upload 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#client-side"&gt;client-side&lt;/a&gt; - when uploading files&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#security"&gt;security&lt;/a&gt; - there is a &lt;a href="https://discord.gg/25J8CdTT6G"&gt;discord server&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#gotchas"&gt;gotchas&lt;/a&gt; - behavior that might be unexpected&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#cors"&gt;cors&lt;/a&gt; - cross-site request config&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#filekeys"&gt;filekeys&lt;/a&gt; - prevent filename bruteforcing 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#dirkeys"&gt;dirkeys&lt;/a&gt; - share specific folders in a volume&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#password-hashing"&gt;password hashing&lt;/a&gt; - you can hash passwords&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#https"&gt;https&lt;/a&gt; - both HTTP and HTTPS are accepted&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#recovering-from-crashes"&gt;recovering from crashes&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#client-crashes"&gt;client crashes&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#firefox-wsod"&gt;firefox wsod&lt;/a&gt; - firefox 87 can crash during uploads&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#HTTP-API"&gt;HTTP API&lt;/a&gt; - see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/devnotes.md#http-api"&gt;devnotes&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#dependencies"&gt;dependencies&lt;/a&gt; - mandatory deps 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#optional-dependencies"&gt;optional dependencies&lt;/a&gt; - install these to enable bonus features 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#dependency-chickenbits"&gt;dependency chickenbits&lt;/a&gt; - prevent loading an optional dependency&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#optional-gpl-stuff"&gt;optional gpl stuff&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#sfx"&gt;sfx&lt;/a&gt; - the self-contained "binary" (recommended!) 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#copypartyexe"&gt;copyparty.exe&lt;/a&gt; - download &lt;a href="https://github.com/9001/copyparty/releases/latest/download/copyparty.exe"&gt;copyparty.exe&lt;/a&gt; (win8+) or &lt;a href="https://github.com/9001/copyparty/releases/latest/download/copyparty32.exe"&gt;copyparty32.exe&lt;/a&gt; (win7+)&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#zipapp"&gt;zipapp&lt;/a&gt; - another emergency alternative, &lt;a href="https://github.com/9001/copyparty/releases/latest/download/copyparty.pyz"&gt;copyparty.pyz&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#install-on-android"&gt;install on android&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#install-on-iOS"&gt;install on iOS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#reporting-bugs"&gt;reporting bugs&lt;/a&gt; - ideas for context to include, and where to submit them&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#devnotes"&gt;devnotes&lt;/a&gt; - for build instructions etc, see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/devnotes.md"&gt;./docs/devnotes.md&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;quickstart&lt;/h2&gt; 
&lt;p&gt;just run &lt;strong&gt;&lt;a href="https://github.com/9001/copyparty/releases/latest/download/copyparty-sfx.py"&gt;copyparty-sfx.py&lt;/a&gt;&lt;/strong&gt; -- that's it! 🎉&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ℹ️ the sfx is a &lt;a href="https://github.com/9001/copyparty/issues/270"&gt;self-extractor&lt;/a&gt; which unpacks an embedded &lt;code&gt;tar.gz&lt;/code&gt; into &lt;code&gt;$TEMP&lt;/code&gt; -- if this looks too scary, you can use the &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#zipapp"&gt;zipapp&lt;/a&gt; which has slightly worse performance&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;or install through &lt;a href="https://pypi.org/project/copyparty/"&gt;pypi&lt;/a&gt;: &lt;code&gt;python3 -m pip install --user -U copyparty&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;or if you cannot install python, you can use &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#copypartyexe"&gt;copyparty.exe&lt;/a&gt; instead&lt;/li&gt; 
 &lt;li&gt;or install &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#arch-package"&gt;on arch&lt;/a&gt; ╱ &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#nixos-module"&gt;on NixOS&lt;/a&gt; ╱ &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#nix-package"&gt;through nix&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;or if you are on android, &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#install-on-android"&gt;install copyparty in termux&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;or maybe an iPhone or iPad? &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#install-on-iOS"&gt;install in a-Shell on iOS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;or maybe you have a &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/synology-dsm.md"&gt;synology nas / dsm&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;or if you have &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt; installed, run &lt;code&gt;uv tool run copyparty&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;or if your computer is messed up and nothing else works, &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#zipapp"&gt;try the pyz&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;or if your OS is dead, give the &lt;a href="https://a.ocv.me/pub/stuff/edcd001/enterprise-edition/"&gt;bootable flashdrive / cd-rom&lt;/a&gt; a spin&lt;/li&gt; 
 &lt;li&gt;or if you don't trust copyparty yet and want to isolate it a little, then... 
  &lt;ul&gt; 
   &lt;li&gt;...maybe &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/bin/prisonparty.sh"&gt;prisonparty&lt;/a&gt; to create a tiny &lt;a href="https://wiki.archlinux.org/title/Chroot"&gt;chroot&lt;/a&gt; (very portable),&lt;/li&gt; 
   &lt;li&gt;...or &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/bin/bubbleparty.sh"&gt;bubbleparty&lt;/a&gt; to wrap it in &lt;a href="https://github.com/containers/bubblewrap"&gt;bubblewrap&lt;/a&gt; (much better)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;or if you prefer to &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/scripts/docker/"&gt;use docker&lt;/a&gt; 🐋 you can do that too 
  &lt;ul&gt; 
   &lt;li&gt;docker has all deps built-in, so skip this step:&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;enable thumbnails (images/audio/video), media indexing, and audio transcoding by installing some recommended deps:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Alpine:&lt;/strong&gt; &lt;code&gt;apk add py3-pillow ffmpeg&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Debian:&lt;/strong&gt; &lt;code&gt;apt install --no-install-recommends python3-pil ffmpeg&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Fedora:&lt;/strong&gt; rpmfusion + &lt;code&gt;dnf install python3-pillow ffmpeg --allowerasing&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;FreeBSD:&lt;/strong&gt; &lt;code&gt;pkg install py39-sqlite3 py39-pillow ffmpeg&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MacOS:&lt;/strong&gt; &lt;code&gt;port install py-Pillow ffmpeg&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MacOS&lt;/strong&gt; (alternative): &lt;code&gt;brew install pillow ffmpeg&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Windows:&lt;/strong&gt; &lt;code&gt;python -m pip install --user -U Pillow&lt;/code&gt; 
  &lt;ul&gt; 
   &lt;li&gt;install &lt;a href="https://www.python.org/downloads/windows/"&gt;python&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#optional-dependencies"&gt;ffmpeg&lt;/a&gt; manually; do not use &lt;code&gt;winget&lt;/code&gt; or &lt;code&gt;Microsoft Store&lt;/code&gt; (it breaks $PATH)&lt;/li&gt; 
   &lt;li&gt;copyparty.exe comes with &lt;code&gt;Pillow&lt;/code&gt; and only needs &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#optional-dependencies"&gt;ffmpeg&lt;/a&gt; for mediatags/videothumbs&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#optional-dependencies"&gt;optional dependencies&lt;/a&gt; to enable even more features&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;running copyparty without arguments (for example doubleclicking it on Windows) will give everyone read/write access to the current folder; you may want &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#accounts-and-volumes"&gt;accounts and volumes&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;or see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#complete-examples"&gt;some usage examples&lt;/a&gt; for inspiration, or the &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/examples/windows.md"&gt;complete windows example&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;some recommended options:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;-e2dsa&lt;/code&gt; enables general &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#file-indexing"&gt;file indexing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-e2ts&lt;/code&gt; enables audio metadata indexing (needs either FFprobe or Mutagen)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-v /mnt/music:/music:r:rw,foo -a foo:bar&lt;/code&gt; shares &lt;code&gt;/mnt/music&lt;/code&gt; as &lt;code&gt;/music&lt;/code&gt;, &lt;code&gt;r&lt;/code&gt;eadable by anyone, and read-write for user &lt;code&gt;foo&lt;/code&gt;, password &lt;code&gt;bar&lt;/code&gt; 
  &lt;ul&gt; 
   &lt;li&gt;replace &lt;code&gt;:r:rw,foo&lt;/code&gt; with &lt;code&gt;:r,foo&lt;/code&gt; to only make the folder readable by &lt;code&gt;foo&lt;/code&gt; and nobody else&lt;/li&gt; 
   &lt;li&gt;see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#accounts-and-volumes"&gt;accounts and volumes&lt;/a&gt; (or &lt;code&gt;--help-accounts&lt;/code&gt;) for the syntax and other permissions&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;at home&lt;/h3&gt; 
&lt;p&gt;make it accessible over the internet by starting a &lt;a href="https://developers.cloudflare.com/cloudflare-one/connections/connect-networks/do-more-with-tunnels/trycloudflare/"&gt;cloudflare quicktunnel&lt;/a&gt; like so:&lt;/p&gt; 
&lt;p&gt;first download &lt;a href="https://developers.cloudflare.com/cloudflare-one/connections/connect-networks/downloads/"&gt;cloudflared&lt;/a&gt; and then start the tunnel with &lt;code&gt;cloudflared tunnel --url http://127.0.0.1:3923&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;as the tunnel starts, it will show a URL which you can share to let anyone browse your stash or upload files to you&lt;/p&gt; 
&lt;p&gt;but if you have a domain, then you probably want to skip the random autogenerated URL and instead make a &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#permanent-cloudflare-tunnel"&gt;permanent cloudflare tunnel&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;since people will be connecting through cloudflare, run copyparty with &lt;code&gt;--xff-hdr cf-connecting-ip&lt;/code&gt; to detect client IPs correctly&lt;/p&gt; 
&lt;h3&gt;on servers&lt;/h3&gt; 
&lt;p&gt;you may also want these, especially on servers:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/contrib/systemd/copyparty.service"&gt;contrib/systemd/copyparty.service&lt;/a&gt; to run copyparty as a systemd service (see guide inside)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/contrib/systemd/prisonparty.service"&gt;contrib/systemd/prisonparty.service&lt;/a&gt; to run it in a chroot (for extra security)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/contrib/openrc/copyparty"&gt;contrib/openrc/copyparty&lt;/a&gt; to run copyparty on Alpine / Gentoo&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/contrib/rc/copyparty"&gt;contrib/rc/copyparty&lt;/a&gt; to run copyparty on FreeBSD&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#nixos-module"&gt;nixos module&lt;/a&gt; to run copyparty on NixOS hosts&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/contrib/nginx/copyparty.conf"&gt;contrib/nginx/copyparty.conf&lt;/a&gt; to &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#reverse-proxy"&gt;reverse-proxy&lt;/a&gt; behind nginx (for better https)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;and remember to open the ports you want; here's a complete example including every feature copyparty has to offer:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;firewall-cmd --permanent --add-port={80,443,3921,3923,3945,3990}/tcp  # --zone=libvirt
firewall-cmd --permanent --add-port=12000-12099/tcp  # --zone=libvirt
firewall-cmd --permanent --add-port={69,1900,3969,5353}/udp  # --zone=libvirt
firewall-cmd --reload
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;(69:tftp, 1900:ssdp, 3921:ftp, 3923:http/https, 3945:smb, 3969:tftp, 3990:ftps, 5353:mdns, 12000:passive-ftp)&lt;/p&gt; 
&lt;h2&gt;features&lt;/h2&gt; 
&lt;p&gt;also see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/versus.md"&gt;comparison to similar software&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;backend stuff 
  &lt;ul&gt; 
   &lt;li&gt;☑ IPv6 + unix-sockets&lt;/li&gt; 
   &lt;li&gt;☑ &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#performance"&gt;multiprocessing&lt;/a&gt; (actual multithreading)&lt;/li&gt; 
   &lt;li&gt;☑ volumes (mountpoints)&lt;/li&gt; 
   &lt;li&gt;☑ &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#accounts-and-volumes"&gt;accounts&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;☑ &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#ftp-server"&gt;ftp server&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;☑ &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#tftp-server"&gt;tftp server&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;☑ &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#webdav-server"&gt;webdav server&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;☑ &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#smb-server"&gt;smb/cifs server&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;☑ &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#qr-code"&gt;qr-code&lt;/a&gt; for quick access&lt;/li&gt; 
   &lt;li&gt;☑ &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#zeroconf"&gt;upnp / zeroconf / mdns / ssdp&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;☑ &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#event-hooks"&gt;event hooks&lt;/a&gt; / script runner&lt;/li&gt; 
   &lt;li&gt;☑ &lt;a href="https://github.com/9001/copyparty#reverse-proxy"&gt;reverse-proxy support&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;☑ cross-platform (Windows, Linux, Macos, Android, iOS, FreeBSD, arm32/arm64, ppc64le, s390x, risc-v/riscv64)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;upload 
  &lt;ul&gt; 
   &lt;li&gt;☑ basic: plain multipart, ie6 support&lt;/li&gt; 
   &lt;li&gt;☑ &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#uploading"&gt;up2k&lt;/a&gt;: js, resumable, multithreaded 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;no filesize limit!&lt;/strong&gt; even on Cloudflare&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;☑ stash: simple PUT filedropper&lt;/li&gt; 
   &lt;li&gt;☑ filename randomizer&lt;/li&gt; 
   &lt;li&gt;☑ write-only folders&lt;/li&gt; 
   &lt;li&gt;☑ &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#unpost"&gt;unpost&lt;/a&gt;: undo/delete accidental uploads&lt;/li&gt; 
   &lt;li&gt;☑ &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#self-destruct"&gt;self-destruct&lt;/a&gt; (specified server-side or client-side)&lt;/li&gt; 
   &lt;li&gt;☑ &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#race-the-beam"&gt;race the beam&lt;/a&gt; (almost like peer-to-peer)&lt;/li&gt; 
   &lt;li&gt;☑ symlink/discard duplicates (content-matching)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;download 
  &lt;ul&gt; 
   &lt;li&gt;☑ single files in browser&lt;/li&gt; 
   &lt;li&gt;☑ &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#zip-downloads"&gt;folders as zip / tar files&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;☑ &lt;a href="https://github.com/9001/copyparty/tree/hovudstraum/bin#partyfusepy"&gt;FUSE client&lt;/a&gt; (read-only)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;browser 
  &lt;ul&gt; 
   &lt;li&gt;☑ &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#navpane"&gt;navpane&lt;/a&gt; (directory tree sidebar)&lt;/li&gt; 
   &lt;li&gt;☑ file manager (cut/paste, delete, &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#batch-rename"&gt;batch-rename&lt;/a&gt;)&lt;/li&gt; 
   &lt;li&gt;☑ audio player (with &lt;a href="https://user-images.githubusercontent.com/241032/215347492-b4250797-6c90-4e09-9a4c-721edf2fb15c.png"&gt;OS media controls&lt;/a&gt; and opus/mp3 transcoding) 
    &lt;ul&gt; 
     &lt;li&gt;☑ play video files as audio (converted on server)&lt;/li&gt; 
     &lt;li&gt;☑ create and play &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#playlists"&gt;m3u8 playlists&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;☑ image gallery with webm player&lt;/li&gt; 
   &lt;li&gt;☑ &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#textfile-viewer"&gt;textfile browser&lt;/a&gt; with syntax highlighting 
    &lt;ul&gt; 
     &lt;li&gt;☑ realtime streaming of growing files (logfiles and such)&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;☑ &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#thumbnails"&gt;thumbnails&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;☑ ...of images using Pillow, pyvips, or FFmpeg&lt;/li&gt; 
     &lt;li&gt;☑ ...of RAW images using rawpy&lt;/li&gt; 
     &lt;li&gt;☑ ...of videos using FFmpeg&lt;/li&gt; 
     &lt;li&gt;☑ ...of audio (spectrograms) using FFmpeg&lt;/li&gt; 
     &lt;li&gt;☑ cache eviction (max-age; maybe max-size eventually)&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;☑ multilingual UI (english, norwegian, chinese, &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/rice/#translations"&gt;add your own&lt;/a&gt;))&lt;/li&gt; 
   &lt;li&gt;☑ SPA (browse while uploading)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;server indexing 
  &lt;ul&gt; 
   &lt;li&gt;☑ &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#file-search"&gt;locate files by contents&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;☑ search by name/path/date/size&lt;/li&gt; 
   &lt;li&gt;☑ &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#searching"&gt;search by ID3-tags etc.&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;client support 
  &lt;ul&gt; 
   &lt;li&gt;☑ &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#folder-sync"&gt;folder sync&lt;/a&gt; (one-way only; full sync will never be supported)&lt;/li&gt; 
   &lt;li&gt;☑ &lt;a href="https://user-images.githubusercontent.com/241032/215322619-ea5fd606-3654-40ad-94ee-2bc058647bb2.png"&gt;curl-friendly&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;☑ &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#opengraph"&gt;opengraph&lt;/a&gt; (discord embeds)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;markdown 
  &lt;ul&gt; 
   &lt;li&gt;☑ &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#markdown-viewer"&gt;viewer&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;☑ editor (sure why not)&lt;/li&gt; 
   &lt;li&gt;☑ &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#markdown-vars"&gt;variables&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;PS: something missing? post any crazy ideas you've got as a &lt;a href="https://github.com/9001/copyparty/issues/new?assignees=9001&amp;amp;labels=enhancement&amp;amp;template=feature_request.md"&gt;feature request&lt;/a&gt; or &lt;a href="https://github.com/9001/copyparty/discussions/new?category=ideas"&gt;discussion&lt;/a&gt; 🤙&lt;/p&gt; 
&lt;h2&gt;testimonials&lt;/h2&gt; 
&lt;p&gt;small collection of user feedback&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;good enough&lt;/code&gt;, &lt;code&gt;surprisingly correct&lt;/code&gt;, &lt;code&gt;certified good software&lt;/code&gt;, &lt;code&gt;just works&lt;/code&gt;, &lt;code&gt;why&lt;/code&gt;, &lt;code&gt;wow this is better than nextcloud&lt;/code&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;UI просто ужасно. Если буду описывать детально не смогу удержаться в рамках приличий&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;motivations&lt;/h1&gt; 
&lt;p&gt;project goals / philosophy&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;inverse linux philosophy -- do all the things, and do an &lt;em&gt;okay&lt;/em&gt; job 
  &lt;ul&gt; 
   &lt;li&gt;quick drop-in service to get a lot of features in a pinch&lt;/li&gt; 
   &lt;li&gt;some of &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/versus.md"&gt;the alternatives&lt;/a&gt; might be a better fit for you&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;run anywhere, support everything 
  &lt;ul&gt; 
   &lt;li&gt;as many web-browsers and python versions as possible 
    &lt;ul&gt; 
     &lt;li&gt;every browser should at least be able to browse, download, upload files&lt;/li&gt; 
     &lt;li&gt;be a good emergency solution for transferring stuff between ancient boxes&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;minimal dependencies 
    &lt;ul&gt; 
     &lt;li&gt;but optional dependencies adding bonus-features are ok&lt;/li&gt; 
     &lt;li&gt;everything being plaintext makes it possible to proofread for malicious code&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;no preparations / setup necessary, just run the sfx (which is also plaintext)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;adaptable, malleable, hackable 
  &lt;ul&gt; 
   &lt;li&gt;no build steps; modify the js/python without needing node.js or anything like that&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;becoming rich is specifically &lt;em&gt;not&lt;/em&gt; a motivation, but if you wanna donate then see my &lt;a href="https://github.com/9001"&gt;github profile&lt;/a&gt; regarding donations for my FOSS stuff in general (also THANKS!)&lt;/p&gt; 
&lt;h2&gt;notes&lt;/h2&gt; 
&lt;p&gt;general notes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;paper-printing is affected by dark/light-mode! use lightmode for color, darkmode for grayscale 
  &lt;ul&gt; 
   &lt;li&gt;because no browsers currently implement the media-query to do this properly orz&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;browser-specific:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;iPhone/iPad: use Firefox to download files&lt;/li&gt; 
 &lt;li&gt;Android-Chrome: increase "parallel uploads" for higher speed (android bug)&lt;/li&gt; 
 &lt;li&gt;Android-Firefox: takes a while to select files (their fix for ☝️)&lt;/li&gt; 
 &lt;li&gt;Desktop-Firefox: &lt;del&gt;may use gigabytes of RAM if your files are massive&lt;/del&gt; &lt;em&gt;seems to be OK now&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;Desktop-Firefox: &lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1792598"&gt;may stop you from unplugging USB flashdrives&lt;/a&gt; until you visit &lt;code&gt;about:memory&lt;/code&gt; and click &lt;code&gt;Minimize memory usage&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;server-os-specific:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;RHEL8 / Rocky8: you can run copyparty using &lt;code&gt;/usr/libexec/platform-python&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;server notes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;pypy is supported but regular cpython is faster if you enable the database&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;bugs&lt;/h1&gt; 
&lt;p&gt;roughly sorted by chance of encounter&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;general:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;--th-ff-jpg&lt;/code&gt; may fix video thumbnails on some FFmpeg versions (macos, some linux)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;--th-ff-swr&lt;/code&gt; may fix audio thumbnails on some FFmpeg versions&lt;/li&gt; 
   &lt;li&gt;if the &lt;code&gt;up2k.db&lt;/code&gt; (filesystem index) is on a samba-share or network disk, you'll get unpredictable behavior if the share is disconnected for a bit 
    &lt;ul&gt; 
     &lt;li&gt;use &lt;code&gt;--hist&lt;/code&gt; or the &lt;code&gt;hist&lt;/code&gt; volflag (&lt;code&gt;-v [...]:c,hist=/tmp/foo&lt;/code&gt;) to place the db and thumbnails on a local disk instead&lt;/li&gt; 
     &lt;li&gt;or, if you only want to move the db (and not the thumbnails), then use &lt;code&gt;--dbpath&lt;/code&gt; or the &lt;code&gt;dbpath&lt;/code&gt; volflag&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;all volumes must exist / be available on startup; up2k (mtp especially) gets funky otherwise&lt;/li&gt; 
   &lt;li&gt;probably more, pls let me know&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;python 3.4 and older (including 2.7):&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;many rare and exciting edge-cases because &lt;a href="https://peps.python.org/pep-0475/"&gt;python didn't handle EINTR yet&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;downloads from copyparty may suddenly fail, but uploads &lt;em&gt;should&lt;/em&gt; be fine&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;python 2.7 on Windows:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;cannot index non-ascii filenames with &lt;code&gt;-e2d&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;cannot handle filenames with mojibake&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;if you have a new exciting bug to share, see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#reporting-bugs"&gt;reporting bugs&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;not my bugs&lt;/h2&gt; 
&lt;p&gt;same order here too&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://bugs.chromium.org/p/chromium/issues/detail?id=1317069"&gt;Chrome issue 1317069&lt;/a&gt; -- if you try to upload a folder which contains symlinks by dragging it into the browser, the symlinked files will not get uploaded&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://bugs.chromium.org/p/chromium/issues/detail?id=1352210"&gt;Chrome issue 1352210&lt;/a&gt; -- plaintext http may be faster at filehashing than https (but also extremely CPU-intensive)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://issues.chromium.org/issues/383568268"&gt;Chrome issue 383568268&lt;/a&gt; -- filereaders in webworkers can OOM / crash the browser-tab&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;copyparty has a workaround which seems to work well enough&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1790500"&gt;Firefox issue 1790500&lt;/a&gt; -- entire browser can crash after uploading ~4000 small files&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Android: music playback randomly stops due to &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#fix-unreliable-playback-on-android"&gt;battery usage settings&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;iPhones: the volume control doesn't work because &lt;a href="https://developer.apple.com/library/archive/documentation/AudioVideo/Conceptual/Using_HTML5_Audio_Video/Device-SpecificConsiderations/Device-SpecificConsiderations.html#//apple_ref/doc/uid/TP40009523-CH5-SW11"&gt;apple doesn't want it to&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;AudioContext&lt;/code&gt; will probably never be a viable workaround as apple introduces new issues faster than they fix current ones&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;iPhones: music volume goes on a rollercoaster during song changes&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;nothing I can do about it because &lt;code&gt;AudioContext&lt;/code&gt; is still broken in safari&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;iPhones: the preload feature (in the media-player-options tab) can cause a tiny audio glitch 20sec before the end of each song, but disabling it may cause worse iOS bugs to appear instead&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;just a hunch, but disabling preloading may cause playback to stop entirely, or possibly mess with bluetooth speakers&lt;/li&gt; 
   &lt;li&gt;tried to add a tooltip regarding this but looks like apple broke my tooltips&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;iPhones: preloaded awo files make safari log MEDIA_ERR_NETWORK errors as playback starts, but the song plays just fine so eh whatever&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;awo, opus-weba, is apple's new take on opus support, replacing opus-caf which was technically limited to cbr opus&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;iPhones: preloading another awo file may cause playback to stop&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;can be somewhat mitigated with &lt;code&gt;mp.au.play()&lt;/code&gt; in &lt;code&gt;mp.onpreload&lt;/code&gt; but that can hit a race condition in safari that starts playing the same audio object twice in parallel...&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Windows: folders cannot be accessed if the name ends with &lt;code&gt;.&lt;/code&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;python or windows bug&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Windows: msys2-python 3.8.6 occasionally throws &lt;code&gt;RuntimeError: release unlocked lock&lt;/code&gt; when leaving a scoped mutex in up2k&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;this is an msys2 bug, the regular windows edition of python is fine&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;VirtualBox: sqlite throws &lt;code&gt;Disk I/O Error&lt;/code&gt; when running in a VM and the up2k database is in a vboxsf&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;use &lt;code&gt;--hist&lt;/code&gt; or the &lt;code&gt;hist&lt;/code&gt; volflag (&lt;code&gt;-v [...]:c,hist=/tmp/foo&lt;/code&gt;) to place the db and thumbnails inside the vm instead 
    &lt;ul&gt; 
     &lt;li&gt;or, if you only want to move the db (and not the thumbnails), then use &lt;code&gt;--dbpath&lt;/code&gt; or the &lt;code&gt;dbpath&lt;/code&gt; volflag&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;also happens on mergerfs, so put the db elsewhere&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Ubuntu: dragging files from certain folders into firefox or chrome is impossible&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;due to snap security policies -- see &lt;code&gt;snap connections firefox&lt;/code&gt; for the allowlist, &lt;code&gt;removable-media&lt;/code&gt; permits all of &lt;code&gt;/mnt&lt;/code&gt; and &lt;code&gt;/media&lt;/code&gt; apparently&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;breaking changes&lt;/h1&gt; 
&lt;p&gt;upgrade notes&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;1.9.16&lt;/code&gt; (2023-11-04): 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;--stats&lt;/code&gt;/prometheus: &lt;code&gt;cpp_bans&lt;/code&gt; renamed to &lt;code&gt;cpp_active_bans&lt;/code&gt;, and that + &lt;code&gt;cpp_uptime&lt;/code&gt; are gauges&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;1.6.0&lt;/code&gt; (2023-01-29): 
  &lt;ul&gt; 
   &lt;li&gt;http-api: delete/move is now &lt;code&gt;POST&lt;/code&gt; instead of &lt;code&gt;GET&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;everything other than &lt;code&gt;GET&lt;/code&gt; and &lt;code&gt;HEAD&lt;/code&gt; must pass &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#cors"&gt;cors validation&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;1.5.0&lt;/code&gt; (2022-12-03): &lt;a href="https://github.com/9001/copyparty/commit/54e1c8d261df"&gt;new chunksize formula&lt;/a&gt; for files larger than 128 GiB 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;users:&lt;/strong&gt; upgrade to the latest &lt;a href="https://github.com/9001/copyparty/raw/hovudstraum/bin/u2c.py"&gt;cli uploader&lt;/a&gt; if you use that&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;devs:&lt;/strong&gt; update third-party up2k clients (if those even exist)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;FAQ&lt;/h1&gt; 
&lt;p&gt;"frequently" asked questions&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;CopyParty?&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;nope! the name is either copyparty (all-lowercase) or Copyparty -- it's &lt;a href="https://en.wiktionary.org/wiki/copyparty"&gt;one word&lt;/a&gt; after all :&amp;gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;can I change the 🌲 spinning pine-tree loading animation?&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://github.com/9001/copyparty/tree/hovudstraum/docs/rice#boring-loader-spinner"&gt;yeah...&lt;/a&gt; :-(&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;is it possible to block read-access to folders unless you know the exact URL for a particular file inside?&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;yes, using the &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#accounts-and-volumes"&gt;&lt;code&gt;g&lt;/code&gt; permission&lt;/a&gt;, see the examples there&lt;/li&gt; 
   &lt;li&gt;you can also do this with linux filesystem permissions; &lt;code&gt;chmod 111 music&lt;/code&gt; will make it possible to access files and folders inside the &lt;code&gt;music&lt;/code&gt; folder but not list the immediate contents -- also works with other software, not just copyparty&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;can I link someone to a password-protected volume/file by including the password in the URL?&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;yes, by adding &lt;code&gt;?pw=hunter2&lt;/code&gt; to the end; replace &lt;code&gt;?&lt;/code&gt; with &lt;code&gt;&amp;amp;&lt;/code&gt; if there are parameters in the URL already, meaning it contains a &lt;code&gt;?&lt;/code&gt; near the end 
    &lt;ul&gt; 
     &lt;li&gt;if you have enabled &lt;code&gt;--usernames&lt;/code&gt; then do &lt;code&gt;?pw=username:password&lt;/code&gt; instead&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;how do I stop &lt;code&gt;.hist&lt;/code&gt; folders from appearing everywhere on my HDD?&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;by default, a &lt;code&gt;.hist&lt;/code&gt; folder is created inside each volume for the filesystem index, thumbnails, audio transcodes, and markdown document history. Use the &lt;code&gt;--hist&lt;/code&gt; global-option or the &lt;code&gt;hist&lt;/code&gt; volflag to move it somewhere else; see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#database-location"&gt;database location&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;can I make copyparty download a file to my server if I give it a URL?&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;yes, using &lt;a href="https://github.com/9001/copyparty/raw/hovudstraum/bin/hooks/wget.py"&gt;hooks&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;firefox refuses to connect over https, saying "Secure Connection Failed" or "SEC_ERROR_BAD_SIGNATURE", but the usual button to "Accept the Risk and Continue" is not shown&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;firefox has corrupted its certstore; fix this by exiting firefox, then find and delete the file named &lt;code&gt;cert9.db&lt;/code&gt; somewhere in your firefox profile folder&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;the server keeps saying &lt;code&gt;thank you for playing&lt;/code&gt; when I try to access the website&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;you've gotten banned for malicious traffic! if this happens by mistake, and you're running a reverse-proxy and/or something like cloudflare, see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#real-ip"&gt;real-ip&lt;/a&gt; on how to fix this&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;copyparty seems to think I am using http, even though the URL is https&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;your reverse-proxy is not sending the &lt;code&gt;X-Forwarded-Proto: https&lt;/code&gt; header; this could be because your reverse-proxy itself is confused. Ensure that none of the intermediates (such as cloudflare) are terminating https before the traffic hits your entrypoint&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;thumbnails are broken (you get a colorful square which says the filetype instead)&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;you need to install &lt;code&gt;FFmpeg&lt;/code&gt; or &lt;code&gt;Pillow&lt;/code&gt;; see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#thumbnails"&gt;thumbnails&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;thumbnails are broken (some images appear, but other files just get a blank box, and/or the broken-image placeholder)&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;probably due to a reverse-proxy messing with the request URLs and stripping the query parameters (&lt;code&gt;?th=w&lt;/code&gt;), so check your URL rewrite rules&lt;/li&gt; 
   &lt;li&gt;could also be due to incorrect caching settings in reverse-proxies and/or CDNs, so make sure that nothing is set to ignore the query string&lt;/li&gt; 
   &lt;li&gt;could also be due to misbehaving privacy-related browser extensions, so try to disable those&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;i want to learn python and/or programming and am considering looking at the copyparty source code in that occasion&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;pre&gt;&lt;code class="language-bash"&gt; _|  _      __   _  _|_
(_| (_)     | | (_)  |_
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;accounts and volumes&lt;/h1&gt; 
&lt;p&gt;per-folder, per-user permissions - if your setup is getting complex, consider making a &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/example.conf"&gt;config file&lt;/a&gt; instead of using arguments&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;much easier to manage, and you can modify the config at runtime with &lt;code&gt;systemctl reload copyparty&lt;/code&gt; or more conveniently using the &lt;code&gt;[reload cfg]&lt;/code&gt; button in the control-panel (if the user has &lt;code&gt;a&lt;/code&gt;/admin in any volume) 
  &lt;ul&gt; 
   &lt;li&gt;changes to the &lt;code&gt;[global]&lt;/code&gt; config section requires a restart to take effect&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;a quick summary can be seen using &lt;code&gt;--help-accounts&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;configuring accounts/volumes with arguments:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;-a usr:pwd&lt;/code&gt; adds account &lt;code&gt;usr&lt;/code&gt; with password &lt;code&gt;pwd&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-v .::r&lt;/code&gt; adds current-folder &lt;code&gt;.&lt;/code&gt; as the webroot, &lt;code&gt;r&lt;/code&gt;eadable by anyone 
  &lt;ul&gt; 
   &lt;li&gt;the syntax is &lt;code&gt;-v src:dst:perm:perm:...&lt;/code&gt; so local-path, url-path, and one or more permissions to set&lt;/li&gt; 
   &lt;li&gt;granting the same permissions to multiple accounts:&lt;br /&gt; &lt;code&gt;-v .::r,usr1,usr2:rw,usr3,usr4&lt;/code&gt; = usr1/2 read-only, 3/4 read-write&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;permissions:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;r&lt;/code&gt; (read): browse folder contents, download files, download as zip/tar, see filekeys/dirkeys&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;w&lt;/code&gt; (write): upload files, move/copy files &lt;em&gt;into&lt;/em&gt; this folder&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;m&lt;/code&gt; (move): move files/folders &lt;em&gt;from&lt;/em&gt; this folder&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;d&lt;/code&gt; (delete): delete files/folders&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;.&lt;/code&gt; (dots): user can ask to show dotfiles in directory listings&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;g&lt;/code&gt; (get): only download files, cannot see folder contents or zip/tar&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;G&lt;/code&gt; (upget): same as &lt;code&gt;g&lt;/code&gt; except uploaders get to see their own &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#filekeys"&gt;filekeys&lt;/a&gt; (see &lt;code&gt;fk&lt;/code&gt; in examples below)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;h&lt;/code&gt; (html): same as &lt;code&gt;g&lt;/code&gt; except folders return their index.html, and filekeys are not necessary for index.html&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;a&lt;/code&gt; (admin): can see upload time, uploader IPs, config-reload&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;A&lt;/code&gt; ("all"): same as &lt;code&gt;rwmda.&lt;/code&gt; (read/write/move/delete/admin/dotfiles)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;examples:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;add accounts named u1, u2, u3 with passwords p1, p2, p3: &lt;code&gt;-a u1:p1 -a u2:p2 -a u3:p3&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;make folder &lt;code&gt;/srv&lt;/code&gt; the root of the filesystem, read-only by anyone: &lt;code&gt;-v /srv::r&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;make folder &lt;code&gt;/mnt/music&lt;/code&gt; available at &lt;code&gt;/music&lt;/code&gt;, read-only for u1 and u2, read-write for u3: &lt;code&gt;-v /mnt/music:music:r,u1,u2:rw,u3&lt;/code&gt; 
  &lt;ul&gt; 
   &lt;li&gt;unauthorized users accessing the webroot can see that the &lt;code&gt;music&lt;/code&gt; folder exists, but cannot open it&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;make folder &lt;code&gt;/mnt/incoming&lt;/code&gt; available at &lt;code&gt;/inc&lt;/code&gt;, write-only for u1, read-move for u2: &lt;code&gt;-v /mnt/incoming:inc:w,u1:rm,u2&lt;/code&gt; 
  &lt;ul&gt; 
   &lt;li&gt;unauthorized users accessing the webroot can see that the &lt;code&gt;inc&lt;/code&gt; folder exists, but cannot open it&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;u1&lt;/code&gt; can open the &lt;code&gt;inc&lt;/code&gt; folder, but cannot see the contents, only upload new files to it&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;u2&lt;/code&gt; can browse it and move files &lt;em&gt;from&lt;/em&gt; &lt;code&gt;/inc&lt;/code&gt; into any folder where &lt;code&gt;u2&lt;/code&gt; has write-access&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;make folder &lt;code&gt;/mnt/ss&lt;/code&gt; available at &lt;code&gt;/i&lt;/code&gt;, read-write for u1, get-only for everyone else, and enable filekeys: &lt;code&gt;-v /mnt/ss:i:rw,u1:g:c,fk=4&lt;/code&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;c,fk=4&lt;/code&gt; sets the &lt;code&gt;fk&lt;/code&gt; (&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#filekeys"&gt;filekey&lt;/a&gt;) volflag to 4, meaning each file gets a 4-character accesskey&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;u1&lt;/code&gt; can upload files, browse the folder, and see the generated filekeys&lt;/li&gt; 
   &lt;li&gt;other users cannot browse the folder, but can access the files if they have the full file URL with the filekey&lt;/li&gt; 
   &lt;li&gt;replacing the &lt;code&gt;g&lt;/code&gt; permission with &lt;code&gt;wg&lt;/code&gt; would let anonymous users upload files, but not see the required filekey to access it&lt;/li&gt; 
   &lt;li&gt;replacing the &lt;code&gt;g&lt;/code&gt; permission with &lt;code&gt;wG&lt;/code&gt; would let anonymous users upload files, receiving a working direct link in return&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;if you want to grant access to all users who are logged in, the group &lt;code&gt;acct&lt;/code&gt; will always contain all known users, so for example &lt;code&gt;-v /mnt/music:music:r,@acct&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;anyone trying to bruteforce a password gets banned according to &lt;code&gt;--ban-pw&lt;/code&gt;; default is 24h ban for 9 failed attempts in 1 hour&lt;/p&gt; 
&lt;p&gt;and if you want to use config files instead of commandline args (good!) then here's the same examples as a configfile; save it as &lt;code&gt;foobar.conf&lt;/code&gt; and use it like this: &lt;code&gt;python copyparty-sfx.py -c foobar.conf&lt;/code&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;you can also &lt;code&gt;PRTY_CONFIG=foobar.conf python copyparty-sfx.py&lt;/code&gt; (convenient in docker etc)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;[accounts]
  u1: p1  # create account "u1" with password "p1"
  u2: p2  #  (note that comments must have
  u3: p3  #   two spaces before the # sign)

[groups]
  g1: u1, u2  # create a group

[/]     # this URL will be mapped to...
  /srv  # ...this folder on the server filesystem
  accs:
    r: *  # read-only for everyone, no account necessary

[/music]       # create another volume at this URL,
  /mnt/music   # which is mapped to this folder
  accs:
    r: u1, u2  # only these accounts can read,
    r: @g1     # (exactly the same, just with a group instead)
    r: @acct   # (alternatively, ALL users who are logged in)
    rw: u3     # and only u3 can read-write

[/inc]
  /mnt/incoming
  accs:
    w: u1   # u1 can upload but not see/download any files,
    rm: u2  # u2 can browse + move files out of this volume

[/i]
  /mnt/ss
  accs:
    rw: u1  # u1 can read-write,
    g: *    # everyone can access files if they know the URL
  flags:
    fk: 4   # each file URL will have a 4-character password
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;shadowing&lt;/h2&gt; 
&lt;p&gt;hiding specific subfolders by mounting another volume on top of them&lt;/p&gt; 
&lt;p&gt;for example &lt;code&gt;-v /mnt::r -v /var/empty:web/certs:r&lt;/code&gt; mounts the server folder &lt;code&gt;/mnt&lt;/code&gt; as the webroot, but another volume is mounted at &lt;code&gt;/web/certs&lt;/code&gt; -- so visitors can only see the contents of &lt;code&gt;/mnt&lt;/code&gt; and &lt;code&gt;/mnt/web&lt;/code&gt; (at URLs &lt;code&gt;/&lt;/code&gt; and &lt;code&gt;/web&lt;/code&gt;), but not &lt;code&gt;/mnt/web/certs&lt;/code&gt; because URL &lt;code&gt;/web/certs&lt;/code&gt; is mapped to &lt;code&gt;/var/empty&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;the example config file right above this section may explain this better; the first volume &lt;code&gt;/&lt;/code&gt; is mapped to &lt;code&gt;/srv&lt;/code&gt; which means &lt;a href="http://127.0.0.1:3923/music"&gt;http://127.0.0.1:3923/music&lt;/a&gt; would try to read &lt;code&gt;/srv/music&lt;/code&gt; on the server filesystem, but since there's another volume at &lt;code&gt;/music&lt;/code&gt; mapped to &lt;code&gt;/mnt/music&lt;/code&gt; then it'll go to &lt;code&gt;/mnt/music&lt;/code&gt; instead&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ℹ️ this also works for single files, because files can also be volumes&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;dotfiles&lt;/h2&gt; 
&lt;p&gt;unix-style hidden files/folders by starting the name with a dot&lt;/p&gt; 
&lt;p&gt;anyone can access these if they know the name, but they normally don't appear in directory listings&lt;/p&gt; 
&lt;p&gt;a client can request to see dotfiles in directory listings if global option &lt;code&gt;-ed&lt;/code&gt; is specified, or the volume has volflag &lt;code&gt;dots&lt;/code&gt;, or the user has permission &lt;code&gt;.&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;dotfiles do not appear in search results unless one of the above is true, &lt;strong&gt;and&lt;/strong&gt; the global option / volflag &lt;code&gt;dotsrch&lt;/code&gt; is set&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;even if user has permission to see dotfiles, they are default-hidden unless &lt;code&gt;--see-dots&lt;/code&gt; is set, and/or user has enabled the &lt;code&gt;dotfiles&lt;/code&gt; option in the settings tab&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;config file example, where the same permission to see dotfiles is given in two different ways just for reference:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;[/foo]
  /srv/foo
  accs:
    r.: ed   # user "ed" has read-access + dot-access in this volume;
             # dotfiles are visible in listings, but not in searches
  flags:
    dotsrch  # dotfiles will now appear in search results too
    dots     # another way to let everyone see dotfiles in this vol
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;the browser&lt;/h1&gt; 
&lt;p&gt;accessing a copyparty server using a web-browser&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/241032/192042695-522b3ec7-6845-494a-abdb-d1c0d0e23801.png" alt="copyparty-browser-fs8" /&gt;&lt;/p&gt; 
&lt;h2&gt;tabs&lt;/h2&gt; 
&lt;p&gt;the main tabs in the ui&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;[🔎]&lt;/code&gt; &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#searching"&gt;search&lt;/a&gt; by size, date, path/name, mp3-tags ...&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[🧯]&lt;/code&gt; &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#unpost"&gt;unpost&lt;/a&gt;: undo/delete accidental uploads&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[🚀]&lt;/code&gt; and &lt;code&gt;[🎈]&lt;/code&gt; are the &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#uploading"&gt;uploaders&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[📂]&lt;/code&gt; mkdir: create directories&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[📝]&lt;/code&gt; new-md: create a new markdown document&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[📟]&lt;/code&gt; send-msg: either to server-log or into textfiles if &lt;code&gt;--urlform save&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[🎺]&lt;/code&gt; audio-player config options&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[⚙️]&lt;/code&gt; general client config options&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;hotkeys&lt;/h2&gt; 
&lt;p&gt;the browser has the following hotkeys (always qwerty)&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;?&lt;/code&gt; show hotkeys help&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;B&lt;/code&gt; toggle breadcrumbs / &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#navpane"&gt;navpane&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;I/K&lt;/code&gt; prev/next folder&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;M&lt;/code&gt; parent folder (or unexpand current)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;V&lt;/code&gt; toggle folders / textfiles in the navpane&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;G&lt;/code&gt; toggle list / &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#thumbnails"&gt;grid view&lt;/a&gt; -- same as &lt;code&gt;田&lt;/code&gt; bottom-right&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;T&lt;/code&gt; toggle thumbnails / icons&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ESC&lt;/code&gt; close various things&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ctrl-K&lt;/code&gt; delete selected files/folders&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ctrl-X&lt;/code&gt; cut selected files/folders&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ctrl-C&lt;/code&gt; copy selected files/folders to clipboard&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ctrl-V&lt;/code&gt; paste (move/copy)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Y&lt;/code&gt; download selected files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;F2&lt;/code&gt; &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#batch-rename"&gt;rename&lt;/a&gt; selected file/folder&lt;/li&gt; 
 &lt;li&gt;when a file/folder is selected (in not-grid-view): 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;Up/Down&lt;/code&gt; move cursor&lt;/li&gt; 
   &lt;li&gt;shift+&lt;code&gt;Up/Down&lt;/code&gt; select and move cursor&lt;/li&gt; 
   &lt;li&gt;ctrl+&lt;code&gt;Up/Down&lt;/code&gt; move cursor and scroll viewport&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;Space&lt;/code&gt; toggle file selection&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;Ctrl-A&lt;/code&gt; toggle select all&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;when a textfile is open: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;I/K&lt;/code&gt; prev/next textfile&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;S&lt;/code&gt; toggle selection of open file&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;M&lt;/code&gt; close textfile&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;when playing audio: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;J/L&lt;/code&gt; prev/next song&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;U/O&lt;/code&gt; skip 10sec back/forward&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;0..9&lt;/code&gt; jump to 0%..90%&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;P&lt;/code&gt; play/pause (also starts playing the folder)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;Y&lt;/code&gt; download file&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;when viewing images / playing videos: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;J/L, Left/Right&lt;/code&gt; prev/next file&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;Home/End&lt;/code&gt; first/last file&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;F&lt;/code&gt; toggle fullscreen&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;S&lt;/code&gt; toggle selection&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;R&lt;/code&gt; rotate clockwise (shift=ccw)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;Y&lt;/code&gt; download file&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;Esc&lt;/code&gt; close viewer&lt;/li&gt; 
   &lt;li&gt;videos: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;code&gt;U/O&lt;/code&gt; skip 10sec back/forward&lt;/li&gt; 
     &lt;li&gt;&lt;code&gt;0..9&lt;/code&gt; jump to 0%..90%&lt;/li&gt; 
     &lt;li&gt;&lt;code&gt;P/K/Space&lt;/code&gt; play/pause&lt;/li&gt; 
     &lt;li&gt;&lt;code&gt;M&lt;/code&gt; mute&lt;/li&gt; 
     &lt;li&gt;&lt;code&gt;C&lt;/code&gt; continue playing next video&lt;/li&gt; 
     &lt;li&gt;&lt;code&gt;V&lt;/code&gt; loop entire file&lt;/li&gt; 
     &lt;li&gt;&lt;code&gt;[&lt;/code&gt; loop range (start)&lt;/li&gt; 
     &lt;li&gt;&lt;code&gt;]&lt;/code&gt; loop range (end)&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;when the navpane is open: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;A/D&lt;/code&gt; adjust tree width&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;in the &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#thumbnails"&gt;grid view&lt;/a&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;S&lt;/code&gt; toggle multiselect&lt;/li&gt; 
   &lt;li&gt;shift+&lt;code&gt;A/D&lt;/code&gt; zoom&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;in the markdown editor: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;^s&lt;/code&gt; save&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;^h&lt;/code&gt; header&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;^k&lt;/code&gt; autoformat table&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;^u&lt;/code&gt; jump to next unicode character&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;^e&lt;/code&gt; toggle editor / preview&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;^up, ^down&lt;/code&gt; jump paragraphs&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;navpane&lt;/h2&gt; 
&lt;p&gt;switching between breadcrumbs or navpane&lt;/p&gt; 
&lt;p&gt;click the &lt;code&gt;🌲&lt;/code&gt; or pressing the &lt;code&gt;B&lt;/code&gt; hotkey to toggle between breadcrumbs path (default), or a navpane (tree-browser sidebar thing)&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;[+]&lt;/code&gt; and &lt;code&gt;[-]&lt;/code&gt; (or hotkeys &lt;code&gt;A&lt;/code&gt;/&lt;code&gt;D&lt;/code&gt;) adjust the size&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[🎯]&lt;/code&gt; jumps to the currently open folder&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[📃]&lt;/code&gt; toggles between showing folders and textfiles&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[📌]&lt;/code&gt; shows the name of all parent folders in a docked panel&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[a]&lt;/code&gt; toggles automatic widening as you go deeper&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[↵]&lt;/code&gt; toggles wordwrap&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[👀]&lt;/code&gt; show full name on hover (if wordwrap is off)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;thumbnails&lt;/h2&gt; 
&lt;p&gt;press &lt;code&gt;g&lt;/code&gt; or &lt;code&gt;田&lt;/code&gt; to toggle grid-view instead of the file listing and &lt;code&gt;t&lt;/code&gt; toggles icons / thumbnails&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;can be made default globally with &lt;code&gt;--grid&lt;/code&gt; or per-volume with volflag &lt;code&gt;grid&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;enable by adding &lt;code&gt;?imgs&lt;/code&gt; to a link, or disable with &lt;code&gt;?imgs=0&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/241032/129636211-abd20fa2-a953-4366-9423-1c88ebb96ba9.png" alt="copyparty-thumbs-fs8" /&gt;&lt;/p&gt; 
&lt;p&gt;it does static images with Pillow / pyvips / FFmpeg, and uses FFmpeg for video files, so you may want to &lt;code&gt;--no-thumb&lt;/code&gt; or maybe just &lt;code&gt;--no-vthumb&lt;/code&gt; depending on how dangerous your users are&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;pyvips is 3x faster than Pillow, Pillow is 3x faster than FFmpeg&lt;/li&gt; 
 &lt;li&gt;disable thumbnails for specific volumes with volflag &lt;code&gt;dthumb&lt;/code&gt; for all, or &lt;code&gt;dvthumb&lt;/code&gt; / &lt;code&gt;dathumb&lt;/code&gt; / &lt;code&gt;dithumb&lt;/code&gt; for video/audio/images only&lt;/li&gt; 
 &lt;li&gt;for installing FFmpeg on windows, see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#optional-dependencies"&gt;optional dependencies&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;audio files are converted into spectrograms using FFmpeg unless you &lt;code&gt;--no-athumb&lt;/code&gt; (and some FFmpeg builds may need &lt;code&gt;--th-ff-swr&lt;/code&gt;)&lt;/p&gt; 
&lt;p&gt;images with the following names (see &lt;code&gt;--th-covers&lt;/code&gt;) become the thumbnail of the folder they're in: &lt;code&gt;folder.png&lt;/code&gt;, &lt;code&gt;folder.jpg&lt;/code&gt;, &lt;code&gt;cover.png&lt;/code&gt;, &lt;code&gt;cover.jpg&lt;/code&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;the order is significant, so if both &lt;code&gt;cover.png&lt;/code&gt; and &lt;code&gt;folder.jpg&lt;/code&gt; exist in a folder, it will pick the first matching &lt;code&gt;--th-covers&lt;/code&gt; entry (&lt;code&gt;folder.jpg&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;and, if you enable &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#file-indexing"&gt;file indexing&lt;/a&gt;, it will also try those names as dotfiles (&lt;code&gt;.folder.jpg&lt;/code&gt; and so), and then fallback on the first picture in the folder (if it has any pictures at all)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;enabling &lt;code&gt;multiselect&lt;/code&gt; lets you click files to select them, and then shift-click another file for range-select&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;multiselect&lt;/code&gt; is mostly intended for phones/tablets, but the &lt;code&gt;sel&lt;/code&gt; option in the &lt;code&gt;[⚙️] settings&lt;/code&gt; tab is better suited for desktop use, allowing selection by CTRL-clicking and range-selection with SHIFT-click, all without affecting regular clicking 
  &lt;ul&gt; 
   &lt;li&gt;the &lt;code&gt;sel&lt;/code&gt; option can be made default globally with &lt;code&gt;--gsel&lt;/code&gt; or per-volume with volflag &lt;code&gt;gsel&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;to show &lt;code&gt;/icons/exe.png&lt;/code&gt; and &lt;code&gt;/icons/elf.gif&lt;/code&gt; as the thumbnail for all &lt;code&gt;.exe&lt;/code&gt; and &lt;code&gt;.elf&lt;/code&gt; files respectively, do this: &lt;code&gt;--ext-th=exe=/icons/exe.png --ext-th=elf=/icons/elf.gif&lt;/code&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;optionally as separate volflags for each mapping; see config file example below&lt;/li&gt; 
 &lt;li&gt;the supported image formats are &lt;a href="https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Image_types"&gt;jpg, png, gif, webp, ico&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;be careful with svg; chrome will crash if you have too many unique svg files showing on the same page (the limit is 250 or so) -- showing the same handful of svg files thousands of times is ok however&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;config file example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;[global]
  no-thumb   # disable ALL thumbnails and audio transcoding
  no-vthumb  # only disable video thumbnails

[/music]
  /mnt/nas/music
  accs:
    r: *     # everyone can read
  flags:
    dthumb   # disable ALL thumbnails and audio transcoding
    dvthumb  # only disable video thumbnails
    ext-th:  exe=/ico/exe.png  # /ico/exe.png is the thumbnail of *.exe
    ext-th:  elf=/ico/elf.gif  # ...and /ico/elf.gif is used for *.elf
    th-covers:  folder.png,folder.jpg,cover.png,cover.jpg  # the default
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;zip downloads&lt;/h2&gt; 
&lt;p&gt;download folders (or file selections) as &lt;code&gt;zip&lt;/code&gt; or &lt;code&gt;tar&lt;/code&gt; files&lt;/p&gt; 
&lt;p&gt;select which type of archive you want in the &lt;code&gt;[⚙️] config&lt;/code&gt; tab:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;name&lt;/th&gt; 
   &lt;th&gt;url-suffix&lt;/th&gt; 
   &lt;th&gt;description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;tar&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;?tar&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;plain gnutar, works great with &lt;code&gt;curl | tar -xv&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;pax&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;?tar=pax&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;pax-format tar, futureproof, not as fast&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;tgz&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;?tar=gz&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;gzip compressed gnu-tar (slow), for &lt;code&gt;curl | tar -xvz&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;txz&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;?tar=xz&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;gnu-tar with xz / lzma compression (v.slow)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;zip&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;?zip&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;works everywhere, glitchy filenames on win7 and older&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;zip_dos&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;?zip=dos&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;traditional cp437 (no unicode) to fix glitchy filenames&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;zip_crc&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;?zip=crc&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;cp437 with crc32 computed early for truly ancient software&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;gzip default level is &lt;code&gt;3&lt;/code&gt; (0=fast, 9=best), change with &lt;code&gt;?tar=gz:9&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;xz default level is &lt;code&gt;1&lt;/code&gt; (0=fast, 9=best), change with &lt;code&gt;?tar=xz:9&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;bz2 default level is &lt;code&gt;2&lt;/code&gt; (1=fast, 9=best), change with &lt;code&gt;?tar=bz2:9&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;hidden files (&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#dotfiles"&gt;dotfiles&lt;/a&gt;) are excluded unless account is allowed to list them 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;up2k.db&lt;/code&gt; and &lt;code&gt;dir.txt&lt;/code&gt; is always excluded&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;bsdtar supports streaming unzipping: &lt;code&gt;curl foo?zip | bsdtar -xv&lt;/code&gt; 
  &lt;ul&gt; 
   &lt;li&gt;good, because copyparty's zip is faster than tar on small files 
    &lt;ul&gt; 
     &lt;li&gt;but &lt;code&gt;?tar&lt;/code&gt; is better for large files, especially if the total exceeds 4 GiB&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;zip_crc&lt;/code&gt; will take longer to download since the server has to read each file twice 
  &lt;ul&gt; 
   &lt;li&gt;this is only to support MS-DOS PKZIP v2.04g (october 1993) and older 
    &lt;ul&gt; 
     &lt;li&gt;how are you accessing copyparty actually&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;you can also zip a selection of files or folders by clicking them in the browser, that brings up a selection editor and zip button in the bottom right&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/241032/129635374-e5136e01-470a-49b1-a762-848e8a4c9cdc.png" alt="copyparty-zipsel-fs8" /&gt;&lt;/p&gt; 
&lt;p&gt;cool trick: download a folder by appending url-params &lt;code&gt;?tar&amp;amp;opus&lt;/code&gt; or &lt;code&gt;?tar&amp;amp;mp3&lt;/code&gt; to transcode all audio files (except aac|m4a|mp3|ogg|opus|wma) to opus/mp3 before they're added to the archive&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;super useful if you're 5 minutes away from takeoff and realize you don't have any music on your phone but your server only has flac files and downloading those will burn through all your data + there wouldn't be enough time anyways&lt;/li&gt; 
 &lt;li&gt;and url-params &lt;code&gt;&amp;amp;j&lt;/code&gt; / &lt;code&gt;&amp;amp;w&lt;/code&gt; produce jpeg/webm thumbnails/spectrograms instead of the original audio/video/images (&lt;code&gt;&amp;amp;p&lt;/code&gt; for audio waveforms) 
  &lt;ul&gt; 
   &lt;li&gt;can also be used to pregenerate thumbnails; combine with &lt;code&gt;--th-maxage=9999999&lt;/code&gt; or &lt;code&gt;--th-clean=0&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;uploading&lt;/h2&gt; 
&lt;p&gt;drag files/folders into the web-browser to upload&lt;/p&gt; 
&lt;p&gt;dragdrop is the recommended way, but you may also:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;select some files (not folders) in your file explorer and press CTRL-V inside the browser window&lt;/li&gt; 
 &lt;li&gt;use the &lt;a href="https://github.com/9001/copyparty/tree/hovudstraum/bin#u2cpy"&gt;command-line uploader&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;upload using &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#client-examples"&gt;curl, sharex, ishare, ...&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;when uploading files through dragdrop or CTRL-V, this initiates an upload using &lt;code&gt;up2k&lt;/code&gt;; there are two browser-based uploaders available:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;[🎈] bup&lt;/code&gt;, the basic uploader, supports almost every browser since netscape 4.0&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[🚀] up2k&lt;/code&gt;, the good / fancy one&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;NB: you can undo/delete your own uploads with &lt;code&gt;[🧯]&lt;/code&gt; &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#unpost"&gt;unpost&lt;/a&gt; (and this is also where you abort unfinished uploads, but you have to refresh the page first)&lt;/p&gt; 
&lt;p&gt;up2k has several advantages:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;you can drop folders into the browser (files are added recursively)&lt;/li&gt; 
 &lt;li&gt;files are processed in chunks, and each chunk is checksummed 
  &lt;ul&gt; 
   &lt;li&gt;uploads autoresume if they are interrupted by network issues&lt;/li&gt; 
   &lt;li&gt;uploads resume if you reboot your browser or pc, just upload the same files again&lt;/li&gt; 
   &lt;li&gt;server detects any corruption; the client reuploads affected chunks&lt;/li&gt; 
   &lt;li&gt;the client doesn't upload anything that already exists on the server&lt;/li&gt; 
   &lt;li&gt;no filesize limit, even when a proxy limits the request size (for example Cloudflare)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;much higher speeds than ftp/scp/tarpipe on some internet connections (mainly american ones) thanks to parallel connections&lt;/li&gt; 
 &lt;li&gt;the last-modified timestamp of the file is preserved&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;it is perfectly safe to restart / upgrade copyparty while someone is uploading to it!&lt;br /&gt; all known up2k clients will resume just fine 💪&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/devnotes.md#up2k"&gt;up2k&lt;/a&gt; for details on how it works, or watch a &lt;a href="https://a.ocv.me/pub/demo/pics-vids/#gf-0f6f5c0d"&gt;demo video&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/241032/129635371-48fc54ca-fa91-48e3-9b1d-ba413e4b68cb.png" alt="copyparty-upload-fs8" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;protip:&lt;/strong&gt; you can avoid scaring away users with &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/contrib/plugins/minimal-up2k.js"&gt;contrib/plugins/minimal-up2k.js&lt;/a&gt; which makes it look &lt;a href="https://user-images.githubusercontent.com/241032/118311195-dd6ca380-b4ef-11eb-86f3-75a3ff2e1332.png"&gt;much simpler&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;protip:&lt;/strong&gt; if you enable &lt;code&gt;favicon&lt;/code&gt; in the &lt;code&gt;[⚙️] settings&lt;/code&gt; tab (by typing something into the textbox), the icon in the browser tab will indicate upload progress -- also, the &lt;code&gt;[🔔]&lt;/code&gt; and/or &lt;code&gt;[🔊]&lt;/code&gt; switches enable visible and/or audible notifications on upload completion&lt;/p&gt; 
&lt;p&gt;the up2k UI is the epitome of polished intuitive experiences:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;"parallel uploads" specifies how many chunks to upload at the same time&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[🏃]&lt;/code&gt; analysis of other files should continue while one is uploading&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[🥔]&lt;/code&gt; shows a simpler UI for faster uploads from slow devices&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[🛡️]&lt;/code&gt; decides when to overwrite existing files on the server 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;🛡️&lt;/code&gt; = never (generate a new filename instead)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;🕒&lt;/code&gt; = overwrite if the server-file is older&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;♻️&lt;/code&gt; = always overwrite if the files are different&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[🎲]&lt;/code&gt; generate random filenames during upload&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[🔎]&lt;/code&gt; switch between upload and &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#file-search"&gt;file-search&lt;/a&gt; mode 
  &lt;ul&gt; 
   &lt;li&gt;ignore &lt;code&gt;[🔎]&lt;/code&gt; if you add files by dragging them into the browser&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;and then there's the tabs below it,&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;[ok]&lt;/code&gt; is the files which completed successfully&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[ng]&lt;/code&gt; is the ones that failed / got rejected (already exists, ...)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[done]&lt;/code&gt; shows a combined list of &lt;code&gt;[ok]&lt;/code&gt; and &lt;code&gt;[ng]&lt;/code&gt;, chronological order&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[busy]&lt;/code&gt; files which are currently hashing, pending-upload, or uploading 
  &lt;ul&gt; 
   &lt;li&gt;plus up to 3 entries each from &lt;code&gt;[done]&lt;/code&gt; and &lt;code&gt;[que]&lt;/code&gt; for context&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[que]&lt;/code&gt; is all the files that are still queued&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;note that since up2k has to read each file twice, &lt;code&gt;[🎈] bup&lt;/code&gt; can &lt;em&gt;theoretically&lt;/em&gt; be up to 2x faster in some extreme cases (files bigger than your ram, combined with an internet connection faster than the read-speed of your HDD, or if you're uploading from a cuo2duo)&lt;/p&gt; 
&lt;p&gt;if you are resuming a massive upload and want to skip hashing the files which already finished, you can enable &lt;code&gt;turbo&lt;/code&gt; in the &lt;code&gt;[⚙️] config&lt;/code&gt; tab, but please read the tooltip on that button&lt;/p&gt; 
&lt;p&gt;if the server is behind a proxy which imposes a request-size limit, you can configure up2k to sneak below the limit with server-option &lt;code&gt;--u2sz&lt;/code&gt; (the default is 96 MiB to support Cloudflare)&lt;/p&gt; 
&lt;p&gt;if you want to replace existing files on the server with new uploads by default, run with &lt;code&gt;--u2ow 2&lt;/code&gt; (only works if users have the delete-permission, and can still be disabled with &lt;code&gt;🛡️&lt;/code&gt; in the UI)&lt;/p&gt; 
&lt;h3&gt;file-search&lt;/h3&gt; 
&lt;p&gt;dropping files into the browser also lets you see if they exist on the server&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/241032/129635361-c79286f0-b8f1-440e-aaf4-6e929428fac9.png" alt="copyparty-fsearch-fs8" /&gt;&lt;/p&gt; 
&lt;p&gt;when you drag/drop files into the browser, you will see two dropzones: &lt;code&gt;Upload&lt;/code&gt; and &lt;code&gt;Search&lt;/code&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;on a phone? toggle the &lt;code&gt;[🔎]&lt;/code&gt; switch green before tapping the big yellow Search button to select your files&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;the files will be hashed on the client-side, and each hash is sent to the server, which checks if that file exists somewhere&lt;/p&gt; 
&lt;p&gt;files go into &lt;code&gt;[ok]&lt;/code&gt; if they exist (and you get a link to where it is), otherwise they land in &lt;code&gt;[ng]&lt;/code&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;the main reason filesearch is combined with the uploader is cause the code was too spaghetti to separate it out somewhere else, this is no longer the case but now i've warmed up to the idea too much&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;unpost&lt;/h3&gt; 
&lt;p&gt;undo/delete accidental uploads using the &lt;code&gt;[🧯]&lt;/code&gt; tab in the UI&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/241032/129635368-3afa6634-c20f-418c-90dc-ec411f3b3897.png" alt="copyparty-unpost-fs8" /&gt;&lt;/p&gt; 
&lt;p&gt;you can unpost even if you don't have regular move/delete access, however only for files uploaded within the past &lt;code&gt;--unpost&lt;/code&gt; seconds (default 12 hours) and the server must be running with &lt;code&gt;-e2d&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;config file example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;[global]
  e2d            # enable up2k database (remember uploads)
  unpost: 43200  # 12 hours (default)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;self-destruct&lt;/h3&gt; 
&lt;p&gt;uploads can be given a lifetime, after which they expire / self-destruct&lt;/p&gt; 
&lt;p&gt;the feature must be enabled per-volume with the &lt;code&gt;lifetime&lt;/code&gt; &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#upload-rules"&gt;upload rule&lt;/a&gt; which sets the upper limit for how long a file gets to stay on the server&lt;/p&gt; 
&lt;p&gt;clients can specify a shorter expiration time using the &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#uploading"&gt;up2k ui&lt;/a&gt; -- the relevant options become visible upon navigating into a folder with &lt;code&gt;lifetimes&lt;/code&gt; enabled -- or by using the &lt;code&gt;life&lt;/code&gt; &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/devnotes.md#write"&gt;upload modifier&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;specifying a custom expiration time client-side will affect the timespan in which unposts are permitted, so keep an eye on the estimates in the up2k ui&lt;/p&gt; 
&lt;h3&gt;race the beam&lt;/h3&gt; 
&lt;p&gt;download files while they're still uploading (&lt;a href="http://a.ocv.me/pub/g/nerd-stuff/cpp/2024-0418-race-the-beam.webm"&gt;demo video&lt;/a&gt;) -- it's almost like peer-to-peer&lt;/p&gt; 
&lt;p&gt;requires the file to be uploaded using up2k (which is the default drag-and-drop uploader), alternatively the command-line program&lt;/p&gt; 
&lt;h3&gt;incoming files&lt;/h3&gt; 
&lt;p&gt;the control-panel shows the ETA for all incoming files , but only for files being uploaded into volumes where you have read-access&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/fd275ffa-698c-4fca-a307-4d2181269a6a" alt="copyparty-cpanel-upload-eta-or8" /&gt;&lt;/p&gt; 
&lt;h2&gt;file manager&lt;/h2&gt; 
&lt;p&gt;cut/paste, rename, and delete files/folders (if you have permission)&lt;/p&gt; 
&lt;p&gt;file selection: click somewhere on the line (not the link itself), then:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;space&lt;/code&gt; to toggle&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;up/down&lt;/code&gt; to move&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;shift-up/down&lt;/code&gt; to move-and-select&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;ctrl-shift-up/down&lt;/code&gt; to also scroll&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;shift-click another line for range-select&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;cut: select some files and &lt;code&gt;ctrl-x&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;copy: select some files and &lt;code&gt;ctrl-c&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;paste: &lt;code&gt;ctrl-v&lt;/code&gt; in another folder&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;rename: &lt;code&gt;F2&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;you can copy/move files across browser tabs (cut/copy in one tab, paste in another)&lt;/p&gt; 
&lt;h2&gt;shares&lt;/h2&gt; 
&lt;p&gt;share a file or folder by creating a temporary link&lt;/p&gt; 
&lt;p&gt;when enabled in the server settings (&lt;code&gt;--shr&lt;/code&gt;), click the bottom-right &lt;code&gt;share&lt;/code&gt; button to share the folder you're currently in, or alternatively:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;select a folder first to share that folder instead&lt;/li&gt; 
 &lt;li&gt;select one or more files to share only those files&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;this feature was made with &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#identity-providers"&gt;identity providers&lt;/a&gt; in mind -- configure your reverseproxy to skip the IdP's access-control for a given URL prefix and use that to safely share specific files/folders sans the usual auth checks&lt;/p&gt; 
&lt;p&gt;when creating a share, the creator can choose any of the following options:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;password-protection&lt;/li&gt; 
 &lt;li&gt;expire after a certain time; &lt;code&gt;0&lt;/code&gt; or blank means infinite&lt;/li&gt; 
 &lt;li&gt;allow visitors to upload (if the user who creates the share has write-access)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;semi-intentional limitations:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;cleanup of expired shares only works when global option &lt;code&gt;e2d&lt;/code&gt; is set, and/or at least one volume on the server has volflag &lt;code&gt;e2d&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;only folders from the same volume are shared; if you are sharing a folder which contains other volumes, then the contents of those volumes will not be available&lt;/li&gt; 
 &lt;li&gt;if you change &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#password-hashing"&gt;password hashing&lt;/a&gt; settings after creating a password-protected share, then that share will stop working&lt;/li&gt; 
 &lt;li&gt;related to &lt;a href="https://github.com/9001/copyparty/raw/hovudstraum/docs/idp.md#idp-volumes-are-forgotten-on-shutdown"&gt;IdP volumes being forgotten on shutdown&lt;/a&gt;, any shares pointing into a user's IdP volume will be unavailable until that user makes their first request after a restart&lt;/li&gt; 
 &lt;li&gt;no option to "delete after first access" because tricky 
  &lt;ul&gt; 
   &lt;li&gt;when linking something to discord (for example) it'll get accessed by their scraper and that would count as a hit&lt;/li&gt; 
   &lt;li&gt;browsers wouldn't be able to resume a broken download unless the requester's IP gets allowlisted for X minutes (ref. tricky)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;specify &lt;code&gt;--shr /foobar&lt;/code&gt; to enable this feature; a toplevel virtual folder named &lt;code&gt;foobar&lt;/code&gt; is then created, and that's where all the shares will be served from&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;you can name it whatever, &lt;code&gt;foobar&lt;/code&gt; is just an example&lt;/li&gt; 
 &lt;li&gt;if you're using config files, put &lt;code&gt;shr: /foobar&lt;/code&gt; inside the &lt;code&gt;[global]&lt;/code&gt; section instead&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;users can delete their own shares in the controlpanel, and a list of privileged users (&lt;code&gt;--shr-adm&lt;/code&gt;) are allowed to see and/or delet any share on the server&lt;/p&gt; 
&lt;p&gt;after a share has expired, it remains visible in the controlpanel for &lt;code&gt;--shr-rt&lt;/code&gt; minutes (default is 1 day), and the owner can revive it by extending the expiration time there&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;security note:&lt;/strong&gt; using this feature does not mean that you can skip the &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#accounts-and-volumes"&gt;accounts and volumes&lt;/a&gt; section -- you still need to restrict access to volumes that you do not intend to share with unauthenticated users! it is not sufficient to use rules in the reverseproxy to restrict access to just the &lt;code&gt;/share&lt;/code&gt; folder.&lt;/p&gt; 
&lt;h2&gt;batch rename&lt;/h2&gt; 
&lt;p&gt;select some files and press &lt;code&gt;F2&lt;/code&gt; to bring up the rename UI&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/241032/128434204-eb136680-3c07-4ec7-92e0-ae86af20c241.png" alt="batch-rename-fs8" /&gt;&lt;/p&gt; 
&lt;p&gt;quick explanation of the buttons,&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;[✅ apply rename]&lt;/code&gt; confirms and begins renaming&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[❌ cancel]&lt;/code&gt; aborts and closes the rename window&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[↺ reset]&lt;/code&gt; reverts any filename changes back to the original name&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[decode]&lt;/code&gt; does a URL-decode on the filename, fixing stuff like &lt;code&gt;&amp;amp;amp;&lt;/code&gt; and &lt;code&gt;%20&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[advanced]&lt;/code&gt; toggles advanced mode&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;advanced mode: rename files based on rules to decide the new names, based on the original name (regex), or based on the tags collected from the file (artist/title/...), or a mix of both&lt;/p&gt; 
&lt;p&gt;in advanced mode,&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;[case]&lt;/code&gt; toggles case-sensitive regex&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;regex&lt;/code&gt; is the regex pattern to apply to the original filename; any files which don't match will be skipped&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;format&lt;/code&gt; is the new filename, taking values from regex capturing groups and/or from file tags 
  &lt;ul&gt; 
   &lt;li&gt;very loosely based on foobar2000 syntax&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;presets&lt;/code&gt; lets you save rename rules for later&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;available functions:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;$lpad(text, length, pad_char)&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;$rpad(text, length, pad_char)&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;so,&lt;/p&gt; 
&lt;p&gt;say you have a file named &lt;a href="https://www.youtube.com/watch?v=-dtb0vDPruI"&gt;&lt;code&gt;meganeko - Eclipse - 07 Sirius A.mp3&lt;/code&gt;&lt;/a&gt; (absolutely fantastic album btw) and the tags are: &lt;code&gt;Album:Eclipse&lt;/code&gt;, &lt;code&gt;Artist:meganeko&lt;/code&gt;, &lt;code&gt;Title:Sirius A&lt;/code&gt;, &lt;code&gt;tn:7&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;you could use just regex to rename it:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;regex&lt;/code&gt; = &lt;code&gt;(.*) - (.*) - ([0-9]{2}) (.*)&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;format&lt;/code&gt; = &lt;code&gt;(3). (1) - (4)&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;output&lt;/code&gt; = &lt;code&gt;07. meganeko - Sirius A.mp3&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;or you could use just tags:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;format&lt;/code&gt; = &lt;code&gt;$lpad((tn),2,0). (artist) - (title).(ext)&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;output&lt;/code&gt; = &lt;code&gt;7. meganeko - Sirius A.mp3&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;or a mix of both:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;regex&lt;/code&gt; = &lt;code&gt;- ([0-9]{2})&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;format&lt;/code&gt; = &lt;code&gt;(1). (artist) - (title).(ext)&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;output&lt;/code&gt; = &lt;code&gt;07. meganeko - Sirius A.mp3&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;the metadata keys you can use in the format field are the ones in the file-browser table header (whatever is collected with &lt;code&gt;-mte&lt;/code&gt; and &lt;code&gt;-mtp&lt;/code&gt;)&lt;/p&gt; 
&lt;h2&gt;rss feeds&lt;/h2&gt; 
&lt;p&gt;monitor a folder with your RSS reader , optionally recursive&lt;/p&gt; 
&lt;p&gt;must be enabled per-volume with volflag &lt;code&gt;rss&lt;/code&gt; or globally with &lt;code&gt;--rss&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;the feed includes itunes metadata for use with podcast readers such as &lt;a href="https://antennapod.org/"&gt;AntennaPod&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;a feed example: &lt;a href="https://cd.ocv.me/a/d2/d22/?rss&amp;amp;fext=mp3"&gt;https://cd.ocv.me/a/d2/d22/?rss&amp;amp;fext=mp3&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;url parameters:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;pw=hunter2&lt;/code&gt; for password auth 
  &lt;ul&gt; 
   &lt;li&gt;if you enabled &lt;code&gt;--usernames&lt;/code&gt; then do &lt;code&gt;pw=username:password&lt;/code&gt; instead&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;recursive&lt;/code&gt; to also include subfolders&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;title=foo&lt;/code&gt; changes the feed title (default: folder name)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;fext=mp3,opus&lt;/code&gt; only include mp3 and opus files (default: all)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;nf=30&lt;/code&gt; only show the first 30 results (default: 250)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;sort=m&lt;/code&gt; sort by mtime (file last-modified), newest first (default) 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;u&lt;/code&gt; = upload-time; NOTE: non-uploaded files have upload-time &lt;code&gt;0&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;n&lt;/code&gt; = filename&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;a&lt;/code&gt; = filesize&lt;/li&gt; 
   &lt;li&gt;uppercase = reverse-sort; &lt;code&gt;M&lt;/code&gt; = oldest file first&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;recent uploads&lt;/h2&gt; 
&lt;p&gt;list all recent uploads by clicking "show recent uploads" in the controlpanel&lt;/p&gt; 
&lt;p&gt;will show uploader IP and upload-time if the visitor has the admin permission&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;global-option &lt;code&gt;--ups-when&lt;/code&gt; makes upload-time visible to all users, and not just admins&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;global-option &lt;code&gt;--ups-who&lt;/code&gt; (volflag &lt;code&gt;ups_who&lt;/code&gt;) specifies who gets access (0=nobody, 1=admins, 2=everyone), default=2&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;note that the &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#unpost"&gt;🧯 unpost&lt;/a&gt; feature is better suited for viewing &lt;em&gt;your own&lt;/em&gt; recent uploads, as it includes the option to undo/delete them&lt;/p&gt; 
&lt;p&gt;config file example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;[global]
  ups-when    # everyone can see upload times
  ups-who: 1  # but only admins can see the list,
              # so ups-when doesn't take effect
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;media player&lt;/h2&gt; 
&lt;p&gt;plays almost every audio format there is (if the server has FFmpeg installed for on-demand transcoding)&lt;/p&gt; 
&lt;p&gt;the following audio formats are usually always playable, even without FFmpeg: &lt;code&gt;aac|flac|m4a|mp3|ogg|opus|wav&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;some highlights:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;OS integration; control playback from your phone's lockscreen (&lt;a href="https://user-images.githubusercontent.com/241032/233213022-298a98ba-721a-4cf1-a3d4-f62634bc53d5.png"&gt;windows&lt;/a&gt; // &lt;a href="https://user-images.githubusercontent.com/241032/142711926-0700be6c-3e31-47b3-9928-53722221f722.png"&gt;iOS&lt;/a&gt; // &lt;a href="https://user-images.githubusercontent.com/241032/233212311-a7368590-08c7-4f9f-a1af-48ccf3f36fad.png"&gt;android&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;shows the audio waveform in the seekbar&lt;/li&gt; 
 &lt;li&gt;not perfectly gapless but can get really close (see settings + eq below); good enough to enjoy gapless albums as intended&lt;/li&gt; 
 &lt;li&gt;videos can be played as audio, without wasting bandwidth on the video&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;click the &lt;code&gt;play&lt;/code&gt; link next to an audio file, or copy the link target to &lt;a href="https://a.ocv.me/pub/demo/music/Ubiktune%20-%20SOUNDSHOCK%202%20-%20FM%20FUNK%20TERRROR!!/#af-1fbfba61&amp;amp;t=18"&gt;share it&lt;/a&gt; (optionally with a timestamp to start playing from, like that example does)&lt;/p&gt; 
&lt;p&gt;open the &lt;code&gt;[🎺]&lt;/code&gt; media-player-settings tab to configure it,&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;"switches": 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;[🔁]&lt;/code&gt; repeats one single song forever&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;[🔀]&lt;/code&gt; shuffles the files inside each folder&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;[preload]&lt;/code&gt; starts loading the next track when it's about to end, reduces the silence between songs&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;[full]&lt;/code&gt; does a full preload by downloading the entire next file; good for unreliable connections, bad for slow connections&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;[~s]&lt;/code&gt; toggles the seekbar waveform display&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;[/np]&lt;/code&gt; enables buttons to copy the now-playing info as an irc message&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;[📻]&lt;/code&gt; enables buttons to create an &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#playlists"&gt;m3u playlist&lt;/a&gt; with the selected songs&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;[os-ctl]&lt;/code&gt; makes it possible to control audio playback from the lockscreen of your device (enables &lt;a href="https://developer.mozilla.org/en-US/docs/Web/API/MediaSession"&gt;mediasession&lt;/a&gt;)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;[seek]&lt;/code&gt; allows seeking with lockscreen controls (buggy on some devices)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;[art]&lt;/code&gt; shows album art on the lockscreen&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;[🎯]&lt;/code&gt; keeps the playing song scrolled into view (good when using the player as a taskbar dock)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;[⟎]&lt;/code&gt; shrinks the playback controls&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;"buttons": 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;[uncache]&lt;/code&gt; may fix songs that won't play correctly due to bad files in browser cache&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;"at end of folder": 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;[loop]&lt;/code&gt; keeps looping the folder&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;[next]&lt;/code&gt; plays into the next folder&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;"transcode": 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;[flac]&lt;/code&gt; converts &lt;code&gt;flac&lt;/code&gt; and &lt;code&gt;wav&lt;/code&gt; files into opus (if supported by browser) or mp3&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;[aac]&lt;/code&gt; converts &lt;code&gt;aac&lt;/code&gt; and &lt;code&gt;m4a&lt;/code&gt; files into opus (if supported by browser) or mp3&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;[oth]&lt;/code&gt; converts all other known formats into opus (if supported by browser) or mp3 
    &lt;ul&gt; 
     &lt;li&gt;&lt;code&gt;aac|ac3|aif|aiff|alac|alaw|amr|ape|au|dfpwm|dts|flac|gsm|it|m4a|mo3|mod|mp2|mp3|mpc|mptm|mt2|mulaw|ogg|okt|opus|ra|s3m|tak|tta|ulaw|wav|wma|wv|xm|xpk&lt;/code&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;"transcode to": 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;[opus]&lt;/code&gt; produces an &lt;code&gt;opus&lt;/code&gt; whenever transcoding is necessary (the best choice on Android and PCs)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;[awo]&lt;/code&gt; is &lt;code&gt;opus&lt;/code&gt; in a &lt;code&gt;weba&lt;/code&gt; file, good for iPhones (iOS 17.5 and newer) but Apple is still fixing some state-confusion bugs as of iOS 18.2.1&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;[caf]&lt;/code&gt; is &lt;code&gt;opus&lt;/code&gt; in a &lt;code&gt;caf&lt;/code&gt; file, good for iPhones (iOS 11 through 17), technically unsupported by Apple but works for the most part&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;[mp3]&lt;/code&gt; -- the myth, the legend, the undying master of mediocre sound quality that definitely works everywhere&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;[flac]&lt;/code&gt; -- lossless but compressed, for LAN and/or fiber playback on electrostatic headphones&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;[wav]&lt;/code&gt; -- lossless and uncompressed, for LAN and/or fiber playback on electrostatic headphones connected to very old equipment 
    &lt;ul&gt; 
     &lt;li&gt;&lt;code&gt;flac&lt;/code&gt; and &lt;code&gt;wav&lt;/code&gt; must be enabled with &lt;code&gt;--allow-flac&lt;/code&gt; / &lt;code&gt;--allow-wav&lt;/code&gt; to allow spending the disk space&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;"tint" reduces the contrast of the playback bar&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;playlists&lt;/h3&gt; 
&lt;p&gt;create and play &lt;a href="https://en.wikipedia.org/wiki/M3U"&gt;m3u8&lt;/a&gt; playlists -- see example &lt;a href="https://a.ocv.me/pub/demo/music/?doc=example-playlist.m3u"&gt;text&lt;/a&gt; and &lt;a href="https://a.ocv.me/pub/demo/music/#m3u=example-playlist.m3u"&gt;player&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;click a file with the extension &lt;code&gt;m3u&lt;/code&gt; or &lt;code&gt;m3u8&lt;/code&gt; (for example &lt;code&gt;mixtape.m3u&lt;/code&gt; or &lt;code&gt;touhou.m3u8&lt;/code&gt; ) and you get two choices: Play / Edit&lt;/p&gt; 
&lt;p&gt;playlists can include songs across folders anywhere on the server, but filekeys/dirkeys are NOT supported, so the listener must have read-access or get-access to the files&lt;/p&gt; 
&lt;h3&gt;creating a playlist&lt;/h3&gt; 
&lt;p&gt;with a standalone mediaplayer or copyparty&lt;/p&gt; 
&lt;p&gt;you can use foobar2000, deadbeef, just about any standalone player should work -- but you might need to edit the filepaths in the playlist so they fit with the server-URLs&lt;/p&gt; 
&lt;p&gt;alternatively, you can create the playlist using copyparty itself:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;open the &lt;code&gt;[🎺]&lt;/code&gt; media-player-settings tab and enable the &lt;code&gt;[📻]&lt;/code&gt; create-playlist feature -- this adds two new buttons in the bottom-right tray, &lt;code&gt;[📻add]&lt;/code&gt; and &lt;code&gt;[📻copy]&lt;/code&gt; which appear when you listen to music, or when you select a few audiofiles&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;click the &lt;code&gt;📻add&lt;/code&gt; button while a song is playing (or when you've selected some songs) and they'll be added to "the list" (you can't see it yet)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;at any time, click &lt;code&gt;📻copy&lt;/code&gt; to send the playlist to your clipboard&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;you can then continue adding more songs if you'd like&lt;/li&gt; 
   &lt;li&gt;if you want to wipe the playlist and start from scratch, just refresh the page&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;create a new textfile, name it &lt;code&gt;something.m3u&lt;/code&gt; and paste the playlist there&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;audio equalizer&lt;/h3&gt; 
&lt;p&gt;and &lt;a href="https://en.wikipedia.org/wiki/Dynamic_range_compression"&gt;dynamic range compressor&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;can also boost the volume in general, or increase/decrease stereo width (like &lt;a href="https://www.foobar2000.org/components/view/foo_dsp_meiercf"&gt;crossfeed&lt;/a&gt; just worse)&lt;/p&gt; 
&lt;p&gt;has the convenient side-effect of reducing the pause between songs, so gapless albums play better with the eq enabled (just make it flat)&lt;/p&gt; 
&lt;p&gt;not available on iPhones / iPads because AudioContext currently breaks background audio playback on iOS (15.7.8)&lt;/p&gt; 
&lt;h3&gt;fix unreliable playback on android&lt;/h3&gt; 
&lt;p&gt;due to phone / app settings, android phones may randomly stop playing music when the power saver kicks in, especially at the end of an album -- you can fix it by &lt;a href="https://user-images.githubusercontent.com/241032/235262123-c328cca9-3930-4948-bd18-3949b9fd3fcf.png"&gt;disabling power saving&lt;/a&gt; in the &lt;a href="https://user-images.githubusercontent.com/241032/235262121-2ffc51ae-7821-4310-a322-c3b7a507890c.png"&gt;app settings&lt;/a&gt; of the browser you use for music streaming (preferably a dedicated one)&lt;/p&gt; 
&lt;h2&gt;textfile viewer&lt;/h2&gt; 
&lt;p&gt;with realtime streaming of logfiles and such (&lt;a href="https://a.ocv.me/pub/demo/logtail/"&gt;demo&lt;/a&gt;) , and terminal colors work too&lt;/p&gt; 
&lt;p&gt;click &lt;code&gt;-txt-&lt;/code&gt; next to a textfile to open the viewer, which has the following toolbar buttons:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;✏️ edit&lt;/code&gt; opens the textfile editor&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;📡 follow&lt;/code&gt; starts monitoring the file for changes, streaming new lines in realtime 
  &lt;ul&gt; 
   &lt;li&gt;similar to &lt;code&gt;tail -f&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://a.ocv.me/pub/demo/logtail/?doc=lipsum.txt&amp;amp;tail"&gt;link directly&lt;/a&gt; to a file with tailing enabled by adding &lt;code&gt;&amp;amp;tail&lt;/code&gt; to the textviewer URL&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;markdown viewer&lt;/h2&gt; 
&lt;p&gt;and there are &lt;em&gt;two&lt;/em&gt; editors&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/241032/115978057-66419080-a57d-11eb-8539-d2be843991aa.png" alt="copyparty-md-read-fs8" /&gt;&lt;/p&gt; 
&lt;p&gt;there is a built-in extension for inline clickable thumbnails;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;enable it by adding &lt;code&gt;&amp;lt;!-- th --&amp;gt;&lt;/code&gt; somewhere in the doc&lt;/li&gt; 
 &lt;li&gt;add thumbnails with &lt;code&gt;!th[l](your.jpg)&lt;/code&gt; where &lt;code&gt;l&lt;/code&gt; means left-align (&lt;code&gt;r&lt;/code&gt; = right-align)&lt;/li&gt; 
 &lt;li&gt;a single line with &lt;code&gt;---&lt;/code&gt; clears the float / inlining&lt;/li&gt; 
 &lt;li&gt;in the case of README.md being displayed below a file listing, thumbnails will open in the gallery viewer&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;other notes,&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;the document preview has a max-width which is the same as an A4 paper when printed&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;markdown vars&lt;/h3&gt; 
&lt;p&gt;dynamic docs with serverside variable expansion to replace stuff like &lt;code&gt;{{self.ip}}&lt;/code&gt; with the client's IP, or &lt;code&gt;{{srv.htime}}&lt;/code&gt; with the current time on the server&lt;/p&gt; 
&lt;p&gt;see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/srv/expand/"&gt;./srv/expand/&lt;/a&gt; for usage and examples&lt;/p&gt; 
&lt;h2&gt;other tricks&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;you can link a particular timestamp in an audio file by adding it to the URL, such as &lt;code&gt;&amp;amp;20&lt;/code&gt; / &lt;code&gt;&amp;amp;20s&lt;/code&gt; / &lt;code&gt;&amp;amp;1m20&lt;/code&gt; / &lt;code&gt;&amp;amp;t=1:20&lt;/code&gt; after the &lt;code&gt;.../#af-c8960dab&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;enabling the audio equalizer can help make gapless albums fully gapless in some browsers (chrome), so consider leaving it on with all the values at zero&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;get a plaintext file listing by adding &lt;code&gt;?ls=t&lt;/code&gt; to a URL, or a compact colored one with &lt;code&gt;?ls=v&lt;/code&gt; (for unix terminals)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;if you are using media hotkeys to switch songs and are getting tired of seeing the OSD popup which Windows doesn't let you disable, consider &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/contrib/#media-osd-bgoneps1"&gt;./contrib/media-osd-bgone.ps1&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;click the bottom-left &lt;code&gt;π&lt;/code&gt; to open a javascript prompt for debugging&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;files named &lt;code&gt;.prologue.html&lt;/code&gt; / &lt;code&gt;.epilogue.html&lt;/code&gt; will be rendered before/after directory listings unless &lt;code&gt;--no-logues&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;files named &lt;code&gt;descript.ion&lt;/code&gt; / &lt;code&gt;DESCRIPT.ION&lt;/code&gt; are parsed and displayed in the file listing, or as the epilogue if nonstandard&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;files named &lt;code&gt;README.md&lt;/code&gt; / &lt;code&gt;readme.md&lt;/code&gt; will be rendered after directory listings unless &lt;code&gt;--no-readme&lt;/code&gt; (but &lt;code&gt;.epilogue.html&lt;/code&gt; takes precedence)&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;and &lt;code&gt;PREADME.md&lt;/code&gt; / &lt;code&gt;preadme.md&lt;/code&gt; is shown above directory listings unless &lt;code&gt;--no-readme&lt;/code&gt; or &lt;code&gt;.prologue.html&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;README.md&lt;/code&gt; and &lt;code&gt;*logue.html&lt;/code&gt; can contain placeholder values which are replaced server-side before embedding into directory listings; see &lt;code&gt;--help-exp&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;searching&lt;/h2&gt; 
&lt;p&gt;search by size, date, path/name, mp3-tags, ...&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/241032/129635365-c0ff2a9f-0ee5-4fc3-8bb6-006033cf67b8.png" alt="copyparty-search-fs8" /&gt;&lt;/p&gt; 
&lt;p&gt;when started with &lt;code&gt;-e2dsa&lt;/code&gt; copyparty will scan/index all your files. This avoids duplicates on upload, and also makes the volumes searchable through the web-ui:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;make search queries by &lt;code&gt;size&lt;/code&gt;/&lt;code&gt;date&lt;/code&gt;/&lt;code&gt;directory-path&lt;/code&gt;/&lt;code&gt;filename&lt;/code&gt;, or...&lt;/li&gt; 
 &lt;li&gt;drag/drop a local file to see if the same contents exist somewhere on the server, see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#file-search"&gt;file-search&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;path/name queries are space-separated, AND'ed together, and words are negated with a &lt;code&gt;-&lt;/code&gt; prefix, so for example:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;path: &lt;code&gt;shibayan -bossa&lt;/code&gt; finds all files where one of the folders contain &lt;code&gt;shibayan&lt;/code&gt; but filters out any results where &lt;code&gt;bossa&lt;/code&gt; exists somewhere in the path&lt;/li&gt; 
 &lt;li&gt;name: &lt;code&gt;demetori styx&lt;/code&gt; gives you &lt;a href="https://www.youtube.com/watch?v=zGh0g14ZJ8I&amp;amp;list=PL3A147BD151EE5218&amp;amp;index=9"&gt;good stuff&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;the &lt;code&gt;raw&lt;/code&gt; field allows for more complex stuff such as &lt;code&gt;( tags like *nhato* or tags like *taishi* ) and ( not tags like *nhato* or not tags like *taishi* )&lt;/code&gt; which finds all songs by either nhato or taishi, excluding collabs (terrible example, why would you do that)&lt;/p&gt; 
&lt;p&gt;for the above example to work, add the commandline argument &lt;code&gt;-e2ts&lt;/code&gt; to also scan/index tags from music files, which brings us over to:&lt;/p&gt; 
&lt;h1&gt;server config&lt;/h1&gt; 
&lt;p&gt;using arguments or config files, or a mix of both:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;config files (&lt;code&gt;-c some.conf&lt;/code&gt;) can set additional commandline arguments; see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/example.conf"&gt;./docs/example.conf&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/example2.conf"&gt;./docs/example2.conf&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;kill -s USR1&lt;/code&gt; (same as &lt;code&gt;systemctl reload copyparty&lt;/code&gt;) to reload accounts and volumes from config files without restarting 
  &lt;ul&gt; 
   &lt;li&gt;or click the &lt;code&gt;[reload cfg]&lt;/code&gt; button in the control-panel if the user has &lt;code&gt;a&lt;/code&gt;/admin in any volume&lt;/li&gt; 
   &lt;li&gt;changes to the &lt;code&gt;[global]&lt;/code&gt; config section requires a restart to take effect&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;NB:&lt;/strong&gt; as humongous as this readme is, there is also a lot of undocumented features. Run copyparty with &lt;code&gt;--help&lt;/code&gt; to see all available global options; all of those can be used in the &lt;code&gt;[global]&lt;/code&gt; section of config files, and everything listed in &lt;code&gt;--help-flags&lt;/code&gt; can be used in volumes as volflags.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;if running in docker/podman, try this: &lt;code&gt;docker run --rm -it copyparty/ac --help&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;or see this: &lt;a href="https://ocv.me/copyparty/helptext.html"&gt;https://ocv.me/copyparty/helptext.html&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;or if you prefer plaintext, &lt;a href="https://ocv.me/copyparty/helptext.txt"&gt;https://ocv.me/copyparty/helptext.txt&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;zeroconf&lt;/h2&gt; 
&lt;p&gt;announce enabled services on the LAN (&lt;a href="https://user-images.githubusercontent.com/241032/215344737-0eae8d98-9496-4256-9aa8-cd2f6971810d.png"&gt;pic&lt;/a&gt;) -- &lt;code&gt;-z&lt;/code&gt; enables both &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#mdns"&gt;mdns&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#ssdp"&gt;ssdp&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--z-on&lt;/code&gt; / &lt;code&gt;--z-off&lt;/code&gt; limits the feature to certain networks&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;config file example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;[global]
  z      # enable all zeroconf features (mdns, ssdp)
  zm     # only enables mdns (does nothing since we already have z)
  z-on: 192.168.0.0/16, 10.1.2.0/24  # restrict to certain subnets
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;mdns&lt;/h3&gt; 
&lt;p&gt;LAN domain-name and feature announcer&lt;/p&gt; 
&lt;p&gt;uses &lt;a href="https://en.wikipedia.org/wiki/Multicast_DNS"&gt;multicast dns&lt;/a&gt; to give copyparty a domain which any machine on the LAN can use to access it&lt;/p&gt; 
&lt;p&gt;all enabled services (&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#webdav-server"&gt;webdav&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#ftp-server"&gt;ftp&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#smb-server"&gt;smb&lt;/a&gt;) will appear in mDNS-aware file managers (KDE, gnome, macOS, ...)&lt;/p&gt; 
&lt;p&gt;the domain will be &lt;code&gt;partybox.local&lt;/code&gt; if the machine's hostname is &lt;code&gt;partybox&lt;/code&gt; unless &lt;code&gt;--name&lt;/code&gt; specifies something else&lt;/p&gt; 
&lt;p&gt;and the web-UI will be available at &lt;a href="http://partybox.local:3923/"&gt;http://partybox.local:3923/&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;if you want to get rid of the &lt;code&gt;:3923&lt;/code&gt; so you can use &lt;a href="http://partybox.local/"&gt;http://partybox.local/&lt;/a&gt; instead then see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#listen-on-port-80-and-443"&gt;listen on port 80 and 443&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ssdp&lt;/h3&gt; 
&lt;p&gt;windows-explorer announcer&lt;/p&gt; 
&lt;p&gt;uses &lt;a href="https://en.wikipedia.org/wiki/Simple_Service_Discovery_Protocol"&gt;ssdp&lt;/a&gt; to make copyparty appear in the windows file explorer on all machines on the LAN&lt;/p&gt; 
&lt;p&gt;doubleclicking the icon opens the "connect" page which explains how to mount copyparty as a local filesystem&lt;/p&gt; 
&lt;p&gt;if copyparty does not appear in windows explorer, use &lt;code&gt;--zsv&lt;/code&gt; to see why:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;maybe the discovery multicast was sent from an IP which does not intersect with the server subnets&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;qr-code&lt;/h2&gt; 
&lt;p&gt;print a qr-code &lt;a href="https://user-images.githubusercontent.com/241032/194728533-6f00849b-c6ac-43c6-9359-83e454d11e00.png"&gt;(screenshot)&lt;/a&gt; for quick access, great between phones on android hotspots which keep changing the subnet&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--qr&lt;/code&gt; enables it&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--qrs&lt;/code&gt; does https instead of http&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--qrl lootbox/?pw=hunter2&lt;/code&gt; appends to the url, linking to the &lt;code&gt;lootbox&lt;/code&gt; folder with password &lt;code&gt;hunter2&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--qrz 1&lt;/code&gt; forces 1x zoom instead of autoscaling to fit the terminal size 
  &lt;ul&gt; 
   &lt;li&gt;1x may render incorrectly on some terminals/fonts, but 2x should always work&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--qr-pin 1&lt;/code&gt; makes the qr-code stick to the bottom of the console (never scrolls away)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--qr-file qr.txt:1:2&lt;/code&gt; writes a small qr-code to &lt;code&gt;qr.txt&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--qr-file qr.txt:2:2&lt;/code&gt; writes a big qr-code to &lt;code&gt;qr.txt&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--qr-file qr.svg:1:2&lt;/code&gt; writes a vector-graphics qr-code to &lt;code&gt;qr.svg&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--qr-file qr.png:8:4:333333:ffcc55&lt;/code&gt; writes an 8x-magnified yellow-on-gray &lt;code&gt;qr.png&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--qr-file qr.png:8:4::ffffff&lt;/code&gt; writes an 8x-magnified white-on-transparent &lt;code&gt;qr.png&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;it uses the server hostname if &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#mdns"&gt;mdns&lt;/a&gt; is enabled, otherwise it'll use your external ip (default route) unless &lt;code&gt;--qri&lt;/code&gt; specifies a specific ip-prefix or domain&lt;/p&gt; 
&lt;h2&gt;ftp server&lt;/h2&gt; 
&lt;p&gt;an FTP server can be started using &lt;code&gt;--ftp 3921&lt;/code&gt;, and/or &lt;code&gt;--ftps&lt;/code&gt; for explicit TLS (ftpes)&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;based on &lt;a href="https://github.com/giampaolo/pyftpdlib"&gt;pyftpdlib&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;needs a dedicated port (cannot share with the HTTP/HTTPS API)&lt;/li&gt; 
 &lt;li&gt;uploads are not resumable -- delete and restart if necessary&lt;/li&gt; 
 &lt;li&gt;runs in active mode by default, you probably want &lt;code&gt;--ftp-pr 12000-13000&lt;/code&gt; 
  &lt;ul&gt; 
   &lt;li&gt;if you enable both &lt;code&gt;ftp&lt;/code&gt; and &lt;code&gt;ftps&lt;/code&gt;, the port-range will be divided in half&lt;/li&gt; 
   &lt;li&gt;some older software (filezilla on debian-stable) cannot passive-mode with TLS&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;login with any username + your password, or put your password in the username field 
  &lt;ul&gt; 
   &lt;li&gt;unless you enabled &lt;code&gt;--usernames&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;some recommended FTP / FTPS clients; &lt;code&gt;wark&lt;/code&gt; = example password:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://winscp.net/eng/download.php"&gt;https://winscp.net/eng/download.php&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://filezilla-project.org/"&gt;https://filezilla-project.org/&lt;/a&gt; struggles a bit with ftps in active-mode, but is fine otherwise&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://rclone.org/"&gt;https://rclone.org/&lt;/a&gt; does FTPS with &lt;code&gt;tls=false explicit_tls=true&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;lftp -u k,wark -p 3921 127.0.0.1 -e ls&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;lftp -u k,wark -p 3990 127.0.0.1 -e 'set ssl:verify-certificate no; ls'&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;curl ftp://127.0.0.1:3921/&lt;/code&gt; (plaintext ftp)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;curl --ssl-reqd ftp://127.0.0.1:3990/&lt;/code&gt; (encrypted ftps)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;config file example, which restricts FTP to only use ports 3921 and 12000-12099 so all of those ports must be opened in your firewall:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;[global]
  ftp: 3921
  ftp-pr: 12000-12099
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;webdav server&lt;/h2&gt; 
&lt;p&gt;with read-write support, supports winXP and later, macos, nautilus/gvfs ... a great way to &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#mount-as-drive"&gt;access copyparty straight from the file explorer in your OS&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;click the &lt;a href="http://127.0.0.1:3923/?hc"&gt;connect&lt;/a&gt; button in the control-panel to see connection instructions for windows, linux, macos&lt;/p&gt; 
&lt;p&gt;general usage:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;login with any username + your password, or put your password in the username field (password field can be empty/whatever) 
  &lt;ul&gt; 
   &lt;li&gt;unless you enabled &lt;code&gt;--usernames&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;on macos, connect from finder:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;[Go] -&amp;gt; [Connect to Server...] -&amp;gt; &lt;a href="http://192.168.123.1:3923/"&gt;http://192.168.123.1:3923/&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;in order to grant full write-access to webdav clients, the volflag &lt;code&gt;daw&lt;/code&gt; must be set and the account must also have delete-access (otherwise the client won't be allowed to replace the contents of existing files, which is how webdav works)&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;note: if you have enabled &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#identity-providers"&gt;IdP authentication&lt;/a&gt; then that may cause issues for some/most webdav clients; see &lt;a href="https://github.com/9001/copyparty/raw/hovudstraum/docs/idp.md#connecting-webdav-clients"&gt;the webdav section in the IdP docs&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;connecting to webdav from windows&lt;/h3&gt; 
&lt;p&gt;using the GUI (winXP or later):&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;rightclick [my computer] -&amp;gt; [map network drive] -&amp;gt; Folder: &lt;code&gt;http://192.168.123.1:3923/&lt;/code&gt; 
  &lt;ul&gt; 
   &lt;li&gt;on winXP only, click the &lt;code&gt;Sign up for online storage&lt;/code&gt; hyperlink instead and put the URL there&lt;/li&gt; 
   &lt;li&gt;providing your password as the username is recommended; the password field can be anything or empty 
    &lt;ul&gt; 
     &lt;li&gt;unless you enabled &lt;code&gt;--usernames&lt;/code&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;the webdav client that's built into windows has the following list of bugs; you can avoid all of these by connecting with rclone instead:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;win7+ doesn't actually send the password to the server when reauthenticating after a reboot unless you first try to login with an incorrect password and then switch to the correct password 
  &lt;ul&gt; 
   &lt;li&gt;or just type your password into the username field instead to get around it entirely&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;connecting to a folder which allows anonymous read will make writing impossible, as windows has decided it doesn't need to login 
  &lt;ul&gt; 
   &lt;li&gt;workaround: connect twice; first to a folder which requires auth, then to the folder you actually want, and leave both of those mounted&lt;/li&gt; 
   &lt;li&gt;or set the server-option &lt;code&gt;--dav-auth&lt;/code&gt; to force password-auth for all webdav clients&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;win7+ may open a new tcp connection for every file and sometimes forgets to close them, eventually needing a reboot 
  &lt;ul&gt; 
   &lt;li&gt;maybe NIC-related (??), happens with win10-ltsc on e1000e but not virtio&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;windows cannot access folders which contain filenames with invalid unicode or forbidden characters (&lt;code&gt;&amp;lt;&amp;gt;:"/\|?*&lt;/code&gt;), or names ending with &lt;code&gt;.&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;winxp cannot show unicode characters outside of &lt;em&gt;some range&lt;/em&gt; 
  &lt;ul&gt; 
   &lt;li&gt;latin-1 is fine, hiragana is not (not even as shift-jis on japanese xp)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;tftp server&lt;/h2&gt; 
&lt;p&gt;a TFTP server (read/write) can be started using &lt;code&gt;--tftp 3969&lt;/code&gt; (you probably want &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#ftp-server"&gt;ftp&lt;/a&gt; instead unless you are &lt;em&gt;actually&lt;/em&gt; communicating with hardware from the 90s (in which case we should definitely hang some time))&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;that makes this the first RTX DECT Base that has been updated using copyparty 🎉&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;based on &lt;a href="https://github.com/9001/partftpy"&gt;partftpy&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;no accounts; read from world-readable folders, write to world-writable, overwrite in world-deletable&lt;/li&gt; 
 &lt;li&gt;needs a dedicated port (cannot share with the HTTP/HTTPS API) 
  &lt;ul&gt; 
   &lt;li&gt;run as root (or see below) to use the spec-recommended port &lt;code&gt;69&lt;/code&gt; (nice)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;can reply from a predefined portrange (good for firewalls)&lt;/li&gt; 
 &lt;li&gt;only supports the binary/octet/image transfer mode (no netascii)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://datatracker.ietf.org/doc/html/rfc7440"&gt;RFC 7440&lt;/a&gt; is &lt;strong&gt;not&lt;/strong&gt; supported, so will be extremely slow over WAN 
  &lt;ul&gt; 
   &lt;li&gt;assuming default blksize (512), expect 1100 KiB/s over 100BASE-T, 400-500 KiB/s over wifi, 200 on bad wifi&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;most clients expect to find TFTP on port 69, but on linux and macos you need to be root to listen on that. Alternatively, listen on 3969 and use NAT on the server to forward 69 to that port;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;on linux: &lt;code&gt;iptables -t nat -A PREROUTING -i eth0 -p udp --dport 69 -j REDIRECT --to-port 3969&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;some recommended TFTP clients:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;curl (cross-platform, read/write) 
  &lt;ul&gt; 
   &lt;li&gt;get: &lt;code&gt;curl --tftp-blksize 1428 tftp://127.0.0.1:3969/firmware.bin&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;put: &lt;code&gt;curl --tftp-blksize 1428 -T firmware.bin tftp://127.0.0.1:3969/&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;windows: &lt;code&gt;tftp.exe&lt;/code&gt; (you probably already have it) 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;tftp -i 127.0.0.1 put firmware.bin&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;linux: &lt;code&gt;tftp-hpa&lt;/code&gt;, &lt;code&gt;atftp&lt;/code&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;atftp --option "blksize 1428" 127.0.0.1 3969 -p -l firmware.bin -r firmware.bin&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;tftp -v -m binary 127.0.0.1 3969 -c put firmware.bin&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;smb server&lt;/h2&gt; 
&lt;p&gt;unsafe, slow, not recommended for wan, enable with &lt;code&gt;--smb&lt;/code&gt; for read-only or &lt;code&gt;--smbw&lt;/code&gt; for read-write&lt;/p&gt; 
&lt;p&gt;click the &lt;a href="http://127.0.0.1:3923/?hc"&gt;connect&lt;/a&gt; button in the control-panel to see connection instructions for windows, linux, macos&lt;/p&gt; 
&lt;p&gt;dependencies: &lt;code&gt;python3 -m pip install --user -U impacket==0.11.0&lt;/code&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;newer versions of impacket will hopefully work just fine but there is monkeypatching so maybe not&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;some &lt;strong&gt;BIG WARNINGS&lt;/strong&gt; specific to SMB/CIFS, in decreasing importance:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;not entirely confident that read-only is read-only&lt;/li&gt; 
 &lt;li&gt;the smb backend is not fully integrated with vfs, meaning there could be security issues (path traversal). Please use &lt;code&gt;--smb-port&lt;/code&gt; (see below) and &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/bin/prisonparty.sh"&gt;prisonparty&lt;/a&gt; or &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/bin/bubbleparty.sh"&gt;bubbleparty&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;account passwords work per-volume as expected, and so does account permissions (read/write/move/delete), but &lt;code&gt;--smbw&lt;/code&gt; must be given to allow write-access from smb&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#shadowing"&gt;shadowing&lt;/a&gt; probably works as expected but no guarantees&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;not compatible with pw-hashing or &lt;code&gt;--usernames&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;and some minor issues,&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;clients only see the first ~400 files in big folders; 
  &lt;ul&gt; 
   &lt;li&gt;this was originally due to &lt;a href="https://github.com/SecureAuthCorp/impacket/issues/1433"&gt;impacket#1433&lt;/a&gt; which was fixed in impacket-0.12, so you can disable the workaround with &lt;code&gt;--smb-nwa-1&lt;/code&gt; but then you get unacceptably poor performance instead&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;hot-reload of server config (&lt;code&gt;/?reload=cfg&lt;/code&gt;) does not include the &lt;code&gt;[global]&lt;/code&gt; section (commandline args)&lt;/li&gt; 
 &lt;li&gt;listens on the first IPv4 &lt;code&gt;-i&lt;/code&gt; interface only (default = :: = 0.0.0.0 = all)&lt;/li&gt; 
 &lt;li&gt;login doesn't work on winxp, but anonymous access is ok -- remove all accounts from copyparty config for that to work 
  &lt;ul&gt; 
   &lt;li&gt;win10 onwards does not allow connecting anonymously / without accounts&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;python3 only&lt;/li&gt; 
 &lt;li&gt;slow (the builtin webdav support in windows is 5x faster, and rclone-webdav is 30x faster) 
  &lt;ul&gt; 
   &lt;li&gt;those numbers are specifically for copyparty's smb-server (because it sucks); other smb-servers should be similar to webdav&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;known client bugs:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;on win7 only, &lt;code&gt;--smb1&lt;/code&gt; is much faster than smb2 (default) because it keeps rescanning folders on smb2 
  &lt;ul&gt; 
   &lt;li&gt;however smb1 is buggy and is not enabled by default on win10 onwards&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;windows cannot access folders which contain filenames with invalid unicode or forbidden characters (&lt;code&gt;&amp;lt;&amp;gt;:"/\|?*&lt;/code&gt;), or names ending with &lt;code&gt;.&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;the smb protocol listens on TCP port 445, which is a privileged port on linux and macos, which would require running copyparty as root. However, this can be avoided by listening on another port using &lt;code&gt;--smb-port 3945&lt;/code&gt; and then using NAT on the server to forward the traffic from 445 to there;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;on linux: &lt;code&gt;iptables -t nat -A PREROUTING -i eth0 -p tcp --dport 445 -j REDIRECT --to-port 3945&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;authenticate with one of the following:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;username &lt;code&gt;$username&lt;/code&gt;, password &lt;code&gt;$password&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;username &lt;code&gt;$password&lt;/code&gt;, password &lt;code&gt;k&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;browser ux&lt;/h2&gt; 
&lt;p&gt;tweaking the ui&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;set default sort order globally with &lt;code&gt;--sort&lt;/code&gt; or per-volume with the &lt;code&gt;sort&lt;/code&gt; volflag; specify one or more comma-separated columns to sort by, and prefix the column name with &lt;code&gt;-&lt;/code&gt; for reverse sort 
  &lt;ul&gt; 
   &lt;li&gt;the column names you can use are visible as tooltips when hovering over the column headers in the directory listing, for example &lt;code&gt;href ext sz ts tags/.up_at tags/Circle tags/.tn tags/Artist tags/Title&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;to sort in music order (album, track, artist, title) with filename as fallback, you could &lt;code&gt;--sort tags/Circle,tags/.tn,tags/Artist,tags/Title,href&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;to sort by upload date, first enable showing the upload date in the listing with &lt;code&gt;-e2d -mte +.up_at&lt;/code&gt; and then &lt;code&gt;--sort tags/.up_at&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/rice"&gt;./docs/rice&lt;/a&gt; for more, including how to add stuff (css/&lt;code&gt;&amp;lt;meta&amp;gt;&lt;/code&gt;/...) to the html &lt;code&gt;&amp;lt;head&amp;gt;&lt;/code&gt; tag, or to add your own translation&lt;/p&gt; 
&lt;h2&gt;opengraph&lt;/h2&gt; 
&lt;p&gt;discord and social-media embeds&lt;/p&gt; 
&lt;p&gt;can be enabled globally with &lt;code&gt;--og&lt;/code&gt; or per-volume with volflag &lt;code&gt;og&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;note that this disables hotlinking because the opengraph spec demands it; to sneak past this intentional limitation, you can enable opengraph selectively by user-agent, for example &lt;code&gt;--og-ua '(Discord|Twitter|Slack)bot'&lt;/code&gt; (or volflag &lt;code&gt;og_ua&lt;/code&gt;)&lt;/p&gt; 
&lt;p&gt;you can also hotlink files regardless by appending &lt;code&gt;?raw&lt;/code&gt; to the url&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;WARNING: if you plan to use WebDAV, then &lt;code&gt;--og-ua&lt;/code&gt; / &lt;code&gt;og_ua&lt;/code&gt; must be configured&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;if you want to entirely replace the copyparty response with your own jinja2 template, give the template filepath to &lt;code&gt;--og-tpl&lt;/code&gt; or volflag &lt;code&gt;og_tpl&lt;/code&gt; (all members of &lt;code&gt;HttpCli&lt;/code&gt; are available through the &lt;code&gt;this&lt;/code&gt; object)&lt;/p&gt; 
&lt;h2&gt;file deduplication&lt;/h2&gt; 
&lt;p&gt;enable symlink-based upload deduplication globally with &lt;code&gt;--dedup&lt;/code&gt; or per-volume with volflag &lt;code&gt;dedup&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;by default, when someone tries to upload a file that already exists on the server, the upload will be politely declined, and the server will copy the existing file over to where the upload would have gone&lt;/p&gt; 
&lt;p&gt;if you enable deduplication with &lt;code&gt;--dedup&lt;/code&gt; then it'll create a symlink instead of a full copy, thus reducing disk space usage&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;on the contrary, if your server is hooked up to s3-glacier or similar storage where reading is expensive, and you cannot use &lt;code&gt;--safe-dedup=1&lt;/code&gt; because you have other software tampering with your files, so you want to entirely disable detection of duplicate data instead, then you can specify &lt;code&gt;--no-clone&lt;/code&gt; globally or &lt;code&gt;noclone&lt;/code&gt; as a volflag&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;warning:&lt;/strong&gt; when enabling dedup, you should also:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;enable indexing with &lt;code&gt;-e2dsa&lt;/code&gt; or volflag &lt;code&gt;e2dsa&lt;/code&gt; (see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#file-indexing"&gt;file indexing&lt;/a&gt; section below); strongly recommended&lt;/li&gt; 
 &lt;li&gt;...and/or &lt;code&gt;--hardlink-only&lt;/code&gt; to use hardlink-based deduplication instead of symlinks; see explanation below&lt;/li&gt; 
 &lt;li&gt;...and/or &lt;code&gt;--reflink&lt;/code&gt; to use CoW/reflink-based dedup (much safer than hardlink, but OS/FS-dependent)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;it will not be safe to rename/delete files if you only enable dedup and none of the above; if you enable indexing then it is not &lt;em&gt;necessary&lt;/em&gt; to also do hardlinks (but you may still want to)&lt;/p&gt; 
&lt;p&gt;by default, deduplication is done based on symlinks (symbolic links); these are tiny files which are pointers to the nearest full copy of the file&lt;/p&gt; 
&lt;p&gt;you can choose to use hardlinks instead of softlinks, globally with &lt;code&gt;--hardlink-only&lt;/code&gt; or volflag &lt;code&gt;hardlinkonly&lt;/code&gt;, and you can choose to use reflinks with &lt;code&gt;--reflink&lt;/code&gt; or volflag &lt;code&gt;reflink&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;advantages of using reflinks (CoW, copy-on-write):&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;entirely safe (when your filesystem supports it correctly); either file can be edited or deleted without affecting other copies&lt;/li&gt; 
 &lt;li&gt;only linux 5.3 or newer, only python 3.14 or newer, only some filesystems (btrfs probably ok, maybe xfs too, but zfs had bugs)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;advantages of using hardlinks:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;hardlinks are more compatible with other software; they behave entirely like regular files&lt;/li&gt; 
 &lt;li&gt;you can safely move and rename files using other file managers 
  &lt;ul&gt; 
   &lt;li&gt;symlinks need to be managed by copyparty to ensure the destinations remain correct&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;advantages of using symlinks (default):&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;each symlink can have its own last-modified timestamp, but a single timestamp is shared by all hardlinks&lt;/li&gt; 
 &lt;li&gt;symlinks make it more obvious to other software that the file is not a regular file, so this can be less dangerous 
  &lt;ul&gt; 
   &lt;li&gt;hardlinks look like regular files, so other software may assume they are safe to edit without affecting the other copies&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;warning:&lt;/strong&gt; if you edit the contents of a deduplicated file, then you will also edit all other copies of that file! This is especially surprising with hardlinks, because they look like regular files, but that same file exists in multiple locations&lt;/p&gt; 
&lt;p&gt;global-option &lt;code&gt;--xlink&lt;/code&gt; / volflag &lt;code&gt;xlink&lt;/code&gt; additionally enables deduplication across volumes, but this is probably buggy and not recommended&lt;/p&gt; 
&lt;p&gt;config file example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;[global]
  e2dsa  # scan and index filesystem on startup
  dedup  # symlink-based deduplication for all volumes

[/media]
  /mnt/nas/media
  flags:
    hardlinkonly  # this vol does hardlinks instead of symlinks
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;file indexing&lt;/h2&gt; 
&lt;p&gt;enable music search, upload-undo, and better dedup&lt;/p&gt; 
&lt;p&gt;file indexing relies on two database tables, the up2k filetree (&lt;code&gt;-e2d&lt;/code&gt;) and the metadata tags (&lt;code&gt;-e2t&lt;/code&gt;), stored in &lt;code&gt;.hist/up2k.db&lt;/code&gt;. Configuration can be done through arguments, volflags, or a mix of both.&lt;/p&gt; 
&lt;p&gt;through arguments:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;-e2d&lt;/code&gt; enables file indexing on upload&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-e2ds&lt;/code&gt; also scans writable folders for new files on startup&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-e2dsa&lt;/code&gt; also scans all mounted volumes (including readonly ones)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-e2t&lt;/code&gt; enables metadata indexing on upload&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-e2ts&lt;/code&gt; also scans for tags in all files that don't have tags yet&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-e2tsr&lt;/code&gt; also deletes all existing tags, doing a full reindex&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-e2v&lt;/code&gt; verifies file integrity at startup, comparing hashes from the db&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-e2vu&lt;/code&gt; patches the database with the new hashes from the filesystem&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-e2vp&lt;/code&gt; panics and kills copyparty instead&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;the same arguments can be set as volflags, in addition to &lt;code&gt;d2d&lt;/code&gt;, &lt;code&gt;d2ds&lt;/code&gt;, &lt;code&gt;d2t&lt;/code&gt;, &lt;code&gt;d2ts&lt;/code&gt;, &lt;code&gt;d2v&lt;/code&gt; for disabling:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;-v ~/music::r:c,e2ds,e2tsr&lt;/code&gt; does a full reindex of everything on startup&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-v ~/music::r:c,d2d&lt;/code&gt; disables &lt;strong&gt;all&lt;/strong&gt; indexing, even if any &lt;code&gt;-e2*&lt;/code&gt; are on&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-v ~/music::r:c,d2t&lt;/code&gt; disables all &lt;code&gt;-e2t*&lt;/code&gt; (tags), does not affect &lt;code&gt;-e2d*&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-v ~/music::r:c,d2ds&lt;/code&gt; disables on-boot scans; only index new uploads&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-v ~/music::r:c,d2ts&lt;/code&gt; same except only affecting tags&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;note:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;upload-times can be displayed in the file listing by enabling the &lt;code&gt;.up_at&lt;/code&gt; metadata key, either globally with &lt;code&gt;-e2d -mte +.up_at&lt;/code&gt; or per-volume with volflags &lt;code&gt;e2d,mte=+.up_at&lt;/code&gt; (will have a ~17% performance impact on directory listings)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;e2tsr&lt;/code&gt; is probably always overkill, since &lt;code&gt;e2ds&lt;/code&gt;/&lt;code&gt;e2dsa&lt;/code&gt; would pick up any file modifications and &lt;code&gt;e2ts&lt;/code&gt; would then reindex those, unless there is a new copyparty version with new parsers and the release note says otherwise&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;config file example (these options are recommended btw):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;[global]
  e2dsa  # scan and index all files in all volumes on startup
  e2ts   # check newly-discovered or uploaded files for media tags
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;exclude-patterns&lt;/h3&gt; 
&lt;p&gt;to save some time, you can provide a regex pattern for filepaths to only index by filename/path/size/last-modified (and not the hash of the file contents) by setting &lt;code&gt;--no-hash '\.iso$'&lt;/code&gt; or the volflag &lt;code&gt;:c,nohash=\.iso$&lt;/code&gt;, this has the following consequences:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;initial indexing is way faster, especially when the volume is on a network disk&lt;/li&gt; 
 &lt;li&gt;makes it impossible to &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#file-search"&gt;file-search&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;if someone uploads the same file contents, the upload will not be detected as a dupe, so it will not get symlinked or rejected&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;similarly, you can fully ignore files/folders using &lt;code&gt;--no-idx [...]&lt;/code&gt; and &lt;code&gt;:c,noidx=\.iso$&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;NOTE: &lt;code&gt;no-idx&lt;/code&gt; and/or &lt;code&gt;no-hash&lt;/code&gt; prevents deduplication of those files&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;when running on macos, all the usual apple metadata files are excluded by default&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;if you set &lt;code&gt;--no-hash [...]&lt;/code&gt; globally, you can enable hashing for specific volumes using flag &lt;code&gt;:c,nohash=&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;to exclude certain filepaths from search-results, use &lt;code&gt;--srch-excl&lt;/code&gt; or volflag &lt;code&gt;srch_excl&lt;/code&gt; instead of &lt;code&gt;--no-idx&lt;/code&gt;, for example &lt;code&gt;--srch-excl 'password|logs/[0-9]'&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;config file example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;[/games]
  /mnt/nas/games
  flags:
    noidx: \.iso$  # skip indexing iso-files
    srch_excl: password|logs/[0-9]  # filter search results
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;filesystem guards&lt;/h3&gt; 
&lt;p&gt;avoid traversing into other filesystems using &lt;code&gt;--xdev&lt;/code&gt; / volflag &lt;code&gt;:c,xdev&lt;/code&gt;, skipping any symlinks or bind-mounts to another HDD for example&lt;/p&gt; 
&lt;p&gt;and/or you can &lt;code&gt;--xvol&lt;/code&gt; / &lt;code&gt;:c,xvol&lt;/code&gt; to ignore all symlinks leaving the volume's top directory, but still allow bind-mounts pointing elsewhere&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;symlinks are permitted with &lt;code&gt;xvol&lt;/code&gt; if they point into another volume where the user has the same level of access&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;these options will reduce performance; unlikely worst-case estimates are 14% reduction for directory listings, 35% for download-as-tar&lt;/p&gt; 
&lt;p&gt;as of copyparty v1.7.0 these options also prevent file access at runtime -- in previous versions it was just hints for the indexer&lt;/p&gt; 
&lt;h3&gt;periodic rescan&lt;/h3&gt; 
&lt;p&gt;filesystem monitoring; if copyparty is not the only software doing stuff on your filesystem, you may want to enable periodic rescans to keep the index up to date&lt;/p&gt; 
&lt;p&gt;argument &lt;code&gt;--re-maxage 60&lt;/code&gt; will rescan all volumes every 60 sec, same as volflag &lt;code&gt;:c,scan=60&lt;/code&gt; to specify it per-volume&lt;/p&gt; 
&lt;p&gt;uploads are disabled while a rescan is happening, so rescans will be delayed by &lt;code&gt;--db-act&lt;/code&gt; (default 10 sec) when there is write-activity going on (uploads, renames, ...)&lt;/p&gt; 
&lt;p&gt;note: folder-thumbnails are selected during filesystem indexing, so periodic rescans can be used to keep them accurate as images are uploaded/deleted (or manually do a rescan with the &lt;code&gt;reload&lt;/code&gt; button in the controlpanel)&lt;/p&gt; 
&lt;p&gt;config file example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;[global]
  re-maxage: 3600

[/pics]
  /mnt/nas/pics
  flags:
    scan: 900
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;upload rules&lt;/h2&gt; 
&lt;p&gt;set upload rules using volflags, some examples:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;:c,sz=1k-3m&lt;/code&gt; sets allowed filesize between 1 KiB and 3 MiB inclusive (suffixes: &lt;code&gt;b&lt;/code&gt;, &lt;code&gt;k&lt;/code&gt;, &lt;code&gt;m&lt;/code&gt;, &lt;code&gt;g&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;:c,df=4g&lt;/code&gt; block uploads if there would be less than 4 GiB free disk space afterwards&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;:c,vmaxb=1g&lt;/code&gt; block uploads if total volume size would exceed 1 GiB afterwards&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;:c,vmaxn=4k&lt;/code&gt; block uploads if volume would contain more than 4096 files afterwards&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;:c,nosub&lt;/code&gt; disallow uploading into subdirectories; goes well with &lt;code&gt;rotn&lt;/code&gt; and &lt;code&gt;rotf&lt;/code&gt;:&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;:c,rotn=1000,2&lt;/code&gt; moves uploads into subfolders, up to 1000 files in each folder before making a new one, two levels deep (must be at least 1)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;:c,rotf=%Y/%m/%d/%H&lt;/code&gt; enforces files to be uploaded into a structure of subfolders according to that date format 
  &lt;ul&gt; 
   &lt;li&gt;if someone uploads to &lt;code&gt;/foo/bar&lt;/code&gt; the path would be rewritten to &lt;code&gt;/foo/bar/2021/08/06/23&lt;/code&gt; for example&lt;/li&gt; 
   &lt;li&gt;but the actual value is not verified, just the structure, so the uploader can choose any values which conform to the format string 
    &lt;ul&gt; 
     &lt;li&gt;just to avoid additional complexity in up2k which is enough of a mess already&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;:c,lifetime=300&lt;/code&gt; delete uploaded files when they become 5 minutes old&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;you can also set transaction limits which apply per-IP and per-volume, but these assume &lt;code&gt;-j 1&lt;/code&gt; (default) otherwise the limits will be off, for example &lt;code&gt;-j 4&lt;/code&gt; would allow anywhere between 1x and 4x the limits you set depending on which processing node the client gets routed to&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;:c,maxn=250,3600&lt;/code&gt; allows 250 files over 1 hour from each IP (tracked per-volume)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;:c,maxb=1g,300&lt;/code&gt; allows 1 GiB total over 5 minutes from each IP (tracked per-volume)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;notes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;vmaxb&lt;/code&gt; and &lt;code&gt;vmaxn&lt;/code&gt; requires either the &lt;code&gt;e2ds&lt;/code&gt; volflag or &lt;code&gt;-e2dsa&lt;/code&gt; global-option&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;config file example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;[/inc]
  /mnt/nas/uploads
  accs:
    w: *    # anyone can upload here
    rw: ed  # only user "ed" can read-write
  flags:
    e2ds       # filesystem indexing is required for many of these:
    sz: 1k-3m  # accept upload only if filesize in this range
    df: 4g     # free disk space cannot go lower than this
    vmaxb: 1g  # volume can never exceed 1 GiB
    vmaxn: 4k  # ...or 4000 files, whichever comes first
    nosub      # must upload to toplevel folder
    lifetime: 300   # uploads are deleted after 5min
    maxn: 250,3600  # each IP can upload 250 files in 1 hour
    maxb: 1g,300    # each IP can upload 1 GiB over 5 minutes
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;compress uploads&lt;/h2&gt; 
&lt;p&gt;files can be autocompressed on upload, either on user-request (if config allows) or forced by server-config&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;volflag &lt;code&gt;gz&lt;/code&gt; allows gz compression&lt;/li&gt; 
 &lt;li&gt;volflag &lt;code&gt;xz&lt;/code&gt; allows lzma compression&lt;/li&gt; 
 &lt;li&gt;volflag &lt;code&gt;pk&lt;/code&gt; &lt;strong&gt;forces&lt;/strong&gt; compression on all files&lt;/li&gt; 
 &lt;li&gt;url parameter &lt;code&gt;pk&lt;/code&gt; requests compression with server-default algorithm&lt;/li&gt; 
 &lt;li&gt;url parameter &lt;code&gt;gz&lt;/code&gt; or &lt;code&gt;xz&lt;/code&gt; requests compression with a specific algorithm&lt;/li&gt; 
 &lt;li&gt;url parameter &lt;code&gt;xz&lt;/code&gt; requests xz compression&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;things to note,&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;the &lt;code&gt;gz&lt;/code&gt; and &lt;code&gt;xz&lt;/code&gt; arguments take a single optional argument, the compression level (range 0 to 9)&lt;/li&gt; 
 &lt;li&gt;the &lt;code&gt;pk&lt;/code&gt; volflag takes the optional argument &lt;code&gt;ALGORITHM,LEVEL&lt;/code&gt; which will then be forced for all uploads, for example &lt;code&gt;gz,9&lt;/code&gt; or &lt;code&gt;xz,0&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;default compression is gzip level 9&lt;/li&gt; 
 &lt;li&gt;all upload methods except up2k are supported&lt;/li&gt; 
 &lt;li&gt;the files will be indexed after compression, so dupe-detection and file-search will not work as expected&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;some examples,&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;-v inc:inc:w:c,pk=xz,0&lt;/code&gt;&lt;br /&gt; folder named inc, shared at inc, write-only for everyone, forces xz compression at level 0&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-v inc:inc:w:c,pk&lt;/code&gt;&lt;br /&gt; same write-only inc, but forces gz compression (default) instead of xz&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-v inc:inc:w:c,gz&lt;/code&gt;&lt;br /&gt; allows (but does not force) gz compression if client uploads to &lt;code&gt;/inc?pk&lt;/code&gt; or &lt;code&gt;/inc?gz&lt;/code&gt; or &lt;code&gt;/inc?gz=4&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;chmod and chown&lt;/h2&gt; 
&lt;p&gt;per-volume filesystem-permissions and ownership&lt;/p&gt; 
&lt;p&gt;by default:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;all folders are chmod 755&lt;/li&gt; 
 &lt;li&gt;files are usually chmod 644 (umask-defined)&lt;/li&gt; 
 &lt;li&gt;user/group is whatever copyparty is running as&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;this can be configured per-volume:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;volflag &lt;code&gt;chmod_f&lt;/code&gt; sets file permissions; default=&lt;code&gt;644&lt;/code&gt; (usually)&lt;/li&gt; 
 &lt;li&gt;volflag &lt;code&gt;chmod_d&lt;/code&gt; sets directory permissions; default=&lt;code&gt;755&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;volflag &lt;code&gt;uid&lt;/code&gt; sets the owner user-id&lt;/li&gt; 
 &lt;li&gt;volflag &lt;code&gt;gid&lt;/code&gt; sets the owner group-id&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;notes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;gid&lt;/code&gt; can only be set to one of the groups which the copyparty process is a member of&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;uid&lt;/code&gt; can only be set if copyparty is running as root (i appreciate your faith)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;other flags&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;:c,magic&lt;/code&gt; enables filetype detection for nameless uploads, same as &lt;code&gt;--magic&lt;/code&gt; 
  &lt;ul&gt; 
   &lt;li&gt;needs &lt;a href="https://pypi.org/project/python-magic/"&gt;https://pypi.org/project/python-magic/&lt;/a&gt; &lt;code&gt;python3 -m pip install --user -U python-magic&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;on windows grab this instead &lt;code&gt;python3 -m pip install --user -U python-magic-bin&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;database location&lt;/h2&gt; 
&lt;p&gt;in-volume (&lt;code&gt;.hist/up2k.db&lt;/code&gt;, default) or somewhere else&lt;/p&gt; 
&lt;p&gt;copyparty creates a subfolder named &lt;code&gt;.hist&lt;/code&gt; inside each volume where it stores the database, thumbnails, and some other stuff&lt;/p&gt; 
&lt;p&gt;this can instead be kept in a single place using the &lt;code&gt;--hist&lt;/code&gt; argument, or the &lt;code&gt;hist=&lt;/code&gt; volflag, or a mix of both:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--hist ~/.cache/copyparty -v ~/music::r:c,hist=-&lt;/code&gt; sets &lt;code&gt;~/.cache/copyparty&lt;/code&gt; as the default place to put volume info, but &lt;code&gt;~/music&lt;/code&gt; gets the regular &lt;code&gt;.hist&lt;/code&gt; subfolder (&lt;code&gt;-&lt;/code&gt; restores default behavior)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;by default, the per-volume &lt;code&gt;up2k.db&lt;/code&gt; sqlite3-database for &lt;code&gt;-e2d&lt;/code&gt; and &lt;code&gt;-e2t&lt;/code&gt; is stored next to the thumbnails according to the &lt;code&gt;--hist&lt;/code&gt; option, but the global-option &lt;code&gt;--dbpath&lt;/code&gt; and/or volflag &lt;code&gt;dbpath&lt;/code&gt; can be used to put the database somewhere else&lt;/p&gt; 
&lt;p&gt;if your storage backend is unreliable (NFS or bad HDDs), you can specify one or more "landmarks" to look for before doing anything database-related. A landmark is a file which is always expected to exist inside the volume. This avoids spurious filesystem rescans in the event of an outage. One line per landmark (see example below)&lt;/p&gt; 
&lt;p&gt;note:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;putting the hist-folders on an SSD is strongly recommended for performance&lt;/li&gt; 
 &lt;li&gt;markdown edits are always stored in a local &lt;code&gt;.hist&lt;/code&gt; subdirectory&lt;/li&gt; 
 &lt;li&gt;on windows the volflag path is cyglike, so &lt;code&gt;/c/temp&lt;/code&gt; means &lt;code&gt;C:\temp&lt;/code&gt; but use regular paths for &lt;code&gt;--hist&lt;/code&gt; 
  &lt;ul&gt; 
   &lt;li&gt;you can use cygpaths for volumes too, &lt;code&gt;-v C:\Users::r&lt;/code&gt; and &lt;code&gt;-v /c/users::r&lt;/code&gt; both work&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;config file example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;[global]
  hist: ~/.cache/copyparty  # put db/thumbs/etc. here by default

[/pics]
  /mnt/nas/pics
  flags:
    hist: -  # restore the default (/mnt/nas/pics/.hist/)
    hist: /mnt/nas/cache/pics/  # can be absolute path
    landmark: me.jpg  # /mnt/nas/pics/me.jpg must be readable to enable db
    landmark: info/a.txt^=ok  # and this textfile must start with "ok"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;metadata from audio files&lt;/h2&gt; 
&lt;p&gt;set &lt;code&gt;-e2t&lt;/code&gt; to index tags on upload&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;-mte&lt;/code&gt; decides which tags to index and display in the browser (and also the display order), this can be changed per-volume:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;-v ~/music::r:c,mte=title,artist&lt;/code&gt; indexes and displays &lt;em&gt;title&lt;/em&gt; followed by &lt;em&gt;artist&lt;/em&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;if you add/remove a tag from &lt;code&gt;mte&lt;/code&gt; you will need to run with &lt;code&gt;-e2tsr&lt;/code&gt; once to rebuild the database, otherwise only new files will be affected&lt;/p&gt; 
&lt;p&gt;but instead of using &lt;code&gt;-mte&lt;/code&gt;, &lt;code&gt;-mth&lt;/code&gt; is a better way to hide tags in the browser: these tags will not be displayed by default, but they still get indexed and become searchable, and users can choose to unhide them in the &lt;code&gt;[⚙️] config&lt;/code&gt; pane&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;-mtm&lt;/code&gt; can be used to add or redefine a metadata mapping, say you have media files with &lt;code&gt;foo&lt;/code&gt; and &lt;code&gt;bar&lt;/code&gt; tags and you want them to display as &lt;code&gt;qux&lt;/code&gt; in the browser (preferring &lt;code&gt;foo&lt;/code&gt; if both are present), then do &lt;code&gt;-mtm qux=foo,bar&lt;/code&gt; and now you can &lt;code&gt;-mte artist,title,qux&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;tags that start with a &lt;code&gt;.&lt;/code&gt; such as &lt;code&gt;.bpm&lt;/code&gt; and &lt;code&gt;.dur&lt;/code&gt;(ation) indicate numeric value&lt;/p&gt; 
&lt;p&gt;see the beautiful mess of a dictionary in &lt;a href="https://github.com/9001/copyparty/raw/hovudstraum/copyparty/mtag.py"&gt;mtag.py&lt;/a&gt; for the default mappings (should cover mp3,opus,flac,m4a,wav,aif,)&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;--no-mutagen&lt;/code&gt; disables Mutagen and uses FFprobe instead, which...&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;is about 20x slower than Mutagen&lt;/li&gt; 
 &lt;li&gt;catches a few tags that Mutagen doesn't 
  &lt;ul&gt; 
   &lt;li&gt;melodic key, video resolution, framerate, pixfmt&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;avoids pulling any GPL code into copyparty&lt;/li&gt; 
 &lt;li&gt;more importantly runs FFprobe on incoming files which is bad if your FFmpeg has a cve&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;code&gt;--mtag-to&lt;/code&gt; sets the tag-scan timeout; very high default (60 sec) to cater for zfs and other randomly-freezing filesystems. Lower values like 10 are usually safe, allowing for faster processing of tricky files&lt;/p&gt; 
&lt;h2&gt;file parser plugins&lt;/h2&gt; 
&lt;p&gt;provide custom parsers to index additional tags, also see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/bin/mtag/README.md"&gt;./bin/mtag/README.md&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;copyparty can invoke external programs to collect additional metadata for files using &lt;code&gt;mtp&lt;/code&gt; (either as argument or volflag), there is a default timeout of 60sec, and only files which contain audio get analyzed by default (see ay/an/ad below)&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;-mtp .bpm=~/bin/audio-bpm.py&lt;/code&gt; will execute &lt;code&gt;~/bin/audio-bpm.py&lt;/code&gt; with the audio file as argument 1 to provide the &lt;code&gt;.bpm&lt;/code&gt; tag, if that does not exist in the audio metadata&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-mtp key=f,t5,~/bin/audio-key.py&lt;/code&gt; uses &lt;code&gt;~/bin/audio-key.py&lt;/code&gt; to get the &lt;code&gt;key&lt;/code&gt; tag, replacing any existing metadata tag (&lt;code&gt;f,&lt;/code&gt;), aborting if it takes longer than 5sec (&lt;code&gt;t5,&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-v ~/music::r:c,mtp=.bpm=~/bin/audio-bpm.py:c,mtp=key=f,t5,~/bin/audio-key.py&lt;/code&gt; both as a per-volume config wow this is getting ugly&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;em&gt;but wait, there's more!&lt;/em&gt; &lt;code&gt;-mtp&lt;/code&gt; can be used for non-audio files as well using the &lt;code&gt;a&lt;/code&gt; flag: &lt;code&gt;ay&lt;/code&gt; only do audio files (default), &lt;code&gt;an&lt;/code&gt; only do non-audio files, or &lt;code&gt;ad&lt;/code&gt; do all files (d as in dontcare)&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;"audio file" also means videos btw, as long as there is an audio stream&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-mtp ext=an,~/bin/file-ext.py&lt;/code&gt; runs &lt;code&gt;~/bin/file-ext.py&lt;/code&gt; to get the &lt;code&gt;ext&lt;/code&gt; tag only if file is not audio (&lt;code&gt;an&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-mtp arch,built,ver,orig=an,eexe,edll,~/bin/exe.py&lt;/code&gt; runs &lt;code&gt;~/bin/exe.py&lt;/code&gt; to get properties about windows-binaries only if file is not audio (&lt;code&gt;an&lt;/code&gt;) and file extension is exe or dll&lt;/li&gt; 
 &lt;li&gt;if you want to daisychain parsers, use the &lt;code&gt;p&lt;/code&gt; flag to set processing order 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;-mtp foo=p1,~/a.py&lt;/code&gt; runs before &lt;code&gt;-mtp foo=p2,~/b.py&lt;/code&gt; and will forward all the tags detected so far as json to the stdin of b.py&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;option &lt;code&gt;c0&lt;/code&gt; disables capturing of stdout/stderr, so copyparty will not receive any tags from the process at all -- instead the invoked program is free to print whatever to the console, just using copyparty as a launcher 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;c1&lt;/code&gt; captures stdout only, &lt;code&gt;c2&lt;/code&gt; only stderr, and &lt;code&gt;c3&lt;/code&gt; (default) captures both&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;you can control how the parser is killed if it times out with option &lt;code&gt;kt&lt;/code&gt; killing the entire process tree (default), &lt;code&gt;km&lt;/code&gt; just the main process, or &lt;code&gt;kn&lt;/code&gt; let it continue running until copyparty is terminated&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;if something doesn't work, try &lt;code&gt;--mtag-v&lt;/code&gt; for verbose error messages&lt;/p&gt; 
&lt;p&gt;config file example; note that &lt;code&gt;mtp&lt;/code&gt; is an additive option so all of the mtp options will take effect:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;[/music]
  /mnt/nas/music
  flags:
    mtp: .bpm=~/bin/audio-bpm.py  # assign ".bpm" (numeric) with script
    mtp: key=f,t5,~/bin/audio-key.py  # force/overwrite, 5sec timeout
    mtp: ext=an,~/bin/file-ext.py  # will only run on non-audio files
    mtp: arch,built,ver,orig=an,eexe,edll,~/bin/exe.py  # only exe/dll
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;event hooks&lt;/h2&gt; 
&lt;p&gt;trigger a program on uploads, renames etc (&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/bin/hooks/"&gt;examples&lt;/a&gt;)&lt;/p&gt; 
&lt;p&gt;you can set hooks before and/or after an event happens, and currently you can hook uploads, moves/renames, and deletes&lt;/p&gt; 
&lt;p&gt;there's a bunch of flags and stuff, see &lt;code&gt;--help-hooks&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;if you want to write your own hooks, see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/devnotes.md#event-hooks"&gt;devnotes&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;zeromq&lt;/h3&gt; 
&lt;p&gt;event-hooks can send zeromq messages instead of running programs&lt;/p&gt; 
&lt;p&gt;to send a 0mq message every time a file is uploaded,&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--xau zmq:pub:tcp://*:5556&lt;/code&gt; sends a PUB to any/all connected SUB clients&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--xau t3,zmq:push:tcp://*:5557&lt;/code&gt; sends a PUSH to exactly one connected PULL client&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--xau t3,j,zmq:req:tcp://localhost:5555&lt;/code&gt; sends a REQ to the connected REP client&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;the PUSH and REQ examples have &lt;code&gt;t3&lt;/code&gt; (timeout after 3 seconds) because they block if there's no clients to talk to&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;the REQ example does &lt;code&gt;t3,j&lt;/code&gt; to send extended upload-info as json instead of just the filesystem-path&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;see &lt;a href="https://github.com/9001/copyparty/raw/hovudstraum/bin/zmq-recv.py"&gt;zmq-recv.py&lt;/a&gt; if you need something to receive the messages with&lt;/p&gt; 
&lt;p&gt;config file example; note that the hooks are additive options, so all of the xau options will take effect:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;[global]
  xau: zmq:pub:tcp://*:5556`  # send a PUB to any/all connected SUB clients
  xau: t3,zmq:push:tcp://*:5557`  # send PUSH to exactly one connected PULL cli
  xau: t3,j,zmq:req:tcp://localhost:5555`  # send REQ to the connected REP cli
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;upload events&lt;/h3&gt; 
&lt;p&gt;the older, more powerful approach (&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/bin/mtag/"&gt;examples&lt;/a&gt;):&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;-v /mnt/inc:inc:w:c,e2d,e2t,mte=+x1:c,mtp=x1=ad,kn,/usr/bin/notify-send
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;that was the commandline example; here's the config file example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;[/inc]
  /mnt/inc
  accs:
    w: *
  flags:
    e2d, e2t  # enable indexing of uploaded files and their tags
    mte: +x1
    mtp: x1=ad,kn,/usr/bin/notify-send
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;so filesystem location &lt;code&gt;/mnt/inc&lt;/code&gt; shared at &lt;code&gt;/inc&lt;/code&gt;, write-only for everyone, appending &lt;code&gt;x1&lt;/code&gt; to the list of tags to index (&lt;code&gt;mte&lt;/code&gt;), and using &lt;code&gt;/usr/bin/notify-send&lt;/code&gt; to "provide" tag &lt;code&gt;x1&lt;/code&gt; for any filetype (&lt;code&gt;ad&lt;/code&gt;) with kill-on-timeout disabled (&lt;code&gt;kn&lt;/code&gt;)&lt;/p&gt; 
&lt;p&gt;that'll run the command &lt;code&gt;notify-send&lt;/code&gt; with the path to the uploaded file as the first and only argument (so on linux it'll show a notification on-screen)&lt;/p&gt; 
&lt;p&gt;note that this is way more complicated than the new &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#event-hooks"&gt;event hooks&lt;/a&gt; but this approach has the following advantages:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;non-blocking and multithreaded; doesn't hold other uploads back&lt;/li&gt; 
 &lt;li&gt;you get access to tags from FFmpeg and other mtp parsers&lt;/li&gt; 
 &lt;li&gt;only trigger on new unique files, not dupes&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;note that it will occupy the parsing threads, so fork anything expensive (or set &lt;code&gt;kn&lt;/code&gt; to have copyparty fork it for you) -- otoh if you want to intentionally queue/singlethread you can combine it with &lt;code&gt;--mtag-mt 1&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;for reference, if you were to do this using event hooks instead, it would be like this: &lt;code&gt;-e2d --xau notify-send,hello,--&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;handlers&lt;/h2&gt; 
&lt;p&gt;redefine behavior with plugins (&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/bin/handlers/"&gt;examples&lt;/a&gt;)&lt;/p&gt; 
&lt;p&gt;replace 404 and 403 errors with something completely different (that's it for now)&lt;/p&gt; 
&lt;p&gt;as for client-side stuff, there is &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/contrib/plugins/"&gt;plugins for modifying UI/UX&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ip auth&lt;/h2&gt; 
&lt;p&gt;autologin based on IP range (CIDR) , using the global-option &lt;code&gt;--ipu&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;for example, if everyone with an IP that starts with &lt;code&gt;192.168.123&lt;/code&gt; should automatically log in as the user &lt;code&gt;spartacus&lt;/code&gt;, then you can either specify &lt;code&gt;--ipu=192.168.123.0/24=spartacus&lt;/code&gt; as a commandline option, or put this in a config file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;[global]
  ipu: 192.168.123.0/24=spartacus
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;repeat the option to map additional subnets&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;be careful with this one!&lt;/strong&gt; if you have a reverseproxy, then you definitely want to make sure you have &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#real-ip"&gt;real-ip&lt;/a&gt; configured correctly, and it's probably a good idea to nullmap the reverseproxy's IP just in case; so if your reverseproxy is sending requests from &lt;code&gt;172.24.27.9&lt;/code&gt; then that would be &lt;code&gt;--ipu=172.24.27.9/32=&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;restrict to ip&lt;/h3&gt; 
&lt;p&gt;limit a user to certain IP ranges (CIDR) , using the global-option &lt;code&gt;--ipr&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;for example, if the user &lt;code&gt;spartacus&lt;/code&gt; should get rejected if they're not connecting from an IP that starts with &lt;code&gt;192.168.123&lt;/code&gt; or &lt;code&gt;172.16&lt;/code&gt;, then you can either specify &lt;code&gt;--ipr=192.168.123.0/24,172.16.0.0/16=spartacus&lt;/code&gt; as a commandline option, or put this in a config file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;[global]
  ipr: 192.168.123.0/24,172.16.0.0/16=spartacus
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;repeat the option to map additional users&lt;/p&gt; 
&lt;h2&gt;identity providers&lt;/h2&gt; 
&lt;p&gt;replace copyparty passwords with oauth and such&lt;/p&gt; 
&lt;p&gt;you can disable the built-in password-based login system, and instead replace it with a separate piece of software (an identity provider) which will then handle authenticating / authorizing of users; this makes it possible to login with passkeys / fido2 / webauthn / yubikey / ldap / active directory / oauth / many other single-sign-on contraptions&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;the regular config-defined users will be used as a fallback for requests which don't include a valid (trusted) IdP username header&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;if your IdP-server is slow, consider &lt;code&gt;--idp-cookie&lt;/code&gt; and let requests with the cookie &lt;code&gt;cppws&lt;/code&gt; bypass the IdP; experimental sessions-based feature added for a party&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;some popular identity providers are &lt;a href="https://www.authelia.com/"&gt;Authelia&lt;/a&gt; (config-file based) and &lt;a href="https://goauthentik.io/"&gt;authentik&lt;/a&gt; (GUI-based, more complex)&lt;/p&gt; 
&lt;p&gt;there is a &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/examples/docker/idp-authelia-traefik"&gt;docker-compose example&lt;/a&gt; which is hopefully a good starting point (alternatively see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/idp.md"&gt;./docs/idp.md&lt;/a&gt; if you're the DIY type)&lt;/p&gt; 
&lt;p&gt;a more complete example of the copyparty configuration options &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/examples/docker/idp/copyparty.conf"&gt;look like this&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;but if you just want to let users change their own passwords, then you probably want &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#user-changeable-passwords"&gt;user-changeable passwords&lt;/a&gt; instead&lt;/p&gt; 
&lt;h3&gt;generic header auth&lt;/h3&gt; 
&lt;p&gt;other ways to auth by header&lt;/p&gt; 
&lt;p&gt;if you have a middleware which adds a header with a user identifier, for example tailscale's &lt;code&gt;Tailscale-User-Login: alice.m@forest.net&lt;/code&gt; then you can automatically auth as &lt;code&gt;alice&lt;/code&gt; by defining that mapping with &lt;code&gt;--idp-hm-usr '^Tailscale-User-Login^alice.m@forest.net^alice'&lt;/code&gt; or the following config file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;[global]
  idp-hm-usr: ^Tailscale-User-Login^alice.m@forest.net^alice
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;repeat the whole &lt;code&gt;idp-hm-usr&lt;/code&gt; option to add more mappings&lt;/p&gt; 
&lt;h2&gt;user-changeable passwords&lt;/h2&gt; 
&lt;p&gt;if permitted, users can change their own passwords in the control-panel&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;not compatible with &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#identity-providers"&gt;identity providers&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;must be enabled with &lt;code&gt;--chpw&lt;/code&gt; because account-sharing is a popular usecase&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;if you want to enable the feature but deny password-changing for a specific list of accounts, you can do that with &lt;code&gt;--chpw-no name1,name2,name3,...&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;to perform a password reset, edit the server config and give the user another password there, then do a &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#server-config"&gt;config reload&lt;/a&gt; or server restart&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;the custom passwords are kept in a textfile at filesystem-path &lt;code&gt;--chpw-db&lt;/code&gt;, by default &lt;code&gt;chpw.json&lt;/code&gt; in the copyparty config folder&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;if you run multiple copyparty instances with different users you &lt;em&gt;almost definitely&lt;/em&gt; want to specify separate DBs for each instance&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;if &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#password-hashing"&gt;password hashing&lt;/a&gt; is enabled, the passwords in the db are also hashed&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;...which means that all user-defined passwords will be forgotten if you change password-hashing settings&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;using the cloud as storage&lt;/h2&gt; 
&lt;p&gt;connecting to an aws s3 bucket and similar&lt;/p&gt; 
&lt;p&gt;there is no built-in support for this, but you can use FUSE-software such as &lt;a href="https://rclone.org/"&gt;rclone&lt;/a&gt; / &lt;a href="https://github.com/yandex-cloud/geesefs"&gt;geesefs&lt;/a&gt; / &lt;a href="https://juicefs.com/en/"&gt;JuiceFS&lt;/a&gt; to first mount your cloud storage as a local disk, and then let copyparty use (a folder in) that disk as a volume&lt;/p&gt; 
&lt;p&gt;if copyparty is unable to access the local folder that rclone/geesefs/JuiceFS provides (for example if it looks invisible) then you may need to run rclone with &lt;code&gt;--allow-other&lt;/code&gt; and/or enable &lt;code&gt;user_allow_other&lt;/code&gt; in &lt;code&gt;/etc/fuse.conf&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;you will probably get decent speeds with the default config, however most likely restricted to using one TCP connection per file, so the upload-client won't be able to send multiple chunks in parallel&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;before &lt;a href="https://github.com/9001/copyparty/releases/tag/v1.13.5"&gt;v1.13.5&lt;/a&gt; it was recommended to use the volflag &lt;code&gt;sparse&lt;/code&gt; to force-allow multiple chunks in parallel; this would improve the upload-speed from &lt;code&gt;1.5 MiB/s&lt;/code&gt; to over &lt;code&gt;80 MiB/s&lt;/code&gt; at the risk of provoking latent bugs in S3 or JuiceFS. But v1.13.5 added chunk-stitching, so this is now probably much less important. On the contrary, &lt;code&gt;nosparse&lt;/code&gt; &lt;em&gt;may&lt;/em&gt; now increase performance in some cases. Please try all three options (default, &lt;code&gt;sparse&lt;/code&gt;, &lt;code&gt;nosparse&lt;/code&gt;) as the optimal choice depends on your network conditions and software stack (both the FUSE-driver and cloud-server)&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;someone has also tested geesefs in combination with &lt;a href="https://nuetzlich.net/gocryptfs/"&gt;gocryptfs&lt;/a&gt; with surprisingly good results, getting 60 MiB/s upload speeds on a gbit line, but JuiceFS won with 80 MiB/s using its built-in encryption&lt;/p&gt; 
&lt;p&gt;you may improve performance by specifying larger values for &lt;code&gt;--iobuf&lt;/code&gt; / &lt;code&gt;--s-rd-sz&lt;/code&gt; / &lt;code&gt;--s-wr-sz&lt;/code&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;if you've experimented with this and made interesting observations, please share your findings so we can add a section with specific recommendations :-)&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;hiding from google&lt;/h2&gt; 
&lt;p&gt;tell search engines you don't wanna be indexed, either using the good old &lt;a href="https://www.robotstxt.org/robotstxt.html"&gt;robots.txt&lt;/a&gt; or through copyparty settings:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--no-robots&lt;/code&gt; adds HTTP (&lt;code&gt;X-Robots-Tag&lt;/code&gt;) and HTML (&lt;code&gt;&amp;lt;meta&amp;gt;&lt;/code&gt;) headers with &lt;code&gt;noindex, nofollow&lt;/code&gt; globally&lt;/li&gt; 
 &lt;li&gt;volflag &lt;code&gt;[...]:c,norobots&lt;/code&gt; does the same thing for that single volume&lt;/li&gt; 
 &lt;li&gt;volflag &lt;code&gt;[...]:c,robots&lt;/code&gt; ALLOWS search-engine crawling for that volume, even if &lt;code&gt;--no-robots&lt;/code&gt; is set globally&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;also, &lt;code&gt;--force-js&lt;/code&gt; disables the plain HTML folder listing, making things harder to parse for &lt;em&gt;some&lt;/em&gt; search engines -- note that crawlers which understand javascript (such as google) will not be affected&lt;/p&gt; 
&lt;h2&gt;themes&lt;/h2&gt; 
&lt;p&gt;you can change the default theme with &lt;code&gt;--theme 2&lt;/code&gt;, and add your own themes by modifying &lt;code&gt;browser.css&lt;/code&gt; or providing your own css to &lt;code&gt;--css-browser&lt;/code&gt;, then telling copyparty they exist by increasing &lt;code&gt;--themes&lt;/code&gt;&lt;/p&gt; 
&lt;table&gt;
 &lt;tbody&gt;
  &lt;tr&gt;
   &lt;td width="33%" align="center"&gt;&lt;a href="https://user-images.githubusercontent.com/241032/165864907-17e2ac7d-319d-4f25-8718-2f376f614b51.png"&gt;&lt;img src="https://user-images.githubusercontent.com/241032/165867551-fceb35dd-38f0-42bb-bef3-25ba651ca69b.png" /&gt;&lt;/a&gt; 0. classic dark&lt;/td&gt;
   &lt;td width="33%" align="center"&gt;&lt;a href="https://user-images.githubusercontent.com/241032/168644399-68938de5-da9b-445f-8d92-b51c74b5f345.png"&gt;&lt;img src="https://user-images.githubusercontent.com/241032/168644404-8e1a2fdc-6e59-4c41-905e-ba5399ed686f.png" /&gt;&lt;/a&gt; 2. flat pm-monokai&lt;/td&gt;
   &lt;td width="33%" align="center"&gt;&lt;a href="https://user-images.githubusercontent.com/241032/165864901-db13a429-a5da-496d-8bc6-ce838547f69d.png"&gt;&lt;img src="https://user-images.githubusercontent.com/241032/165867560-aa834aef-58dc-4abe-baef-7e562b647945.png" /&gt;&lt;/a&gt; 4. vice&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td align="center"&gt;&lt;a href="https://user-images.githubusercontent.com/241032/165864905-692682eb-6fb4-4d40-b6fe-27d2c7d3e2a7.png"&gt;&lt;img src="https://user-images.githubusercontent.com/241032/165867555-080b73b6-6d85-41bb-a7c6-ad277c608365.png" /&gt;&lt;/a&gt; 1. classic light&lt;/td&gt;
   &lt;td align="center"&gt;&lt;a href="https://user-images.githubusercontent.com/241032/168645276-fb02fd19-190a-407a-b8d3-d58fee277e02.png"&gt;&lt;img src="https://user-images.githubusercontent.com/241032/168645280-f0662b3c-9764-4875-a2e2-d91cc8199b23.png" /&gt;&lt;/a&gt; 3. flat light &lt;/td&gt;
   &lt;td align="center"&gt;&lt;a href="https://user-images.githubusercontent.com/241032/165864898-10ce7052-a117-4fcf-845b-b56c91687908.png"&gt;&lt;img src="https://user-images.githubusercontent.com/241032/165867562-f3003d45-dd2a-4564-8aae-fed44c1ae064.png" /&gt;&lt;/a&gt; 5. &lt;a href="https://blog.codinghorror.com/a-tribute-to-the-windows-31-hot-dog-stand-color-scheme/"&gt;hotdog stand&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;p&gt;the classname of the HTML tag is set according to the selected theme, which is used to set colors as css variables ++&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;each theme &lt;em&gt;generally&lt;/em&gt; has a dark theme (even numbers) and a light theme (odd numbers), showing in pairs&lt;/li&gt; 
 &lt;li&gt;the first theme (theme 0 and 1) is &lt;code&gt;html.a&lt;/code&gt;, second theme (2 and 3) is &lt;code&gt;html.b&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;if a light theme is selected, &lt;code&gt;html.y&lt;/code&gt; is set, otherwise &lt;code&gt;html.z&lt;/code&gt; is&lt;/li&gt; 
 &lt;li&gt;so if the dark edition of the 2nd theme is selected, you use any of &lt;code&gt;html.b&lt;/code&gt;, &lt;code&gt;html.z&lt;/code&gt;, &lt;code&gt;html.bz&lt;/code&gt; to specify rules&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;see the top of &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/copyparty/web/browser.css"&gt;./copyparty/web/browser.css&lt;/a&gt; where the color variables are set, and there's layout-specific stuff near the bottom&lt;/p&gt; 
&lt;p&gt;if you want to change the fonts, see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/rice/"&gt;./docs/rice/&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;complete examples&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/examples/windows.md"&gt;running on windows&lt;/a&gt; for a fancy windows setup&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;or use any of the examples below, just replace &lt;code&gt;python copyparty-sfx.py&lt;/code&gt; with &lt;code&gt;copyparty.exe&lt;/code&gt; if you're using the exe edition&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;allow anyone to download or upload files into the current folder:&lt;br /&gt; &lt;code&gt;python copyparty-sfx.py&lt;/code&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;enable searching and music indexing with &lt;code&gt;-e2dsa -e2ts&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;start an FTP server on port 3921 with &lt;code&gt;--ftp 3921&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;announce it on your LAN with &lt;code&gt;-z&lt;/code&gt; so it appears in windows/Linux file managers&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;anyone can upload, but nobody can see any files (even the uploader):&lt;br /&gt; &lt;code&gt;python copyparty-sfx.py -e2dsa -v .::w&lt;/code&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;block uploads if there's less than 4 GiB free disk space with &lt;code&gt;--df 4&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;show a popup on new uploads with &lt;code&gt;--xau bin/hooks/notify.py&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;anyone can upload, and receive "secret" links for each upload they do:&lt;br /&gt; &lt;code&gt;python copyparty-sfx.py -e2dsa -v .::wG:c,fk=8&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;anyone can browse (&lt;code&gt;r&lt;/code&gt;), only &lt;code&gt;kevin&lt;/code&gt; (password &lt;code&gt;okgo&lt;/code&gt;) can upload/move/delete (&lt;code&gt;A&lt;/code&gt;) files:&lt;br /&gt; &lt;code&gt;python copyparty-sfx.py -e2dsa -a kevin:okgo -v .::r:A,kevin&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;read-only music server:&lt;br /&gt; &lt;code&gt;python copyparty-sfx.py -v /mnt/nas/music:/music:r -e2dsa -e2ts --no-robots --force-js --theme 2&lt;/code&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;...with bpm and key scanning&lt;br /&gt; &lt;code&gt;-mtp .bpm=f,audio-bpm.py -mtp key=f,audio-key.py&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;...with a read-write folder for &lt;code&gt;kevin&lt;/code&gt; whose password is &lt;code&gt;okgo&lt;/code&gt;&lt;br /&gt; &lt;code&gt;-a kevin:okgo -v /mnt/nas/inc:/inc:rw,kevin&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;...with logging to disk&lt;br /&gt; &lt;code&gt;-lo log/cpp-%Y-%m%d-%H%M%S.txt.xz&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;listen on port 80 and 443&lt;/h2&gt; 
&lt;p&gt;become a &lt;em&gt;real&lt;/em&gt; webserver which people can access by just going to your IP or domain without specifying a port&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;if you're on windows,&lt;/strong&gt; then you just need to add the commandline argument &lt;code&gt;-p 80,443&lt;/code&gt; and you're done! nice&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;if you're on macos,&lt;/strong&gt; sorry, I don't know&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;if you're on Linux,&lt;/strong&gt; you have the following 4 options:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;option 1:&lt;/strong&gt; set up a &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#reverse-proxy"&gt;reverse-proxy&lt;/a&gt; -- this one makes a lot of sense if you're running on a proper headless server, because that way you get real HTTPS too&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;option 2:&lt;/strong&gt; NAT to port 3923 -- this is cumbersome since you'll need to do it every time you reboot, and the exact command may depend on your linux distribution:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;iptables -t nat -A PREROUTING -p tcp --dport 80 -j REDIRECT --to-port 3923
iptables -t nat -A PREROUTING -p tcp --dport 443 -j REDIRECT --to-port 3923
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;option 3:&lt;/strong&gt; disable the &lt;a href="https://www.w3.org/Daemon/User/Installation/PrivilegedPorts.html"&gt;security policy&lt;/a&gt; which prevents the use of 80 and 443; this is &lt;em&gt;probably&lt;/em&gt; fine:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;setcap CAP_NET_BIND_SERVICE=+eip $(realpath $(which python))
python copyparty-sfx.py -p 80,443
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;option 4:&lt;/strong&gt; run copyparty as root (please don't)&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;reverse-proxy&lt;/h2&gt; 
&lt;p&gt;running copyparty next to other websites hosted on an existing webserver such as nginx, caddy, or apache&lt;/p&gt; 
&lt;p&gt;you can either:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;give copyparty its own domain or subdomain (recommended)&lt;/li&gt; 
 &lt;li&gt;or do location-based proxying, using &lt;code&gt;--rp-loc=/stuff&lt;/code&gt; to tell copyparty where it is mounted -- has a slight performance cost and higher chance of bugs 
  &lt;ul&gt; 
   &lt;li&gt;if copyparty says &lt;code&gt;incorrect --rp-loc or webserver config; expected vpath starting with [...]&lt;/code&gt; it's likely because the webserver is stripping away the proxy location from the request URLs -- see the &lt;code&gt;ProxyPass&lt;/code&gt; in the apache example below&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;when running behind a reverse-proxy (this includes services like cloudflare), it is important to configure real-ip correctly, as many features rely on knowing the client's IP. The best/safest approach is to configure your reverse-proxy so it gives copyparty a header which only contains the client's true/real IP-address, and then setting &lt;code&gt;--xff-hdr theHeaderName --rproxy 1&lt;/code&gt; but alternatively, if you want/need to let copyparty handle this, look out for red and yellow log messages which explain how to do that. Basically, the log will say this:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;set &lt;code&gt;--xff-hdr&lt;/code&gt; to the name of the http-header to read the IP from (usually &lt;code&gt;x-forwarded-for&lt;/code&gt;, but cloudflare uses &lt;code&gt;cf-connecting-ip&lt;/code&gt;), and then &lt;code&gt;--xff-src&lt;/code&gt; to the IP of the reverse-proxy so copyparty will trust the xff-hdr. You will also need to configure &lt;code&gt;--rproxy&lt;/code&gt; to &lt;code&gt;1&lt;/code&gt; if the header only contains one IP (the correct one) or to a &lt;em&gt;negative value&lt;/em&gt; if it contains multiple; &lt;code&gt;-1&lt;/code&gt; being the rightmost and most trusted IP (the nearest proxy, so usually not the correct one), &lt;code&gt;-2&lt;/code&gt; being the second-closest hop, and so on&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Note that &lt;code&gt;--rp-loc&lt;/code&gt; in particular will not work at all unless you configure the above correctly&lt;/p&gt; 
&lt;p&gt;some reverse proxies (such as &lt;a href="https://caddyserver.com/"&gt;Caddy&lt;/a&gt;) can automatically obtain a valid https/tls certificate for you, and some support HTTP/2 and QUIC which &lt;em&gt;could&lt;/em&gt; be a nice speed boost, depending on a lot of factors&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;warning:&lt;/strong&gt; nginx-QUIC (HTTP/3) is still experimental and can make uploads much slower, so HTTP/1.1 is recommended for now&lt;/li&gt; 
 &lt;li&gt;depending on server/client, HTTP/1.1 can also be 5x faster than HTTP/2&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;for improved security (and a 10% performance boost) consider listening on a unix-socket with &lt;code&gt;-i unix:770:www:/dev/shm/party.sock&lt;/code&gt; (permission &lt;code&gt;770&lt;/code&gt; means only members of group &lt;code&gt;www&lt;/code&gt; can access it)&lt;/p&gt; 
&lt;p&gt;example webserver / reverse-proxy configs:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/contrib/apache/copyparty.conf"&gt;apache config&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;caddy uds: &lt;code&gt;caddy reverse-proxy --from :8080 --to unix///dev/shm/party.sock&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;caddy tcp: &lt;code&gt;caddy reverse-proxy --from :8081 --to http://127.0.0.1:3923&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/contrib/haproxy/copyparty.conf"&gt;haproxy config&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/contrib/lighttpd/subdomain.conf"&gt;lighttpd subdomain&lt;/a&gt; -- entire domain/subdomain&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/contrib/lighttpd/subpath.conf"&gt;lighttpd subpath&lt;/a&gt; -- location-based (not optimal, but in case you need it)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/contrib/nginx/copyparty.conf"&gt;nginx config&lt;/a&gt; -- recommended&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/contrib/traefik/copyparty.yaml"&gt;traefik config&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;real-ip&lt;/h3&gt; 
&lt;p&gt;teaching copyparty how to see client IPs when running behind a reverse-proxy, or a WAF, or another protection service such as cloudflare&lt;/p&gt; 
&lt;p&gt;if you (and maybe everybody else) keep getting a message that says &lt;code&gt;thank you for playing&lt;/code&gt;, then you've gotten banned for malicious traffic. This ban applies to the IP address that copyparty &lt;em&gt;thinks&lt;/em&gt; identifies the shady client -- so, depending on your setup, you might have to tell copyparty where to find the correct IP&lt;/p&gt; 
&lt;p&gt;for most common setups, there should be a helpful message in the server-log explaining what to do, but see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/xff.md"&gt;docs/xff.md&lt;/a&gt; if you want to learn more, including a quick hack to &lt;strong&gt;just make it work&lt;/strong&gt; (which is &lt;strong&gt;not&lt;/strong&gt; recommended, but hey...)&lt;/p&gt; 
&lt;h3&gt;reverse-proxy performance&lt;/h3&gt; 
&lt;p&gt;most reverse-proxies support connecting to copyparty either using uds/unix-sockets (&lt;code&gt;/dev/shm/party.sock&lt;/code&gt;, faster/recommended) or using tcp (&lt;code&gt;127.0.0.1&lt;/code&gt;)&lt;/p&gt; 
&lt;p&gt;with copyparty listening on a uds / unix-socket / unix-domain-socket and the reverse-proxy connecting to that:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;index.html&lt;/th&gt; 
   &lt;th&gt;upload&lt;/th&gt; 
   &lt;th&gt;download&lt;/th&gt; 
   &lt;th&gt;software&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;28'900 req/s&lt;/td&gt; 
   &lt;td&gt;6'900 MiB/s&lt;/td&gt; 
   &lt;td&gt;7'400 MiB/s&lt;/td&gt; 
   &lt;td&gt;no-proxy&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;18'750 req/s&lt;/td&gt; 
   &lt;td&gt;3'500 MiB/s&lt;/td&gt; 
   &lt;td&gt;2'370 MiB/s&lt;/td&gt; 
   &lt;td&gt;haproxy&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;9'900 req/s&lt;/td&gt; 
   &lt;td&gt;3'750 MiB/s&lt;/td&gt; 
   &lt;td&gt;2'200 MiB/s&lt;/td&gt; 
   &lt;td&gt;caddy&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;18'700 req/s&lt;/td&gt; 
   &lt;td&gt;2'200 MiB/s&lt;/td&gt; 
   &lt;td&gt;1'570 MiB/s&lt;/td&gt; 
   &lt;td&gt;nginx&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;9'700 req/s&lt;/td&gt; 
   &lt;td&gt;1'750 MiB/s&lt;/td&gt; 
   &lt;td&gt;1'830 MiB/s&lt;/td&gt; 
   &lt;td&gt;apache&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;9'900 req/s&lt;/td&gt; 
   &lt;td&gt;1'300 MiB/s&lt;/td&gt; 
   &lt;td&gt;1'470 MiB/s&lt;/td&gt; 
   &lt;td&gt;lighttpd&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;when connecting the reverse-proxy to &lt;code&gt;127.0.0.1&lt;/code&gt; instead (the basic and/or old-fasioned way), speeds are a bit worse:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;index.html&lt;/th&gt; 
   &lt;th&gt;upload&lt;/th&gt; 
   &lt;th&gt;download&lt;/th&gt; 
   &lt;th&gt;software&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;21'200 req/s&lt;/td&gt; 
   &lt;td&gt;5'700 MiB/s&lt;/td&gt; 
   &lt;td&gt;6'700 MiB/s&lt;/td&gt; 
   &lt;td&gt;no-proxy&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;14'500 req/s&lt;/td&gt; 
   &lt;td&gt;1'700 MiB/s&lt;/td&gt; 
   &lt;td&gt;2'170 MiB/s&lt;/td&gt; 
   &lt;td&gt;haproxy&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;11'100 req/s&lt;/td&gt; 
   &lt;td&gt;2'750 MiB/s&lt;/td&gt; 
   &lt;td&gt;2'000 MiB/s&lt;/td&gt; 
   &lt;td&gt;traefik&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;8'400 req/s&lt;/td&gt; 
   &lt;td&gt;2'300 MiB/s&lt;/td&gt; 
   &lt;td&gt;1'950 MiB/s&lt;/td&gt; 
   &lt;td&gt;caddy&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;13'400 req/s&lt;/td&gt; 
   &lt;td&gt;1'100 MiB/s&lt;/td&gt; 
   &lt;td&gt;1'480 MiB/s&lt;/td&gt; 
   &lt;td&gt;nginx&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;8'400 req/s&lt;/td&gt; 
   &lt;td&gt;1'000 MiB/s&lt;/td&gt; 
   &lt;td&gt;1'000 MiB/s&lt;/td&gt; 
   &lt;td&gt;apache&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;6'500 req/s&lt;/td&gt; 
   &lt;td&gt;1'270 MiB/s&lt;/td&gt; 
   &lt;td&gt;1'500 MiB/s&lt;/td&gt; 
   &lt;td&gt;lighttpd&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;in summary, &lt;code&gt;haproxy &amp;gt; caddy &amp;gt; traefik &amp;gt; nginx &amp;gt; apache &amp;gt; lighttpd&lt;/code&gt;, and use uds when possible (traefik does not support it yet)&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;if these results are bullshit because my config examples are bad, please submit corrections!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;permanent cloudflare tunnel&lt;/h2&gt; 
&lt;p&gt;if you have a domain and want to get your copyparty online real quick, either from your home-PC behind a CGNAT or from a server without an existing &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#reverse-proxy"&gt;reverse-proxy&lt;/a&gt; setup, one approach is to create a &lt;a href="https://developers.cloudflare.com/cloudflare-one/connections/connect-networks/get-started/"&gt;Cloudflare Tunnel&lt;/a&gt; (formerly "Argo Tunnel")&lt;/p&gt; 
&lt;p&gt;I'd recommend making a &lt;code&gt;Locally-managed tunnel&lt;/code&gt; for more control, but if you prefer to make a &lt;code&gt;Remotely-managed tunnel&lt;/code&gt; then this is currently how:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;cloudflare dashboard&lt;/code&gt; » &lt;code&gt;zero trust&lt;/code&gt; » &lt;code&gt;networks&lt;/code&gt; » &lt;code&gt;tunnels&lt;/code&gt; » &lt;code&gt;create a tunnel&lt;/code&gt; » &lt;code&gt;cloudflared&lt;/code&gt; » choose a cool &lt;code&gt;subdomain&lt;/code&gt; and leave the &lt;code&gt;path&lt;/code&gt; blank, and use &lt;code&gt;service type&lt;/code&gt; = &lt;code&gt;http&lt;/code&gt; and &lt;code&gt;URL&lt;/code&gt; = &lt;code&gt;127.0.0.1:3923&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;and if you want to just run the tunnel without installing it, skip the &lt;code&gt;cloudflared service install BASE64&lt;/code&gt; step and instead do &lt;code&gt;cloudflared --no-autoupdate tunnel run --token BASE64&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;NOTE: since people will be connecting through cloudflare, as mentioned in &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#real-ip"&gt;real-ip&lt;/a&gt; you should run copyparty with &lt;code&gt;--xff-hdr cf-connecting-ip&lt;/code&gt; to detect client IPs correctly&lt;/p&gt; 
&lt;p&gt;config file example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;[global]
  xff-hdr: cf-connecting-ip
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;prometheus&lt;/h2&gt; 
&lt;p&gt;metrics/stats can be enabled at URL &lt;code&gt;/.cpr/metrics&lt;/code&gt; for grafana / prometheus / etc (openmetrics 1.0.0)&lt;/p&gt; 
&lt;p&gt;must be enabled with &lt;code&gt;--stats&lt;/code&gt; since it reduces startup time a tiny bit, and you probably want &lt;code&gt;-e2dsa&lt;/code&gt; too&lt;/p&gt; 
&lt;p&gt;the endpoint is only accessible by &lt;code&gt;admin&lt;/code&gt; accounts, meaning the &lt;code&gt;a&lt;/code&gt; in &lt;code&gt;rwmda&lt;/code&gt; in the following example commandline: &lt;code&gt;python3 -m copyparty -a ed:wark -v /mnt/nas::rwmda,ed --stats -e2dsa&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;follow a guide for setting up &lt;code&gt;node_exporter&lt;/code&gt; except have it read from copyparty instead; example &lt;code&gt;/etc/prometheus/prometheus.yml&lt;/code&gt; below&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;scrape_configs:
  - job_name: copyparty
    metrics_path: /.cpr/metrics
    basic_auth:
      password: wark
    static_configs:
      - targets: ['192.168.123.1:3923']
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;currently the following metrics are available,&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;cpp_uptime_seconds&lt;/code&gt; time since last copyparty restart&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;cpp_boot_unixtime_seconds&lt;/code&gt; same but as an absolute timestamp&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;cpp_active_dl&lt;/code&gt; number of active downloads&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;cpp_http_conns&lt;/code&gt; number of open http(s) connections&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;cpp_http_reqs&lt;/code&gt; number of http(s) requests handled&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;cpp_sus_reqs&lt;/code&gt; number of 403/422/malicious requests&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;cpp_active_bans&lt;/code&gt; number of currently banned IPs&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;cpp_total_bans&lt;/code&gt; number of IPs banned since last restart&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;these are available unless &lt;code&gt;--nos-vst&lt;/code&gt; is specified:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;cpp_db_idle_seconds&lt;/code&gt; time since last database activity (upload/rename/delete)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;cpp_db_act_seconds&lt;/code&gt; same but as an absolute timestamp&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;cpp_idle_vols&lt;/code&gt; number of volumes which are idle / ready&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;cpp_busy_vols&lt;/code&gt; number of volumes which are busy / indexing&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;cpp_offline_vols&lt;/code&gt; number of volumes which are offline / unavailable&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;cpp_hashing_files&lt;/code&gt; number of files queued for hashing / indexing&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;cpp_tagq_files&lt;/code&gt; number of files queued for metadata scanning&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;cpp_mtpq_files&lt;/code&gt; number of files queued for plugin-based analysis&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;and these are available per-volume only:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;cpp_disk_size_bytes&lt;/code&gt; total HDD size&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;cpp_disk_free_bytes&lt;/code&gt; free HDD space&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;and these are per-volume and &lt;code&gt;total&lt;/code&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;cpp_vol_bytes&lt;/code&gt; size of all files in volume&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;cpp_vol_files&lt;/code&gt; number of files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;cpp_dupe_bytes&lt;/code&gt; disk space presumably saved by deduplication&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;cpp_dupe_files&lt;/code&gt; number of dupe files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;cpp_unf_bytes&lt;/code&gt; currently unfinished / incoming uploads&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;some of the metrics have additional requirements to function correctly,&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;cpp_vol_*&lt;/code&gt; requires either the &lt;code&gt;e2ds&lt;/code&gt; volflag or &lt;code&gt;-e2dsa&lt;/code&gt; global-option&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;the following options are available to disable some of the metrics:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--nos-hdd&lt;/code&gt; disables &lt;code&gt;cpp_disk_*&lt;/code&gt; which can prevent spinning up HDDs&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--nos-vol&lt;/code&gt; disables &lt;code&gt;cpp_vol_*&lt;/code&gt; which reduces server startup time&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--nos-vst&lt;/code&gt; disables volume state, reducing the worst-case prometheus query time by 0.5 sec&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--nos-dup&lt;/code&gt; disables &lt;code&gt;cpp_dupe_*&lt;/code&gt; which reduces the server load caused by prometheus queries&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--nos-unf&lt;/code&gt; disables &lt;code&gt;cpp_unf_*&lt;/code&gt; for no particular purpose&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;note: the following metrics are counted incorrectly if multiprocessing is enabled with &lt;code&gt;-j&lt;/code&gt;: &lt;code&gt;cpp_http_conns&lt;/code&gt;, &lt;code&gt;cpp_http_reqs&lt;/code&gt;, &lt;code&gt;cpp_sus_reqs&lt;/code&gt;, &lt;code&gt;cpp_active_bans&lt;/code&gt;, &lt;code&gt;cpp_total_bans&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;other extremely specific features&lt;/h2&gt; 
&lt;p&gt;you'll never find a use for these:&lt;/p&gt; 
&lt;h3&gt;custom mimetypes&lt;/h3&gt; 
&lt;p&gt;change the association of a file extension&lt;/p&gt; 
&lt;p&gt;using commandline args, you can do something like &lt;code&gt;--mime gif=image/jif&lt;/code&gt; and &lt;code&gt;--mime ts=text/x.typescript&lt;/code&gt; (can be specified multiple times)&lt;/p&gt; 
&lt;p&gt;in a config file, this is the same as:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;[global]
  mime: gif=image/jif
  mime: ts=text/x.typescript
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;run copyparty with &lt;code&gt;--mimes&lt;/code&gt; to list all the default mappings&lt;/p&gt; 
&lt;h3&gt;GDPR compliance&lt;/h3&gt; 
&lt;p&gt;imagine using copyparty professionally... &lt;strong&gt;TINLA/IANAL; EU laws are hella confusing&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;remember to disable logging, or configure logrotation to an acceptable timeframe with &lt;code&gt;-lo cpp-%Y-%m%d.txt.xz&lt;/code&gt; or similar&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;if running with the database enabled (recommended), then have it forget uploader-IPs after some time using &lt;code&gt;--forget-ip 43200&lt;/code&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;don't set it too low; &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#unpost"&gt;unposting&lt;/a&gt; a file is no longer possible after this takes effect&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;if you actually &lt;em&gt;are&lt;/em&gt; a lawyer then I'm open for feedback, would be fun&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;feature chickenbits&lt;/h3&gt; 
&lt;p&gt;buggy feature? rip it out by setting any of the following environment variables to disable its associated bell or whistle,&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;env-var&lt;/th&gt; 
   &lt;th&gt;what it does&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_NO_DB_LOCK&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;do not lock session/shares-databases for exclusive access&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_NO_IFADDR&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;disable ip/nic discovery by poking into your OS with ctypes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_NO_IMPRESO&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;do not try to load js/css files using &lt;code&gt;importlib.resources&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_NO_IPV6&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;disable some ipv6 support (should not be necessary since windows 2000)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_NO_LZMA&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;disable streaming xz compression of incoming uploads&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_NO_MP&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;disable all use of the python &lt;code&gt;multiprocessing&lt;/code&gt; module (actual multithreading, cpu-count for parsers/thumbnailers)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_NO_SQLITE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;disable all database-related functionality (file indexing, metadata indexing, most file deduplication logic)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_NO_TLS&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;disable native HTTPS support; if you still want to accept HTTPS connections then TLS must now be terminated by a reverse-proxy&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_NO_TPOKE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;disable systemd-tmpfilesd avoider&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;example: &lt;code&gt;PRTY_NO_IFADDR=1 python3 copyparty-sfx.py&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;feature beefybits&lt;/h3&gt; 
&lt;p&gt;force-enable features with known issues on your OS/env by setting any of the following environment variables, also affectionately known as &lt;code&gt;fuckitbits&lt;/code&gt; or &lt;code&gt;hail-mary-bits&lt;/code&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;env-var&lt;/th&gt; 
   &lt;th&gt;what it does&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_FORCE_MP&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;force-enable multiprocessing (real multithreading) on MacOS and other broken platforms&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_FORCE_MAGIC&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;use &lt;a href="https://pypi.org/project/python-magic/"&gt;magic&lt;/a&gt; on Windows (you will segfault)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;packages&lt;/h1&gt; 
&lt;p&gt;the party might be closer than you think&lt;/p&gt; 
&lt;p&gt;if your distro/OS is not mentioned below, there might be some hints in the &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#on-servers"&gt;«on servers»&lt;/a&gt; section&lt;/p&gt; 
&lt;h2&gt;arch package&lt;/h2&gt; 
&lt;p&gt;&lt;code&gt;pacman -S copyparty&lt;/code&gt; (in &lt;a href="https://archlinux.org/packages/extra/any/copyparty/"&gt;arch linux extra&lt;/a&gt;)&lt;/p&gt; 
&lt;p&gt;it comes with a &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/contrib/systemd/copyparty@.service"&gt;systemd service&lt;/a&gt; as well as a &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/contrib/systemd/copyparty-user.service"&gt;user service&lt;/a&gt;, and expects to find a &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/contrib/systemd/copyparty.example.conf"&gt;config file&lt;/a&gt; in &lt;code&gt;/etc/copyparty/copyparty.conf&lt;/code&gt; or &lt;code&gt;~/.config/copyparty/copyparty.conf&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;after installing, start either the system service or the user service and navigate to &lt;a href="http://127.0.0.1:3923"&gt;http://127.0.0.1:3923&lt;/a&gt; for further instructions (unless you already edited the config files, in which case you are good to go, probably)&lt;/p&gt; 
&lt;h2&gt;fedora package&lt;/h2&gt; 
&lt;p&gt;does not exist yet; there are rumours that it is being packaged! keep an eye on this space...&lt;/p&gt; 
&lt;h2&gt;homebrew formulae&lt;/h2&gt; 
&lt;p&gt;&lt;code&gt;brew install copyparty ffmpeg&lt;/code&gt; -- &lt;a href="https://formulae.brew.sh/formula/copyparty"&gt;https://formulae.brew.sh/formula/copyparty&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;should work on all macs (both intel and apple silicon) and all relevant macos versions&lt;/p&gt; 
&lt;p&gt;the homebrew package is maintained by the homebrew team (thanks!)&lt;/p&gt; 
&lt;h2&gt;nix package&lt;/h2&gt; 
&lt;p&gt;&lt;code&gt;nix profile install github:9001/copyparty&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;requires a &lt;a href="https://nixos.wiki/wiki/Flakes"&gt;flake-enabled&lt;/a&gt; installation of nix&lt;/p&gt; 
&lt;p&gt;some recommended dependencies are enabled by default; &lt;a href="https://github.com/9001/copyparty/raw/hovudstraum/contrib/package/nix/copyparty/default.nix#L3-L22"&gt;override the package&lt;/a&gt; if you want to add/remove some features/deps&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;ffmpeg-full&lt;/code&gt; was chosen over &lt;code&gt;ffmpeg-headless&lt;/code&gt; mainly because we need &lt;code&gt;withWebp&lt;/code&gt; (and &lt;code&gt;withOpenmpt&lt;/code&gt; is also nice) and being able to use a cached build felt more important than optimizing for size at the time -- PRs welcome if you disagree 👍&lt;/p&gt; 
&lt;h2&gt;nixos module&lt;/h2&gt; 
&lt;p&gt;for &lt;a href="https://nixos.wiki/wiki/Flakes"&gt;flake-enabled&lt;/a&gt; installations of NixOS:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-nix"&gt;{
  # add copyparty flake to your inputs
  inputs.copyparty.url = "github:9001/copyparty";

  # ensure that copyparty is an allowed argument to the outputs function
  outputs = { self, nixpkgs, copyparty }: {
    nixosConfigurations.yourHostName = nixpkgs.lib.nixosSystem {
      modules = [
        # load the copyparty NixOS module
        copyparty.nixosModules.default
        ({ pkgs, ... }: {
          # add the copyparty overlay to expose the package to the module
          nixpkgs.overlays = [ copyparty.overlays.default ];
          # (optional) install the package globally
          environment.systemPackages = [ pkgs.copyparty ];
          # configure the copyparty module
          services.copyparty.enable = true;
        })
      ];
    };
  };
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;if you don't use a flake in your configuration, you can use other dependency management tools like &lt;a href="https://github.com/andir/npins"&gt;npins&lt;/a&gt;, &lt;a href="https://github.com/nmattia/niv"&gt;niv&lt;/a&gt;, or even plain &lt;a href="https://nix.dev/manual/nix/stable/language/builtins#builtins-fetchTarball"&gt;&lt;code&gt;fetchTarball&lt;/code&gt;&lt;/a&gt;, like so:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-nix"&gt;{ pkgs, ... }:

let
  # npins example, adjust for your setup. copyparty should be a path to the downloaded repo
  # for niv, just replace the npins folder import with the sources.nix file
  copyparty = (import ./npins).copyparty;

  # or with fetchTarball:
  copyparty = fetchTarball "https://github.com/9001/copyparty/archive/hovudstraum.tar.gz";
in

{
  # load the copyparty NixOS module
  imports = [ "${copyparty}/contrib/nixos/modules/copyparty.nix" ];

  # add the copyparty overlay to expose the package to the module
  nixpkgs.overlays = [ (import "${copyparty}/contrib/package/nix/overlay.nix") ];
  # (optional) install the package globally
  environment.systemPackages = [ pkgs.copyparty ];
  # configure the copyparty module
  services.copyparty.enable = true;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;copyparty on NixOS is configured via &lt;code&gt;services.copyparty&lt;/code&gt; options, for example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-nix"&gt;services.copyparty = {
  enable = true;
  # directly maps to values in the [global] section of the copyparty config.
  # see `copyparty --help` for available options
  settings = {
    i = "0.0.0.0";
    # use lists to set multiple values
    p = [ 3210 3211 ];
    # use booleans to set binary flags
    no-reload = true;
    # using 'false' will do nothing and omit the value when generating a config
    ignored-flag = false;
  };

  # create users
  accounts = {
    # specify the account name as the key
    ed = {
      # provide the path to a file containing the password, keeping it out of /nix/store
      # must be readable by the copyparty service user
      passwordFile = "/run/keys/copyparty/ed_password";
    };
    # or do both in one go
    k.passwordFile = "/run/keys/copyparty/k_password";
  };

  # create a volume
  volumes = {
    # create a volume at "/" (the webroot), which will
    "/" = {
      # share the contents of "/srv/copyparty"
      path = "/srv/copyparty";
      # see `copyparty --help-accounts` for available options
      access = {
        # everyone gets read-access, but
        r = "*";
        # users "ed" and "k" get read-write
        rw = [ "ed" "k" ];
      };
      # see `copyparty --help-flags` for available options
      flags = {
        # "fk" enables filekeys (necessary for upget permission) (4 chars long)
        fk = 4;
        # scan for new files every 60sec
        scan = 60;
        # volflag "e2d" enables the uploads database
        e2d = true;
        # "d2t" disables multimedia parsers (in case the uploads are malicious)
        d2t = true;
        # skips hashing file contents if path matches *.iso
        nohash = "\.iso$";
      };
    };
  };
  # you may increase the open file limit for the process
  openFilesLimit = 8192;
};
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;the passwordFile at /run/keys/copyparty/ could for example be generated by &lt;a href="https://github.com/ryantm/agenix"&gt;agenix&lt;/a&gt;, or you could just dump it in the nix store instead if that's acceptable&lt;/p&gt; 
&lt;h1&gt;browser support&lt;/h1&gt; 
&lt;p&gt;TLDR: yes&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/241032/118192791-fb31fe00-b446-11eb-9647-898ea8efc1f7.png" alt="copyparty-ie4-fs8" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;ie&lt;/code&gt; = internet-explorer, &lt;code&gt;ff&lt;/code&gt; = firefox, &lt;code&gt;c&lt;/code&gt; = chrome, &lt;code&gt;iOS&lt;/code&gt; = iPhone/iPad, &lt;code&gt;Andr&lt;/code&gt; = Android&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;feature&lt;/th&gt; 
   &lt;th&gt;ie6&lt;/th&gt; 
   &lt;th&gt;ie9&lt;/th&gt; 
   &lt;th&gt;ie10&lt;/th&gt; 
   &lt;th&gt;ie11&lt;/th&gt; 
   &lt;th&gt;ff 52&lt;/th&gt; 
   &lt;th&gt;c 49&lt;/th&gt; 
   &lt;th&gt;iOS&lt;/th&gt; 
   &lt;th&gt;Andr&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;browse files&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;thumbnail view&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;basic uploader&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;up2k&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;*1&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;*1&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;make directory&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;send message&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;set sort order&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;zip selection&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;file search&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;file rename&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;file cut/paste&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;unpost uploads&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;navpane&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;image viewer&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;video player&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;markdown editor&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;*2&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;*2&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;markdown viewer&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;*2&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;*2&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;*2&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;play mp3/m4a&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;play ogg/opus&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;*3&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;= feature =&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;ie6&lt;/td&gt; 
   &lt;td&gt;ie9&lt;/td&gt; 
   &lt;td&gt;ie10&lt;/td&gt; 
   &lt;td&gt;ie11&lt;/td&gt; 
   &lt;td&gt;ff 52&lt;/td&gt; 
   &lt;td&gt;c 49&lt;/td&gt; 
   &lt;td&gt;iOS&lt;/td&gt; 
   &lt;td&gt;Andr&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;internet explorer 6 through 8 behave the same&lt;/li&gt; 
 &lt;li&gt;firefox 52 and chrome 49 are the final winxp versions&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;*1&lt;/code&gt; yes, but extremely slow (ie10: &lt;code&gt;1 MiB/s&lt;/code&gt;, ie11: &lt;code&gt;270 KiB/s&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;*2&lt;/code&gt; only able to do plaintext documents (no markdown rendering)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;*3&lt;/code&gt; iOS 11 and newer, opus only, and requires FFmpeg on the server&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;quick summary of more eccentric web-browsers trying to view a directory index:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;browser&lt;/th&gt; 
   &lt;th&gt;will it blend&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;links&lt;/strong&gt; (2.21/macports)&lt;/td&gt; 
   &lt;td&gt;can browse, login, upload/mkdir/msg&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;lynx&lt;/strong&gt; (2.8.9/macports)&lt;/td&gt; 
   &lt;td&gt;can browse, login, upload/mkdir/msg&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;w3m&lt;/strong&gt; (0.5.3/macports)&lt;/td&gt; 
   &lt;td&gt;can browse, login, upload at 100kB/s, mkdir/msg&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;netsurf&lt;/strong&gt; (3.10/arch)&lt;/td&gt; 
   &lt;td&gt;is basically ie6 with much better css (javascript has almost no effect)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;opera&lt;/strong&gt; (11.60/winxp)&lt;/td&gt; 
   &lt;td&gt;OK: thumbnails, image-viewer, zip-selection, rename/cut/paste. NG: up2k, navpane, markdown, audio&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ie4&lt;/strong&gt; and &lt;strong&gt;netscape&lt;/strong&gt; 4.0&lt;/td&gt; 
   &lt;td&gt;can browse, upload with &lt;code&gt;?b=u&lt;/code&gt;, auth with &lt;code&gt;&amp;amp;pw=wark&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ncsa mosaic&lt;/strong&gt; 2.7&lt;/td&gt; 
   &lt;td&gt;does not get a pass, &lt;a href="https://user-images.githubusercontent.com/241032/174189227-ae816026-cf6f-4be5-a26e-1b3b072c1b2f.png"&gt;pic1&lt;/a&gt; - &lt;a href="https://user-images.githubusercontent.com/241032/174189225-5651c059-5152-46e9-ac26-7e98e497901b.png"&gt;pic2&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;SerenityOS&lt;/strong&gt; (7e98457)&lt;/td&gt; 
   &lt;td&gt;hits a page fault, works with &lt;code&gt;?b=u&lt;/code&gt;, file upload not-impl&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;sony psp&lt;/strong&gt; 5.50&lt;/td&gt; 
   &lt;td&gt;can browse, upload/mkdir/msg (thx dwarf) &lt;a href="https://github.com/user-attachments/assets/9d21f020-1110-4652-abeb-6fc09c533d4f"&gt;screenshot&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;nintendo 3ds&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;can browse, upload, view thumbnails (thx bnjmn)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Nintendo Wii (Opera 9.0 "Internet Channel")&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;can browse, can't upload or download (no local storage), can view images - works best with &lt;code&gt;?b=u&lt;/code&gt;, default view broken&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p align="center"&gt;&lt;img src="https://github.com/user-attachments/assets/88deab3d-6cad-4017-8841-2f041472b853" /&gt;&lt;/p&gt; 
&lt;h1&gt;client examples&lt;/h1&gt; 
&lt;p&gt;interact with copyparty using non-browser clients&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;javascript: dump some state into a file (two separate examples)&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;await fetch('//127.0.0.1:3923/', {method:"PUT", body: JSON.stringify(foo)});&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;var xhr = new XMLHttpRequest(); xhr.open('POST', '//127.0.0.1:3923/msgs?raw'); xhr.send('foo');&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;curl/wget: upload some files (post=file, chunk=stdin)&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;post(){ curl -F f=@"$1" http://127.0.0.1:3923/?pw=wark;}&lt;/code&gt;&lt;br /&gt; &lt;code&gt;post movie.mkv&lt;/code&gt; (gives HTML in return)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;post(){ curl -F f=@"$1" 'http://127.0.0.1:3923/?want=url&amp;amp;pw=wark';}&lt;/code&gt;&lt;br /&gt; &lt;code&gt;post movie.mkv&lt;/code&gt; (gives hotlink in return)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;post(){ curl -H pw:wark -H rand:8 -T "$1" http://127.0.0.1:3923/;}&lt;/code&gt;&lt;br /&gt; &lt;code&gt;post movie.mkv&lt;/code&gt; (randomized filename)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;post(){ wget --header='pw: wark' --post-file="$1" -O- http://127.0.0.1:3923/?raw;}&lt;/code&gt;&lt;br /&gt; &lt;code&gt;post movie.mkv&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;chunk(){ curl -H pw:wark -T- http://127.0.0.1:3923/;}&lt;/code&gt;&lt;br /&gt; &lt;code&gt;chunk &amp;lt;movie.mkv&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;bash: when curl and wget is not available or too boring&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;(printf 'PUT /junk?pw=wark HTTP/1.1\r\n\r\n'; cat movie.mkv) | nc 127.0.0.1 3923&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;(printf 'PUT / HTTP/1.1\r\n\r\n'; cat movie.mkv) &amp;gt;/dev/tcp/127.0.0.1/3923&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;python: &lt;a href="https://github.com/9001/copyparty/raw/hovudstraum/bin/u2c.py"&gt;u2c.py&lt;/a&gt; is a command-line up2k client &lt;a href="https://ocv.me/stuff/u2cli.webm"&gt;(webm)&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;file uploads, file-search, &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#folder-sync"&gt;folder sync&lt;/a&gt;, autoresume of aborted/broken uploads&lt;/li&gt; 
   &lt;li&gt;can be downloaded from copyparty: controlpanel -&amp;gt; connect -&amp;gt; &lt;a href="http://127.0.0.1:3923/.cpr/a/u2c.py"&gt;u2c.py&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/bin/README.md#u2cpy"&gt;./bin/README.md#u2cpy&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;FUSE: mount a copyparty server as a local filesystem&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;cross-platform python client available in &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/bin/"&gt;./bin/&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;able to mount nginx and iis directory listings too, not just copyparty&lt;/li&gt; 
   &lt;li&gt;can be downloaded from copyparty: controlpanel -&amp;gt; connect -&amp;gt; &lt;a href="http://127.0.0.1:3923/.cpr/a/partyfuse.py"&gt;partyfuse.py&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://rclone.org/"&gt;rclone&lt;/a&gt; as client can give ~5x performance, see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/rclone.md"&gt;./docs/rclone.md&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;sharex (screenshot utility): see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/contrib/#sharexsxcu"&gt;./contrib/sharex.sxcu&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;and for screenshots on macos, see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/contrib/#ishareiscu"&gt;./contrib/ishare.iscu&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;and for screenshots on linux, see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/contrib/flameshot.sh"&gt;./contrib/flameshot.sh&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://f-droid.org/en/packages/com.nyx.custom_uploader/"&gt;Custom Uploader&lt;/a&gt; (an Android app) as an alternative to copyparty's own &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#android-app"&gt;PartyUP!&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;works if you set UploadURL to &lt;code&gt;https://your.com/foo/?want=url&amp;amp;pw=hunter2&lt;/code&gt; and FormDataName &lt;code&gt;f&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;contextlet (web browser integration); see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/contrib/#send-to-cppcontextletjson"&gt;contrib contextlet&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://iglooirc.com/"&gt;igloo irc&lt;/a&gt;: Method: &lt;code&gt;post&lt;/code&gt; Host: &lt;code&gt;https://you.com/up/?want=url&amp;amp;pw=hunter2&lt;/code&gt; Multipart: &lt;code&gt;yes&lt;/code&gt; File parameter: &lt;code&gt;f&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;copyparty returns a truncated sha512sum of your PUT/POST as base64; you can generate the same checksum locally to verify uploads:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;b512(){ printf "$((sha512sum||shasum -a512)|sed -E 's/ .*//;s/(..)/\\x\1/g')"|base64|tr '+/' '-_'|head -c44;}
b512 &amp;lt;movie.mkv
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;you can provide passwords using header &lt;code&gt;PW: hunter2&lt;/code&gt;, cookie &lt;code&gt;cppwd=hunter2&lt;/code&gt;, url-param &lt;code&gt;?pw=hunter2&lt;/code&gt;, or with basic-authentication (either as the username or password)&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;for basic-authentication, all of the following are accepted: &lt;code&gt;password&lt;/code&gt; / &lt;code&gt;whatever:password&lt;/code&gt; / &lt;code&gt;password:whatever&lt;/code&gt; (the username is ignored)&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;unless you've enabled &lt;code&gt;--usernames&lt;/code&gt;, then it's &lt;code&gt;PW: usr:pwd&lt;/code&gt;, cookie &lt;code&gt;cppwd=usr:pwd&lt;/code&gt;, url-param &lt;code&gt;?pw=usr:pwd&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;NOTE: curl will not send the original filename if you use &lt;code&gt;-T&lt;/code&gt; combined with url-params! Also, make sure to always leave a trailing slash in URLs unless you want to override the filename&lt;/p&gt; 
&lt;h2&gt;folder sync&lt;/h2&gt; 
&lt;p&gt;sync folders to/from copyparty&lt;/p&gt; 
&lt;p&gt;NOTE: full bidirectional sync, like what &lt;a href="https://docs.nextcloud.com/server/latest/user_manual/sv/files/desktop_mobile_sync.html"&gt;nextcloud&lt;/a&gt; and &lt;a href="https://syncthing.net/"&gt;syncthing&lt;/a&gt; does, will never be supported! Only single-direction sync (server-to-client, or client-to-server) is possible with copyparty&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;if you want bidirectional sync, then copyparty and syncthing &lt;em&gt;should&lt;/em&gt; be entirely safe to combine; they should be able to collaborate on the same folders without causing any trouble for eachother. Many people do this, and there have been no issues so far. But, if you &lt;em&gt;do&lt;/em&gt; encounter any problems, please &lt;a href="https://github.com/9001/copyparty/issues/new/choose"&gt;file a copyparty bug&lt;/a&gt; and I'll try to help -- just keep in mind I've never used syncthing before :-)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;the commandline uploader &lt;a href="https://github.com/9001/copyparty/tree/hovudstraum/bin#u2cpy"&gt;u2c.py&lt;/a&gt; with &lt;code&gt;--dr&lt;/code&gt; is the best way to sync a folder to copyparty; verifies checksums and does files in parallel, and deletes unexpected files on the server after upload has finished which makes file-renames really cheap (it'll rename serverside and skip uploading)&lt;/p&gt; 
&lt;p&gt;if you want to sync with &lt;code&gt;u2c.py&lt;/code&gt; then:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;the &lt;code&gt;e2dsa&lt;/code&gt; option (either globally or volflag) must be enabled on the server for the volumes you're syncing into&lt;/li&gt; 
 &lt;li&gt;...but DON'T enable global-options &lt;code&gt;no-hash&lt;/code&gt; or &lt;code&gt;no-idx&lt;/code&gt; (or volflags &lt;code&gt;nohash&lt;/code&gt; / &lt;code&gt;noidx&lt;/code&gt;), or at least make sure they are configured so they do not affect anything you are syncing into&lt;/li&gt; 
 &lt;li&gt;...and u2c needs the delete-permission, so either &lt;code&gt;rwd&lt;/code&gt; at minimum, or just &lt;code&gt;A&lt;/code&gt; which is the same as &lt;code&gt;rwmd.a&lt;/code&gt; 
  &lt;ul&gt; 
   &lt;li&gt;quick reminder that &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;A&lt;/code&gt; are different permissions, and &lt;code&gt;.&lt;/code&gt; is very useful for sync&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;alternatively there is &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/rclone.md"&gt;rclone&lt;/a&gt; which allows for bidirectional sync and is &lt;em&gt;way&lt;/em&gt; more flexible (stream files straight from sftp/s3/gcs to copyparty, ...), although there is no integrity check and it won't work with files over 100 MiB if copyparty is behind cloudflare&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;starting from rclone v1.63, rclone is faster than u2c.py on low-latency connections 
  &lt;ul&gt; 
   &lt;li&gt;but this is only true for the initial upload; u2c will be faster for periodic syncing&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;mount as drive&lt;/h2&gt; 
&lt;p&gt;a remote copyparty server as a local filesystem; go to the control-panel and click &lt;code&gt;connect&lt;/code&gt; to see a list of commands to do that&lt;/p&gt; 
&lt;p&gt;alternatively, some alternatives roughly sorted by speed (unreproducible benchmark), best first:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/rclone.md"&gt;rclone-webdav&lt;/a&gt; (25s), read/WRITE (rclone v1.63 or later)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/rclone.md"&gt;rclone-http&lt;/a&gt; (26s), read-only&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/bin/#partyfusepy"&gt;partyfuse.py&lt;/a&gt; (26s), read-only&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/rclone.md"&gt;rclone-ftp&lt;/a&gt; (47s), read/WRITE&lt;/li&gt; 
 &lt;li&gt;davfs2 (103s), read/WRITE&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#webdav-server"&gt;win10-webdav&lt;/a&gt; (138s), read/WRITE&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#smb-server"&gt;win10-smb2&lt;/a&gt; (387s), read/WRITE&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;most clients will fail to mount the root of a copyparty server unless there is a root volume (so you get the admin-panel instead of a browser when accessing it) -- in that case, mount a specific volume instead&lt;/p&gt; 
&lt;p&gt;if you have volumes that are accessible without a password, then some webdav clients (such as davfs2) require the global-option &lt;code&gt;--dav-auth&lt;/code&gt; to access any password-protected areas&lt;/p&gt; 
&lt;h1&gt;android app&lt;/h1&gt; 
&lt;p&gt;upload to copyparty with one tap&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://f-droid.org/packages/me.ocv.partyup/"&gt;&lt;img src="https://ocv.me/fdroid.png" alt="Get it on F-Droid" height="50" /&gt; '' &lt;img src="https://img.shields.io/f-droid/v/me.ocv.partyup.svg?sanitize=true" alt="f-droid version info" /&gt;&lt;/a&gt; '' &lt;a href="https://github.com/9001/party-up"&gt;&lt;img src="https://img.shields.io/github/release/9001/party-up.svg?logo=github" alt="github version info" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;the app is &lt;strong&gt;NOT&lt;/strong&gt; the full copyparty server! just a basic upload client, nothing fancy yet&lt;/p&gt; 
&lt;p&gt;if you want to run the copyparty server on your android device, see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#install-on-android"&gt;install on android&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;iOS shortcuts&lt;/h1&gt; 
&lt;p&gt;there is no iPhone app, but the following shortcuts are almost as good:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.icloud.com/shortcuts/41e98dd985cb4d3bb433222bc1e9e770"&gt;upload to copyparty&lt;/a&gt; (&lt;a href="https://github.com/9001/copyparty/raw/hovudstraum/contrib/ios/upload-to-copyparty.shortcut"&gt;offline&lt;/a&gt;) (&lt;a href="https://user-images.githubusercontent.com/241032/226118053-78623554-b0ed-482e-98e4-6d57ada58ea4.png"&gt;png&lt;/a&gt;) based on the &lt;a href="https://www.icloud.com/shortcuts/ab415d5b4de3467b9ce6f151b439a5d7"&gt;original&lt;/a&gt; by &lt;a href="https://github.com/Daedren"&gt;Daedren&lt;/a&gt; (thx!) 
  &lt;ul&gt; 
   &lt;li&gt;can strip exif, upload files, pics, vids, links, clipboard&lt;/li&gt; 
   &lt;li&gt;can download links and rehost the target file on copyparty (see first comment inside the shortcut)&lt;/li&gt; 
   &lt;li&gt;pics become lowres if you share from gallery to shortcut, so better to launch the shortcut and pick stuff from there&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;if you want to run the copyparty server on your iPhone or iPad, see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#install-on-iOS"&gt;install on iOS&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;performance&lt;/h1&gt; 
&lt;p&gt;defaults are usually fine - expect &lt;code&gt;8 GiB/s&lt;/code&gt; download, &lt;code&gt;1 GiB/s&lt;/code&gt; upload&lt;/p&gt; 
&lt;p&gt;below are some tweaks roughly ordered by usefulness:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;disabling HTTP/2 and HTTP/3 can make uploads 5x faster, depending on server/client software&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;-q&lt;/code&gt; disables logging and can help a bunch, even when combined with &lt;code&gt;-lo&lt;/code&gt; to redirect logs to file&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;--hist&lt;/code&gt; pointing to a fast location (ssd) will make directory listings and searches faster when &lt;code&gt;-e2d&lt;/code&gt; or &lt;code&gt;-e2t&lt;/code&gt; is set&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;and also makes thumbnails load faster, regardless of e2d/e2t&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;--dedup&lt;/code&gt; enables deduplication and thus avoids writing to the HDD if someone uploads a dupe&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;--safe-dedup 1&lt;/code&gt; makes deduplication much faster during upload by skipping verification of file contents; safe if there is no other software editing/moving the files in the volumes&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;--no-dirsz&lt;/code&gt; shows the size of folder inodes instead of the total size of the contents, giving about 30% faster folder listings&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;--no-hash .&lt;/code&gt; when indexing a network-disk if you don't care about the actual filehashes and only want the names/tags searchable&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;if your volumes are on a network-disk such as NFS / SMB / s3, specifying larger values for &lt;code&gt;--iobuf&lt;/code&gt; and/or &lt;code&gt;--s-rd-sz&lt;/code&gt; and/or &lt;code&gt;--s-wr-sz&lt;/code&gt; may help; try setting all of them to &lt;code&gt;524288&lt;/code&gt; or &lt;code&gt;1048576&lt;/code&gt; or &lt;code&gt;4194304&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;--no-htp --hash-mt=0 --mtag-mt=1 --th-mt=1&lt;/code&gt; minimizes the number of threads; can help in some eccentric environments (like the vscode debugger)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;when running on AlpineLinux or other musl-based distro, try mimalloc for higher performance (and twice as much RAM usage); &lt;code&gt;apk add mimalloc2&lt;/code&gt; and run copyparty with env-var &lt;code&gt;LD_PRELOAD=/usr/lib/libmimalloc-secure.so.2&lt;/code&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;note that mimalloc requires special care when combined with prisonparty and/or bubbleparty/bubblewrap; you must give it access to &lt;code&gt;/proc&lt;/code&gt; and &lt;code&gt;/sys&lt;/code&gt; otherwise you'll encounter issues with FFmpeg (audio transcoding, thumbnails)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;-j0&lt;/code&gt; enables multiprocessing (actual multithreading), can reduce latency to &lt;code&gt;20+80/numCores&lt;/code&gt; percent and generally improve performance in cpu-intensive workloads, for example:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;lots of connections (many users or heavy clients)&lt;/li&gt; 
   &lt;li&gt;simultaneous downloads and uploads saturating a 20gbps connection&lt;/li&gt; 
   &lt;li&gt;if &lt;code&gt;-e2d&lt;/code&gt; is enabled, &lt;code&gt;-j2&lt;/code&gt; gives 4x performance for directory listings; &lt;code&gt;-j4&lt;/code&gt; gives 16x&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;...however it also increases the server/filesystem/HDD load during uploads, and adds an overhead to internal communication, so it is usually a better idea to don't&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;using &lt;a href="https://www.pypy.org/"&gt;pypy&lt;/a&gt; instead of &lt;a href="https://www.python.org/"&gt;cpython&lt;/a&gt; &lt;em&gt;can&lt;/em&gt; be 70% faster for some workloads, but slower for many others&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;and pypy can sometimes crash on startup with &lt;code&gt;-j0&lt;/code&gt; (TODO make issue)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;client-side&lt;/h2&gt; 
&lt;p&gt;when uploading files,&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;when uploading from very fast storage (NVMe SSD) with chrome/firefox, enable &lt;code&gt;[wasm]&lt;/code&gt; in the &lt;code&gt;[⚙️] settings&lt;/code&gt; tab to more effectively use all CPU-cores for hashing&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;don't do this on Safari (runs faster without)&lt;/li&gt; 
   &lt;li&gt;don't do this on older browsers; likely to provoke browser-bugs (browser eats all RAM and crashes)&lt;/li&gt; 
   &lt;li&gt;can be made default-enabled serverside with &lt;code&gt;--nosubtle 137&lt;/code&gt; (chrome v137+) or &lt;code&gt;--nosubtle 2&lt;/code&gt; (chrome+firefox)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;chrome is recommended (unfortunately), at least compared to firefox:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;up to 90% faster when hashing, especially on SSDs&lt;/li&gt; 
   &lt;li&gt;up to 40% faster when uploading over extremely fast internets&lt;/li&gt; 
   &lt;li&gt;but &lt;a href="https://github.com/9001/copyparty/raw/hovudstraum/bin/u2c.py"&gt;u2c.py&lt;/a&gt; can be 40% faster than chrome again&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;if you're cpu-bottlenecked, or the browser is maxing a cpu core:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;up to 30% faster uploads if you hide the upload status list by switching away from the &lt;code&gt;[🚀]&lt;/code&gt; up2k ui-tab (or closing it) 
    &lt;ul&gt; 
     &lt;li&gt;optionally you can switch to the lightweight potato ui by clicking the &lt;code&gt;[🥔]&lt;/code&gt;&lt;/li&gt; 
     &lt;li&gt;switching to another browser-tab also works, the favicon will update every 10 seconds in that case&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;unlikely to be a problem, but can happen when uploading many small files, or your internet is too fast, or PC too slow&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;security&lt;/h1&gt; 
&lt;p&gt;there is a &lt;a href="https://discord.gg/25J8CdTT6G"&gt;discord server&lt;/a&gt; with an &lt;code&gt;@everyone&lt;/code&gt; for all important updates (at the lack of better ideas)&lt;/p&gt; 
&lt;p&gt;some notes on hardening&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;set &lt;code&gt;--rproxy 0&lt;/code&gt; &lt;em&gt;if and only if&lt;/em&gt; your copyparty is directly facing the internet (not through a reverse-proxy) 
  &lt;ul&gt; 
   &lt;li&gt;cors doesn't work right otherwise&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;if you allow anonymous uploads or otherwise don't trust the contents of a volume, you can prevent XSS with volflag &lt;code&gt;nohtml&lt;/code&gt; 
  &lt;ul&gt; 
   &lt;li&gt;this returns html documents as plaintext, and also disables markdown rendering&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;when running behind a reverse-proxy, listen on a unix-socket for tighter access control (and more performance); see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#reverse-proxy"&gt;reverse-proxy&lt;/a&gt; or &lt;code&gt;--help-bind&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;safety profiles:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;option &lt;code&gt;-s&lt;/code&gt; is a shortcut to set the following options:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;--no-thumb&lt;/code&gt; disables thumbnails and audio transcoding to stop copyparty from running &lt;code&gt;FFmpeg&lt;/code&gt;/&lt;code&gt;Pillow&lt;/code&gt;/&lt;code&gt;VIPS&lt;/code&gt; on uploaded files, which is a &lt;a href="https://www.cvedetails.com/vulnerability-list.php?vendor_id=3611"&gt;good idea&lt;/a&gt; if anonymous upload is enabled&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;--no-mtag-ff&lt;/code&gt; uses &lt;code&gt;mutagen&lt;/code&gt; to grab music tags instead of &lt;code&gt;FFmpeg&lt;/code&gt;, which is safer and faster but less accurate&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;--dotpart&lt;/code&gt; hides uploads from directory listings while they're still incoming&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;--no-robots&lt;/code&gt; and &lt;code&gt;--force-js&lt;/code&gt; makes life harder for crawlers, see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#hiding-from-google"&gt;hiding from google&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;option &lt;code&gt;-ss&lt;/code&gt; is a shortcut for the above plus:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;--unpost 0&lt;/code&gt;, &lt;code&gt;--no-del&lt;/code&gt;, &lt;code&gt;--no-mv&lt;/code&gt; disables all move/delete support&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;--hardlink&lt;/code&gt; creates hardlinks instead of symlinks when deduplicating uploads, which is less maintenance 
    &lt;ul&gt; 
     &lt;li&gt;however note if you edit one file it will also affect the other copies&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;--vague-403&lt;/code&gt; returns a "404 not found" instead of "401 unauthorized" which is a common enterprise meme&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;-nih&lt;/code&gt; removes the server hostname from directory listings&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;option &lt;code&gt;-sss&lt;/code&gt; is a shortcut for the above plus:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;--no-dav&lt;/code&gt; disables webdav support&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;--no-logues&lt;/code&gt; and &lt;code&gt;--no-readme&lt;/code&gt; disables support for readme's and prologues / epilogues in directory listings, which otherwise lets people upload arbitrary (but sandboxed) &lt;code&gt;&amp;lt;script&amp;gt;&lt;/code&gt; tags&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;-lo cpp-%Y-%m%d-%H%M%S.txt.xz&lt;/code&gt; enables logging to disk&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;-ls **,*,ln,p,r&lt;/code&gt; does a scan on startup for any dangerous symlinks&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;other misc notes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;you can disable directory listings by giving permission &lt;code&gt;g&lt;/code&gt; instead of &lt;code&gt;r&lt;/code&gt;, only accepting direct URLs to files 
  &lt;ul&gt; 
   &lt;li&gt;you may want &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#filekeys"&gt;filekeys&lt;/a&gt; to prevent filename bruteforcing&lt;/li&gt; 
   &lt;li&gt;permission &lt;code&gt;h&lt;/code&gt; instead of &lt;code&gt;r&lt;/code&gt; makes copyparty behave like a traditional webserver with directory listing/index disabled, returning index.html instead 
    &lt;ul&gt; 
     &lt;li&gt;compatibility with filekeys: index.html itself can be retrieved without the correct filekey, but all other files are protected&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;gotchas&lt;/h2&gt; 
&lt;p&gt;behavior that might be unexpected&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;users without read-access to a folder can still see the &lt;code&gt;.prologue.html&lt;/code&gt; / &lt;code&gt;.epilogue.html&lt;/code&gt; / &lt;code&gt;PREADME.md&lt;/code&gt; / &lt;code&gt;README.md&lt;/code&gt; contents, for the purpose of showing a description on how to use the uploader for example&lt;/li&gt; 
 &lt;li&gt;users can submit &lt;code&gt;&amp;lt;script&amp;gt;&lt;/code&gt;s which autorun (in a sandbox) for other visitors in a few ways; 
  &lt;ul&gt; 
   &lt;li&gt;uploading a &lt;code&gt;README.md&lt;/code&gt; -- avoid with &lt;code&gt;--no-readme&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;renaming &lt;code&gt;some.html&lt;/code&gt; to &lt;code&gt;.epilogue.html&lt;/code&gt; -- avoid with either &lt;code&gt;--no-logues&lt;/code&gt; or &lt;code&gt;--no-dot-ren&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;the directory-listing embed is sandboxed (so any malicious scripts can't do any damage) but the markdown editor is not 100% safe, see below&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;markdown documents can contain html and &lt;code&gt;&amp;lt;script&amp;gt;&lt;/code&gt;s; attempts are made to prevent scripts from executing (unless &lt;code&gt;-emp&lt;/code&gt; is specified) but this is not 100% bulletproof, so setting the &lt;code&gt;nohtml&lt;/code&gt; volflag is still the safest choice 
  &lt;ul&gt; 
   &lt;li&gt;or eliminate the problem entirely by only giving write-access to trustworthy people :^)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;cors&lt;/h2&gt; 
&lt;p&gt;cross-site request config&lt;/p&gt; 
&lt;p&gt;by default, except for &lt;code&gt;GET&lt;/code&gt; and &lt;code&gt;HEAD&lt;/code&gt; operations, all requests must either:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;not contain an &lt;code&gt;Origin&lt;/code&gt; header at all&lt;/li&gt; 
 &lt;li&gt;or have an &lt;code&gt;Origin&lt;/code&gt; matching the server domain&lt;/li&gt; 
 &lt;li&gt;or the header &lt;code&gt;PW&lt;/code&gt; with your password as value&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;cors can be configured with &lt;code&gt;--acao&lt;/code&gt; and &lt;code&gt;--acam&lt;/code&gt;, or the protections entirely disabled with &lt;code&gt;--allow-csrf&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;filekeys&lt;/h2&gt; 
&lt;p&gt;prevent filename bruteforcing&lt;/p&gt; 
&lt;p&gt;volflag &lt;code&gt;fk&lt;/code&gt; generates filekeys (per-file accesskeys) for all files; users which have full read-access (permission &lt;code&gt;r&lt;/code&gt;) will then see URLs with the correct filekey &lt;code&gt;?k=...&lt;/code&gt; appended to the end, and &lt;code&gt;g&lt;/code&gt; users must provide that URL including the correct key to avoid a 404&lt;/p&gt; 
&lt;p&gt;by default, filekeys are generated based on salt (&lt;code&gt;--fk-salt&lt;/code&gt;) + filesystem-path + file-size + inode (if not windows); add volflag &lt;code&gt;fka&lt;/code&gt; to generate slightly weaker filekeys which will not be invalidated if the file is edited (only salt + path)&lt;/p&gt; 
&lt;p&gt;permissions &lt;code&gt;wG&lt;/code&gt; (write + upget) lets users upload files and receive their own filekeys, still without being able to see other uploads&lt;/p&gt; 
&lt;h3&gt;dirkeys&lt;/h3&gt; 
&lt;p&gt;share specific folders in a volume without giving away full read-access to the rest -- the visitor only needs the &lt;code&gt;g&lt;/code&gt; (get) permission to view the link&lt;/p&gt; 
&lt;p&gt;volflag &lt;code&gt;dk&lt;/code&gt; generates dirkeys (per-directory accesskeys) for all folders, granting read-access to that folder; by default only that folder itself, no subfolders&lt;/p&gt; 
&lt;p&gt;volflag &lt;code&gt;dky&lt;/code&gt; disables the actual key-check, meaning anyone can see the contents of a folder where they have &lt;code&gt;g&lt;/code&gt; access, but not its subdirectories&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;dk&lt;/code&gt; + &lt;code&gt;dky&lt;/code&gt; gives the same behavior as if all users with &lt;code&gt;g&lt;/code&gt; access have full read-access, but subfolders are hidden files (as if their names start with a dot), so &lt;code&gt;dky&lt;/code&gt; is an alternative to renaming all the folders for that purpose, maybe just for some users&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;volflag &lt;code&gt;dks&lt;/code&gt; lets people enter subfolders as well, and also enables download-as-zip/tar&lt;/p&gt; 
&lt;p&gt;if you enable dirkeys, it is probably a good idea to enable filekeys too, otherwise it will be impossible to hotlink files from a folder which was accessed using a dirkey&lt;/p&gt; 
&lt;p&gt;dirkeys are generated based on another salt (&lt;code&gt;--dk-salt&lt;/code&gt;) + filesystem-path and have a few limitations:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;the key does not change if the contents of the folder is modified 
  &lt;ul&gt; 
   &lt;li&gt;if you need a new dirkey, either change the salt or rename the folder&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;linking to a textfile (so it opens in the textfile viewer) is not possible if recipient doesn't have read-access&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;password hashing&lt;/h2&gt; 
&lt;p&gt;you can hash passwords before putting them into config files / providing them as arguments; see &lt;code&gt;--help-pwhash&lt;/code&gt; for all the details&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;--ah-alg argon2&lt;/code&gt; enables it, and if you have any plaintext passwords then it'll print the hashed versions on startup so you can replace them&lt;/p&gt; 
&lt;p&gt;optionally also specify &lt;code&gt;--ah-cli&lt;/code&gt; to enter an interactive mode where it will hash passwords without ever writing the plaintext ones to disk&lt;/p&gt; 
&lt;p&gt;the default configs take about 0.4 sec and 256 MiB RAM to process a new password on a decent laptop&lt;/p&gt; 
&lt;p&gt;when generating hashes using &lt;code&gt;--ah-cli&lt;/code&gt; for docker or systemd services, make sure it is using the same &lt;code&gt;--ah-salt&lt;/code&gt; by:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;inspecting the generated salt using &lt;code&gt;--show-ah-salt&lt;/code&gt; in copyparty service configuration&lt;/li&gt; 
 &lt;li&gt;setting the same &lt;code&gt;--ah-salt&lt;/code&gt; in both environments&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;⚠️ if you have enabled &lt;code&gt;--usernames&lt;/code&gt; then provide the password as &lt;code&gt;username:password&lt;/code&gt; when hashing it, for example &lt;code&gt;ed:hunter2&lt;/code&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;https&lt;/h2&gt; 
&lt;p&gt;both HTTP and HTTPS are accepted by default, but letting a &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#reverse-proxy"&gt;reverse proxy&lt;/a&gt; handle the https/tls/ssl would be better (probably more secure by default)&lt;/p&gt; 
&lt;p&gt;copyparty doesn't speak HTTP/2 or QUIC, so using a reverse proxy would solve that as well -- but note that HTTP/1 is usually faster than both HTTP/2 and HTTP/3&lt;/p&gt; 
&lt;p&gt;if &lt;a href="https://github.com/cloudflare/cfssl/releases/latest"&gt;cfssl&lt;/a&gt; is installed, copyparty will automatically create a CA and server-cert on startup&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;the certs are written to &lt;code&gt;--crt-dir&lt;/code&gt; for distribution, see &lt;code&gt;--help&lt;/code&gt; for the other &lt;code&gt;--crt&lt;/code&gt; options&lt;/li&gt; 
 &lt;li&gt;this will be a self-signed certificate so you must install your &lt;code&gt;ca.pem&lt;/code&gt; into all your browsers/devices&lt;/li&gt; 
 &lt;li&gt;if you want to avoid the hassle of distributing certs manually, please consider using a reverse proxy&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;recovering from crashes&lt;/h1&gt; 
&lt;h2&gt;client crashes&lt;/h2&gt; 
&lt;h3&gt;firefox wsod&lt;/h3&gt; 
&lt;p&gt;firefox 87 can crash during uploads -- the entire browser goes, including all other browser tabs, everything turns white&lt;/p&gt; 
&lt;p&gt;however you can hit &lt;code&gt;F12&lt;/code&gt; in the up2k tab and use the devtools to see how far you got in the uploads:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;get a complete list of all uploads, organized by status (ok / no-good / busy / queued):&lt;br /&gt; &lt;code&gt;var tabs = { ok:[], ng:[], bz:[], q:[] }; for (var a of up2k.ui.tab) tabs[a.in].push(a); tabs&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;list of filenames which failed:&lt;br /&gt; &lt;code&gt;​var ng = []; for (var a of up2k.ui.tab) if (a.in != 'ok') ng.push(a.hn.split('&amp;lt;a href=\"').slice(-1)[0].split('\"&amp;gt;')[0]); ng&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;send the list of filenames to copyparty for safekeeping:&lt;br /&gt; &lt;code&gt;await fetch('/inc', {method:'PUT', body:JSON.stringify(ng,null,1)})&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;HTTP API&lt;/h1&gt; 
&lt;p&gt;see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/devnotes.md#http-api"&gt;devnotes&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;dependencies&lt;/h1&gt; 
&lt;p&gt;mandatory deps:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;jinja2&lt;/code&gt; (is built into the SFX)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;optional dependencies&lt;/h2&gt; 
&lt;p&gt;install these to enable bonus features&lt;/p&gt; 
&lt;p&gt;enable &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#password-hashing"&gt;hashed passwords&lt;/a&gt; in config: &lt;code&gt;argon2-cffi&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;enable &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#ftp-server"&gt;ftp-server&lt;/a&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;for just plaintext FTP, &lt;code&gt;pyftpdlib&lt;/code&gt; (is built into the SFX)&lt;/li&gt; 
 &lt;li&gt;with TLS encryption, &lt;code&gt;pyftpdlib pyopenssl&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;enable &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#metadata-from-audio-files"&gt;music tags&lt;/a&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;either &lt;code&gt;mutagen&lt;/code&gt; (fast, pure-python, skips a few tags, makes copyparty GPL? idk)&lt;/li&gt; 
 &lt;li&gt;or &lt;code&gt;ffprobe&lt;/code&gt; (20x slower, more accurate, possibly dangerous depending on your distro and users)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;enable &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#thumbnails"&gt;thumbnails&lt;/a&gt; of...&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;images:&lt;/strong&gt; &lt;code&gt;Pillow&lt;/code&gt; and/or &lt;code&gt;pyvips&lt;/code&gt; and/or &lt;code&gt;ffmpeg&lt;/code&gt; (requires py2.7 or py3.5+)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;videos/audio:&lt;/strong&gt; &lt;code&gt;ffmpeg&lt;/code&gt; and &lt;code&gt;ffprobe&lt;/code&gt; somewhere in &lt;code&gt;$PATH&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;HEIF pictures:&lt;/strong&gt; &lt;code&gt;pyvips&lt;/code&gt; or &lt;code&gt;ffmpeg&lt;/code&gt; or &lt;code&gt;pillow-heif&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;AVIF pictures:&lt;/strong&gt; &lt;code&gt;pyvips&lt;/code&gt; or &lt;code&gt;ffmpeg&lt;/code&gt; or &lt;code&gt;pillow-avif-plugin&lt;/code&gt; or pillow v11.3+&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;JPEG XL pictures:&lt;/strong&gt; &lt;code&gt;pyvips&lt;/code&gt; or &lt;code&gt;ffmpeg&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;RAW images:&lt;/strong&gt; &lt;code&gt;rawpy&lt;/code&gt;, plus one of &lt;code&gt;pyvips&lt;/code&gt; or &lt;code&gt;Pillow&lt;/code&gt; (for some formats)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;enable sending &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#zeromq"&gt;zeromq messages&lt;/a&gt; from event-hooks: &lt;code&gt;pyzmq&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;enable &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#smb-server"&gt;smb&lt;/a&gt; support (&lt;strong&gt;not&lt;/strong&gt; recommended): &lt;code&gt;impacket==0.12.0&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pyvips&lt;/code&gt; gives higher quality thumbnails than &lt;code&gt;Pillow&lt;/code&gt; and is 320% faster, using 270% more ram: &lt;code&gt;sudo apt install libvips42 &amp;amp;&amp;amp; python3 -m pip install --user -U pyvips&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;to install FFmpeg on Windows, grab &lt;a href="https://www.gyan.dev/ffmpeg/builds/ffmpeg-git-full.7z"&gt;a recent build&lt;/a&gt; -- you need &lt;code&gt;ffmpeg.exe&lt;/code&gt; and &lt;code&gt;ffprobe.exe&lt;/code&gt; from inside the &lt;code&gt;bin&lt;/code&gt; folder; copy them into &lt;code&gt;C:\Windows\System32&lt;/code&gt; or any other folder that's in your &lt;code&gt;%PATH%&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;dependency chickenbits&lt;/h3&gt; 
&lt;p&gt;prevent loading an optional dependency , for example if:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;you have an incompatible version installed and it causes problems&lt;/li&gt; 
 &lt;li&gt;you just don't want copyparty to use it, maybe to save ram&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;set any of the following environment variables to disable its associated optional feature,&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;env-var&lt;/th&gt; 
   &lt;th&gt;what it does&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_NO_ARGON2&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;disable argon2-cffi password hashing&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_NO_CFSSL&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;never attempt to generate self-signed certificates using &lt;a href="https://github.com/cloudflare/cfssl"&gt;cfssl&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_NO_FFMPEG&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;audio transcoding&lt;/strong&gt; goes byebye, &lt;strong&gt;thumbnailing&lt;/strong&gt; must be handled by Pillow/libvips&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_NO_FFPROBE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;audio transcoding&lt;/strong&gt; goes byebye, &lt;strong&gt;thumbnailing&lt;/strong&gt; must be handled by Pillow/libvips, &lt;strong&gt;metadata-scanning&lt;/strong&gt; must be handled by mutagen&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_NO_MAGIC&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;do not use &lt;a href="https://pypi.org/project/python-magic/"&gt;magic&lt;/a&gt; for filetype detection&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_NO_MUTAGEN&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;do not use &lt;a href="https://pypi.org/project/mutagen/"&gt;mutagen&lt;/a&gt; for reading metadata from media files; will fallback to ffprobe&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_NO_PIL&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;disable all &lt;a href="https://pypi.org/project/pillow/"&gt;Pillow&lt;/a&gt;-based thumbnail support; will fallback to libvips or ffmpeg&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_NO_PILF&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;disable Pillow &lt;code&gt;ImageFont&lt;/code&gt; text rendering, used for folder thumbnails&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_NO_PIL_AVIF&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;disable Pillow avif support (internal and/or &lt;a href="https://pypi.org/project/pillow-avif-plugin/"&gt;plugin&lt;/a&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_NO_PIL_HEIF&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;disable 3rd-party Pillow plugin for &lt;a href="https://pypi.org/project/pillow-heif/"&gt;HEIF support&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_NO_PIL_WEBP&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;disable use of native webp support in Pillow&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_NO_PSUTIL&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;do not use &lt;a href="https://pypi.org/project/psutil/"&gt;psutil&lt;/a&gt; for reaping stuck hooks and plugins on Windows&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_NO_RAW&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;disable all &lt;a href="https://pypi.org/project/rawpy/"&gt;rawpy&lt;/a&gt;-based thumbnail support for RAW images&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_NO_VIPS&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;disable all &lt;a href="https://pypi.org/project/pyvips/"&gt;libvips&lt;/a&gt;-based thumbnail support; will fallback to Pillow or ffmpeg&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;example: &lt;code&gt;PRTY_NO_PIL=1 python3 copyparty-sfx.py&lt;/code&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;PRTY_NO_PIL&lt;/code&gt; saves ram&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;PRTY_NO_VIPS&lt;/code&gt; saves ram and startup time&lt;/li&gt; 
 &lt;li&gt;python2.7 on windows: &lt;code&gt;PRTY_NO_FFMPEG&lt;/code&gt; + &lt;code&gt;PRTY_NO_FFPROBE&lt;/code&gt; saves startup time&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;optional gpl stuff&lt;/h2&gt; 
&lt;p&gt;some bundled tools have copyleft dependencies, see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/bin/#mtag"&gt;./bin/#mtag&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;these are standalone programs and will never be imported / evaluated by copyparty, and must be enabled through &lt;code&gt;-mtp&lt;/code&gt; configs&lt;/p&gt; 
&lt;h1&gt;sfx&lt;/h1&gt; 
&lt;p&gt;the self-contained "binary" (recommended!) &lt;a href="https://github.com/9001/copyparty/releases/latest/download/copyparty-sfx.py"&gt;copyparty-sfx.py&lt;/a&gt; will unpack itself and run copyparty, assuming you have python installed of course&lt;/p&gt; 
&lt;p&gt;if you only need english, &lt;a href="https://github.com/9001/copyparty/releases/latest/download/copyparty-en.py"&gt;copyparty-en.py&lt;/a&gt; is the same thing but smaller&lt;/p&gt; 
&lt;p&gt;you can reduce the sfx size by repacking it; see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/devnotes.md#sfx-repack"&gt;./docs/devnotes.md#sfx-repack&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;copyparty.exe&lt;/h2&gt; 
&lt;p&gt;download &lt;a href="https://github.com/9001/copyparty/releases/latest/download/copyparty.exe"&gt;copyparty.exe&lt;/a&gt; (win8+) or &lt;a href="https://github.com/9001/copyparty/releases/latest/download/copyparty32.exe"&gt;copyparty32.exe&lt;/a&gt; (win7+)&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/241032/221445946-1e328e56-8c5b-44a9-8b9f-dee84d942535.png" alt="copyparty-exe-fs8" /&gt;&lt;/p&gt; 
&lt;p&gt;can be convenient on machines where installing python is problematic, however is &lt;strong&gt;not recommended&lt;/strong&gt; -- if possible, please use &lt;strong&gt;&lt;a href="https://github.com/9001/copyparty/releases/latest/download/copyparty-sfx.py"&gt;copyparty-sfx.py&lt;/a&gt;&lt;/strong&gt; instead&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/9001/copyparty/releases/latest/download/copyparty.exe"&gt;copyparty.exe&lt;/a&gt; runs on win8 or newer, was compiled on win10, does thumbnails + media tags, and is &lt;em&gt;currently&lt;/em&gt; safe to use, but any future python/expat/pillow CVEs can only be remedied by downloading a newer version of the exe&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;on win8 it needs &lt;a href="https://www.microsoft.com/en-us/download/details.aspx?id=48145"&gt;vc redist 2015&lt;/a&gt;, on win10 it just works&lt;/li&gt; 
   &lt;li&gt;some antivirus may freak out (false-positive), possibly &lt;a href="https://www.virustotal.com/gui/file/52391a1e9842cf70ad243ef83844d46d29c0044d101ee0138fcdd3c8de2237d6/detection"&gt;Avast, AVG, and McAfee&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;dangerous: &lt;a href="https://github.com/9001/copyparty/releases/latest/download/copyparty32.exe"&gt;copyparty32.exe&lt;/a&gt; is compatible with &lt;a href="https://user-images.githubusercontent.com/241032/221445944-ae85d1f4-d351-4837-b130-82cab57d6cca.png"&gt;windows7&lt;/a&gt;, which means it uses an ancient copy of python (3.7.9) which cannot be upgraded and should never be exposed to the internet (LAN is fine)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;dangerous and deprecated: &lt;a href="https://github.com/9001/copyparty/releases/download/v1.8.7/copyparty-winpe64.exe"&gt;copyparty-winpe64.exe&lt;/a&gt; lets you &lt;a href="https://user-images.githubusercontent.com/241032/205454984-e6b550df-3c49-486d-9267-1614078dd0dd.png"&gt;run copyparty in WinPE&lt;/a&gt; and is otherwise completely useless&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;meanwhile &lt;a href="https://github.com/9001/copyparty/releases/latest/download/copyparty-sfx.py"&gt;copyparty-sfx.py&lt;/a&gt; instead relies on your system python which gives better performance and will stay safe as long as you keep your python install up-to-date&lt;/p&gt; 
&lt;p&gt;then again, if you are already into downloading shady binaries from the internet, you may also want my &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/scripts/pyinstaller#ffmpeg"&gt;minimal builds&lt;/a&gt; of &lt;a href="https://ocv.me/stuff/bin/ffmpeg.exe"&gt;ffmpeg&lt;/a&gt; and &lt;a href="https://ocv.me/stuff/bin/ffprobe.exe"&gt;ffprobe&lt;/a&gt; which enables copyparty to extract multimedia-info, do audio-transcoding, and thumbnails/spectrograms/waveforms, however it's much better to instead grab a &lt;a href="https://www.gyan.dev/ffmpeg/builds/ffmpeg-git-full.7z"&gt;recent official build&lt;/a&gt; every once ina while if you can afford the size&lt;/p&gt; 
&lt;h2&gt;zipapp&lt;/h2&gt; 
&lt;p&gt;another emergency alternative, &lt;a href="https://github.com/9001/copyparty/releases/latest/download/copyparty.pyz"&gt;copyparty.pyz&lt;/a&gt; has less features, is slow, requires python 3.7 or newer, worse compression, and more importantly is unable to benefit from more recent versions of jinja2 and such (which makes it less secure)... lots of drawbacks with this one really -- but, unlike the sfx, it is a completely normal zipfile which does not unpack any temporary files to disk, so it &lt;em&gt;may&lt;/em&gt; just work if the regular sfx fails to start because the computer is messed up in certain funky ways, so it's worth a shot if all else fails&lt;/p&gt; 
&lt;p&gt;run it by doubleclicking it, or try typing &lt;code&gt;python copyparty.pyz&lt;/code&gt; in your terminal/console/commandline/telex if that fails&lt;/p&gt; 
&lt;p&gt;it is a python &lt;a href="https://docs.python.org/3/library/zipapp.html"&gt;zipapp&lt;/a&gt; meaning it doesn't have to unpack its own python code anywhere to run, so if the filesystem is busted it has a better chance of getting somewhere&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;but note that it currently still needs to extract the web-resources somewhere (they'll land in the default TEMP-folder of your OS)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;install on android&lt;/h1&gt; 
&lt;p&gt;install &lt;a href="https://termux.com/"&gt;Termux&lt;/a&gt; + its companion app &lt;code&gt;Termux:API&lt;/code&gt; (see &lt;a href="https://ocv.me/termux/"&gt;ocv.me/termux&lt;/a&gt;) and then copy-paste this into Termux (long-tap) all at once:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;yes | pkg upgrade &amp;amp;&amp;amp; termux-setup-storage &amp;amp;&amp;amp; yes | pkg install python termux-api &amp;amp;&amp;amp; python -m ensurepip &amp;amp;&amp;amp; python -m pip install --user -U copyparty &amp;amp;&amp;amp; { grep -qE 'PATH=.*\.local/bin' ~/.bashrc 2&amp;gt;/dev/null || { echo 'PATH="$HOME/.local/bin:$PATH"' &amp;gt;&amp;gt; ~/.bashrc &amp;amp;&amp;amp; . ~/.bashrc; }; }
echo $?
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;after the initial setup, you can launch copyparty at any time by running &lt;code&gt;copyparty&lt;/code&gt; anywhere in Termux -- and if you run it with &lt;code&gt;--qr&lt;/code&gt; you'll get a &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#qr-code"&gt;neat qr-code&lt;/a&gt; pointing to your external ip&lt;/p&gt; 
&lt;p&gt;if you want thumbnails (photos+videos) and you're okay with spending another 132 MiB of storage, &lt;code&gt;pkg install ffmpeg &amp;amp;&amp;amp; python3 -m pip install --user -U pillow&lt;/code&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;or if you want to use &lt;code&gt;vips&lt;/code&gt; for photo-thumbs instead, &lt;code&gt;pkg install libvips &amp;amp;&amp;amp; python -m pip install --user -U wheel &amp;amp;&amp;amp; python -m pip install --user -U pyvips &amp;amp;&amp;amp; (cd /data/data/com.termux/files/usr/lib/; ln -s libgobject-2.0.so{,.0}; ln -s libvips.so{,.42})&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;install on iOS&lt;/h1&gt; 
&lt;p&gt;first install one of the following:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://apps.apple.com/us/app/a-shell-mini/id1543537943"&gt;a-Shell mini&lt;/a&gt; gives you the essential features&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://apps.apple.com/us/app/a-shell/id1473805438"&gt;a-Shell&lt;/a&gt; also enables audio transcoding and better thubmnails&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;and then copypaste the following command into &lt;code&gt;a-Shell&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;curl https://github.com/9001/copyparty/raw/refs/heads/hovudstraum/contrib/setup-ashell.sh | sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;what this does:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;creates a basic &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#accounts-and-volumes"&gt;config file&lt;/a&gt; named &lt;code&gt;cpc&lt;/code&gt; which you can edit with &lt;code&gt;vim cpc&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;adds the command &lt;code&gt;cpp&lt;/code&gt; to launch copyparty with that config file&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;known issues:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;cannot run in the background; it needs to be on-screen to accept connections / uploads / downloads&lt;/li&gt; 
 &lt;li&gt;the best way to exit copyparty is to swipe away the app&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;reporting bugs&lt;/h1&gt; 
&lt;p&gt;ideas for context to include, and where to submit them&lt;/p&gt; 
&lt;p&gt;please get in touch using any of the following URLs:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/9001/copyparty/"&gt;https://github.com/9001/copyparty/&lt;/a&gt; &lt;strong&gt;(primary)&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://gitlab.com/9001/copyparty/"&gt;https://gitlab.com/9001/copyparty/&lt;/a&gt; &lt;em&gt;(mirror)&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://codeberg.org/9001/copyparty"&gt;https://codeberg.org/9001/copyparty&lt;/a&gt; &lt;em&gt;(mirror)&lt;/em&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;in general, commandline arguments (and config file if any)&lt;/p&gt; 
&lt;p&gt;if something broke during an upload (replacing FILENAME with a part of the filename that broke):&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;journalctl -aS '48 hour ago' -u copyparty | grep -C10 FILENAME | tee bug.log
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;if there's a wall of base64 in the log (thread stacks) then please include that, especially if you run into something freezing up or getting stuck, for example &lt;code&gt;OperationalError('database is locked')&lt;/code&gt; -- alternatively you can visit &lt;code&gt;/?stack&lt;/code&gt; to see the stacks live, so &lt;a href="http://127.0.0.1:3923/?stack"&gt;http://127.0.0.1:3923/?stack&lt;/a&gt; for example&lt;/p&gt; 
&lt;h1&gt;devnotes&lt;/h1&gt; 
&lt;p&gt;for build instructions etc, see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/devnotes.md"&gt;./docs/devnotes.md&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;specifically you may want to &lt;a href="https://github.com/9001/copyparty/raw/hovudstraum/docs/devnotes.md#just-the-sfx"&gt;build the sfx&lt;/a&gt; or &lt;a href="https://github.com/9001/copyparty/raw/hovudstraum/docs/devnotes.md#build-from-scratch"&gt;build from scratch&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/TODO.md"&gt;./docs/TODO.md&lt;/a&gt; for planned features / fixes / changes&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Dao-AILab/flash-attention</title>
      <link>https://github.com/Dao-AILab/flash-attention</link>
      <description>&lt;p&gt;Fast and memory-efficient exact attention&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;FlashAttention&lt;/h1&gt; 
&lt;p&gt;This repository provides the official implementation of FlashAttention and FlashAttention-2 from the following papers.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness&lt;/strong&gt;&lt;br /&gt; Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, Christopher Ré&lt;br /&gt; Paper: &lt;a href="https://arxiv.org/abs/2205.14135"&gt;https://arxiv.org/abs/2205.14135&lt;/a&gt;&lt;br /&gt; IEEE Spectrum &lt;a href="https://spectrum.ieee.org/mlperf-rankings-2022"&gt;article&lt;/a&gt; about our submission to the MLPerf 2.0 benchmark using FlashAttention. &lt;img src="https://raw.githubusercontent.com/Dao-AILab/flash-attention/main/assets/flashattn_banner.jpg" alt="FlashAttention" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning&lt;/strong&gt;&lt;br /&gt; Tri Dao&lt;/p&gt; 
&lt;p&gt;Paper: &lt;a href="https://tridao.me/publications/flash2/flash2.pdf"&gt;https://tridao.me/publications/flash2/flash2.pdf&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Dao-AILab/flash-attention/main/assets/flashattention_logo.png" alt="FlashAttention-2" /&gt;&lt;/p&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;p&gt;We've been very happy to see FlashAttention being widely adopted in such a short time after its release. This &lt;a href="https://github.com/Dao-AILab/flash-attention/raw/main/usage.md"&gt;page&lt;/a&gt; contains a partial list of places where FlashAttention is being used.&lt;/p&gt; 
&lt;p&gt;FlashAttention and FlashAttention-2 are free to use and modify (see LICENSE). Please cite and credit FlashAttention if you use it.&lt;/p&gt; 
&lt;h2&gt;FlashAttention-3 beta release&lt;/h2&gt; 
&lt;p&gt;FlashAttention-3 is optimized for Hopper GPUs (e.g. H100).&lt;/p&gt; 
&lt;p&gt;Blogpost: &lt;a href="https://tridao.me/blog/2024/flash3/"&gt;https://tridao.me/blog/2024/flash3/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Paper: &lt;a href="https://tridao.me/publications/flash3/flash3.pdf"&gt;https://tridao.me/publications/flash3/flash3.pdf&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Dao-AILab/flash-attention/main/assets/flash3_fp16_fwd.png" alt="FlashAttention-3 speedup on H100 80GB SXM5 with FP16" /&gt;&lt;/p&gt; 
&lt;p&gt;This is a beta release for testing / benchmarking before we integrate that with the rest of the repo.&lt;/p&gt; 
&lt;p&gt;Currently released:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;FP16 / BF16 forward and backward, FP8 forward&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Requirements: H100 / H800 GPU, CUDA &amp;gt;= 12.3.&lt;/p&gt; 
&lt;p&gt;We highly recommend CUDA 12.8 for best performance.&lt;/p&gt; 
&lt;p&gt;To install:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;cd hopper
python setup.py install
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To run the test:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;export PYTHONPATH=$PWD
pytest -q -s test_flash_attn.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Once the package is installed, you can import it as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import flash_attn_interface
flash_attn_interface.flash_attn_func()
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Installation and features&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Requirements:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;CUDA toolkit or ROCm toolkit&lt;/li&gt; 
 &lt;li&gt;PyTorch 2.2 and above.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;packaging&lt;/code&gt; Python package (&lt;code&gt;pip install packaging&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ninja&lt;/code&gt; Python package (&lt;code&gt;pip install ninja&lt;/code&gt;) *&lt;/li&gt; 
 &lt;li&gt;Linux. Might work for Windows starting v2.3.2 (we've seen a few positive &lt;a href="https://github.com/Dao-AILab/flash-attention/issues/595"&gt;reports&lt;/a&gt;) but Windows compilation still requires more testing. If you have ideas on how to set up prebuilt CUDA wheels for Windows, please reach out via Github issue.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;* Make sure that &lt;code&gt;ninja&lt;/code&gt; is installed and that it works correctly (e.g. &lt;code&gt;ninja --version&lt;/code&gt; then &lt;code&gt;echo $?&lt;/code&gt; should return exit code 0). If not (sometimes &lt;code&gt;ninja --version&lt;/code&gt; then &lt;code&gt;echo $?&lt;/code&gt; returns a nonzero exit code), uninstall then reinstall &lt;code&gt;ninja&lt;/code&gt; (&lt;code&gt;pip uninstall -y ninja &amp;amp;&amp;amp; pip install ninja&lt;/code&gt;). Without &lt;code&gt;ninja&lt;/code&gt;, compiling can take a very long time (2h) since it does not use multiple CPU cores. With &lt;code&gt;ninja&lt;/code&gt; compiling takes 3-5 minutes on a 64-core machine using CUDA toolkit.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;To install:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip install flash-attn --no-build-isolation
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively you can compile from source:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python setup.py install
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If your machine has less than 96GB of RAM and lots of CPU cores, &lt;code&gt;ninja&lt;/code&gt; might run too many parallel compilation jobs that could exhaust the amount of RAM. To limit the number of parallel compilation jobs, you can set the environment variable &lt;code&gt;MAX_JOBS&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;MAX_JOBS=4 pip install flash-attn --no-build-isolation
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Interface:&lt;/strong&gt; &lt;code&gt;src/flash_attention_interface.py&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;NVIDIA CUDA Support&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Requirements:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;CUDA 12.0 and above.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We recommend the &lt;a href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch"&gt;Pytorch&lt;/a&gt; container from Nvidia, which has all the required tools to install FlashAttention.&lt;/p&gt; 
&lt;p&gt;FlashAttention-2 with CUDA currently supports:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Ampere, Ada, or Hopper GPUs (e.g., A100, RTX 3090, RTX 4090, H100). Support for Turing GPUs (T4, RTX 2080) is coming soon, please use FlashAttention 1.x for Turing GPUs for now.&lt;/li&gt; 
 &lt;li&gt;Datatype fp16 and bf16 (bf16 requires Ampere, Ada, or Hopper GPUs).&lt;/li&gt; 
 &lt;li&gt;All head dimensions up to 256. &lt;del&gt;Head dim &amp;gt; 192 backward requires A100/A800 or H100/H800&lt;/del&gt;. Head dim 256 backward now works on consumer GPUs (if there's no dropout) as of flash-attn 2.5.5.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;AMD ROCm Support&lt;/h3&gt; 
&lt;p&gt;ROCm version has two backends. There is &lt;a href="https://github.com/ROCm/composable_kernel"&gt;composable_kernel&lt;/a&gt; (ck) which is the default backend and a &lt;a href="https://github.com/triton-lang/triton"&gt;Triton&lt;/a&gt; backend. They provide an implementation of FlashAttention-2.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Requirements:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ROCm 6.0 and above.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We recommend the &lt;a href="https://hub.docker.com/r/rocm/pytorch"&gt;Pytorch&lt;/a&gt; container from ROCm, which has all the required tools to install FlashAttention.&lt;/p&gt; 
&lt;h4&gt;Composable Kernel Backend&lt;/h4&gt; 
&lt;p&gt;FlashAttention-2 ROCm CK backend currently supports:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;MI200 or MI300 GPUs.&lt;/li&gt; 
 &lt;li&gt;Datatype fp16 and bf16&lt;/li&gt; 
 &lt;li&gt;Both forward's and backward's head dimensions up to 256.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Triton Backend&lt;/h4&gt; 
&lt;p&gt;The Triton implementation of the &lt;a href="https://tridao.me/publications/flash2/flash2.pdf"&gt;Flash Attention v2&lt;/a&gt; is currently a work in progress.&lt;/p&gt; 
&lt;p&gt;It supports AMD's CDNA (MI200, MI300) and RDNA GPU's using fp16, bf16 and fp32 datatypes.&lt;/p&gt; 
&lt;p&gt;These features are supported in Fwd and Bwd&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Fwd and Bwd with causal masking&lt;/li&gt; 
 &lt;li&gt;Variable sequence lengths&lt;/li&gt; 
 &lt;li&gt;Arbitrary Q and KV sequence lengths&lt;/li&gt; 
 &lt;li&gt;Arbitrary head sizes&lt;/li&gt; 
 &lt;li&gt;Multi and grouped query attention&lt;/li&gt; 
 &lt;li&gt;Dropout&lt;/li&gt; 
 &lt;li&gt;Rotary embeddings&lt;/li&gt; 
 &lt;li&gt;ALiBi&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;We are working on the following things&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Paged Attention&lt;/li&gt; 
 &lt;li&gt;Sliding Window&lt;/li&gt; 
 &lt;li&gt;FP8&lt;/li&gt; 
 &lt;li&gt;Performance Improvements&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h5&gt;Getting Started&lt;/h5&gt; 
&lt;p&gt;To get started with the triton backend for AMD, follow the steps below.&lt;/p&gt; 
&lt;p&gt;First install the recommended Triton version&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install triton==3.2.0
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then install Flash Attention with the flag &lt;code&gt;FLASH_ATTENTION_TRITON_AMD_ENABLE&lt;/code&gt; set to &lt;code&gt;"TRUE"&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;cd flash-attention
git checkout main_perf
FLASH_ATTENTION_TRITON_AMD_ENABLE="TRUE" python setup.py install
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To test that things are working, you can run our tests. These tests take hours so you don't need to run the full thing.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;FLASH_ATTENTION_TRITON_AMD_ENABLE="TRUE" pytest tests/test_flash_attn_triton_amd.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can use autotune for better performance by using this flag &lt;code&gt;FLASH_ATTENTION_TRITON_AMD_AUTOTUNE="TRUE"&lt;/code&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;FLASH_ATTENTION_TRITON_AMD_ENABLE="TRUE" FLASH_ATTENTION_TRITON_AMD_AUTOTUNE="TRUE" python $PATH_TO_CODE
&lt;/code&gt;&lt;/pre&gt; 
&lt;h6&gt;Docker&lt;/h6&gt; 
&lt;p&gt;You can also use the Dockerfile below which does the above steps on top of the latest rocm/pytorch image.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;FROM rocm/pytorch:latest

WORKDIR /workspace

# install triton
RUN pip install triton==3.2.0

# install flash attention
ENV FLASH_ATTENTION_TRITON_AMD_ENABLE="TRUE"

RUN git clone https://github.com/ROCm/flash-attention.git &amp;amp;&amp;amp;\ 
    cd flash-attention &amp;amp;&amp;amp;\
    git checkout main_perf &amp;amp;&amp;amp;\
    python setup.py install

# set working dir
WORKDIR /workspace/flash-attention
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To build the docker file&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;docker build -t fa_triton .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To run the docker image&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;docker run -it --network=host --user root --group-add video --cap-add=SYS_PTRACE --security-opt seccomp=unconfined --ipc=host --shm-size 16G --device=/dev/kfd --device=/dev/dri fa_triton
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;How to use FlashAttention&lt;/h2&gt; 
&lt;p&gt;The main functions implement scaled dot product attention (softmax(Q @ K^T * softmax_scale) @ V):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from flash_attn import flash_attn_qkvpacked_func, flash_attn_func
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;flash_attn_qkvpacked_func(qkv, dropout_p=0.0, softmax_scale=None, causal=False,
                          window_size=(-1, -1), alibi_slopes=None, deterministic=False):
"""dropout_p should be set to 0.0 during evaluation
If Q, K, V are already stacked into 1 tensor, this function will be faster than
calling flash_attn_func on Q, K, V since the backward pass avoids explicit concatenation
of the gradients of Q, K, V.
If window_size != (-1, -1), implements sliding window local attention. Query at position i
will only attend to keys between [i - window_size[0], i + window_size[1]] inclusive.
Arguments:
    qkv: (batch_size, seqlen, 3, nheads, headdim)
    dropout_p: float. Dropout probability.
    softmax_scale: float. The scaling of QK^T before applying softmax.
        Default to 1 / sqrt(headdim).
    causal: bool. Whether to apply causal attention mask (e.g., for auto-regressive modeling).
    window_size: (left, right). If not (-1, -1), implements sliding window local attention.
    alibi_slopes: (nheads,) or (batch_size, nheads), fp32. A bias of (-alibi_slope * |i - j|) is added to
        the attention score of query i and key j.
    deterministic: bool. Whether to use the deterministic implementation of the backward pass,
        which is slightly slower and uses more memory. The forward pass is always deterministic.
Return:
    out: (batch_size, seqlen, nheads, headdim).
"""
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;flash_attn_func(q, k, v, dropout_p=0.0, softmax_scale=None, causal=False,
                window_size=(-1, -1), alibi_slopes=None, deterministic=False):
"""dropout_p should be set to 0.0 during evaluation
Supports multi-query and grouped-query attention (MQA/GQA) by passing in KV with fewer heads
than Q. Note that the number of heads in Q must be divisible by the number of heads in KV.
For example, if Q has 6 heads and K, V have 2 heads, head 0, 1, 2 of Q will attention to head
0 of K, V, and head 3, 4, 5 of Q will attention to head 1 of K, V.
If window_size != (-1, -1), implements sliding window local attention. Query at position i
will only attend to keys between
[i + seqlen_k - seqlen_q - window_size[0], i + seqlen_k - seqlen_q + window_size[1]] inclusive.

Arguments:
    q: (batch_size, seqlen, nheads, headdim)
    k: (batch_size, seqlen, nheads_k, headdim)
    v: (batch_size, seqlen, nheads_k, headdim)
    dropout_p: float. Dropout probability.
    softmax_scale: float. The scaling of QK^T before applying softmax.
        Default to 1 / sqrt(headdim).
    causal: bool. Whether to apply causal attention mask (e.g., for auto-regressive modeling).
    window_size: (left, right). If not (-1, -1), implements sliding window local attention.
    alibi_slopes: (nheads,) or (batch_size, nheads), fp32. A bias of
        (-alibi_slope * |i + seqlen_k - seqlen_q - j|)
        is added to the attention score of query i and key j.
    deterministic: bool. Whether to use the deterministic implementation of the backward pass,
        which is slightly slower and uses more memory. The forward pass is always deterministic.
Return:
    out: (batch_size, seqlen, nheads, headdim).
"""
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;def flash_attn_with_kvcache(
    q,
    k_cache,
    v_cache,
    k=None,
    v=None,
    rotary_cos=None,
    rotary_sin=None,
    cache_seqlens: Optional[Union[(int, torch.Tensor)]] = None,
    cache_batch_idx: Optional[torch.Tensor] = None,
    block_table: Optional[torch.Tensor] = None,
    softmax_scale=None,
    causal=False,
    window_size=(-1, -1),  # -1 means infinite context window
    rotary_interleaved=True,
    alibi_slopes=None,
):
    """
    If k and v are not None, k_cache and v_cache will be updated *inplace* with the new values from
    k and v. This is useful for incremental decoding: you can pass in the cached keys/values from
    the previous step, and update them with the new keys/values from the current step, and do
    attention with the updated cache, all in 1 kernel.

    If you pass in k / v, you must make sure that the cache is large enough to hold the new values.
    For example, the KV cache could be pre-allocated with the max sequence length, and you can use
    cache_seqlens to keep track of the current sequence lengths of each sequence in the batch.

    Also apply rotary embedding if rotary_cos and rotary_sin are passed in. The key @k will be
    rotated by rotary_cos and rotary_sin at indices cache_seqlens, cache_seqlens + 1, etc.
    If causal or local (i.e., window_size != (-1, -1)), the query @q will be rotated by rotary_cos
    and rotary_sin at indices cache_seqlens, cache_seqlens + 1, etc.
    If not causal and not local, the query @q will be rotated by rotary_cos and rotary_sin at
    indices cache_seqlens only (i.e. we consider all tokens in @q to be at position cache_seqlens).

    See tests/test_flash_attn.py::test_flash_attn_kvcache for examples of how to use this function.

    Supports multi-query and grouped-query attention (MQA/GQA) by passing in KV with fewer heads
    than Q. Note that the number of heads in Q must be divisible by the number of heads in KV.
    For example, if Q has 6 heads and K, V have 2 heads, head 0, 1, 2 of Q will attention to head
    0 of K, V, and head 3, 4, 5 of Q will attention to head 1 of K, V.

    If causal=True, the causal mask is aligned to the bottom right corner of the attention matrix.
    For example, if seqlen_q = 2 and seqlen_k = 5, the causal mask (1 = keep, 0 = masked out) is:
        1 1 1 1 0
        1 1 1 1 1
    If seqlen_q = 5 and seqlen_k = 2, the causal mask is:
        0 0
        0 0
        0 0
        1 0
        1 1
    If the row of the mask is all zero, the output will be zero.

    If window_size != (-1, -1), implements sliding window local attention. Query at position i
    will only attend to keys between
    [i + seqlen_k - seqlen_q - window_size[0], i + seqlen_k - seqlen_q + window_size[1]] inclusive.

    Note: Does not support backward pass.

    Arguments:
        q: (batch_size, seqlen, nheads, headdim)
        k_cache: (batch_size_cache, seqlen_cache, nheads_k, headdim) if there's no block_table,
            or (num_blocks, page_block_size, nheads_k, headdim) if there's a block_table (i.e. paged KV cache)
            page_block_size must be a multiple of 256.
        v_cache: (batch_size_cache, seqlen_cache, nheads_k, headdim) if there's no block_table,
            or (num_blocks, page_block_size, nheads_k, headdim) if there's a block_table (i.e. paged KV cache)
        k [optional]: (batch_size, seqlen_new, nheads_k, headdim). If not None, we concatenate
            k with k_cache, starting at the indices specified by cache_seqlens.
        v [optional]: (batch_size, seqlen_new, nheads_k, headdim). Similar to k.
        rotary_cos [optional]: (seqlen_ro, rotary_dim / 2). If not None, we apply rotary embedding
            to k and q. Only applicable if k and v are passed in. rotary_dim must be divisible by 16.
        rotary_sin [optional]: (seqlen_ro, rotary_dim / 2). Similar to rotary_cos.
        cache_seqlens: int, or (batch_size,), dtype torch.int32. The sequence lengths of the
            KV cache.
        block_table [optional]: (batch_size, max_num_blocks_per_seq), dtype torch.int32.
        cache_batch_idx: (batch_size,), dtype torch.int32. The indices used to index into the KV cache.
            If None, we assume that the batch indices are [0, 1, 2, ..., batch_size - 1].
            If the indices are not distinct, and k and v are provided, the values updated in the cache
                 might come from any of the duplicate indices.
        softmax_scale: float. The scaling of QK^T before applying softmax.
            Default to 1 / sqrt(headdim).
        causal: bool. Whether to apply causal attention mask (e.g., for auto-regressive modeling).
        window_size: (left, right). If not (-1, -1), implements sliding window local attention.
        rotary_interleaved: bool. Only applicable if rotary_cos and rotary_sin are passed in.
            If True, rotary embedding will combine dimensions 0 &amp;amp; 1, 2 &amp;amp; 3, etc. If False,
            rotary embedding will combine dimensions 0 &amp;amp; rotary_dim / 2, 1 &amp;amp; rotary_dim / 2 + 1
            (i.e. GPT-NeoX style).
        alibi_slopes: (nheads,) or (batch_size, nheads), fp32. A bias of
            (-alibi_slope * |i + seqlen_k - seqlen_q - j|)
            is added to the attention score of query i and key j.

    Return:
        out: (batch_size, seqlen, nheads, headdim).
    """
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To see how these functions are used in a multi-head attention layer (which includes QKV projection, output projection), see the MHA &lt;a href="https://github.com/Dao-AILab/flash-attention/raw/main/flash_attn/modules/mha.py"&gt;implementation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Changelog&lt;/h2&gt; 
&lt;h3&gt;2.0: Complete rewrite, 2x faster&lt;/h3&gt; 
&lt;p&gt;Upgrading from FlashAttention (1.x) to FlashAttention-2&lt;/p&gt; 
&lt;p&gt;These functions have been renamed:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;flash_attn_unpadded_func&lt;/code&gt; -&amp;gt; &lt;code&gt;flash_attn_varlen_func&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;flash_attn_unpadded_qkvpacked_func&lt;/code&gt; -&amp;gt; &lt;code&gt;flash_attn_varlen_qkvpacked_func&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;flash_attn_unpadded_kvpacked_func&lt;/code&gt; -&amp;gt; &lt;code&gt;flash_attn_varlen_kvpacked_func&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If the inputs have the same sequence lengths in the same batch, it is simpler and faster to use these functions:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;flash_attn_qkvpacked_func(qkv, dropout_p=0.0, softmax_scale=None, causal=False)
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;flash_attn_func(q, k, v, dropout_p=0.0, softmax_scale=None, causal=False)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2.1: Change behavior of causal flag&lt;/h3&gt; 
&lt;p&gt;If seqlen_q != seqlen_k and causal=True, the causal mask is aligned to the bottom right corner of the attention matrix, instead of the top-left corner.&lt;/p&gt; 
&lt;p&gt;For example, if seqlen_q = 2 and seqlen_k = 5, the causal mask (1 = keep, 0 = masked out) is:&lt;br /&gt; v2.0:&lt;br /&gt; 1 0 0 0 0&lt;br /&gt; 1 1 0 0 0&lt;br /&gt; v2.1:&lt;br /&gt; 1 1 1 1 0&lt;br /&gt; 1 1 1 1 1&lt;/p&gt; 
&lt;p&gt;If seqlen_q = 5 and seqlen_k = 2, the causal mask is:&lt;br /&gt; v2.0:&lt;br /&gt; 1 0&lt;br /&gt; 1 1&lt;br /&gt; 1 1&lt;br /&gt; 1 1&lt;br /&gt; 1 1&lt;br /&gt; v2.1:&lt;br /&gt; 0 0&lt;br /&gt; 0 0&lt;br /&gt; 0 0&lt;br /&gt; 1 0&lt;br /&gt; 1 1&lt;br /&gt; If the row of the mask is all zero, the output will be zero.&lt;/p&gt; 
&lt;h3&gt;2.2: Optimize for inference&lt;/h3&gt; 
&lt;p&gt;Optimize for inference (iterative decoding) when query has very small sequence length (e.g., query sequence length = 1). The bottleneck here is to load KV cache as fast as possible, and we split the loading across different thread blocks, with a separate kernel to combine results.&lt;/p&gt; 
&lt;p&gt;See the function &lt;code&gt;flash_attn_with_kvcache&lt;/code&gt; with more features for inference (perform rotary embedding, updating KV cache inplace).&lt;/p&gt; 
&lt;p&gt;Thanks to the xformers team, and in particular Daniel Haziza, for this collaboration.&lt;/p&gt; 
&lt;h3&gt;2.3: Local (i.e., sliding window) attention&lt;/h3&gt; 
&lt;p&gt;Implement sliding window attention (i.e., local attention). Thanks to &lt;a href="https://mistral.ai/"&gt;Mistral AI&lt;/a&gt; and in particular Timothée Lacroix for this contribution. Sliding window was used in the &lt;a href="https://mistral.ai/news/announcing-mistral-7b/"&gt;Mistral 7B&lt;/a&gt; model.&lt;/p&gt; 
&lt;h3&gt;2.4: ALiBi (attention with linear bias), deterministic backward pass.&lt;/h3&gt; 
&lt;p&gt;Implement ALiBi (Press et al., 2021). Thanks to Sanghun Cho from Kakao Brain for this contribution.&lt;/p&gt; 
&lt;p&gt;Implement deterministic backward pass. Thanks to engineers from &lt;a href="https://raw.githubusercontent.com/Dao-AILab/flash-attention/main/www.meituan.com"&gt;Meituan&lt;/a&gt; for this contribution.&lt;/p&gt; 
&lt;h3&gt;2.5: Paged KV cache.&lt;/h3&gt; 
&lt;p&gt;Support paged KV cache (i.e., &lt;a href="https://arxiv.org/abs/2309.06180"&gt;PagedAttention&lt;/a&gt;). Thanks to @beginlner for this contribution.&lt;/p&gt; 
&lt;h3&gt;2.6: Softcapping.&lt;/h3&gt; 
&lt;p&gt;Support attention with softcapping, as used in Gemma-2 and Grok models. Thanks to @Narsil and @lucidrains for this contribution.&lt;/p&gt; 
&lt;h3&gt;2.7: Compatibility with torch compile&lt;/h3&gt; 
&lt;p&gt;Thanks to @ani300 for this contribution.&lt;/p&gt; 
&lt;h2&gt;Performance&lt;/h2&gt; 
&lt;p&gt;We present expected speedup (combined forward + backward pass) and memory savings from using FlashAttention against PyTorch standard attention, depending on sequence length, on different GPUs (speedup depends on memory bandwidth - we see more speedup on slower GPU memory).&lt;/p&gt; 
&lt;p&gt;We currently have benchmarks for these GPUs:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Dao-AILab/flash-attention/main/#a100"&gt;A100&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Dao-AILab/flash-attention/main/#h100"&gt;H100&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- * [RTX 3090](#rtx-3090) --&gt; 
&lt;!-- * [T4](#t4) --&gt; 
&lt;h3&gt;A100&lt;/h3&gt; 
&lt;p&gt;We display FlashAttention speedup using these parameters:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Head dimension 64 or 128, hidden dimension 2048 (i.e. either 32 or 16 heads).&lt;/li&gt; 
 &lt;li&gt;Sequence length 512, 1k, 2k, 4k, 8k, 16k.&lt;/li&gt; 
 &lt;li&gt;Batch size set to 16k / seqlen.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Speedup&lt;/h4&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Dao-AILab/flash-attention/main/assets/flash2_a100_fwd_bwd_benchmark.png" alt="FlashAttention speedup on A100 80GB SXM5 with FP16/BF16" /&gt;&lt;/p&gt; 
&lt;h4&gt;Memory&lt;/h4&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Dao-AILab/flash-attention/main/assets/flashattn_memory.jpg" alt="FlashAttention memory" /&gt;&lt;/p&gt; 
&lt;p&gt;We show memory savings in this graph (note that memory footprint is the same no matter if you use dropout or masking). Memory savings are proportional to sequence length -- since standard attention has memory quadratic in sequence length, whereas FlashAttention has memory linear in sequence length. We see 10X memory savings at sequence length 2K, and 20X at 4K. As a result, FlashAttention can scale to much longer sequence lengths.&lt;/p&gt; 
&lt;h3&gt;H100&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Dao-AILab/flash-attention/main/assets/flash2_h100_fwd_bwd_benchmark.png" alt="FlashAttention speedup on H100 SXM5 with FP16/BF16" /&gt;&lt;/p&gt; 
&lt;h2&gt;Full model code and training script&lt;/h2&gt; 
&lt;p&gt;We have released the full GPT model &lt;a href="https://github.com/Dao-AILab/flash-attention/raw/main/flash_attn/models/gpt.py"&gt;implementation&lt;/a&gt;. We also provide optimized implementations of other layers (e.g., MLP, LayerNorm, cross-entropy loss, rotary embedding). Overall this speeds up training by 3-5x compared to the baseline implementation from Huggingface, reaching up to 225 TFLOPs/sec per A100, equivalent to 72% model FLOPs utilization (we don't need any activation checkpointing).&lt;/p&gt; 
&lt;p&gt;We also include a training &lt;a href="https://github.com/Dao-AILab/flash-attention/tree/main/training"&gt;script&lt;/a&gt; to train GPT2 on Openwebtext and GPT3 on The Pile.&lt;/p&gt; 
&lt;h2&gt;Triton implementation of FlashAttention&lt;/h2&gt; 
&lt;p&gt;Phil Tillet (OpenAI) has an experimental implementation of FlashAttention in Triton: &lt;a href="https://github.com/openai/triton/raw/master/python/tutorials/06-fused-attention.py"&gt;https://github.com/openai/triton/blob/master/python/tutorials/06-fused-attention.py&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;As Triton is a higher-level language than CUDA, it might be easier to understand and experiment with. The notations in the Triton implementation are also closer to what's used in our paper.&lt;/p&gt; 
&lt;p&gt;We also have an experimental implementation in Triton that support attention bias (e.g. ALiBi): &lt;a href="https://github.com/Dao-AILab/flash-attention/raw/main/flash_attn/flash_attn_triton.py"&gt;https://github.com/Dao-AILab/flash-attention/blob/main/flash_attn/flash_attn_triton.py&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Tests&lt;/h2&gt; 
&lt;p&gt;We test that FlashAttention produces the same output and gradient as a reference implementation, up to some numerical tolerance. In particular, we check that the maximum numerical error of FlashAttention is at most twice the numerical error of a baseline implementation in Pytorch (for different head dimensions, input dtype, sequence length, causal / non-causal).&lt;/p&gt; 
&lt;p&gt;To run the tests:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pytest -q -s tests/test_flash_attn.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;When you encounter issues&lt;/h2&gt; 
&lt;p&gt;This new release of FlashAttention-2 has been tested on several GPT-style models, mostly on A100 GPUs.&lt;/p&gt; 
&lt;p&gt;If you encounter bugs, please open a GitHub Issue!&lt;/p&gt; 
&lt;h2&gt;Tests&lt;/h2&gt; 
&lt;p&gt;To run the tests:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pytest tests/test_flash_attn_ck.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you use this codebase, or otherwise found our work valuable, please cite:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@inproceedings{dao2022flashattention,
  title={Flash{A}ttention: Fast and Memory-Efficient Exact Attention with {IO}-Awareness},
  author={Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2022}
}
@inproceedings{dao2023flashattention2,
  title={Flash{A}ttention-2: Faster Attention with Better Parallelism and Work Partitioning},
  author={Dao, Tri},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2024}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>comfyanonymous/ComfyUI</title>
      <link>https://github.com/comfyanonymous/ComfyUI</link>
      <description>&lt;p&gt;The most powerful and modular diffusion model GUI, api and backend with a graph/nodes interface.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h1&gt;ComfyUI&lt;/h1&gt; 
 &lt;p&gt;&lt;strong&gt;The most powerful and modular visual AI engine and application.&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://www.comfy.org/"&gt;&lt;img src="https://img.shields.io/badge/ComfyOrg-4285F4?style=flat" alt="Website" /&gt;&lt;/a&gt; &lt;a href="https://www.comfy.org/discord"&gt;&lt;img src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fdiscord.com%2Fapi%2Finvites%2Fcomfyorg%3Fwith_counts%3Dtrue&amp;amp;query=%24.approximate_member_count&amp;amp;logo=discord&amp;amp;logoColor=white&amp;amp;label=Discord&amp;amp;color=green&amp;amp;suffix=%20total" alt="Dynamic JSON Badge" /&gt;&lt;/a&gt; &lt;a href="https://x.com/ComfyUI"&gt;&lt;img src="https://img.shields.io/twitter/follow/ComfyUI" alt="Twitter" /&gt;&lt;/a&gt; &lt;a href="https://app.element.io/#/room/%23comfyui_space%3Amatrix.org"&gt;&lt;img src="https://img.shields.io/badge/Matrix-000000?style=flat&amp;amp;logo=matrix&amp;amp;logoColor=white" alt="Matrix" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;a href="https://github.com/comfyanonymous/ComfyUI/releases"&gt;&lt;img src="https://img.shields.io/github/v/release/comfyanonymous/ComfyUI?style=flat&amp;amp;sort=semver" alt="" /&gt;&lt;/a&gt; &lt;a href="https://github.com/comfyanonymous/ComfyUI/releases"&gt;&lt;img src="https://img.shields.io/github/release-date/comfyanonymous/ComfyUI?style=flat" alt="" /&gt;&lt;/a&gt; &lt;a href="https://github.com/comfyanonymous/ComfyUI/releases"&gt;&lt;img src="https://img.shields.io/github/downloads/comfyanonymous/ComfyUI/total?style=flat" alt="" /&gt;&lt;/a&gt; &lt;a href="https://github.com/comfyanonymous/ComfyUI/releases"&gt;&lt;img src="https://img.shields.io/github/downloads/comfyanonymous/ComfyUI/latest/total?style=flat&amp;amp;label=downloads%40latest" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;!-- Workaround to display total user from https://github.com/badges/shields/issues/4500#issuecomment-2060079995 --&gt; 
 &lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/7ccaf2c1-9b72-41ae-9a89-5688c94b7abe" alt="ComfyUI Screenshot" /&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;ComfyUI lets you design and execute advanced stable diffusion pipelines using a graph/nodes/flowchart based interface. Available on Windows, Linux, and macOS.&lt;/p&gt; 
&lt;h2&gt;Get Started&lt;/h2&gt; 
&lt;h4&gt;&lt;a href="https://www.comfy.org/download"&gt;Desktop Application&lt;/a&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;The easiest way to get started.&lt;/li&gt; 
 &lt;li&gt;Available on Windows &amp;amp; macOS.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;&lt;a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/#installing"&gt;Windows Portable Package&lt;/a&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Get the latest commits and completely portable.&lt;/li&gt; 
 &lt;li&gt;Available on Windows.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;&lt;a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/#manual-install-windows-linux"&gt;Manual Install&lt;/a&gt;&lt;/h4&gt; 
&lt;p&gt;Supports all operating systems and GPU types (NVIDIA, AMD, Intel, Apple Silicon, Ascend).&lt;/p&gt; 
&lt;h2&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/"&gt;Examples&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;See what ComfyUI can do with the &lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/"&gt;example workflows&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Nodes/graph/flowchart interface to experiment and create complex Stable Diffusion workflows without needing to code anything.&lt;/li&gt; 
 &lt;li&gt;Image Models 
  &lt;ul&gt; 
   &lt;li&gt;SD1.x, SD2.x (&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/unclip/"&gt;unCLIP&lt;/a&gt;)&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/sdxl/"&gt;SDXL&lt;/a&gt;, &lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/sdturbo/"&gt;SDXL Turbo&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/stable_cascade/"&gt;Stable Cascade&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/sd3/"&gt;SD3 and SD3.5&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Pixart Alpha and Sigma&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/aura_flow/"&gt;AuraFlow&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/hunyuan_dit/"&gt;HunyuanDiT&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/flux/"&gt;Flux&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/lumina2/"&gt;Lumina Image 2.0&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/hidream/"&gt;HiDream&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/qwen_image/"&gt;Qwen Image&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Image Editing Models 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/omnigen/"&gt;Omnigen 2&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/flux/#flux-kontext-image-editing-model"&gt;Flux Kontext&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/hidream/#hidream-e11"&gt;HiDream E1.1&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/qwen_image/#edit-model"&gt;Qwen Image Edit&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Video Models 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/video/"&gt;Stable Video Diffusion&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/mochi/"&gt;Mochi&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/ltxv/"&gt;LTX-Video&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/hunyuan_video/"&gt;Hunyuan Video&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/wan/"&gt;Wan 2.1&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/wan22/"&gt;Wan 2.2&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Audio Models 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/audio/"&gt;Stable Audio&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/audio/"&gt;ACE Step&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;3D Models 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://docs.comfy.org/tutorials/3d/hunyuan3D-2"&gt;Hunyuan3D 2.0&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Asynchronous Queue system&lt;/li&gt; 
 &lt;li&gt;Many optimizations: Only re-executes the parts of the workflow that changes between executions.&lt;/li&gt; 
 &lt;li&gt;Smart memory management: can automatically run large models on GPUs with as low as 1GB vram with smart offloading.&lt;/li&gt; 
 &lt;li&gt;Works even if you don't have a GPU with: &lt;code&gt;--cpu&lt;/code&gt; (slow)&lt;/li&gt; 
 &lt;li&gt;Can load ckpt and safetensors: All in one checkpoints or standalone diffusion models, VAEs and CLIP models.&lt;/li&gt; 
 &lt;li&gt;Safe loading of ckpt, pt, pth, etc.. files.&lt;/li&gt; 
 &lt;li&gt;Embeddings/Textual inversion&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/lora/"&gt;Loras (regular, locon and loha)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/hypernetworks/"&gt;Hypernetworks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Loading full workflows (with seeds) from generated PNG, WebP and FLAC files.&lt;/li&gt; 
 &lt;li&gt;Saving/Loading workflows as Json files.&lt;/li&gt; 
 &lt;li&gt;Nodes interface can be used to create complex workflows like one for &lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/2_pass_txt2img/"&gt;Hires fix&lt;/a&gt; or much more advanced ones.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/area_composition/"&gt;Area Composition&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/inpaint/"&gt;Inpainting&lt;/a&gt; with both regular and inpainting models.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/controlnet/"&gt;ControlNet and T2I-Adapter&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/upscale_models/"&gt;Upscale Models (ESRGAN, ESRGAN variants, SwinIR, Swin2SR, etc...)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/gligen/"&gt;GLIGEN&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/model_merging/"&gt;Model Merging&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/lcm/"&gt;LCM models and Loras&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Latent previews with &lt;a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/#how-to-show-high-quality-previews"&gt;TAESD&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Works fully offline: core will never download anything unless you want to.&lt;/li&gt; 
 &lt;li&gt;Optional API nodes to use paid models from external providers through the online &lt;a href="https://docs.comfy.org/tutorials/api-nodes/overview"&gt;Comfy API&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/extra_model_paths.yaml.example"&gt;Config file&lt;/a&gt; to set the search paths for models.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Workflow examples can be found on the &lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/"&gt;Examples page&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Release Process&lt;/h2&gt; 
&lt;p&gt;ComfyUI follows a weekly release cycle targeting Friday but this regularly changes because of model releases or large changes to the codebase. There are three interconnected repositories:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/comfyanonymous/ComfyUI"&gt;ComfyUI Core&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Releases a new stable version (e.g., v0.7.0)&lt;/li&gt; 
   &lt;li&gt;Serves as the foundation for the desktop release&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/Comfy-Org/desktop"&gt;ComfyUI Desktop&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Builds a new release using the latest stable core version&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/Comfy-Org/ComfyUI_frontend"&gt;ComfyUI Frontend&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Weekly frontend updates are merged into the core repository&lt;/li&gt; 
   &lt;li&gt;Features are frozen for the upcoming core release&lt;/li&gt; 
   &lt;li&gt;Development continues for the next release cycle&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Shortcuts&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Keybind&lt;/th&gt; 
   &lt;th&gt;Explanation&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;Enter&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Queue up current graph for generation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;Shift&lt;/code&gt; + &lt;code&gt;Enter&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Queue up current graph as first for generation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;Alt&lt;/code&gt; + &lt;code&gt;Enter&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Cancel current generation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;Z&lt;/code&gt;/&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;Y&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Undo/Redo&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;S&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Save workflow&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;O&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Load workflow&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;A&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Select all nodes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Alt &lt;/code&gt;+ &lt;code&gt;C&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Collapse/uncollapse selected nodes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;M&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Mute/unmute selected nodes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;B&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Bypass selected nodes (acts like the node was removed from the graph and the wires reconnected through)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Delete&lt;/code&gt;/&lt;code&gt;Backspace&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Delete selected nodes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;Backspace&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Delete the current graph&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Space&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Move the canvas around when held and moving the cursor&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt;/&lt;code&gt;Shift&lt;/code&gt; + &lt;code&gt;Click&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Add clicked node to selection&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;C&lt;/code&gt;/&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;V&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Copy and paste selected nodes (without maintaining connections to outputs of unselected nodes)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;C&lt;/code&gt;/&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;Shift&lt;/code&gt; + &lt;code&gt;V&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Copy and paste selected nodes (maintaining connections from outputs of unselected nodes to inputs of pasted nodes)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Shift&lt;/code&gt; + &lt;code&gt;Drag&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Move multiple selected nodes at the same time&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;D&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Load default graph&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Alt&lt;/code&gt; + &lt;code&gt;+&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Canvas Zoom in&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Alt&lt;/code&gt; + &lt;code&gt;-&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Canvas Zoom out&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;Shift&lt;/code&gt; + LMB + Vertical drag&lt;/td&gt; 
   &lt;td&gt;Canvas Zoom in/out&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;P&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Pin/Unpin selected nodes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;G&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Group selected nodes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Q&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Toggle visibility of the queue&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;H&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Toggle visibility of history&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;R&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Refresh graph&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;F&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Show/Hide menu&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;.&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Fit view to selection (Whole graph when nothing is selected)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Double-Click LMB&lt;/td&gt; 
   &lt;td&gt;Open node quick search palette&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Shift&lt;/code&gt; + Drag&lt;/td&gt; 
   &lt;td&gt;Move multiple wires at once&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;Alt&lt;/code&gt; + LMB&lt;/td&gt; 
   &lt;td&gt;Disconnect all wires from clicked slot&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;code&gt;Ctrl&lt;/code&gt; can also be replaced with &lt;code&gt;Cmd&lt;/code&gt; instead for macOS users&lt;/p&gt; 
&lt;h1&gt;Installing&lt;/h1&gt; 
&lt;h2&gt;Windows Portable&lt;/h2&gt; 
&lt;p&gt;There is a portable standalone build for Windows that should work for running on Nvidia GPUs or for running on your CPU only on the &lt;a href="https://github.com/comfyanonymous/ComfyUI/releases"&gt;releases page&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;&lt;a href="https://github.com/comfyanonymous/ComfyUI/releases/latest/download/ComfyUI_windows_portable_nvidia.7z"&gt;Direct link to download&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Simply download, extract with &lt;a href="https://7-zip.org"&gt;7-Zip&lt;/a&gt; and run. Make sure you put your Stable Diffusion checkpoints/models (the huge ckpt/safetensors files) in: ComfyUI\models\checkpoints&lt;/p&gt; 
&lt;p&gt;If you have trouble extracting it, right click the file -&amp;gt; properties -&amp;gt; unblock&lt;/p&gt; 
&lt;h4&gt;How do I share models between another UI and ComfyUI?&lt;/h4&gt; 
&lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/extra_model_paths.yaml.example"&gt;Config file&lt;/a&gt; to set the search paths for models. In the standalone windows build you can find this file in the ComfyUI directory. Rename this file to extra_model_paths.yaml and edit it with your favorite text editor.&lt;/p&gt; 
&lt;h2&gt;&lt;a href="https://docs.comfy.org/comfy-cli/getting-started"&gt;comfy-cli&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;You can install and start ComfyUI using comfy-cli:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install comfy-cli
comfy install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Manual Install (Windows, Linux)&lt;/h2&gt; 
&lt;p&gt;Python 3.13 is very well supported. If you have trouble with some custom node dependencies you can try 3.12&lt;/p&gt; 
&lt;p&gt;Git clone this repo.&lt;/p&gt; 
&lt;p&gt;Put your SD checkpoints (the huge ckpt/safetensors files) in: models/checkpoints&lt;/p&gt; 
&lt;p&gt;Put your VAE in: models/vae&lt;/p&gt; 
&lt;h3&gt;AMD GPUs (Linux only)&lt;/h3&gt; 
&lt;p&gt;AMD users can install rocm and pytorch with pip if you don't have it already installed, this is the command to install the stable version:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.4&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;This is the command to install the nightly with ROCm 6.4 which might have some performance improvements:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm6.4&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Intel GPUs (Windows and Linux)&lt;/h3&gt; 
&lt;p&gt;(Option 1) Intel Arc GPU users can install native PyTorch with torch.xpu support using pip. More information can be found &lt;a href="https://pytorch.org/docs/main/notes/get_start_xpu.html"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;To install PyTorch xpu, use the following command:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;code&gt;pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/xpu&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;This is the command to install the Pytorch xpu nightly which might have some performance improvements:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/xpu&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;(Option 2) Alternatively, Intel GPUs supported by Intel Extension for PyTorch (IPEX) can leverage IPEX for improved performance.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;visit &lt;a href="https://intel.github.io/intel-extension-for-pytorch/index.html#installation?platform=gpu"&gt;Installation&lt;/a&gt; for more information.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;NVIDIA&lt;/h3&gt; 
&lt;p&gt;Nvidia users should install stable pytorch using this command:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu129&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;This is the command to install pytorch nightly instead which might have performance improvements.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu129&lt;/code&gt;&lt;/p&gt; 
&lt;h4&gt;Troubleshooting&lt;/h4&gt; 
&lt;p&gt;If you get the "Torch not compiled with CUDA enabled" error, uninstall torch with:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip uninstall torch&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;And install it again with the command above.&lt;/p&gt; 
&lt;h3&gt;Dependencies&lt;/h3&gt; 
&lt;p&gt;Install the dependencies by opening your terminal inside the ComfyUI folder and:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;After this you should have everything installed and can proceed to running ComfyUI.&lt;/p&gt; 
&lt;h3&gt;Others:&lt;/h3&gt; 
&lt;h4&gt;Apple Mac silicon&lt;/h4&gt; 
&lt;p&gt;You can install ComfyUI in Apple Mac silicon (M1 or M2) with any recent macOS version.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install pytorch nightly. For instructions, read the &lt;a href="https://developer.apple.com/metal/pytorch/"&gt;Accelerated PyTorch training on Mac&lt;/a&gt; Apple Developer guide (make sure to install the latest pytorch nightly).&lt;/li&gt; 
 &lt;li&gt;Follow the &lt;a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/#manual-install-windows-linux"&gt;ComfyUI manual installation&lt;/a&gt; instructions for Windows and Linux.&lt;/li&gt; 
 &lt;li&gt;Install the ComfyUI &lt;a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/#dependencies"&gt;dependencies&lt;/a&gt;. If you have another Stable Diffusion UI &lt;a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/#i-already-have-another-ui-for-stable-diffusion-installed-do-i-really-have-to-install-all-of-these-dependencies"&gt;you might be able to reuse the dependencies&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Launch ComfyUI by running &lt;code&gt;python main.py&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Remember to add your models, VAE, LoRAs etc. to the corresponding Comfy folders, as discussed in &lt;a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/#manual-install-windows-linux"&gt;ComfyUI manual installation&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;DirectML (AMD Cards on Windows)&lt;/h4&gt; 
&lt;p&gt;This is very badly supported and is not recommended. There are some unofficial builds of pytorch ROCm on windows that exist that will give you a much better experience than this. This readme will be updated once official pytorch ROCm builds for windows come out.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip install torch-directml&lt;/code&gt; Then you can launch ComfyUI with: &lt;code&gt;python main.py --directml&lt;/code&gt;&lt;/p&gt; 
&lt;h4&gt;Ascend NPUs&lt;/h4&gt; 
&lt;p&gt;For models compatible with Ascend Extension for PyTorch (torch_npu). To get started, ensure your environment meets the prerequisites outlined on the &lt;a href="https://ascend.github.io/docs/sources/ascend/quick_install.html"&gt;installation&lt;/a&gt; page. Here's a step-by-step guide tailored to your platform and installation method:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Begin by installing the recommended or newer kernel version for Linux as specified in the Installation page of torch-npu, if necessary.&lt;/li&gt; 
 &lt;li&gt;Proceed with the installation of Ascend Basekit, which includes the driver, firmware, and CANN, following the instructions provided for your specific platform.&lt;/li&gt; 
 &lt;li&gt;Next, install the necessary packages for torch-npu by adhering to the platform-specific instructions on the &lt;a href="https://ascend.github.io/docs/sources/pytorch/install.html#pytorch"&gt;Installation&lt;/a&gt; page.&lt;/li&gt; 
 &lt;li&gt;Finally, adhere to the &lt;a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/#manual-install-windows-linux"&gt;ComfyUI manual installation&lt;/a&gt; guide for Linux. Once all components are installed, you can run ComfyUI as described earlier.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Cambricon MLUs&lt;/h4&gt; 
&lt;p&gt;For models compatible with Cambricon Extension for PyTorch (torch_mlu). Here's a step-by-step guide tailored to your platform and installation method:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install the Cambricon CNToolkit by adhering to the platform-specific instructions on the &lt;a href="https://www.cambricon.com/docs/sdk_1.15.0/cntoolkit_3.7.2/cntoolkit_install_3.7.2/index.html"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Next, install the PyTorch(torch_mlu) following the instructions on the &lt;a href="https://www.cambricon.com/docs/sdk_1.15.0/cambricon_pytorch_1.17.0/user_guide_1.9/index.html"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Launch ComfyUI by running &lt;code&gt;python main.py&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Iluvatar Corex&lt;/h4&gt; 
&lt;p&gt;For models compatible with Iluvatar Extension for PyTorch. Here's a step-by-step guide tailored to your platform and installation method:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install the Iluvatar Corex Toolkit by adhering to the platform-specific instructions on the &lt;a href="https://support.iluvatar.com/#/DocumentCentre?id=1&amp;amp;nameCenter=2&amp;amp;productId=520117912052801536"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Launch ComfyUI by running &lt;code&gt;python main.py&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h1&gt;Running&lt;/h1&gt; 
&lt;p&gt;&lt;code&gt;python main.py&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;For AMD cards not officially supported by ROCm&lt;/h3&gt; 
&lt;p&gt;Try running it with this command if you have issues:&lt;/p&gt; 
&lt;p&gt;For 6700, 6600 and maybe other RDNA2 or older: &lt;code&gt;HSA_OVERRIDE_GFX_VERSION=10.3.0 python main.py&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;For AMD 7600 and maybe other RDNA3 cards: &lt;code&gt;HSA_OVERRIDE_GFX_VERSION=11.0.0 python main.py&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;AMD ROCm Tips&lt;/h3&gt; 
&lt;p&gt;You can enable experimental memory efficient attention on recent pytorch in ComfyUI on some AMD GPUs using this command, it should already be enabled by default on RDNA3. If this improves speed for you on latest pytorch on your GPU please report it so that I can enable it by default.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL=1 python main.py --use-pytorch-cross-attention&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;You can also try setting this env variable &lt;code&gt;PYTORCH_TUNABLEOP_ENABLED=1&lt;/code&gt; which might speed things up at the cost of a very slow initial run.&lt;/p&gt; 
&lt;h1&gt;Notes&lt;/h1&gt; 
&lt;p&gt;Only parts of the graph that have an output with all the correct inputs will be executed.&lt;/p&gt; 
&lt;p&gt;Only parts of the graph that change from each execution to the next will be executed, if you submit the same graph twice only the first will be executed. If you change the last part of the graph only the part you changed and the part that depends on it will be executed.&lt;/p&gt; 
&lt;p&gt;Dragging a generated png on the webpage or loading one will give you the full workflow including seeds that were used to create it.&lt;/p&gt; 
&lt;p&gt;You can use () to change emphasis of a word or phrase like: (good code:1.2) or (bad code:0.8). The default emphasis for () is 1.1. To use () characters in your actual prompt escape them like \( or \).&lt;/p&gt; 
&lt;p&gt;You can use {day|night}, for wildcard/dynamic prompts. With this syntax "{wild|card|test}" will be randomly replaced by either "wild", "card" or "test" by the frontend every time you queue the prompt. To use {} characters in your actual prompt escape them like: \{ or \}.&lt;/p&gt; 
&lt;p&gt;Dynamic prompts also support C-style comments, like &lt;code&gt;// comment&lt;/code&gt; or &lt;code&gt;/* comment */&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;To use a textual inversion concepts/embeddings in a text prompt put them in the models/embeddings directory and use them in the CLIPTextEncode node like this (you can omit the .pt extension):&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;embedding:embedding_filename.pt&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;How to show high-quality previews?&lt;/h2&gt; 
&lt;p&gt;Use &lt;code&gt;--preview-method auto&lt;/code&gt; to enable previews.&lt;/p&gt; 
&lt;p&gt;The default installation includes a fast latent preview method that's low-resolution. To enable higher-quality previews with &lt;a href="https://github.com/madebyollin/taesd"&gt;TAESD&lt;/a&gt;, download the &lt;a href="https://github.com/madebyollin/taesd/"&gt;taesd_decoder.pth, taesdxl_decoder.pth, taesd3_decoder.pth and taef1_decoder.pth&lt;/a&gt; and place them in the &lt;code&gt;models/vae_approx&lt;/code&gt; folder. Once they're installed, restart ComfyUI and launch it with &lt;code&gt;--preview-method taesd&lt;/code&gt; to enable high-quality previews.&lt;/p&gt; 
&lt;h2&gt;How to use TLS/SSL?&lt;/h2&gt; 
&lt;p&gt;Generate a self-signed certificate (not appropriate for shared/production use) and key by running the command: &lt;code&gt;openssl req -x509 -newkey rsa:4096 -keyout key.pem -out cert.pem -sha256 -days 3650 -nodes -subj "/C=XX/ST=StateName/L=CityName/O=CompanyName/OU=CompanySectionName/CN=CommonNameOrHostname"&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Use &lt;code&gt;--tls-keyfile key.pem --tls-certfile cert.pem&lt;/code&gt; to enable TLS/SSL, the app will now be accessible with &lt;code&gt;https://...&lt;/code&gt; instead of &lt;code&gt;http://...&lt;/code&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Note: Windows users can use &lt;a href="https://github.com/alexisrolland/docker-openssl"&gt;alexisrolland/docker-openssl&lt;/a&gt; or one of the &lt;a href="https://wiki.openssl.org/index.php/Binaries"&gt;3rd party binary distributions&lt;/a&gt; to run the command example above. &lt;br /&gt;&lt;br /&gt;If you use a container, note that the volume mount &lt;code&gt;-v&lt;/code&gt; can be a relative path so &lt;code&gt;... -v ".\:/openssl-certs" ...&lt;/code&gt; would create the key &amp;amp; cert files in the current directory of your command prompt or powershell terminal.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Support and dev channel&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://comfy.org/discord"&gt;Discord&lt;/a&gt;: Try the #help or #feedback channels.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://app.element.io/#/room/%23comfyui_space%3Amatrix.org"&gt;Matrix space: #comfyui_space:matrix.org&lt;/a&gt; (it's like discord but open source).&lt;/p&gt; 
&lt;p&gt;See also: &lt;a href="https://www.comfy.org/"&gt;https://www.comfy.org/&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Frontend Development&lt;/h2&gt; 
&lt;p&gt;As of August 15, 2024, we have transitioned to a new frontend, which is now hosted in a separate repository: &lt;a href="https://github.com/Comfy-Org/ComfyUI_frontend"&gt;ComfyUI Frontend&lt;/a&gt;. This repository now hosts the compiled JS (from TS/Vue) under the &lt;code&gt;web/&lt;/code&gt; directory.&lt;/p&gt; 
&lt;h3&gt;Reporting Issues and Requesting Features&lt;/h3&gt; 
&lt;p&gt;For any bugs, issues, or feature requests related to the frontend, please use the &lt;a href="https://github.com/Comfy-Org/ComfyUI_frontend"&gt;ComfyUI Frontend repository&lt;/a&gt;. This will help us manage and address frontend-specific concerns more efficiently.&lt;/p&gt; 
&lt;h3&gt;Using the Latest Frontend&lt;/h3&gt; 
&lt;p&gt;The new frontend is now the default for ComfyUI. However, please note:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;The frontend in the main ComfyUI repository is updated fortnightly.&lt;/li&gt; 
 &lt;li&gt;Daily releases are available in the separate frontend repository.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;To use the most up-to-date frontend version:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;For the latest daily release, launch ComfyUI with this command line argument:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;--front-end-version Comfy-Org/ComfyUI_frontend@latest
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;For a specific version, replace &lt;code&gt;latest&lt;/code&gt; with the desired version number:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;--front-end-version Comfy-Org/ComfyUI_frontend@1.2.2
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;This approach allows you to easily switch between the stable fortnightly release and the cutting-edge daily updates, or even specific versions for testing purposes.&lt;/p&gt; 
&lt;h3&gt;Accessing the Legacy Frontend&lt;/h3&gt; 
&lt;p&gt;If you need to use the legacy frontend for any reason, you can access it using the following command line argument:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;--front-end-version Comfy-Org/ComfyUI_legacy_frontend@latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will use a snapshot of the legacy frontend preserved in the &lt;a href="https://github.com/Comfy-Org/ComfyUI_legacy_frontend"&gt;ComfyUI Legacy Frontend repository&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;QA&lt;/h1&gt; 
&lt;h3&gt;Which GPU should I buy for this?&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/comfyanonymous/ComfyUI/wiki/Which-GPU-should-I-buy-for-ComfyUI"&gt;See this page for some recommendations&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>aliasrobotics/cai</title>
      <link>https://github.com/aliasrobotics/cai</link>
      <description>&lt;p&gt;Cybersecurity AI (CAI), the framework for AI Security&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Cybersecurity AI (&lt;code&gt;CAI&lt;/code&gt;)&lt;/h1&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;a align="center" href="" target="https://github.com/aliasrobotics/CAI"&gt; &lt;img width="100%" src="https://github.com/aliasrobotics/cai/raw/main/media/cai.png" /&gt; &lt;/a&gt; &lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://badge.fury.io/py/cai-framework"&gt;&lt;img src="https://badge.fury.io/py/cai-framework.svg?sanitize=true" alt="version" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/projects/cai-framework"&gt;&lt;img src="https://static.pepy.tech/badge/cai-framework" alt="downloads" /&gt;&lt;/a&gt; &lt;a href="https://github.com/aliasrobotics/cai"&gt;&lt;img src="https://img.shields.io/badge/Linux-Supported-brightgreen?logo=linux&amp;amp;logoColor=white" alt="Linux" /&gt;&lt;/a&gt; &lt;a href="https://github.com/aliasrobotics/cai"&gt;&lt;img src="https://img.shields.io/badge/OS%20X-Supported-brightgreen?logo=apple&amp;amp;logoColor=white" alt="OS X" /&gt;&lt;/a&gt; &lt;a href="https://github.com/aliasrobotics/cai"&gt;&lt;img src="https://img.shields.io/badge/Windows-Supported-brightgreen?logo=windows&amp;amp;logoColor=white" alt="Windows" /&gt;&lt;/a&gt; &lt;a href="https://github.com/aliasrobotics/cai"&gt;&lt;img src="https://img.shields.io/badge/Android-Supported-brightgreen?logo=android&amp;amp;logoColor=white" alt="Android" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/fnUFcTaQAC"&gt;&lt;img src="https://img.shields.io/badge/Discord-7289DA?logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/pdf/2504.06017"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2504.06017-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2506.23592"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2506.23592-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2508.13588"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2508.13588-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2508.21669"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2508.21669-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;Cybersecurity AI (CAI) is a lightweight, open-source framework that empowers security professionals to build and deploy AI-powered offensive and defensive automation. CAI is the &lt;em&gt;de facto&lt;/em&gt; framework for AI Security, already used by thousands of individual users and hundreds of organizations. Whether you're a security researcher, ethical hacker, IT professional, or organization looking to enhance your security posture, CAI provides the building blocks to create specialized AI agents that can assist with mitigation, vulnerability discovery, exploitation, and security assessment.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;🤖 &lt;strong&gt;300+ AI Models&lt;/strong&gt;: Support for OpenAI, Anthropic, DeepSeek, Ollama, and more&lt;/li&gt; 
 &lt;li&gt;🔧 &lt;strong&gt;Built-in Security Tools&lt;/strong&gt;: Ready-to-use tools for reconnaissance, exploitation, and privilege escalation&lt;/li&gt; 
 &lt;li&gt;🏆 &lt;strong&gt;Battle-tested&lt;/strong&gt;: Proven in HackTheBox CTFs, bug bounties, and real-world security &lt;a href="https://aliasrobotics.com/case-studies-robot-cybersecurity.php"&gt;case studies&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;🎯 &lt;strong&gt;Agent-based Architecture&lt;/strong&gt;: Modular framework design to build specialized agents for different security tasks&lt;/li&gt; 
 &lt;li&gt;🛡️ &lt;strong&gt;Guardrails Protection&lt;/strong&gt;: Built-in defenses against prompt injection and dangerous command execution&lt;/li&gt; 
 &lt;li&gt;📚 &lt;strong&gt;Research-oriented&lt;/strong&gt;: Research foundation to democratize cybersecurity AI for the community&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Read the technical report: &lt;a href="https://arxiv.org/pdf/2504.06017"&gt;CAI: An Open, Bug Bounty-Ready Cybersecurity AI&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;For further readings, refer to our &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-impact"&gt;impact&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#citation"&gt;CAI citation&lt;/a&gt; sections.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;a href="https://aliasrobotics.com/case-study-ecoforest.php"&gt;&lt;code&gt;OT&lt;/code&gt; - CAI and alias0 on: Ecoforest Heat Pumps&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://aliasrobotics.com/case-study-cai-mir.php"&gt;&lt;code&gt;Robotics&lt;/code&gt; - CAI and alias0 on: Mobile Industrial Robots (MiR)&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;CAI discovers critical vulnerability in Ecoforest heat pumps allowing unauthorized remote access and potential catastrophic failures. AI-powered security testing reveals exposed credentials and DES encryption weaknesses affecting all of their deployed units across Europe.&lt;/td&gt; 
   &lt;td&gt;CAI-powered security testing of MiR (Mobile Industrial Robot) platform through automated ROS message injection attacks. This study demonstrates how AI-driven vulnerability discovery can expose unauthorized access to robot control systems and alarm triggers.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://aliasrobotics.com/case-study-ecoforest.php"&gt;&lt;img src="https://aliasrobotics.com/img/case-study-portada-ecoforest.png" alt="" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://aliasrobotics.com/case-study-cai-mir.php"&gt;&lt;img src="https://aliasrobotics.com/img/case-study-portada-mir-cai.png" alt="" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;a href="https://aliasrobotics.com/case-study-mercado-libre.php"&gt;&lt;code&gt;IT&lt;/code&gt; (Web) - CAI and alias0 on: Mercado Libre's e-commerce&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://aliasrobotics.com/case-study-cai-mqtt-broker.php"&gt;&lt;code&gt;OT&lt;/code&gt; - CAI and alias0 on: MQTT broker&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;CAI-powered API vulnerability discovery at Mercado Libre through automated enumeration attacks. This study demonstrates how AI-driven security testing can expose user data exposure risks in e-commerce platforms at scale.&lt;/td&gt; 
   &lt;td&gt;CAI-powered testing exposed critical flaws in an MQTT broker within a Dockerized OT network. Without authentication, CAI subscribed to temperature and humidity topics and injected false values, corrupting data shown in Grafana dashboards.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://aliasrobotics.com/case-study-mercado-libre.php"&gt;&lt;img src="https://aliasrobotics.com/img/case-study-portada-mercado-libre.png" alt="" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://aliasrobotics.com/case-study-cai-mqtt-broker.php"&gt;&lt;img src="https://aliasrobotics.com/img/case-study-portada-mqtt-broker-cai.png" alt="" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING] &lt;span&gt;⚠&lt;/span&gt; CAI is in active development, so don't expect it to work flawlessly. Instead, contribute by raising an issue or &lt;a href="https://github.com/aliasrobotics/cai/pulls"&gt;sending a PR&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;Access to this library and the use of information, materials (or portions thereof), is &lt;strong&gt;&lt;u&gt;not intended&lt;/u&gt;, and is &lt;u&gt;prohibited&lt;/u&gt;, where such access or use violates applicable laws or regulations&lt;/strong&gt;. By no means the authors encourage or promote the unauthorized tampering with running systems. This can cause serious human harm and material damages.&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;By no means the authors of CAI encourage or promote the unauthorized tampering with compute systems. Please don't use the source code in here for cybercrime. &lt;u&gt;Pentest for good instead&lt;/u&gt;&lt;/em&gt;. By downloading, using, or modifying this source code, you agree to the terms of the &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/LICENSE"&gt;&lt;code&gt;LICENSE&lt;/code&gt;&lt;/a&gt; and the limitations outlined in the &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/DISCLAIMER"&gt;&lt;code&gt;DISCLAIMER&lt;/code&gt;&lt;/a&gt; file.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;&lt;span&gt;🔖&lt;/span&gt; Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#cybersecurity-ai-cai"&gt;Cybersecurity AI (&lt;code&gt;CAI&lt;/code&gt;)&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#bookmark-table-of-contents"&gt;&lt;span&gt;🔖&lt;/span&gt; Table of Contents&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-impact"&gt;🎯 Impact&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-competitions-and-challenges"&gt;🏆 Competitions and challenges&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-research-impact"&gt;📊 Research Impact&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-research-products-cybersecurity-ai"&gt;📚 Research products: &lt;code&gt;Cybersecurity AI&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#pocs"&gt;PoCs&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#motivation"&gt;Motivation&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#bust_in_silhouette-why-cai"&gt;&lt;span&gt;👤&lt;/span&gt; Why CAI?&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#ethical-principles-behind-cai"&gt;Ethical principles behind CAI&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#closed-source-alternatives"&gt;Closed-source alternatives&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#learn---cai-fluency"&gt;Learn - &lt;code&gt;CAI&lt;/code&gt; Fluency&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#nut_and_bolt-install"&gt;&lt;span&gt;🔩&lt;/span&gt; Install&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#os-x"&gt;OS X&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#ubuntu-2404"&gt;Ubuntu 24.04&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#ubuntu-2004"&gt;Ubuntu 20.04&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#windows-wsl"&gt;Windows WSL&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#android"&gt;Android&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#nut_and_bolt-setup-env-file"&gt;&lt;span&gt;🔩&lt;/span&gt; Setup &lt;code&gt;.env&lt;/code&gt; file&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-custom-openai-base-url-support"&gt;🔹 Custom OpenAI Base URL Support&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#triangular_ruler-architecture"&gt;&lt;span&gt;📐&lt;/span&gt; Architecture:&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-agent"&gt;🔹 Agent&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-tools"&gt;🔹 Tools&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-handoffs"&gt;🔹 Handoffs&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-patterns"&gt;🔹 Patterns&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-turns-and-interactions"&gt;🔹 Turns and Interactions&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-tracing"&gt;🔹 Tracing&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-guardrails"&gt;🔹 Guardrails&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-human-in-the-loop-hitl"&gt;🔹 Human-In-The-Loop (HITL)&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#rocket-quickstart"&gt;&lt;span&gt;🚀&lt;/span&gt; Quickstart&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#environment-variables"&gt;Environment Variables&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#openrouter-integration"&gt;OpenRouter Integration&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#mcp"&gt;MCP&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#development"&gt;Development&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#contributions"&gt;Contributions&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#optional-requirements-caiextensions"&gt;Optional Requirements: caiextensions&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#information_source-usage-data-collection"&gt;&lt;span&gt;ℹ&lt;/span&gt; Usage Data Collection&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#reproduce-ci-setup-locally"&gt;Reproduce CI-Setup locally&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#faq"&gt;FAQ&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#citation"&gt;Citation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#acknowledgements"&gt;Acknowledgements&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#academic-collaborations"&gt;Academic Collaborations&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🎯 Impact&lt;/h2&gt; 
&lt;h3&gt;🏆 Competitions and challenges&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://app.hackthebox.com/users/2268644"&gt;&lt;img src="https://img.shields.io/badge/HTB_ranking-top_90_Spain_(5_days)-red.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://app.hackthebox.com/users/2268644"&gt;&lt;img src="https://img.shields.io/badge/HTB_ranking-top_50_Spain_(6_days)-red.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://app.hackthebox.com/users/2268644"&gt;&lt;img src="https://img.shields.io/badge/HTB_ranking-top_30_Spain_(7_days)-red.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://app.hackthebox.com/users/2268644"&gt;&lt;img src="https://img.shields.io/badge/HTB_ranking-top_500_World_(7_days)-red.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://ctf.hackthebox.com/event/2000/scoreboard"&gt;&lt;img src="https://img.shields.io/badge/HTB_%22Human_vs_AI%22_CTF-top_1_(AIs)_world-red.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://ctf.hackthebox.com/event/2000/scoreboard"&gt;&lt;img src="https://img.shields.io/badge/HTB_%22Human_vs_AI%22_CTF-top_1_Spain-red.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://ctf.hackthebox.com/event/2000/scoreboard"&gt;&lt;img src="https://img.shields.io/badge/HTB_%22Human_vs_AI%22_CTF-top_20_World-red.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://ctf.hackthebox.com/event/2000/scoreboard"&gt;&lt;img src="https://img.shields.io/badge/HTB_%22Human_vs_AI%22_CTF-750_$-yellow.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://lu.ma/roboticshack?tk=RuryKF"&gt;&lt;img src="https://img.shields.io/badge/Mistral_AI_Robotics_Hackathon-2500_$-yellow.svg?sanitize=true" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;📊 Research Impact&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Pioneered LLM-powered AI Security with PentestGPT, establishing the foundation for the &lt;code&gt;Cybersecurity AI&lt;/code&gt; research domain &lt;a href="https://arxiv.org/pdf/2308.06782"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2308.06782-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Established the &lt;code&gt;Cybersecurity AI&lt;/code&gt; research line with &lt;strong&gt;4 peer-reviewed papers and technical reports&lt;/strong&gt; and active research collaborations &lt;a href="https://arxiv.org/pdf/2504.06017"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2504.06017-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2506.23592"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2506.23592-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2508.13588"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2508.13588-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2508.21669"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2508.21669-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Demonstrated &lt;strong&gt;3,600× performance improvement&lt;/strong&gt; over human penetration testers in standardized CTF benchmark evaluations &lt;a href="https://arxiv.org/pdf/2504.06017"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2504.06017-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Identified &lt;strong&gt;CVSS 4.3-7.5 severity vulnerabilities&lt;/strong&gt; in production systems through automated security assessment &lt;a href="https://arxiv.org/pdf/2504.06017"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2504.06017-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Democratization of AI-empowered vulnerability research&lt;/strong&gt;: CAI enables both non-security domain experts and experienced researchers to conduct more efficient vulnerability discovery, expanding the security research community while empowering small and medium enterprises to conduct autonomous security assessments &lt;a href="https://arxiv.org/pdf/2504.06017"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2504.06017-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Systematic evaluation of large language models&lt;/strong&gt; across both proprietary and open-weight architectures, revealing &lt;u&gt;substantial gaps&lt;/u&gt; between vendor-reported capabilities and empirical cybersecurity performance metrics &lt;a href="https://arxiv.org/pdf/2504.06017"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2504.06017-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Established the &lt;strong&gt;autonomy levels in cybersecurity&lt;/strong&gt; and argued about autonomy vs automation in the field &lt;a href="https://arxiv.org/abs/2506.23592"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2506.23592-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Collaborative research initiatives&lt;/strong&gt; with international academic institutions focused on developing cybersecurity education curricula and training methodologies &lt;a href="https://arxiv.org/abs/2508.13588"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2508.13588-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Contributed a comprehensive defense framework against prompt injection in AI security agents&lt;/strong&gt;: developed and empirically validated a multi-layered defense system that addresses the identified prompt injection issues &lt;a href="https://arxiv.org/abs/2508.21669"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2508.21669-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;📚 Research products: &lt;code&gt;Cybersecurity AI&lt;/code&gt;&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;CAI, An Open, Bug Bounty-Ready Cybersecurity AI&lt;/th&gt; 
   &lt;th&gt;The Dangerous Gap Between Automation and Autonomy&lt;/th&gt; 
   &lt;th&gt;CAI Fluency, A Framework for Cybersecurity AI Fluency&lt;/th&gt; 
   &lt;th&gt;Hacking the AI Hackers via Prompt Injection&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/2508.13588"&gt;&lt;img src="https://aliasrobotics.com/img/paper-cai.png" width="350" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.arxiv.org/pdf/2506.23592"&gt;&lt;img src="https://aliasrobotics.com/img/cai_automation_vs_autonomy.png" width="350" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/2504.06017"&gt;&lt;img src="https://aliasrobotics.com/img/cai_fluency_cover.png" width="350" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/2508.21669"&gt;&lt;img src="https://aliasrobotics.com/img/aihackers.jpeg" width="500" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;PoCs&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;CAI with &lt;code&gt;alias0&lt;/code&gt; on ROS message injection attacks in MiR-100 robot&lt;/th&gt; 
   &lt;th&gt;CAI with &lt;code&gt;alias0&lt;/code&gt; on API vulnerability discovery at Mercado Libre&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://asciinema.org/a/dNv705hZel2Rzrw0cju9HBGPh"&gt;&lt;img src="https://asciinema.org/a/dNv705hZel2Rzrw0cju9HBGPh.svg?sanitize=true" alt="asciicast" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://asciinema.org/a/9Hc9z1uFcdNjqP3bY5y7wO1Ww"&gt;&lt;img src="https://asciinema.org/a/9Hc9z1uFcdNjqP3bY5y7wO1Ww.svg?sanitize=true" alt="asciicast" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;CAI on JWT@PortSwigger CTF — Cybersecurity AI&lt;/th&gt; 
   &lt;th&gt;CAI on HackableII Boot2Root CTF — Cybersecurity AI&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://asciinema.org/a/713487"&gt;&lt;img src="https://asciinema.org/a/713487.svg?sanitize=true" alt="asciicast" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://asciinema.org/a/713485"&gt;&lt;img src="https://asciinema.org/a/713485.svg?sanitize=true" alt="asciicast" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;More case studies and PoCs are available at &lt;a href="https://aliasrobotics.com/case-studies-robot-cybersecurity.php"&gt;https://aliasrobotics.com/case-studies-robot-cybersecurity.php&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Motivation&lt;/h2&gt; 
&lt;h3&gt;&lt;span&gt;👤&lt;/span&gt; Why CAI?&lt;/h3&gt; 
&lt;p&gt;The cybersecurity landscape is undergoing a dramatic transformation as AI becomes increasingly integrated into security operations. &lt;strong&gt;We predict that by 2028, AI-powered security testing tools will outnumber human pentesters&lt;/strong&gt;. This shift represents a fundamental change in how we approach cybersecurity challenges. &lt;em&gt;AI is not just another tool - it's becoming essential for addressing complex security vulnerabilities and staying ahead of sophisticated threats. As organizations face more advanced cyber attacks, AI-enhanced security testing will be crucial for maintaining robust defenses.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;This work builds upon prior efforts[^4] and similarly, we believe that democratizing access to advanced cybersecurity AI tools is vital for the entire security community. That's why we're releasing Cybersecurity AI (&lt;code&gt;CAI&lt;/code&gt;) as an open source framework. Our goal is to empower security researchers, ethical hackers, and organizations to build and deploy powerful AI-driven security tools. By making these capabilities openly available, we aim to level the playing field and ensure that cutting-edge security AI technology isn't limited to well-funded private companies or state actors.&lt;/p&gt; 
&lt;p&gt;Bug Bounty programs have become a cornerstone of modern cybersecurity, providing a crucial mechanism for organizations to identify and fix vulnerabilities in their systems before they can be exploited. These programs have proven highly effective at securing both public and private infrastructure, with researchers discovering critical vulnerabilities that might have otherwise gone unnoticed. CAI is specifically designed to enhance these efforts by providing a lightweight, ergonomic framework for building specialized AI agents that can assist in various aspects of Bug Bounty hunting - from initial reconnaissance to vulnerability validation and reporting. Our framework aims to augment human expertise with AI capabilities, helping researchers work more efficiently and thoroughly in their quest to make digital systems more secure.&lt;/p&gt; 
&lt;h3&gt;Ethical principles behind CAI&lt;/h3&gt; 
&lt;p&gt;You might be wondering if releasing CAI &lt;em&gt;in-the-wild&lt;/em&gt; given its capabilities and security implications is ethical. Our decision to open-source this framework is guided by two core ethical principles:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Democratizing Cybersecurity AI&lt;/strong&gt;: We believe that advanced cybersecurity AI tools should be accessible to the entire security community, not just well-funded private companies or state actors. By releasing CAI as an open source framework, we aim to empower security researchers, ethical hackers, and organizations to build and deploy powerful AI-driven security tools, leveling the playing field in cybersecurity.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Transparency in AI Security Capabilities&lt;/strong&gt;: Based on our research results, understanding of the technology, and dissection of top technical reports, we argue that current LLM vendors are undermining their cybersecurity capabilities. This is extremely dangerous and misleading. By developing CAI openly, we provide a transparent benchmark of what AI systems can actually do in cybersecurity contexts, enabling more informed decisions about security postures.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;CAI is built on the following core principles:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Cybersecurity oriented AI framework&lt;/strong&gt;: CAI is specifically designed for cybersecurity use cases, aiming at semi- and fully-automating offensive and defensive security tasks.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Open source, free for research&lt;/strong&gt;: CAI is open source and free for research purposes. We aim at democratizing access to AI and Cybersecurity. For professional or commercial use, including on-premise deployments, dedicated technical support and custom extensions &lt;a href="mailto:research@aliasrobotics.com"&gt;reach out&lt;/a&gt; to obtain a license.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Lightweight&lt;/strong&gt;: CAI is designed to be fast, and easy to use.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Modular and agent-centric design&lt;/strong&gt;: CAI operates on the basis of agents and agentic patterns, which allows flexibility and scalability. You can easily add the most suitable agents and pattern for your cybersecuritytarget case.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Tool-integration&lt;/strong&gt;: CAI integrates already built-in tools, and allows the user to integrate their own tools with their own logic easily.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Logging and tracing integrated&lt;/strong&gt;: using &lt;a href="https://github.com/Arize-ai/phoenix"&gt;&lt;code&gt;phoenix&lt;/code&gt;&lt;/a&gt;, the open source tracing and logging tool for LLMs. This provides the user with a detailed traceability of the agents and their execution.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-Model Support&lt;/strong&gt;: more than 300 supported and empowered by &lt;a href="https://github.com/BerriAI/litellm"&gt;LiteLLM&lt;/a&gt;. The most popular providers: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Anthropic&lt;/strong&gt;: &lt;code&gt;Claude 3.7&lt;/code&gt;, &lt;code&gt;Claude 3.5&lt;/code&gt;, &lt;code&gt;Claude 3&lt;/code&gt;, &lt;code&gt;Claude 3 Opus&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;OpenAI&lt;/strong&gt;: &lt;code&gt;O1&lt;/code&gt;, &lt;code&gt;O1 Mini&lt;/code&gt;, &lt;code&gt;O3 Mini&lt;/code&gt;, &lt;code&gt;GPT-4o&lt;/code&gt;, &lt;code&gt;GPT-4.5 Preview&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;DeepSeek&lt;/strong&gt;: &lt;code&gt;DeepSeek V3&lt;/code&gt;, &lt;code&gt;DeepSeek R1&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Ollama&lt;/strong&gt;: &lt;code&gt;Qwen2.5 72B&lt;/code&gt;, &lt;code&gt;Qwen2.5 14B&lt;/code&gt;, etc&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Closed-source alternatives&lt;/h3&gt; 
&lt;p&gt;Cybersecurity AI is a critical field, yet many groups are misguidedly pursuing it through closed-source methods for pure economic return, leveraging similar techniques and building upon existing closed-source (&lt;em&gt;often third-party owned&lt;/em&gt;) models. This approach not only squanders valuable engineering resources but also represents an economic waste and results in redundant efforts, as they often end up reinventing the wheel. Here are some of the closed-source initiatives we keep track of and attempting to leverage genAI and agentic frameworks in cybersecurity AI:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.acyber.co/"&gt;Autonomous Cyber&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://cracken.ai/"&gt;CrackenAGI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ethiack.com/"&gt;ETHIACK&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://horizon3.ai/"&gt;Horizon3&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.kindo.ai/"&gt;Kindo&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://lakera.ai"&gt;Lakera&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/www.mindfort.ai"&gt;Mindfort&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mindgard.ai/"&gt;Mindgard&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ndaysecurity.com/"&gt;NDAY Security&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.runsybil.com"&gt;Runsybil&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.selfhack.fi"&gt;Selfhack&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://squr.ai/"&gt;SQUR&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://staris.tech/"&gt;Staris&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.sxipher.com/"&gt;Sxipher&lt;/a&gt; (seems discontinued)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.terra.security"&gt;Terra Security&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://xint.io/"&gt;Xint&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.xbow.com"&gt;XBOW&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.zeropath.com"&gt;ZeroPath&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.zynap.com"&gt;Zynap&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://7ai.com"&gt;7ai&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Learn - &lt;code&gt;CAI&lt;/code&gt; Fluency&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;a align="center" href="" target="https://github.com/aliasrobotics/CAI"&gt; &lt;img width="100%" src="https://github.com/aliasrobotics/cai/raw/main/media/caiedu.PNG" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;CAI Fluency technical report (&lt;a href="https://arxiv.org/pdf/2508.13588"&gt;arXiv:2508.13588&lt;/a&gt;) establishes formal educational frameworks for cybersecurity AI literacy.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;English&lt;/th&gt; 
   &lt;th&gt;Spanish&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Episode 0&lt;/strong&gt;: What is CAI?&lt;/td&gt; 
   &lt;td&gt;Cybersecurity AI (&lt;code&gt;CAI&lt;/code&gt;) explained&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=nBdTxbKM4oo"&gt;&lt;img src="https://img.youtube.com/vi/nBdTxbKM4oo/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=FaUL9HXrQ5k"&gt;&lt;img src="https://img.youtube.com/vi/FaUL9HXrQ5k/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Episode 1&lt;/strong&gt;: The &lt;code&gt;CAI&lt;/code&gt; Framework&lt;/td&gt; 
   &lt;td&gt;Vision &amp;amp; Ethics - Explore the core motivation behind CAI and delve into the crucial ethical principles guiding its development. Understand the motivation behind CAI and how you can actively contribute to the future of cybersecurity and the CAI framework.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=QEiGdsMf29M&amp;amp;list=PLLc16OUiZWd4RuFdN5_Wx9xwjCVVbopzr&amp;amp;index=3"&gt;&lt;img src="https://img.youtube.com/vi/QEiGdsMf29M/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Episode 2&lt;/strong&gt;: From Zero to Cyber Hero&lt;/td&gt; 
   &lt;td&gt;Breaking into Cybersecurity with AI - A comprehensive guide for complete beginners to become cybersecurity practitioners using CAI and AI tools. Learn how to leverage artificial intelligence to accelerate your cybersecurity learning journey, from understanding basic security concepts to performing real-world security assessments, all without requiring prior cybersecurity experience.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=hSTLHOOcQoY&amp;amp;list=PLLc16OUiZWd4RuFdN5_Wx9xwjCVVbopzr&amp;amp;index=14"&gt;&lt;img src="https://img.youtube.com/vi/hSTLHOOcQoY/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Episode 3&lt;/strong&gt;: Vibe-Hacking Tutorial&lt;/td&gt; 
   &lt;td&gt;"My first Hack" - A Vibe-Hacking guide for newbies. We demonstrate a simple web security hack using a default agent and show how to leverage tools and interpret CIA output with the help of the CAI Python API. You'll also learn to compare different LLM models to find the best fit for your hacking endeavors.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=9vZ_Iyex7uI&amp;amp;list=PLLc16OUiZWd4RuFdN5_Wx9xwjCVVbopzr&amp;amp;index=1"&gt;&lt;img src="https://img.youtube.com/vi/9vZ_Iyex7uI/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=iAOMaI1ftiA&amp;amp;list=PLLc16OUiZWd4RuFdN5_Wx9xwjCVVbopzr&amp;amp;index=2"&gt;&lt;img src="https://img.youtube.com/vi/iAOMaI1ftiA/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Episode 4&lt;/strong&gt;: Intro ReAct&lt;/td&gt; 
   &lt;td&gt;The Evolution of LLMs - Learn how LLMs evolved from basic language models to advanced multiagency AI systems. From basic LLMs to Chain-of-Thought and Reasoning LLMs towards ReAct and Multi-Agent Architectures. Get to know the basic terms&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=tLdFO1flj_o&amp;amp;list=PLLc16OUiZWd4RuFdN5_Wx9xwjCVVbopzr&amp;amp;index=13"&gt;&lt;img src="https://img.youtube.com/vi/tLdFO1flj_o/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Episode 5&lt;/strong&gt;: CAI on CTF challenges&lt;/td&gt; 
   &lt;td&gt;Dive into Capture The Flag (CTF) competitions using CAI. Learn how to leverage AI agents to solve various cybersecurity challenges including web exploitation, cryptography, reverse engineering, and forensics. Discover how to configure CAI for competitive hacking scenarios and maximize your CTF performance with intelligent automation.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=MrXTQ0e2to4&amp;amp;list=PLLc16OUiZWd4RuFdN5_Wx9xwjCVVbopzr&amp;amp;index=13"&gt;&lt;img src="https://img.youtube.com/vi/MrXTQ0e2to4/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=r9US_JZa9_c&amp;amp;list=PLLc16OUiZWd4RuFdN5_Wx9xwjCVVbopzr&amp;amp;index=12"&gt;&lt;img src="https://img.youtube.com/vi/r9US_JZa9_c/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Annex 1&lt;/strong&gt;: &lt;code&gt;CAI&lt;/code&gt; 0.5.x release&lt;/td&gt; 
   &lt;td&gt;Introduce version 0.5 of &lt;code&gt;CAI&lt;/code&gt; including new multi-agent functionality, new commands such as &lt;code&gt;/history&lt;/code&gt;, &lt;code&gt;/compact&lt;/code&gt;, &lt;code&gt;/graph&lt;/code&gt; or &lt;code&gt;/memory&lt;/code&gt; and a case study showing how &lt;code&gt;CAI&lt;/code&gt; found a critical security flaw in OT heap pumps spread around the world.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=OPFH0ANUMMw"&gt;&lt;img src="https://img.youtube.com/vi/OPFH0ANUMMw/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=Q8AI4E4gH8k"&gt;&lt;img src="https://img.youtube.com/vi/Q8AI4E4gH8k/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Annex 2&lt;/strong&gt;: &lt;code&gt;CAI&lt;/code&gt; 0.4.x release and &lt;code&gt;alias0&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Introducing version 0.4 of &lt;code&gt;CAI&lt;/code&gt; with &lt;em&gt;streaming&lt;/em&gt; and improved MCP support. We also introduce &lt;code&gt;alias0&lt;/code&gt;, the Privacy-First Cybersecurity AI, a Model-of-Models Intelligence that implements a Privacy-by-Design architecture and obtains state-of-the-art results in cybersecurity benchmarks.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=NZjzfnvAZcc"&gt;&lt;img src="https://img.youtube.com/vi/NZjzfnvAZcc/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Annex 3&lt;/strong&gt;: Cybersecurity AI Community Meeting #1&lt;/td&gt; 
   &lt;td&gt;First Cybersecurity AI (&lt;code&gt;CAI&lt;/code&gt;) community meeting, over 40 participants from academia, industry, and defense gathered to discuss the open-source scaffolding behind CAI — a project designed to build agentic AI systems for cybersecurity that are open, modular, and Bug Bounty-ready.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=4JqaTiVlgsw"&gt;&lt;img src="https://img.youtube.com/vi/4JqaTiVlgsw/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;&lt;span&gt;🔩&lt;/span&gt; Install&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install cai-framework
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Always create a new virtual environment to ensure proper dependency installation when updating CAI.&lt;/p&gt; 
&lt;p&gt;The following subsections provide a more detailed walkthrough on selected popular Operating Systems. Refer to the &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#development"&gt;Development&lt;/a&gt; section for developer-related install instructions.&lt;/p&gt; 
&lt;h3&gt;OS X&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;brew update &amp;amp;&amp;amp; \
    brew install git python@3.12

# Create virtual environment
python3.12 -m venv cai_env

# Install the package from the local directory
source cai_env/bin/activate &amp;amp;&amp;amp; pip install cai-framework

# Generate a .env file and set up with defaults
echo -e 'OPENAI_API_KEY="sk-1234"\nANTHROPIC_API_KEY=""\nOLLAMA=""\nPROMPT_TOOLKIT_NO_CPR=1\nCAI_STREAM=false' &amp;gt; .env

# Launch CAI
cai  # first launch it can take up to 30 seconds
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Ubuntu 24.04&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt-get update &amp;amp;&amp;amp; \
    sudo apt-get install -y git python3-pip python3.12-venv

# Create the virtual environment
python3.12 -m venv cai_env

# Install the package from the local directory
source cai_env/bin/activate &amp;amp;&amp;amp; pip install cai-framework

# Generate a .env file and set up with defaults
echo -e 'OPENAI_API_KEY="sk-1234"\nANTHROPIC_API_KEY=""\nOLLAMA=""\nPROMPT_TOOLKIT_NO_CPR=1\nCAI_STREAM=false' &amp;gt; .env

# Launch CAI
cai  # first launch it can take up to 30 seconds
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Ubuntu 20.04&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt-get update &amp;amp;&amp;amp; \
    sudo apt-get install -y software-properties-common

# Fetch Python 3.12
sudo add-apt-repository ppa:deadsnakes/ppa &amp;amp;&amp;amp; sudo apt update
sudo apt install python3.12 python3.12-venv python3.12-dev -y

# Create the virtual environment
python3.12 -m venv cai_env

# Install the package from the local directory
source cai_env/bin/activate &amp;amp;&amp;amp; pip install cai-framework

# Generate a .env file and set up with defaults
echo -e 'OPENAI_API_KEY="sk-1234"\nANTHROPIC_API_KEY=""\nOLLAMA=""\nPROMPT_TOOLKIT_NO_CPR=1\nCAI_STREAM=false' &amp;gt; .env

# Launch CAI
cai  # first launch it can take up to 30 seconds
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Windows WSL&lt;/h3&gt; 
&lt;p&gt;Go to the Microsoft page: &lt;a href="https://learn.microsoft.com/en-us/windows/wsl/install"&gt;https://learn.microsoft.com/en-us/windows/wsl/install&lt;/a&gt;. Here you will find all the instructions to install WSL&lt;/p&gt; 
&lt;p&gt;From Powershell write: wsl --install&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;
sudo apt-get update &amp;amp;&amp;amp; \
    sudo apt-get install -y git python3-pip python3-venv

# Create the virtual environment
python3 -m venv cai_env

# Install the package from the local directory
source cai_env/bin/activate &amp;amp;&amp;amp; pip install cai-framework

# Generate a .env file and set up with defaults
echo -e 'OPENAI_API_KEY="sk-1234"\nANTHROPIC_API_KEY=""\nOLLAMA=""\nPROMPT_TOOLKIT_NO_CPR=1\nCAI_STREAM=false' &amp;gt; .env

# Launch CAI
cai  # first launch it can take up to 30 seconds
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Android&lt;/h3&gt; 
&lt;p&gt;We recommend having at least 8 GB of RAM:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;First of all, install userland &lt;a href="https://play.google.com/store/apps/details?id=tech.ula&amp;amp;hl=es"&gt;https://play.google.com/store/apps/details?id=tech.ula&amp;amp;hl=es&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install Kali minimal in basic options (for free). [Or any other kali option if preferred]&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Update apt keys like in this example: &lt;a href="https://superuser.com/questions/1644520/apt-get-update-issue-in-kali"&gt;https://superuser.com/questions/1644520/apt-get-update-issue-in-kali&lt;/a&gt;, inside UserLand's Kali terminal execute&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Get new apt keys
wget http://http.kali.org/kali/pool/main/k/kali-archive-keyring/kali-archive-keyring_2024.1_all.deb

# Install new apt keys
sudo dpkg -i kali-archive-keyring_2024.1_all.deb &amp;amp;&amp;amp; rm kali-archive-keyring_2024.1_all.deb

# Update APT repository
sudo apt-get update

# CAI requieres python 3.12, lets install it (CAI for kali in Android)
sudo apt-get update &amp;amp;&amp;amp; sudo apt-get install -y git python3-pip build-essential zlib1g-dev libncurses5-dev libgdbm-dev libnss3-dev libssl-dev libreadline-dev libffi-dev libsqlite3-dev wget libbz2-dev pkg-config
wget https://www.python.org/ftp/python/3.12.4/Python-3.12.4.tar.xz
tar xf Python-3.12.4.tar.xz
cd ./configure --enable-optimizations
sudo make altinstall # This command takes long to execute

# Clone CAI's source code
git clone https://github.com/aliasrobotics/cai &amp;amp;&amp;amp; cd cai

# Create virtual environment
python3.12 -m venv cai_env

# Install the package from the local directory
source cai_env/bin/activate &amp;amp;&amp;amp; pip3 install -e .

# Generate a .env file and set up
cp .env.example .env  # edit here your keys/models

# Launch CAI
cai
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;&lt;span&gt;🔩&lt;/span&gt; Setup &lt;code&gt;.env&lt;/code&gt; file&lt;/h3&gt; 
&lt;p&gt;CAI leverages the &lt;code&gt;.env&lt;/code&gt; file to load configuration at launch. To facilitate the setup, the repo provides an exemplary &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/.env.example"&gt;&lt;code&gt;.env.example&lt;/code&gt;&lt;/a&gt; file provides a template for configuring CAI's setup and your LLM API keys to work with desired LLM models.&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;⚠&lt;/span&gt; Important:&lt;/p&gt; 
&lt;p&gt;CAI does NOT provide API keys for any model by default. Don't ask us to provide keys, use your own or host your own models.&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;⚠&lt;/span&gt; Note:&lt;/p&gt; 
&lt;p&gt;The OPENAI_API_KEY must not be left blank. It should contain either "sk-123" (as a placeholder) or your actual API key. See &lt;a href="https://github.com/aliasrobotics/cai/issues/27"&gt;https://github.com/aliasrobotics/cai/issues/27&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;⚠&lt;/span&gt; Note:&lt;/p&gt; 
&lt;p&gt;If you are using alias0 model, make sure that CAI is &amp;gt;0.4.0 version and here you have an .env example to be able to use it.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;OPENAI_API_KEY="sk-1234"
OLLAMA=""
ALIAS_API_KEY="&amp;lt;sk-your-key&amp;gt;"  # note, add yours
CAI_STEAM=False
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;🔹 Custom OpenAI Base URL Support&lt;/h3&gt; 
&lt;p&gt;CAI supports configuring a custom OpenAI API base URL via the &lt;code&gt;OPENAI_BASE_URL&lt;/code&gt; environment variable. This allows users to redirect API calls to a custom endpoint, such as a proxy or self-hosted OpenAI-compatible service.&lt;/p&gt; 
&lt;p&gt;Example &lt;code&gt;.env&lt;/code&gt; entry configuration:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;OLLAMA_API_BASE="https://custom-openai-proxy.com/v1"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or directly from the command line:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;OLLAMA_API_BASE="https://custom-openai-proxy.com/v1" cai
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;&lt;span&gt;📐&lt;/span&gt; Architecture:&lt;/h2&gt; 
&lt;p&gt;CAI focuses on making cybersecurity agent &lt;strong&gt;coordination&lt;/strong&gt; and &lt;strong&gt;execution&lt;/strong&gt; lightweight, highly controllable, and useful for humans. To do so it builds upon 8 pillars: &lt;code&gt;Agent&lt;/code&gt;s, &lt;code&gt;Tools&lt;/code&gt;, &lt;code&gt;Handoffs&lt;/code&gt;, &lt;code&gt;Patterns&lt;/code&gt;, &lt;code&gt;Turns&lt;/code&gt;, &lt;code&gt;Tracing&lt;/code&gt;, &lt;code&gt;Guardrails&lt;/code&gt; and &lt;code&gt;HITL&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;                  ┌───────────────┐           ┌───────────┐
                  │      HITL     │◀─────────▶│   Turns   │
                  └───────┬───────┘           └───────────┘
                          │
                          ▼
┌───────────┐       ┌───────────┐       ┌───────────┐      ┌───────────┐
│  Patterns │◀─────▶│  Handoffs │◀────▶ │   Agents  │◀────▶│    LLMs   │
└───────────┘       └─────┬─────┘       └─────┬─────┘      └───────────┘
                          │                   │
                          │                   ▼
┌────────────┐       ┌────┴──────┐       ┌───────────┐     ┌────────────┐
│ Extensions │◀─────▶│  Tracing  │       │   Tools   │◀───▶│ Guardrails │
└────────────┘       └───────────┘       └───────────┘     └────────────┘
                                              │
                          ┌─────────────┬─────┴────┬─────────────┐
                          ▼             ▼          ▼             ▼
                    ┌───────────┐┌───────────┐┌────────────┐┌───────────┐
                    │ LinuxCmd  ││ WebSearch ││    Code    ││ SSHTunnel │
                    └───────────┘└───────────┘└────────────┘└───────────┘
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you want to dive deeper into the code, check the following files as a start point for using CAI:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/raw/main/src/cai/__init__.py"&gt;&lt;strong&gt;init&lt;/strong&gt;.py&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/raw/main/src/cai/cli.py"&gt;cli.py&lt;/a&gt; - entrypoint for command line interface&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/raw/main/src/cai/util.py"&gt;util.py&lt;/a&gt; - utility functions&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/raw/main/src/cai/agents"&gt;agents&lt;/a&gt; - Agent implementations&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/raw/main/src/cai/internal"&gt;internal&lt;/a&gt; - CAI internal functions (endpoints, metrics, logging, etc.)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/raw/main/src/cai/prompts"&gt;prompts&lt;/a&gt; - Agent Prompt Database&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/raw/main/src/cai/repl"&gt;repl&lt;/a&gt; - CLI aesthetics and commands&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/raw/main/src/cai/sdk"&gt;sdk&lt;/a&gt; - CAI command sdk&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/tree/main/src/cai/tools"&gt;tools&lt;/a&gt; - agent tools&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;🔹 Agent&lt;/h3&gt; 
&lt;p&gt;At its core, CAI abstracts its cybersecurity behavior via &lt;code&gt;Agents&lt;/code&gt; and agentic &lt;code&gt;Patterns&lt;/code&gt;. An Agent in &lt;em&gt;an intelligent system that interacts with some environment&lt;/em&gt;. More technically, within CAI we embrace a robotics-centric definition wherein an agent is anything that can be viewed as a system perceiving its environment through sensors, reasoning about its goals and and acting accordingly upon that environment through actuators (&lt;em&gt;adapted&lt;/em&gt; from Russel &amp;amp; Norvig, AI: A Modern Approach). In cybersecurity, an &lt;code&gt;Agent&lt;/code&gt; interacts with systems and networks, using peripherals and network interfaces as sensors, reasons accordingly and then executes network actions as if actuators. Correspondingly, in CAI, &lt;code&gt;Agent&lt;/code&gt;s implement the &lt;code&gt;ReACT&lt;/code&gt; (Reasoning and Action) agent model[^3]. For more information, see the &lt;a href="https://github.com/aliasrobotics/cai/raw/main/examples/basic/hello_world.py"&gt;example here&lt;/a&gt; for the full execution code, and refer to this &lt;a href="https://github.com/aliasrobotics/cai/raw/main/fluency/my-first-hack/my_first_hack.ipynb"&gt;jupyter notebook&lt;/a&gt; for a tutorial on how to use it.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from cai.sdk.agents import Agent, Runner, OpenAIChatCompletionsModel

import os
from openai import AsyncOpenAI
from dotenv import load_dotenv
load_dotenv()

agent = Agent(
      name="Custom Agent",
      instructions="""You are a Cybersecurity expert Leader""",
      model=OpenAIChatCompletionsModel(
          model=os.getenv('CAI_MODEL', "openai/gpt-4o"),
          openai_client=AsyncOpenAI(),
          )
      )

message = "Tell me about recursion in programming."
result = await Runner.run(agent, message)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;🔹 Tools&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;Tools&lt;/code&gt; let cybersecurity agents take actions by providing interfaces to execute system commands, run security scans, analyze vulnerabilities, and interact with target systems and APIs - they are the core capabilities that enable CAI agents to perform security tasks effectively; in CAI, tools include built-in cybersecurity utilities (like LinuxCmd for command execution, WebSearch for OSINT gathering, Code for dynamic script execution, and SSHTunnel for secure remote access), function calling mechanisms that allow integration of any Python function as a security tool, and agent-as-tool functionality that enables specialized security agents (such as reconnaissance or exploit agents) to be used by other agents, creating powerful collaborative security workflows without requiring formal handoffs between agents. For more information, please refer to the &lt;a href="https://github.com/aliasrobotics/cai/raw/main/examples/basic/tools.py"&gt;example here&lt;/a&gt; for the complete configuration of custom functions.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from cai.sdk.agents import Agent, Runner, OpenAIChatCompletionsModel
from cai.tools.reconnaissance.exec_code import execute_code
from cai.tools.reconnaissance.generic_linux_command import generic_linux_command

import os
from openai import AsyncOpenAI
from dotenv import load_dotenv
load_dotenv()

agent = Agent(
      name="Custom Agent",
      instructions="""You are a Cybersecurity expert Leader""",
      tools= [
        generic_linux_command,
        execute_code
      ],
      model=OpenAIChatCompletionsModel(
          model=os.getenv('CAI_MODEL', "openai/gpt-4o"),
          openai_client=AsyncOpenAI(),
          )
      )

message = "Tell me about recursion in programming."
result = await Runner.run(agent, message)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You may find different &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/cai/tools"&gt;tools&lt;/a&gt;. They are grouped in 6 major categories inspired by the security kill chain [^2]:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Reconnaissance and weaponization - &lt;em&gt;reconnaissance&lt;/em&gt; (crypto, listing, etc)&lt;/li&gt; 
 &lt;li&gt;Exploitation - &lt;em&gt;exploitation&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;Privilege escalation - &lt;em&gt;escalation&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;Lateral movement - &lt;em&gt;lateral&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;Data exfiltration - &lt;em&gt;exfiltration&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;Command and control - &lt;em&gt;control&lt;/em&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;🔹 Handoffs&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;Handoffs&lt;/code&gt; allow an &lt;code&gt;Agent&lt;/code&gt; to delegate tasks to another agent, which is crucial in cybersecurity operations where specialized expertise is needed for different phases of an engagement. In our framework, &lt;code&gt;Handoffs&lt;/code&gt; are implemented as tools for the LLM, where a &lt;strong&gt;handoff/transfer function&lt;/strong&gt; like &lt;code&gt;transfer_to_flag_discriminator&lt;/code&gt; enables the &lt;code&gt;ctf_agent&lt;/code&gt; to pass control to the &lt;code&gt;flag_discriminator_agent&lt;/code&gt; once it believes it has found the flag. This creates a security validation chain where the first agent handles exploitation and flag discovery, while the second agent specializes in flag verification, ensuring proper segregation of duties and leveraging specialized capabilities of different models for distinct security tasks. For more information, please refer to the &lt;a href="https://github.com/aliasrobotics/cai/raw/main/examples/cai/agent_patterns/handoffs.py"&gt;example here&lt;/a&gt; for the full execution code.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from cai.sdk.agents import function_tool
from cai.tools.common import run_command
from cai.sdk.agents import Agent, OpenAIChatCompletionsModel, HandoffInputData, Runner, function_tool, handoff, trace
from cai.sdk.agents.extensions import handoff_filters

import os
from openai import AsyncOpenAI
from dotenv import load_dotenv
load_dotenv()

@function_tool
def execute_cli_command(command: str) -&amp;gt; str:
    return run_command(command)

flag_discriminator = Agent(
    name="Flag discriminator",
    description="Agent focused on extracting the flag from the output",
    instructions="You are an agent tailored to extract the flag from a given output.",
    model=OpenAIChatCompletionsModel(
        model=os.getenv('CAI_MODEL', "qwen2.5:14b"),
        openai_client=AsyncOpenAI(),
    ) 
)

ctf_agent = Agent(
    name="CTF agent",
    description="Agent focused on conquering security challenges",
    instructions="You are a Cybersecurity expert Leader facing a CTF",
    tools=[
        execute_cli_command,
    ],
    model=OpenAIChatCompletionsModel(
        model= os.getenv('CAI_MODEL', "qwen2.5:14b"),
        openai_client=AsyncOpenAI(),
    ), 
    handoffs = [flag_discriminator]
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;🔹 Patterns&lt;/h3&gt; 
&lt;p&gt;An agentic &lt;code&gt;Pattern&lt;/code&gt; is a &lt;em&gt;structured design paradigm&lt;/em&gt; in artificial intelligence systems where autonomous or semi-autonomous agents operate within a defined &lt;em&gt;interaction framework&lt;/em&gt; (the pattern) to achieve a goal. These &lt;code&gt;Patterns&lt;/code&gt; specify the organization, coordination, and communication methods among agents, guiding decision-making, task execution, and delegation.&lt;/p&gt; 
&lt;p&gt;An agentic pattern (&lt;code&gt;AP&lt;/code&gt;) can be formally defined as a tuple:&lt;/p&gt; 
&lt;p&gt;\[ AP = (A, H, D, C, E) \]&lt;/p&gt; 
&lt;p&gt;wherein:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;\(A\) (Agents):&lt;/strong&gt; A set of autonomous entities, \( A = \{a_1, a_2, ..., a_n\} \), each with defined roles, capabilities, and internal states.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;\(H\) (Handoffs):&lt;/strong&gt; A function \( H: A \times T \to A \) that governs how tasks \( T \) are transferred between agents based on predefined logic (e.g., rules, negotiation, bidding).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;\(D\) (Decision Mechanism):&lt;/strong&gt; A decision function \( D: S \to A \) where \( S \) represents system states, and \( D \) determines which agent takes action at any given time.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;\(C\) (Communication Protocol):&lt;/strong&gt; A messaging function \( C: A \times A \to M \), where \( M \) is a message space, defining how agents share information.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;\(E\) (Execution Model):&lt;/strong&gt; A function \( E: A \times I \to O \) where \( I \) is the input space and \( O \) is the output space, defining how agents perform tasks.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;When building &lt;code&gt;Patterns&lt;/code&gt;, we generall y classify them among one of the following categories, though others exist:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;strong&gt;Agentic&lt;/strong&gt; &lt;code&gt;Pattern&lt;/code&gt; &lt;strong&gt;categories&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Swarm&lt;/code&gt; (Decentralized)&lt;/td&gt; 
   &lt;td&gt;Agents share tasks and self-assign responsibilities without a central orchestrator. Handoffs occur dynamically. &lt;em&gt;An example of a peer-to-peer agentic pattern is the &lt;code&gt;CTF Agentic Pattern&lt;/code&gt;, which involves a team of agents working together to solve a CTF challenge with dynamic handoffs.&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Hierarchical&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;A top-level agent (e.g., "PlannerAgent") assigns tasks via structured handoffs to specialized sub-agents. Alternatively, the structure of the agents is harcoded into the agentic pattern with pre-defined handoffs.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Chain-of-Thought&lt;/code&gt; (Sequential Workflow)&lt;/td&gt; 
   &lt;td&gt;A structured pipeline where Agent A produces an output, hands it to Agent B for reuse or refinement, and so on. Handoffs follow a linear sequence. &lt;em&gt;An example of a chain-of-thought agentic pattern is the &lt;code&gt;ReasonerAgent&lt;/code&gt;, which involves a Reasoning-type LLM that provides context to the main agent to solve a CTF challenge with a linear sequence.&lt;/em&gt;[^1]&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Auction-Based&lt;/code&gt; (Competitive Allocation)&lt;/td&gt; 
   &lt;td&gt;Agents "bid" on tasks based on priority, capability, or cost. A decision agent evaluates bids and hands off tasks to the best-fit agent.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Recursive&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;A single agent continuously refines its own output, treating itself as both executor and evaluator, with handoffs (internal or external) to itself. &lt;em&gt;An example of a recursive agentic pattern is the &lt;code&gt;CodeAgent&lt;/code&gt; (when used as a recursive agent), which continuously refines its own output by executing code and updating its own instructions.&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;For more information and examples of common agentic patterns, see the &lt;a href="https://github.com/aliasrobotics/cai/raw/main/examples/agent_patterns/README.md"&gt;examples folder&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;🔹 Turns and Interactions&lt;/h3&gt; 
&lt;p&gt;During the agentic flow (conversation), we distinguish between &lt;strong&gt;interactions&lt;/strong&gt; and &lt;strong&gt;turns&lt;/strong&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Interactions&lt;/strong&gt; are sequential exchanges between one or multiple agents. Each agent executing its logic corresponds with one &lt;em&gt;interaction&lt;/em&gt;. Since an &lt;code&gt;Agent&lt;/code&gt; in CAI generally implements the &lt;code&gt;ReACT&lt;/code&gt; agent model[^3], each &lt;em&gt;interaction&lt;/em&gt; consists of 1) a reasoning step via an LLM inference and 2) act by calling zero-to-n &lt;code&gt;Tools&lt;/code&gt;. This is defined in&lt;code&gt;process_interaction()&lt;/code&gt; in &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/cai/core.py"&gt;core.py&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Turns&lt;/strong&gt;: A turn represents a cycle of one ore more &lt;strong&gt;interactions&lt;/strong&gt; which finishes when the &lt;code&gt;Agent&lt;/code&gt; (or &lt;code&gt;Pattern&lt;/code&gt;) executing returns &lt;code&gt;None&lt;/code&gt;, judging there're no further actions to undertake. This is defined in &lt;code&gt;run()&lt;/code&gt;, see &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/cai/core.py"&gt;core.py&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] CAI Agents are not related to Assistants in the Assistants API. They are named similarly for convenience, but are otherwise completely unrelated. CAI is entirely powered by the Chat Completions API and is hence stateless between calls.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;🔹 Tracing&lt;/h3&gt; 
&lt;p&gt;CAI implements AI observability by adopting the OpenTelemetry standard and to do so, it leverages &lt;a href="https://github.com/Arize-ai/phoenix"&gt;Phoenix&lt;/a&gt; which provides comprehensive tracing capabilities through OpenTelemetry-based instrumentation, allowing you to monitor and analyze your security operations in real-time. This integration enables detailed visibility into agent interactions, tool usage, and attack vectors throughout penetration testing workflows, making it easier to debug complex exploitation chains, track vulnerability discovery processes, and optimize agent performance for more effective security assessments.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/media/tracing.png" alt="" /&gt;&lt;/p&gt; 
&lt;h3&gt;🔹 Guardrails&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;Guardrails&lt;/code&gt; provide a critical security layer for CAI agents, protecting against prompt injection attacks and preventing execution of dangerous commands. These guardrails run in parallel to agents, validating both input and output to ensure safe operation. The framework includes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Input Guardrails&lt;/strong&gt;: Detect and block prompt injection attempts before they reach agents, using pattern matching, Unicode homograph detection, and AI-powered analysis&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Output Guardrails&lt;/strong&gt;: Validate agent outputs before execution, preventing dangerous commands like reverse shells, fork bombs, or data exfiltration&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-layered Defense&lt;/strong&gt;: Protection at input, processing, and execution stages with tool-level validation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Base64/Base32 Aware&lt;/strong&gt;: Automatically decodes and analyzes encoded payloads to detect hidden malicious commands&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Configurable&lt;/strong&gt;: Can be enabled/disabled via &lt;code&gt;CAI_GUARDRAILS&lt;/code&gt; environment variable&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For detailed implementation, see &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/docs/guardrails.md"&gt;docs/guardrails.md&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/docs/cai_prompt_injection.md"&gt;docs/cai_prompt_injection.md&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;🔹 Human-In-The-Loop (HITL)&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;                      ┌─────────────────────────────────┐
                      │                                 │
                      │      Cybersecurity AI (CAI)     │
                      │                                 │
                      │       ┌─────────────────┐       │
                      │       │  Autonomous AI  │       │
                      │       └────────┬────────┘       │
                      │                │                │
                      │                │                │
                      │       ┌────────▼─────────┐      │
                      │       │ HITL Interaction │      │
                      │       └────────┬─────────┘      │
                      │                │                │
                      └────────────────┼────────────────┘
                                       │
                                       │ Ctrl+C (cli.py)
                                       │
                           ┌───────────▼───────────┐
                           │   Human Operator(s)   │
                           │  Expertise | Judgment │
                           │    Teleoperation      │
                           └───────────────────────┘
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;CAI delivers a framework for building Cybersecurity AIs with a strong emphasis on &lt;em&gt;semi-autonomous&lt;/em&gt; operation, as the reality is that &lt;strong&gt;fully-autonomous&lt;/strong&gt; cybersecurity systems remain premature and face significant challenges when tackling complex tasks. While CAI explores autonomous capabilities, we recognize that effective security operations still require human teleoperation providing expertise, judgment, and oversight in the security process.&lt;/p&gt; 
&lt;p&gt;Accordingly, the Human-In-The-Loop (&lt;code&gt;HITL&lt;/code&gt;) module is a core design principle of CAI, acknowledging that human intervention and teleoperation are essential components of responsible security testing. Through the &lt;code&gt;cli.py&lt;/code&gt; interface, users can seamlessly interact with agents at any point during execution by simply pressing &lt;code&gt;Ctrl+C&lt;/code&gt;. This is implemented across &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/cai/core.py"&gt;core.py&lt;/a&gt; and also in the REPL abstractions &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/cai/repl"&gt;REPL&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;&lt;span&gt;🚀&lt;/span&gt; Quickstart&lt;/h2&gt; 
&lt;p&gt;To start CAI after installing it, just type &lt;code&gt;cai&lt;/code&gt; in the CLI:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;└─# cai

          CCCCCCCCCCCCC      ++++++++   ++++++++      IIIIIIIIII
       CCC::::::::::::C  ++++++++++       ++++++++++  I::::::::I
     CC:::::::::::::::C ++++++++++         ++++++++++ I::::::::I
    C:::::CCCCCCCC::::C +++++++++    ++     +++++++++ II::::::II
   C:::::C       CCCCCC +++++++     +++++     +++++++   I::::I
  C:::::C                +++++     +++++++     +++++    I::::I
  C:::::C                ++++                   ++++    I::::I
  C:::::C                 ++                     ++     I::::I
  C:::::C                  +   +++++++++++++++   +      I::::I
  C:::::C                    +++++++++++++++++++        I::::I
  C:::::C                     +++++++++++++++++         I::::I
   C:::::C       CCCCCC        +++++++++++++++          I::::I
    C:::::CCCCCCCC::::C         +++++++++++++         II::::::II
     CC:::::::::::::::C           +++++++++           I::::::::I
       CCC::::::::::::C             +++++             I::::::::I
          CCCCCCCCCCCCC               ++              IIIIIIIIII

                      Cybersecurity AI (CAI), vX.Y.Z
                          Bug bounty-ready AI

CAI&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;That should initialize CAI and provide a prompt to execute any security task you want to perform. The navigation bar at the bottom displays important system information. This information helps you understand your environment while working with CAI.&lt;/p&gt; 
&lt;p&gt;Here's a quick &lt;a href="https://asciinema.org/a/zm7wS5DA2o0S9pu1Tb44pnlvy"&gt;demo video&lt;/a&gt; to help you get started with CAI. We'll walk through the basic steps — from launching the tool to running your first AI-powered task in the terminal. Whether you're a beginner or just curious, this guide will show you how easy it is to begin using CAI.&lt;/p&gt; 
&lt;p&gt;From here on, type on &lt;code&gt;CAI&lt;/code&gt; and start your security exercise. Best way to learn is by example:&lt;/p&gt; 
&lt;h3&gt;Environment Variables&lt;/h3&gt; 
&lt;p&gt;For using private models, you are given a &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/.env.example"&gt;&lt;code&gt;.env.example&lt;/code&gt;&lt;/a&gt; file. Copy it and rename it as &lt;code&gt;.env&lt;/code&gt;. Fill in your corresponding API keys, and you are ready to use CAI.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;List of Environment Variables&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Variable&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CTF_NAME&lt;/td&gt; 
    &lt;td&gt;Name of the CTF challenge to run (e.g. "picoctf_static_flag")&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CTF_CHALLENGE&lt;/td&gt; 
    &lt;td&gt;Specific challenge name within the CTF to test&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CTF_SUBNET&lt;/td&gt; 
    &lt;td&gt;Network subnet for the CTF container&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CTF_IP&lt;/td&gt; 
    &lt;td&gt;IP address for the CTF container&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CTF_INSIDE&lt;/td&gt; 
    &lt;td&gt;Whether to conquer the CTF from within container&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_MODEL&lt;/td&gt; 
    &lt;td&gt;Model to use for agents&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_DEBUG&lt;/td&gt; 
    &lt;td&gt;Set debug output level (0: Only tool outputs, 1: Verbose debug output, 2: CLI debug output)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_BRIEF&lt;/td&gt; 
    &lt;td&gt;Enable/disable brief output mode&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_MAX_TURNS&lt;/td&gt; 
    &lt;td&gt;Maximum number of turns for agent interactions&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_TRACING&lt;/td&gt; 
    &lt;td&gt;Enable/disable OpenTelemetry tracing&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_AGENT_TYPE&lt;/td&gt; 
    &lt;td&gt;Specify the agents to use (boot2root, one_tool...)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_STATE&lt;/td&gt; 
    &lt;td&gt;Enable/disable stateful mode&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_MEMORY&lt;/td&gt; 
    &lt;td&gt;Enable/disable memory mode (episodic, semantic, all)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_MEMORY_ONLINE&lt;/td&gt; 
    &lt;td&gt;Enable/disable online memory mode&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_MEMORY_OFFLINE&lt;/td&gt; 
    &lt;td&gt;Enable/disable offline memory&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_ENV_CONTEXT&lt;/td&gt; 
    &lt;td&gt;Add dirs and current env to llm context&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_MEMORY_ONLINE_INTERVAL&lt;/td&gt; 
    &lt;td&gt;Number of turns between online memory updates&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_PRICE_LIMIT&lt;/td&gt; 
    &lt;td&gt;Price limit for the conversation in dollars&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_REPORT&lt;/td&gt; 
    &lt;td&gt;Enable/disable reporter mode (ctf, nis2, pentesting)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_SUPPORT_MODEL&lt;/td&gt; 
    &lt;td&gt;Model to use for the support agent&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_SUPPORT_INTERVAL&lt;/td&gt; 
    &lt;td&gt;Number of turns between support agent executions&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_WORKSPACE&lt;/td&gt; 
    &lt;td&gt;Defines the name of the workspace&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_WORKSPACE_DIR&lt;/td&gt; 
    &lt;td&gt;Specifies the directory path where the workspace is located&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_GUARDRAILS&lt;/td&gt; 
    &lt;td&gt;Enable/disable guardrails for prompt injection protection (default: true)&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h3&gt;OpenRouter Integration&lt;/h3&gt; 
&lt;p&gt;The Cybersecurity AI (CAI) platform offers seamless integration with OpenRouter, a unified interface for Large Language Models (LLMs). This integration is crucial for users who wish to leverage advanced AI capabilities in their cybersecurity tasks. OpenRouter acts as a bridge, allowing CAI to communicate with various LLMs, thereby enhancing the flexibility and power of the AI agents used within CAI.&lt;/p&gt; 
&lt;p&gt;To enable OpenRouter support in CAI, you need to configure your environment by adding specific entries to your &lt;code&gt;.env&lt;/code&gt; file. This setup ensures that CAI can interact with the OpenRouter API, facilitating the use of sophisticated models like Meta-LLaMA. Here’s how you can configure it:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CAI_AGENT_TYPE=redteam_agent
CAI_MODEL=openrouter/meta-llama/llama-4-maverick
OPENROUTER_API_KEY=&amp;lt;sk-your-key&amp;gt;  # note, add yours
OPENROUTER_API_BASE=https://openrouter.ai/api/v1
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;MCP&lt;/h3&gt; 
&lt;p&gt;CAI supports the Model Context Protocol (MCP) for integrating external tools and services with AI agents. MCP is supported via two transport mechanisms:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;SSE (Server-Sent Events)&lt;/strong&gt; - For web-based servers that push updates over HTTP connections:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CAI&amp;gt;/mcp load http://localhost:9876/sse burp
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;&lt;strong&gt;STDIO (Standard Input/Output)&lt;/strong&gt; - For local inter-process communication:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CAI&amp;gt;/mcp load stdio myserver python mcp_server.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Once connected, you can add the MCP tools to any agent:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CAI&amp;gt;/mcp add burp redteam_agent
Adding tools from MCP server 'burp' to agent 'Red Team Agent'...
                                 Adding tools to Red Team Agent
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Tool                              ┃ Status ┃ Details                                         ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ send_http_request                 │ Added  │ Available as: send_http_request                 │
│ create_repeater_tab               │ Added  │ Available as: create_repeater_tab               │
│ send_to_intruder                  │ Added  │ Available as: send_to_intruder                  │
│ url_encode                        │ Added  │ Available as: url_encode                        │
│ url_decode                        │ Added  │ Available as: url_decode                        │
│ base64encode                      │ Added  │ Available as: base64encode                      │
│ base64decode                      │ Added  │ Available as: base64decode                      │
│ generate_random_string            │ Added  │ Available as: generate_random_string            │
│ output_project_options            │ Added  │ Available as: output_project_options            │
│ output_user_options               │ Added  │ Available as: output_user_options               │
│ set_project_options               │ Added  │ Available as: set_project_options               │
│ set_user_options                  │ Added  │ Available as: set_user_options                  │
│ get_proxy_http_history            │ Added  │ Available as: get_proxy_http_history            │
│ get_proxy_http_history_regex      │ Added  │ Available as: get_proxy_http_history_regex      │
│ get_proxy_websocket_history       │ Added  │ Available as: get_proxy_websocket_history       │
│ get_proxy_websocket_history_regex │ Added  │ Available as: get_proxy_websocket_history_regex │
│ set_task_execution_engine_state   │ Added  │ Available as: set_task_execution_engine_state   │
│ set_proxy_intercept_state         │ Added  │ Available as: set_proxy_intercept_state         │
│ get_active_editor_contents        │ Added  │ Available as: get_active_editor_contents        │
│ set_active_editor_contents        │ Added  │ Available as: set_active_editor_contents        │
└───────────────────────────────────┴────────┴─────────────────────────────────────────────────┘
Added 20 tools from server 'burp' to agent 'Red Team Agent'.
CAI&amp;gt;/agent 13
CAI&amp;gt;Create a repeater tab
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can list all active MCP connections and their transport types:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CAI&amp;gt;/mcp list
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/386a1fd3-3469-4f84-9396-2a5236febe1f"&gt;https://github.com/user-attachments/assets/386a1fd3-3469-4f84-9396-2a5236febe1f&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Development&lt;/h2&gt; 
&lt;p&gt;Development is facilitated via VS Code dev. environments. To try out our development environment, clone the repository, open VS Code and enter de dev. container mode:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/media/cai_devenv.gif" alt="CAI Development Environment" /&gt;&lt;/p&gt; 
&lt;h3&gt;Contributions&lt;/h3&gt; 
&lt;p&gt;If you want to contribute to this project, use &lt;a href="https://pre-commit.com/"&gt;&lt;strong&gt;Pre-commit&lt;/strong&gt;&lt;/a&gt; before your MR&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install pre-commit
pre-commit # files staged
pre-commit run --all-files # all files
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Optional Requirements: caiextensions&lt;/h3&gt; 
&lt;p&gt;Currently, the extensions are not publicly available as the engineering endeavour to maintain them is significant. Instead, we're making selected custom caiextensions available for partner companies across collaborations.&lt;/p&gt; 
&lt;h3&gt;&lt;span&gt;ℹ&lt;/span&gt; Usage Data Collection&lt;/h3&gt; 
&lt;p&gt;CAI is provided free of charge for researchers. To improve CAI’s detection accuracy and publish open security research, instead of payment for research use cases, we ask you to contribute to the CAI community by allowing usage data collection. This data helps us identify areas for improvement, understand how the framework is being used, and prioritize new features. Legal basis of data collection is under Art. 6 (1)(f) GDPR — CAI’s legitimate interest in maintaining and improving security tooling, with Art. 89 safeguards for research. The collected data includes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Basic system information (OS type, Python version)&lt;/li&gt; 
 &lt;li&gt;Username and IP information&lt;/li&gt; 
 &lt;li&gt;Tool usage patterns and performance metrics&lt;/li&gt; 
 &lt;li&gt;Model interactions and token usage statistics&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We take your privacy seriously and only collect what's needed to make CAI better. For further info, reach out to research＠aliasrobotics.com. You can disable some of the data collection features via the &lt;code&gt;CAI_TELEMETRY&lt;/code&gt; environment variable but we encourage you to keep it enabled and contribute back to research:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CAI_TELEMETRY=False cai
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Reproduce CI-Setup locally&lt;/h3&gt; 
&lt;p&gt;To simulate the CI/CD pipeline, you can run the following in the Gitlab runner machines:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run --rm -it \
  --privileged \
  --network=exploitflow_net \
  --add-host="host.docker.internal:host-gateway" \
  -v /cache:/cache \
  -v /var/run/docker.sock:/var/run/docker.sock:rw \
  registry.gitlab.com/aliasrobotics/alias_research/cai:latest bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;FAQ&lt;/h2&gt; 
&lt;details&gt;
 &lt;summary&gt;OLLAMA is giving me 404 errors&lt;/summary&gt; 
 &lt;p&gt;Ollama's API in OpenAI mode uses &lt;code&gt;/v1/chat/completions&lt;/code&gt; whereas the &lt;code&gt;openai&lt;/code&gt; library uses &lt;code&gt;base_url&lt;/code&gt; + &lt;code&gt;/chat/completions&lt;/code&gt;.&lt;/p&gt; 
 &lt;p&gt;We adopt the latter for overall alignment with the gen AI community and empower the former by allowing users to add the &lt;code&gt;v1&lt;/code&gt; themselves via:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;OLLAMA_API_BASE=http://IP:PORT/v1
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;See the following issues that treat this topic in more detail:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/issues/76"&gt;https://github.com/aliasrobotics/cai/issues/76&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/issues/83"&gt;https://github.com/aliasrobotics/cai/issues/83&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/issues/82"&gt;https://github.com/aliasrobotics/cai/issues/82&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Where are all the caiextensions?&lt;/summary&gt; 
 &lt;p&gt;See &lt;a href="https://gitlab.com/aliasrobotics/alias_research/caiextensions"&gt;all caiextensions&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;How do I install the report caiextension?&lt;/summary&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#optional-requirements-caiextensions"&gt;See here&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;How do I set up SSH access for Gitlab?&lt;/summary&gt; 
 &lt;p&gt;Generate a new SSH key&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;ssh-keygen -t ed25519
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Add the key to the SSH agent&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;ssh-add ~/.ssh/id_ed25519
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Add the public key to Gitlab Copy the key and add it to Gitlab under &lt;a href="https://gitlab.com/-/user_settings/ssh_keys"&gt;https://gitlab.com/-/user_settings/ssh_keys&lt;/a&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;cat ~/.ssh/id_ed25519.pub
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;To verify it:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;ssh -T git@gitlab.com
Welcome to GitLab, @vmayoral!
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;How do I clear Python cache?&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;find . -name "*.pyc" -delete &amp;amp;&amp;amp; find . -name "__pycache__" -delete
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;If host networking is not working with ollama check whether it has been disabled in Docker because you are not signed in&lt;/summary&gt; 
 &lt;p&gt;Docker in OS X behaves funny sometimes. Check if the following message has shown up:&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Host networking has been disabled because you are not signed in. Please sign in to enable it&lt;/em&gt;.&lt;/p&gt; 
 &lt;p&gt;Make sure this has been addressed and also that the Dev Container is not forwarding the 8000 port (click on x, if necessary in the ports section).&lt;/p&gt; 
 &lt;p&gt;To verify connection, from within the VSCode devcontainer:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;curl -v http://host.docker.internal:8000/api/version
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Run CAI against any target&lt;/summary&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/imgs/readme_imgs/cai-004-first-message.png" alt="cai-004-first-message" /&gt;&lt;/p&gt; 
 &lt;p&gt;The starting user prompt in this case is: &lt;code&gt;Target IP: 192.168.3.10, perform a full network scan&lt;/code&gt;.&lt;/p&gt; 
 &lt;p&gt;The agent started performing a nmap scan. You could either interact with the agent and give it more instructions, or let it run to see what it explores next.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;How do I interact with the agent? Type twice CTRL + C &lt;/summary&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/imgs/readme_imgs/cai-005-ctrl-c.png" alt="cai-005-ctrl-c" /&gt;&lt;/p&gt; 
 &lt;p&gt;If you want to use the HITL mode, you can do it by presssing twice &lt;code&gt;Ctrl + C&lt;/code&gt;. This will allow you to interact (prompt) with the agent whenever you want. The agent will not lose the previous context, as it is stored in the &lt;code&gt;history&lt;/code&gt; variable, which is passed to it and any agent that is called. This enables any agent to use the previous information and be more accurate and efficient.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; Can I change the model while CAI is running? /model &lt;/summary&gt; 
 &lt;p&gt;Use &lt;code&gt;/model&lt;/code&gt; to change the model.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/imgs/readme_imgs/cai-007-model-change.png" alt="cai-007-model-change" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;How can I list all the agents available? /agent &lt;/summary&gt; 
 &lt;p&gt;Use &lt;code&gt;/agent&lt;/code&gt; to list all the agents available.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/imgs/readme_imgs/cai-010-agents-menu.png" alt="cai-010-agents-menu" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; Where can I list all the environment variables? /config &lt;/summary&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/imgs/readme_imgs/cai-008-config.png" alt="cai-008-config" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; How to know more about the CLI? /help &lt;/summary&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/imgs/readme_imgs/cai-006-help.png" alt="cai-006-help" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;How can I trace the whole execution?&lt;/summary&gt; The environment variable `CAI_TRACING` allows the user to set it to `CAI_TRACING=true` to enable tracing, or `CAI_TRACING=false` to disable it. When CAI is prompted by the first time, the user is provided with two paths, the execution log, and the tracing log. 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/imgs/readme_imgs/cai-009-logs.png" alt="cai-009-logs" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Can I expand CAI capabilities using previous run logs?&lt;/summary&gt; 
 &lt;p&gt;Absolutely! The &lt;strong&gt;memory extension&lt;/strong&gt; allows you to use a previously sucessful runs ( the log object is stored as a &lt;strong&gt;.jsonl file in the &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/cai/logs"&gt;log&lt;/a&gt; folder&lt;/strong&gt; ) in a new run against the same target. The user is also given the path highlighted in orange as shown below.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/imgs/readme_imgs/cai-009-logs.png" alt="cai-009-logs" /&gt;&lt;/p&gt; 
 &lt;p&gt;How to make use of this functionality?&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Run CAI against the target. Let's assume the target name is: &lt;code&gt;target001&lt;/code&gt;.&lt;/li&gt; 
  &lt;li&gt;Get the log file path, something like: &lt;code&gt;logs/cai_20250408_111856.jsonl&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;Generate the memory using any model of your preference: &lt;code&gt;shell JSONL_FILE_PATH="logs/cai_20250408_111856.jsonl" CTF_INSIDE="false" CAI_MEMORY_COLLECTION="target001" CAI_MEMORY="episodic" CAI_MODEL="claude-3-5-sonnet-20241022" python3 tools/2_jsonl_to_memory.py &lt;/code&gt;&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;The script &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/cai/tools/2_jsonl_to_memory.py"&gt;&lt;code&gt;tools/2_jsonl_to_memory.py&lt;/code&gt;&lt;/a&gt; will generate a memory collection file with the most relevant steps. The quality of the memory collection will depend on the model you use.&lt;/p&gt; 
 &lt;ol start="4"&gt; 
  &lt;li&gt;Use the generated memory collection and execute a new run: &lt;code&gt;shell CAI_MEMORY="episodic" CAI_MODEL="gpt-4o" CAI_MEMORY_COLLECTION="target001" CAI_TRACING=false python3 cai/cli.py&lt;/code&gt;&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Can I expand CAI capabilities using scripts or extra information?&lt;/summary&gt; 
 &lt;p&gt;Currently, CAI supports text based information. You can add any extra information on the target you are facing by copy-pasting it directly into the system or user prompt.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;How?&lt;/strong&gt; By adding it to the system (&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/cai/repl/templates/system_master_template.md"&gt;&lt;code&gt;system_master_template.md&lt;/code&gt;&lt;/a&gt;) or the user prompt (&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/cai/repl/templates/user_master_template.md"&gt;&lt;code&gt;user_master_template.md&lt;/code&gt;&lt;/a&gt;). You can always directly prompt the path to the model, and it will &lt;code&gt;cat&lt;/code&gt; it.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;How CAI licence works?&lt;/summary&gt; 
 &lt;p&gt;CAI’s current license does not restrict usage for research purposes. You are free to use CAI for security assessments (pentests), to develop additional features, and to integrate it into your research activities, as long as you comply with local laws.&lt;/p&gt; 
 &lt;p&gt;If you or your organization start benefiting commercially from CAI (e.g., offering pentesting services powered by CAI), then a commercial license will be required to help sustain the project.&lt;/p&gt; 
 &lt;p&gt;CAI itself is not a profit-seeking initiative. Our goal is to build a sustainable open-source project. We simply ask that those who profit from CAI contribute back and support our ongoing development.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you want to cite our work, please use the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{mayoralvilches2025caiopenbugbountyready,
      title={CAI: An Open, Bug Bounty-Ready Cybersecurity AI},
      author={Víctor Mayoral-Vilches and Luis Javier Navarrete-Lozano and María Sanz-Gómez and Lidia Salas Espejo and Martiño Crespo-Álvarez and Francisco Oca-Gonzalez and Francesco Balassone and Alfonso Glera-Picón and Unai Ayucar-Carbajo and Jon Ander Ruiz-Alcalde and Stefan Rass and Martin Pinzger and Endika Gil-Uriarte},
      year={2025},
      eprint={2504.06017},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2504.06017},
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{mayoralvilches2025cybersecurityaidangerousgap,
      title={Cybersecurity AI: The Dangerous Gap Between Automation and Autonomy}, 
      author={Víctor Mayoral-Vilches},
      year={2025},
      eprint={2506.23592},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2506.23592}, 
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{mayoralvilches2025caifluencyframeworkcybersecurity,
      title={CAI Fluency: A Framework for Cybersecurity AI Fluency}, 
      author={Víctor Mayoral-Vilches and Jasmin Wachter and Cristóbal R. J. Veas Chavez and Cathrin Schachner and Luis Javier Navarrete-Lozano and María Sanz-Gómez},
      year={2025},
      eprint={2508.13588},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2508.13588}, 
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{mayoralvilches2025cybersecurityaihackingai,
      title={Cybersecurity AI: Hacking the AI Hackers via Prompt Injection}, 
      author={Víctor Mayoral-Vilches and Per Mannermaa Rynning},
      year={2025},
      eprint={2508.21669},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2508.21669}, 
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;p&gt;CAI was initially developed by &lt;a href="https://aliasrobotics.com"&gt;Alias Robotics&lt;/a&gt; and co-funded by the European EIC accelerator project RIS (GA 101161136) - HORIZON-EIC-2023-ACCELERATOR-01 call. The original agentic principles are inspired from OpenAI's &lt;a href="https://github.com/openai/swarm"&gt;&lt;code&gt;swarm&lt;/code&gt;&lt;/a&gt; library and translated into newer prototypes. This project also makes use of other relevant open source building blocks including &lt;a href="https://github.com/BerriAI/litellm"&gt;&lt;code&gt;LiteLLM&lt;/code&gt;&lt;/a&gt;, and &lt;a href="https://github.com/Arize-ai/phoenix"&gt;&lt;code&gt;phoenix&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Academic Collaborations&lt;/h3&gt; 
&lt;p&gt;CAI benefits from ongoing research collaborations with academic institutions. Researchers interested in collaborative projects, dataset access, or academic licenses should contact &lt;a href="mailto:research@aliasrobotics.com"&gt;research@aliasrobotics.com&lt;/a&gt;. We provide special support for:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;PhD research projects&lt;/li&gt; 
 &lt;li&gt;Academic benchmarking studies&lt;/li&gt; 
 &lt;li&gt;Security education initiatives&lt;/li&gt; 
 &lt;li&gt;Open-source contributions from research labs&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- Footnotes --&gt; 
&lt;p&gt;[^1]: Arguably, the Chain-of-Thought agentic pattern is a special case of the Hierarchical agentic pattern. [^2]: Kamhoua, C. A., Leslie, N. O., &amp;amp; Weisman, M. J. (2018). Game theoretic modeling of advanced persistent threat in internet of things. Journal of Cyber Security and Information Systems. [^3]: Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., &amp;amp; Cao, Y. (2023, January). React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR). [^4]: Deng, G., Liu, Y., Mayoral-Vilches, V., Liu, P., Li, Y., Xu, Y., ... &amp;amp; Rass, S. (2024). {PentestGPT}: Evaluating and harnessing large language models for automated penetration testing. In 33rd USENIX Security Symposium (USENIX Security 24) (pp. 847-864).&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>hao-ai-lab/FastVideo</title>
      <link>https://github.com/hao-ai-lab/FastVideo</link>
      <description>&lt;p&gt;A unified inference and post-training framework for accelerated video generation.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/hao-ai-lab/FastVideo/main/assets/logos/logo.svg?sanitize=true" width="30%" /&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;FastVideo is a unified post-training and inference framework for accelerated video generation.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;FastVideo features an end-to-end unified pipeline for accelerating diffusion models, starting from data preprocessing to model training, finetuning, distillation, and inference. FastVideo is designed to be modular and extensible, allowing users to easily add new optimizations and techniques. Whether it is training-free optimizations or post-training optimizations, FastVideo has you covered.&lt;/p&gt; 
&lt;p align="center"&gt; | 🕹️ &lt;a href="https://fastwan.fastvideo.org/" &lt;b&gt;Online Demo&lt;/a&gt; | &lt;a href="https://hao-ai-lab.github.io/FastVideo"&gt;&lt;b&gt;Documentation&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://hao-ai-lab.github.io/FastVideo/inference/inference_quick_start.html"&gt;&lt;b&gt; Quick Start&lt;/b&gt;&lt;/a&gt; | 🤗 &lt;a href="https://huggingface.co/collections/FastVideo/fastwan-6886a305d9799c8cd1496408" target="_blank"&gt;&lt;b&gt;FastWan&lt;/b&gt;&lt;/a&gt; | 🟣💬 &lt;a href="https://join.slack.com/t/fastvideo/shared_invite/zt-38u6p1jqe-yDI1QJOCEnbtkLoaI5bjZQ" target="_blank"&gt; &lt;b&gt;Slack&lt;/b&gt; &lt;/a&gt; | 🟣💬 &lt;a href="https://ibb.co/rG0QpZdw" target="_blank"&gt; &lt;b&gt; WeChat &lt;/b&gt; &lt;/a&gt; | &lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/hao-ai-lab/FastVideo/main/assets/fastwan.png" width="90%" /&gt; 
&lt;/div&gt; 
&lt;h2&gt;NEWS&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;2025/08/04&lt;/code&gt;: Release &lt;a href="https://hao-ai-lab.github.io/FastVideo/distillation/dmd.html"&gt;FastWan&lt;/a&gt; models and &lt;a href="https://hao-ai-lab.github.io/blogs/fastvideo_post_training/"&gt;Sparse-Distillation&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;2025/06/14&lt;/code&gt;: Release finetuning and inference code for &lt;a href="https://arxiv.org/pdf/2505.13389"&gt;VSA&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;2025/04/24&lt;/code&gt;: &lt;a href="https://hao-ai-lab.github.io/blogs/fastvideo/"&gt;FastVideo V1&lt;/a&gt; is released!&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;2025/02/18&lt;/code&gt;: Release the inference code for &lt;a href="https://hao-ai-lab.github.io/blogs/sta/"&gt;Sliding Tile Attention&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;p&gt;FastVideo has the following features:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;End-to-end post-training support: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://hao-ai-lab.github.io/blogs/fastvideo_post_training/"&gt;Sparse distillation&lt;/a&gt; for Wan2.1 and Wan2.2 to achineve &amp;gt;50x denoising speedup&lt;/li&gt; 
   &lt;li&gt;Data preprocessing pipeline for video data&lt;/li&gt; 
   &lt;li&gt;Support full finetuning and LoRA finetuning for state-of-the-art open video DiTs&lt;/li&gt; 
   &lt;li&gt;Scalable training with FSDP2, sequence parallelism, and selective activation checkpointing, with near linear scaling to 64 GPUs&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;State-of-the-art performance optimizations for inference 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://arxiv.org/pdf/2505.13389"&gt;Video Sparse Attention&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://arxiv.org/pdf/2502.04507"&gt;Sliding Tile Attention&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://arxiv.org/pdf/2411.19108"&gt;TeaCache&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://arxiv.org/abs/2410.02367"&gt;Sage Attention&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Diverse hardware and OS support 
  &lt;ul&gt; 
   &lt;li&gt;Support H100, A100, 4090&lt;/li&gt; 
   &lt;li&gt;Support Linux, Windows, MacOS&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;We recommend using an environment manager such as &lt;code&gt;Conda&lt;/code&gt; to create a clean environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Create and activate a new conda environment
conda create -n fastvideo python=3.12
conda activate fastvideo

# Install FastVideo
pip install fastvideo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Please see our &lt;a href="https://hao-ai-lab.github.io/FastVideo/getting_started/installation.html"&gt;docs&lt;/a&gt; for more detailed installation instructions.&lt;/p&gt; 
&lt;h2&gt;Sparse Distillation&lt;/h2&gt; 
&lt;p&gt;For our sparse distillation techniques, please see our &lt;a href="https://hao-ai-lab.github.io/FastVideo/distillation/dmd.html"&gt;distillation docs&lt;/a&gt; and check out our &lt;a href="https://hao-ai-lab.github.io/blogs/fastvideo_post_training/"&gt;blog&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;See below for recipes and datasets:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Model&lt;/th&gt; 
   &lt;th align="center"&gt;Sparse Distillation&lt;/th&gt; 
   &lt;th align="center"&gt;Dataset&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/FastVideo/FastWan2.1-T2V-1.3B-Diffusers"&gt;FastWan2.1-T2V-1.3B&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/hao-ai-lab/FastVideo/tree/main/examples/distill/Wan2.1-T2V/Wan-Syn-Data-480P"&gt;Recipe&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/datasets/FastVideo/Wan-Syn_77x448x832_600k"&gt;FastVideo Synthetic Wan2.1 480P&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/FastVideo/FastWan2.1-T2V-14B-Diffusers"&gt;FastWan2.1-T2V-14B-Preview&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;Coming soon!&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/datasets/FastVideo/Wan-Syn_77x768x1280_250k"&gt;FastVideo Synthetic Wan2.1 720P&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/FastVideo/FastWan2.2-TI2V-5B-Diffusers"&gt;FastWan2.2-TI2V-5B&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/hao-ai-lab/FastVideo/tree/main/examples/distill/Wan2.2-TI2V-5B-Diffusers/Data-free"&gt;Recipe&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/datasets/FastVideo/Wan2.2-Syn-121x704x1280_32k"&gt;FastVideo Synthetic Wan2.2 720P&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Inference&lt;/h2&gt; 
&lt;h3&gt;Generating Your First Video&lt;/h3&gt; 
&lt;p&gt;Here's a minimal example to generate a video using the default settings. Make sure VSA kernels are &lt;a href="https://hao-ai-lab.github.io/FastVideo/video_sparse_attention/installation.html"&gt;installed&lt;/a&gt;. Create a file called &lt;code&gt;example.py&lt;/code&gt; with the following code:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import os
from fastvideo import VideoGenerator

def main():
    os.environ["FASTVIDEO_ATTENTION_BACKEND"] = "VIDEO_SPARSE_ATTN"

    # Create a video generator with a pre-trained model
    generator = VideoGenerator.from_pretrained(
        "FastVideo/FastWan2.1-T2V-1.3B-Diffusers",
        num_gpus=1,  # Adjust based on your hardware
    )

    # Define a prompt for your video
    prompt = "A curious raccoon peers through a vibrant field of yellow sunflowers, its eyes wide with interest."

    # Generate the video
    video = generator.generate_video(
        prompt,
        return_frames=True,  # Also return frames from this call (defaults to False)
        output_path="my_videos/",  # Controls where videos are saved
        save_video=True
    )

if __name__ == '__main__':
    main()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run the script with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python example.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For a more detailed guide, please see our &lt;a href="https://hao-ai-lab.github.io/FastVideo/inference/inference_quick_start.html"&gt;inference quick start&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Other docs:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://hao-ai-lab.github.io/FastVideo/design/overview.html"&gt;Design Overview&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://hao-ai-lab.github.io/FastVideo/getting_started/installation.html"&gt;Contribution Guide&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Distillation and Finetuning&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://hao-ai-lab.github.io/FastVideo/distillation/dmd.html"&gt;Distillation Guide&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- - [Finetuning Guide](https://hao-ai-lab.github.io/FastVideo/training/finetune.html) --&gt; 
&lt;h2&gt;📑 Development Plan&lt;/h2&gt; 
&lt;!-- - More distillation methods --&gt; 
&lt;!-- - [ ] Add Distribution Matching Distillation --&gt; 
&lt;p&gt;More FastWan Models Coming Soon!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Add FastWan2.1-T2V-14B&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Add FastWan2.2-T2V-14B&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Add FastWan2.2-I2V-14B&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- - Optimization features
- Code updates --&gt; 
&lt;!-- - [ ] fp8 support --&gt; 
&lt;!-- - [ ] faster load model and save model support --&gt; 
&lt;p&gt;See details in &lt;a href="https://github.com/hao-ai-lab/FastVideo/issues/468"&gt;development roadmap&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;🤝 Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome all contributions. Please check out our guide &lt;a href="https://hao-ai-lab.github.io/FastVideo/contributing/overview.html"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Acknowledgement&lt;/h2&gt; 
&lt;p&gt;We learned and reused code from the following projects:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Wan-Video"&gt;Wan-Video&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/HazyResearch/ThunderKittens"&gt;ThunderKittens&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/triton-lang/triton"&gt;Triton&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/tianweiy/DMD2"&gt;DMD2&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/huggingface/diffusers"&gt;diffusers&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/xdit-project/xDiT"&gt;xDiT&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vllm-project/vllm"&gt;vLLM&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/sgl-project/sglang"&gt;SGLang&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We thank &lt;a href="https://ifm.mbzuai.ac.ae/"&gt;MBZUAI&lt;/a&gt;, &lt;a href="https://www.anyscale.com/"&gt;Anyscale&lt;/a&gt;, and &lt;a href="https://www.gmicloud.ai/"&gt;GMI Cloud&lt;/a&gt; for their support throughout this project.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you find FastVideo useful, please considering citing our work:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@software{fastvideo2024,
  title        = {FastVideo: A Unified Framework for Accelerated Video Generation},
  author       = {The FastVideo Team},
  url          = {https://github.com/hao-ai-lab/FastVideo},
  month        = apr,
  year         = {2024},
}

@article{zhang2025vsa,
  title={VSA: Faster Video Diffusion with Trainable Sparse Attention},
  author={Zhang, Peiyuan and Huang, Haofeng and Chen, Yongqi and Lin, Will and Liu, Zhengzhong and Stoica, Ion and Xing, Eric and Zhang, Hao},
  journal={arXiv preprint arXiv:2505.13389},
  year={2025}
}

@article{zhang2025fast,
  title={Fast video generation with sliding tile attention},
  author={Zhang, Peiyuan and Chen, Yongqi and Su, Runlong and Ding, Hangliang and Stoica, Ion and Liu, Zhengzhong and Zhang, Hao},
  journal={arXiv preprint arXiv:2502.04507},
  year={2025}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>laramies/theHarvester</title>
      <link>https://github.com/laramies/theHarvester</link>
      <description>&lt;p&gt;E-mails, subdomains and names Harvester - OSINT&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://github.com/laramies/theHarvester/raw/master/theHarvester-logo.webp" alt="theHarvester" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/laramies/theHarvester/workflows/TheHarvester%20Python%20CI/badge.svg?sanitize=true" alt="TheHarvester CI" /&gt; &lt;img src="https://github.com/laramies/theHarvester/workflows/TheHarvester%20Docker%20Image%20CI/badge.svg?sanitize=true" alt="TheHarvester Docker Image CI" /&gt; &lt;a href="https://inventory.raw.pm/"&gt;&lt;img src="https://inventory.raw.pm/img/badges/Rawsec-inventoried-FF5050_flat_without_logo.svg?sanitize=true" alt="Rawsec's CyberSecurity Inventory" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;About&lt;/h2&gt; 
&lt;p&gt;theHarvester is a simple to use, yet powerful tool designed to be used during the reconnaissance stage of a red team assessment or penetration test. It performs open source intelligence (OSINT) gathering to help determine a domain's external threat landscape. The tool gathers names, emails, IPs, subdomains, and URLs by using multiple public resources that include:&lt;/p&gt; 
&lt;h2&gt;Install and dependencies&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.12 or higher.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/laramies/theHarvester/wiki/Installation"&gt;https://github.com/laramies/theHarvester/wiki/Installation&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Install uv:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -LsSf https://astral.sh/uv/install.sh | sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Clone the repository:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/laramies/theHarvester
cd theHarvester
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Install dependencies and create a virtual environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv sync
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run theHarvester:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run theHarvester
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Development&lt;/h2&gt; 
&lt;p&gt;To install development dependencies:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv sync --extra dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To run tests:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run pytest
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To run linting and formatting:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run ruff check
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run ruff format
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Passive modules&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;baidu: Baidu search engine (&lt;a href="https://www.baidu.com"&gt;https://www.baidu.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;bevigil: CloudSEK BeVigil scans mobile application for OSINT assets (&lt;a href="https://bevigil.com/osint-api"&gt;https://bevigil.com/osint-api&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;brave: Brave search engine - now uses official Brave Search API (&lt;a href="https://api-dashboard.search.brave.com"&gt;https://api-dashboard.search.brave.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;bufferoverun: Fast domain name lookups for TLS certificates in IPv4 space (&lt;a href="https://tls.bufferover.run"&gt;https://tls.bufferover.run&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;builtwith: Find out what websites are built with (&lt;a href="https://builtwith.com"&gt;https://builtwith.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;censys: Uses certificates searches to enumerate subdomains and gather emails (&lt;a href="https://censys.io"&gt;https://censys.io&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;certspotter: Cert Spotter monitors Certificate Transparency logs (&lt;a href="https://sslmate.com/certspotter"&gt;https://sslmate.com/certspotter&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;criminalip: Specialized Cyber Threat Intelligence (CTI) search engine (&lt;a href="https://www.criminalip.io"&gt;https://www.criminalip.io&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;crtsh: Comodo Certificate search (&lt;a href="https://crt.sh"&gt;https://crt.sh&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;dehashed: Take your data security to the next level is (&lt;a href="https://dehashed.com"&gt;https://dehashed.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;dnsdumpster: Domain research tool that can discover hosts related to a domain (&lt;a href="https://dnsdumpster.com"&gt;https://dnsdumpster.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;duckduckgo: DuckDuckGo search engine (&lt;a href="https://duckduckgo.com"&gt;https://duckduckgo.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;fullhunt: Next-generation attack surface security platform (&lt;a href="https://fullhunt.io"&gt;https://fullhunt.io&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;github-code: GitHub code search engine (&lt;a href="https://www.github.com"&gt;https://www.github.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;hackertarget: Online vulnerability scanners and network intelligence to help organizations (&lt;a href="https://hackertarget.com"&gt;https://hackertarget.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;haveibeenpwned: Check if your email address is in a data breach (&lt;a href="https://haveibeenpwned.com"&gt;https://haveibeenpwned.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;hunter: Hunter search engine (&lt;a href="https://hunter.io"&gt;https://hunter.io&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;hunterhow: Internet search engines for security researchers (&lt;a href="https://hunter.how"&gt;https://hunter.how&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;intelx: Intelx search engine (&lt;a href="https://intelx.io"&gt;https://intelx.io&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;leaklookup: Data breach search engine (&lt;a href="https://leak-lookup.com"&gt;https://leak-lookup.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;netlas: A Shodan or Censys competitor (&lt;a href="https://app.netlas.io"&gt;https://app.netlas.io&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;onyphe: Cyber defense search engine (&lt;a href="https://www.onyphe.io"&gt;https://www.onyphe.io&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;otx: AlienVault open threat exchange (&lt;a href="https://otx.alienvault.com"&gt;https://otx.alienvault.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;pentesttools: Cloud-based toolkit for offensive security testing, focused on web applications and network penetration testing (&lt;a href="https://pentest-tools.com"&gt;https://pentest-tools.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;projecdiscovery: Actively collects and maintains internet-wide assets data, to enhance research and analyse changes around DNS for better insights (&lt;a href="https://chaos.projectdiscovery.io"&gt;https://chaos.projectdiscovery.io&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;rapiddns: DNS query tool which make querying subdomains or sites of a same IP easy (&lt;a href="https://rapiddns.io"&gt;https://rapiddns.io&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;rocketreach: Access real-time verified personal/professional emails, phone numbers, and social media links (&lt;a href="https://rocketreach.co"&gt;https://rocketreach.co&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;securityscorecard: helps TPRM and SOC teams detect, prioritize, and remediate vendor risk across their entire supplier ecosystem at scale (&lt;a href="https://securityscorecard.com"&gt;https://securityscorecard.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;securityTrails: Security Trails search engine, the world's largest repository of historical DNS data (&lt;a href="https://securitytrails.com"&gt;https://securitytrails.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;-s, --shodan: Shodan search engine will search for ports and banners from discovered hosts (&lt;a href="https://shodan.io"&gt;https://shodan.io&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;subdomaincenter: A subdomain finder tool used to find subdomains of a given domain (&lt;a href="https://www.subdomain.center"&gt;https://www.subdomain.center&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;subdomainfinderc99: A subdomain finder is a tool used to find the subdomains of a given domain (&lt;a href="https://subdomainfinder.c99.nl"&gt;https://subdomainfinder.c99.nl&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;threatminer: Data mining for threat intelligence (&lt;a href="https://www.threatminer.org"&gt;https://www.threatminer.org&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;tomba: Tomba search engine (&lt;a href="https://tomba.io"&gt;https://tomba.io&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;urlscan: A sandbox for the web that is a URL and website scanner (&lt;a href="https://urlscan.io"&gt;https://urlscan.io&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;venacus: Venacus search engine (&lt;a href="https://venacus.com"&gt;https://venacus.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;virustotal: Domain search (&lt;a href="https://www.virustotal.com"&gt;https://www.virustotal.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;whoisxml: Subdomain search (&lt;a href="https://subdomains.whoisxmlapi.com/api/pricing"&gt;https://subdomains.whoisxmlapi.com/api/pricing&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;yahoo: Yahoo search engine (&lt;a href="https://www.yahoo.com"&gt;https://www.yahoo.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;zoomeye: China's version of Shodan (&lt;a href="https://www.zoomeye.org"&gt;https://www.zoomeye.org&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Active modules&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;DNS brute force: dictionary brute force enumeration&lt;/li&gt; 
 &lt;li&gt;Screenshots: Take screenshots of subdomains that were found&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Modules that require an API key&lt;/h2&gt; 
&lt;p&gt;Documentation to setup API keys can be found at - &lt;a href="https://github.com/laramies/theHarvester/wiki/Installation#api-keys"&gt;https://github.com/laramies/theHarvester/wiki/Installation#api-keys&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;bevigil - 50 free queries/month, 1k queries/month $50&lt;/li&gt; 
 &lt;li&gt;brave - Free plan available, Pro plans for higher limits&lt;/li&gt; 
 &lt;li&gt;bufferoverun - 100 free queries/month, 10k/month $25&lt;/li&gt; 
 &lt;li&gt;builtwith - 50 free queries ever, $2950/yr&lt;/li&gt; 
 &lt;li&gt;censys - 500 credits $100&lt;/li&gt; 
 &lt;li&gt;criminalip - 100 free queries/month, 700k/month $59&lt;/li&gt; 
 &lt;li&gt;dehashed - 500 credts $15, 5k credits $150&lt;/li&gt; 
 &lt;li&gt;dnsdumpster - 50 free querries/day, $49&lt;/li&gt; 
 &lt;li&gt;fullhunt - 50 free queries, 200 queries $29/month, 500 queries $59/month&lt;/li&gt; 
 &lt;li&gt;github-code&lt;/li&gt; 
 &lt;li&gt;haveibeenpwned - 10 email searches/min $4.50, 50 email searches/min $22&lt;/li&gt; 
 &lt;li&gt;hunter - 50 credits/month free, 12k credits/yr $34&lt;/li&gt; 
 &lt;li&gt;hunterhow - 10k free API results per 30 days, 50k API results per 30 days $10&lt;/li&gt; 
 &lt;li&gt;intelx&lt;/li&gt; 
 &lt;li&gt;leaklookup - 20 credits $10, 50 credits $20, 140 credits $50, 300 credits $100&lt;/li&gt; 
 &lt;li&gt;netlas - 50 free requests/day, 1k requests $49, 10k requests $249&lt;/li&gt; 
 &lt;li&gt;onyphe - 10M results/month $587&lt;/li&gt; 
 &lt;li&gt;pentesttools - 5 assets netsec $95/month, 5 assets webnetsec $140/month&lt;/li&gt; 
 &lt;li&gt;projecdiscovery - requires work email. Free monthly discovery and vulnerability scans on sign-up email domain, enterprise $&lt;/li&gt; 
 &lt;li&gt;rocketreach - 100 email lookups/month $48, 250 email lookups/month $108&lt;/li&gt; 
 &lt;li&gt;securityscorecard&lt;/li&gt; 
 &lt;li&gt;securityTrails - 50 free queries/month, 20k queries/month $500&lt;/li&gt; 
 &lt;li&gt;shodan - Freelancer $69 month, Small Business $359 month&lt;/li&gt; 
 &lt;li&gt;tomba - 25 searches/month free, 1k searches/month $39, 5k searches/month $89&lt;/li&gt; 
 &lt;li&gt;venacus - 1 search/day free, 10 searches/day $12, 30 searches/day $36&lt;/li&gt; 
 &lt;li&gt;whoisxml - 2k queries $50, 5k queries $105&lt;/li&gt; 
 &lt;li&gt;zoomeye - 5 results/day free, 30/results/day $190/yr&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Comments, bugs, and requests&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://twitter.com/laramies"&gt;&lt;img src="https://img.shields.io/twitter/follow/laramies.svg?style=social&amp;amp;label=Follow" alt="Twitter Follow" /&gt;&lt;/a&gt; Christian Martorella @laramies &lt;a href="mailto:cmartorella@edge-security.com"&gt;cmartorella@edge-security.com&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://twitter.com/NotoriousRebel1"&gt;&lt;img src="https://img.shields.io/twitter/follow/NotoriousRebel1.svg?style=social&amp;amp;label=Follow" alt="Twitter Follow" /&gt;&lt;/a&gt; Matthew Brown @NotoriousRebel1&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://twitter.com/jay_townsend1"&gt;&lt;img src="https://img.shields.io/twitter/follow/jay_townsend1.svg?style=social&amp;amp;label=Follow" alt="Twitter Follow" /&gt;&lt;/a&gt; Jay "L1ghtn1ng" Townsend @jay_townsend1&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Main contributors&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://twitter.com/NotoriousRebel1"&gt;&lt;img src="https://img.shields.io/twitter/follow/NotoriousRebel1.svg?style=social&amp;amp;label=Follow" alt="Twitter Follow" /&gt;&lt;/a&gt; Matthew Brown @NotoriousRebel1&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://twitter.com/jay_townsend1"&gt;&lt;img src="https://img.shields.io/twitter/follow/jay_townsend1.svg?style=social&amp;amp;label=Follow" alt="Twitter Follow" /&gt;&lt;/a&gt; Jay "L1ghtn1ng" Townsend @jay_townsend1&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://twitter.com/discoverscripts"&gt;&lt;img src="https://img.shields.io/twitter/follow/discoverscripts.svg?style=social&amp;amp;label=Follow" alt="Twitter Follow" /&gt;&lt;/a&gt; Lee Baird @discoverscripts&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Thanks&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;John Matherly - Shodan project&lt;/li&gt; 
 &lt;li&gt;Ahmed Aboul Ela - subdomain names dictionaries (big and small)&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>lfnovo/open-notebook</title>
      <link>https://github.com/lfnovo/open-notebook</link>
      <description>&lt;p&gt;An Open Source implementation of Notebook LM with more flexibility and features&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a id="readme-top"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;!-- [![Contributors][contributors-shield]][contributors-url] --&gt; 
&lt;p&gt;&lt;a href="https://github.com/lfnovo/open-notebook/network/members"&gt;&lt;img src="https://img.shields.io/github/forks/lfnovo/open-notebook.svg?style=for-the-badge" alt="Forks" /&gt;&lt;/a&gt; &lt;a href="https://github.com/lfnovo/open-notebook/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/lfnovo/open-notebook.svg?style=for-the-badge" alt="Stargazers" /&gt;&lt;/a&gt; &lt;a href="https://github.com/lfnovo/open-notebook/issues"&gt;&lt;img src="https://img.shields.io/github/issues/lfnovo/open-notebook.svg?style=for-the-badge" alt="Issues" /&gt;&lt;/a&gt; &lt;a href="https://github.com/lfnovo/open-notebook/raw/master/LICENSE.txt"&gt;&lt;img src="https://img.shields.io/github/license/lfnovo/open-notebook.svg?style=for-the-badge" alt="MIT License" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;!-- [![LinkedIn][linkedin-shield]][linkedin-url] --&gt; 
&lt;!-- PROJECT LOGO --&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/lfnovo/open-notebook"&gt; &lt;img src="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/assets/hero.svg?sanitize=true" alt="Logo" /&gt; &lt;/a&gt; 
 &lt;h3 align="center"&gt;Open Notebook&lt;/h3&gt; 
 &lt;p align="center"&gt; An open source, privacy-focused alternative to Google's Notebook LM! &lt;br /&gt;&lt;strong&gt;Join our &lt;a href="https://discord.gg/37XJPXfz2w"&gt;Discord server&lt;/a&gt; for help, to share workflow ideas, and suggest features!&lt;/strong&gt; &lt;br /&gt; &lt;a href="https://www.open-notebook.ai"&gt;&lt;strong&gt;Checkout our website »&lt;/strong&gt;&lt;/a&gt; &lt;br /&gt; &lt;br /&gt; &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/index.md"&gt;📚 Get Started&lt;/a&gt; · &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/index.md"&gt;📖 User Guide&lt;/a&gt; · &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/features/index.md"&gt;✨ Features&lt;/a&gt; · &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/deployment/index.md"&gt;🚀 Deploy&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;📢 Open Notebook is under very active development&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Open Notebook is under active development! We're moving fast and making improvements every week. Your feedback is incredibly valuable to me during this exciting phase and it gives me motivation to keep improving and building this amazing tool. Please feel free to star the project if you find it useful, and don't hesitate to reach out with any questions or suggestions. I'm excited to see how you'll use it and what ideas you'll bring to the project! Let's build something amazing together! 🚀&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;About The Project&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/assets/asset_list.png" alt="New Notebook" /&gt;&lt;/p&gt; 
&lt;p&gt;An open source, privacy-focused alternative to Google's Notebook LM. Why give Google more of our data when we can take control of our own research workflows?&lt;/p&gt; 
&lt;p&gt;In a world dominated by Artificial Intelligence, having the ability to think 🧠 and acquire new knowledge 💡, is a skill that should not be a privilege for a few, nor restricted to a single provider.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Open Notebook empowers you to:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;🔒 &lt;strong&gt;Control your data&lt;/strong&gt; - Keep your research private and secure&lt;/li&gt; 
 &lt;li&gt;🤖 &lt;strong&gt;Choose your AI models&lt;/strong&gt; - Support for 16+ providers including OpenAI, Anthropic, Ollama, LM Studio, and more&lt;/li&gt; 
 &lt;li&gt;📚 &lt;strong&gt;Organize multi-modal content&lt;/strong&gt; - PDFs, videos, audio, web pages, and more&lt;/li&gt; 
 &lt;li&gt;🎙️ &lt;strong&gt;Generate professional podcasts&lt;/strong&gt; - Advanced multi-speaker podcast generation&lt;/li&gt; 
 &lt;li&gt;🔍 &lt;strong&gt;Search intelligently&lt;/strong&gt; - Full-text and vector search across all your content&lt;/li&gt; 
 &lt;li&gt;💬 &lt;strong&gt;Chat with context&lt;/strong&gt; - AI conversations powered by your research&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Learn more about our project at &lt;a href="https://www.open-notebook.ai"&gt;https://www.open-notebook.ai&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;🆚 Open Notebook vs Google Notebook LM&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Feature&lt;/th&gt; 
   &lt;th&gt;Open Notebook&lt;/th&gt; 
   &lt;th&gt;Google Notebook LM&lt;/th&gt; 
   &lt;th&gt;Advantage&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Privacy &amp;amp; Control&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Self-hosted, your data&lt;/td&gt; 
   &lt;td&gt;Google cloud only&lt;/td&gt; 
   &lt;td&gt;Complete data sovereignty&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;AI Provider Choice&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;16+ providers (OpenAI, Anthropic, Ollama, LM Studio, etc.)&lt;/td&gt; 
   &lt;td&gt;Google models only&lt;/td&gt; 
   &lt;td&gt;Flexibility and cost optimization&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Podcast Speakers&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;1-4 speakers with custom profiles&lt;/td&gt; 
   &lt;td&gt;2 speakers only&lt;/td&gt; 
   &lt;td&gt;Extreme flexibility&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Context Control&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;3 granular levels&lt;/td&gt; 
   &lt;td&gt;All-or-nothing&lt;/td&gt; 
   &lt;td&gt;Privacy and performance tuning&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Content Transformations&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Custom and built-in&lt;/td&gt; 
   &lt;td&gt;Limited options&lt;/td&gt; 
   &lt;td&gt;Unlimited processing power&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;API Access&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Full REST API&lt;/td&gt; 
   &lt;td&gt;No API&lt;/td&gt; 
   &lt;td&gt;Complete automation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Deployment&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Docker, cloud, or local&lt;/td&gt; 
   &lt;td&gt;Google hosted only&lt;/td&gt; 
   &lt;td&gt;Deploy anywhere&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Citations&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Comprehensive with sources&lt;/td&gt; 
   &lt;td&gt;Basic references&lt;/td&gt; 
   &lt;td&gt;Research integrity&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Customization&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Open source, fully customizable&lt;/td&gt; 
   &lt;td&gt;Closed system&lt;/td&gt; 
   &lt;td&gt;Unlimited extensibility&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Cost&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Pay only for AI usage&lt;/td&gt; 
   &lt;td&gt;Monthly subscription + usage&lt;/td&gt; 
   &lt;td&gt;Transparent and controllable&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Why Choose Open Notebook?&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;🔒 &lt;strong&gt;Privacy First&lt;/strong&gt;: Your sensitive research stays completely private&lt;/li&gt; 
 &lt;li&gt;💰 &lt;strong&gt;Cost Control&lt;/strong&gt;: Choose cheaper AI providers or run locally with Ollama&lt;/li&gt; 
 &lt;li&gt;🎙️ &lt;strong&gt;Better Podcasts&lt;/strong&gt;: Full script control and multi-speaker flexibility vs limited 2-speaker deep-dive format&lt;/li&gt; 
 &lt;li&gt;🔧 &lt;strong&gt;Unlimited Customization&lt;/strong&gt;: Modify, extend, and integrate as needed&lt;/li&gt; 
 &lt;li&gt;🌐 &lt;strong&gt;No Vendor Lock-in&lt;/strong&gt;: Switch providers, deploy anywhere, own your data&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Built With&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://www.python.org/"&gt;&lt;img src="https://img.shields.io/badge/Python-3776AB?style=for-the-badge&amp;amp;logo=python&amp;amp;logoColor=white" alt="Python" /&gt;&lt;/a&gt; &lt;a href="https://surrealdb.com/"&gt;&lt;img src="https://img.shields.io/badge/SurrealDB-FF5E00?style=for-the-badge&amp;amp;logo=databricks&amp;amp;logoColor=white" alt="SurrealDB" /&gt;&lt;/a&gt; &lt;a href="https://www.langchain.com/"&gt;&lt;img src="https://img.shields.io/badge/LangChain-3A3A3A?style=for-the-badge&amp;amp;logo=chainlink&amp;amp;logoColor=white" alt="LangChain" /&gt;&lt;/a&gt; &lt;a href="https://streamlit.io/"&gt;&lt;img src="https://img.shields.io/badge/Streamlit-FF4B4B?style=for-the-badge&amp;amp;logo=streamlit&amp;amp;logoColor=white" alt="Streamlit" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;🚀 Quick Start&lt;/h2&gt; 
&lt;p&gt;Ready to try Open Notebook? Choose your preferred method:&lt;/p&gt; 
&lt;h3&gt;⚡ Instant Setup (Recommended)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Create a new directory for your Open Notebook installation
mkdir open-notebook
cd open-notebook

# Using Docker - Get started in 2 minutes
docker run -d \
  --name open-notebook \
  -p 8502:8502 -p 5055:5055 \
  -v ./notebook_data:/app/data \
  -v ./surreal_data:/mydata \
  -e OPENAI_API_KEY=your_key \
  lfnovo/open_notebook:latest-single
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;What gets created:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;open-notebook/
├── notebook_data/     # Your notebooks and research content
└── surreal_data/      # Database files
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Access your installation:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;🖥️ Main Interface&lt;/strong&gt;: &lt;a href="http://localhost:8502"&gt;http://localhost:8502&lt;/a&gt; (Streamlit UI)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🔧 API Access&lt;/strong&gt;: &lt;a href="http://localhost:5055"&gt;http://localhost:5055&lt;/a&gt; (REST API)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;📚 API Documentation&lt;/strong&gt;: &lt;a href="http://localhost:5055/docs"&gt;http://localhost:5055/docs&lt;/a&gt; (Interactive Swagger UI)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;⚠️ Important&lt;/strong&gt;:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;strong&gt;Run from a dedicated folder&lt;/strong&gt;: Create and run this from inside a new &lt;code&gt;open-notebook&lt;/code&gt; folder so your data volumes are properly organized&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Volume persistence&lt;/strong&gt;: The volumes (&lt;code&gt;-v ./notebook_data:/app/data&lt;/code&gt; and &lt;code&gt;-v ./surreal_data:/mydata&lt;/code&gt;) are essential to persist your data between container restarts. Without them, you'll lose all your notebooks and research when the container stops.&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;🛠️ Full Installation&lt;/h3&gt; 
&lt;p&gt;For development or customization:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/lfnovo/open-notebook
cd open-notebook
make start-all
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;📖 Need Help?&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;🤖 AI Installation Assistant&lt;/strong&gt;: We have a &lt;a href="https://chatgpt.com/g/g-68776e2765b48191bd1bae3f30212631-open-notebook-installation-assistant"&gt;CustomGPT built to help you install Open Notebook&lt;/a&gt; - it will guide you through each step!&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;New to Open Notebook?&lt;/strong&gt; Start with our &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/index.md"&gt;Getting Started Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Need installation help?&lt;/strong&gt; Check our &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/installation.md"&gt;Installation Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Want to see it in action?&lt;/strong&gt; Try our &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/quick-start.md"&gt;Quick Start Tutorial&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Provider Support Matrix&lt;/h2&gt; 
&lt;p&gt;Thanks to the &lt;a href="https://github.com/lfnovo/esperanto"&gt;Esperanto&lt;/a&gt; library, we support this providers out of the box!&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Provider&lt;/th&gt; 
   &lt;th&gt;LLM Support&lt;/th&gt; 
   &lt;th&gt;Embedding Support&lt;/th&gt; 
   &lt;th&gt;Speech-to-Text&lt;/th&gt; 
   &lt;th&gt;Text-to-Speech&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenAI&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Anthropic&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Groq&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Google (GenAI)&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Vertex AI&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ollama&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Perplexity&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ElevenLabs&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Azure OpenAI&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Mistral&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;DeepSeek&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Voyage&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;xAI&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenRouter&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenAI Compatible*&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;*Supports LM Studio and any OpenAI-compatible endpoint&lt;/p&gt; 
&lt;h2&gt;✨ Key Features&lt;/h2&gt; 
&lt;h3&gt;Core Capabilities&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;🔒 Privacy-First&lt;/strong&gt;: Your data stays under your control - no cloud dependencies&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🎯 Multi-Notebook Organization&lt;/strong&gt;: Manage multiple research projects seamlessly&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;📚 Universal Content Support&lt;/strong&gt;: PDFs, videos, audio, web pages, Office docs, and more&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🤖 Multi-Model AI Support&lt;/strong&gt;: 16+ providers including OpenAI, Anthropic, Ollama, Google, LM Studio, and more&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🎙️ Professional Podcast Generation&lt;/strong&gt;: Advanced multi-speaker podcasts with Episode Profiles&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🔍 Intelligent Search&lt;/strong&gt;: Full-text and vector search across all your content&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;💬 Context-Aware Chat&lt;/strong&gt;: AI conversations powered by your research materials&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;📝 AI-Assisted Notes&lt;/strong&gt;: Generate insights or write notes manually&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Advanced Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;⚡ Reasoning Model Support&lt;/strong&gt;: Full support for thinking models like DeepSeek-R1 and Qwen3&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🔧 Content Transformations&lt;/strong&gt;: Powerful customizable actions to summarize and extract insights&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🌐 Comprehensive REST API&lt;/strong&gt;: Full programmatic access for custom integrations &lt;a href="http://localhost:5055/docs"&gt;&lt;img src="https://img.shields.io/badge/API-Documentation-blue?style=flat-square" alt="API Docs" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🔐 Optional Password Protection&lt;/strong&gt;: Secure public deployments with authentication&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;📊 Fine-Grained Context Control&lt;/strong&gt;: Choose exactly what to share with AI models&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;📎 Citations&lt;/strong&gt;: Get answers with proper source citations&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Three-Column Interface&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Sources&lt;/strong&gt;: Manage all your research materials&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Notes&lt;/strong&gt;: Create manual or AI-generated notes&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Chat&lt;/strong&gt;: Converse with AI using your content as context&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=D-760MlGwaI"&gt;&lt;img src="https://img.youtube.com/vi/D-760MlGwaI/0.jpg" alt="Check out our podcast sample" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;📚 Documentation&lt;/h2&gt; 
&lt;h3&gt;Getting Started&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/introduction.md"&gt;📖 Introduction&lt;/a&gt;&lt;/strong&gt; - Learn what Open Notebook offers&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/quick-start.md"&gt;⚡ Quick Start&lt;/a&gt;&lt;/strong&gt; - Get up and running in 5 minutes&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/installation.md"&gt;🔧 Installation&lt;/a&gt;&lt;/strong&gt; - Comprehensive setup guide&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/first-notebook.md"&gt;🎯 Your First Notebook&lt;/a&gt;&lt;/strong&gt; - Step-by-step tutorial&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;User Guide&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/interface-overview.md"&gt;📱 Interface Overview&lt;/a&gt;&lt;/strong&gt; - Understanding the layout&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/notebooks.md"&gt;📚 Notebooks&lt;/a&gt;&lt;/strong&gt; - Organizing your research&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/sources.md"&gt;📄 Sources&lt;/a&gt;&lt;/strong&gt; - Managing content types&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/notes.md"&gt;📝 Notes&lt;/a&gt;&lt;/strong&gt; - Creating and managing notes&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/chat.md"&gt;💬 Chat&lt;/a&gt;&lt;/strong&gt; - AI conversations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/search.md"&gt;🔍 Search&lt;/a&gt;&lt;/strong&gt; - Finding information&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Advanced Topics&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/features/podcasts.md"&gt;🎙️ Podcast Generation&lt;/a&gt;&lt;/strong&gt; - Create professional podcasts&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/features/transformations.md"&gt;🔧 Content Transformations&lt;/a&gt;&lt;/strong&gt; - Customize content processing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/features/ai-models.md"&gt;🤖 AI Models&lt;/a&gt;&lt;/strong&gt; - AI model configuration&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/development/api-reference.md"&gt;🔧 REST API Reference&lt;/a&gt;&lt;/strong&gt; - Complete API documentation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/deployment/security.md"&gt;🔐 Security&lt;/a&gt;&lt;/strong&gt; - Password protection and privacy&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/deployment/index.md"&gt;🚀 Deployment&lt;/a&gt;&lt;/strong&gt; - Complete deployment guides for all scenarios&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="right"&gt;(&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/#readme-top"&gt;back to top&lt;/a&gt;)&lt;/p&gt; 
&lt;h2&gt;🗺️ Roadmap&lt;/h2&gt; 
&lt;h3&gt;Upcoming Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;React Frontend&lt;/strong&gt;: Modern React-based frontend to replace Streamlit&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Live Front-End Updates&lt;/strong&gt;: Real-time UI updates for smoother experience&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Async Processing&lt;/strong&gt;: Faster UI through asynchronous content processing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Cross-Notebook Sources&lt;/strong&gt;: Reuse research materials across projects&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Bookmark Integration&lt;/strong&gt;: Connect with your favorite bookmarking apps&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Recently Completed ✅&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Comprehensive REST API&lt;/strong&gt;: Full programmatic access to all functionality&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-Model Support&lt;/strong&gt;: 16+ AI providers including OpenAI, Anthropic, Ollama, LM Studio&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Advanced Podcast Generator&lt;/strong&gt;: Professional multi-speaker podcasts with Episode Profiles&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Content Transformations&lt;/strong&gt;: Powerful customizable actions for content processing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Enhanced Citations&lt;/strong&gt;: Improved layout and finer control for source citations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multiple Chat Sessions&lt;/strong&gt;: Manage different conversations within notebooks&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See the &lt;a href="https://github.com/lfnovo/open-notebook/issues"&gt;open issues&lt;/a&gt; for a full list of proposed features and known issues.&lt;/p&gt; 
&lt;p align="right"&gt;(&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/#readme-top"&gt;back to top&lt;/a&gt;)&lt;/p&gt; 
&lt;h2&gt;🤝 Community &amp;amp; Contributing&lt;/h2&gt; 
&lt;h3&gt;Join the Community&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;💬 &lt;strong&gt;&lt;a href="https://discord.gg/37XJPXfz2w"&gt;Discord Server&lt;/a&gt;&lt;/strong&gt; - Get help, share ideas, and connect with other users&lt;/li&gt; 
 &lt;li&gt;🐛 &lt;strong&gt;&lt;a href="https://github.com/lfnovo/open-notebook/issues"&gt;GitHub Issues&lt;/a&gt;&lt;/strong&gt; - Report bugs and request features&lt;/li&gt; 
 &lt;li&gt;⭐ &lt;strong&gt;Star this repo&lt;/strong&gt; - Show your support and help others discover Open Notebook&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Contributing&lt;/h3&gt; 
&lt;p&gt;We welcome contributions! We're especially looking for help with:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Frontend Development&lt;/strong&gt;: Help build a modern React-based UI (planned replacement for current Streamlit interface)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Testing &amp;amp; Bug Fixes&lt;/strong&gt;: Make Open Notebook more robust&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Feature Development&lt;/strong&gt;: Build the coolest research tool together&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Documentation&lt;/strong&gt;: Improve guides and tutorials&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Current Tech Stack&lt;/strong&gt;: Python, FastAPI, SurrealDB, Streamlit&lt;br /&gt; &lt;strong&gt;Future Roadmap&lt;/strong&gt;: React frontend, enhanced real-time updates&lt;/p&gt; 
&lt;p&gt;See our &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt; for detailed information on how to get started.&lt;/p&gt; 
&lt;p align="right"&gt;(&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/#readme-top"&gt;back to top&lt;/a&gt;)&lt;/p&gt; 
&lt;h2&gt;📄 License&lt;/h2&gt; 
&lt;p&gt;Open Notebook is MIT licensed. See the &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h2&gt;📞 Contact&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Luis Novo&lt;/strong&gt; - &lt;a href="https://twitter.com/lfnovo"&gt;@lfnovo&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Community Support&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;💬 &lt;a href="https://discord.gg/37XJPXfz2w"&gt;Discord Server&lt;/a&gt; - Get help, share ideas, and connect with users&lt;/li&gt; 
 &lt;li&gt;🐛 &lt;a href="https://github.com/lfnovo/open-notebook/issues"&gt;GitHub Issues&lt;/a&gt; - Report bugs and request features&lt;/li&gt; 
 &lt;li&gt;🌐 &lt;a href="https://www.open-notebook.ai"&gt;Website&lt;/a&gt; - Learn more about the project&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🙏 Acknowledgments&lt;/h2&gt; 
&lt;p&gt;Open Notebook is built on the shoulders of amazing open-source projects:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/lfnovo/podcast-creator"&gt;Podcast Creator&lt;/a&gt;&lt;/strong&gt; - Advanced podcast generation capabilities&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/lfnovo/surreal-commands"&gt;Surreal Commands&lt;/a&gt;&lt;/strong&gt; - Background job processing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/lfnovo/content-core"&gt;Content Core&lt;/a&gt;&lt;/strong&gt; - Content processing and management&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/lfnovo/esperanto"&gt;Esperanto&lt;/a&gt;&lt;/strong&gt; - Multi-provider AI model abstraction&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/docling-project/docling"&gt;Docling&lt;/a&gt;&lt;/strong&gt; - Document processing and parsing&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="right"&gt;(&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/#readme-top"&gt;back to top&lt;/a&gt;)&lt;/p&gt; 
&lt;!-- MARKDOWN LINKS &amp; IMAGES --&gt; 
&lt;!-- https://www.markdownguide.org/basic-syntax/#reference-style-links --&gt;</description>
    </item>
    
    <item>
      <title>openai/tiktoken</title>
      <link>https://github.com/openai/tiktoken</link>
      <description>&lt;p&gt;tiktoken is a fast BPE tokeniser for use with OpenAI's models.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;⏳ tiktoken&lt;/h1&gt; 
&lt;p&gt;tiktoken is a fast &lt;a href="https://en.wikipedia.org/wiki/Byte_pair_encoding"&gt;BPE&lt;/a&gt; tokeniser for use with OpenAI's models.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import tiktoken
enc = tiktoken.get_encoding("o200k_base")
assert enc.decode(enc.encode("hello world")) == "hello world"

# To get the tokeniser corresponding to a specific model in the OpenAI API:
enc = tiktoken.encoding_for_model("gpt-4o")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The open source version of &lt;code&gt;tiktoken&lt;/code&gt; can be installed from &lt;a href="https://pypi.org/project/tiktoken"&gt;PyPI&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install tiktoken
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The tokeniser API is documented in &lt;code&gt;tiktoken/core.py&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Example code using &lt;code&gt;tiktoken&lt;/code&gt; can be found in the &lt;a href="https://github.com/openai/openai-cookbook/raw/main/examples/How_to_count_tokens_with_tiktoken.ipynb"&gt;OpenAI Cookbook&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Performance&lt;/h2&gt; 
&lt;p&gt;&lt;code&gt;tiktoken&lt;/code&gt; is between 3-6x faster than a comparable open source tokeniser:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/openai/tiktoken/main/perf.svg?sanitize=true" alt="image" /&gt;&lt;/p&gt; 
&lt;p&gt;Performance measured on 1GB of text using the GPT-2 tokeniser, using &lt;code&gt;GPT2TokenizerFast&lt;/code&gt; from &lt;code&gt;tokenizers==0.13.2&lt;/code&gt;, &lt;code&gt;transformers==4.24.0&lt;/code&gt; and &lt;code&gt;tiktoken==0.2.0&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Getting help&lt;/h2&gt; 
&lt;p&gt;Please post questions in the &lt;a href="https://github.com/openai/tiktoken/issues"&gt;issue tracker&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you work at OpenAI, make sure to check the internal documentation or feel free to contact @shantanu.&lt;/p&gt; 
&lt;h2&gt;What is BPE anyway?&lt;/h2&gt; 
&lt;p&gt;Language models don't see text like you and I, instead they see a sequence of numbers (known as tokens). Byte pair encoding (BPE) is a way of converting text into tokens. It has a couple desirable properties:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;It's reversible and lossless, so you can convert tokens back into the original text&lt;/li&gt; 
 &lt;li&gt;It works on arbitrary text, even text that is not in the tokeniser's training data&lt;/li&gt; 
 &lt;li&gt;It compresses the text: the token sequence is shorter than the bytes corresponding to the original text. On average, in practice, each token corresponds to about 4 bytes.&lt;/li&gt; 
 &lt;li&gt;It attempts to let the model see common subwords. For instance, "ing" is a common subword in English, so BPE encodings will often split "encoding" into tokens like "encod" and "ing" (instead of e.g. "enc" and "oding"). Because the model will then see the "ing" token again and again in different contexts, it helps models generalise and better understand grammar.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;code&gt;tiktoken&lt;/code&gt; contains an educational submodule that is friendlier if you want to learn more about the details of BPE, including code that helps visualise the BPE procedure:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from tiktoken._educational import *

# Train a BPE tokeniser on a small amount of text
enc = train_simple_encoding()

# Visualise how the GPT-4 encoder encodes text
enc = SimpleBytePairEncoding.from_tiktoken("cl100k_base")
enc.encode("hello world aaaaaaaaaaaa")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Extending tiktoken&lt;/h2&gt; 
&lt;p&gt;You may wish to extend &lt;code&gt;tiktoken&lt;/code&gt; to support new encodings. There are two ways to do this.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Create your &lt;code&gt;Encoding&lt;/code&gt; object exactly the way you want and simply pass it around.&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;cl100k_base = tiktoken.get_encoding("cl100k_base")

# In production, load the arguments directly instead of accessing private attributes
# See openai_public.py for examples of arguments for specific encodings
enc = tiktoken.Encoding(
    # If you're changing the set of special tokens, make sure to use a different name
    # It should be clear from the name what behaviour to expect.
    name="cl100k_im",
    pat_str=cl100k_base._pat_str,
    mergeable_ranks=cl100k_base._mergeable_ranks,
    special_tokens={
        **cl100k_base._special_tokens,
        "&amp;lt;|im_start|&amp;gt;": 100264,
        "&amp;lt;|im_end|&amp;gt;": 100265,
    }
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Use the &lt;code&gt;tiktoken_ext&lt;/code&gt; plugin mechanism to register your &lt;code&gt;Encoding&lt;/code&gt; objects with &lt;code&gt;tiktoken&lt;/code&gt;.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;This is only useful if you need &lt;code&gt;tiktoken.get_encoding&lt;/code&gt; to find your encoding, otherwise prefer option 1.&lt;/p&gt; 
&lt;p&gt;To do this, you'll need to create a namespace package under &lt;code&gt;tiktoken_ext&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Layout your project like this, making sure to omit the &lt;code&gt;tiktoken_ext/__init__.py&lt;/code&gt; file:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;my_tiktoken_extension
├── tiktoken_ext
│&amp;nbsp;&amp;nbsp; └── my_encodings.py
└── setup.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;code&gt;my_encodings.py&lt;/code&gt; should be a module that contains a variable named &lt;code&gt;ENCODING_CONSTRUCTORS&lt;/code&gt;. This is a dictionary from an encoding name to a function that takes no arguments and returns arguments that can be passed to &lt;code&gt;tiktoken.Encoding&lt;/code&gt; to construct that encoding. For an example, see &lt;code&gt;tiktoken_ext/openai_public.py&lt;/code&gt;. For precise details, see &lt;code&gt;tiktoken/registry.py&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Your &lt;code&gt;setup.py&lt;/code&gt; should look something like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from setuptools import setup, find_namespace_packages

setup(
    name="my_tiktoken_extension",
    packages=find_namespace_packages(include=['tiktoken_ext*']),
    install_requires=["tiktoken"],
    ...
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then simply &lt;code&gt;pip install ./my_tiktoken_extension&lt;/code&gt; and you should be able to use your custom encodings! Make sure &lt;strong&gt;not&lt;/strong&gt; to use an editable install.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>oraios/serena</title>
      <link>https://github.com/oraios/serena</link>
      <description>&lt;p&gt;A powerful coding agent toolkit providing semantic retrieval and editing capabilities (MCP server &amp; Agno integration)&lt;/p&gt;&lt;hr&gt;&lt;p align="center" style="text-align:center"&gt; &lt;img src="https://raw.githubusercontent.com/oraios/serena/main/resources/serena-logo.svg#gh-light-mode-only" style="width:500px" /&gt; &lt;img src="https://raw.githubusercontent.com/oraios/serena/main/resources/serena-logo-dark-mode.svg#gh-dark-mode-only" style="width:500px" /&gt; &lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span&gt;🚀&lt;/span&gt; Serena is a powerful &lt;strong&gt;coding agent toolkit&lt;/strong&gt; capable of turning an LLM into a fully-featured agent that works &lt;strong&gt;directly on your codebase&lt;/strong&gt;. Unlike most other tools, it is not tied to an LLM, framework or an interface, making it easy to use it in a variety of ways.&lt;/li&gt; 
 &lt;li&gt;&lt;span&gt;🔧&lt;/span&gt; Serena provides essential &lt;strong&gt;semantic code retrieval and editing tools&lt;/strong&gt; that are akin to an IDE's capabilities, extracting code entities at the symbol level and exploiting relational structure. When combined with an existing coding agent, these tools greatly enhance (token) efficiency.&lt;/li&gt; 
 &lt;li&gt;&lt;span&gt;🆓&lt;/span&gt; Serena is &lt;strong&gt;free &amp;amp; open-source&lt;/strong&gt;, enhancing the capabilities of LLMs you already have access to free of charge.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can think of Serena as providing IDE-like tools to your LLM/coding agent. With it, the agent no longer needs to read entire files, perform grep-like searches or string replacements to find and edit the right code. Instead, it can use code centered tools like &lt;code&gt;find_symbol&lt;/code&gt;, &lt;code&gt;find_referencing_symbols&lt;/code&gt; and &lt;code&gt;insert_after_symbol&lt;/code&gt;.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;em&gt;Serena is under active development! See the latest updates, upcoming features, and lessons learned to stay up to date.&lt;/em&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/CHANGELOG.md"&gt; &lt;img src="https://img.shields.io/badge/Updates-1e293b?style=flat&amp;amp;logo=rss&amp;amp;logoColor=white&amp;amp;labelColor=1e293b" alt="Changelog" /&gt; &lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/roadmap.md"&gt; &lt;img src="https://img.shields.io/badge/Roadmap-14532d?style=flat&amp;amp;logo=target&amp;amp;logoColor=white&amp;amp;labelColor=14532d" alt="Roadmap" /&gt; &lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/lessons_learned.md"&gt; &lt;img src="https://img.shields.io/badge/Lessons-Learned-7c4700?style=flat&amp;amp;logo=readthedocs&amp;amp;logoColor=white&amp;amp;labelColor=7c4700" alt="Lessons Learned" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h3&gt;LLM Integration&lt;/h3&gt; 
&lt;p&gt;Serena provides the necessary &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#list-of-tools"&gt;tools&lt;/a&gt; for coding workflows, but an LLM is required to do the actual work, orchestrating tool use.&lt;/p&gt; 
&lt;p&gt;For example, &lt;strong&gt;supercharge the performance of Claude Code&lt;/strong&gt; with a &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#claude-code"&gt;one-line shell command&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;In general, Serena can be integrated with an LLM in several ways:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;by using the &lt;strong&gt;model context protocol (MCP)&lt;/strong&gt;. Serena provides an MCP server which integrates with 
  &lt;ul&gt; 
   &lt;li&gt;Claude Code and Claude Desktop,&lt;/li&gt; 
   &lt;li&gt;Terminal-based clients like Codex, Gemini-CLI, Qwen3-Coder, rovodev, OpenHands CLI and others,&lt;/li&gt; 
   &lt;li&gt;IDEs like VSCode, Cursor or IntelliJ,&lt;/li&gt; 
   &lt;li&gt;Extensions like Cline or Roo Code&lt;/li&gt; 
   &lt;li&gt;Local clients like &lt;a href="https://docs.openwebui.com/openapi-servers/mcp"&gt;OpenWebUI&lt;/a&gt;, &lt;a href="https://jan.ai/docs/mcp-examples/browser/browserbase#enable-mcp"&gt;Jan&lt;/a&gt;, &lt;a href="https://docs.agno.com/introduction/playground"&gt;Agno&lt;/a&gt; and others&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;by using &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/docs/serena_on_chatgpt.md"&gt;mcpo to connect it to ChatGPT&lt;/a&gt; or other clients that don't support MCP but do support tool calling via OpenAPI.&lt;/li&gt; 
 &lt;li&gt;by incorporating Serena's tools into an agent framework of your choice, as illustrated &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/docs/custom_agent.md"&gt;here&lt;/a&gt;. Serena's tool implementation is decoupled from the framework-specific code and can thus easily be adapted to any agent framework.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Serena in Action&lt;/h3&gt; 
&lt;h4&gt;Demonstration 1: Efficient Operation in Claude Code&lt;/h4&gt; 
&lt;p&gt;A demonstration of Serena efficiently retrieving and editing code within Claude Code, thereby saving tokens and time. Efficient operations are not only useful for saving costs, but also for generally improving the generated code's quality. This effect may be less pronounced in very small projects, but often becomes of crucial importance in larger ones.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/ab78ebe0-f77d-43cc-879a-cc399efefd87"&gt;https://github.com/user-attachments/assets/ab78ebe0-f77d-43cc-879a-cc399efefd87&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;Demonstration 2: Serena in Claude Desktop&lt;/h4&gt; 
&lt;p&gt;A demonstration of Serena implementing a small feature for itself (a better log GUI) with Claude Desktop. Note how Serena's tools enable Claude to find and edit the right symbols.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/6eaa9aa1-610d-4723-a2d6-bf1e487ba753"&gt;https://github.com/user-attachments/assets/6eaa9aa1-610d-4723-a2d6-bf1e487ba753&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Programming Language Support &amp;amp; Semantic Analysis Capabilities&lt;/h3&gt; 
&lt;p&gt;Serena's semantic code analysis capabilities build on &lt;strong&gt;language servers&lt;/strong&gt; using the widely implemented language server protocol (LSP). The LSP provides a set of versatile code querying and editing functionalities based on symbolic understanding of the code. Equipped with these capabilities, Serena discovers and edits code just like a seasoned developer making use of an IDE's capabilities would. Serena can efficiently find the right context and do the right thing even in very large and complex projects! So not only is it free and open-source, it frequently achieves better results than existing solutions that charge a premium.&lt;/p&gt; 
&lt;p&gt;Language servers provide support for a wide range of programming languages. With Serena, we provide direct, out-of-the-box support for:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python&lt;/li&gt; 
 &lt;li&gt;TypeScript/Javascript&lt;/li&gt; 
 &lt;li&gt;PHP (uses Intelephense LSP; set &lt;code&gt;INTELEPHENSE_LICENSE_KEY&lt;/code&gt; environment variable for premium features)&lt;/li&gt; 
 &lt;li&gt;Go (requires installation of gopls)&lt;/li&gt; 
 &lt;li&gt;R (requires installation of the &lt;code&gt;languageserver&lt;/code&gt; R package)&lt;/li&gt; 
 &lt;li&gt;Rust (requires &lt;a href="https://rustup.rs/"&gt;rustup&lt;/a&gt; - uses rust-analyzer from your toolchain)&lt;/li&gt; 
 &lt;li&gt;C/C++ (you may experience issues with finding references, we are working on it)&lt;/li&gt; 
 &lt;li&gt;Zig (requires installation of ZLS - Zig Language Server)&lt;/li&gt; 
 &lt;li&gt;C#&lt;/li&gt; 
 &lt;li&gt;Ruby (by default, uses &lt;a href="https://github.com/Shopify/ruby-lsp"&gt;ruby-lsp&lt;/a&gt;, specify ruby_solargraph as your language to use the previous solargraph based implementation)&lt;/li&gt; 
 &lt;li&gt;Swift&lt;/li&gt; 
 &lt;li&gt;Kotlin (uses the pre-alpha &lt;a href="https://github.com/Kotlin/kotlin-lsp"&gt;official kotlin LS&lt;/a&gt;, some issues may appear)&lt;/li&gt; 
 &lt;li&gt;Java (&lt;em&gt;Note&lt;/em&gt;: startup is slow, initial startup especially so. There may be issues with java on macos and linux, we are working on it.)&lt;/li&gt; 
 &lt;li&gt;Clojure&lt;/li&gt; 
 &lt;li&gt;Dart&lt;/li&gt; 
 &lt;li&gt;Bash&lt;/li&gt; 
 &lt;li&gt;Lua (automatically downloads lua-language-server if not installed)&lt;/li&gt; 
 &lt;li&gt;Nix (requires nixd installation)&lt;/li&gt; 
 &lt;li&gt;Elixir (requires installation of NextLS and Elixir; &lt;strong&gt;Windows not supported&lt;/strong&gt;)&lt;/li&gt; 
 &lt;li&gt;Erlang (requires installation of beam and &lt;a href="https://github.com/erlang-ls/erlang_ls"&gt;erlang_ls&lt;/a&gt;, experimental, might be slow or hang)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Support for further languages can easily be added by providing a shallow adapter for a new language server implementation, see Serena's &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/.serena/memories/adding_new_language_support_guide.md"&gt;memory on that&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Community Feedback&lt;/h3&gt; 
&lt;p&gt;Most users report that Serena has strong positive effects on the results of their coding agents, even when used within very capable agents like Claude Code. Serena is often described to be a &lt;a href="https://www.reddit.com/r/ClaudeAI/comments/1lfsdll/try_out_serena_mcp_thank_me_later/"&gt;game changer&lt;/a&gt;, providing an enormous &lt;a href="https://www.reddit.com/r/ClaudeCode/comments/1mguoia/absolutely_insane_improvement_of_claude_code"&gt;productivity boost&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Serena excels at navigating and manipulating complex codebases, providing tools that support precise code retrieval and editing in the presence of large, strongly structured codebases. However, when dealing with tasks that involve only very few/small files, you may not benefit from including Serena on top of your existing coding agent. In particular, when writing code from scratch, Serena will not provide much value initially, as the more complex structures that Serena handles more gracefully than simplistic, file-based approaches are yet to be created.&lt;/p&gt; 
&lt;p&gt;Several videos and blog posts have talked about Serena:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;YouTube:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=wYWyJNs1HVk&amp;amp;t=1s"&gt;AI Labs&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=UqfxuQKuMo8&amp;amp;t=45s"&gt;Yo Van Eyck&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=fzPnM3ySmjE&amp;amp;t=32s"&gt;JeredBlu&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Blog posts:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://medium.com/@souradip1000/deconstructing-serenas-mcp-powered-semantic-code-understanding-architecture-75802515d116"&gt;Serena's Design Principles&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://blog.lai.so/serena/"&gt;Serena with Claude Code (in Japanese)&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://robertmarshall.dev/blog/turning-claude-code-into-a-development-powerhouse/"&gt;Turning Claude Code into a Development Powerhouse&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;!-- Created with markdown-toc -i README.md --&gt; 
&lt;!-- Install it with npm install -g markdown-toc --&gt; 
&lt;!-- toc --&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#quick-start"&gt;Quick Start&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#running-the-serena-mcp-server"&gt;Running the Serena MCP Server&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#usage"&gt;Usage&lt;/a&gt; 
      &lt;ul&gt; 
       &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#using-uvx"&gt;Using uvx&lt;/a&gt; 
        &lt;ul&gt; 
         &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#local-installation"&gt;Local Installation&lt;/a&gt;&lt;/li&gt; 
        &lt;/ul&gt; &lt;/li&gt; 
       &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#using-docker-experimental"&gt;Using Docker (Experimental)&lt;/a&gt;&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#sse-mode"&gt;SSE Mode&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#command-line-arguments"&gt;Command-Line Arguments&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#configuration"&gt;Configuration&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#project-activation--indexing"&gt;Project Activation &amp;amp; Indexing&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#claude-code"&gt;Claude Code&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#codex"&gt;Codex&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#other-terminal-based-clients"&gt;Other Terminal-Based Clients&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#claude-desktop"&gt;Claude Desktop&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#mcp-coding-clients-cline-roo-code-cursor-windsurf-etc"&gt;MCP Coding Clients (Cline, Roo-Code, Cursor, Windsurf, etc.)&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#local-guis-and-frameworks"&gt;Local GUIs and Frameworks&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#detailed-usage-and-recommendations"&gt;Detailed Usage and Recommendations&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#tool-execution"&gt;Tool Execution&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#shell-execution-and-editing-tools"&gt;Shell Execution and Editing Tools&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#modes-and-contexts"&gt;Modes and Contexts&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#contexts"&gt;Contexts&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#modes"&gt;Modes&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#customization"&gt;Customization&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#onboarding-and-memories"&gt;Onboarding and Memories&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#prepare-your-project"&gt;Prepare Your Project&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#structure-your-codebase"&gt;Structure Your Codebase&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#start-from-a-clean-state"&gt;Start from a Clean State&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#logging-linting-and-automated-tests"&gt;Logging, Linting, and Automated Tests&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#prompting-strategies"&gt;Prompting Strategies&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#potential-issues-in-code-editing"&gt;Potential Issues in Code Editing&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#running-out-of-context"&gt;Running Out of Context&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#combining-serena-with-other-mcp-servers"&gt;Combining Serena with Other MCP Servers&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#serenas-logs-the-dashboard-and-gui-tool"&gt;Serena's Logs: The Dashboard and GUI Tool&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#troubleshooting"&gt;Troubleshooting&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#comparison-with-other-coding-agents"&gt;Comparison with Other Coding Agents&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#subscription-based-coding-agents"&gt;Subscription-Based Coding Agents&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#api-based-coding-agents"&gt;API-Based Coding Agents&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#other-mcp-based-coding-agents"&gt;Other MCP-Based Coding Agents&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#acknowledgements"&gt;Acknowledgements&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#customizing-and-extending-serena"&gt;Customizing and Extending Serena&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#list-of-tools"&gt;List of Tools&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- tocstop --&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;Serena can be used in various ways, below you will find instructions for selected integrations.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;For coding with Claude, we recommend using Serena through &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#claude-code"&gt;Claude Code&lt;/a&gt; or &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#claude-desktop"&gt;Claude Desktop&lt;/a&gt;. You can also use Serena in most other &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#other-terminal-based-clients"&gt;terminal-based clients&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;If you want a GUI experience outside an IDE, you can use one of the many &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#local-guis-and-frameworks"&gt;local GUIs&lt;/a&gt; that support MCP servers. You can also connect Serena to many web clients (including ChatGPT) using &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/docs/serena_on_chatgpt.md"&gt;mcpo&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;If you want to use Serena integrated in your IDE, see the section on &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#other-mcp-clients---cline-roo-code-cursor-windsurf-etc"&gt;other MCP clients&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;You can use Serena as a library for building your own applications. We try to keep the public API stable, but you should still expect breaking changes and pin Serena to a fixed version if you use it as a dependency.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Serena is managed by &lt;code&gt;uv&lt;/code&gt;, so you will need to &lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;install it&lt;/a&gt;).&lt;/p&gt; 
&lt;h3&gt;Running the Serena MCP Server&lt;/h3&gt; 
&lt;p&gt;You have several options for running the MCP server, which are explained in the subsections below.&lt;/p&gt; 
&lt;h4&gt;Usage&lt;/h4&gt; 
&lt;p&gt;The typical usage involves the client (Claude Code, Claude Desktop, etc.) running the MCP server as a subprocess (using stdio communication), so the client needs to be provided with the command to run the MCP server. (Alternatively, you can run the MCP server in SSE mode and tell your client how to connect to it.)&lt;/p&gt; 
&lt;p&gt;Note that no matter how you run the MCP server, Serena will, by default, start a small web-based dashboard on localhost that will display logs and allow shutting down the MCP server (since many clients fail to clean up processes correctly). This and other settings can be adjusted in the &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#configuration"&gt;configuration&lt;/a&gt; and/or by providing &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#command-line-arguments"&gt;command-line arguments&lt;/a&gt;.&lt;/p&gt; 
&lt;h5&gt;Using uvx&lt;/h5&gt; 
&lt;p&gt;&lt;code&gt;uvx&lt;/code&gt; can be used to run the latest version of Serena directly from the repository, without an explicit local installation.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;uvx --from git+https://github.com/oraios/serena serena start-mcp-server
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Explore the CLI to see some of the customization options that serena provides (more info on them below).&lt;/p&gt; 
&lt;h6&gt;Local Installation&lt;/h6&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Clone the repository and change into it.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;git clone https://github.com/oraios/serena
cd serena
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Optionally edit the configuration file in your home directory with&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;uv run serena config edit
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you just want the default config, you can skip this part, and a config file will be created when you first run Serena.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run the server with &lt;code&gt;uv&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;uv run serena start-mcp-server
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;When running from outside the serena installation directory, be sure to pass it, i.e., use&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt; uv run --directory /abs/path/to/serena serena start-mcp-server
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h5&gt;Using Docker (Experimental)&lt;/h5&gt; 
&lt;p&gt;⚠️ Docker support is currently experimental with several limitations. Please read the &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/DOCKER.md"&gt;Docker documentation&lt;/a&gt; for important caveats before using it.&lt;/p&gt; 
&lt;p&gt;You can run the Serena MCP server directly via docker as follows, assuming that the projects you want to work on are all located in &lt;code&gt;/path/to/your/projects&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;docker run --rm -i --network host -v /path/to/your/projects:/workspaces/projects ghcr.io/oraios/serena:latest serena start-mcp-server --transport stdio
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Replace &lt;code&gt;/path/to/your/projects&lt;/code&gt; with the absolute path to your projects directory. The Docker approach provides:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Better security isolation for shell command execution&lt;/li&gt; 
 &lt;li&gt;No need to install language servers and dependencies locally&lt;/li&gt; 
 &lt;li&gt;Consistent environment across different systems&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Alternatively, use docker compose with the &lt;code&gt;compose.yml&lt;/code&gt; file provided in the repository.&lt;/p&gt; 
&lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/DOCKER.md"&gt;Docker documentation&lt;/a&gt; for detailed setup instructions, configuration options, and known limitations.&lt;/p&gt; 
&lt;h5&gt;Using Nix&lt;/h5&gt; 
&lt;p&gt;If you are using Nix and &lt;a href="https://nixos.wiki/wiki/flakes"&gt;have enabled the &lt;code&gt;nix-command&lt;/code&gt; and &lt;code&gt;flakes&lt;/code&gt; features&lt;/a&gt;, you can run Serena using the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;nix run github:oraios/serena -- start-mcp-server --transport stdio
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also install Serena by referencing this repo (&lt;code&gt;github:oraios/serena&lt;/code&gt;) and using it in your Nix flake. The package is exported as &lt;code&gt;serena&lt;/code&gt;.&lt;/p&gt; 
&lt;h4&gt;SSE Mode&lt;/h4&gt; 
&lt;p&gt;ℹ️ Note that MCP servers which use stdio as a protocol are somewhat unusual as far as client/server architectures go, as the server necessarily has to be started by the client in order for communication to take place via the server's standard input/output stream. In other words, you do not need to start the server yourself. The client application (e.g. Claude Desktop) takes care of this and therefore needs to be configured with a launch command.&lt;/p&gt; 
&lt;p&gt;When using instead the SSE mode, which uses HTTP-based communication, you control the server lifecycle yourself, i.e. you start the server and provide the client with the URL to connect to it.&lt;/p&gt; 
&lt;p&gt;Simply provide &lt;code&gt;start-mcp-server&lt;/code&gt; with the &lt;code&gt;--transport sse&lt;/code&gt; option and optionally provide the port. For example, to run the Serena MCP server in SSE mode on port 9121 using a local installation, you would run this command from the Serena directory,&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;uv run serena start-mcp-server --transport sse --port 9121
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;and then configure your client to connect to &lt;code&gt;http://localhost:9121/sse&lt;/code&gt;.&lt;/p&gt; 
&lt;h4&gt;Command-Line Arguments&lt;/h4&gt; 
&lt;p&gt;The Serena MCP server supports a wide range of additional command-line options, including the option to run in SSE mode and to adapt Serena to various &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#modes-and-contexts"&gt;contexts and modes of operation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Run with parameter &lt;code&gt;--help&lt;/code&gt; to get a list of available options.&lt;/p&gt; 
&lt;h3&gt;Configuration&lt;/h3&gt; 
&lt;p&gt;Serena is very flexible in terms of configuration. While for most users, the default configurations will work, you can fully adjust it to your needs by editing a few yaml files. You can disable tools, change Serena's instructions (what we denote as the &lt;code&gt;system_prompt&lt;/code&gt;), adjust the output of tools that just provide a prompt, and even adjust tool descriptions.&lt;/p&gt; 
&lt;p&gt;Serena is configured in four places:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;The &lt;code&gt;serena_config.yml&lt;/code&gt; for general settings that apply to all clients and projects. It is located in your user directory under &lt;code&gt;.serena/serena_config.yml&lt;/code&gt;. If you do not explicitly create the file, it will be auto-generated when you first run Serena. You can edit it directly or use&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;uvx --from git+https://github.com/oraios/serena serena config edit
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;(or use the &lt;code&gt;--directory&lt;/code&gt; command version).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;In the arguments passed to the &lt;code&gt;start-mcp-server&lt;/code&gt; in your client's config (see below), which will apply to all sessions started by the respective client. In particular, the &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#contexts"&gt;context&lt;/a&gt; parameter should be set appropriately for Serena to be best adjusted to existing tools and capabilities of your client. See for a detailed explanation. You can override all entries from the &lt;code&gt;serena_config.yml&lt;/code&gt; through command line arguments.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;In the &lt;code&gt;.serena/project.yml&lt;/code&gt; file within your project. This will hold project-level configuration that is used whenever that project is activated. This file will be autogenerated when you first use Serena on that project, but you can also generate it explicitly with&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;uvx --from git+https://github.com/oraios/serena serena project generate-yml
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;(or use the &lt;code&gt;--directory&lt;/code&gt; command version).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Through the context and modes. Explore the &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#modes-and-contexts"&gt;modes and contexts&lt;/a&gt; section for more details.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;After the initial setup, continue with one of the sections below, depending on how you want to use Serena.&lt;/p&gt; 
&lt;h3&gt;Project Activation &amp;amp; Indexing&lt;/h3&gt; 
&lt;p&gt;If you are mostly working with the same project, you can configure to always activate it at startup by passing &lt;code&gt;--project &amp;lt;path_or_name&amp;gt;&lt;/code&gt; to the &lt;code&gt;start-mcp-server&lt;/code&gt; command in your client's MCP config. This is especially useful for clients which configure MCP servers on a per-project basis, like Claude Code.&lt;/p&gt; 
&lt;p&gt;Otherwise, the recommended way is to just ask the LLM to activate a project by providing it an absolute path to, or, in case the project was activated in the past, by its name. The default project name is the directory name.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;"Activate the project /path/to/my_project"&lt;/li&gt; 
 &lt;li&gt;"Activate the project my_project"&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;All projects that have been activated will be automatically added to your &lt;code&gt;serena_config.yml&lt;/code&gt;, and for each project, the file &lt;code&gt;.serena/project.yml&lt;/code&gt; will be generated. You can adjust the latter, e.g., by changing the name (which you refer to during the activation) or other options. Make sure to not have two different projects with the same name.&lt;/p&gt; 
&lt;p&gt;ℹ️ For larger projects, we recommend that you index your project to accelerate Serena's tools; otherwise the first tool application may be very slow. To do so, run this from the project directory (or pass the path to the project as an argument):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;uvx --from git+https://github.com/oraios/serena serena project index
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;(or use the &lt;code&gt;--directory&lt;/code&gt; command version).&lt;/p&gt; 
&lt;h3&gt;Claude Code&lt;/h3&gt; 
&lt;p&gt;Serena is a great way to make Claude Code both cheaper and more powerful!&lt;/p&gt; 
&lt;p&gt;From your project directory, add serena with a command like this,&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;claude mcp add serena -- &amp;lt;serena-mcp-server&amp;gt; --context ide-assistant --project $(pwd)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;where &lt;code&gt;&amp;lt;serena-mcp-server&amp;gt;&lt;/code&gt; is your way of &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#running-the-serena-mcp-server"&gt;running the Serena MCP server&lt;/a&gt;. For example, when using &lt;code&gt;uvx&lt;/code&gt;, you would run&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;claude mcp add serena -- uvx --from git+https://github.com/oraios/serena serena start-mcp-server --context ide-assistant --project $(pwd)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ℹ️ Serena comes with an instruction text, and Claude needs to read it to properly use Serena's tools. As of version &lt;code&gt;v1.0.52&lt;/code&gt;, claude code reads the instructions of the MCP server, so this &lt;strong&gt;is handled automatically&lt;/strong&gt;. If you are using an older version, or if Claude fails to read the instructions, you can ask it explicitly to "read Serena's initial instructions" or run &lt;code&gt;/mcp__serena__initial_instructions&lt;/code&gt; to load the instruction text. If you want to make use of that, you will have to enable the corresponding tool explicitly by adding &lt;code&gt;initial_instructions&lt;/code&gt; to the &lt;code&gt;included_optional_tools&lt;/code&gt; in your config. Note that you may have to make Claude read the instructions when you start a new conversation and after any compacting operation to ensure Claude remains properly configured to use Serena's tools.&lt;/p&gt; 
&lt;h3&gt;Codex&lt;/h3&gt; 
&lt;p&gt;Serena works with OpenAI's Codex CLI out of the box, but you have to use the &lt;code&gt;codex&lt;/code&gt; context for it to work properly. (The technical reason is that Codex doesn't fully support the MCP specifications, so some massaging of tools is required.).&lt;/p&gt; 
&lt;p&gt;Unlike Claude Code, in Codex you add an MCP server globally and not per project. Add the following to &lt;code&gt;~/.codex/config.toml&lt;/code&gt; (create the file if it does not exist):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-toml"&gt;[mcp_servers.serena]
command = "uvx"
args = ["--from", "git+https://github.com/oraios/serena", "serena", "start-mcp-server", "--context", "codex"]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After codex has started, you need to activate the project, which you can do by saying:&lt;/p&gt; 
&lt;p&gt;"Activate the current dir as project using serena"&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;If you don't activate the project, you will not be able to use Serena's tools!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;That's it! Have a look at &lt;code&gt;~/.codex/log/codex-tui.log&lt;/code&gt; to see if any errors occurred.&lt;/p&gt; 
&lt;p&gt;The Serena dashboard will run if you have not disabled it in the configuration, but due to Codex's sandboxing the webbrowser may not open automatically. You can open it manually by going to &lt;code&gt;http://localhost:24282/dashboard/index.html&lt;/code&gt; (or a higher port, if that was already taken).&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Codex will often show the tools as &lt;code&gt;failed&lt;/code&gt; even though they are successfully executed. This is not a problem, seems to be a bug in Codex. Despite the error message, everything works as expected.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Other Terminal-Based Clients&lt;/h3&gt; 
&lt;p&gt;There are many terminal-based coding assistants that support MCP servers, such as &lt;a href="https://github.com/openai/codex?tab=readme-ov-file#model-context-protocol-mcp"&gt;Codex&lt;/a&gt;, &lt;a href="https://github.com/google-gemini/gemini-cli"&gt;Gemini-CLI&lt;/a&gt;, &lt;a href="https://github.com/QwenLM/Qwen3-Coder"&gt;Qwen3-Coder&lt;/a&gt;, &lt;a href="https://community.atlassian.com/forums/Rovo-for-Software-Teams-Beta/Introducing-Rovo-Dev-CLI-AI-Powered-Development-in-your-terminal/ba-p/3043623"&gt;rovodev&lt;/a&gt;, the &lt;a href="https://docs.all-hands.dev/usage/how-to/cli-mode"&gt;OpenHands CLI&lt;/a&gt; and &lt;a href="https://github.com/sst/opencode"&gt;opencode&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;They generally benefit from the symbolic tools provided by Serena. You might want to customize some aspects of Serena by writing your own context, modes or prompts to adjust it to your workflow, to other MCP servers you are using, and to the client's internal capabilities.&lt;/p&gt; 
&lt;h3&gt;Claude Desktop&lt;/h3&gt; 
&lt;p&gt;For &lt;a href="https://claude.ai/download"&gt;Claude Desktop&lt;/a&gt; (available for Windows and macOS), go to File / Settings / Developer / MCP Servers / Edit Config, which will let you open the JSON file &lt;code&gt;claude_desktop_config.json&lt;/code&gt;. Add the &lt;code&gt;serena&lt;/code&gt; MCP server configuration, using a &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#running-the-serena-mcp-server"&gt;run command&lt;/a&gt; depending on your setup.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;local installation:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-json"&gt;{
    "mcpServers": {
        "serena": {
            "command": "/abs/path/to/uv",
            "args": ["run", "--directory", "/abs/path/to/serena", "serena", "start-mcp-server"]
        }
    }
}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;uvx:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-json"&gt;{
    "mcpServers": {
        "serena": {
            "command": "/abs/path/to/uvx",
            "args": ["--from", "git+https://github.com/oraios/serena", "serena", "start-mcp-server"]
        }
    }
}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;docker:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-json"&gt; {
     "mcpServers": {
         "serena": {
             "command": "docker",
             "args": ["run", "--rm", "-i", "--network", "host", "-v", "/path/to/your/projects:/workspaces/projects", "ghcr.io/oraios/serena:latest", "serena", "start-mcp-server", "--transport", "stdio"]
         }
     }
 }
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If you are using paths containing backslashes for paths on Windows (note that you can also just use forward slashes), be sure to escape them correctly (&lt;code&gt;\\&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;That's it! Save the config and then restart Claude Desktop. You are ready for activating your first project.&lt;/p&gt; 
&lt;p&gt;ℹ️ You can further customize the run command using additional arguments (see &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#command-line-arguments"&gt;above&lt;/a&gt;).&lt;/p&gt; 
&lt;p&gt;Note: on Windows and macOS there are official Claude Desktop applications by Anthropic, for Linux there is an &lt;a href="https://github.com/aaddrick/claude-desktop-debian"&gt;open-source community version&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;⚠️ Be sure to fully quit the Claude Desktop application, as closing Claude will just minimize it to the system tray – at least on Windows.&lt;/p&gt; 
&lt;p&gt;⚠️ Some clients may leave behind zombie processes. You will have to find and terminate them manually then. With Serena, you can activate the &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#serenas-logs-the-dashboard-and-gui-tool"&gt;dashboard&lt;/a&gt; to prevent unnoted processes and also use the dashboard for shutting down Serena.&lt;/p&gt; 
&lt;p&gt;After restarting, you should see Serena's tools in your chat interface (notice the small hammer icon).&lt;/p&gt; 
&lt;p&gt;For more information on MCP servers with Claude Desktop, see &lt;a href="https://modelcontextprotocol.io/quickstart/user"&gt;the official quick start guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;MCP Coding Clients (Cline, Roo-Code, Cursor, Windsurf, etc.)&lt;/h3&gt; 
&lt;p&gt;Being an MCP Server, Serena can be included in any MCP Client. The same configuration as above, perhaps with small client-specific modifications, should work. Most of the popular existing coding assistants (IDE extensions or VSCode-like IDEs) support connections to MCP Servers. It is &lt;strong&gt;recommended to use the &lt;code&gt;ide-assistant&lt;/code&gt; context&lt;/strong&gt; for these integrations by adding &lt;code&gt;"--context", "ide-assistant"&lt;/code&gt; to the &lt;code&gt;args&lt;/code&gt; in your MCP client's configuration. Including Serena generally boosts their performance by providing them tools for symbolic operations.&lt;/p&gt; 
&lt;p&gt;In this case, the billing for the usage continues to be controlled by the client of your choice (unlike with the Claude Desktop client). But you may still want to use Serena through such an approach, e.g., for one of the following reasons:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;You are already using a coding assistant (say Cline or Cursor) and just want to make it more powerful.&lt;/li&gt; 
 &lt;li&gt;You are on Linux and don't want to use the &lt;a href="https://github.com/aaddrick/claude-desktop-debian"&gt;community-created Claude Desktop&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;You want tighter integration of Serena into your IDE and don't mind paying for that.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Local GUIs and Frameworks&lt;/h3&gt; 
&lt;p&gt;Over the last months, several technologies have emerged that allow you to run a powerful local GUI and connect it to an MCP server. They will work with Serena out of the box. Some of the leading open source GUI technologies offering this are &lt;a href="https://jan.ai/docs/mcp"&gt;Jan&lt;/a&gt;, &lt;a href="https://github.com/All-Hands-AI/OpenHands/"&gt;OpenHands&lt;/a&gt;, &lt;a href="https://docs.openwebui.com/openapi-servers/mcp"&gt;OpenWebUI&lt;/a&gt; and &lt;a href="https://docs.agno.com/introduction/playground"&gt;Agno&lt;/a&gt;. They allow combining Serena with almost any LLM (including locally running ones) and offer various other integrations.&lt;/p&gt; 
&lt;h2&gt;Detailed Usage and Recommendations&lt;/h2&gt; 
&lt;h3&gt;Tool Execution&lt;/h3&gt; 
&lt;p&gt;Serena combines tools for semantic code retrieval with editing capabilities and shell execution. Serena's behavior can be further customized through &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#modes-and-contexts"&gt;Modes and Contexts&lt;/a&gt;. Find the complete list of tools &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/#full-list-of-tools"&gt;below&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The use of all tools is generally recommended, as this allows Serena to provide the most value: Only by executing shell commands (in particular, tests) can Serena identify and correct mistakes autonomously.&lt;/p&gt; 
&lt;h4&gt;Shell Execution and Editing Tools&lt;/h4&gt; 
&lt;p&gt;However, it should be noted that the &lt;code&gt;execute_shell_command&lt;/code&gt; tool allows for arbitrary code execution. When using Serena as an MCP Server, clients will typically ask the user for permission before executing a tool, so as long as the user inspects execution parameters beforehand, this should not be a problem. However, if you have concerns, you can choose to disable certain commands in your project's .yml configuration file. If you only want to use Serena purely for analyzing code and suggesting implementations without modifying the codebase, you can enable read-only mode by setting &lt;code&gt;read_only: true&lt;/code&gt; in your project configuration file. This will automatically disable all editing tools and prevent any modifications to your codebase while still allowing all analysis and exploration capabilities.&lt;/p&gt; 
&lt;p&gt;In general, be sure to back up your work and use a version control system in order to avoid losing any work.&lt;/p&gt; 
&lt;h3&gt;Modes and Contexts&lt;/h3&gt; 
&lt;p&gt;Serena's behavior and toolset can be adjusted using contexts and modes. These allow for a high degree of customization to best suit your workflow and the environment Serena is operating in.&lt;/p&gt; 
&lt;h4&gt;Contexts&lt;/h4&gt; 
&lt;p&gt;A context defines the general environment in which Serena is operating. It influences the initial system prompt and the set of available tools. A context is set at startup when launching Serena (e.g., via CLI options for an MCP server or in the agent script) and cannot be changed during an active session.&lt;/p&gt; 
&lt;p&gt;Serena comes with pre-defined contexts:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;desktop-app&lt;/code&gt;: Tailored for use with desktop applications like Claude Desktop. This is the default.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;agent&lt;/code&gt;: Designed for scenarios where Serena acts as a more autonomous agent, for example, when used with Agno.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ide-assistant&lt;/code&gt;: Optimized for integration into IDEs like VSCode, Cursor, or Cline, focusing on in-editor coding assistance. Choose the context that best matches the type of integration you are using.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;When launching Serena, specify the context using &lt;code&gt;--context &amp;lt;context-name&amp;gt;&lt;/code&gt;. Note that for cases where parameter lists are specified (e.g. Claude Desktop), you must add two parameters to the list.&lt;/p&gt; 
&lt;p&gt;If you are using a local server (such as Llama.cpp) which requires you to use OpenAI-compatible tool descriptions, use context &lt;code&gt;oaicompat-agent&lt;/code&gt; instead of &lt;code&gt;agent&lt;/code&gt;.&lt;/p&gt; 
&lt;h4&gt;Modes&lt;/h4&gt; 
&lt;p&gt;Modes further refine Serena's behavior for specific types of tasks or interaction styles. Multiple modes can be active simultaneously, allowing you to combine their effects. Modes influence the system prompt and can also alter the set of available tools by excluding certain ones.&lt;/p&gt; 
&lt;p&gt;Examples of built-in modes include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;planning&lt;/code&gt;: Focuses Serena on planning and analysis tasks.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;editing&lt;/code&gt;: Optimizes Serena for direct code modification tasks.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;interactive&lt;/code&gt;: Suitable for a conversational, back-and-forth interaction style.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;one-shot&lt;/code&gt;: Configures Serena for tasks that should be completed in a single response, often used with &lt;code&gt;planning&lt;/code&gt; for generating reports or initial plans.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;no-onboarding&lt;/code&gt;: Skips the initial onboarding process if it's not needed for a particular session.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;onboarding&lt;/code&gt;: (Usually triggered automatically) Focuses on the project onboarding process.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Modes can be set at startup (similar to contexts) but can also be &lt;em&gt;switched dynamically&lt;/em&gt; during a session. You can instruct the LLM to use the &lt;code&gt;switch_modes&lt;/code&gt; tool to activate a different set of modes (e.g., "switch to planning and one-shot modes").&lt;/p&gt; 
&lt;p&gt;When launching Serena, specify modes using &lt;code&gt;--mode &amp;lt;mode-name&amp;gt;&lt;/code&gt;; multiple modes can be specified, e.g. &lt;code&gt;--mode planning --mode no-onboarding&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;⚠&lt;/span&gt; &lt;strong&gt;Mode Compatibility&lt;/strong&gt;: While you can combine modes, some may be semantically incompatible (e.g., &lt;code&gt;interactive&lt;/code&gt; and &lt;code&gt;one-shot&lt;/code&gt;). Serena currently does not prevent incompatible combinations; it is up to the user to choose sensible mode configurations.&lt;/p&gt; 
&lt;h4&gt;Customization&lt;/h4&gt; 
&lt;p&gt;You can create your own contexts and modes to precisely tailor Serena to your needs in two ways:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;You can use Serena's CLI to manage modes and contexts. Check out&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;uvx --from git+https://github.com/oraios/serena serena mode --help
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;and&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;uvx --from git+https://github.com/oraios/serena serena context --help
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;NOTE&lt;/em&gt;: Custom contexts/modes are simply YAML files in &lt;code&gt;&amp;lt;home&amp;gt;/.serena&lt;/code&gt;, they are automatically registered and available for use by their name (filename without the &lt;code&gt;.yml&lt;/code&gt; extension). If you don't want to use Serena's CLI, you can create and manage them in any way you see fit.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Using external YAML files&lt;/strong&gt;: When starting Serena, you can also provide an absolute path to a custom &lt;code&gt;.yml&lt;/code&gt; file for a context or mode.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This customization allows for deep integration and adaptation of Serena to specific project requirements or personal preferences.&lt;/p&gt; 
&lt;h3&gt;Onboarding and Memories&lt;/h3&gt; 
&lt;p&gt;By default, Serena will perform an &lt;strong&gt;onboarding process&lt;/strong&gt; when it is started for the first time for a project. The goal of the onboarding is for Serena to get familiar with the project and to store memories, which it can then draw upon in future interactions. If an LLM should fail to complete the onboarding and does not actually write the respective memories to disk, you may need to ask it to do so explicitly.&lt;/p&gt; 
&lt;p&gt;The onboarding will usually read a lot of content from the project, thus filling up the context. It can therefore be advisable to switch to another conversation once the onboarding is complete. After the onboarding, we recommend that you have a quick look at the memories and, if necessary, edit them or add additional ones.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Memories&lt;/strong&gt; are files stored in &lt;code&gt;.serena/memories/&lt;/code&gt; in the project directory, which the agent can choose to read in subsequent interactions. Feel free to read and adjust them as needed; you can also add new ones manually. Every file in the &lt;code&gt;.serena/memories/&lt;/code&gt; directory is a memory file. Whenever Serena starts working on a project, the list of memories is provided, and the agent can decide to read them. We found that memories can significantly improve the user experience with Serena.&lt;/p&gt; 
&lt;h3&gt;Prepare Your Project&lt;/h3&gt; 
&lt;h4&gt;Structure Your Codebase&lt;/h4&gt; 
&lt;p&gt;Serena uses the code structure for finding, reading and editing code. This means that it will work well with well-structured code but may perform poorly on fully unstructured one (like a "God class" with enormous, non-modular functions). Furthermore, for languages that are not statically typed, type annotations are highly beneficial.&lt;/p&gt; 
&lt;h4&gt;Start from a Clean State&lt;/h4&gt; 
&lt;p&gt;It is best to start a code generation task from a clean git state. Not only will this make it easier for you to inspect the changes, but also the model itself will have a chance of seeing what it has changed by calling &lt;code&gt;git diff&lt;/code&gt; and thereby correct itself or continue working in a followup conversation if needed.&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;⚠&lt;/span&gt; &lt;strong&gt;Important&lt;/strong&gt;: since Serena will write to files using the system-native line endings and it might want to look at the git diff, it is important to set &lt;code&gt;git config core.autocrlf&lt;/code&gt; to &lt;code&gt;true&lt;/code&gt; on Windows. With &lt;code&gt;git config core.autocrlf&lt;/code&gt; set to &lt;code&gt;false&lt;/code&gt; on Windows, you may end up with huge diffs only due to line endings. It is generally a good idea to globally enable this git setting on Windows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;git config --global core.autocrlf true
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Logging, Linting, and Automated Tests&lt;/h4&gt; 
&lt;p&gt;Serena can successfully complete tasks in an &lt;em&gt;agent loop&lt;/em&gt;, where it iteratively acquires information, performs actions, and reflects on the results. However, Serena cannot use a debugger; it must rely on the results of program executions, linting results, and test results to assess the correctness of its actions. Therefore, software that is designed to meaningful interpretable outputs (e.g. log messages) and that has a good test coverage is much easier to work with for Serena.&lt;/p&gt; 
&lt;p&gt;We generally recommend to start an editing task from a state where all linting checks and tests pass.&lt;/p&gt; 
&lt;h3&gt;Prompting Strategies&lt;/h3&gt; 
&lt;p&gt;We found that it is often a good idea to spend some time conceptualizing and planning a task before actually implementing it, especially for non-trivial task. This helps both in achieving better results and in increasing the feeling of control and staying in the loop. You can make a detailed plan in one session, where Serena may read a lot of your code to build up the context, and then continue with the implementation in another (potentially after creating suitable memories).&lt;/p&gt; 
&lt;h3&gt;Potential Issues in Code Editing&lt;/h3&gt; 
&lt;p&gt;In our experience, LLMs are bad at counting, i.e. they have problems inserting blocks of code in the right place. Most editing operations can be performed at the symbolic level, allowing this problem is overcome. However, sometimes, line-level insertions are useful.&lt;/p&gt; 
&lt;p&gt;Serena is instructed to double-check the line numbers and any code blocks that it will edit, but you may find it useful to explicitly tell it how to edit code if you run into problems. We are working on making Serena's editing capabilities more robust.&lt;/p&gt; 
&lt;h3&gt;Running Out of Context&lt;/h3&gt; 
&lt;p&gt;For long and complicated tasks, or tasks where Serena has read a lot of content, you may come close to the limits of context tokens. In that case, it is often a good idea to continue in a new conversation. Serena has a dedicated tool to create a summary of the current state of the progress and all relevant info for continuing it. You can request to create this summary and write it to a memory. Then, in a new conversation, you can just ask Serena to read the memory and continue with the task. In our experience, this worked really well. On the up-side, since in a single session there is no summarization involved, Serena does not usually get lost (unlike some other agents that summarize under the hood), and it is also instructed to occasionally check whether it's on the right track.&lt;/p&gt; 
&lt;p&gt;Moreover, Serena is instructed to be frugal with context (e.g., to not read bodies of code symbols unnecessarily), but we found that Claude is not always very good in being frugal (Gemini seemed better at it). You can explicitly instruct it to not read the bodies if you know that it's not needed.&lt;/p&gt; 
&lt;h3&gt;Combining Serena with Other MCP Servers&lt;/h3&gt; 
&lt;p&gt;When using Serena through an MCP Client, you can use it together with other MCP servers. However, beware of tool name collisions! See info on that above.&lt;/p&gt; 
&lt;p&gt;Currently, there is a collision with the popular Filesystem MCP Server. Since Serena also provides filesystem operations, there is likely no need to ever enable these two simultaneously.&lt;/p&gt; 
&lt;h3&gt;Serena's Logs: The Dashboard and GUI Tool&lt;/h3&gt; 
&lt;p&gt;Serena provides two convenient ways of accessing the logs of the current session:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;via the &lt;strong&gt;web-based dashboard&lt;/strong&gt; (enabled by default)&lt;/p&gt; &lt;p&gt;This is supported on all platforms. By default, it will be accessible at &lt;code&gt;http://localhost:24282/dashboard/index.html&lt;/code&gt;, but a higher port may be used if the default port is unavailable/multiple instances are running.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;via the &lt;strong&gt;GUI tool&lt;/strong&gt; (disabled by default)&lt;/p&gt; &lt;p&gt;This is mainly supported on Windows, but it may also work on Linux; macOS is unsupported.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Both can be enabled, configured or disabled in Serena's configuration file (&lt;code&gt;serena_config.yml&lt;/code&gt;, see above). If enabled, they will automatically be opened as soon as the Serena agent/MCP server is started. The web dashboard will display usage statistics of Serena's tools if you set &lt;code&gt;record_tool_usage_stats: True&lt;/code&gt; in your config.&lt;/p&gt; 
&lt;p&gt;In addition to viewing logs, both tools allow to shut down the Serena agent. This function is provided, because clients like Claude Desktop may fail to terminate the MCP server subprocess when they themselves are closed.&lt;/p&gt; 
&lt;h3&gt;Troubleshooting&lt;/h3&gt; 
&lt;p&gt;Support for MCP Servers in Claude Desktop and the various MCP Server SDKs are relatively new developments and may display instabilities.&lt;/p&gt; 
&lt;p&gt;The working configuration of an MCP server may vary from platform to platform and from client to client. We recommend always using absolute paths, as relative paths may be sources of errors. The language server is running in a separate sub-process and is called with asyncio – sometimes a client may make it crash. If you have Serena's log window enabled, and it disappears, you'll know what happened.&lt;/p&gt; 
&lt;p&gt;Some clients may not properly terminate MCP servers, look out for hanging python processes and terminate them manually, if needed.&lt;/p&gt; 
&lt;h2&gt;Comparison with Other Coding Agents&lt;/h2&gt; 
&lt;p&gt;To our knowledge, Serena is the first fully-featured coding agent where the entire functionality is available through an MCP server, thus not requiring API keys or subscriptions.&lt;/p&gt; 
&lt;h3&gt;Subscription-Based Coding Agents&lt;/h3&gt; 
&lt;p&gt;Many prominent subscription-based coding agents are parts of IDEs like Windsurf, Cursor and VSCode. Serena's functionality is similar to Cursor's Agent, Windsurf's Cascade or VSCode's agent mode.&lt;/p&gt; 
&lt;p&gt;Serena has the advantage of not requiring a subscription. A potential disadvantage is that it is not directly integrated into an IDE, so the inspection of newly written code is not as seamless.&lt;/p&gt; 
&lt;p&gt;More technical differences are:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Serena is not bound to a specific IDE or CLI. Serena's MCP server can be used with any MCP client (including some IDEs), and the Agno-based agent provides additional ways of applying its functionality.&lt;/li&gt; 
 &lt;li&gt;Serena is not bound to a specific large language model or API.&lt;/li&gt; 
 &lt;li&gt;Serena navigates and edits code using a language server, so it has a symbolic understanding of the code. IDE-based tools often use a RAG-based or purely text-based approach, which is often less powerful, especially for large codebases.&lt;/li&gt; 
 &lt;li&gt;Serena is open-source and has a small codebase, so it can be easily extended and modified.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;API-Based Coding Agents&lt;/h3&gt; 
&lt;p&gt;An alternative to subscription-based agents are API-based agents like Claude Code, Cline, Aider, Roo Code and others, where the usage costs map directly to the API costs of the underlying LLM. Some of them (like Cline) can even be included in IDEs as an extension. They are often very powerful and their main downside are the (potentially very high) API costs.&lt;/p&gt; 
&lt;p&gt;Serena itself can be used as an API-based agent (see the section on Agno above). We have not yet written a CLI tool or a dedicated IDE extension for Serena (and there is probably no need for the latter, as Serena can already be used with any IDE that supports MCP servers). If there is demand for a Serena as a CLI tool like Claude Code, we will consider writing one.&lt;/p&gt; 
&lt;p&gt;The main difference between Serena and other API-based agents is that Serena can also be used as an MCP server, thus not requiring an API key and bypassing the API costs. This is a unique feature of Serena.&lt;/p&gt; 
&lt;h3&gt;Other MCP-Based Coding Agents&lt;/h3&gt; 
&lt;p&gt;There are other MCP servers designed for coding, like &lt;a href="https://github.com/wonderwhy-er/DesktopCommanderMCP"&gt;DesktopCommander&lt;/a&gt; and &lt;a href="https://github.com/ezyang/codemcp"&gt;codemcp&lt;/a&gt;. However, to the best of our knowledge, none of them provide semantic code retrieval and editing tools; they rely purely on text-based analysis. It is the integration of language servers and the MCP that makes Serena unique and so powerful for challenging coding tasks, especially in the context of larger codebases.&lt;/p&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;p&gt;We built Serena on top of multiple existing open-source technologies, the most important ones being:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://github.com/microsoft/multilspy"&gt;multilspy&lt;/a&gt;. A library which wraps language server implementations and adapts them for interaction via Python and which provided the basis for our library Solid-LSP (src/solidlsp). Solid-LSP provides pure synchronous LSP calls and extends the original library with the symbolic logic that Serena required.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/modelcontextprotocol/python-sdk"&gt;Python MCP SDK&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/agno-agi/agno"&gt;Agno&lt;/a&gt; and the associated &lt;a href="https://github.com/agno-agi/agent-ui"&gt;agent-ui&lt;/a&gt;, which we use to allow Serena to work with any model, beyond the ones supporting the MCP.&lt;/li&gt; 
 &lt;li&gt;All the language servers that we use through Solid-LSP.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Without these projects, Serena would not have been possible (or would have been significantly more difficult to build).&lt;/p&gt; 
&lt;h2&gt;Customizing and Extending Serena&lt;/h2&gt; 
&lt;p&gt;It is straightforward to extend Serena's AI functionality with your own ideas. Simply implement a new tool by subclassing &lt;code&gt;serena.agent.Tool&lt;/code&gt; and implement the &lt;code&gt;apply&lt;/code&gt; method with a signature that matches the tool's requirements. Once implemented, &lt;code&gt;SerenaAgent&lt;/code&gt; will automatically have access to the new tool.&lt;/p&gt; 
&lt;p&gt;It is also relatively straightforward to add &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/.serena/memories/adding_new_language_support_guide.md"&gt;support for a new programming language&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We look forward to seeing what the community will come up with! For details on contributing, see &lt;a href="https://raw.githubusercontent.com/oraios/serena/main/CONTRIBUTING.md"&gt;contributing guidelines&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;List of Tools&lt;/h2&gt; 
&lt;p&gt;Here is the list of Serena's default tools with a short description (output of &lt;code&gt;uv run serena tools list&lt;/code&gt;):&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;activate_project&lt;/code&gt;: Activates a project by name.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;check_onboarding_performed&lt;/code&gt;: Checks whether project onboarding was already performed.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;create_text_file&lt;/code&gt;: Creates/overwrites a file in the project directory.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;delete_memory&lt;/code&gt;: Deletes a memory from Serena's project-specific memory store.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;execute_shell_command&lt;/code&gt;: Executes a shell command.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;find_file&lt;/code&gt;: Finds files in the given relative paths&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;find_referencing_symbols&lt;/code&gt;: Finds symbols that reference the symbol at the given location (optionally filtered by type).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;find_symbol&lt;/code&gt;: Performs a global (or local) search for symbols with/containing a given name/substring (optionally filtered by type).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;get_symbols_overview&lt;/code&gt;: Gets an overview of the top-level symbols defined in a given file.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;insert_after_symbol&lt;/code&gt;: Inserts content after the end of the definition of a given symbol.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;insert_before_symbol&lt;/code&gt;: Inserts content before the beginning of the definition of a given symbol.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;list_dir&lt;/code&gt;: Lists files and directories in the given directory (optionally with recursion).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;list_memories&lt;/code&gt;: Lists memories in Serena's project-specific memory store.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;onboarding&lt;/code&gt;: Performs onboarding (identifying the project structure and essential tasks, e.g. for testing or building).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;prepare_for_new_conversation&lt;/code&gt;: Provides instructions for preparing for a new conversation (in order to continue with the necessary context).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;read_file&lt;/code&gt;: Reads a file within the project directory.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;read_memory&lt;/code&gt;: Reads the memory with the given name from Serena's project-specific memory store.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;replace_regex&lt;/code&gt;: Replaces content in a file by using regular expressions.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;replace_symbol_body&lt;/code&gt;: Replaces the full definition of a symbol.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;search_for_pattern&lt;/code&gt;: Performs a search for a pattern in the project.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;think_about_collected_information&lt;/code&gt;: Thinking tool for pondering the completeness of collected information.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;think_about_task_adherence&lt;/code&gt;: Thinking tool for determining whether the agent is still on track with the current task.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;think_about_whether_you_are_done&lt;/code&gt;: Thinking tool for determining whether the task is truly completed.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;write_memory&lt;/code&gt;: Writes a named memory (for future reference) to Serena's project-specific memory store.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;There are several tools that are disabled by default, and have to be enabled explicitly, e.g., through the context or modes. Note that several of our default contexts do enable some of these tools. For example, the &lt;code&gt;desktop-app&lt;/code&gt; context enables the &lt;code&gt;execute_shell_command&lt;/code&gt; tool.&lt;/p&gt; 
&lt;p&gt;The full list of optional tools is (output of &lt;code&gt;uv run serena tools list --only-optional&lt;/code&gt;):&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;delete_lines&lt;/code&gt;: Deletes a range of lines within a file.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;get_current_config&lt;/code&gt;: Prints the current configuration of the agent, including the active and available projects, tools, contexts, and modes.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;initial_instructions&lt;/code&gt;: Gets the initial instructions for the current project. Should only be used in settings where the system prompt cannot be set, e.g. in clients you have no control over, like Claude Desktop.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;insert_at_line&lt;/code&gt;: Inserts content at a given line in a file.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;jet_brains_find_referencing_symbols&lt;/code&gt;: Finds symbols that reference the given symbol&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;jet_brains_find_symbol&lt;/code&gt;: Performs a global (or local) search for symbols with/containing a given name/substring (optionally filtered by type).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;jet_brains_get_symbols_overview&lt;/code&gt;: Retrieves an overview of the top-level symbols within a specified file&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;remove_project&lt;/code&gt;: Removes a project from the Serena configuration.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;replace_lines&lt;/code&gt;: Replaces a range of lines within a file with new content.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;restart_language_server&lt;/code&gt;: Restarts the language server, may be necessary when edits not through Serena happen.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;summarize_changes&lt;/code&gt;: Provides instructions for summarizing the changes made to the codebase.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;switch_modes&lt;/code&gt;: Activates modes by providing a list of their names&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>tadata-org/fastapi_mcp</title>
      <link>https://github.com/tadata-org/fastapi_mcp</link>
      <description>&lt;p&gt;Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt;&lt;a href="https://github.com/tadata-org/fastapi_mcp"&gt;&lt;img src="https://github.com/user-attachments/assets/7e44e98b-a0ba-4aff-a68a-4ffee3a6189c" alt="fastapi-to-mcp" height="100/" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;span style="font-size: 0.85em; font-weight: normal;"&gt;Built by &lt;a href="https://tadata.com"&gt;Tadata&lt;/a&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;h1 align="center"&gt; FastAPI-MCP &lt;/h1&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://trendshift.io/repositories/14064" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/14064" alt="tadata-org%2Ffastapi_mcp | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;p align="center"&gt;Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://pypi.org/project/fastapi-mcp/"&gt;&lt;img src="https://img.shields.io/pypi/v/fastapi-mcp?color=%2334D058&amp;amp;label=pypi%20package" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/fastapi-mcp/"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/fastapi-mcp.svg?sanitize=true" alt="Python Versions" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/tadata-org/fastapi_mcp/main/#"&gt;&lt;img src="https://img.shields.io/badge/FastAPI-009485.svg?logo=fastapi&amp;amp;logoColor=white" alt="FastAPI" /&gt;&lt;/a&gt; &lt;a href="https://github.com/tadata-org/fastapi_mcp/actions/workflows/ci.yml"&gt;&lt;img src="https://github.com/tadata-org/fastapi_mcp/actions/workflows/ci.yml/badge.svg?sanitize=true" alt="CI" /&gt;&lt;/a&gt; &lt;a href="https://codecov.io/gh/tadata-org/fastapi_mcp"&gt;&lt;img src="https://codecov.io/gh/tadata-org/fastapi_mcp/branch/main/graph/badge.svg?sanitize=true" alt="Coverage" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p align="center"&gt;&lt;a href="https://github.com/tadata-org/fastapi_mcp"&gt;&lt;img src="https://github.com/user-attachments/assets/b205adc6-28c0-4e3c-a68b-9c1a80eb7d0c" alt="fastapi-mcp-usage" height="400" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Authentication&lt;/strong&gt; built in, using your existing FastAPI dependencies!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;FastAPI-native:&lt;/strong&gt; Not just another OpenAPI -&amp;gt; MCP converter&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Zero/Minimal configuration&lt;/strong&gt; required - just point it at your FastAPI app and it works&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Preserving schemas&lt;/strong&gt; of your request models and response models&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Preserve documentation&lt;/strong&gt; of all your endpoints, just as it is in Swagger&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Flexible deployment&lt;/strong&gt; - Mount your MCP server to the same app, or deploy separately&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ASGI transport&lt;/strong&gt; - Uses FastAPI's ASGI interface directly for efficient communication&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Hosted Solution&lt;/h2&gt; 
&lt;p&gt;If you prefer a managed hosted solution check out &lt;a href="https://tadata.com"&gt;tadata.com&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;We recommend using &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt;, a fast Python package installer:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv add fastapi-mcp
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, you can install with pip:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install fastapi-mcp
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Basic Usage&lt;/h2&gt; 
&lt;p&gt;The simplest way to use FastAPI-MCP is to add an MCP server directly to your FastAPI application:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from fastapi import FastAPI
from fastapi_mcp import FastApiMCP

app = FastAPI()

mcp = FastApiMCP(app)

# Mount the MCP server directly to your FastAPI app
mcp.mount()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;That's it! Your auto-generated MCP server is now available at &lt;code&gt;https://app.base.url/mcp&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Documentation, Examples and Advanced Usage&lt;/h2&gt; 
&lt;p&gt;FastAPI-MCP provides &lt;a href="https://fastapi-mcp.tadata.com/"&gt;comprehensive documentation&lt;/a&gt;. Additionaly, check out the &lt;a href="https://raw.githubusercontent.com/tadata-org/fastapi_mcp/main/examples"&gt;examples directory&lt;/a&gt; for code samples demonstrating these features in action.&lt;/p&gt; 
&lt;h2&gt;FastAPI-first Approach&lt;/h2&gt; 
&lt;p&gt;FastAPI-MCP is designed as a native extension of FastAPI, not just a converter that generates MCP tools from your API. This approach offers several key advantages:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Native dependencies&lt;/strong&gt;: Secure your MCP endpoints using familiar FastAPI &lt;code&gt;Depends()&lt;/code&gt; for authentication and authorization&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ASGI transport&lt;/strong&gt;: Communicates directly with your FastAPI app using its ASGI interface, eliminating the need for HTTP calls from the MCP to your API&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Unified infrastructure&lt;/strong&gt;: Your FastAPI app doesn't need to run separately from the MCP server (though &lt;a href="https://fastapi-mcp.tadata.com/advanced/deploy#deploying-separately-from-original-fastapi-app"&gt;separate deployment&lt;/a&gt; is also supported)&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This design philosophy ensures minimum friction when adding MCP capabilities to your existing FastAPI services.&lt;/p&gt; 
&lt;h2&gt;Development and Contributing&lt;/h2&gt; 
&lt;p&gt;Thank you for considering contributing to FastAPI-MCP! We encourage the community to post Issues and create Pull Requests.&lt;/p&gt; 
&lt;p&gt;Before you get started, please see our &lt;a href="https://raw.githubusercontent.com/tadata-org/fastapi_mcp/main/CONTRIBUTING.md"&gt;Contribution Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;p&gt;Join &lt;a href="https://join.slack.com/t/themcparty/shared_invite/zt-30yxr1zdi-2FG~XjBA0xIgYSYuKe7~Xg"&gt;MCParty Slack community&lt;/a&gt; to connect with other MCP enthusiasts, ask questions, and share your experiences with FastAPI-MCP.&lt;/p&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.10+ (Recommended 3.12)&lt;/li&gt; 
 &lt;li&gt;uv&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;MIT License. Copyright (c) 2025 Tadata Inc.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>willccbb/verifiers</title>
      <link>https://github.com/willccbb/verifiers</link>
      <description>&lt;p&gt;Verifiers for LLM Reinforcement Learning&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p align="center"&gt; &lt;/p&gt;
 &lt;h1&gt;Verifiers&lt;/h1&gt; 
 &lt;p&gt;&lt;/p&gt; 
 &lt;p&gt; Environments for LLM Reinforcement Learning &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;Overview&lt;/h2&gt; 
&lt;p&gt;Verifiers is a library of modular components for creating RL environments and training LLM agents. Environments built with Verifiers can be used directly as LLM evaluations, synthetic data pipelines, or agent harnesses for any OpenAI-compatible model endpoint, in addition to RL training. Verifiers includes an async GRPO implementation built around the &lt;code&gt;transformers&lt;/code&gt; Trainer, is supported by &lt;code&gt;prime-rl&lt;/code&gt; for large-scale FSDP training, and can easily be integrated into any RL framework which exposes an OpenAI-compatible inference client.&lt;/p&gt; 
&lt;p&gt;Full documentation is available &lt;a href="https://verifiers.readthedocs.io/en/latest/"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Setup&lt;/h2&gt; 
&lt;p&gt;We recommend using &lt;code&gt;verifiers&lt;/code&gt; with along &lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;uv&lt;/a&gt; for dependency management in your own project:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -LsSf https://astral.sh/uv/install.sh | sh
uv init # create a fresh project
source .venv/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For local (CPU) development and evaluation with API models, do:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv add verifiers # uv add 'verifiers[dev]' for Jupyter + testing support
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For training on GPUs with &lt;code&gt;vf.GRPOTrainer&lt;/code&gt;, do:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv add 'verifiers[all]' &amp;amp;&amp;amp; uv pip install flash-attn --no-build-isolation
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To use the latest &lt;code&gt;main&lt;/code&gt; branch, do:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv add verifiers @ git+https://github.com/willccbb/verifiers.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To use with &lt;code&gt;prime-rl&lt;/code&gt;, see &lt;a href="https://github.com/PrimeIntellect-ai/prime-rl"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To install &lt;code&gt;verifiers&lt;/code&gt; from source for core library development, do:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/willccbb/verifiers.git
cd verifiers
uv sync --all-extras &amp;amp;&amp;amp; uv pip install flash-attn --no-build-isolation
uv run pre-commit install
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In general, we recommend that you build and train Environments &lt;em&gt;with&lt;/em&gt; &lt;code&gt;verifiers&lt;/code&gt;, not &lt;em&gt;in&lt;/em&gt; &lt;code&gt;verifiers&lt;/code&gt;. If you find yourself needing to clone and modify the core library in order to implement key functionality for your project, we'd love for you to open an issue so that we can try and streamline the development experience. Our aim is for &lt;code&gt;verifiers&lt;/code&gt; to be a reliable toolkit to build on top of, and to minimize the "fork proliferation" which often pervades the RL infrastructure ecosystem.&lt;/p&gt; 
&lt;h2&gt;Environments&lt;/h2&gt; 
&lt;p&gt;Environments in Verifiers are installable Python modules which can specify dependencies in a &lt;code&gt;pyproject.toml&lt;/code&gt;, and which expose a &lt;code&gt;load_environment&lt;/code&gt; function for instantiation by downstream applications (e.g. trainers). See &lt;code&gt;environments/&lt;/code&gt; for examples.&lt;/p&gt; 
&lt;p&gt;To initialize a blank Environment module template, do:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;vf-init vf-environment-name # -p /path/to/environments (defaults to "./environments")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To an install an Environment module into your project, do:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;vf-install vf-environment-name # -p /path/to/environments (defaults to "./environments") 
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To install an Environment module from this repo's &lt;code&gt;environments&lt;/code&gt; folder, do:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;vf-install vf-math-python --from-repo # -b branch_or_commit (defaults to "main")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Once an Environment module is installed, you can create an instance of the Environment using &lt;code&gt;load_environment&lt;/code&gt;, passing any necessary args:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import verifiers as vf
vf_env = vf.load_environment("vf-environment-name", **env_args)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To run a quick evaluation of your Environment with an API-based model, do:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;vf-eval vf-environment-name # vf-eval -h for config options; defaults to gpt-4.1-mini, 5 prompts, 3 rollouts for each
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The core elements of Environments in are:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Datasets: a Hugging Face &lt;code&gt;Dataset&lt;/code&gt; with a &lt;code&gt;prompt&lt;/code&gt; column for inputs, and optionally &lt;code&gt;answer (str)&lt;/code&gt; or &lt;code&gt;info (dict)&lt;/code&gt; columns for evaluation (both can be omitted for environments that evaluate based solely on completion quality)&lt;/li&gt; 
 &lt;li&gt;Rollout logic: interactions between models and the environment (e.g. &lt;code&gt;env_response&lt;/code&gt; + &lt;code&gt;is_completed&lt;/code&gt; for any &lt;code&gt;MultiTurnEnv&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Rubrics: an encapsulation for one or more reward functions&lt;/li&gt; 
 &lt;li&gt;Parsers: optional; an encapsulation for reusable parsing logic&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We support both &lt;code&gt;/v1/chat/completions&lt;/code&gt;-style and &lt;code&gt;/v1/completions&lt;/code&gt;-style inference via OpenAI clients, though we generally recommend &lt;code&gt;/v1/chat/completions&lt;/code&gt;-style inference for the vast majority of applications. Both the included &lt;code&gt;GRPOTrainer&lt;/code&gt; as well as &lt;code&gt;prime-rl&lt;/code&gt; support the full set of &lt;a href="https://docs.vllm.ai/en/v0.6.0/dev/sampling_params.html"&gt;SamplingParams&lt;/a&gt; exposed by vLLM (via their OpenAI-compatible &lt;a href="https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html"&gt;server&lt;/a&gt; interface), and leveraging this will often be the appropriate way to implement rollout strategies requiring finer-grained control, such as interrupting and resuming generations for interleaved tool use, or enforcing reasoning budgets.&lt;/p&gt; 
&lt;p&gt;The primary constraint we impose on rollout logic is that token sequences must be &lt;em&gt;increasing&lt;/em&gt;, i.e. once a token has been added to a model's context in a rollout, it must remain as the rollout progresses. Note that this causes issues with some popular reasoning models such as the Qwen3 and DeepSeek-R1-Distill series; see &lt;a href="https://raw.githubusercontent.com/willccbb/verifiers/main/#footguns"&gt;Footguns&lt;/a&gt; for guidance on adapting these models to support multi-turn rollouts.&lt;/p&gt; 
&lt;h3&gt;SingleTurnEnv&lt;/h3&gt; 
&lt;p&gt;For tasks requiring only a single response from a model for each prompt, you can use &lt;code&gt;SingleTurnEnv&lt;/code&gt; directly by specifying a Dataset and a Rubric. Rubrics are sets of reward functions, which can be either sync or async.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from datasets import load_dataset
import verifiers as vf

dataset = load_dataset("my-account/my-dataset", split="train")

def reward_A(prompt, completion, info) -&amp;gt; float:
	# reward fn, e.g. correctness
	...

def reward_B(parser, completion) -&amp;gt; float:
	# auxiliary reward fn, e.g. format
	...

async def metric(completion) -&amp;gt; float:
	# non-reward metric, e.g. proper noun count
	...

rubric = vf.Rubric(funcs=[reward_A, reward_B, metric], weights=[1.0, 0.5, 0.0])

vf_env = SingleTurnEnv(
	dataset=dataset,
	rubric=rubric
)
results = vf_env.evaluate(client=OpenAI(), model="gpt-4.1-mini", num_examples=100, rollouts_per_example=1)
vf_env.make_dataset(results) # HF dataset format
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Datasets should be formatted with columns for:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;'prompt' (List[ChatMessage])&lt;/code&gt; OR &lt;code&gt;'question' (str)&lt;/code&gt; fields 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;ChatMessage&lt;/code&gt; = e.g. &lt;code&gt;{'role': 'user', 'content': '...'}&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;if &lt;code&gt;question&lt;/code&gt; is set instead of &lt;code&gt;prompt&lt;/code&gt;, you can also pass &lt;code&gt;system_prompt (str)&lt;/code&gt; and/or &lt;code&gt;few_shot (List[ChatMessage])&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;answer (str)&lt;/code&gt; AND/OR &lt;code&gt;info (dict)&lt;/code&gt; (both optional, can be omitted entirely)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;task (str)&lt;/code&gt;: optional, used by &lt;code&gt;EnvGroup&lt;/code&gt; and &lt;code&gt;RubricGroup&lt;/code&gt; for orchestrating composition of Environments and Rubrics&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The following named attributes available for use by reward functions in your Rubric:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;prompt&lt;/code&gt;: sequence of input messages&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;completion&lt;/code&gt;: sequence of messages generated during rollout by model and Environment&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;answer&lt;/code&gt;: primary answer column, optional (defaults to empty string if omitted)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;state&lt;/code&gt;: can be modified during rollout to accumulate any metadata (&lt;code&gt;state['responses']&lt;/code&gt; includes full OpenAI response objects by default)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;info&lt;/code&gt;: auxiliary info needed for reward computation (e.g. test cases), optional (defaults to empty dict if omitted)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;task&lt;/code&gt;: tag for task type (used by &lt;code&gt;EnvGroup&lt;/code&gt; and &lt;code&gt;RubricGroup&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;parser&lt;/code&gt;: the parser object declared. Note: &lt;code&gt;vf.Parser().get_format_reward_func()&lt;/code&gt; is a no-op (always 1.0); use &lt;code&gt;vf.ThinkParser&lt;/code&gt; or a custom parser if you want a real format adherence reward.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Some environments can fully evaluate using only &lt;code&gt;prompt&lt;/code&gt;, &lt;code&gt;completion&lt;/code&gt;, and &lt;code&gt;state&lt;/code&gt; without requiring ground truth &lt;code&gt;answer&lt;/code&gt; or &lt;code&gt;info&lt;/code&gt; data. Examples include format compliance checking, completion quality assessment, or length-based rewards.&lt;/p&gt; 
&lt;p&gt;For tasks involving LLM judges, you may wish to use &lt;code&gt;vf.JudgeRubric()&lt;/code&gt; for managing requests to auxiliary models.&lt;/p&gt; 
&lt;p&gt;Note on concurrency: environment APIs accept &lt;code&gt;max_concurrent&lt;/code&gt; to control parallel rollouts. The &lt;code&gt;vf-eval&lt;/code&gt; CLI currently exposes &lt;code&gt;--max-concurrent-requests&lt;/code&gt;; ensure this maps to your environment’s concurrency as expected.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;vf-eval&lt;/code&gt; also supports specifying &lt;code&gt;sampling_args&lt;/code&gt; as a JSON object, which is sent to the vLLM inference engine:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;vf-eval vf-environment-name --sampling-args '{"reasoning_effort": "low"}'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Use &lt;code&gt;vf-eval -s&lt;/code&gt; to save outputs as dataset-formatted JSON, and view all locally-saved eval results with &lt;code&gt;vf-tui&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;ToolEnv&lt;/h3&gt; 
&lt;p&gt;For many applications involving tool use, you can use &lt;code&gt;ToolEnv&lt;/code&gt; to leverage models' native tool/function-calling capabilities in an agentic loop. Tools can be specified as generic Python functions (with type hints and docstrings), which will then be passed in JSON schema form to each inference request.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import verifiers as vf
vf_env = vf.ToolEnv(
	dataset= ... # HF Dataset with 'prompt'/'question' and optionally 'answer'/'info' columns
	rubric= ... # Rubric object; vf.ToolRubric() can be optionally used for counting tool invocations in each rollout
	tools=[search_tool, read_article_tool, python_tool], # python functions with type hints + docstrings
	max_turns=10
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In cases where your tools require heavy computational resources, we recommend hosting your tools as standalone servers (e.g. MCP servers) and creating lightweight wrapper functions to pass to &lt;code&gt;ToolEnv&lt;/code&gt;. Parallel tool call support is enabled by default.&lt;/p&gt; 
&lt;p&gt;For training, or self-hosted endpoints, you'll want to enable auto tool choice in &lt;a href="https://docs.vllm.ai/en/stable/features/tool_calling.html#automatic-function-calling"&gt;vLLM&lt;/a&gt; with the appropriate parser. If your model does not support native tool calling, you may find the &lt;code&gt;XMLParser&lt;/code&gt; abstraction useful for rolling your own tool call parsing on top of &lt;code&gt;MultiTurnEnv&lt;/code&gt;; see &lt;code&gt;environments/xml_tool_env&lt;/code&gt; for an example.&lt;/p&gt; 
&lt;h3&gt;MultiTurnEnv&lt;/h3&gt; 
&lt;p&gt;Both &lt;code&gt;SingleTurnEnv&lt;/code&gt; and &lt;code&gt;ToolEnv&lt;/code&gt; are instances of &lt;code&gt;MultiTurnEnv&lt;/code&gt;, which exposes an interface for writing custom Environment interaction protocols. The two methods you must override are&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from typing import Tuple
import verifiers as vf
from verifiers.types import Messages, State
class YourMultiTurnEnv(vf.MultiTurnEnv):
    def __init__(self,
                 dataset: Dataset,
                 rubric: Rubric,
				 max_turns: int,
                 **kwargs):
	
  async def is_completed(self, messages: Messages, state: State, **kwargs) -&amp;gt; bool:
    # return whether or not a rollout is completed

  async def env_response(self, messages: Messages, state: State, **kwargs) -&amp;gt; Tuple[Messages, State]:
    # return new environment message(s) + updated state
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If your application requires more fine-grained control than is allowed by &lt;code&gt;MultiTurnEnv&lt;/code&gt;, you may want to inherit from the base &lt;code&gt;Environment&lt;/code&gt; functionality directly and override the &lt;code&gt;rollout&lt;/code&gt; method.&lt;/p&gt; 
&lt;h2&gt;Training&lt;/h2&gt; 
&lt;h3&gt;GRPOTrainer&lt;/h3&gt; 
&lt;p&gt;The included trainer (&lt;code&gt;vf.GRPOTrainer&lt;/code&gt;) supports running GRPO-style RL training via Accelerate/DeepSpeed, and uses vLLM for inference. It supports both full-parameter finetuning, and is optimized for efficiently training dense transformer models on 2-16 GPUs.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# install environment
vf-install vf-wordle (-p /path/to/environments | --from-repo)

# quick eval
vf-eval vf-wordle -m (model_name in configs/endpoints.py) -n NUM_EXAMPLES -r ROLLOUTS_PER_EXAMPLE

# inference (shell 0)
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 vf-vllm --model willcb/Qwen3-1.7B-Wordle \
    --data-parallel-size 7 --enforce-eager --disable-log-requests

# training (shell 1)
CUDA_VISIBLE_DEVICES=6,7 accelerate launch --num-processes 2 \
    --config-file configs/zero3.yaml examples/grpo/train_wordle.py --size 1.7B
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, you can train environments with the external &lt;code&gt;prime-rl&lt;/code&gt; project (FSDP-first orchestration). See the &lt;code&gt;prime-rl&lt;/code&gt; README for installation and examples. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-toml"&gt;# orchestrator config (prime-rl)
[environment]
id = "vf-math-python"  # or your environment ID
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# run (prime-rl)
uv run rl \
  --trainer @ configs/your_exp/train.toml \
  --orchestrator @ configs/your_exp/orch.toml \
  --inference @ configs/your_exp/infer.toml
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Troubleshooting&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Ensure your &lt;code&gt;wandb&lt;/code&gt; and &lt;code&gt;huggingface-cli&lt;/code&gt; logins are set up (or set &lt;code&gt;report_to=None&lt;/code&gt; in &lt;code&gt;training_args&lt;/code&gt;). You should also have something set as your &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; in your environment (can be a dummy key for vLLM).&lt;/li&gt; 
 &lt;li&gt;If using high max concurrency, increase the number of allowed open sockets (e.g. &lt;code&gt;ulimit -n 4096&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;On some setups, inter-GPU communication can &lt;a href="https://github.com/huggingface/trl/issues/2923"&gt;hang&lt;/a&gt; or crash during vLLM weight syncing. This can usually be alleviated by setting (or unsetting) &lt;code&gt;NCCL_P2P_DISABLE=1&lt;/code&gt; in your environment (or potentially &lt;code&gt;NCCL_CUMEM_ENABLE=1&lt;/code&gt;). Try this as your first step if you experience NCCL-related issues.&lt;/li&gt; 
 &lt;li&gt;If problems persist, please open an &lt;a href="https://github.com/willccbb/verifiers/issues"&gt;issue&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Resource Requirements&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;GRPOTrainer&lt;/code&gt; is optimized for setups with at least 2 GPUs, scaling up to multiple nodes. 2-GPU setups with sufficient memory to enable small-scale experimentation can be &lt;a href="https://app.primeintellect.ai/dashboard/create-cluster?image=ubuntu_22_cuda_12"&gt;rented&lt;/a&gt; for &amp;lt;$1/hr.&lt;/p&gt; 
&lt;h3&gt;PRIME-RL&lt;/h3&gt; 
&lt;p&gt;If you do not require LoRA support, you may want to use the &lt;code&gt;prime-rl&lt;/code&gt; trainer, which natively supports Environments created using &lt;code&gt;verifiers&lt;/code&gt;, is more optimized for performance and scalability via FSDP, includes a broader set of configuration options and user experience features, and has more battle-tested defaults. Both trainers support asynchronous rollouts, and use a one-step off-policy delay by default for overlapping training and inference. See the &lt;code&gt;prime-rl&lt;/code&gt; &lt;a href="https://github.com/PrimeIntellect-ai/prime-rl"&gt;docs&lt;/a&gt; for usage instructions.&lt;/p&gt; 
&lt;h2&gt;Further Documentation&lt;/h2&gt; 
&lt;p&gt;See the full &lt;a href="https://verifiers.readthedocs.io/en/latest/"&gt;docs&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;Contributions&lt;/h2&gt; 
&lt;p&gt;Verifiers warmly welcomes community contributions! Please open an issue or PR if you encounter bugs or other pain points during your development, or start a discussion for more open-ended questions.&lt;/p&gt; 
&lt;p&gt;Please note that the core &lt;code&gt;verifiers/&lt;/code&gt; library is intended to be a relatively lightweight set of reusable components rather than an exhaustive catalog of RL environments. For &lt;em&gt;applications&lt;/em&gt; of &lt;code&gt;verifiers&lt;/code&gt; (e.g. "an Environment for XYZ task"), you are welcome to submit a PR for a self-contained module that lives within &lt;code&gt;environments/&lt;/code&gt; if it serves as a canonical example of a new pattern. Stay tuned for more info shortly about our plans for supporting community Environment contributions 🙂&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you use this code in your research, please cite:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{brown_verifiers_2025,
  author       = {William&amp;nbsp;Brown},
  title        = {{Verifiers}: Reinforcement Learning with LLMs in Verifiable Environments},
  howpublished = {\url{https://github.com/willccbb/verifiers}},
  note         = {Commit abcdefg • accessed DD Mon YYYY},
  year         = {2025}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Roadmap&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;A community Environments hub for crowdsourcing, sharing, and discovering new RL environments built with &lt;code&gt;verifiers&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Default patterns for hosted resources such as code sandboxes, auxiliary models, and MCP servers&lt;/li&gt; 
 &lt;li&gt;Multimodal input support&lt;/li&gt; 
 &lt;li&gt;Non-increasing token sequences via REINFORCE&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>exo-explore/exo</title>
      <link>https://github.com/exo-explore/exo</link>
      <description>&lt;p&gt;Run your own AI cluster at home with everyday devices 📱💻 🖥️⌚&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="/docs/exo-logo-black-bg.jpg" /&gt; 
  &lt;img alt="exo logo" src="https://raw.githubusercontent.com/exo-explore/exo/main/docs/exo-logo-transparent.png" width="50%" height="50%" /&gt; 
 &lt;/picture&gt; 
 &lt;p&gt;exo: Run your own AI cluster at home with everyday devices. Maintained by &lt;a href="https://x.com/exolabs"&gt;exo labs&lt;/a&gt;.&lt;/p&gt; 
 &lt;h3&gt; &lt;p&gt;&lt;a href="https://discord.gg/EUnjGpsmWw"&gt;Discord&lt;/a&gt; | &lt;a href="https://t.me/+Kh-KqHTzFYg3MGNk"&gt;Telegram&lt;/a&gt; | &lt;a href="https://x.com/exolabs"&gt;X&lt;/a&gt;&lt;/p&gt; &lt;/h3&gt; 
 &lt;p&gt;&lt;a href="https://github.com/exo-explore/exo/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/exo-explore/exo" alt="GitHub Repo stars" /&gt;&lt;/a&gt; &lt;a href="https://dl.circleci.com/status-badge/redirect/circleci/TrkofJDoGzdQAeL6yVHKsg/4i5hJuafuwZYZQxbRAWS71/tree/main"&gt;&lt;img src="https://dl.circleci.com/status-badge/img/circleci/TrkofJDoGzdQAeL6yVHKsg/4i5hJuafuwZYZQxbRAWS71/tree/main.svg?style=svg" alt="Tests" /&gt;&lt;/a&gt; &lt;a href="https://www.gnu.org/licenses/gpl-3.0"&gt;&lt;img src="https://img.shields.io/badge/License-GPLv3-blue.svg?sanitize=true" alt="License: GPL v3" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/11849" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/11849" alt="exo-explore%2Fexo | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;p&gt;Unify your existing devices into one powerful GPU: iPhone, iPad, Android, Mac, NVIDIA, Raspberry Pi, pretty much any device!&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;h2&gt;Update: exo is hiring. See &lt;a href="https://exolabs.net"&gt;here&lt;/a&gt; for more details.&lt;/h2&gt; 
 &lt;h2&gt;Interested in running exo in your business? &lt;a href="mailto:hello@exolabs.net"&gt;Contact us&lt;/a&gt; to discuss.&lt;/h2&gt; 
&lt;/div&gt; 
&lt;h2&gt;Get Involved&lt;/h2&gt; 
&lt;p&gt;exo is &lt;strong&gt;experimental&lt;/strong&gt; software. Expect bugs early on. Create issues so they can be fixed. The &lt;a href="https://x.com/exolabs"&gt;exo labs&lt;/a&gt; team will strive to resolve issues quickly.&lt;/p&gt; 
&lt;p&gt;We also welcome contributions from the community. We have a list of bounties in &lt;a href="https://docs.google.com/spreadsheets/d/1cTCpTIp48UnnIvHeLEUNg1iMy_Q6lRybgECSFCoVJpE/edit?usp=sharing"&gt;this sheet&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;h3&gt;Wide Model Support&lt;/h3&gt; 
&lt;p&gt;exo supports different models including LLaMA (&lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/inference/mlx/models/llama.py"&gt;MLX&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/inference/tinygrad/models/llama.py"&gt;tinygrad&lt;/a&gt;), Mistral, LlaVA, Qwen, and Deepseek.&lt;/p&gt; 
&lt;h3&gt;Dynamic Model Partitioning&lt;/h3&gt; 
&lt;p&gt;exo &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/topology/ring_memory_weighted_partitioning_strategy.py"&gt;optimally splits up models&lt;/a&gt; based on the current network topology and device resources available. This enables you to run larger models than you would be able to on any single device.&lt;/p&gt; 
&lt;h3&gt;Automatic Device Discovery&lt;/h3&gt; 
&lt;p&gt;exo will &lt;a href="https://github.com/exo-explore/exo/raw/945f90f676182a751d2ad7bcf20987ab7fe0181e/exo/orchestration/node.py#L154"&gt;automatically discover&lt;/a&gt; other devices using the best method available. Zero manual configuration.&lt;/p&gt; 
&lt;h3&gt;ChatGPT-compatible API&lt;/h3&gt; 
&lt;p&gt;exo provides a &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/api/chatgpt_api.py"&gt;ChatGPT-compatible API&lt;/a&gt; for running models. It's a &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/examples/chatgpt_api.sh"&gt;one-line change&lt;/a&gt; in your application to run models on your own hardware using exo.&lt;/p&gt; 
&lt;h3&gt;Device Equality&lt;/h3&gt; 
&lt;p&gt;Unlike other distributed inference frameworks, exo does not use a master-worker architecture. Instead, exo devices &lt;a href="https://github.com/exo-explore/exo/raw/945f90f676182a751d2ad7bcf20987ab7fe0181e/exo/orchestration/node.py#L161"&gt;connect p2p&lt;/a&gt;. As long as a device is connected somewhere in the network, it can be used to run models.&lt;/p&gt; 
&lt;p&gt;Exo supports different &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/topology/partitioning_strategy.py"&gt;partitioning strategies&lt;/a&gt; to split up a model across devices. The default partitioning strategy is &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/topology/ring_memory_weighted_partitioning_strategy.py"&gt;ring memory weighted partitioning&lt;/a&gt;. This runs an inference in a ring where each device runs a number of model layers proportional to the memory of the device.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/exo-explore/exo/main/docs/exo-screenshot.jpg" alt="&amp;quot;A screenshot of exo running 5 nodes" /&gt;&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;The current recommended way to install exo is from source.&lt;/p&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python&amp;gt;=3.12.0 is required because of &lt;a href="https://github.com/exo-explore/exo/issues/5"&gt;issues with asyncio&lt;/a&gt; in previous versions.&lt;/li&gt; 
 &lt;li&gt;For Linux with NVIDIA GPU support (Linux-only, skip if not using Linux or NVIDIA): 
  &lt;ul&gt; 
   &lt;li&gt;NVIDIA driver - verify with &lt;code&gt;nvidia-smi&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;CUDA toolkit - install from &lt;a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#cuda-cross-platform-installation"&gt;NVIDIA CUDA guide&lt;/a&gt;, verify with &lt;code&gt;nvcc --version&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;cuDNN library - download from &lt;a href="https://developer.nvidia.com/cudnn-downloads"&gt;NVIDIA cuDNN page&lt;/a&gt;, verify installation by following &lt;a href="https://docs.nvidia.com/deeplearning/cudnn/latest/installation/linux.html#verifying-the-install-on-linux:~:text=at%20a%20time.-,Verifying%20the%20Install%20on%20Linux,Test%20passed!,-Upgrading%20From%20Older"&gt;these steps&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Hardware Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;The only requirement to run exo is to have enough memory across all your devices to fit the entire model into memory. For example, if you are running llama 3.1 8B (fp16), you need 16GB of memory across all devices. Any of the following configurations would work since they each have more than 16GB of memory in total: 
  &lt;ul&gt; 
   &lt;li&gt;2 x 8GB M3 MacBook Airs&lt;/li&gt; 
   &lt;li&gt;1 x 16GB NVIDIA RTX 4070 Ti Laptop&lt;/li&gt; 
   &lt;li&gt;2 x Raspberry Pi 400 with 4GB of RAM each (running on CPU) + 1 x 8GB Mac Mini&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;exo is designed to run on devices with heterogeneous capabilities. For example, you can have some devices with powerful GPUs and others with integrated GPUs or even CPUs. Adding less capable devices will slow down individual inference latency but will increase the overall throughput of the cluster.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;From source&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;git clone https://github.com/exo-explore/exo.git
cd exo
pip install -e .
# alternatively, with venv
source install.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Troubleshooting&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;If running on Mac, MLX has an &lt;a href="https://ml-explore.github.io/mlx/build/html/install.html"&gt;install guide&lt;/a&gt; with troubleshooting steps.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Performance&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;There are a number of things users have empirically found to improve performance on Apple Silicon Macs:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;ol&gt; 
 &lt;li&gt;Upgrade to the latest version of macOS Sequoia.&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;./configure_mlx.sh&lt;/code&gt;. This runs commands to optimize GPU memory allocation on Apple Silicon Macs.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;h3&gt;Example Usage on Multiple macOS Devices&lt;/h3&gt; 
&lt;h4&gt;Device 1:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Device 2:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;That's it! No configuration required - exo will automatically discover the other device(s).&lt;/p&gt; 
&lt;p&gt;exo starts a ChatGPT-like WebUI (powered by &lt;a href="https://github.com/tinygrad/tinygrad/tree/master/examples/tinychat"&gt;tinygrad tinychat&lt;/a&gt;) on &lt;a href="http://localhost:52415"&gt;http://localhost:52415&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;For developers, exo also starts a ChatGPT-compatible API endpoint on &lt;a href="http://localhost:52415/v1/chat/completions"&gt;http://localhost:52415/v1/chat/completions&lt;/a&gt;. Examples with curl:&lt;/p&gt; 
&lt;h4&gt;Llama 3.2 3B:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;curl http://localhost:52415/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
     "model": "llama-3.2-3b",
     "messages": [{"role": "user", "content": "What is the meaning of exo?"}],
     "temperature": 0.7
   }'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Llama 3.1 405B:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;curl http://localhost:52415/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
     "model": "llama-3.1-405b",
     "messages": [{"role": "user", "content": "What is the meaning of exo?"}],
     "temperature": 0.7
   }'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;DeepSeek R1 (full 671B):&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;curl http://localhost:52415/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
     "model": "deepseek-r1",
     "messages": [{"role": "user", "content": "What is the meaning of exo?"}],
     "temperature": 0.7
   }'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Llava 1.5 7B (Vision Language Model):&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;curl http://localhost:52415/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
     "model": "llava-1.5-7b-hf",
     "messages": [
      {
        "role": "user",
        "content": [
          {
            "type": "text",
            "text": "What are these?"
          },
          {
            "type": "image_url",
            "image_url": {
              "url": "http://images.cocodataset.org/val2017/000000039769.jpg"
            }
          }
        ]
      }
    ],
     "temperature": 0.0
   }'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Example Usage on Multiple Heterogenous Devices (macOS + Linux)&lt;/h3&gt; 
&lt;h4&gt;Device 1 (macOS):&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note: We don't need to explicitly tell exo to use the &lt;strong&gt;tinygrad&lt;/strong&gt; inference engine. &lt;strong&gt;MLX&lt;/strong&gt; and &lt;strong&gt;tinygrad&lt;/strong&gt; are interoperable!&lt;/p&gt; 
&lt;h4&gt;Device 2 (Linux):&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Linux devices will automatically default to using the &lt;strong&gt;tinygrad&lt;/strong&gt; inference engine.&lt;/p&gt; 
&lt;p&gt;You can read about tinygrad-specific env vars &lt;a href="https://docs.tinygrad.org/env_vars/"&gt;here&lt;/a&gt;. For example, you can configure tinygrad to use the cpu by specifying &lt;code&gt;CLANG=1&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Example Usage on a single device with "exo run" command&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;exo run llama-3.2-3b
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;With a custom prompt:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;exo run llama-3.2-3b --prompt "What is the meaning of exo?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Model Storage&lt;/h3&gt; 
&lt;p&gt;Models by default are stored in &lt;code&gt;~/.cache/exo/downloads&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;You can set a different model storage location by setting the &lt;code&gt;EXO_HOME&lt;/code&gt; env var.&lt;/p&gt; 
&lt;h2&gt;Model Downloading&lt;/h2&gt; 
&lt;p&gt;Models are downloaded from Hugging Face. If you are running exo in a country with strict internet censorship, you may need to download the models manually and put them in the &lt;code&gt;~/.cache/exo/downloads&lt;/code&gt; directory.&lt;/p&gt; 
&lt;p&gt;To download models from a proxy endpoint, set the &lt;code&gt;HF_ENDPOINT&lt;/code&gt; environment variable. For example, to run exo with the huggingface mirror endpoint:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;HF_ENDPOINT=https://hf-mirror.com exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Debugging&lt;/h2&gt; 
&lt;p&gt;Enable debug logs with the DEBUG environment variable (0-9).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;DEBUG=9 exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For the &lt;strong&gt;tinygrad&lt;/strong&gt; inference engine specifically, there is a separate DEBUG flag &lt;code&gt;TINYGRAD_DEBUG&lt;/code&gt; that can be used to enable debug logs (1-6).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;TINYGRAD_DEBUG=2 exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Formatting&lt;/h2&gt; 
&lt;p&gt;We use &lt;a href="https://github.com/google/yapf"&gt;yapf&lt;/a&gt; to format the code. To format the code, first install the formatting requirements:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip3 install -e '.[formatting]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then run the formatting script:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python3 format.py ./exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Known Issues&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;On certain versions of Python on macOS, certificates may not installed correctly, potentially causing SSL errors (e.g., when accessing huggingface.co). To resolve this, run the &lt;code&gt;Install Certificates&lt;/code&gt; command, typicall as follows:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;/Applications/Python 3.x/Install Certificates.command
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;🚧 As the library is evolving so quickly, the iOS implementation has fallen behind Python. We have decided for now not to put out the buggy iOS version and receive a bunch of GitHub issues for outdated code. We are working on solving this properly and will make an announcement when it's ready. If you would like access to the iOS implementation now, please email &lt;a href="mailto:alex@exolabs.net"&gt;alex@exolabs.net&lt;/a&gt; with your GitHub username explaining your use-case and you will be granted access on GitHub.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Inference Engines&lt;/h2&gt; 
&lt;p&gt;exo supports the following inference engines:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;✅ &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/inference/mlx/sharded_inference_engine.py"&gt;MLX&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;✅ &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/inference/tinygrad/inference.py"&gt;tinygrad&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;🚧 &lt;a href="https://github.com/exo-explore/exo/pull/139"&gt;PyTorch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;🚧 &lt;a href="https://github.com/exo-explore/exo/issues/167"&gt;llama.cpp&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Discovery Modules&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;✅ &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/networking/udp"&gt;UDP&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;✅ &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/networking/manual"&gt;Manual&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;✅ &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/networking/tailscale"&gt;Tailscale&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;🚧 Radio&lt;/li&gt; 
 &lt;li&gt;🚧 Bluetooth&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Peer Networking Modules&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;✅ &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/networking/grpc"&gt;GRPC&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;🚧 NCCL&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>OpenBMB/MiniCPM-V</title>
      <link>https://github.com/OpenBMB/MiniCPM-V</link>
      <description>&lt;p&gt;MiniCPM-V 4.5: A GPT-4o Level MLLM for Single Image, Multi Image and High-FPS Video Understanding on Your Phone&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpm_v_and_minicpm_o_title.png" width="500em" /&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;A GPT-4o Level MLLM for Single Image, Multi Image and High-FPS Video Understanding on Your Phone&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/README_zh.md"&gt;中文&lt;/a&gt; | English&lt;/strong&gt;&lt;/p&gt; 
 &lt;span style="display: inline-flex; align-items: center; margin-right: 2px;"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/wechat.png" alt="WeChat" style="margin-right: 4px;" /&gt; &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/wechat.md" target="_blank"&gt; WeChat&lt;/a&gt; &amp;nbsp;| &lt;/span&gt; &amp;nbsp; 
 &lt;span style="display: inline-flex; align-items: center; margin-left: -8px;"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/discord.png" alt="Discord" style="margin-right: 4px;" /&gt; &lt;a href="https://discord.gg/rftuRMbqzf" target="_blank"&gt; Discord&lt;/a&gt; &amp;nbsp; &lt;/span&gt; 
 &lt;p align="center"&gt; MiniCPM-V 4.5 &lt;a href="https://huggingface.co/openbmb/MiniCPM-V-4_5"&gt;🤗&lt;/a&gt; &lt;a href="http://101.126.42.235:30910/"&gt;🤖&lt;/a&gt; | MiniCPM-o 2.6 &lt;a href="https://huggingface.co/openbmb/MiniCPM-o-2_6"&gt;🤗&lt;/a&gt; &lt;a href="https://minicpm-omni-webdemo-us.modelbest.cn/"&gt; 🤖&lt;/a&gt; | &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-Cookbook"&gt;🍳 Cookbook&lt;/a&gt; | 📄 Technical Report (Coming Soon) &lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;MiniCPM-V&lt;/strong&gt; is a series of efficient end-side multimodal LLMs (MLLMs), which accept images, videos and text as inputs and deliver high-quality text outputs. &lt;strong&gt;MiniCPM-o&lt;/strong&gt; additionally takes audio as inputs and provides high-quality speech outputs in an end-to-end fashion. Since February 2024, we have released 7 versions of the model, aiming to achieve &lt;strong&gt;strong performance and efficient deployment&lt;/strong&gt;. The most notable models in the series currently include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;MiniCPM-V 4.5&lt;/strong&gt;: 🔥🔥🔥 The latest and most capable model in the MiniCPM-V series. With a total of 8B parameters, this model &lt;strong&gt;outperforms GPT-4o-latest, Gemini-2.0 Pro, and Qwen2.5-VL 72B&lt;/strong&gt; in vision-language capabilities, making it the most performant on-device multimodal model in the open-source community. This version brings &lt;strong&gt;new features including efficient high-FPS and long video understanding (up to 96x compression rate for video tokens), controllable hybrid fast/deep thinking, strong handwritten OCR and complex table/document parsing&lt;/strong&gt;. It also advances MiniCPM-V's popular features such as trustworthy behavior, multilingual support and end-side deployability.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;MiniCPM-o 2.6&lt;/strong&gt;: ⭐️⭐️⭐️ The most capable model in the MiniCPM-o series. With a total of 8B parameters, this end-to-end model &lt;strong&gt;achieves comparable performance to GPT-4o-202405 in vision, speech, and multimodal live streaming&lt;/strong&gt;, making it one of the most versatile and performant models in the open-source community. For the new voice mode, MiniCPM-o 2.6 &lt;strong&gt;supports bilingual real-time speech conversation with configurable voices&lt;/strong&gt;, and also allows for fun capabilities such as emotion/speed/style control, end-to-end voice cloning, role play, etc. Due to its superior token density, MiniCPM-o 2.6 can for the first time &lt;strong&gt;support multimodal live streaming on end-side devices&lt;/strong&gt; such as iPad.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;News 
 &lt;!-- omit in toc --&gt;&lt;/h2&gt; 
&lt;h4&gt;📌 Pinned&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;[2025.09.01] ⭐️⭐️⭐️ MiniCPM-V 4.5 has been officially supported by &lt;a href="https://github.com/ggml-org/llama.cpp/pull/15575"&gt;llama.cpp&lt;/a&gt;, &lt;a href="https://github.com/vllm-project/vllm/pull/23586"&gt;vLLM&lt;/a&gt;, and &lt;a href="https://github.com/hiyouga/LLaMA-Factory/pull/9022"&gt;LLaMA-Factory&lt;/a&gt;. You are welcome to use it directly through these official channels! Support for additional frameworks such as &lt;a href="https://github.com/ollama/ollama/pull/12078"&gt;Ollama&lt;/a&gt; and &lt;a href="https://github.com/sgl-project/sglang/pull/9610"&gt;SGLang&lt;/a&gt; is actively in progress.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2025.08.26] 🔥🔥🔥 We open-source MiniCPM-V 4.5, which outperforms GPT-4o-latest, Gemini-2.0 Pro, and Qwen2.5-VL 72B. It advances popular capabilities of MiniCPM-V, and brings useful new features. Try it now!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2025.08.01] ⭐️⭐️⭐️ We open-sourced the &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook"&gt;MiniCPM-V &amp;amp; o Cookbook&lt;/a&gt;! It provides comprehensive guides for diverse user scenarios, paired with our new &lt;a href="https://minicpm-o.readthedocs.io/en/latest/index.html"&gt;Docs Site&lt;/a&gt; for smoother onboarding.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2025.06.20] ⭐️⭐️⭐️ Our official &lt;a href="https://ollama.com/openbmb"&gt;Ollama repository&lt;/a&gt; is released. Try our latest models with &lt;a href="https://ollama.com/openbmb/minicpm-o2.6"&gt;one click&lt;/a&gt;！&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2025.03.01] 🚀🚀🚀 RLAIF-V, the alignment technique of MiniCPM-o, is accepted by CVPR 2025 Highlights！The &lt;a href="https://github.com/RLHF-V/RLAIF-V"&gt;code&lt;/a&gt;, &lt;a href="https://huggingface.co/datasets/openbmb/RLAIF-V-Dataset"&gt;dataset&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/2405.17220"&gt;paper&lt;/a&gt; are open-sourced!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2025.01.24] 📢📢📢 MiniCPM-o 2.6 technical report is released! See &lt;a href="https://openbmb.notion.site/MiniCPM-o-2-6-A-GPT-4o-Level-MLLM-for-Vision-Speech-and-Multimodal-Live-Streaming-on-Your-Phone-185ede1b7a558042b5d5e45e6b237da9"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2025.01.19] 📢 &lt;strong&gt;ATTENTION!&lt;/strong&gt; We are currently working on merging MiniCPM-o 2.6 into the official repositories of llama.cpp, Ollama, and vllm. Until the merge is complete, please USE OUR LOCAL FORKS of &lt;a href="https://github.com/OpenBMB/llama.cpp/raw/minicpm-omni/examples/llava/README-minicpmo2.6.md"&gt;llama.cpp&lt;/a&gt;, &lt;a href="https://github.com/OpenBMB/ollama/raw/minicpm-v2.6/examples/minicpm-v2.6/README.md"&gt;Ollama&lt;/a&gt;, and &lt;a href="https://github.com/OpenBMB/MiniCPM-o?tab=readme-ov-file#efficient-inference-with-llamacpp-ollama-vllm"&gt;vllm&lt;/a&gt;. &lt;strong&gt;Using the official repositories before the merge may lead to unexpected issues&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2025.01.19] ⭐️⭐️⭐️ MiniCPM-o tops GitHub Trending and reaches top-2 on Hugging Face Trending!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2025.01.17] We have updated the usage of MiniCPM-o 2.6 int4 quantization version and resolved the model initialization error. Click &lt;a href="https://huggingface.co/openbmb/MiniCPM-o-2_6-int4"&gt;here&lt;/a&gt; and try it now!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2025.01.13] 🔥🔥🔥 We open-source MiniCPM-o 2.6, which matches GPT-4o-202405 on vision, speech and multimodal live streaming. It advances popular capabilities of MiniCPM-V 2.6, and supports various new fun features. Try it now!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2024.08.17] 🚀🚀🚀 MiniCPM-V 2.6 is now fully supported by &lt;a href="https://github.com/ggerganov/llama.cpp"&gt;official&lt;/a&gt; llama.cpp! GGUF models of various sizes are available &lt;a href="https://huggingface.co/openbmb/MiniCPM-V-2_6-gguf"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2024.08.06] 🔥🔥🔥 We open-source MiniCPM-V 2.6, which outperforms GPT-4V on single image, multi-image and video understanding. It advances popular features of MiniCPM-Llama3-V 2.5, and can support real-time video understanding on iPad. Try it now!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2024.08.03] MiniCPM-Llama3-V 2.5 technical report is released! See &lt;a href="https://arxiv.org/abs/2408.01800"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2024.05.23] 🔥🔥🔥 MiniCPM-V tops GitHub Trending and Hugging Face Trending! Our demo, recommended by Hugging Face Gradio’s official account, is available &lt;a href="https://huggingface.co/spaces/openbmb/MiniCPM-Llama3-V-2_5"&gt;here&lt;/a&gt;. Come and try it out!&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click to view more news.&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;[2025.08.02] 🚀🚀🚀 We open-source MiniCPM-V 4.0, which outperforms GPT-4.1-mini-20250414 in image understanding. It advances popular features of MiniCPM-V 2.6, and largely improves the efficiency. We also open-source the iOS App on iPhone and iPad. Try it now!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2025.01.23] 💡💡💡 MiniCPM-o 2.6 is now supported by &lt;a href="https://github.com/PKU-Alignment/align-anything"&gt;Align-Anything&lt;/a&gt;, a framework by PKU-Alignment Team for aligning any-to-any modality large models with human intentions. It supports DPO and SFT fine-tuning on both vision and audio. Try it now!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.08.15] We now also support multi-image SFT. For more details, please refer to the &lt;a href="https://github.com/OpenBMB/MiniCPM-V/tree/main/finetune"&gt;document&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.08.14] MiniCPM-V 2.6 now also supports &lt;a href="https://github.com/modelscope/ms-swift/issues/1613"&gt;fine-tuning&lt;/a&gt; with the SWIFT framework!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.08.10] 🚀🚀🚀 MiniCPM-Llama3-V 2.5 is now fully supported by &lt;a href="https://github.com/ggerganov/llama.cpp"&gt;official&lt;/a&gt; llama.cpp! GGUF models of various sizes are available &lt;a href="https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-gguf"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.07.19] MiniCPM-Llama3-V 2.5 supports vLLM now! See &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#inference-with-vllm"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.06.03] Now, you can run MiniCPM-Llama3-V 2.5 on multiple low VRAM GPUs(12 GB or 16 GB) by distributing the model's layers across multiple GPUs. For more details, check this &lt;a href="https://github.com/OpenBMB/MiniCPM-V/raw/main/docs/inference_on_multiple_gpus.md"&gt;link&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.05.28] 🚀🚀🚀 MiniCPM-Llama3-V 2.5 now fully supports its feature in llama.cpp and Ollama! Please pull the latest code &lt;strong&gt;of our provided forks&lt;/strong&gt; (&lt;a href="https://github.com/OpenBMB/llama.cpp/raw/minicpm-v2.5/examples/minicpmv/README.md"&gt;llama.cpp&lt;/a&gt;, &lt;a href="https://github.com/OpenBMB/ollama/tree/minicpm-v2.5/examples/minicpm-v2.5"&gt;Ollama&lt;/a&gt;). GGUF models in various sizes are available &lt;a href="https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-gguf/tree/main"&gt;here&lt;/a&gt;. MiniCPM-Llama3-V 2.5 series is &lt;strong&gt;not supported by the official repositories yet&lt;/strong&gt;, and we are working hard to merge PRs. Please stay tuned!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.05.28] 💫 We now support LoRA fine-tuning for MiniCPM-Llama3-V 2.5, using only 2 V100 GPUs! See more statistics &lt;a href="https://github.com/OpenBMB/MiniCPM-V/tree/main/finetune#model-fine-tuning-memory-usage-statistics"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.05.25] MiniCPM-Llama3-V 2.5 now supports streaming outputs and customized system prompts. Try it &lt;a href="https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5#usage"&gt;here&lt;/a&gt;!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.05.24] We release the MiniCPM-Llama3-V 2.5 &lt;a href="https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-gguf"&gt;gguf&lt;/a&gt;, which supports &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#inference-with-llamacpp"&gt;llama.cpp&lt;/a&gt; inference and provides a 6~8 token/s smooth decoding on mobile phones. Try it now!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.05.23] 🔍 We've released a comprehensive comparison between Phi-3-vision-128k-instruct and MiniCPM-Llama3-V 2.5, including benchmark evaluations, multilingual capabilities, and inference efficiency 🌟📊🌍🚀. Click &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/compare_with_phi-3_vision.md"&gt;here&lt;/a&gt; to view more details.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.05.20] We open-soure MiniCPM-Llama3-V 2.5, it has improved OCR capability and supports 30+ languages, representing the first end-side MLLM achieving GPT-4V level performance! We provide &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#deployment-on-mobile-phone"&gt;efficient inference&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/finetune/readme.md"&gt;simple fine-tuning&lt;/a&gt;. Try it now!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.04.23] MiniCPM-V-2.0 supports vLLM now! Click &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#inference-with-vllm"&gt;here&lt;/a&gt; to view more details.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.04.18] We create a HuggingFace Space to host the demo of MiniCPM-V 2.0 at &lt;a href="https://huggingface.co/spaces/openbmb/MiniCPM-V-2"&gt;here&lt;/a&gt;!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.04.17] MiniCPM-V-2.0 supports deploying &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#webui-demo"&gt;WebUI Demo&lt;/a&gt; now!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.04.15] MiniCPM-V-2.0 now also supports &lt;a href="https://github.com/modelscope/swift/raw/main/docs/source/Multi-Modal/minicpm-v-2%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5.md"&gt;fine-tuning&lt;/a&gt; with the SWIFT framework!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.04.12] We open-source MiniCPM-V 2.0, which achieves comparable performance with Gemini Pro in understanding scene text and outperforms strong Qwen-VL-Chat 9.6B and Yi-VL 34B on &lt;a href="https://rank.opencompass.org.cn/leaderboard-multimodal"&gt;OpenCompass&lt;/a&gt;, a comprehensive evaluation over 11 popular benchmarks. Click &lt;a href="https://openbmb.vercel.app/minicpm-v-2"&gt;here&lt;/a&gt; to view the MiniCPM-V 2.0 technical blog.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.03.14] MiniCPM-V now supports &lt;a href="https://github.com/modelscope/swift/raw/main/docs/source/Multi-Modal/minicpm-v%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5.md"&gt;fine-tuning&lt;/a&gt; with the SWIFT framework. Thanks to &lt;a href="https://github.com/Jintao-Huang"&gt;Jintao&lt;/a&gt; for the contribution！&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.03.01] MiniCPM-V can now be deployed on Mac!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.02.01] We open-source MiniCPM-V and OmniLMM-12B, which support efficient end-side deployment and powerful multimodal capabilities correspondingly.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;Contents 
 &lt;!-- omit in toc --&gt;&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#minicpm-v-45"&gt;MiniCPM-V 4.5&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#inference-efficiency"&gt;Inference Efficiency&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#minicpm-o-26"&gt;MiniCPM-o 2.6&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#minicpm-v--o-cookbook"&gt;MiniCPM-V &amp;amp; o Cookbook&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#chat-with-our-demo-on-gradio-"&gt;Chat with Our Demo on Gradio 🤗&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#inference"&gt;Inference&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#model-zoo"&gt;Model Zoo&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#multi-turn-conversation"&gt;Multi-turn Conversation&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#chat-with-multiple-images"&gt;Chat with Multiple Images&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#in-context-few-shot-learning"&gt;In-context Few-shot Learning&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#chat-with-video"&gt;Chat with Video&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#speech-and-audio-mode"&gt;Speech and Audio Mode&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#multimodal-live-streaming"&gt;Multimodal Live Streaming&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#inference-on-multiple-gpus"&gt;Inference on Multiple GPUs&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#inference-on-mac"&gt;Inference on Mac&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#efficient-inference-with-llamacpp-ollama-vllm"&gt;Efficient Inference with llama.cpp, Ollama, vLLM&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#fine-tuning"&gt;Fine-tuning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#awesome-work-using-minicpm-v--minicpm-o"&gt;Awesome work using MiniCPM-V &amp;amp; MiniCPM-o&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#faqs"&gt;FAQs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#limitations"&gt;Limitations&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;MiniCPM-V 4.5&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;MiniCPM-V 4.5&lt;/strong&gt; is the latest and most capable model in the MiniCPM-V series. The model is built on Qwen3-8B and SigLIP2-400M with a total of 8B parameters. It exhibits a significant performance improvement over previous MiniCPM-V and MiniCPM-o models, and introduces new useful features. Notable features of MiniCPM-V 4.5 include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;🔥 &lt;strong&gt;State-of-the-art Vision-Language Capability.&lt;/strong&gt; MiniCPM-V 4.5 achieves an average score of 77.0 on OpenCompass, a comprehensive evaluation of 8 popular benchmarks. &lt;strong&gt;With only 8B parameters, it surpasses widely used proprietary models like GPT-4o-latest, Gemini-2.0 Pro, and strong open-source models like Qwen2.5-VL 72B&lt;/strong&gt; for vision-language capabilities, making it the most performant MLLM under 30B parameters.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;🎬 &lt;strong&gt;Efficient High-FPS and Long Video Understanding.&lt;/strong&gt; Powered by a new unified 3D-Resampler over images and videos, MiniCPM-V 4.5 can now achieve 96x compression rate for video tokens, where 6 448x448 video frames can be jointly compressed into 64 video tokens (normally 1,536 tokens for most MLLMs). This means that the model can perceive significantly more video frames without increasing the LLM inference cost. This brings state-of-the-art high-FPS (up to 10FPS) video understanding and long video understanding capabilities on Video-MME, LVBench, MLVU, MotionBench, FavorBench, etc., efficiently.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;⚙️ &lt;strong&gt;Controllable Hybrid Fast/Deep Thinking.&lt;/strong&gt; MiniCPM-V 4.5 supports both fast thinking for efficient frequent usage with competitive performance, and deep thinking for more complex problem solving. To cover efficiency and performance trade-offs in different user scenarios, this fast/deep thinking mode can be switched in a highly controlled fashion.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;💪 &lt;strong&gt;Strong OCR, Document Parsing and Others.&lt;/strong&gt; Based on &lt;a href="https://arxiv.org/pdf/2403.11703"&gt;LLaVA-UHD&lt;/a&gt; architecture, MiniCPM-V 4.5 can process high-resolution images with any aspect ratio and up to 1.8 million pixels (e.g., 1344x1344), using 4x fewer visual tokens than most MLLMs. The model achieves &lt;strong&gt;leading performance on OCRBench, surpassing proprietary models such as GPT-4o-latest and Gemini 2.5&lt;/strong&gt;. It also achieves state-of-the-art performance for PDF document parsing capability on OmniDocBench among general MLLMs. Based on the latest &lt;a href="https://github.com/RLHF-V/RLAIF-V/"&gt;RLAIF-V&lt;/a&gt; and &lt;a href="https://github.com/OpenBMB/VisCPM"&gt;VisCPM&lt;/a&gt; techniques, it features &lt;strong&gt;trustworthy behaviors&lt;/strong&gt;, outperforming GPT-4o-latest on MMHal-Bench, and supports &lt;strong&gt;multilingual capabilities&lt;/strong&gt; in more than 30 languages.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;💫 &lt;strong&gt;Easy Usage.&lt;/strong&gt; MiniCPM-V 4.5 can be easily used in various ways: (1) &lt;a href="https://github.com/tc-mb/llama.cpp/raw/Support-MiniCPM-V-4.5/docs/multimodal/minicpmv4.5.md"&gt;llama.cpp&lt;/a&gt; and &lt;a href="https://github.com/tc-mb/ollama/tree/MIniCPM-V"&gt;ollama&lt;/a&gt; support for efficient CPU inference on local devices, (2) &lt;a href="https://huggingface.co/openbmb/MiniCPM-V-4_5-int4"&gt;int4&lt;/a&gt;, &lt;a href="https://huggingface.co/openbmb/MiniCPM-V-4_5-gguf"&gt;GGUF&lt;/a&gt; and &lt;a href="https://github.com/tc-mb/AutoAWQ"&gt;AWQ&lt;/a&gt; format quantized models in 16 sizes, (3) &lt;a href="https://github.com/tc-mb/sglang/tree/main"&gt;SGLang&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#efficient-inference-with-llamacpp-ollama-vllm"&gt;vLLM&lt;/a&gt; support for high-throughput and memory-efficient inference, (4) fine-tuning on new domains and tasks with &lt;a href="https://github.com/tc-mb/transformers/tree/main"&gt;Transformers&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/llamafactory_train_and_infer.md"&gt;LLaMA-Factory&lt;/a&gt;, (5) quick &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#chat-with-our-demo-on-gradio"&gt;local WebUI demo&lt;/a&gt;, (6) optimized &lt;a href="https://github.com/tc-mb/MiniCPM-o-demo-iOS"&gt;local iOS app&lt;/a&gt; on iPhone and iPad, and (7) online web demo on &lt;a href="http://101.126.42.235:30910/"&gt;server&lt;/a&gt;. See our &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook"&gt;Cookbook&lt;/a&gt; for full usage!&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Key Techniques 
 &lt;!-- omit in toc --&gt;&lt;/h3&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpm-v-4dot5-framework.png" , width="100%" /&gt; 
&lt;/div&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Architechture: Unified 3D-Resampler for High-density Video Compression.&lt;/strong&gt; MiniCPM-V 4.5 introduces a 3D-Resampler that overcomes the performance-efficiency trade-off in video understanding. By grouping and jointly compressing up to 6 consecutive video frames into just 64 tokens (the same token count used for a single image in MiniCPM-V series), MiniCPM-V 4.5 achieves a 96× compression rate for video tokens. This allows the model to process more video frames without additional LLM computational cost, enabling high-FPS video and long video understanding. The architecture supports unified encoding for images, multi-image inputs, and videos, ensuring seamless capability and knowledge transfer.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Pre-training: Unified Learning for OCR and Knowledge from Documents.&lt;/strong&gt; Existing MLLMs learn OCR capability and knowledge from documents in isolated training approaches. We observe that the essential difference between these two training approaches is the visibility of the text in images. By dynamically corrupting text regions in documents with varying noise levels and asking the model to reconstruct the text, the model learns to adaptively and properly switch between accurate text recognition (when text is visible) and multimodal context-based knowledge reasoning (when text is heavily obscured). This eliminates reliance on error-prone document parsers in knowledge learning from documents, and prevents hallucinations from over-augmented OCR data, resulting in top-tier OCR and multimodal knowledge performance with minimal engineering overhead.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Post-training: Hybrid Fast/Deep Thinking with Multimodal RL.&lt;/strong&gt; MiniCPM-V 4.5 offers a balanced reasoning experience through two switchable modes: fast thinking for efficient daily use and deep thinking for complex tasks. Using a new hybrid reinforcement learning method, the model jointly optimizes both modes, significantly enhancing fast-mode performance without compromising deep-mode capability. Incorporated with &lt;a href="https://github.com/OpenBMB/RLPR"&gt;RLPR&lt;/a&gt; and &lt;a href="https://github.com/RLHF-V/RLAIF-V"&gt;RLAIF-V&lt;/a&gt;, it generalizes robust reasoning skills from broad multimodal data while effectively reducing hallucinations.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Evaluation 
 &lt;!-- omit in toc --&gt;&lt;/h3&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/radar_minicpm_v45.png" , width="60%" /&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv_4_5_evaluation_result.png" , width="80%" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;Inference Efficiency&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;OpenCompass&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="left"&gt; 
 &lt;table style="margin: 0px auto;"&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="left"&gt;Model&lt;/th&gt; 
    &lt;th&gt;Size&lt;/th&gt; 
    &lt;th&gt;Avg Score ↑&lt;/th&gt; 
    &lt;th&gt;Total Inference Time ↓&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody align="center"&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;GLM-4.1V-9B-Thinking&lt;/td&gt; 
    &lt;td&gt;10.3B&lt;/td&gt; 
    &lt;td&gt;76.6&lt;/td&gt; 
    &lt;td&gt;17.5h&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;MiMo-VL-7B-RL&lt;/td&gt; 
    &lt;td&gt;8.3B&lt;/td&gt; 
    &lt;td&gt;76.4&lt;/td&gt; 
    &lt;td&gt;11h&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;MiniCPM-V 4.5&lt;/td&gt; 
    &lt;td&gt;8.7B&lt;/td&gt; 
    &lt;td&gt;&lt;b&gt;77.0&lt;/b&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;b&gt;7.5h&lt;/b&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Video-MME&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="left"&gt; 
 &lt;table style="margin: 0px auto;"&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="left"&gt;Model&lt;/th&gt; 
    &lt;th&gt;Size&lt;/th&gt; 
    &lt;th&gt;Avg Score ↑&lt;/th&gt; 
    &lt;th&gt;Total Inference Time ↓&lt;/th&gt; 
    &lt;th&gt;GPU Mem ↓&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody align="center"&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;Qwen2.5-VL-7B-Instruct&lt;/td&gt; 
    &lt;td&gt;8.3B&lt;/td&gt; 
    &lt;td&gt;71.6&lt;/td&gt; 
    &lt;td&gt;3h&lt;/td&gt; 
    &lt;td&gt;60G&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;GLM-4.1V-9B-Thinking&lt;/td&gt; 
    &lt;td&gt;10.3B&lt;/td&gt; 
    &lt;td&gt;&lt;b&gt;73.6&lt;/b&gt;&lt;/td&gt; 
    &lt;td&gt;2.63h&lt;/td&gt; 
    &lt;td&gt;32G&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;MiniCPM-V 4.5&lt;/td&gt; 
    &lt;td&gt;8.7B&lt;/td&gt; 
    &lt;td&gt;73.5&lt;/td&gt; 
    &lt;td&gt;&lt;b&gt;0.26h&lt;/b&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;b&gt;28G&lt;/b&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;p&gt;Both Video-MME and OpenCompass were evaluated using 8×A100 GPUs for inference. The reported inference time of Video-MME includes full model-side computation, and excludes the external cost of video frame extraction (dependent on specific frame extraction tools) for fair comparison.&lt;/p&gt; 
&lt;h3&gt;Examples 
 &lt;!-- omit in toc --&gt;&lt;/h3&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://www.youtube.com/watch?v=Cn23FujYMMU"&gt;&lt;img src="./assets/minicpmv4_5/MiniCPM-V 4.5-8.26_img.jpeg" , width="70%" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;div style="display: flex; flex-direction: column; align-items: center;"&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv4_5/en_case1.png" alt="en_case1" style="margin-bottom: 5px;" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv4_5/en_case2.png" alt="en_case2" style="margin-bottom: 5px;" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv4_5/en_case3.jpeg" alt="en_case3" style="margin-bottom: 5px;" /&gt; 
&lt;/div&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click to view more cases.&lt;/summary&gt; 
 &lt;div style="display: flex; flex-direction: column; align-items: center;"&gt; 
  &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv4_5/zh_extra.jpeg" alt="zh_extra" style="margin-bottom: 5px;" /&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;p&gt;We deploy MiniCPM-V 4.5 on iPad M4 with &lt;a href="https://github.com/tc-mb/MiniCPM-o-demo-iOS"&gt;iOS demo&lt;/a&gt;. The demo video is the raw screen recording without edition.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv4_5/v45_en_handwriting.gif" width="45%/" /&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv4_5/v45_en_cot.gif" width="45%/" /&gt; &lt;/p&gt;
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv4_5/v45_cn_handwriting.gif" width="45%/" /&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv4_5/v45_cn_travel.gif" width="45%/" /&gt; &lt;/p&gt;
&lt;table align="center"&gt;   
&lt;/table&gt; 
&lt;h2&gt;MiniCPM-o 2.6&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;MiniCPM-o 2.6&lt;/strong&gt; is the latest and most capable model in the MiniCPM-o series. The model is built in an end-to-end fashion based on SigLip-400M, Whisper-medium-300M, ChatTTS-200M, and Qwen2.5-7B with a total of 8B parameters. It exhibits a significant performance improvement over MiniCPM-V 2.6, and introduces new features for real-time speech conversation and multimodal live streaming. Notable features of MiniCPM-o 2.6 include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;🔥 &lt;strong&gt;Leading Visual Capability.&lt;/strong&gt; MiniCPM-o 2.6 achieves an average score of 70.2 on OpenCompass, a comprehensive evaluation of 8 popular benchmarks. &lt;strong&gt;With only 8B parameters, it surpasses widely used proprietary models like GPT-4o-202405, Gemini 1.5 Pro, and Claude 3.5 Sonnet&lt;/strong&gt; for single image understanding. It also &lt;strong&gt;outperforms GPT-4V and Claude 3.5 Sonnet&lt;/strong&gt; in multi-image and video understanding, and shows promising in-context learning capability.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;🎙 &lt;strong&gt;State-of-the-art Speech Capability.&lt;/strong&gt; MiniCPM-o 2.6 supports &lt;strong&gt;bilingual real-time speech conversation with configurable voices&lt;/strong&gt; in English and Chinese. It &lt;strong&gt;outperforms GPT-4o-realtime on audio understanding tasks&lt;/strong&gt; such as ASR and STT translation, and shows &lt;strong&gt;state-of-the-art performance on speech conversation in both semantic and acoustic evaluations in the open-source community&lt;/strong&gt;. It also allows for fun features such as emotion/speed/style control, end-to-end voice cloning, role play, etc.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;🎬 &lt;strong&gt;Strong Multimodal Live Streaming Capability.&lt;/strong&gt; As a new feature, MiniCPM-o 2.6 can &lt;strong&gt;accept continuous video and audio streams independent of user queries, and support real-time speech interaction&lt;/strong&gt;. It &lt;strong&gt;outperforms GPT-4o-202408 and Claude 3.5 Sonnet and shows state-of-the-art performance in the open-source community on StreamingBench&lt;/strong&gt;, a comprehensive benchmark for real-time video understanding, omni-source (video &amp;amp; audio) understanding, and multimodal contextual understanding.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;💪 &lt;strong&gt;Strong OCR Capability and Others.&lt;/strong&gt; Advancing popular visual capabilities from MiniCPM-V series, MiniCPM-o 2.6 can process images with any aspect ratio and up to 1.8 million pixels (e.g., 1344x1344). It achieves &lt;strong&gt;state-of-the-art performance on OCRBench for models under 25B, surpassing proprietary models such as GPT-4o-202405&lt;/strong&gt;. Based on the latest &lt;a href="https://github.com/RLHF-V/RLAIF-V/"&gt;RLAIF-V&lt;/a&gt; and &lt;a href="https://github.com/OpenBMB/VisCPM"&gt;VisCPM&lt;/a&gt; techniques, it features &lt;strong&gt;trustworthy behaviors&lt;/strong&gt;, outperforming GPT-4o and Claude 3.5 Sonnet on MMHal-Bench, and supports &lt;strong&gt;multilingual capabilities&lt;/strong&gt; on more than 30 languages.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;🚀 &lt;strong&gt;Superior Efficiency.&lt;/strong&gt; In addition to its friendly size, MiniCPM-o 2.6 also shows &lt;strong&gt;state-of-the-art token density&lt;/strong&gt; (i.e., the number of pixels encoded into each visual token). &lt;strong&gt;It produces only 640 tokens when processing a 1.8M pixel image, which is 75% fewer than most models&lt;/strong&gt;. This directly improves the inference speed, first-token latency, memory usage, and power consumption. As a result, MiniCPM-o 2.6 can efficiently support &lt;strong&gt;multimodal live streaming&lt;/strong&gt; on end-side devices such as iPads.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;💫 &lt;strong&gt;Easy Usage.&lt;/strong&gt; MiniCPM-o 2.6 can be easily used in various ways: (1) &lt;a href="https://github.com/OpenBMB/llama.cpp/raw/minicpm-omni/examples/llava/README-minicpmo2.6.md"&gt;llama.cpp&lt;/a&gt; support for efficient CPU inference on local devices, (2) &lt;a href="https://huggingface.co/openbmb/MiniCPM-o-2_6-int4"&gt;int4&lt;/a&gt; and &lt;a href="https://huggingface.co/openbmb/MiniCPM-o-2_6-gguf"&gt;GGUF&lt;/a&gt; format quantized models in 16 sizes, (3) &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#efficient-inference-with-llamacpp-ollama-vllm"&gt;vLLM&lt;/a&gt; support for high-throughput and memory-efficient inference, (4) fine-tuning on new domains and tasks with &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/llamafactory_train_and_infer.md"&gt;LLaMA-Factory&lt;/a&gt;, (5) quick &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#chat-with-our-demo-on-gradio"&gt;local WebUI demo&lt;/a&gt;, and (6) online web demo on &lt;a href="https://minicpm-omni-webdemo-us.modelbest.cn/"&gt;server&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Model Architecture.&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;End-to-end Omni-modal Architecture.&lt;/strong&gt; Different modality encoders/decoders are connected and trained in an &lt;strong&gt;end-to-end&lt;/strong&gt; fashion to fully exploit rich multimodal knowledge. The model is trained in a fully end-to-end manner with only CE loss.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Omni-modal Live Streaming Mechanism.&lt;/strong&gt; (1) We change the offline modality encoder/decoders into online ones for &lt;strong&gt;streaming inputs/outputs.&lt;/strong&gt; (2) We devise a &lt;strong&gt;time-division multiplexing (TDM) mechanism&lt;/strong&gt; for omni-modality streaming processing in the LLM backbone. It divides parallel omni-modality streams into sequential info within small periodic time slices.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Configurable Speech Modeling Design.&lt;/strong&gt; We devise a multimodal system prompt, including traditional text system prompt, and &lt;strong&gt;a new audio system prompt to determine the assistant voice&lt;/strong&gt;. This enables flexible voice configurations in inference time, and also facilitates end-to-end voice cloning and description-based voice creation.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpm-o-26-framework-v2.png" , width="80%" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;Evaluation 
 &lt;!-- omit in toc --&gt;&lt;/h3&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/radar.jpg" , width="80%" /&gt; 
&lt;/div&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click to view visual understanding results.&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;Image Understanding&lt;/strong&gt;&lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;table style="margin: 0px auto;"&gt; 
   &lt;thead&gt; 
    &lt;tr&gt; 
     &lt;th align="left"&gt;Model&lt;/th&gt; 
     &lt;th&gt;Size&lt;/th&gt; 
     &lt;th&gt;Token Density&lt;sup&gt;+&lt;/sup&gt;&lt;/th&gt; 
     &lt;th&gt;OpenCompass&lt;/th&gt; 
     &lt;th&gt;OCRBench&lt;/th&gt; 
     &lt;th&gt;MathVista mini&lt;/th&gt; 
     &lt;th&gt;ChartQA&lt;/th&gt; 
     &lt;th&gt;MMVet&lt;/th&gt; 
     &lt;th&gt;MMStar&lt;/th&gt; 
     &lt;th&gt;MME&lt;/th&gt; 
     &lt;th&gt;MMB1.1 test&lt;/th&gt; 
     &lt;th&gt;AI2D&lt;/th&gt; 
     &lt;th&gt;MMMU val&lt;/th&gt; 
     &lt;th&gt;HallusionBench&lt;/th&gt; 
     &lt;th&gt;TextVQA val&lt;/th&gt; 
     &lt;th&gt;DocVQA test&lt;/th&gt; 
     &lt;th&gt;MathVerse mini&lt;/th&gt; 
     &lt;th&gt;MathVision&lt;/th&gt; 
     &lt;th&gt;MMHal Score&lt;/th&gt; 
    &lt;/tr&gt; 
   &lt;/thead&gt; 
   &lt;tbody align="center"&gt; 
    &lt;tr&gt; 
     &lt;td colspan="19" align="left"&gt;&lt;strong&gt;Proprietary&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;GPT-4o-20240513&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;1088&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;69.9&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;736&lt;/td&gt; 
     &lt;td&gt;61.3&lt;/td&gt; 
     &lt;td&gt;85.7&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;69.1&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;63.9&lt;/td&gt; 
     &lt;td&gt;2328.7&lt;/td&gt; 
     &lt;td&gt;82.2&lt;/td&gt; 
     &lt;td&gt;84.6&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;69.2&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;55.0&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;92.8&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;50.2&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;30.4&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;3.6&lt;/u&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Claude3.5-Sonnet&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;750&lt;/td&gt; 
     &lt;td&gt;67.9&lt;/td&gt; 
     &lt;td&gt;788&lt;/td&gt; 
     &lt;td&gt;61.6&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;90.8&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;66.0&lt;/td&gt; 
     &lt;td&gt;62.2&lt;/td&gt; 
     &lt;td&gt;1920.0&lt;/td&gt; 
     &lt;td&gt;78.5&lt;/td&gt; 
     &lt;td&gt;80.2&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;65.9&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;49.9&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;95.2&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;3.4&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Gemini 1.5 Pro&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;64.4&lt;/td&gt; 
     &lt;td&gt;754&lt;/td&gt; 
     &lt;td&gt;57.7&lt;/td&gt; 
     &lt;td&gt;81.3&lt;/td&gt; 
     &lt;td&gt;64.0&lt;/td&gt; 
     &lt;td&gt;59.1&lt;/td&gt; 
     &lt;td&gt;2110.6&lt;/td&gt; 
     &lt;td&gt;73.9&lt;/td&gt; 
     &lt;td&gt;79.1&lt;/td&gt; 
     &lt;td&gt;60.6&lt;/td&gt; 
     &lt;td&gt;45.6&lt;/td&gt; 
     &lt;td&gt;73.5&lt;/td&gt; 
     &lt;td&gt;86.5&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;19.2&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;GPT-4o-mini-20240718&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;1088&lt;/td&gt; 
     &lt;td&gt;64.1&lt;/td&gt; 
     &lt;td&gt;785&lt;/td&gt; 
     &lt;td&gt;52.4&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;66.9&lt;/td&gt; 
     &lt;td&gt;54.8&lt;/td&gt; 
     &lt;td&gt;2003.4&lt;/td&gt; 
     &lt;td&gt;76.0&lt;/td&gt; 
     &lt;td&gt;77.8&lt;/td&gt; 
     &lt;td&gt;60.0&lt;/td&gt; 
     &lt;td&gt;46.1&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;3.3&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td colspan="19" align="left"&gt;&lt;strong&gt;Open Source&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Cambrian-34B&lt;/td&gt; 
     &lt;td&gt;34B&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;1820&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;58.3&lt;/td&gt; 
     &lt;td&gt;591&lt;/td&gt; 
     &lt;td&gt;50.3&lt;/td&gt; 
     &lt;td&gt;75.6&lt;/td&gt; 
     &lt;td&gt;53.2&lt;/td&gt; 
     &lt;td&gt;54.2&lt;/td&gt; 
     &lt;td&gt;2049.9&lt;/td&gt; 
     &lt;td&gt;77.8&lt;/td&gt; 
     &lt;td&gt;79.5&lt;/td&gt; 
     &lt;td&gt;50.4&lt;/td&gt; 
     &lt;td&gt;41.6&lt;/td&gt; 
     &lt;td&gt;76.7&lt;/td&gt; 
     &lt;td&gt;75.5&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;GLM-4V-9B&lt;/td&gt; 
     &lt;td&gt;13B&lt;/td&gt; 
     &lt;td&gt;784&lt;/td&gt; 
     &lt;td&gt;59.1&lt;/td&gt; 
     &lt;td&gt;776&lt;/td&gt; 
     &lt;td&gt;51.1&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;58.0&lt;/td&gt; 
     &lt;td&gt;54.8&lt;/td&gt; 
     &lt;td&gt;2018.8&lt;/td&gt; 
     &lt;td&gt;67.9&lt;/td&gt; 
     &lt;td&gt;71.2&lt;/td&gt; 
     &lt;td&gt;46.9&lt;/td&gt; 
     &lt;td&gt;45.0&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Pixtral-12B&lt;/td&gt; 
     &lt;td&gt;12B&lt;/td&gt; 
     &lt;td&gt;256&lt;/td&gt; 
     &lt;td&gt;61.0&lt;/td&gt; 
     &lt;td&gt;685&lt;/td&gt; 
     &lt;td&gt;56.9&lt;/td&gt; 
     &lt;td&gt;81.8&lt;/td&gt; 
     &lt;td&gt;58.5&lt;/td&gt; 
     &lt;td&gt;54.5&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;72.7&lt;/td&gt; 
     &lt;td&gt;79.0&lt;/td&gt; 
     &lt;td&gt;51.1&lt;/td&gt; 
     &lt;td&gt;47.0&lt;/td&gt; 
     &lt;td&gt;75.7&lt;/td&gt; 
     &lt;td&gt;90.7&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;VITA-1.5&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;784&lt;/td&gt; 
     &lt;td&gt;63.3&lt;/td&gt; 
     &lt;td&gt;741&lt;/td&gt; 
     &lt;td&gt;66.2&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;52.7&lt;/td&gt; 
     &lt;td&gt;60.2&lt;/td&gt; 
     &lt;td&gt;2328.1&lt;/td&gt; 
     &lt;td&gt;76.8&lt;/td&gt; 
     &lt;td&gt;79.2&lt;/td&gt; 
     &lt;td&gt;52.6&lt;/td&gt; 
     &lt;td&gt;44.6&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;DeepSeek-VL2-27B (4B)&lt;/td&gt; 
     &lt;td&gt;27B&lt;/td&gt; 
     &lt;td&gt;672&lt;/td&gt; 
     &lt;td&gt;66.4&lt;/td&gt; 
     &lt;td&gt;809&lt;/td&gt; 
     &lt;td&gt;63.9&lt;/td&gt; 
     &lt;td&gt;86.0&lt;/td&gt; 
     &lt;td&gt;60.0&lt;/td&gt; 
     &lt;td&gt;61.9&lt;/td&gt; 
     &lt;td&gt;2253.0&lt;/td&gt; 
     &lt;td&gt;81.2&lt;/td&gt; 
     &lt;td&gt;83.8&lt;/td&gt; 
     &lt;td&gt;54.0&lt;/td&gt; 
     &lt;td&gt;45.3&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;84.2&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;93.3&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;3.0&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Qwen2-VL-7B&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;784&lt;/td&gt; 
     &lt;td&gt;67.1&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;866&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;58.2&lt;/td&gt; 
     &lt;td&gt;83.0&lt;/td&gt; 
     &lt;td&gt;62.0&lt;/td&gt; 
     &lt;td&gt;60.7&lt;/td&gt; 
     &lt;td&gt;2326.0&lt;/td&gt; 
     &lt;td&gt;81.8&lt;/td&gt; 
     &lt;td&gt;83.0&lt;/td&gt; 
     &lt;td&gt;54.1&lt;/td&gt; 
     &lt;td&gt;50.6&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;84.3&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;94.5&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;31.9&lt;/td&gt; 
     &lt;td&gt;16.3&lt;/td&gt; 
     &lt;td&gt;3.2&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;LLaVA-OneVision-72B&lt;/td&gt; 
     &lt;td&gt;72B&lt;/td&gt; 
     &lt;td&gt;182&lt;/td&gt; 
     &lt;td&gt;68.1&lt;/td&gt; 
     &lt;td&gt;741&lt;/td&gt; 
     &lt;td&gt;67.5&lt;/td&gt; 
     &lt;td&gt;83.7&lt;/td&gt; 
     &lt;td&gt;60.6&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;65.8&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;2261.0&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;85.0&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;85.6&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;56.8&lt;/td&gt; 
     &lt;td&gt;49.0&lt;/td&gt; 
     &lt;td&gt;80.5&lt;/td&gt; 
     &lt;td&gt;91.3&lt;/td&gt; 
     &lt;td&gt;39.1&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;3.5&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;InternVL2.5-8B&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;706&lt;/td&gt; 
     &lt;td&gt;68.3&lt;/td&gt; 
     &lt;td&gt;822&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;64.4&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;84.8&lt;/td&gt; 
     &lt;td&gt;62.8&lt;/td&gt; 
     &lt;td&gt;62.8&lt;/td&gt; 
     &lt;td&gt;2344.0&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;83.6&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;84.5&lt;/td&gt; 
     &lt;td&gt;56.0&lt;/td&gt; 
     &lt;td&gt;50.1&lt;/td&gt; 
     &lt;td&gt;79.1&lt;/td&gt; 
     &lt;td&gt;93.0&lt;/td&gt; 
     &lt;td&gt;39.5&lt;/td&gt; 
     &lt;td&gt;19.7&lt;/td&gt; 
     &lt;td&gt;3.4&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;MiniCPM-V 2.6&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;2822&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;65.2&lt;/td&gt; 
     &lt;td&gt;852*&lt;/td&gt; 
     &lt;td&gt;60.6&lt;/td&gt; 
     &lt;td&gt;79.4&lt;/td&gt; 
     &lt;td&gt;60.0&lt;/td&gt; 
     &lt;td&gt;57.5&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;2348.4*&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;78.0&lt;/td&gt; 
     &lt;td&gt;82.1&lt;/td&gt; 
     &lt;td&gt;49.8*&lt;/td&gt; 
     &lt;td&gt;48.1*&lt;/td&gt; 
     &lt;td&gt;80.1&lt;/td&gt; 
     &lt;td&gt;90.8&lt;/td&gt; 
     &lt;td&gt;25.7&lt;/td&gt; 
     &lt;td&gt;18.3&lt;/td&gt; 
     &lt;td&gt;3.6&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;MiniCPM-o 2.6&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;2822&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;70.2&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;897*&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;71.9*&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;86.9*&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;67.5&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;64.0&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;2372.0*&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;80.5&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;85.8&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;50.4*&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;51.9&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;82.0&lt;/td&gt; 
     &lt;td&gt;93.5&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;41.4*&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;23.1*&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;3.8&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt; 
  &lt;/table&gt; 
 &lt;/div&gt; * We evaluate this benchmark using chain-of-thought prompting. Specifically, for MME, we used this technique only for the Cognition set. 
 &lt;p&gt;&lt;sup&gt;+&lt;/sup&gt; Token Density: number of pixels encoded into each visual token at maximum resolution, i.e., # pixels at maximum resolution / # visual tokens.&lt;/p&gt; 
 &lt;p&gt;Note: For proprietary models, we calculate token density based on the image encoding charging strategy defined in the official API documentation, which provides an upper-bound estimation.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Multi-image and Video Understanding&lt;/strong&gt;&lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;table style="margin: 0px auto;"&gt; 
   &lt;thead&gt; 
    &lt;tr&gt; 
     &lt;th align="left"&gt;Model&lt;/th&gt; 
     &lt;th&gt;Size&lt;/th&gt; 
     &lt;th&gt;BLINK val&lt;/th&gt; 
     &lt;th&gt;Mantis Eval&lt;/th&gt; 
     &lt;th&gt;MIRB&lt;/th&gt; 
     &lt;th&gt;Video-MME (wo / w subs)&lt;/th&gt; 
    &lt;/tr&gt; 
   &lt;/thead&gt; 
   &lt;tbody align="center"&gt; 
    &lt;tr&gt; 
     &lt;td colspan="6" align="left"&gt;&lt;strong&gt;Proprietary&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;GPT-4o-20240513&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;68.0&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;71.9/77.2&lt;strong&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;GPT4V&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;54.6&lt;/td&gt; 
     &lt;td&gt;62.7&lt;/td&gt; 
     &lt;td&gt;53.1&lt;/td&gt; 
     &lt;td&gt;59.9/63.3&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td colspan="6" align="left"&gt;&lt;strong&gt;Open-source&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;VITA-1.5&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;45.0&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;56.1/58.7&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;LLaVA-NeXT-Interleave 14B&lt;/td&gt; 
     &lt;td&gt;14B&lt;/td&gt; 
     &lt;td&gt;52.6&lt;/td&gt; 
     &lt;td&gt;66.4&lt;/td&gt; 
     &lt;td&gt;30.2&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;LLaVA-OneVision-72B&lt;/td&gt; 
     &lt;td&gt;72B&lt;/td&gt; 
     &lt;td&gt;55.4&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;77.6&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;66.2/69.5&lt;/u&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;MANTIS 8B&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;49.1&lt;/td&gt; 
     &lt;td&gt;59.5&lt;/td&gt; 
     &lt;td&gt;34.8&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Qwen2-VL-7B&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;53.2&lt;/td&gt; 
     &lt;td&gt;69.6*&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;67.6*&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;63.3/69.0&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;InternVL2.5-8B&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;54.8&lt;/td&gt; 
     &lt;td&gt;67.7&lt;/td&gt; 
     &lt;td&gt;52.5&lt;/td&gt; 
     &lt;td&gt;64.2/66.9&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;MiniCPM-V 2.6&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;53.0&lt;/td&gt; 
     &lt;td&gt;69.1&lt;/td&gt; 
     &lt;td&gt;53.8&lt;/td&gt; 
     &lt;td&gt;60.9/63.6&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;MiniCPM-o 2.6&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;56.7&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;71.9&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;58.6&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;63.9/67.9&lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt; 
  &lt;/table&gt; 
 &lt;/div&gt; * We evaluate officially released checkpoints by ourselves. 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click to view audio understanding and speech conversation results.&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;Audio Understanding&lt;/strong&gt;&lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;table style="margin: 0px auto;"&gt; 
   &lt;thead&gt; 
    &lt;tr&gt; 
     &lt;th align="left"&gt;Task&lt;/th&gt; 
     &lt;th&gt;Size&lt;/th&gt; 
     &lt;th colspan="3"&gt;ASR (zh)&lt;/th&gt; 
     &lt;th colspan="3"&gt;ASR (en)&lt;/th&gt; 
     &lt;th colspan="2"&gt;AST&lt;/th&gt; 
     &lt;th&gt;Emotion&lt;/th&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;th align="left"&gt;Metric&lt;/th&gt; 
     &lt;td&gt;&lt;/td&gt; 
     &lt;th colspan="3"&gt;CER↓&lt;/th&gt; 
     &lt;th colspan="3"&gt;WER↓&lt;/th&gt; 
     &lt;th colspan="2"&gt;BLEU↑&lt;/th&gt; 
     &lt;th&gt;ACC↑&lt;/th&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;th align="left"&gt;Dataset&lt;/th&gt; 
     &lt;td&gt;&lt;/td&gt; 
     &lt;th&gt;AISHELL-1&lt;/th&gt; 
     &lt;th&gt;Fleurs zh&lt;/th&gt; 
     &lt;th&gt;WenetSpeech test-net&lt;/th&gt; 
     &lt;th&gt;LibriSpeech test-clean&lt;/th&gt; 
     &lt;th&gt;GigaSpeech&lt;/th&gt; 
     &lt;th&gt;TED-LIUM&lt;/th&gt; 
     &lt;th&gt;CoVoST en2zh&lt;/th&gt; 
     &lt;th&gt;CoVoST zh2en&lt;/th&gt; 
     &lt;th&gt;MELD emotion&lt;/th&gt; 
    &lt;/tr&gt; 
   &lt;/thead&gt; 
   &lt;tbody align="center"&gt; 
    &lt;tr&gt; 
     &lt;td colspan="11" align="left"&gt;&lt;strong&gt;Proprietary&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;GPT-4o-Realtime&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;7.3*&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;5.4*&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;28.9*&lt;/td&gt; 
     &lt;td&gt;2.6*&lt;/td&gt; 
     &lt;td&gt;12.9*&lt;/td&gt; 
     &lt;td&gt;4.8*&lt;/td&gt; 
     &lt;td&gt;37.1*&lt;/td&gt; 
     &lt;td&gt;15.7*&lt;/td&gt; 
     &lt;td&gt;33.2*&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Gemini 1.5 Pro&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;4.5*&lt;/td&gt; 
     &lt;td&gt;5.9*&lt;/td&gt; 
     &lt;td&gt;14.3*&lt;/td&gt; 
     &lt;td&gt;2.9*&lt;/td&gt; 
     &lt;td&gt;10.6*&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;3.0*&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;47.3*&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;22.6*&lt;/td&gt; 
     &lt;td&gt;48.4*&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td colspan="11" align="left"&gt;&lt;strong&gt;Open-Source&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Qwen2-Audio-7B&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;7.5&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;1.6&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;45.2&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;24.4&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;55.3&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Qwen2-Audio-7B-Instruct&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;2.6*&lt;/td&gt; 
     &lt;td&gt;6.9*&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;10.3*&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;3.1*&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;9.7&lt;/u&gt;*&lt;/td&gt; 
     &lt;td&gt;5.9*&lt;/td&gt; 
     &lt;td&gt;39.5*&lt;/td&gt; 
     &lt;td&gt;22.9*&lt;/td&gt; 
     &lt;td&gt;17.4*&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;VITA-1.5&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;2.16&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;8.4&lt;/td&gt; 
     &lt;td&gt;3.4&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;GLM-4-Voice-Base&lt;/td&gt; 
     &lt;td&gt;9B&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;2.5&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;2.8&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;MiniCPM-o 2.6&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;1.6&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;4.4&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;6.9&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;1.7&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;8.7&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;3.0&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;48.2&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;27.2&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;52.4&lt;/u&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt; 
  &lt;/table&gt; 
 &lt;/div&gt; * We evaluate officially released checkpoints by ourselves.
 &lt;br /&gt;
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;Speech Generation&lt;/strong&gt;&lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;table style="margin: 0px auto;"&gt; 
   &lt;thead&gt; 
    &lt;tr&gt; 
     &lt;th align="left"&gt;Task&lt;/th&gt; 
     &lt;th&gt;Size&lt;/th&gt; 
     &lt;th colspan="9"&gt;SpeechQA&lt;/th&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;th align="left"&gt;Metric&lt;/th&gt; 
     &lt;th&gt;&lt;/th&gt; 
     &lt;th colspan="3"&gt;ACC↑&lt;/th&gt; 
     &lt;th&gt;G-Eval (10 point)↑&lt;/th&gt; 
     &lt;th&gt;Semantic ELO score↑&lt;/th&gt; 
     &lt;th&gt;Acoustic ELO score↑&lt;/th&gt; 
     &lt;th&gt;Overall ELO score↑&lt;/th&gt; 
     &lt;th&gt;UTMOS↑&lt;/th&gt; 
     &lt;th&gt;ASR-WER↓&lt;/th&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;th align="left"&gt;Dataset&lt;/th&gt; 
     &lt;th&gt;&lt;/th&gt; 
     &lt;th&gt;Speech Llama Q.&lt;/th&gt; 
     &lt;th&gt;Speech Web Q.&lt;/th&gt; 
     &lt;th&gt;Speech Trivia QA&lt;/th&gt; 
     &lt;th&gt;Speech AlpacaEval&lt;/th&gt; 
     &lt;th colspan="5"&gt;AudioArena&lt;/th&gt; 
    &lt;/tr&gt; 
   &lt;/thead&gt; 
   &lt;tbody align="center"&gt; 
    &lt;tr&gt; 
     &lt;td colspan="11" align="left"&gt;&lt;strong&gt;Proprietary&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;GPT-4o-Realtime&lt;/td&gt; 
     &lt;td&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;71.7&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;51.6&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;69.7&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;7.4&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;1157&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;1203&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;1200&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;4.2&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;2.3&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td colspan="11" align="left"&gt;&lt;strong&gt;Open-Source&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;GLM-4-Voice&lt;/td&gt; 
     &lt;td&gt;9B&lt;/td&gt; 
     &lt;td&gt;50.0&lt;/td&gt; 
     &lt;td&gt;32.0&lt;/td&gt; 
     &lt;td&gt;36.4&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;5.1&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;999&lt;/td&gt; 
     &lt;td&gt;1147&lt;/td&gt; 
     &lt;td&gt;1035&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;4.1&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;11.7&lt;/u&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Llama-Omni&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;45.3&lt;/td&gt; 
     &lt;td&gt;22.9&lt;/td&gt; 
     &lt;td&gt;10.7&lt;/td&gt; 
     &lt;td&gt;3.9&lt;/td&gt; 
     &lt;td&gt;960&lt;/td&gt; 
     &lt;td&gt;878&lt;/td&gt; 
     &lt;td&gt;897&lt;/td&gt; 
     &lt;td&gt;3.2&lt;/td&gt; 
     &lt;td&gt;24.3&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;VITA-1.5&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;46.7&lt;/td&gt; 
     &lt;td&gt;28.1&lt;/td&gt; 
     &lt;td&gt;23.3&lt;/td&gt; 
     &lt;td&gt;2.0&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Moshi&lt;/td&gt; 
     &lt;td&gt;7B&lt;/td&gt; 
     &lt;td&gt;43.7&lt;/td&gt; 
     &lt;td&gt;23.8&lt;/td&gt; 
     &lt;td&gt;16.7&lt;/td&gt; 
     &lt;td&gt;2.4&lt;/td&gt; 
     &lt;td&gt;871&lt;/td&gt; 
     &lt;td&gt;808&lt;/td&gt; 
     &lt;td&gt;875&lt;/td&gt; 
     &lt;td&gt;2.8&lt;/td&gt; 
     &lt;td&gt;8.2&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Mini-Omni&lt;/td&gt; 
     &lt;td&gt;1B&lt;/td&gt; 
     &lt;td&gt;22.0&lt;/td&gt; 
     &lt;td&gt;12.8&lt;/td&gt; 
     &lt;td&gt;6.9&lt;/td&gt; 
     &lt;td&gt;2.5&lt;/td&gt; 
     &lt;td&gt;926&lt;/td&gt; 
     &lt;td&gt;803&lt;/td&gt; 
     &lt;td&gt;865&lt;/td&gt; 
     &lt;td&gt;3.4&lt;/td&gt; 
     &lt;td&gt;10.0&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;MiniCPM-o 2.6&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;61.0&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;40.0&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;40.2&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;5.1&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;1088&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;1163&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;1131&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;4.2&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;9.8&lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt; 
  &lt;/table&gt; 
 &lt;/div&gt; All results are from AudioEvals, and the evaluation methods along with further details can be found in 
 &lt;a href="https://github.com/OpenBMB/UltraEval-Audio" target="_blank"&gt;AudioEvals&lt;/a&gt;.
 &lt;br /&gt;
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;End-to-end Voice Cloning&lt;/strong&gt;&lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;table style="margin: 0px auto;"&gt; 
   &lt;thead&gt; 
    &lt;tr&gt; 
     &lt;th align="left"&gt;Task&lt;/th&gt; 
     &lt;th colspan="2"&gt;Voice cloning&lt;/th&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;th align="left"&gt;Metric&lt;/th&gt; 
     &lt;th&gt;SIMO↑&lt;/th&gt; 
     &lt;th&gt;SIMO↑&lt;/th&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;th align="left"&gt;Dataset&lt;/th&gt; 
     &lt;th&gt;Seed-TTS test-zh&lt;/th&gt; 
     &lt;th&gt;Seed-TTS test-en&lt;/th&gt; 
    &lt;/tr&gt; 
   &lt;/thead&gt; 
   &lt;tbody align="center"&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;F5-TTS&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;76&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;67&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;CosyVoice&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;75&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;64&lt;/u&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;FireRedTTS&lt;/td&gt; 
     &lt;td&gt;63&lt;/td&gt; 
     &lt;td&gt;46&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;MiniCPM-o 2.6&lt;/td&gt; 
     &lt;td&gt;57&lt;/td&gt; 
     &lt;td&gt;47&lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt; 
  &lt;/table&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click to view multimodal live streaming results.&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;Multimodal Live Streaming&lt;/strong&gt;: results on StreamingBench&lt;/p&gt; 
 &lt;table style="margin: 0px auto;"&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="left"&gt;Model&lt;/th&gt; 
    &lt;th&gt;Size&lt;/th&gt; 
    &lt;th&gt;Real-Time Video Understanding&lt;/th&gt; 
    &lt;th&gt;Omni-Source Understanding&lt;/th&gt; 
    &lt;th&gt;Contextual Understanding&lt;/th&gt; 
    &lt;th&gt;Overall&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody align="center"&gt; 
   &lt;tr&gt; 
    &lt;td colspan="7" align="left"&gt;&lt;strong&gt;Proprietary&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;Gemini 1.5 Pro&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;&lt;u&gt;77.4&lt;/u&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;67.8&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;51.1&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;70.3&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;GPT-4o-202408&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;74.5&lt;/td&gt; 
    &lt;td&gt;51.0&lt;/td&gt; 
    &lt;td&gt;&lt;u&gt;48.0&lt;/u&gt;&lt;/td&gt; 
    &lt;td&gt;64.1&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;Claude-3.5-Sonnet&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;74.0&lt;/td&gt; 
    &lt;td&gt;41.4&lt;/td&gt; 
    &lt;td&gt;37.8&lt;/td&gt; 
    &lt;td&gt;59.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td colspan="9" align="left"&gt;&lt;strong&gt;Open-source&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;VILA-1.5&lt;/td&gt; 
    &lt;td&gt;8B&lt;/td&gt; 
    &lt;td&gt;61.5&lt;/td&gt; 
    &lt;td&gt;37.5&lt;/td&gt; 
    &lt;td&gt;26.7&lt;/td&gt; 
    &lt;td&gt;49.5&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;LongVA&lt;/td&gt; 
    &lt;td&gt;7B&lt;/td&gt; 
    &lt;td&gt;63.1&lt;/td&gt; 
    &lt;td&gt;35.9&lt;/td&gt; 
    &lt;td&gt;30.2&lt;/td&gt; 
    &lt;td&gt;50.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;LLaVA-Next-Video-34B&lt;/td&gt; 
    &lt;td&gt;34B&lt;/td&gt; 
    &lt;td&gt;69.8&lt;/td&gt; 
    &lt;td&gt;41.7&lt;/td&gt; 
    &lt;td&gt;34.3&lt;/td&gt; 
    &lt;td&gt;56.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;Qwen2-VL-7B&lt;/td&gt; 
    &lt;td&gt;8B&lt;/td&gt; 
    &lt;td&gt;71.2&lt;/td&gt; 
    &lt;td&gt;40.7&lt;/td&gt; 
    &lt;td&gt;33.1&lt;/td&gt; 
    &lt;td&gt;57.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;InternVL2-8B&lt;/td&gt; 
    &lt;td&gt;8B&lt;/td&gt; 
    &lt;td&gt;70.1&lt;/td&gt; 
    &lt;td&gt;42.7&lt;/td&gt; 
    &lt;td&gt;34.1&lt;/td&gt; 
    &lt;td&gt;57.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;VITA-1.5&lt;/td&gt; 
    &lt;td&gt;8B&lt;/td&gt; 
    &lt;td&gt;70.9&lt;/td&gt; 
    &lt;td&gt;40.8&lt;/td&gt; 
    &lt;td&gt;35.8&lt;/td&gt; 
    &lt;td&gt;57.4&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;LLaVA-OneVision-7B&lt;/td&gt; 
    &lt;td&gt;8B&lt;/td&gt; 
    &lt;td&gt;74.3&lt;/td&gt; 
    &lt;td&gt;40.8&lt;/td&gt; 
    &lt;td&gt;31.0&lt;/td&gt; 
    &lt;td&gt;58.4&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;InternLM-XC2.5-OL-7B&lt;/td&gt; 
    &lt;td&gt;8B&lt;/td&gt; 
    &lt;td&gt;75.4&lt;/td&gt; 
    &lt;td&gt;46.2&lt;/td&gt; 
    &lt;td&gt;33.6&lt;/td&gt; 
    &lt;td&gt;60.8&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;MiniCPM-V 2.6&lt;/td&gt; 
    &lt;td&gt;8B&lt;/td&gt; 
    &lt;td&gt;72.4&lt;/td&gt; 
    &lt;td&gt;40.2&lt;/td&gt; 
    &lt;td&gt;33.4&lt;/td&gt; 
    &lt;td&gt;57.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;MiniCPM-o 2.6&lt;/td&gt; 
    &lt;td&gt;8B&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;79.9&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;u&gt;53.4&lt;/u&gt;&lt;/td&gt; 
    &lt;td&gt;38.5&lt;/td&gt; 
    &lt;td&gt;&lt;u&gt;66.0&lt;/u&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h3&gt;Examples 
 &lt;!-- omit in toc --&gt;&lt;/h3&gt; 
&lt;p&gt;We deploy MiniCPM-o 2.6 on end devices. The demo video is the raw-speed recording on an iPad Pro and a Web demo.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://www.youtube.com/watch?v=vRIMbxJzStY&amp;amp;t=2s"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmo2_6/2dot6_o_demo_video_img.png" , width="70%" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;div style="display: flex; flex-direction: column; align-items: center;"&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmo2_6/minicpmo2_6_math_intersect.png" alt="math" style="margin-bottom: 5px;" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmo2_6/minicpmo2_6_diagram_train_NN.png" alt="diagram" style="margin-bottom: 5px;" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmo2_6/minicpmo2_6_multi-image_bike.png" alt="bike" style="margin-bottom: 5px;" /&gt; 
&lt;/div&gt; 
&lt;h2&gt;Legacy Models 
 &lt;!-- omit in toc --&gt;&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Model&lt;/th&gt; 
   &lt;th align="center"&gt;Introduction and Guidance&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-V 4.0&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/minicpm_v4_en.md"&gt;Document&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-V 2.6&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/minicpm_v2dot6_en.md"&gt;Document&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-Llama3-V 2.5&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/minicpm_llama3_v2dot5.md"&gt;Document&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-V 2.0&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/minicpm_v2.md"&gt;Document&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-V 1.0&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/minicpm_v1.md"&gt;Document&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;OmniLMM-12B&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/omnilmm_en.md"&gt;Document&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;MiniCPM-V &amp;amp; o Cookbook&lt;/h2&gt; 
&lt;p&gt;Discover comprehensive, ready-to-deploy solutions for the MiniCPM-V and MiniCPM-o model series in our structured &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook"&gt;cookbook&lt;/a&gt;, which empowers developers to rapidly implement multimodal AI applications with integrated vision, speech, and live-streaming capabilities. Key features include:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Easy Usage Documentation&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Our comprehensive &lt;a href="https://minicpm-o.readthedocs.io/en/latest/index.html"&gt;documentation website&lt;/a&gt; presents every recipe in a clear, well-organized manner. All features are displayed at a glance, making it easy for you to quickly find exactly what you need.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Broad User Spectrum&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;We support a wide range of users, from individuals to enterprises and researchers.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Individuals&lt;/strong&gt;: Enjoy effortless inference using &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/deployment/ollama/minicpm-v4_ollama.md"&gt;Ollama&lt;/a&gt; and &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/deployment/llama.cpp/minicpm-v4_llamacpp.md"&gt;Llama.cpp&lt;/a&gt; with minimal setup.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Enterprises&lt;/strong&gt;: Achieve high-throughput, scalable performance with &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/deployment/vllm/minicpm-v4_vllm.md"&gt;vLLM&lt;/a&gt; and &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/deployment/sglang/MiniCPM-v4_sglang.md"&gt;SGLang&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Researchers&lt;/strong&gt;: Leverage advanced frameworks including &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/finetune/finetune_full.md"&gt;Transformers&lt;/a&gt;, &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/finetune/finetune_llamafactory.md"&gt;LLaMA-Factory&lt;/a&gt;, &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/finetune/swift.md"&gt;SWIFT&lt;/a&gt;, and &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/finetune/align_anything.md"&gt;Align-anything&lt;/a&gt; to enable flexible model development and cutting-edge experimentation.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Versatile Deployment Scenarios&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Our ecosystem delivers optimal solution for a variety of hardware environments and deployment demands.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Web demo&lt;/strong&gt;: Launch interactive multimodal AI web demo with &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/demo/README.md"&gt;FastAPI&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Quantized deployment&lt;/strong&gt;: Maximize efficiency and minimize resource consumption using &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/quantization/gguf/minicpm-v4_gguf_quantize.md"&gt;GGUF&lt;/a&gt; and &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/quantization/bnb/minicpm-v4_bnb_quantize.md"&gt;BNB&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;End devices&lt;/strong&gt;: Bring powerful AI experiences to &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/demo/ios_demo/ios.md"&gt;iPhone and iPad&lt;/a&gt;, supporting offline and privacy-sensitive applications.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Chat with Our Demo on Gradio 🤗&lt;/h2&gt; 
&lt;p&gt;We provide online and local demos powered by Hugging Face Gradio &lt;a href="https://github.com/gradio-app/gradio"&gt;&lt;img src="https://img.shields.io/github/stars/gradio-app/gradio" /&gt;&lt;/a&gt;, the most popular model deployment framework nowadays. It supports streaming outputs, progress bars, queuing, alerts, and other useful features.&lt;/p&gt; 
&lt;h3&gt;Online Demo 
 &lt;!-- omit in toc --&gt;&lt;/h3&gt; 
&lt;p&gt;Click here to try out the online demo of &lt;a href="https://minicpm-omni-webdemo-us.modelbest.cn/"&gt;MiniCPM-o 2.6&lt;/a&gt; | &lt;a href="http://120.92.209.146:8887/"&gt;MiniCPM-V 2.6&lt;/a&gt; | &lt;a href="https://huggingface.co/spaces/openbmb/MiniCPM-Llama3-V-2_5"&gt;MiniCPM-Llama3-V 2.5&lt;/a&gt; | &lt;a href="https://huggingface.co/spaces/openbmb/MiniCPM-V-2"&gt;MiniCPM-V 2.0&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Local WebUI Demo 
 &lt;!-- omit in toc --&gt;&lt;/h3&gt; 
&lt;p&gt;You can easily build your own local WebUI demo using the following commands.&lt;/p&gt; 
&lt;p&gt;Please ensure that &lt;code&gt;transformers==4.44.2&lt;/code&gt; is installed, as other versions may have compatibility issues.&lt;/p&gt; 
&lt;p&gt;If you are using an older version of PyTorch, you might encounter this issue &lt;code&gt;"weight_norm_fwd_first_dim_kernel" not implemented for 'BFloat16'&lt;/code&gt;, Please add &lt;code&gt;self.minicpmo_model.tts.float()&lt;/code&gt; during the model initialization.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;For real-time voice/video call demo:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;launch model server:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install -r requirements_o2.6.txt

python web_demos/minicpm-o_2.6/model_server.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;launch web server:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# Make sure Node and PNPM is installed.
sudo apt-get update
sudo apt-get install nodejs npm
npm install -g pnpm


cd web_demos/minicpm-o_2.6/web_server
# create ssl cert for https, https is required to request camera and microphone permissions.
bash ./make_ssl_cert.sh  # output key.pem and cert.pem

pnpm install  # install requirements
pnpm run dev  # start server
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Open &lt;code&gt;https://localhost:8088/&lt;/code&gt; in browser and enjoy the real-time voice/video call.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;For chatbot demo:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install -r requirements_o2.6.txt

python web_demos/minicpm-o_2.6/chatbot_web_demo_o2.6.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Open &lt;code&gt;http://localhost:8000/&lt;/code&gt; in browser and enjoy the vision mode chatbot.&lt;/p&gt; 
&lt;h2&gt;Inference&lt;/h2&gt; 
&lt;h3&gt;Model Zoo&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Model&lt;/th&gt; 
   &lt;th align="center"&gt;Device&lt;/th&gt; 
   &lt;th align="center"&gt;Memory&lt;/th&gt; 
   &lt;th align="left"&gt;         Description&lt;/th&gt; 
   &lt;th align="center"&gt;Download&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-V 4.5&lt;/td&gt; 
   &lt;td align="center"&gt;GPU&lt;/td&gt; 
   &lt;td align="center"&gt;18 GB&lt;/td&gt; 
   &lt;td align="left"&gt;The latest version, strong end-side multimodal performance for single image, multi-image and video understanding.&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/openbmb/MiniCPM-V-4_5"&gt;🤗&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href="https://modelscope.cn/models/OpenBMB/MiniCPM-V-4_5"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelscope_logo.png" width="20px" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-V 4.5 gguf&lt;/td&gt; 
   &lt;td align="center"&gt;CPU&lt;/td&gt; 
   &lt;td align="center"&gt;8 GB&lt;/td&gt; 
   &lt;td align="left"&gt;The gguf version, lower memory usage and faster inference.&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/openbmb/MiniCPM-V-4_5-gguf"&gt;🤗&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href="https://modelscope.cn/models/OpenBMB/MiniCPM-V-4_5-gguf"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelscope_logo.png" width="20px" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-V 4.5 int4&lt;/td&gt; 
   &lt;td align="center"&gt;GPU&lt;/td&gt; 
   &lt;td align="center"&gt;9 GB&lt;/td&gt; 
   &lt;td align="left"&gt;The int4 quantized version, lower GPU memory usage.&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/openbmb/MiniCPM-V-4_5-int4"&gt;🤗&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href="https://modelscope.cn/models/OpenBMB/MiniCPM-V-4_5-int4"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelscope_logo.png" width="20px" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-V 4.5 AWQ&lt;/td&gt; 
   &lt;td align="center"&gt;GPU&lt;/td&gt; 
   &lt;td align="center"&gt;9 GB&lt;/td&gt; 
   &lt;td align="left"&gt;The int4 quantized version, lower GPU memory usage.&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/openbmb/MiniCPM-V-4_5-AWQ"&gt;🤗&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href="https://modelscope.cn/models/OpenBMB/MiniCPM-V-4_5-AWQ"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelscope_logo.png" width="20px" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-o 2.6&lt;/td&gt; 
   &lt;td align="center"&gt;GPU&lt;/td&gt; 
   &lt;td align="center"&gt;18 GB&lt;/td&gt; 
   &lt;td align="left"&gt;The latest version, achieving GPT-4o level performance for vision, speech and multimodal live streaming on end-side devices.&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/openbmb/MiniCPM-o-2_6"&gt;🤗&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href="https://modelscope.cn/models/OpenBMB/MiniCPM-o-2_6"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelscope_logo.png" width="20px" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-o 2.6 gguf&lt;/td&gt; 
   &lt;td align="center"&gt;CPU&lt;/td&gt; 
   &lt;td align="center"&gt;8 GB&lt;/td&gt; 
   &lt;td align="left"&gt;The gguf version, lower memory usage and faster inference.&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/openbmb/MiniCPM-o-2_6-gguf"&gt;🤗&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href="https://modelscope.cn/models/OpenBMB/MiniCPM-o-2_6-gguf"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelscope_logo.png" width="20px" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-o 2.6 int4&lt;/td&gt; 
   &lt;td align="center"&gt;GPU&lt;/td&gt; 
   &lt;td align="center"&gt;9 GB&lt;/td&gt; 
   &lt;td align="left"&gt;The int4 quantized version, lower GPU memory usage.&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/openbmb/MiniCPM-o-2_6-int4"&gt;🤗&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href="https://modelscope.cn/models/OpenBMB/MiniCPM-o-2_6-int4"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelscope_logo.png" width="20px" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Multi-turn Conversation&lt;/h3&gt; 
&lt;p&gt;If you wish to enable long-thinking mode, provide the argument &lt;code&gt;enable_thinking=True&lt;/code&gt; to the chat function.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install -r requirements_o2.6.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Please refer to the following codes to run.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmo2_6/show_demo.jpg" width="500px" /&gt; 
&lt;/div&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import torch
from PIL import Image
from transformers import AutoModel, AutoTokenizer

torch.manual_seed(100)

model = AutoModel.from_pretrained('openbmb/MiniCPM-V-4_5', trust_remote_code=True, # or openbmb/MiniCPM-o-2_6
    attn_implementation='sdpa', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager
model = model.eval().cuda()
tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-4_5', trust_remote_code=True) # or openbmb/MiniCPM-o-2_6

image = Image.open('./assets/minicpmo2_6/show_demo.jpg').convert('RGB')

enable_thinking=False # If `enable_thinking=True`, the long-thinking mode is enabled.

# First round chat 
question = "What is the landform in the picture?"
msgs = [{'role': 'user', 'content': [image, question]}]

answer = model.chat(
    msgs=msgs,
    tokenizer=tokenizer,
    enable_thinking=enable_thinking
)
print(answer)

# Second round chat, pass history context of multi-turn conversation
msgs.append({"role": "assistant", "content": [answer]})
msgs.append({"role": "user", "content": ["What should I pay attention to when traveling here?"]})

answer = model.chat(
    msgs=msgs,
    tokenizer=tokenizer
)
print(answer)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You will get the following output:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# round1
The landform in the picture is karst topography. Karst landscapes are characterized by distinctive, jagged limestone hills or mountains with steep, irregular peaks and deep valleys—exactly what you see here These unique formations result from the dissolution of soluble rocks like limestone over millions of years through water erosion.

This scene closely resembles the famous karst landscape of Guilin and Yangshuo in China’s Guangxi Province. The area features dramatic, pointed limestone peaks rising dramatically above serene rivers and lush green forests, creating a breathtaking and iconic natural beauty that attracts millions of visitors each year for its picturesque views.

# round2
When traveling to a karst landscape like this, here are some important tips:

1. Wear comfortable shoes: The terrain can be uneven and hilly.
2. Bring water and snacks for energy during hikes or boat rides.
3. Protect yourself from the sun with sunscreen, hats, and sunglasses—especially since you’ll likely spend time outdoors exploring scenic spots.
4. Respect local customs and nature regulations by not littering or disturbing wildlife.

By following these guidelines, you'll have a safe and enjoyable trip while appreciating the stunning natural beauty of places such as Guilin’s karst mountains.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Chat with Multiple Images&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt; Click to view Python code running MiniCPM-V-4_5 with multiple images input. &lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import torch
from PIL import Image
from transformers import AutoModel, AutoTokenizer

model = AutoModel.from_pretrained('openbmb/MiniCPM-V-4_5', trust_remote_code=True,  # or openbmb/MiniCPM-o-2_6
    attn_implementation='sdpa', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager
model = model.eval().cuda()
tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-4_5', trust_remote_code=True)  # or openbmb/MiniCPM-o-2_6

image1 = Image.open('image1.jpg').convert('RGB')
image2 = Image.open('image2.jpg').convert('RGB')
question = 'Compare image 1 and image 2, tell me about the differences between image 1 and image 2.'

msgs = [{'role': 'user', 'content': [image1, image2, question]}]

answer = model.chat(
    msgs=msgs,
    tokenizer=tokenizer
)
print(answer)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h4&gt;In-context Few-shot Learning&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt; Click to view Python code running MiniCPM-V-4_5 with few-shot input. &lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import torch
from PIL import Image
from transformers import AutoModel, AutoTokenizer

model = AutoModel.from_pretrained('openbmb/MiniCPM-V-4_5', trust_remote_code=True,  # or openbmb/MiniCPM-o-2_6
    attn_implementation='sdpa', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager
model = model.eval().cuda()
tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-4_5', trust_remote_code=True)  # or openbmb/MiniCPM-o-2_6

question = "production date" 
image1 = Image.open('example1.jpg').convert('RGB')
answer1 = "2023.08.04"
image2 = Image.open('example2.jpg').convert('RGB')
answer2 = "2007.04.24"
image_test = Image.open('test.jpg').convert('RGB')

msgs = [
    {'role': 'user', 'content': [image1, question]}, {'role': 'assistant', 'content': [answer1]},
    {'role': 'user', 'content': [image2, question]}, {'role': 'assistant', 'content': [answer2]},
    {'role': 'user', 'content': [image_test, question]}
]

answer = model.chat(
    msgs=msgs,
    tokenizer=tokenizer
)
print(answer)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h4&gt;Chat with Video&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt; Click to view Python code running MiniCPM-V-4_5 by with video input and 3D-Resampler. &lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;## The 3d-resampler compresses multiple frames into 64 tokens by introducing temporal_ids. 
# To achieve this, you need to organize your video data into two corresponding sequences: 
#   frames: List[Image]
#   temporal_ids: List[List[Int]].

import torch
from PIL import Image
from transformers import AutoModel, AutoTokenizer
from decord import VideoReader, cpu    # pip install decord
from scipy.spatial import cKDTree
import numpy as np
import math

model = AutoModel.from_pretrained('openbmb/MiniCPM-V-4_5', trust_remote_code=True,  # or openbmb/MiniCPM-o-2_6
    attn_implementation='sdpa', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager
model = model.eval().cuda()
tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-4_5', trust_remote_code=True)  # or openbmb/MiniCPM-o-2_6

MAX_NUM_FRAMES=180 # Indicates the maximum number of frames received after the videos are packed. The actual maximum number of valid frames is MAX_NUM_FRAMES * MAX_NUM_PACKING.
MAX_NUM_PACKING=3  # indicates the maximum packing number of video frames. valid range: 1-6
TIME_SCALE = 0.1 

def map_to_nearest_scale(values, scale):
    tree = cKDTree(np.asarray(scale)[:, None])
    _, indices = tree.query(np.asarray(values)[:, None])
    return np.asarray(scale)[indices]


def group_array(arr, size):
    return [arr[i:i+size] for i in range(0, len(arr), size)]

def encode_video(video_path, choose_fps=3, force_packing=None):
    def uniform_sample(l, n):
        gap = len(l) / n
        idxs = [int(i * gap + gap / 2) for i in range(n)]
        return [l[i] for i in idxs]
    vr = VideoReader(video_path, ctx=cpu(0))
    fps = vr.get_avg_fps()
    video_duration = len(vr) / fps
        
    if choose_fps * int(video_duration) &amp;lt;= MAX_NUM_FRAMES:
        packing_nums = 1
        choose_frames = round(min(choose_fps, round(fps)) * min(MAX_NUM_FRAMES, video_duration))
        
    else:
        packing_nums = math.ceil(video_duration * choose_fps / MAX_NUM_FRAMES)
        if packing_nums &amp;lt;= MAX_NUM_PACKING:
            choose_frames = round(video_duration * choose_fps)
        else:
            choose_frames = round(MAX_NUM_FRAMES * MAX_NUM_PACKING)
            packing_nums = MAX_NUM_PACKING

    frame_idx = [i for i in range(0, len(vr))]      
    frame_idx =  np.array(uniform_sample(frame_idx, choose_frames))

    if force_packing:
        packing_nums = min(force_packing, MAX_NUM_PACKING)
    
    print(video_path, ' duration:', video_duration)
    print(f'get video frames={len(frame_idx)}, packing_nums={packing_nums}')
    
    frames = vr.get_batch(frame_idx).asnumpy()

    frame_idx_ts = frame_idx / fps
    scale = np.arange(0, video_duration, TIME_SCALE)

    frame_ts_id = map_to_nearest_scale(frame_idx_ts, scale) / TIME_SCALE
    frame_ts_id = frame_ts_id.astype(np.int32)

    assert len(frames) == len(frame_ts_id)

    frames = [Image.fromarray(v.astype('uint8')).convert('RGB') for v in frames]
    frame_ts_id_group = group_array(frame_ts_id, packing_nums)
    
    return frames, frame_ts_id_group


video_path="video_test.mp4"
fps = 5 # fps for video
force_packing = None # You can set force_packing to ensure that 3D packing is forcibly enabled; otherwise, encode_video will dynamically set the packing quantity based on the duration.
frames, frame_ts_id_group = encode_video(video_path, fps, force_packing=force_packing)

question = "Describe the video"
msgs = [
    {'role': 'user', 'content': frames + [question]}, 
]


answer = model.chat(
    msgs=msgs,
    tokenizer=tokenizer,
    use_image_id=False,
    max_slice_nums=1,
    temporal_ids=frame_ts_id_group
)
print(answer)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h4&gt;Speech and Audio Mode&lt;/h4&gt; 
&lt;p&gt;Model initialization&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import torch
import librosa
from transformers import AutoModel, AutoTokenizer

model = AutoModel.from_pretrained('openbmb/MiniCPM-o-2_6', trust_remote_code=True,
    attn_implementation='sdpa', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager
model = model.eval().cuda()
tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-o-2_6', trust_remote_code=True)

model.init_tts()
model.tts.float()
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h5&gt;Mimick 
 &lt;!-- omit in toc --&gt;&lt;/h5&gt; 
&lt;p&gt;&lt;code&gt;Mimick&lt;/code&gt; task reflects a model's end-to-end speech modeling capability. The model takes audio input, and outputs an ASR transcription and subsequently reconstructs the original audio with high similarity. The higher the similarity between the reconstructed audio and the original audio, the stronger the model's foundational capability in end-to-end speech modeling.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;mimick_prompt = "Please repeat each user's speech, including voice style and speech content."
audio_input, _ = librosa.load('./assets/input_examples/Trump_WEF_2018_10s.mp3', sr=16000, mono=True) # load the audio to be mimicked

# `./assets/input_examples/fast-pace.wav`, 
# `./assets/input_examples/chi-english-1.wav` 
# `./assets/input_examples/exciting-emotion.wav` 
# for different aspects of speech-centric features.

msgs = [{'role': 'user', 'content': [mimick_prompt, audio_input]}]
res = model.chat(
    msgs=msgs,
    tokenizer=tokenizer,
    sampling=True,
    max_new_tokens=128,
    use_tts_template=True,
    temperature=0.3,
    generate_audio=True,
    output_audio_path='output_mimick.wav', # save the tts result to output_audio_path
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h5&gt;General Speech Conversation with Configurable Voices 
 &lt;!-- omit in toc --&gt;&lt;/h5&gt; 
&lt;p&gt;A general usage scenario of &lt;code&gt;MiniCPM-o-2.6&lt;/code&gt; is role-playing a specific character based on the audio prompt. It will mimic the voice of the character to some extent and act like the character in text, including language style. In this mode, &lt;code&gt;MiniCPM-o-2.6&lt;/code&gt; sounds &lt;strong&gt;more natural and human-like&lt;/strong&gt;. Self-defined audio prompts can be used to customize the voice of the character in an end-to-end manner.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;ref_audio, _ = librosa.load('./assets/input_examples/icl_20.wav', sr=16000, mono=True) # load the reference audio
sys_prompt = model.get_sys_prompt(ref_audio=ref_audio, mode='audio_roleplay', language='en')

# round one
user_question = {'role': 'user', 'content': [librosa.load('xxx.wav', sr=16000, mono=True)[0]]}
msgs = [sys_prompt, user_question]
res = model.chat(
    msgs=msgs,
    tokenizer=tokenizer,
    sampling=True,
    max_new_tokens=128,
    use_tts_template=True,
    generate_audio=True,
    temperature=0.3,
    output_audio_path='result_roleplay_round_1.wav',
)

# round two
history = msgs.append({'role': 'assistant', 'content': res})
user_question = {'role': 'user', 'content': [librosa.load('xxx.wav', sr=16000, mono=True)[0]]}
msgs = history.append(user_question)
res = model.chat(
    msgs=msgs,
    tokenizer=tokenizer,
    sampling=True,
    max_new_tokens=128,
    use_tts_template=True,
    generate_audio=True,
    temperature=0.3,
    output_audio_path='result_roleplay_round_2.wav',
)
print(res)
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h5&gt;Speech Conversation as an AI Assistant 
 &lt;!-- omit in toc --&gt;&lt;/h5&gt; 
&lt;p&gt;An enhanced feature of &lt;code&gt;MiniCPM-o-2.6&lt;/code&gt; is to act as an AI assistant, but only with limited choice of voices. In this mode, &lt;code&gt;MiniCPM-o-2.6&lt;/code&gt; is &lt;strong&gt;less human-like and more like a voice assistant&lt;/strong&gt;. In this mode, the model is more instruction-following. For demo, you are suggested to use &lt;code&gt;assistant_female_voice&lt;/code&gt;, &lt;code&gt;assistant_male_voice&lt;/code&gt;, and &lt;code&gt;assistant_default_female_voice&lt;/code&gt;. Other voices may work but not as stable as the default voices.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Please note that, &lt;code&gt;assistant_female_voice&lt;/code&gt; and &lt;code&gt;assistant_male_voice&lt;/code&gt; are more stable but sounds like robots, while &lt;code&gt;assistant_default_female_voice&lt;/code&gt; is more human-alike but not stable, its voice often changes in multiple turns. We suggest you to try stable voices &lt;code&gt;assistant_female_voice&lt;/code&gt; and &lt;code&gt;assistant_male_voice&lt;/code&gt;.&lt;/em&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;ref_audio, _ = librosa.load('./assets/input_examples/assistant_female_voice.wav', sr=16000, mono=True) # or use `./assets/input_examples/assistant_male_voice.wav`
sys_prompt = model.get_sys_prompt(ref_audio=ref_audio, mode='audio_assistant', language='en') 
user_question = {'role': 'user', 'content': [librosa.load('xxx.wav', sr=16000, mono=True)[0]]} # load the user's audio question

# round one
msgs = [sys_prompt, user_question]
res = model.chat(
    msgs=msgs,
    tokenizer=tokenizer,
    sampling=True,
    max_new_tokens=128,
    use_tts_template=True,
    generate_audio=True,
    temperature=0.3,
    output_audio_path='result_assistant_round_1.wav',
)

# round two
history = msgs.append({'role': 'assistant', 'content': res})
user_question = {'role': 'user', 'content': [librosa.load('xxx.wav', sr=16000, mono=True)[0]]}
msgs = history.append(user_question)
res = model.chat(
    msgs=msgs,
    tokenizer=tokenizer,
    sampling=True,
    max_new_tokens=128,
    use_tts_template=True,
    generate_audio=True,
    temperature=0.3,
    output_audio_path='result_assistant_round_2.wav',
)
print(res)
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h5&gt;Instruction-to-Speech 
 &lt;!-- omit in toc --&gt;&lt;/h5&gt; 
&lt;p&gt;&lt;code&gt;MiniCPM-o-2.6&lt;/code&gt; can also do Instruction-to-Speech, aka &lt;strong&gt;Voice Creation&lt;/strong&gt;. You can describe a voice in detail, and the model will generate a voice that matches the description. For more Instruction-to-Speech sample instructions, you can refer to &lt;a href="https://voxinstruct.github.io/VoxInstruct/"&gt;https://voxinstruct.github.io/VoxInstruct/&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;instruction = 'Speak like a male charming superstar, radiating confidence and style in every word.'

msgs = [{'role': 'user', 'content': [instruction]}]

res = model.chat(
    msgs=msgs,
    tokenizer=tokenizer,
    sampling=True,
    max_new_tokens=128,
    use_tts_template=True,
    generate_audio=True,
    temperature=0.3,
    output_audio_path='result_voice_creation.wav',
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h5&gt;Voice Cloning 
 &lt;!-- omit in toc --&gt;&lt;/h5&gt; 
&lt;p&gt;&lt;code&gt;MiniCPM-o-2.6&lt;/code&gt; can also do zero-shot text-to-speech, aka &lt;strong&gt;Voice Cloning&lt;/strong&gt;. With this mode, model will act like a TTS model.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;ref_audio, _ = librosa.load('./assets/input_examples/icl_20.wav', sr=16000, mono=True) # load the reference audio
sys_prompt = model.get_sys_prompt(ref_audio=ref_audio, mode='voice_cloning', language='en')
text_prompt = f"Please read the text below."
user_question = {'role': 'user', 'content': [text_prompt, "content that you want to read"]}

msgs = [sys_prompt, user_question]
res = model.chat(
    msgs=msgs,
    tokenizer=tokenizer,
    sampling=True,
    max_new_tokens=128,
    use_tts_template=True,
    generate_audio=True,
    temperature=0.3,
    output_audio_path='result_voice_cloning.wav',
)

&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h5&gt;Addressing Various Audio Understanding Tasks 
 &lt;!-- omit in toc --&gt;&lt;/h5&gt; 
&lt;p&gt;&lt;code&gt;MiniCPM-o-2.6&lt;/code&gt; can also be used to address various audio understanding tasks, such as ASR, speaker analysis, general audio captioning, and sound scene tagging.&lt;/p&gt; 
&lt;p&gt;For audio-to-text tasks, you can use the following prompts:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ASR with ZH(same as AST en2zh): &lt;code&gt;请仔细听这段音频片段，并将其内容逐字记录。&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;ASR with EN(same as AST zh2en): &lt;code&gt;Please listen to the audio snippet carefully and transcribe the content.&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Speaker Analysis: &lt;code&gt;Based on the speaker's content, speculate on their gender, condition, age range, and health status.&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;General Audio Caption: &lt;code&gt;Summarize the main content of the audio.&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;General Sound Scene Tagging: &lt;code&gt;Utilize one keyword to convey the audio's content or the associated scene.&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;task_prompt = "Please listen to the audio snippet carefully and transcribe the content." + "\n" # can change to other prompts.
audio_input, _ = librosa.load('./assets/input_examples/audio_understanding.mp3', sr=16000, mono=True) # load the audio to be captioned

msgs = [{'role': 'user', 'content': [task_prompt, audio_input]}]

res = model.chat(
    msgs=msgs,
    tokenizer=tokenizer,
    sampling=True,
    max_new_tokens=128,
    use_tts_template=True,
    generate_audio=True,
    temperature=0.3,
    output_audio_path='result_audio_understanding.wav',
)
print(res)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Multimodal Live Streaming&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt; Click to view Python code running MiniCPM-o 2.6 with chat inference. &lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import math
import numpy as np
from PIL import Image
from moviepy.editor import VideoFileClip
import tempfile
import librosa
import soundfile as sf
import torch
from transformers import AutoModel, AutoTokenizer

def get_video_chunk_content(video_path, flatten=True):
    video = VideoFileClip(video_path)
    print('video_duration:', video.duration)
    
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=True) as temp_audio_file:
        temp_audio_file_path = temp_audio_file.name
        video.audio.write_audiofile(temp_audio_file_path, codec="pcm_s16le", fps=16000)
        audio_np, sr = librosa.load(temp_audio_file_path, sr=16000, mono=True)
    num_units = math.ceil(video.duration)
    
    # 1 frame + 1s audio chunk
    contents= []
    for i in range(num_units):
        frame = video.get_frame(i+1)
        image = Image.fromarray((frame).astype(np.uint8))
        audio = audio_np[sr*i:sr*(i+1)]
        if flatten:
            contents.extend(["&amp;lt;unit&amp;gt;", image, audio])
        else:
            contents.append(["&amp;lt;unit&amp;gt;", image, audio])
    
    return contents


model = AutoModel.from_pretrained('openbmb/MiniCPM-o-2_6', trust_remote_code=True,
    attn_implementation='sdpa', torch_dtype=torch.bfloat16)
model = model.eval().cuda()
tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-o-2_6', trust_remote_code=True)

model.init_tts()

# If you are using an older version of PyTorch, you might encounter this issue "weight_norm_fwd_first_dim_kernel" not implemented for 'BFloat16', Please convert the TTS to float32 type.
# model.tts.float()

# https://huggingface.co/openbmb/MiniCPM-o-2_6/blob/main/assets/Skiing.mp4
video_path="assets/Skiing.mp4"
sys_msg = model.get_sys_prompt(mode='omni', language='en')
# if use voice clone prompt, please set ref_audio
# ref_audio_path = '/path/to/ref_audio'
# ref_audio, _ = librosa.load(ref_audio_path, sr=16000, mono=True)
# sys_msg = model.get_sys_prompt(ref_audio=ref_audio, mode='omni', language='en')

contents = get_video_chunk_content(video_path)
msg = {"role":"user", "content": contents}
msgs = [sys_msg, msg]

# please set generate_audio=True and output_audio_path to save the tts result
generate_audio = True
output_audio_path = 'output.wav'

res = model.chat(
    msgs=msgs,
    tokenizer=tokenizer,
    sampling=True,
    temperature=0.5,
    max_new_tokens=4096,
    omni_input=True, # please set omni_input=True when omni inference
    use_tts_template=True,
    generate_audio=generate_audio,
    output_audio_path=output_audio_path,
    max_slice_nums=1,
    use_image_id=False,
    return_dict=True
)
print(res)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; Click to view Python code running MiniCPM-o 2.6 with streaming inference. &lt;/summary&gt; 
 &lt;p&gt;Note: The streaming inference has a slight performance degradation because the audio encoding is not global.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# a new conversation need reset session first, it will reset the kv-cache
model.reset_session()

contents = get_video_chunk_content(video_path, flatten=False)
session_id = '123'
generate_audio = True

# 1. prefill system prompt
res = model.streaming_prefill(
    session_id=session_id,
    msgs=[sys_msg], 
    tokenizer=tokenizer
)

# 2. prefill video/audio chunks
for content in contents:
    msgs = [{"role":"user", "content": content}]
    res = model.streaming_prefill(
        session_id=session_id,
        msgs=msgs, 
        tokenizer=tokenizer
    )

# 3. generate
res = model.streaming_generate(
    session_id=session_id,
    tokenizer=tokenizer,
    temperature=0.5,
    generate_audio=generate_audio
)

audios = []
text = ""

if generate_audio:
    for r in res:
        audio_wav = r.audio_wav
        sampling_rate = r.sampling_rate
        txt = r.text

        audios.append(audio_wav)
        text += txt
        
    res = np.concatenate(audios)
    sf.write("output.wav", res, samplerate=sampling_rate)
    print("text:", text)
    print("audio saved to output.wav")
else:
    for r in res:
        text += r['text']
    print("text:", text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Inference on Multiple GPUs&lt;/h3&gt; 
&lt;p&gt;You can run MiniCPM-Llama3-V 2.5 on multiple low VRAM GPUs (12 GB or 16 GB) by distributing the model's layers across multiple GPUs. Please refer to this &lt;a href="https://github.com/OpenBMB/MiniCPM-V/raw/main/docs/inference_on_multiple_gpus.md"&gt;tutorial&lt;/a&gt; for detailed instructions on how to load the model and inference using multiple low VRAM GPUs.&lt;/p&gt; 
&lt;h3&gt;Inference on Mac&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click to view an example, to run MiniCPM-Llama3-V 2.5 on 💻 Mac with MPS (Apple silicon or AMD GPUs). &lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# test.py  Need more than 16GB memory.
import torch
from PIL import Image
from transformers import AutoModel, AutoTokenizer

model = AutoModel.from_pretrained('openbmb/MiniCPM-Llama3-V-2_5', trust_remote_code=True, low_cpu_mem_usage=True)
model = model.to(device='mps')

tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-Llama3-V-2_5', trust_remote_code=True)
model.eval()

image = Image.open('./assets/hk_OCR.jpg').convert('RGB')
question = 'Where is this photo taken?'
msgs = [{'role': 'user', 'content': question}]

answer, context, _ = model.chat(
    image=image,
    msgs=msgs,
    context=None,
    tokenizer=tokenizer,
    sampling=True
)
print(answer)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Run with command:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;PYTORCH_ENABLE_MPS_FALLBACK=1 python test.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Efficient Inference with llama.cpp, Ollama, vLLM&lt;/h3&gt; 
&lt;p&gt;See &lt;a href="https://github.com/OpenBMB/llama.cpp/tree/minicpmv-main/examples/llava/README-minicpmv2.6.md"&gt;our fork of llama.cpp&lt;/a&gt; for more detail. This implementation supports smooth inference of 16~18 token/s on iPad (test environment：iPad Pro + M4).&lt;/p&gt; 
&lt;p&gt;See &lt;a href="https://github.com/OpenBMB/ollama/raw/minicpm-v2.6/examples/minicpm-v2.6/README.md"&gt;our fork of Ollama&lt;/a&gt; for more detail. This implementation supports smooth inference of 16~18 token/s on iPad (test environment：iPad Pro + M4).&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt; vLLM now officially supports MiniCPM-V 2.6, MiniCPM-Llama3-V 2.5 and MiniCPM-V 2.0. And you can use our fork to run MiniCPM-o 2.6 for now. Click to see. &lt;/summary&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Install vLLM(&amp;gt;=0.7.1):&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;pip install vllm
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="2"&gt; 
  &lt;li&gt;Run Example:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://docs.vllm.ai/en/latest/getting_started/examples/vision_language.html"&gt;Vision Language&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://docs.vllm.ai/en/latest/getting_started/examples/audio_language.html"&gt;Audio Language&lt;/a&gt; &lt;/li&gt;
 &lt;/ul&gt;
&lt;/details&gt;   
&lt;h2&gt;Fine-tuning&lt;/h2&gt; 
&lt;h3&gt;Simple Fine-tuning 
 &lt;!-- omit in toc --&gt;&lt;/h3&gt; 
&lt;p&gt;We support simple fine-tuning with Hugging Face for MiniCPM-o 2.6, MiniCPM-V 2.6, MiniCPM-Llama3-V 2.5 and MiniCPM-V 2.0.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/finetune/readme.md"&gt;Reference Document&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;With Align-Anything 
 &lt;!-- omit in toc --&gt;&lt;/h3&gt; 
&lt;p&gt;We support fine-tuning MiniCPM-o 2.6 by PKU-Alignment Team (both vision and audio, SFT and DPO) with the &lt;a href="https://github.com/PKU-Alignment/align-anything"&gt;Align-Anything framework&lt;/a&gt;. Align-Anything is a scalable framework that aims to align any-modality large models with human intentions, open-sourcing the &lt;a href="https://huggingface.co/datasets/PKU-Alignment/align-anything"&gt;datasets, models and benchmarks&lt;/a&gt;. Benefiting from its concise and modular design, it supports 30+ open-source benchmarks, 40+ models and algorithms including SFT, SimPO, RLHF, &lt;em&gt;etc&lt;/em&gt;. It also provides 30+ directly runnable scripts, making it suitable for beginners to quickly get started.&lt;/p&gt; 
&lt;p&gt;Best Practices: &lt;a href="https://github.com/PKU-Alignment/align-anything/tree/main/scripts"&gt;MiniCPM-o 2.6&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;With LLaMA-Factory 
 &lt;!-- omit in toc --&gt;&lt;/h3&gt; 
&lt;p&gt;We support fine-tuning MiniCPM-o 2.6 and MiniCPM-V 2.6 with the LLaMA-Factory framework. LLaMA-Factory provides a solution for flexibly customizing the fine-tuning (Lora/Full/Qlora) of 200+ LLMs without the need for coding through the built-in web UI LLaMABoard. It supports various training methods like sft/ppo/dpo/kto and advanced algorithms like Galore/BAdam/LLaMA-Pro/Pissa/LongLoRA.&lt;/p&gt; 
&lt;p&gt;Best Practices: &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/llamafactory_train_and_infer.md"&gt;MiniCPM-o 2.6 | MiniCPM-V 2.6&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;With the SWIFT Framework 
 &lt;!-- omit in toc --&gt;&lt;/h3&gt; 
&lt;p&gt;We now support MiniCPM-V series fine-tuning with the SWIFT framework. SWIFT supports training, inference, evaluation and deployment of nearly 200 LLMs and MLLMs . It supports the lightweight training solutions provided by PEFT and a complete Adapters Library including techniques such as NEFTune, LoRA+ and LLaMA-PRO.&lt;/p&gt; 
&lt;p&gt;Best Practices：&lt;a href="https://github.com/modelscope/swift/raw/main/docs/source/Multi-Modal/minicpm-v%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5.md"&gt;MiniCPM-V 1.0&lt;/a&gt;, &lt;a href="https://github.com/modelscope/swift/raw/main/docs/source/Multi-Modal/minicpm-v-2%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5.md"&gt;MiniCPM-V 2.0&lt;/a&gt;, &lt;a href="https://github.com/modelscope/ms-swift/issues/1613"&gt;MiniCPM-V 2.6&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Awesome work using MiniCPM-V &amp;amp; MiniCPM-o&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/CatchTheTornado/text-extract-api"&gt;text-extract-api&lt;/a&gt;: Document extraction API using OCRs and Ollama supported models &lt;img src="https://img.shields.io/github/stars/CatchTheTornado/text-extract-api" alt="GitHub Repo stars" /&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/heshengtao/comfyui_LLM_party"&gt;comfyui_LLM_party&lt;/a&gt;: Build LLM workflows and integrate into existing image workflows &lt;img src="https://img.shields.io/github/stars/heshengtao/comfyui_LLM_party" alt="GitHub Repo stars" /&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/imanoop7/Ollama-OCR"&gt;Ollama-OCR&lt;/a&gt;: OCR package uses vlms through Ollama to extract text from images and PDF &lt;img src="https://img.shields.io/github/stars/imanoop7/Ollama-OCR" alt="GitHub Repo stars" /&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/MixLabPro/comfyui-mixlab-nodes"&gt;comfyui-mixlab-nodes&lt;/a&gt;: ComfyUI node suite supports Workflow-to-APP、GPT&amp;amp;3D and more &lt;img src="https://img.shields.io/github/stars/MixLabPro/comfyui-mixlab-nodes" alt="GitHub Repo stars" /&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/HumanAIGC-Engineering/OpenAvatarChat"&gt;OpenAvatarChat&lt;/a&gt;: Interactive digital human conversation implementation on single PC &lt;img src="https://img.shields.io/github/stars/HumanAIGC-Engineering/OpenAvatarChat" alt="GitHub Repo stars" /&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/arkohut/pensieve"&gt;pensieve&lt;/a&gt;: A privacy-focused passive recording project by recording screen content &lt;img src="https://img.shields.io/github/stars/arkohut/pensieve" alt="GitHub Repo stars" /&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/icereed/paperless-gpt"&gt;paperless-gpt&lt;/a&gt;: Use LLMs to handle paperless-ngx, AI-powered titles, tags and OCR &lt;img src="https://img.shields.io/github/stars/icereed/paperless-gpt" alt="GitHub Repo stars" /&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/kimjammer/Neuro"&gt;Neuro&lt;/a&gt;: A recreation of Neuro-Sama, but running on local models on consumer hardware &lt;img src="https://img.shields.io/github/stars/kimjammer/Neuro" alt="GitHub Repo stars" /&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;FAQs&lt;/h2&gt; 
&lt;p&gt;Click here to view the &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/faqs.md"&gt;FAQs&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Limitations&lt;/h2&gt; 
&lt;p&gt;As an experimental trial, we find MiniCPM-o 2.6 has notable limitations worth further investigation and improvement.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Unstable speech output.&lt;/strong&gt; The speech generation can be flawed with noisy backgrounds and unmeaningful sounds.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Repeated response.&lt;/strong&gt; The model tends to repeat its response when encountering similar consecutive user queries.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;High-latency on Web Demo.&lt;/strong&gt; Users may experience unusual high-latency when using web demo hosted on overseas servers. We recommend deploying the demo locally or with good network connections.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Model License 
 &lt;!-- omit in toc --&gt;&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;This repository is released under the &lt;a href="https://github.com/OpenBMB/MiniCPM/raw/main/LICENSE"&gt;Apache-2.0&lt;/a&gt; License.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;The usage of MiniCPM-o/V model weights must strictly follow &lt;a href="https://github.com/OpenBMB/MiniCPM/raw/main/MiniCPM%20Model%20License.md"&gt;MiniCPM Model License.md&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;The models and weights of MiniCPM are completely free for academic research. after filling out a &lt;a href="https://modelbest.feishu.cn/share/base/form/shrcnpV5ZT9EJ6xYjh3Kx0J6v8g"&gt;"questionnaire"&lt;/a&gt; for registration, are also available for free commercial use.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Statement 
 &lt;!-- omit in toc --&gt;&lt;/h2&gt; 
&lt;p&gt;As MLLMs, MiniCPM-o/V models generate content by learning a large number of multimodal corpora, but they cannot comprehend, express personal opinions, or make value judgements. Anything generated by MiniCPM-o/V models does not represent the views and positions of the model developers&lt;/p&gt; 
&lt;p&gt;We will not be liable for any problems arising from the use of MiniCPM-o/V models, including but not limited to data security issues, risk of public opinion, or any risks and problems arising from the misdirection, misuse, dissemination, or misuse of the model.&lt;/p&gt; 
&lt;h2&gt;Institutions 
 &lt;!-- omit in toc --&gt;&lt;/h2&gt; 
&lt;p&gt;This project is developed by the following institutions:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/thunlp.png" width="28px" /&gt; &lt;a href="https://nlp.csai.tsinghua.edu.cn/"&gt;THUNLP&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelbest.png" width="28px" /&gt; &lt;a href="https://modelbest.cn/"&gt;ModelBest&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🌟 Star History 
 &lt;!-- omit in toc --&gt;&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/star-history-25-09-02.png" /&gt; &lt;/p&gt;
&lt;table align="center"&gt;  
&lt;/table&gt; 
&lt;!-- &lt;picture&gt;
  &lt;source
    media="(prefers-color-scheme: dark)"
    srcset="
      https://api.star-history.com/svg?repos=OpenBMB/MiniCPM-o&amp;type=Date&amp;theme=dark
    "
  /&gt;
  &lt;source
    media="(prefers-color-scheme: light)"
    srcset="
      https://api.star-history.com/svg?repos=OpenBMB/MiniCPM-o&amp;type=Date
    "
  /&gt;
  &lt;img
    alt="Star History Chart"
    src="https://api.star-history.com/svg?repos=OpenBMB/MiniCPM-o&amp;type=Date"
  /&gt;
&lt;/picture&gt; --&gt; 
&lt;h2&gt;Key Techniques and Other Multimodal Projects 
 &lt;!-- omit in toc --&gt;&lt;/h2&gt; 
&lt;p&gt;👏 Welcome to explore key techniques of MiniCPM-o/V and other multimodal projects of our team:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/OpenBMB/VisCPM/tree/main"&gt;VisCPM&lt;/a&gt; | &lt;a href="https://github.com/OpenBMB/RLPR"&gt;RLPR&lt;/a&gt; | &lt;a href="https://github.com/RLHF-V/RLHF-V"&gt;RLHF-V&lt;/a&gt; | &lt;a href="https://github.com/thunlp/LLaVA-UHD"&gt;LLaVA-UHD&lt;/a&gt; | &lt;a href="https://github.com/RLHF-V/RLAIF-V"&gt;RLAIF-V&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Citation 
 &lt;!-- omit in toc --&gt;&lt;/h2&gt; 
&lt;p&gt;If you find our model/code/paper helpful, please consider citing our papers 📝 and staring us ⭐️！&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bib"&gt;@article{yao2024minicpm,
  title={MiniCPM-V: A GPT-4V Level MLLM on Your Phone},
  author={Yao, Yuan and Yu, Tianyu and Zhang, Ao and Wang, Chongyi and Cui, Junbo and Zhu, Hongji and Cai, Tianchi and Li, Haoyu and Zhao, Weilin and He, Zhihui and others},
  journal={arXiv preprint arXiv:2408.01800},
  year={2024}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>microsoft/qlib</title>
      <link>https://github.com/microsoft/qlib</link>
      <description>&lt;p&gt;Qlib is an AI-oriented Quant investment platform that aims to use AI tech to empower Quant Research, from exploring ideas to implementing productions. Qlib supports diverse ML modeling paradigms, including supervised learning, market dynamics modeling, and RL, and is now equipped with https://github.com/microsoft/RD-Agent to automate R&amp;D process.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="https://pypi.org/project/pyqlib/#files"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/pyqlib.svg?logo=python&amp;amp;logoColor=white" alt="Python Versions" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/pyqlib/#files"&gt;&lt;img src="https://img.shields.io/badge/platform-linux%20%7C%20windows%20%7C%20macos-lightgrey" alt="Platform" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/pyqlib/#history"&gt;&lt;img src="https://img.shields.io/pypi/v/pyqlib" alt="PypI Versions" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/pyqlib/"&gt;&lt;img src="https://github.com/microsoft/qlib/workflows/Upload%20Python%20Package/badge.svg?sanitize=true" alt="Upload Python Package" /&gt;&lt;/a&gt; &lt;a href="https://github.com/microsoft/qlib/actions"&gt;&lt;img src="https://github.com/microsoft/qlib/workflows/Test/badge.svg?branch=main" alt="Github Actions Test Status" /&gt;&lt;/a&gt; &lt;a href="https://qlib.readthedocs.io/en/latest/?badge=latest"&gt;&lt;img src="https://readthedocs.org/projects/qlib/badge/?version=latest" alt="Documentation Status" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/LICENSE"&gt;&lt;img src="https://img.shields.io/pypi/l/pyqlib" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://gitter.im/Microsoft/qlib?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge"&gt;&lt;img src="https://badges.gitter.im/Microsoft/qlib.svg?sanitize=true" alt="Join the chat at https://gitter.im/Microsoft/qlib" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;span&gt;📰&lt;/span&gt; &lt;strong&gt;What's NEW!&lt;/strong&gt; &amp;nbsp; &lt;span&gt;💖&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;Recent released features&lt;/p&gt; 
&lt;h3&gt;Introducing &lt;a href="https://github.com/microsoft/RD-Agent"&gt;&lt;img src="https://raw.githubusercontent.com/microsoft/qlib/main/docs/_static/img/rdagent_logo.png" alt="RD_Agent" style="height: 2em" /&gt;&lt;/a&gt;: LLM-Based Autonomous Evolving Agents for Industrial Data-Driven R&amp;amp;D&lt;/h3&gt; 
&lt;p&gt;We are excited to announce the release of &lt;strong&gt;RD-Agent&lt;/strong&gt;📢, a powerful tool that supports automated factor mining and model optimization in quant investment R&amp;amp;D.&lt;/p&gt; 
&lt;p&gt;RD-Agent is now available on &lt;a href="https://github.com/microsoft/RD-Agent"&gt;GitHub&lt;/a&gt;, and we welcome your star🌟!&lt;/p&gt; 
&lt;p&gt;To learn more, please visit our &lt;a href="https://rdagent.azurewebsites.net/"&gt;♾️Demo page&lt;/a&gt;. Here, you will find demo videos in both English and Chinese to help you better understand the scenario and usage of RD-Agent.&lt;/p&gt; 
&lt;p&gt;We have prepared several demo videos for you:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Scenario&lt;/th&gt; 
   &lt;th&gt;Demo video (English)&lt;/th&gt; 
   &lt;th&gt;Demo video (中文)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Quant Factor Mining&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://rdagent.azurewebsites.net/factor_loop?lang=en"&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://rdagent.azurewebsites.net/factor_loop?lang=zh"&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Quant Factor Mining from reports&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://rdagent.azurewebsites.net/report_factor?lang=en"&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://rdagent.azurewebsites.net/report_factor?lang=zh"&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Quant Model Optimization&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://rdagent.azurewebsites.net/model_loop?lang=en"&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://rdagent.azurewebsites.net/model_loop?lang=zh"&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;📃&lt;strong&gt;Paper&lt;/strong&gt;: &lt;a href="https://arxiv.org/abs/2505.15155"&gt;R&amp;amp;D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;👾&lt;strong&gt;Code&lt;/strong&gt;: &lt;a href="https://github.com/microsoft/RD-Agent/"&gt;https://github.com/microsoft/RD-Agent/&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-BibTeX"&gt;@misc{li2025rdagentquant,
    title={R\&amp;amp;D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization},
    author={Yuante Li and Xu Yang and Xiao Yang and Minrui Xu and Xisen Wang and Weiqing Liu and Jiang Bian},
    year={2025},
    eprint={2505.15155},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/3198bc10-47ba-4ee0-8a8e-46d5ce44f45d" alt="image" /&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Feature&lt;/th&gt; 
   &lt;th&gt;Status&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/abs/2505.15155"&gt;R&amp;amp;D-Agent-Quant&lt;/a&gt; Published&lt;/td&gt; 
   &lt;td&gt;Apply R&amp;amp;D-Agent to Qlib for quant trading&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BPQP for End-to-end learning&lt;/td&gt; 
   &lt;td&gt;📈Coming soon!(&lt;a href="https://github.com/microsoft/qlib/pull/1863"&gt;Under review&lt;/a&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;🔥LLM-driven Auto Quant Factory🔥&lt;/td&gt; 
   &lt;td&gt;🚀 Released in &lt;a href="https://github.com/microsoft/RD-Agent"&gt;♾️RD-Agent&lt;/a&gt; on Aug 8, 2024&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;KRNN and Sandwich models&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;📈&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/1414/"&gt;Released&lt;/a&gt; on May 26, 2023&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Release Qlib v0.9.0&lt;/td&gt; 
   &lt;td&gt;&lt;img alt="octocat" src="https://github.githubassets.com/images/icons/emoji/octocat.png?v8" /&gt;) &lt;a href="https://github.com/microsoft/qlib/releases/tag/v0.9.0"&gt;Released&lt;/a&gt; on Dec 9, 2022&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;RL Learning Framework&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;🔨&lt;/span&gt; &lt;span&gt;📈&lt;/span&gt; Released on Nov 10, 2022. &lt;a href="https://github.com/microsoft/qlib/pull/1332"&gt;#1332&lt;/a&gt;, &lt;a href="https://github.com/microsoft/qlib/pull/1322"&gt;#1322&lt;/a&gt;, &lt;a href="https://github.com/microsoft/qlib/pull/1316"&gt;#1316&lt;/a&gt;,&lt;a href="https://github.com/microsoft/qlib/pull/1299"&gt;#1299&lt;/a&gt;,&lt;a href="https://github.com/microsoft/qlib/pull/1263"&gt;#1263&lt;/a&gt;, &lt;a href="https://github.com/microsoft/qlib/pull/1244"&gt;#1244&lt;/a&gt;, &lt;a href="https://github.com/microsoft/qlib/pull/1169"&gt;#1169&lt;/a&gt;, &lt;a href="https://github.com/microsoft/qlib/pull/1125"&gt;#1125&lt;/a&gt;, &lt;a href="https://github.com/microsoft/qlib/pull/1076"&gt;#1076&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;HIST and IGMTF models&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;📈&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/1040"&gt;Released&lt;/a&gt; on Apr 10, 2022&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qlib &lt;a href="https://github.com/microsoft/qlib/tree/main/examples/tutorial"&gt;notebook tutorial&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;📖 &lt;a href="https://github.com/microsoft/qlib/pull/1037"&gt;Released&lt;/a&gt; on Apr 7, 2022&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ibovespa index data&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;🍚&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/990"&gt;Released&lt;/a&gt; on Apr 6, 2022&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Point-in-Time database&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;🔨&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/343"&gt;Released&lt;/a&gt; on Mar 10, 2022&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Arctic Provider Backend &amp;amp; Orderbook data example&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;🔨&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/744"&gt;Released&lt;/a&gt; on Jan 17, 2022&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Meta-Learning-based framework &amp;amp; DDG-DA&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;📈&lt;/span&gt; &lt;span&gt;🔨&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/743"&gt;Released&lt;/a&gt; on Jan 10, 2022&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Planning-based portfolio optimization&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;🔨&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/754"&gt;Released&lt;/a&gt; on Dec 28, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Release Qlib v0.8.0&lt;/td&gt; 
   &lt;td&gt;&lt;img alt="octocat" src="https://github.githubassets.com/images/icons/emoji/octocat.png?v8" /&gt;) &lt;a href="https://github.com/microsoft/qlib/releases/tag/v0.8.0"&gt;Released&lt;/a&gt; on Dec 8, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ADD model&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;📈&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/704"&gt;Released&lt;/a&gt; on Nov 22, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ADARNN model&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;📈&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/689"&gt;Released&lt;/a&gt; on Nov 14, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;TCN model&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;📈&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/668"&gt;Released&lt;/a&gt; on Nov 4, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Nested Decision Framework&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;🔨&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/438"&gt;Released&lt;/a&gt; on Oct 1, 2021. &lt;a href="https://github.com/microsoft/qlib/raw/main/examples/nested_decision_execution/workflow.py"&gt;Example&lt;/a&gt; and &lt;a href="https://qlib.readthedocs.io/en/latest/component/highfreq.html"&gt;Doc&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Temporal Routing Adaptor (TRA)&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;📈&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/531"&gt;Released&lt;/a&gt; on July 30, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Transformer &amp;amp; Localformer&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;📈&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/508"&gt;Released&lt;/a&gt; on July 22, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Release Qlib v0.7.0&lt;/td&gt; 
   &lt;td&gt;&lt;img alt="octocat" src="https://github.githubassets.com/images/icons/emoji/octocat.png?v8" /&gt;) &lt;a href="https://github.com/microsoft/qlib/releases/tag/v0.7.0"&gt;Released&lt;/a&gt; on July 12, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;TCTS Model&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;📈&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/491"&gt;Released&lt;/a&gt; on July 1, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Online serving and automatic model rolling&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;🔨&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/290"&gt;Released&lt;/a&gt; on May 17, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;DoubleEnsemble Model&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;📈&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/286"&gt;Released&lt;/a&gt; on Mar 2, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;High-frequency data processing example&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;🔨&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/257"&gt;Released&lt;/a&gt; on Feb 5, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;High-frequency trading example&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;📈&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/227"&gt;Part of code released&lt;/a&gt; on Jan 28, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;High-frequency data(1min)&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;🍚&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/221"&gt;Released&lt;/a&gt; on Jan 27, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Tabnet Model&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;📈&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/205"&gt;Released&lt;/a&gt; on Jan 22, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Features released before 2021 are not listed here.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/qlib/main/docs/_static/img/logo/1.png" /&gt; &lt;/p&gt; 
&lt;p&gt;Qlib is an open-source, AI-oriented quantitative investment platform that aims to realize the potential, empower research, and create value using AI technologies in quantitative investment, from exploring ideas to implementing productions. Qlib supports diverse machine learning modeling paradigms, including supervised learning, market dynamics modeling, and reinforcement learning.&lt;/p&gt; 
&lt;p&gt;An increasing number of SOTA Quant research works/papers in diverse paradigms are being released in Qlib to collaboratively solve key challenges in quantitative investment. For example, 1) using supervised learning to mine the market's complex non-linear patterns from rich and heterogeneous financial data, 2) modeling the dynamic nature of the financial market using adaptive concept drift technology, and 3) using reinforcement learning to model continuous investment decisions and assist investors in optimizing their trading strategies.&lt;/p&gt; 
&lt;p&gt;It contains the full ML pipeline of data processing, model training, back-testing; and covers the entire chain of quantitative investment: alpha seeking, risk modeling, portfolio optimization, and order execution. For more details, please refer to our paper &lt;a href="https://arxiv.org/abs/2009.11189"&gt;"Qlib: An AI-oriented Quantitative Investment Platform"&lt;/a&gt;.&lt;/p&gt; 
&lt;table&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Frameworks, Tutorial, Data &amp;amp; DevOps&lt;/th&gt; 
   &lt;th&gt;Main Challenges &amp;amp; Solutions in Quant Research&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt; &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#plans"&gt;&lt;strong&gt;Plans&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#framework-of-qlib"&gt;Framework of Qlib&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#quick-start"&gt;Quick Start&lt;/a&gt;&lt;/li&gt; 
    &lt;ul dir="auto"&gt; 
     &lt;li type="circle"&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#installation"&gt;Installation&lt;/a&gt; &lt;/li&gt; 
     &lt;li type="circle"&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#data-preparation"&gt;Data Preparation&lt;/a&gt;&lt;/li&gt; 
     &lt;li type="circle"&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#auto-quant-research-workflow"&gt;Auto Quant Research Workflow&lt;/a&gt;&lt;/li&gt; 
     &lt;li type="circle"&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#building-customized-quant-research-workflow-by-code"&gt;Building Customized Quant Research Workflow by Code&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt; &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#quant-dataset-zoo"&gt;&lt;strong&gt;Quant Dataset Zoo&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#learning-framework"&gt;Learning Framework&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#more-about-qlib"&gt;More About Qlib&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#offline-mode-and-online-mode"&gt;Offline Mode and Online Mode&lt;/a&gt; 
     &lt;ul&gt; 
      &lt;li type="circle"&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#performance-of-qlib-data-server"&gt;Performance of Qlib Data Server&lt;/a&gt;&lt;/li&gt;
     &lt;/ul&gt; &lt;/li&gt;&lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#related-reports"&gt;Related Reports&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#contact-us"&gt;Contact Us&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; &lt;/td&gt; 
   &lt;td valign="baseline"&gt; &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#main-challenges--solutions-in-quant-research"&gt;Main Challenges &amp;amp; Solutions in Quant Research&lt;/a&gt; 
     &lt;ul&gt; 
      &lt;li type="circle"&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#forecasting-finding-valuable-signalspatterns"&gt;Forecasting: Finding Valuable Signals/Patterns&lt;/a&gt; 
       &lt;ul&gt; 
        &lt;li type="disc"&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#quant-model-paper-zoo"&gt;&lt;strong&gt;Quant Model (Paper) Zoo&lt;/strong&gt;&lt;/a&gt; 
         &lt;ul&gt; 
          &lt;li type="circle"&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#run-a-single-model"&gt;Run a Single Model&lt;/a&gt;&lt;/li&gt; 
          &lt;li type="circle"&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#run-multiple-models"&gt;Run Multiple Models&lt;/a&gt;&lt;/li&gt; 
         &lt;/ul&gt; &lt;/li&gt; 
       &lt;/ul&gt; &lt;/li&gt; 
      &lt;li type="circle"&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#adapting-to-market-dynamics"&gt;Adapting to Market Dynamics&lt;/a&gt;&lt;/li&gt; 
      &lt;li type="circle"&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#reinforcement-learning-modeling-continuous-decisions"&gt;Reinforcement Learning: modeling continuous decisions&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;Plans&lt;/h1&gt; 
&lt;p&gt;New features under development(order by estimated release time). Your feedbacks about the features are very important.&lt;/p&gt; 
&lt;!-- | Feature                        | Status      | --&gt; 
&lt;!-- | --                      | ------    | --&gt; 
&lt;h1&gt;Framework of Qlib&lt;/h1&gt; 
&lt;div style="align: center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/microsoft/qlib/main/docs/_static/img/framework-abstract.jpg" /&gt; 
&lt;/div&gt; 
&lt;p&gt;The high-level framework of Qlib can be found above(users can find the &lt;a href="https://qlib.readthedocs.io/en/latest/introduction/introduction.html#framework"&gt;detailed framework&lt;/a&gt; of Qlib's design when getting into nitty gritty). The components are designed as loose-coupled modules, and each component could be used stand-alone.&lt;/p&gt; 
&lt;p&gt;Qlib provides a strong infrastructure to support Quant research. &lt;a href="https://qlib.readthedocs.io/en/latest/component/data.html"&gt;Data&lt;/a&gt; is always an important part. A strong learning framework is designed to support diverse learning paradigms (e.g. &lt;a href="https://qlib.readthedocs.io/en/latest/component/rl.html"&gt;reinforcement learning&lt;/a&gt;, &lt;a href="https://qlib.readthedocs.io/en/latest/component/workflow.html#model-section"&gt;supervised learning&lt;/a&gt;) and patterns at different levels(e.g. &lt;a href="https://qlib.readthedocs.io/en/latest/component/meta.html"&gt;market dynamic modeling&lt;/a&gt;). By modeling the market, &lt;a href="https://qlib.readthedocs.io/en/latest/component/strategy.html"&gt;trading strategies&lt;/a&gt; will generate trade decisions that will be executed. Multiple trading strategies and executors in different levels or granularities can be &lt;a href="https://qlib.readthedocs.io/en/latest/component/highfreq.html"&gt;nested to be optimized and run together&lt;/a&gt;. At last, a comprehensive &lt;a href="https://qlib.readthedocs.io/en/latest/component/report.html"&gt;analysis&lt;/a&gt; will be provided and the model can be &lt;a href="https://qlib.readthedocs.io/en/latest/component/online.html"&gt;served online&lt;/a&gt; in a low cost.&lt;/p&gt; 
&lt;h1&gt;Quick Start&lt;/h1&gt; 
&lt;p&gt;This quick start guide tries to demonstrate&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;It's very easy to build a complete Quant research workflow and try your ideas with &lt;em&gt;Qlib&lt;/em&gt;.&lt;/li&gt; 
 &lt;li&gt;Though with &lt;em&gt;public data&lt;/em&gt; and &lt;em&gt;simple models&lt;/em&gt;, machine learning technologies &lt;strong&gt;work very well&lt;/strong&gt; in practical Quant investment.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Here is a quick &lt;strong&gt;&lt;a href="https://terminalizer.com/view/3f24561a4470"&gt;demo&lt;/a&gt;&lt;/strong&gt; shows how to install &lt;code&gt;Qlib&lt;/code&gt;, and run LightGBM with &lt;code&gt;qrun&lt;/code&gt;. &lt;strong&gt;But&lt;/strong&gt;, please make sure you have already prepared the data following the &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#data-preparation"&gt;instruction&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;This table demonstrates the supported Python version of &lt;code&gt;Qlib&lt;/code&gt;:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;install with pip&lt;/th&gt; 
   &lt;th align="center"&gt;install from source&lt;/th&gt; 
   &lt;th align="center"&gt;plot&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Python 3.8&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Python 3.9&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Python 3.10&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Python 3.11&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Python 3.12&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Conda&lt;/strong&gt; is suggested for managing your Python environment. In some cases, using Python outside of a &lt;code&gt;conda&lt;/code&gt; environment may result in missing header files, causing the installation failure of certain packages.&lt;/li&gt; 
 &lt;li&gt;Please pay attention that installing cython in Python 3.6 will raise some error when installing &lt;code&gt;Qlib&lt;/code&gt; from source. If users use Python 3.6 on their machines, it is recommended to &lt;em&gt;upgrade&lt;/em&gt; Python to version 3.8 or higher, or use &lt;code&gt;conda&lt;/code&gt;'s Python to install &lt;code&gt;Qlib&lt;/code&gt; from source.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Install with pip&lt;/h3&gt; 
&lt;p&gt;Users can easily install &lt;code&gt;Qlib&lt;/code&gt; by pip according to the following command.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;  pip install pyqlib
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: pip will install the latest stable qlib. However, the main branch of qlib is in active development. If you want to test the latest scripts or functions in the main branch. Please install qlib with the methods below.&lt;/p&gt; 
&lt;h3&gt;Install from source&lt;/h3&gt; 
&lt;p&gt;Also, users can install the latest dev version &lt;code&gt;Qlib&lt;/code&gt; by the source code according to the following steps:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Before installing &lt;code&gt;Qlib&lt;/code&gt; from source, users need to install some dependencies:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install numpy
pip install --upgrade cython
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Clone the repository and install &lt;code&gt;Qlib&lt;/code&gt; as follows.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/microsoft/qlib.git &amp;amp;&amp;amp; cd qlib
pip install .  # `pip install -e .[dev]` is recommended for development. check details in docs/developer/code_standard_and_dev_guide.rst
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Tips&lt;/strong&gt;: If you fail to install &lt;code&gt;Qlib&lt;/code&gt; or run the examples in your environment, comparing your steps and the &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/.github/workflows/test_qlib_from_source.yml"&gt;CI workflow&lt;/a&gt; may help you find the problem.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Tips for Mac&lt;/strong&gt;: If you are using Mac with M1, you might encounter issues in building the wheel for LightGBM, which is due to missing dependencies from OpenMP. To solve the problem, install openmp first with &lt;code&gt;brew install libomp&lt;/code&gt; and then run &lt;code&gt;pip install .&lt;/code&gt; to build it successfully.&lt;/p&gt; 
&lt;h2&gt;Data Preparation&lt;/h2&gt; 
&lt;p&gt;❗ Due to more restrict data security policy. The official dataset is disabled temporarily. You can try &lt;a href="https://github.com/chenditc/investment_data/releases"&gt;this data source&lt;/a&gt; contributed by the community. Here is an example to download the latest data.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;wget https://github.com/chenditc/investment_data/releases/latest/download/qlib_bin.tar.gz
mkdir -p ~/.qlib/qlib_data/cn_data
tar -zxvf qlib_bin.tar.gz -C ~/.qlib/qlib_data/cn_data --strip-components=1
rm -f qlib_bin.tar.gz
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The official dataset below will resume in short future.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;Load and prepare data by running the following code:&lt;/p&gt; 
&lt;h3&gt;Get with module&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# get 1d data
python -m qlib.cli.data qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn

# get 1min data
python -m qlib.cli.data qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min

&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Get from source&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# get 1d data
python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn

# get 1min data
python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This dataset is created by public data collected by &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/scripts/data_collector/"&gt;crawler scripts&lt;/a&gt;, which have been released in the same repository. Users could create the same dataset with it. &lt;a href="https://github.com/microsoft/qlib/tree/main/scripts/data_collector#description-of-dataset"&gt;Description of dataset&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Please pay &lt;strong&gt;ATTENTION&lt;/strong&gt; that the data is collected from &lt;a href="https://finance.yahoo.com/lookup"&gt;Yahoo Finance&lt;/a&gt;, and the data might not be perfect. We recommend users to prepare their own data if they have a high-quality dataset. For more information, users can refer to the &lt;a href="https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format"&gt;related document&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt; 
&lt;h3&gt;Automatic update of daily frequency data (from yahoo finance)&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;This step is &lt;em&gt;Optional&lt;/em&gt; if users only want to try their models and strategies on history data.&lt;/p&gt; 
 &lt;p&gt;It is recommended that users update the data manually once (--trading_date 2021-05-25) and then set it to update automatically.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: Users can't incrementally update data based on the offline data provided by Qlib(some fields are removed to reduce the data size). Users should use &lt;a href="https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance"&gt;yahoo collector&lt;/a&gt; to download Yahoo data from scratch and then incrementally update it.&lt;/p&gt; 
 &lt;p&gt;For more information, please refer to: &lt;a href="https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance"&gt;yahoo collector&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Automatic update of data to the "qlib" directory each trading day(Linux)&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;use &lt;em&gt;crontab&lt;/em&gt;: &lt;code&gt;crontab -e&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;set up timed tasks:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;* * * * 1-5 python &amp;lt;script path&amp;gt; update_data_to_bin --qlib_data_1d_dir &amp;lt;user data dir&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;script path&lt;/strong&gt;: &lt;em&gt;scripts/data_collector/yahoo/collector.py&lt;/em&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Manual update of data&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir &amp;lt;user data dir&amp;gt; --trading_date &amp;lt;start date&amp;gt; --end_date &amp;lt;end date&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;em&gt;trading_date&lt;/em&gt;: start of trading day&lt;/li&gt; 
   &lt;li&gt;&lt;em&gt;end_date&lt;/em&gt;: end of trading day(not included)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Checking the health of the data&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;We provide a script to check the health of the data, you can run the following commands to check whether the data is healthy or not. &lt;pre&gt;&lt;code&gt;python scripts/check_data_health.py check_data --qlib_dir ~/.qlib/qlib_data/cn_data
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Of course, you can also add some parameters to adjust the test results, such as this. &lt;pre&gt;&lt;code&gt;python scripts/check_data_health.py check_data --qlib_dir ~/.qlib/qlib_data/cn_data --missing_data_num 30055 --large_step_threshold_volume 94485 --large_step_threshold_price 20
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;If you want more information about &lt;code&gt;check_data_health&lt;/code&gt;, please refer to the &lt;a href="https://qlib.readthedocs.io/en/latest/component/data.html#checking-the-health-of-the-data"&gt;documentation&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- 
- Run the initialization code and get stock data:

  ```python
  import qlib
  from qlib.data import D
  from qlib.constant import REG_CN

  # Initialization
  mount_path = "~/.qlib/qlib_data/cn_data"  # target_dir
  qlib.init(mount_path=mount_path, region=REG_CN)

  # Get stock data by Qlib
  # Load trading calendar with the given time range and frequency
  print(D.calendar(start_time='2010-01-01', end_time='2017-12-31', freq='day')[:2])

  # Parse a given market name into a stockpool config
  instruments = D.instruments('csi500')
  print(D.list_instruments(instruments=instruments, start_time='2010-01-01', end_time='2017-12-31', as_list=True)[:6])

  # Load features of certain instruments in given time range
  instruments = ['SH600000']
  fields = ['$close', '$volume', 'Ref($close, 1)', 'Mean($close, 3)', '$high-$low']
  print(D.features(instruments, fields, start_time='2010-01-01', end_time='2017-12-31', freq='day').head())
  ```
 --&gt; 
&lt;h2&gt;Docker images&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Pulling a docker image from a docker hub repository &lt;pre&gt;&lt;code class="language-bash"&gt;docker pull pyqlib/qlib_image_stable:stable
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Start a new Docker container &lt;pre&gt;&lt;code class="language-bash"&gt;docker run -it --name &amp;lt;container name&amp;gt; -v &amp;lt;Mounted local directory&amp;gt;:/app qlib_image_stable
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;At this point you are in the docker environment and can run the qlib scripts. An example: &lt;pre&gt;&lt;code class="language-bash"&gt;&amp;gt;&amp;gt;&amp;gt; python scripts/get_data.py qlib_data --name qlib_data_simple --target_dir ~/.qlib/qlib_data/cn_data --interval 1d --region cn
&amp;gt;&amp;gt;&amp;gt; python qlib/cli/run.py examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Exit the container &lt;pre&gt;&lt;code class="language-bash"&gt;&amp;gt;&amp;gt;&amp;gt; exit
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Restart the container &lt;pre&gt;&lt;code class="language-bash"&gt;docker start -i -a &amp;lt;container name&amp;gt;
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Stop the container &lt;pre&gt;&lt;code class="language-bash"&gt;docker stop &amp;lt;container name&amp;gt;
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Delete the container &lt;pre&gt;&lt;code class="language-bash"&gt;docker rm &amp;lt;container name&amp;gt;
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;If you want to know more information, please refer to the &lt;a href="https://qlib.readthedocs.io/en/latest/developer/how_to_build_image.html"&gt;documentation&lt;/a&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Auto Quant Research Workflow&lt;/h2&gt; 
&lt;p&gt;Qlib provides a tool named &lt;code&gt;qrun&lt;/code&gt; to run the whole workflow automatically (including building dataset, training models, backtest and evaluation). You can start an auto quant research workflow and have a graphical reports analysis according to the following steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Quant Research Workflow: Run &lt;code&gt;qrun&lt;/code&gt; with lightgbm workflow config (&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml"&gt;workflow_config_lightgbm_Alpha158.yaml&lt;/a&gt; as following.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;  cd examples  # Avoid running program under the directory contains `qlib`
  qrun benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If users want to use &lt;code&gt;qrun&lt;/code&gt; under debug mode, please use the following command:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;python -m pdb qlib/cli/run.py examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The result of &lt;code&gt;qrun&lt;/code&gt; is as follows, please refer to &lt;a href="https://qlib.readthedocs.io/en/latest/component/strategy.html#result"&gt;docs&lt;/a&gt; for more explanations about the result.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;
'The following are analysis results of the excess return without cost.'
                       risk
mean               0.000708
std                0.005626
annualized_return  0.178316
information_ratio  1.996555
max_drawdown      -0.081806
'The following are analysis results of the excess return with cost.'
                       risk
mean               0.000512
std                0.005626
annualized_return  0.128982
information_ratio  1.444287
max_drawdown      -0.091078
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here are detailed documents for &lt;code&gt;qrun&lt;/code&gt; and &lt;a href="https://qlib.readthedocs.io/en/latest/component/workflow.html"&gt;workflow&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Graphical Reports Analysis: First, run &lt;code&gt;python -m pip install .[analysis]&lt;/code&gt; to install the required dependencies. Then run &lt;code&gt;examples/workflow_by_code.ipynb&lt;/code&gt; with &lt;code&gt;jupyter notebook&lt;/code&gt; to get graphical reports.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;Forecasting signal (model prediction) analysis&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Cumulative Return of groups &lt;img src="https://github.com/microsoft/qlib/raw/main/docs/_static/img/analysis/analysis_model_cumulative_return.png" alt="Cumulative Return" /&gt;&lt;/li&gt; 
     &lt;li&gt;Return distribution &lt;img src="https://github.com/microsoft/qlib/raw/main/docs/_static/img/analysis/analysis_model_long_short.png" alt="long_short" /&gt;&lt;/li&gt; 
     &lt;li&gt;Information Coefficient (IC) &lt;img src="https://github.com/microsoft/qlib/raw/main/docs/_static/img/analysis/analysis_model_IC.png" alt="Information Coefficient" /&gt; &lt;img src="https://github.com/microsoft/qlib/raw/main/docs/_static/img/analysis/analysis_model_monthly_IC.png" alt="Monthly IC" /&gt; &lt;img src="https://github.com/microsoft/qlib/raw/main/docs/_static/img/analysis/analysis_model_NDQ.png" alt="IC" /&gt;&lt;/li&gt; 
     &lt;li&gt;Auto Correlation of forecasting signal (model prediction) &lt;img src="https://github.com/microsoft/qlib/raw/main/docs/_static/img/analysis/analysis_model_auto_correlation.png" alt="Auto Correlation" /&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Portfolio analysis&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Backtest return &lt;img src="https://github.com/microsoft/qlib/raw/main/docs/_static/img/analysis/report.png" alt="Report" /&gt;&lt;/li&gt; 
    &lt;/ul&gt; 
    &lt;!-- 
- Score IC
![Score IC](docs/_static/img/score_ic.png)
- Cumulative Return
![Cumulative Return](docs/_static/img/cumulative_return.png)
- Risk Analysis
![Risk Analysis](docs/_static/img/risk_analysis.png)
- Rank Label
![Rank Label](docs/_static/img/rank_label.png)
--&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;a href="https://qlib.readthedocs.io/en/latest/component/report.html"&gt;Explanation&lt;/a&gt; of above results&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Building Customized Quant Research Workflow by Code&lt;/h2&gt; 
&lt;p&gt;The automatic workflow may not suit the research workflow of all Quant researchers. To support a flexible Quant research workflow, Qlib also provides a modularized interface to allow researchers to build their own workflow by code. &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/workflow_by_code.ipynb"&gt;Here&lt;/a&gt; is a demo for customized Quant research workflow by code.&lt;/p&gt; 
&lt;h1&gt;Main Challenges &amp;amp; Solutions in Quant Research&lt;/h1&gt; 
&lt;p&gt;Quant investment is a very unique scenario with lots of key challenges to be solved. Currently, Qlib provides some solutions for several of them.&lt;/p&gt; 
&lt;h2&gt;Forecasting: Finding Valuable Signals/Patterns&lt;/h2&gt; 
&lt;p&gt;Accurate forecasting of the stock price trend is a very important part to construct profitable portfolios. However, huge amount of data with various formats in the financial market which make it challenging to build forecasting models.&lt;/p&gt; 
&lt;p&gt;An increasing number of SOTA Quant research works/papers, which focus on building forecasting models to mine valuable signals/patterns in complex financial data, are released in &lt;code&gt;Qlib&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks"&gt;Quant Model (Paper) Zoo&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Here is a list of models built on &lt;code&gt;Qlib&lt;/code&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/XGBoost/"&gt;GBDT based on XGBoost (Tianqi Chen, et al. KDD 2016)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/LightGBM/"&gt;GBDT based on LightGBM (Guolin Ke, et al. NIPS 2017)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/CatBoost/"&gt;GBDT based on Catboost (Liudmila Prokhorenkova, et al. NIPS 2018)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/MLP/"&gt;MLP based on pytorch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/LSTM/"&gt;LSTM based on pytorch (Sepp Hochreiter, et al. Neural computation 1997)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/GRU/"&gt;GRU based on pytorch (Kyunghyun Cho, et al. 2014)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/ALSTM"&gt;ALSTM based on pytorch (Yao Qin, et al. IJCAI 2017)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/GATs/"&gt;GATs based on pytorch (Petar Velickovic, et al. 2017)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/SFM/"&gt;SFM based on pytorch (Liheng Zhang, et al. KDD 2017)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/TFT/"&gt;TFT based on tensorflow (Bryan Lim, et al. International Journal of Forecasting 2019)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/TabNet/"&gt;TabNet based on pytorch (Sercan O. Arik, et al. AAAI 2019)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/DoubleEnsemble/"&gt;DoubleEnsemble based on LightGBM (Chuheng Zhang, et al. ICDM 2020)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/TCTS/"&gt;TCTS based on pytorch (Xueqing Wu, et al. ICML 2021)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/Transformer/"&gt;Transformer based on pytorch (Ashish Vaswani, et al. NeurIPS 2017)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/Localformer/"&gt;Localformer based on pytorch (Juyong Jiang, et al.)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/TRA/"&gt;TRA based on pytorch (Hengxu, Dong, et al. KDD 2021)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/TCN/"&gt;TCN based on pytorch (Shaojie Bai, et al. 2018)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/ADARNN/"&gt;ADARNN based on pytorch (YunTao Du, et al. 2021)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/ADD/"&gt;ADD based on pytorch (Hongshun Tang, et al.2020)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/IGMTF/"&gt;IGMTF based on pytorch (Wentao Xu, et al.2021)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/HIST/"&gt;HIST based on pytorch (Wentao Xu, et al.2021)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/KRNN/"&gt;KRNN based on pytorch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/Sandwich/"&gt;Sandwich based on pytorch&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Your PR of new Quant models is highly welcomed.&lt;/p&gt; 
&lt;p&gt;The performance of each model on the &lt;code&gt;Alpha158&lt;/code&gt; and &lt;code&gt;Alpha360&lt;/code&gt; datasets can be found &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/README.md"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Run a single model&lt;/h3&gt; 
&lt;p&gt;All the models listed above are runnable with &lt;code&gt;Qlib&lt;/code&gt;. Users can find the config files we provide and some details about the model through the &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks"&gt;benchmarks&lt;/a&gt; folder. More information can be retrieved at the model files listed above.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;Qlib&lt;/code&gt; provides three different ways to run a single model, users can pick the one that fits their cases best:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Users can use the tool &lt;code&gt;qrun&lt;/code&gt; mentioned above to run a model's workflow based from a config file.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Users can create a &lt;code&gt;workflow_by_code&lt;/code&gt; python script based on the &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/workflow_by_code.py"&gt;one&lt;/a&gt; listed in the &lt;code&gt;examples&lt;/code&gt; folder.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Users can use the script &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/run_all_model.py"&gt;&lt;code&gt;run_all_model.py&lt;/code&gt;&lt;/a&gt; listed in the &lt;code&gt;examples&lt;/code&gt; folder to run a model. Here is an example of the specific shell command to be used: &lt;code&gt;python run_all_model.py run --models=lightgbm&lt;/code&gt;, where the &lt;code&gt;--models&lt;/code&gt; arguments can take any number of models listed above(the available models can be found in &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/"&gt;benchmarks&lt;/a&gt;). For more use cases, please refer to the file's &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/run_all_model.py"&gt;docstrings&lt;/a&gt;.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: Each baseline has different environment dependencies, please make sure that your python version aligns with the requirements(e.g. TFT only supports Python 3.6~3.7 due to the limitation of &lt;code&gt;tensorflow==1.15.0&lt;/code&gt;)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Run multiple models&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;Qlib&lt;/code&gt; also provides a script &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/run_all_model.py"&gt;&lt;code&gt;run_all_model.py&lt;/code&gt;&lt;/a&gt; which can run multiple models for several iterations. (&lt;strong&gt;Note&lt;/strong&gt;: the script only support &lt;em&gt;Linux&lt;/em&gt; for now. Other OS will be supported in the future. Besides, it doesn't support parallel running the same model for multiple times as well, and this will be fixed in the future development too.)&lt;/p&gt; 
&lt;p&gt;The script will create a unique virtual environment for each model, and delete the environments after training. Thus, only experiment results such as &lt;code&gt;IC&lt;/code&gt; and &lt;code&gt;backtest&lt;/code&gt; results will be generated and stored.&lt;/p&gt; 
&lt;p&gt;Here is an example of running all the models for 10 iterations:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;python run_all_model.py run 10
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It also provides the API to run specific models at once. For more use cases, please refer to the file's &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/run_all_model.py"&gt;docstrings&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Break change&lt;/h3&gt; 
&lt;p&gt;In &lt;code&gt;pandas&lt;/code&gt;, &lt;code&gt;group_key&lt;/code&gt; is one of the parameters of the &lt;code&gt;groupby&lt;/code&gt; method. From version 1.5 to 2.0 of &lt;code&gt;pandas&lt;/code&gt;, the default value of &lt;code&gt;group_key&lt;/code&gt; has been changed from &lt;code&gt;no default&lt;/code&gt; to &lt;code&gt;True&lt;/code&gt;, which will cause qlib to report an error during operation. So we set &lt;code&gt;group_key=False&lt;/code&gt;, but it doesn't guarantee that some programmes will run correctly, including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;qlib\examples\rl_order_execution\scripts\gen_training_orders.py&lt;/li&gt; 
 &lt;li&gt;qlib\examples\benchmarks\TRA\src\dataset.MTSDatasetH.py&lt;/li&gt; 
 &lt;li&gt;qlib\examples\benchmarks\TFT\tft.py&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks_dynamic"&gt;Adapting to Market Dynamics&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;Due to the non-stationary nature of the environment of the financial market, the data distribution may change in different periods, which makes the performance of models build on training data decays in the future test data. So adapting the forecasting models/strategies to market dynamics is very important to the model/strategies' performance.&lt;/p&gt; 
&lt;p&gt;Here is a list of solutions built on &lt;code&gt;Qlib&lt;/code&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks_dynamic/baseline/"&gt;Rolling Retraining&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks_dynamic/DDG-DA/"&gt;DDG-DA on pytorch (Wendi, et al. AAAI 2022)&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Reinforcement Learning: modeling continuous decisions&lt;/h2&gt; 
&lt;p&gt;Qlib now supports reinforcement learning, a feature designed to model continuous investment decisions. This functionality assists investors in optimizing their trading strategies by learning from interactions with the environment to maximize some notion of cumulative reward.&lt;/p&gt; 
&lt;p&gt;Here is a list of solutions built on &lt;code&gt;Qlib&lt;/code&gt; categorized by scenarios.&lt;/p&gt; 
&lt;h3&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/rl_order_execution"&gt;RL for order execution&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://qlib.readthedocs.io/en/latest/component/rl/overall.html#order-execution"&gt;Here&lt;/a&gt; is the introduction of this scenario. All the methods below are compared &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/rl_order_execution"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/rl_order_execution/exp_configs/backtest_twap.yml"&gt;TWAP&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/rl_order_execution/exp_configs/backtest_ppo.yml"&gt;PPO: "An End-to-End Optimal Trade Execution Framework based on Proximal Policy Optimization", IJCAL 2020&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/rl_order_execution/exp_configs/backtest_opds.yml"&gt;OPDS: "Universal Trading for Order Execution with Oracle Policy Distillation", AAAI 2021&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Quant Dataset Zoo&lt;/h1&gt; 
&lt;p&gt;Dataset plays a very important role in Quant. Here is a list of the datasets built on &lt;code&gt;Qlib&lt;/code&gt;:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Dataset&lt;/th&gt; 
   &lt;th&gt;US Market&lt;/th&gt; 
   &lt;th&gt;China Market&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/qlib/contrib/data/handler.py"&gt;Alpha360&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;√&lt;/td&gt; 
   &lt;td&gt;√&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/qlib/contrib/data/handler.py"&gt;Alpha158&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;√&lt;/td&gt; 
   &lt;td&gt;√&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;a href="https://qlib.readthedocs.io/en/latest/advanced/alpha.html"&gt;Here&lt;/a&gt; is a tutorial to build dataset with &lt;code&gt;Qlib&lt;/code&gt;. Your PR to build new Quant dataset is highly welcomed.&lt;/p&gt; 
&lt;h1&gt;Learning Framework&lt;/h1&gt; 
&lt;p&gt;Qlib is high customizable and a lot of its components are learnable. The learnable components are instances of &lt;code&gt;Forecast Model&lt;/code&gt; and &lt;code&gt;Trading Agent&lt;/code&gt;. They are learned based on the &lt;code&gt;Learning Framework&lt;/code&gt; layer and then applied to multiple scenarios in &lt;code&gt;Workflow&lt;/code&gt; layer. The learning framework leverages the &lt;code&gt;Workflow&lt;/code&gt; layer as well(e.g. sharing &lt;code&gt;Information Extractor&lt;/code&gt;, creating environments based on &lt;code&gt;Execution Env&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;Based on learning paradigms, they can be categorized into reinforcement learning and supervised learning.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;For supervised learning, the detailed docs can be found &lt;a href="https://qlib.readthedocs.io/en/latest/component/model.html"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;For reinforcement learning, the detailed docs can be found &lt;a href="https://qlib.readthedocs.io/en/latest/component/rl.html"&gt;here&lt;/a&gt;. Qlib's RL learning framework leverages &lt;code&gt;Execution Env&lt;/code&gt; in &lt;code&gt;Workflow&lt;/code&gt; layer to create environments. It's worth noting that &lt;code&gt;NestedExecutor&lt;/code&gt; is supported as well. This empowers users to optimize different level of strategies/models/agents together (e.g. optimizing an order execution strategy for a specific portfolio management strategy).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;More About Qlib&lt;/h1&gt; 
&lt;p&gt;If you want to have a quick glance at the most frequently used components of qlib, you can try notebooks &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/tutorial/"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The detailed documents are organized in &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/docs/"&gt;docs&lt;/a&gt;. &lt;a href="http://www.sphinx-doc.org"&gt;Sphinx&lt;/a&gt; and the readthedocs theme is required to build the documentation in html formats.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd docs/
conda install sphinx sphinx_rtd_theme -y
# Otherwise, you can install them with pip
# pip install sphinx sphinx_rtd_theme
make html
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also view the &lt;a href="http://qlib.readthedocs.io/"&gt;latest document&lt;/a&gt; online directly.&lt;/p&gt; 
&lt;p&gt;Qlib is in active and continuing development. Our plan is in the roadmap, which is managed as a &lt;a href="https://github.com/microsoft/qlib/projects/1"&gt;github project&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Offline Mode and Online Mode&lt;/h1&gt; 
&lt;p&gt;The data server of Qlib can either deployed as &lt;code&gt;Offline&lt;/code&gt; mode or &lt;code&gt;Online&lt;/code&gt; mode. The default mode is offline mode.&lt;/p&gt; 
&lt;p&gt;Under &lt;code&gt;Offline&lt;/code&gt; mode, the data will be deployed locally.&lt;/p&gt; 
&lt;p&gt;Under &lt;code&gt;Online&lt;/code&gt; mode, the data will be deployed as a shared data service. The data and their cache will be shared by all the clients. The data retrieval performance is expected to be improved due to a higher rate of cache hits. It will consume less disk space, too. The documents of the online mode can be found in &lt;a href="https://qlib-server.readthedocs.io/"&gt;Qlib-Server&lt;/a&gt;. The online mode can be deployed automatically with &lt;a href="https://qlib-server.readthedocs.io/en/latest/build.html#one-click-deployment-in-azure"&gt;Azure CLI based scripts&lt;/a&gt;. The source code of online data server can be found in &lt;a href="https://github.com/microsoft/qlib-server"&gt;Qlib-Server repository&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Performance of Qlib Data Server&lt;/h2&gt; 
&lt;p&gt;The performance of data processing is important to data-driven methods like AI technologies. As an AI-oriented platform, Qlib provides a solution for data storage and data processing. To demonstrate the performance of Qlib data server, we compare it with several other data storage solutions.&lt;/p&gt; 
&lt;p&gt;We evaluate the performance of several storage solutions by finishing the same task, which creates a dataset (14 features/factors) from the basic OHLCV daily data of a stock market (800 stocks each day from 2007 to 2020). The task involves data queries and processing.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;HDF5&lt;/th&gt; 
   &lt;th&gt;MySQL&lt;/th&gt; 
   &lt;th&gt;MongoDB&lt;/th&gt; 
   &lt;th&gt;InfluxDB&lt;/th&gt; 
   &lt;th&gt;Qlib -E -D&lt;/th&gt; 
   &lt;th&gt;Qlib +E -D&lt;/th&gt; 
   &lt;th&gt;Qlib +E +D&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Total (1CPU) (seconds)&lt;/td&gt; 
   &lt;td&gt;184.4±3.7&lt;/td&gt; 
   &lt;td&gt;365.3±7.5&lt;/td&gt; 
   &lt;td&gt;253.6±6.7&lt;/td&gt; 
   &lt;td&gt;368.2±3.6&lt;/td&gt; 
   &lt;td&gt;147.0±8.8&lt;/td&gt; 
   &lt;td&gt;47.6±1.0&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;7.4±0.3&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Total (64CPU) (seconds)&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;8.8±0.6&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;4.2±0.2&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;+(-)E&lt;/code&gt; indicates with (out) &lt;code&gt;ExpressionCache&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;+(-)D&lt;/code&gt; indicates with (out) &lt;code&gt;DatasetCache&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Most general-purpose databases take too much time to load data. After looking into the underlying implementation, we find that data go through too many layers of interfaces and unnecessary format transformations in general-purpose database solutions. Such overheads greatly slow down the data loading process. Qlib data are stored in a compact format, which is efficient to be combined into arrays for scientific computation.&lt;/p&gt; 
&lt;h1&gt;Related Reports&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://analyticsindiamag.com/qlib/"&gt;Guide To Qlib: Microsoft’s AI Investment Platform&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mp.weixin.qq.com/s/47bP5YwxfTp2uTHjUBzJQQ"&gt;微软也搞AI量化平台？还是开源的！&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mp.weixin.qq.com/s/vsJv7lsgjEi-ALYUz4CvtQ"&gt;微矿Qlib：业内首个AI量化投资开源平台&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Contact Us&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;If you have any issues, please create issue &lt;a href="https://github.com/microsoft/qlib/issues/new/choose"&gt;here&lt;/a&gt; or send messages in &lt;a href="https://gitter.im/Microsoft/qlib"&gt;gitter&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;If you want to make contributions to &lt;code&gt;Qlib&lt;/code&gt;, please &lt;a href="https://github.com/microsoft/qlib/compare"&gt;create pull requests&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;For other reasons, you are welcome to contact us by email(&lt;a href="mailto:qlib@microsoft.com"&gt;qlib@microsoft.com&lt;/a&gt;). 
  &lt;ul&gt; 
   &lt;li&gt;We are recruiting new members(both FTEs and interns), your resumes are welcome!&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Join IM discussion groups:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;a href="https://gitter.im/Microsoft/qlib"&gt;Gitter&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src="https://github.com/microsoft/qlib/raw/main/docs/_static/img/qrcode/gitter_qr.png" alt="image" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;Contributing&lt;/h1&gt; 
&lt;p&gt;We appreciate all contributions and thank all the contributors! &lt;a href="https://github.com/microsoft/qlib/graphs/contributors"&gt;&lt;img src="https://contrib.rocks/image?repo=microsoft/qlib" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Before we released Qlib as an open-source project on Github in Sep 2020, Qlib is an internal project in our group. Unfortunately, the internal commit history is not kept. A lot of members in our group have also contributed a lot to Qlib, which includes Ruihua Wang, Yinda Zhang, Haisu Yu, Shuyu Wang, Bochen Pang, and &lt;a href="https://github.com/evanzd/evanzd"&gt;Dong Zhou&lt;/a&gt;. Especially thanks to &lt;a href="https://github.com/evanzd/evanzd"&gt;Dong Zhou&lt;/a&gt; due to his initial version of Qlib.&lt;/p&gt; 
&lt;h2&gt;Guidance&lt;/h2&gt; 
&lt;p&gt;This project welcomes contributions and suggestions.&lt;br /&gt; &lt;strong&gt;Here are some &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/docs/developer/code_standard_and_dev_guide.rst"&gt;code standards and development guidance&lt;/a&gt; for submiting a pull request.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Making contributions is not a hard thing. Solving an issue(maybe just answering a question raised in &lt;a href="https://github.com/microsoft/qlib/issues"&gt;issues list&lt;/a&gt; or &lt;a href="https://gitter.im/Microsoft/qlib"&gt;gitter&lt;/a&gt;), fixing/issuing a bug, improving the documents and even fixing a typo are important contributions to Qlib.&lt;/p&gt; 
&lt;p&gt;For example, if you want to contribute to Qlib's document/code, you can follow the steps in the figure below.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://github.com/demon143/qlib/raw/main/docs/_static/img/change%20doc.gif" /&gt; &lt;/p&gt; 
&lt;p&gt;If you don't know how to start to contribute, you can refer to the following examples.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Examples&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Solving issues&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/qlib/issues/749"&gt;Answer a question&lt;/a&gt;; &lt;a href="https://github.com/microsoft/qlib/issues/765"&gt;issuing&lt;/a&gt; or &lt;a href="https://github.com/microsoft/qlib/pull/792"&gt;fixing&lt;/a&gt; a bug&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Docs&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/qlib/pull/797/files"&gt;Improve docs quality&lt;/a&gt; ; &lt;a href="https://github.com/microsoft/qlib/pull/774"&gt;Fix a typo&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Feature&lt;/td&gt; 
   &lt;td&gt;Implement a &lt;a href="https://github.com/microsoft/qlib/projects"&gt;requested feature&lt;/a&gt; like &lt;a href="https://github.com/microsoft/qlib/pull/754"&gt;this&lt;/a&gt;; &lt;a href="https://github.com/microsoft/qlib/pull/539/files"&gt;Refactor interfaces&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Dataset&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/qlib/pull/733"&gt;Add a dataset&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Models&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/qlib/pull/689"&gt;Implement a new model&lt;/a&gt;, &lt;a href="https://github.com/microsoft/qlib/tree/main/examples/benchmarks#contributing"&gt;some instructions to contribute models&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;a href="https://github.com/microsoft/qlib/labels/good%20first%20issue"&gt;Good first issues&lt;/a&gt; are labelled to indicate that they are easy to start your contributions.&lt;/p&gt; 
&lt;p&gt;You can find some impefect implementation in Qlib by &lt;code&gt;rg 'TODO|FIXME' qlib&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;If you would like to become one of Qlib's maintainers to contribute more (e.g. help merge PR, triage issues), please contact us by email(&lt;a href="mailto:qlib@microsoft.com"&gt;qlib@microsoft.com&lt;/a&gt;). We are glad to help to upgrade your permission.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the right to use your contribution. For details, visit &lt;a href="https://cla.opensource.microsoft.com"&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; 
&lt;p&gt;This project has adopted the &lt;a href="https://opensource.microsoft.com/codeofconduct/"&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href="https://opensource.microsoft.com/codeofconduct/faq/"&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href="mailto:opencode@microsoft.com"&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>