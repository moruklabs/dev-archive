<rss version="2.0">
  <channel>
    <title>GitHub Python Weekly Trending</title>
    <description>Weekly Trending of Python in GitHub</description>
    <pubDate>Wed, 20 Aug 2025 01:44:16 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>denizsafak/abogen</title>
      <link>https://github.com/denizsafak/abogen</link>
      <description>&lt;p&gt;Generate audiobooks from EPUBs, PDFs and text with synchronized captions.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;abogen &lt;img width="40px" title="abogen icon" src="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/abogen/assets/icon.ico" align="right" style="padding-left: 10px; padding-top:5px;" /&gt;&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/denizsafak/abogen/actions"&gt;&lt;img src="https://github.com/denizsafak/abogen/actions/workflows/test_pip.yml/badge.svg?sanitize=true" alt="Build Status" /&gt;&lt;/a&gt; &lt;a href="https://github.com/denizsafak/abogen/releases/latest"&gt;&lt;img src="https://img.shields.io/github/v/release/denizsafak/abogen" alt="GitHub Release" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/abogen/"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/abogen" alt="Abogen PyPi Python Versions" /&gt;&lt;/a&gt; &lt;a href="https://github.com/denizsafak/abogen/releases/latest"&gt;&lt;img src="https://img.shields.io/badge/os-windows%20%7C%20linux%20%7C%20macos%20-blue" alt="Operating Systems" /&gt;&lt;/a&gt; &lt;a href="https://github.com/psf/black"&gt;&lt;img src="https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true" alt="Code style: black" /&gt;&lt;/a&gt; &lt;a href="https://opensource.org/licenses/MIT"&gt;&lt;img src="https://img.shields.io/badge/License-MIT-maroon.svg?sanitize=true" alt="License: MIT" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Abogen is a powerful text-to-speech conversion tool that makes it easy to turn ePub, PDF, or text files into high-quality audio with matching subtitles in seconds. Use it for audiobooks, voiceovers for Instagram, YouTube, TikTok, or any project that needs natural-sounding text-to-speech, using &lt;a href="https://huggingface.co/hexgrad/Kokoro-82M"&gt;Kokoro-82M&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;img title="Abogen Main" src="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/abogen.png" width="380" /&gt; &lt;img title="Abogen Processing" src="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/abogen2.png" width="380" /&gt;&lt;/p&gt; 
&lt;h2&gt;Demo&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/cb66512d-0a52-48c3-bda4-f1e6a03fb8d6"&gt;https://github.com/user-attachments/assets/cb66512d-0a52-48c3-bda4-f1e6a03fb8d6&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;This demo was generated in just 5&amp;nbsp;seconds, producing ‚àº1&amp;nbsp;minute of audio with perfectly synced subtitles. To create a similar video, see &lt;a href="https://github.com/denizsafak/abogen/tree/main/demo"&gt;the demo guide&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;&lt;code&gt;How to install?&lt;/code&gt; &lt;a href="https://pypi.org/project/abogen/" target="_blank"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/abogen" alt="Abogen Compatible PyPi Python Versions" align="right" style="margin-top:6px;" /&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;h3&gt;Windows&lt;/h3&gt; 
&lt;p&gt;Go to &lt;a href="https://github.com/espeak-ng/espeak-ng/releases/latest"&gt;espeak-ng latest release&lt;/a&gt; download and run the *.msi file.&lt;/p&gt; 
&lt;h4&gt;OPTION 1: Install using script&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://github.com/denizsafak/abogen/archive/refs/heads/main.zip"&gt;Download&lt;/a&gt; the repository&lt;/li&gt; 
 &lt;li&gt;Extract the ZIP file&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;WINDOWS_INSTALL.bat&lt;/code&gt; by double-clicking it&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;This method handles everything automatically - installing all dependencies including CUDA in a self-contained environment without requiring a separate Python installation. (You still need to install &lt;a href="https://github.com/espeak-ng/espeak-ng/releases/latest"&gt;espeak-ng&lt;/a&gt;.)&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] You don't need to install Python separately. The script will install Python automatically.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;OPTION 2: Install using pip&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Create a virtual environment (optional)
mkdir abogen &amp;amp;&amp;amp; cd abogen
python -m venv venv
venv\Scripts\activate

# For NVIDIA GPUs:
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128

# For AMD GPUs:
# Not supported yet, because ROCm is not available on Windows. Use Linux if you have AMD GPU.

# Install abogen
pip install abogen
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Mac&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install espeak-ng
brew install espeak-ng

# Create a virtual environment (recommended)
mkdir abogen &amp;amp;&amp;amp; cd abogen
python3 -m venv venv
source venv/bin/activate

# Install abogen
pip3 install abogen
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Linux&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install espeak-ng
sudo apt install espeak-ng # Ubuntu/Debian
sudo pacman -S espeak-ng # Arch Linux
sudo dnf install espeak-ng # Fedora

# Create a virtual environment (recommended)
mkdir abogen &amp;amp;&amp;amp; cd abogen
python3 -m venv venv
source venv/bin/activate

# Install abogen
pip3 install abogen

# For NVIDIA GPUs:
# Already supported, no need to install CUDA separately.

# For AMD GPUs:
# After installing abogen, we need to uninstall the existing torch package
pip3 uninstall torch 
pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm6.4
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] If you get &lt;code&gt;WARNING: The script abogen-cli is installed in '/home/username/.local/bin' which is not on PATH.&lt;/code&gt; error, run the following command to add it to your PATH:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;echo "export PATH=\"/home/$USER/.local/bin:\$PATH\"" &amp;gt;&amp;gt; ~/.bashrc &amp;amp;&amp;amp; source ~/.bashrc
&lt;/code&gt;&lt;/pre&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] If you get "No matching distribution found" error, try installing it on supported Python (3.10 to 3.12). You can use &lt;a href="https://github.com/pyenv/pyenv"&gt;pyenv&lt;/a&gt; to manage multiple Python versions easily in Linux. Watch this &lt;a href="https://www.youtube.com/watch?v=MVyb-nI4KyI"&gt;video&lt;/a&gt; by NetworkChuck for a quick guide.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Special thanks to &lt;a href="https://github.com/hg000125"&gt;@hg000125&lt;/a&gt; for his contribution in &lt;a href="https://github.com/denizsafak/abogen/issues/23"&gt;#23&lt;/a&gt;. AMD GPU support is possible thanks to his work.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;&lt;code&gt;How to run?&lt;/code&gt;&lt;/h2&gt; 
&lt;p&gt;If you installed using pip, you can simply run the following command to start Abogen:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;abogen
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] If you installed using the Windows installer &lt;code&gt;(WINDOWS_INSTALL.bat)&lt;/code&gt;, It should have created a shortcut in the same folder, or your desktop. You can run it from there. If you lost the shortcut, Abogen is located in &lt;code&gt;python_embedded/Scripts/abogen.exe&lt;/code&gt;. You can run it from there directly.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;&lt;code&gt;How to use?&lt;/code&gt;&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Drag and drop any ePub, PDF, or text file (or use the built-in text editor)&lt;/li&gt; 
 &lt;li&gt;Configure the settings: 
  &lt;ul&gt; 
   &lt;li&gt;Set speech speed&lt;/li&gt; 
   &lt;li&gt;Select a voice (or create a custom voice using voice mixer)&lt;/li&gt; 
   &lt;li&gt;Select subtitle generation style (by sentence, word, etc.)&lt;/li&gt; 
   &lt;li&gt;Select output format&lt;/li&gt; 
   &lt;li&gt;Select where to save the output&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Hit Start&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;&lt;code&gt;In action&lt;/code&gt;&lt;/h2&gt; 
&lt;img title="Abogen in action" src="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/abogen.gif" /&gt; 
&lt;p&gt;Here‚Äôs Abogen in action: in this demo, it processes ‚àº3,000 characters of text in just 11 seconds and turns it into 3 minutes and 28 seconds of audio, and I have a low-end &lt;strong&gt;RTX&amp;nbsp;2060&amp;nbsp;Mobile laptop GPU&lt;/strong&gt;. Your results may vary depending on your hardware.&lt;/p&gt; 
&lt;h2&gt;&lt;code&gt;Configuration&lt;/code&gt;&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Options&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Input Box&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Drag and drop &lt;code&gt;ePub&lt;/code&gt;, &lt;code&gt;PDF&lt;/code&gt;, or &lt;code&gt;.TXT&lt;/code&gt; files (or use built-in text editor)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Queue options&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Add multiple files to a queue and process them in batch, with individual settings for each file. See &lt;a href="https://raw.githubusercontent.com/denizsafak/abogen/main/#queue-mode"&gt;Queue mode&lt;/a&gt; for more details.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Speed&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Adjust speech rate from &lt;code&gt;0.1x&lt;/code&gt; to &lt;code&gt;2.0x&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Select Voice&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;First letter of the language code (e.g., &lt;code&gt;a&lt;/code&gt; for American English, &lt;code&gt;b&lt;/code&gt; for British English, etc.), second letter is for &lt;code&gt;m&lt;/code&gt; for male and &lt;code&gt;f&lt;/code&gt; for female.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Voice mixer&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Create custom voices by mixing different voice models with a profile system. See &lt;a href="https://raw.githubusercontent.com/denizsafak/abogen/main/#voice-mixer"&gt;Voice Mixer&lt;/a&gt; for more details.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Voice preview&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Listen to the selected voice before processing.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Generate subtitles&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;Disabled&lt;/code&gt;, &lt;code&gt;Sentence&lt;/code&gt;, &lt;code&gt;Sentence + Comma&lt;/code&gt;, &lt;code&gt;1 word&lt;/code&gt;, &lt;code&gt;2 words&lt;/code&gt;, &lt;code&gt;3 words&lt;/code&gt;, etc. (Represents the number of words in each subtitle entry)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Output voice format&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;.WAV&lt;/code&gt;, &lt;code&gt;.FLAC&lt;/code&gt;, &lt;code&gt;.MP3&lt;/code&gt;, &lt;code&gt;.OPUS (best compression)&lt;/code&gt; and &lt;code&gt;M4B (with chapters)&lt;/code&gt; (Special thanks to &lt;a href="https://github.com/jborza"&gt;@jborza&lt;/a&gt; for chapter support in PR &lt;a href="https://github.com/denizsafak/abogen/pull/10"&gt;#10&lt;/a&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Output subtitle format&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Configures the subtitle format as &lt;code&gt;SRT (standard)&lt;/code&gt;, &lt;code&gt;ASS (wide)&lt;/code&gt;, &lt;code&gt;ASS (narrow)&lt;/code&gt;, &lt;code&gt;ASS (centered wide)&lt;/code&gt;, or &lt;code&gt;ASS (centered narrow)&lt;/code&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Replace single newlines with spaces&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Replaces single newlines with spaces in the text. This is useful for texts that have imaginary line breaks.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Save location&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;Save next to input file&lt;/code&gt;, &lt;code&gt;Save to desktop&lt;/code&gt;, or &lt;code&gt;Choose output folder&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Book handler options&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Chapter Control&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Select specific &lt;code&gt;chapters&lt;/code&gt; from ePUBs or &lt;code&gt;chapters + pages&lt;/code&gt; from PDFs.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Save each chapter separately&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Save each chapter in e-books as a separate audio file.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Create a merged version&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Create a single audio file that combines all chapters. (If &lt;code&gt;Save each chapter separately&lt;/code&gt; is disabled, this option will be the default behavior.)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Save in a project folder with metadata&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Save the converted items in a project folder with available metadata files.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Menu options&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Theme&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Change the application's theme using &lt;code&gt;System&lt;/code&gt;, &lt;code&gt;Light&lt;/code&gt;, or &lt;code&gt;Dark&lt;/code&gt; options.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Configure max words per subtitle&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Configures the maximum number of words per subtitle entry.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Configure max lines in log window&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Configures the maximum number of lines to display in the log window.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Separate chapters audio format&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Configures the audio format for separate chapters as &lt;code&gt;wav&lt;/code&gt;, &lt;code&gt;flac&lt;/code&gt;, &lt;code&gt;mp3&lt;/code&gt;, or &lt;code&gt;opus&lt;/code&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Create desktop shortcut&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Creates a shortcut on your desktop for easy access.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Open config directory&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Opens the directory where the configuration file is stored.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Open cache directory&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Opens the cache directory where converted text files are stored.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Clear cache files&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Deletes cache files created during the conversion or preview.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Check for updates at startup&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Automatically checks for updates when the program starts.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Disable Kokoro's internet access&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Prevents Kokoro from downloading models or voices from HuggingFace Hub, useful for offline use.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Reset to default settings&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Resets all settings to their default values.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;&lt;code&gt;Voice Mixer&lt;/code&gt;&lt;/h2&gt; 
&lt;img title="Abogen Voice Mixer" src="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/voice_mixer.png" /&gt; 
&lt;p&gt;With voice mixer, you can create custom voices by mixing different voice models. You can adjust the weight of each voice and save your custom voice as a profile for future use. The voice mixer allows you to create unique and personalized voices. (Huge thanks to &lt;a href="https://github.com/jborza"&gt;@jborza&lt;/a&gt; for making this possible through his contributions in &lt;a href="https://github.com/denizsafak/abogen/pull/5"&gt;#5&lt;/a&gt;)&lt;/p&gt; 
&lt;h2&gt;&lt;code&gt;Queue Mode&lt;/code&gt;&lt;/h2&gt; 
&lt;img title="Abogen queue mode" src="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/queue.png" /&gt; 
&lt;p&gt;Abogen supports &lt;strong&gt;queue mode&lt;/strong&gt;, allowing you to add multiple files to a processing queue. This is useful if you want to convert several files in one batch.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;You can add text files (&lt;code&gt;.txt&lt;/code&gt;) directly using the &lt;strong&gt;Add files&lt;/strong&gt; button in the Queue Manager. To add PDF or EPUB files, use the input box in the main window and click the &lt;strong&gt;Add to Queue&lt;/strong&gt; button.&lt;/li&gt; 
 &lt;li&gt;Each file in the queue keeps the configuration settings that were active when it was added. Changing the main window configuration afterward does &lt;strong&gt;not&lt;/strong&gt; affect files already in the queue.&lt;/li&gt; 
 &lt;li&gt;You can view each file's configuration by hovering over them.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Abogen will process each item in the queue automatically, saving outputs as configured.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Special thanks to &lt;a href="https://github.com/jborza"&gt;@jborza&lt;/a&gt; for adding queue mode in PR &lt;a href="https://github.com/denizsafak/abogen/pull/35"&gt;#35&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;&lt;code&gt;About Chapter Markers&lt;/code&gt;&lt;/h2&gt; 
&lt;p&gt;When you process ePUB or PDF files, Abogen converts them into text files stored in your cache directory. When you click "Edit," you're actually modifying these converted text files. In these text files, you'll notice tags that look like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;lt;&amp;lt;CHAPTER_MARKER:Chapter Title&amp;gt;&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;These are chapter markers. They are automatically added when you process ePUB or PDF files, based on the chapters you select. They serve an important purpose:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Allow you to split the text into separate audio files for each chapter&lt;/li&gt; 
 &lt;li&gt;Save time by letting you reprocess only specific chapters if errors occur, rather than the entire file&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can manually add these markers to plain text files for the same benefits. Simply include them in your text like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;lt;&amp;lt;CHAPTER_MARKER:Introduction&amp;gt;&amp;gt;
This is the beginning of my text...  

&amp;lt;&amp;lt;CHAPTER_MARKER:Main Content&amp;gt;&amp;gt; 
Here's another part...  
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;When you process the text file, Abogen will detect these markers automatically and ask if you want to save each chapter separately and create a merged version.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/chapter_marker.png" alt="Abogen Chapter Marker" /&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;code&gt;About Metadata Tags&lt;/code&gt;&lt;/h2&gt; 
&lt;p&gt;Similar to chapter markers, it is possible to add metadata tags for &lt;code&gt;M4B&lt;/code&gt; files. This is useful for audiobook players that support metadata, allowing you to add information like title, author, year, etc. Abogen automatically adds these tags when you process ePUB or PDF files, but you can also add them manually to your text files. Add metadata tags &lt;strong&gt;at the beginning of your text file&lt;/strong&gt; like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;lt;&amp;lt;METADATA_TITLE:Title&amp;gt;&amp;gt;
&amp;lt;&amp;lt;METADATA_ARTIST:Author&amp;gt;&amp;gt;
&amp;lt;&amp;lt;METADATA_ALBUM:Album Title&amp;gt;&amp;gt;
&amp;lt;&amp;lt;METADATA_YEAR:Year&amp;gt;&amp;gt;
&amp;lt;&amp;lt;METADATA_ALBUM_ARTIST:Album Artist&amp;gt;&amp;gt;
&amp;lt;&amp;lt;METADATA_COMPOSER:Narrator&amp;gt;&amp;gt;
&amp;lt;&amp;lt;METADATA_GENRE:Audiobook&amp;gt;&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;&lt;code&gt;Supported Languages&lt;/code&gt;&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;# üá∫üá∏ 'a' =&amp;gt; American English, üá¨üáß 'b' =&amp;gt; British English
# üá™üá∏ 'e' =&amp;gt; Spanish es
# üá´üá∑ 'f' =&amp;gt; French fr-fr
# üáÆüá≥ 'h' =&amp;gt; Hindi hi
# üáÆüáπ 'i' =&amp;gt; Italian it
# üáØüáµ 'j' =&amp;gt; Japanese: pip install misaki[ja]
# üáßüá∑ 'p' =&amp;gt; Brazilian Portuguese pt-br
# üá®üá≥ 'z' =&amp;gt; Mandarin Chinese: pip install misaki[zh]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For a complete list of supported languages and voices, refer to Kokoro's &lt;a href="https://huggingface.co/hexgrad/Kokoro-82M/blob/main/VOICES.md"&gt;VOICES.md&lt;/a&gt;. To listen to sample audio outputs, see &lt;a href="https://huggingface.co/hexgrad/Kokoro-82M/blob/main/SAMPLES.md"&gt;SAMPLES.md&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;&lt;code&gt;MPV Config&lt;/code&gt;&lt;/h2&gt; 
&lt;p&gt;I highly recommend using &lt;a href="https://mpv.io/installation/"&gt;MPV&lt;/a&gt; to play your audio files, as it supports displaying subtitles even without a video track. Here's my &lt;code&gt;mpv.conf&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;# --- MPV Settings ---
save-position-on-quit
keep-open=yes
# --- Subtitle ---
sub-ass-override=no
sub-margin-y=50
sub-margin-x=50
# --- Audio Quality ---
audio-spdif=ac3,dts,eac3,truehd,dts-hd
audio-channels=auto
audio-samplerate=48000
volume-max=200
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;&lt;code&gt;Docker Guide&lt;/code&gt;&lt;/h2&gt; 
&lt;p&gt;If you want to run Abogen in a Docker container:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://github.com/denizsafak/abogen/archive/refs/heads/main.zip"&gt;Download the repository&lt;/a&gt; and extract, or clone it using git.&lt;/li&gt; 
 &lt;li&gt;Go to &lt;code&gt;abogen&lt;/code&gt; folder. You should see &lt;code&gt;Dockerfile&lt;/code&gt; there.&lt;/li&gt; 
 &lt;li&gt;Open your termminal in that directory and run the following commands:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Build the Docker image:
docker build --progress plain -t abogen .

# Note that building the image may take a while.
# After building is complete, run the Docker container:

# Windows
docker run --name abogen -v %cd%:/shared -p 5800:5800 -p 5900:5900 --gpus all abogen

# Linux
docker run --name abogen -v $(pwd):/shared -p 5800:5800 -p 5900:5900 --gpus all abogen

# MacOS
docker run --name abogen -v $(pwd):/shared -p 5800:5800 -p 5900:5900 abogen

# We expose port 5800 for use by a web browser, 5900 if you want to connect with a VNC client.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Abogen launches automatically inside the container.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;You can access it via a web browser at &lt;a href="http://localhost:5800"&gt;http://localhost:5800&lt;/a&gt; or connect to it using a VNC client at &lt;code&gt;localhost:5900&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;You can use &lt;code&gt;/shared&lt;/code&gt; directory to share files between your host and the container.&lt;/li&gt; 
 &lt;li&gt;For later use, start it with &lt;code&gt;docker start abogen&lt;/code&gt; and stop it with &lt;code&gt;docker stop abogen&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Known issues:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Audio preview is not working inside container (ALSA error).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Open cache directory&lt;/code&gt; and &lt;code&gt;Open configuration directory&lt;/code&gt; options in settings not working. (Tried pcmanfm, did not work with Abogen).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;(Special thanks to &lt;a href="https://www.reddit.com/user/geo38/"&gt;@geo38&lt;/a&gt; from Reddit, who provided the Dockerfile and instructions in &lt;a href="https://www.reddit.com/r/selfhosted/comments/1k8x1yo/comment/mpe0bz8/"&gt;this comment&lt;/a&gt;.)&lt;/p&gt; 
&lt;h2&gt;&lt;code&gt;Similar Projects&lt;/code&gt;&lt;/h2&gt; 
&lt;p&gt;Abogen is a standalone project, but it is inspired by and shares some similarities with other projects. Here are a few:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/santinic/audiblez"&gt;audiblez&lt;/a&gt;: Generate audiobooks from e-books. &lt;strong&gt;(Has CLI and GUI support)&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/plusuncold/autiobooks"&gt;autiobooks&lt;/a&gt;: Automatically convert epubs to audiobooks&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mateogon/pdf-narrator"&gt;pdf-narrator&lt;/a&gt;: Convert your PDFs and EPUBs into audiobooks effortlessly.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/p0n1/epub_to_audiobook"&gt;epub_to_audiobook&lt;/a&gt;: EPUB to audiobook converter, optimized for Audiobookshelf&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/DrewThomasson/ebook2audiobook"&gt;ebook2audiobook&lt;/a&gt;: Convert ebooks to audiobooks with chapters and metadata using dynamic AI models and voice cloning&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;code&gt;Roadmap&lt;/code&gt;&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Add OCR scan feature for PDF files using docling/teserract.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Add chapter metadata for .m4a files. (Issue &lt;a href="https://github.com/denizsafak/abogen/issues/9"&gt;#9&lt;/a&gt;, PR &lt;a href="https://github.com/denizsafak/abogen/pull/10"&gt;#10&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Add support for different languages in GUI.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Add voice formula feature that enables mixing different voice models. (Issue &lt;a href="https://github.com/denizsafak/abogen/issues/1"&gt;#1&lt;/a&gt;, PR &lt;a href="https://github.com/denizsafak/abogen/pull/5"&gt;#5&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Add support for kokoro-onnx (If it's necessary).&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Add dark mode.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;code&gt;Troubleshooting&lt;/code&gt;&lt;/h2&gt; 
&lt;p&gt;If you encounter any issues while running Abogen, try launching it from the command line with:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;abogen-cli
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will start Abogen in command-line mode and display detailed error messages. Please open a new issue on the &lt;a href="https://github.com/denizsafak/abogen/issues"&gt;Issues&lt;/a&gt; page with the error message and a description of your problem.&lt;/p&gt; 
&lt;h2&gt;&lt;code&gt;Contributing&lt;/code&gt;&lt;/h2&gt; 
&lt;p&gt;I welcome contributions! If you have ideas for new features, improvements, or bug fixes, please fork the repository and submit a pull request.&lt;/p&gt; 
&lt;h3&gt;For developers and contributors&lt;/h3&gt; 
&lt;p&gt;If you'd like to modify the code and contribute to development, you can &lt;a href="https://github.com/denizsafak/abogen/archive/refs/heads/main.zip"&gt;download the repository&lt;/a&gt;, extract it and run the following commands to build &lt;strong&gt;or&lt;/strong&gt; install the package:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Go to the directory where you extracted the repository and run:
pip install -e .      # Installs the package in editable mode
pip install build     # Install the build package
python -m build       # Builds the package in dist folder (optional)
abogen                # Opens the GUI
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Feel free to explore the code and make any changes you like.&lt;/p&gt; 
&lt;h2&gt;&lt;code&gt;Credits&lt;/code&gt;&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Abogen uses &lt;a href="https://github.com/hexgrad/kokoro"&gt;Kokoro&lt;/a&gt; for its high-quality, natural-sounding text-to-speech synthesis. Huge thanks to the Kokoro team for making this possible.&lt;/li&gt; 
 &lt;li&gt;Thanks to &lt;a href="https://github.com/wojiushixiaobai"&gt;@wojiushixiaobai&lt;/a&gt; for &lt;a href="https://github.com/wojiushixiaobai/Python-Embed-Win64"&gt;Embedded Python&lt;/a&gt; packages. These modified packages include pip pre-installed, enabling Abogen to function as a standalone application without requiring users to separately install Python in Windows.&lt;/li&gt; 
 &lt;li&gt;Thanks to creators of &lt;a href="https://github.com/aerkalov/ebooklib"&gt;EbookLib&lt;/a&gt;, a Python library for reading and writing ePub files, which is used for extracting text from ePub files.&lt;/li&gt; 
 &lt;li&gt;Special thanks to the &lt;a href="https://www.riverbankcomputing.com/software/pyqt/"&gt;PyQt&lt;/a&gt; team for providing the cross-platform GUI toolkit that powers Abogen's interface.&lt;/li&gt; 
 &lt;li&gt;Icons: &lt;a href="https://icons8.com/icon/aRiu1GGi6Aoe/usa"&gt;US&lt;/a&gt;, &lt;a href="https://icons8.com/icon/t3NE3BsOAQwq/great-britain"&gt;Great Britain&lt;/a&gt;, &lt;a href="https://icons8.com/icon/ly7tzANRt33n/spain"&gt;Spain&lt;/a&gt;, &lt;a href="https://icons8.com/icon/3muzEmi4dpD5/france"&gt;France&lt;/a&gt;, &lt;a href="https://icons8.com/icon/esGVrxg9VCJ1/india"&gt;India&lt;/a&gt;, &lt;a href="https://icons8.com/icon/PW8KZnP7qXzO/italy"&gt;Italy&lt;/a&gt;, &lt;a href="https://icons8.com/icon/McQbrq9qaQye/japan"&gt;Japan&lt;/a&gt;, &lt;a href="https://icons8.com/icon/zHmH8HpOmM90/brazil"&gt;Brazil&lt;/a&gt;, &lt;a href="https://icons8.com/icon/Ej50Oe3crXwF/china"&gt;China&lt;/a&gt;, &lt;a href="https://icons8.com/icon/uI49hxbpxTkp/female"&gt;Female&lt;/a&gt;, &lt;a href="https://icons8.com/icon/12351/male"&gt;Male&lt;/a&gt;, &lt;a href="https://icons8.com/icon/21698/adjust"&gt;Adjust&lt;/a&gt; and &lt;a href="https://icons8.com/icon/GskSeVoroQ7u/voice-id"&gt;Voice Id&lt;/a&gt; icons by &lt;a href="https://icons8.com/"&gt;Icons8&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;code&gt;License&lt;/code&gt;&lt;/h2&gt; 
&lt;p&gt;This project is available under the MIT License - see the &lt;a href="https://github.com/denizsafak/abogen/raw/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details. &lt;a href="https://github.com/hexgrad/kokoro"&gt;Kokoro&lt;/a&gt; is licensed under &lt;a href="https://github.com/hexgrad/kokoro/raw/main/LICENSE"&gt;Apache-2.0&lt;/a&gt; which allows commercial use, modification, distribution, and private use.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] Subtitle generation currently works only for English. This is because Kokoro provides timestamp tokens only for English text. If you want subtitles in other languages, please request this feature in the &lt;a href="https://github.com/hexgrad/kokoro"&gt;Kokoro project&lt;/a&gt;. For more technical details, see &lt;a href="https://github.com/hexgrad/kokoro/raw/6d87f4ae7abc2d14dbc4b3ef2e5f19852e861ac2/kokoro/pipeline.py#L383"&gt;this line&lt;/a&gt; in the Kokoro's code.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Tags: audiobook, kokoro, text-to-speech, TTS, audiobook generator, audiobooks, text to speech, audiobook maker, audiobook creator, audiobook generator, voice-synthesis, text to audio, text to audio converter, text to speech converter, text to speech generator, text to speech software, text to speech app, epub to audio, pdf to audio, content-creation, media-generation&lt;/p&gt; 
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>LMCache/LMCache</title>
      <link>https://github.com/LMCache/LMCache</link>
      <description>&lt;p&gt;Supercharge Your LLM with the Fastest KV Cache Layer&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/LMCache/LMCache/dev/asset/logo.png" width="720" alt="lmcache logo" /&gt; &lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://docs.lmcache.ai/"&gt;&lt;img src="https://img.shields.io/badge/docs-live-brightgreen" alt="Docs" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/lmcache/"&gt;&lt;img src="https://img.shields.io/pypi/v/lmcache" alt="PyPI" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/lmcache/"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/lmcache" alt="PyPI - Python Version" /&gt;&lt;/a&gt; &lt;a href="https://buildkite.com/lmcache/lmcache-unittests"&gt;&lt;img src="https://badge.buildkite.com/ce25f1819a274b7966273bfa54f0e02f092c3de0d7563c5c9d.svg?sanitize=true" alt="Unit Tests" /&gt;&lt;/a&gt; &lt;a href="https://github.com/LMCache/LMCache/actions/workflows/code_quality_checks.yml"&gt;&lt;img src="https://github.com/lmcache/lmcache/actions/workflows/code_quality_checks.yml/badge.svg?branch=dev&amp;amp;label=tests" alt="Code Quality" /&gt;&lt;/a&gt; &lt;a href="https://buildkite.com/lmcache/lmcache-vllm-integration-tests"&gt;&lt;img src="https://badge.buildkite.com/108ddd4ab482a2480999dec8c62a640a3315ed4e6c4e86798e.svg?sanitize=true" alt="Integration Tests" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://www.bestpractices.dev/projects/10841"&gt;&lt;img src="https://www.bestpractices.dev/projects/10841/badge" alt="OpenSSF Best Practices" /&gt;&lt;/a&gt; &lt;a href="https://scorecard.dev/viewer/?uri=github.com/LMCache/LMCache"&gt;&lt;img src="https://api.scorecard.dev/projects/github.com/LMCache/LMCache/badge" alt="OpenSSF Scorecard" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/LMCache/LMCache/"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt; &lt;a href="https://github.com/LMCache/LMCache/graphs/commit-activity"&gt;&lt;img src="https://img.shields.io/github/commit-activity/w/LMCache/LMCache" alt="GitHub commit activity" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/lmcache/"&gt;&lt;img src="https://img.shields.io/pypi/dm/lmcache" alt="PyPI - Downloads" /&gt;&lt;/a&gt; &lt;a href="https://www.youtube.com/channel/UC58zMz55n70rtf1Ak2PULJA"&gt;&lt;img src="https://img.shields.io/youtube/channel/views/UC58zMz55n70rtf1Ak2PULJA" alt="YouTube Channel Views" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;p&gt;| &lt;a href="https://blog.lmcache.ai/"&gt;&lt;strong&gt;Blog&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://docs.lmcache.ai/"&gt;&lt;strong&gt;Documentation&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://join.slack.com/t/lmcacheworkspace/shared_invite/zt-36x1m765z-8FgDA_73vcXtlZ_4XvpE6Q"&gt;&lt;strong&gt;Join Slack&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://forms.gle/MHwLiYDU6kcW3dLj7"&gt;&lt;strong&gt;Interest Form&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://github.com/LMCache/LMCache/issues/1253"&gt;&lt;strong&gt;Roadmap&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;üî• &lt;strong&gt;NEW: For enterprise-scale deployment of LMCache and vLLM, please check out vLLM &lt;a href="https://github.com/vllm-project/production-stack"&gt;Production Stack&lt;/a&gt;. LMCache is also officially supported in &lt;a href="https://github.com/llm-d/llm-d/"&gt;llm-d&lt;/a&gt; and &lt;a href="https://github.com/kserve/kserve"&gt;KServe&lt;/a&gt;!&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Summary&lt;/h2&gt; 
&lt;p&gt;LMCache is an &lt;strong&gt;LLM&lt;/strong&gt; serving engine extension to &lt;strong&gt;reduce TTFT&lt;/strong&gt; and &lt;strong&gt;increase throughput&lt;/strong&gt;, especially under long-context scenarios. By storing the KV caches of reusable texts across various locations, including (GPU, CPU DRAM, Local Disk), LMCache reuses the KV caches of &lt;strong&gt;&lt;em&gt;any&lt;/em&gt;&lt;/strong&gt; reused text (not necessarily prefix) in &lt;strong&gt;&lt;em&gt;any&lt;/em&gt;&lt;/strong&gt; serving engine instance. Thus, LMCache saves precious GPU cycles and reduces user response delay.&lt;/p&gt; 
&lt;p&gt;By combining LMCache with vLLM, developers achieve 3-10x delay savings and GPU cycle reduction in many LLM use cases, including multi-round QA and RAG.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/86137f17-f216-41a0-96a7-e537764f7a4c" alt="performance" /&gt;&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; üî• Integration with vLLM v1 with the following features: 
  &lt;ul&gt; 
   &lt;li&gt;High performance CPU KVCache offloading&lt;/li&gt; 
   &lt;li&gt;Disaggregated prefill&lt;/li&gt; 
   &lt;li&gt;P2P KVCache sharing&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; LMCache is supported in the &lt;a href="https://github.com/vllm-project/production-stack/"&gt;vLLM production stack&lt;/a&gt;, &lt;a href="https://github.com/llm-d/llm-d/"&gt;llm-d&lt;/a&gt;, and &lt;a href="https://github.com/kserve/kserve"&gt;KServe&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Stable support for non-prefix KV caches&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Storage support as follows: 
  &lt;ul&gt; 
   &lt;li&gt;CPU&lt;/li&gt; 
   &lt;li&gt;Disk&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/ai-dynamo/nixl"&gt;NIXL&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Installation support through pip and latest vLLM&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;To use LMCache, simply install &lt;code&gt;lmcache&lt;/code&gt; from your package manager, e.g. pip:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install lmcache
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Works on Linux NVIDIA GPU platform.&lt;/p&gt; 
&lt;p&gt;More &lt;a href="https://docs.lmcache.ai/getting_started/installation"&gt;detailed installation instructions&lt;/a&gt; are available in the docs, particularly if you are not using the latest stable version of vllm or using another serving engine with different dependencies. Any "undefined symbol" or torch mismatch versions can be resolved in the documentation.&lt;/p&gt; 
&lt;h2&gt;Getting started&lt;/h2&gt; 
&lt;p&gt;The best way to get started is to checkout the &lt;a href="https://docs.lmcache.ai/getting_started/quickstart/"&gt;Quickstart Examples&lt;/a&gt; in the docs.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;Check out the LMCache &lt;a href="https://docs.lmcache.ai/"&gt;documentation&lt;/a&gt; which is available online.&lt;/p&gt; 
&lt;p&gt;We also post regularly in &lt;a href="https://blog.lmcache.ai/"&gt;LMCache blogs&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;p&gt;Go hands-on with our &lt;a href="https://github.com/LMCache/LMCache/tree/dev/examples"&gt;examples&lt;/a&gt;, demonstrating how to address different use cases with LMCache.&lt;/p&gt; 
&lt;h2&gt;Interested in Connecting?&lt;/h2&gt; 
&lt;p&gt;Fill out the &lt;a href="https://forms.gle/mQfQDUXbKfp2St1z7"&gt;interest form&lt;/a&gt;, &lt;a href="https://mailchi.mp/tensormesh/lmcache-sign-up-newsletter"&gt;sign up for our newsletter&lt;/a&gt;, &lt;a href="https://join.slack.com/t/lmcacheworkspace/shared_invite/zt-2viziwhue-5Amprc9k5hcIdXT7XevTaQ"&gt;join LMCache slack&lt;/a&gt;, &lt;a href="https://lmcache.ai/"&gt;check out LMCache website&lt;/a&gt;, or &lt;a href="mailto:contact@lmcache.ai"&gt;drop an email&lt;/a&gt;, and our team will reach out to you!&lt;/p&gt; 
&lt;h2&gt;Community meeting&lt;/h2&gt; 
&lt;p&gt;The &lt;a href="https://uchicago.zoom.us/j/6603596916?pwd=Z1E5MDRWUSt2am5XbEt4dTFkNGx6QT09"&gt;community meeting&lt;/a&gt; for LMCache is hosted bi-weekly. All are welcome to join!&lt;/p&gt; 
&lt;p&gt;Meetings are held bi-weekly on: Tuesdays at 9:00 AM PT ‚Äì &lt;a href="https://drive.usercontent.google.com/u/0/uc?id=1f5EXbooGcwNwzIpTgn5u4PHqXgfypMtu&amp;amp;export=download"&gt;Add to Calendar&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;We keep notes from each meeting on this &lt;a href="https://docs.google.com/document/d/1_Fl3vLtERFa3vTH00cezri78NihNBtSClK-_1tSrcow"&gt;document&lt;/a&gt; for summaries of standups, discussion, and action items.&lt;/p&gt; 
&lt;p&gt;Recordings of meetings are available on the &lt;a href="https://www.youtube.com/channel/UC58zMz55n70rtf1Ak2PULJA"&gt;YouTube LMCache channel&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome and value all contributions and collaborations. Please check out &lt;a href="https://raw.githubusercontent.com/LMCache/LMCache/dev/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt; on how to contribute.&lt;/p&gt; 
&lt;p&gt;We continually update &lt;a href="https://github.com/LMCache/LMCache/issues/627"&gt;[Onboarding] Welcoming contributors with good first issues!&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you use LMCache for your research, please cite our papers:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@inproceedings{liu2024cachegen,
  title={Cachegen: Kv cache compression and streaming for fast large language model serving},
  author={Liu, Yuhan and Li, Hanchen and Cheng, Yihua and Ray, Siddhant and Huang, Yuyang and Zhang, Qizheng and Du, Kuntai and Yao, Jiayi and Lu, Shan and Ananthanarayanan, Ganesh and others},
  booktitle={Proceedings of the ACM SIGCOMM 2024 Conference},
  pages={38--56},
  year={2024}
}

@article{cheng2024large,
  title={Do Large Language Models Need a Content Delivery Network?},
  author={Cheng, Yihua and Du, Kuntai and Yao, Jiayi and Jiang, Junchen},
  journal={arXiv preprint arXiv:2409.13761},
  year={2024}
}

@inproceedings{10.1145/3689031.3696098,
  author = {Yao, Jiayi and Li, Hanchen and Liu, Yuhan and Ray, Siddhant and Cheng, Yihua and Zhang, Qizheng and Du, Kuntai and Lu, Shan and Jiang, Junchen},
  title = {CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion},
  year = {2025},
  url = {https://doi.org/10.1145/3689031.3696098},
  doi = {10.1145/3689031.3696098},
  booktitle = {Proceedings of the Twentieth European Conference on Computer Systems},
  pages = {94‚Äì109},
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Socials&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.linkedin.com/company/lmcache-lab/?viewAsMember=true"&gt;Linkedin&lt;/a&gt; | &lt;a href="https://x.com/lmcache"&gt;Twitter&lt;/a&gt; | &lt;a href="https://www.youtube.com/@LMCacheTeam"&gt;Youtube&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;The LMCache codebase is licensed under Apache License 2.0. See the &lt;a href="https://raw.githubusercontent.com/LMCache/LMCache/dev/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Shubhamsaboo/awesome-llm-apps</title>
      <link>https://github.com/Shubhamsaboo/awesome-llm-apps</link>
      <description>&lt;p&gt;Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="http://www.theunwindai.com"&gt; &lt;img src="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/unwind_black.png" width="900px" alt="Unwind AI" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://www.linkedin.com/in/shubhamsaboo/"&gt; &lt;img src="https://img.shields.io/badge/-Follow%20Shubham%20Saboo-blue?logo=linkedin&amp;amp;style=flat-square" alt="LinkedIn" /&gt; &lt;/a&gt; &lt;a href="https://twitter.com/Saboo_Shubham_"&gt; &lt;img src="https://img.shields.io/twitter/follow/Shubham_Saboo" alt="Twitter" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; 
 &lt;!-- Keep these links. Translations will automatically update with the README. --&gt; &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=de"&gt;Deutsch&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=es"&gt;Espa√±ol&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=fr"&gt;fran√ßais&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ja"&gt;Êó•Êú¨Ë™û&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ko"&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=pt"&gt;Portugu√™s&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ru"&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=zh"&gt;‰∏≠Êñá&lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h1&gt;üåü Awesome LLM Apps&lt;/h1&gt; 
&lt;p&gt;A curated collection of &lt;strong&gt;Awesome LLM apps built with RAG, AI Agents, Multi-agent Teams, MCP, Voice Agents, and more.&lt;/strong&gt; This repository features LLM apps that use models from OpenAI, Anthropic, Google, and open-source models like DeepSeek, Qwen or Llama that you can run locally on your computer.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/9876" target="_blank"&gt; &lt;img src="https://trendshift.io/api/badge/repositories/9876" alt="Shubhamsaboo%2Fawesome-llm-apps | Trendshift" style="width: 250px; height: 55px;" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;ü§î Why Awesome LLM Apps?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;üí° Discover practical and creative ways LLMs can be applied across different domains, from code repositories to email inboxes and more.&lt;/li&gt; 
 &lt;li&gt;üî• Explore apps that combine LLMs from OpenAI, Anthropic, Gemini, and open-source alternatives with AI Agents, Agent Teams, MCP &amp;amp; RAG.&lt;/li&gt; 
 &lt;li&gt;üéì Learn from well-documented projects and contribute to the growing open-source ecosystem of LLM-powered applications.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìÇ Featured AI Projects&lt;/h2&gt; 
&lt;h3&gt;AI Agents&lt;/h3&gt; 
&lt;h3&gt;üå± Starter AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_blog_to_podcast_agent/"&gt;üéôÔ∏è AI Blog to Podcast Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_breakup_recovery_agent/"&gt;‚ù§Ô∏è‚Äçü©π AI Breakup Recovery Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_data_analysis_agent/"&gt;üìä AI Data Analysis Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_medical_imaging_agent/"&gt;ü©ª AI Medical Imaging Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_meme_generator_agent_browseruse/"&gt;üòÇ AI Meme Generator Agent (Browser)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_music_generator_agent/"&gt;üéµ AI Music Generator Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_travel_agent/"&gt;üõ´ AI Travel Agent (Local &amp;amp; Cloud)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/gemini_multimodal_agent_demo/"&gt;‚ú® Gemini Multimodal Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/local_news_agent_openai_swarm/"&gt;üåê Local News Agent (OpenAI Swarm)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/mixture_of_agents/"&gt;üîÑ Mixture of Agents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/xai_finance_agent/"&gt;üìä xAI Finance Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/opeani_research_agent/"&gt;üîç OpenAI Research Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/web_scrapping_ai_agent/"&gt;üï∏Ô∏è Web Scrapping AI Agent (Local &amp;amp; Cloud)&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üöÄ Advanced AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_deep_research_agent/"&gt;üîç AI Deep Research Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_consultant_agent"&gt;ü§ù AI Consultant Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_system_architect_r1/"&gt;üèóÔ∏è AI System Architect Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_lead_generation_agent/"&gt;üéØ AI Lead Generation Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_financial_coach_agent/"&gt;üí∞ AI Financial Coach Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_movie_production_agent/"&gt;üé¨ AI Movie Production Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_investment_agent/"&gt;üìà AI Investment Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_health_fitness_agent/"&gt;üèãÔ∏è‚Äç‚ôÇÔ∏è AI Health &amp;amp; Fitness Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/product_launch_intelligence_agent"&gt;üöÄ AI Product Launch Intelligence Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_journalist_agent/"&gt;üóûÔ∏è AI Journalist Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_mental_wellbeing_agent/"&gt;üß† AI Mental Wellbeing Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_meeting_agent/"&gt;üìë AI Meeting Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_Self-Evolving_agent/"&gt;üß¨ AI Self-Evolving Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_news_and_podcast_agents/"&gt;üéß AI Social Media News and Podcast Agent&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üéÆ Autonomous Game Playing Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/autonomous_game_playing_agent_apps/ai_3dpygame_r1/"&gt;üéÆ AI 3D Pygame Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/autonomous_game_playing_agent_apps/ai_chess_agent/"&gt;‚ôú AI Chess Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/autonomous_game_playing_agent_apps/ai_tic_tac_toe_agent/"&gt;üé≤ AI Tic-Tac-Toe Agent&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ü§ù Multi-agent Teams&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_competitor_intelligence_agent_team/"&gt;üß≤ AI Competitor Intelligence Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_finance_agent_team/"&gt;üí≤ AI Finance Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_game_design_agent_team/"&gt;üé® AI Game Design Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_legal_agent_team/"&gt;üë®‚Äç‚öñÔ∏è AI Legal Agent Team (Cloud &amp;amp; Local)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_recruitment_agent_team/"&gt;üíº AI Recruitment Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_real_estate_agent_team"&gt;üè† AI Real Estate Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_services_agency/"&gt;üë®‚Äçüíº AI Services Agency (CrewAI)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_teaching_agent_team/"&gt;üë®‚Äçüè´ AI Teaching Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_coding_agent_team/"&gt;üíª Multimodal Coding Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_design_agent_team/"&gt;‚ú® Multimodal Design Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_travel_planner_agent_team/"&gt;üåè AI Travel Planner Agent Team&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üó£Ô∏è Voice AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/voice_ai_agents/ai_audio_tour_agent/"&gt;üó£Ô∏è AI Audio Tour Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/voice_ai_agents/customer_support_voice_agent/"&gt;üìû Customer Support Voice Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/voice_ai_agents/voice_rag_openaisdk/"&gt;üîä Voice RAG Agent (OpenAI SDK)&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üåê MCP AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/mcp_ai_agents/browser_mcp_agent/"&gt;‚ôæÔ∏è Browser MCP Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/mcp_ai_agents/github_mcp_agent/"&gt;üêô GitHub MCP Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/mcp_ai_agents/notion_mcp_agent"&gt;üìë Notion MCP Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/mcp_ai_agents/ai_travel_planner_mcp_agent_team"&gt;üåç AI Travel Planner MCP Agent&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üìÄ RAG (Retrieval Augmented Generation)&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/agentic_rag/"&gt;üîó Agentic RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/agentic_rag_with_reasoning/"&gt;üßê Agentic RAG with Reasoning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/ai_blog_search/"&gt;üì∞ AI Blog Search (RAG)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/autonomous_rag/"&gt;üîç Autonomous RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/corrective_rag/"&gt;üîÑ Corrective RAG (CRAG)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/deepseek_local_rag_agent/"&gt;üêã Deepseek Local RAG Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/gemini_agentic_rag/"&gt;ü§î Gemini Agentic RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/hybrid_search_rag/"&gt;üëÄ Hybrid Search RAG (Cloud)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/llama3.1_local_rag/"&gt;üîÑ Llama 3.1 Local RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/local_hybrid_search_rag/"&gt;üñ•Ô∏è Local Hybrid Search RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/local_rag_agent/"&gt;ü¶ô Local RAG Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/rag-as-a-service/"&gt;üß© RAG-as-a-Service&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/rag_agent_cohere/"&gt;‚ú® RAG Agent with Cohere&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/rag_chain/"&gt;‚õìÔ∏è Basic RAG Chain&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/rag_database_routing/"&gt;üì† RAG with Database Routing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/vision_rag/"&gt;üñºÔ∏è Vision RAG&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üíæ LLM Apps with Memory Tutorials&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/ai_arxiv_agent_memory/"&gt;üíæ AI ArXiv Agent with Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/ai_travel_agent_memory/"&gt;üõ©Ô∏è AI Travel Agent with Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/llama3_stateful_chat/"&gt;üí¨ Llama3 Stateful Chat&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/llm_app_personalized_memory/"&gt;üìù LLM App with Personalized Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/local_chatgpt_with_memory/"&gt;üóÑÔ∏è Local ChatGPT Clone with Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/multi_llm_memory/"&gt;üß† Multi-LLM Application with Shared Memory&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üí¨ Chat with X Tutorials&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_github/"&gt;üí¨ Chat with GitHub (GPT &amp;amp; Llama3)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_gmail/"&gt;üì® Chat with Gmail&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_pdf/"&gt;üìÑ Chat with PDF (GPT &amp;amp; Llama3)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_research_papers/"&gt;üìö Chat with Research Papers (ArXiv) (GPT &amp;amp; Llama3)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_substack/"&gt;üìù Chat with Substack&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_youtube_videos/"&gt;üìΩÔ∏è Chat with YouTube Videos&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üîß LLM Fine-tuning Tutorials&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_finetuning_tutorials/llama3.2_finetuning/"&gt;üîß Llama 3.2 Fine-tuning&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üßë‚Äçüè´ AI Agent Framework Crash Course&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/ai_agent_framework_crash_course/google_adk_crash_course/"&gt;Google ADK Crash Course&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Starter agent; model‚Äëagnostic (OpenAI, Claude)&lt;/li&gt; 
   &lt;li&gt;Structured outputs (Pydantic)&lt;/li&gt; 
   &lt;li&gt;Tools: built‚Äëin, function, third‚Äëparty, MCP tools&lt;/li&gt; 
   &lt;li&gt;Memory; callbacks; Plugins&lt;/li&gt; 
   &lt;li&gt;Simple multi‚Äëagent; Multi‚Äëagent patterns&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üöÄ Getting Started&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Clone the repository&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/Shubhamsaboo/awesome-llm-apps.git 
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Navigate to the desired project directory&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cd awesome-llm-apps/starter_ai_agents/ai_travel_agent
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install the required dependencies&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Follow the project-specific instructions&lt;/strong&gt; in each project's &lt;code&gt;README.md&lt;/code&gt; file to set up and run the app.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;ü§ù Contributing to Open Source&lt;/h2&gt; 
&lt;p&gt;Contributions are welcome! If you have any ideas, improvements, or new apps to add, please create a new &lt;a href="https://github.com/Shubhamsaboo/awesome-llm-apps/issues"&gt;GitHub Issue&lt;/a&gt; or submit a pull request. Make sure to follow the existing project structure and include a detailed &lt;code&gt;README.md&lt;/code&gt; for each new app.&lt;/p&gt; 
&lt;h3&gt;Thank You, Community, for the Support! üôè&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#Shubhamsaboo/awesome-llm-apps&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=Shubhamsaboo/awesome-llm-apps&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;üåü &lt;strong&gt;Don‚Äôt miss out on future updates! Star the repo now and be the first to know about new and exciting LLM apps with RAG and AI Agents.&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>dreammis/social-auto-upload</title>
      <link>https://github.com/dreammis/social-auto-upload</link>
      <description>&lt;p&gt;Ëá™Âä®Âåñ‰∏ä‰º†ËßÜÈ¢ëÂà∞Á§æ‰∫§Â™í‰ΩìÔºöÊäñÈü≥„ÄÅÂ∞èÁ∫¢‰π¶„ÄÅËßÜÈ¢ëÂè∑„ÄÅtiktok„ÄÅyoutube„ÄÅbilibili&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;social-auto-upload&lt;/h1&gt; 
&lt;p&gt;&lt;code&gt;social-auto-upload&lt;/code&gt; ÊòØ‰∏Ä‰∏™Âº∫Â§ßÁöÑËá™Âä®ÂåñÂ∑•ÂÖ∑ÔºåÊó®Âú®Â∏ÆÂä©ÂÜÖÂÆπÂàõ‰ΩúËÄÖÂíåËøêËê•ËÄÖÈ´òÊïàÂú∞Â∞ÜËßÜÈ¢ëÂÜÖÂÆπ‰∏ÄÈîÆÂèëÂ∏ÉÂà∞Â§ö‰∏™ÂõΩÂÜÖÂ§ñ‰∏ªÊµÅÁ§æ‰∫§Â™í‰ΩìÂπ≥Âè∞„ÄÇ È°πÁõÆÂÆûÁé∞‰∫ÜÂØπ &lt;code&gt;ÊäñÈü≥&lt;/code&gt;„ÄÅ&lt;code&gt;Bilibili&lt;/code&gt;„ÄÅ&lt;code&gt;Â∞èÁ∫¢‰π¶&lt;/code&gt;„ÄÅ&lt;code&gt;Âø´Êâã&lt;/code&gt;„ÄÅ&lt;code&gt;ËßÜÈ¢ëÂè∑&lt;/code&gt;„ÄÅ&lt;code&gt;ÁôæÂÆ∂Âè∑&lt;/code&gt; ‰ª•Âèä &lt;code&gt;TikTok&lt;/code&gt; Á≠âÂπ≥Âè∞ÁöÑËßÜÈ¢ë‰∏ä‰º†„ÄÅÂÆöÊó∂ÂèëÂ∏ÉÁ≠âÂäüËÉΩ„ÄÇ ÁªìÂêàÂêÑÂπ≥Âè∞ &lt;code&gt;uploader&lt;/code&gt; Ê®°ÂùóÔºåÊÇ®ÂèØ‰ª•ËΩªÊùæÈÖçÁΩÆÂíåÊâ©Â±ïÊîØÊåÅÁöÑÂπ≥Âè∞ÔºåÂπ∂ÈÄöËøáÁ§∫‰æãËÑöÊú¨Âø´ÈÄü‰∏äÊâã„ÄÇ&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/dreammis/social-auto-upload/main/media/show/tkupload.gif" alt="tiktok show" width="800" /&gt; 
&lt;h2&gt;ÁõÆÂΩï&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/dreammis/social-auto-upload/main/#%F0%9F%92%A1%E5%8A%9F%E8%83%BD%E7%89%B9%E6%80%A7"&gt;üí° ÂäüËÉΩÁâπÊÄß&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/dreammis/social-auto-upload/main/#%F0%9F%9A%80%E6%94%AF%E6%8C%81%E7%9A%84%E5%B9%B3%E5%8F%B0"&gt;üöÄ ÊîØÊåÅÁöÑÂπ≥Âè∞&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/dreammis/social-auto-upload/main/#%F0%9F%92%BE%E5%AE%89%E8%A3%85%E6%8C%87%E5%8D%97"&gt;üíæ ÂÆâË£ÖÊåáÂçó&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/dreammis/social-auto-upload/main/#%F0%9F%8F%81%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B"&gt;üèÅ Âø´ÈÄüÂºÄÂßã&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/dreammis/social-auto-upload/main/#%F0%9F%90%87%E9%A1%B9%E7%9B%AE%E8%83%8C%E6%99%AF"&gt;üêá È°πÁõÆËÉåÊôØ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/dreammis/social-auto-upload/main/#%F0%9F%93%83%E8%AF%A6%E7%BB%86%E6%96%87%E6%A1%A3"&gt;üìÉ ËØ¶ÁªÜÊñáÊ°£&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/dreammis/social-auto-upload/main/#%F0%9F%90%BE%E4%BA%A4%E6%B5%81%E4%B8%8E%E6%94%AF%E6%8C%81"&gt;üêæ ‰∫§ÊµÅ‰∏éÊîØÊåÅ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/dreammis/social-auto-upload/main/#%F0%9F%A4%9D%E8%B4%A1%E7%8C%AE%E6%8C%87%E5%8D%97"&gt;ü§ù Ë¥°ÁåÆÊåáÂçó&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/dreammis/social-auto-upload/main/#%F0%9F%93%9C%E8%AE%B8%E5%8F%AF%E8%AF%81"&gt;üìú ËÆ∏ÂèØËØÅ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/dreammis/social-auto-upload/main/#%E2%AD%90Star-History"&gt;‚≠ê Star History&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üí°ÂäüËÉΩÁâπÊÄß&lt;/h2&gt; 
&lt;h3&gt;Â∑≤ÊîØÊåÅÂπ≥Âè∞&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ÂõΩÂÜÖÂπ≥Âè∞&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; ÊäñÈü≥&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; ËßÜÈ¢ëÂè∑&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Bilibili&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Â∞èÁ∫¢‰π¶&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Âø´Êâã&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; ÁôæÂÆ∂Âè∑&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ÂõΩÂ§ñÂπ≥Âè∞&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; TikTok&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Ê†∏ÂøÉÂäüËÉΩ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; ÂÆöÊó∂‰∏ä‰º† (Cron Job / Scheduled Upload)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Cookie ÁÆ°ÁêÜ (ÈÉ®ÂàÜÂÆûÁé∞ÔºåÊåÅÁª≠‰ºòÂåñ‰∏≠)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; ÂõΩÂ§ñÂπ≥Âè∞ Proxy ËÆæÁΩÆ (ÈÉ®ÂàÜÂÆûÁé∞)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ËÆ°ÂàíÊîØÊåÅ‰∏éÂºÄÂèë‰∏≠&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Âπ≥Âè∞Êâ©Â±ï&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" disabled /&gt; YouTube&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ÂäüËÉΩÂ¢ûÂº∫&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Êõ¥ÊòìÁî®ÁöÑÁâàÊú¨ (GUI / CLI ‰∫§‰∫í‰ºòÂåñ)&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; API Â∞ÅË£Ö&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Docker ÈÉ®ÁΩ≤&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Ëá™Âä®Âåñ‰∏ä‰º† (Êõ¥Êô∫ËÉΩÁöÑË∞ÉÂ∫¶Á≠ñÁï•)&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Â§öÁ∫øÁ®ã/ÂºÇÊ≠•‰∏ä‰º†‰ºòÂåñ&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Slack/Ê∂àÊÅØÊé®ÈÄÅÈÄöÁü•&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üöÄÊîØÊåÅÁöÑÂπ≥Âè∞&lt;/h2&gt; 
&lt;p&gt;Êú¨È°πÁõÆÈÄöËøáÂêÑÂπ≥Âè∞ÂØπÂ∫îÁöÑ &lt;code&gt;uploader&lt;/code&gt; Ê®°ÂùóÂÆûÁé∞ËßÜÈ¢ë‰∏ä‰º†ÂäüËÉΩ„ÄÇÊÇ®ÂèØ‰ª•Âú® &lt;code&gt;examples&lt;/code&gt; ÁõÆÂΩï‰∏ãÊâæÂà∞ÂêÑ‰∏™Âπ≥Âè∞ÁöÑ‰ΩøÁî®Á§∫‰æãËÑöÊú¨„ÄÇ&lt;/p&gt; 
&lt;p&gt;ÊØè‰∏™Á§∫‰æãËÑöÊú¨Â±ïÁ§∫‰∫ÜÂ¶Ç‰ΩïÈÖçÁΩÆÂíåË∞ÉÁî®Áõ∏Â∫îÁöÑ uploader„ÄÇ&lt;/p&gt; 
&lt;h2&gt;üíæÂÆâË£ÖÊåáÂçó&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ÂÖãÈöÜÈ°πÁõÆ&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/dreammis/social-auto-upload.git
cd social-auto-upload
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ÂÆâË£Ö‰æùËµñ&lt;/strong&gt;: Âª∫ËÆÆÂú®ËôöÊãüÁéØÂ¢É‰∏≠ÂÆâË£Ö‰æùËµñ„ÄÇ&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;conda create -n social-auto-upload python=3.10
conda activate social-auto-upload
# ÊåÇËΩΩÊ∏ÖÂçéÈïúÂÉè or ÂëΩ‰ª§Ë°å‰ª£ÁêÜ
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ÂÆâË£Ö Playwright ÊµèËßàÂô®È©±Âä®&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;playwright install chromium firefox
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Ê†πÊçÆÊÇ®ÁöÑÈúÄÊ±ÇÔºåËá≥Â∞ëÈúÄË¶ÅÂÆâË£Ö &lt;code&gt;chromium&lt;/code&gt;„ÄÇ&lt;code&gt;firefox&lt;/code&gt; ‰∏ªË¶ÅÁî®‰∫é TikTok ‰∏ä‰º†ÔºàÊóßÁâàÔºâ„ÄÇ&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;‰øÆÊîπÈÖçÁΩÆÊñá‰ª∂&lt;/strong&gt;: Â§çÂà∂ &lt;code&gt;conf.example.py&lt;/code&gt; Âπ∂ÈáçÂëΩÂêç‰∏∫ &lt;code&gt;conf.py&lt;/code&gt;„ÄÇ Âú® &lt;code&gt;conf.py&lt;/code&gt; ‰∏≠ÔºåÊÇ®ÈúÄË¶ÅÈÖçÁΩÆ‰ª•‰∏ãÂÜÖÂÆπÔºö&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;LOCAL_CHROME_PATH&lt;/code&gt;: Êú¨Âú∞ Chrome ÊµèËßàÂô®ÁöÑË∑ØÂæÑÔºåÊØîÂ¶Ç &lt;code&gt;C:\Program Files\Google\Chrome\Application\chrome.exe&lt;/code&gt; ‰øùÂ≠ò„ÄÇ&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;‰∏¥Êó∂Ëß£ÂÜ≥ÊñπÊ°à&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;ÈúÄË¶ÅÂú®Ê†πÁõÆÂΩïÂàõÂª∫ &lt;code&gt;cookiesFile&lt;/code&gt; Âíå &lt;code&gt;videoFile&lt;/code&gt; ‰∏§‰∏™Êñá‰ª∂Â§πÔºåÂàÜÂà´ÊòØ Â≠òÂÇ®cookieÊñá‰ª∂ Âíå Â≠òÂÇ®‰∏ä‰º†Êñá‰ª∂ ÁöÑÊñá‰ª∂Â§π&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ÈÖçÁΩÆÊï∞ÊçÆÂ∫ì&lt;/strong&gt;: Â¶ÇÊûú db/database.db Êñá‰ª∂‰∏çÂ≠òÂú®ÔºåÊÇ®ÂèØ‰ª•ËøêË°å‰ª•‰∏ãÂëΩ‰ª§Êù•ÂàùÂßãÂåñÊï∞ÊçÆÂ∫ìÔºö&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cd db
python createTable.py
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Ê≠§ÂëΩ‰ª§Â∞ÜÂàùÂßãÂåñ SQLite Êï∞ÊçÆÂ∫ì„ÄÇ&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ÂêØÂä®ÂêéÁ´ØÈ°πÁõÆ&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;python sau_backend.py
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;ÂêéÁ´ØÈ°πÁõÆÂ∞ÜÂú® &lt;code&gt;http://localhost:5409&lt;/code&gt; ÂêØÂä®„ÄÇ&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ÂêØÂä®ÂâçÁ´ØÈ°πÁõÆ&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cd sau_frontend
npm install
npm run dev
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;ÂâçÁ´ØÈ°πÁõÆÂ∞ÜÂú® &lt;code&gt;http://localhost:5173&lt;/code&gt; ÂêØÂä®ÔºåÂú®ÊµèËßàÂô®‰∏≠ÊâìÂºÄÊ≠§ÈìæÊé•Âç≥ÂèØËÆøÈóÆ„ÄÇ&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ÈùûÁ®ãÂ∫èÂëòÁî®Êà∑ÂèØ‰ª•ÂèÇËÄÉÔºö&lt;a href="https://juejin.cn/post/7372114027840208911"&gt;Êñ∞ÊâãÁ∫ßÊïôÁ®ã&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;üèÅÂø´ÈÄüÂºÄÂßã&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ÂáÜÂ§á Cookie&lt;/strong&gt;: Â§ßÂ§öÊï∞Âπ≥Âè∞ÈúÄË¶ÅÁôªÂΩïÂêéÁöÑ Cookie ‰ø°ÊÅØÊâçËÉΩËøõË°åÊìç‰Ωú„ÄÇËØ∑ÂèÇÁÖß examples ÁõÆÂΩï‰∏ãÂêÑ &lt;code&gt;get_xxx_cookie.py&lt;/code&gt; ËÑöÊú¨Ôºà‰æãÂ¶Ç get_douyin_cookie.py, get_ks_cookie.pyÔºâÁöÑËØ¥ÊòéÔºåËøêË°åËÑöÊú¨‰ª•ÁîüÊàêÂπ∂‰øùÂ≠ò Cookie Êñá‰ª∂ÔºàÈÄöÂ∏∏Âú® &lt;code&gt;cookies/[PLATFORM]_uploader/account.json&lt;/code&gt;Ôºâ„ÄÇ&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ÂáÜÂ§áËßÜÈ¢ëÊñá‰ª∂&lt;/strong&gt;: Â∞ÜÈúÄË¶Å‰∏ä‰º†ÁöÑËßÜÈ¢ëÊñá‰ª∂ÔºàÈÄöÂ∏∏‰∏∫ &lt;code&gt;.mp4&lt;/code&gt; Ê†ºÂºèÔºâÊîæÁΩÆÂú® videos ÁõÆÂΩï‰∏ã„ÄÇ ÈÉ®ÂàÜÂπ≥Âè∞ÊîØÊåÅËßÜÈ¢ëÂ∞ÅÈù¢ÔºåÂèØ‰ª•Â∞ÜÂ∞ÅÈù¢ÂõæÁâáÔºà‰æãÂ¶Ç &lt;code&gt;.png&lt;/code&gt; Ê†ºÂºèÔºå‰∏éËßÜÈ¢ëÂêåÂêçÔºâ‰πüÊîæÂú®Ê≠§ÁõÆÂΩï„ÄÇ Â¶ÇÊûúÈúÄË¶Å‰∏ä‰º†Ê†áÈ¢òÂèäÊ†áÁ≠æÔºåËØ∑Âú®ËßÜÈ¢ëÊñá‰ª∂ÊóÅËæπÂàõÂª∫‰∏Ä‰∏™ÂêåÂêçÁöÑ &lt;code&gt;.txt&lt;/code&gt; Êñá‰ª∂ÔºåÂÜÖÂÆπ‰∏∫Ê†áÈ¢òÂíåÊ†áÁ≠æÔºå‰ª•Êç¢Ë°åÂàÜÈöî„ÄÇ&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;‰øÆÊîπÂπ∂ËøêË°åÁ§∫‰æãËÑöÊú¨&lt;/strong&gt;: ÊâìÂºÄ examples ÁõÆÂΩï‰∏≠ÊÇ®ÊÉ≥‰ΩøÁî®ÁöÑÂπ≥Âè∞ÁöÑ‰∏ä‰º†ËÑöÊú¨Ôºà‰æãÂ¶Ç upload_video_to_douyin.pyÔºâ„ÄÇ&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Ê†πÊçÆËÑöÊú¨ÂÜÖÁöÑÊ≥®ÈáäÂíåËØ¥ÊòéÔºåÁ°ÆËÆ§ Cookie Êñá‰ª∂Ë∑ØÂæÑ„ÄÅËßÜÈ¢ëÊñá‰ª∂Ë∑ØÂæÑÁ≠âÈÖçÁΩÆÊòØÂê¶Ê≠£Á°Æ„ÄÇ&lt;/li&gt; 
   &lt;li&gt;ÊÇ®ÂèØ‰ª•‰øÆÊîπËÑöÊú¨‰ª•ÈÄÇÂ∫îÊÇ®ÁöÑÂÖ∑‰ΩìÈúÄÊ±ÇÔºå‰æãÂ¶ÇÊâπÈáè‰∏ä‰º†„ÄÅËá™ÂÆö‰πâÊ†áÈ¢ò„ÄÅÊ†áÁ≠æÁ≠â„ÄÇ&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ÊâßË°å‰∏ä‰º†&lt;/strong&gt;: ËøêË°å‰øÆÊîπÂêéÁöÑÁ§∫‰æãËÑöÊú¨Ôºå‰æãÂ¶ÇÔºö&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;python examples/upload_video_to_douyin.py
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;üêáÈ°πÁõÆËÉåÊôØ&lt;/h2&gt; 
&lt;p&gt;ËØ•È°πÁõÆÊúÄÂàùÊòØÊàë‰∏™‰∫∫Áî®‰∫éËá™Âä®ÂåñÁÆ°ÁêÜÁ§æ‰∫§Â™í‰ΩìËßÜÈ¢ëÂèëÂ∏ÉÁöÑÂ∑•ÂÖ∑„ÄÇÊàëÁöÑ‰∏ªË¶ÅÂèëÂ∏ÉÁ≠ñÁï•ÊòØÊèêÂâç‰∏ÄÂ§©ËÆæÁΩÆÂÆöÊó∂ÂèëÂ∏ÉÔºåÂõ†Ê≠§È°πÁõÆ‰∏≠ÂæàÂ§öÂÆöÊó∂ÂèëÂ∏ÉÁõ∏ÂÖ≥ÁöÑÈÄªËæëÊòØÂü∫‰∫é‚ÄúÁ¨¨‰∫åÂ§©‚ÄùÁöÑÊó∂Èó¥ËøõË°åËÆ°ÁÆóÁöÑ„ÄÇ&lt;/p&gt; 
&lt;p&gt;Â¶ÇÊûúÊÇ®ÈúÄË¶ÅÁ´ãÂç≥ÂèëÂ∏ÉÊàñÂÖ∂‰ªñÂÆöÂà∂ÂåñÁöÑÂèëÂ∏ÉÁ≠ñÁï•ÔºåÊ¨¢ËøéÁ†îÁ©∂Ê∫êÁ†ÅÊàñÂú®Á§æÂå∫ÊèêÈóÆ„ÄÇ&lt;/p&gt; 
&lt;h2&gt;üìÉËØ¶ÁªÜÊñáÊ°£&lt;/h2&gt; 
&lt;p&gt;Êõ¥ËØ¶ÁªÜÁöÑÊñáÊ°£ÂíåËØ¥ÊòéÔºåËØ∑Êü•ÁúãÔºö&lt;a href="https://sap-doc.nasdaddy.com/"&gt;social-auto-upload ÂÆòÊñπÊñáÊ°£&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üêæ‰∫§ÊµÅ‰∏éÊîØÊåÅ&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.buymeacoffee.com/hysn2001m"&gt;‚òï Donate as u like&lt;/a&gt; - Â¶ÇÊûúÊÇ®ËßâÂæóËøô‰∏™È°πÁõÆÂØπÊÇ®ÊúâÂ∏ÆÂä©ÔºåÂèØ‰ª•ËÄÉËôëËµûÂä©„ÄÇ&lt;/p&gt; 
&lt;p&gt;Â¶ÇÊûúÊÇ®‰πüÊòØÁã¨Á´ãÂºÄÂèëËÄÖ„ÄÅÊäÄÊúØÁà±Â•ΩËÄÖÔºåÂØπ #ÊäÄÊúØÂèòÁé∞ #AIÂàõ‰∏ö #Ë∑®Â¢ÉÁîµÂïÜ #Ëá™Âä®ÂåñÂ∑•ÂÖ∑ #ËßÜÈ¢ëÂàõ‰Ωú Á≠âËØùÈ¢òÊÑüÂÖ¥Ë∂£ÔºåÊ¨¢ËøéÂä†ÂÖ•Á§æÁæ§‰∫§ÊµÅ„ÄÇ&lt;/p&gt; 
&lt;h3&gt;Creator&lt;/h3&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt;
   &lt;td align="center"&gt; &lt;a href="https://sap-doc.nasdaddy.com/"&gt; &lt;img src="https://raw.githubusercontent.com/dreammis/social-auto-upload/main/media/mp.jpg" width="200px" alt="NasDaddyÂÖ¨‰ºóÂè∑" /&gt; &lt;br /&gt; &lt;sub&gt;&lt;b&gt;ÂæÆ‰ø°ÂÖ¨‰ºóÂè∑&lt;/b&gt;&lt;/sub&gt; &lt;/a&gt; &lt;br /&gt; &lt;a href="https://github.com/dreammis/social-auto-upload/commits?author=dreammis" title="Code"&gt;üíª&lt;/a&gt; &lt;br /&gt; ÂÖ≥Ê≥®ÂÖ¨‰ºóÂè∑ÔºåÂêéÂè∞ÂõûÂ§ç `‰∏ä‰º†` Ëé∑ÂèñÂä†Áæ§ÊñπÂºè &lt;/td&gt; 
   &lt;td align="center"&gt; &lt;a href="https://sap-doc.nasdaddy.com/"&gt; &lt;img src="https://raw.githubusercontent.com/dreammis/social-auto-upload/main/media/QR.png" width="200px" alt="ËµûËµèÁ†Å/ÂÖ•Áæ§ÂºïÂØº" /&gt; &lt;br /&gt; &lt;sub&gt;&lt;b&gt;‰∫§ÊµÅÁæ§ (ÈÄöËøáÂÖ¨‰ºóÂè∑Ëé∑Âèñ)&lt;/b&gt;&lt;/sub&gt; &lt;/a&gt; &lt;br /&gt; &lt;a href="https://sap-doc.nasdaddy.com/" title="Documentation"&gt;üìñ&lt;/a&gt; &lt;br /&gt; Â¶ÇÊûúÊÇ®ËßâÂæóÈ°πÁõÆÊúâÁî®ÔºåÂèØ‰ª•ËÄÉËôëÊâìËµèÊîØÊåÅ‰∏Ä‰∏ã &lt;/td&gt; 
  &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;Active Core Team&lt;/h3&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt;
   &lt;td align="center"&gt; &lt;a href="https://leedebug.github.io/"&gt; &lt;img src="https://raw.githubusercontent.com/dreammis/social-auto-upload/main/media/edan-qrcode.png" width="200px" alt="Edan Lee" /&gt; &lt;br /&gt; &lt;sub&gt;&lt;b&gt;Edan Lee&lt;/b&gt;&lt;/sub&gt; &lt;/a&gt; &lt;br /&gt; &lt;a href="https://github.com/dreammis/social-auto-upload/commits?author=LeeDebug" title="Code"&gt;üíª&lt;/a&gt; &lt;a href="https://leedebug.github.io/" title="Documentation"&gt;üìñ&lt;/a&gt; &lt;br /&gt; Â∞ÅË£Ö‰∫Ü api Êé•Âè£Âíå web ÂâçÁ´ØÁÆ°ÁêÜÁïåÈù¢ &lt;br /&gt; ÔºàËØ∑Ê≥®ÊòéÊù•ÊÑèÔºöËøõÁæ§„ÄÅÂ≠¶‰π†„ÄÅ‰ºÅ‰∏öÂí®ËØ¢Á≠âÔºâ &lt;/td&gt; 
  &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;ü§ùË¥°ÁåÆÊåáÂçó&lt;/h2&gt; 
&lt;p&gt;Ê¨¢ËøéÂêÑÁßçÂΩ¢ÂºèÁöÑË¥°ÁåÆÔºåÂåÖÊã¨‰ΩÜ‰∏çÈôê‰∫éÔºö&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Êèê‰∫§ BugÊä•Âëä Âíå FeatureËØ∑Ê±Ç„ÄÇ&lt;/li&gt; 
 &lt;li&gt;ÊîπËøõ‰ª£Á†Å„ÄÅÊñáÊ°£„ÄÇ&lt;/li&gt; 
 &lt;li&gt;ÂàÜ‰∫´‰ΩøÁî®ÁªèÈ™åÂíåÊïôÁ®ã„ÄÇ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Â¶ÇÊûúÊÇ®Â∏åÊúõË¥°ÁåÆ‰ª£Á†ÅÔºåËØ∑ÈÅµÂæ™‰ª•‰∏ãÊ≠•È™§Ôºö&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Fork Êú¨‰ªìÂ∫ì„ÄÇ&lt;/li&gt; 
 &lt;li&gt;ÂàõÂª∫‰∏Ä‰∏™Êñ∞ÁöÑÂàÜÊîØ (&lt;code&gt;git checkout -b feature/YourFeature&lt;/code&gt; Êàñ &lt;code&gt;bugfix/YourBugfix&lt;/code&gt;)„ÄÇ&lt;/li&gt; 
 &lt;li&gt;Êèê‰∫§ÊÇ®ÁöÑÊõ¥Êîπ (&lt;code&gt;git commit -m 'Add some feature'&lt;/code&gt;)„ÄÇ&lt;/li&gt; 
 &lt;li&gt;PushÂà∞ÊÇ®ÁöÑÂàÜÊîØ (&lt;code&gt;git push origin feature/YourFeature&lt;/code&gt;)„ÄÇ&lt;/li&gt; 
 &lt;li&gt;ÂàõÂª∫‰∏Ä‰∏™ Pull Request„ÄÇ&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;üìúËÆ∏ÂèØËØÅ&lt;/h2&gt; 
&lt;p&gt;Êú¨È°πÁõÆÊöÇÊó∂ÈááÁî® &lt;a href="https://raw.githubusercontent.com/dreammis/social-auto-upload/main/LICENSE"&gt;MIT License&lt;/a&gt; ÂºÄÊ∫êËÆ∏ÂèØËØÅ„ÄÇ&lt;/p&gt; 
&lt;h2&gt;‚≠êStar-History&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Â¶ÇÊûúËøô‰∏™È°πÁõÆÂØπÊÇ®ÊúâÂ∏ÆÂä©ÔºåËØ∑Áªô‰∏Ä‰∏™ ‚≠ê Star ‰ª•Ë°®Á§∫ÊîØÊåÅÔºÅ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#dreammis/social-auto-upload&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=dreammis/social-auto-upload&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>paperless-ngx/paperless-ngx</title>
      <link>https://github.com/paperless-ngx/paperless-ngx</link>
      <description>&lt;p&gt;A community-supported supercharged document management system: scan, index and archive all your documents&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="https://github.com/paperless-ngx/paperless-ngx/actions"&gt;&lt;img src="https://github.com/paperless-ngx/paperless-ngx/workflows/ci/badge.svg?sanitize=true" alt="ci" /&gt;&lt;/a&gt; &lt;a href="https://crowdin.com/project/paperless-ngx"&gt;&lt;img src="https://badges.crowdin.net/paperless-ngx/localized.svg?sanitize=true" alt="Crowdin" /&gt;&lt;/a&gt; &lt;a href="https://docs.paperless-ngx.com"&gt;&lt;img src="https://img.shields.io/github/deployments/paperless-ngx/paperless-ngx/github-pages?label=docs" alt="Documentation Status" /&gt;&lt;/a&gt; &lt;a href="https://codecov.io/gh/paperless-ngx/paperless-ngx"&gt;&lt;img src="https://codecov.io/gh/paperless-ngx/paperless-ngx/branch/main/graph/badge.svg?token=VK6OUPJ3TY" alt="codecov" /&gt;&lt;/a&gt; &lt;a href="https://matrix.to/#/%23paperlessngx%3Amatrix.org"&gt;&lt;img src="https://matrix.to/img/matrix-badge.svg?sanitize=true" alt="Chat on Matrix" /&gt;&lt;/a&gt; &lt;a href="https://demo.paperless-ngx.com"&gt;&lt;img src="https://cronitor.io/badges/ve7ItY/production/W5E_B9jkelG9ZbDiNHUPQEVH3MY.svg?sanitize=true" alt="demo" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://github.com/paperless-ngx/paperless-ngx/blob/main/resources/logo/web/png/White%20logo%20-%20no%20background.png" width="50%" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://github.com/paperless-ngx/paperless-ngx/raw/main/resources/logo/web/png/Black%20logo%20-%20no%20background.png" width="50%" /&gt; 
  &lt;img src="https://github.com/paperless-ngx/paperless-ngx/raw/main/resources/logo/web/png/Black%20logo%20-%20no%20background.png" width="50%" /&gt; 
 &lt;/picture&gt; &lt;/p&gt; 
&lt;!-- omit in toc --&gt; 
&lt;h1&gt;Paperless-ngx&lt;/h1&gt; 
&lt;p&gt;Paperless-ngx is a document management system that transforms your physical documents into a searchable online archive so you can keep, well, &lt;em&gt;less paper&lt;/em&gt;.&lt;/p&gt; 
&lt;p&gt;Paperless-ngx is the official successor to the original &lt;a href="https://github.com/the-paperless-project/paperless"&gt;Paperless&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/jonaswinkler/paperless-ng"&gt;Paperless-ng&lt;/a&gt; projects and is designed to distribute the responsibility of advancing and supporting the project among a team of people. &lt;a href="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/dev/#community-support"&gt;Consider joining us!&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Thanks to the generous folks at &lt;a href="https://m.do.co/c/8d70b916d462"&gt;DigitalOcean&lt;/a&gt;, a demo is available at &lt;a href="https://demo.paperless-ngx.com"&gt;demo.paperless-ngx.com&lt;/a&gt; using login &lt;code&gt;demo&lt;/code&gt; / &lt;code&gt;demo&lt;/code&gt;. &lt;em&gt;Note: demo content is reset frequently and confidential information should not be uploaded.&lt;/em&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/dev/#features"&gt;Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/dev/#getting-started"&gt;Getting started&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/dev/#contributing"&gt;Contributing&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/dev/#community-support"&gt;Community Support&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/dev/#translation"&gt;Translation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/dev/#feature-requests"&gt;Feature Requests&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/dev/#bugs"&gt;Bugs&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/dev/#related-projects"&gt;Related Projects&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/dev/#important-note"&gt;Important Note&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="right"&gt;This project is supported by:&lt;br /&gt; &lt;a href="https://m.do.co/c/8d70b916d462" style="padding-top: 4px; display: block;"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="https://opensource.nyc3.cdn.digitaloceanspaces.com/attribution/assets/SVG/DO_Logo_horizontal_white.svg" width="140px" /&gt; 
   &lt;source media="(prefers-color-scheme: light)" srcset="https://opensource.nyc3.cdn.digitaloceanspaces.com/attribution/assets/SVG/DO_Logo_horizontal_blue.svg" width="140px" /&gt; 
   &lt;img src="https://opensource.nyc3.cdn.digitaloceanspaces.com/attribution/assets/SVG/DO_Logo_horizontal_black_.svg?sanitize=true" width="140px" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h1&gt;Features&lt;/h1&gt; 
&lt;picture&gt; 
 &lt;source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/docs/assets/screenshots/documents-smallcards-dark.png" /&gt; 
 &lt;source media="(prefers-color-scheme: light)" srcset="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/docs/assets/screenshots/documents-smallcards.png" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/docs/assets/screenshots/documents-smallcards.png" /&gt; 
&lt;/picture&gt; 
&lt;p&gt;A full list of &lt;a href="https://docs.paperless-ngx.com/#features"&gt;features&lt;/a&gt; and &lt;a href="https://docs.paperless-ngx.com/#screenshots"&gt;screenshots&lt;/a&gt; are available in the &lt;a href="https://docs.paperless-ngx.com/"&gt;documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Getting started&lt;/h1&gt; 
&lt;p&gt;The easiest way to deploy paperless is &lt;code&gt;docker compose&lt;/code&gt;. The files in the &lt;a href="https://github.com/paperless-ngx/paperless-ngx/tree/main/docker/compose"&gt;&lt;code&gt;/docker/compose&lt;/code&gt; directory&lt;/a&gt; are configured to pull the image from the GitHub container registry.&lt;/p&gt; 
&lt;p&gt;If you'd like to jump right in, you can configure a &lt;code&gt;docker compose&lt;/code&gt; environment with our install script:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;bash -c "$(curl -L https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/install-paperless-ngx.sh)"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;More details and step-by-step guides for alternative installation methods can be found in &lt;a href="https://docs.paperless-ngx.com/setup/#installation"&gt;the documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Migrating from Paperless-ng is easy, just drop in the new docker image! See the &lt;a href="https://docs.paperless-ngx.com/setup/#migrating-to-paperless-ngx"&gt;documentation on migrating&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;!-- omit in toc --&gt; 
&lt;h3&gt;Documentation&lt;/h3&gt; 
&lt;p&gt;The documentation for Paperless-ngx is available at &lt;a href="https://docs.paperless-ngx.com/"&gt;https://docs.paperless-ngx.com&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Contributing&lt;/h1&gt; 
&lt;p&gt;If you feel like contributing to the project, please do! Bug fixes, enhancements, visual fixes etc. are always welcome. If you want to implement something big: Please start a discussion about that! The &lt;a href="https://docs.paperless-ngx.com/development/"&gt;documentation&lt;/a&gt; has some basic information on how to get started.&lt;/p&gt; 
&lt;h2&gt;Community Support&lt;/h2&gt; 
&lt;p&gt;People interested in continuing the work on paperless-ngx are encouraged to reach out here on github and in the &lt;a href="https://matrix.to/#/#paperless:matrix.org"&gt;Matrix Room&lt;/a&gt;. If you would like to contribute to the project on an ongoing basis there are multiple &lt;a href="https://github.com/orgs/paperless-ngx/people"&gt;teams&lt;/a&gt; (frontend, ci/cd, etc) that could use your help so please reach out!&lt;/p&gt; 
&lt;h2&gt;Translation&lt;/h2&gt; 
&lt;p&gt;Paperless-ngx is available in many languages that are coordinated on Crowdin. If you want to help out by translating paperless-ngx into your language, please head over to &lt;a href="https://crowdin.com/project/paperless-ngx"&gt;https://crowdin.com/project/paperless-ngx&lt;/a&gt;, and thank you! More details can be found in &lt;a href="https://github.com/paperless-ngx/paperless-ngx/raw/main/CONTRIBUTING.md#translating-paperless-ngx"&gt;CONTRIBUTING.md&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Feature Requests&lt;/h2&gt; 
&lt;p&gt;Feature requests can be submitted via &lt;a href="https://github.com/paperless-ngx/paperless-ngx/discussions/categories/feature-requests"&gt;GitHub Discussions&lt;/a&gt;, you can search for existing ideas, add your own and vote for the ones you care about.&lt;/p&gt; 
&lt;h2&gt;Bugs&lt;/h2&gt; 
&lt;p&gt;For bugs please &lt;a href="https://github.com/paperless-ngx/paperless-ngx/issues"&gt;open an issue&lt;/a&gt; or &lt;a href="https://github.com/paperless-ngx/paperless-ngx/discussions"&gt;start a discussion&lt;/a&gt; if you have questions.&lt;/p&gt; 
&lt;h1&gt;Related Projects&lt;/h1&gt; 
&lt;p&gt;Please see &lt;a href="https://github.com/paperless-ngx/paperless-ngx/wiki/Related-Projects"&gt;the wiki&lt;/a&gt; for a user-maintained list of related projects and software that is compatible with Paperless-ngx.&lt;/p&gt; 
&lt;h1&gt;Important Note&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Document scanners are typically used to scan sensitive documents like your social insurance number, tax records, invoices, etc. &lt;strong&gt;Paperless-ngx should never be run on an untrusted host&lt;/strong&gt; because information is stored in clear text without encryption. No guarantees are made regarding security (but we do try!) and you use the app at your own risk. &lt;strong&gt;The safest way to run Paperless-ngx is on a local server in your own home with backups in place&lt;/strong&gt;.&lt;/p&gt; 
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>aliasrobotics/cai</title>
      <link>https://github.com/aliasrobotics/cai</link>
      <description>&lt;p&gt;Cybersecurity AI (CAI), the framework for AI Security&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Cybersecurity AI (&lt;code&gt;CAI&lt;/code&gt;)&lt;/h1&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;a align="center" href="" target="https://github.com/aliasrobotics/CAI"&gt; &lt;img width="100%" src="https://github.com/aliasrobotics/cai/raw/main/media/cai.png" /&gt; &lt;/a&gt; &lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://badge.fury.io/py/cai-framework"&gt;&lt;img src="https://badge.fury.io/py/cai-framework.svg?sanitize=true" alt="version" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/projects/cai-framework"&gt;&lt;img src="https://static.pepy.tech/badge/cai-framework" alt="downloads" /&gt;&lt;/a&gt; &lt;a href="https://github.com/aliasrobotics/cai"&gt;&lt;img src="https://img.shields.io/badge/Linux-Supported-brightgreen?logo=linux&amp;amp;logoColor=white" alt="Linux" /&gt;&lt;/a&gt; &lt;a href="https://github.com/aliasrobotics/cai"&gt;&lt;img src="https://img.shields.io/badge/OS%20X-Supported-brightgreen?logo=apple&amp;amp;logoColor=white" alt="OS X" /&gt;&lt;/a&gt; &lt;a href="https://github.com/aliasrobotics/cai"&gt;&lt;img src="https://img.shields.io/badge/Windows-Supported-brightgreen?logo=windows&amp;amp;logoColor=white" alt="Windows" /&gt;&lt;/a&gt; &lt;a href="https://github.com/aliasrobotics/cai"&gt;&lt;img src="https://img.shields.io/badge/Android-Supported-brightgreen?logo=android&amp;amp;logoColor=white" alt="Android" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/fnUFcTaQAC"&gt;&lt;img src="https://img.shields.io/badge/Discord-7289DA?logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/pdf/2504.06017"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2504.06017-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2506.23592"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2506.23592-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;Cybersecurity AI (CAI) is a lightweight, open-source framework that empowers security professionals to build and deploy AI-powered offensive and defensive automation. CAI is the &lt;em&gt;de facto&lt;/em&gt; framework for AI Security, already used by thousands of individual users and hundreds of organizations. Whether you're a security researcher, ethical hacker, IT professional, or organization looking to enhance your security posture, CAI provides the building blocks to create specialized AI agents that can assist with mitigation, vulnerability discovery, exploitation, and security assessment.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ü§ñ &lt;strong&gt;300+ AI Models&lt;/strong&gt;: Support for OpenAI, Anthropic, DeepSeek, Ollama, and more&lt;/li&gt; 
 &lt;li&gt;üîß &lt;strong&gt;Built-in Security Tools&lt;/strong&gt;: Ready-to-use tools for reconnaissance, exploitation, and privilege escalation&lt;/li&gt; 
 &lt;li&gt;üèÜ &lt;strong&gt;Battle-tested&lt;/strong&gt;: Proven in HackTheBox CTFs, bug bounties, and real-world security &lt;a href="https://aliasrobotics.com/case-studies-robot-cybersecurity.php"&gt;case studies&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üéØ &lt;strong&gt;Agent-based Architecture&lt;/strong&gt;: Modular design with specialized agents for different security tasks&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Read the technical report: &lt;a href="https://arxiv.org/pdf/2504.06017"&gt;CAI: An Open, Bug Bounty-Ready Cybersecurity AI&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;a href="https://aliasrobotics.com/case-study-ecoforest.php"&gt;&lt;code&gt;OT&lt;/code&gt; - CAI and alias0 on: Ecoforest Heat Pumps&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://aliasrobotics.com/case-study-cai-mir.php"&gt;&lt;code&gt;Robotics&lt;/code&gt; - CAI and alias0 on: Mobile Industrial Robots (MiR)&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;CAI discovers critical vulnerability in Ecoforest heat pumps allowing unauthorized remote access and potential catastrophic failures. AI-powered security testing reveals exposed credentials and DES encryption weaknesses affecting all of their deployed units across Europe.&lt;/td&gt; 
   &lt;td&gt;CAI-powered security testing of MiR (Mobile Industrial Robot) platform through automated ROS message injection attacks. This study demonstrates how AI-driven vulnerability discovery can expose unauthorized access to robot control systems and alarm triggers.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://aliasrobotics.com/case-study-ecoforest.php"&gt;&lt;img src="https://aliasrobotics.com/img/case-study-portada-ecoforest.png" alt="" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://aliasrobotics.com/case-study-cai-mir.php"&gt;&lt;img src="https://aliasrobotics.com/img/case-study-portada-mir-cai.png" alt="" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;a href="https://aliasrobotics.com/case-study-mercado-libre.php"&gt;&lt;code&gt;IT&lt;/code&gt; (Web) - CAI and alias0 on: Mercado Libre's e-commerce&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://aliasrobotics.com/case-study-cai-mqtt-broker.php"&gt;&lt;code&gt;OT&lt;/code&gt; - CAI and alias0 on: MQTT broker&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;CAI-powered API vulnerability discovery at Mercado Libre through automated enumeration attacks. This study demonstrates how AI-driven security testing can expose user data exposure risks in e-commerce platforms at scale.&lt;/td&gt; 
   &lt;td&gt;CAI-powered testing exposed critical flaws in an MQTT broker within a Dockerized OT network. Without authentication, CAI subscribed to temperature and humidity topics and injected false values, corrupting data shown in Grafana dashboards.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://aliasrobotics.com/case-study-mercado-libre.php"&gt;&lt;img src="https://aliasrobotics.com/img/case-study-portada-mercado-libre.png" alt="" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://aliasrobotics.com/case-study-cai-mqtt-broker.php"&gt;&lt;img src="https://aliasrobotics.com/img/case-study-portada-mqtt-broker-cai.png" alt="" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING] &lt;span&gt;‚ö†&lt;/span&gt; CAI is in active development, so don't expect it to work flawlessly. Instead, contribute by raising an issue or &lt;a href="https://github.com/aliasrobotics/cai/pulls"&gt;sending a PR&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;Access to this library and the use of information, materials (or portions thereof), is &lt;strong&gt;&lt;u&gt;not intended&lt;/u&gt;, and is &lt;u&gt;prohibited&lt;/u&gt;, where such access or use violates applicable laws or regulations&lt;/strong&gt;. By no means the authors encourage or promote the unauthorized tampering with running systems. This can cause serious human harm and material damages.&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;By no means the authors of CAI encourage or promote the unauthorized tampering with compute systems. Please don't use the source code in here for cybercrime. &lt;u&gt;Pentest for good instead&lt;/u&gt;&lt;/em&gt;. By downloading, using, or modifying this source code, you agree to the terms of the &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/LICENSE"&gt;&lt;code&gt;LICENSE&lt;/code&gt;&lt;/a&gt; and the limitations outlined in the &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/DISCLAIMER"&gt;&lt;code&gt;DISCLAIMER&lt;/code&gt;&lt;/a&gt; file.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;&lt;span&gt;üîñ&lt;/span&gt; Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#cybersecurity-ai-cai"&gt;Cybersecurity AI (&lt;code&gt;CAI&lt;/code&gt;)&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#bookmark-table-of-contents"&gt;&lt;span&gt;üîñ&lt;/span&gt; Table of Contents&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-milestones"&gt;üéØ Milestones&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#pocs"&gt;PoCs&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#motivation"&gt;Motivation&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#bust_in_silhouette-why-cai"&gt;&lt;span&gt;üë§&lt;/span&gt; Why CAI?&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#ethical-principles-behind-cai"&gt;Ethical principles behind CAI&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#closed-source-alternatives"&gt;Closed-source alternatives&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#learn---cai-fluency"&gt;Learn - &lt;code&gt;CAI&lt;/code&gt; Fluency&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#nut_and_bolt-install"&gt;&lt;span&gt;üî©&lt;/span&gt; Install&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#os-x"&gt;OS X&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#ubuntu-2404"&gt;Ubuntu 24.04&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#ubuntu-2004"&gt;Ubuntu 20.04&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#windows-wsl"&gt;Windows WSL&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#android"&gt;Android&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#nut_and_bolt-setup-env-file"&gt;&lt;span&gt;üî©&lt;/span&gt; Setup &lt;code&gt;.env&lt;/code&gt; file&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-custom-openai-base-url-support"&gt;üîπ Custom OpenAI Base URL Support&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#triangular_ruler-architecture"&gt;&lt;span&gt;üìê&lt;/span&gt; Architecture:&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-agent"&gt;üîπ Agent&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-tools"&gt;üîπ Tools&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-handoffs"&gt;üîπ Handoffs&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-patterns"&gt;üîπ Patterns&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-turns-and-interactions"&gt;üîπ Turns and Interactions&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-tracing"&gt;üîπ Tracing&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-human-in-the-loop-hitl"&gt;üîπ Human-In-The-Loop (HITL)&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#rocket-quickstart"&gt;&lt;span&gt;üöÄ&lt;/span&gt; Quickstart&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#environment-variables"&gt;Environment Variables&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#openrouter-integration"&gt;OpenRouter Integration&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#mcp"&gt;MCP&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#development"&gt;Development&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#contributions"&gt;Contributions&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#optional-requirements-caiextensions"&gt;Optional Requirements: caiextensions&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#information_source-usage-data-collection"&gt;&lt;span&gt;‚Ñπ&lt;/span&gt; Usage Data Collection&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#reproduce-ci-setup-locally"&gt;Reproduce CI-Setup locally&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#faq"&gt;FAQ&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#citation"&gt;Citation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#acknowledgements"&gt;Acknowledgements&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üéØ Milestones&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://app.hackthebox.com/users/2268644"&gt;&lt;img src="https://img.shields.io/badge/HTB_ranking-top_90_Spain_(5_days)-red.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://app.hackthebox.com/users/2268644"&gt;&lt;img src="https://img.shields.io/badge/HTB_ranking-top_50_Spain_(6_days)-red.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://app.hackthebox.com/users/2268644"&gt;&lt;img src="https://img.shields.io/badge/HTB_ranking-top_30_Spain_(7_days)-red.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://app.hackthebox.com/users/2268644"&gt;&lt;img src="https://img.shields.io/badge/HTB_ranking-top_500_World_(7_days)-red.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://ctf.hackthebox.com/event/2000/scoreboard"&gt;&lt;img src="https://img.shields.io/badge/HTB_%22Human_vs_AI%22_CTF-top_1_(AIs)_world-red.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://ctf.hackthebox.com/event/2000/scoreboard"&gt;&lt;img src="https://img.shields.io/badge/HTB_%22Human_vs_AI%22_CTF-top_1_Spain-red.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://ctf.hackthebox.com/event/2000/scoreboard"&gt;&lt;img src="https://img.shields.io/badge/HTB_%22Human_vs_AI%22_CTF-top_20_World-red.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://ctf.hackthebox.com/event/2000/scoreboard"&gt;&lt;img src="https://img.shields.io/badge/HTB_%22Human_vs_AI%22_CTF-750_$-yellow.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://lu.ma/roboticshack?tk=RuryKF"&gt;&lt;img src="https://img.shields.io/badge/Mistral_AI_Robotics_Hackathon-2500_$-yellow.svg?sanitize=true" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;PoCs&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;CAI with &lt;code&gt;alias0&lt;/code&gt; on ROS message injection attacks in MiR-100 robot&lt;/th&gt; 
   &lt;th&gt;CAI with &lt;code&gt;alias0&lt;/code&gt; on API vulnerability discovery at Mercado Libre&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://asciinema.org/a/dNv705hZel2Rzrw0cju9HBGPh"&gt;&lt;img src="https://asciinema.org/a/dNv705hZel2Rzrw0cju9HBGPh.svg?sanitize=true" alt="asciicast" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://asciinema.org/a/9Hc9z1uFcdNjqP3bY5y7wO1Ww"&gt;&lt;img src="https://asciinema.org/a/9Hc9z1uFcdNjqP3bY5y7wO1Ww.svg?sanitize=true" alt="asciicast" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;CAI on JWT@PortSwigger CTF ‚Äî Cybersecurity AI&lt;/th&gt; 
   &lt;th&gt;CAI on HackableII Boot2Root CTF ‚Äî Cybersecurity AI&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://asciinema.org/a/713487"&gt;&lt;img src="https://asciinema.org/a/713487.svg?sanitize=true" alt="asciicast" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://asciinema.org/a/713485"&gt;&lt;img src="https://asciinema.org/a/713485.svg?sanitize=true" alt="asciicast" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;More case studies and PoCs are available at &lt;a href="https://aliasrobotics.com/case-studies-robot-cybersecurity.php"&gt;https://aliasrobotics.com/case-studies-robot-cybersecurity.php&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Motivation&lt;/h2&gt; 
&lt;h3&gt;&lt;span&gt;üë§&lt;/span&gt; Why CAI?&lt;/h3&gt; 
&lt;p&gt;The cybersecurity landscape is undergoing a dramatic transformation as AI becomes increasingly integrated into security operations. &lt;strong&gt;We predict that by 2028, AI-powered security testing tools will outnumber human pentesters&lt;/strong&gt;. This shift represents a fundamental change in how we approach cybersecurity challenges. &lt;em&gt;AI is not just another tool - it's becoming essential for addressing complex security vulnerabilities and staying ahead of sophisticated threats. As organizations face more advanced cyber attacks, AI-enhanced security testing will be crucial for maintaining robust defenses.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;This work builds upon prior efforts[^4] and similarly, we believe that democratizing access to advanced cybersecurity AI tools is vital for the entire security community. That's why we're releasing Cybersecurity AI (&lt;code&gt;CAI&lt;/code&gt;) as an open source framework. Our goal is to empower security researchers, ethical hackers, and organizations to build and deploy powerful AI-driven security tools. By making these capabilities openly available, we aim to level the playing field and ensure that cutting-edge security AI technology isn't limited to well-funded private companies or state actors.&lt;/p&gt; 
&lt;p&gt;Bug Bounty programs have become a cornerstone of modern cybersecurity, providing a crucial mechanism for organizations to identify and fix vulnerabilities in their systems before they can be exploited. These programs have proven highly effective at securing both public and private infrastructure, with researchers discovering critical vulnerabilities that might have otherwise gone unnoticed. CAI is specifically designed to enhance these efforts by providing a lightweight, ergonomic framework for building specialized AI agents that can assist in various aspects of Bug Bounty hunting - from initial reconnaissance to vulnerability validation and reporting. Our framework aims to augment human expertise with AI capabilities, helping researchers work more efficiently and thoroughly in their quest to make digital systems more secure.&lt;/p&gt; 
&lt;h3&gt;Ethical principles behind CAI&lt;/h3&gt; 
&lt;p&gt;You might be wondering if releasing CAI &lt;em&gt;in-the-wild&lt;/em&gt; given its capabilities and security implications is ethical. Our decision to open-source this framework is guided by two core ethical principles:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Democratizing Cybersecurity AI&lt;/strong&gt;: We believe that advanced cybersecurity AI tools should be accessible to the entire security community, not just well-funded private companies or state actors. By releasing CAI as an open source framework, we aim to empower security researchers, ethical hackers, and organizations to build and deploy powerful AI-driven security tools, leveling the playing field in cybersecurity.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Transparency in AI Security Capabilities&lt;/strong&gt;: Based on our research results, understanding of the technology, and dissection of top technical reports, we argue that current LLM vendors are undermining their cybersecurity capabilities. This is extremely dangerous and misleading. By developing CAI openly, we provide a transparent benchmark of what AI systems can actually do in cybersecurity contexts, enabling more informed decisions about security postures.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;CAI is built on the following core principles:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Cybersecurity oriented AI framework&lt;/strong&gt;: CAI is specifically designed for cybersecurity use cases, aiming at semi- and fully-automating offensive and defensive security tasks.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Open source, free for research&lt;/strong&gt;: CAI is open source and free for research purposes. We aim at democratizing access to AI and Cybersecurity. For professional or commercial use, including on-premise deployments, dedicated technical support and custom extensions &lt;a href="mailto:research@aliasrobotics.com"&gt;reach out&lt;/a&gt; to obtain a license.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Lightweight&lt;/strong&gt;: CAI is designed to be fast, and easy to use.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Modular and agent-centric design&lt;/strong&gt;: CAI operates on the basis of agents and agentic patterns, which allows flexibility and scalability. You can easily add the most suitable agents and pattern for your cybersecuritytarget case.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Tool-integration&lt;/strong&gt;: CAI integrates already built-in tools, and allows the user to integrate their own tools with their own logic easily.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Logging and tracing integrated&lt;/strong&gt;: using &lt;a href="https://github.com/Arize-ai/phoenix"&gt;&lt;code&gt;phoenix&lt;/code&gt;&lt;/a&gt;, the open source tracing and logging tool for LLMs. This provides the user with a detailed traceability of the agents and their execution.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-Model Support&lt;/strong&gt;: more than 300 supported and empowered by &lt;a href="https://github.com/BerriAI/litellm"&gt;LiteLLM&lt;/a&gt;. The most popular providers: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Anthropic&lt;/strong&gt;: &lt;code&gt;Claude 3.7&lt;/code&gt;, &lt;code&gt;Claude 3.5&lt;/code&gt;, &lt;code&gt;Claude 3&lt;/code&gt;, &lt;code&gt;Claude 3 Opus&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;OpenAI&lt;/strong&gt;: &lt;code&gt;O1&lt;/code&gt;, &lt;code&gt;O1 Mini&lt;/code&gt;, &lt;code&gt;O3 Mini&lt;/code&gt;, &lt;code&gt;GPT-4o&lt;/code&gt;, &lt;code&gt;GPT-4.5 Preview&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;DeepSeek&lt;/strong&gt;: &lt;code&gt;DeepSeek V3&lt;/code&gt;, &lt;code&gt;DeepSeek R1&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Ollama&lt;/strong&gt;: &lt;code&gt;Qwen2.5 72B&lt;/code&gt;, &lt;code&gt;Qwen2.5 14B&lt;/code&gt;, etc&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Closed-source alternatives&lt;/h3&gt; 
&lt;p&gt;Cybersecurity AI is a critical field, yet many groups are misguidedly pursuing it through closed-source methods for pure economic return, leveraging similar techniques and building upon existing closed-source (&lt;em&gt;often third-party owned&lt;/em&gt;) models. This approach not only squanders valuable engineering resources but also represents an economic waste and results in redundant efforts, as they often end up reinventing the wheel. Here are some of the closed-source initiatives we keep track of and attempting to leverage genAI and agentic frameworks in cybersecurity AI:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.acyber.co/"&gt;Autonomous Cyber&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://cracken.ai/"&gt;CrackenAGI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ethiack.com/"&gt;ETHIACK&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://horizon3.ai/"&gt;Horizon3&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.kindo.ai/"&gt;Kindo&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://lakera.ai"&gt;Lakera&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/www.mindfort.ai"&gt;Mindfort&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mindgard.ai/"&gt;Mindgard&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ndaysecurity.com/"&gt;NDAY Security&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.runsybil.com"&gt;Runsybil&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.selfhack.fi"&gt;Selfhack&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://squr.ai/"&gt;SQUR&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://staris.tech/"&gt;Staris&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.sxipher.com/"&gt;Sxipher&lt;/a&gt; (seems discontinued)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.terra.security"&gt;Terra Security&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://xint.io/"&gt;Xint&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.xbow.com"&gt;XBOW&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.zeropath.com"&gt;ZeroPath&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.zynap.com"&gt;Zynap&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://7ai.com"&gt;7ai&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Learn - &lt;code&gt;CAI&lt;/code&gt; Fluency&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;a align="center" href="" target="https://github.com/aliasrobotics/CAI"&gt; &lt;img width="100%" src="https://github.com/aliasrobotics/cai/raw/main/media/caiedu.PNG" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;English&lt;/th&gt; 
   &lt;th&gt;Spanish&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Episode 0&lt;/strong&gt;: What is CAI?&lt;/td&gt; 
   &lt;td&gt;Cybersecurity AI (&lt;code&gt;CAI&lt;/code&gt;) explained&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=nBdTxbKM4oo"&gt;&lt;img src="https://img.youtube.com/vi/nBdTxbKM4oo/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=FaUL9HXrQ5k"&gt;&lt;img src="https://img.youtube.com/vi/FaUL9HXrQ5k/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Episode 1&lt;/strong&gt;: The &lt;code&gt;CAI&lt;/code&gt; Framework&lt;/td&gt; 
   &lt;td&gt;Vision &amp;amp; Ethics - Explore the core motivation behind CAI and delve into the crucial ethical principles guiding its development. Understand the motivation behind CAI and how you can actively contribute to the future of cybersecurity and the CAI framework.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=QEiGdsMf29M&amp;amp;list=PLLc16OUiZWd4RuFdN5_Wx9xwjCVVbopzr&amp;amp;index=3"&gt;&lt;img src="https://img.youtube.com/vi/QEiGdsMf29M/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Episode 2&lt;/strong&gt;: From Zero to Cyber Hero&lt;/td&gt; 
   &lt;td&gt;Breaking into Cybersecurity with AI - A comprehensive guide for complete beginners to become cybersecurity practitioners using CAI and AI tools. Learn how to leverage artificial intelligence to accelerate your cybersecurity learning journey, from understanding basic security concepts to performing real-world security assessments, all without requiring prior cybersecurity experience.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=hSTLHOOcQoY&amp;amp;list=PLLc16OUiZWd4RuFdN5_Wx9xwjCVVbopzr&amp;amp;index=14"&gt;&lt;img src="https://img.youtube.com/vi/hSTLHOOcQoY/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Episode 3&lt;/strong&gt;: Vibe-Hacking Tutorial&lt;/td&gt; 
   &lt;td&gt;"My first Hack" - A Vibe-Hacking guide for newbies. We demonstrate a simple web security hack using a default agent and show how to leverage tools and interpret CIA output with the help of the CAI Python API. You'll also learn to compare different LLM models to find the best fit for your hacking endeavors.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=9vZ_Iyex7uI&amp;amp;list=PLLc16OUiZWd4RuFdN5_Wx9xwjCVVbopzr&amp;amp;index=1"&gt;&lt;img src="https://img.youtube.com/vi/9vZ_Iyex7uI/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=iAOMaI1ftiA&amp;amp;list=PLLc16OUiZWd4RuFdN5_Wx9xwjCVVbopzr&amp;amp;index=2"&gt;&lt;img src="https://img.youtube.com/vi/iAOMaI1ftiA/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Episode 4&lt;/strong&gt;: Intro ReAct&lt;/td&gt; 
   &lt;td&gt;The Evolution of LLMs - Learn how LLMs evolved from basic language models to advanced multiagency AI systems. From basic LLMs to Chain-of-Thought and Reasoning LLMs towards ReAct and Multi-Agent Architectures. Get to know the basic terms&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=tLdFO1flj_o&amp;amp;list=PLLc16OUiZWd4RuFdN5_Wx9xwjCVVbopzr&amp;amp;index=13"&gt;&lt;img src="https://img.youtube.com/vi/tLdFO1flj_o/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Episode 5&lt;/strong&gt;: CAI on CTF challenges&lt;/td&gt; 
   &lt;td&gt;Dive into Capture The Flag (CTF) competitions using CAI. Learn how to leverage AI agents to solve various cybersecurity challenges including web exploitation, cryptography, reverse engineering, and forensics. Discover how to configure CAI for competitive hacking scenarios and maximize your CTF performance with intelligent automation.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=MrXTQ0e2to4&amp;amp;list=PLLc16OUiZWd4RuFdN5_Wx9xwjCVVbopzr&amp;amp;index=13"&gt;&lt;img src="https://img.youtube.com/vi/MrXTQ0e2to4/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=r9US_JZa9_c&amp;amp;list=PLLc16OUiZWd4RuFdN5_Wx9xwjCVVbopzr&amp;amp;index=12"&gt;&lt;img src="https://img.youtube.com/vi/r9US_JZa9_c/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Annex 1&lt;/strong&gt;: &lt;code&gt;CAI&lt;/code&gt; 0.5.x release&lt;/td&gt; 
   &lt;td&gt;Introduce version 0.5 of &lt;code&gt;CAI&lt;/code&gt; including new multi-agent functionality, new commands such as &lt;code&gt;/history&lt;/code&gt;, &lt;code&gt;/compact&lt;/code&gt;, &lt;code&gt;/graph&lt;/code&gt; or &lt;code&gt;/memory&lt;/code&gt; and a case study showing how &lt;code&gt;CAI&lt;/code&gt; found a critical security flaw in OT heap pumps spread around the world.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=OPFH0ANUMMw"&gt;&lt;img src="https://img.youtube.com/vi/OPFH0ANUMMw/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=Q8AI4E4gH8k"&gt;&lt;img src="https://img.youtube.com/vi/Q8AI4E4gH8k/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Annex 2&lt;/strong&gt;: &lt;code&gt;CAI&lt;/code&gt; 0.4.x release and &lt;code&gt;alias0&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Introducing version 0.4 of &lt;code&gt;CAI&lt;/code&gt; with &lt;em&gt;streaming&lt;/em&gt; and improved MCP support. We also introduce &lt;code&gt;alias0&lt;/code&gt;, the Privacy-First Cybersecurity AI, a Model-of-Models Intelligence that implements a Privacy-by-Design architecture and obtains state-of-the-art results in cybersecurity benchmarks.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=NZjzfnvAZcc"&gt;&lt;img src="https://img.youtube.com/vi/NZjzfnvAZcc/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Annex 3&lt;/strong&gt;: Cybersecurity AI Community Meeting #1&lt;/td&gt; 
   &lt;td&gt;First Cybersecurity AI (&lt;code&gt;CAI&lt;/code&gt;) community meeting, over 40 participants from academia, industry, and defense gathered to discuss the open-source scaffolding behind CAI ‚Äî a project designed to build agentic AI systems for cybersecurity that are open, modular, and Bug Bounty-ready.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=4JqaTiVlgsw"&gt;&lt;img src="https://img.youtube.com/vi/4JqaTiVlgsw/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;&lt;span&gt;üî©&lt;/span&gt; Install&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install cai-framework
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Always create a new virtual environment to ensure proper dependency installation when updating CAI.&lt;/p&gt; 
&lt;p&gt;The following subsections provide a more detailed walkthrough on selected popular Operating Systems. Refer to the &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#development"&gt;Development&lt;/a&gt; section for developer-related install instructions.&lt;/p&gt; 
&lt;h3&gt;OS X&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;brew update &amp;amp;&amp;amp; \
    brew install git python@3.12

# Create virtual environment
python3.12 -m venv cai_env

# Install the package from the local directory
source cai_env/bin/activate &amp;amp;&amp;amp; pip install cai-framework

# Generate a .env file and set up with defaults
echo -e 'OPENAI_API_KEY="sk-1234"\nANTHROPIC_API_KEY=""\nOLLAMA=""\nPROMPT_TOOLKIT_NO_CPR=1\nCAI_STREAM=false' &amp;gt; .env

# Launch CAI
cai  # first launch it can take up to 30 seconds
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Ubuntu 24.04&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt-get update &amp;amp;&amp;amp; \
    sudo apt-get install -y git python3-pip python3.12-venv

# Create the virtual environment
python3.12 -m venv cai_env

# Install the package from the local directory
source cai_env/bin/activate &amp;amp;&amp;amp; pip install cai-framework

# Generate a .env file and set up with defaults
echo -e 'OPENAI_API_KEY="sk-1234"\nANTHROPIC_API_KEY=""\nOLLAMA=""\nPROMPT_TOOLKIT_NO_CPR=1\nCAI_STREAM=false' &amp;gt; .env

# Launch CAI
cai  # first launch it can take up to 30 seconds
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Ubuntu 20.04&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt-get update &amp;amp;&amp;amp; \
    sudo apt-get install -y software-properties-common

# Fetch Python 3.12
sudo add-apt-repository ppa:deadsnakes/ppa &amp;amp;&amp;amp; sudo apt update
sudo apt install python3.12 python3.12-venv python3.12-dev -y

# Create the virtual environment
python3.12 -m venv cai_env

# Install the package from the local directory
source cai_env/bin/activate &amp;amp;&amp;amp; pip install cai-framework

# Generate a .env file and set up with defaults
echo -e 'OPENAI_API_KEY="sk-1234"\nANTHROPIC_API_KEY=""\nOLLAMA=""\nPROMPT_TOOLKIT_NO_CPR=1\nCAI_STREAM=false' &amp;gt; .env

# Launch CAI
cai  # first launch it can take up to 30 seconds
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Windows WSL&lt;/h3&gt; 
&lt;p&gt;Go to the Microsoft page: &lt;a href="https://learn.microsoft.com/en-us/windows/wsl/install"&gt;https://learn.microsoft.com/en-us/windows/wsl/install&lt;/a&gt;. Here you will find all the instructions to install WSL&lt;/p&gt; 
&lt;p&gt;From Powershell write: wsl --install&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;
sudo apt-get update &amp;amp;&amp;amp; \
    sudo apt-get install -y git python3-pip python3-venv

# Create the virtual environment
python3 -m venv cai_env

# Install the package from the local directory
source cai_env/bin/activate &amp;amp;&amp;amp; pip install cai-framework

# Generate a .env file and set up with defaults
echo -e 'OPENAI_API_KEY="sk-1234"\nANTHROPIC_API_KEY=""\nOLLAMA=""\nPROMPT_TOOLKIT_NO_CPR=1\nCAI_STREAM=false' &amp;gt; .env

# Launch CAI
cai  # first launch it can take up to 30 seconds
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Android&lt;/h3&gt; 
&lt;p&gt;We recommend having at least 8 GB of RAM:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;First of all, install userland &lt;a href="https://play.google.com/store/apps/details?id=tech.ula&amp;amp;hl=es"&gt;https://play.google.com/store/apps/details?id=tech.ula&amp;amp;hl=es&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install Kali minimal in basic options (for free). [Or any other kali option if preferred]&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Update apt keys like in this example: &lt;a href="https://superuser.com/questions/1644520/apt-get-update-issue-in-kali"&gt;https://superuser.com/questions/1644520/apt-get-update-issue-in-kali&lt;/a&gt;, inside UserLand's Kali terminal execute&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Get new apt keys
wget http://http.kali.org/kali/pool/main/k/kali-archive-keyring/kali-archive-keyring_2024.1_all.deb

# Install new apt keys
sudo dpkg -i kali-archive-keyring_2024.1_all.deb &amp;amp;&amp;amp; rm kali-archive-keyring_2024.1_all.deb

# Update APT repository
sudo apt-get update

# CAI requieres python 3.12, lets install it (CAI for kali in Android)
sudo apt-get update &amp;amp;&amp;amp; sudo apt-get install -y git python3-pip build-essential zlib1g-dev libncurses5-dev libgdbm-dev libnss3-dev libssl-dev libreadline-dev libffi-dev libsqlite3-dev wget libbz2-dev pkg-config
wget https://www.python.org/ftp/python/3.12.4/Python-3.12.4.tar.xz
tar xf Python-3.12.4.tar.xz
cd ./configure --enable-optimizations
sudo make altinstall # This command takes long to execute

# Clone CAI's source code
git clone https://github.com/aliasrobotics/cai &amp;amp;&amp;amp; cd cai

# Create virtual environment
python3.12 -m venv cai_env

# Install the package from the local directory
source cai_env/bin/activate &amp;amp;&amp;amp; pip3 install -e .

# Generate a .env file and set up
cp .env.example .env  # edit here your keys/models

# Launch CAI
cai
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;&lt;span&gt;üî©&lt;/span&gt; Setup &lt;code&gt;.env&lt;/code&gt; file&lt;/h3&gt; 
&lt;p&gt;CAI leverages the &lt;code&gt;.env&lt;/code&gt; file to load configuration at launch. To facilitate the setup, the repo provides an exemplary &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/.env.example"&gt;&lt;code&gt;.env.example&lt;/code&gt;&lt;/a&gt; file provides a template for configuring CAI's setup and your LLM API keys to work with desired LLM models.&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;‚ö†&lt;/span&gt; Important:&lt;/p&gt; 
&lt;p&gt;CAI does NOT provide API keys for any model by default. Don't ask us to provide keys, use your own or host your own models.&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;‚ö†&lt;/span&gt; Note:&lt;/p&gt; 
&lt;p&gt;The OPENAI_API_KEY must not be left blank. It should contain either "sk-123" (as a placeholder) or your actual API key. See &lt;a href="https://github.com/aliasrobotics/cai/issues/27"&gt;https://github.com/aliasrobotics/cai/issues/27&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;‚ö†&lt;/span&gt; Note:&lt;/p&gt; 
&lt;p&gt;If you are using alias0 model, make sure that CAI is &amp;gt;0.4.0 version and here you have an .env example to be able to use it.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;OPENAI_API_KEY="sk-1234"
OLLAMA=""
ALIAS_API_KEY="&amp;lt;sk-your-key&amp;gt;"  # note, add yours
CAI_STEAM=False
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;üîπ Custom OpenAI Base URL Support&lt;/h3&gt; 
&lt;p&gt;CAI supports configuring a custom OpenAI API base URL via the &lt;code&gt;OPENAI_BASE_URL&lt;/code&gt; environment variable. This allows users to redirect API calls to a custom endpoint, such as a proxy or self-hosted OpenAI-compatible service.&lt;/p&gt; 
&lt;p&gt;Example &lt;code&gt;.env&lt;/code&gt; entry configuration:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;OLLAMA_API_BASE="https://custom-openai-proxy.com/v1"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or directly from the command line:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;OLLAMA_API_BASE="https://custom-openai-proxy.com/v1" cai
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;&lt;span&gt;üìê&lt;/span&gt; Architecture:&lt;/h2&gt; 
&lt;p&gt;CAI focuses on making cybersecurity agent &lt;strong&gt;coordination&lt;/strong&gt; and &lt;strong&gt;execution&lt;/strong&gt; lightweight, highly controllable, and useful for humans. To do so it builds upon 7 pillars: &lt;code&gt;Agent&lt;/code&gt;s, &lt;code&gt;Tools&lt;/code&gt;, &lt;code&gt;Handoffs&lt;/code&gt;, &lt;code&gt;Patterns&lt;/code&gt;, &lt;code&gt;Turns&lt;/code&gt;, &lt;code&gt;Tracing&lt;/code&gt; and &lt;code&gt;HITL&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                  ‚îÇ      HITL     ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Turns   ‚îÇ
                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚îÇ
                          ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Patterns ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Handoffs ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ ‚îÇ   Agents  ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ    LLMs   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚îÇ                   ‚îÇ
                          ‚îÇ                   ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Extensions ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Tracing  ‚îÇ       ‚îÇ   Tools   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                              ‚îÇ
                          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                          ‚ñº             ‚ñº          ‚ñº             ‚ñº
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ LinuxCmd  ‚îÇ‚îÇ WebSearch ‚îÇ‚îÇ    Code    ‚îÇ‚îÇ SSHTunnel ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you want to dive deeper into the code, check the following files as a start point for using CAI:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/raw/main/src/cai/__init__.py"&gt;&lt;strong&gt;init&lt;/strong&gt;.py&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/raw/main/src/cai/cli.py"&gt;cli.py&lt;/a&gt; - entrypoint for command line interface&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/raw/main/src/cai/util.py"&gt;util.py&lt;/a&gt; - utility functions&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/raw/main/src/cai/agents"&gt;agents&lt;/a&gt; - Agent implementations&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/raw/main/src/cai/internal"&gt;internal&lt;/a&gt; - CAI internal functions (endpoints, metrics, logging, etc.)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/raw/main/src/cai/prompts"&gt;prompts&lt;/a&gt; - Agent Prompt Database&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/raw/main/src/cai/repl"&gt;repl&lt;/a&gt; - CLI aesthetics and commands&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/raw/main/src/cai/sdk"&gt;sdk&lt;/a&gt; - CAI command sdk&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/tree/main/src/cai/tools"&gt;tools&lt;/a&gt; - agent tools&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üîπ Agent&lt;/h3&gt; 
&lt;p&gt;At its core, CAI abstracts its cybersecurity behavior via &lt;code&gt;Agents&lt;/code&gt; and agentic &lt;code&gt;Patterns&lt;/code&gt;. An Agent in &lt;em&gt;an intelligent system that interacts with some environment&lt;/em&gt;. More technically, within CAI we embrace a robotics-centric definition wherein an agent is anything that can be viewed as a system perceiving its environment through sensors, reasoning about its goals and and acting accordingly upon that environment through actuators (&lt;em&gt;adapted&lt;/em&gt; from Russel &amp;amp; Norvig, AI: A Modern Approach). In cybersecurity, an &lt;code&gt;Agent&lt;/code&gt; interacts with systems and networks, using peripherals and network interfaces as sensors, reasons accordingly and then executes network actions as if actuators. Correspondingly, in CAI, &lt;code&gt;Agent&lt;/code&gt;s implement the &lt;code&gt;ReACT&lt;/code&gt; (Reasoning and Action) agent model[^3]. For more information, see the &lt;a href="https://github.com/aliasrobotics/cai/raw/main/examples/basic/hello_world.py"&gt;example here&lt;/a&gt; for the full execution code, and refer to this &lt;a href="https://github.com/aliasrobotics/cai/raw/main/fluency/my-first-hack/my_first_hack.ipynb"&gt;jupyter notebook&lt;/a&gt; for a tutorial on how to use it.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from cai.sdk.agents import Agent, Runner, OpenAIChatCompletionsModel

import os
from openai import AsyncOpenAI
from dotenv import load_dotenv
load_dotenv()

agent = Agent(
      name="Custom Agent",
      instructions="""You are a Cybersecurity expert Leader""",
      model=OpenAIChatCompletionsModel(
          model=os.getenv('CAI_MODEL', "openai/gpt-4o"),
          openai_client=AsyncOpenAI(),
          )
      )

message = "Tell me about recursion in programming."
result = await Runner.run(agent, message)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;üîπ Tools&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;Tools&lt;/code&gt; let cybersecurity agents take actions by providing interfaces to execute system commands, run security scans, analyze vulnerabilities, and interact with target systems and APIs - they are the core capabilities that enable CAI agents to perform security tasks effectively; in CAI, tools include built-in cybersecurity utilities (like LinuxCmd for command execution, WebSearch for OSINT gathering, Code for dynamic script execution, and SSHTunnel for secure remote access), function calling mechanisms that allow integration of any Python function as a security tool, and agent-as-tool functionality that enables specialized security agents (such as reconnaissance or exploit agents) to be used by other agents, creating powerful collaborative security workflows without requiring formal handoffs between agents. For more information, please refer to the &lt;a href="https://github.com/aliasrobotics/cai/raw/main/examples/basic/tools.py"&gt;example here&lt;/a&gt; for the complete configuration of custom functions.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from cai.sdk.agents import Agent, Runner, OpenAIChatCompletionsModel
from cai.tools.reconnaissance.exec_code import execute_code
from cai.tools.reconnaissance.generic_linux_command import generic_linux_command

import os
from openai import AsyncOpenAI
from dotenv import load_dotenv
load_dotenv()

agent = Agent(
      name="Custom Agent",
      instructions="""You are a Cybersecurity expert Leader""",
      tools= [
        generic_linux_command,
        execute_code
      ],
      model=OpenAIChatCompletionsModel(
          model=os.getenv('CAI_MODEL', "openai/gpt-4o"),
          openai_client=AsyncOpenAI(),
          )
      )

message = "Tell me about recursion in programming."
result = await Runner.run(agent, message)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You may find different &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/cai/tools"&gt;tools&lt;/a&gt;. They are grouped in 6 major categories inspired by the security kill chain [^2]:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Reconnaissance and weaponization - &lt;em&gt;reconnaissance&lt;/em&gt; (crypto, listing, etc)&lt;/li&gt; 
 &lt;li&gt;Exploitation - &lt;em&gt;exploitation&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;Privilege escalation - &lt;em&gt;escalation&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;Lateral movement - &lt;em&gt;lateral&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;Data exfiltration - &lt;em&gt;exfiltration&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;Command and control - &lt;em&gt;control&lt;/em&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;üîπ Handoffs&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;Handoffs&lt;/code&gt; allow an &lt;code&gt;Agent&lt;/code&gt; to delegate tasks to another agent, which is crucial in cybersecurity operations where specialized expertise is needed for different phases of an engagement. In our framework, &lt;code&gt;Handoffs&lt;/code&gt; are implemented as tools for the LLM, where a &lt;strong&gt;handoff/transfer function&lt;/strong&gt; like &lt;code&gt;transfer_to_flag_discriminator&lt;/code&gt; enables the &lt;code&gt;ctf_agent&lt;/code&gt; to pass control to the &lt;code&gt;flag_discriminator_agent&lt;/code&gt; once it believes it has found the flag. This creates a security validation chain where the first agent handles exploitation and flag discovery, while the second agent specializes in flag verification, ensuring proper segregation of duties and leveraging specialized capabilities of different models for distinct security tasks. For more information, please refer to the &lt;a href="https://github.com/aliasrobotics/cai/raw/main/examples/cai/agent_patterns/handoffs.py"&gt;example here&lt;/a&gt; for the full execution code.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from cai.sdk.agents import function_tool
from cai.tools.common import run_command
from cai.sdk.agents import Agent, OpenAIChatCompletionsModel, HandoffInputData, Runner, function_tool, handoff, trace
from cai.sdk.agents.extensions import handoff_filters

import os
from openai import AsyncOpenAI
from dotenv import load_dotenv
load_dotenv()

@function_tool
def execute_cli_command(command: str) -&amp;gt; str:
    return run_command(command)

flag_discriminator = Agent(
    name="Flag discriminator",
    description="Agent focused on extracting the flag from the output",
    instructions="You are an agent tailored to extract the flag from a given output.",
    model=OpenAIChatCompletionsModel(
        model=os.getenv('CAI_MODEL', "qwen2.5:14b"),
        openai_client=AsyncOpenAI(),
    ) 
)

ctf_agent = Agent(
    name="CTF agent",
    description="Agent focused on conquering security challenges",
    instructions="You are a Cybersecurity expert Leader facing a CTF",
    tools=[
        execute_cli_command,
    ],
    model=OpenAIChatCompletionsModel(
        model= os.getenv('CAI_MODEL', "qwen2.5:14b"),
        openai_client=AsyncOpenAI(),
    ), 
    handoffs = [flag_discriminator]
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;üîπ Patterns&lt;/h3&gt; 
&lt;p&gt;An agentic &lt;code&gt;Pattern&lt;/code&gt; is a &lt;em&gt;structured design paradigm&lt;/em&gt; in artificial intelligence systems where autonomous or semi-autonomous agents operate within a defined &lt;em&gt;interaction framework&lt;/em&gt; (the pattern) to achieve a goal. These &lt;code&gt;Patterns&lt;/code&gt; specify the organization, coordination, and communication methods among agents, guiding decision-making, task execution, and delegation.&lt;/p&gt; 
&lt;p&gt;An agentic pattern (&lt;code&gt;AP&lt;/code&gt;) can be formally defined as a tuple:&lt;/p&gt; 
&lt;p&gt;\[ AP = (A, H, D, C, E) \]&lt;/p&gt; 
&lt;p&gt;wherein:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;\(A\) (Agents):&lt;/strong&gt; A set of autonomous entities, \( A = \{a_1, a_2, ..., a_n\} \), each with defined roles, capabilities, and internal states.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;\(H\) (Handoffs):&lt;/strong&gt; A function \( H: A \times T \to A \) that governs how tasks \( T \) are transferred between agents based on predefined logic (e.g., rules, negotiation, bidding).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;\(D\) (Decision Mechanism):&lt;/strong&gt; A decision function \( D: S \to A \) where \( S \) represents system states, and \( D \) determines which agent takes action at any given time.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;\(C\) (Communication Protocol):&lt;/strong&gt; A messaging function \( C: A \times A \to M \), where \( M \) is a message space, defining how agents share information.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;\(E\) (Execution Model):&lt;/strong&gt; A function \( E: A \times I \to O \) where \( I \) is the input space and \( O \) is the output space, defining how agents perform tasks.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;When building &lt;code&gt;Patterns&lt;/code&gt;, we generall y classify them among one of the following categories, though others exist:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;strong&gt;Agentic&lt;/strong&gt; &lt;code&gt;Pattern&lt;/code&gt; &lt;strong&gt;categories&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Swarm&lt;/code&gt; (Decentralized)&lt;/td&gt; 
   &lt;td&gt;Agents share tasks and self-assign responsibilities without a central orchestrator. Handoffs occur dynamically. &lt;em&gt;An example of a peer-to-peer agentic pattern is the &lt;code&gt;CTF Agentic Pattern&lt;/code&gt;, which involves a team of agents working together to solve a CTF challenge with dynamic handoffs.&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Hierarchical&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;A top-level agent (e.g., "PlannerAgent") assigns tasks via structured handoffs to specialized sub-agents. Alternatively, the structure of the agents is harcoded into the agentic pattern with pre-defined handoffs.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Chain-of-Thought&lt;/code&gt; (Sequential Workflow)&lt;/td&gt; 
   &lt;td&gt;A structured pipeline where Agent A produces an output, hands it to Agent B for reuse or refinement, and so on. Handoffs follow a linear sequence. &lt;em&gt;An example of a chain-of-thought agentic pattern is the &lt;code&gt;ReasonerAgent&lt;/code&gt;, which involves a Reasoning-type LLM that provides context to the main agent to solve a CTF challenge with a linear sequence.&lt;/em&gt;[^1]&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Auction-Based&lt;/code&gt; (Competitive Allocation)&lt;/td&gt; 
   &lt;td&gt;Agents "bid" on tasks based on priority, capability, or cost. A decision agent evaluates bids and hands off tasks to the best-fit agent.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Recursive&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;A single agent continuously refines its own output, treating itself as both executor and evaluator, with handoffs (internal or external) to itself. &lt;em&gt;An example of a recursive agentic pattern is the &lt;code&gt;CodeAgent&lt;/code&gt; (when used as a recursive agent), which continuously refines its own output by executing code and updating its own instructions.&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;For more information and examples of common agentic patterns, see the &lt;a href="https://github.com/aliasrobotics/cai/raw/main/examples/agent_patterns/README.md"&gt;examples folder&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;üîπ Turns and Interactions&lt;/h3&gt; 
&lt;p&gt;During the agentic flow (conversation), we distinguish between &lt;strong&gt;interactions&lt;/strong&gt; and &lt;strong&gt;turns&lt;/strong&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Interactions&lt;/strong&gt; are sequential exchanges between one or multiple agents. Each agent executing its logic corresponds with one &lt;em&gt;interaction&lt;/em&gt;. Since an &lt;code&gt;Agent&lt;/code&gt; in CAI generally implements the &lt;code&gt;ReACT&lt;/code&gt; agent model[^3], each &lt;em&gt;interaction&lt;/em&gt; consists of 1) a reasoning step via an LLM inference and 2) act by calling zero-to-n &lt;code&gt;Tools&lt;/code&gt;. This is defined in&lt;code&gt;process_interaction()&lt;/code&gt; in &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/cai/core.py"&gt;core.py&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Turns&lt;/strong&gt;: A turn represents a cycle of one ore more &lt;strong&gt;interactions&lt;/strong&gt; which finishes when the &lt;code&gt;Agent&lt;/code&gt; (or &lt;code&gt;Pattern&lt;/code&gt;) executing returns &lt;code&gt;None&lt;/code&gt;, judging there're no further actions to undertake. This is defined in &lt;code&gt;run()&lt;/code&gt;, see &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/cai/core.py"&gt;core.py&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] CAI Agents are not related to Assistants in the Assistants API. They are named similarly for convenience, but are otherwise completely unrelated. CAI is entirely powered by the Chat Completions API and is hence stateless between calls.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;üîπ Tracing&lt;/h3&gt; 
&lt;p&gt;CAI implements AI observability by adopting the OpenTelemetry standard and to do so, it leverages &lt;a href="https://github.com/Arize-ai/phoenix"&gt;Phoenix&lt;/a&gt; which provides comprehensive tracing capabilities through OpenTelemetry-based instrumentation, allowing you to monitor and analyze your security operations in real-time. This integration enables detailed visibility into agent interactions, tool usage, and attack vectors throughout penetration testing workflows, making it easier to debug complex exploitation chains, track vulnerability discovery processes, and optimize agent performance for more effective security assessments.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/media/tracing.png" alt="" /&gt;&lt;/p&gt; 
&lt;h3&gt;üîπ Human-In-The-Loop (HITL)&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;                      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                      ‚îÇ                                 ‚îÇ
                      ‚îÇ      Cybersecurity AI (CAI)     ‚îÇ
                      ‚îÇ                                 ‚îÇ
                      ‚îÇ       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ
                      ‚îÇ       ‚îÇ  Autonomous AI  ‚îÇ       ‚îÇ
                      ‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ
                      ‚îÇ                ‚îÇ                ‚îÇ
                      ‚îÇ                ‚îÇ                ‚îÇ
                      ‚îÇ       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
                      ‚îÇ       ‚îÇ HITL Interaction ‚îÇ      ‚îÇ
                      ‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
                      ‚îÇ                ‚îÇ                ‚îÇ
                      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                       ‚îÇ
                                       ‚îÇ Ctrl+C (cli.py)
                                       ‚îÇ
                           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                           ‚îÇ   Human Operator(s)   ‚îÇ
                           ‚îÇ  Expertise | Judgment ‚îÇ
                           ‚îÇ    Teleoperation      ‚îÇ
                           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;CAI delivers a framework for building Cybersecurity AIs with a strong emphasis on &lt;em&gt;semi-autonomous&lt;/em&gt; operation, as the reality is that &lt;strong&gt;fully-autonomous&lt;/strong&gt; cybersecurity systems remain premature and face significant challenges when tackling complex tasks. While CAI explores autonomous capabilities, we recognize that effective security operations still require human teleoperation providing expertise, judgment, and oversight in the security process.&lt;/p&gt; 
&lt;p&gt;Accordingly, the Human-In-The-Loop (&lt;code&gt;HITL&lt;/code&gt;) module is a core design principle of CAI, acknowledging that human intervention and teleoperation are essential components of responsible security testing. Through the &lt;code&gt;cli.py&lt;/code&gt; interface, users can seamlessly interact with agents at any point during execution by simply pressing &lt;code&gt;Ctrl+C&lt;/code&gt;. This is implemented across &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/cai/core.py"&gt;core.py&lt;/a&gt; and also in the REPL abstractions &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/cai/repl"&gt;REPL&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;&lt;span&gt;üöÄ&lt;/span&gt; Quickstart&lt;/h2&gt; 
&lt;p&gt;To start CAI after installing it, just type &lt;code&gt;cai&lt;/code&gt; in the CLI:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;‚îî‚îÄ# cai

          CCCCCCCCCCCCC      ++++++++   ++++++++      IIIIIIIIII
       CCC::::::::::::C  ++++++++++       ++++++++++  I::::::::I
     CC:::::::::::::::C ++++++++++         ++++++++++ I::::::::I
    C:::::CCCCCCCC::::C +++++++++    ++     +++++++++ II::::::II
   C:::::C       CCCCCC +++++++     +++++     +++++++   I::::I
  C:::::C                +++++     +++++++     +++++    I::::I
  C:::::C                ++++                   ++++    I::::I
  C:::::C                 ++                     ++     I::::I
  C:::::C                  +   +++++++++++++++   +      I::::I
  C:::::C                    +++++++++++++++++++        I::::I
  C:::::C                     +++++++++++++++++         I::::I
   C:::::C       CCCCCC        +++++++++++++++          I::::I
    C:::::CCCCCCCC::::C         +++++++++++++         II::::::II
     CC:::::::::::::::C           +++++++++           I::::::::I
       CCC::::::::::::C             +++++             I::::::::I
          CCCCCCCCCCCCC               ++              IIIIIIIIII

                      Cybersecurity AI (CAI), vX.Y.Z
                          Bug bounty-ready AI

CAI&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;That should initialize CAI and provide a prompt to execute any security task you want to perform. The navigation bar at the bottom displays important system information. This information helps you understand your environment while working with CAI.&lt;/p&gt; 
&lt;p&gt;Here's a quick &lt;a href="https://asciinema.org/a/zm7wS5DA2o0S9pu1Tb44pnlvy"&gt;demo video&lt;/a&gt; to help you get started with CAI. We'll walk through the basic steps ‚Äî from launching the tool to running your first AI-powered task in the terminal. Whether you're a beginner or just curious, this guide will show you how easy it is to begin using CAI.&lt;/p&gt; 
&lt;p&gt;From here on, type on &lt;code&gt;CAI&lt;/code&gt; and start your security exercise. Best way to learn is by example:&lt;/p&gt; 
&lt;h3&gt;Environment Variables&lt;/h3&gt; 
&lt;p&gt;For using private models, you are given a &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/.env.example"&gt;&lt;code&gt;.env.example&lt;/code&gt;&lt;/a&gt; file. Copy it and rename it as &lt;code&gt;.env&lt;/code&gt;. Fill in your corresponding API keys, and you are ready to use CAI.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;List of Environment Variables&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Variable&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CTF_NAME&lt;/td&gt; 
    &lt;td&gt;Name of the CTF challenge to run (e.g. "picoctf_static_flag")&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CTF_CHALLENGE&lt;/td&gt; 
    &lt;td&gt;Specific challenge name within the CTF to test&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CTF_SUBNET&lt;/td&gt; 
    &lt;td&gt;Network subnet for the CTF container&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CTF_IP&lt;/td&gt; 
    &lt;td&gt;IP address for the CTF container&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CTF_INSIDE&lt;/td&gt; 
    &lt;td&gt;Whether to conquer the CTF from within container&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_MODEL&lt;/td&gt; 
    &lt;td&gt;Model to use for agents&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_DEBUG&lt;/td&gt; 
    &lt;td&gt;Set debug output level (0: Only tool outputs, 1: Verbose debug output, 2: CLI debug output)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_BRIEF&lt;/td&gt; 
    &lt;td&gt;Enable/disable brief output mode&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_MAX_TURNS&lt;/td&gt; 
    &lt;td&gt;Maximum number of turns for agent interactions&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_TRACING&lt;/td&gt; 
    &lt;td&gt;Enable/disable OpenTelemetry tracing&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_AGENT_TYPE&lt;/td&gt; 
    &lt;td&gt;Specify the agents to use (boot2root, one_tool...)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_STATE&lt;/td&gt; 
    &lt;td&gt;Enable/disable stateful mode&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_MEMORY&lt;/td&gt; 
    &lt;td&gt;Enable/disable memory mode (episodic, semantic, all)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_MEMORY_ONLINE&lt;/td&gt; 
    &lt;td&gt;Enable/disable online memory mode&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_MEMORY_OFFLINE&lt;/td&gt; 
    &lt;td&gt;Enable/disable offline memory&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_ENV_CONTEXT&lt;/td&gt; 
    &lt;td&gt;Add dirs and current env to llm context&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_MEMORY_ONLINE_INTERVAL&lt;/td&gt; 
    &lt;td&gt;Number of turns between online memory updates&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_PRICE_LIMIT&lt;/td&gt; 
    &lt;td&gt;Price limit for the conversation in dollars&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_REPORT&lt;/td&gt; 
    &lt;td&gt;Enable/disable reporter mode (ctf, nis2, pentesting)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_SUPPORT_MODEL&lt;/td&gt; 
    &lt;td&gt;Model to use for the support agent&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_SUPPORT_INTERVAL&lt;/td&gt; 
    &lt;td&gt;Number of turns between support agent executions&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_WORKSPACE&lt;/td&gt; 
    &lt;td&gt;Defines the name of the workspace&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_WORKSPACE_DIR&lt;/td&gt; 
    &lt;td&gt;Specifies the directory path where the workspace is located&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h3&gt;OpenRouter Integration&lt;/h3&gt; 
&lt;p&gt;The Cybersecurity AI (CAI) platform offers seamless integration with OpenRouter, a unified interface for Large Language Models (LLMs). This integration is crucial for users who wish to leverage advanced AI capabilities in their cybersecurity tasks. OpenRouter acts as a bridge, allowing CAI to communicate with various LLMs, thereby enhancing the flexibility and power of the AI agents used within CAI.&lt;/p&gt; 
&lt;p&gt;To enable OpenRouter support in CAI, you need to configure your environment by adding specific entries to your &lt;code&gt;.env&lt;/code&gt; file. This setup ensures that CAI can interact with the OpenRouter API, facilitating the use of sophisticated models like Meta-LLaMA. Here‚Äôs how you can configure it:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CAI_AGENT_TYPE=redteam_agent
CAI_MODEL=openrouter/meta-llama/llama-4-maverick
OPENROUTER_API_KEY=&amp;lt;sk-your-key&amp;gt;  # note, add yours
OPENROUTER_API_BASE=https://openrouter.ai/api/v1
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;MCP&lt;/h3&gt; 
&lt;p&gt;CAI supports the Model Context Protocol (MCP) for integrating external tools and services with AI agents. MCP is supported via two transport mechanisms:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;SSE (Server-Sent Events)&lt;/strong&gt; - For web-based servers that push updates over HTTP connections:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CAI&amp;gt;/mcp load http://localhost:9876/sse burp
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;&lt;strong&gt;STDIO (Standard Input/Output)&lt;/strong&gt; - For local inter-process communication:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CAI&amp;gt;/mcp load stdio myserver python mcp_server.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Once connected, you can add the MCP tools to any agent:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CAI&amp;gt;/mcp add burp redteam_agent
Adding tools from MCP server 'burp' to agent 'Red Team Agent'...
                                 Adding tools to Red Team Agent
‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì
‚îÉ Tool                              ‚îÉ Status ‚îÉ Details                                         ‚îÉ
‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©
‚îÇ send_http_request                 ‚îÇ Added  ‚îÇ Available as: send_http_request                 ‚îÇ
‚îÇ create_repeater_tab               ‚îÇ Added  ‚îÇ Available as: create_repeater_tab               ‚îÇ
‚îÇ send_to_intruder                  ‚îÇ Added  ‚îÇ Available as: send_to_intruder                  ‚îÇ
‚îÇ url_encode                        ‚îÇ Added  ‚îÇ Available as: url_encode                        ‚îÇ
‚îÇ url_decode                        ‚îÇ Added  ‚îÇ Available as: url_decode                        ‚îÇ
‚îÇ base64encode                      ‚îÇ Added  ‚îÇ Available as: base64encode                      ‚îÇ
‚îÇ base64decode                      ‚îÇ Added  ‚îÇ Available as: base64decode                      ‚îÇ
‚îÇ generate_random_string            ‚îÇ Added  ‚îÇ Available as: generate_random_string            ‚îÇ
‚îÇ output_project_options            ‚îÇ Added  ‚îÇ Available as: output_project_options            ‚îÇ
‚îÇ output_user_options               ‚îÇ Added  ‚îÇ Available as: output_user_options               ‚îÇ
‚îÇ set_project_options               ‚îÇ Added  ‚îÇ Available as: set_project_options               ‚îÇ
‚îÇ set_user_options                  ‚îÇ Added  ‚îÇ Available as: set_user_options                  ‚îÇ
‚îÇ get_proxy_http_history            ‚îÇ Added  ‚îÇ Available as: get_proxy_http_history            ‚îÇ
‚îÇ get_proxy_http_history_regex      ‚îÇ Added  ‚îÇ Available as: get_proxy_http_history_regex      ‚îÇ
‚îÇ get_proxy_websocket_history       ‚îÇ Added  ‚îÇ Available as: get_proxy_websocket_history       ‚îÇ
‚îÇ get_proxy_websocket_history_regex ‚îÇ Added  ‚îÇ Available as: get_proxy_websocket_history_regex ‚îÇ
‚îÇ set_task_execution_engine_state   ‚îÇ Added  ‚îÇ Available as: set_task_execution_engine_state   ‚îÇ
‚îÇ set_proxy_intercept_state         ‚îÇ Added  ‚îÇ Available as: set_proxy_intercept_state         ‚îÇ
‚îÇ get_active_editor_contents        ‚îÇ Added  ‚îÇ Available as: get_active_editor_contents        ‚îÇ
‚îÇ set_active_editor_contents        ‚îÇ Added  ‚îÇ Available as: set_active_editor_contents        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
Added 20 tools from server 'burp' to agent 'Red Team Agent'.
CAI&amp;gt;/agent 13
CAI&amp;gt;Create a repeater tab
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can list all active MCP connections and their transport types:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CAI&amp;gt;/mcp list
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/386a1fd3-3469-4f84-9396-2a5236febe1f"&gt;https://github.com/user-attachments/assets/386a1fd3-3469-4f84-9396-2a5236febe1f&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Development&lt;/h2&gt; 
&lt;p&gt;Development is facilitated via VS Code dev. environments. To try out our development environment, clone the repository, open VS Code and enter de dev. container mode:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/media/cai_devenv.gif" alt="CAI Development Environment" /&gt;&lt;/p&gt; 
&lt;h3&gt;Contributions&lt;/h3&gt; 
&lt;p&gt;If you want to contribute to this project, use &lt;a href="https://pre-commit.com/"&gt;&lt;strong&gt;Pre-commit&lt;/strong&gt;&lt;/a&gt; before your MR&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install pre-commit
pre-commit # files staged
pre-commit run --all-files # all files
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Optional Requirements: caiextensions&lt;/h3&gt; 
&lt;p&gt;Currently, the extensions are not publicly available as the engineering endeavour to maintain them is significant. Instead, we're making selected custom caiextensions available for partner companies across collaborations.&lt;/p&gt; 
&lt;h3&gt;&lt;span&gt;‚Ñπ&lt;/span&gt; Usage Data Collection&lt;/h3&gt; 
&lt;p&gt;CAI is provided free of charge for researchers. To improve CAI‚Äôs detection accuracy and publish open security research, instead of payment for research use cases, we ask you to contribute to the CAI community by allowing usage data collection. This data helps us identify areas for improvement, understand how the framework is being used, and prioritize new features. Legal basis of data collection is under Art. 6 (1)(f) GDPR ‚Äî CAI‚Äôs legitimate interest in maintaining and improving security tooling, with Art. 89 safeguards for research. The collected data includes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Basic system information (OS type, Python version)&lt;/li&gt; 
 &lt;li&gt;Username and IP information&lt;/li&gt; 
 &lt;li&gt;Tool usage patterns and performance metrics&lt;/li&gt; 
 &lt;li&gt;Model interactions and token usage statistics&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We take your privacy seriously and only collect what's needed to make CAI better. For further info, reach out to researchÔº†aliasrobotics.com. You can disable some of the data collection features via the &lt;code&gt;CAI_TELEMETRY&lt;/code&gt; environment variable but we encourage you to keep it enabled and contribute back to research:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CAI_TELEMETRY=False cai
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Reproduce CI-Setup locally&lt;/h3&gt; 
&lt;p&gt;To simulate the CI/CD pipeline, you can run the following in the Gitlab runner machines:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run --rm -it \
  --privileged \
  --network=exploitflow_net \
  --add-host="host.docker.internal:host-gateway" \
  -v /cache:/cache \
  -v /var/run/docker.sock:/var/run/docker.sock:rw \
  registry.gitlab.com/aliasrobotics/alias_research/cai:latest bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;FAQ&lt;/h2&gt; 
&lt;details&gt;
 &lt;summary&gt;OLLAMA is giving me 404 errors&lt;/summary&gt; 
 &lt;p&gt;Ollama's API in OpenAI mode uses &lt;code&gt;/v1/chat/completions&lt;/code&gt; whereas the &lt;code&gt;openai&lt;/code&gt; library uses &lt;code&gt;base_url&lt;/code&gt; + &lt;code&gt;/chat/completions&lt;/code&gt;.&lt;/p&gt; 
 &lt;p&gt;We adopt the latter for overall alignment with the gen AI community and empower the former by allowing users to add the &lt;code&gt;v1&lt;/code&gt; themselves via:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;OLLAMA_API_BASE=http://IP:PORT/v1
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;See the following issues that treat this topic in more detail:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/issues/76"&gt;https://github.com/aliasrobotics/cai/issues/76&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/issues/83"&gt;https://github.com/aliasrobotics/cai/issues/83&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/issues/82"&gt;https://github.com/aliasrobotics/cai/issues/82&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Where are all the caiextensions?&lt;/summary&gt; 
 &lt;p&gt;See &lt;a href="https://gitlab.com/aliasrobotics/alias_research/caiextensions"&gt;all caiextensions&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;How do I install the report caiextension?&lt;/summary&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#optional-requirements-caiextensions"&gt;See here&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;How do I set up SSH access for Gitlab?&lt;/summary&gt; 
 &lt;p&gt;Generate a new SSH key&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;ssh-keygen -t ed25519
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Add the key to the SSH agent&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;ssh-add ~/.ssh/id_ed25519
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Add the public key to Gitlab Copy the key and add it to Gitlab under &lt;a href="https://gitlab.com/-/user_settings/ssh_keys"&gt;https://gitlab.com/-/user_settings/ssh_keys&lt;/a&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;cat ~/.ssh/id_ed25519.pub
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;To verify it:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;ssh -T git@gitlab.com
Welcome to GitLab, @vmayoral!
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;How do I clear Python cache?&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;find . -name "*.pyc" -delete &amp;amp;&amp;amp; find . -name "__pycache__" -delete
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;If host networking is not working with ollama check whether it has been disabled in Docker because you are not signed in&lt;/summary&gt; 
 &lt;p&gt;Docker in OS X behaves funny sometimes. Check if the following message has shown up:&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Host networking has been disabled because you are not signed in. Please sign in to enable it&lt;/em&gt;.&lt;/p&gt; 
 &lt;p&gt;Make sure this has been addressed and also that the Dev Container is not forwarding the 8000 port (click on x, if necessary in the ports section).&lt;/p&gt; 
 &lt;p&gt;To verify connection, from within the VSCode devcontainer:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;curl -v http://host.docker.internal:8000/api/version
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Run CAI against any target&lt;/summary&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/imgs/readme_imgs/cai-004-first-message.png" alt="cai-004-first-message" /&gt;&lt;/p&gt; 
 &lt;p&gt;The starting user prompt in this case is: &lt;code&gt;Target IP: 192.168.3.10, perform a full network scan&lt;/code&gt;.&lt;/p&gt; 
 &lt;p&gt;The agent started performing a nmap scan. You could either interact with the agent and give it more instructions, or let it run to see what it explores next.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;How do I interact with the agent? Type twice CTRL + C &lt;/summary&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/imgs/readme_imgs/cai-005-ctrl-c.png" alt="cai-005-ctrl-c" /&gt;&lt;/p&gt; 
 &lt;p&gt;If you want to use the HITL mode, you can do it by presssing twice &lt;code&gt;Ctrl + C&lt;/code&gt;. This will allow you to interact (prompt) with the agent whenever you want. The agent will not lose the previous context, as it is stored in the &lt;code&gt;history&lt;/code&gt; variable, which is passed to it and any agent that is called. This enables any agent to use the previous information and be more accurate and efficient.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; Can I change the model while CAI is running? /model &lt;/summary&gt; 
 &lt;p&gt;Use &lt;code&gt;/model&lt;/code&gt; to change the model.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/imgs/readme_imgs/cai-007-model-change.png" alt="cai-007-model-change" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;How can I list all the agents available? /agent &lt;/summary&gt; 
 &lt;p&gt;Use &lt;code&gt;/agent&lt;/code&gt; to list all the agents available.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/imgs/readme_imgs/cai-010-agents-menu.png" alt="cai-010-agents-menu" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; Where can I list all the environment variables? /config &lt;/summary&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/imgs/readme_imgs/cai-008-config.png" alt="cai-008-config" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; How to know more about the CLI? /help &lt;/summary&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/imgs/readme_imgs/cai-006-help.png" alt="cai-006-help" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;How can I trace the whole execution?&lt;/summary&gt; The environment variable `CAI_TRACING` allows the user to set it to `CAI_TRACING=true` to enable tracing, or `CAI_TRACING=false` to disable it. When CAI is prompted by the first time, the user is provided with two paths, the execution log, and the tracing log. 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/imgs/readme_imgs/cai-009-logs.png" alt="cai-009-logs" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Can I expand CAI capabilities using previous run logs?&lt;/summary&gt; 
 &lt;p&gt;Absolutely! The &lt;strong&gt;memory extension&lt;/strong&gt; allows you to use a previously sucessful runs ( the log object is stored as a &lt;strong&gt;.jsonl file in the &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/cai/logs"&gt;log&lt;/a&gt; folder&lt;/strong&gt; ) in a new run against the same target. The user is also given the path highlighted in orange as shown below.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/imgs/readme_imgs/cai-009-logs.png" alt="cai-009-logs" /&gt;&lt;/p&gt; 
 &lt;p&gt;How to make use of this functionality?&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Run CAI against the target. Let's assume the target name is: &lt;code&gt;target001&lt;/code&gt;.&lt;/li&gt; 
  &lt;li&gt;Get the log file path, something like: &lt;code&gt;logs/cai_20250408_111856.jsonl&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;Generate the memory using any model of your preference: &lt;code&gt;shell JSONL_FILE_PATH="logs/cai_20250408_111856.jsonl" CTF_INSIDE="false" CAI_MEMORY_COLLECTION="target001" CAI_MEMORY="episodic" CAI_MODEL="claude-3-5-sonnet-20241022" python3 tools/2_jsonl_to_memory.py &lt;/code&gt;&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;The script &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/cai/tools/2_jsonl_to_memory.py"&gt;&lt;code&gt;tools/2_jsonl_to_memory.py&lt;/code&gt;&lt;/a&gt; will generate a memory collection file with the most relevant steps. The quality of the memory collection will depend on the model you use.&lt;/p&gt; 
 &lt;ol start="4"&gt; 
  &lt;li&gt;Use the generated memory collection and execute a new run: &lt;code&gt;shell CAI_MEMORY="episodic" CAI_MODEL="gpt-4o" CAI_MEMORY_COLLECTION="target001" CAI_TRACING=false python3 cai/cli.py&lt;/code&gt;&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Can I expand CAI capabilities using scripts or extra information?&lt;/summary&gt; 
 &lt;p&gt;Currently, CAI supports text based information. You can add any extra information on the target you are facing by copy-pasting it directly into the system or user prompt.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;How?&lt;/strong&gt; By adding it to the system (&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/cai/repl/templates/system_master_template.md"&gt;&lt;code&gt;system_master_template.md&lt;/code&gt;&lt;/a&gt;) or the user prompt (&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/cai/repl/templates/user_master_template.md"&gt;&lt;code&gt;user_master_template.md&lt;/code&gt;&lt;/a&gt;). You can always directly prompt the path to the model, and it will &lt;code&gt;cat&lt;/code&gt; it.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;How CAI licence works?&lt;/summary&gt; 
 &lt;p&gt;CAI‚Äôs current license does not restrict usage for research purposes. You are free to use CAI for security assessments (pentests), to develop additional features, and to integrate it into your research activities, as long as you comply with local laws.&lt;/p&gt; 
 &lt;p&gt;If you or your organization start benefiting commercially from CAI (e.g., offering pentesting services powered by CAI), then a commercial license will be required to help sustain the project.&lt;/p&gt; 
 &lt;p&gt;CAI itself is not a profit-seeking initiative. Our goal is to build a sustainable open-source project. We simply ask that those who profit from CAI contribute back and support our ongoing development.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you want to cite our work, please use the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{mayoralvilches2025caiopenbugbountyready,
      title={CAI: An Open, Bug Bounty-Ready Cybersecurity AI},
      author={V√≠ctor Mayoral-Vilches and Luis Javier Navarrete-Lozano and Mar√≠a Sanz-G√≥mez and Lidia Salas Espejo and Marti√±o Crespo-√Ålvarez and Francisco Oca-Gonzalez and Francesco Balassone and Alfonso Glera-Pic√≥n and Unai Ayucar-Carbajo and Jon Ander Ruiz-Alcalde and Stefan Rass and Martin Pinzger and Endika Gil-Uriarte},
      year={2025},
      eprint={2504.06017},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2504.06017},
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{mayoralvilches2025cybersecurityaidangerousgap,
      title={Cybersecurity AI: The Dangerous Gap Between Automation and Autonomy}, 
      author={V√≠ctor Mayoral-Vilches},
      year={2025},
      eprint={2506.23592},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2506.23592}, 
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;p&gt;CAI was initially developed by &lt;a href="https://aliasrobotics.com"&gt;Alias Robotics&lt;/a&gt; and co-funded by the European EIC accelerator project RIS (GA 101161136) - HORIZON-EIC-2023-ACCELERATOR-01 call. The original agentic principles are inspired from OpenAI's &lt;a href="https://github.com/openai/swarm"&gt;&lt;code&gt;swarm&lt;/code&gt;&lt;/a&gt; library and translated into newer prototypes. This project also makes use of other relevant open source building blocks including &lt;a href="https://github.com/BerriAI/litellm"&gt;&lt;code&gt;LiteLLM&lt;/code&gt;&lt;/a&gt;, and &lt;a href="https://github.com/Arize-ai/phoenix"&gt;&lt;code&gt;phoenix&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;!-- Footnotes --&gt; 
&lt;p&gt;[^1]: Arguably, the Chain-of-Thought agentic pattern is a special case of the Hierarchical agentic pattern. [^2]: Kamhoua, C. A., Leslie, N. O., &amp;amp; Weisman, M. J. (2018). Game theoretic modeling of advanced persistent threat in internet of things. Journal of Cyber Security and Information Systems. [^3]: Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., &amp;amp; Cao, Y. (2023, January). React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR). [^4]: Deng, G., Liu, Y., Mayoral-Vilches, V., Liu, P., Li, Y., Xu, Y., ... &amp;amp; Rass, S. (2024). {PentestGPT}: Evaluating and harnessing large language models for automated penetration testing. In 33rd USENIX Security Symposium (USENIX Security 24) (pp. 847-864).&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>manycore-research/SpatialLM</title>
      <link>https://github.com/manycore-research/SpatialLM</link>
      <description>&lt;p&gt;SpatialLM: Training Large Language Models for Structured Indoor Modeling&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;SpatialLM&lt;/h1&gt; 
&lt;!-- markdownlint-disable first-line-h1 --&gt; 
&lt;!-- markdownlint-disable html --&gt; 
&lt;!-- markdownlint-disable no-duplicate-header --&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/manycore-research/SpatialLM/main/figures/logo_light.png#gh-light-mode-only" width="60%" alt="SpatialLM" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/manycore-research/SpatialLM/main/figures/logo_dark.png#gh-dark-mode-only" width="60%" alt="SpatialLM" /&gt; 
&lt;/div&gt; 
&lt;hr style="margin-top: 0; margin-bottom: 8px;" /&gt; 
&lt;div align="center" style="margin-top: 0; padding-top: 0; line-height: 1;"&gt; 
 &lt;a href="https://manycore-research.github.io/SpatialLM" target="_blank" style="margin: 2px;"&gt;&lt;img alt="Project" src="https://img.shields.io/badge/üåê%20Website-SpatialLM-ffc107?color=42a5f5&amp;amp;logoColor=white" style="display: inline-block; vertical-align: middle;" /&gt;&lt;/a&gt; 
 &lt;a href="https://arxiv.org/abs/2506.07491" target="_blank" style="margin: 2px;"&gt;&lt;img alt="arXiv" src="https://img.shields.io/badge/arXiv-Techreport-b31b1b?logo=arxiv&amp;amp;logoColor=white" style="display: inline-block; vertical-align: middle;" /&gt;&lt;/a&gt; 
 &lt;a href="https://github.com/manycore-research/SpatialLM" target="_blank" style="margin: 2px;"&gt;&lt;img alt="GitHub" src="https://img.shields.io/badge/GitHub-SpatialLM-24292e?logo=github&amp;amp;logoColor=white" style="display: inline-block; vertical-align: middle;" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;div align="center" style="line-height: 1;"&gt; 
 &lt;a href="https://huggingface.co/manycore-research/SpatialLM1.1-Qwen-0.5B" target="_blank" style="margin: 2px;"&gt;&lt;img alt="Hugging Face" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-SpatialLM-ffc107?color=ffc107&amp;amp;logoColor=white" style="display: inline-block; vertical-align: middle;" /&gt;&lt;/a&gt; 
 &lt;a href="https://huggingface.co/datasets/manycore-research/SpatialLM-Testset" target="_blank" style="margin: 2px;"&gt;&lt;img alt="Dataset" src="https://img.shields.io/badge/%F0%9F%A4%97%20Dataset-Testset-ffc107?color=ffc107&amp;amp;logoColor=white" style="display: inline-block; vertical-align: middle;" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;‚ú® News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;[Jun, 2025] Check out our new models: &lt;a href="https://huggingface.co/manycore-research/SpatialLM1.1-Llama-1B"&gt;SpatialLM1.1-Llama-1B&lt;/a&gt; and &lt;a href="https://huggingface.co/manycore-research/SpatialLM1.1-Qwen-0.5B"&gt;SpatialLM1.1-Qwen-0.5B&lt;/a&gt;, now available on Hugging Face. SpatialLM1.1 doubles the point cloud resolution, incorporates a more powerful point cloud encoder &lt;a href="https://xywu.me/sonata/"&gt;Sonata&lt;/a&gt; and supports detection with user-specified categories.&lt;/li&gt; 
 &lt;li&gt;[Jun, 2025] SpatialLM &lt;a href="https://arxiv.org/abs/2506.07491"&gt;Technical Report&lt;/a&gt; is now on arXiv.&lt;/li&gt; 
 &lt;li&gt;[Mar, 2025] We're excited to release the &lt;a href="https://huggingface.co/manycore-research/SpatialLM-Llama-1B"&gt;SpatialLM-Llama-1B&lt;/a&gt; and &lt;a href="https://huggingface.co/manycore-research/SpatialLM-Qwen-0.5B"&gt;SpatialLM-Qwen-0.5B&lt;/a&gt; on Hugging Face.&lt;/li&gt; 
 &lt;li&gt;[Mar, 2025] Initial release of SpatialLM!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;SpatialLM is a 3D large language model designed to process 3D point cloud data and generate structured 3D scene understanding outputs. These outputs include architectural elements like walls, doors, windows, and oriented object bounding boxes with their semantic categories. Unlike previous methods that require specialized equipment for data collection, SpatialLM can handle point clouds from diverse sources such as monocular video sequences, RGBD images, and LiDAR sensors. This multimodal architecture effectively bridges the gap between unstructured 3D geometric data and structured 3D representations, offering high-level semantic understanding. It enhances spatial reasoning capabilities for applications in embodied robotics, autonomous navigation, and other complex 3D scene analysis tasks.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;video src="https://github.com/user-attachments/assets/c0218d6a-f676-41f8-ae76-bba228866306" poster="figures/cover.png"&gt; 
 &lt;/video&gt; 
 &lt;p&gt;&lt;i&gt;SpatialLM reconstructs 3D layout from a monocular RGB video with MASt3R-SLAM. Results aligned to video with GT cameras for visualization.&lt;/i&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;SpatialLM Models&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;Download&lt;/strong&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;SpatialLM1.1-Llama-1B&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://huggingface.co/manycore-research/SpatialLM1.1-Llama-1B"&gt;ü§ó HuggingFace&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;SpatialLM1.1-Qwen-0.5B&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://huggingface.co/manycore-research/SpatialLM1.1-Qwen-0.5B"&gt;ü§ó HuggingFace&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;SpatialLM1.0-Llama-1B&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://huggingface.co/manycore-research/SpatialLM-Llama-1B"&gt;ü§ó HuggingFace&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;SpatialLM1.0-Qwen-0.5B&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://huggingface.co/manycore-research/SpatialLM-Qwen-0.5B"&gt;ü§ó HuggingFace&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;p&gt;Tested with the following environment:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.11&lt;/li&gt; 
 &lt;li&gt;Pytorch 2.4.1&lt;/li&gt; 
 &lt;li&gt;CUDA Version 12.4&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# clone the repository
git clone https://github.com/manycore-research/SpatialLM.git
cd SpatialLM

# create a conda environment with cuda 12.4
conda create -n spatiallm python=3.11
conda activate spatiallm
conda install -y -c nvidia/label/cuda-12.4.0 cuda-toolkit conda-forge::sparsehash

# Install dependencies with poetry
pip install poetry &amp;amp;&amp;amp; poetry config virtualenvs.create false --local
poetry install
# SpatialLM1.0 dependency
poe install-torchsparse # Building wheel for torchsparse will take a while
# SpatialLM1.1 dependency
poe install-sonata # Building wheel for flash-attn will take a while
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Inference&lt;/h3&gt; 
&lt;p&gt;In the current version of SpatialLM, input point clouds are considered axis-aligned where the z-axis is the up axis. This orientation is crucial for maintaining consistency in spatial understanding and scene interpretation across different datasets and applications. Example preprocessed point clouds, reconstructed from RGB videos using &lt;a href="https://github.com/rmurai0610/MASt3R-SLAM"&gt;MASt3R-SLAM&lt;/a&gt;, are available in &lt;a href="https://raw.githubusercontent.com/manycore-research/SpatialLM/main/#spatiallm-testset"&gt;SpatialLM-Testset&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Download an example point cloud:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;huggingface-cli download manycore-research/SpatialLM-Testset pcd/scene0000_00.ply --repo-type dataset --local-dir .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run inference:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python inference.py --point_cloud pcd/scene0000_00.ply --output scene0000_00.txt --model_path manycore-research/SpatialLM1.1-Qwen-0.5B
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Detection with user-specified categories&lt;/h3&gt; 
&lt;p&gt;SpatialLM1.1 supports object detection conditioned on user-specified categories by leveraging the flexibility of LLMs.&lt;/p&gt; 
&lt;p&gt;SpatialLM1.1 offers three variants of structured indoor modeling tasks:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Structured Reconstruction&lt;/strong&gt;: Detect walls, doors, windows, boxes.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Layout Estimation&lt;/strong&gt;: Detect walls, doors, windows.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;3D Object Detection&lt;/strong&gt;: Detect boxes.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For tasks that include object box estimation, you can specify a subset of the 59 furniture categories, and the model will only predict objects within those specified categories. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python inference.py --point_cloud pcd/scene0000_00.ply --output scene0000_00.txt --model_path manycore-research/SpatialLM1.1-Qwen-0.5B --detect_type object --category bed nightstand
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Visualization&lt;/h3&gt; 
&lt;p&gt;Use &lt;code&gt;rerun&lt;/code&gt; to visualize the point cloud and the predicted structured 3D layout output:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Convert the predicted layout to Rerun format
python visualize.py --point_cloud pcd/scene0000_00.ply --layout scene0000_00.txt --save scene0000_00.rrd

# Visualize the point cloud and the predicted layout
rerun scene0000_00.rrd
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Evaluation&lt;/h3&gt; 
&lt;p&gt;To evaluate the performance of SpatialLM, we provide &lt;code&gt;eval.py&lt;/code&gt; script that reports the benchmark results on the SpatialLM-Testset in the table below in section &lt;a href="https://raw.githubusercontent.com/manycore-research/SpatialLM/main/#benchmark-results"&gt;Benchmark Results&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Download the testset:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;huggingface-cli download manycore-research/SpatialLM-Testset --repo-type dataset --local-dir SpatialLM-Testset
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run evaluation:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run inference on the PLY point clouds in folder SpatialLM-Testset/pcd with SpatialLM1.1-Qwen-0.5B model
python inference.py --point_cloud SpatialLM-Testset/pcd --output SpatialLM-Testset/pred --model_path manycore-research/SpatialLM1.1-Qwen-0.5B

# Evaluate the predicted layouts
python eval.py --metadata SpatialLM-Testset/test.csv --gt_dir SpatialLM-Testset/layout --pred_dir SpatialLM-Testset/pred --label_mapping SpatialLM-Testset/benchmark_categories.tsv
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Example using a custom video&lt;/h3&gt; 
&lt;p&gt;We provide an example of how to use our model to estimate scene layout starting from a RGB video with the newly released &lt;a href="https://github.com/PKU-VCL-3DV/SLAM3R"&gt;SLAM3R&lt;/a&gt; in &lt;a href="https://raw.githubusercontent.com/manycore-research/SpatialLM/main/EXAMPLE.md"&gt;EXAMPLE.md&lt;/a&gt;. These steps work for MASt3R-SLAM, and other reconstruction methods as well.&lt;/p&gt; 
&lt;h2&gt;SpatialLM Testset&lt;/h2&gt; 
&lt;p&gt;We provide a test set of 107 preprocessed point clouds, reconstructed from RGB videos using &lt;a href="https://github.com/rmurai0610/MASt3R-SLAM"&gt;MASt3R-SLAM&lt;/a&gt;. SpatialLM-Testset is quite challenging compared to prior clean RGBD scans datasets due to the noises and occlusions in the point clouds reconstructed from monocular RGB videos.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;Dataset&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;Download&lt;/strong&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;SpatialLM-Testset&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://huggingface.co/datasets/manycore-research/SpatialLM-TestSet"&gt;ü§ó Datasets&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;Benchmark Results&lt;/h2&gt; 
&lt;h3&gt;Layout Estimation&lt;/h3&gt; 
&lt;p&gt;Layout estimation focuses on predicting architectural elements, i.e., walls, doors, and windows, within an indoor scene. We evaluated this task on the &lt;a href="https://structured3d-dataset.org"&gt;Structured3D&lt;/a&gt; dataset. For &lt;a href="https://github.com/ywyue/RoomFormer"&gt;RoomFormer&lt;/a&gt;, we directly downloaded the model checkpoint. SceneScript and SpatialLM were first trained on our dataset, and further fine-tuned on Structured3D.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;Method&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;RoomFormer&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;SceneScript (finetuned)&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;SpatialLM1.1-Qwen-0.5B (finetuned)&lt;/strong&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;&lt;strong&gt;F1 @.25 IoU&lt;/strong&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;70.4&lt;/td&gt; 
    &lt;td align="center"&gt;83.1&lt;/td&gt; 
    &lt;td align="center"&gt;86.5&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;&lt;strong&gt;F1 @.5 IoU&lt;/strong&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;67.2&lt;/td&gt; 
    &lt;td align="center"&gt;80.8&lt;/td&gt; 
    &lt;td align="center"&gt;84.6&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h3&gt;3D Object Detection&lt;/h3&gt; 
&lt;p&gt;We evaluate 3D object detection on &lt;a href="http://www.scan-net.org"&gt;ScanNet&lt;/a&gt; with annotations of 18 object categories. For &lt;a href="https://github.com/V-DETR/V-DETR"&gt;V-DETR&lt;/a&gt;, we directly download the model checkpoint. SceneScript and SpatialLM were first trained on our dataset, and further fine-tuned on ScanNet.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;Method&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;V-DETR&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;SceneScript (finetuned)&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;SpatialLM1.1-Qwen-0.5B (finetuned)&lt;/strong&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;&lt;strong&gt;F1 @.25 IoU&lt;/strong&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;65.1&lt;/td&gt; 
    &lt;td align="center"&gt;49.1&lt;/td&gt; 
    &lt;td align="center"&gt;65.6&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;&lt;strong&gt;F1 @.5 IoU&lt;/strong&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;56.8&lt;/td&gt; 
    &lt;td align="center"&gt;36.8&lt;/td&gt; 
    &lt;td align="center"&gt;52.6&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h3&gt;Zero-shot Detection on Videos&lt;/h3&gt; 
&lt;p&gt;Zero-shot detection results on the challenging SpatialLM-Testset are reported in the following table:&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;Method&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;SpatialLM1.1-Llama-1B&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;SpatialLM1.1-Qwen-0.5B&lt;/strong&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;&lt;strong&gt;Layout&lt;/strong&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;strong&gt;F1 @.25 IoU (2D)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;strong&gt;F1 @.25 IoU (2D)&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;wall&lt;/td&gt; 
    &lt;td align="center"&gt;68.9&lt;/td&gt; 
    &lt;td align="center"&gt;68.2&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;door&lt;/td&gt; 
    &lt;td align="center"&gt;46.3&lt;/td&gt; 
    &lt;td align="center"&gt;43.1&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;window&lt;/td&gt; 
    &lt;td align="center"&gt;43.8&lt;/td&gt; 
    &lt;td align="center"&gt;47.4&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;&lt;strong&gt;Objects&lt;/strong&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;strong&gt;F1 @.25 IoU (3D)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;strong&gt;F1 @.25 IoU (2D)&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;curtain&lt;/td&gt; 
    &lt;td align="center"&gt;34.9&lt;/td&gt; 
    &lt;td align="center"&gt;37.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;nightstand&lt;/td&gt; 
    &lt;td align="center"&gt;62.8&lt;/td&gt; 
    &lt;td align="center"&gt;67.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;chandelier&lt;/td&gt; 
    &lt;td align="center"&gt;53.5&lt;/td&gt; 
    &lt;td align="center"&gt;36.8&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;wardrobe&lt;/td&gt; 
    &lt;td align="center"&gt;29.4&lt;/td&gt; 
    &lt;td align="center"&gt;39.6&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;bed&lt;/td&gt; 
    &lt;td align="center"&gt;96.8&lt;/td&gt; 
    &lt;td align="center"&gt;95.2&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;sofa&lt;/td&gt; 
    &lt;td align="center"&gt;66.9&lt;/td&gt; 
    &lt;td align="center"&gt;69.1&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;chair&lt;/td&gt; 
    &lt;td align="center"&gt;20.8&lt;/td&gt; 
    &lt;td align="center"&gt;32.3&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;cabinet&lt;/td&gt; 
    &lt;td align="center"&gt;15.2&lt;/td&gt; 
    &lt;td align="center"&gt;11.2&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;dining table&lt;/td&gt; 
    &lt;td align="center"&gt;40.7&lt;/td&gt; 
    &lt;td align="center"&gt;24.2&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;plants&lt;/td&gt; 
    &lt;td align="center"&gt;29.5&lt;/td&gt; 
    &lt;td align="center"&gt;26.3&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;tv cabinet&lt;/td&gt; 
    &lt;td align="center"&gt;34.4&lt;/td&gt; 
    &lt;td align="center"&gt;27.3&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;coffee table&lt;/td&gt; 
    &lt;td align="center"&gt;56.4&lt;/td&gt; 
    &lt;td align="center"&gt;64.9&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;side table&lt;/td&gt; 
    &lt;td align="center"&gt;14.6&lt;/td&gt; 
    &lt;td align="center"&gt;9.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;air conditioner&lt;/td&gt; 
    &lt;td align="center"&gt;16.7&lt;/td&gt; 
    &lt;td align="center"&gt;24.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;dresser&lt;/td&gt; 
    &lt;td align="center"&gt;46.7&lt;/td&gt; 
    &lt;td align="center"&gt;46.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;stool&lt;/td&gt; 
    &lt;td align="center"&gt;17.6&lt;/td&gt; 
    &lt;td align="center"&gt;30.8&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;refrigerator&lt;/td&gt; 
    &lt;td align="center"&gt;0.0&lt;/td&gt; 
    &lt;td align="center"&gt;16.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;painting&lt;/td&gt; 
    &lt;td align="center"&gt;34.9&lt;/td&gt; 
    &lt;td align="center"&gt;38.2&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;carpet&lt;/td&gt; 
    &lt;td align="center"&gt;40.3&lt;/td&gt; 
    &lt;td align="center"&gt;24.1&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;tv&lt;/td&gt; 
    &lt;td align="center"&gt;16.0&lt;/td&gt; 
    &lt;td align="center"&gt;18.0&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h3&gt;Result Visualizations&lt;/h3&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;Layout Estimation&lt;/th&gt; 
    &lt;th align="center"&gt;Object Detection&lt;/th&gt; 
    &lt;th align="center"&gt;Zero-shot Reconstruction&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/manycore-research/SpatialLM/main/figures/stru3d.jpg" alt="Structured3D" /&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/manycore-research/SpatialLM/main/figures/scannet.jpg" alt="ScanNet" /&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/manycore-research/SpatialLM/main/figures/zeroshot.jpg" alt="Zero-shot" /&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;&lt;a href="https://manycore-research-azure.kujiale.com/manycore-research/SpatialLM/supplementary/visualization_layout.html"&gt;Structured3D Results&lt;/a&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://manycore-research-azure.kujiale.com/manycore-research/SpatialLM/supplementary/visualization_object.html"&gt;ScanNet Results&lt;/a&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://manycore-research-azure.kujiale.com/manycore-research/SpatialLM/supplementary/visualization_zeroshot.html"&gt;Zeroshot Results&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;SpatialLM-Llama-1B is derived from Llama3.2-1B-Instruct, which is licensed under the Llama3.2 license. SpatialLM-Qwen-0.5B is derived from the Qwen-2.5 series, originally licensed under the Apache 2.0 License.&lt;/p&gt; 
&lt;p&gt;SpatialLM1.0 are built upon the SceneScript point cloud encoder, licensed under the CC-BY-NC-4.0 License. TorchSparse, utilized in this project, is licensed under the MIT License.&lt;/p&gt; 
&lt;p&gt;SpatialLM1.1 are built upon Sonata point cloud encoder, model weight is licensed under the CC-BY-NC-4.0 License. Code built on Pointcept is licensed under the Apache 2.0 License.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you find this work useful, please consider citing:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@article{SpatialLM,
    title         = {SpatialLM: Training Large Language Models for Structured Indoor Modeling},
    author        = {Mao, Yongsen and Zhong, Junhao and Fang, Chuan and Zheng, Jia and Tang, Rui and Zhu, Hao and Tan, Ping and Zhou, Zihan},
    journal       = {arXiv preprint},
    year          = {2025},
    eprint        = {2506.07491},
    archivePrefix = {arXiv},
    primaryClass  = {cs.CV}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;p&gt;We would like to thank the following projects that made this work possible:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/meta-llama"&gt;Llama3.2&lt;/a&gt; | &lt;a href="https://github.com/QwenLM/Qwen2.5"&gt;Qwen2.5&lt;/a&gt; | &lt;a href="https://github.com/huggingface/transformers"&gt;Transformers&lt;/a&gt; | &lt;a href="https://github.com/facebookresearch/scenescript"&gt;SceneScript&lt;/a&gt; | &lt;a href="https://github.com/mit-han-lab/torchsparse"&gt;TorchSparse&lt;/a&gt; | &lt;a href="https://xywu.me/sonata/"&gt;Sonata&lt;/a&gt; | &lt;a href="https://github.com/Pointcept/Pointcept"&gt;Pointcept&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>oop7/YTSage</title>
      <link>https://github.com/oop7/YTSage</link>
      <description>&lt;p&gt;Modern YouTube downloader with a clean PySide6 interface. Download videos in any quality, extract audio, fetch subtitles, sponserBlock, and view video metadata. Built with yt-dlp for reliable performance.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h1&gt;üé• YTSage&lt;/h1&gt; 
 &lt;img src="https://github.com/user-attachments/assets/f95f7bfb-8591-4d32-b795-68e61efd670c" width="800" alt="YTSage Interface" /&gt; 
 &lt;p&gt;&lt;a href="https://badge.fury.io/py/ytsage"&gt;&lt;img src="https://img.shields.io/pypi/v/ytsage?color=dc2626&amp;amp;style=for-the-badge&amp;amp;logo=pypi&amp;amp;logoColor=white" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://opensource.org/licenses/MIT"&gt;&lt;img src="https://img.shields.io/badge/License-MIT-374151?style=for-the-badge&amp;amp;logo=opensource&amp;amp;logoColor=white" alt="License: MIT" /&gt;&lt;/a&gt; &lt;a href="https://www.python.org/downloads/"&gt;&lt;img src="https://img.shields.io/badge/python-3.7+-1f2937?style=for-the-badge&amp;amp;logo=python&amp;amp;logoColor=white" alt="Python 3.7+" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/ytsage"&gt;&lt;img src="https://img.shields.io/pepy/dt/ytsage?color=4b5563&amp;amp;style=for-the-badge&amp;amp;label=downloads&amp;amp;logo=download&amp;amp;logoColor=white" alt="Downloads" /&gt;&lt;/a&gt; &lt;a href="https://github.com/oop7/YTSage/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/oop7/YTSage?color=dc2626&amp;amp;style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white" alt="GitHub Stars" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;A modern YouTube downloader with a clean PySide6 interface.&lt;/strong&gt;&lt;br /&gt; Download videos in any quality, extract audio, fetch subtitles, and more.&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;a href="https://raw.githubusercontent.com/oop7/YTSage/main/#installation"&gt;Installation&lt;/a&gt; ‚Ä¢ &lt;a href="https://raw.githubusercontent.com/oop7/YTSage/main/#features"&gt;Features&lt;/a&gt; ‚Ä¢ &lt;a href="https://raw.githubusercontent.com/oop7/YTSage/main/#usage"&gt;Usage&lt;/a&gt; ‚Ä¢ &lt;a href="https://raw.githubusercontent.com/oop7/YTSage/main/#screenshots"&gt;Screenshots&lt;/a&gt; ‚Ä¢ &lt;a href="https://raw.githubusercontent.com/oop7/YTSage/main/#troubleshooting"&gt;Troubleshooting&lt;/a&gt; ‚Ä¢ &lt;a href="https://raw.githubusercontent.com/oop7/YTSage/main/#contributing"&gt;Contributing&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;a id="why-ytsage"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;‚ùì Why YTSage?&lt;/h2&gt; 
&lt;p&gt;YTSage is designed for users who want a &lt;strong&gt;simple yet powerful YouTube downloader&lt;/strong&gt;. Unlike other tools, it offers:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;A clean, modern PySide6 interface&lt;/li&gt; 
 &lt;li&gt;One-click downloads for video, audio, and subtitles&lt;/li&gt; 
 &lt;li&gt;Advanced features like SponsorBlock, subtitle merging, and playlist selection&lt;/li&gt; 
 &lt;li&gt;Cross-platform support and easy installation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a id="features"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;‚ú® Features&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Core Features&lt;/th&gt; 
    &lt;th&gt;Advanced Features&lt;/th&gt; 
    &lt;th&gt;Extra Features&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;üé• Format Table&lt;/td&gt; 
    &lt;td&gt;üö´ SponsorBlock Integration&lt;/td&gt; 
    &lt;td&gt;üíæ Save Download Path&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;üéµ Audio Extraction&lt;/td&gt; 
    &lt;td&gt;üìù Multi-Subtitle Select &amp;amp; Merge&lt;/td&gt; 
    &lt;td&gt;üîÑ Auto-Update yt-dlp&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;‚ú® Simple UI&lt;/td&gt; 
    &lt;td&gt;üíæ Save Description&lt;/td&gt; 
    &lt;td&gt;üõ†Ô∏è FFmpeg/yt-dlp Detection&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;üìã Playlist Support&lt;/td&gt; 
    &lt;td&gt;üñºÔ∏è Save thumbnail&lt;/td&gt; 
    &lt;td&gt;‚öôÔ∏è Custom Commands&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;üñºÔ∏è Playlist Selector&lt;/td&gt; 
    &lt;td&gt;üöÄ Speed Limiter&lt;/td&gt; 
    &lt;td&gt;üç™ Login with Cookies&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;üìë Embed Chapters&lt;/td&gt; 
    &lt;td&gt;‚úÇÔ∏è Trim Video Sections&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;a id="installation"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üöÄ Installation&lt;/h2&gt; 
&lt;h3&gt;‚ö° Quick Install (Recommended)&lt;/h3&gt; 
&lt;p&gt;Install YTSage from PyPI:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install ytsage
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then launch the app:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;ytsage
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;üì¶ Pre-built Executables&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ü™ü &lt;strong&gt;Windows:&lt;/strong&gt; &lt;code style="background-color: #333842; color: #C9D1D9; padding: 3px 6px; border-radius: 6px; font-family: monospace;"&gt;YTSage-v&amp;lt;version&amp;gt;.exe&lt;/code&gt; / &lt;code style="background-color: #333842; color: #C9D1D9; padding: 3px 6px; border-radius: 6px; font-family: monospace;"&gt;YTSage-v&amp;lt;version&amp;gt;-ffmpeg.exe&lt;/code&gt; (with FFmpeg)&lt;/li&gt; 
 &lt;li&gt;üêß &lt;strong&gt;Linux:&lt;/strong&gt; &lt;code style="background-color: #333842; color: #C9D1D9; padding: 3px 6px; border-radius: 6px; font-family: monospace;"&gt;YTSage-v&amp;lt;version&amp;gt;-amd64.deb&lt;/code&gt; / &lt;code style="background-color: #333842; color: #C9D1D9; padding: 3px 6px; border-radius: 6px; font-family: monospace;"&gt;YTSage-v&amp;lt;version&amp;gt;-x86_64.AppImage&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;üçé &lt;strong&gt;macOS:&lt;/strong&gt; &lt;code style="background-color: #333842; color: #C9D1D9; padding: 3px 6px; border-radius: 6px; font-family: monospace;"&gt;YTSage-v&amp;lt;version&amp;gt;-macOS.zip&lt;/code&gt; / &lt;code style="background-color: #333842; color: #C9D1D9; padding: 3px 6px; border-radius: 6px; font-family: monospace;"&gt;YTSage-v&amp;lt;version&amp;gt;.dmg&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;a href="https://github.com/oop7/YTSage/releases/latest"&gt;üëâ Download Latest Release&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;details&gt; 
 &lt;summary&gt;üõ†Ô∏è Manual Installation from Source&lt;/summary&gt; 
 &lt;h3&gt;1. Clone the Repository&lt;/h3&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/oop7/YTSage.git
cd YTSage
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;2. Install Dependencies&lt;/h3&gt; 
 &lt;h4&gt;‚ö° With uv&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;uv pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;üì¶ Or with standard pip&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;3. Run the Application&lt;/h3&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python main.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p&gt;&lt;a id="screenshots"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üì∏ Screenshots&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/f95f7bfb-8591-4d32-b795-68e61efd670c" alt="Main Interface" width="400" /&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/f7b3ebab-3054-4c77-8109-c899a8b10047" alt="Playlist Download" width="400" /&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;&lt;em&gt;Main Interface&lt;/em&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;em&gt;Playlist Download&lt;/em&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/a80d2ae2-0031-4ed0-bee4-93293634c62a" alt="Audio Format Selection with Save Thumbnail" width="400" /&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/5236e3cc-8a8d-4d85-a660-782a740ef9af" alt="Subtitle Options merged with Remove Sponsor Segments" width="400" /&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;&lt;em&gt;Audio Format&lt;/em&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;em&gt;Subtitle Options&lt;/em&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;a id="usage"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üìñ Usage&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;üéØ Basic Usage&lt;/summary&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;strong&gt;Launch YTSage&lt;/strong&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Paste YouTube URL&lt;/strong&gt; (or use "Paste URL" button)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Click "Analyze"&lt;/strong&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Select Format:&lt;/strong&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;code&gt;Video&lt;/code&gt; for video downloads&lt;/li&gt; 
    &lt;li&gt;&lt;code&gt;Audio Only&lt;/code&gt; for audio extraction&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Choose Options:&lt;/strong&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Enable subtitles &amp;amp; select language&lt;/li&gt; 
    &lt;li&gt;Enable subtitle merge&lt;/li&gt; 
    &lt;li&gt;Save thumbnail&lt;/li&gt; 
    &lt;li&gt;Remove sponsor segments&lt;/li&gt; 
    &lt;li&gt;Save description&lt;/li&gt; 
    &lt;li&gt;Embed chapters&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Select Output Directory&lt;/strong&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Click "Download"&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;üìã Playlist Download&lt;/summary&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;strong&gt;Paste Playlist URL&lt;/strong&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Click "Analyze"&lt;/strong&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Select videos from the playlist selector (optional, defaults to all)&lt;/strong&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Choose desired format/quality&lt;/strong&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Click "Download"&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;üí° The application automatically handles the download queue&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;üß∞ Advanced Options&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Quality Selection:&lt;/strong&gt; Choose the highest resolution for best quality&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Subtitle Options:&lt;/strong&gt; Filter languages and embed into video&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Custom Commands:&lt;/strong&gt; Access advanced yt-dlp features&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Save Description:&lt;/strong&gt; Save the description of the video&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Save Thumbnail:&lt;/strong&gt; Save the thumbnail of the video&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Embed Chapters:&lt;/strong&gt; Embed chapter markers as metadata in the downloaded video file for compatible video players&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Remove Sponsor Segments:&lt;/strong&gt; Remove sponsor segments from the video&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Speed Limiter:&lt;/strong&gt; Limit the download speed&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Login with Cookies:&lt;/strong&gt; Login to YouTube using cookies to access private content&lt;br /&gt; How to use it: 
   &lt;ol&gt; 
    &lt;li&gt;Extract cookies from your browser using an extension like &lt;a href="https://github.com/moustachauve/cookie-editor?tab=readme-ov-file"&gt;cookie-editor&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;Copy the cookies in Netscape format&lt;/li&gt; 
    &lt;li&gt;Create a file named &lt;code&gt;cookies.txt&lt;/code&gt; and paste the cookies into it&lt;/li&gt; 
    &lt;li&gt;Select the &lt;code&gt;cookies.txt&lt;/code&gt; file in the app&lt;/li&gt; 
   &lt;/ol&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Save Download Path:&lt;/strong&gt; Save the download path&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Update yt-dlp:&lt;/strong&gt; Update yt-dlp&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;FFmpeg/yt-dlp Detection:&lt;/strong&gt; Automatically detect FFmpeg/yt-dlp&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Trim Video:&lt;/strong&gt; Download only specific parts of a video by specifying time ranges (HH:MM:SS format)&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;p&gt;&lt;a id="troubleshooting"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üõ†Ô∏è Troubleshooting&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click to view common issues and solutions&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Format table not displaying:&lt;/strong&gt; Update yt-dlp to the latest version.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Download fails:&lt;/strong&gt; Check your internet connection and ensure the video is available.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Separate video and audio files after download:&lt;/strong&gt; This happens when FFmpeg is missing or not detected. YTSage requires FFmpeg to merge high-quality video and audio streams. 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Solution:&lt;/strong&gt; Ensure FFmpeg is installed and accessible in your system's PATH. For Windows users, the easiest option is to download the &lt;code&gt;YTSage-v&amp;lt;version&amp;gt;-ffmpeg.exe&lt;/code&gt; file, which comes bundled with FFmpeg.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;hr /&gt; 
 &lt;h4&gt;üõ°Ô∏è Windows Defender / Antivirus Warning&lt;/h4&gt; 
 &lt;p&gt;Some antivirus software may flag the &lt;code&gt;.exe&lt;/code&gt; files as false positives. This is a &lt;strong&gt;known limitation&lt;/strong&gt; of PyInstaller-packaged applications.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Why this happens:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;PyInstaller bundles Python runtime and libraries together&lt;/li&gt; 
  &lt;li&gt;Antivirus heuristics can misidentify packed executables as suspicious&lt;/li&gt; 
  &lt;li&gt;This affects many legitimate Python applications built with PyInstaller&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;Safe alternatives:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;‚úÖ &lt;strong&gt;Use pip installation:&lt;/strong&gt; &lt;code&gt;pip install ytsage&lt;/code&gt; (recommended)&lt;/li&gt; 
  &lt;li&gt;‚úÖ &lt;strong&gt;Build from source&lt;/strong&gt;&lt;/li&gt; 
  &lt;li&gt;‚úÖ &lt;strong&gt;Whitelist the application&lt;/strong&gt; in your antivirus software&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;üìã &lt;strong&gt;Related Issues:&lt;/strong&gt; &lt;a href="https://github.com/oop7/YTSage/issues/33"&gt;#33&lt;/a&gt; - This is a known PyInstaller limitation, not a security issue with YTSage itself.&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;hr /&gt; 
 &lt;h4&gt;üçé macOS: "App is damaged and can‚Äôt be opened"&lt;/h4&gt; 
 &lt;p&gt;If you see this error on macOS Sonoma or newer, you need to remove the quarantine attribute.&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;strong&gt;Open Terminal&lt;/strong&gt; (you can find it using Spotlight).&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Type the following command&lt;/strong&gt; but &lt;strong&gt;do not&lt;/strong&gt; press Enter yet. Make sure to include the space at the end: &lt;pre&gt;&lt;code class="language-bash"&gt;xattr -d com.apple.quarantine 
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Drag the &lt;code&gt;YTSage.app&lt;/code&gt; file&lt;/strong&gt; from your Finder window and drop it directly into the Terminal window. This will automatically paste the correct file path.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Press Enter&lt;/strong&gt; to run the command.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Try opening YTSage.app again.&lt;/strong&gt; It should now launch correctly.&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;hr /&gt; 
 &lt;h4&gt;&lt;strong&gt;Configuration Locations (Advanced)&lt;/strong&gt;&lt;/h4&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Windows:&lt;/strong&gt; &lt;code&gt;%LOCALAPPDATA%\YTSage&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;macOS:&lt;/strong&gt; &lt;code&gt;~/Library/Application Support/YTSage&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Linux:&lt;/strong&gt; &lt;code&gt;~/.local/share/YTSage&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;p&gt;&lt;a id="contributing"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üë• Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions! Here's how you can help:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;üç¥ Fork the repository&lt;/li&gt; 
 &lt;li&gt;üåø Create your feature branch:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git checkout -b feature/AmazingFeature
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;üíæ Commit your changes:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git commit -m 'Add some AmazingFeature'
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="4"&gt; 
 &lt;li&gt;üì§ Push to the branch:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git push origin feature/AmazingFeature
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="5"&gt; 
 &lt;li&gt;üîÑ Open a Pull Request&lt;/li&gt; 
&lt;/ol&gt; 
&lt;details&gt; 
 &lt;summary&gt;üìÇ Project Structure&lt;/summary&gt; 
 &lt;h2&gt;YTSage - Project Structure&lt;/h2&gt; 
 &lt;p&gt;This document describes the organized folder structure of YTSage.&lt;/p&gt; 
 &lt;h3&gt;üìÅ Project Structure&lt;/h3&gt; 
 &lt;pre&gt;&lt;code&gt;YTSage-main/
‚îú‚îÄ‚îÄ üìÅ assets/                    # Static assets and resources
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ Icon/                  # Application icons
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ icon.png
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ sound/                 # Audio files
‚îÇ       ‚îî‚îÄ‚îÄ notification.mp3
‚îÇ
‚îú‚îÄ‚îÄ üìÅ src/                       # Source code
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ core/                  # Core business logic
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py           # Core package init
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ytsage_downloader.py  # Download functionality
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ytsage_ffmpeg.py      # FFmpeg integration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ytsage_style.py       # UI styling
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ytsage_utils.py       # Utility functions
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ytsage_yt_dlp.py      # yt-dlp integration
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ gui/                   # User interface components
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ dialogs/           # Dialog classes
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py       # Dialogs package init (re-exports all)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ytsage_dialogs_base.py     # Basic dialogs (Log, About)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ytsage_dialogs_custom.py   # Custom functionality dialogs
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ytsage_dialogs_ffmpeg.py   # FFmpeg-related dialogs
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ytsage_dialogs_selection.py # Selection dialogs
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ytsage_dialogs_settings.py  # Settings dialogs
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ytsage_dialogs_update.py    # Update dialogs
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py           # GUI package init
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ytsage_gui_dialogs.py # Dialog aggregator (backward compatibility)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ytsage_gui_format_table.py # Format table functionality
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ytsage_gui_main.py    # Main application window
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ytsage_gui_video_info.py # Video information display
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py               # Main package init
‚îÇ
‚îú‚îÄ‚îÄ üìÑ main.py                    # Application entry point
‚îú‚îÄ‚îÄ üìÑ requirements.txt           # Python dependencies
‚îú‚îÄ‚îÄ üìÑ README.md                  # Project documentation
‚îú‚îÄ‚îÄ üìÑ LICENSE                    # License file
‚îî‚îÄ‚îÄ üìÑ .gitignore                 # Git ignore rules
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;‚≠êÔ∏è Star History&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://next.ossinsight.io/widgets/official/analyze-repo-stars-history?repo_id=896163475" target="_blank" style="display: block" align="center"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="https://next.ossinsight.io/widgets/official/analyze-repo-stars-history/thumbnail.png?repo_id=896163475&amp;amp;image_size=auto&amp;amp;color_scheme=dark" width="721" height="auto" /&gt; 
   &lt;img alt="Star History of oop7/YTSage" src="https://next.ossinsight.io/widgets/official/analyze-repo-stars-history/thumbnail.png?repo_id=896163475&amp;amp;image_size=auto&amp;amp;color_scheme=light" width="721" height="auto" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;üìú License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the MIT License - see the &lt;a href="https://raw.githubusercontent.com/oop7/YTSage/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h2&gt;üôè Acknowledgments&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;Show Acknowledgments&lt;/summary&gt; 
 &lt;div align="center"&gt; 
  &lt;p&gt;A heartfelt thank you to everyone who has contributed to this project by opening an issue to suggest an improvement or report a bug.&lt;/p&gt; 
  &lt;table&gt; 
   &lt;tbody&gt;
    &lt;tr class="section"&gt;
     &lt;th colspan="2"&gt;Core Components&lt;/th&gt;
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td width="35%"&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp"&gt;yt-dlp&lt;/a&gt;&lt;/td&gt; 
     &lt;td&gt;Download Engine&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;a href="https://ffmpeg.org/"&gt;FFmpeg&lt;/a&gt;&lt;/td&gt; 
     &lt;td&gt;Media Processing&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr class="section"&gt;
     &lt;th colspan="2"&gt;Libraries &amp;amp; Frameworks&lt;/th&gt;
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;a href="https://wiki.qt.io/Qt_for_Python"&gt;PySide6&lt;/a&gt;&lt;/td&gt; 
     &lt;td&gt;GUI Framework&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;a href="https://python-pillow.org/"&gt;Pillow&lt;/a&gt;&lt;/td&gt; 
     &lt;td&gt;Image Processing&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;a href="https://requests.readthedocs.io/"&gt;requests&lt;/a&gt;&lt;/td&gt; 
     &lt;td&gt;HTTP Requests&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;a href="https://packaging.python.org/"&gt;packaging&lt;/a&gt;&lt;/td&gt; 
     &lt;td&gt;Version &amp;amp; Package Handling&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;a href="https://python-markdown.github.io/"&gt;markdown&lt;/a&gt;&lt;/td&gt; 
     &lt;td&gt;Markdown Rendering&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;a href="https://www.pygame.org/"&gt;pygame&lt;/a&gt;&lt;/td&gt; 
     &lt;td&gt;Audio Playback&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;a href="https://github.com/Delgan/loguru"&gt;loguru&lt;/a&gt;&lt;/td&gt; 
     &lt;td&gt;Logging&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr class="section"&gt;
     &lt;th colspan="2"&gt;Assets &amp;amp; Contributors&lt;/th&gt;
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;a href="https://pixabay.com/sound-effects/new-notification-09-352705/"&gt;New Notification 09 by Universfield&lt;/a&gt;&lt;/td&gt; 
     &lt;td&gt;Notification Sound&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;a href="https://github.com/viru185"&gt;viru185&lt;/a&gt;&lt;/td&gt; 
     &lt;td&gt;Code Contributor&lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt;
  &lt;/table&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;h2&gt;‚ö†Ô∏è Disclaimer&lt;/h2&gt; 
&lt;p&gt;This tool is for personal use only. Please respect YouTube's terms of service and content creators' rights.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;Made with ‚ù§Ô∏è by &lt;a href="https://github.com/oop7"&gt;oop7&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>LeCAR-Lab/ASAP</title>
      <link>https://github.com/LeCAR-Lab/ASAP</link>
      <description>&lt;p&gt;[RSS 2025] "ASAP: Aligning Simulation and Real-World Physics for Learning Agile Humanoid Whole-Body Skills"&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt; ASAP: Aligning Simulation and Real-World Physics for &lt;p&gt;Learning Agile Humanoid Whole-Body Skills &lt;/p&gt;&lt;/h1&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;Robotics: Science and Systems (RSS) 2025&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://agile.human2humanoid.com/"&gt;[Website]&lt;/a&gt; &lt;a href="https://arxiv.org/pdf/2502.01143"&gt;[Arxiv]&lt;/a&gt; &lt;a href="https://www.youtube.com/watch?v=tu7LSNYWDTs&amp;amp;ab_channel=LeCARLabatCMU"&gt;[Video]&lt;/a&gt;&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/imgs/CMU-NV-logo-crop-png.png" height="50&amp;quot;" /&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://developer.nvidia.com/isaac-gym"&gt;&lt;img src="https://img.shields.io/badge/IsaacGym-Preview4-b.svg?sanitize=true" alt="IsaacGym" /&gt;&lt;/a&gt; &lt;a href="https://docs.isaacsim.omniverse.nvidia.com/4.2.0/index.html"&gt;&lt;img src="https://img.shields.io/badge/IsaacSim-4.2.0-b.svg?sanitize=true" alt="IsaacSim" /&gt;&lt;/a&gt; &lt;a href="https://docs.isaacsim.omniverse.nvidia.com/4.2.0/index.html"&gt;&lt;img src="https://img.shields.io/badge/Genesis-0.2.1-b.svg?sanitize=true" alt="IsaacSim" /&gt;&lt;/a&gt; &lt;a href="https://ubuntu.com/blog/tag/22-04-lts"&gt;&lt;img src="https://img.shields.io/badge/Platform-linux--64-orange.svg?sanitize=true" alt="Linux platform" /&gt;&lt;/a&gt; &lt;a href=""&gt;&lt;img src="https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true" alt="License: MIT" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;img src="https://agile.human2humanoid.com/static/images/asap-preview-gif-480P.gif" width="400px" /&gt; 
&lt;/div&gt; 
&lt;!-- # Table of Contents --&gt; 
&lt;h2&gt;üìö Table of Contents&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#overview"&gt;Overview&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Links: &lt;a href="https://agile.human2humanoid.com/"&gt;Website&lt;/a&gt; ‚Ä¢ &lt;a href="https://arxiv.org/pdf/2502.01143"&gt;Arxiv&lt;/a&gt; ‚Ä¢ &lt;a href="https://www.youtube.com/watch?v=tu7LSNYWDTs&amp;amp;ab_channel=LeCARLabatCMU"&gt;Video&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#installation"&gt;Installation &amp;amp; Setup&lt;/a&gt;&lt;/strong&gt;&lt;br /&gt; 2.1 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#isaacgym-conda-env"&gt;Base Frameworks&lt;/a&gt;&lt;br /&gt; 2.2 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#install-isaacgym"&gt;IsaacGym Setup&lt;/a&gt;&lt;br /&gt; 2.3 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#install-humanoidverse"&gt;HumanoidVerse Setup&lt;/a&gt;&lt;br /&gt; 2.4 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#isaaclab-environment"&gt;IsaacSim + IsaacLab Setup&lt;/a&gt;&lt;br /&gt; 2.5 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#genesis-environment"&gt;Genesis Environment Setup&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#motion-tracking-training"&gt;Training Pipelines&lt;/a&gt;&lt;/strong&gt;&lt;br /&gt; 3.1 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#motion-tracking-training"&gt;Phase-Based Motion Tracking&lt;/a&gt;&lt;br /&gt; 3.2 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#asap-delta-action-model-training"&gt;ASAP Delta Action Model&lt;/a&gt;&lt;br /&gt; - &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#train-delta-action-model"&gt;Train Delta Action Model&lt;/a&gt;&lt;br /&gt; - &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#use-delta-action-model-for-policy-finetuning"&gt;Finetune Policy with Delta Action Model&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#motion-retargeting-to-any-humanoid"&gt;Motion Retargeting to Any Humanoid&lt;/a&gt;&lt;/strong&gt;&lt;br /&gt; 4.1 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#1-smpl-shape-preparation"&gt;Step 1: SMPL Shape Preparation&lt;/a&gt;&lt;br /&gt; 4.2 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#2-smpl-motion-preparation-amass"&gt;Step 2: SMPL Motion Preparation (AMASS)&lt;/a&gt;&lt;br /&gt; 4.3 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#3-robot-xml-and-motion-config-preparation"&gt;Step 3: Robot XML &amp;amp; Motion Config&lt;/a&gt;&lt;br /&gt; 4.4 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#4-humanoid-smpl-shape-fitting"&gt;Step 4: Humanoid-SMPL Shape Fitting&lt;/a&gt;&lt;br /&gt; 4.5 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#5-humanoid-smpl-motion-retargeting"&gt;Step 5: Humanoid-SMPL Motion Retargeting&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#sim2simsim2real"&gt;Deployment: Sim2Sim &amp;amp; Sim2Real&lt;/a&gt;&lt;/strong&gt;&lt;br /&gt; 5.1 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#environment-setup"&gt;Environment Setup&lt;/a&gt;&lt;br /&gt; 5.2 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#sim2sim"&gt;Sim2Sim Deployment&lt;/a&gt;&lt;br /&gt; 5.3 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#sim2real"&gt;Sim2Real Deployment&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#citation"&gt;Citation&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#license"&gt;License&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;TODO&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Release code backbone&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Release phase-based motion tracking training pipeline&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Release ASAP motion datasets&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Release motion retargeting pipeline&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Release sim2sim in MuJoCo&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Release sim2real with UnitreeSDK&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Release ASAP delta action model training pipeline&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Installation&lt;/h1&gt; 
&lt;p&gt;ASAP codebase is built on top of &lt;a href="https://github.com/LeCAR-Lab/HumanoidVerse"&gt;HumanoidVerse&lt;/a&gt; (a multi-simulator framework for humanoid learning) and &lt;a href="https://github.com/LeCAR-Lab/human2humanoid"&gt;Human2Humanoid&lt;/a&gt; (our prior work on humanoid whole-body tracking).&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/LeCAR-Lab/HumanoidVerse"&gt;HumanoidVerse&lt;/a&gt; allows you to train humanoid skills in multiple simulators, including IsaacGym, IsaacSim, and Genesis. Its key design logic is the separation and modularization of simulators, tasks, and algorithms, which enables smooth transfers between different simulators and the real world with minimum effort (just one line of code change). We leverage this framework to develop &lt;a href="https://agile.human2humanoid.com/"&gt;ASAP&lt;/a&gt; and study how to best transfer policies across simulators and the real world.&lt;/p&gt; 
&lt;h2&gt;IsaacGym Conda Env&lt;/h2&gt; 
&lt;p&gt;Create mamba/conda environment, in the following we use conda for example, but you can use mamba as well.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;conda create -n hvgym python=3.8
conda activate hvgym
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Install IsaacGym&lt;/h3&gt; 
&lt;p&gt;Download &lt;a href="https://developer.nvidia.com/isaac-gym/download"&gt;IsaacGym&lt;/a&gt; and extract:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;wget https://developer.nvidia.com/isaac-gym-preview-4
tar -xvzf isaac-gym-preview-4
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Install IsaacGym Python API:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -e isaacgym/python
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Test installation:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python 1080_balls_of_solitude.py  # or
python joint_monkey.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For libpython error:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Check conda path: &lt;pre&gt;&lt;code class="language-bash"&gt;conda info -e
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Set LD_LIBRARY_PATH: &lt;pre&gt;&lt;code class="language-bash"&gt;export LD_LIBRARY_PATH=&amp;lt;/path/to/conda/envs/your_env/lib&amp;gt;:$LD_LIBRARY_PATH
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Install HumanoidVerse&lt;/h3&gt; 
&lt;p&gt;Install dependencies:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -e .
pip install -e isaac_utils
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Test with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;HYDRA_FULL_ERROR=1 python humanoidverse/train_agent.py \
+simulator=isaacgym \
+exp=locomotion \
+domain_rand=NO_domain_rand \
+rewards=loco/reward_g1_locomotion \
+robot=g1/g1_29dof_anneal_23dof \
+terrain=terrain_locomotion_plane \
+obs=loco/leggedloco_obs_singlestep_withlinvel \
num_envs=1 \
project_name=TestIsaacGymInstallation \
experiment_name=G123dof_loco \
headless=False
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;Note:&lt;/summary&gt; This is ONLY for testing, NOT how we train the locomotion policy in the ASAP paper. But still, you can train a locomotion policy by: 
 &lt;pre&gt;&lt;code class="language-bash"&gt;HYDRA_FULL_ERROR=1 python humanoidverse/train_agent.py \
+simulator=isaacgym \
+exp=locomotion \
+domain_rand=NO_domain_rand \
+rewards=loco/reward_g1_locomotion \
+robot=g1/g1_29dof_anneal_23dof \
+terrain=terrain_locomotion_plane \
+obs=loco/leggedloco_obs_singlestep_withlinvel \
num_envs=4096 \
project_name=TestIsaacGymInstallation \
experiment_name=G123dof_loco \
headless=True \
rewards.reward_penalty_curriculum=True \
rewards.reward_initial_penalty_scale=0.1 \
rewards.reward_penalty_degree=0.00003 
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;IsaacLab Environment&lt;/h2&gt; 
&lt;h3&gt;Install IsaacSim&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Download Omniverse Launcher&lt;/li&gt; 
 &lt;li&gt;Install Isaac Sim through launcher&lt;/li&gt; 
 &lt;li&gt;Set environment variables:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export ISAACSIM_PATH="${HOME}/.local/share/ov/pkg/isaac-sim-4.2.0"
export ISAACSIM_PYTHON_EXE="${ISAACSIM_PATH}/python.sh"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Install IsaacLab&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/isaac-sim/IsaacLab.git
cd IsaacLab &amp;amp;&amp;amp; ./isaaclab.sh --conda hvlab
mamba activate hvlab
sudo apt install cmake build-essential
./isaaclab.sh --install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Setup HumanoidVerse&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -e .
pip install -e isaac_utils
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Genesis Environment&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mamba create -n hvgen python=3.10
mamba activate hvgen
pip install genesis-world torch
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Install dependencies:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -e .
pip install -e isaac_utils
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Motion Tracking Training&lt;/h1&gt; 
&lt;p&gt;Train a phase-based motion tracking policy to imitate Cristiano Ronaldo's signature Siuuu move&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python humanoidverse/train_agent.py \
+simulator=isaacgym \
+exp=motion_tracking \
+domain_rand=NO_domain_rand \
+rewards=motion_tracking/reward_motion_tracking_dm_2real \
+robot=g1/g1_29dof_anneal_23dof \
+terrain=terrain_locomotion_plane \
+obs=motion_tracking/deepmimic_a2c_nolinvel_LARGEnoise_history \
num_envs=4096 \
project_name=MotionTracking \
experiment_name=MotionTracking_CR7 \
robot.motion.motion_file="humanoidverse/data/motions/g1_29dof_anneal_23dof/TairanTestbed/singles/0-TairanTestbed_TairanTestbed_CR7_video_CR7_level1_filter_amass.pkl" \
rewards.reward_penalty_curriculum=True \
rewards.reward_penalty_degree=0.00001 \
env.config.resample_motion_when_training=False \
env.config.termination.terminate_when_motion_far=True \
env.config.termination_curriculum.terminate_when_motion_far_curriculum=True \
env.config.termination_curriculum.terminate_when_motion_far_threshold_min=0.3 \
env.config.termination_curriculum.terminate_when_motion_far_curriculum_degree=0.000025 \
robot.asset.self_collisions=0
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After training, you can visualize the policy by:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python humanoidverse/eval_agent.py \
+checkpoint=logs/MotionTracking/xxxxxxxx_xxxxxxx-MotionTracking_CR7-motion_tracking-g1_29dof_anneal_23dof/model_5800.pt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This is the visualization of the policy after traning 5800 iters. The policy is able to imitate the motion of Cristiano Ronaldo's Siuuu move. With more training, the policy will be more accurate and smooth (see the video in the &lt;a href="https://arxiv.org/pdf/2502.01143"&gt;paper&lt;/a&gt;).&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/imgs/motion_tracking_5800.gif" width="400px" /&gt; 
&lt;h1&gt;ASAP delta action model training&lt;/h1&gt; 
&lt;p&gt;Note that the only difference between the delta action model training and naive motion tracking training is that delta action model needs a motion file with extra keyname &lt;code&gt;"action"&lt;/code&gt; in the motion file, so that the resulting RL policy we are training is able to use the delta action model to &lt;code&gt;"control the robot"&lt;/code&gt; to match the real-world/sim2sim motions.&lt;/p&gt; 
&lt;h2&gt;Train delta action model&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;python humanoidverse/train_agent.py \                                                                                   
  +simulator=isaacgym \
  +exp=train_delta_a_open_loop \
  +domain_rand=NO_domain_rand \
  +rewards=motion_tracking/delta_a/reward_delta_a_openloop \
  +robot=g1/g1_29dof_anneal_23dof \
  +terrain=terrain_locomotion_plane \
  +obs=delta_a/open_loop \
  num_envs=5000 \
  project_name=DeltaA_Training \
  experiment_name=openloopDeltaA_training \
  robot.motion.motion_file="&amp;lt;PATH_TO_YOUR_MOTION_FILE_WITH_ACTION_KEYNAME&amp;gt;" \
  env.config.max_episode_length_s=1.0 \
  rewards.reward_scales.penalty_minimal_action_norm=-0.1 \
  +device=cuda:0 \
  env.config.resample_motion_when_training=True \
  env.config.resample_time_interval_s=10000
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Use delta action model for policy finetuning&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;HYDRA_FULL_ERROR=1 \
python humanoidverse/train_agent.py \
+simulator=isaacgym \
+exp=train_delta_a_closed_loop \
algo.config.policy_checkpoint='&amp;lt;PATH_TO_YOUR_DELTA_A_MODEL&amp;gt;' \
+domain_rand=NO_domain_rand_finetune_with_deltaA \
+rewards=motion_tracking/reward_motion_tracking_dm_simfinetuning \
+robot=g1/g1_29dof_anneal_23dof \
+terrain=terrain_locomotion_plane \
+obs=delta_a/train_policy_with_delta_a \
num_envs=4096 \
project_name=DeltaA_Finetune \
experiment_name=finetune_with_deltaA \
robot.motion.motion_file="&amp;lt;PATH_TO_YOUR_MOTION_FILE&amp;gt;" \
+opt=wandb \
env.config.add_extra_action=True \
+checkpoint="&amp;lt;PATH_TO_YOUR_POLICY_TO_BE_FINETUNED&amp;gt;" \
domain_rand.push_robots=False \
env.config.noise_to_initial_level=1 \
rewards.reward_penalty_curriculum=True \
+device=cuda:0 \
algo.config.save_interval=5 \
algo.config.num_learning_iterations=1000 

&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Motion Retargeting to Any Humanoid&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] Here we share a generic humanoid motion retargeting pipeline to any humanoid from &lt;a href="https://github.com/ZhengyiLuo/PHC"&gt;PHC&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] We have provided all the SMPL motions (&lt;code&gt;ASAP/humanoidverse/data/motions/raw_tairantestbed_smpl&lt;/code&gt;) and retargtted G1 motions (&lt;code&gt;ASAP/humanoidverse/data/motions/g1_29dof_anneal_23dof/TairanTestbed/singles&lt;/code&gt;) used in the ASAP paper in this codebase. If you are interested in using these motions G1, you can ignore this section. If you are interested in retargeting other humanoids or other motions, you can follow the steps below to prepare the SMPL shapes and motions.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;It has three steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;SMPL Shape preparation&lt;/li&gt; 
 &lt;li&gt;SMPL Motion preparation&lt;/li&gt; 
 &lt;li&gt;Robot XML and Motion Config preparation&lt;/li&gt; 
 &lt;li&gt;Humanoid-SMPL shape fitting&lt;/li&gt; 
 &lt;li&gt;Humanoid-SMPL motion retargeting&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;1. SMPL Shape preparation&lt;/h2&gt; 
&lt;p&gt;Download &lt;a href="https://download.is.tue.mpg.de/download.php?domain=smpl&amp;amp;sfile=SMPL_python_v.1.1.0.zip"&gt;v1.1.0 SMPL files with pkl format&lt;/a&gt; and put it under &lt;code&gt;humanoidverse/data/smpl/&lt;/code&gt;, and you should have:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;|-- ASAP
    |-- humanoidverse
        |-- data
            |-- smpl
                |-- SMPL_python_v.1.1.0.zip
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then &lt;code&gt;cd ASAP/humanoidverse/data/smpl/&lt;/code&gt; and &lt;code&gt;unzip SMPL_python_v.1.1.0.zip&lt;/code&gt;, after some copying and moving, you should have:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;|-- ASAP
    |-- humanoidverse
        |-- data
            |-- smpl
                |-- SMPL_python_v.1.1.0
                |-- models
                    |-- basicmodel_f_lbs_10_207_0_v1.1.0.pkl
                    |-- basicmodel_m_lbs_10_207_0_v1.1.0.pkl
                    |-- basicmodel_neutral_lbs_10_207_0_v1.1.0.pkl
                |-- smpl_webuser
                |-- ...

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Rename these three pkl files and move it under smpl like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;|-- ASAP
    |-- humanoidverse
        |-- data
            |-- smpl
                |-- SMPL_FEMALE.pkl
                |-- SMPL_MALE.pkl
                |-- SMPL_NEUTRAL.pkl
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;2. SMPL Motion preparation (AMASS)&lt;/h2&gt; 
&lt;p&gt;Download &lt;a href="https://amass.is.tue.mpg.de/index.html"&gt;AMASS Dataset&lt;/a&gt; with &lt;code&gt;SMPL + H G format&lt;/code&gt; and put it under &lt;code&gt;humanoidverse/data/motions/AMASS/AMASS_Complete/&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;|-- ASAP
    |-- humanoidverse
        |-- data
            |-- AMASS
                |-- AMASS_Complete
                    |-- ACCAD.tar.bz2
                    |-- BMLhandball.tar.bz2
                    |-- BMLmovi.tar.bz2
                    |-- BMLrub.tar
                    |-- CMU.tar.bz2
                    |-- ...
                    |-- Transitions.tar.bz2
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;And then cd ASAP/humanoidverse/data/motions/AMASS/AMASS_Complete/ and extract all the motion files by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;for file in *.tar.bz2; do
    tar -xvjf "$file"
done
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then you should have:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;|-- ASAP
    |-- humanoidverse
        |-- data
            |-- AMASS
                |-- AMASS_Complete
                    |-- ACCAD
                    |-- BioMotionLab_NTroje
                    |-- BMLhandball
                    |-- BMLmovi
                    |-- CMU
                    |-- ...
                    |-- Transitions
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;3. Robot XML and Motion Config preparation&lt;/h2&gt; 
&lt;p&gt;Make sure you have robot xml and meshes ready at (G1 as example) &lt;code&gt;humanoidverse/data/robots/g1/g1_29dof_anneal_23dof_fitmotionONLY.xml&lt;/code&gt; And add your config for the robot motion in &lt;code&gt;humanoidverse/config/robot/g1_29dof_anneal_23dof.yaml&lt;/code&gt; with like the following. Remember to link the xml path in the config.&lt;/p&gt; 
&lt;h2&gt;4. Humanoid-SMPL shape fitting&lt;/h2&gt; 
&lt;p&gt;Run the following command to fit the SMPL shape to the humanoid.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python scripts/data_process/fit_smpl_shape.py +robot=g1/g1_29dof_anneal_23dof
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;And you should have you shape file located at &lt;code&gt;humanoidverse/data/shape/g1_29dof_anneal_23dof/shape_optimized_v1.pkl&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;If you want to visualize the shape, you can run with flag &lt;code&gt;+vis=True&lt;/code&gt;, then you can have visualization of the fitted SMPL body shape and the humanoid body keypoints like this shape. The blue is the humanoid body keypoints and the orange is the fitted SMPL body keypoint. You can tune the &lt;code&gt;robot motion&lt;/code&gt; in &lt;code&gt;humanoidverse/config/robot/g1_29dof_anneal_23dof.yaml&lt;/code&gt; to adjust the correspondence, extend links lengths to get better fitted SMPL shape.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/imgs/g1_29dof_anneal_23dof_shape.png" width="400px" /&gt; 
&lt;h2&gt;5. Humanoid-SMPL motion retargeting&lt;/h2&gt; 
&lt;p&gt;Run the following command to retarget the motion to the humanoid.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python scripts/data_process/fit_smpl_motion.py +robot=g1/g1_29dof_anneal_23dof
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Visualize motion&lt;/h3&gt; 
&lt;p&gt;Run&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python scripts/vis/vis_q_mj.py +robot=g1/g1_29dof_anneal_23dof +visualize_motion_file="humanoidverse/data/motions/g1_29dof_anneal_23dof/TairanTestbed/singles/0-motions_raw_tairantestbed_smpl_video_side_jump_level4_filter_amass.pkl"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To test, and you should have you one single motion file located at &lt;code&gt;humanoidverse/data/motions/g1_29dof_anneal_23dof/TairanTestbed/singles/0-motions_raw_tairantestbed_smpl_video_side_jump_level4_filter_amass.pkl&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;If you want to visualize the motion, you can run&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python scripts/vis/vis_q_mj.py +robot=g1/g1_29dof_anneal_23dof +visualize_motion_file="humanoidverse/data/motions/g1_29dof_anneal_23dof/TairanTestbed/singles/0-motions_raw_tairantestbed_smpl_video_side_jump_level4_filter_amass.pkl"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You should have&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/imgs/g1_29dof_anneal_23dof_motion.gif" width="400px" /&gt; 
&lt;h1&gt;Sim2Sim/Sim2Real&lt;/h1&gt; 
&lt;h2&gt;Environment Setup&lt;/h2&gt; 
&lt;p&gt;Env Installation:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;mamba create -n asap_deploy python=3.10
mamba activate asap_deploy
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Install ros2-python&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# this adds the conda-forge channel to the new created environment configuration 
conda config --env --add channels conda-forge
# and the robostack channel
conda config --env --add channels robostack-staging
# remove the defaults channel just in case, this might return an error if it is not in the list which is ok
conda config --env --remove channels defaults
# install the ros2-python package
conda install ros-humble-desktop
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Test Ros2Installation:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;rviz2
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You should see the UI like this:&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/imgs/rviz.png" width="400px" /&gt; 
&lt;p&gt;Install Unitree SDK&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone git@github.com:unitreerobotics/unitree_sdk2_python.git
cd unitree_sdk2_python
pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;minor issue to fix:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install --upgrade numpy scipy
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Sim2Sim&lt;/h2&gt; 
&lt;p&gt;start the simulation in the sim2real folder:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python sim_env/base_sim.py --config=config/g1_29dof_hist.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;in another terminal, start the policy:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python rl_policy/deepmimic_dec_loco_height.py --config=config/g1_29dof_hist.yaml --loco_model_path=./models/dec_loco/20250109_231507-noDR_rand_history_loco_stand_height_noise-decoupled_locomotion-g1_29dof/model_6600.onnx --mimic_model_paths=./models/mimic
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;click to the policy terminal and press &lt;code&gt;]&lt;/code&gt; to activate the locomotion policy&lt;/li&gt; 
 &lt;li&gt;click to the policy terminal and press &lt;code&gt;[&lt;/code&gt; to activate the asap policy (phase-based motion tracking policy)&lt;/li&gt; 
 &lt;li&gt;click to the policy terminal and press &lt;code&gt;;&lt;/code&gt; to switch to the asap policy&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;i&lt;/code&gt; to make the robot the initial position&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;o&lt;/code&gt; to emergence stop the robot&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;9&lt;/code&gt; in mujoco viewer to release the robostack&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;=&lt;/code&gt; to switch between tapping and walking for the locomotion policy&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;w/a/s/d&lt;/code&gt; to control the linear velocity&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;q/e&lt;/code&gt; to control the angular velocity&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;z&lt;/code&gt; to set all commands to zero&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;And you should be able to play around with some checkpoints from the ASAP paper. Have fun!&lt;/p&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/imgs/asap-sim2sim-clip0-ezgif.com-video-to-gif-converter.gif" width="300px" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/imgs/asap-sim2sim-clip1-ezgif.com-video-to-gif-converter.gif" width="300px" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/imgs/asap-sim2sim-clip3-ezgif.com-video-to-gif-converter.gif" width="300px" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/imgs/asap-sim2sim-clip4-ezgif.com-video-to-gif-converter.gif" width="300px" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/imgs/asap-sim2sim-clip5-ezgif.com-video-to-gif-converter.gif" width="300px" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/imgs/asap-sim2sim-clip6-ezgif.com-video-to-gif-converter.gif" width="300px" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;!-- Replace gif1.gif ... gif6.gif with your actual gif filenames and optionally add captions below each if desired --&gt; 
&lt;h2&gt;Sim2Real&lt;/h2&gt; 
&lt;p&gt;&lt;code&gt;Note from Tairan&lt;/code&gt;: make sure to make the G1 robot to 29dof following this &lt;a href="https://support.unitree.com/home/en/G1_developer/waist_fastener"&gt;doc&lt;/a&gt; and restart the robot after waist unlocking. If you don't know how to log into the Unitree Explore APP, contact unitree support.&lt;/p&gt; 
&lt;p&gt;Enter Low-Level for g1&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Open humanoid and wait until the head blue light is constantly on&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;L2+R2&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;L2+A&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;L2+B&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Connect PC to the G1 by ethernet cable and configure the network following &lt;a href="https://support.unitree.com/home/en/G1_developer/quick_development"&gt;this document&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Before starting the policy, modify the &lt;code&gt;config/g1_29dof_hist.yaml&lt;/code&gt; to set &lt;code&gt;INTERFACE&lt;/code&gt; to &lt;code&gt;eth0&lt;/code&gt; (if you are using linux), basically the network interface that you are using to connect to the robot with your PC's IP shown as &lt;code&gt;192.168.123.xxx&lt;/code&gt; in &lt;code&gt;ifconfig&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;start the policy:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python rl_policy/deepmimic_dec_loco_height.py --config=config/g1_29dof_hist.yaml --loco_model_path=./models/dec_loco/20250109_231507-noDR_rand_history_loco_stand_height_noise-decoupled_locomotion-g1_29dof/model_6600.onnx --mimic_model_paths=./models/mimic
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;click to the policy terminal and press &lt;code&gt;]&lt;/code&gt; to activate the locomotion policy&lt;/li&gt; 
 &lt;li&gt;click to the policy terminal and press &lt;code&gt;[&lt;/code&gt; to activate the asap policy (phase-based motion tracking policy)&lt;/li&gt; 
 &lt;li&gt;click to the policy terminal and press &lt;code&gt;;&lt;/code&gt; to switch to the asap policy&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;i&lt;/code&gt; to make the robot the initial position&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;o&lt;/code&gt; to emergence stop the robot&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;9&lt;/code&gt; in mujoco viewer to release the robostack&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;=&lt;/code&gt; to switch between tapping and walking for the locomotion policy&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;w/a/s/d&lt;/code&gt; to control the linear velocity&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;q/e&lt;/code&gt; to control the angular velocity&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;z&lt;/code&gt; to set all commands to zero&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;o&lt;/code&gt; to emergence stop the robot&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;‚ÄºÔ∏èAlert &amp;amp; Disclaimer&lt;/h3&gt; 
&lt;p&gt;Deploying these models on physical hardware can be hazardous. Unless you have deep sim‚Äëto‚Äëreal expertise and robust safety protocols, we strongly advise against running the model on real robots. These models are supplied for research use only, and we disclaim all responsibility for any harm, loss, or malfunction arising from their deployment.&lt;/p&gt; 
&lt;h3&gt;Demo code to collect real-world data&lt;/h3&gt; 
&lt;p&gt;We provide a demo code to collect real-world data in the &lt;code&gt;sim2real/rl_policy/listener_deltaa.py&lt;/code&gt; file. Since MoCap setup is hard to transfer across different robots/labs, we hope this code can help you to collect data for your own experiments. Contact us (&lt;a href="mailto:tairanh@andrew.cmu.edu"&gt;tairanh@andrew.cmu.edu&lt;/a&gt;) if you have any questions.&lt;/p&gt; 
&lt;h1&gt;Citation&lt;/h1&gt; 
&lt;p&gt;If you find our work useful, please consider citing us!&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@article{he2025asap,
  title={ASAP: Aligning Simulation and Real-World Physics for Learning Agile Humanoid Whole-Body Skills},
  author={He, Tairan and Gao, Jiawei and Xiao, Wenli and Zhang, Yuanhang and Wang, Zi and Wang, Jiashun and Luo, Zhengyi and He, Guanqi and Sobanbabu, Nikhil and Pan, Chaoyi and Yi, Zeji and Qu, Guannan and Kitani, Kris and Hodgins, Jessica and Fan, Linxi "Jim" and Zhu, Yuke and Liu, Changliu and Shi, Guanya},
  journal={arXiv preprint arXiv:2502.01143},
  year={2025}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;p&gt;This project is licensed under the MIT License - see the &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>exo-explore/exo</title>
      <link>https://github.com/exo-explore/exo</link>
      <description>&lt;p&gt;Run your own AI cluster at home with everyday devices üì±üíª üñ•Ô∏è‚åö&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="/docs/exo-logo-black-bg.jpg" /&gt; 
  &lt;img alt="exo logo" src="https://raw.githubusercontent.com/exo-explore/exo/main/docs/exo-logo-transparent.png" width="50%" height="50%" /&gt; 
 &lt;/picture&gt; 
 &lt;p&gt;exo: Run your own AI cluster at home with everyday devices. Maintained by &lt;a href="https://x.com/exolabs"&gt;exo labs&lt;/a&gt;.&lt;/p&gt; 
 &lt;h3&gt; &lt;p&gt;&lt;a href="https://discord.gg/EUnjGpsmWw"&gt;Discord&lt;/a&gt; | &lt;a href="https://t.me/+Kh-KqHTzFYg3MGNk"&gt;Telegram&lt;/a&gt; | &lt;a href="https://x.com/exolabs"&gt;X&lt;/a&gt;&lt;/p&gt; &lt;/h3&gt; 
 &lt;p&gt;&lt;a href="https://github.com/exo-explore/exo/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/exo-explore/exo" alt="GitHub Repo stars" /&gt;&lt;/a&gt; &lt;a href="https://dl.circleci.com/status-badge/redirect/circleci/TrkofJDoGzdQAeL6yVHKsg/4i5hJuafuwZYZQxbRAWS71/tree/main"&gt;&lt;img src="https://dl.circleci.com/status-badge/img/circleci/TrkofJDoGzdQAeL6yVHKsg/4i5hJuafuwZYZQxbRAWS71/tree/main.svg?style=svg" alt="Tests" /&gt;&lt;/a&gt; &lt;a href="https://www.gnu.org/licenses/gpl-3.0"&gt;&lt;img src="https://img.shields.io/badge/License-GPLv3-blue.svg?sanitize=true" alt="License: GPL v3" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/11849" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/11849" alt="exo-explore%2Fexo | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;p&gt;Unify your existing devices into one powerful GPU: iPhone, iPad, Android, Mac, NVIDIA, Raspberry Pi, pretty much any device!&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;h2&gt;Update: exo is hiring. See &lt;a href="https://exolabs.net"&gt;here&lt;/a&gt; for more details.&lt;/h2&gt; 
 &lt;h2&gt;Interested in running exo in your business? &lt;a href="mailto:hello@exolabs.net"&gt;Contact us&lt;/a&gt; to discuss.&lt;/h2&gt; 
&lt;/div&gt; 
&lt;h2&gt;Get Involved&lt;/h2&gt; 
&lt;p&gt;exo is &lt;strong&gt;experimental&lt;/strong&gt; software. Expect bugs early on. Create issues so they can be fixed. The &lt;a href="https://x.com/exolabs"&gt;exo labs&lt;/a&gt; team will strive to resolve issues quickly.&lt;/p&gt; 
&lt;p&gt;We also welcome contributions from the community. We have a list of bounties in &lt;a href="https://docs.google.com/spreadsheets/d/1cTCpTIp48UnnIvHeLEUNg1iMy_Q6lRybgECSFCoVJpE/edit?usp=sharing"&gt;this sheet&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;h3&gt;Wide Model Support&lt;/h3&gt; 
&lt;p&gt;exo supports different models including LLaMA (&lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/inference/mlx/models/llama.py"&gt;MLX&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/inference/tinygrad/models/llama.py"&gt;tinygrad&lt;/a&gt;), Mistral, LlaVA, Qwen, and Deepseek.&lt;/p&gt; 
&lt;h3&gt;Dynamic Model Partitioning&lt;/h3&gt; 
&lt;p&gt;exo &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/topology/ring_memory_weighted_partitioning_strategy.py"&gt;optimally splits up models&lt;/a&gt; based on the current network topology and device resources available. This enables you to run larger models than you would be able to on any single device.&lt;/p&gt; 
&lt;h3&gt;Automatic Device Discovery&lt;/h3&gt; 
&lt;p&gt;exo will &lt;a href="https://github.com/exo-explore/exo/raw/945f90f676182a751d2ad7bcf20987ab7fe0181e/exo/orchestration/node.py#L154"&gt;automatically discover&lt;/a&gt; other devices using the best method available. Zero manual configuration.&lt;/p&gt; 
&lt;h3&gt;ChatGPT-compatible API&lt;/h3&gt; 
&lt;p&gt;exo provides a &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/api/chatgpt_api.py"&gt;ChatGPT-compatible API&lt;/a&gt; for running models. It's a &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/examples/chatgpt_api.sh"&gt;one-line change&lt;/a&gt; in your application to run models on your own hardware using exo.&lt;/p&gt; 
&lt;h3&gt;Device Equality&lt;/h3&gt; 
&lt;p&gt;Unlike other distributed inference frameworks, exo does not use a master-worker architecture. Instead, exo devices &lt;a href="https://github.com/exo-explore/exo/raw/945f90f676182a751d2ad7bcf20987ab7fe0181e/exo/orchestration/node.py#L161"&gt;connect p2p&lt;/a&gt;. As long as a device is connected somewhere in the network, it can be used to run models.&lt;/p&gt; 
&lt;p&gt;Exo supports different &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/topology/partitioning_strategy.py"&gt;partitioning strategies&lt;/a&gt; to split up a model across devices. The default partitioning strategy is &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/topology/ring_memory_weighted_partitioning_strategy.py"&gt;ring memory weighted partitioning&lt;/a&gt;. This runs an inference in a ring where each device runs a number of model layers proportional to the memory of the device.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/exo-explore/exo/main/docs/exo-screenshot.jpg" alt="&amp;quot;A screenshot of exo running 5 nodes" /&gt;&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;The current recommended way to install exo is from source.&lt;/p&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python&amp;gt;=3.12.0 is required because of &lt;a href="https://github.com/exo-explore/exo/issues/5"&gt;issues with asyncio&lt;/a&gt; in previous versions.&lt;/li&gt; 
 &lt;li&gt;For Linux with NVIDIA GPU support (Linux-only, skip if not using Linux or NVIDIA): 
  &lt;ul&gt; 
   &lt;li&gt;NVIDIA driver - verify with &lt;code&gt;nvidia-smi&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;CUDA toolkit - install from &lt;a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#cuda-cross-platform-installation"&gt;NVIDIA CUDA guide&lt;/a&gt;, verify with &lt;code&gt;nvcc --version&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;cuDNN library - download from &lt;a href="https://developer.nvidia.com/cudnn-downloads"&gt;NVIDIA cuDNN page&lt;/a&gt;, verify installation by following &lt;a href="https://docs.nvidia.com/deeplearning/cudnn/latest/installation/linux.html#verifying-the-install-on-linux:~:text=at%20a%20time.-,Verifying%20the%20Install%20on%20Linux,Test%20passed!,-Upgrading%20From%20Older"&gt;these steps&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Hardware Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;The only requirement to run exo is to have enough memory across all your devices to fit the entire model into memory. For example, if you are running llama 3.1 8B (fp16), you need 16GB of memory across all devices. Any of the following configurations would work since they each have more than 16GB of memory in total: 
  &lt;ul&gt; 
   &lt;li&gt;2 x 8GB M3 MacBook Airs&lt;/li&gt; 
   &lt;li&gt;1 x 16GB NVIDIA RTX 4070 Ti Laptop&lt;/li&gt; 
   &lt;li&gt;2 x Raspberry Pi 400 with 4GB of RAM each (running on CPU) + 1 x 8GB Mac Mini&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;exo is designed to run on devices with heterogeneous capabilities. For example, you can have some devices with powerful GPUs and others with integrated GPUs or even CPUs. Adding less capable devices will slow down individual inference latency but will increase the overall throughput of the cluster.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;From source&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;git clone https://github.com/exo-explore/exo.git
cd exo
pip install -e .
# alternatively, with venv
source install.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Troubleshooting&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;If running on Mac, MLX has an &lt;a href="https://ml-explore.github.io/mlx/build/html/install.html"&gt;install guide&lt;/a&gt; with troubleshooting steps.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Performance&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;There are a number of things users have empirically found to improve performance on Apple Silicon Macs:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;ol&gt; 
 &lt;li&gt;Upgrade to the latest version of macOS Sequoia.&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;./configure_mlx.sh&lt;/code&gt;. This runs commands to optimize GPU memory allocation on Apple Silicon Macs.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;h3&gt;Example Usage on Multiple macOS Devices&lt;/h3&gt; 
&lt;h4&gt;Device 1:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Device 2:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;That's it! No configuration required - exo will automatically discover the other device(s).&lt;/p&gt; 
&lt;p&gt;exo starts a ChatGPT-like WebUI (powered by &lt;a href="https://github.com/tinygrad/tinygrad/tree/master/examples/tinychat"&gt;tinygrad tinychat&lt;/a&gt;) on &lt;a href="http://localhost:52415"&gt;http://localhost:52415&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;For developers, exo also starts a ChatGPT-compatible API endpoint on &lt;a href="http://localhost:52415/v1/chat/completions"&gt;http://localhost:52415/v1/chat/completions&lt;/a&gt;. Examples with curl:&lt;/p&gt; 
&lt;h4&gt;Llama 3.2 3B:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;curl http://localhost:52415/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
     "model": "llama-3.2-3b",
     "messages": [{"role": "user", "content": "What is the meaning of exo?"}],
     "temperature": 0.7
   }'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Llama 3.1 405B:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;curl http://localhost:52415/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
     "model": "llama-3.1-405b",
     "messages": [{"role": "user", "content": "What is the meaning of exo?"}],
     "temperature": 0.7
   }'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;DeepSeek R1 (full 671B):&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;curl http://localhost:52415/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
     "model": "deepseek-r1",
     "messages": [{"role": "user", "content": "What is the meaning of exo?"}],
     "temperature": 0.7
   }'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Llava 1.5 7B (Vision Language Model):&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;curl http://localhost:52415/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
     "model": "llava-1.5-7b-hf",
     "messages": [
      {
        "role": "user",
        "content": [
          {
            "type": "text",
            "text": "What are these?"
          },
          {
            "type": "image_url",
            "image_url": {
              "url": "http://images.cocodataset.org/val2017/000000039769.jpg"
            }
          }
        ]
      }
    ],
     "temperature": 0.0
   }'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Example Usage on Multiple Heterogenous Devices (macOS + Linux)&lt;/h3&gt; 
&lt;h4&gt;Device 1 (macOS):&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note: We don't need to explicitly tell exo to use the &lt;strong&gt;tinygrad&lt;/strong&gt; inference engine. &lt;strong&gt;MLX&lt;/strong&gt; and &lt;strong&gt;tinygrad&lt;/strong&gt; are interoperable!&lt;/p&gt; 
&lt;h4&gt;Device 2 (Linux):&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Linux devices will automatically default to using the &lt;strong&gt;tinygrad&lt;/strong&gt; inference engine.&lt;/p&gt; 
&lt;p&gt;You can read about tinygrad-specific env vars &lt;a href="https://docs.tinygrad.org/env_vars/"&gt;here&lt;/a&gt;. For example, you can configure tinygrad to use the cpu by specifying &lt;code&gt;CLANG=1&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Example Usage on a single device with "exo run" command&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;exo run llama-3.2-3b
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;With a custom prompt:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;exo run llama-3.2-3b --prompt "What is the meaning of exo?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Model Storage&lt;/h3&gt; 
&lt;p&gt;Models by default are stored in &lt;code&gt;~/.cache/exo/downloads&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;You can set a different model storage location by setting the &lt;code&gt;EXO_HOME&lt;/code&gt; env var.&lt;/p&gt; 
&lt;h2&gt;Model Downloading&lt;/h2&gt; 
&lt;p&gt;Models are downloaded from Hugging Face. If you are running exo in a country with strict internet censorship, you may need to download the models manually and put them in the &lt;code&gt;~/.cache/exo/downloads&lt;/code&gt; directory.&lt;/p&gt; 
&lt;p&gt;To download models from a proxy endpoint, set the &lt;code&gt;HF_ENDPOINT&lt;/code&gt; environment variable. For example, to run exo with the huggingface mirror endpoint:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;HF_ENDPOINT=https://hf-mirror.com exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Debugging&lt;/h2&gt; 
&lt;p&gt;Enable debug logs with the DEBUG environment variable (0-9).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;DEBUG=9 exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For the &lt;strong&gt;tinygrad&lt;/strong&gt; inference engine specifically, there is a separate DEBUG flag &lt;code&gt;TINYGRAD_DEBUG&lt;/code&gt; that can be used to enable debug logs (1-6).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;TINYGRAD_DEBUG=2 exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Formatting&lt;/h2&gt; 
&lt;p&gt;We use &lt;a href="https://github.com/google/yapf"&gt;yapf&lt;/a&gt; to format the code. To format the code, first install the formatting requirements:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip3 install -e '.[formatting]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then run the formatting script:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python3 format.py ./exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Known Issues&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;On certain versions of Python on macOS, certificates may not installed correctly, potentially causing SSL errors (e.g., when accessing huggingface.co). To resolve this, run the &lt;code&gt;Install Certificates&lt;/code&gt; command, typicall as follows:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;/Applications/Python 3.x/Install Certificates.command
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;üöß As the library is evolving so quickly, the iOS implementation has fallen behind Python. We have decided for now not to put out the buggy iOS version and receive a bunch of GitHub issues for outdated code. We are working on solving this properly and will make an announcement when it's ready. If you would like access to the iOS implementation now, please email &lt;a href="mailto:alex@exolabs.net"&gt;alex@exolabs.net&lt;/a&gt; with your GitHub username explaining your use-case and you will be granted access on GitHub.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Inference Engines&lt;/h2&gt; 
&lt;p&gt;exo supports the following inference engines:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚úÖ &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/inference/mlx/sharded_inference_engine.py"&gt;MLX&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;‚úÖ &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/inference/tinygrad/inference.py"&gt;tinygrad&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üöß &lt;a href="https://github.com/exo-explore/exo/pull/139"&gt;PyTorch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üöß &lt;a href="https://github.com/exo-explore/exo/issues/167"&gt;llama.cpp&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Discovery Modules&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚úÖ &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/networking/udp"&gt;UDP&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;‚úÖ &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/networking/manual"&gt;Manual&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;‚úÖ &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/networking/tailscale"&gt;Tailscale&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üöß Radio&lt;/li&gt; 
 &lt;li&gt;üöß Bluetooth&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Peer Networking Modules&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚úÖ &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/networking/grpc"&gt;GRPC&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üöß NCCL&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>instaloader/instaloader</title>
      <link>https://github.com/instaloader/instaloader</link>
      <description>&lt;p&gt;Download pictures (or videos) along with their captions and other metadata from Instagram.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;.. image:: &lt;a href="https://raw.githubusercontent.com/instaloader/instaloader/master/docs/logo_heading.png"&gt;https://raw.githubusercontent.com/instaloader/instaloader/master/docs/logo_heading.png&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;.. badges-start&lt;/p&gt; 
&lt;p&gt;|pypi| |pyversion| |license| |aur| |contributors| |downloads|&lt;/p&gt; 
&lt;p&gt;.. |pypi| image:: &lt;a href="https://img.shields.io/pypi/v/instaloader.svg"&gt;https://img.shields.io/pypi/v/instaloader.svg&lt;/a&gt; :alt: Instaloader PyPI Project Page :target: &lt;a href="https://pypi.org/project/instaloader/"&gt;https://pypi.org/project/instaloader/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;.. |license| image:: &lt;a href="https://img.shields.io/github/license/instaloader/instaloader.svg"&gt;https://img.shields.io/github/license/instaloader/instaloader.svg&lt;/a&gt; :alt: MIT License :target: &lt;a href="https://github.com/instaloader/instaloader/raw/master/LICENSE"&gt;https://github.com/instaloader/instaloader/blob/master/LICENSE&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;.. |pyversion| image:: &lt;a href="https://img.shields.io/pypi/pyversions/instaloader.svg"&gt;https://img.shields.io/pypi/pyversions/instaloader.svg&lt;/a&gt; :alt: Supported Python Versions&lt;/p&gt; 
&lt;p&gt;.. |contributors| image:: &lt;a href="https://img.shields.io/github/contributors/instaloader/instaloader.svg"&gt;https://img.shields.io/github/contributors/instaloader/instaloader.svg&lt;/a&gt; :alt: Contributor Count :target: &lt;a href="https://github.com/instaloader/instaloader/graphs/contributors"&gt;https://github.com/instaloader/instaloader/graphs/contributors&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;.. |aur| image:: &lt;a href="https://img.shields.io/aur/version/instaloader.svg"&gt;https://img.shields.io/aur/version/instaloader.svg&lt;/a&gt; :alt: Arch User Repository Package :target: &lt;a href="https://aur.archlinux.org/packages/instaloader/"&gt;https://aur.archlinux.org/packages/instaloader/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;.. |downloads| image:: &lt;a href="https://pepy.tech/badge/instaloader/month"&gt;https://pepy.tech/badge/instaloader/month&lt;/a&gt; :alt: PyPI Download Count :target: &lt;a href="https://pepy.tech/project/instaloader"&gt;https://pepy.tech/project/instaloader&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;.. badges-end&lt;/p&gt; 
&lt;p&gt;::&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ pip3 install instaloader

$ instaloader profile [profile ...]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Instaloader&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;downloads &lt;strong&gt;public and private profiles, hashtags, user stories, feeds and saved media&lt;/strong&gt;,&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;downloads &lt;strong&gt;comments, geotags and captions&lt;/strong&gt; of each post,&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;automatically &lt;strong&gt;detects profile name changes&lt;/strong&gt; and renames the target directory accordingly,&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;allows &lt;strong&gt;fine-grained customization&lt;/strong&gt; of filters and where to store downloaded media,&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;automatically &lt;strong&gt;resumes previously-interrupted&lt;/strong&gt; download iterations.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;::&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;instaloader [--comments] [--geotags]
            [--stories] [--highlights] [--tagged] [--reels] [--igtv]
            [--login YOUR-USERNAME] [--fast-update]
            profile | "#hashtag" | :stories | :feed | :saved
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;code&gt;Instaloader Documentation &amp;lt;https://instaloader.github.io/&amp;gt;&lt;/code&gt;__&lt;/p&gt; 
&lt;h2&gt;How to Automatically Download Pictures from Instagram&lt;/h2&gt; 
&lt;p&gt;To &lt;strong&gt;download all pictures and videos of a profile&lt;/strong&gt;, as well as the &lt;strong&gt;profile picture&lt;/strong&gt;, do&lt;/p&gt; 
&lt;p&gt;::&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;instaloader profile [profile ...]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;where &lt;code&gt;profile&lt;/code&gt; is the name of a profile you want to download. Instead of only one profile, you may also specify a list of profiles.&lt;/p&gt; 
&lt;p&gt;To later &lt;strong&gt;update your local copy&lt;/strong&gt; of that profiles, you may run&lt;/p&gt; 
&lt;p&gt;::&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;instaloader --fast-update profile [profile ...]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If &lt;code&gt;--fast-update&lt;/code&gt; is given, Instaloader stops when arriving at the first already-downloaded picture.&lt;/p&gt; 
&lt;p&gt;Alternatively, you can use &lt;code&gt;--latest-stamps&lt;/code&gt; to have Instaloader store the time each profile was last downloaded and only download newer media:&lt;/p&gt; 
&lt;p&gt;::&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;instaloader --latest-stamps -- profile [profile ...]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;With this option it's possible to move or delete downloaded media and still keep the archive updated.&lt;/p&gt; 
&lt;p&gt;When updating profiles, Instaloader automatically &lt;strong&gt;detects profile name changes&lt;/strong&gt; and renames the target directory accordingly.&lt;/p&gt; 
&lt;p&gt;Instaloader can also be used to &lt;strong&gt;download private profiles&lt;/strong&gt;. To do so, invoke it with&lt;/p&gt; 
&lt;p&gt;::&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;instaloader --login=your_username profile [profile ...]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;When logging in, Instaloader &lt;strong&gt;stores the session cookies&lt;/strong&gt; in a file in your temporary directory, which will be reused later the next time &lt;code&gt;--login&lt;/code&gt; is given. So you can download private profiles &lt;strong&gt;non-interactively&lt;/strong&gt; when you already have a valid session cookie file.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;Instaloader Documentation &amp;lt;https://instaloader.github.io/basic-usage.html&amp;gt;&lt;/code&gt;__&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;As an open source project, Instaloader heavily depends on the contributions from its community. See &lt;code&gt;contributing &amp;lt;https://instaloader.github.io/contributing.html&amp;gt;&lt;/code&gt;__ for how you may help Instaloader to become an even greater tool.&lt;/p&gt; 
&lt;h2&gt;Supporters&lt;/h2&gt; 
&lt;p&gt;.. current-sponsors-start&lt;/p&gt; 
&lt;p&gt;| Instaloader is proudly sponsored by | &lt;code&gt;@rocketapi-io &amp;lt;https://github.com/rocketapi-io&amp;gt;&lt;/code&gt;__&lt;/p&gt; 
&lt;p&gt;See &lt;code&gt;Alex' GitHub Sponsors &amp;lt;https://github.com/sponsors/aandergr&amp;gt;&lt;/code&gt;__ page for how you can sponsor the development of Instaloader!&lt;/p&gt; 
&lt;p&gt;.. current-sponsors-end&lt;/p&gt; 
&lt;p&gt;It is a pleasure for us to share our Instaloader to the world, and we are proud to have attracted such an active and motivating community, with so many users who share their suggestions and ideas with us. Buying a community-sponsored beer or coffee from time to time is very likely to further raise our passion for the development of Instaloader.&lt;/p&gt; 
&lt;p&gt;| For Donations, we provide GitHub Sponsors page, a PayPal.Me link and a Bitcoin address. | GitHub Sponsors: &lt;code&gt;Sponsor @aandergr on GitHub Sponsors &amp;lt;https://github.com/sponsors/aandergr&amp;gt;&lt;/code&gt;__ | PayPal: &lt;code&gt;PayPal.me/aandergr &amp;lt;https://www.paypal.me/aandergr&amp;gt;&lt;/code&gt;__ | BTC: 1Nst4LoadeYzrKjJ1DX9CpbLXBYE9RKLwY&lt;/p&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;.. disclaimer-start&lt;/p&gt; 
&lt;p&gt;Instaloader is in no way affiliated with, authorized, maintained or endorsed by Instagram or any of its affiliates or subsidiaries. This is an independent and unofficial project. Use at your own risk.&lt;/p&gt; 
&lt;p&gt;Instaloader is licensed under an MIT license. Refer to &lt;code&gt;LICENSE&lt;/code&gt; file for more information.&lt;/p&gt; 
&lt;p&gt;.. disclaimer-end&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>budtmo/docker-android</title>
      <link>https://github.com/budtmo/docker-android</link>
      <description>&lt;p&gt;Android in docker solution with noVNC supported and video recording&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img id="header" src="https://raw.githubusercontent.com/budtmo/docker-android/master/images/logo_docker-android.png" /&gt; &lt;/p&gt; 
&lt;p&gt;&lt;a href="http://paypal.me/budtmo"&gt;&lt;img src="https://img.shields.io/badge/paypal-donate-blue.svg?sanitize=true" alt="Paypal Donate" /&gt;&lt;/a&gt; &lt;a href="http://makeapullrequest.com"&gt;&lt;img src="https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square" alt="PRs Welcome" /&gt;&lt;/a&gt; &lt;a href="https://codecov.io/gh/budtmo/docker-android"&gt;&lt;img src="https://codecov.io/gh/budtmo/docker-android/branch/master/graph/badge.svg?sanitize=true" alt="codecov" /&gt;&lt;/a&gt; &lt;a href="https://gitter.im/budtmo/docker-android?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge"&gt;&lt;img src="https://badges.gitter.im/budtmo/docker-android.svg?sanitize=true" alt="Join the chat at https://gitter.im/budtmo/docker-android" /&gt;&lt;/a&gt; &lt;a href="https://github.com/budtmo/docker-android/releases"&gt;&lt;img src="https://img.shields.io/github/release/budtmo/docker-android.svg?sanitize=true" alt="GitHub release" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Docker-Android is a docker image built to be used for everything related to Android. It can be used for Application development and testing (native, web and hybrid-app).&lt;/p&gt; 
&lt;h2&gt;Advantages of using this project&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Emulator with different device profile and skins, such as Samsung Galaxy S6, LG Nexus 4, HTC Nexus One and more.&lt;/li&gt; 
 &lt;li&gt;Support vnc to be able to see what happen inside docker container&lt;/li&gt; 
 &lt;li&gt;Support log sharing feature where all logs can be accessed from web-UI&lt;/li&gt; 
 &lt;li&gt;Ability to control emulator from outside container by using adb connect&lt;/li&gt; 
 &lt;li&gt;Integrated with other cloud solutions, e.g. &lt;a href="https://www.genymotion.com/cloud/"&gt;Genymotion Cloud&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;It can be used to build Android project&lt;/li&gt; 
 &lt;li&gt;It can be used to run unit and UI-Test with different test-frameworks, e.g. Appium, Espresso, etc.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;List of Docker-Images&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Android&lt;/th&gt; 
   &lt;th align="left"&gt;API&lt;/th&gt; 
   &lt;th align="left"&gt;Image with latest release version&lt;/th&gt; 
   &lt;th align="left"&gt;Image with specific release version&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;9.0&lt;/td&gt; 
   &lt;td align="left"&gt;28&lt;/td&gt; 
   &lt;td align="left"&gt;budtmo/docker-android:emulator_9.0&lt;/td&gt; 
   &lt;td align="left"&gt;budtmo/docker-android:emulator_9.0_&amp;lt;release_version&amp;gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;10.0&lt;/td&gt; 
   &lt;td align="left"&gt;29&lt;/td&gt; 
   &lt;td align="left"&gt;budtmo/docker-android:emulator_10.0&lt;/td&gt; 
   &lt;td align="left"&gt;budtmo/docker-android:emulator_10.0_&amp;lt;release_version&amp;gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;11.0&lt;/td&gt; 
   &lt;td align="left"&gt;30&lt;/td&gt; 
   &lt;td align="left"&gt;budtmo/docker-android:emulator_11.0&lt;/td&gt; 
   &lt;td align="left"&gt;budtmo/docker-android:emulator_11.0_&amp;lt;release_version&amp;gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;12.0&lt;/td&gt; 
   &lt;td align="left"&gt;32&lt;/td&gt; 
   &lt;td align="left"&gt;budtmo/docker-android:emulator_12.0&lt;/td&gt; 
   &lt;td align="left"&gt;budtmo/docker-android:emulator_12.0_&amp;lt;release_version&amp;gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;13.0&lt;/td&gt; 
   &lt;td align="left"&gt;33&lt;/td&gt; 
   &lt;td align="left"&gt;budtmo/docker-android:emulator_13.0&lt;/td&gt; 
   &lt;td align="left"&gt;budtmo/docker-android:emulator_13.0_&amp;lt;release_version&amp;gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;14.0&lt;/td&gt; 
   &lt;td align="left"&gt;34&lt;/td&gt; 
   &lt;td align="left"&gt;budtmo/docker-android:emulator_14.0&lt;/td&gt; 
   &lt;td align="left"&gt;budtmo/docker-android:emulator_14.0_&amp;lt;release_version&amp;gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;-&lt;/td&gt; 
   &lt;td align="left"&gt;-&lt;/td&gt; 
   &lt;td align="left"&gt;budtmo/docker-android:genymotion&lt;/td&gt; 
   &lt;td align="left"&gt;budtmo/docker-android:genymotion_&amp;lt;release_version&amp;gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;List of Devices&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Device Name&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Phone&lt;/td&gt; 
   &lt;td&gt;Samsung Galaxy S10&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Phone&lt;/td&gt; 
   &lt;td&gt;Samsung Galaxy S9&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Phone&lt;/td&gt; 
   &lt;td&gt;Samsung Galaxy S8&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Phone&lt;/td&gt; 
   &lt;td&gt;Samsung Galaxy S7 Edge&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Phone&lt;/td&gt; 
   &lt;td&gt;Samsung Galaxy S7&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Phone&lt;/td&gt; 
   &lt;td&gt;Samsung Galaxy S6&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Phone&lt;/td&gt; 
   &lt;td&gt;Nexus 4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Phone&lt;/td&gt; 
   &lt;td&gt;Nexus 5&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Phone&lt;/td&gt; 
   &lt;td&gt;Nexus One&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Phone&lt;/td&gt; 
   &lt;td&gt;Nexus S&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Tablet&lt;/td&gt; 
   &lt;td&gt;Nexus 7&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Tablet&lt;/td&gt; 
   &lt;td&gt;Pixel C&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Docker is installed on your system.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;If you use &lt;em&gt;&lt;strong&gt;Ubuntu OS&lt;/strong&gt;&lt;/em&gt; on your host machine, you can skip this step. For &lt;em&gt;&lt;strong&gt;OSX&lt;/strong&gt;&lt;/em&gt; and &lt;em&gt;&lt;strong&gt;Windows OS&lt;/strong&gt;&lt;/em&gt; user, you need to use Virtual Machine that support Virtualization with Ubuntu OS because the image can be run under &lt;em&gt;&lt;strong&gt;Ubuntu OS only&lt;/strong&gt;&lt;/em&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Your machine should support virtualization. To check if the virtualization is enabled is:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;sudo apt install cpu-checker
kvm-ok
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run Docker-Android container&lt;/p&gt; &lt;pre&gt;&lt;code&gt;docker run -d -p 6080:6080 -e EMULATOR_DEVICE="Samsung Galaxy S10" -e WEB_VNC=true --device /dev/kvm --name android-container budtmo/docker-android:emulator_11.0
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Open &lt;em&gt;&lt;strong&gt;&lt;a href="http://localhost:6080"&gt;http://localhost:6080&lt;/a&gt;&lt;/strong&gt;&lt;/em&gt; to see inside running container.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;To check the status of the emulator&lt;/p&gt; &lt;pre&gt;&lt;code&gt;docker exec -it android-container cat device_status
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Persisting data&lt;/h2&gt; 
&lt;p&gt;The default behaviour is to destroy the emulated device on container restart. To persist data, you need to mount a volume at &lt;code&gt;/home/androidusr&lt;/code&gt;: &lt;code&gt;docker run -v data:/home/androidusr budtmo/docker-android:emulator_11.0&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;WSL2 Hardware acceleration (Windows 11 only)&lt;/h2&gt; 
&lt;p&gt;Credit goes to &lt;a href="https://www.paralint.com/2022/11/find-new-modified-and-unversioned-subversion-files-on-windows"&gt;Guillaume - The Parallel Interface blog&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://learn.microsoft.com/en-us/windows/wsl/wsl-config"&gt;Microsoft - Advanced settings configuration in WSL&lt;/a&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Add yourself to the &lt;code&gt;kvm&lt;/code&gt; usergroup.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;sudo usermod -a -G kvm ${USER}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Add necessary flags to &lt;code&gt;/etc/wsl2.conf&lt;/code&gt; to their respective sections.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;[boot]
command = /bin/bash -c 'chown -v root:kvm /dev/kvm &amp;amp;&amp;amp; chmod 660 /dev/kvm'

[wsl2]
nestedVirtualization=true
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Restart WSL2 via CMD prompt or Powershell&lt;/p&gt; &lt;pre&gt;&lt;code&gt;wsl --shutdown
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;code&gt;command = /bin/bash -c 'chown -v root:kvm /dev/kvm &amp;amp;&amp;amp; chmod 660 /dev/kvm'&lt;/code&gt; sets &lt;code&gt;/dev/kvm&lt;/code&gt; to &lt;code&gt;kvm&lt;/code&gt; usergroup rather than the default &lt;code&gt;root&lt;/code&gt; usergroup on WSL2 startup.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;nestedVirtualization&lt;/code&gt; flag is only available to Windows 11.&lt;/p&gt; 
&lt;h2&gt;Use-Cases&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/budtmo/docker-android/master/documentations/USE_CASE_BUILD_ANDROID_PROJECT.md"&gt;Build Android project&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/budtmo/docker-android/master/documentations/USE_CASE_APPIUM.md"&gt;UI-Test with Appium&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/budtmo/docker-android/master/documentations/USE_CASE_CONTROL_EMULATOR.md"&gt;Control Android emulator on host machine&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/budtmo/docker-android/master/documentations/USE_CASE_SMS.md"&gt;SMS Simulation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/budtmo/docker-android/master/documentations/USE_CASE_JENKINS.md"&gt;Jenkins&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/budtmo/docker-android/master/documentations/USE_CASE_CLOUD.md"&gt;Deploying on cloud (Azure, AWS, GCP)&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Custom-Configurations&lt;/h2&gt; 
&lt;p&gt;This &lt;a href="https://raw.githubusercontent.com/budtmo/docker-android/master/documentations/CUSTOM_CONFIGURATIONS.md"&gt;document&lt;/a&gt; contains information about configurations that can be used to enable some features, e.g. log-sharing, etc.&lt;/p&gt; 
&lt;h2&gt;Genymotion&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;img id="geny" src="https://raw.githubusercontent.com/budtmo/docker-android/master/images/logo_genymotion_and_dockerandroid.png" /&gt; &lt;/p&gt; 
&lt;p&gt;For you who do not have ressources to maintain the simulator or to buy machines or need different device profiles, you can give a try by using &lt;a href="https://cloud.geny.io/"&gt;Genymotion SAAS&lt;/a&gt;. Docker-Android is &lt;a href="https://www.genymotion.com/blog/partner_tag/docker/"&gt;integrated with Genymotion&lt;/a&gt; on different cloud services, e.g. Genymotion SAAS, AWS, GCP, Alibaba Cloud. Please follow &lt;a href="https://raw.githubusercontent.com/budtmo/docker-android/master/documentations/THIRD_PARTY_GENYMOTION.md"&gt;this document&lt;/a&gt; for more detail.&lt;/p&gt; 
&lt;h2&gt;Emulator Skins&lt;/h2&gt; 
&lt;p&gt;The Emulator skins are taken from &lt;a href="https://developer.android.com/studio"&gt;Android Studio IDE&lt;/a&gt; and &lt;a href="https://developer.samsung.com/"&gt;Samsung Developer Website&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;USERS&lt;/h2&gt; 
&lt;a href="https://lookerstudio.google.com/s/iGaemHJqQvg"&gt; &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/budtmo/docker-android/master/images/docker-android_users.png" alt="docker-android-users" width="800" height="600" /&gt; &lt;/p&gt; &lt;/a&gt; 
&lt;h2&gt;PRO VERSION&lt;/h2&gt; 
&lt;p&gt;Due to high requests for help and to be able to actively maintain the projects, the creator has decided to create docker-android-pro. Docker-Android-Pro is a sponsor based project which mean that the docker image of pro-version can be pulled only by &lt;a href="https://github.com/sponsors/budtmo"&gt;active sponsor&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The differences between normal version and pro version are:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Feature&lt;/th&gt; 
   &lt;th align="left"&gt;Normal&lt;/th&gt; 
   &lt;th align="left"&gt;Pro&lt;/th&gt; 
   &lt;th align="left"&gt;Comment&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;user-behavior-analytics&lt;/td&gt; 
   &lt;td align="left"&gt;Yes&lt;/td&gt; 
   &lt;td align="left"&gt;No&lt;/td&gt; 
   &lt;td align="left"&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;proxy&lt;/td&gt; 
   &lt;td align="left"&gt;No&lt;/td&gt; 
   &lt;td align="left"&gt;Yes&lt;/td&gt; 
   &lt;td align="left"&gt;Set up company proxy on Android emulator on fly&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;language&lt;/td&gt; 
   &lt;td align="left"&gt;No&lt;/td&gt; 
   &lt;td align="left"&gt;Yes&lt;/td&gt; 
   &lt;td align="left"&gt;Set up language on Android emulator on fly&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;Newer Android version&lt;/td&gt; 
   &lt;td align="left"&gt;No&lt;/td&gt; 
   &lt;td align="left"&gt;Yes&lt;/td&gt; 
   &lt;td align="left"&gt;Support other newer Android version e.g. Android 15, Android 16, etc&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;root-privileged&lt;/td&gt; 
   &lt;td align="left"&gt;No&lt;/td&gt; 
   &lt;td align="left"&gt;Yes&lt;/td&gt; 
   &lt;td align="left"&gt;Able to run command with security privileged&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;headless-mode&lt;/td&gt; 
   &lt;td align="left"&gt;No&lt;/td&gt; 
   &lt;td align="left"&gt;Yes&lt;/td&gt; 
   &lt;td align="left"&gt;Save resources by using headless mode&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;Selenium 4.x integration&lt;/td&gt; 
   &lt;td align="left"&gt;No&lt;/td&gt; 
   &lt;td align="left"&gt;Yes&lt;/td&gt; 
   &lt;td align="left"&gt;Running Appium UI-Tests againt one (Selenium Hub) endpoint for Android- and iOS emulator(s) / device(s)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;multiple Android-Simulators&lt;/td&gt; 
   &lt;td align="left"&gt;No&lt;/td&gt; 
   &lt;td align="left"&gt;Yes (soon)&lt;/td&gt; 
   &lt;td align="left"&gt;Save resources by having multiple Android-Simulators on one docker-container&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;Google Play Store&lt;/td&gt; 
   &lt;td align="left"&gt;No&lt;/td&gt; 
   &lt;td align="left"&gt;Yes (soon)&lt;/td&gt; 
   &lt;td align="left"&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;Video Recording&lt;/td&gt; 
   &lt;td align="left"&gt;No&lt;/td&gt; 
   &lt;td align="left"&gt;Yes (soon)&lt;/td&gt; 
   &lt;td align="left"&gt;Helpful for debugging&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;This &lt;a href="https://raw.githubusercontent.com/budtmo/docker-android/master/documentations/DOCKER-ANDROID-PRO.md"&gt;document&lt;/a&gt; contains detail information about how to use docker-android-pro.&lt;/p&gt; 
&lt;h2&gt;LICENSE&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/budtmo/docker-android/master/LICENSE.md"&gt;License&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>tadata-org/fastapi_mcp</title>
      <link>https://github.com/tadata-org/fastapi_mcp</link>
      <description>&lt;p&gt;Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt;&lt;a href="https://github.com/tadata-org/fastapi_mcp"&gt;&lt;img src="https://github.com/user-attachments/assets/7e44e98b-a0ba-4aff-a68a-4ffee3a6189c" alt="fastapi-to-mcp" height="100/" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;span style="font-size: 0.85em; font-weight: normal;"&gt;Built by &lt;a href="https://tadata.com"&gt;Tadata&lt;/a&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;h1 align="center"&gt; FastAPI-MCP &lt;/h1&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://trendshift.io/repositories/14064" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/14064" alt="tadata-org%2Ffastapi_mcp | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;p align="center"&gt;Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://pypi.org/project/fastapi-mcp/"&gt;&lt;img src="https://img.shields.io/pypi/v/fastapi-mcp?color=%2334D058&amp;amp;label=pypi%20package" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/fastapi-mcp/"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/fastapi-mcp.svg?sanitize=true" alt="Python Versions" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/tadata-org/fastapi_mcp/main/#"&gt;&lt;img src="https://img.shields.io/badge/FastAPI-009485.svg?logo=fastapi&amp;amp;logoColor=white" alt="FastAPI" /&gt;&lt;/a&gt; &lt;a href="https://github.com/tadata-org/fastapi_mcp/actions/workflows/ci.yml"&gt;&lt;img src="https://github.com/tadata-org/fastapi_mcp/actions/workflows/ci.yml/badge.svg?sanitize=true" alt="CI" /&gt;&lt;/a&gt; &lt;a href="https://codecov.io/gh/tadata-org/fastapi_mcp"&gt;&lt;img src="https://codecov.io/gh/tadata-org/fastapi_mcp/branch/main/graph/badge.svg?sanitize=true" alt="Coverage" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p align="center"&gt;&lt;a href="https://github.com/tadata-org/fastapi_mcp"&gt;&lt;img src="https://github.com/user-attachments/assets/b205adc6-28c0-4e3c-a68b-9c1a80eb7d0c" alt="fastapi-mcp-usage" height="400" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Authentication&lt;/strong&gt; built in, using your existing FastAPI dependencies!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;FastAPI-native:&lt;/strong&gt; Not just another OpenAPI -&amp;gt; MCP converter&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Zero/Minimal configuration&lt;/strong&gt; required - just point it at your FastAPI app and it works&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Preserving schemas&lt;/strong&gt; of your request models and response models&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Preserve documentation&lt;/strong&gt; of all your endpoints, just as it is in Swagger&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Flexible deployment&lt;/strong&gt; - Mount your MCP server to the same app, or deploy separately&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ASGI transport&lt;/strong&gt; - Uses FastAPI's ASGI interface directly for efficient communication&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Hosted Solution&lt;/h2&gt; 
&lt;p&gt;If you prefer a managed hosted solution check out &lt;a href="https://tadata.com"&gt;tadata.com&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;We recommend using &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt;, a fast Python package installer:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv add fastapi-mcp
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, you can install with pip:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install fastapi-mcp
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Basic Usage&lt;/h2&gt; 
&lt;p&gt;The simplest way to use FastAPI-MCP is to add an MCP server directly to your FastAPI application:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from fastapi import FastAPI
from fastapi_mcp import FastApiMCP

app = FastAPI()

mcp = FastApiMCP(app)

# Mount the MCP server directly to your FastAPI app
mcp.mount()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;That's it! Your auto-generated MCP server is now available at &lt;code&gt;https://app.base.url/mcp&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Documentation, Examples and Advanced Usage&lt;/h2&gt; 
&lt;p&gt;FastAPI-MCP provides &lt;a href="https://fastapi-mcp.tadata.com/"&gt;comprehensive documentation&lt;/a&gt;. Additionaly, check out the &lt;a href="https://raw.githubusercontent.com/tadata-org/fastapi_mcp/main/examples"&gt;examples directory&lt;/a&gt; for code samples demonstrating these features in action.&lt;/p&gt; 
&lt;h2&gt;FastAPI-first Approach&lt;/h2&gt; 
&lt;p&gt;FastAPI-MCP is designed as a native extension of FastAPI, not just a converter that generates MCP tools from your API. This approach offers several key advantages:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Native dependencies&lt;/strong&gt;: Secure your MCP endpoints using familiar FastAPI &lt;code&gt;Depends()&lt;/code&gt; for authentication and authorization&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ASGI transport&lt;/strong&gt;: Communicates directly with your FastAPI app using its ASGI interface, eliminating the need for HTTP calls from the MCP to your API&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Unified infrastructure&lt;/strong&gt;: Your FastAPI app doesn't need to run separately from the MCP server (though &lt;a href="https://fastapi-mcp.tadata.com/advanced/deploy#deploying-separately-from-original-fastapi-app"&gt;separate deployment&lt;/a&gt; is also supported)&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This design philosophy ensures minimum friction when adding MCP capabilities to your existing FastAPI services.&lt;/p&gt; 
&lt;h2&gt;Development and Contributing&lt;/h2&gt; 
&lt;p&gt;Thank you for considering contributing to FastAPI-MCP! We encourage the community to post Issues and create Pull Requests.&lt;/p&gt; 
&lt;p&gt;Before you get started, please see our &lt;a href="https://raw.githubusercontent.com/tadata-org/fastapi_mcp/main/CONTRIBUTING.md"&gt;Contribution Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;p&gt;Join &lt;a href="https://join.slack.com/t/themcparty/shared_invite/zt-30yxr1zdi-2FG~XjBA0xIgYSYuKe7~Xg"&gt;MCParty Slack community&lt;/a&gt; to connect with other MCP enthusiasts, ask questions, and share your experiences with FastAPI-MCP.&lt;/p&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.10+ (Recommended 3.12)&lt;/li&gt; 
 &lt;li&gt;uv&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;MIT License. Copyright (c) 2025 Tadata Inc.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>trailofbits/buttercup</title>
      <link>https://github.com/trailofbits/buttercup</link>
      <description>&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Buttercup Cyber Reasoning System (CRS)&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/trailofbits/buttercup/actions/workflows/tests.yml"&gt;&lt;img src="https://github.com/trailofbits/buttercup/actions/workflows/tests.yml/badge.svg?sanitize=true" alt="Tests" /&gt;&lt;/a&gt; &lt;a href="https://github.com/trailofbits/buttercup/actions/workflows/tests.yml"&gt;&lt;img src="https://github.com/trailofbits/buttercup/actions/workflows/tests.yml/badge.svg?event=schedule" alt="Tests (Nightly)" /&gt;&lt;/a&gt; &lt;a href="https://github.com/trailofbits/buttercup/actions/workflows/integration.yml"&gt;&lt;img src="https://github.com/trailofbits/buttercup/actions/workflows/integration.yml/badge.svg?sanitize=true" alt="Integration" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Buttercup&lt;/strong&gt; is a Cyber Reasoning System (CRS) developed by &lt;strong&gt;Trail of Bits&lt;/strong&gt; for the &lt;strong&gt;DARPA AIxCC (AI Cyber Challenge)&lt;/strong&gt;. Buttercup finds and patches software vulnerabilities in open-source code repositories like &lt;a href="https://github.com/tob-challenges/example-libpng"&gt;example-libpng&lt;/a&gt;. It starts by running an AI/ML-assisted fuzzing campaign (built on oss-fuzz) for the program. When vulnerabilities are found, Buttercup analyzes them and uses a multi-agent AI-driven patcher to repair the vulnerability. &lt;strong&gt;Buttercup&lt;/strong&gt; system consists of several components:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Orchestrator&lt;/strong&gt;: Coordinates the overall task process and manages the workflow&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Seed Generator&lt;/strong&gt;: Creates inputs for vulnerability discovery&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Fuzzer&lt;/strong&gt;: Discovers vulnerabilities through intelligent fuzzing techniques&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Program Model&lt;/strong&gt;: Analyzes code structure and semantics for better understanding&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Patcher&lt;/strong&gt;: Generates and applies security patches to fix vulnerabilities&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;System Requirements&lt;/h2&gt; 
&lt;h3&gt;Minimum Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;CPU:&lt;/strong&gt; 8 cores&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Memory:&lt;/strong&gt; 16 GB RAM&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Storage:&lt;/strong&gt; 100 GB available disk space&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Network:&lt;/strong&gt; Stable internet connection for downloading dependencies&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Buttercup uses third-party AI providers (LLMs from companies like OpenAI, Anthropic and Google), which cost money. Please ensure that you manage per-deployment costs by using the built-in LLM budget setting.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Buttercup works best with access to models from OpenAI &lt;strong&gt;and&lt;/strong&gt; Anthropic, but can be run with at least one API key from one third-party provider (support for Gemini coming soon).&lt;/p&gt; 
&lt;h3&gt;Supported Systems&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Linux x86_64&lt;/strong&gt; (fully supported)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ARM64&lt;/strong&gt; (partial support for upstream Google OSS-Fuzz projects)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Required System Packages&lt;/h3&gt; 
&lt;p&gt;Before setup, ensure you have these packages installed:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Ubuntu/Debian
sudo apt-get update
sudo apt-get install -y make curl git

# RHEL/CentOS/Fedora
sudo yum install -y make curl git
# or
sudo dnf install -y make curl git

# MacOS
brew install make curl git
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Supported Targets&lt;/h3&gt; 
&lt;p&gt;Buttercup works with:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;C source code repositories&lt;/strong&gt; that are OSS-Fuzz compatible&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Java source code repositories&lt;/strong&gt; that are OSS-Fuzz compatible&lt;/li&gt; 
 &lt;li&gt;Projects that build successfully and have existing fuzzing harnesses&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Clone the repository with submodules:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone --recurse-submodules https://github.com/trailofbits/buttercup.git
cd buttercup
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Run automated setup (Recommended)&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;make setup-local
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This script will install all dependencies, configure the environment, and guide you through the setup process.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; If you prefer manual setup, see the &lt;a href="https://raw.githubusercontent.com/trailofbits/buttercup/main/MANUAL_SETUP.md"&gt;Manual Setup Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Start Buttercup locally&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;make deploy-local
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="4"&gt; 
 &lt;li&gt;Verify local deployment:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;make status
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;When a deployment is successful, you should see all pods in "Running" or "Completed" status.&lt;/p&gt; 
&lt;ol start="5"&gt; 
 &lt;li&gt;Send Buttercup a simple task&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; When tasked, Buttercup will start consuming third-party AI resources.&lt;/p&gt; 
&lt;p&gt;This command will make Buttercup pull down an example repo &lt;a href="https://github.com/tob-challenges/example-libpng"&gt;example-libpng&lt;/a&gt; with a known vulnerability. Buttercup will start fuzzing it to find and patch vulnerabilities.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;make send-libpng-task
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="6"&gt; 
 &lt;li&gt;Access Buttercup's web-based GUI&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;make web-ui
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then navigate to &lt;code&gt;http://localhost:31323&lt;/code&gt; in your web browser.&lt;/p&gt; 
&lt;p&gt;In the GUI you can monitor active tasks and see when Buttercup finds bugs and generates patches for them.&lt;/p&gt; 
&lt;ol start="7"&gt; 
 &lt;li&gt;Stop Buttercup&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; This is an important step to ensure Buttercup shuts down and stops consuming third-party AI resources.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;make undeploy
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Accessing Logs&lt;/h2&gt; 
&lt;p&gt;Buttercup includes local SigNoz deployment by default for comprehensive system observability. You can access logs, traces, and metrics through the SigNoz UI:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;make signoz-ui
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then navigate to &lt;code&gt;http://localhost:33301&lt;/code&gt; in your web browser to view:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Distributed traces&lt;/li&gt; 
 &lt;li&gt;Application metrics&lt;/li&gt; 
 &lt;li&gt;Error monitoring&lt;/li&gt; 
 &lt;li&gt;Performance insights&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If you configured LangFuse during setup, you can also monitor LLM usage and costs there.&lt;/p&gt; 
&lt;p&gt;For additional log access methods, see the &lt;a href="https://raw.githubusercontent.com/trailofbits/buttercup/main/QUICK_REFERENCE.md"&gt;Quick Reference Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Additional Resources&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/trailofbits/buttercup/main/QUICK_REFERENCE.md"&gt;Quick Reference Guide&lt;/a&gt; - Common commands and troubleshooting&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/trailofbits/buttercup/main/MANUAL_SETUP.md"&gt;Manual Setup Guide&lt;/a&gt; - Detailed manual installation steps&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/trailofbits/buttercup/main/AKS_DEPLOYMENT.md"&gt;AKS Deployment Guide&lt;/a&gt; - Production deployment on Azure&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/trailofbits/buttercup/main/CONTRIBUTING.md"&gt;Contributing Guidelines&lt;/a&gt; - Development workflow and standards&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/trailofbits/buttercup/main/deployment/README.md"&gt;Deployment Documentation&lt;/a&gt; - Advanced deployment configuration&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/trailofbits/buttercup/main/CUSTOM_CHALLENGES.md"&gt;Writing Custom Challenges&lt;/a&gt; - Custom project configuration and setup&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>coleam00/Archon</title>
      <link>https://github.com/coleam00/Archon</link>
      <description>&lt;p&gt;Beta release of Archon OS - the knowledge and task management backbone for AI coding assistants.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/coleam00/Archon/main/archon-ui-main/public/archon-main-graphic.png" alt="Archon Main Graphic" width="853" height="422" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;em&gt;Power up your AI coding assistants with your own custom knowledge base and task management as an MCP server&lt;/em&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://raw.githubusercontent.com/coleam00/Archon/main/#quick-start"&gt;Quick Start&lt;/a&gt; ‚Ä¢ &lt;a href="https://raw.githubusercontent.com/coleam00/Archon/main/#whats-included"&gt;What's Included&lt;/a&gt; ‚Ä¢ &lt;a href="https://raw.githubusercontent.com/coleam00/Archon/main/#architecture"&gt;Architecture&lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üéØ What is Archon?&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Archon is currently in beta! Expect things to not work 100%, and please feel free to share any feedback and contribute with fixes/new features! Thank you to everyone for all the excitement we have for Archon already, as well as the bug reports, PRs, and discussions. It's a lot for our small team to get through but we're committed to addressing everything and making Archon into the best tool it possibly can be!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Archon is the &lt;strong&gt;command center&lt;/strong&gt; for AI coding assistants. For you, it's a sleek interface to manage knowledge, context, and tasks for your projects. For the AI coding assistant(s), it's a &lt;strong&gt;Model Context Protocol (MCP) server&lt;/strong&gt; to collaborate on and leverage the same knowledge, context, and tasks. Connect Claude Code, Kiro, Cursor, Windsurf, etc. to give your AI agents access to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Your documentation&lt;/strong&gt; (crawled websites, uploaded PDFs/docs)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Smart search capabilities&lt;/strong&gt; with advanced RAG strategies&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Task management&lt;/strong&gt; integrated with your knowledge base&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Real-time updates&lt;/strong&gt; as you add new content and collaborate with your coding assistant on tasks&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Much more&lt;/strong&gt; coming soon to build Archon into an integrated environment for all context engineering&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This new vision for Archon replaces the old one (the agenteer). Archon used to be the AI agent that builds other agents, and now you can use Archon to do that and more.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;It doesn't matter what you're building or if it's a new/existing codebase - Archon's knowledge and task management capabilities will improve the output of &lt;strong&gt;any&lt;/strong&gt; AI driven coding.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;üîó Important Links&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/coleam00/Archon/discussions"&gt;GitHub Discussions&lt;/a&gt;&lt;/strong&gt; - Join the conversation and share ideas about Archon&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/coleam00/Archon/main/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt;&lt;/strong&gt; - How to get involved and contribute to Archon&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://youtu.be/8pRc_s2VQIo"&gt;Introduction Video&lt;/a&gt;&lt;/strong&gt; - Getting Started Guide and Vision for Archon&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://dynamous.ai"&gt;Dynamous AI Mastery&lt;/a&gt;&lt;/strong&gt; - The birthplace of Archon - come join a vibrant community of other early AI adopters all helping each other transform their careers and businesses!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.docker.com/products/docker-desktop/"&gt;Docker Desktop&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://supabase.com/"&gt;Supabase&lt;/a&gt; account (free tier or local Supabase both work)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://platform.openai.com/api-keys"&gt;OpenAI API key&lt;/a&gt; (Gemini and Ollama are supported too!)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Setup Instructions&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Clone Repository&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/coleam00/archon.git
&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cd archon
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Environment Configuration&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cp .env.example .env
# Edit .env and add your Supabase credentials:
# SUPABASE_URL=https://your-project.supabase.co
# SUPABASE_SERVICE_KEY=your-service-key-here
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;NOTE: Supabase introduced a new type of service key but use the legacy one (the longer one).&lt;/p&gt; &lt;p&gt;OPTIONAL: If you want to enable the reranking RAG strategy, uncomment lines 20-22 in &lt;code&gt;python\requirements.server.txt&lt;/code&gt;. This will significantly increase the size of the Archon Server container which is why it's off by default.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Database Setup&lt;/strong&gt;: In your &lt;a href="https://supabase.com/dashboard"&gt;Supabase project&lt;/a&gt; SQL Editor, copy, paste, and execute the contents of &lt;code&gt;migration/complete_setup.sql&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Start Services&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;docker-compose up --build -d
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This starts the core microservices:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Server&lt;/strong&gt;: Core API and business logic (Port: 8181)&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;MCP Server&lt;/strong&gt;: Protocol interface for AI clients (Port: 8051)&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Agents (coming soon!)&lt;/strong&gt;: AI operations and streaming (Port: 8052)&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;UI&lt;/strong&gt;: Web interface (Port: 3737)&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;Ports are configurable in your .env as well!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Configure API Keys&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Open &lt;a href="http://localhost:3737"&gt;http://localhost:3737&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Go to &lt;strong&gt;Settings&lt;/strong&gt; ‚Üí Select your LLM/embedding provider and set the API key (OpenAI is default)&lt;/li&gt; 
   &lt;li&gt;Test by uploading a document or crawling a website&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;üîÑ Database Reset (Start Fresh if Needed)&lt;/h2&gt; 
&lt;p&gt;If you need to completely reset your database and start fresh:&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;‚ö†Ô∏è &lt;strong&gt;Reset Database - This will delete ALL data for Archon!&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Run Reset Script&lt;/strong&gt;: In your Supabase SQL Editor, run the contents of &lt;code&gt;migration/RESET_DB.sql&lt;/code&gt;&lt;/p&gt; &lt;p&gt;‚ö†Ô∏è WARNING: This will delete all Archon specific tables and data! Nothing else will be touched in your DB though.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Rebuild Database&lt;/strong&gt;: After reset, run &lt;code&gt;migration/complete_setup.sql&lt;/code&gt; to create all the tables again.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Restart Services&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;docker-compose up -d
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Reconfigure&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Select your LLM/embedding provider and set the API key again&lt;/li&gt; 
    &lt;li&gt;Re-upload any documents or re-crawl websites&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;The reset script safely removes all tables, functions, triggers, and policies with proper dependency handling.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;‚ö° Quick Test&lt;/h2&gt; 
&lt;p&gt;Once everything is running:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Test Web Crawling&lt;/strong&gt;: Go to &lt;a href="http://localhost:3737"&gt;http://localhost:3737&lt;/a&gt; ‚Üí Knowledge Base ‚Üí "Crawl Website" ‚Üí Enter a doc URL (such as &lt;a href="https://ai.pydantic.dev/llms-full.txt"&gt;https://ai.pydantic.dev/llms-full.txt&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Test Document Upload&lt;/strong&gt;: Knowledge Base ‚Üí Upload a PDF&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Test Projects&lt;/strong&gt;: Projects ‚Üí Create a new project and add tasks&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Integrate with your AI coding assistant&lt;/strong&gt;: MCP Dashboard ‚Üí Copy connection config for your AI coding assistant&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;üìö Documentation&lt;/h2&gt; 
&lt;h3&gt;Core Services&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Service&lt;/th&gt; 
   &lt;th&gt;Container Name&lt;/th&gt; 
   &lt;th&gt;Default URL&lt;/th&gt; 
   &lt;th&gt;Purpose&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Web Interface&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;archon-ui&lt;/td&gt; 
   &lt;td&gt;&lt;a href="http://localhost:3737"&gt;http://localhost:3737&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Main dashboard and controls&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;API Service&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;archon-server&lt;/td&gt; 
   &lt;td&gt;&lt;a href="http://localhost:8181"&gt;http://localhost:8181&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Web crawling, document processing&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;MCP Server&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;archon-mcp&lt;/td&gt; 
   &lt;td&gt;&lt;a href="http://localhost:8051"&gt;http://localhost:8051&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Model Context Protocol interface&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Agents Service&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;archon-agents&lt;/td&gt; 
   &lt;td&gt;&lt;a href="http://localhost:8052"&gt;http://localhost:8052&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;AI/ML operations, reranking&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;What's Included&lt;/h2&gt; 
&lt;h3&gt;üß† Knowledge Management&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Smart Web Crawling&lt;/strong&gt;: Automatically detects and crawls entire documentation sites, sitemaps, and individual pages&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Document Processing&lt;/strong&gt;: Upload and process PDFs, Word docs, markdown files, and text documents with intelligent chunking&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Code Example Extraction&lt;/strong&gt;: Automatically identifies and indexes code examples from documentation for enhanced search&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Vector Search&lt;/strong&gt;: Advanced semantic search with contextual embeddings for precise knowledge retrieval&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Source Management&lt;/strong&gt;: Organize knowledge by source, type, and tags for easy filtering&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ü§ñ AI Integration&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Model Context Protocol (MCP)&lt;/strong&gt;: Connect any MCP-compatible client (Claude Code, Cursor, even non-AI coding assistants like Claude Desktop)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;10 MCP Tools&lt;/strong&gt;: Comprehensive yet simple set of tools for RAG queries, task management, and project operations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-LLM Support&lt;/strong&gt;: Works with OpenAI, Ollama, and Google Gemini models&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;RAG Strategies&lt;/strong&gt;: Hybrid search, contextual embeddings, and result reranking for optimal AI responses&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Real-time Streaming&lt;/strong&gt;: Live responses from AI agents with progress tracking&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üìã Project &amp;amp; Task Management&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Hierarchical Projects&lt;/strong&gt;: Organize work with projects, features, and tasks in a structured workflow&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;AI-Assisted Creation&lt;/strong&gt;: Generate project requirements and tasks using integrated AI agents&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Document Management&lt;/strong&gt;: Version-controlled documents with collaborative editing capabilities&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Progress Tracking&lt;/strong&gt;: Real-time updates and status management across all project activities&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üîÑ Real-time Collaboration&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;WebSocket Updates&lt;/strong&gt;: Live progress tracking for crawling, processing, and AI operations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-user Support&lt;/strong&gt;: Collaborative knowledge building and project management&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Background Processing&lt;/strong&gt;: Asynchronous operations that don't block the user interface&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Health Monitoring&lt;/strong&gt;: Built-in service health checks and automatic reconnection&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Architecture&lt;/h2&gt; 
&lt;h3&gt;Microservices Structure&lt;/h3&gt; 
&lt;p&gt;Archon uses true microservices architecture with clear separation of concerns:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Frontend UI   ‚îÇ    ‚îÇ  Server (API)   ‚îÇ    ‚îÇ   MCP Server    ‚îÇ    ‚îÇ Agents Service  ‚îÇ
‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ
‚îÇ  React + Vite   ‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇ    FastAPI +    ‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇ    Lightweight  ‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇ   PydanticAI    ‚îÇ
‚îÇ  Port 3737      ‚îÇ    ‚îÇ    SocketIO     ‚îÇ    ‚îÇ    HTTP Wrapper ‚îÇ    ‚îÇ   Port 8052     ‚îÇ
‚îÇ                 ‚îÇ    ‚îÇ    Port 8181    ‚îÇ    ‚îÇ    Port 8051    ‚îÇ    ‚îÇ                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                        ‚îÇ                        ‚îÇ                        ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                  ‚îÇ                        ‚îÇ
                         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚îÇ
                         ‚îÇ    Database     ‚îÇ               ‚îÇ
                         ‚îÇ                 ‚îÇ               ‚îÇ
                         ‚îÇ    Supabase     ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ    PostgreSQL   ‚îÇ
                         ‚îÇ    PGVector     ‚îÇ
                         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Service Responsibilities&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Service&lt;/th&gt; 
   &lt;th&gt;Location&lt;/th&gt; 
   &lt;th&gt;Purpose&lt;/th&gt; 
   &lt;th&gt;Key Features&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Frontend&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;archon-ui-main/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Web interface and dashboard&lt;/td&gt; 
   &lt;td&gt;React, TypeScript, TailwindCSS, Socket.IO client&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Server&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;python/src/server/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Core business logic and APIs&lt;/td&gt; 
   &lt;td&gt;FastAPI, service layer, Socket.IO broadcasts, all ML/AI operations&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;MCP Server&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;python/src/mcp/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;MCP protocol interface&lt;/td&gt; 
   &lt;td&gt;Lightweight HTTP wrapper, 10 MCP tools, session management&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Agents&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;python/src/agents/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;PydanticAI agent hosting&lt;/td&gt; 
   &lt;td&gt;Document and RAG agents, streaming responses&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Communication Patterns&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;HTTP-based&lt;/strong&gt;: All inter-service communication uses HTTP APIs&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Socket.IO&lt;/strong&gt;: Real-time updates from Server to Frontend&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MCP Protocol&lt;/strong&gt;: AI clients connect to MCP Server via SSE or stdio&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;No Direct Imports&lt;/strong&gt;: Services are truly independent with no shared code dependencies&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Key Architectural Benefits&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Lightweight Containers&lt;/strong&gt;: Each service contains only required dependencies&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Independent Scaling&lt;/strong&gt;: Services can be scaled independently based on load&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Development Flexibility&lt;/strong&gt;: Teams can work on different services without conflicts&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Technology Diversity&lt;/strong&gt;: Each service uses the best tools for its specific purpose&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üîß Configuring Custom Ports &amp;amp; Hostname&lt;/h2&gt; 
&lt;p&gt;By default, Archon services run on the following ports:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Archon-UI&lt;/strong&gt;: 3737&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Archon-Server&lt;/strong&gt;: 8181&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Archon-MCP&lt;/strong&gt;: 8051&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Archon-Agents&lt;/strong&gt;: 8052&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Archon-Docs&lt;/strong&gt;: 3838 (optional)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Changing Ports&lt;/h3&gt; 
&lt;p&gt;To use custom ports, add these variables to your &lt;code&gt;.env&lt;/code&gt; file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Service Ports Configuration
ARCHON_UI_PORT=3737
ARCHON_SERVER_PORT=8181
ARCHON_MCP_PORT=8051
ARCHON_AGENTS_PORT=8052
ARCHON_DOCS_PORT=3838
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Example: Running on different ports:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;ARCHON_SERVER_PORT=8282
ARCHON_MCP_PORT=8151
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Configuring Hostname&lt;/h3&gt; 
&lt;p&gt;By default, Archon uses &lt;code&gt;localhost&lt;/code&gt; as the hostname. You can configure a custom hostname or IP address by setting the &lt;code&gt;HOST&lt;/code&gt; variable in your &lt;code&gt;.env&lt;/code&gt; file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Hostname Configuration
HOST=localhost  # Default

# Examples of custom hostnames:
HOST=192.168.1.100     # Use specific IP address
HOST=archon.local      # Use custom domain
HOST=myserver.com      # Use public domain
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This is useful when:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Running Archon on a different machine and accessing it remotely&lt;/li&gt; 
 &lt;li&gt;Using a custom domain name for your installation&lt;/li&gt; 
 &lt;li&gt;Deploying in a network environment where &lt;code&gt;localhost&lt;/code&gt; isn't accessible&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;After changing hostname or ports:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Restart Docker containers: &lt;code&gt;docker-compose down &amp;amp;&amp;amp; docker-compose up -d&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Access the UI at: &lt;code&gt;http://${HOST}:${ARCHON_UI_PORT}&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Update your AI client configuration with the new hostname and MCP port&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;üîß Development&lt;/h2&gt; 
&lt;p&gt;For development with hot reload:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Backend services (with auto-reload)
docker-compose up archon-server archon-mcp archon-agents --build

# Frontend (with hot reload)
cd archon-ui-main &amp;amp;&amp;amp; npm run dev

# Documentation (with hot reload)
cd docs &amp;amp;&amp;amp; npm start
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The backend services are configured with &lt;code&gt;--reload&lt;/code&gt; flag in their uvicorn commands and have source code mounted as volumes for automatic hot reloading when you make changes.&lt;/p&gt; 
&lt;h2&gt;üìÑ License&lt;/h2&gt; 
&lt;p&gt;Archon Community License (ACL) v1.2 - see &lt;a href="https://raw.githubusercontent.com/coleam00/Archon/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: Archon is free, open, and hackable. Run it, fork it, share it - just don't sell it as-a-service without permission.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>