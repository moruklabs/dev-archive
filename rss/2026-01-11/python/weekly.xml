<rss version="2.0">
  <channel>
    <title>GitHub Python Weekly Trending</title>
    <description>Weekly Trending of Python in GitHub</description>
    <pubDate>Sat, 10 Jan 2026 01:46:13 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>OpenBMB/ChatDev</title>
      <link>https://github.com/OpenBMB/ChatDev</link>
      <description>&lt;p&gt;ChatDev 2.0: Dev All through LLM-powered Multi-Agent Collaboration&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ChatDev 2.0 - DevAll&lt;/h1&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/frontend/public/media/logo.png" alt="DevAll Logo" width="500" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;strong&gt;A Zero-Code Multi-Agent Platform for Developing Everything&lt;/strong&gt; &lt;/p&gt; 
&lt;p align="center"&gt; „Äê&lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/README.md"&gt;English&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/README-zh.md"&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt;„Äë &lt;/p&gt; 
&lt;p align="center"&gt; „Äêüìö &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/#developers"&gt;Developers&lt;/a&gt; | üë• &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/#primary-contributors"&gt;Contributors&lt;/a&gt;ÔΩú‚≠êÔ∏è &lt;a href="https://github.com/OpenBMB/ChatDev/tree/chatdev1.0"&gt;ChatDev 1.0 (Legacy)&lt;/a&gt;„Äë &lt;/p&gt; 
&lt;h2&gt;üìñ Overview&lt;/h2&gt; 
&lt;p&gt;ChatDev has evolved from a specialized software development multi-agent system into a comprehensive multi-agent orchestration platform.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenBMB/ChatDev/tree/main"&gt;&lt;strong&gt;ChatDev 2.0 (DevAll)&lt;/strong&gt;&lt;/a&gt; is a &lt;strong&gt;Zero-Code Multi-Agent Platform&lt;/strong&gt; for "Developing Everything". It empowers users to rapidly build and execute customized multi-agent systems through simple configuration. No coding is required‚Äîusers can define agents, workflows, and tasks to orchestrate complex scenarios such as data visualization, 3D generation, and deep research.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenBMB/ChatDev/tree/chatdev1.0"&gt;&lt;strong&gt;ChatDev 1.0 (Legacy)&lt;/strong&gt;&lt;/a&gt; operates as a &lt;strong&gt;Virtual Software Company&lt;/strong&gt;. It utilizes various intelligent agents (e.g., CEO, CTO, Programmer) participating in specialized functional seminars to automate the entire software development life cycle‚Äîincluding designing, coding, testing, and documenting. It serves as the foundational paradigm for communicative agent collaboration.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üéâ News&lt;/h2&gt; 
&lt;p&gt;‚Ä¢ &lt;strong&gt;Jan 07, 2026: üöÄ We are excited to announce the official release of ChatDev 2.0 (DevAll)!&lt;/strong&gt; This version introduces a zero-code multi-agent orchestration platform. The classic ChatDev (v1.x) has been moved to the &lt;a href="https://github.com/OpenBMB/ChatDev/tree/chatdev1.0"&gt;&lt;code&gt;chatdev1.0&lt;/code&gt;&lt;/a&gt; branch for maintenance. More details about ChatDev 2.0 can be found on &lt;a href="https://x.com/OpenBMB/status/2008916790399701335"&gt;our official post&lt;/a&gt;.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Old News&lt;/summary&gt; 
 &lt;p&gt;‚Ä¢Sep 24, 2025: üéâ Our paper &lt;a href="https://arxiv.org/abs/2505.19591"&gt;Multi-Agent Collaboration via Evolving Orchestration&lt;/a&gt; has been accepted to NeurIPS 2025. The implementation is available in the &lt;code&gt;puppeteer&lt;/code&gt; branch of this repository.&lt;/p&gt; 
 &lt;p&gt;‚Ä¢May 26, 2025: üéâ We propose a novel puppeteer-style paradigm for multi-agent collaboration among large language model based agents. By leveraging a learnable central orchestrator optimized with reinforcement learning, our method dynamically activates and sequences agents to construct efficient, context-aware reasoning paths. This approach not only improves reasoning quality but also reduces computational costs, enabling scalable and adaptable multi-agent cooperation in complex tasks. See our paper in &lt;a href="https://arxiv.org/abs/2505.19591"&gt;Multi-Agent Collaboration via Evolving Orchestration&lt;/a&gt;.&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/puppeteer.png" width="800" /&gt; &lt;/p&gt; 
 &lt;p&gt;‚Ä¢June 25, 2024: üéâTo foster development in LLM-powered multi-agent collaborationü§ñü§ñ and related fields, the ChatDev team has curated a collection of seminal papersüìÑ presented in a &lt;a href="https://github.com/OpenBMB/ChatDev/tree/main/MultiAgentEbook"&gt;open-source&lt;/a&gt; interactive e-booküìö format. Now you can explore the latest advancements on the &lt;a href="https://thinkwee.top/multiagent_ebook"&gt;Ebook Website&lt;/a&gt; and download the &lt;a href="https://github.com/OpenBMB/ChatDev/raw/main/MultiAgentEbook/papers.csv"&gt;paper list&lt;/a&gt;.&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/ebook.png" width="800" /&gt; &lt;/p&gt; 
 &lt;p&gt;‚Ä¢June 12, 2024: We introduced Multi-Agent Collaboration Networks (MacNet) üéâ, which utilize directed acyclic graphs to facilitate effective task-oriented collaboration among agents through linguistic interactions ü§ñü§ñ. MacNet supports co-operation across various topologies and among more than a thousand agents without exceeding context limits. More versatile and scalable, MacNet can be considered as a more advanced version of ChatDev's chain-shaped topology. Our preprint paper is available at &lt;a href="https://arxiv.org/abs/2406.07155"&gt;https://arxiv.org/abs/2406.07155&lt;/a&gt;. This technique has been incorporated into the &lt;a href="https://github.com/OpenBMB/ChatDev/tree/macnet"&gt;macnet&lt;/a&gt; branch, enhancing support for diverse organizational structures and offering richer solutions beyond software development (e.g., logical reasoning, data analysis, story generation, and more).&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/macnet.png" width="500" /&gt; &lt;/p&gt; 
 &lt;p&gt;‚Ä¢ May 07, 2024, we introduced "Iterative Experience Refinement" (IER), a novel method where instructor and assistant agents enhance shortcut-oriented experiences to efficiently adapt to new tasks. This approach encompasses experience acquisition, utilization, propagation and elimination across a series of tasks and making the pricess shorter and efficient. Our preprint paper is available at &lt;a href="https://arxiv.org/abs/2405.04219"&gt;https://arxiv.org/abs/2405.04219&lt;/a&gt;, and this technique will soon be incorporated into ChatDev.&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/ier.png" width="220" /&gt; &lt;/p&gt; 
 &lt;p&gt;‚Ä¢ January 25, 2024: We have integrated Experiential Co-Learning Module into ChatDev. Please see the &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/wiki.md#co-tracking"&gt;Experiential Co-Learning Guide&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;‚Ä¢ December 28, 2023: We present Experiential Co-Learning, an innovative approach where instructor and assistant agents accumulate shortcut-oriented experiences to effectively solve new tasks, reducing repetitive errors and enhancing efficiency. Check out our preprint paper at &lt;a href="https://arxiv.org/abs/2312.17025"&gt;https://arxiv.org/abs/2312.17025&lt;/a&gt; and this technique will soon be integrated into ChatDev.&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/ecl.png" width="860" /&gt; &lt;/p&gt; ‚Ä¢ November 15, 2023: We launched ChatDev as a SaaS platform that enables software developers and innovative entrepreneurs to build software efficiently at a very low cost and remove the barrier to entry. Try it out at https://chatdev.modelbest.cn/. 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/saas.png" width="560" /&gt; &lt;/p&gt; 
 &lt;p&gt;‚Ä¢ November 2, 2023: ChatDev is now supported with a new feature: incremental development, which allows agents to develop upon existing codes. Try &lt;code&gt;--config "incremental" --path "[source_code_directory_path]"&lt;/code&gt; to start it.&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/increment.png" width="700" /&gt; &lt;/p&gt; 
 &lt;p&gt;‚Ä¢ October 26, 2023: ChatDev is now supported with Docker for safe execution (thanks to contribution from &lt;a href="https://github.com/ManindraDeMel"&gt;ManindraDeMel&lt;/a&gt;). Please see &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/wiki.md#docker-start"&gt;Docker Start Guide&lt;/a&gt;.&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/docker.png" width="400" /&gt; &lt;/p&gt; 
 &lt;p&gt;‚Ä¢ September 25, 2023: The &lt;strong&gt;Git&lt;/strong&gt; mode is now available, enabling the programmer &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/visualizer/static/figures/programmer.png" height="20" /&gt; to utilize Git for version control. To enable this feature, simply set &lt;code&gt;"git_management"&lt;/code&gt; to &lt;code&gt;"True"&lt;/code&gt; in &lt;code&gt;ChatChainConfig.json&lt;/code&gt;. See &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/wiki.md#git-mode"&gt;guide&lt;/a&gt;.&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/github.png" width="600" /&gt; &lt;/p&gt; 
 &lt;p&gt;‚Ä¢ September 20, 2023: The &lt;strong&gt;Human-Agent-Interaction&lt;/strong&gt; mode is now available! You can get involved with the ChatDev team by playing the role of reviewer &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/visualizer/static/figures/reviewer.png" height="20" /&gt; and making suggestions to the programmer &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/visualizer/static/figures/programmer.png" height="20" /&gt;; try &lt;code&gt;python3 run.py --task [description_of_your_idea] --config "Human"&lt;/code&gt;. See &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/wiki.md#human-agent-interaction"&gt;guide&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/WareHouse/Gomoku_HumanAgentInteraction_20230920135038"&gt;example&lt;/a&gt;.&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/Human_intro.png" width="600" /&gt; &lt;/p&gt; 
 &lt;p&gt;‚Ä¢ September 1, 2023: The &lt;strong&gt;Art&lt;/strong&gt; mode is available now! You can activate the designer agent &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/visualizer/static/figures/designer.png" height="20" /&gt; to generate images used in the software; try &lt;code&gt;python3 run.py --task [description_of_your_idea] --config "Art"&lt;/code&gt;. See &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/wiki.md#art"&gt;guide&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/WareHouse/gomokugameArtExample_THUNLP_20230831122822"&gt;example&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;‚Ä¢ August 28, 2023: The system is publicly available.&lt;/p&gt; 
 &lt;p&gt;‚Ä¢ August 17, 2023: The v1.0.0 version was ready for release.&lt;/p&gt; 
 &lt;p&gt;‚Ä¢ July 30, 2023: Users can customize ChatChain, Phasea and Role settings. Additionally, both online Log mode and replay mode are now supported.&lt;/p&gt; 
 &lt;p&gt;‚Ä¢ July 16, 2023: The &lt;a href="https://arxiv.org/abs/2307.07924"&gt;preprint paper&lt;/a&gt; associated with this project was published.&lt;/p&gt; 
 &lt;p&gt;‚Ä¢ June 30, 2023: The initial version of the ChatDev repository was released.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;üöÄ Quick Start&lt;/h2&gt; 
&lt;h3&gt;üìã Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;OS&lt;/strong&gt;: macOS / Linux / WSL / Windows&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Python&lt;/strong&gt;: 3.12+&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Node.js&lt;/strong&gt;: 18+&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Package Manager&lt;/strong&gt;: &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üì¶ Installation&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Backend Dependencies&lt;/strong&gt; (Python managed by &lt;code&gt;uv&lt;/code&gt;):&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;uv sync
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Frontend Dependencies&lt;/strong&gt; (Vite + Vue 3):&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cd frontend &amp;amp;&amp;amp; npm install
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;‚ö°Ô∏è Run the Application&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Start Backend&lt;/strong&gt; :&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Run from the project root
uv run python server_main.py --port 6400 --reload
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Start Frontend&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cd frontend
VITE_API_BASE_URL=http://localhost:6400 npm run dev
&lt;/code&gt;&lt;/pre&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;Then access the Web Console at &lt;strong&gt;&lt;a href="http://localhost:5173"&gt;http://localhost:5173&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;üîë Configuration&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Environment Variables&lt;/strong&gt;: Create a &lt;code&gt;.env&lt;/code&gt; file in the project root.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Model Keys&lt;/strong&gt;: Set &lt;code&gt;API_KEY&lt;/code&gt; and &lt;code&gt;BASE_URL&lt;/code&gt; in &lt;code&gt;.env&lt;/code&gt; for your LLM provider.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;YAML placeholders&lt;/strong&gt;: Use &lt;code&gt;${VAR}&lt;/code&gt;Ôºàe.g., &lt;code&gt;${API_KEY}&lt;/code&gt;Ôºâin configuration files to reference these variables.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üí° How to Use&lt;/h2&gt; 
&lt;h3&gt;üñ•Ô∏è Web Console&lt;/h3&gt; 
&lt;p&gt;The DevAll interface provides a seamless experience for both construction and execution&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Tutorial&lt;/strong&gt;: Comprehensive step-by-step guides and documentation integrated directly into the platform to help you get started quickly.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/tutorial-en.png" /&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Workflow&lt;/strong&gt;: A visual canvas to design your multi-agent systems. Configure node parameters, define context flows, and orchestrate complex agent interactions with drag-and-drop ease.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/workflow.gif" /&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Launch&lt;/strong&gt;: Initiate workflows, monitor real-time logs, inspect intermediate artifacts, and provide human-in-the-loop feedback.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/launch.gif" /&gt; 
&lt;h3&gt;üß∞ Python SDK&lt;/h3&gt; 
&lt;p&gt;For automation and batch processing, use our lightweight Python SDK to execute workflows programmatically and retrieve results directly.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from runtime.sdk import run_workflow

# Execute a workflow and get the final node message
result = run_workflow(
    yaml_file="yaml_instance/demo.yaml",
    task_prompt="Summarize the attached document in one sentence.",
    attachments=["/path/to/document.pdf"],
    variables={"API_KEY": "sk-xxxx"} # Override .env variables if needed
)

if result.final_message:
    print(f"Output: {result.final_message.text_content()}")
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;a id="developers"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;‚öôÔ∏è For Developers&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;For secondary development and extensions, please proceed with this section.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Extend DevAll with new nodes, providers, and tools. The project is organized into a modular structure:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Core Systems&lt;/strong&gt;: &lt;code&gt;server/&lt;/code&gt; hosts the FastAPI backend, while &lt;code&gt;runtime/&lt;/code&gt; manages agent abstraction and tool execution.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Orchestration&lt;/strong&gt;: &lt;code&gt;workflow/&lt;/code&gt; handles the multi-agent logic, driven by configurations in &lt;code&gt;entity/&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Frontend&lt;/strong&gt;: &lt;code&gt;frontend/&lt;/code&gt; contains the Vue 3 Web Console.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Extensibility&lt;/strong&gt;: &lt;code&gt;functions/&lt;/code&gt; is the place for custom Python tools.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Relevant reference documentation:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Getting Started&lt;/strong&gt;: &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/docs/user_guide/en/index.md"&gt;Start Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Core Modules&lt;/strong&gt;: &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/docs/user_guide/en/workflow_authoring.md"&gt;Workflow Authoring&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/docs/user_guide/en/modules/memory.md"&gt;Memory&lt;/a&gt;, and &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/docs/user_guide/en/modules/tooling/index.md"&gt;Tooling&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üåü Featured Workflows&lt;/h2&gt; 
&lt;p&gt;We provide robust, out-of-the-box templates for common scenarios. All runnable workflow configs are located in &lt;code&gt;yaml_instance/&lt;/code&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Demos&lt;/strong&gt;: Files named &lt;code&gt;demo_*.yaml&lt;/code&gt; showcase specific features or modules.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Implementations&lt;/strong&gt;: Files named directly (e.g., &lt;code&gt;ChatDev_v1.yaml&lt;/code&gt;) are full in-house or recreated workflows. As follows:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üìã Workflow Collection&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Category&lt;/th&gt; 
   &lt;th align="left"&gt;Workflow&lt;/th&gt; 
   &lt;th align="left"&gt;Case&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;üìà Data Visualization&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;data_visualization_basic.yaml&lt;/code&gt;&lt;br /&gt;&lt;code&gt;data_visualization_enhanced.yaml&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/cases/data_analysis/data_analysis.gif" width="100%" /&gt;&lt;br /&gt;Prompt: &lt;em&gt;"Create 4‚Äì6 high-quality PNG charts for my large real-estate transactions dataset."&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;üõ†Ô∏è 3D Generation&lt;/strong&gt;&lt;br /&gt;&lt;em&gt;(Requires &lt;a href="https://www.blender.org/"&gt;Blender&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/ahujasid/blender-mcp"&gt;blender-mcp&lt;/a&gt;)&lt;/em&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;blender_3d_builder_simple.yaml&lt;/code&gt;&lt;br /&gt;&lt;code&gt;blender_3d_builder_hub.yaml&lt;/code&gt;&lt;br /&gt;&lt;code&gt;blender_scientific_illustration.yaml&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/cases/3d_generation/3d.gif" width="100%" /&gt;&lt;br /&gt;Prompt: &lt;em&gt;"Please build a Christmas tree."&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;üéÆ Game Dev&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;GameDev_v1.yaml&lt;/code&gt;&lt;br /&gt;&lt;code&gt;ChatDev_v1.yaml&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/cases/game_development/game.gif" width="100%" /&gt;&lt;br /&gt;Prompt: &lt;em&gt;"Please help me design and develop a Tank Battle game."&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;üìö Deep Research&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;deep_research_v1.yaml&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/cases/deep_research/deep_research.gif" width="85%" /&gt;&lt;br /&gt;Prompt: &lt;em&gt;"Research about recent advances in the field of LLM-based agent RL"&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;üéì Teach Video&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;teach_video.yaml&lt;/code&gt; (Please run command &lt;code&gt;uv add manim&lt;/code&gt; before running this workflow)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/cases/video_generation/video.gif" width="140%" /&gt;&lt;br /&gt;Prompt: &lt;em&gt;"ËÆ≤‰∏Ä‰∏ã‰ªÄ‰πàÊòØÂá∏‰ºòÂåñ"&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h3&gt;üí° Usage Guide&lt;/h3&gt; 
&lt;p&gt;For those implementations, you can use the &lt;strong&gt;Launch&lt;/strong&gt; tab to execute them.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Select&lt;/strong&gt;: Choose a workflow in the &lt;strong&gt;Launch&lt;/strong&gt; tab.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt;: Upload necessary files (e.g., &lt;code&gt;.csv&lt;/code&gt; for data analysis) if required.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Prompt&lt;/strong&gt;: Enter your request (e.g., &lt;em&gt;"Visualize the sales trends"&lt;/em&gt; or &lt;em&gt;"Design a snake game"&lt;/em&gt;).&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions from the community! Whether you're fixing bugs, adding new workflow templates, or sharing high-quality cases/artifacts produced by DevAll, your help is much appreciated. Feel free to contribute by submitting &lt;strong&gt;Issues&lt;/strong&gt; or &lt;strong&gt;Pull Requests&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;By contributing to DevAll, you'll be recognized in our &lt;strong&gt;Contributors&lt;/strong&gt; list below. Check out our &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/#developers"&gt;Developer Guide&lt;/a&gt; to get started!&lt;/p&gt; 
&lt;h3&gt;üë• Contributors&lt;/h3&gt; 
&lt;h4&gt;Primary Contributors&lt;/h4&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/NA-Wen"&gt;&lt;img src="https://github.com/NA-Wen.png?size=100" width="64px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;NA-Wen&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/zxrys"&gt;&lt;img src="https://github.com/zxrys.png?size=100" width="64px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;zxrys&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/swugi"&gt;&lt;img src="https://github.com/swugi.png?size=100" width="64px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;swugi&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/huatl98"&gt;&lt;img src="https://github.com/huatl98.png?size=100" width="64px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;huatl98&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h4&gt;Contributors&lt;/h4&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/shiowen"&gt;&lt;img src="https://github.com/shiowen.png?size=100" width="64px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;shiowen&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/kilo2127"&gt;&lt;img src="https://github.com/kilo2127.png?size=100" width="64px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;kilo2127&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/AckerlyLau"&gt;&lt;img src="https://github.com/AckerlyLau.png?size=100" width="64px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;AckerlyLau&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;ü§ù Acknowledgments&lt;/h2&gt; 
&lt;p&gt;&lt;a href="http://nlp.csai.tsinghua.edu.cn/"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/thunlp.png" height="50pt" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp; &lt;a href="https://modelbest.cn/"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/modelbest.png" height="50pt" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp; &lt;a href="https://github.com/OpenBMB/AgentVerse/"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/agentverse.png" height="50pt" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp; &lt;a href="https://github.com/OpenBMB/RepoAgent"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/repoagent.png" height="50pt" /&gt;&lt;/a&gt; &lt;a href="https://app.commanddash.io/agent?github=https://github.com/OpenBMB/ChatDev"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/CommandDash.png" height="50pt" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/www.teachmaster.cn"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/teachmaster.png" height="50pt" /&gt;&lt;/a&gt; &lt;a href="https://github.com/OpenBMB/AppCopilot"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/appcopilot.png" height="50pt" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üîé Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;@article{chatdev,
    title = {ChatDev: Communicative Agents for Software Development},
    author = {Chen Qian and Wei Liu and Hongzhang Liu and Nuo Chen and Yufan Dang and Jiahao Li and Cheng Yang and Weize Chen and Yusheng Su and Xin Cong and Juyuan Xu and Dahai Li and Zhiyuan Liu and Maosong Sun},
    journal = {arXiv preprint arXiv:2307.07924},
    url = {https://arxiv.org/abs/2307.07924},
    year = {2023}
}

@article{colearning,
    title = {Experiential Co-Learning of Software-Developing Agents},
    author = {Chen Qian and Yufan Dang and Jiahao Li and Wei Liu and Zihao Xie and Yifei Wang and Weize Chen and Cheng Yang and Xin Cong and Xiaoyin Che and Zhiyuan Liu and Maosong Sun},
    journal = {arXiv preprint arXiv:2312.17025},
    url = {https://arxiv.org/abs/2312.17025},
    year = {2023}
}

@article{macnet,
    title={Scaling Large-Language-Model-based Multi-Agent Collaboration},
    author={Chen Qian and Zihao Xie and Yifei Wang and Wei Liu and Yufan Dang and Zhuoyun Du and Weize Chen and Cheng Yang and Zhiyuan Liu and Maosong Sun}
    journal={arXiv preprint arXiv:2406.07155},
    url = {https://arxiv.org/abs/2406.07155},
    year={2024}
}

@article{iagents,
    title={Autonomous Agents for Collaborative Task under Information Asymmetry},
    author={Wei Liu and Chenxi Wang and Yifei Wang and Zihao Xie and Rennai Qiu and Yufan Dnag and Zhuoyun Du and Weize Chen and Cheng Yang and Chen Qian},
    journal={arXiv preprint arXiv:2406.14928},
    url = {https://arxiv.org/abs/2406.14928},
    year={2024}
}

@article{puppeteer,
      title={Multi-Agent Collaboration via Evolving Orchestration}, 
      author={Yufan Dang and Chen Qian and Xueheng Luo and Jingru Fan and Zihao Xie and Ruijie Shi and Weize Chen and Cheng Yang and Xiaoyin Che and Ye Tian and Xuantang Xiong and Lei Han and Zhiyuan Liu and Maosong Sun},
      journal={arXiv preprint arXiv:2505.19591},
      url={https://arxiv.org/abs/2505.19591},
      year={2025}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;üì¨ Contact&lt;/h2&gt; 
&lt;p&gt;If you have any questions, feedback, or would like to get in touch, please feel free to reach out to us via email at &lt;a href="mailto:qianc62@gmail.com"&gt;qianc62@gmail.com&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>microsoft/BitNet</title>
      <link>https://github.com/microsoft/BitNet</link>
      <description>&lt;p&gt;Official inference framework for 1-bit LLMs&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;bitnet.cpp&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://opensource.org/licenses/MIT"&gt;&lt;img src="https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true" alt="License: MIT" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/badge/version-1.0-blue" alt="version" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/microsoft/BitNet-b1.58-2B-4T"&gt;&lt;img src="https://raw.githubusercontent.com/microsoft/BitNet/main/assets/header_model_release.png" alt="BitNet Model on Hugging Face" width="800" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Try it out via this &lt;a href="https://bitnet-demo.azurewebsites.net/"&gt;demo&lt;/a&gt;, or build and run it on your own &lt;a href="https://github.com/microsoft/BitNet?tab=readme-ov-file#build-from-source"&gt;CPU&lt;/a&gt; or &lt;a href="https://github.com/microsoft/BitNet/raw/main/gpu/README.md"&gt;GPU&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;bitnet.cpp is the official inference framework for 1-bit LLMs (e.g., BitNet b1.58). It offers a suite of optimized kernels, that support &lt;strong&gt;fast&lt;/strong&gt; and &lt;strong&gt;lossless&lt;/strong&gt; inference of 1.58-bit models on CPU and GPU (NPU support will coming next).&lt;/p&gt; 
&lt;p&gt;The first release of bitnet.cpp is to support inference on CPUs. bitnet.cpp achieves speedups of &lt;strong&gt;1.37x&lt;/strong&gt; to &lt;strong&gt;5.07x&lt;/strong&gt; on ARM CPUs, with larger models experiencing greater performance gains. Additionally, it reduces energy consumption by &lt;strong&gt;55.4%&lt;/strong&gt; to &lt;strong&gt;70.0%&lt;/strong&gt;, further boosting overall efficiency. On x86 CPUs, speedups range from &lt;strong&gt;2.37x&lt;/strong&gt; to &lt;strong&gt;6.17x&lt;/strong&gt; with energy reductions between &lt;strong&gt;71.9%&lt;/strong&gt; to &lt;strong&gt;82.2%&lt;/strong&gt;. Furthermore, bitnet.cpp can run a 100B BitNet b1.58 model on a single CPU, achieving speeds comparable to human reading (5-7 tokens per second), significantly enhancing the potential for running LLMs on local devices. Please refer to the &lt;a href="https://arxiv.org/abs/2410.16144"&gt;technical report&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/microsoft/BitNet/main/assets/m2_performance.jpg" alt="m2_performance" width="800" /&gt; 
&lt;img src="https://raw.githubusercontent.com/microsoft/BitNet/main/assets/intel_performance.jpg" alt="m2_performance" width="800" /&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;The tested models are dummy setups used in a research context to demonstrate the inference performance of bitnet.cpp.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Demo&lt;/h2&gt; 
&lt;p&gt;A demo of bitnet.cpp running a BitNet b1.58 3B model on Apple M2:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/7f46b736-edec-4828-b809-4be780a3e5b1"&gt;https://github.com/user-attachments/assets/7f46b736-edec-4828-b809-4be780a3e5b1&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;What's New:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;05/20/2025 &lt;a href="https://github.com/microsoft/BitNet/raw/main/gpu/README.md"&gt;BitNet Official GPU inference kernel&lt;/a&gt; &lt;img src="https://img.shields.io/badge/NEW-red" alt="NEW" /&gt;&lt;/li&gt; 
 &lt;li&gt;04/14/2025 &lt;a href="https://huggingface.co/microsoft/BitNet-b1.58-2B-4T"&gt;BitNet Official 2B Parameter Model on Hugging Face&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;02/18/2025 &lt;a href="https://arxiv.org/abs/2502.11880"&gt;Bitnet.cpp: Efficient Edge Inference for Ternary LLMs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;11/08/2024 &lt;a href="https://arxiv.org/abs/2411.04965"&gt;BitNet a4.8: 4-bit Activations for 1-bit LLMs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;10/21/2024 &lt;a href="https://arxiv.org/abs/2410.16144"&gt;1-bit AI Infra: Part 1.1, Fast and Lossless BitNet b1.58 Inference on CPUs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;10/17/2024 bitnet.cpp 1.0 released.&lt;/li&gt; 
 &lt;li&gt;03/21/2024 &lt;a href="https://github.com/microsoft/unilm/raw/master/bitnet/The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ.pdf"&gt;The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;02/27/2024 &lt;a href="https://arxiv.org/abs/2402.17764"&gt;The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;10/17/2023 &lt;a href="https://arxiv.org/abs/2310.11453"&gt;BitNet: Scaling 1-bit Transformers for Large Language Models&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;p&gt;This project is based on the &lt;a href="https://github.com/ggerganov/llama.cpp"&gt;llama.cpp&lt;/a&gt; framework. We would like to thank all the authors for their contributions to the open-source community. Also, bitnet.cpp's kernels are built on top of the Lookup Table methodologies pioneered in &lt;a href="https://github.com/microsoft/T-MAC/"&gt;T-MAC&lt;/a&gt;. For inference of general low-bit LLMs beyond ternary models, we recommend using T-MAC.&lt;/p&gt; 
&lt;h2&gt;Official Models&lt;/h2&gt; 
&lt;table&gt;  
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;th rowspan="2"&gt;Model&lt;/th&gt; 
   &lt;th rowspan="2"&gt;Parameters&lt;/th&gt; 
   &lt;th rowspan="2"&gt;CPU&lt;/th&gt; 
   &lt;th colspan="3"&gt;Kernel&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th&gt;I2_S&lt;/th&gt; 
   &lt;th&gt;TL1&lt;/th&gt; 
   &lt;th&gt;TL2&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/microsoft/BitNet-b1.58-2B-4T"&gt;BitNet-b1.58-2B-4T&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;2.4B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Supported Models&lt;/h2&gt; 
&lt;p&gt;‚ùóÔ∏è&lt;strong&gt;We use existing 1-bit LLMs available on &lt;a href="https://huggingface.co/"&gt;Hugging Face&lt;/a&gt; to demonstrate the inference capabilities of bitnet.cpp. We hope the release of bitnet.cpp will inspire the development of 1-bit LLMs in large-scale settings in terms of model size and training tokens.&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt;  
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;th rowspan="2"&gt;Model&lt;/th&gt; 
   &lt;th rowspan="2"&gt;Parameters&lt;/th&gt; 
   &lt;th rowspan="2"&gt;CPU&lt;/th&gt; 
   &lt;th colspan="3"&gt;Kernel&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th&gt;I2_S&lt;/th&gt; 
   &lt;th&gt;TL1&lt;/th&gt; 
   &lt;th&gt;TL2&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/1bitLLM/bitnet_b1_58-large"&gt;bitnet_b1_58-large&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;0.7B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/1bitLLM/bitnet_b1_58-3B"&gt;bitnet_b1_58-3B&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;3.3B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/HF1BitLLM/Llama3-8B-1.58-100B-tokens"&gt;Llama3-8B-1.58-100B-tokens&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;8.0B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/collections/tiiuae/falcon3-67605ae03578be86e4e87026"&gt;Falcon3 Family&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;1B-10B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/collections/tiiuae/falcon-edge-series-6804fd13344d6d8a8fa71130"&gt;Falcon-E Family&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;1B-3B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;python&amp;gt;=3.9&lt;/li&gt; 
 &lt;li&gt;cmake&amp;gt;=3.22&lt;/li&gt; 
 &lt;li&gt;clang&amp;gt;=18 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;For Windows users, install &lt;a href="https://visualstudio.microsoft.com/downloads/"&gt;Visual Studio 2022&lt;/a&gt;. In the installer, toggle on at least the following options(this also automatically installs the required additional tools like CMake):&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Desktop-development with C++&lt;/li&gt; 
     &lt;li&gt;C++-CMake Tools for Windows&lt;/li&gt; 
     &lt;li&gt;Git for Windows&lt;/li&gt; 
     &lt;li&gt;C++-Clang Compiler for Windows&lt;/li&gt; 
     &lt;li&gt;MS-Build Support for LLVM-Toolset (clang)&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;For Debian/Ubuntu users, you can download with &lt;a href="https://apt.llvm.org/"&gt;Automatic installation script&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash -c "$(wget -O - https://apt.llvm.org/llvm.sh)"&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;conda (highly recommend)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Build from source&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] If you are using Windows, please remember to always use a Developer Command Prompt / PowerShell for VS2022 for the following commands. Please refer to the FAQs below if you see any issues.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ol&gt; 
 &lt;li&gt;Clone the repo&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone --recursive https://github.com/microsoft/BitNet.git
cd BitNet
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Install the dependencies&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# (Recommended) Create a new conda environment
conda create -n bitnet-cpp python=3.9
conda activate bitnet-cpp

pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Build the project&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Manually download the model and run with local path
huggingface-cli download microsoft/BitNet-b1.58-2B-4T-gguf --local-dir models/BitNet-b1.58-2B-4T
python setup_env.py -md models/BitNet-b1.58-2B-4T -q i2_s

&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;
usage: setup_env.py [-h] [--hf-repo {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}] [--model-dir MODEL_DIR] [--log-dir LOG_DIR] [--quant-type {i2_s,tl1}] [--quant-embd]
                    [--use-pretuned]

Setup the environment for running inference

optional arguments:
  -h, --help            show this help message and exit
  --hf-repo {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}, -hr {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}
                        Model used for inference
  --model-dir MODEL_DIR, -md MODEL_DIR
                        Directory to save/load the model
  --log-dir LOG_DIR, -ld LOG_DIR
                        Directory to save the logging info
  --quant-type {i2_s,tl1}, -q {i2_s,tl1}
                        Quantization type
  --quant-embd          Quantize the embeddings to f16
  --use-pretuned, -p    Use the pretuned kernel parameters
&lt;/pre&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Basic usage&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run inference with the quantized model
python run_inference.py -m models/BitNet-b1.58-2B-4T/ggml-model-i2_s.gguf -p "You are a helpful assistant" -cnv
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;
usage: run_inference.py [-h] [-m MODEL] [-n N_PREDICT] -p PROMPT [-t THREADS] [-c CTX_SIZE] [-temp TEMPERATURE] [-cnv]

Run inference

optional arguments:
  -h, --help            show this help message and exit
  -m MODEL, --model MODEL
                        Path to model file
  -n N_PREDICT, --n-predict N_PREDICT
                        Number of tokens to predict when generating text
  -p PROMPT, --prompt PROMPT
                        Prompt to generate text from
  -t THREADS, --threads THREADS
                        Number of threads to use
  -c CTX_SIZE, --ctx-size CTX_SIZE
                        Size of the prompt context
  -temp TEMPERATURE, --temperature TEMPERATURE
                        Temperature, a hyperparameter that controls the randomness of the generated text
  -cnv, --conversation  Whether to enable chat mode or not (for instruct models.)
                        (When this option is turned on, the prompt specified by -p will be used as the system prompt.)
&lt;/pre&gt; 
&lt;h3&gt;Benchmark&lt;/h3&gt; 
&lt;p&gt;We provide scripts to run the inference benchmark providing a model.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;usage: e2e_benchmark.py -m MODEL [-n N_TOKEN] [-p N_PROMPT] [-t THREADS]  
   
Setup the environment for running the inference  
   
required arguments:  
  -m MODEL, --model MODEL  
                        Path to the model file. 
   
optional arguments:  
  -h, --help  
                        Show this help message and exit. 
  -n N_TOKEN, --n-token N_TOKEN  
                        Number of generated tokens. 
  -p N_PROMPT, --n-prompt N_PROMPT  
                        Prompt to generate text from. 
  -t THREADS, --threads THREADS  
                        Number of threads to use. 
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Here's a brief explanation of each argument:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;-m&lt;/code&gt;, &lt;code&gt;--model&lt;/code&gt;: The path to the model file. This is a required argument that must be provided when running the script.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-n&lt;/code&gt;, &lt;code&gt;--n-token&lt;/code&gt;: The number of tokens to generate during the inference. It is an optional argument with a default value of 128.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-p&lt;/code&gt;, &lt;code&gt;--n-prompt&lt;/code&gt;: The number of prompt tokens to use for generating text. This is an optional argument with a default value of 512.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-t&lt;/code&gt;, &lt;code&gt;--threads&lt;/code&gt;: The number of threads to use for running the inference. It is an optional argument with a default value of 2.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-h&lt;/code&gt;, &lt;code&gt;--help&lt;/code&gt;: Show the help message and exit. Use this argument to display usage information.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python utils/e2e_benchmark.py -m /path/to/model -n 200 -p 256 -t 4  
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command would run the inference benchmark using the model located at &lt;code&gt;/path/to/model&lt;/code&gt;, generating 200 tokens from a 256 token prompt, utilizing 4 threads.&lt;/p&gt; 
&lt;p&gt;For the model layout that do not supported by any public model, we provide scripts to generate a dummy model with the given model layout, and run the benchmark on your machine:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python utils/generate-dummy-bitnet-model.py models/bitnet_b1_58-large --outfile models/dummy-bitnet-125m.tl1.gguf --outtype tl1 --model-size 125M

# Run benchmark with the generated model, use -m to specify the model path, -p to specify the prompt processed, -n to specify the number of token to generate
python utils/e2e_benchmark.py -m models/dummy-bitnet-125m.tl1.gguf -p 512 -n 128
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Convert from &lt;code&gt;.safetensors&lt;/code&gt; Checkpoints&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Prepare the .safetensors model file
huggingface-cli download microsoft/bitnet-b1.58-2B-4T-bf16 --local-dir ./models/bitnet-b1.58-2B-4T-bf16

# Convert to gguf model
python ./utils/convert-helper-bitnet.py ./models/bitnet-b1.58-2B-4T-bf16
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;FAQ (Frequently Asked Questions)üìå&lt;/h3&gt; 
&lt;h4&gt;Q1: The build dies with errors building llama.cpp due to issues with std::chrono in log.cpp?&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; This is an issue introduced in recent version of llama.cpp. Please refer to this &lt;a href="https://github.com/tinglou/llama.cpp/commit/4e3db1e3d78cc1bcd22bcb3af54bd2a4628dd323"&gt;commit&lt;/a&gt; in the &lt;a href="https://github.com/abetlen/llama-cpp-python/issues/1942"&gt;discussion&lt;/a&gt; to fix this issue.&lt;/p&gt; 
&lt;h4&gt;Q2: How to build with clang in conda environment on windows?&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Before building the project, verify your clang installation and access to Visual Studio tools by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;clang -v
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command checks that you are using the correct version of clang and that the Visual Studio tools are available. If you see an error message such as:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;'clang' is not recognized as an internal or external command, operable program or batch file.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It indicates that your command line window is not properly initialized for Visual Studio tools.&lt;/p&gt; 
&lt;p&gt;‚Ä¢ If you are using Command Prompt, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;"C:\Program Files\Microsoft Visual Studio\2022\Professional\Common7\Tools\VsDevCmd.bat" -startdir=none -arch=x64 -host_arch=x64
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;‚Ä¢ If you are using Windows PowerShell, run the following commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Import-Module "C:\Program Files\Microsoft Visual Studio\2022\Professional\Common7\Tools\Microsoft.VisualStudio.DevShell.dll" Enter-VsDevShell 3f0e31ad -SkipAutomaticLocation -DevCmdArguments "-arch=x64 -host_arch=x64"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;These steps will initialize your environment and allow you to use the correct Visual Studio tools.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Asabeneh/30-Days-Of-Python</title>
      <link>https://github.com/Asabeneh/30-Days-Of-Python</link>
      <description>&lt;p&gt;The 30 Days of Python programming challenge is a step-by-step guide to learn the Python programming language in 30 days. This challenge may take more than 100 days. Follow your own pace. These videos may help too: https://www.youtube.com/channel/UC7PNRuno1rzYPb1xLa4yktw&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;üêç 30 Days Of Python&lt;/h1&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;# Day&lt;/th&gt; 
   &lt;th align="center"&gt;Topics&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;01&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/readme.md"&gt;Introduction&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;02&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/02_Day_Variables_builtin_functions/02_variables_builtin_functions.md"&gt;Variables, Built-in Functions&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;03&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/03_Day_Operators/03_operators.md"&gt;Operators&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;04&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/04_Day_Strings/04_strings.md"&gt;Strings&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;05&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/05_Day_Lists/05_lists.md"&gt;Lists&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;06&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/06_Day_Tuples/06_tuples.md"&gt;Tuples&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;07&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/07_Day_Sets/07_sets.md"&gt;Sets&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;08&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/08_Day_Dictionaries/08_dictionaries.md"&gt;Dictionaries&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;09&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/09_Day_Conditionals/09_conditionals.md"&gt;Conditionals&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;10&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/10_Day_Loops/10_loops.md"&gt;Loops&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;11&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/11_Day_Functions/11_functions.md"&gt;Functions&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;12&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/12_Day_Modules/12_modules.md"&gt;Modules&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;13&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/13_Day_List_comprehension/13_list_comprehension.md"&gt;List Comprehension&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;14&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/14_Day_Higher_order_functions/14_higher_order_functions.md"&gt;Higher Order Functions&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;15&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/15_Day_Python_type_errors/15_python_type_errors.md"&gt;Python Type Errors&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/16_Day_Python_date_time/16_python_datetime.md"&gt;Python Date time&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;17&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/17_Day_Exception_handling/17_exception_handling.md"&gt;Exception Handling&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;18&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/18_Day_Regular_expressions/18_regular_expressions.md"&gt;Regular Expressions&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;19&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/19_Day_File_handling/19_file_handling.md"&gt;File Handling&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;20&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/20_Day_Python_package_manager/20_python_package_manager.md"&gt;Python Package Manager&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;21&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/21_Day_Classes_and_objects/21_classes_and_objects.md"&gt;Classes and Objects&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;22&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/22_Day_Web_scraping/22_web_scraping.md"&gt;Web Scraping&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;23&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/23_Day_Virtual_environment/23_virtual_environment.md"&gt;Virtual Environment&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;24&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/24_Day_Statistics/24_statistics.md"&gt;Statistics&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;25&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/25_Day_Pandas/25_pandas.md"&gt;Pandas&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;26&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/26_Day_Python_web/26_python_web.md"&gt;Python web&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;27&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/27_Day_Python_with_mongodb/27_python_with_mongodb.md"&gt;Python with MongoDB&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;28&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/28_Day_API/28_API.md"&gt;API&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;29&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/29_Day_Building_API/29_building_API.md"&gt;Building API&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;30&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/30_Day_Conclusions/30_conclusions.md"&gt;Conclusions&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;small&gt;üß°üß°üß° HAPPY CODING üß°üß°üß°&lt;/small&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;div&gt; 
 &lt;h2&gt;üíñ Sponsors&lt;/h2&gt; 
 &lt;p&gt;Our amazing sponsors for supporting my open-source contribution and the &lt;strong&gt;30 Days of Challenge&lt;/strong&gt; series!&lt;/p&gt; 
 &lt;h3&gt;Current Sponsors&lt;/h3&gt; 
 &lt;hr /&gt; 
 &lt;div align="center"&gt; 
  &lt;a href="https://ref.wisprflow.ai/MPMzRGE" target="_blank" rel="noopener noreferrer"&gt; 
   &lt;picture&gt; 
    &lt;!-- Dark mode --&gt; 
    &lt;source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/Asabeneh/asabeneh/master/images/Wispr_Flow-Logo-white.png" /&gt; 
    &lt;!-- Light mode (fallback) --&gt; 
    &lt;img src="https://raw.githubusercontent.com/Asabeneh/asabeneh/master/images/Wispr_Flow-logo.png" width="400px" alt="Wispr Flow Logo" title="Wispr Flow" /&gt; 
   &lt;/picture&gt; &lt;/a&gt; 
  &lt;h1&gt; &lt;a href="https://ref.wisprflow.ai/MPMzRGE" target="_blank" rel="noopener noreferrer"&gt; Talk to code, stay in the Flow. &lt;/a&gt; &lt;/h1&gt; 
  &lt;h2&gt; &lt;a href="https://ref.wisprflow.ai/MPMzRGE" target="_blank" rel="noopener noreferrer"&gt; Flow is built for devs who live in their tools. Speak and give more context, get better results. &lt;/a&gt; &lt;/h2&gt; 
 &lt;/div&gt; 
 &lt;hr /&gt; 
 &lt;div align="center"&gt; 
  &lt;a href="https://client.petrosky.io/aff.php?aff=402" target="_blank" rel="noopener noreferrer"&gt; 
   &lt;picture&gt; 
    &lt;!-- Dark mode --&gt; 
    &lt;source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/Asabeneh/asabeneh/master/images/petrosky-logo-white.png" /&gt; 
    &lt;!-- Light mode (fallback) --&gt; 
    &lt;img src="https://raw.githubusercontent.com/Asabeneh/asabeneh/master/images/petrosky-logo-black.png" width="400px" alt="Petrosky Logo" title="Petrosky" /&gt; 
   &lt;/picture&gt; &lt;/a&gt; 
  &lt;h1&gt; &lt;a href="https://client.petrosky.io/aff.php?aff=402" target="_blank" rel="noopener noreferrer"&gt; A hosting for your entire journey! &lt;/a&gt; &lt;/h1&gt; 
  &lt;h2&gt; &lt;a href="https://client.petrosky.io/aff.php?aff=402" target="_blank" rel="noopener noreferrer"&gt; Affordable VPS Hosting Services For All Your Needs &lt;/a&gt; &lt;/h2&gt; 
 &lt;/div&gt; 
 &lt;hr /&gt; 
 &lt;h3&gt;üôå Become a Sponsor&lt;/h3&gt; 
 &lt;p&gt;You can support this project by becoming a sponsor on &lt;strong&gt;&lt;a href="https://github.com/sponsors/asabeneh"&gt;GitHub Sponsors&lt;/a&gt;&lt;/strong&gt; or through &lt;a href="https://www.paypal.me/asabeneh"&gt;PayPal&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;Every contribution, big or small, makes a huge difference. Thank you for your support! üåü&lt;/p&gt; 
 &lt;hr /&gt; 
 &lt;div align="center"&gt; 
  &lt;h1&gt; 30 Days Of Python: Day 1 - Introduction&lt;/h1&gt; 
  &lt;a class="header-badge" target="_blank" href="https://www.linkedin.com/in/asabeneh/"&gt; &lt;img src="https://img.shields.io/badge/style--5eba00.svg?label=LinkedIn&amp;amp;logo=linkedin&amp;amp;style=social" /&gt; &lt;/a&gt; 
  &lt;a class="header-badge" target="_blank" href="https://twitter.com/Asabeneh"&gt; &lt;img alt="Twitter Follow" src="https://img.shields.io/twitter/follow/asabeneh?style=social" /&gt; &lt;/a&gt; 
  &lt;p&gt;&lt;sub&gt;Author: &lt;a href="https://www.linkedin.com/in/asabeneh/" target="_blank"&gt;Asabeneh Yetayeh&lt;/a&gt;&lt;br /&gt; &lt;small&gt; Second Edition: July, 2021&lt;/small&gt; &lt;/sub&gt;&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;p&gt;üáßüá∑ &lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/Portuguese/README.md"&gt;Portuguese&lt;/a&gt; üá®üá≥ &lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/Chinese/README.md"&gt;‰∏≠Êñá&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/02_Day_Variables_builtin_functions/02_variables_builtin_functions.md"&gt;Day 2 &amp;gt;&amp;gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/30DaysOfPython_banner3@2x.png" alt="30DaysOfPython" /&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#-30-days-of-python"&gt;üêç 30 Days Of Python&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#-become-a-sponsor"&gt;üôå Become a Sponsor&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#-day-1"&gt;üìò Day 1&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#welcome"&gt;Welcome&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#why-python-"&gt;Why Python ?&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#environment-setup"&gt;Environment Setup&lt;/a&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#installing-python"&gt;Installing Python&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#python-shell"&gt;Python Shell&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#installing-visual-studio-code"&gt;Installing Visual Studio Code&lt;/a&gt; 
       &lt;ul&gt; 
        &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#how-to-use-visual-studio-code"&gt;How to use visual studio code&lt;/a&gt;&lt;/li&gt; 
       &lt;/ul&gt; &lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#basic-python"&gt;Basic Python&lt;/a&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#python-syntax"&gt;Python Syntax&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#python-indentation"&gt;Python Indentation&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#comments"&gt;Comments&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#data-types"&gt;Data types&lt;/a&gt; 
       &lt;ul&gt; 
        &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#number"&gt;Number&lt;/a&gt;&lt;/li&gt; 
        &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#string"&gt;String&lt;/a&gt;&lt;/li&gt; 
        &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#booleans"&gt;Booleans&lt;/a&gt;&lt;/li&gt; 
        &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#list"&gt;List&lt;/a&gt;&lt;/li&gt; 
        &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#dictionary"&gt;Dictionary&lt;/a&gt;&lt;/li&gt; 
        &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#tuple"&gt;Tuple&lt;/a&gt;&lt;/li&gt; 
        &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#set"&gt;Set&lt;/a&gt;&lt;/li&gt; 
       &lt;/ul&gt; &lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#checking-data-types"&gt;Checking Data types&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#python-file"&gt;Python File&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#-exercises---day-1"&gt;üíª Exercises - Day 1&lt;/a&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#exercise-level-1"&gt;Exercise: Level 1&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#exercise-level-2"&gt;Exercise: Level 2&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#exercise-level-3"&gt;Exercise: Level 3&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h1&gt;üìò Day 1&lt;/h1&gt; 
 &lt;h2&gt;Welcome&lt;/h2&gt; 
 &lt;p&gt;&lt;strong&gt;Congratulations&lt;/strong&gt; for deciding to participate in a &lt;em&gt;30 days of Python&lt;/em&gt; programming challenge. In this challenge, you will learn everything you need to be a python programmer and the whole concept of programming. At the end of the challenge, you will get a &lt;em&gt;30DaysOfPython&lt;/em&gt; programming challenge certificate.&lt;/p&gt; 
 &lt;p&gt;If you would like to actively engage in the challenge, you may join the &lt;a href="https://t.me/ThirtyDaysOfPython"&gt;30DaysOfPython challenge&lt;/a&gt; telegram group.&lt;/p&gt; 
 &lt;h2&gt;Introduction&lt;/h2&gt; 
 &lt;p&gt;Python is a high-level programming language for general-purpose programming. It is an open source, interpreted, objected-oriented programming language. Python was created by a Dutch programmer, Guido van Rossum. The name of the Python programming language was derived from a British sketch comedy series, &lt;em&gt;Monty Python's Flying Circus&lt;/em&gt;. The first version was released on February 20, 1991. This 30 days of Python challenge will help you learn the latest version of Python, Python 3 step by step. The topics are broken down into 30 days, where each day contains several topics with easy-to-understand explanations, real-world examples, and many hands on exercises and projects.&lt;/p&gt; 
 &lt;p&gt;This challenge is designed for beginners and professionals who want to learn python programming language. It may take 30 to 100 days to complete the challenge. People who actively participate in the telegram group have a high probability of completing the challenge.&lt;/p&gt; 
 &lt;p&gt;This challenge is easy to read, written in conversational English, engaging, motivating and at the same time, it is very demanding. You need to allocate much time to finish this challenge. If you are a visual learner, you may get the video lesson on &lt;a href="https://www.youtube.com/channel/UC7PNRuno1rzYPb1xLa4yktw"&gt; Washera&lt;/a&gt; YouTube channel. You may start from &lt;a href="https://youtu.be/OCCWZheOesI"&gt;Python for Absolute Beginners video&lt;/a&gt;. Subscribe the channel, comment and ask questions on YouTube vidoes and be proactive, the author will eventually notice you.&lt;/p&gt; 
 &lt;p&gt;The author likes to hear your opinion about the challenge, share the author by expressing your thoughts about the 30DaysOfPython challenge. You can leave your testimonial on this &lt;a href="https://www.asabeneh.com/testimonials"&gt;link&lt;/a&gt;&lt;/p&gt; 
 &lt;h2&gt;Why Python ?&lt;/h2&gt; 
 &lt;p&gt;It is a programming language which is very close to human language and because of that, it is easy to learn and use. Python is used by various industries and companies (including Google). It has been used to develop web applications, desktop applications, system administration, and machine learning libraries. Python is a highly embraced language in the data science and machine learning community. I hope this is enough to convince you to start learning Python. Python is eating the world and you are killing it before it eats you.&lt;/p&gt; 
 &lt;h2&gt;Environment Setup&lt;/h2&gt; 
 &lt;h3&gt;Installing Python&lt;/h3&gt; 
 &lt;p&gt;To run a python script you need to install python. Let's &lt;a href="https://www.python.org/"&gt;download&lt;/a&gt; python. If your are a windows user. Click the button encircled in red.&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://www.python.org/"&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/installing_on_windows.png" alt="installing on Windows" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;If you are a macOS user. Click the button encircled in red.&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://www.python.org/"&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/installing_on_macOS.png" alt="installing on Windows" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;To check if python is installed write the following command on your device terminal.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;python --version
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/python_versio.png" alt="Python Version" /&gt;&lt;/p&gt; 
 &lt;p&gt;As you can see from the terminal, I am using &lt;em&gt;Python 3.7.5&lt;/em&gt; version at the moment. Your version of Python might be different from mine by but it should be 3.6 or above. If you mange to see the python version, well done. Python has been installed on your machine. Continue to the next section.&lt;/p&gt; 
 &lt;h3&gt;Python Shell&lt;/h3&gt; 
 &lt;p&gt;Python is an interpreted scripting language, so it does not need to be compiled. It means it executes the code line by line. Python comes with a &lt;em&gt;Python Shell (Python Interactive Shell)&lt;/em&gt;. It is used to execute a single python command and get the result.&lt;/p&gt; 
 &lt;p&gt;Python Shell waits for the Python code from the user. When you enter the code, it interprets the code and shows the result in the next line. Open your terminal or command prompt(cmd) and write:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;python
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/opening_python_shell.png" alt="Python Scripting Shell" /&gt;&lt;/p&gt; 
 &lt;p&gt;The Python interactive shell is opened and it is waiting for you to write Python code(Python script). You will write your Python script next to this symbol &amp;gt;&amp;gt;&amp;gt; and then click Enter. Let us write our very first script on the Python scripting shell.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/adding_on_python_shell.png" alt="Python script on Python shell" /&gt;&lt;/p&gt; 
 &lt;p&gt;Well done, you wrote your first Python script on Python interactive shell. How do we close the Python interactive shell ? To close the shell, next to this symbol &amp;gt;&amp;gt; write &lt;strong&gt;exit()&lt;/strong&gt; command and press Enter.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/exit_from_shell.png" alt="Exit from python shell" /&gt;&lt;/p&gt; 
 &lt;p&gt;Now, you know how to open the Python interactive shell and how to exit from it.&lt;/p&gt; 
 &lt;p&gt;Python will give you results if you write scripts that Python understands, if not it returns errors. Let's make a deliberate mistake and see what Python will return.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/invalid_syntax_error.png" alt="Invalid Syntax Error" /&gt;&lt;/p&gt; 
 &lt;p&gt;As you can see from the returned error, Python is so clever that it knows the mistake we made and which was &lt;em&gt;Syntax Error: invalid syntax&lt;/em&gt;. Using x as multiplication in Python is a syntax error because (x) is not a valid syntax in Python. Instead of (&lt;strong&gt;x&lt;/strong&gt;) we use asterisk (*) for multiplication. The returned error clearly shows what to fix.&lt;/p&gt; 
 &lt;p&gt;The process of identifying and removing errors from a program is called &lt;em&gt;debugging&lt;/em&gt;. Let us debug it by putting * in place of &lt;strong&gt;x&lt;/strong&gt;.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/fixing_syntax_error.png" alt="Fixing Syntax Error" /&gt;&lt;/p&gt; 
 &lt;p&gt;Our bug was fixed, the code ran and we got a result we were expecting. As a programmer you will see such kind of errors on daily basis. It is good to know how to debug. To be good at debugging you should understand what kind of errors you are facing. Some of the Python errors you may encounter are &lt;em&gt;SyntaxError&lt;/em&gt;, &lt;em&gt;IndexError&lt;/em&gt;, &lt;em&gt;NameError&lt;/em&gt;, &lt;em&gt;ModuleNotFoundError&lt;/em&gt;, &lt;em&gt;KeyError&lt;/em&gt;, &lt;em&gt;ImportError&lt;/em&gt;, &lt;em&gt;AttributeError&lt;/em&gt;, &lt;em&gt;TypeError&lt;/em&gt;, &lt;em&gt;ValueError&lt;/em&gt;, &lt;em&gt;ZeroDivisionError&lt;/em&gt; etc. We will see more about different Python &lt;strong&gt;&lt;em&gt;error types&lt;/em&gt;&lt;/strong&gt; in later sections.&lt;/p&gt; 
 &lt;p&gt;Let us practice more how to use Python interactive shell. Go to your terminal or command prompt and write the word &lt;strong&gt;python&lt;/strong&gt;.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/opening_python_shell.png" alt="Python Scripting Shell" /&gt;&lt;/p&gt; 
 &lt;p&gt;The Python interactive shell is opened. Let us do some basic mathematical operations (addition, subtraction, multiplication, division, modulus, exponential).&lt;/p&gt; 
 &lt;p&gt;Let us do some maths first before we write any Python code:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;2 + 3 is 5&lt;/li&gt; 
  &lt;li&gt;3 - 2 is 1&lt;/li&gt; 
  &lt;li&gt;3 * 2 is 6&lt;/li&gt; 
  &lt;li&gt;3 / 2 is 1.5&lt;/li&gt; 
  &lt;li&gt;3 ** 2 is the same as 3 * 3&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;In python we have the following additional operations:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;3 % 2 = 1 =&amp;gt; which means finding the remainder&lt;/li&gt; 
  &lt;li&gt;3 // 2 = 1 =&amp;gt; which means removing the remainder&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;Let us change the above mathematical expressions to Python code. The Python shell has been opened and let us write a comment at the very beginning of the shell.&lt;/p&gt; 
 &lt;p&gt;A &lt;em&gt;comment&lt;/em&gt; is a part of the code which is not executed by python. So we can leave some text in our code to make our code more readable. Python does not run the comment part. A comment in python starts with hash(#) symbol. This is how you write a comment in python&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt; # comment starts with hash
 # this is a python comment, because it starts with a (#) symbol
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/maths_on_python_shell.png" alt="Maths on python shell" /&gt;&lt;/p&gt; 
 &lt;p&gt;Before we move on to the next section, let us practice more on the Python interactive shell. Close the opened shell by writing &lt;em&gt;exit()&lt;/em&gt; on the shell and open it again and let us practice how to write text on the Python shell.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/writing_string_on_shell.png" alt="Writing String on python shell" /&gt;&lt;/p&gt; 
 &lt;h3&gt;Installing Visual Studio Code&lt;/h3&gt; 
 &lt;p&gt;The Python interactive shell is good to try and test small script codes but it will not be for a big project. In real work environment, developers use different code editors to write codes. In this 30 days of Python programming challenge we will use visual studio code. Visual studio code is a very popular open source text editor. I am a fan of vscode and I would recommend to &lt;a href="https://code.visualstudio.com/"&gt;download&lt;/a&gt; visual studio code, but if you are in favor of other editors, feel free to follow with what you have.&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://code.visualstudio.com/"&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/vscode.png" alt="Visual Studio Code" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;If you installed visual studio code, let us see how to use it. If you prefer a video, you can follow this Visual Studio Code for Python &lt;a href="https://www.youtube.com/watch?v=bn7Cx4z-vSo"&gt;Video tutorial&lt;/a&gt;&lt;/p&gt; 
 &lt;h4&gt;How to use visual studio code&lt;/h4&gt; 
 &lt;p&gt;Open the visual studio code by double clicking the visual studio icon. When you open it you will get this kind of interface. Try to interact with the labeled icons.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/vscode_ui.png" alt="Visual studio Code" /&gt;&lt;/p&gt; 
 &lt;p&gt;Create a folder named 30DaysOfPython on your desktop. Then open it using visual studio code.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/how_to_open_project_on_vscode.png" alt="Opening Project on Visual studio" /&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/opening_project.png" alt="Opening a project" /&gt;&lt;/p&gt; 
 &lt;p&gt;After opening it you will see shortcuts for creating files and folders inside of 30DaysOfPython project's directory. As you can see below, I have created the very first file, helloworld.py. You can do the same.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/helloworld.png" alt="Creating a python file" /&gt;&lt;/p&gt; 
 &lt;p&gt;After a long day of coding, you want to close your code editor, right? This is how you will close the opened project.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/closing_opened_project.png" alt="Closing project" /&gt;&lt;/p&gt; 
 &lt;p&gt;Congratulations, you have finished setting up the development environment. Let us start coding.&lt;/p&gt; 
 &lt;h2&gt;Basic Python&lt;/h2&gt; 
 &lt;h3&gt;Python Syntax&lt;/h3&gt; 
 &lt;p&gt;A Python script can be written in Python interactive shell or in the code editor. A Python file has an extension .py.&lt;/p&gt; 
 &lt;h3&gt;Python Indentation&lt;/h3&gt; 
 &lt;p&gt;An indentation is a white space in a text. Indentation in many languages is used to increase code readability; however, Python uses indentation to create blocks of code. In other programming languages, curly brackets are used to create code blocks instead of indentation. One of the common bugs when writing Python code is incorrect indentation.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/indentation.png" alt="Indentation Error" /&gt;&lt;/p&gt; 
 &lt;h3&gt;Comments&lt;/h3&gt; 
 &lt;p&gt;Comments play a crucial role in enhancing code readability and allowing developers to leave notes within their code. In Python, any text preceded by a hash (#) symbol is considered a comment and is not executed when the code runs.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Example: Single Line Comment&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;    # This is the first comment
    # This is the second comment
    # Python is eating the world
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Example: Multiline Comment&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Triple quote can be used for multiline comment if it is not assigned to a variable&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;"""This is multiline comment
multiline comment takes multiple lines.
python is eating the world
"""
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;Data types&lt;/h3&gt; 
 &lt;p&gt;In Python there are several types of data types. Let us get started with the most common ones. Different data types will be covered in detail in other sections. For the time being, let us just go through the different data types and get familiar with them. You do not have to have a clear understanding now.&lt;/p&gt; 
 &lt;h4&gt;Number&lt;/h4&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Integer: Integer(negative, zero and positive) numbers Example: ... -3, -2, -1, 0, 1, 2, 3 ...&lt;/li&gt; 
  &lt;li&gt;Float: Decimal number Example ... -3.5, -2.25, -1.0, 0.0, 1.1, 2.2, 3.5 ...&lt;/li&gt; 
  &lt;li&gt;Complex Example 1 + j, 2 + 4j&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h4&gt;String&lt;/h4&gt; 
 &lt;p&gt;A collection of one or more characters under a single or double quote. If a string is more than one sentence then we use a triple quote.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-py"&gt;'Asabeneh'
'Finland'
'Python'
'I love teaching'
'I hope you are enjoying the first day of 30DaysOfPython Challenge'
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Booleans&lt;/h4&gt; 
 &lt;p&gt;A boolean data type is either a True or False value. T and F should be always uppercase.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;    True  #  Is the light on? If it is on, then the value is True
    False # Is the light on? If it is off, then the value is False
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;List&lt;/h4&gt; 
 &lt;p&gt;Python list is an ordered collection which allows to store different data type items. A list is similar to an array in JavaScript.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-py"&gt;[0, 1, 2, 3, 4, 5]  # all are the same data types - a list of numbers
['Banana', 'Orange', 'Mango', 'Avocado'] # all the same data types - a list of strings (fruits)
['Finland','Estonia', 'Sweden','Norway'] # all the same data types - a list of strings (countries)
['Banana', 10, False, 9.81] # different data types in the list - string, integer, boolean and float
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Dictionary&lt;/h4&gt; 
 &lt;p&gt;A Python dictionary object is an unordered collection of data in a key value pair format.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-py"&gt;{
'first_name':'Asabeneh',
'last_name':'Yetayeh',
'country':'Finland', 
'age':250, 
'is_married':True,
'skills':['JS', 'React', 'Node', 'Python']
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Tuple&lt;/h4&gt; 
 &lt;p&gt;A tuple is an ordered collection of different data types like list but tuples can not be modified once they are created. They are immutable.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-py"&gt;('Asabeneh', 'Pawel', 'Brook', 'Abraham', 'Lidiya') # Names
&lt;/code&gt;&lt;/pre&gt; 
 &lt;pre&gt;&lt;code class="language-py"&gt;('Earth', 'Jupiter', 'Neptune', 'Mars', 'Venus', 'Saturn', 'Uranus', 'Mercury') # planets
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Set&lt;/h4&gt; 
 &lt;p&gt;A set is a collection of data types similar to list and tuple. Unlike list and tuple, set is not an ordered collection of items. Like in Mathematics, set in Python stores only unique items.&lt;/p&gt; 
 &lt;p&gt;In later sections, we will go in detail about each and every Python data type.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-py"&gt;{2, 4, 3, 5}
{3.14, 9.81, 2.7} # order is not important in set
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;Checking Data types&lt;/h3&gt; 
 &lt;p&gt;To check the data type of certain data/variable we use the &lt;strong&gt;type&lt;/strong&gt; function. In the following terminal you will see different python data types:&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/checking_data_types.png" alt="Checking Data types" /&gt;&lt;/p&gt; 
 &lt;h3&gt;Python File&lt;/h3&gt; 
 &lt;p&gt;First open your project folder, 30DaysOfPython. If you don't have this folder, create a folder name called 30DaysOfPython. Inside this folder, create a file called helloworld.py. Now, let's do what we did on python interactive shell using visual studio code.&lt;/p&gt; 
 &lt;p&gt;The Python interactive shell was printing without using &lt;strong&gt;print&lt;/strong&gt; but on visual studio code to see our result we should use a built in function _print(). The &lt;em&gt;print()&lt;/em&gt; built-in function takes one or more arguments as follows &lt;em&gt;print('arument1', 'argument2', 'argument3')&lt;/em&gt;. See the examples below.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;The file name is helloworld.py&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-py"&gt;# Day 1 - 30DaysOfPython Challenge

print(2 + 3)             # addition(+)
print(3 - 1)             # subtraction(-)
print(2 * 3)             # multiplication(*)
print(3 / 2)             # division(/)
print(3 ** 2)            # exponential(**)
print(3 % 2)             # modulus(%)
print(3 // 2)            # Floor division operator(//)

# Checking data types
print(type(10))          # Int
print(type(3.14))        # Float
print(type(1 + 3j))      # Complex number
print(type('Asabeneh'))  # String
print(type([1, 2, 3]))   # List
print(type({'name':'Asabeneh'})) # Dictionary
print(type({9.8, 3.14, 2.7}))    # Set
print(type((9.8, 3.14, 2.7)))    # Tuple
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;To run the python file check the image below. You can run the python file either by running the green button on Visual Studio Code or by typing &lt;em&gt;python helloworld.py&lt;/em&gt; in the terminal .&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/running_python_script.png" alt="Running python script" /&gt;&lt;/p&gt; 
 &lt;p&gt;üåï You are amazing. You have just completed day 1 challenge and you are on your way to greatness. Now do some exercises for your brain and muscles.&lt;/p&gt; 
 &lt;h2&gt;üíª Exercises - Day 1&lt;/h2&gt; 
 &lt;h3&gt;Exercise: Level 1&lt;/h3&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Check the python version you are using&lt;/li&gt; 
  &lt;li&gt;Open the python interactive shell and do the following operations. The operands are 3 and 4. 
   &lt;ul&gt; 
    &lt;li&gt;addition(+)&lt;/li&gt; 
    &lt;li&gt;subtraction(-)&lt;/li&gt; 
    &lt;li&gt;multiplication(*)&lt;/li&gt; 
    &lt;li&gt;modulus(%)&lt;/li&gt; 
    &lt;li&gt;division(/)&lt;/li&gt; 
    &lt;li&gt;exponential(**)&lt;/li&gt; 
    &lt;li&gt;floor division operator(//)&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;Write strings on the python interactive shell. The strings are the following: 
   &lt;ul&gt; 
    &lt;li&gt;Your name&lt;/li&gt; 
    &lt;li&gt;Your family name&lt;/li&gt; 
    &lt;li&gt;Your country&lt;/li&gt; 
    &lt;li&gt;I am enjoying 30 days of python&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;Check the data types of the following data: 
   &lt;ul&gt; 
    &lt;li&gt;10&lt;/li&gt; 
    &lt;li&gt;9.8&lt;/li&gt; 
    &lt;li&gt;3.14&lt;/li&gt; 
    &lt;li&gt;4 - 4j&lt;/li&gt; 
    &lt;li&gt;['Asabeneh', 'Python', 'Finland']&lt;/li&gt; 
    &lt;li&gt;Your name&lt;/li&gt; 
    &lt;li&gt;Your family name&lt;/li&gt; 
    &lt;li&gt;Your country&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;h3&gt;Exercise: Level 2&lt;/h3&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Create a folder named day_1 inside 30DaysOfPython folder. Inside day_1 folder, create a python file helloworld.py and repeat questions 1, 2, 3 and 4. Remember to use &lt;em&gt;print()&lt;/em&gt; when you are working on a python file. Navigate to the directory where you have saved your file, and run it.&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;h3&gt;Exercise: Level 3&lt;/h3&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Write an example for different Python data types such as Number(Integer, Float, Complex), String, Boolean, List, Tuple, Set and Dictionary.&lt;/li&gt; 
  &lt;li&gt;Find an &lt;a href="https://en.wikipedia.org/wiki/Euclidean_distance#:~:text=In%20mathematics%2C%20the%20Euclidean%20distance,being%20called%20the%20Pythagorean%20distance."&gt;Euclidian distance&lt;/a&gt; between (2, 3) and (10, 8)&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;üéâ CONGRATULATIONS ! üéâ&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/02_Day_Variables_builtin_functions/02_variables_builtin_functions.md"&gt;Day 2 &amp;gt;&amp;gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Aider-AI/aider</title>
      <link>https://github.com/Aider-AI/aider</link>
      <description>&lt;p&gt;aider is AI pair programming in your terminal&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://aider.chat/"&gt;&lt;img src="https://aider.chat/assets/logo.svg?sanitize=true" alt="Aider Logo" width="300" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;h1 align="center"&gt; AI Pair Programming in Your Terminal &lt;/h1&gt; 
&lt;p align="center"&gt; Aider lets you pair program with LLMs to start a new project or build on your existing codebase. &lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://aider.chat/assets/screencast.svg?sanitize=true" alt="aider screencast" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; 
 &lt;!--[[[cog
from scripts.homepage import get_badges_md
text = get_badges_md()
cog.out(text)
]]]--&gt; &lt;a href="https://github.com/Aider-AI/aider/stargazers"&gt;&lt;img alt="GitHub Stars" title="Total number of GitHub stars the Aider project has received" src="https://img.shields.io/github/stars/Aider-AI/aider?style=flat-square&amp;amp;logo=github&amp;amp;color=f1c40f&amp;amp;labelColor=555555" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/aider-chat/"&gt;&lt;img alt="PyPI Downloads" title="Total number of installations via pip from PyPI" src="https://img.shields.io/badge/üì¶%20Installs-4.1M-2ecc71?style=flat-square&amp;amp;labelColor=555555" /&gt;&lt;/a&gt; &lt;img alt="Tokens per week" title="Number of tokens processed weekly by Aider users" src="https://img.shields.io/badge/üìà%20Tokens%2Fweek-15B-3498db?style=flat-square&amp;amp;labelColor=555555" /&gt; &lt;a href="https://openrouter.ai/#options-menu"&gt;&lt;img alt="OpenRouter Ranking" title="Aider's ranking among applications on the OpenRouter platform" src="https://img.shields.io/badge/üèÜ%20OpenRouter-Top%2020-9b59b6?style=flat-square&amp;amp;labelColor=555555" /&gt;&lt;/a&gt; &lt;a href="https://aider.chat/HISTORY.html"&gt;&lt;img alt="Singularity" title="Percentage of the new code in Aider's last release written by Aider itself" src="https://img.shields.io/badge/üîÑ%20Singularity-88%25-e74c3c?style=flat-square&amp;amp;labelColor=555555" /&gt;&lt;/a&gt; 
 &lt;!--[[[end]]]--&gt; &lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;h3&gt;&lt;a href="https://aider.chat/docs/llms.html"&gt;Cloud and local LLMs&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aider.chat/docs/llms.html"&gt;&lt;img src="https://aider.chat/assets/icons/brain.svg?sanitize=true" width="32" height="32" align="left" valign="middle" style="margin-right:10px" /&gt;&lt;/a&gt; Aider works best with Claude 3.7 Sonnet, DeepSeek R1 &amp;amp; Chat V3, OpenAI o1, o3-mini &amp;amp; GPT-4o, but can connect to almost any LLM, including local models.&lt;/p&gt; 
&lt;br /&gt; 
&lt;h3&gt;&lt;a href="https://aider.chat/docs/repomap.html"&gt;Maps your codebase&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aider.chat/docs/repomap.html"&gt;&lt;img src="https://aider.chat/assets/icons/map-outline.svg?sanitize=true" width="32" height="32" align="left" valign="middle" style="margin-right:10px" /&gt;&lt;/a&gt; Aider makes a map of your entire codebase, which helps it work well in larger projects.&lt;/p&gt; 
&lt;br /&gt; 
&lt;h3&gt;&lt;a href="https://aider.chat/docs/languages.html"&gt;100+ code languages&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aider.chat/docs/languages.html"&gt;&lt;img src="https://aider.chat/assets/icons/code-tags.svg?sanitize=true" width="32" height="32" align="left" valign="middle" style="margin-right:10px" /&gt;&lt;/a&gt; Aider works with most popular programming languages: python, javascript, rust, ruby, go, cpp, php, html, css, and dozens more.&lt;/p&gt; 
&lt;br /&gt; 
&lt;h3&gt;&lt;a href="https://aider.chat/docs/git.html"&gt;Git integration&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aider.chat/docs/git.html"&gt;&lt;img src="https://aider.chat/assets/icons/source-branch.svg?sanitize=true" width="32" height="32" align="left" valign="middle" style="margin-right:10px" /&gt;&lt;/a&gt; Aider automatically commits changes with sensible commit messages. Use familiar git tools to easily diff, manage and undo AI changes.&lt;/p&gt; 
&lt;br /&gt; 
&lt;h3&gt;&lt;a href="https://aider.chat/docs/usage/watch.html"&gt;Use in your IDE&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aider.chat/docs/usage/watch.html"&gt;&lt;img src="https://aider.chat/assets/icons/monitor.svg?sanitize=true" width="32" height="32" align="left" valign="middle" style="margin-right:10px" /&gt;&lt;/a&gt; Use aider from within your favorite IDE or editor. Ask for changes by adding comments to your code and aider will get to work.&lt;/p&gt; 
&lt;br /&gt; 
&lt;h3&gt;&lt;a href="https://aider.chat/docs/usage/images-urls.html"&gt;Images &amp;amp; web pages&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aider.chat/docs/usage/images-urls.html"&gt;&lt;img src="https://aider.chat/assets/icons/image-multiple.svg?sanitize=true" width="32" height="32" align="left" valign="middle" style="margin-right:10px" /&gt;&lt;/a&gt; Add images and web pages to the chat to provide visual context, screenshots, reference docs, etc.&lt;/p&gt; 
&lt;br /&gt; 
&lt;h3&gt;&lt;a href="https://aider.chat/docs/usage/voice.html"&gt;Voice-to-code&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aider.chat/docs/usage/voice.html"&gt;&lt;img src="https://aider.chat/assets/icons/microphone.svg?sanitize=true" width="32" height="32" align="left" valign="middle" style="margin-right:10px" /&gt;&lt;/a&gt; Speak with aider about your code! Request new features, test cases or bug fixes using your voice and let aider implement the changes.&lt;/p&gt; 
&lt;br /&gt; 
&lt;h3&gt;&lt;a href="https://aider.chat/docs/usage/lint-test.html"&gt;Linting &amp;amp; testing&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aider.chat/docs/usage/lint-test.html"&gt;&lt;img src="https://aider.chat/assets/icons/check-all.svg?sanitize=true" width="32" height="32" align="left" valign="middle" style="margin-right:10px" /&gt;&lt;/a&gt; Automatically lint and test your code every time aider makes changes. Aider can fix problems detected by your linters and test suites.&lt;/p&gt; 
&lt;br /&gt; 
&lt;h3&gt;&lt;a href="https://aider.chat/docs/usage/copypaste.html"&gt;Copy/paste to web chat&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aider.chat/docs/usage/copypaste.html"&gt;&lt;img src="https://aider.chat/assets/icons/content-copy.svg?sanitize=true" width="32" height="32" align="left" valign="middle" style="margin-right:10px" /&gt;&lt;/a&gt; Work with any LLM via its web chat interface. Aider streamlines copy/pasting code context and edits back and forth with a browser.&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m pip install aider-install
aider-install

# Change directory into your codebase
cd /to/your/project

# DeepSeek
aider --model deepseek --api-key deepseek=&amp;lt;key&amp;gt;

# Claude 3.7 Sonnet
aider --model sonnet --api-key anthropic=&amp;lt;key&amp;gt;

# o3-mini
aider --model o3-mini --api-key openai=&amp;lt;key&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See the &lt;a href="https://aider.chat/docs/install.html"&gt;installation instructions&lt;/a&gt; and &lt;a href="https://aider.chat/docs/usage.html"&gt;usage documentation&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;More Information&lt;/h2&gt; 
&lt;h3&gt;Documentation&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://aider.chat/docs/install.html"&gt;Installation Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aider.chat/docs/usage.html"&gt;Usage Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aider.chat/docs/usage/tutorials.html"&gt;Tutorial Videos&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aider.chat/docs/llms.html"&gt;Connecting to LLMs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aider.chat/docs/config.html"&gt;Configuration Options&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aider.chat/docs/troubleshooting.html"&gt;Troubleshooting&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aider.chat/docs/faq.html"&gt;FAQ&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Community &amp;amp; Resources&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://aider.chat/docs/leaderboards/"&gt;LLM Leaderboards&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Aider-AI/aider"&gt;GitHub Repository&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://discord.gg/Y7X7bhMQFV"&gt;Discord Community&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aider.chat/HISTORY.html"&gt;Release notes&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aider.chat/blog/"&gt;Blog&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Kind Words From Users&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;em&gt;"My life has changed... Aider... It's going to rock your world."&lt;/em&gt; ‚Äî &lt;a href="https://x.com/esrtweet/status/1910809356381413593"&gt;Eric S. Raymond on X&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"The best free open source AI coding assistant."&lt;/em&gt; ‚Äî &lt;a href="https://youtu.be/YALpX8oOn78"&gt;IndyDevDan on YouTube&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"The best AI coding assistant so far."&lt;/em&gt; ‚Äî &lt;a href="https://www.youtube.com/watch?v=df8afeb1FY8"&gt;Matthew Berman on YouTube&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"Aider ... has easily quadrupled my coding productivity."&lt;/em&gt; ‚Äî &lt;a href="https://news.ycombinator.com/item?id=36212100"&gt;SOLAR_FIELDS on Hacker News&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"It's a cool workflow... Aider's ergonomics are perfect for me."&lt;/em&gt; ‚Äî &lt;a href="https://news.ycombinator.com/item?id=38185326"&gt;qup on Hacker News&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"It's really like having your senior developer live right in your Git repo - truly amazing!"&lt;/em&gt; ‚Äî &lt;a href="https://github.com/Aider-AI/aider/issues/124"&gt;rappster on GitHub&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"What an amazing tool. It's incredible."&lt;/em&gt; ‚Äî &lt;a href="https://github.com/Aider-AI/aider/issues/6#issue-1722897858"&gt;valyagolev on GitHub&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"Aider is such an astounding thing!"&lt;/em&gt; ‚Äî &lt;a href="https://github.com/Aider-AI/aider/issues/82#issuecomment-1631876700"&gt;cgrothaus on GitHub&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"It was WAY faster than I would be getting off the ground and making the first few working versions."&lt;/em&gt; ‚Äî &lt;a href="https://twitter.com/d_feldman/status/1662295077387923456"&gt;Daniel Feldman on X&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"THANK YOU for Aider! It really feels like a glimpse into the future of coding."&lt;/em&gt; ‚Äî &lt;a href="https://news.ycombinator.com/item?id=38205643"&gt;derwiki on Hacker News&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"It's just amazing. It is freeing me to do things I felt were out my comfort zone before."&lt;/em&gt; ‚Äî &lt;a href="https://discord.com/channels/1131200896827654144/1174002618058678323/1174084556257775656"&gt;Dougie on Discord&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"This project is stellar."&lt;/em&gt; ‚Äî &lt;a href="https://github.com/Aider-AI/aider/issues/112#issuecomment-1637429008"&gt;funkytaco on GitHub&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"Amazing project, definitely the best AI coding assistant I've used."&lt;/em&gt; ‚Äî &lt;a href="https://github.com/Aider-AI/aider/issues/84"&gt;joshuavial on GitHub&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"I absolutely love using Aider ... It makes software development feel so much lighter as an experience."&lt;/em&gt; ‚Äî &lt;a href="https://discord.com/channels/1131200896827654144/1133421607499595858/1229689636012691468"&gt;principalideal0 on Discord&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"I have been recovering from ... surgeries ... aider ... has allowed me to continue productivity."&lt;/em&gt; ‚Äî &lt;a href="https://www.reddit.com/r/OpenAI/s/nmNwkHy1zG"&gt;codeninja on Reddit&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"I am an aider addict. I'm getting so much more work done, but in less time."&lt;/em&gt; ‚Äî &lt;a href="https://discord.com/channels/1131200896827654144/1131200896827654149/1135913253483069470"&gt;dandandan on Discord&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"Aider... blows everything else out of the water hands down, there's no competition whatsoever."&lt;/em&gt; ‚Äî &lt;a href="https://discord.com/channels/1131200896827654144/1131200896827654149/1178736602797846548"&gt;SystemSculpt on Discord&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"Aider is amazing, coupled with Sonnet 3.5 it's quite mind blowing."&lt;/em&gt; ‚Äî &lt;a href="https://discord.com/channels/1131200896827654144/1133060684540813372/1262374225298198548"&gt;Josh Dingus on Discord&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"Hands down, this is the best AI coding assistant tool so far."&lt;/em&gt; ‚Äî &lt;a href="https://www.youtube.com/watch?v=MPYFPvxfGZs"&gt;IndyDevDan on YouTube&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"[Aider] changed my daily coding workflows. It's mind-blowing how ...(it)... can change your life."&lt;/em&gt; ‚Äî &lt;a href="https://discord.com/channels/1131200896827654144/1131200896827654149/1258453375620747264"&gt;maledorak on Discord&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"Best agent for actual dev work in existing codebases."&lt;/em&gt; ‚Äî &lt;a href="https://twitter.com/NickADobos/status/1690408967963652097?s=20"&gt;Nick Dobos on X&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"One of my favorite pieces of software. Blazing trails on new paradigms!"&lt;/em&gt; ‚Äî &lt;a href="https://x.com/chris65536/status/1905053299251798432"&gt;Chris Wall on X&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"Aider has been revolutionary for me and my work."&lt;/em&gt; ‚Äî &lt;a href="https://x.com/starryhopeblog/status/1904985812137132056"&gt;Starry Hope on X&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"Try aider! One of the best ways to vibe code."&lt;/em&gt; ‚Äî &lt;a href="https://x.com/Chris65536/status/1905053418961391929"&gt;Chris Wall on X&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"Freaking love Aider."&lt;/em&gt; ‚Äî &lt;a href="https://news.ycombinator.com/item?id=44035015"&gt;hztar on Hacker News&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"Aider is hands down the best. And it's free and opensource."&lt;/em&gt; ‚Äî &lt;a href="https://www.reddit.com/r/ChatGPTCoding/comments/1ik16y6/whats_your_take_on_aider/mbip39n/"&gt;AriyaSavakaLurker on Reddit&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"Aider is also my best friend."&lt;/em&gt; ‚Äî &lt;a href="https://www.reddit.com/r/ChatGPTCoding/comments/1heuvuo/aider_vs_cline_vs_windsurf_vs_cursor/m27dcnb/"&gt;jzn21 on Reddit&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"Try Aider, it's worth it."&lt;/em&gt; ‚Äî &lt;a href="https://www.reddit.com/r/ChatGPTCoding/comments/1heuvuo/aider_vs_cline_vs_windsurf_vs_cursor/m27cp99/"&gt;jorgejhms on Reddit&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"I like aider :)"&lt;/em&gt; ‚Äî &lt;a href="https://x.com/ccui42/status/1904965344999145698"&gt;Chenwei Cui on X&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"Aider is the precision tool of LLM code gen... Minimal, thoughtful and capable of surgical changes ... while keeping the developer in control."&lt;/em&gt; ‚Äî &lt;a href="https://x.com/rsweetland/status/1904963807237259586"&gt;Reilly Sweetland on X&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"Cannot believe aider vibe coded a 650 LOC feature across service and cli today in 1 shot."&lt;/em&gt; - &lt;a href="https://discord.com/channels/1131200896827654144/1131200896827654149/1355675042259796101"&gt;autopoietist on Discord&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"Oh no the secret is out! Yes, Aider is the best coding tool around. I highly, highly recommend it to anyone."&lt;/em&gt; ‚Äî &lt;a href="https://x.com/jodavaho/status/1911154899057795218"&gt;Joshua D Vander Hook on X&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"thanks to aider, i have started and finished three personal projects within the last two days"&lt;/em&gt; ‚Äî &lt;a href="https://x.com/anitaheeder/status/1908338609645904160"&gt;joseph stalzyn on X&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"Been using aider as my daily driver for over a year ... I absolutely love the tool, like beyond words."&lt;/em&gt; ‚Äî &lt;a href="https://discord.com/channels/1131200896827654144/1273248471394291754/1356727448372252783"&gt;koleok on Discord&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"Aider ... is the tool to benchmark against."&lt;/em&gt; ‚Äî &lt;a href="https://news.ycombinator.com/item?id=43930201"&gt;BeetleB on Hacker News&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;"aider is really cool"&lt;/em&gt; ‚Äî &lt;a href="https://x.com/yacineMTB/status/1911224442430124387"&gt;kache on X&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>camel-ai/owl</title>
      <link>https://github.com/camel-ai/owl</link>
      <description>&lt;p&gt;ü¶â OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt; ü¶â OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation &lt;/h1&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://camel-ai.github.io/camel/index.html"&gt;&lt;img src="https://img.shields.io/badge/Documentation-EB3ECC" alt="Documentation" /&gt;&lt;/a&gt; &lt;a href="https://discord.camel-ai.org/"&gt;&lt;img src="https://img.shields.io/discord/1082486657678311454?logo=discord&amp;amp;labelColor=%20%235462eb&amp;amp;logoColor=%20%23f5f5f5&amp;amp;color=%20%235462eb" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://x.com/CamelAIOrg"&gt;&lt;img src="https://img.shields.io/twitter/follow/CamelAIOrg?style=social" alt="X" /&gt;&lt;/a&gt; &lt;a href="https://www.reddit.com/r/CamelAI/"&gt;&lt;img src="https://img.shields.io/reddit/subreddit-subscribers/CamelAI?style=plastic&amp;amp;logo=reddit&amp;amp;label=r%2FCAMEL&amp;amp;labelColor=white" alt="Reddit" /&gt;&lt;/a&gt; &lt;a href="https://ghli.org/camel/wechat.png"&gt;&lt;img src="https://img.shields.io/badge/WeChat-CamelAIOrg-brightgreen?logo=wechat&amp;amp;logoColor=white" alt="Wechat" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/assets/qr_code.jpg"&gt;&lt;img src="https://img.shields.io/badge/WeChat-OWLProject-brightgreen?logo=wechat&amp;amp;logoColor=white" alt="Wechat" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/camel-ai"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-CAMEL--AI-ffc107?color=ffc107&amp;amp;logoColor=white" alt="Hugging Face" /&gt;&lt;/a&gt; &lt;a href="https://github.com/camel-ai/owl/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/camel-ai/owl?label=stars&amp;amp;logo=github&amp;amp;color=brightgreen" alt="Star" /&gt;&lt;/a&gt; &lt;a href="https://github.com/camel-ai/owl/raw/main/licenses/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/License-Apache_2.0-blue.svg?sanitize=true" alt="Package License" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;div align="center" style="background-color: #e3f2fd; padding: 20px; border-radius: 15px; border: 3px solid #1976d2; margin: 25px 0;"&gt; 
 &lt;h2 style="color: #1976d2; margin: 0 0 15px 0; font-size: 1.8em;"&gt; üöÄ &lt;b&gt;Introducing Eigent: The World's First Multi-Agent Workforce Desktop Application&lt;/b&gt; üöÄ &lt;/h2&gt; 
 &lt;p style="font-size: 1.2em; margin: 10px 0; line-height: 1.6;"&gt; &lt;b&gt;Eigent&lt;/b&gt; empowers you to build, manage, and deploy a custom AI workforce that can turn your most complex workflows into automated tasks. &lt;/p&gt; 
 &lt;p style="font-size: 1.1em; margin: 15px 0;"&gt; ‚ú® &lt;b&gt;100% Open Source&lt;/b&gt; ‚Ä¢ üîß &lt;b&gt;Fully Customizable&lt;/b&gt; ‚Ä¢ üîí &lt;b&gt;Privacy-First&lt;/b&gt; ‚Ä¢ ‚ö° &lt;b&gt;Parallel Execution&lt;/b&gt; &lt;/p&gt; 
 &lt;p style="font-size: 1em; margin: 15px 0; font-style: italic;"&gt; Built on CAMEL-AI's acclaimed open-source project, Eigent introduces a Multi-Agent Workforce that boosts productivity through parallel execution, customization, and privacy protection. &lt;/p&gt; 
 &lt;div style="margin-top: 20px;"&gt; 
  &lt;a href="https://github.com/eigent-ai/eigent" style="background-color: #d81b60; color: white; padding: 10px 20px; text-decoration: none; border-radius: 5px; font-weight: bold; margin: 0 5px;"&gt;üîó Visit Eigent Repo&lt;/a&gt; 
  &lt;a href="https://www.eigent.ai/" style="background-color: #1976d2; color: white; padding: 10px 20px; text-decoration: none; border-radius: 5px; font-weight: bold; margin: 0 5px;"&gt;Learn More&lt;/a&gt; 
  &lt;a href="https://www.eigent.ai/download" style="background-color: #43a047; color: white; padding: 10px 20px; text-decoration: none; border-radius: 5px; font-weight: bold; margin: 0 5px;"&gt;Get Started&lt;/a&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;h4 align="center"&gt; &lt;p&gt;&lt;a href="https://github.com/camel-ai/owl/tree/main/README_zh.md"&gt;‰∏≠ÊñáÈòÖËØª&lt;/a&gt; | &lt;a href="https://github.com/camel-ai/owl#community"&gt;Community&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#%EF%B8%8F-installation"&gt;Installation&lt;/a&gt; | &lt;a href="https://github.com/camel-ai/owl/tree/main/owl"&gt;Examples&lt;/a&gt; | &lt;a href="https://arxiv.org/abs/2505.23885"&gt;Paper&lt;/a&gt; |&lt;/p&gt; 
  &lt;!-- [Technical Report](https://hypnotic-mind-6bd.notion.site/OWL-Optimized-Workforce-Learning-for-General-Multi-Agent-Assistance-in-Real-World-Task-Automation-1d4004aeb21380158749c7f84b20643f) | --&gt; &lt;p&gt;&lt;a href="https://github.com/camel-ai/owl#citation"&gt;Citation&lt;/a&gt; | &lt;a href="https://github.com/camel-ai/owl/graphs/contributors"&gt;Contributing&lt;/a&gt; | &lt;a href="https://www.camel-ai.org/"&gt;CAMEL-AI&lt;/a&gt;&lt;/p&gt; &lt;/h4&gt; 
 &lt;div align="center" style="background-color: #f0f7ff; padding: 10px; border-radius: 5px; margin: 15px 0;"&gt; 
  &lt;h3 style="color: #1e88e5; margin: 0;"&gt; üèÜ OWL achieves &lt;span style="color: #d81b60; font-weight: bold; font-size: 1.2em;"&gt;69.09&lt;/span&gt; average score on GAIA benchmark and ranks &lt;span style="color: #d81b60; font-weight: bold; font-size: 1.2em;"&gt;üèÖÔ∏è #1&lt;/span&gt; among open-source frameworks! üèÜ &lt;/h3&gt; 
 &lt;/div&gt; 
 &lt;div align="center"&gt; 
  &lt;p&gt;ü¶â OWL is a cutting-edge framework for multi-agent collaboration that pushes the boundaries of task automation, built on top of the &lt;a href="https://github.com/camel-ai/camel"&gt;CAMEL-AI Framework&lt;/a&gt;.&lt;/p&gt; 
  &lt;!-- OWL achieves **58.18** average score on [GAIA](https://huggingface.co/spaces/gaia-benchmark/leaderboard) benchmark and ranks üèÖÔ∏è #1 among open-source frameworks. --&gt; 
  &lt;p&gt;Our vision is to revolutionize how AI agents collaborate to solve real-world tasks. By leveraging dynamic agent interactions, OWL enables more natural, efficient, and robust task automation across diverse domains.&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/camel-ai/owl/main/assets/owl_architecture.png" alt="" /&gt;&lt;/p&gt; 
 &lt;br /&gt; 
&lt;/div&gt; 
&lt;!-- # Key Features --&gt; 
&lt;h1&gt;üìã Table of Contents&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-table-of-contents"&gt;üìã Table of Contents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-eigent-multi-agent-workforce-desktop-application"&gt;üöÄ Eigent: Multi-Agent Workforce Desktop Application&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-news"&gt;üî• News&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-demo-video"&gt;üé¨ Demo Video&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#%EF%B8%8F-core-features"&gt;‚ú®Ô∏è Core Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#%EF%B8%8F-installation"&gt;üõ†Ô∏è Installation&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#prerequisites"&gt;&lt;strong&gt;Prerequisites&lt;/strong&gt;&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#install-python"&gt;Install Python&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#installation-options"&gt;&lt;strong&gt;Installation Options&lt;/strong&gt;&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#option-1-using-uv-recommended"&gt;Option 1: Using uv (Recommended)&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#option-2-using-venv-and-pip"&gt;Option 2: Using venv and pip&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#option-3-using-conda"&gt;Option 3: Using conda&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#option-4-using-docker"&gt;Option 4: Using Docker&lt;/a&gt; 
      &lt;ul&gt; 
       &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#using-pre-built-image-recommended"&gt;&lt;strong&gt;Using Pre-built Image (Recommended)&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
       &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#building-image-locally"&gt;&lt;strong&gt;Building Image Locally&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
       &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#using-convenience-scripts"&gt;&lt;strong&gt;Using Convenience Scripts&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#setup-environment-variables"&gt;&lt;strong&gt;Setup Environment Variables&lt;/strong&gt;&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#setting-environment-variables-directly"&gt;Setting Environment Variables Directly&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#alternative-using-a-env-file"&gt;Alternative: Using a &lt;code&gt;.env&lt;/code&gt; File&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#mcp-desktop-commander-setup"&gt;&lt;strong&gt;MCP Desktop Commander Setup&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-quick-start"&gt;üöÄ Quick Start&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#basic-usage"&gt;Basic Usage&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#running-with-different-models"&gt;Running with Different Models&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#model-requirements"&gt;Model Requirements&lt;/a&gt; 
      &lt;ul&gt; 
       &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#supported-models"&gt;Supported Models&lt;/a&gt;&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#example-tasks"&gt;Example Tasks&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-toolkits-and-capabilities"&gt;üß∞ Toolkits and Capabilities&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#model-context-protocol-mcp"&gt;Model Context Protocol (MCP)&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#install-nodejs"&gt;&lt;strong&gt;Install Node.js&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#windows"&gt;Windows&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#linux"&gt;Linux&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#mac"&gt;Mac&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#install-playwright-mcp-service"&gt;&lt;strong&gt;Install Playwright MCP Service&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#available-toolkits"&gt;Available Toolkits&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#available-toolkits-1"&gt;Available Toolkits&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#multimodal-toolkits-require-multimodal-model-capabilities"&gt;Multimodal Toolkits (Require multimodal model capabilities)&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#text-based-toolkits"&gt;Text-Based Toolkits&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#customizing-your-configuration"&gt;Customizing Your Configuration&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-web-interface"&gt;üåê Web Interface&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#starting-the-web-ui"&gt;Starting the Web UI&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#features"&gt;Features&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-experiments"&gt;üß™ Experiments&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#%EF%B8%8F-future-plans"&gt;‚è±Ô∏è Future Plans&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-license"&gt;üìÑ License&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-contributing"&gt;ü§ù Contributing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-community"&gt;üî• Community&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-faq"&gt;‚ùì FAQ&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#general-questions"&gt;General Questions&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#experiment-questions"&gt;Experiment Questions&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-exploring-camel-dependency"&gt;üìö Exploring CAMEL Dependency&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#accessing-camel-source-code"&gt;Accessing CAMEL Source Code&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#%EF%B8%8F-cite"&gt;üñäÔ∏è Cite&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-star-history"&gt;‚≠ê Star History&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;üöÄ Eigent: Multi-Agent Workforce Desktop Application&lt;/h1&gt; 
&lt;div align="center" style="background-color: #f5f5f5; padding: 20px; border-radius: 10px; margin: 20px 0;"&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/eigent-ai/eigent"&gt;Eigent&lt;/a&gt;&lt;/strong&gt; is revolutionizing the way we work with AI agents. As the world's first Multi-Agent Workforce desktop application, Eigent transforms complex workflows into automated, intelligent processes.&lt;/p&gt; 
 &lt;h3&gt;Why Eigent?&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;ü§ñ Multi-Agent Collaboration&lt;/strong&gt;: Deploy multiple specialized AI agents that work together seamlessly&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;üöÄ Parallel Execution&lt;/strong&gt;: Boost productivity with agents that can work on multiple tasks simultaneously&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;üé® Full Customization&lt;/strong&gt;: Build and configure your AI workforce to match your specific needs&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;üîí Privacy-First Design&lt;/strong&gt;: Your data stays on your machine - no cloud dependencies required&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;üíØ 100% Open Source&lt;/strong&gt;: Complete transparency and community-driven development&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;Key Capabilities&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Build Custom Workflows&lt;/strong&gt;: Design complex multi-step processes that agents can execute autonomously&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Manage AI Teams&lt;/strong&gt;: Orchestrate multiple agents with different specializations working in concert&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Deploy Instantly&lt;/strong&gt;: From idea to execution in minutes, not hours&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Monitor Progress&lt;/strong&gt;: Real-time visibility into agent activities and task completion&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;Use Cases&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;üìä &lt;strong&gt;Data Analysis&lt;/strong&gt;: Automate complex data processing and analysis workflows&lt;/li&gt; 
  &lt;li&gt;üîç &lt;strong&gt;Research&lt;/strong&gt;: Deploy agents to gather, synthesize, and report on information&lt;/li&gt; 
  &lt;li&gt;üíª &lt;strong&gt;Development&lt;/strong&gt;: Accelerate coding tasks with AI-powered development teams&lt;/li&gt; 
  &lt;li&gt;üìù &lt;strong&gt;Content Creation&lt;/strong&gt;: Generate, edit, and optimize content at scale&lt;/li&gt; 
  &lt;li&gt;ü§ù &lt;strong&gt;Business Automation&lt;/strong&gt;: Transform repetitive business processes into automated workflows&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;Get Started with Eigent&lt;/h3&gt; 
 &lt;p&gt;Eigent is built on top of the OWL framework, leveraging CAMEL-AI's powerful multi-agent capabilities.&lt;/p&gt; 
 &lt;p&gt;üîó &lt;strong&gt;&lt;a href="https://github.com/eigent-ai/eigent"&gt;Visit the Eigent Repository&lt;/a&gt;&lt;/strong&gt; to explore the codebase, contribute, or learn more about building your own AI workforce.&lt;/p&gt; 
 &lt;p&gt;Follow our &lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#%EF%B8%8F-installation"&gt;installation guide&lt;/a&gt; to start building your own AI workforce today!&lt;/p&gt; 
&lt;/div&gt; 
&lt;h1&gt;üî• News&lt;/h1&gt; 
&lt;div align="center" style="background-color: #e8f5e9; padding: 15px; border-radius: 10px; border: 2px solid #4caf50; margin: 20px 0;"&gt; 
 &lt;h3 style="color: #2e7d32; margin: 0; font-size: 1.3em;"&gt; üß© &lt;b&gt;NEW: COMMUNITY AGENT CHALLENGES!&lt;/b&gt; üß© &lt;/h3&gt; 
 &lt;p style="font-size: 1.1em; margin: 10px 0;"&gt; Showcase your creativity by designing unique challenges for AI agents! &lt;br /&gt; Join our community and see your innovative ideas tackled by cutting-edge AI. &lt;/p&gt; 
 &lt;p&gt; &lt;a href="https://github.com/camel-ai/owl/raw/main/community_challenges.md" style="background-color: #2e7d32; color: white; padding: 8px 15px; text-decoration: none; border-radius: 5px; font-weight: bold;"&gt;View &amp;amp; Submit Challenges&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;!-- &lt;div style="background-color: #e3f2fd; padding: 12px; border-radius: 8px; border-left: 4px solid #1e88e5; margin: 10px 0;"&gt;
  &lt;h4 style="color: #1e88e5; margin: 0 0 8px 0;"&gt;
    üéâ Latest Major Update - March 15, 2025
  &lt;/h4&gt;
  &lt;p style="margin: 0;"&gt;
    &lt;b&gt;Significant Improvements:&lt;/b&gt;
    &lt;ul style="margin: 5px 0 0 0; padding-left: 20px;"&gt;
      &lt;li&gt;Restructured web-based UI architecture for enhanced stability üèóÔ∏è&lt;/li&gt;
      &lt;li&gt;Optimized OWL Agent execution mechanisms for better performance üöÄ&lt;/li&gt;
    &lt;/ul&gt;
    &lt;i&gt;Try it now and experience the improved performance in your automation tasks!&lt;/i&gt;
  &lt;/p&gt;
&lt;/div&gt; --&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.09.22]&lt;/strong&gt;: Exicited to announce that OWL has been accepted by NeurIPS 2025!üöÄ Check the latest paper &lt;a href="https://arxiv.org/abs/2505.23885"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.07.21]&lt;/strong&gt;: We open-sourced the training dataset and model checkpoints of OWL project. Training code coming soon. &lt;a href="https://huggingface.co/collections/camel-ai/optimized-workforce-learning-682ef4ab498befb9426e6e27"&gt;huggingface link&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.05.27]&lt;/strong&gt;: We released the technical report of OWL, including more details on the workforce (framework) and optimized workforce learning (training methodology). &lt;a href="https://arxiv.org/abs/2505.23885"&gt;paper&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.05.18]&lt;/strong&gt;: We open-sourced an initial version for replicating workforce experiment on GAIA &lt;a href="https://github.com/camel-ai/owl/tree/gaia69"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.04.18]&lt;/strong&gt;: We uploaded OWL's new GAIA benchmark score of &lt;strong&gt;69.09%&lt;/strong&gt;, ranking #1 among open-source frameworks. Check the technical report &lt;a href="https://hypnotic-mind-6bd.notion.site/OWL-Optimized-Workforce-Learning-for-General-Multi-Agent-Assistance-in-Real-World-Task-Automation-1d4004aeb21380158749c7f84b20643f"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.03.27]&lt;/strong&gt;: Integrate SearxNGToolkit performing web searches using SearxNG search engine.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.03.26]&lt;/strong&gt;: Enhanced Browser Toolkit with multi-browser support for "chrome", "msedge", and "chromium" channels.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.03.25]&lt;/strong&gt;: Supported Gemini 2.5 Pro, added example run code&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.03.21]&lt;/strong&gt;: Integrated OpenRouter model platform, fix bug with Gemini tool calling.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.03.20]&lt;/strong&gt;: Accept header in MCP Toolkit, support automatic playwright installation.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.03.16]&lt;/strong&gt;: Support Bing search, Baidu search.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.03.12]&lt;/strong&gt;: Added Bocha search in SearchToolkit, integrated Volcano Engine model platform, and enhanced Azure and OpenAI Compatible models with structured output and tool calling.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.03.11]&lt;/strong&gt;: We added MCPToolkit, FileWriteToolkit, and TerminalToolkit to enhance OWL agents with MCP tool calling, file writing capabilities, and terminal command execution.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.03.09]&lt;/strong&gt;: We added a web-based user interface that makes it easier to interact with the system.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.03.07]&lt;/strong&gt;: We open-sourced the codebase of the ü¶â OWL project.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.03.03]&lt;/strong&gt;: OWL achieved the #1 position among open-source frameworks on the GAIA benchmark with a score of 58.18.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;üé¨ Demo Video&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/2a2a825d-39ea-45c5-9ba1-f9d58efbc372"&gt;https://github.com/user-attachments/assets/2a2a825d-39ea-45c5-9ba1-f9d58efbc372&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://private-user-images.githubusercontent.com/55657767/420212194-e813fc05-136a-485f-8df3-f10d9b4e63ec.mp4"&gt;https://private-user-images.githubusercontent.com/55657767/420212194-e813fc05-136a-485f-8df3-f10d9b4e63ec.mp4&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;This video demonstrates how to install OWL locally and showcases its capabilities as a cutting-edge framework for multi-agent collaboration: &lt;a href="https://www.youtube.com/watch?v=8XlqVyAZOr8"&gt;https://www.youtube.com/watch?v=8XlqVyAZOr8&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;‚ú®Ô∏è Core Features&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Online Search&lt;/strong&gt;: Support for multiple search engines (including Wikipedia, Google, DuckDuckGo, Baidu, Bocha, etc.) for real-time information retrieval and knowledge acquisition.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multimodal Processing&lt;/strong&gt;: Support for handling internet or local videos, images, and audio data.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Browser Automation&lt;/strong&gt;: Utilize the Playwright framework for simulating browser interactions, including scrolling, clicking, input handling, downloading, navigation, and more.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Document Parsing&lt;/strong&gt;: Extract content from Word, Excel, PDF, and PowerPoint files, converting them into text or Markdown format.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Code Execution&lt;/strong&gt;: Write and execute Python code using interpreter.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Built-in Toolkits&lt;/strong&gt;: Access to a comprehensive set of built-in toolkits including: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Model Context Protocol (MCP)&lt;/strong&gt;: A universal protocol layer that standardizes AI model interactions with various tools and data sources&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Core Toolkits&lt;/strong&gt;: ArxivToolkit, AudioAnalysisToolkit, CodeExecutionToolkit, DalleToolkit, DataCommonsToolkit, ExcelToolkit, GitHubToolkit, GoogleMapsToolkit, GoogleScholarToolkit, ImageAnalysisToolkit, MathToolkit, NetworkXToolkit, NotionToolkit, OpenAPIToolkit, RedditToolkit, SearchToolkit, SemanticScholarToolkit, SymPyToolkit, VideoAnalysisToolkit, WeatherToolkit, BrowserToolkit, and many more for specialized tasks&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;üõ†Ô∏è Installation&lt;/h1&gt; 
&lt;h2&gt;&lt;strong&gt;Prerequisites&lt;/strong&gt;&lt;/h2&gt; 
&lt;h3&gt;Install Python&lt;/h3&gt; 
&lt;p&gt;Before installing OWL, ensure you have Python installed (version 3.10, 3.11, or 3.12 is supported):&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note for GAIA Benchmark Users&lt;/strong&gt;: When running the GAIA benchmark evaluation, please use the &lt;code&gt;gaia58.18&lt;/code&gt; branch which includes a customized version of the CAMEL framework in the &lt;code&gt;owl/camel&lt;/code&gt; directory. This version contains enhanced toolkits with improved stability specifically optimized for the GAIA benchmark compared to the standard CAMEL installation.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Check if Python is installed
python --version

# If not installed, download and install from https://www.python.org/downloads/
# For macOS users with Homebrew:
brew install python@3.10

# For Ubuntu/Debian:
sudo apt update
sudo apt install python3.10 python3.10-venv python3-pip
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;&lt;strong&gt;Installation Options&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;OWL supports multiple installation methods to fit your workflow preferences.&lt;/p&gt; 
&lt;h3&gt;Option 1: Using uv (Recommended)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone github repo
git clone https://github.com/camel-ai/owl.git

# Change directory into project directory
cd owl

# Install uv if you don't have it already
pip install uv

# Create a virtual environment and install dependencies
uv venv .venv --python=3.10

# Activate the virtual environment
# For macOS/Linux
source .venv/bin/activate
# For Windows
.venv\Scripts\activate

# Install CAMEL with all dependencies
uv pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Option 2: Using venv and pip&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone github repo
git clone https://github.com/camel-ai/owl.git

# Change directory into project directory
cd owl

# Create a virtual environment
# For Python 3.10 (also works with 3.11, 3.12)
python3.10 -m venv .venv

# Activate the virtual environment
# For macOS/Linux
source .venv/bin/activate
# For Windows
.venv\Scripts\activate

# Install from requirements.txt
pip install -r requirements.txt --use-pep517
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Option 3: Using conda&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone github repo
git clone https://github.com/camel-ai/owl.git

# Change directory into project directory
cd owl

# Create a conda environment
conda create -n owl python=3.10

# Activate the conda environment
conda activate owl

# Option 1: Install as a package (recommended)
pip install -e .

# Option 2: Install from requirements.txt
pip install -r requirements.txt --use-pep517
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Option 4: Using Docker&lt;/h3&gt; 
&lt;h4&gt;&lt;strong&gt;Using Pre-built Image (Recommended)&lt;/strong&gt;&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# This option downloads a ready-to-use image from Docker Hub
# Fastest and recommended for most users
docker compose up -d

# Run OWL inside the container
docker compose exec owl bash
cd .. &amp;amp;&amp;amp; source .venv/bin/activate
playwright install-deps
xvfb-python examples/run.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;&lt;strong&gt;Building Image Locally&lt;/strong&gt;&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# For users who need to customize the Docker image or cannot access Docker Hub:
# 1. Open docker-compose.yml
# 2. Comment out the "image: mugglejinx/owl:latest" line
# 3. Uncomment the "build:" section and its nested properties
# 4. Then run:
docker compose up -d --build

# Run OWL inside the container
docker compose exec owl bash
cd .. &amp;amp;&amp;amp; source .venv/bin/activate
playwright install-deps
xvfb-python examples/run.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;&lt;strong&gt;Using Convenience Scripts&lt;/strong&gt;&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Navigate to container directory
cd .container

# Make the script executable and build the Docker image
chmod +x build_docker.sh
./build_docker.sh

# Run OWL with your question
./run_in_docker.sh "your question"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;&lt;strong&gt;Setup Environment Variables&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;OWL requires various API keys to interact with different services.&lt;/p&gt; 
&lt;h3&gt;Setting Environment Variables Directly&lt;/h3&gt; 
&lt;p&gt;You can set environment variables directly in your terminal:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;macOS/Linux (Bash/Zsh)&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;export OPENAI_API_KEY="your-openai-api-key-here"
# Add other required API keys as needed
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Windows (Command Prompt)&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-batch"&gt;set OPENAI_API_KEY=your-openai-api-key-here
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Windows (PowerShell)&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-powershell"&gt;$env:OPENAI_API_KEY = "your-openai-api-key-here"
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Environment variables set directly in the terminal will only persist for the current session.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Alternative: Using a &lt;code&gt;.env&lt;/code&gt; File&lt;/h3&gt; 
&lt;p&gt;If you prefer using a &lt;code&gt;.env&lt;/code&gt; file instead, you can:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Copy and Rename the Template&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# For macOS/Linux
cd owl
cp .env_template .env

# For Windows
cd owl
copy .env_template .env
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Alternatively, you can manually create a new file named &lt;code&gt;.env&lt;/code&gt; in the owl directory and copy the contents from &lt;code&gt;.env_template&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Configure Your API Keys&lt;/strong&gt;: Open the &lt;code&gt;.env&lt;/code&gt; file in your preferred text editor and insert your API keys in the corresponding fields.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: For the minimal example (&lt;code&gt;examples/run_mini.py&lt;/code&gt;), you only need to configure the LLM API key (e.g., &lt;code&gt;OPENAI_API_KEY&lt;/code&gt;).&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;&lt;strong&gt;MCP Desktop Commander Setup&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;If using MCP Desktop Commander within Docker, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npx -y @wonderwhy-er/desktop-commander setup --force-file-protocol
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more detailed Docker usage instructions, including cross-platform support, optimized configurations, and troubleshooting, please refer to &lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/.container/DOCKER_README_en.md"&gt;DOCKER_README.md&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;üöÄ Quick Start&lt;/h1&gt; 
&lt;h2&gt;Basic Usage&lt;/h2&gt; 
&lt;p&gt;After installation and setting up your environment variables, you can start using OWL right away:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python examples/run.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Running with Different Models&lt;/h2&gt; 
&lt;h3&gt;Model Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Tool Calling&lt;/strong&gt;: OWL requires models with robust tool calling capabilities to interact with various toolkits. Models must be able to understand tool descriptions, generate appropriate tool calls, and process tool outputs.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Multimodal Understanding&lt;/strong&gt;: For tasks involving web interaction, image analysis, or video processing, models with multimodal capabilities are required to interpret visual content and context.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Supported Models&lt;/h4&gt; 
&lt;p&gt;For information on configuring AI models, please refer to our &lt;a href="https://docs.camel-ai.org/key_modules/models.html#supported-model-platforms-in-camel"&gt;CAMEL models documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: For optimal performance, we strongly recommend using OpenAI models (GPT-4 or later versions). Our experiments show that other models may result in significantly lower performance on complex tasks and benchmarks, especially those requiring advanced multi-modal understanding and tool use.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;OWL supports various LLM backends, though capabilities may vary depending on the model's tool calling and multimodal abilities. You can use the following scripts to run with different models:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run with Claude model
python examples/run_claude.py

# Run with Qwen model
python examples/run_qwen_zh.py

# Run with Deepseek model
python examples/run_deepseek_zh.py

# Run with other OpenAI-compatible models
python examples/run_openai_compatible_model.py

# Run with Gemini model
python examples/run_gemini.py

# Run with Azure OpenAI
python examples/run_azure_openai.py

# Run with Ollama
python examples/run_ollama.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For a simpler version that only requires an LLM API key, you can try our minimal example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python examples/run_mini.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can run OWL agent with your own task by modifying the &lt;code&gt;examples/run.py&lt;/code&gt; script:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Define your own task
task = "Task description here."

society = construct_society(question)
answer, chat_history, token_count = run_society(society)

print(f"\033[94mAnswer: {answer}\033[0m")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For uploading files, simply provide the file path along with your question:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Task with a local file (e.g., file path: `tmp/example.docx`)
task = "What is in the given DOCX file? Here is the file path: tmp/example.docx"

society = construct_society(question)
answer, chat_history, token_count = run_society(society)
print(f"\033[94mAnswer: {answer}\033[0m")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;OWL will then automatically invoke document-related tools to process the file and extract the answer.&lt;/p&gt; 
&lt;h3&gt;Example Tasks&lt;/h3&gt; 
&lt;p&gt;Here are some tasks you can try with OWL:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;"Find the latest stock price for Apple Inc."&lt;/li&gt; 
 &lt;li&gt;"Analyze the sentiment of recent tweets about climate change"&lt;/li&gt; 
 &lt;li&gt;"Help me debug this Python code: [your code here]"&lt;/li&gt; 
 &lt;li&gt;"Summarize the main points from this research paper: [paper URL]"&lt;/li&gt; 
 &lt;li&gt;"Create a data visualization for this dataset: [dataset path]"&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;üß∞ Toolkits and Capabilities&lt;/h1&gt; 
&lt;h2&gt;Model Context Protocol (MCP)&lt;/h2&gt; 
&lt;p&gt;OWL's MCP integration provides a standardized way for AI models to interact with various tools and data sources:&lt;/p&gt; 
&lt;p&gt;Before using MCP, you need to install Node.js first.&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;Install Node.js&lt;/strong&gt;&lt;/h3&gt; 
&lt;h3&gt;Windows&lt;/h3&gt; 
&lt;p&gt;Download the official installer: &lt;a href="https://nodejs.org/en"&gt;Node.js&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Check "Add to PATH" option during installation.&lt;/p&gt; 
&lt;h3&gt;Linux&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt update
sudo apt install nodejs npm -y
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Mac&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;brew install node
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;&lt;strong&gt;Install Playwright MCP Service&lt;/strong&gt;&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npm install -g @executeautomation/playwright-mcp-server
npx playwright install-deps
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Try our comprehensive MCP examples:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;examples/run_mcp.py&lt;/code&gt; - Basic MCP functionality demonstration (local call, requires dependencies)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;examples/run_mcp_sse.py&lt;/code&gt; - Example using the SSE protocol (Use remote services, no dependencies)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Available Toolkits&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: Effective use of toolkits requires models with strong tool calling capabilities. For multimodal toolkits (Web, Image, Video), models must also have multimodal understanding abilities.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;OWL supports various toolkits that can be customized by modifying the &lt;code&gt;tools&lt;/code&gt; list in your script:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Configure toolkits
tools = [
    *BrowserToolkit(headless=False).get_tools(),  # Browser automation
    *VideoAnalysisToolkit(model=models["video"]).get_tools(),
    *AudioAnalysisToolkit().get_tools(),  # Requires OpenAI Key
    *CodeExecutionToolkit(sandbox="subprocess").get_tools(),
    *ImageAnalysisToolkit(model=models["image"]).get_tools(),
    SearchToolkit().search_duckduckgo,
    SearchToolkit().search_google,  # Comment out if unavailable
    SearchToolkit().search_wiki,
    SearchToolkit().search_bocha,
    SearchToolkit().search_baidu,
    *ExcelToolkit().get_tools(),
    *DocumentProcessingToolkit(model=models["document"]).get_tools(),
    *FileWriteToolkit(output_dir="./").get_tools(),
]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Available Toolkits&lt;/h2&gt; 
&lt;p&gt;Key toolkits include:&lt;/p&gt; 
&lt;h3&gt;Multimodal Toolkits (Require multimodal model capabilities)&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;BrowserToolkit&lt;/strong&gt;: Browser automation for web interaction and navigation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;VideoAnalysisToolkit&lt;/strong&gt;: Video processing and content analysis&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ImageAnalysisToolkit&lt;/strong&gt;: Image analysis and interpretation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Text-Based Toolkits&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;AudioAnalysisToolkit&lt;/strong&gt;: Audio processing (requires OpenAI API)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;CodeExecutionToolkit&lt;/strong&gt;: Python code execution and evaluation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;SearchToolkit&lt;/strong&gt;: Web searches (Google, DuckDuckGo, Wikipedia)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;DocumentProcessingToolkit&lt;/strong&gt;: Document parsing (PDF, DOCX, etc.)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Additional specialized toolkits: ArxivToolkit, GitHubToolkit, GoogleMapsToolkit, MathToolkit, NetworkXToolkit, NotionToolkit, RedditToolkit, WeatherToolkit, and more. For a complete list, see the &lt;a href="https://docs.camel-ai.org/key_modules/tools.html#built-in-toolkits"&gt;CAMEL toolkits documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Customizing Your Configuration&lt;/h2&gt; 
&lt;p&gt;To customize available tools:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# 1. Import toolkits
from camel.toolkits import BrowserToolkit, SearchToolkit, CodeExecutionToolkit

# 2. Configure tools list
tools = [
    *BrowserToolkit(headless=True).get_tools(),
    SearchToolkit().search_wiki,
    *CodeExecutionToolkit(sandbox="subprocess").get_tools(),
]

# 3. Pass to assistant agent
assistant_agent_kwargs = {"model": models["assistant"], "tools": tools}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Selecting only necessary toolkits optimizes performance and reduces resource usage.&lt;/p&gt; 
&lt;h1&gt;üåê Web Interface&lt;/h1&gt; 
&lt;div align="center" style="background-color: #f0f7ff; padding: 15px; border-radius: 10px; border: 2px solid #1e88e5; margin: 20px 0;"&gt; 
 &lt;h3 style="color: #1e88e5; margin: 0;"&gt; üöÄ Enhanced Web Interface Now Available! &lt;/h3&gt; 
 &lt;p style="margin: 10px 0;"&gt; Experience improved system stability and optimized performance with our latest update. Start exploring the power of OWL through our user-friendly interface! &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;Starting the Web UI&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Start the Chinese version
python owl/webapp_zh.py

# Start the English version
python owl/webapp.py

# Start the Japanese version
python owl/webapp_jp.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Easy Model Selection&lt;/strong&gt;: Choose between different models (OpenAI, Qwen, DeepSeek, etc.)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Environment Variable Management&lt;/strong&gt;: Configure your API keys and other settings directly from the UI&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Interactive Chat Interface&lt;/strong&gt;: Communicate with OWL agents through a user-friendly interface&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Task History&lt;/strong&gt;: View the history and results of your interactions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The web interface is built using Gradio and runs locally on your machine. No data is sent to external servers beyond what's required for the model API calls you configure.&lt;/p&gt; 
&lt;h1&gt;üß™ Experiments&lt;/h1&gt; 
&lt;p&gt;To reproduce OWL's GAIA benchmark score: Furthermore, to ensure optimal performance on the GAIA benchmark, please note that our &lt;code&gt;gaia69&lt;/code&gt; branch includes a customized version of the CAMEL framework in the &lt;code&gt;owl/camel&lt;/code&gt; directory. This version contains enhanced toolkits with improved stability for gaia benchmark compared to the standard CAMEL installation.&lt;/p&gt; 
&lt;p&gt;When running the benchmark evaluation:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Switch to the &lt;code&gt;gaia69&lt;/code&gt; branch:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git checkout gaia69
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run the evaluation script:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;python run_gaia_workforce_claude.py
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;This will execute the same configuration that achieved our top-ranking performance on the GAIA benchmark.&lt;/p&gt; 
&lt;h1&gt;‚è±Ô∏è Future Plans&lt;/h1&gt; 
&lt;p&gt;We're continuously working to improve OWL. Here's what's on our roadmap:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Write a technical blog post detailing our exploration and insights in multi-agent collaboration in real-world tasks&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Enhance the toolkit ecosystem with more specialized tools for domain-specific tasks&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Develop more sophisticated agent interaction patterns and communication protocols&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Improve performance on complex multi-step reasoning tasks&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;üìÑ License&lt;/h1&gt; 
&lt;p&gt;The source code is licensed under Apache 2.0.&lt;/p&gt; 
&lt;h1&gt;ü§ù Contributing&lt;/h1&gt; 
&lt;p&gt;We welcome contributions from the community! Here's how you can help:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Read our &lt;a href="https://github.com/camel-ai/camel/raw/master/CONTRIBUTING.md"&gt;Contribution Guidelines&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Check &lt;a href="https://github.com/camel-ai/camel/issues"&gt;open issues&lt;/a&gt; or create new ones&lt;/li&gt; 
 &lt;li&gt;Submit pull requests with your improvements&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Current Issues Open for Contribution:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/camel-ai/camel/issues/1915"&gt;#1915&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/camel-ai/camel/issues/2190"&gt;#2190&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/camel-ai/camel/issues/2165"&gt;#2165&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/camel-ai/camel/issues/2121"&gt;#2121&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/camel-ai/camel/issues/1908"&gt;#1908&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/camel-ai/camel/issues/1538"&gt;#1538&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/camel-ai/camel/issues/1481"&gt;#1481&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To take on an issue, simply leave a comment stating your interest.&lt;/p&gt; 
&lt;h1&gt;üî• Community&lt;/h1&gt; 
&lt;p&gt;Join us (&lt;a href="https://discord.camel-ai.org/"&gt;&lt;em&gt;Discord&lt;/em&gt;&lt;/a&gt; or &lt;a href="https://ghli.org/camel/wechat.png"&gt;&lt;em&gt;WeChat&lt;/em&gt;&lt;/a&gt;) in pushing the boundaries of finding the scaling laws of agents.&lt;/p&gt; 
&lt;p&gt;Join us for further discussions!&lt;/p&gt; 
&lt;!-- ![](./assets/community.png) --&gt; 
&lt;img src="https://raw.githubusercontent.com/camel-ai/owl/main/assets/community_code.jpeg" width="50%" /&gt; 
&lt;h1&gt;‚ùì FAQ&lt;/h1&gt; 
&lt;h2&gt;General Questions&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Q: Why don't I see Chrome running locally after starting the example script?&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;A: If OWL determines that a task can be completed using non-browser tools (such as search or code execution), the browser will not be launched. The browser window will only appear when OWL determines that browser-based interaction is necessary.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Q: Which Python version should I use?&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;A: OWL supports Python 3.10, 3.11, and 3.12.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Q: How can I contribute to the project?&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;A: See our &lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-contributing"&gt;Contributing&lt;/a&gt; section for details on how to get involved. We welcome contributions of all kinds, from code improvements to documentation updates.&lt;/p&gt; 
&lt;h2&gt;Experiment Questions&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Q: Which CAMEL version should I use for replicate the role playing result?&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;A: We provide a modified version of CAMEL (owl/camel) in the gaia58.18 branch. Please make sure you use this CAMEL version for your experiments.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Q: Why are my experiment results lower than the reported numbers?&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;A: Since the GAIA benchmark evaluates LLM agents in a realistic world, it introduces a significant amount of randomness. Based on user feedback, one of the most common issues for replication is, for example, agents being blocked on certain webpages due to network reasons. We have uploaded a keywords matching script to help quickly filter out these errors &lt;a href="https://github.com/camel-ai/owl/raw/gaia58.18/owl/filter_failed_cases.py"&gt;here&lt;/a&gt;. You can also check this &lt;a href="https://hypnotic-mind-6bd.notion.site/OWL-Optimized-Workforce-Learning-for-General-Multi-Agent-Assistance-in-Real-World-Task-Automation-1d4004aeb21380158749c7f84b20643f?pvs=74"&gt;technical report&lt;/a&gt; for more details when evaluating LLM agents in realistic open-world environments.&lt;/p&gt; 
&lt;h1&gt;üìö Exploring CAMEL Dependency&lt;/h1&gt; 
&lt;p&gt;OWL is built on top of the &lt;a href="https://github.com/camel-ai/camel"&gt;CAMEL&lt;/a&gt; Framework, here's how you can explore the CAMEL source code and understand how it works with OWL:&lt;/p&gt; 
&lt;h2&gt;Accessing CAMEL Source Code&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone the CAMEL repository
git clone https://github.com/camel-ai/camel.git
cd camel
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;üñäÔ∏è Cite&lt;/h1&gt; 
&lt;p&gt;If you find this repo useful, please cite:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@misc{hu2025owl,
      title={OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation}, 
      author={Mengkang Hu and Yuhang Zhou and Wendong Fan and Yuzhou Nie and Bowei Xia and Tao Sun and Ziyu Ye and Zhaoxuan Jin and Yingru Li and Qiguang Chen and Zeyu Zhang and Yifeng Wang and Qianshuo Ye and Bernard Ghanem and Ping Luo and Guohao Li},
      year={2025},
      eprint={2505.23885},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2505.23885}, 
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;‚≠ê Star History&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#camel-ai/owl&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=camel-ai/owl&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>mealie-recipes/mealie</title>
      <link>https://github.com/mealie-recipes/mealie</link>
      <description>&lt;p&gt;Mealie is a self hosted recipe manager and meal planner with a RestAPI backend and a reactive frontend application built in Vue for a pleasant user experience for the whole family. Easily add recipes into your database by providing the url and mealie will automatically import the relevant data or add a family recipe with the UI editor&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="https://github.com/mealie-recipes/mealie/releases"&gt;&lt;img src="https://img.shields.io/github/v/release/mealie-recipes/mealie?style=flat-square&amp;amp;label=latest%20release" alt="Latest Release" /&gt;&lt;/a&gt; &lt;a href="https://github.com/mealie-recipes/mealie/graphs/contributors"&gt;&lt;img src="https://img.shields.io/github/contributors/mealie-recipes/mealie.svg?style=flat-square" alt="Contributors" /&gt;&lt;/a&gt; &lt;a href="https://github.com/mealie-recipes/mealie/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/mealie-recipes/mealie.svg?style=flat-square" alt="Stargazers" /&gt;&lt;/a&gt; &lt;a href="https://github.com/mealie-recipes/mealie/issues"&gt;&lt;img src="https://img.shields.io/github/issues/mealie-recipes/mealie.svg?style=flat-square" alt="Issues" /&gt;&lt;/a&gt; &lt;a href="https://github.com/mealie-recipes/mealie/raw/mealie-next/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/mealie-recipes/mealie.svg?style=flat-square" alt="AGPL License" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/hkotel/mealie"&gt;&lt;img src="https://img.shields.io/docker/pulls/hkotel/mealie?style=flat-square" alt="Docker Pulls" /&gt;&lt;/a&gt; &lt;a href="https://github.com/mealie-recipes/mealie/pkgs/container/mealie"&gt;&lt;img src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fipitio.github.io%2Fbackage%2Fmealie-recipes%2Fmealie%2Fmealie.json&amp;amp;query=%24.downloads&amp;amp;style=flat-square&amp;amp;label=ghcr%20pulls" alt="GHCR Pulls" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;!-- PROJECT LOGO --&gt; 
&lt;br /&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/mealie-recipes/mealie"&gt; 
  &lt;svg style="width:100px;height:100px" viewbox="0 0 24 24"&gt; 
   &lt;path fill="currentColor" d="M8.1,13.34L3.91,9.16C2.35,7.59 2.35,5.06 3.91,3.5L10.93,10.5L8.1,13.34M13.41,13L20.29,19.88L18.88,21.29L12,14.41L5.12,21.29L3.71,19.88L13.36,10.22L13.16,10C12.38,9.23 12.38,7.97 13.16,7.19L17.5,2.82L18.43,3.74L15.19,7L16.15,7.94L19.39,4.69L20.31,5.61L17.06,8.85L18,9.81L21.26,6.56L22.18,7.5L17.81,11.84C17.03,12.62 15.77,12.62 15,11.84L14.78,11.64L13.41,13Z" /&gt; 
  &lt;/svg&gt; &lt;/a&gt; &lt;/p&gt;
&lt;h3 align="center"&gt;Mealie&lt;/h3&gt; 
&lt;p align="center"&gt; A Place For All Your Recipes &lt;br /&gt; &lt;a href="https://docs.mealie.io/"&gt;&lt;strong&gt;Explore the docs ¬ª&lt;/strong&gt;&lt;/a&gt; &lt;a href="https://github.com/mealie-recipes/mealie"&gt; &lt;/a&gt; &lt;br /&gt; &lt;a href="https://demo.mealie.io/"&gt;View Demo&lt;/a&gt; ¬∑ &lt;a href="https://github.com/mealie-recipes/mealie/issues"&gt;Report Bug&lt;/a&gt; ¬∑ &lt;a href="https://github.com/mealie-recipes/mealie/pkgs/container/mealie"&gt;GitHub Container Registry&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;&lt;a href="https://docs.mealie.io"&gt;&lt;img src="https://raw.githubusercontent.com/mealie-recipes/mealie/mealie-next/docs/docs/assets/img/home_screenshot.png" alt="Product Name Screen Shot" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;About The Project&lt;/h1&gt; 
&lt;p&gt;Mealie is a self hosted recipe manager, meal planner and shopping list with a RestAPI backend and a reactive frontend built in Vue for a pleasant user experience for the whole family. Easily add recipes into your database by providing the URL and Mealie will automatically import the relevant data, or add a family recipe with the UI editor. Mealie also provides an API for interactions from 3rd party applications.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://discord.gg/QuStdQGSGK"&gt;Remember to join the Discord&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.mealie.io/"&gt;Documentation&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Recipe imports: Create recipes, by &lt;strong&gt;importing from a URL&lt;/strong&gt; or entering data manually&lt;/li&gt; 
 &lt;li&gt;Meal Planner: Use the &lt;strong&gt;Meal Planner&lt;/strong&gt; to plan your what you'll cook for the next week&lt;/li&gt; 
 &lt;li&gt;Shopping List: Put the necessary ingredients on your &lt;strong&gt;Shopping List&lt;/strong&gt;, organised into sections of your local supermarket&lt;/li&gt; 
 &lt;li&gt;Cookbooks: Group recipes into &lt;strong&gt;Cookbooks&lt;/strong&gt; based on your own criteria&lt;/li&gt; 
 &lt;li&gt;Docker: Easy &lt;strong&gt;Docker&lt;/strong&gt; deployment&lt;/li&gt; 
 &lt;li&gt;Localisation: &lt;strong&gt;Translations&lt;/strong&gt; for 35+ languages&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- CONTRIBUTING --&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions are what make the open source community such an amazing place to learn, inspire, and create. Any contributions you make are &lt;strong&gt;greatly appreciated&lt;/strong&gt;. If you're going to be working on the code-base, you'll want to use the nightly documentation to ensure you get the latest information.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;See the &lt;a href="https://nightly.mealie.io/contributors/developers-guide/code-contributions/"&gt;Contributors Guide&lt;/a&gt; for help getting started.&lt;/li&gt; 
 &lt;li&gt;We use &lt;a href="https://code.visualstudio.com/docs/remote/containers"&gt;VSCode Dev Containers&lt;/a&gt; to make it easy for contributors to get started!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If you are not a coder, you can still contribute financially. Financial contributions help me prioritize working on this project over others and helps me know that there is a real demand for project development.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.buymeacoffee.com/haykot" target="_blank"&gt;&lt;img src="https://cdn.buymeacoffee.com/buttons/v2/default-green.png" alt="Buy Me A Coffee" style="height: 30px !important;width: 107px !important;" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Translations&lt;/h3&gt; 
&lt;p&gt;Translations can be a great way for &lt;strong&gt;non-coders&lt;/strong&gt; to contribute to the project. We use &lt;a href="https://crowdin.com/project/mealie"&gt;Crowdin&lt;/a&gt; to allow several contributors to work on translating Mealie. You can simply help by voting for your preferred translations, or even by completely translating Mealie into a new language.&lt;/p&gt; 
&lt;p&gt;For more information, check out the translation page on the &lt;a href="https://nightly.mealie.io/contributors/translating/"&gt;contributor's guide&lt;/a&gt;.&lt;/p&gt; 
&lt;!-- LICENSE --&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Distributed under the AGPL License. See &lt;code&gt;LICENSE&lt;/code&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;Sponsors&lt;/h2&gt; 
&lt;p&gt;Huge thanks to all the sponsors of this project on &lt;a href="https://github.com/sponsors/hay-kot"&gt;Github Sponsors&lt;/a&gt; and Buy Me a Coffee. Without you, this project would surely not be possible.&lt;/p&gt; 
&lt;p&gt;Thanks to Depot for providing build instances for our Docker image builds.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://depot.dev?utm_source=Mealie"&gt;&lt;img src="https://depot.dev/badges/built-with-depot.svg?sanitize=true" alt="Built with Depot" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;!-- MARKDOWN LINKS &amp; IMAGES --&gt; 
&lt;!-- https://www.markdownguide.org/basic-syntax/#reference-style-links --&gt;</description>
    </item>
    
    <item>
      <title>OpenBB-finance/OpenBB</title>
      <link>https://github.com/OpenBB-finance/OpenBB</link>
      <description>&lt;p&gt;Financial data platform for analysts, quants and AI agents.&lt;/p&gt;&lt;hr&gt;&lt;br /&gt; 
&lt;img src="https://github.com/OpenBB-finance/OpenBB/raw/develop/images/odp-light.svg?raw=true#gh-light-mode-only" alt="Open Data Platform by OpenBB logo" width="600" /&gt; 
&lt;img src="https://github.com/OpenBB-finance/OpenBB/raw/develop/images/odp-dark.svg?raw=true#gh-dark-mode-only" alt="Open Data Platform by OpenBB logo" width="600" /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;a href="https://x.com/openbb_finance"&gt;&lt;img src="https://img.shields.io/twitter/url/https/twitter.com/openbb_finance.svg?style=social&amp;amp;label=Follow%20%40openbb_finance" alt="Twitter" /&gt;&lt;/a&gt; &lt;a href="https://discord.com/invite/xPHTuHCmuV"&gt;&lt;img src="https://img.shields.io/discord/831165782750789672" alt="Discord Shield" /&gt;&lt;/a&gt; &lt;a href="https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/OpenBB-finance/OpenBB"&gt;&lt;img src="https://img.shields.io/static/v1?label=Dev%20Containers&amp;amp;message=Open&amp;amp;color=blue&amp;amp;logo=visualstudiocode" alt="Open in Dev Containers" /&gt;&lt;/a&gt; &lt;a href="https://codespaces.new/OpenBB-finance/OpenBB"&gt; &lt;img src="https://github.com/codespaces/badge.svg?sanitize=true" height="20" /&gt; &lt;/a&gt; &lt;a target="_blank" href="https://colab.research.google.com/github/OpenBB-finance/OpenBB/blob/develop/examples/googleColab.ipynb"&gt; &lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt; &lt;/a&gt; &lt;a href="https://pypi.org/project/openbb/"&gt;&lt;img src="https://img.shields.io/pypi/v/openbb?color=blue&amp;amp;label=PyPI%20Package" alt="PyPI" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Open Data Platform by OpenBB (ODP) is the open-source toolset that helps data engineers integrate proprietary, licensed, and public data sources into downstream applications like AI copilots and research dashboards.&lt;/p&gt; 
&lt;p&gt;ODP operates as the "connect once, consume everywhere" infrastructure layer that consolidates and exposes data to multiple surfaces at once: Python environments for quants, OpenBB Workspace and Excel for analysts, MCP servers for AI agents, and REST APIs for other applications.&lt;/p&gt; 
&lt;a href="https://pro.openbb.co"&gt; 
 &lt;div align="center"&gt; 
  &lt;img src="https://openbb-cms.directus.app/assets/70b971ef-7a7e-486e-b5ae-1cc602f2162c.png" alt="Logo" width="1000" /&gt; 
 &lt;/div&gt; &lt;/a&gt; 
&lt;p&gt;Get started with: &lt;code&gt;pip install openbb&lt;/code&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openbb import obb
output = obb.equity.price.historical("AAPL")
df = output.to_dataframe()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Data integrations available can be found here: &lt;a href="https://docs.openbb.co/python/reference"&gt;https://docs.openbb.co/python/reference&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;OpenBB Workspace&lt;/h2&gt; 
&lt;p&gt;While the Open Data Platform provides the open-source data integration foundation, &lt;strong&gt;OpenBB Workspace&lt;/strong&gt; offers the enterprise UI for analysts to visualize datasets and leverage AI agents. The platform's "connect once, consume everywhere" architecture enables seamless integration between the two.&lt;/p&gt; 
&lt;p&gt;You can find OpenBB Workspace at &lt;a href="https://pro.openbb.co"&gt;https://pro.openbb.co&lt;/a&gt;. &lt;a href="https://pro.openbb.co"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;a href="https://pro.openbb.co"&gt; 
 &lt;div align="center"&gt; 
  &lt;img src="https://openbb-cms.directus.app/assets/f69b6aaf-0821-4bc8-a43c-715e03a924ef.png" alt="Logo" width="1000" /&gt; 
 &lt;/div&gt; &lt;/a&gt; 
&lt;p&gt;Data integration:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;You can learn more about adding data to the OpenBB workspace from the &lt;a href="https://docs.openbb.co/workspace"&gt;docs&lt;/a&gt; or &lt;a href="https://github.com/OpenBB-finance/backends-for-openbb"&gt;this open source repository&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;AI Agents integration:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;You can learn more about adding AI agents to the OpenBB workspace from &lt;a href="https://github.com/OpenBB-finance/agents-for-openbb"&gt;this open source repository&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Integrating Open Data Platform to the OpenBB Workspace&lt;/h3&gt; 
&lt;p&gt;Connect this library to the OpenBB Workspace with a few simple commands, in a Python (3.9.21 - 3.12) environment.&lt;/p&gt; 
&lt;h4&gt;Run an ODP backend&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Install the packages.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip install "openbb[all]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Start the API server over localhost.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;openbb-api
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will launch a FastAPI server, via Uvicorn, at &lt;code&gt;127.0.0.1:6900&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;You can check that it works by going to &lt;a href="http://127.0.0.1:6900"&gt;http://127.0.0.1:6900&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;Integrate the ODP Backend to OpenBB Workspace&lt;/h4&gt; 
&lt;p&gt;Sign-in to the &lt;a href="https://pro.openbb.co/"&gt;OpenBB Workspace&lt;/a&gt;, and follow the following steps:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/75cffb4a-5e95-470a-b9d0-6ffd4067e069" alt="CleanShot 2025-05-17 at 09 51 56@2x" /&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Go to the "Apps" tab&lt;/li&gt; 
 &lt;li&gt;Click on "Connect backend"&lt;/li&gt; 
 &lt;li&gt;Fill in the form with: Name: Open Data Platform URL: &lt;a href="http://127.0.0.1:6900"&gt;http://127.0.0.1:6900&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Click on "Test". You should get a "Test successful" with the number of apps found.&lt;/li&gt; 
 &lt;li&gt;Click on "Add".&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;That's it.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;!-- TABLE OF CONTENTS --&gt; 
&lt;details closed="closed"&gt; 
 &lt;summary&gt;&lt;h2 style="display: inline-block"&gt;Table of Contents&lt;/h2&gt;&lt;/summary&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#1-installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#2-contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#3-license"&gt;License&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#4-disclaimer"&gt;Disclaimer&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#5-contacts"&gt;Contacts&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#6-star-history"&gt;Star History&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#7-contributors"&gt;Contributors&lt;/a&gt;&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;h2&gt;1. Installation&lt;/h2&gt; 
&lt;p&gt;The ODP Python Package can be installed from &lt;a href="https://pypi.org/project/openbb/"&gt;PyPI package&lt;/a&gt; by running &lt;code&gt;pip install openbb&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;or by cloning the repository directly with &lt;code&gt;git clone https://github.com/OpenBB-finance/OpenBB.git&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Please find more about the installation process, in the &lt;a href="https://docs.openbb.co/python/installation"&gt;OpenBB Documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;ODP CLI installation&lt;/h3&gt; 
&lt;p&gt;The ODP CLI is a command-line interface that allows you to access the ODP directly from your command line.&lt;/p&gt; 
&lt;p&gt;It can be installed by running &lt;code&gt;pip install openbb-cli&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;or by cloning the repository directly with &lt;code&gt;git clone https://github.com/OpenBB-finance/OpenBB.git&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Please find more about the installation process in the &lt;a href="https://docs.openbb.co/cli/installation"&gt;OpenBB Documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;2. Contributing&lt;/h2&gt; 
&lt;p&gt;There are three main ways of contributing to this project. (Hopefully you have starred the project by now ‚≠êÔ∏è)&lt;/p&gt; 
&lt;h3&gt;Become a Contributor&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;More information on our &lt;a href="https://docs.openbb.co/python/developer"&gt;Developer Documentation&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Create a GitHub ticket&lt;/h3&gt; 
&lt;p&gt;Before creating a ticket make sure the one you are creating doesn't exist already &lt;a href="https://github.com/OpenBB-finance/OpenBB/issues"&gt;among the existing issues&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&amp;amp;labels=bug&amp;amp;template=bug_report.md&amp;amp;title=%5BBug%5D"&gt;Report bug&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&amp;amp;labels=enhancement&amp;amp;template=enhancement.md&amp;amp;title=%5BIMPROVE%5D"&gt;Suggest improvement&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&amp;amp;labels=new+feature&amp;amp;template=feature_request.md&amp;amp;title=%5BFR%5D"&gt;Request a feature&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Provide feedback&lt;/h3&gt; 
&lt;p&gt;We are most active on &lt;a href="https://openbb.co/discord"&gt;our Discord&lt;/a&gt;, but feel free to reach out to us in any of &lt;a href="https://openbb.co/links"&gt;our social media&lt;/a&gt; for feedback.&lt;/p&gt; 
&lt;h2&gt;3. License&lt;/h2&gt; 
&lt;p&gt;Distributed under the AGPLv3 License. See &lt;a href="https://github.com/OpenBB-finance/OpenBB/raw/main/LICENSE"&gt;LICENSE&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;4. Disclaimer&lt;/h2&gt; 
&lt;p&gt;Trading in financial instruments involves high risks including the risk of losing some, or all, of your investment amount, and may not be suitable for all investors.&lt;/p&gt; 
&lt;p&gt;Before deciding to trade in a financial instrument you should be fully informed of the risks and costs associated with trading the financial markets, carefully consider your investment objectives, level of experience, and risk appetite, and seek professional advice where needed.&lt;/p&gt; 
&lt;p&gt;The data contained in the Open Data Platform is not necessarily accurate.&lt;/p&gt; 
&lt;p&gt;OpenBB and any provider of the data contained in this website will not accept liability for any loss or damage as a result of your trading, or your reliance on the information displayed.&lt;/p&gt; 
&lt;p&gt;All names, logos, and brands of third parties that may be referenced in our sites, products or documentation are trademarks of their respective owners. Unless otherwise specified, OpenBB and its products and services are not endorsed by, sponsored by, or affiliated with these third parties.&lt;/p&gt; 
&lt;p&gt;Our use of these names, logos, and brands is for identification purposes only, and does not imply any such endorsement, sponsorship, or affiliation.&lt;/p&gt; 
&lt;h2&gt;5. Contacts&lt;/h2&gt; 
&lt;p&gt;If you have any questions about the platform or anything OpenBB, feel free to email us at &lt;code&gt;support@openbb.co&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;If you want to say hi, or are interested in partnering with us, feel free to reach us at &lt;code&gt;hello@openbb.co&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Any of our social media platforms: &lt;a href="https://openbb.co/links"&gt;openbb.co/links&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;6. Star History&lt;/h2&gt; 
&lt;p&gt;This is a proxy of our growth and that we are just getting started.&lt;/p&gt; 
&lt;p&gt;But for more metrics important to us check &lt;a href="https://openbb.co/open"&gt;openbb.co/open&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://api.star-history.com/svg?repos=openbb-finance/OpenBB&amp;amp;type=Date&amp;amp;theme=dark"&gt;&lt;img src="https://api.star-history.com/svg?repos=openbb-finance/OpenBB&amp;amp;type=Date&amp;amp;theme=dark" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;7. Contributors&lt;/h2&gt; 
&lt;p&gt;OpenBB wouldn't be OpenBB without you. If we are going to disrupt financial industry, every contribution counts. Thank you for being part of this journey.&lt;/p&gt; 
&lt;a href="https://github.com/OpenBB-finance/OpenBB/graphs/contributors"&gt; &lt;img src="https://contributors-img.web.app/image?repo=OpenBB-finance/OpenBB" width="800" /&gt; &lt;/a&gt; 
&lt;!-- MARKDOWN LINKS &amp; IMAGES --&gt; 
&lt;!-- https://www.markdownguide.org/basic-syntax/#reference-style-links --&gt;</description>
    </item>
    
    <item>
      <title>icloud-photos-downloader/icloud_photos_downloader</title>
      <link>https://github.com/icloud-photos-downloader/icloud_photos_downloader</link>
      <description>&lt;p&gt;A command-line tool to download photos from iCloud&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;!!!! &lt;a href="https://github.com/icloud-photos-downloader/icloud_photos_downloader/issues/1305"&gt;Looking for MAINTAINER for this project&lt;/a&gt; !!!!&lt;/h1&gt; 
&lt;h1&gt;iCloud Photos Downloader &lt;a href="https://github.com/icloud-photos-downloader/icloud_photos_downloader/actions/workflows/quality-checks.yml"&gt;&lt;img src="https://github.com/icloud-photos-downloader/icloud_photos_downloader/workflows/Quality%20Checks/badge.svg?sanitize=true" alt="Quality Checks" /&gt;&lt;/a&gt; &lt;a href="https://github.com/icloud-photos-downloader/icloud_photos_downloader/actions/workflows/produce-artifacts.yml"&gt;&lt;img src="https://github.com/icloud-photos-downloader/icloud_photos_downloader/workflows/Produce%20Artifacts/badge.svg?sanitize=true" alt="Build and Package" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/icloud-photos-downloader/icloud_photos_downloader/master/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true" alt="MIT License" /&gt;&lt;/a&gt;&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;A command-line tool to download all your iCloud photos.&lt;/li&gt; 
 &lt;li&gt;Works on Linux, Windows, and macOS; laptop, desktop, and NAS&lt;/li&gt; 
 &lt;li&gt;Available as an executable for direct downloading and through package managers/ecosystems (&lt;a href="https://icloud-photos-downloader.github.io/icloud_photos_downloader/install.html#docker"&gt;Docker&lt;/a&gt;, &lt;a href="https://icloud-photos-downloader.github.io/icloud_photos_downloader/install.html#pypi"&gt;PyPI&lt;/a&gt;, &lt;a href="https://icloud-photos-downloader.github.io/icloud_photos_downloader/install.html#aur"&gt;AUR&lt;/a&gt;, &lt;a href="https://icloud-photos-downloader.github.io/icloud_photos_downloader/install.html#npm"&gt;npm&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;Developed and maintained by volunteers (we are always looking for &lt;a href="https://raw.githubusercontent.com/icloud-photos-downloader/icloud_photos_downloader/master/CONTRIBUTING.md"&gt;help&lt;/a&gt;).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See &lt;a href="https://icloud-photos-downloader.github.io/icloud_photos_downloader/"&gt;Documentation&lt;/a&gt; for more details. Also, check &lt;a href="https://github.com/icloud-photos-downloader/icloud_photos_downloader/issues"&gt;Issues&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;We aim to release new versions once a week (Friday), if there is something worth delivering.&lt;/p&gt; 
&lt;h2&gt;iCloud Prerequisites&lt;/h2&gt; 
&lt;p&gt;To make iCloud Photo Downloader work, ensure the iCloud account is configured with the following settings, otherwise Apple Servers will return an ACCESS_DENIED error:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Enable Access iCloud Data on the Web:&lt;/strong&gt; On your iPhone / iPad, enable &lt;code&gt;Settings &amp;gt; Apple ID &amp;gt; iCloud &amp;gt; Access iCloud Data on the Web&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Disable Advanced Data Protection:&lt;/strong&gt; On your iPhone /iPad disable &lt;code&gt;Settings &amp;gt; Apple ID &amp;gt; iCloud &amp;gt; Advanced Data Protection&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Install and Run&lt;/h2&gt; 
&lt;p&gt;There are three ways to run &lt;code&gt;icloudpd&lt;/code&gt;:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Download executable for your platform from the GitHub &lt;a href="https://github.com/icloud-photos-downloader/icloud_photos_downloader/releases/tag/v1.32.2"&gt;Release&lt;/a&gt; and run it&lt;/li&gt; 
 &lt;li&gt;Use package manager to install, update, and, in some cases, run (&lt;a href="https://icloud-photos-downloader.github.io/icloud_photos_downloader/install.html#docker"&gt;Docker&lt;/a&gt;, &lt;a href="https://icloud-photos-downloader.github.io/icloud_photos_downloader/install.html#pypi"&gt;PyPI&lt;/a&gt;, &lt;a href="https://icloud-photos-downloader.github.io/icloud_photos_downloader/install.html#aur"&gt;AUR&lt;/a&gt;, &lt;a href="https://icloud-photos-downloader.github.io/icloud_photos_downloader/install.html#npm"&gt;npm&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;Build and run from the source&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;See &lt;a href="https://icloud-photos-downloader.github.io/icloud_photos_downloader/install.html"&gt;Documentation&lt;/a&gt; for more details&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;!-- start features --&gt; 
&lt;ul&gt; 
 &lt;li&gt;Three modes of operation: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Copy&lt;/strong&gt; - download new photos from iCloud (default mode)&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Sync&lt;/strong&gt; - download new photos from iCloud and delete local files that were removed in iCloud (&lt;code&gt;--auto-delete&lt;/code&gt; option)&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Move&lt;/strong&gt; - download new photos from iCloud and delete photos in iCloud (&lt;code&gt;--keep-icloud-recent-days&lt;/code&gt; option)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Support for Live Photos (image and video as separate files) and RAW images (including RAW+JPEG)&lt;/li&gt; 
 &lt;li&gt;Automatic de-duplication of photos with the same name&lt;/li&gt; 
 &lt;li&gt;One time download and an option to monitor for iCloud changes continuously (&lt;code&gt;--watch-with-interval&lt;/code&gt; option)&lt;/li&gt; 
 &lt;li&gt;Optimizations for incremental runs (&lt;code&gt;--until-found&lt;/code&gt; and &lt;code&gt;--recent&lt;/code&gt; options)&lt;/li&gt; 
 &lt;li&gt;Photo metadata (EXIF) updates (&lt;code&gt;--set-exif-datetime&lt;/code&gt; option)&lt;/li&gt; 
 &lt;li&gt;... and many more (use &lt;code&gt;--help&lt;/code&gt; option to get full list)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- end features --&gt; 
&lt;h2&gt;Experimental Mode&lt;/h2&gt; 
&lt;p&gt;Some changes are added to the experimental mode before they graduate into the main package. &lt;a href="https://raw.githubusercontent.com/icloud-photos-downloader/icloud_photos_downloader/master/EXPERIMENTAL.md"&gt;Details&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;p&gt;To keep your iCloud photo collection synchronized to your local system:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;icloudpd --directory /data --username my@email.address --watch-with-interval 3600
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] It is &lt;code&gt;icloudpd&lt;/code&gt;, not &lt;code&gt;icloud&lt;/code&gt; executable&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] Synchronization logic can be adjusted with command-line parameters. Run &lt;code&gt;icloudpd --help&lt;/code&gt; to get full list.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;To independently create and authorize a session (and complete 2SA/2FA validation if needed) on your local system:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;icloudpd --username my@email.address --password my_password --auth-only
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] This feature can also be used to check and verify that the session is still authenticated.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Want to contribute to iCloud Photos Downloader? Awesome! Check out the &lt;a href="https://raw.githubusercontent.com/icloud-photos-downloader/icloud_photos_downloader/master/CONTRIBUTING.md"&gt;contributing guidelines&lt;/a&gt; to get involved.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Shubhamsaboo/awesome-llm-apps</title>
      <link>https://github.com/Shubhamsaboo/awesome-llm-apps</link>
      <description>&lt;p&gt;Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="http://www.theunwindai.com"&gt; &lt;img src="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/unwind_black.png" width="900px" alt="Unwind AI" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://www.linkedin.com/in/shubhamsaboo/"&gt; &lt;img src="https://img.shields.io/badge/-Follow%20Shubham%20Saboo-blue?logo=linkedin&amp;amp;style=flat-square" alt="LinkedIn" /&gt; &lt;/a&gt; &lt;a href="https://twitter.com/Saboo_Shubham_"&gt; &lt;img src="https://img.shields.io/twitter/follow/Shubham_Saboo" alt="Twitter" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; 
 &lt;!-- Keep these links. Translations will automatically update with the README. --&gt; &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=de"&gt;Deutsch&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=es"&gt;Espa√±ol&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=fr"&gt;fran√ßais&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ja"&gt;Êó•Êú¨Ë™û&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ko"&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=pt"&gt;Portugu√™s&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ru"&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=zh"&gt;‰∏≠Êñá&lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h1&gt;üåü Awesome LLM Apps&lt;/h1&gt; 
&lt;p&gt;A curated collection of &lt;strong&gt;Awesome LLM apps built with RAG, AI Agents, Multi-agent Teams, MCP, Voice Agents, and more.&lt;/strong&gt; This repository features LLM apps that use models from &lt;img src="https://cdn.simpleicons.org/openai" alt="openai logo" width="25" height="15" /&gt;&lt;strong&gt;OpenAI&lt;/strong&gt; , &lt;img src="https://cdn.simpleicons.org/anthropic" alt="anthropic logo" width="25" height="15" /&gt;&lt;strong&gt;Anthropic&lt;/strong&gt;, &lt;img src="https://cdn.simpleicons.org/googlegemini" alt="google logo" width="25" height="18" /&gt;&lt;strong&gt;Google&lt;/strong&gt;, &lt;img src="https://cdn.simpleicons.org/x" alt="X logo" width="25" height="15" /&gt;&lt;strong&gt;xAI&lt;/strong&gt; and open-source models like &lt;img src="https://cdn.simpleicons.org/alibabacloud" alt="alibaba logo" width="25" height="15" /&gt;&lt;strong&gt;Qwen&lt;/strong&gt; or &lt;img src="https://cdn.simpleicons.org/meta" alt="meta logo" width="25" height="15" /&gt;&lt;strong&gt;Llama&lt;/strong&gt; that you can run locally on your computer.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/9876" target="_blank"&gt; &lt;img src="https://trendshift.io/api/badge/repositories/9876" alt="Shubhamsaboo%2Fawesome-llm-apps | Trendshift" style="width: 250px; height: 55px;" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;ü§î Why Awesome LLM Apps?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;üí° Discover practical and creative ways LLMs can be applied across different domains, from code repositories to email inboxes and more.&lt;/li&gt; 
 &lt;li&gt;üî• Explore apps that combine LLMs from OpenAI, Anthropic, Gemini, and open-source alternatives with AI Agents, Agent Teams, MCP &amp;amp; RAG.&lt;/li&gt; 
 &lt;li&gt;üéì Learn from well-documented projects and contribute to the growing open-source ecosystem of LLM-powered applications.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üôè Thanks to our sponsors&lt;/h2&gt; 
&lt;table align="center" cellpadding="16" cellspacing="12"&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td align="center"&gt; &lt;a href="https://tsdb.co/shubham-gh" target="_blank" rel="noopener" title="Tiger Data"&gt; &lt;img src="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/sponsors/tigerdata.png" alt="Tiger Data" width="500" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;a href="https://tsdb.co/shubham-gh" target="_blank" rel="noopener" style="text-decoration: none; color: #333; font-weight: bold; font-size: 18px;"&gt; Tiger Data MCP &lt;/a&gt; &lt;/td&gt; 
   &lt;td align="center"&gt; &lt;a href="https://github.com/speechmatics/speechmatics-academy" target="_blank" rel="noopener" title="Speechmatics"&gt; &lt;img src="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/sponsors/speechmatics.png" alt="Speechmatics" width="500" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;a href="https://github.com/speechmatics/speechmatics-academy" target="_blank" rel="noopener" style="text-decoration: none; color: #333; font-weight: bold; font-size: 18px;"&gt; Speechmatics &lt;/a&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt; &lt;a href="https://okara.ai/?utm_source=oss&amp;amp;utm_medium=sponsorship&amp;amp;utm_campaign=awesome-llm-apps" title="Okara"&gt; &lt;img src="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/sponsors/okara.png" alt="Okara" width="500" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;a href="https://okara.ai/?utm_source=oss&amp;amp;utm_medium=sponsorship&amp;amp;utm_campaign=awesome-llm-apps" style="text-decoration: none; color: #333; font-weight: bold; font-size: 18px;"&gt; Okara AI &lt;/a&gt; &lt;/td&gt; 
   &lt;td align="center"&gt; &lt;a href="https://sponsorunwindai.com/" title="Become a Sponsor"&gt; &lt;img src="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/sponsor_awesome_llm_apps.png" alt="Become a Sponsor" width="500" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;a href="https://sponsorunwindai.com/" style="text-decoration: none; color: #333; font-weight: bold; font-size: 18px;"&gt; Become a Sponsor &lt;/a&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;üìÇ Featured AI Projects&lt;/h2&gt; 
&lt;h3&gt;AI Agents&lt;/h3&gt; 
&lt;h3&gt;üå± Starter AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_blog_to_podcast_agent/"&gt;üéôÔ∏è AI Blog to Podcast Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_breakup_recovery_agent/"&gt;‚ù§Ô∏è‚Äçü©π AI Breakup Recovery Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_data_analysis_agent/"&gt;üìä AI Data Analysis Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_medical_imaging_agent/"&gt;ü©ª AI Medical Imaging Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_meme_generator_agent_browseruse/"&gt;üòÇ AI Meme Generator Agent (Browser)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_music_generator_agent/"&gt;üéµ AI Music Generator Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_travel_agent/"&gt;üõ´ AI Travel Agent (Local &amp;amp; Cloud)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/gemini_multimodal_agent_demo/"&gt;‚ú® Gemini Multimodal Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/mixture_of_agents/"&gt;üîÑ Mixture of Agents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/xai_finance_agent/"&gt;üìä xAI Finance Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/opeani_research_agent/"&gt;üîç OpenAI Research Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/web_scrapping_ai_agent/"&gt;üï∏Ô∏è Web Scraping AI Agent (Local &amp;amp; Cloud SDK)&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üöÄ Advanced AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_home_renovation_agent"&gt;üèöÔ∏è üçå AI Home Renovation Agent with Nano Banana Pro&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_deep_research_agent/"&gt;üîç AI Deep Research Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_vc_due_diligence_agent_team"&gt;üìä AI VC Due Diligence Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/research_agent_gemini_interaction_api"&gt;üî¨ AI Research Planner &amp;amp; Executor (Google Interactions API)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_consultant_agent"&gt;ü§ù AI Consultant Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_system_architect_r1/"&gt;üèóÔ∏è AI System Architect Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_financial_coach_agent/"&gt;üí∞ AI Financial Coach Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_movie_production_agent/"&gt;üé¨ AI Movie Production Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_investment_agent/"&gt;üìà AI Investment Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_health_fitness_agent/"&gt;üèãÔ∏è‚Äç‚ôÇÔ∏è AI Health &amp;amp; Fitness Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/product_launch_intelligence_agent"&gt;üöÄ AI Product Launch Intelligence Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_journalist_agent/"&gt;üóûÔ∏è AI Journalist Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_mental_wellbeing_agent/"&gt;üß† AI Mental Wellbeing Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_meeting_agent/"&gt;üìë AI Meeting Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_Self-Evolving_agent/"&gt;üß¨ AI Self-Evolving Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_sales_intelligence_agent_team"&gt;üë®üèª‚Äçüíº AI Sales Intelligence Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_news_and_podcast_agents/"&gt;üéß AI Social Media News and Podcast Agent&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üéÆ Autonomous Game Playing Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/autonomous_game_playing_agent_apps/ai_3dpygame_r1/"&gt;üéÆ AI 3D Pygame Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/autonomous_game_playing_agent_apps/ai_chess_agent/"&gt;‚ôú AI Chess Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/autonomous_game_playing_agent_apps/ai_tic_tac_toe_agent/"&gt;üé≤ AI Tic-Tac-Toe Agent&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ü§ù Multi-agent Teams&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_competitor_intelligence_agent_team/"&gt;üß≤ AI Competitor Intelligence Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_finance_agent_team/"&gt;üí≤ AI Finance Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_game_design_agent_team/"&gt;üé® AI Game Design Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_legal_agent_team/"&gt;üë®‚Äç‚öñÔ∏è AI Legal Agent Team (Cloud &amp;amp; Local)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_recruitment_agent_team/"&gt;üíº AI Recruitment Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_real_estate_agent_team"&gt;üè† AI Real Estate Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_services_agency/"&gt;üë®‚Äçüíº AI Services Agency (CrewAI)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_teaching_agent_team/"&gt;üë®‚Äçüè´ AI Teaching Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_coding_agent_team/"&gt;üíª Multimodal Coding Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_design_agent_team/"&gt;‚ú® Multimodal Design Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_uiux_feedback_agent_team/"&gt;üé® üçå Multimodal UI/UX Feedback Agent Team with Nano Banana&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_travel_planner_agent_team/"&gt;üåè AI Travel Planner Agent Team&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üó£Ô∏è Voice AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/voice_ai_agents/ai_audio_tour_agent/"&gt;üó£Ô∏è AI Audio Tour Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/voice_ai_agents/customer_support_voice_agent/"&gt;üìû Customer Support Voice Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/voice_ai_agents/voice_rag_openaisdk/"&gt;üîä Voice RAG Agent (OpenAI SDK)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/akshayaggarwal99/jarvis-ai-assistant"&gt;üéôÔ∏è OpenSource Voice Dictation Agent (like Wispr Flow&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;img src="https://cdn.simpleicons.org/modelcontextprotocol" alt="mcp logo" width="25" height="20" /&gt; MCP AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/mcp_ai_agents/browser_mcp_agent/"&gt;‚ôæÔ∏è Browser MCP Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/mcp_ai_agents/github_mcp_agent/"&gt;üêô GitHub MCP Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/mcp_ai_agents/notion_mcp_agent"&gt;üìë Notion MCP Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/mcp_ai_agents/ai_travel_planner_mcp_agent_team"&gt;üåç AI Travel Planner MCP Agent&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üìÄ RAG (Retrieval Augmented Generation)&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/agentic_rag_embedding_gemma"&gt;üî• Agentic RAG with Embedding Gemma&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/agentic_rag_with_reasoning/"&gt;üßê Agentic RAG with Reasoning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/ai_blog_search/"&gt;üì∞ AI Blog Search (RAG)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/autonomous_rag/"&gt;üîç Autonomous RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/contextualai_rag_agent/"&gt;üîÑ Contextual AI RAG Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/corrective_rag/"&gt;üîÑ Corrective RAG (CRAG)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/deepseek_local_rag_agent/"&gt;üêã Deepseek Local RAG Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/gemini_agentic_rag/"&gt;ü§î Gemini Agentic RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/hybrid_search_rag/"&gt;üëÄ Hybrid Search RAG (Cloud)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/llama3.1_local_rag/"&gt;üîÑ Llama 3.1 Local RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/local_hybrid_search_rag/"&gt;üñ•Ô∏è Local Hybrid Search RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/local_rag_agent/"&gt;ü¶ô Local RAG Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/rag-as-a-service/"&gt;üß© RAG-as-a-Service&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/rag_agent_cohere/"&gt;‚ú® RAG Agent with Cohere&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/rag_chain/"&gt;‚õìÔ∏è Basic RAG Chain&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/rag_database_routing/"&gt;üì† RAG with Database Routing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/vision_rag/"&gt;üñºÔ∏è Vision RAG&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üíæ LLM Apps with Memory Tutorials&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/ai_arxiv_agent_memory/"&gt;üíæ AI ArXiv Agent with Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/ai_travel_agent_memory/"&gt;üõ©Ô∏è AI Travel Agent with Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/llama3_stateful_chat/"&gt;üí¨ Llama3 Stateful Chat&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/llm_app_personalized_memory/"&gt;üìù LLM App with Personalized Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/local_chatgpt_with_memory/"&gt;üóÑÔ∏è Local ChatGPT Clone with Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/multi_llm_memory/"&gt;üß† Multi-LLM Application with Shared Memory&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üí¨ Chat with X Tutorials&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_github/"&gt;üí¨ Chat with GitHub (GPT &amp;amp; Llama3)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_gmail/"&gt;üì® Chat with Gmail&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_pdf/"&gt;üìÑ Chat with PDF (GPT &amp;amp; Llama3)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_research_papers/"&gt;üìö Chat with Research Papers (ArXiv) (GPT &amp;amp; Llama3)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_substack/"&gt;üìù Chat with Substack&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_youtube_videos/"&gt;üìΩÔ∏è Chat with YouTube Videos&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üéØ LLM Optimization Tools&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_optimization_tools/toonify_token_optimization/"&gt;üéØ Toonify Token Optimization&lt;/a&gt; - Reduce LLM API costs by 30-60% using TOON format&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üîß LLM Fine-tuning Tutorials&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;img src="https://cdn.simpleicons.org/google" alt="google logo" width="20" height="15" /&gt; &lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_finetuning_tutorials/gemma3_finetuning/"&gt;Gemma 3 Fine-tuning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;img src="https://cdn.simpleicons.org/meta" alt="meta logo" width="25" height="15" /&gt; &lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_finetuning_tutorials/llama3.2_finetuning/"&gt;Llama 3.2 Fine-tuning&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üßë‚Äçüè´ AI Agent Framework Crash Course&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://cdn.simpleicons.org/google" alt="google logo" width="25" height="15" /&gt; &lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/ai_agent_framework_crash_course/google_adk_crash_course/"&gt;Google ADK Crash Course&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Starter agent; model‚Äëagnostic (OpenAI, Claude)&lt;/li&gt; 
 &lt;li&gt;Structured outputs (Pydantic)&lt;/li&gt; 
 &lt;li&gt;Tools: built‚Äëin, function, third‚Äëparty, MCP tools&lt;/li&gt; 
 &lt;li&gt;Memory; callbacks; Plugins&lt;/li&gt; 
 &lt;li&gt;Simple multi‚Äëagent; Multi‚Äëagent patterns&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://cdn.simpleicons.org/openai" alt="openai logo" width="25" height="15" /&gt; &lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/ai_agent_framework_crash_course/openai_sdk_crash_course/"&gt;OpenAI Agents SDK Crash Course&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Starter agent; function calling; structured outputs&lt;/li&gt; 
 &lt;li&gt;Tools: built‚Äëin, function, third‚Äëparty integrations&lt;/li&gt; 
 &lt;li&gt;Memory; callbacks; evaluation&lt;/li&gt; 
 &lt;li&gt;Multi‚Äëagent patterns; agent handoffs&lt;/li&gt; 
 &lt;li&gt;Swarm orchestration; routing logic&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üöÄ Getting Started&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Clone the repository&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/Shubhamsaboo/awesome-llm-apps.git 
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Navigate to the desired project directory&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cd awesome-llm-apps/starter_ai_agents/ai_travel_agent
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install the required dependencies&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Follow the project-specific instructions&lt;/strong&gt; in each project's &lt;code&gt;README.md&lt;/code&gt; file to set up and run the app.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;&lt;img src="https://cdn.simpleicons.org/github" alt="github logo" width="25" height="20" /&gt; Thank You, Community, for the Support! üôè&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#Shubhamsaboo/awesome-llm-apps&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=Shubhamsaboo/awesome-llm-apps&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;üåü &lt;strong&gt;Don‚Äôt miss out on future updates! Star the repo now and be the first to know about new and exciting LLM apps with RAG and AI Agents.&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>OpenHands/OpenHands</title>
      <link>https://github.com/OpenHands/OpenHands</link>
      <description>&lt;p&gt;üôå OpenHands: AI-Driven Development&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a name="readme-top"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenHands/docs/main/openhands/static/img/logo.png" alt="Logo" width="200" /&gt; 
 &lt;h1 align="center" style="border-bottom: none"&gt;OpenHands: AI-Driven Development&lt;/h1&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/OpenHands/OpenHands/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/LICENSE-MIT-20B2AA?style=for-the-badge" alt="MIT License" /&gt;&lt;/a&gt; 
 &lt;a href="https://docs.google.com/spreadsheets/d/1wOUdFCMyY6Nt0AIqF705KN4JKOWgeI4wUGUP60krXXs/edit?gid=811504672#gid=811504672"&gt;&lt;img src="https://img.shields.io/badge/SWEBench-77.6-00cc00?logoColor=FFE165&amp;amp;style=for-the-badge" alt="Benchmark Score" /&gt;&lt;/a&gt; 
 &lt;br /&gt; 
 &lt;a href="https://docs.openhands.dev/sdk"&gt;&lt;img src="https://img.shields.io/badge/Documentation-000?logo=googledocs&amp;amp;logoColor=FFE165&amp;amp;style=for-the-badge" alt="Check out the documentation" /&gt;&lt;/a&gt; 
 &lt;a href="https://arxiv.org/abs/2511.03690"&gt;&lt;img src="https://img.shields.io/badge/Paper-000?logoColor=FFE165&amp;amp;logo=arxiv&amp;amp;style=for-the-badge" alt="Tech Report" /&gt;&lt;/a&gt; 
 &lt;!-- Keep these links. Translations will automatically update with the README. --&gt; 
 &lt;p&gt;&lt;a href="https://www.readme-i18n.com/OpenHands/OpenHands?lang=de"&gt;Deutsch&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/OpenHands/OpenHands?lang=es"&gt;Espa√±ol&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/OpenHands/OpenHands?lang=fr"&gt;fran√ßais&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/OpenHands/OpenHands?lang=ja"&gt;Êó•Êú¨Ë™û&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/OpenHands/OpenHands?lang=ko"&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/OpenHands/OpenHands?lang=pt"&gt;Portugu√™s&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/OpenHands/OpenHands?lang=ru"&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/OpenHands/OpenHands?lang=zh"&gt;‰∏≠Êñá&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;p&gt;üôå&amp;nbsp;Welcome to OpenHands, a &lt;a href="https://raw.githubusercontent.com/OpenHands/OpenHands/main/COMMUNITY.md"&gt;community&lt;/a&gt; focused on AI-driven development. We‚Äôd love for you to &lt;a href="https://dub.sh/openhands"&gt;join us on Slack&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;There are a few ways to work with OpenHands:&lt;/p&gt; 
&lt;h3&gt;OpenHands Software Agent SDK&lt;/h3&gt; 
&lt;p&gt;The SDK is a composable Python library that contains all of our agentic tech. It's the engine that powers everything else below.&lt;/p&gt; 
&lt;p&gt;Define agents in code, then run them locally, or scale to 1000s of agents in the cloud.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://docs.openhands.dev/sdk"&gt;Check out the docs&lt;/a&gt; or &lt;a href="https://github.com/OpenHands/software-agent-sdk/"&gt;view the source&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;OpenHands CLI&lt;/h3&gt; 
&lt;p&gt;The CLI is the easiest way to start using OpenHands. The experience will be familiar to anyone who has worked with e.g. Claude Code or Codex. You can power it with Claude, GPT, or any other LLM.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://docs.openhands.dev/openhands/usage/run-openhands/cli-mode"&gt;Check out the docs&lt;/a&gt; or &lt;a href="https://github.com/OpenHands/OpenHands-CLI"&gt;view the source&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;OpenHands Local GUI&lt;/h3&gt; 
&lt;p&gt;Use the Local GUI for running agents on your laptop. It comes with a REST API and a single-page React application. The experience will be familiar to anyone who has used Devin or Jules.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://docs.openhands.dev/openhands/usage/run-openhands/local-setup"&gt;Check out the docs&lt;/a&gt; or view the source in this repo.&lt;/p&gt; 
&lt;h3&gt;OpenHands Cloud&lt;/h3&gt; 
&lt;p&gt;This is a deployment of OpenHands GUI, running on hosted infrastructure.&lt;/p&gt; 
&lt;p&gt;You can try it with a free $10 credit by &lt;a href="https://app.all-hands.dev"&gt;signing in with your GitHub or GitLab account&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;OpenHands Cloud comes with source-available features and integrations:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Integrations with Slack, Jira, and Linear&lt;/li&gt; 
 &lt;li&gt;Multi-user support&lt;/li&gt; 
 &lt;li&gt;RBAC and permissions&lt;/li&gt; 
 &lt;li&gt;Collaboration features (e.g., conversation sharing)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;OpenHands Enterprise&lt;/h3&gt; 
&lt;p&gt;Large enterprises can work with us to self-host OpenHands Cloud in their own VPC, via Kubernetes. OpenHands Enterprise can also work with the CLI and SDK above.&lt;/p&gt; 
&lt;p&gt;OpenHands Enterprise is source-available--you can see all the source code here in the enterprise/ directory, but you'll need to purchase a license if you want to run it for more than one month.&lt;/p&gt; 
&lt;p&gt;Enterprise contracts also come with extended support and access to our research team.&lt;/p&gt; 
&lt;p&gt;Learn more at &lt;a href="https://openhands.dev/enterprise"&gt;openhands.dev/enterprise&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Everything Else&lt;/h3&gt; 
&lt;p&gt;Check out our &lt;a href="https://github.com/orgs/openhands/projects/1"&gt;Product Roadmap&lt;/a&gt;, and feel free to &lt;a href="https://github.com/OpenHands/OpenHands/issues"&gt;open up an issue&lt;/a&gt; if there's something you'd like to see!&lt;/p&gt; 
&lt;p&gt;You might also be interested in our &lt;a href="https://github.com/OpenHands/benchmarks"&gt;evaluation infrastructure&lt;/a&gt;, our &lt;a href="https://github.com/OpenHands/openhands-chrome-extension/"&gt;chrome extension&lt;/a&gt;, or our &lt;a href="https://github.com/OpenHands/ToM-SWE"&gt;Theory-of-Mind module&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;All our work is available under the MIT license, except for the &lt;code&gt;enterprise/&lt;/code&gt; directory in this repository (see the &lt;a href="https://raw.githubusercontent.com/OpenHands/OpenHands/main/enterprise/LICENSE"&gt;enterprise license&lt;/a&gt; for details). The core &lt;code&gt;openhands&lt;/code&gt; and &lt;code&gt;agent-server&lt;/code&gt; Docker images are fully MIT-licensed as well.&lt;/p&gt; 
&lt;p&gt;If you need help with anything, or just want to chat, &lt;a href="https://dub.sh/openhands"&gt;come find us on Slack&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>crewAIInc/crewAI</title>
      <link>https://github.com/crewAIInc/crewAI</link>
      <description>&lt;p&gt;Framework for orchestrating role-playing, autonomous AI agents. By fostering collaborative intelligence, CrewAI empowers agents to work together seamlessly, tackling complex tasks.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://github.com/crewAIInc/crewAI"&gt; &lt;img src="https://raw.githubusercontent.com/crewAIInc/crewAI/main/docs/images/crewai_logo.png" width="600px" alt="Open source Multi-AI Agent orchestration framework" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center" style="display: flex; justify-content: center; gap: 20px; align-items: center;"&gt; &lt;a href="https://trendshift.io/repositories/11239" target="_blank"&gt; &lt;img src="https://trendshift.io/api/badge/repositories/11239" alt="crewAIInc%2FcrewAI | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://crewai.com"&gt;Homepage&lt;/a&gt; ¬∑ &lt;a href="https://docs.crewai.com"&gt;Docs&lt;/a&gt; ¬∑ &lt;a href="https://app.crewai.com"&gt;Start Cloud Trial&lt;/a&gt; ¬∑ &lt;a href="https://blog.crewai.com"&gt;Blog&lt;/a&gt; ¬∑ &lt;a href="https://community.crewai.com"&gt;Forum&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/crewAIInc/crewAI"&gt; &lt;img src="https://img.shields.io/github/stars/crewAIInc/crewAI" alt="GitHub Repo stars" /&gt; &lt;/a&gt; &lt;a href="https://github.com/crewAIInc/crewAI/network/members"&gt; &lt;img src="https://img.shields.io/github/forks/crewAIInc/crewAI" alt="GitHub forks" /&gt; &lt;/a&gt; &lt;a href="https://github.com/crewAIInc/crewAI/issues"&gt; &lt;img src="https://img.shields.io/github/issues/crewAIInc/crewAI" alt="GitHub issues" /&gt; &lt;/a&gt; &lt;a href="https://github.com/crewAIInc/crewAI/pulls"&gt; &lt;img src="https://img.shields.io/github/issues-pr/crewAIInc/crewAI" alt="GitHub pull requests" /&gt; &lt;/a&gt; &lt;a href="https://opensource.org/licenses/MIT"&gt; &lt;img src="https://img.shields.io/badge/License-MIT-green.svg?sanitize=true" alt="License: MIT" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://pypi.org/project/crewai/"&gt; &lt;img src="https://img.shields.io/pypi/v/crewai" alt="PyPI version" /&gt; &lt;/a&gt; &lt;a href="https://pypi.org/project/crewai/"&gt; &lt;img src="https://img.shields.io/pypi/dm/crewai" alt="PyPI downloads" /&gt; &lt;/a&gt; &lt;a href="https://twitter.com/crewAIInc"&gt; &lt;img src="https://img.shields.io/twitter/follow/crewAIInc?style=social" alt="Twitter Follow" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h3&gt;Fast and Flexible Multi-Agent Automation Framework&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;CrewAI is a lean, lightning-fast Python framework built entirely from scratch‚Äîcompletely &lt;strong&gt;independent of LangChain or other agent frameworks&lt;/strong&gt;. It empowers developers with both high-level simplicity and precise low-level control, ideal for creating autonomous AI agents tailored to any scenario.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;CrewAI Crews&lt;/strong&gt;: Optimize for autonomy and collaborative intelligence.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;CrewAI Flows&lt;/strong&gt;: The &lt;strong&gt;enterprise and production architecture&lt;/strong&gt; for building and deploying multi-agent systems. Enable granular, event-driven control, single LLM calls for precise task orchestration and supports Crews natively&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;With over 100,000 developers certified through our community courses at &lt;a href="https://learn.crewai.com"&gt;learn.crewai.com&lt;/a&gt;, CrewAI is rapidly becoming the standard for enterprise-ready AI automation.&lt;/p&gt; 
&lt;h1&gt;CrewAI AMP Suite&lt;/h1&gt; 
&lt;p&gt;CrewAI AMP Suite is a comprehensive bundle tailored for organizations that require secure, scalable, and easy-to-manage agent-driven automation.&lt;/p&gt; 
&lt;p&gt;You can try one part of the suite the &lt;a href="https://app.crewai.com"&gt;Crew Control Plane for free&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Crew Control Plane Key Features:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Tracing &amp;amp; Observability&lt;/strong&gt;: Monitor and track your AI agents and workflows in real-time, including metrics, logs, and traces.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Unified Control Plane&lt;/strong&gt;: A centralized platform for managing, monitoring, and scaling your AI agents and workflows.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Seamless Integrations&lt;/strong&gt;: Easily connect with existing enterprise systems, data sources, and cloud infrastructure.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Advanced Security&lt;/strong&gt;: Built-in robust security and compliance measures ensuring safe deployment and management.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Actionable Insights&lt;/strong&gt;: Real-time analytics and reporting to optimize performance and decision-making.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;24/7 Support&lt;/strong&gt;: Dedicated enterprise support to ensure uninterrupted operation and quick resolution of issues.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;On-premise and Cloud Deployment Options&lt;/strong&gt;: Deploy CrewAI AMP on-premise or in the cloud, depending on your security and compliance requirements.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;CrewAI AMP is designed for enterprises seeking a powerful, reliable solution to transform complex business processes into efficient, intelligent automations.&lt;/p&gt; 
&lt;h2&gt;Table of contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#why-crewai"&gt;Why CrewAI?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#getting-started"&gt;Getting Started&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#key-features"&gt;Key Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#understanding-flows-and-crews"&gt;Understanding Flows and Crews&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#how-crewai-compares"&gt;CrewAI vs LangGraph&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#examples"&gt;Examples&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#quick-tutorial"&gt;Quick Tutorial&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#write-job-descriptions"&gt;Write Job Descriptions&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#trip-planner"&gt;Trip Planner&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#stock-analysis"&gt;Stock Analysis&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#using-crews-and-flows-together"&gt;Using Crews and Flows Together&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#connecting-your-crew-to-a-model"&gt;Connecting Your Crew to a Model&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#how-crewai-compares"&gt;How CrewAI Compares&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#frequently-asked-questions-faq"&gt;Frequently Asked Questions (FAQ)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#contribution"&gt;Contribution&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#telemetry"&gt;Telemetry&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Why CrewAI?&lt;/h2&gt; 
&lt;div align="center" style="margin-bottom: 30px;"&gt; 
 &lt;img src="https://raw.githubusercontent.com/crewAIInc/crewAI/main/docs/images/asset.png" alt="CrewAI Logo" width="100%" /&gt; 
&lt;/div&gt; 
&lt;p&gt;CrewAI unlocks the true potential of multi-agent automation, delivering the best-in-class combination of speed, flexibility, and control with either Crews of AI Agents or Flows of Events:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Standalone Framework&lt;/strong&gt;: Built from scratch, independent of LangChain or any other agent framework.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;High Performance&lt;/strong&gt;: Optimized for speed and minimal resource usage, enabling faster execution.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible Low Level Customization&lt;/strong&gt;: Complete freedom to customize at both high and low levels - from overall workflows and system architecture to granular agent behaviors, internal prompts, and execution logic.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Ideal for Every Use Case&lt;/strong&gt;: Proven effective for both simple tasks and highly complex, real-world, enterprise-grade scenarios.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Robust Community&lt;/strong&gt;: Backed by a rapidly growing community of over &lt;strong&gt;100,000 certified&lt;/strong&gt; developers offering comprehensive support and resources.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;CrewAI empowers developers and enterprises to confidently build intelligent automations, bridging the gap between simplicity, flexibility, and performance.&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;Setup and run your first CrewAI agents by following this tutorial.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=-kSOTtYzgEw" title="CrewAI Getting Started Tutorial"&gt;&lt;img src="https://img.youtube.com/vi/-kSOTtYzgEw/hqdefault.jpg" alt="CrewAI Getting Started Tutorial" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;&lt;/h3&gt; 
&lt;p&gt;Learning Resources&lt;/p&gt; 
&lt;p&gt;Learn CrewAI through our comprehensive courses:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.deeplearning.ai/short-courses/multi-ai-agent-systems-with-crewai/"&gt;Multi AI Agent Systems with CrewAI&lt;/a&gt; - Master the fundamentals of multi-agent systems&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.deeplearning.ai/short-courses/practical-multi-ai-agents-and-advanced-use-cases-with-crewai/"&gt;Practical Multi AI Agents and Advanced Use Cases&lt;/a&gt; - Deep dive into advanced implementations&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Understanding Flows and Crews&lt;/h3&gt; 
&lt;p&gt;CrewAI offers two powerful, complementary approaches that work seamlessly together to build sophisticated AI applications:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Crews&lt;/strong&gt;: Teams of AI agents with true autonomy and agency, working together to accomplish complex tasks through role-based collaboration. Crews enable:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Natural, autonomous decision-making between agents&lt;/li&gt; 
   &lt;li&gt;Dynamic task delegation and collaboration&lt;/li&gt; 
   &lt;li&gt;Specialized roles with defined goals and expertise&lt;/li&gt; 
   &lt;li&gt;Flexible problem-solving approaches&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Flows&lt;/strong&gt;: Production-ready, event-driven workflows that deliver precise control over complex automations. Flows provide:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fine-grained control over execution paths for real-world scenarios&lt;/li&gt; 
   &lt;li&gt;Secure, consistent state management between tasks&lt;/li&gt; 
   &lt;li&gt;Clean integration of AI agents with production Python code&lt;/li&gt; 
   &lt;li&gt;Conditional branching for complex business logic&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;The true power of CrewAI emerges when combining Crews and Flows. This synergy allows you to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Build complex, production-grade applications&lt;/li&gt; 
 &lt;li&gt;Balance autonomy with precise control&lt;/li&gt; 
 &lt;li&gt;Handle sophisticated real-world scenarios&lt;/li&gt; 
 &lt;li&gt;Maintain clean, maintainable code structure&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Getting Started with Installation&lt;/h3&gt; 
&lt;p&gt;To get started with CrewAI, follow these simple steps:&lt;/p&gt; 
&lt;h3&gt;1. Installation&lt;/h3&gt; 
&lt;p&gt;Ensure you have Python &amp;gt;=3.10 &amp;lt;3.14 installed on your system. CrewAI uses &lt;a href="https://docs.astral.sh/uv/"&gt;UV&lt;/a&gt; for dependency management and package handling, offering a seamless setup and execution experience.&lt;/p&gt; 
&lt;p&gt;First, install CrewAI:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;uv pip install crewai
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you want to install the 'crewai' package along with its optional features that include additional tools for agents, you can do so by using the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;uv pip install 'crewai[tools]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The command above installs the basic package and also adds extra components which require more dependencies to function.&lt;/p&gt; 
&lt;h3&gt;Troubleshooting Dependencies&lt;/h3&gt; 
&lt;p&gt;If you encounter issues during installation or usage, here are some common solutions:&lt;/p&gt; 
&lt;h4&gt;Common Issues&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ModuleNotFoundError: No module named 'tiktoken'&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Install tiktoken explicitly: &lt;code&gt;uv pip install 'crewai[embeddings]'&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;If using embedchain or other tools: &lt;code&gt;uv pip install 'crewai[tools]'&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Failed building wheel for tiktoken&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Ensure Rust compiler is installed (see installation steps above)&lt;/li&gt; 
   &lt;li&gt;For Windows: Verify Visual C++ Build Tools are installed&lt;/li&gt; 
   &lt;li&gt;Try upgrading pip: &lt;code&gt;uv pip install --upgrade pip&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;If issues persist, use a pre-built wheel: &lt;code&gt;uv pip install tiktoken --prefer-binary&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;2. Setting Up Your Crew with the YAML Configuration&lt;/h3&gt; 
&lt;p&gt;To create a new CrewAI project, run the following CLI (Command Line Interface) command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;crewai create crew &amp;lt;project_name&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command creates a new project folder with the following structure:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;my_project/
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ pyproject.toml
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ .env
‚îî‚îÄ‚îÄ src/
    ‚îî‚îÄ‚îÄ my_project/
        ‚îú‚îÄ‚îÄ __init__.py
        ‚îú‚îÄ‚îÄ main.py
        ‚îú‚îÄ‚îÄ crew.py
        ‚îú‚îÄ‚îÄ tools/
        ‚îÇ   ‚îú‚îÄ‚îÄ custom_tool.py
        ‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
        ‚îî‚îÄ‚îÄ config/
            ‚îú‚îÄ‚îÄ agents.yaml
            ‚îî‚îÄ‚îÄ tasks.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can now start developing your crew by editing the files in the &lt;code&gt;src/my_project&lt;/code&gt; folder. The &lt;code&gt;main.py&lt;/code&gt; file is the entry point of the project, the &lt;code&gt;crew.py&lt;/code&gt; file is where you define your crew, the &lt;code&gt;agents.yaml&lt;/code&gt; file is where you define your agents, and the &lt;code&gt;tasks.yaml&lt;/code&gt; file is where you define your tasks.&lt;/p&gt; 
&lt;h4&gt;To customize your project, you can:&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Modify &lt;code&gt;src/my_project/config/agents.yaml&lt;/code&gt; to define your agents.&lt;/li&gt; 
 &lt;li&gt;Modify &lt;code&gt;src/my_project/config/tasks.yaml&lt;/code&gt; to define your tasks.&lt;/li&gt; 
 &lt;li&gt;Modify &lt;code&gt;src/my_project/crew.py&lt;/code&gt; to add your own logic, tools, and specific arguments.&lt;/li&gt; 
 &lt;li&gt;Modify &lt;code&gt;src/my_project/main.py&lt;/code&gt; to add custom inputs for your agents and tasks.&lt;/li&gt; 
 &lt;li&gt;Add your environment variables into the &lt;code&gt;.env&lt;/code&gt; file.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Example of a simple crew with a sequential process:&lt;/h4&gt; 
&lt;p&gt;Instantiate your crew:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;crewai create crew latest-ai-development
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Modify the files as needed to fit your use case:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;agents.yaml&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;# src/my_project/config/agents.yaml
researcher:
  role: &amp;gt;
    {topic} Senior Data Researcher
  goal: &amp;gt;
    Uncover cutting-edge developments in {topic}
  backstory: &amp;gt;
    You're a seasoned researcher with a knack for uncovering the latest
    developments in {topic}. Known for your ability to find the most relevant
    information and present it in a clear and concise manner.

reporting_analyst:
  role: &amp;gt;
    {topic} Reporting Analyst
  goal: &amp;gt;
    Create detailed reports based on {topic} data analysis and research findings
  backstory: &amp;gt;
    You're a meticulous analyst with a keen eye for detail. You're known for
    your ability to turn complex data into clear and concise reports, making
    it easy for others to understand and act on the information you provide.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;tasks.yaml&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;# src/my_project/config/tasks.yaml
research_task:
  description: &amp;gt;
    Conduct a thorough research about {topic}
    Make sure you find any interesting and relevant information given
    the current year is 2025.
  expected_output: &amp;gt;
    A list with 10 bullet points of the most relevant information about {topic}
  agent: researcher

reporting_task:
  description: &amp;gt;
    Review the context you got and expand each topic into a full section for a report.
    Make sure the report is detailed and contains any and all relevant information.
  expected_output: &amp;gt;
    A fully fledge reports with the mains topics, each with a full section of information.
    Formatted as markdown without '```'
  agent: reporting_analyst
  output_file: report.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;crew.py&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# src/my_project/crew.py
from crewai import Agent, Crew, Process, Task
from crewai.project import CrewBase, agent, crew, task
from crewai_tools import SerperDevTool
from crewai.agents.agent_builder.base_agent import BaseAgent
from typing import List

@CrewBase
class LatestAiDevelopmentCrew():
	"""LatestAiDevelopment crew"""
	agents: List[BaseAgent]
	tasks: List[Task]

	@agent
	def researcher(self) -&amp;gt; Agent:
		return Agent(
			config=self.agents_config['researcher'],
			verbose=True,
			tools=[SerperDevTool()]
		)

	@agent
	def reporting_analyst(self) -&amp;gt; Agent:
		return Agent(
			config=self.agents_config['reporting_analyst'],
			verbose=True
		)

	@task
	def research_task(self) -&amp;gt; Task:
		return Task(
			config=self.tasks_config['research_task'],
		)

	@task
	def reporting_task(self) -&amp;gt; Task:
		return Task(
			config=self.tasks_config['reporting_task'],
			output_file='report.md'
		)

	@crew
	def crew(self) -&amp;gt; Crew:
		"""Creates the LatestAiDevelopment crew"""
		return Crew(
			agents=self.agents, # Automatically created by the @agent decorator
			tasks=self.tasks, # Automatically created by the @task decorator
			process=Process.sequential,
			verbose=True,
		)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;main.py&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;#!/usr/bin/env python
# src/my_project/main.py
import sys
from latest_ai_development.crew import LatestAiDevelopmentCrew

def run():
    """
    Run the crew.
    """
    inputs = {
        'topic': 'AI Agents'
    }
    LatestAiDevelopmentCrew().crew().kickoff(inputs=inputs)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. Running Your Crew&lt;/h3&gt; 
&lt;p&gt;Before running your crew, make sure you have the following keys set as environment variables in your &lt;code&gt;.env&lt;/code&gt; file:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;An &lt;a href="https://platform.openai.com/account/api-keys"&gt;OpenAI API key&lt;/a&gt; (or other LLM API key): &lt;code&gt;OPENAI_API_KEY=sk-...&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;A &lt;a href="https://serper.dev/"&gt;Serper.dev&lt;/a&gt; API key: &lt;code&gt;SERPER_API_KEY=YOUR_KEY_HERE&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Lock the dependencies and install them by using the CLI command but first, navigate to your project directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;cd my_project
crewai install (Optional)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To run your crew, execute the following command in the root of your project:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;crewai run
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python src/my_project/main.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If an error happens due to the usage of poetry, please run the following command to update your crewai package:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;crewai update
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You should see the output in the console and the &lt;code&gt;report.md&lt;/code&gt; file should be created in the root of your project with the full final report.&lt;/p&gt; 
&lt;p&gt;In addition to the sequential process, you can use the hierarchical process, which automatically assigns a manager to the defined crew to properly coordinate the planning and execution of tasks through delegation and validation of results. &lt;a href="https://docs.crewai.com/core-concepts/Processes/"&gt;See more about the processes here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;p&gt;CrewAI stands apart as a lean, standalone, high-performance multi-AI Agent framework delivering simplicity, flexibility, and precise control‚Äîfree from the complexity and limitations found in other agent frameworks.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Standalone &amp;amp; Lean&lt;/strong&gt;: Completely independent from other frameworks like LangChain, offering faster execution and lighter resource demands.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible &amp;amp; Precise&lt;/strong&gt;: Easily orchestrate autonomous agents through intuitive &lt;a href="https://docs.crewai.com/concepts/crews"&gt;Crews&lt;/a&gt; or precise &lt;a href="https://docs.crewai.com/concepts/flows"&gt;Flows&lt;/a&gt;, achieving perfect balance for your needs.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Seamless Integration&lt;/strong&gt;: Effortlessly combine Crews (autonomy) and Flows (precision) to create complex, real-world automations.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Deep Customization&lt;/strong&gt;: Tailor every aspect‚Äîfrom high-level workflows down to low-level internal prompts and agent behaviors.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Reliable Performance&lt;/strong&gt;: Consistent results across simple tasks and complex, enterprise-level automations.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Thriving Community&lt;/strong&gt;: Backed by robust documentation and over 100,000 certified developers, providing exceptional support and guidance.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Choose CrewAI to easily build powerful, adaptable, and production-ready AI automations.&lt;/p&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;p&gt;You can test different real life examples of AI crews in the &lt;a href="https://github.com/crewAIInc/crewAI-examples?tab=readme-ov-file"&gt;CrewAI-examples repo&lt;/a&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/crewAIInc/crewAI-examples/tree/main/crews/landing_page_generator"&gt;Landing Page Generator&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.crewai.com/how-to/Human-Input-on-Execution"&gt;Having Human input on the execution&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/crewAIInc/crewAI-examples/tree/main/crews/trip_planner"&gt;Trip Planner&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/crewAIInc/crewAI-examples/tree/main/crews/stock_analysis"&gt;Stock Analysis&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Quick Tutorial&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=tnejrr-0a94" title="CrewAI Tutorial"&gt;&lt;img src="https://img.youtube.com/vi/tnejrr-0a94/maxresdefault.jpg" alt="CrewAI Tutorial" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Write Job Descriptions&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/crewAIInc/crewAI-examples/tree/main/crews/job-posting"&gt;Check out code for this example&lt;/a&gt; or watch a video below:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=u98wEMz-9to" title="Jobs postings"&gt;&lt;img src="https://img.youtube.com/vi/u98wEMz-9to/maxresdefault.jpg" alt="Jobs postings" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Trip Planner&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/crewAIInc/crewAI-examples/tree/main/crews/trip_planner"&gt;Check out code for this example&lt;/a&gt; or watch a video below:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=xis7rWp-hjs" title="Trip Planner"&gt;&lt;img src="https://img.youtube.com/vi/xis7rWp-hjs/maxresdefault.jpg" alt="Trip Planner" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Stock Analysis&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/crewAIInc/crewAI-examples/tree/main/crews/stock_analysis"&gt;Check out code for this example&lt;/a&gt; or watch a video below:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=e0Uj4yWdaAg" title="Stock Analysis"&gt;&lt;img src="https://img.youtube.com/vi/e0Uj4yWdaAg/maxresdefault.jpg" alt="Stock Analysis" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Using Crews and Flows Together&lt;/h3&gt; 
&lt;p&gt;CrewAI's power truly shines when combining Crews with Flows to create sophisticated automation pipelines. CrewAI flows support logical operators like &lt;code&gt;or_&lt;/code&gt; and &lt;code&gt;and_&lt;/code&gt; to combine multiple conditions. This can be used with &lt;code&gt;@start&lt;/code&gt;, &lt;code&gt;@listen&lt;/code&gt;, or &lt;code&gt;@router&lt;/code&gt; decorators to create complex triggering conditions.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;or_&lt;/code&gt;: Triggers when any of the specified conditions are met.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;and_&lt;/code&gt;Triggers when all of the specified conditions are met.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Here's how you can orchestrate multiple Crews within a Flow:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from crewai.flow.flow import Flow, listen, start, router, or_
from crewai import Crew, Agent, Task, Process
from pydantic import BaseModel

# Define structured state for precise control
class MarketState(BaseModel):
    sentiment: str = "neutral"
    confidence: float = 0.0
    recommendations: list = []

class AdvancedAnalysisFlow(Flow[MarketState]):
    @start()
    def fetch_market_data(self):
        # Demonstrate low-level control with structured state
        self.state.sentiment = "analyzing"
        return {"sector": "tech", "timeframe": "1W"}  # These parameters match the task description template

    @listen(fetch_market_data)
    def analyze_with_crew(self, market_data):
        # Show crew agency through specialized roles
        analyst = Agent(
            role="Senior Market Analyst",
            goal="Conduct deep market analysis with expert insight",
            backstory="You're a veteran analyst known for identifying subtle market patterns"
        )
        researcher = Agent(
            role="Data Researcher",
            goal="Gather and validate supporting market data",
            backstory="You excel at finding and correlating multiple data sources"
        )

        analysis_task = Task(
            description="Analyze {sector} sector data for the past {timeframe}",
            expected_output="Detailed market analysis with confidence score",
            agent=analyst
        )
        research_task = Task(
            description="Find supporting data to validate the analysis",
            expected_output="Corroborating evidence and potential contradictions",
            agent=researcher
        )

        # Demonstrate crew autonomy
        analysis_crew = Crew(
            agents=[analyst, researcher],
            tasks=[analysis_task, research_task],
            process=Process.sequential,
            verbose=True
        )
        return analysis_crew.kickoff(inputs=market_data)  # Pass market_data as named inputs

    @router(analyze_with_crew)
    def determine_next_steps(self):
        # Show flow control with conditional routing
        if self.state.confidence &amp;gt; 0.8:
            return "high_confidence"
        elif self.state.confidence &amp;gt; 0.5:
            return "medium_confidence"
        return "low_confidence"

    @listen("high_confidence")
    def execute_strategy(self):
        # Demonstrate complex decision making
        strategy_crew = Crew(
            agents=[
                Agent(role="Strategy Expert",
                      goal="Develop optimal market strategy")
            ],
            tasks=[
                Task(description="Create detailed strategy based on analysis",
                     expected_output="Step-by-step action plan")
            ]
        )
        return strategy_crew.kickoff()

    @listen(or_("medium_confidence", "low_confidence"))
    def request_additional_analysis(self):
        self.state.recommendations.append("Gather more data")
        return "Additional analysis required"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This example demonstrates how to:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Use Python code for basic data operations&lt;/li&gt; 
 &lt;li&gt;Create and execute Crews as steps in your workflow&lt;/li&gt; 
 &lt;li&gt;Use Flow decorators to manage the sequence of operations&lt;/li&gt; 
 &lt;li&gt;Implement conditional branching based on Crew results&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Connecting Your Crew to a Model&lt;/h2&gt; 
&lt;p&gt;CrewAI supports using various LLMs through a variety of connection options. By default your agents will use the OpenAI API when querying the model. However, there are several other ways to allow your agents to connect to models. For example, you can configure your agents to use a local model via the Ollama tool.&lt;/p&gt; 
&lt;p&gt;Please refer to the &lt;a href="https://docs.crewai.com/how-to/LLM-Connections/"&gt;Connect CrewAI to LLMs&lt;/a&gt; page for details on configuring your agents' connections to models.&lt;/p&gt; 
&lt;h2&gt;How CrewAI Compares&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;CrewAI's Advantage&lt;/strong&gt;: CrewAI combines autonomous agent intelligence with precise workflow control through its unique Crews and Flows architecture. The framework excels at both high-level orchestration and low-level customization, enabling complex, production-grade systems with granular control.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;LangGraph&lt;/strong&gt;: While LangGraph provides a foundation for building agent workflows, its approach requires significant boilerplate code and complex state management patterns. The framework's tight coupling with LangChain can limit flexibility when implementing custom agent behaviors or integrating with external systems.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;em&gt;P.S. CrewAI demonstrates significant performance advantages over LangGraph, executing 5.76x faster in certain cases like this QA task example (&lt;a href="https://github.com/crewAIInc/crewAI-examples/tree/main/Notebooks/CrewAI%20Flows%20%26%20Langgraph/QA%20Agent"&gt;see comparison&lt;/a&gt;) while achieving higher evaluation scores with faster completion times in certain coding tasks, like in this example (&lt;a href="https://github.com/crewAIInc/crewAI-examples/raw/main/Notebooks/CrewAI%20Flows%20%26%20Langgraph/Coding%20Assistant/coding_assistant_eval.ipynb"&gt;detailed analysis&lt;/a&gt;).&lt;/em&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Autogen&lt;/strong&gt;: While Autogen excels at creating conversational agents capable of working together, it lacks an inherent concept of process. In Autogen, orchestrating agents' interactions requires additional programming, which can become complex and cumbersome as the scale of tasks grows.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ChatDev&lt;/strong&gt;: ChatDev introduced the idea of processes into the realm of AI agents, but its implementation is quite rigid. Customizations in ChatDev are limited and not geared towards production environments, which can hinder scalability and flexibility in real-world applications.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contribution&lt;/h2&gt; 
&lt;p&gt;CrewAI is open-source and we welcome contributions. If you're looking to contribute, please:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Fork the repository.&lt;/li&gt; 
 &lt;li&gt;Create a new branch for your feature.&lt;/li&gt; 
 &lt;li&gt;Add your feature or improvement.&lt;/li&gt; 
 &lt;li&gt;Send a pull request.&lt;/li&gt; 
 &lt;li&gt;We appreciate your input!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Installing Dependencies&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv lock
uv sync
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Virtual Env&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv venv
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Pre-commit hooks&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pre-commit install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Running Tests&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run pytest .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Running static type checks&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uvx mypy src
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Packaging&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv build
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Installing Locally&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv pip install dist/*.tar.gz
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Telemetry&lt;/h2&gt; 
&lt;p&gt;CrewAI uses anonymous telemetry to collect usage data with the main purpose of helping us improve the library by focusing our efforts on the most used features, integrations and tools.&lt;/p&gt; 
&lt;p&gt;It's pivotal to understand that &lt;strong&gt;NO data is collected&lt;/strong&gt; concerning prompts, task descriptions, agents' backstories or goals, usage of tools, API calls, responses, any data processed by the agents, or secrets and environment variables, with the exception of the conditions mentioned. When the &lt;code&gt;share_crew&lt;/code&gt; feature is enabled, detailed data including task descriptions, agents' backstories or goals, and other specific attributes are collected to provide deeper insights while respecting user privacy. Users can disable telemetry by setting the environment variable OTEL_SDK_DISABLED to true.&lt;/p&gt; 
&lt;p&gt;Data collected includes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Version of CrewAI 
  &lt;ul&gt; 
   &lt;li&gt;So we can understand how many users are using the latest version&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Version of Python 
  &lt;ul&gt; 
   &lt;li&gt;So we can decide on what versions to better support&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;General OS (e.g. number of CPUs, macOS/Windows/Linux) 
  &lt;ul&gt; 
   &lt;li&gt;So we know what OS we should focus on and if we could build specific OS related features&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Number of agents and tasks in a crew 
  &lt;ul&gt; 
   &lt;li&gt;So we make sure we are testing internally with similar use cases and educate people on the best practices&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Crew Process being used 
  &lt;ul&gt; 
   &lt;li&gt;Understand where we should focus our efforts&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;If Agents are using memory or allowing delegation 
  &lt;ul&gt; 
   &lt;li&gt;Understand if we improved the features or maybe even drop them&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;If Tasks are being executed in parallel or sequentially 
  &lt;ul&gt; 
   &lt;li&gt;Understand if we should focus more on parallel execution&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Language model being used 
  &lt;ul&gt; 
   &lt;li&gt;Improved support on most used languages&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Roles of agents in a crew 
  &lt;ul&gt; 
   &lt;li&gt;Understand high level use cases so we can build better tools, integrations and examples about it&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Tools names available 
  &lt;ul&gt; 
   &lt;li&gt;Understand out of the publicly available tools, which ones are being used the most so we can improve them&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Users can opt-in to Further Telemetry, sharing the complete telemetry data by setting the &lt;code&gt;share_crew&lt;/code&gt; attribute to &lt;code&gt;True&lt;/code&gt; on their Crews. Enabling &lt;code&gt;share_crew&lt;/code&gt; results in the collection of detailed crew and task execution data, including &lt;code&gt;goal&lt;/code&gt;, &lt;code&gt;backstory&lt;/code&gt;, &lt;code&gt;context&lt;/code&gt;, and &lt;code&gt;output&lt;/code&gt; of tasks. This enables a deeper insight into usage patterns while respecting the user's choice to share.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;CrewAI is released under the &lt;a href="https://github.com/crewAIInc/crewAI/raw/main/LICENSE"&gt;MIT License&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Frequently Asked Questions (FAQ)&lt;/h2&gt; 
&lt;h3&gt;General&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-what-exactly-is-crewai"&gt;What exactly is CrewAI?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-how-do-i-install-crewai"&gt;How do I install CrewAI?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-does-crewai-depend-on-langchain"&gt;Does CrewAI depend on LangChain?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-is-crewai-open-source"&gt;Is CrewAI open-source?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-does-crewai-collect-data-from-users"&gt;Does CrewAI collect data from users?&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Features and Capabilities&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-can-crewai-handle-complex-use-cases"&gt;Can CrewAI handle complex use cases?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-can-i-use-crewai-with-local-ai-models"&gt;Can I use CrewAI with local AI models?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-what-makes-crews-different-from-flows"&gt;What makes Crews different from Flows?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-how-is-crewai-better-than-langchain"&gt;How is CrewAI better than LangChain?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-does-crewai-support-fine-tuning-or-training-custom-models"&gt;Does CrewAI support fine-tuning or training custom models?&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Resources and Community&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-where-can-i-find-real-world-crewai-examples"&gt;Where can I find real-world CrewAI examples?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-how-can-i-contribute-to-crewai"&gt;How can I contribute to CrewAI?&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Enterprise Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-what-additional-features-does-crewai-amp-offer"&gt;What additional features does CrewAI AMP offer?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-is-crewai-amp-available-for-cloud-and-on-premise-deployments"&gt;Is CrewAI AMP available for cloud and on-premise deployments?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-can-i-try-crewai-amp-for-free"&gt;Can I try CrewAI AMP for free?&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Q: What exactly is CrewAI?&lt;/h3&gt; 
&lt;p&gt;A: CrewAI is a standalone, lean, and fast Python framework built specifically for orchestrating autonomous AI agents. Unlike frameworks like LangChain, CrewAI does not rely on external dependencies, making it leaner, faster, and simpler.&lt;/p&gt; 
&lt;h3&gt;Q: How do I install CrewAI?&lt;/h3&gt; 
&lt;p&gt;A: Install CrewAI using pip:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;uv pip install crewai
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For additional tools, use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;uv pip install 'crewai[tools]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Q: Does CrewAI depend on LangChain?&lt;/h3&gt; 
&lt;p&gt;A: No. CrewAI is built entirely from the ground up, with no dependencies on LangChain or other agent frameworks. This ensures a lean, fast, and flexible experience.&lt;/p&gt; 
&lt;h3&gt;Q: Can CrewAI handle complex use cases?&lt;/h3&gt; 
&lt;p&gt;A: Yes. CrewAI excels at both simple and highly complex real-world scenarios, offering deep customization options at both high and low levels, from internal prompts to sophisticated workflow orchestration.&lt;/p&gt; 
&lt;h3&gt;Q: Can I use CrewAI with local AI models?&lt;/h3&gt; 
&lt;p&gt;A: Absolutely! CrewAI supports various language models, including local ones. Tools like Ollama and LM Studio allow seamless integration. Check the &lt;a href="https://docs.crewai.com/how-to/LLM-Connections/"&gt;LLM Connections documentation&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h3&gt;Q: What makes Crews different from Flows?&lt;/h3&gt; 
&lt;p&gt;A: Crews provide autonomous agent collaboration, ideal for tasks requiring flexible decision-making and dynamic interaction. Flows offer precise, event-driven control, ideal for managing detailed execution paths and secure state management. You can seamlessly combine both for maximum effectiveness.&lt;/p&gt; 
&lt;h3&gt;Q: How is CrewAI better than LangChain?&lt;/h3&gt; 
&lt;p&gt;A: CrewAI provides simpler, more intuitive APIs, faster execution speeds, more reliable and consistent results, robust documentation, and an active community‚Äîaddressing common criticisms and limitations associated with LangChain.&lt;/p&gt; 
&lt;h3&gt;Q: Is CrewAI open-source?&lt;/h3&gt; 
&lt;p&gt;A: Yes, CrewAI is open-source and actively encourages community contributions and collaboration.&lt;/p&gt; 
&lt;h3&gt;Q: Does CrewAI collect data from users?&lt;/h3&gt; 
&lt;p&gt;A: CrewAI collects anonymous telemetry data strictly for improvement purposes. Sensitive data such as prompts, tasks, or API responses are never collected unless explicitly enabled by the user.&lt;/p&gt; 
&lt;h3&gt;Q: Where can I find real-world CrewAI examples?&lt;/h3&gt; 
&lt;p&gt;A: Check out practical examples in the &lt;a href="https://github.com/crewAIInc/crewAI-examples"&gt;CrewAI-examples repository&lt;/a&gt;, covering use cases like trip planners, stock analysis, and job postings.&lt;/p&gt; 
&lt;h3&gt;Q: How can I contribute to CrewAI?&lt;/h3&gt; 
&lt;p&gt;A: Contributions are warmly welcomed! Fork the repository, create your branch, implement your changes, and submit a pull request. See the Contribution section of the README for detailed guidelines.&lt;/p&gt; 
&lt;h3&gt;Q: What additional features does CrewAI AMP offer?&lt;/h3&gt; 
&lt;p&gt;A: CrewAI AMP provides advanced features such as a unified control plane, real-time observability, secure integrations, advanced security, actionable insights, and dedicated 24/7 enterprise support.&lt;/p&gt; 
&lt;h3&gt;Q: Is CrewAI AMP available for cloud and on-premise deployments?&lt;/h3&gt; 
&lt;p&gt;A: Yes, CrewAI AMP supports both cloud-based and on-premise deployment options, allowing enterprises to meet their specific security and compliance requirements.&lt;/p&gt; 
&lt;h3&gt;Q: Can I try CrewAI AMP for free?&lt;/h3&gt; 
&lt;p&gt;A: Yes, you can explore part of the CrewAI AMP Suite by accessing the &lt;a href="https://app.crewai.com"&gt;Crew Control Plane&lt;/a&gt; for free.&lt;/p&gt; 
&lt;h3&gt;Q: Does CrewAI support fine-tuning or training custom models?&lt;/h3&gt; 
&lt;p&gt;A: Yes, CrewAI can integrate with custom-trained or fine-tuned models, allowing you to enhance your agents with domain-specific knowledge and accuracy.&lt;/p&gt; 
&lt;h3&gt;Q: Can CrewAI agents interact with external tools and APIs?&lt;/h3&gt; 
&lt;p&gt;A: Absolutely! CrewAI agents can easily integrate with external tools, APIs, and databases, empowering them to leverage real-world data and resources.&lt;/p&gt; 
&lt;h3&gt;Q: Is CrewAI suitable for production environments?&lt;/h3&gt; 
&lt;p&gt;A: Yes, CrewAI is explicitly designed with production-grade standards, ensuring reliability, stability, and scalability for enterprise deployments.&lt;/p&gt; 
&lt;h3&gt;Q: How scalable is CrewAI?&lt;/h3&gt; 
&lt;p&gt;A: CrewAI is highly scalable, supporting simple automations and large-scale enterprise workflows involving numerous agents and complex tasks simultaneously.&lt;/p&gt; 
&lt;h3&gt;Q: Does CrewAI offer debugging and monitoring tools?&lt;/h3&gt; 
&lt;p&gt;A: Yes, CrewAI AMP includes advanced debugging, tracing, and real-time observability features, simplifying the management and troubleshooting of your automations.&lt;/p&gt; 
&lt;h3&gt;Q: What programming languages does CrewAI support?&lt;/h3&gt; 
&lt;p&gt;A: CrewAI is primarily Python-based but easily integrates with services and APIs written in any programming language through its flexible API integration capabilities.&lt;/p&gt; 
&lt;h3&gt;Q: Does CrewAI offer educational resources for beginners?&lt;/h3&gt; 
&lt;p&gt;A: Yes, CrewAI provides extensive beginner-friendly tutorials, courses, and documentation through learn.crewai.com, supporting developers at all skill levels.&lt;/p&gt; 
&lt;h3&gt;Q: Can CrewAI automate human-in-the-loop workflows?&lt;/h3&gt; 
&lt;p&gt;A: Yes, CrewAI fully supports human-in-the-loop workflows, allowing seamless collaboration between human experts and AI agents for enhanced decision-making.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>3b1b/manim</title>
      <link>https://github.com/3b1b/manim</link>
      <description>&lt;p&gt;Animation engine for explanatory math videos&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://github.com/3b1b/manim"&gt; &lt;img src="https://raw.githubusercontent.com/3b1b/manim/master/logo/cropped.png" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;&lt;a href="https://pypi.org/project/manimgl/"&gt;&lt;img src="https://img.shields.io/pypi/v/manimgl?logo=pypi" alt="pypi version" /&gt;&lt;/a&gt; &lt;a href="http://choosealicense.com/licenses/mit/"&gt;&lt;img src="https://img.shields.io/badge/license-MIT-blue.svg?style=flat" alt="MIT License" /&gt;&lt;/a&gt; &lt;a href="https://www.reddit.com/r/manim/"&gt;&lt;img src="https://img.shields.io/reddit/subreddit-subscribers/manim.svg?color=ff4301&amp;amp;label=reddit&amp;amp;logo=reddit" alt="Manim Subreddit" /&gt;&lt;/a&gt; &lt;a href="https://discord.com/invite/bYCyhM9Kz2"&gt;&lt;img src="https://img.shields.io/discord/581738731934056449.svg?label=discord&amp;amp;logo=discord" alt="Manim Discord" /&gt;&lt;/a&gt; &lt;a href="https://3b1b.github.io/manim/"&gt;&lt;img src="https://github.com/3b1b/manim/workflows/docs/badge.svg?sanitize=true" alt="docs" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Manim is an engine for precise programmatic animations, designed for creating explanatory math videos.&lt;/p&gt; 
&lt;p&gt;Note, there are two versions of manim. This repository began as a personal project by the author of &lt;a href="https://www.3blue1brown.com/"&gt;3Blue1Brown&lt;/a&gt; for the purpose of animating those videos, with video-specific code available &lt;a href="https://github.com/3b1b/videos"&gt;here&lt;/a&gt;. In 2020 a group of developers forked it into what is now the &lt;a href="https://github.com/ManimCommunity/manim/"&gt;community edition&lt;/a&gt;, with a goal of being more stable, better tested, quicker to respond to community contributions, and all around friendlier to get started with. See &lt;a href="https://docs.manim.community/en/stable/faq/installation.html#different-versions"&gt;this page&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Warning] &lt;strong&gt;WARNING:&lt;/strong&gt; These instructions are for ManimGL &lt;em&gt;only&lt;/em&gt;. Trying to use these instructions to install &lt;a href="https://github.com/ManimCommunity/manim"&gt;Manim Community/manim&lt;/a&gt; or instructions there to install this version will cause problems. You should first decide which version you wish to install, then only follow the instructions for your desired version.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Note] &lt;strong&gt;Note&lt;/strong&gt;: To install manim directly through pip, please pay attention to the name of the installed package. This repository is ManimGL of 3b1b. The package name is &lt;code&gt;manimgl&lt;/code&gt; instead of &lt;code&gt;manim&lt;/code&gt; or &lt;code&gt;manimlib&lt;/code&gt;. Please use &lt;code&gt;pip install manimgl&lt;/code&gt; to install the version in this repository.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Manim runs on Python 3.7 or higher.&lt;/p&gt; 
&lt;p&gt;System requirements are &lt;a href="https://ffmpeg.org/"&gt;FFmpeg&lt;/a&gt;, &lt;a href="https://www.opengl.org/"&gt;OpenGL&lt;/a&gt; and &lt;a href="https://www.latex-project.org"&gt;LaTeX&lt;/a&gt; (optional, if you want to use LaTeX). For Linux, &lt;a href="https://pango.org"&gt;Pango&lt;/a&gt; along with its development headers are required. See instruction &lt;a href="https://github.com/ManimCommunity/ManimPango#building"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Directly&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Install manimgl
pip install manimgl

# Try it out
manimgl
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more options, take a look at the &lt;a href="https://raw.githubusercontent.com/3b1b/manim/master/#using-manim"&gt;Using manim&lt;/a&gt; sections further below.&lt;/p&gt; 
&lt;p&gt;If you want to hack on manimlib itself, clone this repository and in that directory execute:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Install manimgl
pip install -e .

# Try it out
manimgl example_scenes.py OpeningManimExample
# or
manim-render example_scenes.py OpeningManimExample
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Directly (Windows)&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://www.wikihow.com/Install-FFmpeg-on-Windows"&gt;Install FFmpeg&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Install a LaTeX distribution. &lt;a href="https://miktex.org/download"&gt;MiKTeX&lt;/a&gt; is recommended.&lt;/li&gt; 
 &lt;li&gt;Install the remaining Python packages. &lt;pre&gt;&lt;code class="language-sh"&gt;git clone https://github.com/3b1b/manim.git
cd manim
pip install -e .
manimgl example_scenes.py OpeningManimExample
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Mac OSX&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Install FFmpeg, LaTeX in terminal using homebrew.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;brew install ffmpeg mactex
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;If you are using an ARM-based processor, install Cairo.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;arch -arm64 brew install pkg-config cairo
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install latest version of manim using these command.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;git clone https://github.com/3b1b/manim.git
cd manim
pip install -e .
manimgl example_scenes.py OpeningManimExample (make sure to add manimgl to path first.)
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Anaconda Install&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install LaTeX as above.&lt;/li&gt; 
 &lt;li&gt;Create a conda environment using &lt;code&gt;conda create -n manim python=3.8&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Activate the environment using &lt;code&gt;conda activate manim&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Install manimgl using &lt;code&gt;pip install -e .&lt;/code&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Using manim&lt;/h2&gt; 
&lt;p&gt;Try running the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;manimgl example_scenes.py OpeningManimExample
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This should pop up a window playing a simple scene.&lt;/p&gt; 
&lt;p&gt;Look through the &lt;a href="https://3b1b.github.io/manim/getting_started/example_scenes.html"&gt;example scenes&lt;/a&gt; to see examples of the library's syntax, animation types and object types. In the &lt;a href="https://github.com/3b1b/videos"&gt;3b1b/videos&lt;/a&gt; repo, you can see all the code for 3blue1brown videos, though code from older videos may not be compatible with the most recent version of manim. The readme of that repo also outlines some details for how to set up a more interactive workflow, as shown in &lt;a href="https://www.youtube.com/watch?v=rbu7Zu5X1zI"&gt;this manim demo video&lt;/a&gt; for example.&lt;/p&gt; 
&lt;p&gt;When running in the CLI, some useful flags include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;-w&lt;/code&gt; to write the scene to a file&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-o&lt;/code&gt; to write the scene to a file and open the result&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-s&lt;/code&gt; to skip to the end and just show the final frame. 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;-so&lt;/code&gt; will save the final frame to an image and show it&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-n &amp;lt;number&amp;gt;&lt;/code&gt; to skip ahead to the &lt;code&gt;n&lt;/code&gt;'th animation of a scene.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-f&lt;/code&gt; to make the playback window fullscreen&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Take a look at custom_config.yml for further configuration. To add your customization, you can either edit this file, or add another file by the same name "custom_config.yml" to whatever directory you are running manim from. For example &lt;a href="https://github.com/3b1b/videos/raw/master/custom_config.yml"&gt;this is the one&lt;/a&gt; for 3blue1brown videos. There you can specify where videos should be output to, where manim should look for image files and sounds you want to read in, and other defaults regarding style and video quality.&lt;/p&gt; 
&lt;h3&gt;Documentation&lt;/h3&gt; 
&lt;p&gt;Documentation is in progress at &lt;a href="https://3b1b.github.io/manim/"&gt;3b1b.github.io/manim&lt;/a&gt;. And there is also a Chinese version maintained by &lt;a href="https://manim.org.cn"&gt;&lt;strong&gt;@manim-kindergarten&lt;/strong&gt;&lt;/a&gt;: &lt;a href="https://docs.manim.org.cn/"&gt;docs.manim.org.cn&lt;/a&gt; (in Chinese).&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/manim-kindergarten/"&gt;manim-kindergarten&lt;/a&gt; wrote and collected some useful extra classes and some codes of videos in &lt;a href="https://github.com/manim-kindergarten/manim_sandbox"&gt;manim_sandbox repo&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Is always welcome. As mentioned above, the &lt;a href="https://github.com/ManimCommunity/manim"&gt;community edition&lt;/a&gt; has the most active ecosystem for contributions, with testing and continuous integration, but pull requests are welcome here too. Please explain the motivation for a given change and examples of its effect.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project falls under the MIT license.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>QwenLM/Qwen-Image</title>
      <link>https://github.com/QwenLM/Qwen-Image</link>
      <description>&lt;p&gt;Qwen-Image is a powerful image generation foundation model capable of complex text rendering and precise image editing.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/qwen_image_logo.png" width="400" /&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p align="center"&gt;&amp;nbsp;&amp;nbsp;üíú &lt;a href="https://chat.qwen.ai/"&gt;Qwen Chat&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ü§ó &lt;a href="https://huggingface.co/Qwen/Qwen-Image"&gt;HuggingFace(T2I)&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ü§ó &lt;a href="https://huggingface.co/Qwen/Qwen-Image-Edit-2511"&gt;HuggingFace(Edit)&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ü§ñ &lt;a href="https://modelscope.cn/models/Qwen/Qwen-Image"&gt;ModelScope-T2I&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ü§ñ &lt;a href="https://modelscope.cn/models/Qwen/Qwen-Image-Edit-2511"&gt;ModelScope-Edit&lt;/a&gt;&amp;nbsp;&amp;nbsp;| &amp;nbsp;&amp;nbsp; üìë &lt;a href="https://arxiv.org/abs/2508.02324"&gt;Tech Report&lt;/a&gt; &amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; üìë &lt;a href="https://qwenlm.github.io/blog/qwen-image/"&gt;Blog(T2I)&lt;/a&gt; &amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; üìë &lt;a href="https://qwenlm.github.io/blog/qwen-image-edit-2511/"&gt;Blog(Edit)&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;br /&gt; üñ•Ô∏è &lt;a href="https://huggingface.co/spaces/Qwen/Qwen-Image"&gt;T2I Demo&lt;/a&gt;&amp;nbsp;&amp;nbsp; | üñ•Ô∏è &lt;a href="https://huggingface.co/spaces/Qwen/Qwen-Image-Edit-2511"&gt;Edit Demo&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;üí¨ &lt;a href="https://github.com/QwenLM/Qwen-Image/raw/main/assets/wechat.png"&gt;WeChat (ÂæÆ‰ø°)&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ü´® &lt;a href="https://discord.gg/CV4E9rpNSD"&gt;Discord&lt;/a&gt;&amp;nbsp;&amp;nbsp; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/merge3.jpg" width="1024" /&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;We are thrilled to release &lt;strong&gt;Qwen-Image&lt;/strong&gt;, a 20B MMDiT image foundation model that achieves significant advances in &lt;strong&gt;complex text rendering&lt;/strong&gt; and &lt;strong&gt;precise image editing&lt;/strong&gt;. Experiments show strong general capabilities in both image generation and editing, with exceptional performance in text rendering, especially for Chinese.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/bench.png#center" alt="" /&gt;&lt;/p&gt; 
&lt;h2&gt;News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;2025.12.31: We released Qwen-Image-2512 weights! Check at &lt;a href="https://huggingface.co/Qwen/Qwen-Image-2512"&gt;Huggingface&lt;/a&gt; and &lt;a href="https://modelscope.cn/models/Qwen/Qwen-Image-2512"&gt;ModelScope&lt;/a&gt;!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.12.31: We released Qwen-Image-2512! Check our &lt;a href="https://qwen.ai/blog?id=qwen-image-2512"&gt;Blog&lt;/a&gt; for more details! üöÄ Our December upgrade to Qwen-Image, just in time for the New Year.&lt;/p&gt; &lt;p&gt;‚ú® What‚Äôs new: ‚Ä¢ More realistic humans ‚Äî dramatically reduced ‚ÄúAI look,‚Äù richer facial &amp;amp; age details ‚Ä¢ Finer natural textures ‚Äî sharper landscapes, water, fur, and materials ‚Ä¢ Stronger text rendering ‚Äî better layout, higher accuracy in text‚Äìimage composition&lt;/p&gt; &lt;p&gt;üèÜ Tested in 10,000+ blind rounds on AI Arena, Qwen-Image-2512 ranks as the strongest open-source image model, while staying competitive with closed-source systems. &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/arena.png#center" alt="" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.12.31: &lt;a href="https://github.com/ModelTC/Qwen-Image-Lightning"&gt;Qwen-Image-Lightning&lt;/a&gt;, developed by &lt;a href="https://github.com/ModelTC/LightX2V"&gt;Lightx2v&lt;/a&gt;, provides &lt;a href="https://huggingface.co/lightx2v/Qwen-Image-2512-Lightning"&gt;Day 0 acceleration support for Qwen-Image-2512&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.12.31:vLLM-Omni supports high performance Qwen-Image-2512 inference from Day-0, with long sequence parallelism, cache acceleration and fast kernels, please check &lt;a href="https://github.com/vllm-project/vllm-omni/tree/main/examples/offline_inference/text_to_image"&gt;here&lt;/a&gt; for details.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.12.23: We released Qwen-Image-Edit-2511 weights! Check at &lt;a href="https://huggingface.co/Qwen/Qwen-Image-Edit-2511"&gt;Huggingface&lt;/a&gt; and &lt;a href="https://modelscope.cn/models/Qwen/Qwen-Image-Edit-2511"&gt;ModelScope&lt;/a&gt;!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.12.23: We released Qwen-Image-Edit-2511! Check our &lt;a href="https://qwen.ai/blog?id=qwen-image-edit-2511"&gt;Blog&lt;/a&gt; for more details!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.12.23: &lt;strong&gt;&lt;a href="https://github.com/ModelTC/LightX2V/"&gt;LightX2V&lt;/a&gt;&lt;/strong&gt; delivers Day 0 acceleration for Qwen-Image-Edit-2511, with native support for a wide range of hardware, including &lt;strong&gt;NVIDIA, Hygon, Metax, Ascend, and Cambricon&lt;/strong&gt;. By combining &lt;strong&gt;&lt;a href="https://github.com/ModelTC/Qwen-Image-Lightning"&gt;diffusion distillation&lt;/a&gt;&lt;/strong&gt; with cutting-edge inference optimizations, LightX2V achieves a &lt;strong&gt;25x reduction in DiT NFEs&lt;/strong&gt; and &lt;strong&gt;an order-of-magnitude 42.55x overall speedup&lt;/strong&gt;, enabling real-time image editing across diverse AI accelerators.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.12.23: &lt;strong&gt;vLLM-Omni&lt;/strong&gt; supports high performance &lt;code&gt;Qwen-Image-Edit-2511&lt;/code&gt;, &lt;code&gt;Qwen-Image-Layered&lt;/code&gt; inference from Day-0, with long sequence parallelism, cache acceleration and fast kernels, please check &lt;a href="https://github.com/vllm-project/vllm-omni/tree/main/examples/offline_inference/image_to_image"&gt;here&lt;/a&gt; for details.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.12.23: &lt;strong&gt;SGLang-Diffusion&lt;/strong&gt; provides day-0 support for Qwen-Image models. To play with &lt;code&gt;Qwen-Image-Edit-2511&lt;/code&gt; in SGlang, please check community supports section for details.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.12.19: We released Qwen-Image-Layered weights! Check at &lt;a href="https://huggingface.co/Qwen/Qwen-Image-Layered"&gt;Huggingface&lt;/a&gt; and &lt;a href="https://modelscope.cn/models/Qwen/Qwen-Image-Layered"&gt;ModelScope&lt;/a&gt;!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.12.19: We released Qwen-Image-Layered! Check our &lt;a href="https://qwenlm.github.io/blog/qwen-image-layered"&gt;Blog&lt;/a&gt; for more details!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.12.18: We released our &lt;a href="https://arxiv.org/abs/2512.15603"&gt;Research Paper&lt;/a&gt; on Arxiv!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.11.11: &lt;strong&gt;&lt;a href="https://t2i-corebench.github.io/"&gt;T2I-CoreBench&lt;/a&gt;&lt;/strong&gt; offers a comprehensive and complex evaluation of T2I models in real-world scenarios. On this benchmark, Qwen-Image achieves state-of-the-art performance under real-world complexities in both composition and reasoning T2I tasks, surpassing other open-source models and showing comparable results to closed-source ones.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.11.07: LeMiCa is a diffusion model inference acceleration solution developed by China Unicom Data Science and Artificial Intelligence Research Institute. By leveraging cache-based techniques and global denoising path optimization, LeMiCa provides efficient inference support for Qwen-Image, achieving nearly 3x lossless acceleration while maintaining visual consistency and quality. For more details, please visit the homepage: &lt;a href="https://unicomai.github.io/LeMiCa/"&gt;https://unicomai.github.io/LeMiCa/&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.09.22: This September, we are pleased to introduce Qwen-Image-Edit-2509, the monthly iteration of Qwen-Image-Edit. To experience the latest model, please visit &lt;a href="https://qwen.ai"&gt;Qwen Chat&lt;/a&gt; and select the "Image Editing" feature. Compared with Qwen-Image-Edit released in August, the main improvements of Qwen-Image-Edit-2509 include:&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.08.19: We have observed performance misalignments of Qwen-Image-Edit. To ensure optimal results, please update to the latest diffusers commit. Improvements are expected, especially in identity preservation and instruction following.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.08.18: We‚Äôre excited to announce the open-sourcing of Qwen-Image-Edit! üéâ Try it out in your local environment with the quick start guide below, or head over to &lt;a href="https://chat.qwen.ai/"&gt;Qwen Chat&lt;/a&gt; or &lt;a href="https://huggingface.co/spaces/Qwen/Qwen-Image-Edit"&gt;Huggingface Demo&lt;/a&gt; to experience the online demo right away! If you enjoy our work, please show your support by giving our repository a star. Your encouragement means a lot to us!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.08.09: Qwen-Image now supports a variety of LoRA models, such as MajicBeauty LoRA, enabling the generation of highly realistic beauty images. Check out the available weights on &lt;a href="https://modelscope.cn/models/merjic/majicbeauty-qwen1/summary"&gt;ModelScope&lt;/a&gt;. &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/magicbeauty.png#center" alt="" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.08.05: Qwen-Image is now natively supported in ComfyUI, see &lt;a href="https://blog.comfy.org/p/qwen-image-in-comfyui-new-era-of"&gt;Qwen-Image in ComfyUI: New Era of Text Generation in Images!&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.08.05: Qwen-Image is now on Qwen Chat. Click &lt;a href="https://chat.qwen.ai/"&gt;Qwen Chat&lt;/a&gt; and choose "Image Generation".&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.08.05: We released our &lt;a href="https://arxiv.org/abs/2508.02324"&gt;Technical Report&lt;/a&gt; on Arxiv!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.08.04: We released Qwen-Image weights! Check at &lt;a href="https://huggingface.co/Qwen/Qwen-Image"&gt;Huggingface&lt;/a&gt; and &lt;a href="https://modelscope.cn/models/Qwen/Qwen-Image"&gt;ModelScope&lt;/a&gt;!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.08.04: We released Qwen-Image! Check our &lt;a href="https://qwenlm.github.io/blog/qwen-image"&gt;Blog&lt;/a&gt; for more details!&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Due to heavy traffic, if you'd like to experience our demo online, we also recommend visiting DashScope, WaveSpeed, and LibLib. Please find the links below in the community support.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Make sure your transformers&amp;gt;=4.51.3 (Supporting Qwen2.5-VL)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install the latest version of diffusers&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code&gt;pip install git+https://github.com/huggingface/diffusers
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Qwen-Image-2512 (for Text to Image generation, better character realism/texture quality)&lt;/h3&gt; 
&lt;p&gt;We recommand use the latest prompt enhancing tools for Qwen-Image-2512, please check &lt;code&gt;src/examples/tools/prompt_utils_2512.py&lt;/code&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from diffusers import QwenImagePipeline
import torch
# Load the pipeline
if torch.cuda.is_available():
    torch_dtype = torch.bfloat16
    device = "cuda"
else:
    torch_dtype = torch.float32
    device = "cpu"

pipe = QwenImagePipeline.from_pretrained("Qwen/Qwen-Image-2512", torch_dtype=torch_dtype).to(device)

# Generate image
prompt = '''A 20-year-old East Asian girl with delicate, charming features and large, bright brown eyes‚Äîexpressive and lively, with a cheerful or subtly smiling expression. Her naturally wavy long hair is either loose or tied in twin ponytails. She has fair skin and light makeup accentuating her youthful freshness. She wears a modern, cute dress or relaxed outfit in bright, soft colors‚Äîlightweight fabric, minimalist cut. She stands indoors at an anime convention, surrounded by banners, posters, or stalls. Lighting is typical indoor illumination‚Äîno staged lighting‚Äîand the image resembles a casual iPhone snapshot: unpretentious composition, yet brimming with vivid, fresh, youthful charm.'''

negative_prompt = "‰ΩéÂàÜËæ®ÁéáÔºå‰ΩéÁîªË¥®ÔºåËÇ¢‰ΩìÁï∏ÂΩ¢ÔºåÊâãÊåáÁï∏ÂΩ¢ÔºåÁîªÈù¢ËøáÈ•±ÂíåÔºåËú°ÂÉèÊÑüÔºå‰∫∫ËÑ∏Êó†ÁªÜËäÇÔºåËøáÂ∫¶ÂÖâÊªëÔºåÁîªÈù¢ÂÖ∑ÊúâAIÊÑü„ÄÇÊûÑÂõæÊ∑∑‰π±„ÄÇÊñáÂ≠óÊ®°Á≥äÔºåÊâ≠Êõ≤„ÄÇ"


# Generate with different aspect ratios
aspect_ratios = {
    "1:1": (1328, 1328),
    "16:9": (1664, 928),
    "9:16": (928, 1664),
    "4:3": (1472, 1104),
    "3:4": (1104, 1472),
    "3:2": (1584, 1056),
    "2:3": (1056, 1584),
}

width, height = aspect_ratios["16:9"]

image = pipe(
    prompt=prompt,
    negative_prompt=negative_prompt,
    width=width,
    height=height,
    num_inference_steps=50,
    true_cfg_scale=4.0,
    generator=torch.Generator(device="cuda").manual_seed(42)
).images[0]

image.save("example.png")

&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Qwen-Image-Edit-2511 (for Image Editing, Multiple Image Support and Improved Consistency)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import os
import torch
from PIL import Image
from diffusers import QwenImageEditPlusPipeline
from io import BytesIO
import requests

pipeline = QwenImageEditPlusPipeline.from_pretrained("Qwen/Qwen-Image-Edit-2511", torch_dtype=torch.bfloat16)
print("pipeline loaded")

pipeline.to('cuda')
pipeline.set_progress_bar_config(disable=None)
image1 = Image.open(BytesIO(requests.get("https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen-Image/edit2511/edit2511input.png").content))
prompt = "Ëøô‰∏™Â•≥ÁîüÁúãÁùÄÈù¢ÂâçÁöÑÁîµËßÜÂ±èÂπïÔºåÂ±èÂπï‰∏äÈù¢ÂÜôÁùÄ‚ÄúÈòøÈáåÂ∑¥Â∑¥‚Äù"
inputs = {
    "image": [image1],
    "prompt": prompt,
    "generator": torch.manual_seed(0),
    "true_cfg_scale": 4.0,
    "negative_prompt": " ",
    "num_inference_steps": 40,
    "guidance_scale": 1.0,
    "num_images_per_prompt": 1,
}
with torch.inference_mode():
    output = pipeline(**inputs)
    output_image = output.images[0]
    output_image.save("output_image_edit_2511.png")
    print("image saved at", os.path.abspath("output_image_edit_2511.png"))
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt; Previous Version &lt;/summary&gt; 
 &lt;h3&gt;Qwen-Image (for Text-to-Image)&lt;/h3&gt; 
 &lt;p&gt;The following contains a code snippet illustrating how to use the model to generate images based on text prompts:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from diffusers import DiffusionPipeline
import torch

model_name = "Qwen/Qwen-Image"

# Load the pipeline
if torch.cuda.is_available():
    torch_dtype = torch.bfloat16
    device = "cuda"
else:
    torch_dtype = torch.float32
    device = "cpu"

pipe = DiffusionPipeline.from_pretrained(model_name, torch_dtype=torch_dtype).to(device)

positive_magic = {
    "en": ", Ultra HD, 4K, cinematic composition.", # for english prompt
    "zh": ", Ë∂ÖÊ∏ÖÔºå4KÔºåÁîµÂΩ±Á∫ßÊûÑÂõæ." # for chinese prompt
}

# Generate image
prompt = '''A coffee shop entrance features a chalkboard sign reading "Qwen Coffee üòä $2 per cup," with a neon light beside it displaying "ÈÄö‰πâÂçÉÈóÆ". Next to it hangs a poster showing a beautiful Chinese woman, and beneath the poster is written "œÄ‚âà3.1415926-53589793-23846264-33832795-02384197".'''

negative_prompt = " " # Recommended if you don't use a negative prompt.


# Generate with different aspect ratios
aspect_ratios = {
    "1:1": (1328, 1328),
    "16:9": (1664, 928),
    "9:16": (928, 1664),
    "4:3": (1472, 1104),
    "3:4": (1104, 1472),
    "3:2": (1584, 1056),
    "2:3": (1056, 1584),
}

width, height = aspect_ratios["16:9"]

image = pipe(
    prompt=prompt + positive_magic["en"],
    negative_prompt=negative_prompt,
    width=width,
    height=height,
    num_inference_steps=50,
    true_cfg_scale=4.0,
    generator=torch.Generator(device="cuda").manual_seed(42)
).images[0]

image.save("example.png")
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;Qwen-Image-Edit (for Image Editing, Only Support Single Image Input)&lt;/h3&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;[!NOTE] Qwen-Image-Edit-2509 has better consistency than Qwen-Image-Edit; it is recommended to use Qwen-Image-Edit-2509 directlyÔºåfor both single image input and multiple image inputs.&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import os
from PIL import Image
import torch

from diffusers import QwenImageEditPipeline

pipeline = QwenImageEditPipeline.from_pretrained("Qwen/Qwen-Image-Edit")
print("pipeline loaded")
pipeline.to(torch.bfloat16)
pipeline.to("cuda")
pipeline.set_progress_bar_config(disable=None)

image = Image.open("./input.png").convert("RGB")
prompt = "Change the rabbit's color to purple, with a flash light background."


inputs = {
    "image": image,
    "prompt": prompt,
    "generator": torch.manual_seed(0),
    "true_cfg_scale": 4.0,
    "negative_prompt": " ",
    "num_inference_steps": 50,
}

with torch.inference_mode():
    output = pipeline(**inputs)
    output_image = output.images[0]
    output_image.save("output_image_edit.png")
    print("image saved at", os.path.abspath("output_image_edit.png"))
&lt;/code&gt;&lt;/pre&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;[!NOTE] We have observed that editing results may become unstable if prompt rewriting is not used. Therefore, we strongly recommend applying prompt rewriting to improve the stability of editing tasks. For reference, please see our official &lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen-Image/main/src/examples/tools/prompt_utils.py"&gt;demo script&lt;/a&gt; or Advanced Usage below, which includes example system prompts. Qwen-Image-Edit is actively evolving with ongoing development. Stay tuned for future enhancements!&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;h3&gt;Qwen-Image-Edit-2509 (for Image Editing, Multiple Image Support and Improved Consistency)&lt;/h3&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import os
import torch
from PIL import Image
from diffusers import QwenImageEditPlusPipeline
from io import BytesIO
import requests

pipeline = QwenImageEditPlusPipeline.from_pretrained("Qwen/Qwen-Image-Edit-2509", torch_dtype=torch.bfloat16)
print("pipeline loaded")

pipeline.to('cuda')
pipeline.set_progress_bar_config(disable=None)
image1 = Image.open(BytesIO(requests.get("https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/edit2509_1.jpg").content))
image2 = Image.open(BytesIO(requests.get("https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/edit2509_2.jpg").content))
prompt = "The magician bear is on the left, the alchemist bear is on the right, facing each other in the central park square."
inputs = {
    "image": [image1, image2],
    "prompt": prompt,
    "generator": torch.manual_seed(0),
    "true_cfg_scale": 4.0,
    "negative_prompt": " ",
    "num_inference_steps": 40,
    "guidance_scale": 1.0,
    "num_images_per_prompt": 1,
}
with torch.inference_mode():
    output = pipeline(**inputs)
    output_image = output.images[0]
    output_image.save("output_image_edit_plus.png")
    print("image saved at", os.path.abspath("output_image_edit_plus.png"))
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Advanced Usage&lt;/h3&gt; 
&lt;h4&gt;Prompt Enhance for Text-to-Image&lt;/h4&gt; 
&lt;p&gt;For enhanced prompt optimization and multi-language support, we recommend using our official Prompt Enhancement Tool powered by Qwen-Plus .&lt;/p&gt; 
&lt;p&gt;You can integrate it directly into your code:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from tools.prompt_utils import rewrite
prompt = rewrite(prompt)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, run the example script from the command line:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd src
DASHSCOPE_API_KEY=sk-xxxxxxxxxxxxxxxxxxxx python examples/generate_w_prompt_enhance.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Prompt Enhance for Image Edit&lt;/h4&gt; 
&lt;p&gt;For enhanced stability, we recommend using our official Prompt Enhancement Tool powered by Qwen-VL-Max.&lt;/p&gt; 
&lt;p&gt;You can integrate it directly into your code:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from tools.prompt_utils import polish_edit_prompt
prompt = polish_edit_prompt(prompt, pil_image)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Deploy Qwen-Image&lt;/h2&gt; 
&lt;p&gt;Qwen-Image supports Multi-GPU API Server for local deployment:&lt;/p&gt; 
&lt;h3&gt;Multi-GPU API Server Pipeline &amp;amp; Usage&lt;/h3&gt; 
&lt;p&gt;The Multi-GPU API Server will start a Gradio-based web interface with:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Multi-GPU parallel processing&lt;/li&gt; 
 &lt;li&gt;Queue management for high concurrency&lt;/li&gt; 
 &lt;li&gt;Automatic prompt optimization&lt;/li&gt; 
 &lt;li&gt;Support for multiple aspect ratios&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Configuration via environment variables:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export NUM_GPUS_TO_USE=4          # Number of GPUs to use
export TASK_QUEUE_SIZE=100        # Task queue size
export TASK_TIMEOUT=300           # Task timeout in seconds
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Start the gradio demo server, api key for prompt enhance
cd src
DASHSCOPE_API_KEY=sk-xxxxxxxxxxxxxxxxx python examples/demo.py 
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Showcase&lt;/h2&gt; 
&lt;p&gt;For previous showcases, click the following links:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen-Image/main/Qwen-Image.md"&gt;Qwen-Image&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen-Image/main/Qwen-Image-Edit.md"&gt;Qwen-Image-Edit&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen-Image/main/Qwen-Image-Edit-2509.md"&gt;Qwen-Image-Edit-2509&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Showcase of Qwen-Image-2512&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Enhanced Huamn Realism&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;In Qwen-Image-2512, human depiction has been substantially refined. Compared to the August release, Qwen-Image-2512 adds significantly richer facial details and better environmental context. For example:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;A Chinese female college student, around 20 years old, with a very short haircut that conveys a gentle, artistic vibe. Her hair naturally falls to partially cover her cheeks, projecting a tomboyish yet charming demeanor. She has cool-toned fair skin and delicate features, with a slightly shy yet subtly confident expression‚Äîher mouth crooked in a playful, youthful smirk. She wears an off-shoulder top, revealing one shoulder, with a well-proportioned figure. The image is framed as a close-up selfie: she dominates the foreground, while the background clearly shows her dormitory‚Äîa neatly made bed with white linens on the top bunk, a tidy study desk with organized stationery, and wooden cabinets and drawers. The photo is captured on a smartphone under soft, even ambient lighting, with natural tones, high clarity, and a bright, lively atmosphere full of youthful, everyday energy.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/%E5%B9%BB%E7%81%AF%E7%89%871.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;For the same prompt, Qwen-Image-2512 yields notably more lifelike facial features, and background objects‚Äîe.g., the desk, stationery, and bedding‚Äîare rendered with significantly greater clarity than in Qwen-Image.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;A 20-year-old East Asian girl with delicate, charming features and large, bright brown eyes‚Äîexpressive and lively, with a cheerful or subtly smiling expression. Her naturally wavy long hair is either loose or tied in twin ponytails. She has fair skin and light makeup accentuating her youthful freshness. She wears a modern, cute dress or relaxed outfit in bright, soft colors‚Äîlightweight fabric, minimalist cut. She stands indoors at an anime convention, surrounded by banners, posters, or stalls. Lighting is typical indoor illumination‚Äîno staged lighting‚Äîand the image resembles a casual iPhone snapshot: unpretentious composition, yet brimming with vivid, fresh, youthful charm.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/%E5%B9%BB%E7%81%AF%E7%89%872.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;Here, hair strands serve as a key differentiator: Qwen-Image‚Äôs August version tends to blur them together, losing fine detail, whereas Qwen-Image-2512 renders individual strands with precision, resulting in a more natural and realistic appearance.&lt;/p&gt; 
&lt;p&gt;Another case:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;An East Asian teenage boy, aged 15‚Äì18, with soft, fluffy black short hair and refined facial contours. His large, warm brown eyes sparkle with energy. His fair skin and sunny, open smile convey an approachable, friendly demeanor‚Äîno makeup or blemishes. He wears a blue-and-white summer uniform shirt, slightly unbuttoned, made of thin breathable fabric, with black headphones hanging around his neck. His hands are in his pockets, body leaning slightly forward in a relaxed pose, as if engaged in conversation. Behind him lies a summer school playground: lush green grass and a red rubber track in the foreground, blurred school buildings in the distance, a clear blue sky with fluffy white clouds. The bright, airy lighting evokes a joyful, carefree adolescent atmosphere.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/%E5%B9%BB%E7%81%AF%E7%89%873.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;In this example, Qwen-Image-2512 better adheres to semantic instructions‚Äîfor instance, the prompt specifies ‚Äúbody leaning slightly forward,‚Äù and Qwen-Image-2512 accurately captures this posture, unlike its predecessor.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;An elderly Chinese couple in their 70s in a clean, organized home kitchen. The woman has a kind face and a warm smile, wearing a patterned apron; the man stands behind her, also smiling, as they both gaze at a steaming pot of buns on the stove. The kitchen is bright and tidy, exuding warmth and harmony. The scene is captured with a wide-angle lens to fully show the subjects and their surroundings.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/%E5%B9%BB%E7%81%AF%E7%89%874.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;This comparison starkly highlights the gap between the August and December models. The original Qwen-Image struggles to accurately render aged facial features (e.g., wrinkles), resulting in an artificial ‚ÄúAI look.‚Äù In contrast, Qwen-Image-2512 precisely captures age cues, dramatically boosting realism.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Finer Natural Detail&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Qwen-Image-2512‚Äôs enhanced detail rendering extends beyond humans‚Äîto landscapes, wildlife, and more. For instance:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;A turquoise river winds through a lush canyon. Thick moss and dense ferns blanket the rocky walls; multiple waterfalls cascade from above, enveloped in mist. At noon, sunlight filters through the dense canopy, dappling the river surface with shimmering light. The atmosphere is humid and fresh, pulsing with primal jungle vitality. No humans, text, or artificial traces present.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/%E5%B9%BB%E7%81%AF%E7%89%875.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;Side-by-side, Qwen-Image-2512 exhibits superior fidelity in water flow, foliage, and waterfall mist‚Äîand renders richer gradation in greens. Another example (wave rendering):&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;At dawn, a thin mist veils the sea. An ancient stone lighthouse stands at the cliff‚Äôs edge, its beacon faintly visible through the fog. Black rocks are pounded by waves, sending up bursts of white spray. The sky glows in soft blue-purple hues under cool, hazy light‚Äîevoking solitude and solemn grandeur.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/%E5%B9%BB%E7%81%AF%E7%89%876.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;Fur detail is another highlight‚Äîhere, a golden retriever portrait:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;An ultra-realistic close-up of a golden retriever outdoors under soft daylight. Hair is exquisitely detailed: strands distinct, color transitioning naturally from warm gold to light cream, light glinting delicately at the tips; a gentle breeze adds subtle volume. Undercoat is soft and dense; guard hairs are long and well-defined, with visible layering. Eyes are moist, expressive; nose is slightly damp with fine specular highlights. Background is softly blurred to emphasize the dog‚Äôs tangible texture and vivid expression.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/%E5%B9%BB%E7%81%AF%E7%89%877.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;Similarly, texture quality improves in depictions of rugged wildlife‚Äîfor example, a male argali sheep:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;A male argali stands atop a barren, rocky mountainside. Its coarse, dense grey-brown coat covers a powerful, muscular body. Most striking are its massive, thick, outward-spiraling horns‚Äîa symbol of wild strength. Its gaze is alert and sharp. The background reveals steep alpine terrain: jagged peaks, sparse low vegetation, and abundant sunlight‚Äîconveying the harsh yet majestic wilderness and the animal‚Äôs resilient vitality.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/%E5%B9%BB%E7%81%AF%E7%89%878.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Improved Text Rendering&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Qwen-Image-2512 further elevates text rendering‚Äîalready a strength of the original‚Äîby improving accuracy, layout, and multimodal integration.&lt;/p&gt; 
&lt;p&gt;For instance, this prompt requests a complete PPT slide illustrating Qwen-Image‚Äôs development roadmap (generation and editing tracks):&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ËøôÊòØ‰∏ÄÂº†Áé∞‰ª£È£éÊ†ºÁöÑÁßëÊäÄÊÑüÂπªÁÅØÁâáÔºåÊï¥‰ΩìÈááÁî®Ê∑±ËìùËâ≤Ê∏êÂèòËÉåÊôØ„ÄÇÊ†áÈ¢òÊòØ‚ÄúQwen-ImageÂèëÂ±ïÂéÜÁ®ã‚Äù„ÄÇ‰∏ãÊñπ‰∏ÄÊù°Ê∞¥Âπ≥Âª∂‰º∏ÁöÑÂèëÂÖâÊó∂Èó¥ËΩ¥ÔºåËΩ¥Á∫ø‰∏≠Èó¥ÂÜôÁùÄ‚ÄúÁîüÂõæË∑ØÁ∫ø‚Äù„ÄÇÁî±Â∑¶‰æßÊ∑°ËìùËâ≤Ê∏êÂèò‰∏∫Âè≥‰æßÊ∑±Á¥´Ëâ≤ÔºåÂπ∂‰ª•Á≤æËá¥ÁöÑÁÆ≠Â§¥Êî∂Â∞æ„ÄÇÊó∂Èó¥ËΩ¥‰∏äÊØè‰∏™ËäÇÁÇπÈÄöËøáËôöÁ∫øËøûÊé•Ëá≥‰∏ãÊñπÈÜíÁõÆÁöÑËìùËâ≤ÂúÜËßíÁü©ÂΩ¢Êó•ÊúüÊ†áÁ≠æÔºåÊ†áÁ≠æÂÜÖ‰∏∫Ê∏ÖÊô∞ÁôΩËâ≤Â≠ó‰ΩìÔºå‰ªéÂ∑¶ÂêëÂè≥‰æùÊ¨°ÂÜôÁùÄÔºö‚Äú2025Âπ¥5Êúà6Êó• Qwen-Image È°πÁõÆÂêØÂä®‚Äù‚Äú2025Âπ¥8Êúà4Êó• Qwen-Image ÂºÄÊ∫êÂèëÂ∏É‚Äù‚Äú2025Âπ¥12Êúà31Êó• Qwen-Image-2512 ÂºÄÊ∫êÂèëÂ∏É‚Äù ÔºàÂë®Âõ¥ÂÖâÊôïÊòæËëóÔºâÂú®‰∏ãÊñπ‰∏ÄÊù°Ê∞¥Âπ≥Âª∂‰º∏ÁöÑÂèëÂÖâÊó∂Èó¥ËΩ¥ÔºåËΩ¥Á∫ø‰∏≠Èó¥ÂÜôÁùÄ‚ÄúÁºñËæëË∑ØÁ∫ø‚Äù„ÄÇÁî±Â∑¶‰æßÊ∑°ËìùËâ≤Ê∏êÂèò‰∏∫Âè≥‰æßÊ∑±Á¥´Ëâ≤ÔºåÂπ∂‰ª•Á≤æËá¥ÁöÑÁÆ≠Â§¥Êî∂Â∞æ„ÄÇÊó∂Èó¥ËΩ¥‰∏äÊØè‰∏™ËäÇÁÇπÈÄöËøáËôöÁ∫øËøûÊé•Ëá≥‰∏ãÊñπÈÜíÁõÆÁöÑËìùËâ≤ÂúÜËßíÁü©ÂΩ¢Êó•ÊúüÊ†áÁ≠æÔºåÊ†áÁ≠æÂÜÖ‰∏∫Ê∏ÖÊô∞ÁôΩËâ≤Â≠ó‰ΩìÔºå‰ªéÂ∑¶ÂêëÂè≥‰æùÊ¨°ÂÜôÁùÄÔºö‚Äú2025Âπ¥8Êúà18Êó• Qwen-Image-Edit ÂºÄÊ∫êÂèëÂ∏É‚Äù‚Äú2025Âπ¥9Êúà22Êó• Qwen-Image-Edit-2509 ÂºÄÊ∫êÂèëÂ∏É‚Äù‚Äú2025Âπ¥12Êúà19Êó• Qwen-Image-Layered ÂºÄÊ∫êÂèëÂ∏É‚Äù‚Äú2025Âπ¥12Êúà23Êó• Qwen-Image-Edit-2511 ÂºÄÊ∫êÂèëÂ∏É‚Äù&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/%E5%B9%BB%E7%81%AF%E7%89%879.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;We can even generate a before-and-after comparison slide to highlight the leap from ‚ÄúAI-blurry‚Äù to ‚Äúphotorealistic‚Äù:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ËøôÊòØ‰∏ÄÂº†Áé∞‰ª£È£éÊ†ºÁöÑÁßëÊäÄÊÑüÂπªÁÅØÁâáÔºåÊï¥‰ΩìÈááÁî®Ê∑±ËìùËâ≤Ê∏êÂèòËÉåÊôØ„ÄÇÈ°∂ÈÉ®‰∏≠Â§Æ‰∏∫ÁôΩËâ≤Êó†Ë°¨Á∫øÁ≤ó‰ΩìÂ§ßÂ≠óÊ†áÈ¢ò‚ÄúQwen-Image-2512ÈáçÁ£ÖÂèëÂ∏É‚Äù„ÄÇÁîªÈù¢‰∏ª‰Ωì‰∏∫Ê®™ÂêëÂØπÊØîÂõæÔºåËßÜËßâÁÑ¶ÁÇπÈõÜ‰∏≠‰∫é‰∏≠Èó¥ÁöÑÂçáÁ∫ßÂØπÊØîÂå∫Âüü„ÄÇÂ∑¶‰æß‰∏∫Èù¢ÈÉ®ÂÖâÊªëÊ≤°Êúâ‰ªª‰ΩïÁªÜËäÇÁöÑÂ•≥ÊÄß‰∫∫ÂÉèÔºåË¥®ÊÑüÂ∑ÆÔºõÂè≥‰æß‰∏∫È´òÂ∫¶ÂÜôÂÆûÁöÑÂπ¥ËΩªÂ•≥ÊÄßËÇñÂÉèÔºåÁöÆËÇ§ÂëàÁé∞ÁúüÂÆûÊØõÂ≠îÁ∫πÁêÜ‰∏éÁªÜÂæÆÂÖâÂΩ±ÂèòÂåñÔºåÂèë‰∏ùÊ†πÊ†πÂàÜÊòéÔºåÁúºÁú∏ÈÄè‰∫ÆÔºåË°®ÊÉÖËá™ÁÑ∂ÔºåÊï¥‰ΩìË¥®ÊÑüÊé•ËøëÂÜôÂÆûÊëÑÂΩ±„ÄÇ‰∏§ÂõæÂÉè‰πãÈó¥‰ª•‰∏Ä‰∏™ÁªøËâ≤ÊµÅÁ∫øÂûãÁÆ≠Â§¥ÈìæÊé•„ÄÇÈÄ†ÂûãÁßëÊäÄÊÑüÂçÅË∂≥Ôºå‰∏≠ÈÉ®Ê†áÊ≥®‚Äú2512Ë¥®ÊÑüÂçáÁ∫ß‚ÄùÔºå‰ΩøÁî®ÁôΩËâ≤Âä†Á≤óÂ≠ó‰ΩìÔºåÂ±Ö‰∏≠ÊòæÁ§∫„ÄÇÁÆ≠Â§¥‰∏§‰æßÊúâÂæÆÂº±ÂÖâÊôïÊïàÊûúÔºåÂ¢ûÂº∫Âä®ÊÄÅÊÑü„ÄÇÂú®ÂõæÂÉè‰∏ãÊñπÔºå‰ª•ÁôΩËâ≤ÊñáÂ≠óÂëàÁé∞‰∏âË°åËØ¥ÊòéÔºö‚Äú‚óè Êõ¥ÁúüÂÆûÁöÑ‰∫∫Áâ©Ë¥®ÊÑü„ÄÇÂ§ßÂπÖÂ∫¶Èôç‰Ωé‰∫ÜÁîüÊàêÂõæÁâáÁöÑAIÊÑüÔºåÊèêÂçá‰∫ÜÂõæÂÉèÁúüÂÆûÊÄß ‚óè Êõ¥ÁªÜËÖªÁöÑËá™ÁÑ∂Á∫πÁêÜ„ÄÇÂ§ßÂπÖÂ∫¶ÊèêÂçá‰∫ÜÁîüÊàêÂõæÁâáÁöÑÁ∫πÁêÜÁªÜËäÇ„ÄÇÈ£éÊôØÂõæÔºåÂä®Áâ©ÊØõÂèëÂàªÁîªÊõ¥ÁªÜËÖª„ÄÇ‚óè Êõ¥Â§çÊùÇÁöÑÊñáÂ≠óÊ∏≤Êüì„ÄÇÂ§ßÂπÖÊèêÂçá‰∫ÜÊñáÂ≠óÊ∏≤ÊüìÁöÑË¥®Èáè„ÄÇÂõæÊñáÊ∑∑ÂêàÊ∏≤ÊüìÊõ¥ÂáÜÁ°ÆÔºåÊéíÁâàÊõ¥Â•Ω‚Äù&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/%E5%B9%BB%E7%81%AF%E7%89%8710.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;A more complex infographic example:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ËøôÊòØ‰∏ÄÂπÖ‰∏ì‰∏öÁ∫ßÂ∑•‰∏öÊäÄÊúØ‰ø°ÊÅØÂõæË°®ÔºåÊï¥‰ΩìÈááÁî®Ê∑±ËìùËâ≤ÁßëÊäÄÊÑüËÉåÊôØÔºåÂÖâÁ∫øÂùáÂåÄÊüîÂíåÔºåËê•ÈÄ†Âá∫ÂÜ∑Èùô„ÄÅÁ≤æÂáÜÁöÑÁé∞‰ª£Â∑•‰∏öÊ∞õÂõ¥„ÄÇÁîªÈù¢ÂàÜ‰∏∫Â∑¶Âè≥‰∏§Â§ßÊùøÂùóÔºåÂ∏ÉÂ±ÄÊ∏ÖÊô∞ÔºåËßÜËßâÂ±ÇÊ¨°ÂàÜÊòé„ÄÇÂ∑¶‰æßÊùøÂùóÊ†áÈ¢ò‰∏∫‚ÄúÂÆûÈôÖÂèëÁîüÁöÑÁé∞Ë±°‚ÄùÔºå‰ª•ÊµÖËìùËâ≤ÂúÜËßíÁü©ÂΩ¢Ê°ÜÁ™ÅÂá∫ÊòæÁ§∫ÔºåÂÜÖÈÉ®ÊéíÂàó‰∏â‰∏™Ê∑±ËìùËâ≤ÊåâÈíÆÂºèÊù°ÁõÆÔºåÁ¨¨‰∏Ä‰∏™Êù°ÁõÆÂ±ïÁ§∫‰∏ÄÂ†ÜÊ£ïËâ≤Á≤âÊú´Áä∂ÂéüÊñô‰∏äÊª¥ËêΩÊ∞¥Êª¥ÁöÑÂõæÊ†áÔºåÊñáÂ≠ó‰∏∫‚ÄúÂõ¢ËÅö/ÁªìÂùó‚ÄùÔºåÂêéÈù¢ÈÖçÊúâÁªøËâ≤ÂØπÈí©ÔºõÁ¨¨‰∫å‰∏™Êù°ÁõÆ‰∏∫‰∏Ä‰∏™Ë£ÖÊúâËìùËâ≤Ê∂≤‰ΩìÂπ∂ÂÜíÂá∫Ê∞îÊ≥°ÁöÑÈî•ÂΩ¢Áì∂ÔºåÊñáÂ≠ó‰∏∫‚Äú‰∫ßÁîüÊ∞îÊ≥°/Áº∫Èô∑‚ÄùÔºåÂêéÈù¢ÈÖçÊúâÁªøËâ≤ÂØπÈí©ÔºõÁ¨¨‰∏â‰∏™Êù°ÁõÆ‰∏∫‰∏§‰∏™ÁîüÈîàÁöÑÈΩøËΩÆÔºåÊñáÂ≠ó‰∏∫‚ÄúËÆæÂ§áËÖêËöÄ/ÂÇ¨ÂåñÂâÇÂ§±Ê¥ª‚ÄùÔºåÂêéÈù¢ÈÖçÊúâÁªøËâ≤ÂØπÈí©„ÄÇÂè≥‰æßÊùøÂùóÊ†áÈ¢ò‰∏∫‚Äú„Äê‰∏ç‰ºö„ÄëÂèëÁîüÁöÑÁé∞Ë±°‚ÄùÔºå‰ΩøÁî®Á±≥ÈªÑËâ≤ÂúÜËßíÁü©ÂΩ¢Ê°ÜÂëàÁé∞ÔºåÂÜÖÈÉ®Âõõ‰∏™Êù°ÁõÆÂùáÁΩÆ‰∫éÊ∑±ÁÅ∞Ëâ≤ËÉåÊôØÊñπÊ°Ü‰∏≠„ÄÇÂõæÊ†áÂàÜÂà´‰∏∫Ôºö‰∏ÄÁªÑÁ≤æÂØÜÂïÆÂêàÁöÑÈáëÂ±ûÈΩøËΩÆÔºåÊñáÂ≠ó‰∏∫‚ÄúÂèçÂ∫îÊïàÁéá„ÄêÊòæËëóÊèêÈ´ò„Äë‚ÄùÔºå‰∏äÊñπË¶ÜÁõñÈÜíÁõÆÁöÑÁ∫¢Ëâ≤ÂèâÂè∑Ôºõ‰∏ÄÊçÜÊï¥ÈΩêÊéíÂàóÁöÑÈáëÂ±ûÁÆ°ÊùêÔºåÊñáÂ≠ó‰∏∫‚ÄúÊàêÂìÅÂÜÖÈÉ®„ÄêÁªùÂØπÊó†Ê∞îÊ≥°/Â≠îÈöô„Äë‚ÄùÔºå‰∏äÊñπË¶ÜÁõñÈÜíÁõÆÁöÑÁ∫¢Ëâ≤ÂèâÂè∑Ôºõ‰∏ÄÊù°ÂùöÂõ∫ÁöÑÈáëÂ±ûÈìæÊù°Ê≠£Âú®ÊâøÂèóÊãâÂäõÔºåÊñáÂ≠ó‰∏∫‚ÄúÊùêÊñôÂº∫Â∫¶‰∏éËÄê‰πÖÊÄß„ÄêÂæóÂà∞Â¢ûÂº∫„Äë‚ÄùÔºå‰∏äÊñπË¶ÜÁõñÈÜíÁõÆÁöÑÁ∫¢Ëâ≤ÂèâÂè∑Ôºõ‰∏ÄÂ†ÜËÖêËöÄÁöÑÊâ≥ÊâãÔºåÊñáÂ≠ó‰∏∫‚ÄúÂä†Â∑•ËøáÁ®ã„ÄêÈõ∂ËÖêËöÄ/Èõ∂ÂâØÂèçÂ∫îÈ£éÈô©„Äë‚ÄùÔºå‰∏äÊñπË¶ÜÁõñÈÜíÁõÆÁöÑÁ∫¢Ëâ≤ÂèâÂè∑„ÄÇÂ∫ïÈÉ®‰∏≠Â§ÆÊúâ‰∏ÄË°åÂ∞èÂ≠óÊ≥®ÈáäÔºö‚ÄúÊ≥®ÔºöÊ∞¥ÂàÜÁöÑÂ≠òÂú®ÈÄöÂ∏∏‰ºöÂØºËá¥Ë¥üÈù¢ÊàñÂπ≤Êâ∞ÊÄßÁöÑÁªìÊûúÔºåËÄåÈùûÁêÜÊÉ≥ÊàñÂ¢ûÂº∫ÁöÑÁä∂ÊÄÅ‚ÄùÔºåÂ≠ó‰Ωì‰∏∫ÁôΩËâ≤ÔºåÊ∏ÖÊô∞ÂèØËØª„ÄÇÊï¥‰ΩìÈ£éÊ†ºÁé∞‰ª£ÁÆÄÁ∫¶ÔºåÈÖçËâ≤ÂØπÊØîÂº∫ÁÉàÔºåÂõæÂΩ¢Á¨¶Âè∑ÂáÜÁ°Æ‰º†ËææÊäÄÊúØÈÄªËæëÔºåÈÄÇÂêàÁî®‰∫éÂ∑•‰∏öÂüπËÆ≠ÊàñÁßëÊôÆÊºîÁ§∫Âú∫ÊôØ„ÄÇ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/%E5%B9%BB%E7%81%AF%E7%89%8711.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;Or even a full educational poster:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ËøôÊòØ‰∏ÄÂπÖÁî±ÂçÅ‰∫å‰∏™ÂàÜÊ†ºÁªÑÊàêÁöÑ3√ó4ÁΩëÊ†ºÂ∏ÉÂ±ÄÁöÑÂÜôÂÆûÊëÑÂΩ±‰ΩúÂìÅÔºåÊï¥‰ΩìÂëàÁé∞‚ÄúÂÅ•Â∫∑ÁöÑ‰∏ÄÂ§©‚Äù‰∏ªÈ¢òÔºåÁîªÈù¢È£éÊ†ºÁÆÄÊ¥ÅÊ∏ÖÊô∞ÔºåÊØè‰∏ÄÂàÜÊ†ºÁã¨Á´ãÊàêÊôØÂèàÁªü‰∏Ä‰∫éÁîüÊ¥ªËäÇÂ•èÁöÑÂèô‰∫ãËÑâÁªú„ÄÇÁ¨¨‰∏ÄË°åÂàÜÂà´ÊòØ‚Äú06:00 Êô®Ë∑ëÂî§ÈÜíË∫´‰Ωì‚ÄùÔºöÈù¢ÈÉ®ÁâπÂÜôÔºå‰∏Ä‰ΩçÂ•≥ÊÄßË∫´Á©øÁÅ∞Ëâ≤ËøêÂä®Â•óË£ÖÔºåËÉåÊôØÊòØÂàùÂçáÁöÑÊúùÈò≥‰∏éËë±ÈÉÅÁªøÊ†ëÔºõ‚Äú06:30 Âä®ÊÄÅÊãâ‰º∏ÊøÄÊ¥ªÂÖ≥ËäÇ‚ÄùÔºöÂ•≥ÊÄßË∫´ÁùÄÁëú‰ºΩÊúçÂú®Èò≥Âè∞ÂÅöÊô®Èó¥Êãâ‰º∏ÔºåË∫´‰ΩìËàíÂ±ïÔºåËÉåÊôØ‰∏∫Ê∑°Á≤âËâ≤Â§©Á©∫‰∏éËøúÂ±±ËΩÆÂªìÔºõ‚Äú07:30 ÂùáË°°Ëê•ÂÖªÊó©È§ê‚ÄùÔºöÊ°å‰∏äÊëÜÊîæÂÖ®È∫¶Èù¢ÂåÖ„ÄÅÁâõÊ≤πÊûúÂíå‰∏ÄÊùØÊ©ôÊ±ÅÔºåÂ•≥ÊÄßÂæÆÁ¨ëÁùÄÂáÜÂ§áÁî®È§êÔºõ‚Äú08:00 Ë°•Ê∞¥Ê∂¶Áá•‚ÄùÔºöÈÄèÊòéÁéªÁíÉÊ∞¥ÊùØ‰∏≠ÊµÆÊúâÊü†Ê™¨ÁâáÔºåÂ•≥ÊÄßÊâãÊåÅÊ∞¥ÊùØËΩªÂïúÔºåÈò≥ÂÖâ‰ªéÂ∑¶‰æßÊñúÁÖßÂÖ•ÂÆ§ÔºåÊùØÂ£ÅÊ∞¥Áè†ÊªëËêΩÔºõÁ¨¨‰∫åË°åÂàÜÂà´ÊòØÔºö‚Äú09:00 ‰∏ìÊ≥®È´òÊïàÂ∑•‰Ωú‚ÄùÔºöÂ•≥ÊÄß‰∏ìÊ≥®Êï≤ÂáªÈîÆÁõòÔºåÂ±èÂπïÊòæÁ§∫ÁÆÄÊ¥ÅÁïåÈù¢ÔºåË∫´ÊóÅÊîæÊúâ‰∏ÄÊùØÂíñÂï°‰∏é‰∏ÄÁõÜÁªøÊ§çÔºõ‚Äú12:00 ÈùôÂøÉÈòÖËØªÊó∂ÂÖâ‚ÄùÔºöÂ•≥ÊÄßÂùêÂú®‰π¶Ê°åÂâçÁøªÈòÖÁ∫∏Ë¥®‰π¶Á±çÔºåÂè∞ÁÅØÊï£ÂèëÊöñÂÖâÔºå‰π¶È°µÊ≥õÈªÑÔºåÊóÅÊîæÂçäÊùØÁ∫¢Ëå∂Ôºõ‚Äú12:30 ÂçàÂêéËΩªÊùæÊº´Ê≠•‚ÄùÔºöÂ•≥ÊÄßÂú®ÊûóËç´ÈÅì‰∏äÊº´Ê≠•ÔºåËÑ∏ÈÉ®ÁâπÂÜôÔºõ‚Äú15:00 Ëå∂È¶ô‰º¥ÂçàÂêé‚ÄùÔºöÂ•≥ÊÄßÁ´ØÁùÄÈ™®Áì∑Ëå∂ÊùØÁ´ôÂú®Á™óËæπÔºåÁ™óÂ§ñÊòØÂüéÂ∏ÇË°óÊôØ‰∏éÈ£òÂä®‰∫ëÊúµÔºåËå∂È¶ôË¢ÖË¢ÖÔºõÁ¨¨‰∏âË°åÂàÜÂà´ÊòØÔºö‚Äú18:00 ËøêÂä®ÈáäÊîæÂéãÂäõ‚ÄùÔºöÂÅ•Ë∫´ÊàøÂÜÖÔºåÂ•≥ÊÄßÊ≠£Âú®ÁªÉ‰π†Áëú‰ºΩÔºõ‚Äú19:00 ÁæéÂë≥ÊôöÈ§ê‚ÄùÔºöÂ•≥ÊÄßÂú®ÂºÄÊîæÂºèÂé®Êàø‰∏≠ÂàáËèúÔºåÁ†ßÊùø‰∏äÊúâÁï™ËåÑ‰∏éÈùíÊ§íÔºåÈîÖ‰∏≠ÁÉ≠Ê∞îÂçáËÖæÔºåÁÅØÂÖâÊ∏©ÊöñÔºõ‚Äú21:00 ÂÜ•ÊÉ≥Âä©Áú†‚ÄùÔºöÂ•≥ÊÄßÁõòËÖøÂùêÂú®ÊüîËΩØÂú∞ÊØØ‰∏äÂÜ•ÊÉ≥ÔºåÂèåÊâãËΩªÊîæËÜù‰∏äÔºåÈó≠ÁõÆÂÆÅÈùôÔºõ‚Äú21:30 ËøõÂÖ•Áù°Áú†‚ÄùÔºöÂ•≥ÊÄßË∫∫Âú®Â∫ä‰∏ä‰ºëÊÅØ„ÄÇÊï¥‰ΩìÈááÁî®Ëá™ÁÑ∂ÂÖâÁ∫ø‰∏∫‰∏ªÔºåËâ≤Ë∞É‰ª•ÊöñÁôΩ‰∏éÁ±≥ÁÅ∞‰∏∫Âü∫Ë∞ÉÔºåÂÖâÂΩ±Â±ÇÊ¨°ÂàÜÊòéÔºåÁîªÈù¢ÂÖÖÊª°Ê∏©È¶®ÁöÑÁîüÊ¥ªÊ∞îÊÅØ‰∏éËßÑÂæãÁöÑËäÇÂ•èÊÑü„ÄÇ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/%E5%B9%BB%E7%81%AF%E7%89%8712.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;These are the core enhancements in this update. We hope you enjoy using Qwen-Image-2512!&lt;/p&gt; 
&lt;h3&gt;Showcase of Qwen-Image-Edit-2511&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Qwen-Image-Edit-2511 Enhances Character Consistency&lt;/strong&gt; In Qwen-Image-Edit-2511, character consistency has been significantly improved. The model can perform imaginative edits based on an input portrait while preserving the identity and visual characteristics of the subject.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%871.JPG#center" alt="" /&gt; &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%872.JPG#center" alt="" /&gt; &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%873.JPG#center" alt="" /&gt; &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%874.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Improved Multi-Person Consistency&lt;/strong&gt; While Qwen-Image-Edit-2509 already improved consistency for single-subject editing, Qwen-Image-Edit-2511 further enhances consistency in multi-person group photos‚Äîenabling high-fidelity fusion of two separate person images into a coherent group shot: &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%875.JPG#center" alt="" /&gt; &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%876.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Built-in Support for Community-Created LoRAs&lt;/strong&gt; Since Qwen-Image-Edit‚Äôs release, the community has developed many creative and high-quality LoRAs‚Äîgreatly expanding its expressive potential. Qwen-Image-Edit-2511 integrates selected popular LoRAs directly into the base model, unlocking their effects without extra tuning.&lt;/p&gt; 
&lt;p&gt;For example, Lighting Enhancement LoRA Realistic lighting control is now achievable out-of-the-box: &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%877.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%878.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;Another example, generating new viewpoints can now be done directly with the base model:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%879.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%8710.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Industrial Design Applications&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;We‚Äôve paid special attention to practical engineering scenarios‚Äîfor instance, batch industrial product design:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%8711.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%8712.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;‚Ä¶and material replacement for industrial components: &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%8713.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%8714.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Enhanced Geometric Reasoning&lt;/strong&gt; Qwen-Image-Edit-2511 introduces stronger geometric reasoning capability‚Äîe.g., directly generating auxiliary construction lines for design or annotation purposes:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%8715.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%8716.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;h2&gt;AI Arena&lt;/h2&gt; 
&lt;p&gt;To comprehensively evaluate the general image generation capabilities of Qwen-Image and objectively compare it with state-of-the-art closed-source APIs, we introduce &lt;a href="https://aiarena.alibaba-inc.com"&gt;AI Arena&lt;/a&gt;, an open benchmarking platform built on the Elo rating system. AI Arena provides a fair, transparent, and dynamic environment for model evaluation.&lt;/p&gt; 
&lt;p&gt;In each round, two images‚Äîgenerated by randomly selected models from the same prompt‚Äîare anonymously presented to users for pairwise comparison. Users vote for the better image, and the results are used to update both personal and global leaderboards via the Elo algorithm, enabling developers, researchers, and the public to assess model performance in a robust and data-driven way. AI Arena is now publicly available, welcoming everyone to participate in model evaluations.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/figure_aiarena_website.png" alt="AI Arena" /&gt;&lt;/p&gt; 
&lt;p&gt;The latest leaderboard rankings can be viewed at &lt;a href="https://aiarena.alibaba-inc.com/corpora/arena/leaderboard?arenaType=text2image"&gt;AI Arena Learboard&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you wish to deploy your model on AI Arena and participate in the evaluation, please contact &lt;a href="mailto:weiyue.wy@alibaba-inc.com"&gt;weiyue.wy@alibaba-inc.com&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Community Support&lt;/h2&gt; 
&lt;h3&gt;Huggingface&lt;/h3&gt; 
&lt;p&gt;Diffusers has supported Qwen-Image since day 0. Support for LoRA and finetuning workflows is currently in development and will be available soon.&lt;/p&gt; 
&lt;h3&gt;ModelScope&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/modelscope/DiffSynth-Studio"&gt;DiffSynth-Studio&lt;/a&gt;&lt;/strong&gt; provides comprehensive support for Qwen-Image, including low-GPU-memory layer-by-layer offload (inference within 4GB VRAM), FP8 quantization, LoRA / full training.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/modelscope/DiffSynth-Engine"&gt;DiffSynth-Engine&lt;/a&gt;&lt;/strong&gt; delivers advanced optimizations for Qwen-Image inference and deployment, including FBCache-based acceleration, classifier-free guidance (CFG) parallel, and more.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://www.modelscope.cn/aigc"&gt;ModelScope AIGC Central&lt;/a&gt;&lt;/strong&gt; provides hands-on experiences on Qwen Image, including: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://www.modelscope.cn/aigc/imageGeneration"&gt;Image Generation&lt;/a&gt;: Generate high fidelity images using the Qwen Image model.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.modelscope.cn/aigc/modelTraining"&gt;LoRA Training&lt;/a&gt;: Easily train Qwen Image LoRAs for personalized concepts.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;SGLang&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;SGLang-Diffusion&lt;/strong&gt; provides day-0 support for Qwen-Image models. To play with &lt;code&gt;Qwen-Image-Edit-2511&lt;/code&gt;, use the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;sglang generate --model-path Qwen/Qwen-Image-Edit-2511 --prompt "make the girl in Figure 1 dance with the capybara in Figure 2."  --image-path "https://github.com/lm-sys/lm-sys.github.io/releases/download/test/TI2I_Qwen_Image_Edit_Input.jpg" "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/edit2509_2.jpg"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The output should be like &lt;img src="https://github.com/lm-sys/lm-sys.github.io/releases/download/test/SGLang_Diffusion_Qwen_Image_Edit_2511_example_output.jpg" alt="" /&gt;&lt;/p&gt; 
&lt;h3&gt;WaveSpeedAI&lt;/h3&gt; 
&lt;p&gt;WaveSpeed has deployed Qwen-Image on their platform from day 0, visit their &lt;a href="https://wavespeed.ai/models/wavespeed-ai/qwen-image/text-to-image"&gt;model page&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h3&gt;LiblibAI&lt;/h3&gt; 
&lt;p&gt;LiblibAI offers native support for Qwen-Image from day 0. Visit their &lt;a href="https://www.liblib.art/modelinfo/c62a103bd98a4246a2334e2d952f7b21?from=sd&amp;amp;versionUuid=75e0be0c93b34dd8baeec9c968013e0c"&gt;community&lt;/a&gt; page for more details and discussions.&lt;/p&gt; 
&lt;h3&gt;Inference Acceleration Method: cache-dit&lt;/h3&gt; 
&lt;p&gt;cache-dit offers cache acceleration support for Qwen-Image with DBCache, TaylorSeer and Cache CFG. Visit their &lt;a href="https://github.com/vipshop/cache-dit/raw/main/examples/pipeline/run_qwen_image.py"&gt;example&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;License Agreement&lt;/h2&gt; 
&lt;p&gt;Qwen-Image is licensed under Apache 2.0.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;We kindly encourage citation of our work if you find it useful.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{wu2025qwenimagetechnicalreport,
      title={Qwen-Image Technical Report}, 
      author={Chenfei Wu and Jiahao Li and Jingren Zhou and Junyang Lin and Kaiyuan Gao and Kun Yan and Sheng-ming Yin and Shuai Bai and Xiao Xu and Yilei Chen and Yuxiang Chen and Zecheng Tang and Zekai Zhang and Zhengyi Wang and An Yang and Bowen Yu and Chen Cheng and Dayiheng Liu and Deqing Li and Hang Zhang and Hao Meng and Hu Wei and Jingyuan Ni and Kai Chen and Kuan Cao and Liang Peng and Lin Qu and Minggang Wu and Peng Wang and Shuting Yu and Tingkun Wen and Wensen Feng and Xiaoxiao Xu and Yi Wang and Yichang Zhang and Yongqiang Zhu and Yujia Wu and Yuxuan Cai and Zenan Liu},
      year={2025},
      eprint={2508.02324},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2508.02324}, 
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contact and Join Us&lt;/h2&gt; 
&lt;p&gt;If you'd like to get in touch with our research team, we'd love to hear from you! Join our &lt;a href="https://discord.gg/z3GAxXZ9Ce"&gt;Discord&lt;/a&gt; or scan the QR code to connect via our &lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen-Image/main/assets/wechat.png"&gt;WeChat groups&lt;/a&gt; ‚Äî we're always open to discussion and collaboration.&lt;/p&gt; 
&lt;p&gt;If you have questions about this repository, feedback to share, or want to contribute directly, we welcome your issues and pull requests on GitHub. Your contributions help make Qwen-Image better for everyone.&lt;/p&gt; 
&lt;p&gt;If you're passionate about fundamental research, we're hiring full-time employees (FTEs) and research interns. Don't wait ‚Äî reach out to us at &lt;a href="mailto:fulai.hr@alibaba-inc.com"&gt;fulai.hr@alibaba-inc.com&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#QwenLM/Qwen-Image&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=QwenLM/Qwen-Image&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Lightricks/LTX-Video</title>
      <link>https://github.com/Lightricks/LTX-Video</link>
      <description>&lt;p&gt;Official repository for LTX-Video&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h1&gt;LTX-Video&lt;/h1&gt; 
 &lt;p&gt;&lt;a href="https://ltx.video"&gt;&lt;img src="https://img.shields.io/badge/Website-LTXV-181717?logo=google-chrome" alt="Website" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/Lightricks/LTX-Video"&gt;&lt;img src="https://img.shields.io/badge/HuggingFace-Model-orange?logo=huggingface" alt="Model" /&gt;&lt;/a&gt; &lt;a href="https://app.ltx.studio/ltx-2-playground/t2v"&gt;&lt;img src="https://img.shields.io/badge/Demo-Try%20Now-brightgreen?logo=vercel" alt="Demo" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2501.00103"&gt;&lt;img src="https://img.shields.io/badge/Paper-arXiv-B31B1B?logo=arxiv" alt="Paper" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Lightricks/LTX-Video-Trainer"&gt;&lt;img src="https://img.shields.io/badge/LTXV-Trainer-9146FF?logo=github" alt="Trainer" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/ltxplatform"&gt;&lt;img src="https://img.shields.io/badge/Join-Discord-5865F2?logo=discord" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;This is the official repository for LTX-Video.&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üöÄ &lt;strong&gt;New: LTX-2 is Now Available!&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;We're excited to announce &lt;a href="https://github.com/Lightricks/LTX-2"&gt;LTX-2&lt;/a&gt; - the next generation of LTX with synchronized audio+video generation!&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;LTX-2 is the first DiT-based audio-video foundation model that contains all core capabilities of modern video generation in one model. &lt;strong&gt;LTX-2 is now the primary home for LTX development&lt;/strong&gt; and includes significant improvements:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üéµ &lt;strong&gt;Synchronized Audio+Video Generation&lt;/strong&gt; - Generate videos with perfectly synchronized audio&lt;/li&gt; 
 &lt;li&gt;üé¨ &lt;strong&gt;Latest Model&lt;/strong&gt; - LTX-2 with improved quality and capabilities&lt;/li&gt; 
 &lt;li&gt;üîå &lt;strong&gt;ComfyUI Integration&lt;/strong&gt; - Built into ComfyUI core for seamless workflows&lt;/li&gt; 
 &lt;li&gt;üéØ &lt;strong&gt;Advanced Features:&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Multiple keyframe support&lt;/li&gt; 
   &lt;li&gt;IC-LoRA control models for precise generation&lt;/li&gt; 
   &lt;li&gt;Standard LoRA support for style customization&lt;/li&gt; 
   &lt;li&gt;Latent upsampler for multiscale pipelines&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;üõ†Ô∏è &lt;strong&gt;Training Tools&lt;/strong&gt; - LoRA training capabilities&lt;/li&gt; 
 &lt;li&gt;üìö &lt;strong&gt;Comprehensive Documentation&lt;/strong&gt; - Full documentation at &lt;a href="https://docs.ltx.video"&gt;https://docs.ltx.video&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üîÑ &lt;strong&gt;Active Development&lt;/strong&gt; - Ongoing improvements and community support&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/Lightricks/LTX-2"&gt;üëâ Check out LTX-2 here&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://docs.ltx.video"&gt;üìñ View Documentation&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#news"&gt;What's New&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#models"&gt;Models&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#quick-start-guide"&gt;Quick Start Guide&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#online-inference"&gt;Online demo&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#run-locally"&gt;Run locally&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#inference"&gt;Inference&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#comfyui-integration"&gt;ComfyUI Integration&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#diffusers-integration"&gt;Diffusers Integration&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#model-user-guide"&gt;Model User Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#community-contribution"&gt;Community Contribution&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#training"&gt;Training&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#control-models"&gt;Control Models&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#join-us"&gt;Join Us!&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#acknowledgement"&gt;Acknowledgement&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Introduction&lt;/h1&gt; 
&lt;p&gt;LTX-Video is the first DiT-based video generation model that contains all core capabilities of modern video generation in one model: synchronized audio and video, high fidelity, multiple performance modes, production-ready outputs, API access, and open access. It can generate up to 50 FPS videos at native 4K resolution with synchronized audio in one pass. The model is trained on a large-scale dataset of diverse videos and can generate high-resolution videos with realistic and diverse content.&lt;/p&gt; 
&lt;p&gt;The model supports image-to-video, multi-keyframe conditioning, keyframe-based animation, video extension (both forward and backward), video-to-video transformations, and any combination of these features.&lt;/p&gt; 
&lt;h3&gt;Image-to-video examples&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00001.gif" alt="example1" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00002.gif" alt="example2" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00003.gif" alt="example3" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00004.gif" alt="example4" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00005.gif" alt="example5" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00006.gif" alt="example6" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00007.gif" alt="example7" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00008.gif" alt="example8" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00009.gif" alt="example9" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Controlled video examples&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_ic_2v_example_00000.gif" alt="control0" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_ic_2v_example_00001.gif" alt="control1" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_ic_2v_example_00002.gif" alt="control2" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_ic_2v_example_00003.gif" alt="control3" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_ic_2v_example_00004.gif" alt="control4" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;News&lt;/h1&gt; 
&lt;h2&gt;October 23, 2025: LTX-2 Announced&lt;/h2&gt; 
&lt;p&gt;Today we announced our newest foundation model, LTX-2. LTX-2 represents a major leap forward from our previous model, LTXV 0.9.8. Here‚Äôs what‚Äôs new:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Audio + Video, Together&lt;/strong&gt;: Visuals and sound are generated in one coherent process, with motion, dialogue, ambience, and music flowing simultaneously.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;4K Fidelity&lt;/strong&gt;: Professional-grade precision with native 4K and up to 50 fps, sharp textures, clean motion, and synchronized audio.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Longer Generations&lt;/strong&gt;: LTX-2 supports longer, continuous clips with synchronized audio up to 10 seconds.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Low Cost &amp;amp; Efficiency&lt;/strong&gt;: Up to 50% lower compute cost than competing models, powered by a multi-GPU inference stack.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Creative Control&lt;/strong&gt;: Multi-keyframe conditioning, 3D camera logic, and LoRA fine-tuning deliver frame-level precision and style consistency.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For more details, please see our &lt;a href="https://website.ltx.video/blog/introducing-ltx-2"&gt;blog post&lt;/a&gt;. LTX-2 model weights, code, and benchmarks will be released to the community later in 2025.&lt;/p&gt; 
&lt;h2&gt;July, 16th, 2025: New Distilled models v0.9.8 with up to 60 seconds of video:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Long shot generation in LTXV-13B! 
  &lt;ul&gt; 
   &lt;li&gt;LTX-Video now supports up to 60 seconds of video.&lt;/li&gt; 
   &lt;li&gt;Compatible also with the official IC-LoRAs.&lt;/li&gt; 
   &lt;li&gt;Try now in &lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/tree/master/example_workflows/ltxv-13b-i2v-long-multi-prompt.json"&gt;ComfyUI&lt;/a&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Release a new distilled models: 
  &lt;ul&gt; 
   &lt;li&gt;13B distilled model &lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-13b-0.9.8-distilled.yaml"&gt;ltxv-13b-0.9.8-distilled&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;2B distilled model &lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-2b-0.9.8-distilled.yaml"&gt;ltxv-2b-0.9.8-distilled&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Both models are distilled from the same base model &lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-13b-0.9.8-dev.yaml"&gt;ltxv-13b-0.9.8-dev&lt;/a&gt; and are compatible for use together in the same multiscale pipeline.&lt;/li&gt; 
   &lt;li&gt;Improved prompt understanding and detail generation&lt;/li&gt; 
   &lt;li&gt;Includes corresponding FP8 weights and workflows.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Release a new detailer model &lt;a href="https://huggingface.co/Lightricks/LTX-Video-ICLoRA-detailer-13b-0.9.8"&gt;LTX-Video-ICLoRA-detailer-13B-0.9.8&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Available in &lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/tree/master/example_workflows/ltxv-13b-upscale.json"&gt;ComfyUI&lt;/a&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;July, 8th, 2025: New Control Models Released!&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Released three new control models for LTX-Video on HuggingFace: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Depth Control&lt;/strong&gt;: &lt;a href="https://huggingface.co/Lightricks/LTX-Video-ICLoRA-depth-13b-0.9.7"&gt;LTX-Video-ICLoRA-depth-13b-0.9.7&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Pose Control&lt;/strong&gt;: &lt;a href="https://huggingface.co/Lightricks/LTX-Video-ICLoRA-pose-13b-0.9.7"&gt;LTX-Video-ICLoRA-pose-13b-0.9.7&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Canny Control&lt;/strong&gt;: &lt;a href="https://huggingface.co/Lightricks/LTX-Video-ICLoRA-canny-13b-0.9.7"&gt;LTX-Video-ICLoRA-canny-13b-0.9.7&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;May, 14th, 2025: New distilled model 13B v0.9.7:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Release a new 13B distilled model &lt;a href="https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-distilled.safetensors"&gt;ltxv-13b-0.9.7-distilled&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Amazing for iterative work - generates HD videos in 10 seconds, with low-res preview after just 3 seconds (on H100)!&lt;/li&gt; 
   &lt;li&gt;Does not require classifier-free guidance and spatio-temporal guidance.&lt;/li&gt; 
   &lt;li&gt;Supports sampling with 8 (recommended), or less diffusion steps.&lt;/li&gt; 
   &lt;li&gt;Also released a LoRA version of the distilled model, &lt;a href="https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-distilled-lora128.safetensors"&gt;ltxv-13b-0.9.7-distilled-lora128&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Requires only 1GB of VRAM&lt;/li&gt; 
     &lt;li&gt;Can be used with the full 13B model for fast inference&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Release a new quantized distilled model &lt;a href="https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-distilled-fp8.safetensors"&gt;ltxv-13b-0.9.7-distilled-fp8&lt;/a&gt; for &lt;em&gt;real-time&lt;/em&gt; generation (on H100) with even less VRAM&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;May, 5th, 2025: New model 13B v0.9.7:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Release a new 13B model &lt;a href="https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-dev.safetensors"&gt;ltxv-13b-0.9.7-dev&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Release a new quantized model &lt;a href="https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-dev-fp8.safetensors"&gt;ltxv-13b-0.9.7-dev-fp8&lt;/a&gt; for faster inference with less VRam&lt;/li&gt; 
 &lt;li&gt;Release a new upscalers 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-temporal-upscaler-0.9.7.safetensors"&gt;ltxv-temporal-upscaler-0.9.7&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-spatial-upscaler-0.9.7.safetensors"&gt;ltxv-spatial-upscaler-0.9.7&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Breakthrough prompt adherence and physical understanding.&lt;/li&gt; 
 &lt;li&gt;New Pipeline for multi-scale video rendering for fast and high quality results&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;April, 15th, 2025: New checkpoints v0.9.6:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Release a new checkpoint &lt;a href="https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-2b-0.9.6-dev-04-25.safetensors"&gt;ltxv-2b-0.9.6-dev-04-25&lt;/a&gt; with improved quality&lt;/li&gt; 
 &lt;li&gt;Release a new distilled model &lt;a href="https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-2b-0.9.6-distilled-04-25.safetensors"&gt;ltxv-2b-0.9.6-distilled-04-25&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;15x faster inference than non-distilled model.&lt;/li&gt; 
   &lt;li&gt;Does not require classifier-free guidance and spatio-temporal guidance.&lt;/li&gt; 
   &lt;li&gt;Supports sampling with 8 (recommended), or less diffusion steps.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Improved prompt adherence, motion quality and fine details.&lt;/li&gt; 
 &lt;li&gt;New default resolution and FPS: 1216 √ó 704 pixels at 30 FPS 
  &lt;ul&gt; 
   &lt;li&gt;Still real time on H100 with the distilled model.&lt;/li&gt; 
   &lt;li&gt;Other resolutions and FPS are still supported.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Support stochastic inference (can improve visual quality when using the distilled model)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;March, 5th, 2025: New checkpoint v0.9.5&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;New license for commercial use (&lt;a href="https://huggingface.co/Lightricks/LTX-Video/ltx-video-2b-v0.9.5.license.txt"&gt;OpenRail-M&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;Release a new checkpoint v0.9.5 with improved quality&lt;/li&gt; 
 &lt;li&gt;Support keyframes and video extension&lt;/li&gt; 
 &lt;li&gt;Support higher resolutions&lt;/li&gt; 
 &lt;li&gt;Improved prompt understanding&lt;/li&gt; 
 &lt;li&gt;Improved VAE&lt;/li&gt; 
 &lt;li&gt;New online web app in &lt;a href="https://app.ltx.studio/ltx-video"&gt;LTX-Studio&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Automatic prompt enhancement&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;February, 20th, 2025: More inference options&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Improve STG (Spatiotemporal Guidance) for LTX-Video&lt;/li&gt; 
 &lt;li&gt;Support MPS on macOS with PyTorch 2.3.0&lt;/li&gt; 
 &lt;li&gt;Add support for 8-bit model, LTX-VideoQ8&lt;/li&gt; 
 &lt;li&gt;Add TeaCache for LTX-Video&lt;/li&gt; 
 &lt;li&gt;Add &lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#comfyui-integration"&gt;ComfyUI-LTXTricks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Add Diffusion-Pipe&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;December 31st, 2024: Research paper&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Release the &lt;a href="https://arxiv.org/abs/2501.00103"&gt;research paper&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;December 20th, 2024: New checkpoint v0.9.1&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Release a new checkpoint v0.9.1 with improved quality&lt;/li&gt; 
 &lt;li&gt;Support for STG / PAG&lt;/li&gt; 
 &lt;li&gt;Support loading checkpoints of LTX-Video in Diffusers format (conversion is done on-the-fly)&lt;/li&gt; 
 &lt;li&gt;Support offloading unused parts to CPU&lt;/li&gt; 
 &lt;li&gt;Support the new timestep-conditioned VAE decoder&lt;/li&gt; 
 &lt;li&gt;Reference contributions from the community in the readme file&lt;/li&gt; 
 &lt;li&gt;Relax transformers dependency&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;November 21th, 2024: Initial release v0.9.0&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Initial release of LTX-Video&lt;/li&gt; 
 &lt;li&gt;Support text-to-video and image-to-video generation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Models&lt;/h1&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Name&lt;/th&gt; 
   &lt;th&gt;Notes&lt;/th&gt; 
   &lt;th&gt;inference.py config&lt;/th&gt; 
   &lt;th&gt;ComfyUI workflow (Recommended)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ltxv-13b-0.9.8-dev&lt;/td&gt; 
   &lt;td&gt;Highest quality, requires more VRAM&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-13b-0.9.8-dev.yaml"&gt;ltxv-13b-0.9.8-dev.yaml&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/ltxv-13b-i2v-base.json"&gt;ltxv-13b-i2v-base.json&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://app.ltx.studio/motion-workspace?videoModel=ltxv-13b"&gt;ltxv-13b-0.9.8-mix&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Mix ltxv-13b-dev and ltxv-13b-distilled in the same multi-scale rendering workflow for balanced speed-quality&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/ltxv-13b-i2v-mixed-multiscale.json"&gt;ltxv-13b-i2v-mixed-multiscale.json&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://app.ltx.studio/motion-workspace?videoModel=ltxv"&gt;ltxv-13b-0.9.8-distilled&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Faster, less VRAM usage, slight quality reduction compared to 13b. Ideal for rapid iterations&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-13b-0.9.8-distilled.yaml"&gt;ltxv-13b-0.9.8-distilled.yaml&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/13b-distilled/ltxv-13b-dist-i2v-base.json"&gt;ltxv-13b-dist-i2v-base.json&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ltxv-2b-0.9.8-distilled&lt;/td&gt; 
   &lt;td&gt;Smaller model, slight quality reduction compared to 13b distilled. Ideal for fast generation with light VRAM usage&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-2b-0.9.8-distilled.yaml"&gt;ltxv-2b-0.9.8-distilled.yaml&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ltxv-13b-0.9.8-dev-fp8&lt;/td&gt; 
   &lt;td&gt;Quantized version of ltxv-13b&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-13b-0.9.8-dev-fp8.yaml"&gt;ltxv-13b-0.9.8-dev-fp8.yaml&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/ltxv-13b-i2v-base-fp8.json"&gt;ltxv-13b-i2v-base-fp8.json&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ltxv-13b-0.9.8-distilled-fp8&lt;/td&gt; 
   &lt;td&gt;Quantized version of ltxv-13b-distilled&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-13b-0.9.8-distilled-fp8.yaml"&gt;ltxv-13b-0.9.8-distilled-fp8.yaml&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/13b-distilled/ltxv-13b-dist-i2v-base-fp8.json"&gt;ltxv-13b-dist-i2v-base-fp8.json&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ltxv-2b-0.9.8-distilled-fp8&lt;/td&gt; 
   &lt;td&gt;Quantized version of ltxv-2b-distilled&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-2b-0.9.8-distilled-fp8.yaml"&gt;ltxv-2b-0.9.8-distilled-fp8.yaml&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ltxv-2b-0.9.6&lt;/td&gt; 
   &lt;td&gt;Good quality, lower VRAM requirement than ltxv-13b&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-2b-0.9.6-dev.yaml"&gt;ltxv-2b-0.9.6-dev.yaml&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/low_level/ltxvideo-i2v.json"&gt;ltxvideo-i2v.json&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ltxv-2b-0.9.6-distilled&lt;/td&gt; 
   &lt;td&gt;15√ó faster, real-time capable, fewer steps needed, no STG/CFG required&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-2b-0.9.6-distilled.yaml"&gt;ltxv-2b-0.9.6-distilled.yaml&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/low_level/ltxvideo-i2v-distilled.json"&gt;ltxvideo-i2v-distilled.json&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;Quick Start Guide&lt;/h1&gt; 
&lt;h2&gt;Online inference&lt;/h2&gt; 
&lt;p&gt;The model is accessible right away via the following links:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://app.ltx.studio/motion-workspace?videoModel=ltxv-13b"&gt;LTX-Studio image-to-video (13B-mix)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://app.ltx.studio/motion-workspace?videoModel=ltxv"&gt;LTX-Studio image-to-video (13B distilled)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://fal.ai/models/fal-ai/ltx-video-13b-dev/image-to-video"&gt;Fal.ai image-to-video (13B full)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://fal.ai/models/fal-ai/ltx-video-13b-distilled/image-to-video"&gt;Fal.ai image-to-video (13B distilled)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://replicate.com/lightricks/ltx-video"&gt;Replicate image-to-video&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Run locally&lt;/h2&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;p&gt;The codebase was tested with Python 3.10.5, CUDA version 12.2, and supports PyTorch &amp;gt;= 2.1.2. On macOS, MPS was tested with PyTorch 2.3.0, and should support PyTorch == 2.3 or &amp;gt;= 2.6.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/Lightricks/LTX-Video.git
cd LTX-Video

# create env
python -m venv env
source env/bin/activate
python -m pip install -e .\[inference\]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;FP8 Kernels (optional)&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://github.com/Lightricks/LTXVideo-Q8-Kernels"&gt;FP8 kernels&lt;/a&gt; developed for LTX-Video provide performance boost on supported graphics cards (Ada architecture and later). To install FP8 kernels, follow the instructions in that repository.&lt;/p&gt; 
&lt;h3&gt;Inference&lt;/h3&gt; 
&lt;p&gt;üìù &lt;strong&gt;Note:&lt;/strong&gt; For best results, we recommend using our &lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#comfyui-integration"&gt;ComfyUI&lt;/a&gt; workflow. We're working on updating the inference.py script to match the high quality and output fidelity of ComfyUI.&lt;/p&gt; 
&lt;p&gt;To use our model, please follow the inference code in &lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/inference.py"&gt;inference.py&lt;/a&gt;:&lt;/p&gt; 
&lt;h4&gt;For image-to-video generation:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python inference.py --prompt "PROMPT" --conditioning_media_paths IMAGE_PATH --conditioning_start_frames 0 --height HEIGHT --width WIDTH --num_frames NUM_FRAMES --seed SEED --pipeline_config configs/ltxv-13b-0.9.8-distilled.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Extending a video:&lt;/h4&gt; 
&lt;p&gt;üìù &lt;strong&gt;Note:&lt;/strong&gt; Input video segments must contain a multiple of 8 frames plus 1 (e.g., 9, 17, 25, etc.), and the target frame number should be a multiple of 8.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python inference.py --prompt "PROMPT" --conditioning_media_paths VIDEO_PATH --conditioning_start_frames START_FRAME --height HEIGHT --width WIDTH --num_frames NUM_FRAMES --seed SEED --pipeline_config configs/ltxv-13b-0.9.8-distilled.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;For video generation with multiple conditions:&lt;/h4&gt; 
&lt;p&gt;You can now generate a video conditioned on a set of images and/or short video segments. Simply provide a list of paths to the images or video segments you want to condition on, along with their target frame numbers in the generated video. You can also specify the conditioning strength for each item (default: 1.0).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python inference.py --prompt "PROMPT" --conditioning_media_paths IMAGE_OR_VIDEO_PATH_1 IMAGE_OR_VIDEO_PATH_2 --conditioning_start_frames TARGET_FRAME_1 TARGET_FRAME_2 --height HEIGHT --width WIDTH --num_frames NUM_FRAMES --seed SEED --pipeline_config configs/ltxv-13b-0.9.8-distilled.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Using as a library&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from ltx_video.inference import infer, InferenceConfig

infer(
    InferenceConfig(
        pipeline_config="configs/ltxv-13b-0.9.8-distilled.yaml",
        prompt=PROMPT,
        height=HEIGHT,
        width=WIDTH,
        num_frames=NUM_FRAMES,
        output_path="output.mp4",
    )
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ComfyUI Integration&lt;/h2&gt; 
&lt;p&gt;To use our model with ComfyUI, please follow the instructions at &lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/"&gt;https://github.com/Lightricks/ComfyUI-LTXVideo/&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Diffusers Integration&lt;/h2&gt; 
&lt;p&gt;To use our model with the Diffusers Python library, check out the &lt;a href="https://huggingface.co/docs/diffusers/main/en/api/pipelines/ltx_video"&gt;official documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Diffusers also support an 8-bit version of LTX-Video, &lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#ltx-videoq8"&gt;see details below&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Model User Guide&lt;/h1&gt; 
&lt;h2&gt;üìù Prompt Engineering&lt;/h2&gt; 
&lt;p&gt;When writing prompts, focus on detailed, chronological descriptions of actions and scenes. Include specific movements, appearances, camera angles, and environmental details - all in a single flowing paragraph. Start directly with the action, and keep descriptions literal and precise. Think like a cinematographer describing a shot list. Keep within 200 words. For best results, build your prompts using this structure:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Start with main action in a single sentence&lt;/li&gt; 
 &lt;li&gt;Add specific details about movements and gestures&lt;/li&gt; 
 &lt;li&gt;Describe character/object appearances precisely&lt;/li&gt; 
 &lt;li&gt;Include background and environment details&lt;/li&gt; 
 &lt;li&gt;Specify camera angles and movements&lt;/li&gt; 
 &lt;li&gt;Describe lighting and colors&lt;/li&gt; 
 &lt;li&gt;Note any changes or sudden events&lt;/li&gt; 
 &lt;li&gt;See &lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#introduction"&gt;examples&lt;/a&gt; for more inspiration.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Automatic Prompt Enhancement&lt;/h3&gt; 
&lt;p&gt;When using &lt;code&gt;LTXVideoPipeline&lt;/code&gt; directly, you can enable prompt enhancement by setting &lt;code&gt;enhance_prompt=True&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;üéÆ Parameter Guide&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Resolution Preset: Higher resolutions for detailed scenes, lower for faster generation and simpler scenes. The model works on resolutions that are divisible by 32 and number of frames that are divisible by 8 + 1 (e.g. 257). In case the resolution or number of frames are not divisible by 32 or 8 + 1, the input will be padded with -1 and then cropped to the desired resolution and number of frames. The model works best on resolutions under 720 x 1280 and number of frames below 257&lt;/li&gt; 
 &lt;li&gt;Seed: Save seed values to recreate specific styles or compositions you like&lt;/li&gt; 
 &lt;li&gt;Guidance Scale: 3-3.5 are the recommended values&lt;/li&gt; 
 &lt;li&gt;Inference Steps: More steps (40+) for quality, fewer steps (20-30) for speed&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;üìù For advanced parameters usage, please see &lt;code&gt;python inference.py --help&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;Community Contribution&lt;/h2&gt; 
&lt;h3&gt;ComfyUI-LTXTricks üõ†Ô∏è&lt;/h3&gt; 
&lt;p&gt;A community project providing additional nodes for enhanced control over the LTX Video model. It includes implementations of advanced techniques like RF-Inversion, RF-Edit, FlowEdit, and more. These nodes enable workflows such as Image and Video to Video (I+V2V), enhanced sampling via Spatiotemporal Skip Guidance (STG), and interpolation with precise frame settings.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Repository:&lt;/strong&gt; &lt;a href="https://github.com/logtd/ComfyUI-LTXTricks"&gt;ComfyUI-LTXTricks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Features:&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;üîÑ &lt;strong&gt;RF-Inversion:&lt;/strong&gt; Implements &lt;a href="https://rf-inversion.github.io/"&gt;RF-Inversion&lt;/a&gt; with an &lt;a href="https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltx_inversion.json"&gt;example workflow here&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;‚úÇÔ∏è &lt;strong&gt;RF-Edit:&lt;/strong&gt; Implements &lt;a href="https://github.com/wangjiangshan0725/RF-Solver-Edit"&gt;RF-Solver-Edit&lt;/a&gt; with an &lt;a href="https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltx_rf_edit.json"&gt;example workflow here&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;üåä &lt;strong&gt;FlowEdit:&lt;/strong&gt; Implements &lt;a href="https://github.com/fallenshock/FlowEdit"&gt;FlowEdit&lt;/a&gt; with an &lt;a href="https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltx_flow_edit.json"&gt;example workflow here&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;üé• &lt;strong&gt;I+V2V:&lt;/strong&gt; Enables Video to Video with a reference image. &lt;a href="https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltx_iv2v.json"&gt;Example workflow&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;‚ú® &lt;strong&gt;Enhance:&lt;/strong&gt; Partial implementation of &lt;a href="https://junhahyung.github.io/STGuidance/"&gt;STGuidance&lt;/a&gt;. &lt;a href="https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltxv_stg.json"&gt;Example workflow&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;üñºÔ∏è &lt;strong&gt;Interpolation and Frame Setting:&lt;/strong&gt; Nodes for precise control of latents per frame. &lt;a href="https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltx_interpolation.json"&gt;Example workflow&lt;/a&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;LTX-VideoQ8 üé± &lt;a id="ltx-videoq8"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;LTX-VideoQ8&lt;/strong&gt; is an 8-bit optimized version of &lt;a href="https://github.com/Lightricks/LTX-Video"&gt;LTX-Video&lt;/a&gt;, designed for faster performance on NVIDIA ADA GPUs.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Repository:&lt;/strong&gt; &lt;a href="https://github.com/KONAKONA666/LTX-Video"&gt;LTX-VideoQ8&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Features:&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;üöÄ Up to 3X speed-up with no accuracy loss&lt;/li&gt; 
   &lt;li&gt;üé• Generate 720x480x121 videos in under a minute on RTX 4060 (8GB VRAM)&lt;/li&gt; 
   &lt;li&gt;üõ†Ô∏è Fine-tune 2B transformer models with precalculated latents&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Community Discussion:&lt;/strong&gt; &lt;a href="https://www.reddit.com/r/StableDiffusion/comments/1h79ks2/fast_ltx_video_on_rtx_4060_and_other_ada_gpus/"&gt;Reddit Thread&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Diffusers integration:&lt;/strong&gt; A diffusers integration for the 8-bit model is already out! &lt;a href="https://github.com/sayakpaul/q8-ltx-video"&gt;Details here&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;TeaCache for LTX-Video üçµ &lt;a id="TeaCache"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;TeaCache&lt;/strong&gt; is a training-free caching approach that leverages timestep differences across model outputs to accelerate LTX-Video inference by up to 2x without significant visual quality degradation.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Repository:&lt;/strong&gt; &lt;a href="https://github.com/ali-vilab/TeaCache/tree/main/TeaCache4LTX-Video"&gt;TeaCache4LTX-Video&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Features:&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;üöÄ Speeds up LTX-Video inference.&lt;/li&gt; 
   &lt;li&gt;üìä Adjustable trade-offs between speed (up to 2x) and visual quality using configurable parameters.&lt;/li&gt; 
   &lt;li&gt;üõ†Ô∏è No retraining required: Works directly with existing models.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Your Contribution&lt;/h3&gt; 
&lt;p&gt;...is welcome! If you have a project or tool that integrates with LTX-Video, please let us know by opening an issue or pull request.&lt;/p&gt; 
&lt;h1&gt;Training&lt;/h1&gt; 
&lt;p&gt;We provide an open-source repository for fine-tuning the LTX-Video model: &lt;a href="https://github.com/Lightricks/LTX-Video-Trainer"&gt;LTX-Video-Trainer&lt;/a&gt;. This repository supports both the 2B and 13B model variants, enabling full fine-tuning as well as LoRA (Low-Rank Adaptation) fine-tuning for more efficient training. This includes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Control LoRAs&lt;/strong&gt;: Train custom control models like depth, pose, and canny control&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Effect LoRAs&lt;/strong&gt;: Create specialized effects and transformations for video generation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Explore the repository to customize the model for your specific use cases! More information and training instructions can be found in the &lt;a href="https://github.com/Lightricks/LTX-Video-Trainer/raw/main/README.md"&gt;README&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Control Models&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo"&gt;ComfyUI-LTXVideo&lt;/a&gt; repository now contains workflows and models for 3 specialized models that enable precise control over LTX-Video generation:&lt;/p&gt; 
&lt;p&gt;Pose Control, Depth Control and Canny Control&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Example ComfyUI Workflow (for all control types):&lt;/strong&gt; &lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/ic_lora/ic-lora.json"&gt;ic-lora.json&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Join Us&lt;/h1&gt; 
&lt;p&gt;Want to work on cutting-edge AI research and make a real impact on millions of users worldwide?&lt;/p&gt; 
&lt;p&gt;At &lt;strong&gt;Lightricks&lt;/strong&gt;, an AI-first company, we're revolutionizing how visual content is created.&lt;/p&gt; 
&lt;p&gt;If you are passionate about AI, computer vision, and video generation, we would love to hear from you!&lt;/p&gt; 
&lt;p&gt;Please visit our &lt;a href="https://careers.lightricks.com/careers?query=&amp;amp;office=all&amp;amp;department=R%26D"&gt;careers page&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h1&gt;Acknowledgement&lt;/h1&gt; 
&lt;p&gt;We are grateful for the following awesome projects when implementing LTX-Video:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/facebookresearch/DiT"&gt;DiT&lt;/a&gt; and &lt;a href="https://github.com/PixArt-alpha/PixArt-alpha"&gt;PixArt-alpha&lt;/a&gt;: vision transformers for image generation.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;üìÑ Our tech report is out! If you find our work helpful, please ‚≠êÔ∏è star the repository and cite our paper.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@article{HaCohen2024LTXVideo,
  title={LTX-Video: Realtime Video Latent Diffusion},
  author={HaCohen, Yoav and Chiprut, Nisan and Brazowski, Benny and Shalem, Daniel and Moshe, Dudu and Richardson, Eitan and Levin, Eran and Shiran, Guy and Zabari, Nir and Gordon, Ori and Panet, Poriya and Weissbuch, Sapir and Kulikov, Victor and Bitterman, Yaki and Melumian, Zeev and Bibi, Ofir},
  journal={arXiv preprint arXiv:2501.00103},
  year={2024}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
  </channel>
</rss>