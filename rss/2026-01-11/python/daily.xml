<rss version="2.0">
  <channel>
    <title>GitHub Python Daily Trending</title>
    <description>Daily Trending of Python in GitHub</description>
    <pubDate>Sat, 10 Jan 2026 01:37:52 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>TauricResearch/TradingAgents</title>
      <link>https://github.com/TauricResearch/TradingAgents</link>
      <description>&lt;p&gt;TradingAgents: Multi-Agents LLM Financial Trading Framework&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/TauricResearch/TradingAgents/main/assets/TauricResearch.png" style="width: 60%; height: auto;" /&gt; &lt;/p&gt; 
&lt;div align="center" style="line-height: 1;"&gt; 
 &lt;a href="https://arxiv.org/abs/2412.20138" target="_blank"&gt;&lt;img alt="arXiv" src="https://img.shields.io/badge/arXiv-2412.20138-B31B1B?logo=arxiv" /&gt;&lt;/a&gt; 
 &lt;a href="https://discord.com/invite/hk9PGKShPK" target="_blank"&gt;&lt;img alt="Discord" src="https://img.shields.io/badge/Discord-TradingResearch-7289da?logo=discord&amp;amp;logoColor=white&amp;amp;color=7289da" /&gt;&lt;/a&gt; 
 &lt;a href="https://raw.githubusercontent.com/TauricResearch/TradingAgents/main/assets/wechat.png" target="_blank"&gt;&lt;img alt="WeChat" src="https://img.shields.io/badge/WeChat-TauricResearch-brightgreen?logo=wechat&amp;amp;logoColor=white" /&gt;&lt;/a&gt; 
 &lt;a href="https://x.com/TauricResearch" target="_blank"&gt;&lt;img alt="X Follow" src="https://img.shields.io/badge/X-TauricResearch-white?logo=x&amp;amp;logoColor=white" /&gt;&lt;/a&gt; 
 &lt;br /&gt; 
 &lt;a href="https://github.com/TauricResearch/" target="_blank"&gt;&lt;img alt="Community" src="https://img.shields.io/badge/Join_GitHub_Community-TauricResearch-14C290?logo=discourse" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;!-- Keep these links. Translations will automatically update with the README. --&gt; 
 &lt;a href="https://www.readme-i18n.com/TauricResearch/TradingAgents?lang=de"&gt;Deutsch&lt;/a&gt; | 
 &lt;a href="https://www.readme-i18n.com/TauricResearch/TradingAgents?lang=es"&gt;Espa√±ol&lt;/a&gt; | 
 &lt;a href="https://www.readme-i18n.com/TauricResearch/TradingAgents?lang=fr"&gt;fran√ßais&lt;/a&gt; | 
 &lt;a href="https://www.readme-i18n.com/TauricResearch/TradingAgents?lang=ja"&gt;Êó•Êú¨Ë™û&lt;/a&gt; | 
 &lt;a href="https://www.readme-i18n.com/TauricResearch/TradingAgents?lang=ko"&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | 
 &lt;a href="https://www.readme-i18n.com/TauricResearch/TradingAgents?lang=pt"&gt;Portugu√™s&lt;/a&gt; | 
 &lt;a href="https://www.readme-i18n.com/TauricResearch/TradingAgents?lang=ru"&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; | 
 &lt;a href="https://www.readme-i18n.com/TauricResearch/TradingAgents?lang=zh"&gt;‰∏≠Êñá&lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h1&gt;TradingAgents: Multi-Agents LLM Financial Trading Framework&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;üéâ &lt;strong&gt;TradingAgents&lt;/strong&gt; officially released! We have received numerous inquiries about the work, and we would like to express our thanks for the enthusiasm in our community.&lt;/p&gt; 
 &lt;p&gt;So we decided to fully open-source the framework. Looking forward to building impactful projects with you!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://www.star-history.com/#TauricResearch/TradingAgents&amp;amp;Date"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=TauricResearch/TradingAgents&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
   &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=TauricResearch/TradingAgents&amp;amp;type=Date" /&gt; 
   &lt;img alt="TradingAgents Star History" src="https://api.star-history.com/svg?repos=TauricResearch/TradingAgents&amp;amp;type=Date" style="width: 80%; height: auto;" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;üöÄ &lt;a href="https://raw.githubusercontent.com/TauricResearch/TradingAgents/main/#tradingagents-framework"&gt;TradingAgents&lt;/a&gt; | ‚ö° &lt;a href="https://raw.githubusercontent.com/TauricResearch/TradingAgents/main/#installation-and-cli"&gt;Installation &amp;amp; CLI&lt;/a&gt; | üé¨ &lt;a href="https://www.youtube.com/watch?v=90gr5lwjIho"&gt;Demo&lt;/a&gt; | üì¶ &lt;a href="https://raw.githubusercontent.com/TauricResearch/TradingAgents/main/#tradingagents-package"&gt;Package Usage&lt;/a&gt; | ü§ù &lt;a href="https://raw.githubusercontent.com/TauricResearch/TradingAgents/main/#contributing"&gt;Contributing&lt;/a&gt; | üìÑ &lt;a href="https://raw.githubusercontent.com/TauricResearch/TradingAgents/main/#citation"&gt;Citation&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;TradingAgents Framework&lt;/h2&gt; 
&lt;p&gt;TradingAgents is a multi-agent trading framework that mirrors the dynamics of real-world trading firms. By deploying specialized LLM-powered agents: from fundamental analysts, sentiment experts, and technical analysts, to trader, risk management team, the platform collaboratively evaluates market conditions and informs trading decisions. Moreover, these agents engage in dynamic discussions to pinpoint the optimal strategy.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/TauricResearch/TradingAgents/main/assets/schema.png" style="width: 100%; height: auto;" /&gt; &lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;TradingAgents framework is designed for research purposes. Trading performance may vary based on many factors, including the chosen backbone language models, model temperature, trading periods, the quality of data, and other non-deterministic factors. &lt;a href="https://tauric.ai/disclaimer/"&gt;It is not intended as financial, investment, or trading advice.&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Our framework decomposes complex trading tasks into specialized roles. This ensures the system achieves a robust, scalable approach to market analysis and decision-making.&lt;/p&gt; 
&lt;h3&gt;Analyst Team&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Fundamentals Analyst: Evaluates company financials and performance metrics, identifying intrinsic values and potential red flags.&lt;/li&gt; 
 &lt;li&gt;Sentiment Analyst: Analyzes social media and public sentiment using sentiment scoring algorithms to gauge short-term market mood.&lt;/li&gt; 
 &lt;li&gt;News Analyst: Monitors global news and macroeconomic indicators, interpreting the impact of events on market conditions.&lt;/li&gt; 
 &lt;li&gt;Technical Analyst: Utilizes technical indicators (like MACD and RSI) to detect trading patterns and forecast price movements.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/TauricResearch/TradingAgents/main/assets/analyst.png" width="100%" style="display: inline-block; margin: 0 2%;" /&gt; &lt;/p&gt; 
&lt;h3&gt;Researcher Team&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Comprises both bullish and bearish researchers who critically assess the insights provided by the Analyst Team. Through structured debates, they balance potential gains against inherent risks.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/TauricResearch/TradingAgents/main/assets/researcher.png" width="70%" style="display: inline-block; margin: 0 2%;" /&gt; &lt;/p&gt; 
&lt;h3&gt;Trader Agent&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Composes reports from the analysts and researchers to make informed trading decisions. It determines the timing and magnitude of trades based on comprehensive market insights.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/TauricResearch/TradingAgents/main/assets/trader.png" width="70%" style="display: inline-block; margin: 0 2%;" /&gt; &lt;/p&gt; 
&lt;h3&gt;Risk Management and Portfolio Manager&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Continuously evaluates portfolio risk by assessing market volatility, liquidity, and other risk factors. The risk management team evaluates and adjusts trading strategies, providing assessment reports to the Portfolio Manager for final decision.&lt;/li&gt; 
 &lt;li&gt;The Portfolio Manager approves/rejects the transaction proposal. If approved, the order will be sent to the simulated exchange and executed.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/TauricResearch/TradingAgents/main/assets/risk.png" width="70%" style="display: inline-block; margin: 0 2%;" /&gt; &lt;/p&gt; 
&lt;h2&gt;Installation and CLI&lt;/h2&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;p&gt;Clone TradingAgents:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/TauricResearch/TradingAgents.git
cd TradingAgents
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Create a virtual environment in any of your favorite environment managers:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;conda create -n tradingagents python=3.13
conda activate tradingagents
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Install dependencies:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Required APIs&lt;/h3&gt; 
&lt;p&gt;You will need the OpenAI API for all the agents, and &lt;a href="https://www.alphavantage.co/support/#api-key"&gt;Alpha Vantage API&lt;/a&gt; for fundamental and news data (default configuration).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export OPENAI_API_KEY=$YOUR_OPENAI_API_KEY
export ALPHA_VANTAGE_API_KEY=$YOUR_ALPHA_VANTAGE_API_KEY
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, you can create a &lt;code&gt;.env&lt;/code&gt; file in the project root with your API keys (see &lt;code&gt;.env.example&lt;/code&gt; for reference):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cp .env.example .env
# Edit .env with your actual API keys
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; We are happy to partner with Alpha Vantage to provide robust API support for TradingAgents. You can get a free AlphaVantage API &lt;a href="https://www.alphavantage.co/support/#api-key"&gt;here&lt;/a&gt;, TradingAgents-sourced requests also have increased rate limits to 60 requests per minute with no daily limits. Typically the quota is sufficient for performing complex tasks with TradingAgents thanks to Alpha Vantage‚Äôs open-source support program. If you prefer to use OpenAI for these data sources instead, you can modify the data vendor settings in &lt;code&gt;tradingagents/default_config.py&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;CLI Usage&lt;/h3&gt; 
&lt;p&gt;You can also try out the CLI directly by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m cli.main
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You will see a screen where you can select your desired tickers, date, LLMs, research depth, etc.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/TauricResearch/TradingAgents/main/assets/cli/cli_init.png" width="100%" style="display: inline-block; margin: 0 2%;" /&gt; &lt;/p&gt; 
&lt;p&gt;An interface will appear showing results as they load, letting you track the agent's progress as it runs.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/TauricResearch/TradingAgents/main/assets/cli/cli_news.png" width="100%" style="display: inline-block; margin: 0 2%;" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/TauricResearch/TradingAgents/main/assets/cli/cli_transaction.png" width="100%" style="display: inline-block; margin: 0 2%;" /&gt; &lt;/p&gt; 
&lt;h2&gt;TradingAgents Package&lt;/h2&gt; 
&lt;h3&gt;Implementation Details&lt;/h3&gt; 
&lt;p&gt;We built TradingAgents with LangGraph to ensure flexibility and modularity. We utilize &lt;code&gt;o1-preview&lt;/code&gt; and &lt;code&gt;gpt-4o&lt;/code&gt; as our deep thinking and fast thinking LLMs for our experiments. However, for testing purposes, we recommend you use &lt;code&gt;o4-mini&lt;/code&gt; and &lt;code&gt;gpt-4.1-mini&lt;/code&gt; to save on costs as our framework makes &lt;strong&gt;lots of&lt;/strong&gt; API calls.&lt;/p&gt; 
&lt;h3&gt;Python Usage&lt;/h3&gt; 
&lt;p&gt;To use TradingAgents inside your code, you can import the &lt;code&gt;tradingagents&lt;/code&gt; module and initialize a &lt;code&gt;TradingAgentsGraph()&lt;/code&gt; object. The &lt;code&gt;.propagate()&lt;/code&gt; function will return a decision. You can run &lt;code&gt;main.py&lt;/code&gt;, here's also a quick example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from tradingagents.graph.trading_graph import TradingAgentsGraph
from tradingagents.default_config import DEFAULT_CONFIG

ta = TradingAgentsGraph(debug=True, config=DEFAULT_CONFIG.copy())

# forward propagate
_, decision = ta.propagate("NVDA", "2024-05-10")
print(decision)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also adjust the default configuration to set your own choice of LLMs, debate rounds, etc.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from tradingagents.graph.trading_graph import TradingAgentsGraph
from tradingagents.default_config import DEFAULT_CONFIG

# Create a custom config
config = DEFAULT_CONFIG.copy()
config["deep_think_llm"] = "gpt-4.1-nano"  # Use a different model
config["quick_think_llm"] = "gpt-4.1-nano"  # Use a different model
config["max_debate_rounds"] = 1  # Increase debate rounds

# Configure data vendors (default uses yfinance and Alpha Vantage)
config["data_vendors"] = {
    "core_stock_apis": "yfinance",           # Options: yfinance, alpha_vantage, local
    "technical_indicators": "yfinance",      # Options: yfinance, alpha_vantage, local
    "fundamental_data": "alpha_vantage",     # Options: openai, alpha_vantage, local
    "news_data": "alpha_vantage",            # Options: openai, alpha_vantage, google, local
}

# Initialize with custom config
ta = TradingAgentsGraph(debug=True, config=config)

# forward propagate
_, decision = ta.propagate("NVDA", "2024-05-10")
print(decision)
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;The default configuration uses yfinance for stock price and technical data, and Alpha Vantage for fundamental and news data. For production use or if you encounter rate limits, consider upgrading to &lt;a href="https://www.alphavantage.co/premium/"&gt;Alpha Vantage Premium&lt;/a&gt; for more stable and reliable data access. For offline experimentation, there's a local data vendor option that uses our &lt;strong&gt;Tauric TradingDB&lt;/strong&gt;, a curated dataset for backtesting, though this is still in development. We're currently refining this dataset and plan to release it soon alongside our upcoming projects. Stay tuned!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;You can view the full list of configurations in &lt;code&gt;tradingagents/default_config.py&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions from the community! Whether it's fixing a bug, improving documentation, or suggesting a new feature, your input helps make this project better. If you are interested in this line of research, please consider joining our open-source financial AI research community &lt;a href="https://tauric.ai/"&gt;Tauric Research&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;Please reference our work if you find &lt;em&gt;TradingAgents&lt;/em&gt; provides you with some help :)&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@misc{xiao2025tradingagentsmultiagentsllmfinancial,
      title={TradingAgents: Multi-Agents LLM Financial Trading Framework}, 
      author={Yijia Xiao and Edward Sun and Di Luo and Wei Wang},
      year={2025},
      eprint={2412.20138},
      archivePrefix={arXiv},
      primaryClass={q-fin.TR},
      url={https://arxiv.org/abs/2412.20138}, 
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Comfy-Org/ComfyUI</title>
      <link>https://github.com/Comfy-Org/ComfyUI</link>
      <description>&lt;p&gt;The most powerful and modular diffusion model GUI, api and backend with a graph/nodes interface.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h1&gt;ComfyUI&lt;/h1&gt; 
 &lt;p&gt;&lt;strong&gt;The most powerful and modular visual AI engine and application.&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://www.comfy.org/"&gt;&lt;img src="https://img.shields.io/badge/ComfyOrg-4285F4?style=flat" alt="Website" /&gt;&lt;/a&gt; &lt;a href="https://www.comfy.org/discord"&gt;&lt;img src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fdiscord.com%2Fapi%2Finvites%2Fcomfyorg%3Fwith_counts%3Dtrue&amp;amp;query=%24.approximate_member_count&amp;amp;logo=discord&amp;amp;logoColor=white&amp;amp;label=Discord&amp;amp;color=green&amp;amp;suffix=%20total" alt="Dynamic JSON Badge" /&gt;&lt;/a&gt; &lt;a href="https://x.com/ComfyUI"&gt;&lt;img src="https://img.shields.io/twitter/follow/ComfyUI" alt="Twitter" /&gt;&lt;/a&gt; &lt;a href="https://app.element.io/#/room/%23comfyui_space%3Amatrix.org"&gt;&lt;img src="https://img.shields.io/badge/Matrix-000000?style=flat&amp;amp;logo=matrix&amp;amp;logoColor=white" alt="Matrix" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;a href="https://github.com/comfyanonymous/ComfyUI/releases"&gt;&lt;img src="https://img.shields.io/github/v/release/comfyanonymous/ComfyUI?style=flat&amp;amp;sort=semver" alt="" /&gt;&lt;/a&gt; &lt;a href="https://github.com/comfyanonymous/ComfyUI/releases"&gt;&lt;img src="https://img.shields.io/github/release-date/comfyanonymous/ComfyUI?style=flat" alt="" /&gt;&lt;/a&gt; &lt;a href="https://github.com/comfyanonymous/ComfyUI/releases"&gt;&lt;img src="https://img.shields.io/github/downloads/comfyanonymous/ComfyUI/total?style=flat" alt="" /&gt;&lt;/a&gt; &lt;a href="https://github.com/comfyanonymous/ComfyUI/releases"&gt;&lt;img src="https://img.shields.io/github/downloads/comfyanonymous/ComfyUI/latest/total?style=flat&amp;amp;label=downloads%40latest" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;!-- Workaround to display total user from https://github.com/badges/shields/issues/4500#issuecomment-2060079995 --&gt; 
 &lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/7ccaf2c1-9b72-41ae-9a89-5688c94b7abe" alt="ComfyUI Screenshot" /&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;ComfyUI lets you design and execute advanced stable diffusion pipelines using a graph/nodes/flowchart based interface. Available on Windows, Linux, and macOS.&lt;/p&gt; 
&lt;h2&gt;Get Started&lt;/h2&gt; 
&lt;h4&gt;&lt;a href="https://www.comfy.org/download"&gt;Desktop Application&lt;/a&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;The easiest way to get started.&lt;/li&gt; 
 &lt;li&gt;Available on Windows &amp;amp; macOS.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;&lt;a href="https://raw.githubusercontent.com/Comfy-Org/ComfyUI/master/#installing"&gt;Windows Portable Package&lt;/a&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Get the latest commits and completely portable.&lt;/li&gt; 
 &lt;li&gt;Available on Windows.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;&lt;a href="https://raw.githubusercontent.com/Comfy-Org/ComfyUI/master/#manual-install-windows-linux"&gt;Manual Install&lt;/a&gt;&lt;/h4&gt; 
&lt;p&gt;Supports all operating systems and GPU types (NVIDIA, AMD, Intel, Apple Silicon, Ascend).&lt;/p&gt; 
&lt;h2&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/"&gt;Examples&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;See what ComfyUI can do with the &lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/"&gt;example workflows&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Nodes/graph/flowchart interface to experiment and create complex Stable Diffusion workflows without needing to code anything.&lt;/li&gt; 
 &lt;li&gt;Image Models 
  &lt;ul&gt; 
   &lt;li&gt;SD1.x, SD2.x (&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/unclip/"&gt;unCLIP&lt;/a&gt;)&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/sdxl/"&gt;SDXL&lt;/a&gt;, &lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/sdturbo/"&gt;SDXL Turbo&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/stable_cascade/"&gt;Stable Cascade&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/sd3/"&gt;SD3 and SD3.5&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Pixart Alpha and Sigma&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/aura_flow/"&gt;AuraFlow&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/hunyuan_dit/"&gt;HunyuanDiT&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/flux/"&gt;Flux&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/lumina2/"&gt;Lumina Image 2.0&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/hidream/"&gt;HiDream&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/qwen_image/"&gt;Qwen Image&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/hunyuan_image/"&gt;Hunyuan Image 2.1&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/flux2/"&gt;Flux 2&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/z_image/"&gt;Z Image&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Image Editing Models 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/omnigen/"&gt;Omnigen 2&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/flux/#flux-kontext-image-editing-model"&gt;Flux Kontext&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/hidream/#hidream-e11"&gt;HiDream E1.1&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/qwen_image/#edit-model"&gt;Qwen Image Edit&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Video Models 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/video/"&gt;Stable Video Diffusion&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/mochi/"&gt;Mochi&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/ltxv/"&gt;LTX-Video&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/hunyuan_video/"&gt;Hunyuan Video&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/wan/"&gt;Wan 2.1&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/wan22/"&gt;Wan 2.2&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://docs.comfy.org/tutorials/video/hunyuan/hunyuan-video-1-5"&gt;Hunyuan Video 1.5&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Audio Models 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/audio/"&gt;Stable Audio&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/audio/"&gt;ACE Step&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;3D Models 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://docs.comfy.org/tutorials/3d/hunyuan3D-2"&gt;Hunyuan3D 2.0&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Asynchronous Queue system&lt;/li&gt; 
 &lt;li&gt;Many optimizations: Only re-executes the parts of the workflow that changes between executions.&lt;/li&gt; 
 &lt;li&gt;Smart memory management: can automatically run large models on GPUs with as low as 1GB vram with smart offloading.&lt;/li&gt; 
 &lt;li&gt;Works even if you don't have a GPU with: &lt;code&gt;--cpu&lt;/code&gt; (slow)&lt;/li&gt; 
 &lt;li&gt;Can load ckpt and safetensors: All in one checkpoints or standalone diffusion models, VAEs and CLIP models.&lt;/li&gt; 
 &lt;li&gt;Safe loading of ckpt, pt, pth, etc.. files.&lt;/li&gt; 
 &lt;li&gt;Embeddings/Textual inversion&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/lora/"&gt;Loras (regular, locon and loha)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/hypernetworks/"&gt;Hypernetworks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Loading full workflows (with seeds) from generated PNG, WebP and FLAC files.&lt;/li&gt; 
 &lt;li&gt;Saving/Loading workflows as Json files.&lt;/li&gt; 
 &lt;li&gt;Nodes interface can be used to create complex workflows like one for &lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/2_pass_txt2img/"&gt;Hires fix&lt;/a&gt; or much more advanced ones.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/area_composition/"&gt;Area Composition&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/inpaint/"&gt;Inpainting&lt;/a&gt; with both regular and inpainting models.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/controlnet/"&gt;ControlNet and T2I-Adapter&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/upscale_models/"&gt;Upscale Models (ESRGAN, ESRGAN variants, SwinIR, Swin2SR, etc...)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/gligen/"&gt;GLIGEN&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/model_merging/"&gt;Model Merging&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/lcm/"&gt;LCM models and Loras&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Latent previews with &lt;a href="https://raw.githubusercontent.com/Comfy-Org/ComfyUI/master/#how-to-show-high-quality-previews"&gt;TAESD&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Works fully offline: core will never download anything unless you want to.&lt;/li&gt; 
 &lt;li&gt;Optional API nodes to use paid models from external providers through the online &lt;a href="https://docs.comfy.org/tutorials/api-nodes/overview"&gt;Comfy API&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Comfy-Org/ComfyUI/master/extra_model_paths.yaml.example"&gt;Config file&lt;/a&gt; to set the search paths for models.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Workflow examples can be found on the &lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/"&gt;Examples page&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Release Process&lt;/h2&gt; 
&lt;p&gt;ComfyUI follows a weekly release cycle targeting Monday but this regularly changes because of model releases or large changes to the codebase. There are three interconnected repositories:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/comfyanonymous/ComfyUI"&gt;ComfyUI Core&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Releases a new stable version (e.g., v0.7.0) roughly every week.&lt;/li&gt; 
   &lt;li&gt;Starting from v0.4.0 patch versions will be used for fixes backported onto the current stable release.&lt;/li&gt; 
   &lt;li&gt;Minor versions will be used for releases off the master branch.&lt;/li&gt; 
   &lt;li&gt;Patch versions may still be used for releases on the master branch in cases where a backport would not make sense.&lt;/li&gt; 
   &lt;li&gt;Commits outside of the stable release tags may be very unstable and break many custom nodes.&lt;/li&gt; 
   &lt;li&gt;Serves as the foundation for the desktop release&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/Comfy-Org/desktop"&gt;ComfyUI Desktop&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Builds a new release using the latest stable core version&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/Comfy-Org/ComfyUI_frontend"&gt;ComfyUI Frontend&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Weekly frontend updates are merged into the core repository&lt;/li&gt; 
   &lt;li&gt;Features are frozen for the upcoming core release&lt;/li&gt; 
   &lt;li&gt;Development continues for the next release cycle&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Shortcuts&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Keybind&lt;/th&gt; 
   &lt;th&gt;Explanation&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;Enter&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Queue up current graph for generation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;Shift&lt;/code&gt; + &lt;code&gt;Enter&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Queue up current graph as first for generation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;Alt&lt;/code&gt; + &lt;code&gt;Enter&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Cancel current generation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;Z&lt;/code&gt;/&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;Y&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Undo/Redo&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;S&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Save workflow&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;O&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Load workflow&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;A&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Select all nodes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Alt &lt;/code&gt;+ &lt;code&gt;C&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Collapse/uncollapse selected nodes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;M&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Mute/unmute selected nodes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;B&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Bypass selected nodes (acts like the node was removed from the graph and the wires reconnected through)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Delete&lt;/code&gt;/&lt;code&gt;Backspace&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Delete selected nodes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;Backspace&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Delete the current graph&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Space&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Move the canvas around when held and moving the cursor&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt;/&lt;code&gt;Shift&lt;/code&gt; + &lt;code&gt;Click&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Add clicked node to selection&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;C&lt;/code&gt;/&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;V&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Copy and paste selected nodes (without maintaining connections to outputs of unselected nodes)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;C&lt;/code&gt;/&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;Shift&lt;/code&gt; + &lt;code&gt;V&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Copy and paste selected nodes (maintaining connections from outputs of unselected nodes to inputs of pasted nodes)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Shift&lt;/code&gt; + &lt;code&gt;Drag&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Move multiple selected nodes at the same time&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;D&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Load default graph&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Alt&lt;/code&gt; + &lt;code&gt;+&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Canvas Zoom in&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Alt&lt;/code&gt; + &lt;code&gt;-&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Canvas Zoom out&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;Shift&lt;/code&gt; + LMB + Vertical drag&lt;/td&gt; 
   &lt;td&gt;Canvas Zoom in/out&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;P&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Pin/Unpin selected nodes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;G&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Group selected nodes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Q&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Toggle visibility of the queue&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;H&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Toggle visibility of history&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;R&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Refresh graph&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;F&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Show/Hide menu&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;.&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Fit view to selection (Whole graph when nothing is selected)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Double-Click LMB&lt;/td&gt; 
   &lt;td&gt;Open node quick search palette&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Shift&lt;/code&gt; + Drag&lt;/td&gt; 
   &lt;td&gt;Move multiple wires at once&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;Alt&lt;/code&gt; + LMB&lt;/td&gt; 
   &lt;td&gt;Disconnect all wires from clicked slot&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;code&gt;Ctrl&lt;/code&gt; can also be replaced with &lt;code&gt;Cmd&lt;/code&gt; instead for macOS users&lt;/p&gt; 
&lt;h1&gt;Installing&lt;/h1&gt; 
&lt;h2&gt;Windows Portable&lt;/h2&gt; 
&lt;p&gt;There is a portable standalone build for Windows that should work for running on Nvidia GPUs or for running on your CPU only on the &lt;a href="https://github.com/comfyanonymous/ComfyUI/releases"&gt;releases page&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;&lt;a href="https://github.com/comfyanonymous/ComfyUI/releases/latest/download/ComfyUI_windows_portable_nvidia.7z"&gt;Direct link to download&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Simply download, extract with &lt;a href="https://7-zip.org"&gt;7-Zip&lt;/a&gt; or with the windows explorer on recent windows versions and run. For smaller models you normally only need to put the checkpoints (the huge ckpt/safetensors files) in: ComfyUI\models\checkpoints but many of the larger models have multiple files. Make sure to follow the instructions to know which subfolder to put them in ComfyUI\models\&lt;/p&gt; 
&lt;p&gt;If you have trouble extracting it, right click the file -&amp;gt; properties -&amp;gt; unblock&lt;/p&gt; 
&lt;p&gt;Update your Nvidia drivers if it doesn't start.&lt;/p&gt; 
&lt;h4&gt;Alternative Downloads:&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://github.com/comfyanonymous/ComfyUI/releases/latest/download/ComfyUI_windows_portable_amd.7z"&gt;Experimental portable for AMD GPUs&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/comfyanonymous/ComfyUI/releases/latest/download/ComfyUI_windows_portable_nvidia_cu128.7z"&gt;Portable with pytorch cuda 12.8 and python 3.12&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/comfyanonymous/ComfyUI/releases/latest/download/ComfyUI_windows_portable_nvidia_cu126.7z"&gt;Portable with pytorch cuda 12.6 and python 3.12&lt;/a&gt; (Supports Nvidia 10 series and older GPUs).&lt;/p&gt; 
&lt;h4&gt;How do I share models between another UI and ComfyUI?&lt;/h4&gt; 
&lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/Comfy-Org/ComfyUI/master/extra_model_paths.yaml.example"&gt;Config file&lt;/a&gt; to set the search paths for models. In the standalone windows build you can find this file in the ComfyUI directory. Rename this file to extra_model_paths.yaml and edit it with your favorite text editor.&lt;/p&gt; 
&lt;h2&gt;&lt;a href="https://docs.comfy.org/comfy-cli/getting-started"&gt;comfy-cli&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;You can install and start ComfyUI using comfy-cli:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install comfy-cli
comfy install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Manual Install (Windows, Linux)&lt;/h2&gt; 
&lt;p&gt;Python 3.14 works but you may encounter issues with the torch compile node. The free threaded variant is still missing some dependencies.&lt;/p&gt; 
&lt;p&gt;Python 3.13 is very well supported. If you have trouble with some custom node dependencies on 3.13 you can try 3.12&lt;/p&gt; 
&lt;p&gt;torch 2.4 and above is supported but some features might only work on newer versions. We generally recommend using the latest major version of pytorch unless it is less than 2 weeks old.&lt;/p&gt; 
&lt;h3&gt;Instructions:&lt;/h3&gt; 
&lt;p&gt;Git clone this repo.&lt;/p&gt; 
&lt;p&gt;Put your SD checkpoints (the huge ckpt/safetensors files) in: models/checkpoints&lt;/p&gt; 
&lt;p&gt;Put your VAE in: models/vae&lt;/p&gt; 
&lt;h3&gt;AMD GPUs (Linux)&lt;/h3&gt; 
&lt;p&gt;AMD users can install rocm and pytorch with pip if you don't have it already installed, this is the command to install the stable version:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.4&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;This is the command to install the nightly with ROCm 7.0 which might have some performance improvements:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm7.1&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;AMD GPUs (Experimental: Windows and Linux), RDNA 3, 3.5 and 4 only.&lt;/h3&gt; 
&lt;p&gt;These have less hardware support than the builds above but they work on windows. You also need to install the pytorch version specific to your hardware.&lt;/p&gt; 
&lt;p&gt;RDNA 3 (RX 7000 series):&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip install --pre torch torchvision torchaudio --index-url https://rocm.nightlies.amd.com/v2/gfx110X-dgpu/&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;RDNA 3.5 (Strix halo/Ryzen AI Max+ 365):&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip install --pre torch torchvision torchaudio --index-url https://rocm.nightlies.amd.com/v2/gfx1151/&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;RDNA 4 (RX 9000 series):&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip install --pre torch torchvision torchaudio --index-url https://rocm.nightlies.amd.com/v2/gfx120X-all/&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Intel GPUs (Windows and Linux)&lt;/h3&gt; 
&lt;p&gt;Intel Arc GPU users can install native PyTorch with torch.xpu support using pip. More information can be found &lt;a href="https://pytorch.org/docs/main/notes/get_start_xpu.html"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;To install PyTorch xpu, use the following command:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;code&gt;pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/xpu&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;This is the command to install the Pytorch xpu nightly which might have some performance improvements:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/xpu&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;NVIDIA&lt;/h3&gt; 
&lt;p&gt;Nvidia users should install stable pytorch using this command:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu130&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;This is the command to install pytorch nightly instead which might have performance improvements.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu130&lt;/code&gt;&lt;/p&gt; 
&lt;h4&gt;Troubleshooting&lt;/h4&gt; 
&lt;p&gt;If you get the "Torch not compiled with CUDA enabled" error, uninstall torch with:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip uninstall torch&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;And install it again with the command above.&lt;/p&gt; 
&lt;h3&gt;Dependencies&lt;/h3&gt; 
&lt;p&gt;Install the dependencies by opening your terminal inside the ComfyUI folder and:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;After this you should have everything installed and can proceed to running ComfyUI.&lt;/p&gt; 
&lt;h3&gt;Others:&lt;/h3&gt; 
&lt;h4&gt;Apple Mac silicon&lt;/h4&gt; 
&lt;p&gt;You can install ComfyUI in Apple Mac silicon (M1 or M2) with any recent macOS version.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install pytorch nightly. For instructions, read the &lt;a href="https://developer.apple.com/metal/pytorch/"&gt;Accelerated PyTorch training on Mac&lt;/a&gt; Apple Developer guide (make sure to install the latest pytorch nightly).&lt;/li&gt; 
 &lt;li&gt;Follow the &lt;a href="https://raw.githubusercontent.com/Comfy-Org/ComfyUI/master/#manual-install-windows-linux"&gt;ComfyUI manual installation&lt;/a&gt; instructions for Windows and Linux.&lt;/li&gt; 
 &lt;li&gt;Install the ComfyUI &lt;a href="https://raw.githubusercontent.com/Comfy-Org/ComfyUI/master/#dependencies"&gt;dependencies&lt;/a&gt;. If you have another Stable Diffusion UI &lt;a href="https://raw.githubusercontent.com/Comfy-Org/ComfyUI/master/#i-already-have-another-ui-for-stable-diffusion-installed-do-i-really-have-to-install-all-of-these-dependencies"&gt;you might be able to reuse the dependencies&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Launch ComfyUI by running &lt;code&gt;python main.py&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Remember to add your models, VAE, LoRAs etc. to the corresponding Comfy folders, as discussed in &lt;a href="https://raw.githubusercontent.com/Comfy-Org/ComfyUI/master/#manual-install-windows-linux"&gt;ComfyUI manual installation&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Ascend NPUs&lt;/h4&gt; 
&lt;p&gt;For models compatible with Ascend Extension for PyTorch (torch_npu). To get started, ensure your environment meets the prerequisites outlined on the &lt;a href="https://ascend.github.io/docs/sources/ascend/quick_install.html"&gt;installation&lt;/a&gt; page. Here's a step-by-step guide tailored to your platform and installation method:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Begin by installing the recommended or newer kernel version for Linux as specified in the Installation page of torch-npu, if necessary.&lt;/li&gt; 
 &lt;li&gt;Proceed with the installation of Ascend Basekit, which includes the driver, firmware, and CANN, following the instructions provided for your specific platform.&lt;/li&gt; 
 &lt;li&gt;Next, install the necessary packages for torch-npu by adhering to the platform-specific instructions on the &lt;a href="https://ascend.github.io/docs/sources/pytorch/install.html#pytorch"&gt;Installation&lt;/a&gt; page.&lt;/li&gt; 
 &lt;li&gt;Finally, adhere to the &lt;a href="https://raw.githubusercontent.com/Comfy-Org/ComfyUI/master/#manual-install-windows-linux"&gt;ComfyUI manual installation&lt;/a&gt; guide for Linux. Once all components are installed, you can run ComfyUI as described earlier.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Cambricon MLUs&lt;/h4&gt; 
&lt;p&gt;For models compatible with Cambricon Extension for PyTorch (torch_mlu). Here's a step-by-step guide tailored to your platform and installation method:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install the Cambricon CNToolkit by adhering to the platform-specific instructions on the &lt;a href="https://www.cambricon.com/docs/sdk_1.15.0/cntoolkit_3.7.2/cntoolkit_install_3.7.2/index.html"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Next, install the PyTorch(torch_mlu) following the instructions on the &lt;a href="https://www.cambricon.com/docs/sdk_1.15.0/cambricon_pytorch_1.17.0/user_guide_1.9/index.html"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Launch ComfyUI by running &lt;code&gt;python main.py&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Iluvatar Corex&lt;/h4&gt; 
&lt;p&gt;For models compatible with Iluvatar Extension for PyTorch. Here's a step-by-step guide tailored to your platform and installation method:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install the Iluvatar Corex Toolkit by adhering to the platform-specific instructions on the &lt;a href="https://support.iluvatar.com/#/DocumentCentre?id=1&amp;amp;nameCenter=2&amp;amp;productId=520117912052801536"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Launch ComfyUI by running &lt;code&gt;python main.py&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;&lt;a href="https://github.com/Comfy-Org/ComfyUI-Manager/tree/manager-v4"&gt;ComfyUI-Manager&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;ComfyUI-Manager&lt;/strong&gt; is an extension that allows you to easily install, update, and manage custom nodes for ComfyUI.&lt;/p&gt; 
&lt;h3&gt;Setup&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Install the manager dependencies:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r manager_requirements.txt
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Enable the manager with the &lt;code&gt;--enable-manager&lt;/code&gt; flag when running ComfyUI:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;python main.py --enable-manager
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Command Line Options&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Flag&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--enable-manager&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Enable ComfyUI-Manager&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--enable-manager-legacy-ui&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Use the legacy manager UI instead of the new UI (requires &lt;code&gt;--enable-manager&lt;/code&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--disable-manager-ui&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Disable the manager UI and endpoints while keeping background features like security checks and scheduled installation completion (requires &lt;code&gt;--enable-manager&lt;/code&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;Running&lt;/h1&gt; 
&lt;p&gt;&lt;code&gt;python main.py&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;For AMD cards not officially supported by ROCm&lt;/h3&gt; 
&lt;p&gt;Try running it with this command if you have issues:&lt;/p&gt; 
&lt;p&gt;For 6700, 6600 and maybe other RDNA2 or older: &lt;code&gt;HSA_OVERRIDE_GFX_VERSION=10.3.0 python main.py&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;For AMD 7600 and maybe other RDNA3 cards: &lt;code&gt;HSA_OVERRIDE_GFX_VERSION=11.0.0 python main.py&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;AMD ROCm Tips&lt;/h3&gt; 
&lt;p&gt;You can enable experimental memory efficient attention on recent pytorch in ComfyUI on some AMD GPUs using this command, it should already be enabled by default on RDNA3. If this improves speed for you on latest pytorch on your GPU please report it so that I can enable it by default.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL=1 python main.py --use-pytorch-cross-attention&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;You can also try setting this env variable &lt;code&gt;PYTORCH_TUNABLEOP_ENABLED=1&lt;/code&gt; which might speed things up at the cost of a very slow initial run.&lt;/p&gt; 
&lt;h1&gt;Notes&lt;/h1&gt; 
&lt;p&gt;Only parts of the graph that have an output with all the correct inputs will be executed.&lt;/p&gt; 
&lt;p&gt;Only parts of the graph that change from each execution to the next will be executed, if you submit the same graph twice only the first will be executed. If you change the last part of the graph only the part you changed and the part that depends on it will be executed.&lt;/p&gt; 
&lt;p&gt;Dragging a generated png on the webpage or loading one will give you the full workflow including seeds that were used to create it.&lt;/p&gt; 
&lt;p&gt;You can use () to change emphasis of a word or phrase like: (good code:1.2) or (bad code:0.8). The default emphasis for () is 1.1. To use () characters in your actual prompt escape them like \( or \).&lt;/p&gt; 
&lt;p&gt;You can use {day|night}, for wildcard/dynamic prompts. With this syntax "{wild|card|test}" will be randomly replaced by either "wild", "card" or "test" by the frontend every time you queue the prompt. To use {} characters in your actual prompt escape them like: \{ or \}.&lt;/p&gt; 
&lt;p&gt;Dynamic prompts also support C-style comments, like &lt;code&gt;// comment&lt;/code&gt; or &lt;code&gt;/* comment */&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;To use a textual inversion concepts/embeddings in a text prompt put them in the models/embeddings directory and use them in the CLIPTextEncode node like this (you can omit the .pt extension):&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;embedding:embedding_filename.pt&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;How to show high-quality previews?&lt;/h2&gt; 
&lt;p&gt;Use &lt;code&gt;--preview-method auto&lt;/code&gt; to enable previews.&lt;/p&gt; 
&lt;p&gt;The default installation includes a fast latent preview method that's low-resolution. To enable higher-quality previews with &lt;a href="https://github.com/madebyollin/taesd"&gt;TAESD&lt;/a&gt;, download the &lt;a href="https://github.com/madebyollin/taesd/"&gt;taesd_decoder.pth, taesdxl_decoder.pth, taesd3_decoder.pth and taef1_decoder.pth&lt;/a&gt; and place them in the &lt;code&gt;models/vae_approx&lt;/code&gt; folder. Once they're installed, restart ComfyUI and launch it with &lt;code&gt;--preview-method taesd&lt;/code&gt; to enable high-quality previews.&lt;/p&gt; 
&lt;h2&gt;How to use TLS/SSL?&lt;/h2&gt; 
&lt;p&gt;Generate a self-signed certificate (not appropriate for shared/production use) and key by running the command: &lt;code&gt;openssl req -x509 -newkey rsa:4096 -keyout key.pem -out cert.pem -sha256 -days 3650 -nodes -subj "/C=XX/ST=StateName/L=CityName/O=CompanyName/OU=CompanySectionName/CN=CommonNameOrHostname"&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Use &lt;code&gt;--tls-keyfile key.pem --tls-certfile cert.pem&lt;/code&gt; to enable TLS/SSL, the app will now be accessible with &lt;code&gt;https://...&lt;/code&gt; instead of &lt;code&gt;http://...&lt;/code&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Note: Windows users can use &lt;a href="https://github.com/alexisrolland/docker-openssl"&gt;alexisrolland/docker-openssl&lt;/a&gt; or one of the &lt;a href="https://wiki.openssl.org/index.php/Binaries"&gt;3rd party binary distributions&lt;/a&gt; to run the command example above. &lt;br /&gt;&lt;br /&gt;If you use a container, note that the volume mount &lt;code&gt;-v&lt;/code&gt; can be a relative path so &lt;code&gt;... -v ".\:/openssl-certs" ...&lt;/code&gt; would create the key &amp;amp; cert files in the current directory of your command prompt or powershell terminal.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Support and dev channel&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://comfy.org/discord"&gt;Discord&lt;/a&gt;: Try the #help or #feedback channels.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://app.element.io/#/room/%23comfyui_space%3Amatrix.org"&gt;Matrix space: #comfyui_space:matrix.org&lt;/a&gt; (it's like discord but open source).&lt;/p&gt; 
&lt;p&gt;See also: &lt;a href="https://www.comfy.org/"&gt;https://www.comfy.org/&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Frontend Development&lt;/h2&gt; 
&lt;p&gt;As of August 15, 2024, we have transitioned to a new frontend, which is now hosted in a separate repository: &lt;a href="https://github.com/Comfy-Org/ComfyUI_frontend"&gt;ComfyUI Frontend&lt;/a&gt;. This repository now hosts the compiled JS (from TS/Vue) under the &lt;code&gt;web/&lt;/code&gt; directory.&lt;/p&gt; 
&lt;h3&gt;Reporting Issues and Requesting Features&lt;/h3&gt; 
&lt;p&gt;For any bugs, issues, or feature requests related to the frontend, please use the &lt;a href="https://github.com/Comfy-Org/ComfyUI_frontend"&gt;ComfyUI Frontend repository&lt;/a&gt;. This will help us manage and address frontend-specific concerns more efficiently.&lt;/p&gt; 
&lt;h3&gt;Using the Latest Frontend&lt;/h3&gt; 
&lt;p&gt;The new frontend is now the default for ComfyUI. However, please note:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;The frontend in the main ComfyUI repository is updated fortnightly.&lt;/li&gt; 
 &lt;li&gt;Daily releases are available in the separate frontend repository.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;To use the most up-to-date frontend version:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;For the latest daily release, launch ComfyUI with this command line argument:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;--front-end-version Comfy-Org/ComfyUI_frontend@latest
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;For a specific version, replace &lt;code&gt;latest&lt;/code&gt; with the desired version number:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;--front-end-version Comfy-Org/ComfyUI_frontend@1.2.2
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;This approach allows you to easily switch between the stable fortnightly release and the cutting-edge daily updates, or even specific versions for testing purposes.&lt;/p&gt; 
&lt;h3&gt;Accessing the Legacy Frontend&lt;/h3&gt; 
&lt;p&gt;If you need to use the legacy frontend for any reason, you can access it using the following command line argument:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;--front-end-version Comfy-Org/ComfyUI_legacy_frontend@latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will use a snapshot of the legacy frontend preserved in the &lt;a href="https://github.com/Comfy-Org/ComfyUI_legacy_frontend"&gt;ComfyUI Legacy Frontend repository&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;QA&lt;/h1&gt; 
&lt;h3&gt;Which GPU should I buy for this?&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/comfyanonymous/ComfyUI/wiki/Which-GPU-should-I-buy-for-ComfyUI"&gt;See this page for some recommendations&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>DrewThomasson/ebook2audiobook</title>
      <link>https://github.com/DrewThomasson/ebook2audiobook</link>
      <description>&lt;p&gt;Generate audiobooks from e-books, voice cloning &amp; 1158+ languages!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;üìö ebook2audiobook&lt;/h1&gt; 
&lt;p&gt;CPU/GPU Converter from eBooks to audiobooks with chapters and metadata&lt;br /&gt; using XTTSv2, Bark, Vits, Fairseq, YourTTS, Tacotron2 and more. Supports voice cloning and 1158 languages!&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] &lt;strong&gt;This tool is intended for use with non-DRM, legally acquired eBooks only.&lt;/strong&gt; &lt;br /&gt; The authors are not responsible for any misuse of this software or any resulting legal consequences. &lt;br /&gt; Use this tool responsibly and in accordance with all applicable laws.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;a href="https://discord.gg/63Tv3F65k6"&gt;&lt;img src="https://dcbadge.limes.pink/api/server/https://discord.gg/63Tv3F65k6" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Thanks to support ebook2audiobook developers!&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://ko-fi.com/athomasson2"&gt;&lt;img src="https://img.shields.io/badge/Ko--fi-F16061?style=for-the-badge&amp;amp;logo=ko-fi&amp;amp;logoColor=white" alt="Ko-Fi" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Run locally&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#launching-gradio-web-interface"&gt;&lt;img src="https://img.shields.io/badge/Quick%20Start-blue?style=for-the-badge" alt="Quick Start" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/DrewThomasson/ebook2audiobook/actions/workflows/Docker-Build.yml"&gt;&lt;img src="https://github.com/DrewThomasson/ebook2audiobook/actions/workflows/Docker-Build.yml/badge.svg?sanitize=true" alt="Docker Build" /&gt;&lt;/a&gt; &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/releases/latest"&gt;&lt;img src="https://img.shields.io/badge/Download-Now-blue.svg?sanitize=true" alt="Download" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;a href="https://github.com/DrewThomasson/ebook2audiobook"&gt; &lt;img src="https://img.shields.io/badge/Platform-mac%20|%20linux%20|%20windows-lightgrey" alt="Platform" /&gt; &lt;/a&gt;
&lt;a href="https://hub.docker.com/r/athomasson2/ebook2audiobook"&gt; &lt;img alt="Docker Pull Count" src="https://img.shields.io/docker/pulls/athomasson2/ebook2audiobook.svg?sanitize=true" /&gt; &lt;/a&gt; 
&lt;h3&gt;Run Remotely&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/spaces/drewThomasson/ebook2audiobook"&gt;&lt;img src="https://img.shields.io/badge/Hugging%20Face-Spaces-yellow?style=flat&amp;amp;logo=huggingface" alt="Hugging Face" /&gt;&lt;/a&gt; &lt;a href="https://colab.research.google.com/github/DrewThomasson/ebook2audiobook/blob/main/Notebooks/colab_ebook2audiobook.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Free Google Colab" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Rihcus/ebook2audiobookXTTS/raw/main/Notebooks/kaggle-ebook2audiobook.ipynb"&gt;&lt;img src="https://img.shields.io/badge/Kaggle-035a7d?style=flat&amp;amp;logo=kaggle&amp;amp;logoColor=white" alt="Kaggle" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;GUI Interface&lt;/h4&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/assets/demo_web_gui.gif" alt="demo_web_gui" /&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click to see images of Web GUI&lt;/summary&gt; 
 &lt;img width="1728" alt="GUI Screen 1" src="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/assets/gui_1.png" /&gt; 
 &lt;img width="1728" alt="GUI Screen 2" src="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/assets/gui_2.png" /&gt; 
 &lt;img width="1728" alt="GUI Screen 3" src="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/assets/gui_3.png" /&gt; 
&lt;/details&gt; 
&lt;h2&gt;Demos&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;New Default Voice Demo&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/750035dc-e355-46f1-9286-05c1d9e88cea"&gt;https://github.com/user-attachments/assets/750035dc-e355-46f1-9286-05c1d9e88cea&lt;/a&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;More Demos&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;ASMR Voice&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/68eee9a1-6f71-4903-aacd-47397e47e422"&gt;https://github.com/user-attachments/assets/68eee9a1-6f71-4903-aacd-47397e47e422&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Rainy Day Voice&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/d25034d9-c77f-43a9-8f14-0d167172b080"&gt;https://github.com/user-attachments/assets/d25034d9-c77f-43a9-8f14-0d167172b080&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Scarlett Voice&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/b12009ee-ec0d-45ce-a1ef-b3a52b9f8693"&gt;https://github.com/user-attachments/assets/b12009ee-ec0d-45ce-a1ef-b3a52b9f8693&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;David Attenborough Voice&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/81c4baad-117e-4db5-ac86-efc2b7fea921"&gt;https://github.com/user-attachments/assets/81c4baad-117e-4db5-ac86-efc2b7fea921&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://github.com/DrewThomasson/VoxNovel/raw/dc5197dff97252fa44c391dc0596902d71278a88/readme_files/example_in_app.jpeg" alt="Example" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;README.md&lt;/h2&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#-ebook2audiobook"&gt;ebook2audiobook&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#features"&gt;Features&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#gui-interface"&gt;GUI Interface&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#demos"&gt;Demos&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#supported-languages"&gt;Supported Languages&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#hardware-requirements"&gt;Minimum Requirements&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#launching-gradio-web-interface"&gt;Usage&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#launching-gradio-web-interface"&gt;Run Locally&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#launching-gradio-web-interface"&gt;Launching Gradio Web Interface&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#basic--usage"&gt;Basic Headless Usage&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#example-of-custom-model-zip-upload"&gt;Headless Custom XTTS Model Usage&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#help-command-output"&gt;Help command output&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#run-remotely"&gt;Run Remotely&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#docker"&gt;Docker&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#steps-to-run"&gt;Steps to Run&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#common-docker-issues"&gt;Common Docker Issues&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#fine-tuned-tts-models"&gt;Fine Tuned TTS models&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#fine-tuned-tts-collection"&gt;Collection of Fine-Tuned TTS Models&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#fine-tune-your-own-xttsv2-model"&gt;Train XTTSv2&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#supported-ebook-formats"&gt;Supported eBook Formats&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#output-formats"&gt;Output Formats&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#updating-to-latest-version"&gt;Updating to Latest Version&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#reverting-to-older-versions"&gt;Revert to older Version&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#common-issues"&gt;Common Issues&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#special-thanks"&gt;Special Thanks&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#table-of-contents"&gt;Table of Contents&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìö Splits eBook into chapters for organized audio.&lt;/li&gt; 
 &lt;li&gt;üéôÔ∏è High-quality text-to-speech with &lt;a href="https://huggingface.co/coqui/XTTS-v2"&gt;XTTSv2&lt;/a&gt;, &lt;a href="https://github.com/facebookresearch/fairseq/tree/main/examples/mms"&gt;Fairseq&lt;/a&gt; and much more.&lt;/li&gt; 
 &lt;li&gt;üó£Ô∏è Optional voice cloning with your own voice file.&lt;/li&gt; 
 &lt;li&gt;üó£Ô∏è Optional custom model with your own training model.&lt;/li&gt; 
 &lt;li&gt;üåç Supports 1158 languages. &lt;a href="https://dl.fbaipublicfiles.com/mms/tts/all-tts-languages.html"&gt;List of Supported languages&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üñ•Ô∏è Designed to run on 2GB RAM 1GB VRAM Min.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Supported Languages&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;&lt;strong&gt;Arabic (ar)&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;strong&gt;Chinese (zh)&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;strong&gt;English (en)&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;strong&gt;Spanish (es)&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;French (fr)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;German (de)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Italian (it)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Portuguese (pt)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Polish (pl)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Turkish (tr)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Russian (ru)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Dutch (nl)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Czech (cs)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Japanese (ja)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Hindi (hi)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Bengali (bn)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Hungarian (hu)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Korean (ko)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Vietnamese (vi)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Swedish (sv)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Persian (fa)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Yoruba (yo)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Swahili (sw)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Indonesian (id)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Slovak (sk)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Croatian (hr)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Tamil (ta)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Danish (da)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://dl.fbaipublicfiles.com/mms/tts/all-tts-languages.html"&gt;&lt;strong&gt;+1130 languages and dialects here&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Hardware Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;2GB RAM min, 8GB recommended.&lt;/li&gt; 
 &lt;li&gt;1GB VRAM min, 4GB recommended.&lt;/li&gt; 
 &lt;li&gt;Virtualization enabled if running on windows (Docker only).&lt;/li&gt; 
 &lt;li&gt;CPU (intel, AMD, ARM)*.&lt;/li&gt; 
 &lt;li&gt;GPU (CUDA, ROCm, XPU).&lt;/li&gt; 
 &lt;li&gt;MPS (Apple Silicon CPU).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;*&lt;i&gt; Modern TTS engines are very slow on CPU&lt;/i&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] &lt;strong&gt;Before to post an install or bug issue search carefully to the opened and closed issues TAB&lt;br /&gt; to be sure your issue does not exist already.&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] &lt;strong&gt;EPUB format lacks any standard structure like what is a chapter, paragraph, preface etc.&lt;br /&gt; So you should first remove manually any text you don't want to be converted in audio.&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Instructions&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Clone repo&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/DrewThomasson/ebook2audiobook.git
cd ebook2audiobook
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install / Run ebook2audiobook&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Linux/MacOS&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;./ebook2audiobook.sh  # Run launch script
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;i&gt;Note for MacOS users: homebrew is installed to install missing programs.&lt;/i&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Mac Launcher&lt;/strong&gt;&lt;br /&gt; Double click &lt;code&gt;Mac Ebook2Audiobook Launcher.command&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Windows&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;ebook2audiobook.cmd
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;or Double click &lt;code&gt;ebook2audiobook.cmd&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;i&gt;Note for Windows users: scoop is installed to install missing programs without administrator privileges.&lt;/i&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Open the Web App&lt;/strong&gt;: Click the URL provided in the terminal to access the web app and convert eBooks. &lt;code&gt;http://localhost:7860/&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;For Public Link&lt;/strong&gt;: &lt;code&gt;./ebook2audiobook.sh --share&lt;/code&gt; (Linux/MacOS) &lt;code&gt;ebook2audiobook.cmd --share&lt;/code&gt; (Windows) &lt;code&gt;python app.py --share&lt;/code&gt; (all OS)&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] &lt;strong&gt;If the script is stopped and run again, you need to refresh your gradio GUI interface&lt;br /&gt; to let the web page reconnect to the new connection socket.&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Basic Usage&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Linux/MacOS&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;./ebook2audiobook.sh --headless --ebook &amp;lt;path_to_ebook_file&amp;gt; --voice [path_to_voice_file] --language [language_code]
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Windows&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;ebook2audiobook.cmd --headless --ebook &amp;lt;path_to_ebook_file&amp;gt; --voice [path_to_voice_file] --language [language_code]
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;[--ebook]&lt;/strong&gt;: Path to your eBook file&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;[--voice]&lt;/strong&gt;: Voice cloning file path (optional)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;[--language]&lt;/strong&gt;: Language code in ISO-639-3 (i.e.: ita for italian, eng for english, deu for german...).&lt;br /&gt; Default language is eng and --language is optional for default language set in ./lib/lang.py.&lt;br /&gt; The ISO-639-1 2 letters codes are also supported.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Example of Custom Model Zip Upload&lt;/h3&gt; 
&lt;p&gt;(must be a .zip file containing the mandatory model files. Example for XTTSv2: config.json, model.pth, vocab.json and ref.wav)&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Linux/MacOS&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;./ebook2audiobook.sh --headless --ebook &amp;lt;ebook_file_path&amp;gt; --language &amp;lt;language&amp;gt; --custom_model &amp;lt;custom_model_path&amp;gt;
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Windows&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;ebook2audiobook.cmd --headless --ebook &amp;lt;ebook_file_path&amp;gt; --language &amp;lt;language&amp;gt; --custom_model &amp;lt;custom_model_path&amp;gt;
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;i&gt;Note: the ref.wav of your custom model is always the voice selected for the conversion&lt;/i&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&amp;lt;custom_model_path&amp;gt;&lt;/strong&gt;: Path to &lt;code&gt;model_name.zip&lt;/code&gt; file, which must contain (according to the tts engine) all the mandatory files&lt;br /&gt; (see ./lib/models.py).&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;For Detailed Guide with list of all Parameters to use&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Linux/MacOS&lt;/strong&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;./ebook2audiobook.sh --help
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;ebook2audiobook.cmd --help
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Or for all OS&lt;/strong&gt; &lt;code&gt;python app.py --help &lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a id="help-command-output"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;usage: app.py [-h] [--session SESSION] [--share] [--headless] [--ebook EBOOK] [--ebooks_dir EBOOKS_DIR]
              [--language LANGUAGE] [--voice VOICE] [--device {CPU,CUDA,MPS,ROCM,XPU,JETSON}]
              [--tts_engine {XTTSv2,BARK,VITS,FAIRSEQ,TACOTRON2,YOURTTS,xtts,bark,vits,fairseq,tacotron,yourtts}]
              [--custom_model CUSTOM_MODEL] [--fine_tuned FINE_TUNED] [--output_format OUTPUT_FORMAT]
              [--output_channel OUTPUT_CHANNEL] [--temperature TEMPERATURE] [--length_penalty LENGTH_PENALTY]
              [--num_beams NUM_BEAMS] [--repetition_penalty REPETITION_PENALTY] [--top_k TOP_K] [--top_p TOP_P]
              [--speed SPEED] [--enable_text_splitting] [--text_temp TEXT_TEMP] [--waveform_temp WAVEFORM_TEMP]
              [--output_dir OUTPUT_DIR] [--version]

Convert eBooks to Audiobooks using a Text-to-Speech model. You can either launch the Gradio interface or run the script in headless mode for direct conversion.

options:
  -h, --help            show this help message and exit
  --session SESSION     Session to resume the conversion in case of interruption, crash,
                            or reuse of custom models and custom cloning voices.

**** The following options are for all modes:
  Optional

**** The following option are for gradio/gui mode only:
  Optional

  --share               Enable a public shareable Gradio link.

**** The following options are for --headless mode only:
  --headless            Run the script in headless mode
  --ebook EBOOK         Path to the ebook file for conversion. Cannot be used when --ebooks_dir is present.
  --ebooks_dir EBOOKS_DIR
                        Relative or absolute path of the directory containing the files to convert.
                            Cannot be used when --ebook is present.
  --language LANGUAGE   Language of the e-book. Default language is set
                            in ./lib/lang.py sed as default if not present. All compatible language codes are in ./lib/lang.py

optional parameters:
  --voice VOICE         (Optional) Path to the voice cloning file for TTS engine.
                            Uses the default voice if not present.
  --device {CPU,CUDA,MPS,ROCM,XPU,JETSON}
                        (Optional) Processor unit type for the conversion.
                            Default is set in ./lib/conf.py if not present. Fall back to CPU if CUDA or MPS is not available.
  --tts_engine {XTTSv2,BARK,VITS,FAIRSEQ,TACOTRON2,YOURTTS,xtts,bark,vits,fairseq,tacotron,yourtts}
                        (Optional) Preferred TTS engine (available are: ['XTTSv2', 'BARK', 'VITS', 'FAIRSEQ', 'TACOTRON2', 'YOURTTS', 'xtts', 'bark', 'vits', 'fairseq', 'tacotron', 'yourtts'].
                            Default depends on the selected language. The tts engine should be compatible with the chosen language
  --custom_model CUSTOM_MODEL
                        (Optional) Path to the custom model zip file cntaining mandatory model files.
                            Please refer to ./lib/models.py
  --fine_tuned FINE_TUNED
                        (Optional) Fine tuned model path. Default is builtin model.
  --output_format OUTPUT_FORMAT
                        (Optional) Output audio format. Default is m4b set in ./lib/conf.py
  --output_channel OUTPUT_CHANNEL
                        (Optional) Output audio channel. Default is mono set in ./lib/conf.py
  --temperature TEMPERATURE
                        (xtts only, optional) Temperature for the model.
                            Default to config.json model. Higher temperatures lead to more creative outputs.
  --length_penalty LENGTH_PENALTY
                        (xtts only, optional) A length penalty applied to the autoregressive decoder.
                            Default to config.json model. Not applied to custom models.
  --num_beams NUM_BEAMS
                        (xtts only, optional) Controls how many alternative sequences the model explores. Must be equal or greater than length penalty.
                            Default to config.json model.
  --repetition_penalty REPETITION_PENALTY
                        (xtts only, optional) A penalty that prevents the autoregressive decoder from repeating itself.
                            Default to config.json model.
  --top_k TOP_K         (xtts only, optional) Top-k sampling.
                            Lower values mean more likely outputs and increased audio generation speed.
                            Default to config.json model.
  --top_p TOP_P         (xtts only, optional) Top-p sampling.
                            Lower values mean more likely outputs and increased audio generation speed. Default to config.json model.
  --speed SPEED         (xtts only, optional) Speed factor for the speech generation.
                            Default to config.json model.
  --enable_text_splitting
                        (xtts only, optional) Enable TTS text splitting. This option is known to not be very efficient.
                            Default to config.json model.
  --text_temp TEXT_TEMP
                        (bark only, optional) Text Temperature for the model.
                            Default to config.json model.
  --waveform_temp WAVEFORM_TEMP
                        (bark only, optional) Waveform Temperature for the model.
                            Default to config.json model.
  --output_dir OUTPUT_DIR
                        (Optional) Path to the output directory. Default is set in ./lib/conf.py
  --version             Show the version of the script and exit

Example usage:
Windows:
    Gradio/GUI:
    ebook2audiobook.cmd
    Headless mode:
    ebook2audiobook.cmd --headless --ebook '/path/to/file' --language eng
Linux/Mac:
    Gradio/GUI:
    ./ebook2audiobook.sh
    Headless mode:
    ./ebook2audiobook.sh --headless --ebook '/path/to/file' --language eng

Docker build image:
    Windows:
    ebook2audiobook.cmd --script_mode build_docker
    Linux/Mac
    ./ebook2audiobook.sh --script_mode build_docker
Docker run image:
    Gradio/GUI:
        CPU:
        docker run --rm -it -p 7860:7860 ebook2audiobook:cpu
        CUDA:
        docker run --gpus all --rm -it -p 7860:7860 ebook2audiobook:cu[118/121/128 etc..]
        ROCM:
        docker run --device=/dev/kfd --device=/dev/dri --rm -it -p 7860:7860 ebook2audiobook:rocm[5.5/6.1/6.4 etc..]
        XPU:
        docker run --device=/dev/dri --rm -it -p 7860:7860 ebook2audiobook:xpu
        JETSON:
        docker run --runtime nvidia  --rm -it -p 7860:7860 ebook2audiobook:jetson[51/60/61 etc...]
    Headless mode:
        CPU:
        docker run --rm -it -v "/my/real/ebooks/folder/absolute/path:/app/ebooks" -v "/my/real/output/folder/absolute/path:/app/audiobooks" -p 7860:7860 ebook2audiobook:cpu --headless --ebook "/app/ebooks/myfile.pdf" [--voice /app/my/voicepath/voice.mp3 etc..]
        CUDA:
        docker run --gpus all --rm -it -v "/my/real/ebooks/folder/absolute/path:/app/ebooks" -v "/my/real/output/folder/absolute/path:/app/audiobooks" -p 7860:7860 ebook2audiobook:cu[118/121/128 etc..] --headless --ebook "/app/ebooks/myfile.pdf" [--voice /app/my/voicepath/voice.mp3 etc..]
        ROCM:
        docker run --device=/dev/kfd --device=/dev/dri --rm -it -v "/my/real/ebooks/folder/absolute/path:/app/ebooks" -v "/my/real/output/folder/absolute/path:/app/audiobooks" -p 7860:7860 ebook2audiobook:rocm[5.5/6.1/6.4 etc..] --headless --ebook "/app/ebooks/myfile.pdf" [--voice /app/my/voicepath/voice.mp3 etc..]
        XPU:
        docker run --device=/dev/dri --rm -it -v "/my/real/ebooks/folder/absolute/path:/app/ebooks" -v "/my/real/output/folder/absolute/path:/app/audiobooks" -p 7860:7860 ebook2audiobook:xpu --headless --ebook "/app/ebooks/myfile.pdf" [--voice /app/my/voicepath/voice.mp3 etc..]
        JETSON:
        docker run --runtime nvidia --rm -it -v "/my/real/ebooks/folder/absolute/path:/app/ebooks" -v "/my/real/output/folder/absolute/path:/app/audiobooks" -p 7860:7860 ebook2audiobook:jetson[51/60/61 etc...] --headless --ebook "/app/ebooks/myfile.pdf" [--voice /app/my/voicepath/voice.mp3 etc..]

    Docker Compose (i.e. for cuda 11.8, add --build to rebuild):
        DEVICE_TAG=cu118 docker compose up -d

    Podman Compose (i.e. for cuda 12.4, add --build to rebuild):
        DEVICE_TAG=cu124 podman-compose up -d

    * MPS is not exposed in docker so CPU must be used.

Tip: to add of silence (random duration between 1.0 and 1.8 seconds) into your text just use "###" or "[pause]".

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;NOTE: in gradio/gui mode, to cancel a running conversion, just click on the [X] from the ebook upload component.&lt;/p&gt; 
&lt;p&gt;TIP: if it needs some more pauses, just add '###' or '[pause]' between the words you wish more pause. one [pause] is a random between 0.8 to 1.6 seconds&lt;/p&gt; 
&lt;h3&gt;Docker&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Clone the Repository&lt;/strong&gt;:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;   git clone https://github.com/DrewThomasson/ebook2audiobook.git
   cd ebook2audiobook
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;&lt;strong&gt;Build the container&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;   # Windows
   ebook2audiobook.cmd --script_mode build_docker

   # Linux/MacOS
   ./ebook2audiobook.sh --script_mode build_docker 
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="4"&gt; 
 &lt;li&gt;&lt;strong&gt;Run the Container:&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;	# Gradio/GUI:

	# CPU:
		docker run --rm -it -p 7860:7860 ebook2audiobook:cpu
	# CUDA:
		docker run --gpus all --rm -it -p 7860:7860 ebook2audiobook:cu[118/121/128 etc..]
	# ROCM:
		docker run --device=/dev/kfd --device=/dev/dri --rm -it -p 7860:7860 ebook2audiobook:rocm[5.5/6.1/6.4 etc..]
	# XPU:
		docker run --device=/dev/dri --rm -it -p 7860:7860 ebook2audiobook:xpu
	# JETSON:
		docker run --runtime nvidia  --rm -it -p 7860:7860 ebook2audiobook:jetson[51/60/61 etc...]
	
	# Headless mode examples:
	
	# CPU:
		docker run --rm -it -v "/my/real/ebooks/folder/absolute/path:/app/ebooks" -v "/my/real/output/folder/absolute/path:/app/audiobooks" -p 7860:7860 ebook2audiobook:cpu --headless --ebook "/app/ebooks/myfile.pdf" [--voice /app/my/voicepath/voice.mp3 etc..]
	# CUDA:
		docker run --gpus all --rm -it -v "/my/real/ebooks/folder/absolute/path:/app/ebooks" -v "/my/real/output/folder/absolute/path:/app/audiobooks" -p 7860:7860 ebook2audiobook:cu[118/121/128 etc..] --headless --ebook "/app/ebooks/myfile.pdf" [--voice /app/my/voicepath/voice.mp3 etc..]
	# ROCM:
		docker run --device=/dev/kfd --device=/dev/dri --rm -it -v "/my/real/ebooks/folder/absolute/path:/app/ebooks" -v "/my/real/output/folder/absolute/path:/app/audiobooks" -p 7860:7860 ebook2audiobook:rocm[5.5/6.1/6.4 etc..] --headless --ebook "/app/ebooks/myfile.pdf" [--voice /app/my/voicepath/voice.mp3 etc..]
	# XPU:
		docker run --device=/dev/dri --rm -it -v "/my/real/ebooks/folder/absolute/path:/app/ebooks" -v "/my/real/output/folder/absolute/path:/app/audiobooks" -p 7860:7860 ebook2audiobook:xpu --headless --ebook "/app/ebooks/myfile.pdf" [--voice /app/my/voicepath/voice.mp3 etc..]
	# JETSON:
		docker run --runtime nvidia --rm -it -v "/my/real/ebooks/folder/absolute/path:/app/ebooks" -v "/my/real/output/folder/absolute/path:/app/audiobooks" -p 7860:7860 ebook2audiobook:jetson[51/60/61 etc...] --headless --ebook "/app/ebooks/myfile.pdf" [--voice /app/my/voicepath/voice.mp3 etc..]

    # Docker Compose (example for cuda 12.9)
    docker-compose up -d
    DEVICE_TAG=cu128 docker compose up -d
    # To stop -&amp;gt; docker-compose down

    # Podman Compose (example for cuda 12.8)
    podman compose -f podman-compose.yml up
    DEVICE_TAG=cu128 podman-compose up -d
    # To stop -&amp;gt; podman compose -f podman-compose.yml down
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;NOTE: MPS is not exposed in docker so CPU must be used&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Common Docker Issues&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;My NVIDIA GPU isn't being detected?? -&amp;gt; &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/wiki/GPU-ISSUES"&gt;GPU ISSUES Wiki Page&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Fine Tuned TTS models&lt;/h2&gt; 
&lt;h4&gt;Fine Tune your own XTTSv2 model&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/spaces/drewThomasson/xtts-finetune-webui-gpu"&gt;&lt;img src="https://img.shields.io/badge/Hugging%20Face-Spaces-yellow?style=flat&amp;amp;logo=huggingface" alt="Hugging Face" /&gt;&lt;/a&gt; &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/raw/v25/Notebooks/finetune/xtts/kaggle-xtts-finetune-webui-gradio-gui.ipynb"&gt;&lt;img src="https://img.shields.io/badge/Kaggle-035a7d?style=flat&amp;amp;logo=kaggle&amp;amp;logoColor=white" alt="Kaggle" /&gt;&lt;/a&gt; &lt;a href="https://colab.research.google.com/github/DrewThomasson/ebook2audiobook/blob/v25/Notebooks/finetune/xtts/colab_xtts_finetune_webui.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;De-noise training data&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/spaces/drewThomasson/DeepFilterNet2_no_limit"&gt;&lt;img src="https://img.shields.io/badge/Hugging%20Face-Spaces-yellow?style=flat&amp;amp;logo=huggingface" alt="Hugging Face" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Rikorose/DeepFilterNet"&gt;&lt;img src="https://img.shields.io/badge/DeepFilterNet-181717?logo=github" alt="GitHub Repo" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Fine Tuned TTS Collection&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/drewThomasson/fineTunedTTSModels/tree/main"&gt;&lt;img src="https://img.shields.io/badge/Hugging%20Face-Models-yellow?style=flat&amp;amp;logo=huggingface" alt="Hugging Face" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;For an XTTSv2 custom model a ref audio clip of the voice reference is mandatory:&lt;/p&gt; 
&lt;h2&gt;Supported eBook Formats&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;.epub&lt;/code&gt;, &lt;code&gt;.pdf&lt;/code&gt;, &lt;code&gt;.mobi&lt;/code&gt;, &lt;code&gt;.txt&lt;/code&gt;, &lt;code&gt;.html&lt;/code&gt;, &lt;code&gt;.rtf&lt;/code&gt;, &lt;code&gt;.chm&lt;/code&gt;, &lt;code&gt;.lit&lt;/code&gt;, &lt;code&gt;.pdb&lt;/code&gt;, &lt;code&gt;.fb2&lt;/code&gt;, &lt;code&gt;.odt&lt;/code&gt;, &lt;code&gt;.cbr&lt;/code&gt;, &lt;code&gt;.cbz&lt;/code&gt;, &lt;code&gt;.prc&lt;/code&gt;, &lt;code&gt;.lrf&lt;/code&gt;, &lt;code&gt;.pml&lt;/code&gt;, &lt;code&gt;.snb&lt;/code&gt;, &lt;code&gt;.cbc&lt;/code&gt;, &lt;code&gt;.rb&lt;/code&gt;, &lt;code&gt;.tcr&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Best results&lt;/strong&gt;: &lt;code&gt;.epub&lt;/code&gt; or &lt;code&gt;.mobi&lt;/code&gt; for automatic chapter detection&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Output and process Formats&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;.m4b&lt;/code&gt;, &lt;code&gt;.m4a&lt;/code&gt;, &lt;code&gt;.mp4&lt;/code&gt;, &lt;code&gt;.webm&lt;/code&gt;, &lt;code&gt;.mov&lt;/code&gt;, &lt;code&gt;.mp3&lt;/code&gt;, &lt;code&gt;.flac&lt;/code&gt;, &lt;code&gt;.wav&lt;/code&gt;, &lt;code&gt;.ogg&lt;/code&gt;, &lt;code&gt;.aac&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Process format can be changed in lib/conf.py&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Your own Ebook2Audiobook customization&lt;/h2&gt; 
&lt;p&gt;You are free to modify libs/conf.py to add or remove the settings you wish. If you plan to do it just make a copy of the original conf.py so on each ebook2audiobook update you will backup your modified conf.py and put back the original one. You must plan the same process for models.py. If you wish to make your own custom model as an official ebook2audiobook fine tuned model so please contact us and we'll ad it to the models.py list.&lt;/p&gt; 
&lt;h2&gt;Reverting to older Versions&lt;/h2&gt; 
&lt;p&gt;Releases can be found -&amp;gt; &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/releases"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git checkout tags/VERSION_NUM # Locally/Compose -&amp;gt; Example: git checkout tags/v25.7.7
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Common Issues:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;My NVIDIA/ROCm/XPU/MPS GPU isn't being detected?? -&amp;gt; &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/wiki/GPU-ISSUES"&gt;GPU ISSUES Wiki Page&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;CPU is slow (better on server smp CPU) while GPU can have almost real time conversion. &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/discussions/19#discussioncomment-10879846"&gt;Discussion about this&lt;/a&gt; For faster multilingual generation I would suggest my other &lt;a href="https://github.com/DrewThomasson/ebook2audiobookpiper-tts"&gt;project that uses piper-tts&lt;/a&gt; instead (It doesn't have zero-shot voice cloning though, and is Siri quality voices, but it is much faster on cpu).&lt;/li&gt; 
 &lt;li&gt;"I'm having dependency issues" - Just use the docker, its fully self contained and has a headless mode, add &lt;code&gt;--help&lt;/code&gt; parameter at the end of the docker run command for more information.&lt;/li&gt; 
 &lt;li&gt;"Im getting a truncated audio issue!" - PLEASE MAKE AN ISSUE OF THIS, we don't speak every language and need advise from users to fine tune the sentence splitting logic.üòä&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;What we need help with! üôå&lt;/h2&gt; 
&lt;h2&gt;&lt;a href="https://github.com/DrewThomasson/ebook2audiobook/issues/32"&gt;Full list of things can be found here&lt;/a&gt;&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Any help from people speaking any of the supported languages to help us improve the models&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!--
## Do you need to rent a GPU to boost service from us?
- A poll is open here https://github.com/DrewThomasson/ebook2audiobook/discussions/889
--&gt; 
&lt;h2&gt;Special Thanks&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Coqui TTS&lt;/strong&gt;: &lt;a href="https://github.com/idiap/coqui-ai-TTS"&gt;Coqui TTS GitHub&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Calibre&lt;/strong&gt;: &lt;a href="https://calibre-ebook.com"&gt;Calibre Website&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;FFmpeg&lt;/strong&gt;: &lt;a href="https://ffmpeg.org"&gt;FFmpeg Website&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/DrewThomasson/ebook2audiobook/issues/8"&gt;@shakenbake15 for better chapter saving method&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Genesis-Embodied-AI/Genesis</title>
      <link>https://github.com/Genesis-Embodied-AI/Genesis</link>
      <description>&lt;p&gt;A generative world for general-purpose robotics &amp; embodied AI learning.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/imgs/big_text.png" alt="Genesis" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/imgs/teaser.png" alt="Teaser" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://pypi.org/project/genesis-world/"&gt;&lt;img src="https://img.shields.io/pypi/v/genesis-world" alt="PyPI - Version" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/projects/genesis-world"&gt;&lt;img src="https://static.pepy.tech/badge/genesis-world" alt="PyPI Downloads" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Genesis-Embodied-AI/Genesis/issues"&gt;&lt;img src="https://img.shields.io/github/issues/Genesis-Embodied-AI/Genesis" alt="GitHub Issues" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Genesis-Embodied-AI/Genesis/discussions"&gt;&lt;img src="https://img.shields.io/github/discussions/Genesis-Embodied-AI/Genesis" alt="GitHub Discussions" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/nukCuhB47p"&gt;&lt;img src="https://img.shields.io/discord/1322086972302430269?logo=discord" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://drive.google.com/uc?export=view&amp;amp;id=1ZS9nnbQ-t1IwkzJlENBYqYIIOOZhXuBZ"&gt;&lt;img src="https://img.shields.io/badge/WeChat-07C160?style=for-the-badge&amp;amp;logo=wechat&amp;amp;logoColor=white" height="20" style="display:inline" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/README.md"&gt;&lt;img src="https://img.shields.io/badge/English-d9d9d9" alt="README in English" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/README_FR.md"&gt;&lt;img src="https://img.shields.io/badge/Francais-d9d9d9" alt="README en Fran√ßais" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/README_KR.md"&gt;&lt;img src="https://img.shields.io/badge/%ED%95%9C%EA%B5%AD%EC%96%B4-d9d9d9" alt="ÌïúÍµ≠Ïñ¥ README" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/README_CN.md"&gt;&lt;img src="https://img.shields.io/badge/%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87-d9d9d9" alt="ÁÆÄ‰Ωì‰∏≠ÊñáÁâàËá™Ëø∞Êñá‰ª∂" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/README_JA.md"&gt;&lt;img src="https://img.shields.io/badge/%E6%97%A5%E6%9C%AC%E8%AA%9E-d9d9d9" alt="Êó•Êú¨Ë™ûÁâà README" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Genesis&lt;/h1&gt; 
&lt;h2&gt;üî• News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;[2025-08-05] Released v0.3.0 üéä üéâ&lt;/li&gt; 
 &lt;li&gt;[2025-07-02] The development of Genesis is now officially supported by &lt;a href="https://genesis-ai.company/"&gt;Genesis AI&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[2025-01-09] We released a &lt;a href="https://github.com/zhouxian/genesis-speed-benchmark"&gt;detailed performance benchmarking and comparison report&lt;/a&gt; on Genesis, together with all the test scripts.&lt;/li&gt; 
 &lt;li&gt;[2025-01-08] Released v0.2.1 üéä üéâ&lt;/li&gt; 
 &lt;li&gt;[2025-01-08] Created &lt;a href="https://discord.gg/nukCuhB47p"&gt;Discord&lt;/a&gt; and &lt;a href="https://drive.google.com/uc?export=view&amp;amp;id=1ZS9nnbQ-t1IwkzJlENBYqYIIOOZhXuBZ"&gt;Wechat&lt;/a&gt; group.&lt;/li&gt; 
 &lt;li&gt;[2024-12-25] Added a &lt;a href="https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/#docker"&gt;docker&lt;/a&gt; including support for the ray-tracing renderer&lt;/li&gt; 
 &lt;li&gt;[2024-12-24] Added guidelines for &lt;a href="https://github.com/Genesis-Embodied-AI/Genesis/raw/main/.github/CONTRIBUTING.md"&gt;contributing to Genesis&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/#what-is-genesis"&gt;What is Genesis?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/#key-features"&gt;Key Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/#quick-installation"&gt;Quick Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/#docker"&gt;Docker&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/#documentation"&gt;Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/#contributing-to-genesis"&gt;Contributing to Genesis&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/#support"&gt;Support&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/#license-and-acknowledgments"&gt;License and Acknowledgments&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/#associated-papers"&gt;Associated Papers&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/#citation"&gt;Citation&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;What is Genesis?&lt;/h2&gt; 
&lt;p&gt;Genesis is a physics platform designed for general-purpose &lt;em&gt;Robotics/Embodied AI/Physical AI&lt;/em&gt; applications. It is simultaneously multiple things:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;A &lt;strong&gt;universal physics engine&lt;/strong&gt; re-built from the ground up, capable of simulating a wide range of materials and physical phenomena.&lt;/li&gt; 
 &lt;li&gt;A &lt;strong&gt;lightweight&lt;/strong&gt;, &lt;strong&gt;ultra-fast&lt;/strong&gt;, &lt;strong&gt;pythonic&lt;/strong&gt;, and &lt;strong&gt;user-friendly&lt;/strong&gt; robotics simulation platform.&lt;/li&gt; 
 &lt;li&gt;A powerful and fast &lt;strong&gt;photo-realistic rendering system&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;A &lt;strong&gt;generative data engine&lt;/strong&gt; that transforms user-prompted natural language description into various modalities of data.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Powered by a universal physics engine re-designed and re-built from the ground up, Genesis integrates various physics solvers and their coupling into a unified framework. This core physics engine is further enhanced by a generative agent framework that operates at an upper level, aiming towards fully automated data generation for robotics and beyond.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Currently, we are open-sourcing the &lt;em&gt;underlying physics engine&lt;/em&gt; and the &lt;em&gt;simulation platform&lt;/em&gt;. Our &lt;em&gt;generative framework&lt;/em&gt; is a modular system that incorporates many different generative modules, each handling a certain range of data modalities, routed by a high level agent. Some of the modules integrated existing papers and some are still under submission. Access to our generative feature will be gradually rolled out in the near future. If you are interested, feel free to explore more in the &lt;a href="https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/#associated-papers"&gt;paper list&lt;/a&gt; below.&lt;/p&gt; 
&lt;p&gt;Genesis aims to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Lower the barrier&lt;/strong&gt; to using physics simulations, making robotics research accessible to everyone. See our &lt;a href="https://genesis-world.readthedocs.io/en/latest/user_guide/overview/mission.html"&gt;mission statement&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Unify diverse physics solvers&lt;/strong&gt; into a single framework to recreate the physical world with the highest fidelity.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Automate data generation&lt;/strong&gt;, reducing human effort and letting the data flywheel spin on its own.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Project Page: &lt;a href="https://genesis-embodied-ai.github.io/"&gt;https://genesis-embodied-ai.github.io/&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Speed&lt;/strong&gt;: Over 43 million FPS when simulating a Franka robotic arm with a single RTX 4090 (430,000 times faster than real-time).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Cross-platform&lt;/strong&gt;: Runs on Linux, macOS, Windows, and supports multiple compute backends (CPU, Nvidia/AMD GPUs, Apple Metal).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Integration of diverse physics solvers&lt;/strong&gt;: Rigid body, MPM, SPH, FEM, PBD, Stable Fluid.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Wide range of material models&lt;/strong&gt;: Simulation and coupling of rigid bodies, liquids, gases, deformable objects, thin-shell objects, and granular materials.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Compatibility with various robots&lt;/strong&gt;: Robotic arms, legged robots, drones, &lt;em&gt;soft robots&lt;/em&gt;, and support for loading &lt;code&gt;MJCF (.xml)&lt;/code&gt;, &lt;code&gt;URDF&lt;/code&gt;, &lt;code&gt;.obj&lt;/code&gt;, &lt;code&gt;.glb&lt;/code&gt;, &lt;code&gt;.ply&lt;/code&gt;, &lt;code&gt;.stl&lt;/code&gt;, and more.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Photo-realistic rendering&lt;/strong&gt;: Native ray-tracing-based rendering.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Differentiability&lt;/strong&gt;: Genesis is designed to be fully differentiable. Currently, our MPM solver and Tool Solver support differentiability, with other solvers planned for future versions (starting with rigid &amp;amp; articulated body solver).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;User-friendliness&lt;/strong&gt;: Designed for simplicity, with intuitive installation and APIs.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quick Installation&lt;/h2&gt; 
&lt;p&gt;Install &lt;strong&gt;PyTorch&lt;/strong&gt; first following the &lt;a href="https://pytorch.org/get-started/locally/"&gt;official instructions&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Then, install Genesis via PyPI:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install genesis-world  # Requires Python&amp;gt;=3.10,&amp;lt;3.14;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For the latest version to date, make sure that &lt;code&gt;pip&lt;/code&gt; is up-to-date via &lt;code&gt;pip install --upgrade pip&lt;/code&gt;, then run command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install git+https://github.com/Genesis-Embodied-AI/Genesis.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note that the package must still be updated manually to sync with main branch.&lt;/p&gt; 
&lt;p&gt;Users seeking to contribute are encouraged to install Genesis in editable mode. First, make sure that &lt;code&gt;genesis-world&lt;/code&gt; has been uninstalled, then clone the repository and install locally:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/Genesis-Embodied-AI/Genesis.git
cd Genesis
pip install -e ".[dev]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It is recommended to systematically execute &lt;code&gt;pip install -e ".[dev]"&lt;/code&gt; after moving HEAD to make sure that all dependencies and entrypoints are up-to-date.&lt;/p&gt; 
&lt;h2&gt;Docker&lt;/h2&gt; 
&lt;p&gt;If you want to use Genesis from Docker, you can first build the Docker image as:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker build -t genesis -f docker/Dockerfile docker
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then you can run the examples inside the docker image (mounted to &lt;code&gt;/workspace/examples&lt;/code&gt;):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;xhost +local:root # Allow the container to access the display

docker run --gpus all --rm -it \
-e DISPLAY=$DISPLAY \
-e LOCAL_USER_ID="$(id -u)" \
-v /dev/dri:/dev/dri \
-v /tmp/.X11-unix/:/tmp/.X11-unix \
-v $(pwd):/workspace \
--name genesis genesis:latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;AMD users&lt;/h3&gt; 
&lt;p&gt;AMD users can use Genesis using the &lt;code&gt;docker/Dockerfile.amdgpu&lt;/code&gt; file, which is built by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;docker build -t genesis-amd -f docker/Dockerfile.amdgpu docker
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;and can then be used by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-xhost"&gt;docker run -it --network=host \
 --device=/dev/kfd \
 --device=/dev/dri \
 --group-add=video \
 --ipc=host \
 --cap-add=SYS_PTRACE \
 --security-opt seccomp=unconfined \
 --shm-size 8G \
 -v $PWD:/workspace \
 -e DISPLAY=$DISPLAY \
 genesis-amd
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The examples will be accessible from &lt;code&gt;/workspace/examples&lt;/code&gt;. Note: AMD users should use the vulkan backend. This means you will need to call &lt;code&gt;gs.init(vulkan)&lt;/code&gt; to initialise Genesis.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;Comprehensive documentation is available in &lt;a href="https://genesis-world.readthedocs.io/en/latest/user_guide/index.html"&gt;English&lt;/a&gt;, &lt;a href="https://genesis-world.readthedocs.io/zh-cn/latest/user_guide/index.html"&gt;Chinese&lt;/a&gt;, and &lt;a href="https://genesis-world.readthedocs.io/ja/latest/user_guide/index.html"&gt;Japanese&lt;/a&gt;. This includes detailed installation steps, tutorials, and API references.&lt;/p&gt; 
&lt;h2&gt;Contributing to Genesis&lt;/h2&gt; 
&lt;p&gt;The Genesis project is an open and collaborative effort. We welcome all forms of contributions from the community, including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Pull requests&lt;/strong&gt; for new features or bug fixes.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Bug reports&lt;/strong&gt; through GitHub Issues.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Suggestions&lt;/strong&gt; to improve Genesis's usability.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Refer to our &lt;a href="https://github.com/Genesis-Embodied-AI/Genesis/raw/main/.github/CONTRIBUTING.md"&gt;contribution guide&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Support&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Report bugs or request features via GitHub &lt;a href="https://github.com/Genesis-Embodied-AI/Genesis/issues"&gt;Issues&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Join discussions or ask questions on GitHub &lt;a href="https://github.com/Genesis-Embodied-AI/Genesis/discussions"&gt;Discussions&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License and Acknowledgments&lt;/h2&gt; 
&lt;p&gt;The Genesis source code is licensed under Apache 2.0.&lt;/p&gt; 
&lt;p&gt;Genesis's development has been made possible thanks to these open-source projects:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/taichi-dev/taichi"&gt;Taichi&lt;/a&gt;: High-performance cross-platform compute backend. Kudos to the Taichi team for their technical support!&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/zhouxian/FluidLab"&gt;FluidLab&lt;/a&gt;: Reference MPM solver implementation.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/erizmr/SPH_Taichi"&gt;SPH_Taichi&lt;/a&gt;: Reference SPH solver implementation.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://matthias-research.github.io/pages/tenMinutePhysics/index.html"&gt;Ten Minute Physics&lt;/a&gt; and &lt;a href="https://github.com/WASD4959/PBF3D"&gt;PBF3D&lt;/a&gt;: Reference PBD solver implementations.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/google-deepmind/mujoco"&gt;MuJoCo&lt;/a&gt;: Reference for rigid body dynamics.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/danfis/libccd"&gt;libccd&lt;/a&gt;: Reference for collision detection.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mmatl/pyrender"&gt;PyRender&lt;/a&gt;: Rasterization-based renderer.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/LuisaGroup/LuisaCompute"&gt;LuisaCompute&lt;/a&gt; and &lt;a href="https://github.com/LuisaGroup/LuisaRender"&gt;LuisaRender&lt;/a&gt;: Ray-tracing DSL.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/shacklettbp/madrona"&gt;Madrona&lt;/a&gt; and &lt;a href="https://github.com/shacklettbp/madrona_mjx"&gt;Madrona-mjx&lt;/a&gt;: Batch renderer backend&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Associated Papers&lt;/h2&gt; 
&lt;p&gt;Genesis is a large scale effort that integrates state-of-the-art technologies of various existing and on-going research work into a single system. Here we include a non-exhaustive list of all the papers that contributed to the Genesis project in one way or another:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Xian, Zhou, et al. "Fluidlab: A differentiable environment for benchmarking complex fluid manipulation." arXiv preprint arXiv:2303.02346 (2023).&lt;/li&gt; 
 &lt;li&gt;Xu, Zhenjia, et al. "Roboninja: Learning an adaptive cutting policy for multi-material objects." arXiv preprint arXiv:2302.11553 (2023).&lt;/li&gt; 
 &lt;li&gt;Wang, Yufei, et al. "Robogen: Towards unleashing infinite data for automated robot learning via generative simulation." arXiv preprint arXiv:2311.01455 (2023).&lt;/li&gt; 
 &lt;li&gt;Wang, Tsun-Hsuan, et al. "Softzoo: A soft robot co-design benchmark for locomotion in diverse environments." arXiv preprint arXiv:2303.09555 (2023).&lt;/li&gt; 
 &lt;li&gt;Wang, Tsun-Hsuan Johnson, et al. "Diffusebot: Breeding soft robots with physics-augmented generative diffusion models." Advances in Neural Information Processing Systems 36 (2023): 44398-44423.&lt;/li&gt; 
 &lt;li&gt;Katara, Pushkal, Zhou Xian, and Katerina Fragkiadaki. "Gen2sim: Scaling up robot learning in simulation with generative models." 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2024.&lt;/li&gt; 
 &lt;li&gt;Si, Zilin, et al. "DiffTactile: A Physics-based Differentiable Tactile Simulator for Contact-rich Robotic Manipulation." arXiv preprint arXiv:2403.08716 (2024).&lt;/li&gt; 
 &lt;li&gt;Wang, Yian, et al. "Thin-Shell Object Manipulations With Differentiable Physics Simulations." arXiv preprint arXiv:2404.00451 (2024).&lt;/li&gt; 
 &lt;li&gt;Lin, Chunru, et al. "UBSoft: A Simulation Platform for Robotic Skill Learning in Unbounded Soft Environments." arXiv preprint arXiv:2411.12711 (2024).&lt;/li&gt; 
 &lt;li&gt;Zhou, Wenyang, et al. "EMDM: Efficient motion diffusion model for fast and high-quality motion generation." European Conference on Computer Vision. Springer, Cham, 2025.&lt;/li&gt; 
 &lt;li&gt;Qiao, Yi-Ling, Junbang Liang, Vladlen Koltun, and Ming C. Lin. "Scalable differentiable physics for learning and control." International Conference on Machine Learning. PMLR, 2020.&lt;/li&gt; 
 &lt;li&gt;Qiao, Yi-Ling, Junbang Liang, Vladlen Koltun, and Ming C. Lin. "Efficient differentiable simulation of articulated bodies." In International Conference on Machine Learning, PMLR, 2021.&lt;/li&gt; 
 &lt;li&gt;Qiao, Yi-Ling, Junbang Liang, Vladlen Koltun, and Ming Lin. "Differentiable simulation of soft multi-body systems." Advances in Neural Information Processing Systems 34 (2021).&lt;/li&gt; 
 &lt;li&gt;Wan, Weilin, et al. "Tlcontrol: Trajectory and language control for human motion synthesis." arXiv preprint arXiv:2311.17135 (2023).&lt;/li&gt; 
 &lt;li&gt;Wang, Yian, et al. "Architect: Generating Vivid and Interactive 3D Scenes with Hierarchical 2D Inpainting." arXiv preprint arXiv:2411.09823 (2024).&lt;/li&gt; 
 &lt;li&gt;Zheng, Shaokun, et al. "LuisaRender: A high-performance rendering framework with layered and unified interfaces on stream architectures." ACM Transactions on Graphics (TOG) 41.6 (2022): 1-19.&lt;/li&gt; 
 &lt;li&gt;Fan, Yingruo, et al. "Faceformer: Speech-driven 3d facial animation with transformers." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.&lt;/li&gt; 
 &lt;li&gt;Wu, Sichun, Kazi Injamamul Haque, and Zerrin Yumak. "ProbTalk3D: Non-Deterministic Emotion Controllable Speech-Driven 3D Facial Animation Synthesis Using VQ-VAE." Proceedings of the 17th ACM SIGGRAPH Conference on Motion, Interaction, and Games. 2024.&lt;/li&gt; 
 &lt;li&gt;Dou, Zhiyang, et al. "C¬∑ ase: Learning conditional adversarial skill embeddings for physics-based characters." SIGGRAPH Asia 2023 Conference Papers. 2023.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;... and many more on-going work.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you use Genesis in your research, please consider citing:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{Genesis,
  author = {Genesis Authors},
  title = {Genesis: A Generative and Universal Physics Engine for Robotics and Beyond},
  month = {December},
  year = {2024},
  url = {https://github.com/Genesis-Embodied-AI/Genesis}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>VectifyAI/PageIndex</title>
      <link>https://github.com/VectifyAI/PageIndex</link>
      <description>&lt;p&gt;üìë PageIndex: Document Index for Reasoning-based RAG&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;a href="https://vectify.ai/pageindex" target="_blank"&gt; &lt;img src="https://github.com/user-attachments/assets/46201e72-675b-43bc-bfbd-081cc6b65a1d" alt="PageIndex Banner" /&gt; &lt;/a&gt; 
 &lt;br /&gt; 
 &lt;br /&gt; 
 &lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/14736" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/14736" alt="VectifyAI%2FPageIndex | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;p align="center"&gt;&lt;b&gt;Reasoning-based RAG&amp;nbsp; ‚ó¶ &amp;nbsp;No Vector DB&amp;nbsp; ‚ó¶ &amp;nbsp;No Chunking&amp;nbsp; ‚ó¶ &amp;nbsp;Human-like Retrieval&lt;/b&gt;&lt;/p&gt; 
 &lt;h4 align="center"&gt; &lt;a href="https://vectify.ai"&gt;üè† Homepage&lt;/a&gt;&amp;nbsp; ‚Ä¢ &amp;nbsp; &lt;a href="https://chat.pageindex.ai"&gt;üñ•Ô∏è Chat Platform&lt;/a&gt;&amp;nbsp; ‚Ä¢ &amp;nbsp; &lt;a href="https://pageindex.ai/mcp"&gt;üîå MCP&lt;/a&gt;&amp;nbsp; ‚Ä¢ &amp;nbsp; &lt;a href="https://docs.pageindex.ai"&gt;üìö Docs&lt;/a&gt;&amp;nbsp; ‚Ä¢ &amp;nbsp; &lt;a href="https://discord.com/invite/VuXuf29EUj"&gt;üí¨ Discord&lt;/a&gt;&amp;nbsp; ‚Ä¢ &amp;nbsp; &lt;a href="https://ii2abc2jejf.typeform.com/to/tK3AXl8T"&gt;‚úâÔ∏è Contact&lt;/a&gt;&amp;nbsp; &lt;/h4&gt; 
&lt;/div&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;h2&gt;üì¢ Latest Updates&lt;/h2&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;üî• Releases:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://chat.pageindex.ai"&gt;&lt;strong&gt;PageIndex Chat&lt;/strong&gt;&lt;/a&gt;: The first human-like document-analysis agent &lt;a href="https://chat.pageindex.ai"&gt;platform&lt;/a&gt; built for professional long documents. Can also be integrated via &lt;a href="https://pageindex.ai/mcp"&gt;MCP&lt;/a&gt; or &lt;a href="https://docs.pageindex.ai/quickstart"&gt;API&lt;/a&gt; (beta).&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;!-- - [**PageIndex Chat API**](https://docs.pageindex.ai/quickstart): An API that brings PageIndex's advanced long-document intelligence directly into your applications and workflows. --&gt; 
 &lt;!-- - [PageIndex MCP](https://pageindex.ai/mcp): Bring PageIndex into Claude, Cursor, or any MCP-enabled agent. Chat with long PDFs in a reasoning-based, human-like way. --&gt; 
 &lt;p&gt;&lt;strong&gt;üìù Articles:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://pageindex.ai/blog/pageindex-intro"&gt;&lt;strong&gt;PageIndex Framework&lt;/strong&gt;&lt;/a&gt;: Introduces the PageIndex framework ‚Äî an &lt;em&gt;agentic, in-context&lt;/em&gt; &lt;em&gt;tree index&lt;/em&gt; that enables LLMs to perform &lt;em&gt;reasoning-based&lt;/em&gt;, &lt;em&gt;human-like retrieval&lt;/em&gt; over long documents, without vector DB or chunking.&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;!-- - [Do We Still Need OCR?](https://pageindex.ai/blog/do-we-need-ocr): Explores how vision-based, reasoning-native RAG challenges the traditional OCR pipeline, and why the future of document AI might be *vectorless* and *vision-based*. --&gt; 
 &lt;p&gt;&lt;strong&gt;üß™ Cookbooks:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://docs.pageindex.ai/cookbook/vectorless-rag-pageindex"&gt;Vectorless RAG&lt;/a&gt;: A minimal, hands-on example of reasoning-based RAG using PageIndex. No vectors, no chunking, and human-like retrieval.&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://docs.pageindex.ai/cookbook/vision-rag-pageindex"&gt;Vision-based Vectorless RAG&lt;/a&gt;: OCR-free, vision-only RAG with PageIndex's reasoning-native retrieval workflow that works directly over PDF page images.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h1&gt;üìë Introduction to PageIndex&lt;/h1&gt; 
&lt;p&gt;Are you frustrated with vector database retrieval accuracy for long professional documents? Traditional vector-based RAG relies on semantic &lt;em&gt;similarity&lt;/em&gt; rather than true &lt;em&gt;relevance&lt;/em&gt;. But &lt;strong&gt;similarity ‚â† relevance&lt;/strong&gt; ‚Äî what we truly need in retrieval is &lt;strong&gt;relevance&lt;/strong&gt;, and that requires &lt;strong&gt;reasoning&lt;/strong&gt;. When working with professional documents that demand domain expertise and multi-step reasoning, similarity search often falls short.&lt;/p&gt; 
&lt;p&gt;Inspired by AlphaGo, we propose &lt;strong&gt;&lt;a href="https://vectify.ai/pageindex"&gt;PageIndex&lt;/a&gt;&lt;/strong&gt; ‚Äî a &lt;strong&gt;vectorless&lt;/strong&gt;, &lt;strong&gt;reasoning-based RAG&lt;/strong&gt; system that builds a &lt;strong&gt;hierarchical tree index&lt;/strong&gt; from long documents and uses LLMs to &lt;strong&gt;reason&lt;/strong&gt; &lt;em&gt;over that index&lt;/em&gt; for &lt;strong&gt;agentic, context-aware retrieval&lt;/strong&gt;. It simulates how &lt;em&gt;human experts&lt;/em&gt; navigate and extract knowledge from complex documents through &lt;em&gt;tree search&lt;/em&gt;, enabling LLMs to &lt;em&gt;think&lt;/em&gt; and &lt;em&gt;reason&lt;/em&gt; their way to the most relevant document sections. PageIndex performs retrieval in two steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Generate a ‚ÄúTable-of-Contents‚Äù &lt;strong&gt;tree structure index&lt;/strong&gt; of documents&lt;/li&gt; 
 &lt;li&gt;Perform reasoning-based retrieval through &lt;strong&gt;tree search&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://pageindex.ai/blog/pageindex-intro" target="_blank" title="The PageIndex Framework"&gt; &lt;img src="https://docs.pageindex.ai/images/cookbook/vectorless-rag.png" width="70%" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h3&gt;üéØ Features&lt;/h3&gt; 
&lt;p&gt;Compared to traditional vector-based RAG, &lt;strong&gt;PageIndex&lt;/strong&gt; features:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;No Vector DB&lt;/strong&gt;: Uses document structure and LLM reasoning for retrieval, instead of vector similarity search.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;No Chunking&lt;/strong&gt;: Documents are organized into natural sections, not artificial chunks.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Human-like Retrieval&lt;/strong&gt;: Simulates how human experts navigate and extract knowledge from complex documents.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Better Explainability and Traceability&lt;/strong&gt;: Retrieval is based on reasoning ‚Äî traceable and interpretable, with page and section references. No more opaque, approximate vector search (‚Äúvibe retrieval‚Äù).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;PageIndex powers a reasoning-based RAG system that achieved &lt;strong&gt;state-of-the-art&lt;/strong&gt; &lt;a href="https://github.com/VectifyAI/Mafin2.5-FinanceBench"&gt;98.7% accuracy&lt;/a&gt; on FinanceBench, demonstrating superior performance over vector-based RAG solutions in professional document analysis (see our &lt;a href="https://vectify.ai/blog/Mafin2.5"&gt;blog post&lt;/a&gt; for details).&lt;/p&gt; 
&lt;h3&gt;üìç Explore PageIndex&lt;/h3&gt; 
&lt;p&gt;To learn more, please see a detailed introduction of the &lt;a href="https://pageindex.ai/blog/pageindex-intro"&gt;PageIndex framework&lt;/a&gt;. Check out this GitHub repo for open-source code, and the &lt;a href="https://docs.pageindex.ai/cookbook"&gt;cookbooks&lt;/a&gt;, &lt;a href="https://docs.pageindex.ai/tutorials"&gt;tutorials&lt;/a&gt;, and &lt;a href="https://pageindex.ai/blog"&gt;blog&lt;/a&gt; for additional usage guides and examples.&lt;/p&gt; 
&lt;p&gt;The PageIndex service is available as a ChatGPT-style &lt;a href="https://chat.pageindex.ai"&gt;chat platform&lt;/a&gt;, or can be integrated via &lt;a href="https://pageindex.ai/mcp"&gt;MCP&lt;/a&gt; or &lt;a href="https://docs.pageindex.ai/quickstart"&gt;API&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;üõ†Ô∏è Deployment Options&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Self-host ‚Äî run locally with this open-source repo.&lt;/li&gt; 
 &lt;li&gt;Cloud Service ‚Äî try instantly with our &lt;a href="https://chat.pageindex.ai/"&gt;Chat Platform&lt;/a&gt;, or integrate with &lt;a href="https://pageindex.ai/mcp"&gt;MCP&lt;/a&gt; or &lt;a href="https://docs.pageindex.ai/quickstart"&gt;API&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;Enterprise&lt;/em&gt; ‚Äî private or on-prem deployment. &lt;a href="https://ii2abc2jejf.typeform.com/to/tK3AXl8T"&gt;Contact us&lt;/a&gt; or &lt;a href="https://calendly.com/pageindex/meet"&gt;book a demo&lt;/a&gt; for more details.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üß™ Quick Hands-on&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Try the &lt;a href="https://github.com/VectifyAI/PageIndex/raw/main/cookbook/pageindex_RAG_simple.ipynb"&gt;&lt;strong&gt;Vectorless RAG&lt;/strong&gt;&lt;/a&gt; notebook ‚Äî a &lt;em&gt;minimal&lt;/em&gt;, hands-on example of reasoning-based RAG using PageIndex.&lt;/li&gt; 
 &lt;li&gt;Experiment with &lt;a href="https://github.com/VectifyAI/PageIndex/raw/main/cookbook/vision_RAG_pageindex.ipynb"&gt;&lt;em&gt;Vision-based Vectorless RAG&lt;/em&gt;&lt;/a&gt; ‚Äî no OCR; a minimal, reasoning-native RAG pipeline that works directly over page images.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://colab.research.google.com/github/VectifyAI/PageIndex/blob/main/cookbook/pageindex_RAG_simple.ipynb" target="_blank" rel="noopener"&gt; &lt;img src="https://img.shields.io/badge/Open_In_Colab-Vectorless_RAG-orange?style=for-the-badge&amp;amp;logo=googlecolab" alt="Open in Colab: Vectorless RAG" /&gt; &lt;/a&gt; &amp;nbsp;&amp;nbsp; 
 &lt;a href="https://colab.research.google.com/github/VectifyAI/PageIndex/blob/main/cookbook/vision_RAG_pageindex.ipynb" target="_blank" rel="noopener"&gt; &lt;img src="https://img.shields.io/badge/Open_In_Colab-Vision_RAG-orange?style=for-the-badge&amp;amp;logo=googlecolab" alt="Open in Colab: Vision RAG" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h1&gt;üå≤ PageIndex Tree Structure&lt;/h1&gt; 
&lt;p&gt;PageIndex can transform lengthy PDF documents into a semantic &lt;strong&gt;tree structure&lt;/strong&gt;, similar to a &lt;em&gt;"table of contents"&lt;/em&gt; but optimized for use with Large Language Models (LLMs). It's ideal for: financial reports, regulatory filings, academic textbooks, legal or technical manuals, and any document that exceeds LLM context limits.&lt;/p&gt; 
&lt;p&gt;Below is an example PageIndex tree structure. Also see more example &lt;a href="https://github.com/VectifyAI/PageIndex/tree/main/tests/pdfs"&gt;documents&lt;/a&gt; and generated &lt;a href="https://github.com/VectifyAI/PageIndex/tree/main/tests/results"&gt;tree structures&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsonc"&gt;...
{
  "title": "Financial Stability",
  "node_id": "0006",
  "start_index": 21,
  "end_index": 22,
  "summary": "The Federal Reserve ...",
  "nodes": [
    {
      "title": "Monitoring Financial Vulnerabilities",
      "node_id": "0007",
      "start_index": 22,
      "end_index": 28,
      "summary": "The Federal Reserve's monitoring ..."
    },
    {
      "title": "Domestic and International Cooperation and Coordination",
      "node_id": "0008",
      "start_index": 28,
      "end_index": 31,
      "summary": "In 2023, the Federal Reserve collaborated ..."
    }
  ]
}
...
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can generate the PageIndex tree structure with this open-source repo, or use our &lt;a href="https://docs.pageindex.ai/quickstart"&gt;API&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h1&gt;‚öôÔ∏è Package Usage&lt;/h1&gt; 
&lt;p&gt;You can follow these steps to generate a PageIndex tree from a PDF document.&lt;/p&gt; 
&lt;h3&gt;1. Install dependencies&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip3 install --upgrade -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Set your OpenAI API key&lt;/h3&gt; 
&lt;p&gt;Create a &lt;code&gt;.env&lt;/code&gt; file in the root directory and add your API key:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CHATGPT_API_KEY=your_openai_key_here
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. Run PageIndex on your PDF&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python3 run_pageindex.py --pdf_path /path/to/your/document.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Optional parameters&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; You can customize the processing with additional optional arguments: 
 &lt;pre&gt;&lt;code&gt;--model                 OpenAI model to use (default: gpt-4o-2024-11-20)
--toc-check-pages       Pages to check for table of contents (default: 20)
--max-pages-per-node    Max pages per node (default: 10)
--max-tokens-per-node   Max tokens per node (default: 20000)
--if-add-node-id        Add node ID (yes/no, default: yes)
--if-add-node-summary   Add node summary (yes/no, default: yes)
--if-add-doc-description Add doc description (yes/no, default: yes)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Markdown support&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; We also provide markdown support for PageIndex. You can use the `-md_path` flag to generate a tree structure for a markdown file. 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python3 run_pageindex.py --md_path /path/to/your/document.md
&lt;/code&gt;&lt;/pre&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;Note: in this function, we use "#" to determine node heading and their levels. For example, "##" is level 2, "###" is level 3, etc. Make sure your markdown file is formatted correctly. If your Markdown file was converted from a PDF or HTML, we don't recommend using this function, since most existing conversion tools cannot preserve the original hierarchy. Instead, use our &lt;a href="https://pageindex.ai/blog/ocr"&gt;PageIndex OCR&lt;/a&gt;, which is designed to preserve the original hierarchy, to convert the PDF to a markdown file and then use this function.&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/details&gt; 
&lt;!-- 
# ‚òÅÔ∏è Improved Tree Generation with PageIndex OCR

This repo is designed for generating PageIndex tree structure for simple PDFs, but many real-world use cases involve complex PDFs that are hard to parse by classic Python tools. However, extracting high-quality text from PDF documents remains a non-trivial challenge. Most OCR tools only extract page-level content, losing the broader document context and hierarchy.

To address this, we introduced PageIndex OCR ‚Äî the first long-context OCR model designed to preserve the global structure of documents. PageIndex OCR significantly outperforms other leading OCR tools, such as those from Mistral and Contextual AI, in recognizing true hierarchy and semantic relationships across document pages.

- Experience next-level OCR quality with PageIndex OCR at our [Dashboard](https://dash.pageindex.ai/).
- Integrate PageIndex OCR seamlessly into your stack via our [API](https://docs.pageindex.ai/quickstart).

&lt;p align="center"&gt;
  &lt;img src="https://github.com/user-attachments/assets/eb35d8ae-865c-4e60-a33b-ebbd00c41732" width="80%"&gt;
&lt;/p&gt;
--&gt; 
&lt;hr /&gt; 
&lt;h1&gt;üìà Case Study: PageIndex Leads Finance QA Benchmark&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://vectify.ai/mafin"&gt;Mafin 2.5&lt;/a&gt; is a reasoning-based RAG system for financial document analysis, powered by &lt;strong&gt;PageIndex&lt;/strong&gt;. It achieved a state-of-the-art &lt;a href="https://vectify.ai/blog/Mafin2.5"&gt;&lt;strong&gt;98.7% accuracy&lt;/strong&gt;&lt;/a&gt; on the &lt;a href="https://arxiv.org/abs/2311.11944"&gt;FinanceBench&lt;/a&gt; benchmark, significantly outperforming traditional vector-based RAG systems.&lt;/p&gt; 
&lt;p&gt;PageIndex's hierarchical indexing and reasoning-driven retrieval enable precise navigation and extraction of relevant context from complex financial reports, such as SEC filings and earnings disclosures.&lt;/p&gt; 
&lt;p&gt;Explore the full &lt;a href="https://github.com/VectifyAI/Mafin2.5-FinanceBench"&gt;benchmark results&lt;/a&gt; and our &lt;a href="https://vectify.ai/blog/Mafin2.5"&gt;blog post&lt;/a&gt; for detailed comparisons and performance metrics.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/VectifyAI/Mafin2.5-FinanceBench"&gt; &lt;img src="https://github.com/user-attachments/assets/571aa074-d803-43c7-80c4-a04254b782a3" width="70%" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h1&gt;üß≠ Resources&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;üß™ &lt;a href="https://docs.pageindex.ai/cookbook/vectorless-rag-pageindex"&gt;Cookbooks&lt;/a&gt;: hands-on, runnable examples and advanced use cases.&lt;/li&gt; 
 &lt;li&gt;üìñ &lt;a href="https://docs.pageindex.ai/doc-search"&gt;Tutorials&lt;/a&gt;: practical guides and strategies, including &lt;em&gt;Document Search&lt;/em&gt; and &lt;em&gt;Tree Search&lt;/em&gt;.&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://pageindex.ai/blog"&gt;Blog&lt;/a&gt;: technical articles, research insights, and product updates.&lt;/li&gt; 
 &lt;li&gt;üîå &lt;a href="https://pageindex.ai/mcp#quick-setup"&gt;MCP setup&lt;/a&gt; &amp;amp; &lt;a href="https://docs.pageindex.ai/quickstart"&gt;API docs&lt;/a&gt;: integration details and configuration options.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h1&gt;‚≠ê Support Us&lt;/h1&gt; 
&lt;p&gt;Leave us a star üåü if you like our project. Thank you!&lt;/p&gt; 
&lt;p&gt; &lt;img src="https://github.com/user-attachments/assets/eae4ff38-48ae-4a7c-b19f-eab81201d794" width="80%" /&gt; &lt;/p&gt; 
&lt;h3&gt;Connect with Us&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://x.com/VectifyAI"&gt;&lt;img src="https://img.shields.io/badge/Twitter-000000?style=for-the-badge&amp;amp;logo=x&amp;amp;logoColor=white" alt="Twitter" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://www.linkedin.com/company/vectify-ai/"&gt;&lt;img src="https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&amp;amp;logo=linkedin&amp;amp;logoColor=white" alt="LinkedIn" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://discord.com/invite/VuXuf29EUj"&gt;&lt;img src="https://img.shields.io/badge/Discord-5865F2?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://ii2abc2jejf.typeform.com/to/tK3AXl8T"&gt;&lt;img src="https://img.shields.io/badge/Contact_Us-3B82F6?style=for-the-badge&amp;amp;logo=envelope&amp;amp;logoColor=white" alt="Contact Us" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;¬© 2025 &lt;a href="https://vectify.ai"&gt;Vectify AI&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Lightricks/ComfyUI-LTXVideo</title>
      <link>https://github.com/Lightricks/ComfyUI-LTXVideo</link>
      <description>&lt;p&gt;LTX-Video Support for ComfyUI&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ComfyUI-LTXVideo&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/Lightricks/LTX-2"&gt;&lt;img src="https://img.shields.io/badge/LTX-Repo-blue?logo=github" alt="GitHub" /&gt;&lt;/a&gt; &lt;a href="https://ltx.io/model"&gt;&lt;img src="https://img.shields.io/badge/Website-LTX-181717?logo=google-chrome" alt="Website" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/Lightricks/LTX-2"&gt;&lt;img src="https://img.shields.io/badge/HuggingFace-Model-orange?logo=huggingface" alt="Model" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Lightricks/LTX-2/tree/main/packages/ltx-trainer"&gt;&lt;img src="https://img.shields.io/badge/LTX-Trainer%20Repo-9146FF" alt="LTXV Trainer" /&gt;&lt;/a&gt; &lt;a href="https://app.ltx.studio/ltx-2-playground/i2v"&gt;&lt;img src="https://img.shields.io/badge/Demo-Try%20Now-brightgreen?logo=vercel" alt="Demo" /&gt;&lt;/a&gt; &lt;a href="https://videos.ltx.io/LTX-2/grants/LTX_2_Technical_Report_compressed.pdf"&gt;&lt;img src="https://img.shields.io/badge/Paper-arXiv-B31B1B?logo=arxiv" alt="Paper" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/ltxplatform"&gt;&lt;img src="https://img.shields.io/badge/Join-Discord-5865F2?logo=discord" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;A collection of powerful custom nodes that extend ComfyUI's capabilities for the LTX-2 video generation model.&lt;/p&gt; 
&lt;p&gt;LTX-2 is built into ComfyUI core (&lt;a href="https://github.com/comfyanonymous/ComfyUI/tree/master/comfy/ldm/lightricks"&gt;see it here&lt;/a&gt;), making it readily accessible to all ComfyUI users. This repository hosts additional nodes and workflows to help you get the most out of LTX-2's advanced features.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;To learn more about LTX-2&lt;/strong&gt; See the &lt;a href="https://github.com/Lightricks/LTX-2"&gt;main LTX-2 repository&lt;/a&gt; for model details and additional resources.&lt;/p&gt; 
&lt;h2&gt;Prerequisites&lt;/h2&gt; 
&lt;p&gt;Before you begin using an LTX-2 workflow in ComfyUI, make sure you have:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ComfyUI installed (Download here](&lt;a href="https://www.comfy.org/download"&gt;https://www.comfy.org/download&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;CUDA-compatible GPU with 32GB+ VRAM&lt;/li&gt; 
 &lt;li&gt;100GB+ free disk space for models and cache&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quick Start üöÄ&lt;/h2&gt; 
&lt;p&gt;We recommend using the LTX-2 workflows available in Comfy Manager.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Open ComfyUI&lt;/li&gt; 
 &lt;li&gt;Click the Manager button (or press Ctrl+M)&lt;/li&gt; 
 &lt;li&gt;Select Install Custom Nodes&lt;/li&gt; 
 &lt;li&gt;Search for ‚ÄúLTXVideo‚Äù&lt;/li&gt; 
 &lt;li&gt;Click Install&lt;/li&gt; 
 &lt;li&gt;Wait for installation to complete&lt;/li&gt; 
 &lt;li&gt;Restart ComfyUI&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;The nodes will appear in your node menu under the ‚ÄúLTXVideo‚Äù category. Required models will be downloaded on first use.&lt;/p&gt; 
&lt;h2&gt;Example Workflows&lt;/h2&gt; 
&lt;p&gt;The ComfyUI-LTXVideo installation includes several example workflows. You can see them all at: ''' ComfyUI/custom_nodes/ComfyUI-LTXVideo/example_workflows/ '''&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/ComfyUI-LTXVideo/master/example_workflows/LTX-2_T2V_Full_wLora.json"&gt;&lt;code&gt;Text to video full model&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/ComfyUI-LTXVideo/master/example_workflows/LTX-2_T2V_Distilled_wLora.json"&gt;&lt;code&gt;Text to video distilled model (Fast)&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/ComfyUI-LTXVideo/master/example_workflows/LTX-2_I2V_Full_wLora.json"&gt;&lt;code&gt;Image to video full model&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/ComfyUI-LTXVideo/master/example_workflows/LTX-2_I2V_Distilled_wLora.json"&gt;&lt;code&gt;Image to video distilled model (Fast)&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/ComfyUI-LTXVideo/master/example_workflows/LTX-2_V2V_Detailer.json"&gt;&lt;code&gt;Video to video detailer&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/ComfyUI-LTXVideo/master/example_workflows/LTX-2_ICLoRA_All_Distilled.json"&gt;&lt;code&gt;IC-LoRA distilled model (depth + human pose + edges)&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Required Models&lt;/h2&gt; 
&lt;p&gt;Download the following models:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;LTX-2 Model Checkpoint&lt;/strong&gt; - Choose and download one of the models to &lt;code&gt;COMFYUI_ROOT_FOLDER/models/checkpoints&lt;/code&gt; folder.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-19b-dev-fp8.safetensors"&gt;&lt;code&gt;ltx-2-19b-dev-fp8.safetensors&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-19b-distilled-fp8.safetensors"&gt;&lt;code&gt;ltx-2-19b-distilled-fp8.safetensors&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-19b-dev.safetensors"&gt;&lt;code&gt;ltx-2-19b-dev.safetensors&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-19b-distilled.safetensors"&gt;&lt;code&gt;ltx-2-19b-distilled.safetensors&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Spatial Upscaler&lt;/strong&gt; - Required for current two-stage pipeline implementations in this repository. Download to &lt;code&gt;COMFYUI_ROOT_FOLDER/models/latent_upscale_models&lt;/code&gt; folder.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-spatial-upscaler-x2-1.0.safetensors"&gt;&lt;code&gt;ltx-2-spatial-upscaler-x2-1.0.safetensors&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Temporal Upscaler&lt;/strong&gt; - Required for current two-stage pipeline implementations in this repository. Download to &lt;code&gt;COMFYUI_ROOT_FOLDER/models/latent_upscale_models&lt;/code&gt; folder.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-temporal-upscaler-x2-1.0.safetensors"&gt;&lt;code&gt;ltx-2-temporal-upscaler-x2-1.0.safetensors&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Distilled LoRA&lt;/strong&gt; - Required for current two-stage pipeline implementations in this repository (except DistilledPipeline and ICLoraPipeline). Download to &lt;code&gt;COMFYUI_ROOT_FOLDER/models/loras&lt;/code&gt; folder.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-19b-distilled-lora-384.safetensors"&gt;&lt;code&gt;ltx-2-19b-distilled-lora-384.safetensors&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Gemma Text Encoder&lt;/strong&gt; Download all files from the repository to &lt;code&gt;COMFYUI_ROOT_FOLDER/models/text_encoders/gemma-3-12b-it-qat-q4_0-unquantized&lt;/code&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/google/gemma-3-12b-it-qat-q4_0-unquantized"&gt;&lt;code&gt;Gemma 3&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;LoRAs&lt;/strong&gt; Choose and download to &lt;code&gt;COMFYUI_ROOT_FOLDER/models/loras&lt;/code&gt; folder.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2-19b-IC-LoRA-Canny-Control/blob/main/ltx-2-19b-ic-lora-canny-control.safetensors"&gt;&lt;code&gt;ltx-2-19b-ic-lora-canny-control.safetensors&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2-19b-IC-LoRA-Depth-Control/blob/main/ltx-2-19b-ic-lora-depth-control.safetensors"&gt;&lt;code&gt;ltx-2-19b-ic-lora-depth-control.safetensors&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2-19b-IC-LoRA-Detailer/blob/main/ltx-2-19b-ic-lora-detailer.safetensors"&gt;&lt;code&gt;ltx-2-19b-ic-lora-detailer.safetensors&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2-19b-IC-LoRA-Pose-Control/blob/main/ltx-2-19b-ic-lora-pose-control.safetensors"&gt;&lt;code&gt;ltx-2-19b-ic-lora-pose-control.safetensors&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2-19b-LoRA-Camera-Control-Dolly-In/blob/main/ltx-2-19b-lora-camera-control-dolly-in.safetensors"&gt;&lt;code&gt;ltx-2-19b-lora-camera-control-dolly-in.safetensors&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2-19b-LoRA-Camera-Control-Dolly-Left/blob/main/ltx-2-19b-lora-camera-control-dolly-left.safetensors"&gt;&lt;code&gt;ltx-2-19b-lora-camera-control-dolly-left.safetensors&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2-19b-LoRA-Camera-Control-Dolly-Out/blob/main/ltx-2-19b-lora-camera-control-dolly-out.safetensors"&gt;&lt;code&gt;ltx-2-19b-lora-camera-control-dolly-out.safetensors&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2-19b-LoRA-Camera-Control-Dolly-Right/blob/main/ltx-2-19b-lora-camera-control-dolly-right.safetensors"&gt;&lt;code&gt;ltx-2-19b-lora-camera-control-dolly-right.safetensors&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2-19b-LoRA-Camera-Control-Jib-Down/blob/main/ltx-2-19b-lora-camera-control-jib-down.safetensors"&gt;&lt;code&gt;ltx-2-19b-lora-camera-control-jib-down.safetensors&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2-19b-LoRA-Camera-Control-Jib-Up/blob/main/ltx-2-19b-lora-camera-control-jib-up.safetensors"&gt;&lt;code&gt;ltx-2-19b-lora-camera-control-jib-up.safetensors&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2-19b-LoRA-Camera-Control-Static/blob/main/ltx-2-19b-lora-camera-control-static.safetensors"&gt;&lt;code&gt;ltx-2-19b-lora-camera-control-static.safetensors&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Advanced Techniques&lt;/h2&gt; 
&lt;h3&gt;Low VRAM&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;For systems with low VRAM you can use the model loader nodes from &lt;a href="https://raw.githubusercontent.com/Lightricks/ComfyUI-LTXVideo/master/low_vram_loaders.py"&gt;low_vram_loaders.py&lt;/a&gt;. Those nodes ensure the correct order of execution and perform the model offloading such that generation fits in 32 GB VRAM.&lt;/li&gt; 
 &lt;li&gt;Use --reserve-vram ComfyUI parameter: &lt;code&gt;python -m main --reserve-vram 5&lt;/code&gt; (or other number in GB).&lt;/li&gt; 
 &lt;li&gt;For complete information about using LTX-2 models, workflows, and nodes in ComfyUI, please visit our &lt;a href="https://docs.ltx.video/open-source-model/integration-tools/comfy-ui"&gt;Open Source documentation&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>flashinfer-ai/flashinfer</title>
      <link>https://github.com/flashinfer-ai/flashinfer</link>
      <description>&lt;p&gt;FlashInfer: Kernel Library for LLM Serving&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://github.com/flashinfer-ai/web-data/blob/main/logo/FlashInfer-black-background.png?raw=true" /&gt; 
  &lt;img alt="FlashInfer" src="https://github.com/flashinfer-ai/web-data/raw/main/logo/FlashInfer-white-background.png?raw=true" width="55%" /&gt; 
 &lt;/picture&gt; &lt;/p&gt; 
&lt;h1 align="center"&gt; Kernel Library for LLM Serving &lt;/h1&gt; 
&lt;p align="center"&gt; | &lt;a href="https://flashinfer.ai"&gt;&lt;b&gt;Blog&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://docs.flashinfer.ai"&gt;&lt;b&gt;Documentation&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://join.slack.com/t/flashinfer/shared_invite/zt-379wct3hc-D5jR~1ZKQcU00WHsXhgvtA"&gt;&lt;b&gt;Slack&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://github.com/orgs/flashinfer-ai/discussions"&gt;&lt;b&gt;Discussion Forum&lt;/b&gt;&lt;/a&gt; | &lt;/p&gt; 
&lt;p&gt;&lt;a href="https://ci.tlcpack.ai/job/flashinfer-ci/job/main/"&gt;&lt;img src="https://ci.tlcpack.ai/job/flashinfer-ci/job/main/badge/icon" alt="Build Status" /&gt;&lt;/a&gt; &lt;a href="https://github.com/flashinfer-ai/flashinfer/actions/workflows/build-doc.yml"&gt;&lt;img src="https://github.com/flashinfer-ai/flashinfer/actions/workflows/build-doc.yml/badge.svg?sanitize=true" alt="Documentation" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;FlashInfer is a library and kernel generator for Large Language Models that provides high-performance implementation of LLM GPU kernels such as FlashAttention, SparseAttention, PageAttention, Sampling, and more. FlashInfer focuses on LLM serving and inference, and delivers state-of-the-art performance across diverse scenarios.&lt;/p&gt; 
&lt;p&gt;Check our &lt;a href="https://flashinfer.ai/2024/12/16/flashinfer-v02-release.html"&gt;v0.2 release blog&lt;/a&gt; for new features!&lt;/p&gt; 
&lt;p&gt;The core features of FlashInfer include:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Efficient Sparse/Dense Attention Kernels&lt;/strong&gt;: Efficient single/batch attention for sparse(paged)/dense KV-storage on CUDA Cores and Tensor Cores (both FA2 &amp;amp; FA3) templates. The vector-sparse attention can achieve 90% of the bandwidth of dense kernels with same problem size.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Load-Balanced Scheduling&lt;/strong&gt;: FlashInfer decouples &lt;code&gt;plan&lt;/code&gt;/&lt;code&gt;run&lt;/code&gt; stage of attention computation where we schedule the computation of variable-length inputs in &lt;code&gt;plan&lt;/code&gt; stage to alleviate load-imbalance issue.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Memory Efficiency&lt;/strong&gt;: FlashInfer offers &lt;a href="https://docs.flashinfer.ai/api/cascade.html#flashinfer.cascade.MultiLevelCascadeAttentionWrapper"&gt;Cascade Attention&lt;/a&gt; for hierarchical KV-Cache, and implements Head-Query fusion for accelerating Grouped-Query Attention, and efficient kernels for low-precision attention and fused-RoPE attention for compressed KV-Cache.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Customizable Attention&lt;/strong&gt;: Bring your own attention variants through JIT-compilation.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;CUDAGraph and torch.compile Compatibility&lt;/strong&gt;: FlashInfer kernels can be captured by CUDAGraphs and torch.compile for low-latency inference.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Efficient LLM-specific Operators&lt;/strong&gt;: High-Performance &lt;a href="https://docs.flashinfer.ai/api/sampling.html"&gt;fused kernel for Top-P, Top-K/Min-P sampling&lt;/a&gt; without the need to sorting.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;FlashInfer supports PyTorch, TVM and C++ (header-only) APIs, and can be easily integrated into existing projects.&lt;/p&gt; 
&lt;h2&gt;News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;[Mar 10, 2025] &lt;a href="https://flashinfer.ai/2025/03/10/sampling.html"&gt;Blog Post&lt;/a&gt; Sorting-Free GPU Kernels for LLM Sampling, which explains the design of sampling kernels in FlashInfer.&lt;/li&gt; 
 &lt;li&gt;[Mar 1, 2025] Checkout flashinfer's &lt;a href="https://github.com/flashinfer-ai/flashinfer/tree/main/profiler"&gt;intra-kernel profiler&lt;/a&gt; for visualizing the timeline of each threadblock in GPU kernels.&lt;/li&gt; 
 &lt;li&gt;[Dec 16, 2024] &lt;a href="https://flashinfer.ai/2024/12/16/flashinfer-v02-release.html"&gt;Blog Post&lt;/a&gt; FlashInfer 0.2 - Efficient and Customizable Kernels for LLM Inference Serving&lt;/li&gt; 
 &lt;li&gt;[Sept 2024] We've launched a &lt;a href="https://join.slack.com/t/flashinfer/shared_invite/zt-2r93kj2aq-wZnC2n_Z2~mf73N5qnVGGA"&gt;Slack&lt;/a&gt; workspace for Flashinfer users and developers. Join us for timely support, discussions, updates and knowledge sharing!&lt;/li&gt; 
 &lt;li&gt;[Jan 31, 2024] &lt;a href="https://flashinfer.ai/2024/01/08/cascade-inference.html"&gt;Blog Post&lt;/a&gt; Cascade Inference: Memory-Efficient Shared Prefix Batch Decoding&lt;/li&gt; 
 &lt;li&gt;[Jan 31, 2024] &lt;a href="https://flashinfer.ai/2024/01/03/introduce-flashinfer.html"&gt;Blog Post&lt;/a&gt; Accelerating Self-Attentions for LLM Serving with FlashInfer&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;Using our PyTorch API is the easiest way to get started:&lt;/p&gt; 
&lt;h3&gt;Install from PyPI&lt;/h3&gt; 
&lt;p&gt;FlashInfer is available as a Python package for Linux. Install the core package with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install flashinfer-python
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Package Options:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;flashinfer-python&lt;/strong&gt;: Core package that compiles/downloads kernels on first use&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;flashinfer-cubin&lt;/strong&gt;: Pre-compiled kernel binaries for all supported GPU architectures&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;flashinfer-jit-cache&lt;/strong&gt;: Pre-built kernel cache for specific CUDA versions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;For faster initialization and offline usage&lt;/strong&gt;, install the optional packages to have most kernels pre-compiled:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install flashinfer-python flashinfer-cubin
# JIT cache package (replace cu129 with your CUDA version: cu128, cu129, or cu130)
pip install flashinfer-jit-cache --index-url https://flashinfer.ai/whl/cu129
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This eliminates compilation and downloading overhead at runtime.&lt;/p&gt; 
&lt;h3&gt;Install from Source&lt;/h3&gt; 
&lt;p&gt;Build the core package from source:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/flashinfer-ai/flashinfer.git --recursive
cd flashinfer
python -m pip install -v .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;For development&lt;/strong&gt;, install in editable mode:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m pip install --no-build-isolation -e . -v
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Build optional packages:&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;flashinfer-cubin&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd flashinfer-cubin
python -m build --no-isolation --wheel
python -m pip install dist/*.whl
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;code&gt;flashinfer-jit-cache&lt;/code&gt; (customize &lt;code&gt;FLASHINFER_CUDA_ARCH_LIST&lt;/code&gt; for your target GPUs):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export FLASHINFER_CUDA_ARCH_LIST="7.5 8.0 8.9 9.0a 10.0a 10.3a 11.0a 12.0f"
cd flashinfer-jit-cache
python -m build --no-isolation --wheel
python -m pip install dist/*.whl
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more details, see the &lt;a href="https://docs.flashinfer.ai/installation.html#install-from-source"&gt;Install from Source documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Install Nightly Build&lt;/h3&gt; 
&lt;p&gt;Nightly builds are available for testing the latest features:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Core and cubin packages
pip install -U --pre flashinfer-python --index-url https://flashinfer.ai/whl/nightly/ --no-deps # Install the nightly package from custom index, without installing dependencies
pip install flashinfer-python  # Install flashinfer-python's dependencies from PyPI
pip install -U --pre flashinfer-cubin --index-url https://flashinfer.ai/whl/nightly/
# JIT cache package (replace cu129 with your CUDA version: cu128, cu129, or cu130)
pip install -U --pre flashinfer-jit-cache --index-url https://flashinfer.ai/whl/nightly/cu129
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;CLI Tools&lt;/h3&gt; 
&lt;p&gt;FlashInfer provides several CLI commands for configuration, module management, and development:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Verify installation and view configuration
flashinfer show-config

# List and inspect modules
flashinfer list-modules
flashinfer module-status

# Manage artifacts and cache
flashinfer download-cubin
flashinfer clear-cache

# For developers: generate compile_commands.json for IDE integration
flashinfer export-compile-commands [output_path]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For complete documentation, see the &lt;a href="https://docs.flashinfer.ai/cli.html"&gt;CLI reference&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Trying it out&lt;/h3&gt; 
&lt;p&gt;Below is a minimal example of using FlashInfer's single-request decode/append/prefill attention kernels:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import torch
import flashinfer

kv_len = 2048
num_kv_heads = 32
head_dim = 128

k = torch.randn(kv_len, num_kv_heads, head_dim).half().to(0)
v = torch.randn(kv_len, num_kv_heads, head_dim).half().to(0)

# decode attention

num_qo_heads = 32
q = torch.randn(num_qo_heads, head_dim).half().to(0)

o = flashinfer.single_decode_with_kv_cache(q, k, v) # decode attention without RoPE on-the-fly
o_rope_on_the_fly = flashinfer.single_decode_with_kv_cache(q, k, v, pos_encoding_mode="ROPE_LLAMA") # decode with LLaMA style RoPE on-the-fly

# append attention
append_qo_len = 128
q = torch.randn(append_qo_len, num_qo_heads, head_dim).half().to(0) # append attention, the last 128 tokens in the KV-Cache are the new tokens
o = flashinfer.single_prefill_with_kv_cache(q, k, v, causal=True) # append attention without RoPE on-the-fly, apply causal mask
o_rope_on_the_fly = flashinfer.single_prefill_with_kv_cache(q, k, v, causal=True, pos_encoding_mode="ROPE_LLAMA") # append attention with LLaMA style RoPE on-the-fly, apply causal mask

# prefill attention
qo_len = 2048
q = torch.randn(qo_len, num_qo_heads, head_dim).half().to(0) # prefill attention
o = flashinfer.single_prefill_with_kv_cache(q, k, v, causal=False) # prefill attention without RoPE on-the-fly, do not apply causal mask
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Check out &lt;a href="https://docs.flashinfer.ai/"&gt;documentation&lt;/a&gt; for usage of batch decode/append/prefill kernels and shared-prefix cascading kernels.&lt;/p&gt; 
&lt;h2&gt;API Logging&lt;/h2&gt; 
&lt;p&gt;FlashInfer provides comprehensive API logging for debugging. Enable it using environment variables:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Enable logging (levels: 0=off (default), 1=basic, 3=detailed, 5=statistics)
export FLASHINFER_LOGLEVEL=3

# Set log destination (stdout (default), stderr, or file path)
export FLASHINFER_LOGDEST=stdout
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For detailed information about logging levels, configuration, and advanced features, see &lt;a href="https://docs.flashinfer.ai/logging.html"&gt;Logging&lt;/a&gt; in our documentation.&lt;/p&gt; 
&lt;h2&gt;Custom Attention Variants&lt;/h2&gt; 
&lt;p&gt;Starting from FlashInfer v0.2, users can customize their own attention variants with additional parameters. For more details, refer to our &lt;a href="https://github.com/flashinfer-ai/flashinfer/raw/main/tests/utils/test_jit_example.py"&gt;JIT examples&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;GPU and CUDA Support&lt;/h2&gt; 
&lt;p&gt;FlashInfer currently provides support for NVIDIA SM architectures 75 and higher and beta support for 103, 110, 120, and 121.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Supported CUDA Versions:&lt;/strong&gt; 12.6, 12.8, 13.0, 13.1&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; FlashInfer strives to follow PyTorch's supported CUDA versions plus the latest CUDA release.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Adoption&lt;/h2&gt; 
&lt;p&gt;We are thrilled to share that FlashInfer is being adopted by many cutting-edge projects, including but not limited to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mlc-ai/mlc-llm"&gt;MLC-LLM&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/punica-ai/punica"&gt;Punica&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/sgl-project/sglang"&gt;SGLang&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vectorch-ai/ScaleLLM"&gt;ScaleLLM&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vllm-project/vllm"&gt;vLLM&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/huggingface/text-generation-inference"&gt;TGI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/predibase/lorax"&gt;lorax&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/NVIDIA/TensorRT-LLM"&gt;TensorRT-LLM&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ModelTC/lightllm"&gt;LightLLM&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Acknowledgement&lt;/h2&gt; 
&lt;p&gt;FlashInfer is inspired by &lt;a href="https://github.com/dao-AILab/flash-attention/"&gt;FlashAttention 1&amp;amp;2&lt;/a&gt;, &lt;a href="https://github.com/vllm-project/vllm"&gt;vLLM&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/2301.03598"&gt;stream-K&lt;/a&gt;, &lt;a href="https://github.com/nvidia/cutlass"&gt;cutlass&lt;/a&gt; and &lt;a href="https://github.com/facebookincubator/AITemplate"&gt;AITemplate&lt;/a&gt; projects.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you find FlashInfer helpful in your project or research, please consider citing our &lt;a href="https://arxiv.org/abs/2501.01005"&gt;paper&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@article{ye2025flashinfer,
    title = {FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving},
    author = {
      Ye, Zihao and
      Chen, Lequn and
      Lai, Ruihang and
      Lin, Wuwei and
      Zhang, Yineng and
      Wang, Stephanie and
      Chen, Tianqi and
      Kasikci, Baris and
      Grover, Vinod and
      Krishnamurthy, Arvind and
      Ceze, Luis
    },
    journal = {arXiv preprint arXiv:2501.01005},
    year = {2025},
    url = {https://arxiv.org/abs/2501.01005}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>NVIDIA-NeMo/NeMo</title>
      <link>https://github.com/NVIDIA-NeMo/NeMo</link>
      <description>&lt;p&gt;A scalable generative AI framework built for researchers and developers working on Large Language Models, Multimodal, and Speech AI (Automatic Speech Recognition and Text-to-Speech)&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="http://www.repostatus.org/#active"&gt;&lt;img src="http://www.repostatus.org/badges/latest/active.svg?sanitize=true" alt="Project Status: Active -- The project has reached a stable, usable state and is being actively developed." /&gt;&lt;/a&gt; &lt;a href="https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/"&gt;&lt;img src="https://readthedocs.com/projects/nvidia-nemo/badge/?version=main" alt="Documentation" /&gt;&lt;/a&gt; &lt;a href="https://github.com/nvidia/nemo/actions/workflows/codeql.yml"&gt;&lt;img src="https://github.com/nvidia/nemo/actions/workflows/codeql.yml/badge.svg?branch=main&amp;amp;event=push" alt="CodeQL" /&gt;&lt;/a&gt; &lt;a href="https://github.com/NVIDIA/NeMo/raw/master/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/License-Apache%202.0-brightgreen.svg?sanitize=true" alt="NeMo core license and license for collections in this repo" /&gt;&lt;/a&gt; &lt;a href="https://badge.fury.io/py/nemo-toolkit"&gt;&lt;img src="https://badge.fury.io/py/nemo-toolkit.svg?sanitize=true" alt="Release version" /&gt;&lt;/a&gt; &lt;a href="https://badge.fury.io/py/nemo-toolkit"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/nemo-toolkit.svg?sanitize=true" alt="Python version" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/nemo-toolkit"&gt;&lt;img src="https://static.pepy.tech/personalized-badge/nemo-toolkit?period=total&amp;amp;units=international_system&amp;amp;left_color=grey&amp;amp;right_color=brightgreen&amp;amp;left_text=downloads" alt="PyPi total downloads" /&gt;&lt;/a&gt; &lt;a href="https://github.com/psf/black"&gt;&lt;img src="https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true" alt="Code style: black" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;&lt;strong&gt;NVIDIA NeMo Speech Collection&lt;/strong&gt;&lt;/h1&gt; 
&lt;h2&gt;Latest News&lt;/h2&gt; 
&lt;!-- markdownlint-disable --&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;b&gt;&lt;a href="https://huggingface.co/nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16"&gt;NVIDIA-Nemotron-3-Nano-30B-A3B&lt;/a&gt; is out with full reproducible script and recipes! Check out &lt;a href="https://github.com/NVIDIA-NeMo/Megatron-Bridge/tree/nano-v3"&gt;NeMo Megatron-Bridge&lt;/a&gt;, &lt;a href="https://github.com/NVIDIA-NeMo/AutoModel/raw/main/examples/llm_finetune/nemotron/nemotron_nano_v3_squad.yaml"&gt;NeMo AutoModel&lt;/a&gt;, &lt;a href="https://github.com/NVIDIA-NeMo/RL"&gt;NeMo-RL&lt;/a&gt; and &lt;a href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/nemo?version=25.11.nemotron_3_nano"&gt;NGC container&lt;/a&gt; to try them!&lt;/b&gt; (2025-12-15) &lt;/summary&gt;
&lt;/details&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;b&gt;‚ö†Ô∏è Pivot notice: This repo will pivot to focus on speech models collections only. Please refer to &lt;a href="https://github.com/NVIDIA-NeMo"&gt;NeMo Framework Github Org&lt;/a&gt; for the complete list of repos under NeMo Framework&lt;/b&gt;&lt;/summary&gt; NeMo 2.0, with its support for LLMs and VLMs will be deprecated by 25.11, and replaced by 
 &lt;a href="https://github.com/NVIDIA-NeMo/Megatron-Bridge"&gt;NeMo Megatron-Bridge&lt;/a&gt; and 
 &lt;a href="https://github.com/NVIDIA-NeMo/AutoModel"&gt;NeMo AutoModel&lt;/a&gt;. More details can be found in the 
 &lt;a href="https://github.com/NVIDIA-NeMo"&gt;NeMo Framework GitHub org readme&lt;/a&gt;. (2025-10-10) 
 &lt;pre&gt;&lt;code&gt;  Following collections are deprecated and will be removed in a later release, please use previous versions if you are using them:
  - nlp
  - llm
  - vlm
  - vision
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details closed&gt; 
 &lt;summary&gt;&lt;b&gt;Pretrain and finetune &lt;span&gt;ü§ó&lt;/span&gt;Hugging Face models via AutoModel&lt;/b&gt;&lt;/summary&gt; NeMo Framework's latest feature AutoModel enables broad support for 
 &lt;span&gt;ü§ó&lt;/span&gt;Hugging Face models, with 25.04 focusing on 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/transformers/v3.5.1/model_doc/auto.html#automodelforcausallm"&gt;AutoModelForCausalLM&lt;/a&gt; in the &lt;a href="https://huggingface.co/models?pipeline_tag=text-generation&amp;amp;sort=trending"&gt;Text Generation&lt;/a&gt; category&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/docs/transformers/main/model_doc/auto#transformers.AutoModelForImageTextToText"&gt;AutoModelForImageTextToText&lt;/a&gt; in the &lt;a href="https://huggingface.co/models?pipeline_tag=image-text-to-text&amp;amp;sort=trending"&gt;Image-Text-to-Text&lt;/a&gt; category&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;More Details in Blog: &lt;a href="https://developer.nvidia.com/blog/run-hugging-face-models-instantly-with-day-0-support-from-nvidia-nemo-framework"&gt;Run Hugging Face Models Instantly with Day-0 Support from NVIDIA NeMo Framework&lt;/a&gt;. Future releases will enable support for more model families such as Video Generation models.(2025-05-19)&lt;/p&gt; 
&lt;/details&gt; 
&lt;details closed&gt; 
 &lt;summary&gt;&lt;b&gt;Training on Blackwell using NeMo&lt;/b&gt;&lt;/summary&gt; NeMo Framework has added Blackwell support, with 
 &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/performance/performance_summary.html"&gt;performance benchmarks on GB200 &amp;amp; B200&lt;/a&gt;. More optimizations to come in the upcoming releases.(2025-05-19) 
&lt;/details&gt; 
&lt;details closed&gt; 
 &lt;summary&gt;&lt;b&gt;Training Performance on GPU Tuning Guide&lt;/b&gt;&lt;/summary&gt; NeMo Framework has published 
 &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/performance/performance-guide.html"&gt;a comprehensive guide for performance tuning to achieve optimal throughput&lt;/a&gt;! (2025-05-19) 
&lt;/details&gt; 
&lt;details closed&gt; 
 &lt;summary&gt;&lt;b&gt;New Models Support&lt;/b&gt;&lt;/summary&gt; NeMo Framework has added support for latest community models - 
 &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/vlms/llama4.html"&gt;Llama 4&lt;/a&gt;, 
 &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/vision/diffusionmodels/flux.html"&gt;Flux&lt;/a&gt;, 
 &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/llms/llama_nemotron.html"&gt;Llama Nemotron&lt;/a&gt;, 
 &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/llms/hyena.html#"&gt;Hyena &amp;amp; Evo2&lt;/a&gt;, 
 &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/vlms/qwen2vl.html"&gt;Qwen2-VL&lt;/a&gt;, 
 &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/llms/qwen2.html"&gt;Qwen2.5&lt;/a&gt;, Gemma3, Qwen3-30B&amp;amp;32B.(2025-05-19) 
&lt;/details&gt; 
&lt;details closed&gt; 
 &lt;summary&gt;&lt;b&gt;NeMo Framework 2.0&lt;/b&gt;&lt;/summary&gt; We've released NeMo 2.0, an update on the NeMo Framework which prioritizes modularity and ease-of-use. Please refer to the 
 &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/nemo-2.0/index.html"&gt;NeMo Framework User Guide&lt;/a&gt; to get started. 
&lt;/details&gt; 
&lt;details closed&gt; 
 &lt;summary&gt;&lt;b&gt;New Cosmos World Foundation Models Support&lt;/b&gt;&lt;/summary&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://developer.nvidia.com/blog/advancing-physical-ai-with-nvidia-cosmos-world-foundation-model-platform"&gt;Advancing Physical AI with NVIDIA Cosmos World Foundation Model Platform &lt;/a&gt; (2025-01-09) &lt;/summary&gt; The end-to-end NVIDIA Cosmos platform accelerates world model development for physical AI systems. Built on CUDA, Cosmos combines state-of-the-art world foundation models, video tokenizers, and AI-accelerated data processing pipelines. Developers can accelerate world model development by fine-tuning Cosmos world foundation models or building new ones from the ground up. These models create realistic synthetic videos of environments and interactions, providing a scalable foundation for training complex systems, from simulating humanoid robots performing advanced actions to developing end-to-end autonomous driving models. 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://developer.nvidia.com/blog/accelerate-custom-video-foundation-model-pipelines-with-new-nvidia-nemo-framework-capabilities/"&gt; Accelerate Custom Video Foundation Model Pipelines with New NVIDIA NeMo Framework Capabilities &lt;/a&gt; (2025-01-07) &lt;/summary&gt; The NeMo Framework now supports training and customizing the 
  &lt;a href="https://github.com/NVIDIA/Cosmos"&gt;NVIDIA Cosmos&lt;/a&gt; collection of world foundation models. Cosmos leverages advanced text-to-world generation techniques to create fluid, coherent video content from natural language prompts. 
  &lt;br /&gt;
  &lt;br /&gt; You can also now accelerate your video processing step using the 
  &lt;a href="https://developer.nvidia.com/nemo-curator-video-processing-early-access"&gt;NeMo Curator&lt;/a&gt; library, which provides optimized video processing and captioning features that can deliver up to 89x faster video processing when compared to an unoptimized CPU pipeline. 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
&lt;/details&gt; 
&lt;details closed&gt; 
 &lt;summary&gt;&lt;b&gt;Large Language Models and Multimodal Models&lt;/b&gt;&lt;/summary&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://developer.nvidia.com/blog/state-of-the-art-multimodal-generative-ai-model-development-with-nvidia-nemo/"&gt; State-of-the-Art Multimodal Generative AI Model Development with NVIDIA NeMo &lt;/a&gt; (2024-11-06) &lt;/summary&gt; NVIDIA recently announced significant enhancements to the NeMo platform, focusing on multimodal generative AI models. The update includes NeMo Curator and the Cosmos tokenizer, which streamline the data curation process and enhance the quality of visual data. These tools are designed to handle large-scale data efficiently, making it easier to develop high-quality AI models for various applications, including robotics and autonomous driving. The Cosmos tokenizers, in particular, efficiently map visual data into compact, semantic tokens, which is crucial for training large-scale generative models. The tokenizer is available now on the 
  &lt;a href="https://github.com/NVIDIA/cosmos-tokenizer"&gt;NVIDIA/cosmos-tokenizer&lt;/a&gt; GitHub repo and on 
  &lt;a href="https://huggingface.co/nvidia/Cosmos-Tokenizer-CV8x8x8"&gt;Hugging Face&lt;/a&gt;. 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/llms/llama/index.html#new-llama-3-1-support for more information/"&gt; New Llama 3.1 Support &lt;/a&gt; (2024-07-23) &lt;/summary&gt; The NeMo Framework now supports training and customizing the Llama 3.1 collection of LLMs from Meta. 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://aws.amazon.com/blogs/machine-learning/accelerate-your-generative-ai-distributed-training-workloads-with-the-nvidia-nemo-framework-on-amazon-eks/"&gt; Accelerate your Generative AI Distributed Training Workloads with the NVIDIA NeMo Framework on Amazon EKS &lt;/a&gt; (2024-07-16) &lt;/summary&gt; NVIDIA NeMo Framework now runs distributed training workloads on an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. For step-by-step instructions on creating an EKS cluster and running distributed training workloads with NeMo, see the GitHub repository 
  &lt;a href="https://github.com/aws-samples/awsome-distributed-training/tree/main/3.test_cases/2.nemo-launcher/EKS/"&gt; here.&lt;/a&gt; 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://developer.nvidia.com/blog/nvidia-nemo-accelerates-llm-innovation-with-hybrid-state-space-model-support/"&gt; NVIDIA NeMo Accelerates LLM Innovation with Hybrid State Space Model Support &lt;/a&gt; (2024/06/17) &lt;/summary&gt; NVIDIA NeMo and Megatron Core now support pre-training and fine-tuning of state space models (SSMs). NeMo also supports training models based on the Griffin architecture as described by Google DeepMind. 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://huggingface.co/models?sort=trending&amp;amp;search=nvidia%2Fnemotron-4-340B"&gt; NVIDIA releases 340B base, instruct, and reward models pretrained on a total of 9T tokens. &lt;/a&gt; (2024-06-18) &lt;/summary&gt; See documentation and tutorials for SFT, PEFT, and PTQ with 
  &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/llms/nemotron/index.html"&gt; Nemotron 340B &lt;/a&gt; in the NeMo Framework User Guide. 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://developer.nvidia.com/blog/nvidia-sets-new-generative-ai-performance-and-scale-records-in-mlperf-training-v4-0/"&gt; NVIDIA sets new generative AI performance and scale records in MLPerf Training v4.0 &lt;/a&gt; (2024/06/12) &lt;/summary&gt; Using NVIDIA NeMo Framework and NVIDIA Hopper GPUs NVIDIA was able to scale to 11,616 H100 GPUs and achieve near-linear performance scaling on LLM pretraining. NVIDIA also achieved the highest LLM fine-tuning performance and raised the bar for text-to-image training. 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://cloud.google.com/blog/products/compute/gke-and-nvidia-nemo-framework-to-train-generative-ai-models"&gt; Accelerate your generative AI journey with NVIDIA NeMo Framework on GKE &lt;/a&gt; (2024/03/16) &lt;/summary&gt; An end-to-end walkthrough to train generative AI models on the Google Kubernetes Engine (GKE) using the NVIDIA NeMo Framework is available at https://github.com/GoogleCloudPlatform/nvidia-nemo-on-gke. The walkthrough includes detailed instructions on how to set up a Google Cloud Project and pre-train a GPT model using the NeMo Framework. 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
&lt;/details&gt; 
&lt;details closed&gt; 
 &lt;summary&gt;&lt;b&gt;Speech Recognition&lt;/b&gt;&lt;/summary&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://developer.nvidia.com/blog/accelerating-leaderboard-topping-asr-models-10x-with-nvidia-nemo/"&gt; Accelerating Leaderboard-Topping ASR Models 10x with NVIDIA NeMo &lt;/a&gt; (2024/09/24) &lt;/summary&gt; NVIDIA NeMo team released a number of inference optimizations for CTC, RNN-T, and TDT models that resulted in up to 10x inference speed-up. These models now exceed an inverse real-time factor (RTFx) of 2,000, with some reaching RTFx of even 6,000. 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://developer.nvidia.com/blog/new-standard-for-speech-recognition-and-translation-from-the-nvidia-nemo-canary-model/"&gt; New Standard for Speech Recognition and Translation from the NVIDIA NeMo Canary Model &lt;/a&gt; (2024/04/18) &lt;/summary&gt; The NeMo team just released Canary, a multilingual model that transcribes speech in English, Spanish, German, and French with punctuation and capitalization. Canary also provides bi-directional translation, between English and the three other supported languages. 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://developer.nvidia.com/blog/pushing-the-boundaries-of-speech-recognition-with-nemo-parakeet-asr-models/"&gt; Pushing the Boundaries of Speech Recognition with NVIDIA NeMo Parakeet ASR Models &lt;/a&gt; (2024/04/18) &lt;/summary&gt; NVIDIA NeMo, an end-to-end platform for the development of multimodal generative AI models at scale anywhere‚Äîon any cloud and on-premises‚Äîreleased the Parakeet family of automatic speech recognition (ASR) models. These state-of-the-art ASR models, developed in collaboration with Suno.ai, transcribe spoken English with exceptional accuracy. 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;a href="https://developer.nvidia.com/blog/turbocharge-asr-accuracy-and-speed-with-nvidia-nemo-parakeet-tdt/"&gt; Turbocharge ASR Accuracy and Speed with NVIDIA NeMo Parakeet-TDT &lt;/a&gt; (2024/04/18) &lt;/summary&gt; NVIDIA NeMo, an end-to-end platform for developing multimodal generative AI models at scale anywhere‚Äîon any cloud and on-premises‚Äîrecently released Parakeet-TDT. This new addition to the ‚ÄØNeMo ASR Parakeet model family boasts better accuracy and 64% greater speed over the previously best model, Parakeet-RNNT-1.1B. 
  &lt;br /&gt;
  &lt;br /&gt; 
 &lt;/details&gt; 
&lt;/details&gt; 
&lt;!-- markdownlint-enable --&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;NVIDIA NeMo Framework is a scalable and cloud-native generative AI framework built for researchers and PyTorch developers working on Large Language Models (LLMs), Multimodal Models (MMs), Automatic Speech Recognition (ASR), Text to Speech (TTS), and Computer Vision (CV) domains. It is designed to help you efficiently create, customize, and deploy new generative AI models by leveraging existing code and pre-trained model checkpoints.&lt;/p&gt; 
&lt;p&gt;For technical documentation, please see the &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/playbooks/index.html"&gt;NeMo Framework User Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;What's New in NeMo 2.0&lt;/h2&gt; 
&lt;p&gt;NVIDIA NeMo 2.0 introduces several significant improvements over its predecessor, NeMo 1.0, enhancing flexibility, performance, and scalability.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Python-Based Configuration&lt;/strong&gt; - NeMo 2.0 transitions from YAML files to a Python-based configuration, providing more flexibility and control. This shift makes it easier to extend and customize configurations programmatically.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Modular Abstractions&lt;/strong&gt; - By adopting PyTorch Lightning‚Äôs modular abstractions, NeMo 2.0 simplifies adaptation and experimentation. This modular approach allows developers to more easily modify and experiment with different components of their models.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Scalability&lt;/strong&gt; - NeMo 2.0 seamlessly scaling large-scale experiments across thousands of GPUs using &lt;a href="https://github.com/NVIDIA/NeMo-Run"&gt;NeMo-Run&lt;/a&gt;, a powerful tool designed to streamline the configuration, execution, and management of machine learning experiments across computing environments.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Overall, these enhancements make NeMo 2.0 a powerful, scalable, and user-friendly framework for AI model development.&lt;/p&gt; 
&lt;h3&gt;Get Started with NeMo 2.0&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Refer to the &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/nemo-2.0/quickstart.html"&gt;Quickstart&lt;/a&gt; for examples of using NeMo-Run to launch NeMo 2.0 experiments locally and on a slurm cluster.&lt;/li&gt; 
 &lt;li&gt;For more information about NeMo 2.0, see the &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/nemo-2.0/index.html"&gt;NeMo Framework User Guide&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;For an in-depth exploration of the main features of NeMo 2.0, see the &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/nemo-2.0/features/index.html#feature-guide"&gt;Feature Guide&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;To transition from NeMo 1.0 to 2.0, see the &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/nemo-2.0/migration/index.html#migration-guide"&gt;Migration Guide&lt;/a&gt; for step-by-step instructions.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Training and Customization&lt;/h2&gt; 
&lt;p&gt;All NeMo models are trained with &lt;a href="https://github.com/Lightning-AI/lightning"&gt;Lightning&lt;/a&gt;. Training is automatically scalable to 1000s of GPUs. You can check the performance benchmarks using the latest NeMo Framework container &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/performance/performance_summary.html"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;When applicable, NeMo models leverage cutting-edge distributed training techniques, incorporating &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/modeloverview.html"&gt;parallelism strategies&lt;/a&gt; to enable efficient training of very large models. These techniques include Tensor Parallelism (TP), Pipeline Parallelism (PP), Fully Sharded Data Parallelism (FSDP), Mixture-of-Experts (MoE), and Mixed Precision Training with BFloat16 and FP8, as well as others.&lt;/p&gt; 
&lt;p&gt;In addition to supervised fine-tuning (SFT), NeMo also supports the latest parameter efficient fine-tuning (PEFT) techniques such as LoRA, P-Tuning, Adapters, and IA3.&lt;/p&gt; 
&lt;h2&gt;Speech AI&lt;/h2&gt; 
&lt;p&gt;NeMo ASR and TTS models can be optimized for inference and deployed for production use cases with &lt;a href="https://developer.nvidia.com/riva"&gt;NVIDIA Riva&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Get Started with NeMo Framework&lt;/h2&gt; 
&lt;p&gt;Getting started with NeMo Framework is easy. State-of-the-art pretrained NeMo models are freely available on &lt;a href="https://huggingface.co/models?library=nemo&amp;amp;sort=downloads&amp;amp;search=nvidia"&gt;Hugging Face Hub&lt;/a&gt; and &lt;a href="https://catalog.ngc.nvidia.com/models?query=nemo&amp;amp;orderBy=weightPopularDESC"&gt;NVIDIA NGC&lt;/a&gt;. These models can be used to generate text or images, transcribe audio, and synthesize speech in just a few lines of code.&lt;/p&gt; 
&lt;p&gt;We have extensive &lt;a href="https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/starthere/tutorials.html"&gt;tutorials&lt;/a&gt; that can be run on &lt;a href="https://colab.research.google.com"&gt;Google Colab&lt;/a&gt; or with our &lt;a href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/nemo"&gt;NGC NeMo Framework Container&lt;/a&gt;. We also have &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/playbooks/index.html"&gt;playbooks&lt;/a&gt; for users who want to train NeMo models with the NeMo Framework Launcher.&lt;/p&gt; 
&lt;p&gt;For advanced users who want to train NeMo models from scratch or fine-tune existing NeMo models, we have a full suite of &lt;a href="https://github.com/NVIDIA/NeMo/tree/main/examples"&gt;example scripts&lt;/a&gt; that support multi-GPU/multi-node training.&lt;/p&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/NeMo/main/nemo/collections/multimodal/README.md"&gt;Multimodal&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/NeMo/main/nemo/collections/asr/README.md"&gt;Automatic Speech Recognition&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/NeMo/main/nemo/collections/tts/README.md"&gt;Text to Speech&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.10 or above&lt;/li&gt; 
 &lt;li&gt;Pytorch 2.5 or above&lt;/li&gt; 
 &lt;li&gt;NVIDIA GPU (if you intend to do model training)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Developer Documentation&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Version&lt;/th&gt; 
   &lt;th&gt;Status&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Latest&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/"&gt;&lt;img src="https://readthedocs.com/projects/nvidia-nemo/badge/?version=main" alt="Documentation Status" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/"&gt;Documentation of the latest (i.e. main) branch.&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Stable&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/"&gt;&lt;img src="https://readthedocs.com/projects/nvidia-nemo/badge/?version=stable" alt="Documentation Status" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/"&gt;Documentation of the stable (i.e. most recent release)&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Install NeMo Framework&lt;/h2&gt; 
&lt;p&gt;The NeMo Framework can be installed in a variety of ways, depending on your needs. Depending on the domain, you may find one of the following installation methods more suitable.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/NeMo/main/#conda--pip"&gt;Conda / Pip&lt;/a&gt;: Install NeMo-Framework with native Pip into a virtual environment. 
  &lt;ul&gt; 
   &lt;li&gt;Used to explore NeMo on any supported platform.&lt;/li&gt; 
   &lt;li&gt;This is the recommended method for ASR and TTS domains.&lt;/li&gt; 
   &lt;li&gt;Limited feature-completeness for other domains.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/NeMo/main/#ngc-pytorch-container"&gt;NGC PyTorch container&lt;/a&gt;: Install NeMo-Framework from source with feature-completeness into a highly optimized container. 
  &lt;ul&gt; 
   &lt;li&gt;For users that want to install from source in a highly optimized container.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/NeMo/main/#ngc-nemo-container"&gt;NGC NeMo container&lt;/a&gt;: Ready-to-go solution of NeMo-Framework 
  &lt;ul&gt; 
   &lt;li&gt;For users that seek highest performance.&lt;/li&gt; 
   &lt;li&gt;Contains all dependencies installed and tested for performance and convergence.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Support matrix&lt;/h3&gt; 
&lt;p&gt;NeMo-Framework provides tiers of support based on OS / Platform and mode of installation. Please refer the following overview of support levels:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Fully supported: Max performance and feature-completeness.&lt;/li&gt; 
 &lt;li&gt;Limited supported: Used to explore NeMo.&lt;/li&gt; 
 &lt;li&gt;No support yet: In development.&lt;/li&gt; 
 &lt;li&gt;Deprecated: Support has reached end of life.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Please refer to the following table for current support levels:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;OS / Platform&lt;/th&gt; 
   &lt;th&gt;Install from PyPi&lt;/th&gt; 
   &lt;th&gt;Source into NGC container&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;linux&lt;/code&gt; - &lt;code&gt;amd64/x84_64&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Limited support&lt;/td&gt; 
   &lt;td&gt;Full support&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;linux&lt;/code&gt; - &lt;code&gt;arm64&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Limited support&lt;/td&gt; 
   &lt;td&gt;Limited support&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;darwin&lt;/code&gt; - &lt;code&gt;amd64/x64_64&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Deprecated&lt;/td&gt; 
   &lt;td&gt;Deprecated&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;darwin&lt;/code&gt; - &lt;code&gt;arm64&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Limited support&lt;/td&gt; 
   &lt;td&gt;Limited support&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;windows&lt;/code&gt; - &lt;code&gt;amd64/x64_64&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;No support yet&lt;/td&gt; 
   &lt;td&gt;No support yet&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;windows&lt;/code&gt; - &lt;code&gt;arm64&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;No support yet&lt;/td&gt; 
   &lt;td&gt;No support yet&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Conda / Pip&lt;/h3&gt; 
&lt;p&gt;Install NeMo in a fresh Conda environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;conda create --name nemo python==3.10.12
conda activate nemo
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Pick the right version&lt;/h4&gt; 
&lt;p&gt;NeMo-Framework publishes pre-built wheels with each release. To install nemo_toolkit from such a wheel, use the following installation method:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install "nemo_toolkit[all]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If a more specific version is desired, we recommend a Pip-VCS install. From &lt;a href="https://github.com/NVIDIA/NeMo"&gt;NVIDIA/NeMo&lt;/a&gt;, fetch the commit, branch, or tag that you would like to install.&lt;br /&gt; To install nemo_toolkit from this Git reference &lt;code&gt;$REF&lt;/code&gt;, use the following installation method:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/NVIDIA/NeMo
cd NeMo
git checkout @${REF:-'main'}
pip install '.[all]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Install a specific Domain&lt;/h4&gt; 
&lt;p&gt;To install a specific domain of NeMo, you must first install the nemo_toolkit using the instructions listed above. Then, you run the following domain-specific commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install nemo_toolkit['all'] # or pip install "nemo_toolkit['all']@git+https://github.com/NVIDIA/NeMo@${REF:-'main'}"
pip install nemo_toolkit['asr'] # or pip install "nemo_toolkit['asr']@git+https://github.com/NVIDIA/NeMo@$REF:-'main'}"
pip install nemo_toolkit['tts'] # or pip install "nemo_toolkit['tts']@git+https://github.com/NVIDIA/NeMo@${REF:-'main'}"
pip install nemo_toolkit['multimodal'] # or pip install "nemo_toolkit['multimodal']@git+https://github.com/NVIDIA/NeMo@${REF:-'main'}"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;NGC PyTorch container&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;NOTE: The following steps are supported beginning with 25.09 (NeMo-Toolkit 2.6.0)&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;We recommended that you start with a base NVIDIA PyTorch container: nvcr.io/nvidia/pytorch:25.09-py3.&lt;/p&gt; 
&lt;p&gt;If starting with a base NVIDIA PyTorch container, you must first launch the container:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run \
  --gpus all \
  -it \
  --rm \
  --shm-size=16g \
  --ulimit memlock=-1 \
  --ulimit stack=67108864 \
  ${NV_PYTORCH_TAG:-'nvcr.io/nvidia/pytorch:25.09-py3'}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;From &lt;a href="https://github.com/NVIDIA/NeMo"&gt;NVIDIA/NeMo&lt;/a&gt;, fetch the commit/branch/tag that you want to install.&lt;br /&gt; To install nemo_toolkit including all of its dependencies from this Git reference &lt;code&gt;$REF&lt;/code&gt;, use the following installation method:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd /opt
git clone https://github.com/NVIDIA/NeMo
cd NeMo
git checkout ${REF:-'main'}
bash docker/common/install_dep.sh --library all
pip install ".[all]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;NGC NeMo container&lt;/h2&gt; 
&lt;p&gt;NeMo containers are launched concurrently with NeMo version updates. NeMo Framework now supports LLMs, MMs, ASR, and TTS in a single consolidated Docker container. The latest container is based on NeMo 2.6.0. You can find additional information about released containers on the &lt;a href="https://github.com/NVIDIA/NeMo/releases"&gt;NeMo releases page&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To use a pre-built container, run the following code:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run \
  --gpus all \
  -it \
  --rm \
  --shm-size=16g \
  --ulimit memlock=-1 \
  --ulimit stack=67108864 \
  nvcr.io/nvidia/nemo:25.11.01
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Discussions Board&lt;/h2&gt; 
&lt;p&gt;FAQ can be found on the NeMo &lt;a href="https://github.com/NVIDIA/NeMo/discussions"&gt;Discussions board&lt;/a&gt;. You are welcome to ask questions or start discussions on the board.&lt;/p&gt; 
&lt;h2&gt;Contribute to NeMo&lt;/h2&gt; 
&lt;p&gt;We welcome community contributions! Please refer to &lt;a href="https://github.com/NVIDIA/NeMo/raw/stable/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for the process.&lt;/p&gt; 
&lt;h2&gt;Publications&lt;/h2&gt; 
&lt;p&gt;We provide an ever-growing list of &lt;a href="https://nvidia.github.io/NeMo/publications/"&gt;publications&lt;/a&gt; that utilize the NeMo Framework.&lt;/p&gt; 
&lt;p&gt;To contribute an article to the collection, please submit a pull request to the &lt;code&gt;gh-pages-src&lt;/code&gt; branch of this repository. For detailed information, please consult the README located at the &lt;a href="https://github.com/NVIDIA/NeMo/tree/gh-pages-src#readme"&gt;gh-pages-src branch&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Licenses&lt;/h2&gt; 
&lt;p&gt;NeMo is licensed under the &lt;a href="https://github.com/NVIDIA/NeMo?tab=Apache-2.0-1-ov-file"&gt;Apache License 2.0&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>modelscope/ms-swift</title>
      <link>https://github.com/modelscope/ms-swift</link>
      <description>&lt;p&gt;Use PEFT or Full-parameter to CPT/SFT/DPO/GRPO 600+ LLMs (Qwen3, Qwen3-MoE, DeepSeek-R1, GLM4.5, InternLM3, Llama4, ...) and 300+ MLLMs (Qwen3-VL, Qwen3-Omni, InternVL3.5, Ovis2.5, GLM4.5v, Llava, Phi4, ...) (AAAI 2025).&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;SWIFT (Scalable lightWeight Infrastructure for Fine-Tuning)&lt;/h1&gt; 
&lt;p align="center"&gt; &lt;br /&gt; &lt;img src="https://raw.githubusercontent.com/modelscope/ms-swift/main/asset/banner.png" /&gt; &lt;br /&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p align="center"&gt; &lt;a href="https://modelscope.cn/home"&gt;ModelScope Community Website&lt;/a&gt; &lt;br /&gt; &lt;a href="https://raw.githubusercontent.com/modelscope/ms-swift/main/README_CN.md"&gt;‰∏≠Êñá&lt;/a&gt; &amp;nbsp; ÔΩú &amp;nbsp; English &amp;nbsp; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://img.shields.io/badge/python-3.11-5be.svg?sanitize=true" /&gt; &lt;img src="https://img.shields.io/badge/pytorch-%E2%89%A52.0-orange.svg?sanitize=true" /&gt; &lt;a href="https://github.com/modelscope/modelscope/"&gt;&lt;img src="https://img.shields.io/badge/modelscope-%E2%89%A51.23-5D91D4.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/ms-swift/"&gt;&lt;img src="https://badge.fury.io/py/ms-swift.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://github.com/modelscope/ms-swift/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/modelscope/ms-swift" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/ms-swift"&gt;&lt;img src="https://pepy.tech/badge/ms-swift" /&gt;&lt;/a&gt; &lt;a href="https://github.com/modelscope/ms-swift/pulls"&gt;&lt;img src="https://img.shields.io/badge/PR-welcome-55EB99.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/6427" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/6427" alt="modelscope%2Fswift | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://arxiv.org/abs/2408.05517"&gt;Paper&lt;/a&gt; &amp;nbsp; ÔΩú &lt;a href="https://swift.readthedocs.io/en/latest/"&gt;English Documentation&lt;/a&gt; &amp;nbsp; ÔΩú &amp;nbsp; &lt;a href="https://swift.readthedocs.io/zh-cn/latest/"&gt;‰∏≠ÊñáÊñáÊ°£&lt;/a&gt; &amp;nbsp; &lt;/p&gt; 
&lt;h2&gt;üìñ Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/ms-swift/main/#-Groups"&gt;Groups&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/ms-swift/main/#-introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/ms-swift/main/#-news"&gt;News&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/ms-swift/main/#%EF%B8%8F-installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/ms-swift/main/#-quick-Start"&gt;Quick Start&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/ms-swift/main/#-Usage"&gt;Usage&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/ms-swift/main/#-License"&gt;License&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/ms-swift/main/#-citation"&gt;Citation&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;‚òé Groups&lt;/h2&gt; 
&lt;p&gt;You can contact us and communicate with us by adding our group:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;&lt;a href="https://discord.com/invite/D27yfEFVz5"&gt;Discord Group&lt;/a&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;WeChat Group&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/modelscope/ms-swift/main/asset/discord_qr.jpg" width="200" height="200" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/modelscope/ms-swift/main/asset/wechat.png" width="200" height="200" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;üìù Introduction&lt;/h2&gt; 
&lt;p&gt;üç≤ &lt;strong&gt;ms-swift&lt;/strong&gt; is a large model and multimodal large model fine-tuning and deployment framework provided by the ModelScope community. It now supports training (pre-training, fine-tuning, human alignment), inference, evaluation, quantization, and deployment for 600+ text-only large models and 300+ multimodal large models. Large models include: Qwen3, Qwen3-Next, InternLM3, GLM4.5, Mistral, DeepSeek-R1, Llama4, etc. Multimodal large models include: Qwen3-VL, Qwen3-Omni, Llava, InternVL3.5, MiniCPM-V-4, Ovis2.5, GLM4.5-V, DeepSeek-VL2, etc.&lt;/p&gt; 
&lt;p&gt;üçî In addition, ms-swift integrates the latest training technologies, including Megatron parallelism techniques such as TP, PP, CP, EP to accelerate training, as well as numerous GRPO algorithm family reinforcement learning algorithms including: GRPO, DAPO, GSPO, SAPO, CISPO, RLOO, Reinforce++, etc. to enhance model intelligence. ms-swift supports a wide range of training tasks, including preference learning algorithms such as DPO, KTO, RM, CPO, SimPO, ORPO, as well as Embedding, Reranker, and sequence classification tasks. ms-swift provides full-pipeline support for large model training, including acceleration for inference, evaluation, and deployment modules using vLLM, SGLang, and LMDeploy, as well as model quantization using GPTQ, AWQ, BNB, and FP8 technologies.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Why Choose ms-swift?&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üçé &lt;strong&gt;Model Types&lt;/strong&gt;: Supports &lt;strong&gt;600+ text-only large models&lt;/strong&gt;, &lt;strong&gt;300+ multimodal large models&lt;/strong&gt;, and All-to-All full modality models from training to deployment full pipeline, with Day-0 support for popular models.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Dataset Types&lt;/strong&gt;: Built-in 150+ datasets for pre-training, fine-tuning, human alignment, multimodal and various other tasks, with support for custom datasets. Users only need to prepare datasets for one-click training.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Hardware Support&lt;/strong&gt;: Supports A10/A100/H100, RTX series, T4/V100, CPU, MPS, and domestic hardware Ascend NPU, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Lightweight Training&lt;/strong&gt;: Supports lightweight fine-tuning methods such as LoRA, QLoRA, DoRA, LoRA+, LLaMAPro, LongLoRA, LoRA-GA, ReFT, RS-LoRA, Adapter, LISA, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Quantized Training&lt;/strong&gt;: Supports training on BNB, AWQ, GPTQ, AQLM, HQQ, EETQ quantized models, requiring only 9GB training resources for 7B models.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Memory Optimization&lt;/strong&gt;: GaLore, Q-Galore, UnSloth, Liger-Kernel, Flash-Attention 2/3, and &lt;strong&gt;Ulysses and Ring-Attention sequence parallelism techniques&lt;/strong&gt; support, reducing memory consumption for long-text training.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Distributed Training&lt;/strong&gt;: Supports distributed data parallelism (DDP), device_map simple model parallelism, DeepSpeed ZeRO2 ZeRO3, FSDP/FSDP2, and Megatron distributed training technologies.&lt;/li&gt; 
 &lt;li&gt;üçì &lt;strong&gt;Multimodal Training&lt;/strong&gt;: Supports multimodal packing technology to improve training speed by 100%+, supports mixed modality data training with text, images, video and audio, and supports independent control of vit/aligner/llm.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Agent Training&lt;/strong&gt;: Supports Agent templates, allowing one dataset to be used for training different models.&lt;/li&gt; 
 &lt;li&gt;üçä &lt;strong&gt;Training Tasks&lt;/strong&gt;: Supports pre-training and instruction fine-tuning, as well as training tasks such as DPO, GKD, KTO, RM, CPO, SimPO, ORPO, and supports &lt;strong&gt;Embedding/Reranker&lt;/strong&gt; and sequence classification tasks.&lt;/li&gt; 
 &lt;li&gt;ü•• &lt;strong&gt;Megatron Parallelism&lt;/strong&gt;: Provides TP/PP/SP/CP/ETP/EP/VPP parallel strategies, &lt;strong&gt;MoE model acceleration up to 10x&lt;/strong&gt;. Supports full-parameter and LoRA training methods for 250+ text-only large models and 100+ multimodal large models. Supports CPT/SFT/GRPO/DPO/KTO/RM training tasks.&lt;/li&gt; 
 &lt;li&gt;üçâ &lt;strong&gt;Reinforcement Learning&lt;/strong&gt;: Built-in &lt;strong&gt;rich GRPO family algorithms&lt;/strong&gt;, including GRPO, DAPO, GSPO, SAPO, CISPO, CHORD, RLOO, Reinforce++, etc. Supports synchronous and asynchronous vLLM engine inference acceleration, with extensible reward functions, multi-turn inference Schedulers, and environments through plugins.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Full-Pipeline Capabilities&lt;/strong&gt;: Covers the entire workflow of training, inference, evaluation, quantization, and deployment.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;UI Training&lt;/strong&gt;: Provides Web-UI interface for training, inference, evaluation, and quantization, completing the full pipeline for large models.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Inference Acceleration&lt;/strong&gt;: Supports PyTorch, vLLM, SGLang, and LmDeploy inference acceleration engines, providing OpenAI interfaces for accelerating inference, deployment, and evaluation modules.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Model Evaluation&lt;/strong&gt;: Uses EvalScope as the evaluation backend, supporting 100+ evaluation datasets for evaluating text-only and multimodal models.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Model Quantization&lt;/strong&gt;: Supports quantization export for AWQ, GPTQ, FP8, and BNB. Exported models support inference acceleration using vLLM/SGLang/LmDeploy.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üéâ News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;üéÅ 2025.11.14: Megatron GRPO is now available! Check out the &lt;a href="https://raw.githubusercontent.com/modelscope/ms-swift/main/docs/source_en/Megatron-SWIFT/GRPO.md"&gt;docs&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/modelscope/ms-swift/main/examples/megatron/grpo"&gt;examples&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;üéÅ 2025.11.04: Support for &lt;a href="https://raw.githubusercontent.com/modelscope/ms-swift/main/docs/source_en/Megatron-SWIFT/Mcore-Bridge.md"&gt;Mcore-Bridge&lt;/a&gt;, making Megatron training as simple and easy to use as transformers.&lt;/li&gt; 
 &lt;li&gt;üéÅ 2025.10.28: Ray &lt;a href="https://raw.githubusercontent.com/modelscope/ms-swift/main/docs/source_en/Instruction/Ray.md"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;üéÅ 2025.09.07: Added support for CHORD training algorithm. See the &lt;a href="https://raw.githubusercontent.com/modelscope/ms-swift/main/docs/source_en/Instruction/GRPO/AdvancedResearch/CHORD.md"&gt;documentation&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;üéÅ 2025.09.06: Ulysses can now be used with ring-attention, allowing sequences to be sharded into any number of chunks (no longer limited by the number of heads). The argument remains &lt;code&gt;--sequence_parallel_size N&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;üéÅ 2025.09.02: Megatron-SWIFT now supports multimodal model training. Documentation can be found &lt;a href="https://raw.githubusercontent.com/modelscope/ms-swift/main/docs/source_en/Megatron-SWIFT/Multimodal-Model.md"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;üéÅ 2025.08.12: Support &lt;a href="https://arxiv.org/abs/2508.05629"&gt;Dynamic Fine-Tuning&lt;/a&gt;(DFT) in SFT training, use parameter &lt;code&gt;--enable_dft_loss true&lt;/code&gt;. Training scripts can be found &lt;a href="https://github.com/modelscope/ms-swift/raw/main/examples/train/full/dft.sh"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;üéÅ 2025.07.09: Megatron-SWIFT supports LoRA training. Compared to ms-swift, it achieves significant speedup on MoE models. Training scripts can be found &lt;a href="https://github.com/modelscope/ms-swift/raw/main/examples/megatron/lora"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;üéÅ 2025.06.23: Fine-tuning of reranker models is supported. Training scripts can be found here: &lt;a href="https://github.com/modelscope/ms-swift/raw/main/examples/train/reranker/train_reranker.sh"&gt;Reranker&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;üéÅ 2025.06.15: Support for GKD training on both pure text large models and multimodal models. Training scripts can be found here: &lt;a href="https://github.com/modelscope/ms-swift/raw/main/examples/train/rlhf/gkd"&gt;Pure Text&lt;/a&gt;, &lt;a href="https://github.com/modelscope/ms-swift/raw/main/examples/train/multimodal/rlhf/gkd"&gt;Multimodal&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt;
 &lt;summary&gt;More&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;üéÅ 2025.06.11: Support for using Megatron parallelism techniques for RLHF training. The training script can be found &lt;a href="https://github.com/modelscope/ms-swift/tree/main/examples/megatron/rlhf"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;üéÅ 2025.05.29: Support sequence parallel in pt, sft, dpo and grpo, check script &lt;a href="https://github.com/modelscope/ms-swift/tree/main/examples/train/sequence_parallel"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;üéÅ 2025.05.11: GRPO now supports custom processing logic for reward models. See the GenRM example &lt;a href="https://raw.githubusercontent.com/modelscope/ms-swift/main/docs/source_en/Instruction/GRPO/DeveloperGuide/reward_model.md"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;üéÅ 2025.04.15: The ms-swift paper has been accepted by AAAI 2025. You can find the paper at &lt;a href="https://ojs.aaai.org/index.php/AAAI/article/view/35383"&gt;this link&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;üéÅ 2025.03.23: Multi-round GRPO is now supported for training multi-turn dialogue scenarios (e.g., agent tool calling). Please refer to the &lt;a href="https://raw.githubusercontent.com/modelscope/ms-swift/main/docs/source_en/Instruction/GRPO/DeveloperGuide/multi_turn.md"&gt;doc&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;üéÅ 2025.03.16: Support for Megatron's parallel training techniques is now available. Please see the &lt;a href="https://swift.readthedocs.io/en/latest/Megatron-SWIFT/Quick-start.html"&gt;Megatron-SWIFT training documentation&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;üéÅ 2025.03.15: Fine-tuning of embedding models for both pure text and multimodal models is supported. Please check the &lt;a href="https://raw.githubusercontent.com/modelscope/ms-swift/main/examples/train/embedding"&gt;training script&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;üéÅ 2025.03.05: The hybrid mode for GRPO is supported, with a script for training a 72B model on 4 GPUs (4*80G) available &lt;a href="https://raw.githubusercontent.com/modelscope/ms-swift/main/examples/train/grpo/internal/vllm_72b_4gpu.sh"&gt;here&lt;/a&gt;. Tensor parallelism with vllm is also supported, with the training script available &lt;a href="https://raw.githubusercontent.com/modelscope/ms-swift/main/examples/train/grpo/internal"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;üéÅ 2025.02.21: The GRPO algorithm now supports LMDeploy, with the training script available &lt;a href="https://raw.githubusercontent.com/modelscope/ms-swift/main/examples/train/grpo/internal/full_lmdeploy.sh"&gt;here&lt;/a&gt;. Additionally, the performance of the GRPO algorithm has been tested, achieving a training speed increase of up to 300% using various tricks. Please check the WanDB table &lt;a href="https://wandb.ai/tastelikefeet/grpo_perf_test?nw=nwuseryuzezyz"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;üéÅ 2025.02.21: The &lt;code&gt;swift sample&lt;/code&gt; command is now supported. The reinforcement fine-tuning script can be found &lt;a href="https://raw.githubusercontent.com/modelscope/ms-swift/main/docs/source_en/Instruction/Reinforced-Fine-tuning.md"&gt;here&lt;/a&gt;, and the large model API distillation sampling script is available &lt;a href="https://raw.githubusercontent.com/modelscope/ms-swift/main/examples/sampler/distill/distill.sh"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;üî• 2025.02.12: Support for the GRPO (Group Relative Policy Optimization) training algorithm has been added. Documentation is available &lt;a href="https://raw.githubusercontent.com/modelscope/ms-swift/main/docs/source_en/Instruction/GRPO/GetStarted/GRPO.md"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;üéÅ 2024.12.04: Major update to &lt;strong&gt;ms-swift 3.0&lt;/strong&gt;. Please refer to the &lt;a href="https://raw.githubusercontent.com/modelscope/ms-swift/main/docs/source_en/Instruction/ReleaseNote3.0.md"&gt;release notes and changes&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;üéâ 2024.08.12: The ms-swift paper has been published on arXiv and can be read &lt;a href="https://arxiv.org/abs/2408.05517"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;üî• 2024.08.05: Support for using &lt;a href="https://github.com/modelscope/evalscope/"&gt;evalscope&lt;/a&gt; as a backend for evaluating large models and multimodal models.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;üî• 2024.07.29: Support for using &lt;a href="https://github.com/vllm-project/vllm"&gt;vllm&lt;/a&gt; and &lt;a href="https://github.com/InternLM/lmdeploy"&gt;lmdeploy&lt;/a&gt; to accelerate inference for large models and multimodal models. When performing infer/deploy/eval, you can specify &lt;code&gt;--infer_backend vllm/lmdeploy&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;üî• 2024.07.24: Support for human preference alignment training for multimodal large models, including DPO/ORPO/SimPO/CPO/KTO/RM/PPO.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;üî• 2024.02.01: Support for Agent training! The training algorithm is derived from &lt;a href="https://arxiv.org/pdf/2309.00986.pdf"&gt;this paper&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;üõ†Ô∏è Installation&lt;/h2&gt; 
&lt;p&gt;To install using pip:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install ms-swift -U
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To install from source:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# pip install git+https://github.com/modelscope/ms-swift.git

git clone https://github.com/modelscope/ms-swift.git
cd ms-swift
pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Running Environment:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;Range&lt;/th&gt; 
   &lt;th&gt;Recommended&lt;/th&gt; 
   &lt;th&gt;Notes&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;python&lt;/td&gt; 
   &lt;td&gt;&amp;gt;=3.9&lt;/td&gt; 
   &lt;td&gt;3.10/3.11&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;cuda&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;cuda12&lt;/td&gt; 
   &lt;td&gt;No need to install if using CPU, NPU, MPS&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;torch&lt;/td&gt; 
   &lt;td&gt;&amp;gt;=2.0&lt;/td&gt; 
   &lt;td&gt;2.8.0/2.9.0&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;transformers&lt;/td&gt; 
   &lt;td&gt;&amp;gt;=4.33&lt;/td&gt; 
   &lt;td&gt;4.57.3&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;modelscope&lt;/td&gt; 
   &lt;td&gt;&amp;gt;=1.23&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;peft&lt;/td&gt; 
   &lt;td&gt;&amp;gt;=0.11,&amp;lt;0.19&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;flash_attn&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;2.8.3/3.0.0b1&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;trl&lt;/td&gt; 
   &lt;td&gt;&amp;gt;=0.15,&amp;lt;0.25&lt;/td&gt; 
   &lt;td&gt;0.24.0&lt;/td&gt; 
   &lt;td&gt;RLHF&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;deepspeed&lt;/td&gt; 
   &lt;td&gt;&amp;gt;=0.14&lt;/td&gt; 
   &lt;td&gt;0.17.6&lt;/td&gt; 
   &lt;td&gt;Training&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;vllm&lt;/td&gt; 
   &lt;td&gt;&amp;gt;=0.5.1&lt;/td&gt; 
   &lt;td&gt;0.11.0/0.13.0&lt;/td&gt; 
   &lt;td&gt;Inference/Deployment&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;sglang&lt;/td&gt; 
   &lt;td&gt;&amp;gt;=0.4.6&lt;/td&gt; 
   &lt;td&gt;0.5.5.post3&lt;/td&gt; 
   &lt;td&gt;Inference/Deployment&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;lmdeploy&lt;/td&gt; 
   &lt;td&gt;&amp;gt;=0.5&lt;/td&gt; 
   &lt;td&gt;0.10.1&lt;/td&gt; 
   &lt;td&gt;Inference/Deployment&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;evalscope&lt;/td&gt; 
   &lt;td&gt;&amp;gt;=1.0&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;Evaluation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;gradio&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;5.32.1&lt;/td&gt; 
   &lt;td&gt;Web-UI/App&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;For more optional dependencies, you can refer to &lt;a href="https://github.com/modelscope/ms-swift/raw/main/requirements/install_all.sh"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;üöÄ Quick Start&lt;/h2&gt; 
&lt;p&gt;10 minutes of self-cognition fine-tuning of Qwen2.5-7B-Instruct on a single 3090 GPU:&lt;/p&gt; 
&lt;h3&gt;Command Line Interface (Recommended)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# 22GB
CUDA_VISIBLE_DEVICES=0 \
swift sft \
    --model Qwen/Qwen2.5-7B-Instruct \
    --train_type lora \
    --dataset 'AI-ModelScope/alpaca-gpt4-data-zh#500' \
              'AI-ModelScope/alpaca-gpt4-data-en#500' \
              'swift/self-cognition#500' \
    --torch_dtype bfloat16 \
    --num_train_epochs 1 \
    --per_device_train_batch_size 1 \
    --per_device_eval_batch_size 1 \
    --learning_rate 1e-4 \
    --lora_rank 8 \
    --lora_alpha 32 \
    --target_modules all-linear \
    --gradient_accumulation_steps 16 \
    --eval_steps 50 \
    --save_steps 50 \
    --save_total_limit 2 \
    --logging_steps 5 \
    --max_length 2048 \
    --output_dir output \
    --system 'You are a helpful assistant.' \
    --warmup_ratio 0.05 \
    --dataloader_num_workers 4 \
    --model_author swift \
    --model_name swift-robot
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Tips:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;If you want to train with a custom dataset, you can refer to &lt;a href="https://swift.readthedocs.io/en/latest/Customization/Custom-dataset.html"&gt;this guide&lt;/a&gt; to organize your dataset format and specify &lt;code&gt;--dataset &amp;lt;dataset_path&amp;gt;&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;The &lt;code&gt;--model_author&lt;/code&gt; and &lt;code&gt;--model_name&lt;/code&gt; parameters are only effective when the dataset includes &lt;code&gt;swift/self-cognition&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;To train with a different model, simply modify &lt;code&gt;--model &amp;lt;model_id/model_path&amp;gt;&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;By default, ModelScope is used for downloading models and datasets. If you want to use HuggingFace, simply specify &lt;code&gt;--use_hf true&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;After training is complete, use the following command to infer with the trained weights:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Here, &lt;code&gt;--adapters&lt;/code&gt; should be replaced with the last checkpoint folder generated during training. Since the adapters folder contains the training parameter file &lt;code&gt;args.json&lt;/code&gt;, there is no need to specify &lt;code&gt;--model&lt;/code&gt;, &lt;code&gt;--system&lt;/code&gt; separately; Swift will automatically read these parameters. To disable this behavior, you can set &lt;code&gt;--load_args false&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# Using an interactive command line for inference.
CUDA_VISIBLE_DEVICES=0 \
swift infer \
    --adapters output/vx-xxx/checkpoint-xxx \
    --stream true \
    --temperature 0 \
    --max_new_tokens 2048

# merge-lora and use vLLM for inference acceleration
CUDA_VISIBLE_DEVICES=0 \
swift infer \
    --adapters output/vx-xxx/checkpoint-xxx \
    --stream true \
    --merge_lora true \
    --infer_backend vllm \
    --vllm_max_model_len 8192 \
    --temperature 0 \
    --max_new_tokens 2048
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Finally, use the following command to push the model to ModelScope:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;CUDA_VISIBLE_DEVICES=0 \
swift export \
    --adapters output/vx-xxx/checkpoint-xxx \
    --push_to_hub true \
    --hub_model_id '&amp;lt;your-model-id&amp;gt;' \
    --hub_token '&amp;lt;your-sdk-token&amp;gt;' \
    --use_hf false
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Web-UI&lt;/h3&gt; 
&lt;p&gt;The Web-UI is a &lt;strong&gt;zero-threshold&lt;/strong&gt; training and deployment interface solution based on Gradio interface technology. For more details, you can check &lt;a href="https://swift.readthedocs.io/en/latest/GetStarted/Web-UI.html"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;SWIFT_UI_LANG=en swift web-ui
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/modelscope/ms-swift/main/docs/resources/web-ui-en.jpg" alt="image.png" /&gt;&lt;/p&gt; 
&lt;h3&gt;Using Python&lt;/h3&gt; 
&lt;p&gt;ms-swift also supports training and inference using Python. Below is pseudocode for training and inference. For more details, you can refer to &lt;a href="https://github.com/modelscope/ms-swift/raw/main/examples/notebook/qwen2_5-self-cognition/self-cognition-sft.ipynb"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Training:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Retrieve the model and template, and add a trainable LoRA module
model, tokenizer = get_model_tokenizer(model_id_or_path, ...)
template = get_template(model.model_meta.template, tokenizer, ...)
model = Swift.prepare_model(model, lora_config)

# Download and load the dataset, and encode the text into tokens
train_dataset, val_dataset = load_dataset(dataset_id_or_path, ...)
train_dataset = EncodePreprocessor(template=template)(train_dataset, num_proc=num_proc)
val_dataset = EncodePreprocessor(template=template)(val_dataset, num_proc=num_proc)

# Train the model
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    data_collator=template.data_collator,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    template=template,
)
trainer.train()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Inference:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Perform inference using the native PyTorch engine
engine = PtEngine(model_id_or_path, adapters=[lora_checkpoint])
infer_request = InferRequest(messages=[{'role': 'user', 'content': 'who are you?'}])
request_config = RequestConfig(max_tokens=max_new_tokens, temperature=temperature)

resp_list = engine.infer([infer_request], request_config)
print(f'response: {resp_list[0].choices[0].message.content}')
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;‚ú® Usage&lt;/h2&gt; 
&lt;p&gt;Here is a minimal example of training to deployment using ms-swift. For more details, you can check the &lt;a href="https://github.com/modelscope/ms-swift/tree/main/examples"&gt;examples&lt;/a&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;If you want to use other models or datasets (including multimodal models and datasets), you only need to modify &lt;code&gt;--model&lt;/code&gt; to specify the corresponding model's ID or path, and modify &lt;code&gt;--dataset&lt;/code&gt; to specify the corresponding dataset's ID or path.&lt;/li&gt; 
 &lt;li&gt;By default, ModelScope is used for downloading models and datasets. If you want to use HuggingFace, simply specify &lt;code&gt;--use_hf true&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Useful Links&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://swift.readthedocs.io/en/latest/Instruction/Command-line-parameters.html"&gt;üî•Command Line Parameters&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://swift.readthedocs.io/en/latest/Megatron-SWIFT/Quick-start.html"&gt;Megatron-SWIFT&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://swift.readthedocs.io/en/latest/Instruction/GRPO/GetStarted/GRPO.html"&gt;GRPO&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://swift.readthedocs.io/en/latest/Instruction/Supported-models-and-datasets.html"&gt;Supported Models and Datasets&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://swift.readthedocs.io/en/latest/Customization/Custom-model.html"&gt;Custom Models&lt;/a&gt;, &lt;a href="https://swift.readthedocs.io/en/latest/Customization/Custom-dataset.html"&gt;üî•Custom Datasets&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/modelscope/modelscope-classroom/tree/main/LLM-tutorial"&gt;LLM Tutorial&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Training&lt;/h3&gt; 
&lt;p&gt;Supported Training Methods:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Method&lt;/th&gt; 
   &lt;th&gt;Full-Parameter&lt;/th&gt; 
   &lt;th&gt;LoRA&lt;/th&gt; 
   &lt;th&gt;QLoRA&lt;/th&gt; 
   &lt;th&gt;Deepspeed&lt;/th&gt; 
   &lt;th&gt;Multi-Machine&lt;/th&gt; 
   &lt;th&gt;Multimodal&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/modelscope/ms-swift/raw/main/examples/train/pretrain"&gt;Pre-training&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/modelscope/ms-swift/raw/main/examples/train/lora_sft.sh"&gt;Supervised Fine-Tuning&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/modelscope/ms-swift/raw/main/examples/train/full/train.sh"&gt;‚úÖ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/modelscope/ms-swift/tree/main/examples/train/qlora"&gt;‚úÖ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/modelscope/ms-swift/tree/main/examples/train/multi-gpu/deepspeed"&gt;‚úÖ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/modelscope/ms-swift/tree/main/examples/train/multi-node"&gt;‚úÖ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/modelscope/ms-swift/tree/main/examples/train/multimodal"&gt;‚úÖ&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/modelscope/ms-swift/raw/main/examples/train/grpo"&gt;GRPO&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/modelscope/ms-swift/raw/main/examples/train/rlhf/gkd"&gt;GKD&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/modelscope/ms-swift/raw/main/examples/train/multimodal/rlhf/gkd"&gt;‚úÖ&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/modelscope/ms-swift/raw/main/examples/train/rlhf/ppo"&gt;PPO&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/modelscope/ms-swift/raw/main/examples/train/rlhf/dpo"&gt;DPO&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/modelscope/ms-swift/raw/main/examples/train/multimodal/rlhf/dpo"&gt;‚úÖ&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/modelscope/ms-swift/raw/main/examples/train/rlhf/kto.sh"&gt;KTO&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/modelscope/ms-swift/raw/main/examples/train/multimodal/rlhf/kto.sh"&gt;‚úÖ&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/modelscope/ms-swift/raw/main/examples/train/rlhf/rm.sh"&gt;Reward Model&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/modelscope/ms-swift/raw/main/examples/train/rlhf/cpo.sh"&gt;CPO&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/modelscope/ms-swift/raw/main/examples/train/rlhf/simpo.sh"&gt;SimPO&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/modelscope/ms-swift/raw/main/examples/train/rlhf/orpo.sh"&gt;ORPO&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/modelscope/ms-swift/raw/main/examples/train/seq_cls"&gt;Sequence Classification&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/modelscope/ms-swift/raw/main/examples/train/embedding"&gt;Embedding&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/modelscope/ms-swift/tree/main/examples/train/reranker"&gt;Reranker&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Pre-training:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# 8*A100
NPROC_PER_NODE=8 \
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 \
swift pt \
    --model Qwen/Qwen2.5-7B \
    --dataset swift/chinese-c4 \
    --streaming true \
    --train_type full \
    --deepspeed zero2 \
    --output_dir output \
    --max_steps 10000 \
    ...
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Fine-tuning:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;CUDA_VISIBLE_DEVICES=0 swift sft \
    --model Qwen/Qwen2.5-7B-Instruct \
    --dataset AI-ModelScope/alpaca-gpt4-data-en \
    --train_type lora \
    --output_dir output \
    ...
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;RLHF:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;CUDA_VISIBLE_DEVICES=0 swift rlhf \
    --rlhf_type dpo \
    --model Qwen/Qwen2.5-7B-Instruct \
    --dataset hjh0119/shareAI-Llama3-DPO-zh-en-emoji \
    --train_type lora \
    --output_dir output \
    ...
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Megatron-SWIFT&lt;/h3&gt; 
&lt;p&gt;ms-swift supports using Megatron parallelism techniques to accelerate training, including large-scale cluster training and MoE model training. The following training methods are supported:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Method&lt;/th&gt; 
   &lt;th&gt;Full-Parameter&lt;/th&gt; 
   &lt;th&gt;LoRA&lt;/th&gt; 
   &lt;th&gt;MoE&lt;/th&gt; 
   &lt;th&gt;Multimodal&lt;/th&gt; 
   &lt;th&gt;FP8&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Pre-training&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/modelscope/ms-swift/tree/main/examples/megatron"&gt;Supervised Fine-Tuning&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/modelscope/ms-swift/tree/main/examples/megatron/grpo"&gt;GRPO&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/modelscope/ms-swift/tree/main/examples/megatron/rlhf/dpo"&gt;DPO&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/modelscope/ms-swift/tree/main/examples/megatron/rlhf/kto"&gt;KTO&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/modelscope/ms-swift/tree/main/examples/megatron/rlhf/rm"&gt;RM&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/modelscope/ms-swift/tree/main/examples/megatron/seq_cls"&gt;Sequence Classification&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;NPROC_PER_NODE=2 CUDA_VISIBLE_DEVICES=0,1 megatron sft \
    --model Qwen/Qwen2.5-7B-Instruct \
    --load_safetensors true \
    --save_safetensors true \
    --dataset AI-ModelScope/alpaca-gpt4-data-zh \
    --train_type lora \
    --save output \
    ...
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Reinforcement Learning&lt;/h3&gt; 
&lt;p&gt;ms-swift supports a rich set of GRPO family algorithms:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Method&lt;/th&gt; 
   &lt;th&gt;Full-Parameter&lt;/th&gt; 
   &lt;th&gt;LoRA&lt;/th&gt; 
   &lt;th&gt;Multimodal&lt;/th&gt; 
   &lt;th&gt;Multi-Machine&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://swift.readthedocs.io/en/latest/Instruction/GRPO/GetStarted/GRPO.html"&gt;GRPO&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://swift.readthedocs.io/en/latest/Instruction/GRPO/AdvancedResearch/DAPO.html"&gt;DAPO&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://swift.readthedocs.io/en/latest/Instruction/GRPO/AdvancedResearch/GSPO.html"&gt;GSPO&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://swift.readthedocs.io/en/latest/Instruction/GRPO/AdvancedResearch/SAPO.html"&gt;SAPO&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://swift.readthedocs.io/en/latest/Instruction/GRPO/AdvancedResearch/CISPO.html"&gt;CISPO&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://swift.readthedocs.io/en/latest/Instruction/GRPO/AdvancedResearch/CHORD.html"&gt;CHORD&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://swift.readthedocs.io/en/latest/Instruction/GRPO/AdvancedResearch/RLOO.html"&gt;RLOO&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://swift.readthedocs.io/en/latest/Instruction/GRPO/AdvancedResearch/REINFORCEPP.html"&gt;Reinforce++&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;CUDA_VISIBLE_DEVICES=0,1,2,3 NPROC_PER_NODE=4 \
swift rlhf \
    --rlhf_type grpo \
    --model Qwen/Qwen2.5-7B-Instruct \
    --train_type lora \
    --use_vllm true \
    --vllm_mode colocate \
    --dataset AI-MO/NuminaMath-TIR#10000 \
    --output_dir output \
    ...
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Inference&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;CUDA_VISIBLE_DEVICES=0 swift infer \
    --model Qwen/Qwen2.5-7B-Instruct \
    --stream true \
    --infer_backend pt \
    --max_new_tokens 2048

# LoRA
CUDA_VISIBLE_DEVICES=0 swift infer \
    --model Qwen/Qwen2.5-7B-Instruct \
    --adapters swift/test_lora \
    --stream true \
    --infer_backend pt \
    --temperature 0 \
    --max_new_tokens 2048
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Interface Inference&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;CUDA_VISIBLE_DEVICES=0 swift app \
    --model Qwen/Qwen2.5-7B-Instruct \
    --stream true \
    --infer_backend pt \
    --max_new_tokens 2048
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Deployment&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;CUDA_VISIBLE_DEVICES=0 swift deploy \
    --model Qwen/Qwen2.5-7B-Instruct \
    --infer_backend vllm
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Sampling&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;CUDA_VISIBLE_DEVICES=0 swift sample \
    --model LLM-Research/Meta-Llama-3.1-8B-Instruct \
    --sampler_engine pt \
    --num_return_sequences 5 \
    --dataset AI-ModelScope/alpaca-gpt4-data-zh#5
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Evaluation&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;CUDA_VISIBLE_DEVICES=0 swift eval \
    --model Qwen/Qwen2.5-7B-Instruct \
    --infer_backend lmdeploy \
    --eval_backend OpenCompass \
    --eval_dataset ARC_c
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Quantization&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;CUDA_VISIBLE_DEVICES=0 swift export \
    --model Qwen/Qwen2.5-7B-Instruct \
    --quant_bits 4 --quant_method awq \
    --dataset AI-ModelScope/alpaca-gpt4-data-zh \
    --output_dir Qwen2.5-7B-Instruct-AWQ
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Push Model&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;swift export \
    --model &amp;lt;model-path&amp;gt; \
    --push_to_hub true \
    --hub_model_id '&amp;lt;model-id&amp;gt;' \
    --hub_token '&amp;lt;sdk-token&amp;gt;'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;üèõ License&lt;/h2&gt; 
&lt;p&gt;This framework is licensed under the &lt;a href="https://github.com/modelscope/modelscope/raw/master/LICENSE"&gt;Apache License (Version 2.0)&lt;/a&gt;. For models and datasets, please refer to the original resource page and follow the corresponding License.&lt;/p&gt; 
&lt;h2&gt;üìé Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{zhao2024swiftascalablelightweightinfrastructure,
      title={SWIFT:A Scalable lightWeight Infrastructure for Fine-Tuning},
      author={Yuze Zhao and Jintao Huang and Jinghan Hu and Xingjun Wang and Yunlin Mao and Daoze Zhang and Zeyinzi Jiang and Zhikai Wu and Baole Ai and Ang Wang and Wenmeng Zhou and Yingda Chen},
      year={2024},
      eprint={2408.05517},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.05517},
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#modelscope/ms-swift&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=modelscope/swift&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>HKUDS/VideoRAG</title>
      <link>https://github.com/HKUDS/VideoRAG</link>
      <description>&lt;p&gt;[KDD'2026] "VideoRAG: Chat with Your Videos"&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;picture&gt; 
  &lt;img src="https://raw.githubusercontent.com/HKUDS/VideoRAG/main/cover.png" width="80%" style="border: none; box-shadow: none;" alt="Vimo: Chat with Your Videos" /&gt; 
 &lt;/picture&gt; 
 &lt;h1&gt; &lt;strong&gt;VideoRAG: Chat with Your Videos&lt;/strong&gt; ‚Ä¢ &lt;strong&gt;Vimo Desktop&lt;/strong&gt; &lt;/h1&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/16146" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/16146" alt="HKUDS%2FVideoRAG | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://arxiv.org/abs/2502.01549"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2502.01549-b31b1b" /&gt;&lt;/a&gt; &lt;a href="https://github.com/HKUDS/VideoRAG/issues/1"&gt;&lt;img src="https://img.shields.io/badge/Áæ§ËÅä-wechat/feishu-green" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/ZzU55kz3"&gt;&lt;img src="https://discordapp.com/api/guilds/1296348098003734629/widget.png?style=shield" /&gt;&lt;/a&gt; &lt;a href="https://www.youtube.com/watch?v=D5vsxcp4QZI"&gt;&lt;img src="https://img.shields.io/badge/YouTube-Watch%20Demo-red?style=flat&amp;amp;logo=youtube" /&gt;&lt;/a&gt; &lt;a href="https://learnopencv.com/videorag-long-context-video-comprehension/"&gt;&lt;img src="https://img.shields.io/badge/Blog-LearnOpenCV-blue?style=flat&amp;amp;logo=data:image/png;base64,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" alt="Blog" /&gt;&lt;/a&gt; &lt;a href=""&gt;&lt;img src="https://img.shields.io/badge/platform-macOS%20%7C%20Windows%20%7C%20Linux-lightgrey.svg?sanitize=true" alt="Platform" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;üé¨ Intelligent Video Conversations | Powered by Advanced AI | Extreme Long-Context Processing&lt;/strong&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;img src="https://raw.githubusercontent.com/HKUDS/VideoRAG/main/VideoRAG-algorithm/VideoRAG_cover.png" /&gt; 
&lt;p&gt;Vimo is a revolutionary desktop application that lets you &lt;strong&gt;chat with your videos&lt;/strong&gt; using cutting-edge AI technology. Built on the powerful &lt;a href="https://arxiv.org/abs/2502.01549"&gt;VideoRAG framework&lt;/a&gt;, Vimo can understand and analyze videos of any length - from short clips to hundreds of hours of content - and answer your questions with remarkable accuracy.&lt;/p&gt; 
&lt;h3&gt;üé• Watch Vimo in Action&lt;/h3&gt; 
&lt;p&gt;See how Vimo transforms video interaction with intelligent conversations and deep understanding capabilities.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://www.youtube.com/watch?v=D5vsxcp4QZI"&gt; &lt;img src="https://img.youtube.com/vi/D5vsxcp4QZI/maxresdefault.jpg" width="80%" alt="Vimo Introduction Video" /&gt; &lt;/a&gt; 
 &lt;p&gt;&lt;em&gt;üëÜ Click to watch the Vimo demo video&lt;/em&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;‚ú® Key Features&lt;/h2&gt; 
&lt;h3&gt;For Everyone&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Drag &amp;amp; Drop Upload&lt;/strong&gt;: Simply drag video files into Vimo&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Smart Conversations&lt;/strong&gt;: Ask questions in natural language&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-Format Support&lt;/strong&gt;: Works with MP4, MKV, AVI, and more&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Cross-Platform&lt;/strong&gt;: Available on macOS, Windows, and Linux&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;For Power Users&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Extreme Long Videos&lt;/strong&gt;: Process videos up to hundreds of hours&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-Video Analysis&lt;/strong&gt;: Compare and analyze multiple videos simultaneously&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Advanced Retrieval&lt;/strong&gt;: Find specific moments and scenes with precision&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Export Capabilities&lt;/strong&gt;: Save insights and references for later use&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;For Researchers&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;VideoRAG Framework&lt;/strong&gt;: Access to cutting-edge retrieval-augmented generation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Benchmark Dataset&lt;/strong&gt;: LongerVideos benchmark with 134+ hours of content&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Performance Metrics&lt;/strong&gt;: Detailed evaluation against existing methods&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Extensible Architecture&lt;/strong&gt;: Build upon our open-source foundation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üåü Why Vimo?&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;For Video Enthusiasts &amp;amp; Professionals:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Effortless Video Analysis&lt;/strong&gt;: Upload any video and start asking questions immediately&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Natural Conversations&lt;/strong&gt;: Chat with your videos as if talking to a human expert&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;No Length Limits&lt;/strong&gt;: Process everything from 30-second clips to 100+ hour documentaries&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Deep Understanding&lt;/strong&gt;: Combines visual content, audio, and context for comprehensive answers&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;For Researchers &amp;amp; Developers:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;State-of-the-Art Algorithm&lt;/strong&gt;: Built on VideoRAG, featuring graph-driven knowledge indexing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Benchmark Performance&lt;/strong&gt;: Evaluated on 134+ hours across lectures, documentaries, and entertainment&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Open Source&lt;/strong&gt;: Full access to VideoRAG implementation and research findings&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Scalable Architecture&lt;/strong&gt;: Efficient processing with single GPU (RTX 3090) capability&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìã Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/VideoRAG/main/#-quick-start"&gt;üöÄ Quick Start&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/VideoRAG/main/#-key-features"&gt;‚ú® Key Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/VideoRAG/main/#-videorag-algorithm"&gt;üî¨ VideoRAG Algorithm&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/VideoRAG/main/#%EF%B8%8F-development-setup"&gt;üõ†Ô∏è Development Setup&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/VideoRAG/main/#-benchmarks--evaluation"&gt;üß™ Benchmarks &amp;amp; Evaluation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/VideoRAG/main/#-citation"&gt;üìñ Citation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/VideoRAG/main/#-contributing"&gt;ü§ù Contributing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/VideoRAG/main/#-acknowledgement"&gt;üôè Acknowledgement&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üöÄ Quick Start of Vimo&lt;/h2&gt; 
&lt;h3&gt;Option 1: Download Vimo App (Coming Soon)&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] We are preparing the &lt;strong&gt;Beta release&lt;/strong&gt; for macOS Apple Silicon first, with Windows and Linux versions coming soon!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;div align="left"&gt; 
 &lt;a href="https://github.com/HKUDS/Vimo/releases"&gt; &lt;img src="https://img.shields.io/badge/Coming%20Soon-Mac%20Download-007ACC?style=for-the-badge&amp;amp;logo=apple&amp;amp;logoColor=white" alt="Coming Soon - Mac Release" height="50" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h3&gt;Option 2: Run from Source Code&lt;/h3&gt; 
&lt;p&gt;For detailed setup instructions:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Vimo Desktop App&lt;/strong&gt;: See &lt;a href="https://raw.githubusercontent.com/HKUDS/VideoRAG/main/Vimo-desktop"&gt;Vimo-desktop&lt;/a&gt; for complete installation and configuration steps&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Quick Overview:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Set up the Python backend environment and start the VideoRAG server&lt;/li&gt; 
 &lt;li&gt;Launch the Electron frontend application&lt;/li&gt; 
 &lt;li&gt;Start chatting with your videos!&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;üî¨ VideoRAG Algorithm&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/VideoRAG/main/VideoRAG-algorithm/VideoRAG.png" alt="VideoRAG Architecture" width="80%" /&gt; &lt;/p&gt; 
&lt;p&gt;VideoRAG introduces a novel dual-channel architecture that combines:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Graph-Driven Knowledge Indexing&lt;/strong&gt;: Multi-modal knowledge graphs for structured video understanding&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Hierarchical Context Encoding&lt;/strong&gt;: Preserves spatiotemporal visual patterns across long sequences&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Adaptive Retrieval&lt;/strong&gt;: Dynamic retrieval mechanisms optimized for video content&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Cross-Video Understanding&lt;/strong&gt;: Semantic relationship modeling across multiple videos&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Technical Highlights&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Efficient Processing&lt;/strong&gt;: Handle hundreds of hours on a single RTX 3090 (24GB)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Structured Indexing&lt;/strong&gt;: Distill long videos into concise knowledge representations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-Modal Retrieval&lt;/strong&gt;: Align textual queries with visual and audio content&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;LongerVideos Benchmark&lt;/strong&gt;: 160+ videos, 134+ hours across diverse domains&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Performance Comparison&lt;/h3&gt; 
&lt;p&gt;Our VideoRAG algorithm significantly outperforms existing methods in long-context video understanding:&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/HKUDS/VideoRAG/main/Vimo-desktop/figures/table.png" width="80%" alt="Performance Comparison" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;Experiments and Evaluation&lt;/h3&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/HKUDS/VideoRAG/main/VideoRAG-algorithm"&gt;VideoRAG-algorithm&lt;/a&gt; for detailed development setup including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Conda environment creation&lt;/li&gt; 
 &lt;li&gt;Model checkpoints download&lt;/li&gt; 
 &lt;li&gt;Dependencies installation&lt;/li&gt; 
 &lt;li&gt;Evaluation scripts&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üß™ LongerVideos Benchmark&lt;/h2&gt; 
&lt;p&gt;We created the LongerVideos benchmark to evaluate long-context video understanding:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Video Type&lt;/th&gt; 
   &lt;th&gt;#Collections&lt;/th&gt; 
   &lt;th&gt;#Videos&lt;/th&gt; 
   &lt;th&gt;#Queries&lt;/th&gt; 
   &lt;th&gt;Avg. Duration&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Lectures&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;12&lt;/td&gt; 
   &lt;td&gt;135&lt;/td&gt; 
   &lt;td&gt;376&lt;/td&gt; 
   &lt;td&gt;~64.3 hours&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Documentaries&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;12&lt;/td&gt; 
   &lt;td&gt;114&lt;/td&gt; 
   &lt;td&gt;~28.5 hours&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Entertainment&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;17&lt;/td&gt; 
   &lt;td&gt;112&lt;/td&gt; 
   &lt;td&gt;~41.9 hours&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Total&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;22&lt;/td&gt; 
   &lt;td&gt;164&lt;/td&gt; 
   &lt;td&gt;602&lt;/td&gt; 
   &lt;td&gt;~134.6 hours&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;For detailed evaluation instructions and reproduction scripts, see &lt;a href="https://raw.githubusercontent.com/HKUDS/VideoRAG/main/VideoRAG-algorithm/reproduce"&gt;VideoRAG-algorithm/reproduce&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;üìñ Citation&lt;/h2&gt; 
&lt;p&gt;If you find Vimo or VideoRAG helpful in your research, please cite our paper:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@article{VideoRAG,
  title={VideoRAG: Retrieval-Augmented Generation with Extreme Long-Context Videos},
  author={Ren, Xubin and Xu, Lingrui and Xia, Long and Wang, Shuaiqiang and Yin, Dawei and Huang, Chao},
  journal={arXiv preprint arXiv:2502.01549},
  year={2025}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions from the community! Whether you're:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Reporting bugs&lt;/strong&gt; or suggesting features for Vimo&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Improving VideoRAG algorithms&lt;/strong&gt; or adding new capabilities&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Enhancing documentation&lt;/strong&gt; or creating tutorials&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Designing UI/UX improvements&lt;/strong&gt; for better user experience&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Feel free to submit issues and pull requests. Together, we're building the future of intelligent video interaction!&lt;/p&gt; 
&lt;h2&gt;üôè Acknowledgement&lt;/h2&gt; 
&lt;p&gt;Vimo builds upon the incredible work of the open-source community:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://arxiv.org/abs/2502.01549"&gt;VideoRAG&lt;/a&gt;&lt;/strong&gt;: The core algorithm powering Vimo's intelligence&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/gusye1234/nano-graphrag"&gt;nano-graphrag&lt;/a&gt;&lt;/strong&gt; &amp;amp; &lt;strong&gt;&lt;a href="https://github.com/HKUDS/LightRAG"&gt;LightRAG&lt;/a&gt;&lt;/strong&gt;: Graph-based retrieval foundations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/facebookresearch/ImageBind"&gt;ImageBind&lt;/a&gt;&lt;/strong&gt;: Multi-modal representation learning&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/bytedance/UI-TARS-desktop"&gt;uitars-desktop&lt;/a&gt;&lt;/strong&gt;: Desktop application architecture inspiration&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;üåü Transform how you interact with videos. Start your journey with Vimo today!&lt;/strong&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;sub&gt;Built with ‚ù§Ô∏è by the VideoRAG@HKUDS team.&lt;/sub&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>NevaMind-AI/memU</title>
      <link>https://github.com/NevaMind-AI/memU</link>
      <description>&lt;p&gt;Memory infrastructure for LLMs and AI agents&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/banner.png" alt="MemU Banner" /&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;h1&gt;MemU&lt;/h1&gt; 
 &lt;h3&gt;A Future-Oriented Agentic Memory System&lt;/h3&gt; 
 &lt;p&gt;&lt;a href="https://badge.fury.io/py/memu-py"&gt;&lt;img src="https://badge.fury.io/py/memu-py.svg?sanitize=true" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://opensource.org/licenses/Apache-2.0"&gt;&lt;img src="https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true" alt="License: Apache 2.0" /&gt;&lt;/a&gt; &lt;a href="https://www.python.org/downloads/"&gt;&lt;img src="https://img.shields.io/badge/python-3.13+-blue.svg?sanitize=true" alt="Python 3.13+" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/memu"&gt;&lt;img src="https://img.shields.io/badge/Discord-Join%20Chat-5865F2?logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://x.com/memU_ai"&gt;&lt;img src="https://img.shields.io/badge/Twitter-Follow-1DA1F2?logo=x&amp;amp;logoColor=white" alt="Twitter" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/17374" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/17374" alt="NevaMind-AI%2FmemU | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;p&gt;MemU is an agentic memory framework for LLM and AI agent backends. It receives &lt;strong&gt;multimodal inputs&lt;/strong&gt; (conversations, documents, images), extracts them into structured memory, and organizes them into a &lt;strong&gt;hierarchical file system&lt;/strong&gt; that supports both &lt;strong&gt;embedding-based (RAG)&lt;/strong&gt; and &lt;strong&gt;non-embedding (LLM)&lt;/strong&gt; retrieval.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;‚≠êÔ∏è Star the repository&lt;/h2&gt; 
&lt;img width="100%" src="https://github.com/NevaMind-AI/memU/raw/main/assets/star.gif" /&gt; If you find memU useful or interesting, a GitHub Star ‚≠êÔ∏è would be greatly appreciated. 
&lt;hr /&gt; 
&lt;p&gt;MemU is collaborating with four open-source projects to launch the 2026 New Year Challenge. üéâBetween January 8‚Äì18, contributors can submit PRs to memU and earn cash rewards, community recognition, and platform credits. üéÅ&lt;a href="https://discord.gg/KaWy6SBAsx"&gt;Learn more &amp;amp; get involved&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;‚ú® Core Features&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Feature&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üóÇÔ∏è &lt;strong&gt;Hierarchical File System&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Three-layer architecture: Resource ‚Üí Item ‚Üí Category with full traceability&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üîç &lt;strong&gt;Dual Retrieval Methods&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;RAG (embedding-based) for speed, LLM (non-embedding) for deep semantic understanding&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üé® &lt;strong&gt;Multimodal Support&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Process conversations, documents, images, audio, and video&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üîÑ &lt;strong&gt;Self-Evolving Memory&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Memory structure adapts and improves based on usage patterns&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üóÇÔ∏è Hierarchical File System&lt;/h2&gt; 
&lt;p&gt;MemU organizes memory using a &lt;strong&gt;three-layer architecture&lt;/strong&gt; inspired by hierarchical storage systems:&lt;/p&gt; 
&lt;img width="100%" alt="structure" src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/structure.png" /&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Layer&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Examples&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Resource&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Raw multimodal data warehouse&lt;/td&gt; 
   &lt;td&gt;JSON conversations, text documents, images, videos&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Item&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Discrete extracted memory units&lt;/td&gt; 
   &lt;td&gt;Individual preferences, skills, opinions, habits&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Category&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Aggregated textual memory with summaries&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;preferences.md&lt;/code&gt;, &lt;code&gt;work_life.md&lt;/code&gt;, &lt;code&gt;relationships.md&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Key Benefits:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Full Traceability&lt;/strong&gt;: Track from raw data ‚Üí items ‚Üí categories and back&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Progressive Summarization&lt;/strong&gt;: Each layer provides increasingly abstracted views&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible Organization&lt;/strong&gt;: Categories evolve based on content patterns&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üé® Multimodal Support&lt;/h2&gt; 
&lt;p&gt;MemU processes diverse content types into unified memory:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Modality&lt;/th&gt; 
   &lt;th&gt;Input&lt;/th&gt; 
   &lt;th&gt;Processing&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;conversation&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;JSON chat logs&lt;/td&gt; 
   &lt;td&gt;Extract preferences, opinions, habits, relationships&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;document&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Text files (.txt, .md)&lt;/td&gt; 
   &lt;td&gt;Extract knowledge, skills, facts&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;image&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;PNG, JPG, etc.&lt;/td&gt; 
   &lt;td&gt;Vision model extracts visual concepts and descriptions&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;video&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Video files&lt;/td&gt; 
   &lt;td&gt;Frame extraction + vision analysis&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;audio&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Audio files&lt;/td&gt; 
   &lt;td&gt;Transcription + text processing&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;All modalities are unified into the same three-layer hierarchy, enabling cross-modal retrieval.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üöÄ Quick Start&lt;/h2&gt; 
&lt;h3&gt;Option 1: Cloud Version&lt;/h3&gt; 
&lt;p&gt;Try MemU instantly without any setup:&lt;/p&gt; 
&lt;p&gt;üëâ &lt;strong&gt;&lt;a href="https://memu.so"&gt;memu.so&lt;/a&gt;&lt;/strong&gt; - Hosted cloud service with full API access&lt;/p&gt; 
&lt;p&gt;For enterprise deployment and custom solutions, contact &lt;strong&gt;&lt;a href="mailto:info@nevamind.ai"&gt;info@nevamind.ai&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h4&gt;Cloud API (v3)&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Base URL&lt;/th&gt; 
   &lt;th&gt;&lt;code&gt;https://api.memu.so&lt;/code&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Auth&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;Authorization: Bearer YOUR_API_KEY&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Method&lt;/th&gt; 
   &lt;th&gt;Endpoint&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;POST&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;/api/v3/memory/memorize&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register a memorization task&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;GET&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;/api/v3/memory/memorize/status/{task_id}&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Get task status&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;POST&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;/api/v3/memory/categories&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;List memory categories&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;POST&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;/api/v3/memory/retrieve&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Retrieve memories (semantic search)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;üìö &lt;strong&gt;&lt;a href="https://memu.pro/docs#cloud-version"&gt;Full API Documentation&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Option 2: Self-Hosted&lt;/h3&gt; 
&lt;h4&gt;Installation&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Basic Example&lt;/h4&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Requirements&lt;/strong&gt;: Python 3.13+ and an OpenAI API key&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Test with In-Memory Storage&lt;/strong&gt; (no database required):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export OPENAI_API_KEY=your_api_key
cd tests
python test_inmemory.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Test with PostgreSQL Storage&lt;/strong&gt; (requires pgvector):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Start PostgreSQL with pgvector
docker run -d \
  --name memu-postgres \
  -e POSTGRES_USER=postgres \
  -e POSTGRES_PASSWORD=postgres \
  -e POSTGRES_DB=memu \
  -p 5432:5432 \
  pgvector/pgvector:pg16

# Run the test
export OPENAI_API_KEY=your_api_key
cd tests
python test_postgres.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Both examples demonstrate the complete workflow:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Memorize&lt;/strong&gt;: Process a conversation file and extract structured memory&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Retrieve (RAG)&lt;/strong&gt;: Fast embedding-based search&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Retrieve (LLM)&lt;/strong&gt;: Deep semantic understanding search&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/NevaMind-AI/memU/main/tests/test_inmemory.py"&gt;&lt;code&gt;tests/test_inmemory.py&lt;/code&gt;&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/NevaMind-AI/memU/main/tests/test_postgres.py"&gt;&lt;code&gt;tests/test_postgres.py&lt;/code&gt;&lt;/a&gt; for the full source code.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Custom LLM and Embedding Providers&lt;/h3&gt; 
&lt;p&gt;MemU supports custom LLM and embedding providers beyond OpenAI. Configure them via &lt;code&gt;llm_profiles&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from memu import MemUService

service = MemUService(
    llm_profiles={
        # Default profile for LLM operations
        "default": {
            "base_url": "https://dashscope.aliyuncs.com/compatible-mode/v1",
            "api_key": "your_api_key",
            "chat_model": "qwen3-max",
            "client_backend": "sdk"  # "sdk" or "http"
        },
        # Separate profile for embeddings
        "embedding": {
            "base_url": "https://api.voyageai.com/v1",
            "api_key": "your_voyage_api_key",
            "embed_model": "voyage-3.5-lite"
        }
    },
    # ... other configuration
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìñ Core APIs&lt;/h2&gt; 
&lt;h3&gt;&lt;code&gt;memorize()&lt;/code&gt; - Extract and Store Memory&lt;/h3&gt; 
&lt;p&gt;Processes input resources and extracts structured memory:&lt;/p&gt; 
&lt;img width="100%" alt="memorize" src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/memorize.png" /&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;result = await service.memorize(
    resource_url="path/to/file.json",  # File path or URL
    modality="conversation",            # conversation | document | image | video | audio
    user={"user_id": "123"}             # Optional: scope to a user
)

# Returns:
{
    "resource": {...},      # Stored resource metadata
    "items": [...],         # Extracted memory items
    "categories": [...]     # Updated category summaries
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;&lt;code&gt;retrieve()&lt;/code&gt; - Query Memory&lt;/h3&gt; 
&lt;p&gt;Retrieves relevant memory based on queries. MemU supports &lt;strong&gt;two retrieval strategies&lt;/strong&gt;:&lt;/p&gt; 
&lt;img width="100%" alt="retrieve" src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/retrieve.png" /&gt; 
&lt;h4&gt;RAG-based Retrieval (&lt;code&gt;method="rag"&lt;/code&gt;)&lt;/h4&gt; 
&lt;p&gt;Fast &lt;strong&gt;embedding vector search&lt;/strong&gt; using cosine similarity:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;Fast&lt;/strong&gt;: Pure vector computation&lt;/li&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;Scalable&lt;/strong&gt;: Efficient for large memory stores&lt;/li&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;Returns scores&lt;/strong&gt;: Each result includes similarity score&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;LLM-based Retrieval (&lt;code&gt;method="llm"&lt;/code&gt;)&lt;/h4&gt; 
&lt;p&gt;Deep &lt;strong&gt;semantic understanding&lt;/strong&gt; through direct LLM reasoning:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;Deep understanding&lt;/strong&gt;: LLM comprehends context and nuance&lt;/li&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;Query rewriting&lt;/strong&gt;: Automatically refines query at each tier&lt;/li&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;Adaptive&lt;/strong&gt;: Stops early when sufficient information is found&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Comparison&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Aspect&lt;/th&gt; 
   &lt;th&gt;RAG&lt;/th&gt; 
   &lt;th&gt;LLM&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Speed&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;‚ö° Fast&lt;/td&gt; 
   &lt;td&gt;üê¢ Slower&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Cost&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;üí∞ Low&lt;/td&gt; 
   &lt;td&gt;üí∞üí∞ Higher&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Semantic depth&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Medium&lt;/td&gt; 
   &lt;td&gt;Deep&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Tier 2 scope&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;All items&lt;/td&gt; 
   &lt;td&gt;Only items in relevant categories&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Output&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;With similarity scores&lt;/td&gt; 
   &lt;td&gt;Ranked by LLM reasoning&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Both methods support:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Context-aware rewriting&lt;/strong&gt;: Resolves pronouns using conversation history&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Progressive search&lt;/strong&gt;: Categories ‚Üí Items ‚Üí Resources&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Sufficiency checking&lt;/strong&gt;: Stops when enough information is retrieved&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Usage&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;result = await service.retrieve(
    queries=[
        {"role": "user", "content": {"text": "What are their preferences?"}},
        {"role": "user", "content": {"text": "Tell me about work habits"}}
    ],
    where={"user_id": "123"}  # Optional: scope filter
)

# Returns:
{
    "categories": [...],     # Relevant categories (with scores for RAG)
    "items": [...],          # Relevant memory items
    "resources": [...],      # Related raw resources
    "next_step_query": "..." # Rewritten query for follow-up (if applicable)
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Scope Filtering&lt;/strong&gt;: Use &lt;code&gt;where&lt;/code&gt; to filter by user model fields:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;where={"user_id": "123"}&lt;/code&gt; - exact match&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;where={"agent_id__in": ["1", "2"]}&lt;/code&gt; - match any in list&lt;/li&gt; 
 &lt;li&gt;Omit &lt;code&gt;where&lt;/code&gt; to retrieve across all scopes&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;üìö &lt;strong&gt;For complete API documentation&lt;/strong&gt;, see &lt;a href="https://raw.githubusercontent.com/NevaMind-AI/memU/main/docs/SERVICE_API.md"&gt;SERVICE_API.md&lt;/a&gt; - includes all methods, CRUD operations, pipeline configuration, and configuration types.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üí° Use Cases&lt;/h2&gt; 
&lt;h3&gt;Example 1: Conversation Memory&lt;/h3&gt; 
&lt;p&gt;Extract and organize memory from multi-turn conversations:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export OPENAI_API_KEY=your_api_key
python examples/example_1_conversation_memory.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Processes multiple conversation JSON files&lt;/li&gt; 
 &lt;li&gt;Extracts memory items (preferences, habits, opinions, relationships)&lt;/li&gt; 
 &lt;li&gt;Generates category markdown files (&lt;code&gt;preferences.md&lt;/code&gt;, &lt;code&gt;work_life.md&lt;/code&gt;, etc.)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Best for:&lt;/strong&gt; Personal AI assistants, customer support bots, social chatbots&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Example 2: Skill Extraction from Logs&lt;/h3&gt; 
&lt;p&gt;Extract skills and lessons learned from agent execution logs:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export OPENAI_API_KEY=your_api_key
python examples/example_2_skill_extraction.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Processes agent logs sequentially&lt;/li&gt; 
 &lt;li&gt;Extracts actions, outcomes, and lessons learned&lt;/li&gt; 
 &lt;li&gt;Demonstrates &lt;strong&gt;incremental learning&lt;/strong&gt; - memory evolves with each file&lt;/li&gt; 
 &lt;li&gt;Generates evolving skill guides (&lt;code&gt;log_1.md&lt;/code&gt; ‚Üí &lt;code&gt;log_2.md&lt;/code&gt; ‚Üí &lt;code&gt;skill.md&lt;/code&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Best for:&lt;/strong&gt; DevOps teams, agent self-improvement, knowledge management&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Example 3: Multimodal Memory&lt;/h3&gt; 
&lt;p&gt;Process diverse content types into unified memory:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export OPENAI_API_KEY=your_api_key
python examples/example_3_multimodal_memory.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Processes documents and images together&lt;/li&gt; 
 &lt;li&gt;Extracts memory from different content types&lt;/li&gt; 
 &lt;li&gt;Unifies into cross-modal categories (&lt;code&gt;technical_documentation&lt;/code&gt;, &lt;code&gt;visual_diagrams&lt;/code&gt;, etc.)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Best for:&lt;/strong&gt; Documentation systems, learning platforms, research tools&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìä Performance&lt;/h2&gt; 
&lt;p&gt;MemU achieves &lt;strong&gt;92.09% average accuracy&lt;/strong&gt; on the Locomo benchmark across all reasoning tasks.&lt;/p&gt; 
&lt;img width="100%" alt="benchmark" src="https://github.com/user-attachments/assets/6fec4884-94e5-4058-ad5c-baac3d7e76d9" /&gt; 
&lt;p&gt;View detailed experimental data: &lt;a href="https://github.com/NevaMind-AI/memU-experiment"&gt;memU-experiment&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üß© Ecosystem&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Repository&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Use Case&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;&lt;a href="https://github.com/NevaMind-AI/memU"&gt;memU&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Core algorithm engine&lt;/td&gt; 
   &lt;td&gt;Embed AI memory into your product&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;&lt;a href="https://github.com/NevaMind-AI/memU-server"&gt;memU-server&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Backend service with CRUD, user system, RBAC&lt;/td&gt; 
   &lt;td&gt;Self-host a memory backend&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;&lt;a href="https://github.com/NevaMind-AI/memU-ui"&gt;memU-ui&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Visual dashboard&lt;/td&gt; 
   &lt;td&gt;Ready-to-use memory console&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Quick Links:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üöÄ &lt;a href="https://app.memu.so/quick-start"&gt;Try MemU Cloud&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üìö &lt;a href="https://memu.pro/docs"&gt;API Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üí¨ &lt;a href="https://discord.gg/memu"&gt;Discord Community&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ü§ù Partners&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/TEN-framework/ten-framework"&gt;&lt;img src="https://avatars.githubusercontent.com/u/113095513?s=200&amp;amp;v=4" alt="Ten" height="40" style="margin: 10px;" /&gt;&lt;/a&gt; &lt;a href="GitHub - openagents-org/openagents: OpenAgents - AI Agent Networks for Open Collaboration"&gt;&lt;img src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/partners/openagents.png" alt="OpenAgents" height="40" style="margin: 10px;" /&gt;&lt;/a&gt; &lt;a href="https://github.com/milvus-io/milvus"&gt;&lt;img src="https://miro.medium.com/v2/resize:fit:2400/1*-VEGyAgcIBD62XtZWavy8w.png" alt="Milvus" height="40" style="margin: 10px;" /&gt;&lt;/a&gt; &lt;a href="https://xroute.ai/"&gt;&lt;img src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/partners/xroute.png" alt="xRoute" height="40" style="margin: 10px;" /&gt;&lt;/a&gt; &lt;a href="https://jaaz.app/"&gt;&lt;img src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/partners/jazz.png" alt="Jazz" height="40" style="margin: 10px;" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Buddie-AI/Buddie"&gt;&lt;img src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/partners/buddie.png" alt="Buddie" height="40" style="margin: 10px;" /&gt;&lt;/a&gt; &lt;a href="https://github.com/bytebase/bytebase"&gt;&lt;img src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/partners/bytebase.png" alt="Bytebase" height="40" style="margin: 10px;" /&gt;&lt;/a&gt; &lt;a href="https://github.com/LazyAGI/LazyLLM"&gt;&lt;img src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/partners/LazyLLM.png" alt="LazyLLM" height="40" style="margin: 10px;" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìÑ License&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/NevaMind-AI/memU/main/LICENSE.txt"&gt;Apache License 2.0&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üåç Community&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;GitHub Issues&lt;/strong&gt;: &lt;a href="https://raw.githubusercontent.com/NevaMind-AI/memU/main/NevaMind-AI/memU"&gt;Report bugs &amp;amp; request features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Discord&lt;/strong&gt;: &lt;a href="https://discord.com/invite/hQZntfGsbJ"&gt;Join the community&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;X (Twitter)&lt;/strong&gt;: &lt;a href="https://x.com/memU_ai"&gt;Follow @memU_ai&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Contact&lt;/strong&gt;: &lt;a href="mailto:info@nevamind.ai"&gt;info@nevamind.ai&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;‚≠ê &lt;strong&gt;Star us on GitHub&lt;/strong&gt; to get notified about new releases!&lt;/p&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>ultralytics/yolov5</title>
      <link>https://github.com/ultralytics/yolov5</link>
      <description>&lt;p&gt;YOLOv5 üöÄ in PyTorch &gt; ONNX &gt; CoreML &gt; TFLite&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt; &lt;a href="https://www.ultralytics.com/events/yolovision?utm_source=github&amp;amp;utm_medium=org&amp;amp;utm_campaign=yv25_event" target="_blank"&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/banner-yolov8.png" alt="Ultralytics YOLO banner" /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://docs.ultralytics.com/zh/"&gt;‰∏≠Êñá&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/ko/"&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/ja/"&gt;Êó•Êú¨Ë™û&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/ru/"&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/de/"&gt;Deutsch&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/fr/"&gt;Fran√ßais&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/es"&gt;Espa√±ol&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/pt/"&gt;Portugu√™s&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/tr/"&gt;T√ºrk√ße&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/vi/"&gt;Ti·∫øng Vi·ªát&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/ar/"&gt;ÿßŸÑÿπÿ±ÿ®Ÿäÿ©&lt;/a&gt;&lt;/p&gt; 
 &lt;div&gt; 
  &lt;a href="https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml"&gt;&lt;img src="https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg?sanitize=true" alt="YOLOv5 CI Testing" /&gt;&lt;/a&gt; 
  &lt;a href="https://zenodo.org/badge/latestdoi/264818686"&gt;&lt;img src="https://zenodo.org/badge/264818686.svg?sanitize=true" alt="YOLOv5 Citation" /&gt;&lt;/a&gt; 
  &lt;a href="https://hub.docker.com/r/ultralytics/yolov5"&gt;&lt;img src="https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker" alt="Docker Pulls" /&gt;&lt;/a&gt; 
  &lt;a href="https://discord.com/invite/ultralytics"&gt;&lt;img alt="Discord" src="https://img.shields.io/discord/1089800235347353640?logo=discord&amp;amp;logoColor=white&amp;amp;label=Discord&amp;amp;color=blue" /&gt;&lt;/a&gt; 
  &lt;a href="https://community.ultralytics.com/"&gt;&lt;img alt="Ultralytics Forums" src="https://img.shields.io/discourse/users?server=https%3A%2F%2Fcommunity.ultralytics.com&amp;amp;logo=discourse&amp;amp;label=Forums&amp;amp;color=blue" /&gt;&lt;/a&gt; 
  &lt;a href="https://www.reddit.com/r/ultralytics/"&gt;&lt;img alt="Ultralytics Reddit" src="https://img.shields.io/reddit/subreddit-subscribers/ultralytics?style=flat&amp;amp;logo=reddit&amp;amp;logoColor=white&amp;amp;label=Reddit&amp;amp;color=blue" /&gt;&lt;/a&gt; 
  &lt;br /&gt; 
  &lt;a href="https://bit.ly/yolov5-paperspace-notebook"&gt;&lt;img src="https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true" alt="Run on Gradient" /&gt;&lt;/a&gt; 
  &lt;a href="https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt; 
  &lt;a href="https://www.kaggle.com/models/ultralytics/yolov5"&gt;&lt;img src="https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true" alt="Open In Kaggle" /&gt;&lt;/a&gt; 
 &lt;/div&gt; 
 &lt;br /&gt; 
 &lt;p&gt;Ultralytics YOLOv5 üöÄ is a cutting-edge, state-of-the-art (SOTA) computer vision model developed by &lt;a href="https://www.ultralytics.com/"&gt;Ultralytics&lt;/a&gt;. Based on the &lt;a href="https://pytorch.org/"&gt;PyTorch&lt;/a&gt; framework, YOLOv5 is renowned for its ease of use, speed, and accuracy. It incorporates insights and best practices from extensive research and development, making it a popular choice for a wide range of vision AI tasks, including &lt;a href="https://docs.ultralytics.com/tasks/detect/"&gt;object detection&lt;/a&gt;, &lt;a href="https://docs.ultralytics.com/tasks/segment/"&gt;image segmentation&lt;/a&gt;, and &lt;a href="https://docs.ultralytics.com/tasks/classify/"&gt;image classification&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;We hope the resources here help you get the most out of YOLOv5. Please browse the &lt;a href="https://docs.ultralytics.com/yolov5/"&gt;YOLOv5 Docs&lt;/a&gt; for detailed information, raise an issue on &lt;a href="https://github.com/ultralytics/yolov5/issues/new/choose"&gt;GitHub&lt;/a&gt; for support, and join our &lt;a href="https://discord.com/invite/ultralytics"&gt;Discord community&lt;/a&gt; for questions and discussions!&lt;/p&gt; 
 &lt;p&gt;To request an Enterprise License, please complete the form at &lt;a href="https://www.ultralytics.com/license"&gt;Ultralytics Licensing&lt;/a&gt;.&lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;a href="https://github.com/ultralytics"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-github.png" width="2%" alt="Ultralytics GitHub" /&gt;&lt;/a&gt; 
  &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="2%" alt="space" /&gt; 
  &lt;a href="https://www.linkedin.com/company/ultralytics/"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-linkedin.png" width="2%" alt="Ultralytics LinkedIn" /&gt;&lt;/a&gt; 
  &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="2%" alt="space" /&gt; 
  &lt;a href="https://twitter.com/ultralytics"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-twitter.png" width="2%" alt="Ultralytics Twitter" /&gt;&lt;/a&gt; 
  &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="2%" alt="space" /&gt; 
  &lt;a href="https://youtube.com/ultralytics?sub_confirmation=1"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-youtube.png" width="2%" alt="Ultralytics YouTube" /&gt;&lt;/a&gt; 
  &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="2%" alt="space" /&gt; 
  &lt;a href="https://www.tiktok.com/@ultralytics"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-tiktok.png" width="2%" alt="Ultralytics TikTok" /&gt;&lt;/a&gt; 
  &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="2%" alt="space" /&gt; 
  &lt;a href="https://ultralytics.com/bilibili"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-bilibili.png" width="2%" alt="Ultralytics BiliBili" /&gt;&lt;/a&gt; 
  &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="2%" alt="space" /&gt; 
  &lt;a href="https://discord.com/invite/ultralytics"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-discord.png" width="2%" alt="Ultralytics Discord" /&gt;&lt;/a&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;h2&gt;üöÄ YOLO11: The Next Evolution&lt;/h2&gt; 
&lt;p&gt;We are excited to announce the launch of &lt;strong&gt;Ultralytics YOLO11&lt;/strong&gt; üöÄ, the latest advancement in our state-of-the-art (SOTA) vision models! Available now at the &lt;a href="https://github.com/ultralytics/ultralytics"&gt;Ultralytics YOLO GitHub repository&lt;/a&gt;, YOLO11 builds on our legacy of speed, precision, and ease of use. Whether you're tackling &lt;a href="https://docs.ultralytics.com/tasks/detect/"&gt;object detection&lt;/a&gt;, &lt;a href="https://docs.ultralytics.com/tasks/segment/"&gt;instance segmentation&lt;/a&gt;, &lt;a href="https://docs.ultralytics.com/tasks/pose/"&gt;pose estimation&lt;/a&gt;, &lt;a href="https://docs.ultralytics.com/tasks/classify/"&gt;image classification&lt;/a&gt;, or &lt;a href="https://docs.ultralytics.com/tasks/obb/"&gt;oriented object detection (OBB)&lt;/a&gt;, YOLO11 delivers the performance and versatility needed to excel in diverse applications.&lt;/p&gt; 
&lt;p&gt;Get started today and unlock the full potential of YOLO11! Visit the &lt;a href="https://docs.ultralytics.com/"&gt;Ultralytics Docs&lt;/a&gt; for comprehensive guides and resources:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://badge.fury.io/py/ultralytics"&gt;&lt;img src="https://badge.fury.io/py/ultralytics.svg?sanitize=true" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://clickpy.clickhouse.com/dashboard/ultralytics"&gt;&lt;img src="https://static.pepy.tech/badge/ultralytics" alt="Downloads" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install the ultralytics package
pip install ultralytics
&lt;/code&gt;&lt;/pre&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://www.ultralytics.com/yolo" target="_blank"&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/ultralytics/assets/refs/heads/main/yolo/performance-comparison.png" alt="Ultralytics YOLO Performance Comparison" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;üìö Documentation&lt;/h2&gt; 
&lt;p&gt;See the &lt;a href="https://docs.ultralytics.com/yolov5/"&gt;YOLOv5 Docs&lt;/a&gt; for full documentation on training, testing, and deployment. See below for quickstart examples.&lt;/p&gt; 
&lt;details open&gt; 
 &lt;summary&gt;Install&lt;/summary&gt; 
 &lt;p&gt;Clone the repository and install dependencies in a &lt;a href="https://www.python.org/"&gt;&lt;strong&gt;Python&amp;gt;=3.8.0&lt;/strong&gt;&lt;/a&gt; environment. Ensure you have &lt;a href="https://pytorch.org/get-started/locally/"&gt;&lt;strong&gt;PyTorch&amp;gt;=1.8&lt;/strong&gt;&lt;/a&gt; installed.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Clone the YOLOv5 repository
git clone https://github.com/ultralytics/yolov5

# Navigate to the cloned directory
cd yolov5

# Install required packages
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details open&gt; 
 &lt;summary&gt;Inference with PyTorch Hub&lt;/summary&gt; 
 &lt;p&gt;Use YOLOv5 via &lt;a href="https://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading/"&gt;PyTorch Hub&lt;/a&gt; for inference. &lt;a href="https://github.com/ultralytics/yolov5/tree/master/models"&gt;Models&lt;/a&gt; are automatically downloaded from the latest YOLOv5 &lt;a href="https://github.com/ultralytics/yolov5/releases"&gt;release&lt;/a&gt;.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import torch

# Load a YOLOv5 model (options: yolov5n, yolov5s, yolov5m, yolov5l, yolov5x)
model = torch.hub.load("ultralytics/yolov5", "yolov5s")  # Default: yolov5s

# Define the input image source (URL, local file, PIL image, OpenCV frame, numpy array, or list)
img = "https://ultralytics.com/images/zidane.jpg"  # Example image

# Perform inference (handles batching, resizing, normalization automatically)
results = model(img)

# Process the results (options: .print(), .show(), .save(), .crop(), .pandas())
results.print()  # Print results to console
results.show()  # Display results in a window
results.save()  # Save results to runs/detect/exp
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Inference with detect.py&lt;/summary&gt; 
 &lt;p&gt;The &lt;code&gt;detect.py&lt;/code&gt; script runs inference on various sources. It automatically downloads &lt;a href="https://github.com/ultralytics/yolov5/tree/master/models"&gt;models&lt;/a&gt; from the latest YOLOv5 &lt;a href="https://github.com/ultralytics/yolov5/releases"&gt;release&lt;/a&gt; and saves the results to the &lt;code&gt;runs/detect&lt;/code&gt; directory.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Run inference using a webcam
python detect.py --weights yolov5s.pt --source 0

# Run inference on a local image file
python detect.py --weights yolov5s.pt --source img.jpg

# Run inference on a local video file
python detect.py --weights yolov5s.pt --source vid.mp4

# Run inference on a screen capture
python detect.py --weights yolov5s.pt --source screen

# Run inference on a directory of images
python detect.py --weights yolov5s.pt --source path/to/images/

# Run inference on a text file listing image paths
python detect.py --weights yolov5s.pt --source list.txt

# Run inference on a text file listing stream URLs
python detect.py --weights yolov5s.pt --source list.streams

# Run inference using a glob pattern for images
python detect.py --weights yolov5s.pt --source 'path/to/*.jpg'

# Run inference on a YouTube video URL
python detect.py --weights yolov5s.pt --source 'https://youtu.be/LNwODJXcvt4'

# Run inference on an RTSP, RTMP, or HTTP stream
python detect.py --weights yolov5s.pt --source 'rtsp://example.com/media.mp4'
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Training&lt;/summary&gt; 
 &lt;p&gt;The commands below demonstrate how to reproduce YOLOv5 &lt;a href="https://docs.ultralytics.com/datasets/detect/coco/"&gt;COCO dataset&lt;/a&gt; results. Both &lt;a href="https://github.com/ultralytics/yolov5/tree/master/models"&gt;models&lt;/a&gt; and &lt;a href="https://github.com/ultralytics/yolov5/tree/master/data"&gt;datasets&lt;/a&gt; are downloaded automatically from the latest YOLOv5 &lt;a href="https://github.com/ultralytics/yolov5/releases"&gt;release&lt;/a&gt;. Training times for YOLOv5n/s/m/l/x are approximately 1/2/4/6/8 days on a single &lt;a href="https://www.nvidia.com/en-us/data-center/v100/"&gt;NVIDIA V100 GPU&lt;/a&gt;. Using &lt;a href="https://docs.ultralytics.com/yolov5/tutorials/multi_gpu_training/"&gt;Multi-GPU training&lt;/a&gt; can significantly reduce training time. Use the largest &lt;code&gt;--batch-size&lt;/code&gt; your hardware allows, or use &lt;code&gt;--batch-size -1&lt;/code&gt; for YOLOv5 &lt;a href="https://github.com/ultralytics/yolov5/pull/5092"&gt;AutoBatch&lt;/a&gt;. The batch sizes shown below are for V100-16GB GPUs.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Train YOLOv5n on COCO for 300 epochs
python train.py --data coco.yaml --epochs 300 --weights '' --cfg yolov5n.yaml --batch-size 128

# Train YOLOv5s on COCO for 300 epochs
python train.py --data coco.yaml --epochs 300 --weights '' --cfg yolov5s.yaml --batch-size 64

# Train YOLOv5m on COCO for 300 epochs
python train.py --data coco.yaml --epochs 300 --weights '' --cfg yolov5m.yaml --batch-size 40

# Train YOLOv5l on COCO for 300 epochs
python train.py --data coco.yaml --epochs 300 --weights '' --cfg yolov5l.yaml --batch-size 24

# Train YOLOv5x on COCO for 300 epochs
python train.py --data coco.yaml --epochs 300 --weights '' --cfg yolov5x.yaml --batch-size 16
&lt;/code&gt;&lt;/pre&gt; 
 &lt;img width="800" src="https://user-images.githubusercontent.com/26833433/90222759-949d8800-ddc1-11ea-9fa1-1c97eed2b963.png" alt="YOLOv5 Training Results" /&gt; 
&lt;/details&gt; 
&lt;details open&gt; 
 &lt;summary&gt;Tutorials&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/"&gt;Train Custom Data&lt;/a&gt;&lt;/strong&gt; üöÄ &lt;strong&gt;RECOMMENDED&lt;/strong&gt;: Learn how to train YOLOv5 on your own datasets.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://docs.ultralytics.com/guides/model-training-tips/"&gt;Tips for Best Training Results&lt;/a&gt;&lt;/strong&gt; ‚òòÔ∏è: Improve your model's performance with expert tips.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://docs.ultralytics.com/yolov5/tutorials/multi_gpu_training/"&gt;Multi-GPU Training&lt;/a&gt;&lt;/strong&gt;: Speed up training using multiple GPUs.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading/"&gt;PyTorch Hub Integration&lt;/a&gt;&lt;/strong&gt; üåü &lt;strong&gt;NEW&lt;/strong&gt;: Easily load models using PyTorch Hub.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://docs.ultralytics.com/yolov5/tutorials/model_export/"&gt;Model Export (TFLite, ONNX, CoreML, TensorRT)&lt;/a&gt;&lt;/strong&gt; üöÄ: Convert your models to various deployment formats like &lt;a href="https://onnx.ai/"&gt;ONNX&lt;/a&gt; or &lt;a href="https://developer.nvidia.com/tensorrt"&gt;TensorRT&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://docs.ultralytics.com/guides/nvidia-jetson/"&gt;NVIDIA Jetson Deployment&lt;/a&gt;&lt;/strong&gt; üåü &lt;strong&gt;NEW&lt;/strong&gt;: Deploy YOLOv5 on &lt;a href="https://developer.nvidia.com/embedded-computing"&gt;NVIDIA Jetson&lt;/a&gt; devices.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://docs.ultralytics.com/yolov5/tutorials/test_time_augmentation/"&gt;Test-Time Augmentation (TTA)&lt;/a&gt;&lt;/strong&gt;: Enhance prediction accuracy with TTA.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://docs.ultralytics.com/yolov5/tutorials/model_ensembling/"&gt;Model Ensembling&lt;/a&gt;&lt;/strong&gt;: Combine multiple models for better performance.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://docs.ultralytics.com/yolov5/tutorials/model_pruning_and_sparsity/"&gt;Model Pruning/Sparsity&lt;/a&gt;&lt;/strong&gt;: Optimize models for size and speed.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/"&gt;Hyperparameter Evolution&lt;/a&gt;&lt;/strong&gt;: Automatically find the best training hyperparameters.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://docs.ultralytics.com/yolov5/tutorials/transfer_learning_with_frozen_layers/"&gt;Transfer Learning with Frozen Layers&lt;/a&gt;&lt;/strong&gt;: Adapt pretrained models to new tasks efficiently using &lt;a href="https://www.ultralytics.com/glossary/transfer-learning"&gt;transfer learning&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://docs.ultralytics.com/yolov5/tutorials/architecture_description/"&gt;Architecture Summary&lt;/a&gt;&lt;/strong&gt; üåü &lt;strong&gt;NEW&lt;/strong&gt;: Understand the YOLOv5 model architecture.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://www.ultralytics.com/hub"&gt;Ultralytics HUB Training&lt;/a&gt;&lt;/strong&gt; üöÄ &lt;strong&gt;RECOMMENDED&lt;/strong&gt;: Train and deploy YOLO models using Ultralytics HUB.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://docs.ultralytics.com/yolov5/tutorials/clearml_logging_integration/"&gt;ClearML Logging&lt;/a&gt;&lt;/strong&gt;: Integrate with &lt;a href="https://clear.ml/"&gt;ClearML&lt;/a&gt; for experiment tracking.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://docs.ultralytics.com/yolov5/tutorials/neural_magic_pruning_quantization/"&gt;Neural Magic DeepSparse Integration&lt;/a&gt;&lt;/strong&gt;: Accelerate inference with DeepSparse.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href="https://docs.ultralytics.com/yolov5/tutorials/comet_logging_integration/"&gt;Comet Logging&lt;/a&gt;&lt;/strong&gt; üåü &lt;strong&gt;NEW&lt;/strong&gt;: Log experiments using &lt;a href="https://www.comet.com/site/"&gt;Comet ML&lt;/a&gt;.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;üß© Integrations&lt;/h2&gt; 
&lt;p&gt;Our key integrations with leading AI platforms extend the functionality of Ultralytics' offerings, enhancing tasks like dataset labeling, training, visualization, and model management. Discover how Ultralytics, in collaboration with partners like &lt;a href="https://docs.ultralytics.com/integrations/weights-biases/"&gt;Weights &amp;amp; Biases&lt;/a&gt;, &lt;a href="https://docs.ultralytics.com/integrations/comet/"&gt;Comet ML&lt;/a&gt;, &lt;a href="https://docs.ultralytics.com/integrations/roboflow/"&gt;Roboflow&lt;/a&gt;, and &lt;a href="https://docs.ultralytics.com/integrations/openvino/"&gt;Intel OpenVINO&lt;/a&gt;, can optimize your AI workflow. Explore more at &lt;a href="https://docs.ultralytics.com/integrations/"&gt;Ultralytics Integrations&lt;/a&gt;.&lt;/p&gt; 
&lt;a href="https://docs.ultralytics.com/integrations/" target="_blank"&gt; &lt;img width="100%" src="https://github.com/ultralytics/assets/raw/main/yolov8/banner-integrations.png" alt="Ultralytics active learning integrations" /&gt; &lt;/a&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://www.ultralytics.com/hub"&gt; &lt;img src="https://github.com/ultralytics/assets/raw/main/partners/logo-ultralytics-hub.png" width="10%" alt="Ultralytics HUB logo" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="15%" height="0" alt="space" /&gt; 
 &lt;a href="https://docs.ultralytics.com/integrations/weights-biases/"&gt; &lt;img src="https://github.com/ultralytics/assets/raw/main/partners/logo-wb.png" width="10%" alt="Weights &amp;amp; Biases logo" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="15%" height="0" alt="space" /&gt; 
 &lt;a href="https://docs.ultralytics.com/integrations/comet/"&gt; &lt;img src="https://github.com/ultralytics/assets/raw/main/partners/logo-comet.png" width="10%" alt="Comet ML logo" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="15%" height="0" alt="space" /&gt; 
 &lt;a href="https://docs.ultralytics.com/integrations/neural-magic/"&gt; &lt;img src="https://github.com/ultralytics/assets/raw/main/partners/logo-neuralmagic.png" width="10%" alt="Neural Magic logo" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Ultralytics HUB üåü&lt;/th&gt; 
   &lt;th align="center"&gt;Weights &amp;amp; Biases&lt;/th&gt; 
   &lt;th align="center"&gt;Comet&lt;/th&gt; 
   &lt;th align="center"&gt;Neural Magic&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Streamline YOLO workflows: Label, train, and deploy effortlessly with &lt;a href="https://hub.ultralytics.com/"&gt;Ultralytics HUB&lt;/a&gt;. Try now!&lt;/td&gt; 
   &lt;td align="center"&gt;Track experiments, hyperparameters, and results with &lt;a href="https://docs.ultralytics.com/integrations/weights-biases/"&gt;Weights &amp;amp; Biases&lt;/a&gt;.&lt;/td&gt; 
   &lt;td align="center"&gt;Free forever, &lt;a href="https://docs.ultralytics.com/integrations/comet/"&gt;Comet ML&lt;/a&gt; lets you save YOLO models, resume training, and interactively visualize predictions.&lt;/td&gt; 
   &lt;td align="center"&gt;Run YOLO inference up to 6x faster with &lt;a href="https://docs.ultralytics.com/integrations/neural-magic/"&gt;Neural Magic DeepSparse&lt;/a&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;‚≠ê Ultralytics HUB&lt;/h2&gt; 
&lt;p&gt;Experience seamless AI development with &lt;a href="https://www.ultralytics.com/hub"&gt;Ultralytics HUB&lt;/a&gt; ‚≠ê, the ultimate platform for building, training, and deploying &lt;a href="https://www.ultralytics.com/glossary/computer-vision-cv"&gt;computer vision&lt;/a&gt; models. Visualize datasets, train &lt;a href="https://docs.ultralytics.com/models/yolov5/"&gt;YOLOv5&lt;/a&gt; and &lt;a href="https://docs.ultralytics.com/models/yolov8/"&gt;YOLOv8&lt;/a&gt; üöÄ models, and deploy them to real-world applications without writing any code. Transform images into actionable insights using our cutting-edge tools and user-friendly &lt;a href="https://www.ultralytics.com/app-install"&gt;Ultralytics App&lt;/a&gt;. Start your journey for &lt;strong&gt;Free&lt;/strong&gt; today!&lt;/p&gt; 
&lt;a align="center" href="https://www.ultralytics.com/hub" target="_blank"&gt; &lt;img width="100%" src="https://github.com/ultralytics/assets/raw/main/im/ultralytics-hub.png" alt="Ultralytics HUB Platform Screenshot" /&gt;&lt;/a&gt; 
&lt;h2&gt;ü§î Why YOLOv5?&lt;/h2&gt; 
&lt;p&gt;YOLOv5 is designed for simplicity and ease of use. We prioritize real-world performance and accessibility.&lt;/p&gt; 
&lt;p align="left"&gt;&lt;img width="800" src="https://user-images.githubusercontent.com/26833433/155040763-93c22a27-347c-4e3c-847a-8094621d3f4e.png" alt="YOLOv5 Performance Chart" /&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;YOLOv5-P5 640 Figure&lt;/summary&gt; 
 &lt;p align="left"&gt;&lt;img width="800" src="https://user-images.githubusercontent.com/26833433/155040757-ce0934a3-06a6-43dc-a979-2edbbd69ea0e.png" alt="YOLOv5 P5 640 Performance Chart" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Figure Notes&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;COCO AP val&lt;/strong&gt; denotes the &lt;a href="https://www.ultralytics.com/glossary/mean-average-precision-map"&gt;mean Average Precision (mAP)&lt;/a&gt; at &lt;a href="https://www.ultralytics.com/glossary/intersection-over-union-iou"&gt;Intersection over Union (IoU)&lt;/a&gt; thresholds from 0.5 to 0.95, measured on the 5,000-image &lt;a href="https://docs.ultralytics.com/datasets/detect/coco/"&gt;COCO val2017 dataset&lt;/a&gt; across various inference sizes (256 to 1536 pixels).&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;GPU Speed&lt;/strong&gt; measures the average inference time per image on the &lt;a href="https://docs.ultralytics.com/datasets/detect/coco/"&gt;COCO val2017 dataset&lt;/a&gt; using an &lt;a href="https://aws.amazon.com/ec2/instance-types/p4/"&gt;AWS p3.2xlarge V100 instance&lt;/a&gt; with a batch size of 32.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;EfficientDet&lt;/strong&gt; data is sourced from the &lt;a href="https://github.com/google/automl"&gt;google/automl repository&lt;/a&gt; at batch size 8.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Reproduce&lt;/strong&gt; these results using the command: &lt;code&gt;python val.py --task study --data coco.yaml --iou 0.7 --weights yolov5n6.pt yolov5s6.pt yolov5m6.pt yolov5l6.pt yolov5x6.pt&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;Pretrained Checkpoints&lt;/h3&gt; 
&lt;p&gt;This table shows the performance metrics for various YOLOv5 models trained on the COCO dataset.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Size&lt;br /&gt;&lt;sup&gt;(pixels)&lt;/sup&gt;&lt;/th&gt; 
   &lt;th&gt;mAP&lt;sup&gt;val&lt;br /&gt;50-95&lt;/sup&gt;&lt;/th&gt; 
   &lt;th&gt;mAP&lt;sup&gt;val&lt;br /&gt;50&lt;/sup&gt;&lt;/th&gt; 
   &lt;th&gt;Speed&lt;br /&gt;&lt;sup&gt;CPU b1&lt;br /&gt;(ms)&lt;/sup&gt;&lt;/th&gt; 
   &lt;th&gt;Speed&lt;br /&gt;&lt;sup&gt;V100 b1&lt;br /&gt;(ms)&lt;/sup&gt;&lt;/th&gt; 
   &lt;th&gt;Speed&lt;br /&gt;&lt;sup&gt;V100 b32&lt;br /&gt;(ms)&lt;/sup&gt;&lt;/th&gt; 
   &lt;th&gt;Params&lt;br /&gt;&lt;sup&gt;(M)&lt;/sup&gt;&lt;/th&gt; 
   &lt;th&gt;FLOPs&lt;br /&gt;&lt;sup&gt;@640 (B)&lt;/sup&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5n.pt"&gt;YOLOv5n&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;640&lt;/td&gt; 
   &lt;td&gt;28.0&lt;/td&gt; 
   &lt;td&gt;45.7&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;45&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;6.3&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;0.6&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;1.9&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;4.5&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt"&gt;YOLOv5s&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;640&lt;/td&gt; 
   &lt;td&gt;37.4&lt;/td&gt; 
   &lt;td&gt;56.8&lt;/td&gt; 
   &lt;td&gt;98&lt;/td&gt; 
   &lt;td&gt;6.4&lt;/td&gt; 
   &lt;td&gt;0.9&lt;/td&gt; 
   &lt;td&gt;7.2&lt;/td&gt; 
   &lt;td&gt;16.5&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5m.pt"&gt;YOLOv5m&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;640&lt;/td&gt; 
   &lt;td&gt;45.4&lt;/td&gt; 
   &lt;td&gt;64.1&lt;/td&gt; 
   &lt;td&gt;224&lt;/td&gt; 
   &lt;td&gt;8.2&lt;/td&gt; 
   &lt;td&gt;1.7&lt;/td&gt; 
   &lt;td&gt;21.2&lt;/td&gt; 
   &lt;td&gt;49.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5l.pt"&gt;YOLOv5l&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;640&lt;/td&gt; 
   &lt;td&gt;49.0&lt;/td&gt; 
   &lt;td&gt;67.3&lt;/td&gt; 
   &lt;td&gt;430&lt;/td&gt; 
   &lt;td&gt;10.1&lt;/td&gt; 
   &lt;td&gt;2.7&lt;/td&gt; 
   &lt;td&gt;46.5&lt;/td&gt; 
   &lt;td&gt;109.1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5x.pt"&gt;YOLOv5x&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;640&lt;/td&gt; 
   &lt;td&gt;50.7&lt;/td&gt; 
   &lt;td&gt;68.9&lt;/td&gt; 
   &lt;td&gt;766&lt;/td&gt; 
   &lt;td&gt;12.1&lt;/td&gt; 
   &lt;td&gt;4.8&lt;/td&gt; 
   &lt;td&gt;86.7&lt;/td&gt; 
   &lt;td&gt;205.7&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5n6.pt"&gt;YOLOv5n6&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1280&lt;/td&gt; 
   &lt;td&gt;36.0&lt;/td&gt; 
   &lt;td&gt;54.4&lt;/td&gt; 
   &lt;td&gt;153&lt;/td&gt; 
   &lt;td&gt;8.1&lt;/td&gt; 
   &lt;td&gt;2.1&lt;/td&gt; 
   &lt;td&gt;3.2&lt;/td&gt; 
   &lt;td&gt;4.6&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s6.pt"&gt;YOLOv5s6&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1280&lt;/td&gt; 
   &lt;td&gt;44.8&lt;/td&gt; 
   &lt;td&gt;63.7&lt;/td&gt; 
   &lt;td&gt;385&lt;/td&gt; 
   &lt;td&gt;8.2&lt;/td&gt; 
   &lt;td&gt;3.6&lt;/td&gt; 
   &lt;td&gt;12.6&lt;/td&gt; 
   &lt;td&gt;16.8&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5m6.pt"&gt;YOLOv5m6&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1280&lt;/td&gt; 
   &lt;td&gt;51.3&lt;/td&gt; 
   &lt;td&gt;69.3&lt;/td&gt; 
   &lt;td&gt;887&lt;/td&gt; 
   &lt;td&gt;11.1&lt;/td&gt; 
   &lt;td&gt;6.8&lt;/td&gt; 
   &lt;td&gt;35.7&lt;/td&gt; 
   &lt;td&gt;50.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5l6.pt"&gt;YOLOv5l6&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1280&lt;/td&gt; 
   &lt;td&gt;53.7&lt;/td&gt; 
   &lt;td&gt;71.3&lt;/td&gt; 
   &lt;td&gt;1784&lt;/td&gt; 
   &lt;td&gt;15.8&lt;/td&gt; 
   &lt;td&gt;10.5&lt;/td&gt; 
   &lt;td&gt;76.8&lt;/td&gt; 
   &lt;td&gt;111.4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5x6.pt"&gt;YOLOv5x6&lt;/a&gt;&lt;br /&gt;+ &lt;a href="https://docs.ultralytics.com/yolov5/tutorials/test_time_augmentation/"&gt;[TTA]&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1280&lt;br /&gt;1536&lt;/td&gt; 
   &lt;td&gt;55.0&lt;br /&gt;&lt;strong&gt;55.8&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;72.7&lt;br /&gt;&lt;strong&gt;72.7&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;3136&lt;br /&gt;-&lt;/td&gt; 
   &lt;td&gt;26.2&lt;br /&gt;-&lt;/td&gt; 
   &lt;td&gt;19.4&lt;br /&gt;-&lt;/td&gt; 
   &lt;td&gt;140.7&lt;br /&gt;-&lt;/td&gt; 
   &lt;td&gt;209.8&lt;br /&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;details&gt; 
 &lt;summary&gt;Table Notes&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;All checkpoints were trained for 300 epochs using default settings. Nano (n) and Small (s) models use &lt;a href="https://github.com/ultralytics/yolov5/raw/master/data/hyps/hyp.scratch-low.yaml"&gt;hyp.scratch-low.yaml&lt;/a&gt; hyperparameters, while Medium (m), Large (l), and Extra-Large (x) models use &lt;a href="https://github.com/ultralytics/yolov5/raw/master/data/hyps/hyp.scratch-high.yaml"&gt;hyp.scratch-high.yaml&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;mAP&lt;sup&gt;val&lt;/sup&gt;&lt;/strong&gt; values represent single-model, single-scale performance on the &lt;a href="https://docs.ultralytics.com/datasets/detect/coco/"&gt;COCO val2017 dataset&lt;/a&gt;.&lt;br /&gt;Reproduce using: &lt;code&gt;python val.py --data coco.yaml --img 640 --conf 0.001 --iou 0.65&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Speed&lt;/strong&gt; metrics are averaged over COCO val images using an &lt;a href="https://aws.amazon.com/ec2/instance-types/p4/"&gt;AWS p3.2xlarge V100 instance&lt;/a&gt;. Non-Maximum Suppression (NMS) time (~1 ms/image) is not included.&lt;br /&gt;Reproduce using: &lt;code&gt;python val.py --data coco.yaml --img 640 --task speed --batch 1&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;TTA&lt;/strong&gt; (&lt;a href="https://docs.ultralytics.com/yolov5/tutorials/test_time_augmentation/"&gt;Test Time Augmentation&lt;/a&gt;) includes reflection and scale augmentations for improved accuracy.&lt;br /&gt;Reproduce using: &lt;code&gt;python val.py --data coco.yaml --img 1536 --iou 0.7 --augment&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;üñºÔ∏è Segmentation&lt;/h2&gt; 
&lt;p&gt;The YOLOv5 &lt;a href="https://github.com/ultralytics/yolov5/releases/v7.0"&gt;release v7.0&lt;/a&gt; introduced &lt;a href="https://docs.ultralytics.com/tasks/segment/"&gt;instance segmentation&lt;/a&gt; models that achieve state-of-the-art performance. These models are designed for easy training, validation, and deployment. For full details, see the &lt;a href="https://github.com/ultralytics/yolov5/releases/v7.0"&gt;Release Notes&lt;/a&gt; and explore the &lt;a href="https://github.com/ultralytics/yolov5/raw/master/segment/tutorial.ipynb"&gt;YOLOv5 Segmentation Colab Notebook&lt;/a&gt; for quickstart examples.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Segmentation Checkpoints&lt;/summary&gt; 
 &lt;div align="center"&gt; 
  &lt;a align="center" href="https://www.ultralytics.com/yolo" target="_blank"&gt; &lt;img width="800" src="https://user-images.githubusercontent.com/61612323/204180385-84f3aca9-a5e9-43d8-a617-dda7ca12e54a.png" alt="YOLOv5 Segmentation Performance Chart" /&gt;&lt;/a&gt; 
 &lt;/div&gt; 
 &lt;p&gt;YOLOv5 segmentation models were trained on the &lt;a href="https://docs.ultralytics.com/datasets/segment/coco/"&gt;COCO dataset&lt;/a&gt; for 300 epochs at an image size of 640 pixels using A100 GPUs. Models were exported to &lt;a href="https://onnx.ai/"&gt;ONNX&lt;/a&gt; FP32 for CPU speed tests and &lt;a href="https://developer.nvidia.com/tensorrt"&gt;TensorRT&lt;/a&gt; FP16 for GPU speed tests. All speed tests were conducted on Google &lt;a href="https://colab.research.google.com/signup"&gt;Colab Pro&lt;/a&gt; notebooks for reproducibility.&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;Size&lt;br /&gt;&lt;sup&gt;(pixels)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;mAP&lt;sup&gt;box&lt;br /&gt;50-95&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;mAP&lt;sup&gt;mask&lt;br /&gt;50-95&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Train Time&lt;br /&gt;&lt;sup&gt;300 epochs&lt;br /&gt;A100 (hours)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Speed&lt;br /&gt;&lt;sup&gt;ONNX CPU&lt;br /&gt;(ms)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Speed&lt;br /&gt;&lt;sup&gt;TRT A100&lt;br /&gt;(ms)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Params&lt;br /&gt;&lt;sup&gt;(M)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;FLOPs&lt;br /&gt;&lt;sup&gt;@640 (B)&lt;/sup&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5n-seg.pt"&gt;YOLOv5n-seg&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;27.6&lt;/td&gt; 
    &lt;td&gt;23.4&lt;/td&gt; 
    &lt;td&gt;80:17&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;62.7&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;1.2&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;2.0&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;7.1&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s-seg.pt"&gt;YOLOv5s-seg&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;37.6&lt;/td&gt; 
    &lt;td&gt;31.7&lt;/td&gt; 
    &lt;td&gt;88:16&lt;/td&gt; 
    &lt;td&gt;173.3&lt;/td&gt; 
    &lt;td&gt;1.4&lt;/td&gt; 
    &lt;td&gt;7.6&lt;/td&gt; 
    &lt;td&gt;26.4&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5m-seg.pt"&gt;YOLOv5m-seg&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;45.0&lt;/td&gt; 
    &lt;td&gt;37.1&lt;/td&gt; 
    &lt;td&gt;108:36&lt;/td&gt; 
    &lt;td&gt;427.0&lt;/td&gt; 
    &lt;td&gt;2.2&lt;/td&gt; 
    &lt;td&gt;22.0&lt;/td&gt; 
    &lt;td&gt;70.8&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5l-seg.pt"&gt;YOLOv5l-seg&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;49.0&lt;/td&gt; 
    &lt;td&gt;39.9&lt;/td&gt; 
    &lt;td&gt;66:43 (2x)&lt;/td&gt; 
    &lt;td&gt;857.4&lt;/td&gt; 
    &lt;td&gt;2.9&lt;/td&gt; 
    &lt;td&gt;47.9&lt;/td&gt; 
    &lt;td&gt;147.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5x-seg.pt"&gt;YOLOv5x-seg&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;50.7&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;41.4&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;62:56 (3x)&lt;/td&gt; 
    &lt;td&gt;1579.2&lt;/td&gt; 
    &lt;td&gt;4.5&lt;/td&gt; 
    &lt;td&gt;88.8&lt;/td&gt; 
    &lt;td&gt;265.7&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;ul&gt; 
  &lt;li&gt;All checkpoints were trained for 300 epochs using the SGD optimizer with &lt;code&gt;lr0=0.01&lt;/code&gt; and &lt;code&gt;weight_decay=5e-5&lt;/code&gt; at an image size of 640 pixels, using default settings.&lt;br /&gt;Training runs are logged at &lt;a href="https://wandb.ai/glenn-jocher/YOLOv5_v70_official"&gt;https://wandb.ai/glenn-jocher/YOLOv5_v70_official&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Accuracy&lt;/strong&gt; values represent single-model, single-scale performance on the COCO dataset.&lt;br /&gt;Reproduce using: &lt;code&gt;python segment/val.py --data coco.yaml --weights yolov5s-seg.pt&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Speed&lt;/strong&gt; metrics are averaged over 100 inference images using a &lt;a href="https://colab.research.google.com/signup"&gt;Colab Pro A100 High-RAM instance&lt;/a&gt;. Values indicate inference speed only (NMS adds approximately 1ms per image).&lt;br /&gt;Reproduce using: &lt;code&gt;python segment/val.py --data coco.yaml --weights yolov5s-seg.pt --batch 1&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Export&lt;/strong&gt; to ONNX (FP32) and TensorRT (FP16) was performed using &lt;code&gt;export.py&lt;/code&gt;.&lt;br /&gt;Reproduce using: &lt;code&gt;python export.py --weights yolov5s-seg.pt --include engine --device 0 --half&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Segmentation Usage Examples &amp;nbsp;&lt;a href="https://colab.research.google.com/github/ultralytics/yolov5/blob/master/segment/tutorial.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/summary&gt; 
 &lt;h3&gt;Train&lt;/h3&gt; 
 &lt;p&gt;YOLOv5 segmentation training supports automatic download of the &lt;a href="https://docs.ultralytics.com/datasets/segment/coco8-seg/"&gt;COCO128-seg dataset&lt;/a&gt; via the &lt;code&gt;--data coco128-seg.yaml&lt;/code&gt; argument. For the full &lt;a href="https://docs.ultralytics.com/datasets/segment/coco/"&gt;COCO-segments dataset&lt;/a&gt;, download it manually using &lt;code&gt;bash data/scripts/get_coco.sh --train --val --segments&lt;/code&gt; and then train with &lt;code&gt;python train.py --data coco.yaml&lt;/code&gt;.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Train on a single GPU
python segment/train.py --data coco128-seg.yaml --weights yolov5s-seg.pt --img 640

# Train using Multi-GPU Distributed Data Parallel (DDP)
python -m torch.distributed.run --nproc_per_node 4 --master_port 1 segment/train.py --data coco128-seg.yaml --weights yolov5s-seg.pt --img 640 --device 0,1,2,3
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;Val&lt;/h3&gt; 
 &lt;p&gt;Validate the mask &lt;a href="https://www.ultralytics.com/glossary/mean-average-precision-map"&gt;mean Average Precision (mAP)&lt;/a&gt; of YOLOv5s-seg on the COCO dataset:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Download COCO validation segments split (780MB, 5000 images)
bash data/scripts/get_coco.sh --val --segments

# Validate the model
python segment/val.py --weights yolov5s-seg.pt --data coco.yaml --img 640
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;Predict&lt;/h3&gt; 
 &lt;p&gt;Use the pretrained YOLOv5m-seg.pt model to perform segmentation on &lt;code&gt;bus.jpg&lt;/code&gt;:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Run prediction
python segment/predict.py --weights yolov5m-seg.pt --source data/images/bus.jpg
&lt;/code&gt;&lt;/pre&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Load model from PyTorch Hub (Note: Inference support might vary)
model = torch.hub.load("ultralytics/yolov5", "custom", "yolov5m-seg.pt")
&lt;/code&gt;&lt;/pre&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;&lt;img src="https://user-images.githubusercontent.com/26833433/203113421-decef4c4-183d-4a0a-a6c2-6435b33bc5d3.jpg" alt="Zidane Segmentation Example" /&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;img src="https://user-images.githubusercontent.com/26833433/203113416-11fe0025-69f7-4874-a0a6-65d0bfe2999a.jpg" alt="Bus Segmentation Example" /&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
 &lt;/table&gt; 
 &lt;h3&gt;Export&lt;/h3&gt; 
 &lt;p&gt;Export the YOLOv5s-seg model to ONNX and TensorRT formats:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Export model
python export.py --weights yolov5s-seg.pt --include onnx engine --img 640 --device 0
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;üè∑Ô∏è Classification&lt;/h2&gt; 
&lt;p&gt;YOLOv5 &lt;a href="https://github.com/ultralytics/yolov5/releases/v6.2"&gt;release v6.2&lt;/a&gt; introduced support for &lt;a href="https://docs.ultralytics.com/tasks/classify/"&gt;image classification&lt;/a&gt; model training, validation, and deployment. Check the &lt;a href="https://github.com/ultralytics/yolov5/releases/v6.2"&gt;Release Notes&lt;/a&gt; for details and the &lt;a href="https://github.com/ultralytics/yolov5/raw/master/classify/tutorial.ipynb"&gt;YOLOv5 Classification Colab Notebook&lt;/a&gt; for quickstart guides.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Classification Checkpoints&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;YOLOv5-cls classification models were trained on &lt;a href="https://docs.ultralytics.com/datasets/classify/imagenet/"&gt;ImageNet&lt;/a&gt; for 90 epochs using a 4xA100 instance. &lt;a href="https://arxiv.org/abs/1512.03385"&gt;ResNet&lt;/a&gt; and &lt;a href="https://arxiv.org/abs/1905.11946"&gt;EfficientNet&lt;/a&gt; models were trained alongside under identical settings for comparison. Models were exported to &lt;a href="https://onnx.ai/"&gt;ONNX&lt;/a&gt; FP32 (CPU speed tests) and &lt;a href="https://developer.nvidia.com/tensorrt"&gt;TensorRT&lt;/a&gt; FP16 (GPU speed tests). All speed tests were run on Google &lt;a href="https://colab.research.google.com/signup"&gt;Colab Pro&lt;/a&gt; for reproducibility.&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;Size&lt;br /&gt;&lt;sup&gt;(pixels)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Acc&lt;br /&gt;&lt;sup&gt;top1&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Acc&lt;br /&gt;&lt;sup&gt;top5&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Training&lt;br /&gt;&lt;sup&gt;90 epochs&lt;br /&gt;4xA100 (hours)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Speed&lt;br /&gt;&lt;sup&gt;ONNX CPU&lt;br /&gt;(ms)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Speed&lt;br /&gt;&lt;sup&gt;TensorRT V100&lt;br /&gt;(ms)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Params&lt;br /&gt;&lt;sup&gt;(M)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;FLOPs&lt;br /&gt;&lt;sup&gt;@224 (B)&lt;/sup&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5n-cls.pt"&gt;YOLOv5n-cls&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;224&lt;/td&gt; 
    &lt;td&gt;64.6&lt;/td&gt; 
    &lt;td&gt;85.4&lt;/td&gt; 
    &lt;td&gt;7:59&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;3.3&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;0.5&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;2.5&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;0.5&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s-cls.pt"&gt;YOLOv5s-cls&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;224&lt;/td&gt; 
    &lt;td&gt;71.5&lt;/td&gt; 
    &lt;td&gt;90.2&lt;/td&gt; 
    &lt;td&gt;8:09&lt;/td&gt; 
    &lt;td&gt;6.6&lt;/td&gt; 
    &lt;td&gt;0.6&lt;/td&gt; 
    &lt;td&gt;5.4&lt;/td&gt; 
    &lt;td&gt;1.4&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5m-cls.pt"&gt;YOLOv5m-cls&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;224&lt;/td&gt; 
    &lt;td&gt;75.9&lt;/td&gt; 
    &lt;td&gt;92.9&lt;/td&gt; 
    &lt;td&gt;10:06&lt;/td&gt; 
    &lt;td&gt;15.5&lt;/td&gt; 
    &lt;td&gt;0.9&lt;/td&gt; 
    &lt;td&gt;12.9&lt;/td&gt; 
    &lt;td&gt;3.9&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5l-cls.pt"&gt;YOLOv5l-cls&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;224&lt;/td&gt; 
    &lt;td&gt;78.0&lt;/td&gt; 
    &lt;td&gt;94.0&lt;/td&gt; 
    &lt;td&gt;11:56&lt;/td&gt; 
    &lt;td&gt;26.9&lt;/td&gt; 
    &lt;td&gt;1.4&lt;/td&gt; 
    &lt;td&gt;26.5&lt;/td&gt; 
    &lt;td&gt;8.5&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5x-cls.pt"&gt;YOLOv5x-cls&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;224&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;79.0&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;94.4&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;15:04&lt;/td&gt; 
    &lt;td&gt;54.3&lt;/td&gt; 
    &lt;td&gt;1.8&lt;/td&gt; 
    &lt;td&gt;48.1&lt;/td&gt; 
    &lt;td&gt;15.9&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/yolov5/releases/download/v7.0/resnet18.pt"&gt;ResNet18&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;224&lt;/td&gt; 
    &lt;td&gt;70.3&lt;/td&gt; 
    &lt;td&gt;89.5&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;6:47&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;11.2&lt;/td&gt; 
    &lt;td&gt;0.5&lt;/td&gt; 
    &lt;td&gt;11.7&lt;/td&gt; 
    &lt;td&gt;3.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/yolov5/releases/download/v7.0/resnet34.pt"&gt;ResNet34&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;224&lt;/td&gt; 
    &lt;td&gt;73.9&lt;/td&gt; 
    &lt;td&gt;91.8&lt;/td&gt; 
    &lt;td&gt;8:33&lt;/td&gt; 
    &lt;td&gt;20.6&lt;/td&gt; 
    &lt;td&gt;0.9&lt;/td&gt; 
    &lt;td&gt;21.8&lt;/td&gt; 
    &lt;td&gt;7.4&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/yolov5/releases/download/v7.0/resnet50.pt"&gt;ResNet50&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;224&lt;/td&gt; 
    &lt;td&gt;76.8&lt;/td&gt; 
    &lt;td&gt;93.4&lt;/td&gt; 
    &lt;td&gt;11:10&lt;/td&gt; 
    &lt;td&gt;23.4&lt;/td&gt; 
    &lt;td&gt;1.0&lt;/td&gt; 
    &lt;td&gt;25.6&lt;/td&gt; 
    &lt;td&gt;8.5&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/yolov5/releases/download/v7.0/resnet101.pt"&gt;ResNet101&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;224&lt;/td&gt; 
    &lt;td&gt;78.5&lt;/td&gt; 
    &lt;td&gt;94.3&lt;/td&gt; 
    &lt;td&gt;17:10&lt;/td&gt; 
    &lt;td&gt;42.1&lt;/td&gt; 
    &lt;td&gt;1.9&lt;/td&gt; 
    &lt;td&gt;44.5&lt;/td&gt; 
    &lt;td&gt;15.9&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/yolov5/releases/download/v7.0/efficientnet_b0.pt"&gt;EfficientNet_b0&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;224&lt;/td&gt; 
    &lt;td&gt;75.1&lt;/td&gt; 
    &lt;td&gt;92.4&lt;/td&gt; 
    &lt;td&gt;13:03&lt;/td&gt; 
    &lt;td&gt;12.5&lt;/td&gt; 
    &lt;td&gt;1.3&lt;/td&gt; 
    &lt;td&gt;5.3&lt;/td&gt; 
    &lt;td&gt;1.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/yolov5/releases/download/v7.0/efficientnet_b1.pt"&gt;EfficientNet_b1&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;224&lt;/td&gt; 
    &lt;td&gt;76.4&lt;/td&gt; 
    &lt;td&gt;93.2&lt;/td&gt; 
    &lt;td&gt;17:04&lt;/td&gt; 
    &lt;td&gt;14.9&lt;/td&gt; 
    &lt;td&gt;1.6&lt;/td&gt; 
    &lt;td&gt;7.8&lt;/td&gt; 
    &lt;td&gt;1.5&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/yolov5/releases/download/v7.0/efficientnet_b2.pt"&gt;EfficientNet_b2&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;224&lt;/td&gt; 
    &lt;td&gt;76.6&lt;/td&gt; 
    &lt;td&gt;93.4&lt;/td&gt; 
    &lt;td&gt;17:10&lt;/td&gt; 
    &lt;td&gt;15.9&lt;/td&gt; 
    &lt;td&gt;1.6&lt;/td&gt; 
    &lt;td&gt;9.1&lt;/td&gt; 
    &lt;td&gt;1.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/yolov5/releases/download/v7.0/efficientnet_b3.pt"&gt;EfficientNet_b3&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;224&lt;/td&gt; 
    &lt;td&gt;77.7&lt;/td&gt; 
    &lt;td&gt;94.0&lt;/td&gt; 
    &lt;td&gt;19:19&lt;/td&gt; 
    &lt;td&gt;18.9&lt;/td&gt; 
    &lt;td&gt;1.9&lt;/td&gt; 
    &lt;td&gt;12.2&lt;/td&gt; 
    &lt;td&gt;2.4&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;details&gt; 
  &lt;summary&gt;Table Notes (click to expand)&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;All checkpoints were trained for 90 epochs using the SGD optimizer with &lt;code&gt;lr0=0.001&lt;/code&gt; and &lt;code&gt;weight_decay=5e-5&lt;/code&gt; at an image size of 224 pixels, using default settings.&lt;br /&gt;Training runs are logged at &lt;a href="https://wandb.ai/glenn-jocher/YOLOv5-Classifier-v6-2"&gt;https://wandb.ai/glenn-jocher/YOLOv5-Classifier-v6-2&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Accuracy&lt;/strong&gt; values (top-1 and top-5) represent single-model, single-scale performance on the &lt;a href="https://docs.ultralytics.com/datasets/classify/imagenet/"&gt;ImageNet-1k dataset&lt;/a&gt;.&lt;br /&gt;Reproduce using: &lt;code&gt;python classify/val.py --data ../datasets/imagenet --img 224&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Speed&lt;/strong&gt; metrics are averaged over 100 inference images using a Google &lt;a href="https://colab.research.google.com/signup"&gt;Colab Pro V100 High-RAM instance&lt;/a&gt;.&lt;br /&gt;Reproduce using: &lt;code&gt;python classify/val.py --data ../datasets/imagenet --img 224 --batch 1&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Export&lt;/strong&gt; to ONNX (FP32) and TensorRT (FP16) was performed using &lt;code&gt;export.py&lt;/code&gt;.&lt;br /&gt;Reproduce using: &lt;code&gt;python export.py --weights yolov5s-cls.pt --include engine onnx --imgsz 224&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Classification Usage Examples &amp;nbsp;&lt;a href="https://colab.research.google.com/github/ultralytics/yolov5/blob/master/classify/tutorial.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/summary&gt; 
 &lt;h3&gt;Train&lt;/h3&gt; 
 &lt;p&gt;YOLOv5 classification training supports automatic download for datasets like &lt;a href="https://docs.ultralytics.com/datasets/classify/mnist/"&gt;MNIST&lt;/a&gt;, &lt;a href="https://docs.ultralytics.com/datasets/classify/fashion-mnist/"&gt;Fashion-MNIST&lt;/a&gt;, &lt;a href="https://docs.ultralytics.com/datasets/classify/cifar10/"&gt;CIFAR10&lt;/a&gt;, &lt;a href="https://docs.ultralytics.com/datasets/classify/cifar100/"&gt;CIFAR100&lt;/a&gt;, &lt;a href="https://docs.ultralytics.com/datasets/classify/imagenette/"&gt;Imagenette&lt;/a&gt;, &lt;a href="https://docs.ultralytics.com/datasets/classify/imagewoof/"&gt;Imagewoof&lt;/a&gt;, and &lt;a href="https://docs.ultralytics.com/datasets/classify/imagenet/"&gt;ImageNet&lt;/a&gt; using the &lt;code&gt;--data&lt;/code&gt; argument. For example, start training on MNIST with &lt;code&gt;--data mnist&lt;/code&gt;.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Train on a single GPU using CIFAR-100 dataset
python classify/train.py --model yolov5s-cls.pt --data cifar100 --epochs 5 --img 224 --batch 128

# Train using Multi-GPU DDP on ImageNet dataset
python -m torch.distributed.run --nproc_per_node 4 --master_port 1 classify/train.py --model yolov5s-cls.pt --data imagenet --epochs 5 --img 224 --device 0,1,2,3
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;Val&lt;/h3&gt; 
 &lt;p&gt;Validate the accuracy of the YOLOv5m-cls model on the ImageNet-1k validation dataset:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Download ImageNet validation split (6.3GB, 50,000 images)
bash data/scripts/get_imagenet.sh --val

# Validate the model
python classify/val.py --weights yolov5m-cls.pt --data ../datasets/imagenet --img 224
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;Predict&lt;/h3&gt; 
 &lt;p&gt;Use the pretrained YOLOv5s-cls.pt model to classify the image &lt;code&gt;bus.jpg&lt;/code&gt;:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Run prediction
python classify/predict.py --weights yolov5s-cls.pt --source data/images/bus.jpg
&lt;/code&gt;&lt;/pre&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Load model from PyTorch Hub
model = torch.hub.load("ultralytics/yolov5", "custom", "yolov5s-cls.pt")
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;Export&lt;/h3&gt; 
 &lt;p&gt;Export trained YOLOv5s-cls, ResNet50, and EfficientNet_b0 models to ONNX and TensorRT formats:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Export models
python export.py --weights yolov5s-cls.pt resnet50.pt efficientnet_b0.pt --include onnx engine --img 224
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;‚òÅÔ∏è Environments&lt;/h2&gt; 
&lt;p&gt;Get started quickly with our pre-configured environments. Click the icons below for setup details.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://bit.ly/yolov5-paperspace-notebook" title="Run on Paperspace Gradient"&gt; &lt;img src="https://github.com/ultralytics/assets/releases/download/v0.0.0/logo-gradient.png" width="10%" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="5%" alt="" /&gt; 
 &lt;a href="https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb" title="Open in Google Colab"&gt; &lt;img src="https://github.com/ultralytics/assets/releases/download/v0.0.0/logo-colab-small.png" width="10%" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="5%" alt="" /&gt; 
 &lt;a href="https://www.kaggle.com/models/ultralytics/yolov5" title="Open in Kaggle"&gt; &lt;img src="https://github.com/ultralytics/assets/releases/download/v0.0.0/logo-kaggle-small.png" width="10%" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="5%" alt="" /&gt; 
 &lt;a href="https://hub.docker.com/r/ultralytics/yolov5" title="Pull Docker Image"&gt; &lt;img src="https://github.com/ultralytics/assets/releases/download/v0.0.0/logo-docker-small.png" width="10%" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="5%" alt="" /&gt; 
 &lt;a href="https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/" title="AWS Quickstart Guide"&gt; &lt;img src="https://github.com/ultralytics/assets/releases/download/v0.0.0/logo-aws-small.png" width="10%" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="5%" alt="" /&gt; 
 &lt;a href="https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/" title="GCP Quickstart Guide"&gt; &lt;img src="https://github.com/ultralytics/assets/releases/download/v0.0.0/logo-gcp-small.png" width="10%" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;ü§ù Contribute&lt;/h2&gt; 
&lt;p&gt;We welcome your contributions! Making YOLOv5 accessible and effective is a community effort. Please see our &lt;a href="https://docs.ultralytics.com/help/contributing/"&gt;Contributing Guide&lt;/a&gt; to get started. Share your feedback through the &lt;a href="https://www.ultralytics.com/survey?utm_source=github&amp;amp;utm_medium=social&amp;amp;utm_campaign=Survey"&gt;YOLOv5 Survey&lt;/a&gt;. Thank you to all our contributors for making YOLOv5 better!&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/ultralytics/yolov5/graphs/contributors"&gt;&lt;img src="https://raw.githubusercontent.com/ultralytics/assets/main/im/image-contributors.png" alt="Ultralytics open-source contributors" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üìú License&lt;/h2&gt; 
&lt;p&gt;Ultralytics provides two licensing options to meet different needs:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;AGPL-3.0 License&lt;/strong&gt;: An &lt;a href="https://opensource.org/license/agpl-v3"&gt;OSI-approved&lt;/a&gt; open-source license ideal for academic research, personal projects, and testing. It promotes open collaboration and knowledge sharing. See the &lt;a href="https://github.com/ultralytics/yolov5/raw/master/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Enterprise License&lt;/strong&gt;: Tailored for commercial applications, this license allows seamless integration of Ultralytics software and AI models into commercial products and services, bypassing the open-source requirements of AGPL-3.0. For commercial use cases, please contact us via &lt;a href="https://www.ultralytics.com/license"&gt;Ultralytics Licensing&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìß Contact&lt;/h2&gt; 
&lt;p&gt;For bug reports and feature requests related to YOLOv5, please visit &lt;a href="https://github.com/ultralytics/yolov5/issues"&gt;GitHub Issues&lt;/a&gt;. For general questions, discussions, and community support, join our &lt;a href="https://discord.com/invite/ultralytics"&gt;Discord server&lt;/a&gt;!&lt;/p&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/ultralytics"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-github.png" width="3%" alt="Ultralytics GitHub" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="3%" alt="space" /&gt; 
 &lt;a href="https://www.linkedin.com/company/ultralytics/"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-linkedin.png" width="3%" alt="Ultralytics LinkedIn" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="3%" alt="space" /&gt; 
 &lt;a href="https://twitter.com/ultralytics"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-twitter.png" width="3%" alt="Ultralytics Twitter" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="3%" alt="space" /&gt; 
 &lt;a href="https://youtube.com/ultralytics?sub_confirmation=1"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-youtube.png" width="3%" alt="Ultralytics YouTube" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="3%" alt="space" /&gt; 
 &lt;a href="https://www.tiktok.com/@ultralytics"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-tiktok.png" width="3%" alt="Ultralytics TikTok" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="3%" alt="space" /&gt; 
 &lt;a href="https://ultralytics.com/bilibili"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-bilibili.png" width="3%" alt="Ultralytics BiliBili" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="3%" alt="space" /&gt; 
 &lt;a href="https://discord.com/invite/ultralytics"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-discord.png" width="3%" alt="Ultralytics Discord" /&gt;&lt;/a&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>dbt-labs/dbt-core</title>
      <link>https://github.com/dbt-labs/dbt-core</link>
      <description>&lt;p&gt;dbt enables data analysts and engineers to transform their data using the same practices that software engineers use to build applications.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/dbt-labs/dbt-core/fa1ea14ddfb1d5ae319d5141844910dd53ab2834/etc/dbt-core.svg?sanitize=true" alt="dbt logo" width="750" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/dbt-labs/dbt-core/actions/workflows/main.yml"&gt; &lt;img src="https://github.com/dbt-labs/dbt-core/actions/workflows/main.yml/badge.svg?event=push" alt="CI Badge" /&gt; &lt;/a&gt; &lt;a href="https://www.bestpractices.dev/projects/11095"&gt;&lt;img src="https://www.bestpractices.dev/projects/11095/badge" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://www.getdbt.com/"&gt;dbt&lt;/a&gt;&lt;/strong&gt; enables data analysts and engineers to transform their data using the same practices that software engineers use to build applications.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/dbt-labs/dbt-core/raw/202cb7e51e218c7b29eb3b11ad058bd56b7739de/etc/dbt-transform.png" alt="architecture" /&gt;&lt;/p&gt; 
&lt;h2&gt;Understanding dbt&lt;/h2&gt; 
&lt;p&gt;Analysts using dbt can transform their data by simply writing select statements, while dbt handles turning these statements into tables and views in a data warehouse.&lt;/p&gt; 
&lt;p&gt;These select statements, or "models", form a dbt project. Models frequently build on top of one another ‚Äì dbt makes it easy to &lt;a href="https://docs.getdbt.com/docs/ref"&gt;manage relationships&lt;/a&gt; between models, and &lt;a href="https://docs.getdbt.com/docs/documentation"&gt;visualize these relationships&lt;/a&gt;, as well as assure the quality of your transformations through &lt;a href="https://docs.getdbt.com/docs/testing"&gt;testing&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/dbt-labs/dbt-core/6c6649f9129d5d108aa3b0526f634cd8f3a9d1ed/etc/dbt-dag.png" alt="dbt dag" /&gt;&lt;/p&gt; 
&lt;h2&gt;Getting started&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.getdbt.com/docs/get-started/installation"&gt;Install dbt Core&lt;/a&gt; or explore the &lt;a href="https://docs.getdbt.com/docs/cloud/cloud-cli-installation"&gt;dbt Cloud CLI&lt;/a&gt;, a command-line interface powered by &lt;a href="https://docs.getdbt.com/docs/cloud/about-cloud/dbt-cloud-features"&gt;dbt Cloud&lt;/a&gt; that enhances collaboration.&lt;/li&gt; 
 &lt;li&gt;Read the &lt;a href="https://docs.getdbt.com/docs/introduction/"&gt;introduction&lt;/a&gt; and &lt;a href="https://docs.getdbt.com/docs/about/viewpoint/"&gt;viewpoint&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Join the dbt Community&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Be part of the conversation in the &lt;a href="http://community.getdbt.com/"&gt;dbt Community Slack&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Read more on the &lt;a href="https://discourse.getdbt.com"&gt;dbt Community Discourse&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Reporting bugs and contributing code&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Want to report a bug or request a feature? Let us know and open &lt;a href="https://github.com/dbt-labs/dbt-core/issues/new/choose"&gt;an issue&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Want to help us build dbt? Check out the &lt;a href="https://github.com/dbt-labs/dbt-core/raw/HEAD/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Code of Conduct&lt;/h2&gt; 
&lt;p&gt;Everyone interacting in the dbt project's codebases, issue trackers, chat rooms, and mailing lists is expected to follow the &lt;a href="https://community.getdbt.com/code-of-conduct"&gt;dbt Code of Conduct&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>browser-use/browser-use</title>
      <link>https://github.com/browser-use/browser-use</link>
      <description>&lt;p&gt;üåê Make websites accessible for AI agents. Automate tasks online with ease.&lt;/p&gt;&lt;hr&gt;&lt;picture&gt; 
 &lt;source media="(prefers-color-scheme: light)" srcset="https://github.com/user-attachments/assets/2ccdb752-22fb-41c7-8948-857fc1ad7e24" " /&gt; 
 &lt;source media="(prefers-color-scheme: dark)" srcset="https://github.com/user-attachments/assets/774a46d5-27a0-490c-b7d0-e65fcbbfa358" /&gt; 
 &lt;img alt="Shows a black Browser Use Logo in light color mode and a white one in dark color mode." src="https://github.com/user-attachments/assets/2ccdb752-22fb-41c7-8948-857fc1ad7e24" width="full" /&gt; 
&lt;/picture&gt; 
&lt;div align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://github.com/user-attachments/assets/9955dda9-ede3-4971-8ee0-91cbc3850125" " /&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://github.com/user-attachments/assets/6797d09b-8ac3-4cb9-ba07-b289e080765a" /&gt; 
  &lt;img alt="The AI browser agent." src="https://github.com/user-attachments/assets/9955dda9-ede3-4971-8ee0-91cbc3850125" width="400" /&gt; 
 &lt;/picture&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://cloud.browser-use.com"&gt;&lt;img src="https://media.browser-use.tools/badges/package" height="48" alt="Browser-Use Package Download Statistics" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://raw.githubusercontent.com/browser-use/browser-use/main/#demos"&gt;&lt;img src="https://media.browser-use.tools/badges/demos" alt="Demos" /&gt;&lt;/a&gt; 
 &lt;img width="16" height="1" alt="" /&gt; 
 &lt;a href="https://docs.browser-use.com"&gt;&lt;img src="https://media.browser-use.tools/badges/docs" alt="Docs" /&gt;&lt;/a&gt; 
 &lt;img width="16" height="1" alt="" /&gt; 
 &lt;a href="https://browser-use.com/posts"&gt;&lt;img src="https://media.browser-use.tools/badges/blog" alt="Blog" /&gt;&lt;/a&gt; 
 &lt;img width="16" height="1" alt="" /&gt; 
 &lt;a href="https://browsermerch.com"&gt;&lt;img src="https://media.browser-use.tools/badges/merch" alt="Merch" /&gt;&lt;/a&gt; 
 &lt;img width="100" height="1" alt="" /&gt; 
 &lt;a href="https://github.com/browser-use/browser-use"&gt;&lt;img src="https://media.browser-use.tools/badges/github" alt="Github Stars" /&gt;&lt;/a&gt; 
 &lt;img width="4" height="1" alt="" /&gt; 
 &lt;a href="https://x.com/intent/user?screen_name=browser_use"&gt;&lt;img src="https://media.browser-use.tools/badges/twitter" alt="Twitter" /&gt;&lt;/a&gt; 
 &lt;img width="4 height=" 1" alt="" /&gt; 
 &lt;a href="https://link.browser-use.com/discord"&gt;&lt;img src="https://media.browser-use.tools/badges/discord" alt="Discord" /&gt;&lt;/a&gt; 
 &lt;img width="4" height="1" alt="" /&gt; 
 &lt;a href="https://cloud.browser-use.com"&gt;&lt;img src="https://media.browser-use.tools/badges/cloud" height="48" alt="Browser-Use Cloud" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;p&gt;üå§Ô∏è Want to skip the setup? Use our &lt;b&gt;&lt;a href="https://cloud.browser-use.com"&gt;cloud&lt;/a&gt;&lt;/b&gt; for faster, scalable, stealth-enabled browser automation!&lt;/p&gt; 
&lt;h1&gt;ü§ñ LLM Quickstart&lt;/h1&gt; 
&lt;ol&gt; 
 &lt;li&gt;Direct your favorite coding agent (Cursor, Claude Code, etc) to &lt;a href="https://docs.browser-use.com/llms-full.txt"&gt;Agents.md&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Prompt away!&lt;/li&gt; 
&lt;/ol&gt; 
&lt;br /&gt; 
&lt;h1&gt;üëã Human Quickstart&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;1. Create environment with &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt; (Python&amp;gt;=3.11):&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv init
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;2. Install Browser-Use package:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;#  We ship every day - use the latest version!
uv add browser-use
uv sync
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;3. Get your API key from &lt;a href="https://cloud.browser-use.com/new-api-key"&gt;Browser Use Cloud&lt;/a&gt; and add it to your &lt;code&gt;.env&lt;/code&gt; file (new signups get $10 free credits):&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;# .env
BROWSER_USE_API_KEY=your-key
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;4. Install Chromium browser:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uvx browser-use install
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;5. Run your first agent:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from browser_use import Agent, Browser, ChatBrowserUse
import asyncio

async def example():
    browser = Browser(
        # use_cloud=True,  # Uncomment to use a stealth browser on Browser Use Cloud
    )

    llm = ChatBrowserUse()

    agent = Agent(
        task="Find the number of stars of the browser-use repo",
        llm=llm,
        browser=browser,
    )

    history = await agent.run()
    return history

if __name__ == "__main__":
    history = asyncio.run(example())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Check out the &lt;a href="https://docs.browser-use.com"&gt;library docs&lt;/a&gt; and the &lt;a href="https://docs.cloud.browser-use.com"&gt;cloud docs&lt;/a&gt; for more!&lt;/p&gt; 
&lt;br /&gt; 
&lt;h1&gt;üî• Deploy on Sandboxes&lt;/h1&gt; 
&lt;p&gt;We handle agents, browsers, persistence, auth, cookies, and LLMs. The agent runs right next to the browser for minimal latency.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from browser_use import Browser, sandbox, ChatBrowserUse
from browser_use.agent.service import Agent
import asyncio

@sandbox()
async def my_task(browser: Browser):
    agent = Agent(task="Find the top HN post", browser=browser, llm=ChatBrowserUse())
    await agent.run()

# Just call it like any async function
asyncio.run(my_task())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href="https://docs.browser-use.com/production"&gt;Going to Production&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;br /&gt; 
&lt;h1&gt;üöÄ Template Quickstart&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;Want to get started even faster?&lt;/strong&gt; Generate a ready-to-run template:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uvx browser-use init --template default
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This creates a &lt;code&gt;browser_use_default.py&lt;/code&gt; file with a working example. Available templates:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;default&lt;/code&gt; - Minimal setup to get started quickly&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;advanced&lt;/code&gt; - All configuration options with detailed comments&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;tools&lt;/code&gt; - Examples of custom tools and extending the agent&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can also specify a custom output path:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uvx browser-use init --template default --output my_agent.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;br /&gt; 
&lt;h1&gt;Demos&lt;/h1&gt; 
&lt;h3&gt;üìã Form-Filling&lt;/h3&gt; 
&lt;h4&gt;Task = "Fill in this job application with my resume and information."&lt;/h4&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/57865ee6-6004-49d5-b2c2-6dff39ec2ba9" alt="Job Application Demo" /&gt; &lt;a href="https://github.com/browser-use/browser-use/raw/main/examples/use-cases/apply_to_job.py"&gt;Example code ‚Üó&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;üçé Grocery-Shopping&lt;/h3&gt; 
&lt;h4&gt;Task = "Put this list of items into my instacart."&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/a6813fa7-4a7c-40a6-b4aa-382bf88b1850"&gt;https://github.com/user-attachments/assets/a6813fa7-4a7c-40a6-b4aa-382bf88b1850&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/browser-use/browser-use/raw/main/examples/use-cases/buy_groceries.py"&gt;Example code ‚Üó&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;üíª Personal-Assistant.&lt;/h3&gt; 
&lt;h4&gt;Task = "Help me find parts for a custom PC."&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/ac34f75c-057a-43ef-ad06-5b2c9d42bf06"&gt;https://github.com/user-attachments/assets/ac34f75c-057a-43ef-ad06-5b2c9d42bf06&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/browser-use/browser-use/raw/main/examples/use-cases/pcpartpicker.py"&gt;Example code ‚Üó&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;üí°See &lt;a href="https://docs.browser-use.com/examples"&gt;more examples here ‚Üó&lt;/a&gt; and give us a star!&lt;/h3&gt; 
&lt;br /&gt; 
&lt;h2&gt;Integrations, hosting, custom tools, MCP, and more on our &lt;a href="https://docs.browser-use.com"&gt;Docs ‚Üó&lt;/a&gt;&lt;/h2&gt; 
&lt;br /&gt; 
&lt;h1&gt;FAQ&lt;/h1&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;What's the best model to use?&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;We optimized &lt;strong&gt;ChatBrowserUse()&lt;/strong&gt; specifically for browser automation tasks. On avg it completes tasks 3-5x faster than other models with SOTA accuracy.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Pricing (per 1M tokens):&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Input tokens: $0.20&lt;/li&gt; 
  &lt;li&gt;Cached input tokens: $0.02&lt;/li&gt; 
  &lt;li&gt;Output tokens: $2.00&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;For other LLM providers, see our &lt;a href="https://docs.browser-use.com/supported-models"&gt;supported models documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Can I use custom tools with the agent?&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;Yes! You can add custom tools to extend the agent's capabilities:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from browser_use import Tools

tools = Tools()

@tools.action(description='Description of what this tool does.')
def custom_tool(param: str) -&amp;gt; str:
    return f"Result: {param}"

agent = Agent(
    task="Your task",
    llm=llm,
    browser=browser,
    tools=tools,
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Can I use this for free?&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;Yes! Browser-Use is open source and free to use. You only need to choose an LLM provider (like OpenAI, Google, ChatBrowserUse, or run local models with Ollama).&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;How do I handle authentication?&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;Check out our authentication examples:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://github.com/browser-use/browser-use/raw/main/examples/browser/real_browser.py"&gt;Using real browser profiles&lt;/a&gt; - Reuse your existing Chrome profile with saved logins&lt;/li&gt; 
  &lt;li&gt;If you want to use temporary accounts with inbox, choose AgentMail&lt;/li&gt; 
  &lt;li&gt;To sync your auth profile with the remote browser, run &lt;code&gt;curl -fsSL https://browser-use.com/profile.sh | BROWSER_USE_API_KEY=XXXX sh&lt;/code&gt; (replace XXXX with your API key)&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;These examples show how to maintain sessions and handle authentication seamlessly.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;How do I solve CAPTCHAs?&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;For CAPTCHA handling, you need better browser fingerprinting and proxies. Use &lt;a href="https://cloud.browser-use.com"&gt;Browser Use Cloud&lt;/a&gt; which provides stealth browsers designed to avoid detection and CAPTCHA challenges.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;How do I go into production?&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;Chrome can consume a lot of memory, and running many agents in parallel can be tricky to manage.&lt;/p&gt; 
 &lt;p&gt;For production use cases, use our &lt;a href="https://cloud.browser-use.com"&gt;Browser Use Cloud API&lt;/a&gt; which handles:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Scalable browser infrastructure&lt;/li&gt; 
  &lt;li&gt;Memory management&lt;/li&gt; 
  &lt;li&gt;Proxy rotation&lt;/li&gt; 
  &lt;li&gt;Stealth browser fingerprinting&lt;/li&gt; 
  &lt;li&gt;High-performance parallel execution&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;strong&gt;Tell your computer what to do, and it gets it done.&lt;/strong&gt;&lt;/p&gt; 
 &lt;img src="https://github.com/user-attachments/assets/06fa3078-8461-4560-b434-445510c1766f" width="400" /&gt; 
 &lt;p&gt;&lt;a href="https://x.com/intent/user?screen_name=mamagnus00"&gt;&lt;img src="https://img.shields.io/twitter/follow/Magnus?style=social" alt="Twitter Follow" /&gt;&lt;/a&gt; ‚ÄÉ‚ÄÉ‚ÄÉ &lt;a href="https://x.com/intent/user?screen_name=gregpr07"&gt;&lt;img src="https://img.shields.io/twitter/follow/Gregor?style=social" alt="Twitter Follow" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt;
  Made with ‚ù§Ô∏è in Zurich and San Francisco 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>pipecat-ai/pipecat</title>
      <link>https://github.com/pipecat-ai/pipecat</link>
      <description>&lt;p&gt;Open Source framework for voice and multimodal conversational AI&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;
 &lt;div align="center"&gt; 
  &lt;img alt="pipecat" width="300px" height="auto" src="https://raw.githubusercontent.com/pipecat-ai/pipecat/main/pipecat.png" /&gt; 
 &lt;/div&gt;&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://pypi.org/project/pipecat-ai"&gt;&lt;img src="https://img.shields.io/pypi/v/pipecat-ai" alt="PyPI" /&gt;&lt;/a&gt; &lt;img src="https://github.com/pipecat-ai/pipecat/actions/workflows/tests.yaml/badge.svg?sanitize=true" alt="Tests" /&gt; &lt;a href="https://codecov.io/gh/pipecat-ai/pipecat"&gt;&lt;img src="https://codecov.io/gh/pipecat-ai/pipecat/graph/badge.svg?token=LNVUIVO4Y9" alt="codecov" /&gt;&lt;/a&gt; &lt;a href="https://docs.pipecat.ai"&gt;&lt;img src="https://img.shields.io/badge/Documentation-blue" alt="Docs" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/pipecat"&gt;&lt;img src="https://img.shields.io/discord/1239284677165056021" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/pipecat-ai/pipecat"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;üéôÔ∏è Pipecat: Real-Time Voice &amp;amp; Multimodal AI Agents&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;Pipecat&lt;/strong&gt; is an open-source Python framework for building real-time voice and multimodal conversational agents. Orchestrate audio and video, AI services, different transports, and conversation pipelines effortlessly‚Äîso you can focus on what makes your agent unique.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Want to dive right in? Try the &lt;a href="https://docs.pipecat.ai/getting-started/quickstart"&gt;quickstart&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;üöÄ What You Can Build&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Voice Assistants&lt;/strong&gt; ‚Äì natural, streaming conversations with AI&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;AI Companions&lt;/strong&gt; ‚Äì coaches, meeting assistants, characters&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multimodal Interfaces&lt;/strong&gt; ‚Äì voice, video, images, and more&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Interactive Storytelling&lt;/strong&gt; ‚Äì creative tools with generative media&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Business Agents&lt;/strong&gt; ‚Äì customer intake, support bots, guided flows&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Complex Dialog Systems&lt;/strong&gt; ‚Äì design logic with structured conversations&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üß† Why Pipecat?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Voice-first&lt;/strong&gt;: Integrates speech recognition, text-to-speech, and conversation handling&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Pluggable&lt;/strong&gt;: Supports many AI services and tools&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Composable Pipelines&lt;/strong&gt;: Build complex behavior from modular components&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Real-Time&lt;/strong&gt;: Ultra-low latency interaction with different transports (e.g. WebSockets or WebRTC)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üåê Pipecat Ecosystem&lt;/h2&gt; 
&lt;h3&gt;üì± Client SDKs&lt;/h3&gt; 
&lt;p&gt;Building client applications? You can connect to Pipecat from any platform using our official SDKs:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://docs.pipecat.ai/client/js/introduction"&gt;JavaScript&lt;/a&gt; | &lt;a href="https://docs.pipecat.ai/client/react/introduction"&gt;React&lt;/a&gt; | &lt;a href="https://docs.pipecat.ai/client/react-native/introduction"&gt;React Native&lt;/a&gt; | &lt;a href="https://docs.pipecat.ai/client/ios/introduction"&gt;Swift&lt;/a&gt; | &lt;a href="https://docs.pipecat.ai/client/android/introduction"&gt;Kotlin&lt;/a&gt; | &lt;a href="https://docs.pipecat.ai/client/c++/introduction"&gt;C++&lt;/a&gt; | &lt;a href="https://github.com/pipecat-ai/pipecat-esp32"&gt;ESP32&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;üß≠ Structured conversations&lt;/h3&gt; 
&lt;p&gt;Looking to build structured conversations? Check out &lt;a href="https://github.com/pipecat-ai/pipecat-flows"&gt;Pipecat Flows&lt;/a&gt; for managing complex conversational states and transitions.&lt;/p&gt; 
&lt;h3&gt;ü™Ñ Beautiful UIs&lt;/h3&gt; 
&lt;p&gt;Want to build beautiful and engaging experiences? Checkout the &lt;a href="https://github.com/pipecat-ai/voice-ui-kit"&gt;Voice UI Kit&lt;/a&gt;, a collection of components, hooks and templates for building voice AI applications quickly.&lt;/p&gt; 
&lt;h3&gt;üõ†Ô∏è Create and deploy projects&lt;/h3&gt; 
&lt;p&gt;Create a new project in under a minute with the &lt;a href="https://github.com/pipecat-ai/pipecat-cli"&gt;Pipecat CLI&lt;/a&gt;. Then use the CLI to monitor and deploy your agent to production.&lt;/p&gt; 
&lt;h3&gt;üîç Debugging&lt;/h3&gt; 
&lt;p&gt;Looking for help debugging your pipeline and processors? Check out &lt;a href="https://github.com/pipecat-ai/whisker"&gt;Whisker&lt;/a&gt;, a real-time Pipecat debugger.&lt;/p&gt; 
&lt;h3&gt;üñ•Ô∏è Terminal&lt;/h3&gt; 
&lt;p&gt;Love terminal applications? Check out &lt;a href="https://github.com/pipecat-ai/tail"&gt;Tail&lt;/a&gt;, a terminal dashboard for Pipecat.&lt;/p&gt; 
&lt;h3&gt;üì∫Ô∏è Pipecat TV Channel&lt;/h3&gt; 
&lt;p&gt;Catch new features, interviews, and how-tos on our &lt;a href="https://www.youtube.com/playlist?list=PLzU2zoMTQIHjqC3v4q2XVSR3hGSzwKFwH"&gt;Pipecat TV&lt;/a&gt; channel.&lt;/p&gt; 
&lt;h2&gt;üé¨ See it in action&lt;/h2&gt; 
&lt;p float="left"&gt; &lt;a href="https://github.com/pipecat-ai/pipecat-examples/tree/main/simple-chatbot"&gt;&lt;img src="https://raw.githubusercontent.com/pipecat-ai/pipecat-examples/main/simple-chatbot/image.png" width="400" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/pipecat-ai/pipecat-examples/tree/main/storytelling-chatbot"&gt;&lt;img src="https://raw.githubusercontent.com/pipecat-ai/pipecat-examples/main/storytelling-chatbot/image.png" width="400" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;a href="https://github.com/pipecat-ai/pipecat-examples/tree/main/translation-chatbot"&gt;&lt;img src="https://raw.githubusercontent.com/pipecat-ai/pipecat-examples/main/translation-chatbot/image.png" width="400" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/pipecat-ai/pipecat/raw/main/examples/foundational/12-describe-video.py"&gt;&lt;img src="https://github.com/pipecat-ai/pipecat/raw/main/examples/foundational/assets/moondream.png" width="400" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;üß© Available services&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Category&lt;/th&gt; 
   &lt;th&gt;Services&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Speech-to-Text&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.pipecat.ai/server/services/stt/assemblyai"&gt;AssemblyAI&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/stt/aws"&gt;AWS&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/stt/azure"&gt;Azure&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/stt/cartesia"&gt;Cartesia&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/stt/deepgram"&gt;Deepgram&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/stt/elevenlabs"&gt;ElevenLabs&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/stt/fal"&gt;Fal Wizper&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/stt/gladia"&gt;Gladia&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/stt/google"&gt;Google&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/stt/gradium"&gt;Gradium&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/stt/groq"&gt;Groq (Whisper)&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/stt/riva"&gt;NVIDIA Riva&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/stt/openai"&gt;OpenAI (Whisper)&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/stt/sambanova"&gt;SambaNova (Whisper)&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/stt/sarvam"&gt;Sarvam&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/stt/soniox"&gt;Soniox&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/stt/speechmatics"&gt;Speechmatics&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/stt/whisper"&gt;Whisper&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;LLMs&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.pipecat.ai/server/services/llm/anthropic"&gt;Anthropic&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/llm/aws"&gt;AWS&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/llm/azure"&gt;Azure&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/llm/cerebras"&gt;Cerebras&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/llm/deepseek"&gt;DeepSeek&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/llm/fireworks"&gt;Fireworks AI&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/llm/gemini"&gt;Gemini&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/llm/grok"&gt;Grok&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/llm/groq"&gt;Groq&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/llm/mistral"&gt;Mistral&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/llm/nim"&gt;NVIDIA NIM&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/llm/ollama"&gt;Ollama&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/llm/openai"&gt;OpenAI&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/llm/openrouter"&gt;OpenRouter&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/llm/perplexity"&gt;Perplexity&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/llm/qwen"&gt;Qwen&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/llm/sambanova"&gt;SambaNova&lt;/a&gt; &lt;a href="https://docs.pipecat.ai/server/services/llm/together"&gt;Together AI&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Text-to-Speech&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.pipecat.ai/server/services/tts/asyncai"&gt;Async&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/aws"&gt;AWS&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/azure"&gt;Azure&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/cartesia"&gt;Cartesia&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/deepgram"&gt;Deepgram&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/elevenlabs"&gt;ElevenLabs&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/fish"&gt;Fish&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/google"&gt;Google&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/gradium"&gt;Gradium&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/groq"&gt;Groq&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/hume"&gt;Hume&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/inworld"&gt;Inworld&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/lmnt"&gt;LMNT&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/minimax"&gt;MiniMax&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/neuphonic"&gt;Neuphonic&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/riva"&gt;NVIDIA Riva&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/openai"&gt;OpenAI&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/piper"&gt;Piper&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/playht"&gt;PlayHT&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/rime"&gt;Rime&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/sarvam"&gt;Sarvam&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/speechmatics"&gt;Speechmatics&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/xtts"&gt;XTTS&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Speech-to-Speech&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.pipecat.ai/server/services/s2s/aws"&gt;AWS Nova Sonic&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/s2s/gemini"&gt;Gemini Multimodal Live&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/s2s/grok"&gt;Grok Voice Agent&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/s2s/openai"&gt;OpenAI Realtime&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/s2s/ultravox"&gt;Ultravox&lt;/a&gt;,&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Transport&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.pipecat.ai/server/services/transport/daily"&gt;Daily (WebRTC)&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/transport/fastapi-websocket"&gt;FastAPI Websocket&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/transport/small-webrtc"&gt;SmallWebRTCTransport&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/transport/websocket-server"&gt;WebSocket Server&lt;/a&gt;, Local&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Serializers&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.pipecat.ai/server/utilities/serializers/plivo"&gt;Plivo&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/utilities/serializers/twilio"&gt;Twilio&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/utilities/serializers/telnyx"&gt;Telnyx&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Video&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.pipecat.ai/server/services/video/heygen"&gt;HeyGen&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/video/tavus"&gt;Tavus&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/video/simli"&gt;Simli&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Memory&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.pipecat.ai/server/services/memory/mem0"&gt;mem0&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Vision &amp;amp; Image&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.pipecat.ai/server/services/image-generation/fal"&gt;fal&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/image-generation/fal"&gt;Google Imagen&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/vision/moondream"&gt;Moondream&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Audio Processing&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.pipecat.ai/server/utilities/audio/silero-vad-analyzer"&gt;Silero VAD&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/utilities/audio/krisp-filter"&gt;Krisp&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/utilities/audio/koala-filter"&gt;Koala&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/utilities/audio/aic-filter"&gt;ai-coustics&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Analytics &amp;amp; Metrics&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.pipecat.ai/server/utilities/opentelemetry"&gt;OpenTelemetry&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/analytics/sentry"&gt;Sentry&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;üìö &lt;a href="https://docs.pipecat.ai/server/services/supported-services"&gt;View full services documentation ‚Üí&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;‚ö° Getting started&lt;/h2&gt; 
&lt;p&gt;You can get started with Pipecat running on your local machine, then move your agent processes to the cloud when you're ready.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Install uv&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;curl -LsSf https://astral.sh/uv/install.sh | sh
&lt;/code&gt;&lt;/pre&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;&lt;strong&gt;Need help?&lt;/strong&gt; Refer to the &lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;uv install documentation&lt;/a&gt;.&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install the module&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# For new projects
uv init my-pipecat-app
cd my-pipecat-app
uv add pipecat-ai

# Or for existing projects
uv add pipecat-ai
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Set up your environment&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cp env.example .env
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;To keep things lightweight, only the core framework is included by default. If you need support for third-party AI services, you can add the necessary dependencies with:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;uv add "pipecat-ai[option,...]"
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Using pip?&lt;/strong&gt; You can still use &lt;code&gt;pip install pipecat-ai&lt;/code&gt; and &lt;code&gt;pip install "pipecat-ai[option,...]"&lt;/code&gt; to get set up.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;üß™ Code examples&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pipecat-ai/pipecat/tree/main/examples/foundational"&gt;Foundational&lt;/a&gt; ‚Äî small snippets that build on each other, introducing one or two concepts at a time&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pipecat-ai/pipecat-examples"&gt;Example apps&lt;/a&gt; ‚Äî complete applications that you can use as starting points for development&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üõ†Ô∏è Contributing to the framework&lt;/h2&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Minimum Python Version:&lt;/strong&gt; 3.10 &lt;strong&gt;Recommended Python Version:&lt;/strong&gt; 3.12&lt;/p&gt; 
&lt;h3&gt;Setup Steps&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Clone the repository and navigate to it:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/pipecat-ai/pipecat.git
cd pipecat
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install development and testing dependencies:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;uv sync --group dev --all-extras \
  --no-extra gstreamer \
  --no-extra krisp \
  --no-extra local \
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install the git pre-commit hooks:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;uv run pre-commit install
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Some extras (local, gstreamer) require system dependencies. See documentation if you encounter build errors.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Running tests&lt;/h3&gt; 
&lt;p&gt;To run all tests, from the root directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run pytest
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run a specific test suite:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run pytest tests/test_name.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions from the community! Whether you're fixing bugs, improving documentation, or adding new features, here's how you can help:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Found a bug?&lt;/strong&gt; Open an &lt;a href="https://github.com/pipecat-ai/pipecat/issues"&gt;issue&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Have a feature idea?&lt;/strong&gt; Start a &lt;a href="https://discord.gg/pipecat"&gt;discussion&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Want to contribute code?&lt;/strong&gt; Check our &lt;a href="https://raw.githubusercontent.com/pipecat-ai/pipecat/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; guide&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Documentation improvements?&lt;/strong&gt; &lt;a href="https://github.com/pipecat-ai/docs"&gt;Docs&lt;/a&gt; PRs are always welcome&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Before submitting a pull request, please check existing issues and PRs to avoid duplicates.&lt;/p&gt; 
&lt;p&gt;We aim to review all contributions promptly and provide constructive feedback to help get your changes merged.&lt;/p&gt; 
&lt;h2&gt;üõü Getting help&lt;/h2&gt; 
&lt;p&gt;‚û°Ô∏è &lt;a href="https://discord.gg/pipecat"&gt;Join our Discord&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;‚û°Ô∏è &lt;a href="https://docs.pipecat.ai"&gt;Read the docs&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;‚û°Ô∏è &lt;a href="https://x.com/pipecat_ai"&gt;Reach us on X&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>HKUDS/RAG-Anything</title>
      <link>https://github.com/HKUDS/RAG-Anything</link>
      <description>&lt;p&gt;"RAG-Anything: All-in-One RAG Framework"&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;div style="margin: 20px 0;"&gt; 
  &lt;img src="https://raw.githubusercontent.com/HKUDS/RAG-Anything/main/assets/logo.png" width="120" height="120" alt="RAG-Anything Logo" style="border-radius: 20px; box-shadow: 0 8px 32px rgba(0, 217, 255, 0.3);" /&gt; 
 &lt;/div&gt; 
 &lt;h1&gt;üöÄ RAG-Anything: All-in-One RAG Framework&lt;/h1&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/14959" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/14959" alt="HKUDS%2FRAG-Anything | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;img src="https://readme-typing-svg.herokuapp.com?font=Orbitron&amp;amp;size=24&amp;amp;duration=3000&amp;amp;pause=1000&amp;amp;color=00D9FF&amp;amp;center=true&amp;amp;vCenter=true&amp;amp;width=600&amp;amp;lines=Welcome+to+RAG-Anything;Next-Gen+Multimodal+RAG+System;Powered+by+Advanced+AI+Technology" alt="Typing Animation" /&gt; 
 &lt;/div&gt; 
 &lt;div align="center"&gt; 
  &lt;div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; padding: 25px; text-align: center;"&gt; 
   &lt;p&gt; &lt;a href="https://github.com/HKUDS/RAG-Anything"&gt;&lt;img src="https://img.shields.io/badge/üî•Project-Page-00d9ff?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2510.12323"&gt;&lt;img src="https://img.shields.io/badge/üìÑarXiv-2510.12323-ff6b6b?style=for-the-badge&amp;amp;logo=arxiv&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;a href="https://github.com/HKUDS/LightRAG"&gt;&lt;img src="https://img.shields.io/badge/‚ö°Based%20on-LightRAG-4ecdc4?style=for-the-badge&amp;amp;logo=lightning&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;/p&gt; 
   &lt;p&gt; &lt;a href="https://github.com/HKUDS/RAG-Anything/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/HKUDS/RAG-Anything?color=00d9ff&amp;amp;style=for-the-badge&amp;amp;logo=star&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/badge/üêçPython-3.10-4ecdc4?style=for-the-badge&amp;amp;logo=python&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt; &lt;a href="https://pypi.org/project/raganything/"&gt;&lt;img src="https://img.shields.io/pypi/v/raganything.svg?style=for-the-badge&amp;amp;logo=pypi&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e&amp;amp;color=ff6b6b" /&gt;&lt;/a&gt; &lt;a href="https://github.com/astral-sh/uv"&gt;&lt;img src="https://img.shields.io/badge/‚ö°uv-Ready-ff6b6b?style=for-the-badge&amp;amp;logo=python&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;/p&gt; 
   &lt;p&gt; &lt;a href="https://discord.gg/yF2MmDJyGJ"&gt;&lt;img src="https://img.shields.io/badge/üí¨Discord-Community-7289da?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;a href="https://github.com/HKUDS/RAG-Anything/issues/7"&gt;&lt;img src="https://img.shields.io/badge/üí¨WeChat-Group-07c160?style=for-the-badge&amp;amp;logo=wechat&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;/p&gt; 
   &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/RAG-Anything/main/README_zh.md"&gt;&lt;img src="https://img.shields.io/badge/üá®üá≥‰∏≠ÊñáÁâà-1a1a2e?style=for-the-badge" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/RAG-Anything/main/README.md"&gt;&lt;img src="https://img.shields.io/badge/üá∫üá∏English-1a1a2e?style=for-the-badge" /&gt;&lt;/a&gt; &lt;/p&gt; 
  &lt;/div&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;div style="width: 100%; height: 2px; margin: 20px 0; background: linear-gradient(90deg, transparent, #00d9ff, transparent);"&gt;&lt;/div&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://raw.githubusercontent.com/HKUDS/RAG-Anything/main/#-quick-start" style="text-decoration: none;"&gt; &lt;img src="https://img.shields.io/badge/Quick%20Start-Get%20Started%20Now-00d9ff?style=for-the-badge&amp;amp;logo=rocket&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üéâ News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; [2025.10]üéØüì¢ üöÄ We have released the technical report of &lt;a href="http://arxiv.org/abs/2510.12323"&gt;RAG-Anything&lt;/a&gt;. Access it now to explore our latest research findings.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; [2025.08]üéØüì¢ üîç RAG-Anything now features &lt;strong&gt;VLM-Enhanced Query&lt;/strong&gt; mode! When documents include images, the system seamlessly integrates them into VLM for advanced multimodal analysis, combining visual and textual context for deeper insights.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; [2025.07]üéØüì¢ RAG-Anything now features a &lt;a href="https://raw.githubusercontent.com/HKUDS/RAG-Anything/main/docs/context_aware_processing.md"&gt;context configuration module&lt;/a&gt;, enabling intelligent integration of relevant contextual information to enhance multimodal content processing.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; [2025.07]üéØüì¢ üöÄ RAG-Anything now supports multimodal query capabilities, enabling enhanced RAG with seamless processing of text, images, tables, and equations.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; [2025.07]üéØüì¢ üéâ RAG-Anything has reached 1küåü stars on GitHub! Thank you for your incredible support and valuable contributions to the project.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üåü System Overview&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;Next-Generation Multimodal Intelligence&lt;/em&gt;&lt;/p&gt; 
&lt;div style="background: linear-gradient(135deg, #1a1a2e 0%, #16213e 50%, #0f3460 100%); border-radius: 15px; padding: 25px; margin: 20px 0; border: 2px solid #00d9ff; box-shadow: 0 0 30px rgba(0, 217, 255, 0.3);"&gt; 
 &lt;p&gt;Modern documents increasingly contain diverse multimodal content‚Äîtext, images, tables, equations, charts, and multimedia‚Äîthat traditional text-focused RAG systems cannot effectively process. &lt;strong&gt;RAG-Anything&lt;/strong&gt; addresses this challenge as a comprehensive &lt;strong&gt;All-in-One Multimodal Document Processing RAG system&lt;/strong&gt; built on &lt;a href="https://github.com/HKUDS/LightRAG"&gt;LightRAG&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;As a unified solution, RAG-Anything &lt;strong&gt;eliminates the need for multiple specialized tools&lt;/strong&gt;. It provides &lt;strong&gt;seamless processing and querying across all content modalities&lt;/strong&gt; within a single integrated framework. Unlike conventional RAG approaches that struggle with non-textual elements, our all-in-one system delivers &lt;strong&gt;comprehensive multimodal retrieval capabilities&lt;/strong&gt;.&lt;/p&gt; 
 &lt;p&gt;Users can query documents containing &lt;strong&gt;interleaved text&lt;/strong&gt;, &lt;strong&gt;visual diagrams&lt;/strong&gt;, &lt;strong&gt;structured tables&lt;/strong&gt;, and &lt;strong&gt;mathematical formulations&lt;/strong&gt; through &lt;strong&gt;one cohesive interface&lt;/strong&gt;. This consolidated approach makes RAG-Anything particularly valuable for academic research, technical documentation, financial reports, and enterprise knowledge management where rich, mixed-content documents demand a &lt;strong&gt;unified processing framework&lt;/strong&gt;.&lt;/p&gt; 
 &lt;img src="https://raw.githubusercontent.com/HKUDS/RAG-Anything/main/assets/rag_anything_framework.png" alt="RAG-Anything" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;üéØ Key Features&lt;/h3&gt; 
&lt;div style="background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%); border-radius: 15px; padding: 25px; margin: 20px 0;"&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;üîÑ End-to-End Multimodal Pipeline&lt;/strong&gt; - Complete workflow from document ingestion and parsing to intelligent multimodal query answering&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;üìÑ Universal Document Support&lt;/strong&gt; - Seamless processing of PDFs, Office documents, images, and diverse file formats&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;üß† Specialized Content Analysis&lt;/strong&gt; - Dedicated processors for images, tables, mathematical equations, and heterogeneous content types&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;üîó Multimodal Knowledge Graph&lt;/strong&gt; - Automatic entity extraction and cross-modal relationship discovery for enhanced understanding&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;‚ö° Adaptive Processing Modes&lt;/strong&gt; - Flexible MinerU-based parsing or direct multimodal content injection workflows&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;üìã Direct Content List Insertion&lt;/strong&gt; - Bypass document parsing by directly inserting pre-parsed content lists from external sources&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;üéØ Hybrid Intelligent Retrieval&lt;/strong&gt; - Advanced search capabilities spanning textual and multimodal content with contextual understanding&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üèóÔ∏è Algorithm &amp;amp; Architecture&lt;/h2&gt; 
&lt;div style="background: linear-gradient(135deg, #0f0f23 0%, #1a1a2e 100%); border-radius: 15px; padding: 25px; margin: 20px 0; border-left: 5px solid #00d9ff;"&gt; 
 &lt;h3&gt;Core Algorithm&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;RAG-Anything&lt;/strong&gt; implements an effective &lt;strong&gt;multi-stage multimodal pipeline&lt;/strong&gt; that fundamentally extends traditional RAG architectures to seamlessly handle diverse content modalities through intelligent orchestration and cross-modal understanding.&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;div style="width: 100%; max-width: 600px; margin: 20px auto; padding: 20px; background: linear-gradient(135deg, rgba(0, 217, 255, 0.1) 0%, rgba(0, 217, 255, 0.05) 100%); border-radius: 15px; border: 1px solid rgba(0, 217, 255, 0.2);"&gt; 
  &lt;div style="display: flex; justify-content: space-around; align-items: center; flex-wrap: wrap; gap: 20px;"&gt; 
   &lt;div style="text-align: center;"&gt; 
    &lt;div style="font-size: 24px; margin-bottom: 10px;"&gt;
     üìÑ
    &lt;/div&gt; 
    &lt;div style="font-size: 14px; color: #00d9ff;"&gt;
     Document Parsing
    &lt;/div&gt; 
   &lt;/div&gt; 
   &lt;div style="font-size: 20px; color: #00d9ff;"&gt;
    ‚Üí
   &lt;/div&gt; 
   &lt;div style="text-align: center;"&gt; 
    &lt;div style="font-size: 24px; margin-bottom: 10px;"&gt;
     üß†
    &lt;/div&gt; 
    &lt;div style="font-size: 14px; color: #00d9ff;"&gt;
     Content Analysis
    &lt;/div&gt; 
   &lt;/div&gt; 
   &lt;div style="font-size: 20px; color: #00d9ff;"&gt;
    ‚Üí
   &lt;/div&gt; 
   &lt;div style="text-align: center;"&gt; 
    &lt;div style="font-size: 24px; margin-bottom: 10px;"&gt;
     üîç
    &lt;/div&gt; 
    &lt;div style="font-size: 14px; color: #00d9ff;"&gt;
     Knowledge Graph
    &lt;/div&gt; 
   &lt;/div&gt; 
   &lt;div style="font-size: 20px; color: #00d9ff;"&gt;
    ‚Üí
   &lt;/div&gt; 
   &lt;div style="text-align: center;"&gt; 
    &lt;div style="font-size: 24px; margin-bottom: 10px;"&gt;
     üéØ
    &lt;/div&gt; 
    &lt;div style="font-size: 14px; color: #00d9ff;"&gt;
     Intelligent Retrieval
    &lt;/div&gt; 
   &lt;/div&gt; 
  &lt;/div&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;h3&gt;1. Document Parsing Stage&lt;/h3&gt; 
&lt;div style="background: linear-gradient(90deg, #1a1a2e 0%, #16213e 100%); border-radius: 10px; padding: 20px; margin: 15px 0; border-left: 4px solid #4ecdc4;"&gt; 
 &lt;p&gt;The system provides high-fidelity document extraction through adaptive content decomposition. It intelligently segments heterogeneous elements while preserving contextual relationships. Universal format compatibility is achieved via specialized optimized parsers.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Key Components:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;‚öôÔ∏è MinerU Integration&lt;/strong&gt;: Leverages &lt;a href="https://github.com/opendatalab/MinerU"&gt;MinerU&lt;/a&gt; for high-fidelity document structure extraction and semantic preservation across complex layouts.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;üß© Adaptive Content Decomposition&lt;/strong&gt;: Automatically segments documents into coherent text blocks, visual elements, structured tables, mathematical equations, and specialized content types while preserving contextual relationships.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;üìÅ Universal Format Support&lt;/strong&gt;: Provides comprehensive handling of PDFs, Office documents (DOC/DOCX/PPT/PPTX/XLS/XLSX), images, and emerging formats through specialized parsers with format-specific optimization.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/div&gt; 
&lt;h3&gt;2. Multi-Modal Content Understanding &amp;amp; Processing&lt;/h3&gt; 
&lt;div style="background: linear-gradient(90deg, #16213e 0%, #0f3460 100%); border-radius: 10px; padding: 20px; margin: 15px 0; border-left: 4px solid #ff6b6b;"&gt; 
 &lt;p&gt;The system automatically categorizes and routes content through optimized channels. It uses concurrent pipelines for parallel text and multimodal processing. Document hierarchy and relationships are preserved during transformation.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Key Components:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;üéØ Autonomous Content Categorization and Routing&lt;/strong&gt;: Automatically identify, categorize, and route different content types through optimized execution channels.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;‚ö° Concurrent Multi-Pipeline Architecture&lt;/strong&gt;: Implements concurrent execution of textual and multimodal content through dedicated processing pipelines. This approach maximizes throughput efficiency while preserving content integrity.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;üèóÔ∏è Document Hierarchy Extraction&lt;/strong&gt;: Extracts and preserves original document hierarchy and inter-element relationships during content transformation.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/div&gt; 
&lt;h3&gt;3. Multimodal Analysis Engine&lt;/h3&gt; 
&lt;div style="background: linear-gradient(90deg, #0f3460 0%, #1a1a2e 100%); border-radius: 10px; padding: 20px; margin: 15px 0; border-left: 4px solid #00d9ff;"&gt; 
 &lt;p&gt;The system deploys modality-aware processing units for heterogeneous data modalities:&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Specialized Analyzers:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;üîç Visual Content Analyzer&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Integrate vision model for image analysis.&lt;/li&gt; 
    &lt;li&gt;Generates context-aware descriptive captions based on visual semantics.&lt;/li&gt; 
    &lt;li&gt;Extracts spatial relationships and hierarchical structures between visual elements.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;üìä Structured Data Interpreter&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Performs systematic interpretation of tabular and structured data formats.&lt;/li&gt; 
    &lt;li&gt;Implements statistical pattern recognition algorithms for data trend analysis.&lt;/li&gt; 
    &lt;li&gt;Identifies semantic relationships and dependencies across multiple tabular datasets.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;üìê Mathematical Expression Parser&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Parses complex mathematical expressions and formulas with high accuracy.&lt;/li&gt; 
    &lt;li&gt;Provides native LaTeX format support for seamless integration with academic workflows.&lt;/li&gt; 
    &lt;li&gt;Establishes conceptual mappings between mathematical equations and domain-specific knowledge bases.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;üîß Extensible Modality Handler&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Provides configurable processing framework for custom and emerging content types.&lt;/li&gt; 
    &lt;li&gt;Enables dynamic integration of new modality processors through plugin architecture.&lt;/li&gt; 
    &lt;li&gt;Supports runtime configuration of processing pipelines for specialized use cases.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/div&gt; 
&lt;h3&gt;4. Multimodal Knowledge Graph Index&lt;/h3&gt; 
&lt;div style="background: linear-gradient(90deg, #1a1a2e 0%, #16213e 100%); border-radius: 10px; padding: 20px; margin: 15px 0; border-left: 4px solid #4ecdc4;"&gt; 
 &lt;p&gt;The multi-modal knowledge graph construction module transforms document content into structured semantic representations. It extracts multimodal entities, establishes cross-modal relationships, and preserves hierarchical organization. The system applies weighted relevance scoring for optimized knowledge retrieval.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Core Functions:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;üîç Multi-Modal Entity Extraction&lt;/strong&gt;: Transforms significant multimodal elements into structured knowledge graph entities. The process includes semantic annotations and metadata preservation.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;üîó Cross-Modal Relationship Mapping&lt;/strong&gt;: Establishes semantic connections and dependencies between textual entities and multimodal components. This is achieved through automated relationship inference algorithms.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;üèóÔ∏è Hierarchical Structure Preservation&lt;/strong&gt;: Maintains original document organization through "belongs_to" relationship chains. These chains preserve logical content hierarchy and sectional dependencies.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;‚öñÔ∏è Weighted Relationship Scoring&lt;/strong&gt;: Assigns quantitative relevance scores to relationship types. Scoring is based on semantic proximity and contextual significance within the document structure.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/div&gt; 
&lt;h3&gt;5. Modality-Aware Retrieval&lt;/h3&gt; 
&lt;div style="background: linear-gradient(90deg, #16213e 0%, #0f3460 100%); border-radius: 10px; padding: 20px; margin: 15px 0; border-left: 4px solid #ff6b6b;"&gt; 
 &lt;p&gt;The hybrid retrieval system combines vector similarity search with graph traversal algorithms for comprehensive content retrieval. It implements modality-aware ranking mechanisms and maintains relational coherence between retrieved elements to ensure contextually integrated information delivery.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Retrieval Mechanisms:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;üîÄ Vector-Graph Fusion&lt;/strong&gt;: Integrates vector similarity search with graph traversal algorithms. This approach leverages both semantic embeddings and structural relationships for comprehensive content retrieval.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;üìä Modality-Aware Ranking&lt;/strong&gt;: Implements adaptive scoring mechanisms that weight retrieval results based on content type relevance. The system adjusts rankings according to query-specific modality preferences.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;üîó Relational Coherence Maintenance&lt;/strong&gt;: Maintains semantic and structural relationships between retrieved elements. This ensures coherent information delivery and contextual integrity.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üöÄ Quick Start&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;Initialize Your AI Journey&lt;/em&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://user-images.githubusercontent.com/74038190/212284158-e840e285-664b-44d7-b79b-e264b5e54825.gif" width="400" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;h4&gt;Option 1: Install from PyPI (Recommended)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Basic installation
pip install raganything

# With optional dependencies for extended format support:
pip install 'raganything[all]'              # All optional features
pip install 'raganything[image]'            # Image format conversion (BMP, TIFF, GIF, WebP)
pip install 'raganything[text]'             # Text file processing (TXT, MD)
pip install 'raganything[image,text]'       # Multiple features
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Option 2: Install from Source&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install uv (if not already installed)
curl -LsSf https://astral.sh/uv/install.sh | sh

# Clone and setup the project with uv
git clone https://github.com/HKUDS/RAG-Anything.git
cd RAG-Anything

# Install the package and dependencies in a virtual environment
uv sync

# If you encounter network timeouts (especially for opencv packages):
# UV_HTTP_TIMEOUT=120 uv sync

# Run commands directly with uv (recommended approach)
uv run python examples/raganything_example.py --help

# Install with optional dependencies
uv sync --extra image --extra text  # Specific extras
uv sync --all-extras                 # All optional features
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Optional Dependencies&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;[image]&lt;/code&gt;&lt;/strong&gt; - Enables processing of BMP, TIFF, GIF, WebP image formats (requires Pillow)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;[text]&lt;/code&gt;&lt;/strong&gt; - Enables processing of TXT and MD files (requires ReportLab)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;[all]&lt;/code&gt;&lt;/strong&gt; - Includes all Python optional dependencies&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;‚ö†Ô∏è Office Document Processing Requirements:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Office documents (.doc, .docx, .ppt, .pptx, .xls, .xlsx) require &lt;strong&gt;LibreOffice&lt;/strong&gt; installation&lt;/li&gt; 
  &lt;li&gt;Download from &lt;a href="https://www.libreoffice.org/download/download/"&gt;LibreOffice official website&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt;: Download installer from official website&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;macOS&lt;/strong&gt;: &lt;code&gt;brew install --cask libreoffice&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Ubuntu/Debian&lt;/strong&gt;: &lt;code&gt;sudo apt-get install libreoffice&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;CentOS/RHEL&lt;/strong&gt;: &lt;code&gt;sudo yum install libreoffice&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Check MinerU installation:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Verify installation
mineru --version

# Check if properly configured
python -c "from raganything import RAGAnything; rag = RAGAnything(); print('‚úÖ MinerU installed properly' if rag.check_parser_installation() else '‚ùå MinerU installation issue')"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Models are downloaded automatically on first use. For manual download, refer to &lt;a href="https://github.com/opendatalab/MinerU/raw/master/README.md#22-model-source-configuration"&gt;MinerU Model Source Configuration&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Usage Examples&lt;/h3&gt; 
&lt;h4&gt;1. End-to-End Document Processing&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import asyncio
from raganything import RAGAnything, RAGAnythingConfig
from lightrag.llm.openai import openai_complete_if_cache, openai_embed
from lightrag.utils import EmbeddingFunc

async def main():
    # Set up API configuration
    api_key = "your-api-key"
    base_url = "your-base-url"  # Optional

    # Create RAGAnything configuration
    config = RAGAnythingConfig(
        working_dir="./rag_storage",
        parser="mineru",  # Parser selection: mineru or docling
        parse_method="auto",  # Parse method: auto, ocr, or txt
        enable_image_processing=True,
        enable_table_processing=True,
        enable_equation_processing=True,
    )

    # Define LLM model function
    def llm_model_func(prompt, system_prompt=None, history_messages=[], **kwargs):
        return openai_complete_if_cache(
            "gpt-4o-mini",
            prompt,
            system_prompt=system_prompt,
            history_messages=history_messages,
            api_key=api_key,
            base_url=base_url,
            **kwargs,
        )

    # Define vision model function for image processing
    def vision_model_func(
        prompt, system_prompt=None, history_messages=[], image_data=None, messages=None, **kwargs
    ):
        # If messages format is provided (for multimodal VLM enhanced query), use it directly
        if messages:
            return openai_complete_if_cache(
                "gpt-4o",
                "",
                system_prompt=None,
                history_messages=[],
                messages=messages,
                api_key=api_key,
                base_url=base_url,
                **kwargs,
            )
        # Traditional single image format
        elif image_data:
            return openai_complete_if_cache(
                "gpt-4o",
                "",
                system_prompt=None,
                history_messages=[],
                messages=[
                    {"role": "system", "content": system_prompt}
                    if system_prompt
                    else None,
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{image_data}"
                                },
                            },
                        ],
                    }
                    if image_data
                    else {"role": "user", "content": prompt},
                ],
                api_key=api_key,
                base_url=base_url,
                **kwargs,
            )
        # Pure text format
        else:
            return llm_model_func(prompt, system_prompt, history_messages, **kwargs)

    # Define embedding function
    embedding_func = EmbeddingFunc(
        embedding_dim=3072,
        max_token_size=8192,
        func=lambda texts: openai_embed(
            texts,
            model="text-embedding-3-large",
            api_key=api_key,
            base_url=base_url,
        ),
    )

    # Initialize RAGAnything
    rag = RAGAnything(
        config=config,
        llm_model_func=llm_model_func,
        vision_model_func=vision_model_func,
        embedding_func=embedding_func,
    )

    # Process a document
    await rag.process_document_complete(
        file_path="path/to/your/document.pdf",
        output_dir="./output",
        parse_method="auto"
    )

    # Query the processed content
    # Pure text query - for basic knowledge base search
    text_result = await rag.aquery(
        "What are the main findings shown in the figures and tables?",
        mode="hybrid"
    )
    print("Text query result:", text_result)

    # Multimodal query with specific multimodal content
    multimodal_result = await rag.aquery_with_multimodal(
    "Explain this formula and its relevance to the document content",
    multimodal_content=[{
        "type": "equation",
        "latex": "P(d|q) = \\frac{P(q|d) \\cdot P(d)}{P(q)}",
        "equation_caption": "Document relevance probability"
    }],
    mode="hybrid"
)
    print("Multimodal query result:", multimodal_result)

if __name__ == "__main__":
    asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;2. Direct Multimodal Content Processing&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import asyncio
from lightrag import LightRAG
from lightrag.llm.openai import openai_complete_if_cache, openai_embed
from lightrag.utils import EmbeddingFunc
from raganything.modalprocessors import ImageModalProcessor, TableModalProcessor

async def process_multimodal_content():
    # Set up API configuration
    api_key = "your-api-key"
    base_url = "your-base-url"  # Optional

    # Initialize LightRAG
    rag = LightRAG(
        working_dir="./rag_storage",
        llm_model_func=lambda prompt, system_prompt=None, history_messages=[], **kwargs: openai_complete_if_cache(
            "gpt-4o-mini",
            prompt,
            system_prompt=system_prompt,
            history_messages=history_messages,
            api_key=api_key,
            base_url=base_url,
            **kwargs,
        ),
        embedding_func=EmbeddingFunc(
            embedding_dim=3072,
            max_token_size=8192,
            func=lambda texts: openai_embed(
                texts,
                model="text-embedding-3-large",
                api_key=api_key,
                base_url=base_url,
            ),
        )
    )
    await rag.initialize_storages()

    # Process an image
    image_processor = ImageModalProcessor(
        lightrag=rag,
        modal_caption_func=lambda prompt, system_prompt=None, history_messages=[], image_data=None, **kwargs: openai_complete_if_cache(
            "gpt-4o",
            "",
            system_prompt=None,
            history_messages=[],
            messages=[
                {"role": "system", "content": system_prompt} if system_prompt else None,
                {"role": "user", "content": [
                    {"type": "text", "text": prompt},
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}}
                ]} if image_data else {"role": "user", "content": prompt}
            ],
            api_key=api_key,
            base_url=base_url,
            **kwargs,
        ) if image_data else openai_complete_if_cache(
            "gpt-4o-mini",
            prompt,
            system_prompt=system_prompt,
            history_messages=history_messages,
            api_key=api_key,
            base_url=base_url,
            **kwargs,
        )
    )

    image_content = {
        "img_path": "path/to/image.jpg",
        "image_caption": ["Figure 1: Experimental results"],
        "image_footnote": ["Data collected in 2024"]
    }

    description, entity_info = await image_processor.process_multimodal_content(
        modal_content=image_content,
        content_type="image",
        file_path="research_paper.pdf",
        entity_name="Experimental Results Figure"
    )

    # Process a table
    table_processor = TableModalProcessor(
        lightrag=rag,
        modal_caption_func=lambda prompt, system_prompt=None, history_messages=[], **kwargs: openai_complete_if_cache(
            "gpt-4o-mini",
            prompt,
            system_prompt=system_prompt,
            history_messages=history_messages,
            api_key=api_key,
            base_url=base_url,
            **kwargs,
        )
    )

    table_content = {
        "table_body": """
        | Method | Accuracy | F1-Score |
        |--------|----------|----------|
        | RAGAnything | 95.2% | 0.94 |
        | Baseline | 87.3% | 0.85 |
        """,
        "table_caption": ["Performance Comparison"],
        "table_footnote": ["Results on test dataset"]
    }

    description, entity_info = await table_processor.process_multimodal_content(
        modal_content=table_content,
        content_type="table",
        file_path="research_paper.pdf",
        entity_name="Performance Results Table"
    )

if __name__ == "__main__":
    asyncio.run(process_multimodal_content())
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;3. Batch Processing&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Process multiple documents
await rag.process_folder_complete(
    folder_path="./documents",
    output_dir="./output",
    file_extensions=[".pdf", ".docx", ".pptx"],
    recursive=True,
    max_workers=4
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;4. Custom Modal Processors&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from raganything.modalprocessors import GenericModalProcessor

class CustomModalProcessor(GenericModalProcessor):
    async def process_multimodal_content(self, modal_content, content_type, file_path, entity_name):
        # Your custom processing logic
        enhanced_description = await self.analyze_custom_content(modal_content)
        entity_info = self.create_custom_entity(enhanced_description, entity_name)
        return await self._create_entity_and_chunk(enhanced_description, entity_info, file_path)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;5. Query Options&lt;/h4&gt; 
&lt;p&gt;RAG-Anything provides three types of query methods:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Pure Text Queries&lt;/strong&gt; - Direct knowledge base search using LightRAG:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Different query modes for text queries
text_result_hybrid = await rag.aquery("Your question", mode="hybrid")
text_result_local = await rag.aquery("Your question", mode="local")
text_result_global = await rag.aquery("Your question", mode="global")
text_result_naive = await rag.aquery("Your question", mode="naive")

# Synchronous version
sync_text_result = rag.query("Your question", mode="hybrid")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;VLM Enhanced Queries&lt;/strong&gt; - Automatically analyze images in retrieved context using VLM:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# VLM enhanced query (automatically enabled when vision_model_func is provided)
vlm_result = await rag.aquery(
    "Analyze the charts and figures in the document",
    mode="hybrid"
    # vlm_enhanced=True is automatically set when vision_model_func is available
)

# Manually control VLM enhancement
vlm_enabled = await rag.aquery(
    "What do the images show in this document?",
    mode="hybrid",
    vlm_enhanced=True  # Force enable VLM enhancement
)

vlm_disabled = await rag.aquery(
    "What do the images show in this document?",
    mode="hybrid",
    vlm_enhanced=False  # Force disable VLM enhancement
)

# When documents contain images, VLM can see and analyze them directly
# The system will automatically:
# 1. Retrieve relevant context containing image paths
# 2. Load and encode images as base64
# 3. Send both text context and images to VLM for comprehensive analysis
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Multimodal Queries&lt;/strong&gt; - Enhanced queries with specific multimodal content analysis:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Query with table data
table_result = await rag.aquery_with_multimodal(
    "Compare these performance metrics with the document content",
    multimodal_content=[{
        "type": "table",
        "table_data": """Method,Accuracy,Speed
                        RAGAnything,95.2%,120ms
                        Traditional,87.3%,180ms""",
        "table_caption": "Performance comparison"
    }],
    mode="hybrid"
)

# Query with equation content
equation_result = await rag.aquery_with_multimodal(
    "Explain this formula and its relevance to the document content",
    multimodal_content=[{
        "type": "equation",
        "latex": "P(d|q) = \\frac{P(q|d) \\cdot P(d)}{P(q)}",
        "equation_caption": "Document relevance probability"
    }],
    mode="hybrid"
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;6. Loading Existing LightRAG Instance&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import asyncio
from raganything import RAGAnything, RAGAnythingConfig
from lightrag import LightRAG
from lightrag.llm.openai import openai_complete_if_cache, openai_embed
from lightrag.kg.shared_storage import initialize_pipeline_status
from lightrag.utils import EmbeddingFunc
import os

async def load_existing_lightrag():
    # Set up API configuration
    api_key = "your-api-key"
    base_url = "your-base-url"  # Optional

    # First, create or load existing LightRAG instance
    lightrag_working_dir = "./existing_lightrag_storage"

    # Check if previous LightRAG instance exists
    if os.path.exists(lightrag_working_dir) and os.listdir(lightrag_working_dir):
        print("‚úÖ Found existing LightRAG instance, loading...")
    else:
        print("‚ùå No existing LightRAG instance found, will create new one")

    # Create/load LightRAG instance with your configuration
    lightrag_instance = LightRAG(
        working_dir=lightrag_working_dir,
        llm_model_func=lambda prompt, system_prompt=None, history_messages=[], **kwargs: openai_complete_if_cache(
            "gpt-4o-mini",
            prompt,
            system_prompt=system_prompt,
            history_messages=history_messages,
            api_key=api_key,
            base_url=base_url,
            **kwargs,
        ),
        embedding_func=EmbeddingFunc(
            embedding_dim=3072,
            max_token_size=8192,
            func=lambda texts: openai_embed(
                texts,
                model="text-embedding-3-large",
                api_key=api_key,
                base_url=base_url,
            ),
        )
    )

    # Initialize storage (this will load existing data if available)
    await lightrag_instance.initialize_storages()
    await initialize_pipeline_status()

    # Define vision model function for image processing
    def vision_model_func(
        prompt, system_prompt=None, history_messages=[], image_data=None, messages=None, **kwargs
    ):
        # If messages format is provided (for multimodal VLM enhanced query), use it directly
        if messages:
            return openai_complete_if_cache(
                "gpt-4o",
                "",
                system_prompt=None,
                history_messages=[],
                messages=messages,
                api_key=api_key,
                base_url=base_url,
                **kwargs,
            )
        # Traditional single image format
        elif image_data:
            return openai_complete_if_cache(
                "gpt-4o",
                "",
                system_prompt=None,
                history_messages=[],
                messages=[
                    {"role": "system", "content": system_prompt}
                    if system_prompt
                    else None,
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{image_data}"
                                },
                            },
                        ],
                    }
                    if image_data
                    else {"role": "user", "content": prompt},
                ],
                api_key=api_key,
                base_url=base_url,
                **kwargs,
            )
        # Pure text format
        else:
            return lightrag_instance.llm_model_func(prompt, system_prompt, history_messages, **kwargs)

    # Now use existing LightRAG instance to initialize RAGAnything
    rag = RAGAnything(
        lightrag=lightrag_instance,  # Pass existing LightRAG instance
        vision_model_func=vision_model_func,
        # Note: working_dir, llm_model_func, embedding_func, etc. are inherited from lightrag_instance
    )

    # Query existing knowledge base
    result = await rag.aquery(
        "What data has been processed in this LightRAG instance?",
        mode="hybrid"
    )
    print("Query result:", result)

    # Add new multimodal document to existing LightRAG instance
    await rag.process_document_complete(
        file_path="path/to/new/multimodal_document.pdf",
        output_dir="./output"
    )

if __name__ == "__main__":
    asyncio.run(load_existing_lightrag())
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;7. Direct Content List Insertion&lt;/h4&gt; 
&lt;p&gt;For scenarios where you already have a pre-parsed content list (e.g., from external parsers or previous processing), you can directly insert it into RAGAnything without document parsing:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import asyncio
from raganything import RAGAnything, RAGAnythingConfig
from lightrag.llm.openai import openai_complete_if_cache, openai_embed
from lightrag.utils import EmbeddingFunc

async def insert_content_list_example():
    # Set up API configuration
    api_key = "your-api-key"
    base_url = "your-base-url"  # Optional

    # Create RAGAnything configuration
    config = RAGAnythingConfig(
        working_dir="./rag_storage",
        enable_image_processing=True,
        enable_table_processing=True,
        enable_equation_processing=True,
    )

    # Define model functions
    def llm_model_func(prompt, system_prompt=None, history_messages=[], **kwargs):
        return openai_complete_if_cache(
            "gpt-4o-mini",
            prompt,
            system_prompt=system_prompt,
            history_messages=history_messages,
            api_key=api_key,
            base_url=base_url,
            **kwargs,
        )

    def vision_model_func(prompt, system_prompt=None, history_messages=[], image_data=None, messages=None, **kwargs):
        # If messages format is provided (for multimodal VLM enhanced query), use it directly
        if messages:
            return openai_complete_if_cache(
                "gpt-4o",
                "",
                system_prompt=None,
                history_messages=[],
                messages=messages,
                api_key=api_key,
                base_url=base_url,
                **kwargs,
            )
        # Traditional single image format
        elif image_data:
            return openai_complete_if_cache(
                "gpt-4o",
                "",
                system_prompt=None,
                history_messages=[],
                messages=[
                    {"role": "system", "content": system_prompt} if system_prompt else None,
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}}
                        ],
                    } if image_data else {"role": "user", "content": prompt},
                ],
                api_key=api_key,
                base_url=base_url,
                **kwargs,
            )
        # Pure text format
        else:
            return llm_model_func(prompt, system_prompt, history_messages, **kwargs)

    embedding_func = EmbeddingFunc(
        embedding_dim=3072,
        max_token_size=8192,
        func=lambda texts: openai_embed(
            texts,
            model="text-embedding-3-large",
            api_key=api_key,
            base_url=base_url,
        ),
    )

    # Initialize RAGAnything
    rag = RAGAnything(
        config=config,
        llm_model_func=llm_model_func,
        vision_model_func=vision_model_func,
        embedding_func=embedding_func,
    )

    # Example: Pre-parsed content list from external source
    content_list = [
        {
            "type": "text",
            "text": "This is the introduction section of our research paper.",
            "page_idx": 0  # Page number where this content appears
        },
        {
            "type": "image",
            "img_path": "/absolute/path/to/figure1.jpg",  # IMPORTANT: Use absolute path
            "image_caption": ["Figure 1: System Architecture"],
            "image_footnote": ["Source: Authors' original design"],
            "page_idx": 1  # Page number where this image appears
        },
        {
            "type": "table",
            "table_body": "| Method | Accuracy | F1-Score |\n|--------|----------|----------|\n| Ours | 95.2% | 0.94 |\n| Baseline | 87.3% | 0.85 |",
            "table_caption": ["Table 1: Performance Comparison"],
            "table_footnote": ["Results on test dataset"],
            "page_idx": 2  # Page number where this table appears
        },
        {
            "type": "equation",
            "latex": "P(d|q) = \\frac{P(q|d) \\cdot P(d)}{P(q)}",
            "text": "Document relevance probability formula",
            "page_idx": 3  # Page number where this equation appears
        },
        {
            "type": "text",
            "text": "In conclusion, our method demonstrates superior performance across all metrics.",
            "page_idx": 4  # Page number where this content appears
        }
    ]

    # Insert the content list directly
    await rag.insert_content_list(
        content_list=content_list,
        file_path="research_paper.pdf",  # Reference file name for citation
        split_by_character=None,         # Optional text splitting
        split_by_character_only=False,   # Optional text splitting mode
        doc_id=None,                     # Optional custom document ID (will be auto-generated if not provided)
        display_stats=True               # Show content statistics
    )

    # Query the inserted content
    result = await rag.aquery(
        "What are the key findings and performance metrics mentioned in the research?",
        mode="hybrid"
    )
    print("Query result:", result)

    # You can also insert multiple content lists with different document IDs
    another_content_list = [
        {
            "type": "text",
            "text": "This is content from another document.",
            "page_idx": 0  # Page number where this content appears
        },
        {
            "type": "table",
            "table_body": "| Feature | Value |\n|---------|-------|\n| Speed | Fast |\n| Accuracy | High |",
            "table_caption": ["Feature Comparison"],
            "page_idx": 1  # Page number where this table appears
        }
    ]

    await rag.insert_content_list(
        content_list=another_content_list,
        file_path="another_document.pdf",
        doc_id="custom-doc-id-123"  # Custom document ID
    )

if __name__ == "__main__":
    asyncio.run(insert_content_list_example())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Content List Format:&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;The &lt;code&gt;content_list&lt;/code&gt; should follow the standard format with each item being a dictionary containing:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Text content&lt;/strong&gt;: &lt;code&gt;{"type": "text", "text": "content text", "page_idx": 0}&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Image content&lt;/strong&gt;: &lt;code&gt;{"type": "image", "img_path": "/absolute/path/to/image.jpg", "image_caption": ["caption"], "image_footnote": ["note"], "page_idx": 1}&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Table content&lt;/strong&gt;: &lt;code&gt;{"type": "table", "table_body": "markdown table", "table_caption": ["caption"], "table_footnote": ["note"], "page_idx": 2}&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Equation content&lt;/strong&gt;: &lt;code&gt;{"type": "equation", "latex": "LaTeX formula", "text": "description", "page_idx": 3}&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Generic content&lt;/strong&gt;: &lt;code&gt;{"type": "custom_type", "content": "any content", "page_idx": 4}&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Important Notes:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;img_path&lt;/code&gt;&lt;/strong&gt;: Must be an absolute path to the image file (e.g., &lt;code&gt;/home/user/images/chart.jpg&lt;/code&gt; or &lt;code&gt;C:\Users\user\images\chart.jpg&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;page_idx&lt;/code&gt;&lt;/strong&gt;: Represents the page number where the content appears in the original document (0-based indexing)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Content ordering&lt;/strong&gt;: Items are processed in the order they appear in the list&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This method is particularly useful when:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;You have content from external parsers (non-MinerU/Docling)&lt;/li&gt; 
 &lt;li&gt;You want to process programmatically generated content&lt;/li&gt; 
 &lt;li&gt;You need to insert content from multiple sources into a single knowledge base&lt;/li&gt; 
 &lt;li&gt;You have cached parsing results that you want to reuse&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üõ†Ô∏è Examples&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;Practical Implementation Demos&lt;/em&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://user-images.githubusercontent.com/74038190/212257455-13e3e01e-d6a6-45dc-bb92-3ab87b12dfc1.gif" width="300" /&gt; 
&lt;/div&gt; 
&lt;p&gt;The &lt;code&gt;examples/&lt;/code&gt; directory contains comprehensive usage examples:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;raganything_example.py&lt;/code&gt;&lt;/strong&gt;: End-to-end document processing with MinerU&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;modalprocessors_example.py&lt;/code&gt;&lt;/strong&gt;: Direct multimodal content processing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;office_document_test.py&lt;/code&gt;&lt;/strong&gt;: Office document parsing test with MinerU (no API key required)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;image_format_test.py&lt;/code&gt;&lt;/strong&gt;: Image format parsing test with MinerU (no API key required)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;text_format_test.py&lt;/code&gt;&lt;/strong&gt;: Text format parsing test with MinerU (no API key required)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Run examples:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# End-to-end processing with parser selection
python examples/raganything_example.py path/to/document.pdf --api-key YOUR_API_KEY --parser mineru

# Direct modal processing
python examples/modalprocessors_example.py --api-key YOUR_API_KEY

# Office document parsing test (MinerU only)
python examples/office_document_test.py --file path/to/document.docx

# Image format parsing test (MinerU only)
python examples/image_format_test.py --file path/to/image.bmp

# Text format parsing test (MinerU only)
python examples/text_format_test.py --file path/to/document.md

# Check LibreOffice installation
python examples/office_document_test.py --check-libreoffice --file dummy

# Check PIL/Pillow installation
python examples/image_format_test.py --check-pillow --file dummy

# Check ReportLab installation
python examples/text_format_test.py --check-reportlab --file dummy
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üîß Configuration&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;System Optimization Parameters&lt;/em&gt;&lt;/p&gt; 
&lt;h3&gt;Environment Variables&lt;/h3&gt; 
&lt;p&gt;Create a &lt;code&gt;.env&lt;/code&gt; file (refer to &lt;code&gt;.env.example&lt;/code&gt;):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;OPENAI_API_KEY=your_openai_api_key
OPENAI_BASE_URL=your_base_url  # Optional
OUTPUT_DIR=./output             # Default output directory for parsed documents
PARSER=mineru                   # Parser selection: mineru or docling
PARSE_METHOD=auto              # Parse method: auto, ocr, or txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; For backward compatibility, legacy environment variable names are still supported:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;MINERU_PARSE_METHOD&lt;/code&gt; is deprecated, please use &lt;code&gt;PARSE_METHOD&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: API keys are only required for full RAG processing with LLM integration. The parsing test files (&lt;code&gt;office_document_test.py&lt;/code&gt; and &lt;code&gt;image_format_test.py&lt;/code&gt;) only test parser functionality and do not require API keys.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Parser Configuration&lt;/h3&gt; 
&lt;p&gt;RAGAnything now supports multiple parsers, each with specific advantages:&lt;/p&gt; 
&lt;h4&gt;MinerU Parser&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Supports PDF, images, Office documents, and more formats&lt;/li&gt; 
 &lt;li&gt;Powerful OCR and table extraction capabilities&lt;/li&gt; 
 &lt;li&gt;GPU acceleration support&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Docling Parser&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Optimized for Office documents and HTML files&lt;/li&gt; 
 &lt;li&gt;Better document structure preservation&lt;/li&gt; 
 &lt;li&gt;Native support for multiple Office formats&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;MinerU Configuration&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# MinerU 2.0 uses command-line parameters instead of config files
# Check available options:
mineru --help

# Common configurations:
mineru -p input.pdf -o output_dir -m auto    # Automatic parsing mode
mineru -p input.pdf -o output_dir -m ocr     # OCR-focused parsing
mineru -p input.pdf -o output_dir -b pipeline --device cuda  # GPU acceleration
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also configure parsing through RAGAnything parameters:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Basic parsing configuration with parser selection
await rag.process_document_complete(
    file_path="document.pdf",
    output_dir="./output/",
    parse_method="auto",          # or "ocr", "txt"
    parser="mineru"               # Optional: "mineru" or "docling"
)

# Advanced parsing configuration with special parameters
await rag.process_document_complete(
    file_path="document.pdf",
    output_dir="./output/",
    parse_method="auto",          # Parsing method: "auto", "ocr", "txt"
    parser="mineru",              # Parser selection: "mineru" or "docling"

    # MinerU special parameters - all supported kwargs:
    lang="ch",                   # Document language for OCR optimization (e.g., "ch", "en", "ja")
    device="cuda:0",             # Inference device: "cpu", "cuda", "cuda:0", "npu", "mps"
    start_page=0,                # Starting page number (0-based, for PDF)
    end_page=10,                 # Ending page number (0-based, for PDF)
    formula=True,                # Enable formula parsing
    table=True,                  # Enable table parsing
    backend="pipeline",          # Parsing backend: pipeline|vlm-transformers|vlm-sglang-engine|vlm-sglang-client.
    source="huggingface",        # Model source: "huggingface", "modelscope", "local"
    # vlm_url="http://127.0.0.1:3000" # Service address when using backend=vlm-sglang-client

    # Standard RAGAnything parameters
    display_stats=True,          # Display content statistics
    split_by_character=None,     # Optional character to split text by
    doc_id=None                  # Optional document ID
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: MinerU 2.0 no longer uses the &lt;code&gt;magic-pdf.json&lt;/code&gt; configuration file. All settings are now passed as command-line parameters or function arguments. RAG-Anything now supports multiple document parsers - you can choose between MinerU and Docling based on your needs.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Processing Requirements&lt;/h3&gt; 
&lt;p&gt;Different content types require specific optional dependencies:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Office Documents&lt;/strong&gt; (.doc, .docx, .ppt, .pptx, .xls, .xlsx): Install &lt;a href="https://www.libreoffice.org/download/download/"&gt;LibreOffice&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Extended Image Formats&lt;/strong&gt; (.bmp, .tiff, .gif, .webp): Install with &lt;code&gt;pip install raganything[image]&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Text Files&lt;/strong&gt; (.txt, .md): Install with &lt;code&gt;pip install raganything[text]&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;üìã Quick Install&lt;/strong&gt;: Use &lt;code&gt;pip install raganything[all]&lt;/code&gt; to enable all format support (Python dependencies only - LibreOffice still needs separate installation)&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üß™ Supported Content Types&lt;/h2&gt; 
&lt;h3&gt;Document Formats&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;PDFs&lt;/strong&gt; - Research papers, reports, presentations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Office Documents&lt;/strong&gt; - DOC, DOCX, PPT, PPTX, XLS, XLSX&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Images&lt;/strong&gt; - JPG, PNG, BMP, TIFF, GIF, WebP&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Text Files&lt;/strong&gt; - TXT, MD&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Multimodal Elements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Images&lt;/strong&gt; - Photographs, diagrams, charts, screenshots&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Tables&lt;/strong&gt; - Data tables, comparison charts, statistical summaries&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Equations&lt;/strong&gt; - Mathematical formulas in LaTeX format&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Generic Content&lt;/strong&gt; - Custom content types via extensible processors&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;em&gt;For installation of format-specific dependencies, see the &lt;a href="https://raw.githubusercontent.com/HKUDS/RAG-Anything/main/#-configuration"&gt;Configuration&lt;/a&gt; section.&lt;/em&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìñ Citation&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;Academic Reference&lt;/em&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;div style="width: 60px; height: 60px; margin: 20px auto; position: relative;"&gt; 
  &lt;div style="width: 100%; height: 100%; border: 2px solid #00d9ff; border-radius: 50%; position: relative;"&gt; 
   &lt;div style="position: absolute; top: 50%; left: 50%; transform: translate(-50%, -50%); font-size: 24px; color: #00d9ff;"&gt;
    üìñ
   &lt;/div&gt; 
  &lt;/div&gt; 
  &lt;div style="position: absolute; bottom: -5px; left: 50%; transform: translateX(-50%); width: 20px; height: 20px; background: white; border-right: 2px solid #00d9ff; border-bottom: 2px solid #00d9ff; transform: rotate(45deg);"&gt;&lt;/div&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;p&gt;If you find RAG-Anything useful in your research, please cite our paper:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{guo2025raganythingallinoneragframework,
      title={RAG-Anything: All-in-One RAG Framework},
      author={Zirui Guo and Xubin Ren and Lingrui Xu and Jiahao Zhang and Chao Huang},
      year={2025},
      eprint={2510.12323},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2510.12323},
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üîó Related Projects&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;Ecosystem &amp;amp; Extensions&lt;/em&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td align="center"&gt; &lt;a href="https://github.com/HKUDS/LightRAG"&gt; 
      &lt;div style="width: 100px; height: 100px; background: linear-gradient(135deg, rgba(0, 217, 255, 0.1) 0%, rgba(0, 217, 255, 0.05) 100%); border-radius: 15px; border: 1px solid rgba(0, 217, 255, 0.2); display: flex; align-items: center; justify-content: center; margin-bottom: 10px;"&gt; 
       &lt;span style="font-size: 32px;"&gt;‚ö°&lt;/span&gt; 
      &lt;/div&gt; &lt;b&gt;LightRAG&lt;/b&gt;&lt;br /&gt; &lt;sub&gt;Simple and Fast RAG&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt; 
    &lt;td align="center"&gt; &lt;a href="https://github.com/HKUDS/VideoRAG"&gt; 
      &lt;div style="width: 100px; height: 100px; background: linear-gradient(135deg, rgba(0, 217, 255, 0.1) 0%, rgba(0, 217, 255, 0.05) 100%); border-radius: 15px; border: 1px solid rgba(0, 217, 255, 0.2); display: flex; align-items: center; justify-content: center; margin-bottom: 10px;"&gt; 
       &lt;span style="font-size: 32px;"&gt;üé•&lt;/span&gt; 
      &lt;/div&gt; &lt;b&gt;VideoRAG&lt;/b&gt;&lt;br /&gt; &lt;sub&gt;Extreme Long-Context Video RAG&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt; 
    &lt;td align="center"&gt; &lt;a href="https://github.com/HKUDS/MiniRAG"&gt; 
      &lt;div style="width: 100px; height: 100px; background: linear-gradient(135deg, rgba(0, 217, 255, 0.1) 0%, rgba(0, 217, 255, 0.05) 100%); border-radius: 15px; border: 1px solid rgba(0, 217, 255, 0.2); display: flex; align-items: center; justify-content: center; margin-bottom: 10px;"&gt; 
       &lt;span style="font-size: 32px;"&gt;‚ú®&lt;/span&gt; 
      &lt;/div&gt; &lt;b&gt;MiniRAG&lt;/b&gt;&lt;br /&gt; &lt;sub&gt;Extremely Simple RAG&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;‚≠ê Star History&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;Community Growth Trajectory&lt;/em&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://star-history.com/#HKUDS/RAG-Anything&amp;amp;Date"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=HKUDS/RAG-Anything&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
   &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=HKUDS/RAG-Anything&amp;amp;type=Date" /&gt; 
   &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=HKUDS/RAG-Anything&amp;amp;type=Date" style="border-radius: 15px; box-shadow: 0 0 30px rgba(0, 217, 255, 0.3);" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ü§ù Contribution&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;Join the Innovation&lt;/em&gt;&lt;/p&gt; 
&lt;div align="center"&gt;
  We thank all our contributors for their valuable contributions. 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/HKUDS/RAG-Anything/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=HKUDS/RAG-Anything" style="border-radius: 15px; box-shadow: 0 0 20px rgba(0, 217, 255, 0.3);" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;div align="center" style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; padding: 30px; margin: 30px 0;"&gt; 
 &lt;div&gt; 
  &lt;img src="https://user-images.githubusercontent.com/74038190/212284100-561aa473-3905-4a80-b561-0d28506553ee.gif" width="500" /&gt; 
 &lt;/div&gt; 
 &lt;div style="margin-top: 20px;"&gt; 
  &lt;a href="https://github.com/HKUDS/RAG-Anything" style="text-decoration: none;"&gt; &lt;img src="https://img.shields.io/badge/‚≠ê%20Star%20us%20on%20GitHub-1a1a2e?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white" /&gt; &lt;/a&gt; 
  &lt;a href="https://github.com/HKUDS/RAG-Anything/issues" style="text-decoration: none;"&gt; &lt;img src="https://img.shields.io/badge/üêõ%20Report%20Issues-ff6b6b?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white" /&gt; &lt;/a&gt; 
  &lt;a href="https://github.com/HKUDS/RAG-Anything/discussions" style="text-decoration: none;"&gt; &lt;img src="https://img.shields.io/badge/üí¨%20Discussions-4ecdc4?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white" /&gt; &lt;/a&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;div style="width: 100%; max-width: 600px; margin: 20px auto; padding: 20px; background: linear-gradient(135deg, rgba(0, 217, 255, 0.1) 0%, rgba(0, 217, 255, 0.05) 100%); border-radius: 15px; border: 1px solid rgba(0, 217, 255, 0.2);"&gt; 
  &lt;div style="display: flex; justify-content: center; align-items: center; gap: 15px;"&gt; 
   &lt;span style="font-size: 24px;"&gt;‚≠ê&lt;/span&gt; 
   &lt;span style="color: #00d9ff; font-size: 18px;"&gt;Thank you for visiting RAG-Anything!&lt;/span&gt; 
   &lt;span style="font-size: 24px;"&gt;‚≠ê&lt;/span&gt; 
  &lt;/div&gt; 
  &lt;div style="margin-top: 10px; color: #00d9ff; font-size: 16px;"&gt;
   Building the Future of Multimodal AI
  &lt;/div&gt; 
 &lt;/div&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>psf/black</title>
      <link>https://github.com/psf/black</link>
      <description>&lt;p&gt;The uncompromising Python code formatter&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="https://black.readthedocs.io/en/stable/"&gt;&lt;img src="https://raw.githubusercontent.com/psf/black/main/docs/_static/logo2-readme.png" alt="Black Logo" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2 align="center"&gt;The Uncompromising Code Formatter&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/psf/black/actions"&gt;&lt;img alt="Actions Status" src="https://github.com/psf/black/workflows/Test/badge.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://black.readthedocs.io/en/stable/?badge=stable"&gt;&lt;img alt="Documentation Status" src="https://readthedocs.org/projects/black/badge/?version=stable" /&gt;&lt;/a&gt; &lt;a href="https://coveralls.io/github/psf/black?branch=main"&gt;&lt;img alt="Coverage Status" src="https://coveralls.io/repos/github/psf/black/badge.svg?branch=main" /&gt;&lt;/a&gt; &lt;a href="https://github.com/psf/black/raw/main/LICENSE"&gt;&lt;img alt="License: MIT" src="https://black.readthedocs.io/en/stable/_static/license.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/black/"&gt;&lt;img alt="PyPI" src="https://img.shields.io/pypi/v/black" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/black"&gt;&lt;img alt="Supported Python Versions" src="https://img.shields.io/pypi/pyversions/black?color=brightgreen" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/black"&gt;&lt;img alt="Downloads" src="https://static.pepy.tech/badge/black" /&gt;&lt;/a&gt; &lt;a href="https://anaconda.org/conda-forge/black/"&gt;&lt;img alt="conda-forge" src="https://img.shields.io/conda/dn/conda-forge/black.svg?label=conda-forge" /&gt;&lt;/a&gt; &lt;a href="https://github.com/psf/black"&gt;&lt;img alt="Code style: black" src="https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;‚ÄúAny color you like.‚Äù&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; is the uncompromising Python code formatter. By using it, you agree to cede control over minutiae of hand-formatting. In return, &lt;em&gt;Black&lt;/em&gt; gives you speed, determinism, and freedom from &lt;code&gt;pycodestyle&lt;/code&gt; nagging about formatting. You will save time and mental energy for more important matters.&lt;/p&gt; 
&lt;p&gt;Blackened code looks the same regardless of the project you're reading. Formatting becomes transparent after a while and you can focus on the content instead.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; makes code review faster by producing the smallest diffs possible.&lt;/p&gt; 
&lt;p&gt;Try it out now using the &lt;a href="https://black.vercel.app"&gt;Black Playground&lt;/a&gt;. Watch the &lt;a href="https://youtu.be/esZLCuWs_2Y"&gt;PyCon 2019 talk&lt;/a&gt; to learn more.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://black.readthedocs.io/en/stable"&gt;Read the documentation on ReadTheDocs!&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Installation and usage&lt;/h2&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; can be installed by running &lt;code&gt;pip install black&lt;/code&gt;. It requires Python 3.10+ to run. If you want to format Jupyter Notebooks, install with &lt;code&gt;pip install "black[jupyter]"&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Usage&lt;/h3&gt; 
&lt;p&gt;To get started right away with sensible defaults:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;black {source_file_or_directory}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can run &lt;em&gt;Black&lt;/em&gt; as a package if running it as a script doesn't work:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python -m black {source_file_or_directory}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Further information can be found in our docs:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://black.readthedocs.io/en/stable/usage_and_configuration/index.html"&gt;Usage and Configuration&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; is already &lt;a href="https://github.com/psf/black#used-by"&gt;successfully used&lt;/a&gt; by many projects, small and big. &lt;em&gt;Black&lt;/em&gt; has a comprehensive test suite, with efficient parallel tests, and our own auto formatting and parallel Continuous Integration runner. Now that we have become stable, you should not expect large formatting changes in the future. Stylistic changes will mostly be responses to bug reports and support for new Python syntax. For more information please refer to &lt;a href="https://black.readthedocs.io/en/stable/the_black_code_style/index.html"&gt;The Black Code Style&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Also, as a safety measure which slows down processing, &lt;em&gt;Black&lt;/em&gt; will check that the reformatted code still produces a valid AST that is effectively equivalent to the original (see the &lt;a href="https://black.readthedocs.io/en/stable/the_black_code_style/current_style.html#ast-before-and-after-formatting"&gt;Pragmatism&lt;/a&gt; section for details). If you're feeling confident, use &lt;code&gt;--fast&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;The &lt;em&gt;Black&lt;/em&gt; code style&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; is a PEP 8 compliant opinionated formatter. &lt;em&gt;Black&lt;/em&gt; reformats entire files in place. Style configuration options are deliberately limited and rarely added. It doesn't take previous formatting into account (see &lt;a href="https://black.readthedocs.io/en/stable/the_black_code_style/current_style.html#pragmatism"&gt;Pragmatism&lt;/a&gt; for exceptions).&lt;/p&gt; 
&lt;p&gt;Our documentation covers the current &lt;em&gt;Black&lt;/em&gt; code style, but planned changes to it are also documented. They're both worth taking a look at:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://black.readthedocs.io/en/stable/the_black_code_style/current_style.html"&gt;The &lt;em&gt;Black&lt;/em&gt; Code Style: Current style&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://black.readthedocs.io/en/stable/the_black_code_style/future_style.html"&gt;The &lt;em&gt;Black&lt;/em&gt; Code Style: Future style&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Changes to the &lt;em&gt;Black&lt;/em&gt; code style are bound by the Stability Policy:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://black.readthedocs.io/en/stable/the_black_code_style/index.html#stability-policy"&gt;The &lt;em&gt;Black&lt;/em&gt; Code Style: Stability Policy&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Please refer to this document before submitting an issue. What seems like a bug might be intended behaviour.&lt;/p&gt; 
&lt;h3&gt;Pragmatism&lt;/h3&gt; 
&lt;p&gt;Early versions of &lt;em&gt;Black&lt;/em&gt; used to be absolutist in some respects. They took after its initial author. This was fine at the time as it made the implementation simpler and there were not many users anyway. Not many edge cases were reported. As a mature tool, &lt;em&gt;Black&lt;/em&gt; does make some exceptions to rules it otherwise holds.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://black.readthedocs.io/en/stable/the_black_code_style/current_style.html#pragmatism"&gt;The &lt;em&gt;Black&lt;/em&gt; code style: Pragmatism&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Please refer to this document before submitting an issue just like with the document above. What seems like a bug might be intended behaviour.&lt;/p&gt; 
&lt;h2&gt;Configuration&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; is able to read project-specific default values for its command line options from a &lt;code&gt;pyproject.toml&lt;/code&gt; file. This is especially useful for specifying custom &lt;code&gt;--include&lt;/code&gt; and &lt;code&gt;--exclude&lt;/code&gt;/&lt;code&gt;--force-exclude&lt;/code&gt;/&lt;code&gt;--extend-exclude&lt;/code&gt; patterns for your project.&lt;/p&gt; 
&lt;p&gt;You can find more details in our documentation:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://black.readthedocs.io/en/stable/usage_and_configuration/the_basics.html#configuration-via-a-file"&gt;The basics: Configuration via a file&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;And if you're looking for more general configuration documentation:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://black.readthedocs.io/en/stable/usage_and_configuration/index.html"&gt;Usage and Configuration&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Pro-tip&lt;/strong&gt;: If you're asking yourself "Do I need to configure anything?" the answer is "No". &lt;em&gt;Black&lt;/em&gt; is all about sensible defaults. Applying those defaults will have your code in compliance with many other &lt;em&gt;Black&lt;/em&gt; formatted projects.&lt;/p&gt; 
&lt;h2&gt;Used by&lt;/h2&gt; 
&lt;p&gt;The following notable open-source projects trust &lt;em&gt;Black&lt;/em&gt; with enforcing a consistent code style: pytest, tox, Pyramid, Django, Django Channels, Hypothesis, attrs, SQLAlchemy, Poetry, PyPA applications (Warehouse, Bandersnatch, Pipenv, virtualenv), pandas, Pillow, Twisted, LocalStack, every Datadog Agent Integration, Home Assistant, Zulip, Kedro, OpenOA, FLORIS, ORBIT, WOMBAT, and many more.&lt;/p&gt; 
&lt;p&gt;The following organizations use &lt;em&gt;Black&lt;/em&gt;: Dropbox, KeepTruckin, Lyft, Mozilla, Quora, Duolingo, QuantumBlack, Tesla, Archer Aviation.&lt;/p&gt; 
&lt;p&gt;Are we missing anyone? Let us know.&lt;/p&gt; 
&lt;h2&gt;Testimonials&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Mike Bayer&lt;/strong&gt;, &lt;a href="https://www.sqlalchemy.org/"&gt;author of &lt;code&gt;SQLAlchemy&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;I can't think of any single tool in my entire programming career that has given me a bigger productivity increase by its introduction. I can now do refactorings in about 1% of the keystrokes that it would have taken me previously when we had no way for code to format itself.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Dusty Phillips&lt;/strong&gt;, &lt;a href="https://smile.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Daps&amp;amp;field-keywords=dusty+phillips"&gt;writer&lt;/a&gt;:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;Black&lt;/em&gt; is opinionated so you don't have to be.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Hynek Schlawack&lt;/strong&gt;, &lt;a href="https://www.attrs.org/"&gt;creator of &lt;code&gt;attrs&lt;/code&gt;&lt;/a&gt;, core developer of Twisted and CPython:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;An auto-formatter that doesn't suck is all I want for Xmas!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Carl Meyer&lt;/strong&gt;, &lt;a href="https://www.djangoproject.com/"&gt;Django&lt;/a&gt; core developer:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;At least the name is good.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Kenneth Reitz&lt;/strong&gt;, creator of &lt;a href="https://requests.readthedocs.io/en/latest/"&gt;&lt;code&gt;requests&lt;/code&gt;&lt;/a&gt; and &lt;a href="https://readthedocs.org/projects/pipenv/"&gt;&lt;code&gt;pipenv&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;This vastly improves the formatting of our code. Thanks a ton!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Show your style&lt;/h2&gt; 
&lt;p&gt;Use the badge in your project's README.md:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-md"&gt;[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Using the badge in README.rst:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-rst"&gt;.. image:: https://img.shields.io/badge/code%20style-black-000000.svg
    :target: https://github.com/psf/black
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Looks like this: &lt;a href="https://github.com/psf/black"&gt;&lt;img src="https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true" alt="Code style: black" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;MIT&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Welcome! Happy to see you willing to make the project better. You can get started by reading this:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://black.readthedocs.io/en/latest/contributing/the_basics.html"&gt;Contributing: The basics&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can also take a look at the rest of the contributing docs or talk with the developers:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://black.readthedocs.io/en/latest/contributing/index.html"&gt;Contributing documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://discord.gg/RtVdv86PrH"&gt;Chat on Discord&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Change log&lt;/h2&gt; 
&lt;p&gt;The log has become rather long. It moved to its own file.&lt;/p&gt; 
&lt;p&gt;See &lt;a href="https://black.readthedocs.io/en/latest/change_log.html"&gt;CHANGES&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Authors&lt;/h2&gt; 
&lt;p&gt;The author list is quite long nowadays, so it lives in its own file.&lt;/p&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/psf/black/main/AUTHORS.md"&gt;AUTHORS.md&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Code of Conduct&lt;/h2&gt; 
&lt;p&gt;Everyone participating in the &lt;em&gt;Black&lt;/em&gt; project, and in particular in the issue tracker, pull requests, and social media activity, is expected to treat other people with respect and more generally to follow the guidelines articulated in the &lt;a href="https://www.python.org/psf/codeofconduct/"&gt;Python Community Code of Conduct&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;At the same time, humor is encouraged. In fact, basic familiarity with Monty Python's Flying Circus is expected. We are not savages.&lt;/p&gt; 
&lt;p&gt;And if you &lt;em&gt;really&lt;/em&gt; need to slap somebody, do it with a fish while dancing.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>MiroMindAI/MiroThinker</title>
      <link>https://github.com/MiroMindAI/MiroThinker</link>
      <description>&lt;p&gt;MiroThinker is an open-source search agent model, built for tool-augmented reasoning and real-world information seeking, aiming to match the deep research experience of OpenAI Deep Research and Gemini Deep Research.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/assets/miro_thinker.png" width="55%" alt="MiroThinker" /&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://dr.miromind.ai/"&gt;&lt;img src="https://img.shields.io/badge/Demo-FFB300?style=for-the-badge&amp;amp;logo=airplayvideo&amp;amp;logoColor=white" alt="DEMO" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/collections/miromind-ai/mirothinker-v15"&gt;&lt;img src="https://img.shields.io/badge/Models-5EDDD2?style=for-the-badge&amp;amp;logo=huggingface&amp;amp;logoColor=ffffff&amp;amp;labelColor" alt="MODELS" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2511.11793"&gt;&lt;img src="https://img.shields.io/badge/Paper-B31B1B?style=for-the-badge&amp;amp;logo=arxiv&amp;amp;logoColor=white" alt="Paper" /&gt;&lt;/a&gt; &lt;a href="https://miromind.ai/#blog"&gt;&lt;img src="https://img.shields.io/badge/Blog-4285F4?style=for-the-badge&amp;amp;logo=google-chrome&amp;amp;logoColor=white" alt="Blog" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/datasets/miromind-ai/MiroVerse-v0.1"&gt;&lt;img src="https://img.shields.io/badge/Data-0040A1?style=for-the-badge&amp;amp;logo=huggingface&amp;amp;logoColor=ffffff&amp;amp;labelColor" alt="DATA" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/MiroMindAI"&gt;&lt;img src="https://img.shields.io/badge/Github-24292F?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white" alt="GITHUB" /&gt;&lt;/a&gt; &lt;a href="https://miromind.ai/"&gt;&lt;img src="https://img.shields.io/badge/Website-4285F4?style=for-the-badge&amp;amp;logo=google-chrome&amp;amp;logoColor=white" alt="WEBSITE" /&gt;&lt;/a&gt; &lt;a href="https://discord.com/invite/GPqEnkzQZd"&gt;&lt;img src="https://img.shields.io/badge/Discord-5865F2?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white" alt="DISCORD" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/refs/heads/main/assets/miromind_wechat.png"&gt;&lt;img src="https://img.shields.io/badge/WeChat-07C160?style=for-the-badge&amp;amp;logo=wechat&amp;amp;logoColor=white" alt="WeChat" /&gt;&lt;/a&gt; &lt;a href="https://www.xiaohongshu.com/user/profile/5e353bd80000000001000239"&gt;&lt;img src="https://img.shields.io/badge/RedNote-FF2442?style=for-the-badge&amp;amp;logo=revoltdotchat&amp;amp;logoColor=white" alt="RedNote" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;h3&gt;üöÄ &lt;a href="https://dr.miromind.ai/"&gt;Try our Demo!&lt;/a&gt;&lt;/h3&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;MiroThinker&lt;/strong&gt; is MiroMind's Flagship Research Agent Model. It is an open-source search model designed to advance tool-augmented reasoning and information-seeking capabilities, enabling complex real-world research workflows across diverse challenges.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;The project currently comprises four key components:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üí° &lt;strong&gt;MiroThinker&lt;/strong&gt;: An open-source search &lt;strong&gt;model&lt;/strong&gt; that natively supports tool-assisted reasoning, achieving leading performance across multiple benchmarks (e.g., HLE, HLE-Text-2158, HLE-Text-500, BrowseComp, BrowseComp-ZH, GAIA, XBench-DeepSearch, FutureX, and Frames). See &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/#-quick-start"&gt;Quick Start&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;ü§ñ &lt;strong&gt;MiroFlow&lt;/strong&gt;: An open-source research agent framework that offers reproducible state-of-the-art performance across multiple benchmarks. See &lt;a href="https://github.com/MiroMindAI/MiroFlow"&gt;MiroFlow&lt;/a&gt; for details.&lt;/li&gt; 
 &lt;li&gt;üìö &lt;strong&gt;MiroVerse&lt;/strong&gt;: A premium open-source training dataset with 147k samples supporting research agent training. See &lt;a href="https://huggingface.co/datasets/miromind-ai/MiroVerse-v0.1"&gt;MiroVerse&lt;/a&gt; on HuggingFace.&lt;/li&gt; 
 &lt;li&gt;üîß &lt;strong&gt;MiroTrain / MiroRL&lt;/strong&gt;: Training infrastructure that supports stable and efficient training for research agent models. See &lt;a href="https://github.com/MiroMindAI/MiroTrain"&gt;MiroTrain&lt;/a&gt; and &lt;a href="https://github.com/MiroMindAI/MiroRL"&gt;MiroRL&lt;/a&gt; for details.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìã Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;üì∞ &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/#-news--updates"&gt;News &amp;amp; Updates&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/#-introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;‚ú® &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/#-key-features"&gt;Key Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üìà &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/#-performance-on-benchmarks"&gt;Performance on Benchmarks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üöÄ &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/#-quick-start"&gt;Quick Start&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üìä &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/#-benchmark-evaluation"&gt;Benchmark Evaluation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üî¨ &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/#-trace-collection"&gt;Trace Collection&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;‚ùì &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/#-faq--troubleshooting"&gt;FAQ &amp;amp; Troubleshooting&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üìÑ &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/#-license"&gt;License&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üôè &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/#-acknowledgments"&gt;Acknowledgments&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üì∞ News &amp;amp; Updates&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;[2026-01-05]&lt;/strong&gt; üéâüéâ We release &lt;a href="https://huggingface.co/collections/miromind-ai/mirothinker-v15"&gt;MiroThinker-v1.5&lt;/a&gt;, a world-leading open-source search agent. &lt;a href="https://huggingface.co/miromind-ai/MiroThinker-v1.5-30B"&gt;MiroThinker-v1.5-30B&lt;/a&gt; surpasses Kimi-K2-Thinking on BrowseComp-ZH at much lower cost, using only 1/30 of the parameters. &lt;a href="https://huggingface.co/miromind-ai/MiroThinker-v1.5-235B"&gt;MiroThinker-v1.5-235B&lt;/a&gt; scores 39.2% on HLE-Text, 69.8% on BrowseComp, 71.5% on BrowseComp-ZH, and 80.8% on GAIA-Val-165, setting a new state-of-the-art among search agents.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025-11-13]&lt;/strong&gt; üéâ &lt;a href="https://huggingface.co/collections/miromind-ai/mirothinker-v10"&gt;MiroThinker-v1.0&lt;/a&gt; is now released! Introducing &lt;strong&gt;interactive scaling&lt;/strong&gt; as a third dimension of performance improvement, MiroThinker v1.0 supports 256K context window and up to 600 tool calls per task. Available in 8B, 30B, and 72B parameter scales, achieving 37.7%, 47.1%, 55.6%, and 81.9% on HLE-Text, BrowseComp, BrowseComp-ZH, and GAIA-Text-103, respectively. See &lt;a href="https://arxiv.org/abs/2511.11793"&gt;Technical Report&lt;/a&gt; for more details.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025-09-11]&lt;/strong&gt; MiroThinker-72B-Preview ranked 4th in this week's FutureX benchmark. See &lt;a href="https://futurex-ai.github.io/"&gt;FutureX&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;üìú Click to expand older updates&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;[2025-09-08]&lt;/strong&gt; &lt;a href="https://huggingface.co/collections/miromind-ai/mirothinker-v02"&gt;MiroThinker-v0.2&lt;/a&gt; is now released, achieving open-source SOTA performance across multiple benchmarks, including HLE (17.8%), HLE-Text-Only (19.1%), BrowseComp-EN (17.2%), BrowseComp-ZH (29.4%), XBench-DeepSearch (56.0%), and Frames (74.8%).&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;[2025-09-07]&lt;/strong&gt; We supported more benchmarks, including &lt;a href="https://arxiv.org/abs/2504.19314"&gt;BrowseComp-ZH&lt;/a&gt;, &lt;a href="https://xbench.org/agi/aisearch"&gt;XBench-DeepSearch&lt;/a&gt;, and &lt;a href="https://futurex-ai.github.io/"&gt;FutureX&lt;/a&gt;. We plan to add more benchmarks in the future.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;[2025-08-22]&lt;/strong&gt; Introducing streamlined deployment options for MiroThinker models with optimized resource usage and faster startup times. Experience the interactive demo: &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/apps/gradio-demo"&gt;üöÄ Try Gradio Demo&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;[2025-08-08]&lt;/strong&gt; &lt;a href="https://huggingface.co/collections/miromind-ai/mirothinker-v01-689301b6d0563321862d44a1"&gt;MiroThinker-v0.1&lt;/a&gt; released. Models, framework, and data are now fully open-sourced!&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;üìù Introduction&lt;/h2&gt; 
&lt;h3&gt;MiroThinker-v1.5&lt;/h3&gt; 
&lt;p&gt;MiroThinker v1.5 is the world-leading open-source search agent that advances tool-augmented reasoning through &lt;strong&gt;interactive scaling&lt;/strong&gt; ‚Äî training the model to handle deeper and more frequent agent-environment interactions as a third dimension of performance improvement, beyond model size and context length.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/assets/mirothinker_v1.5_framework.png" alt="image" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Key Features&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üöÄ MiroThinker v1.5 supports a 256K context window, long-horizon reasoning, and deep multi-step analysis.&lt;/li&gt; 
 &lt;li&gt;üîß Handles up to 400 tool calls per task ‚Äî a substantial improvement over previous open-source research agents.&lt;/li&gt; 
 &lt;li&gt;üì¶ Released in 30B and 235B parameter scales, accompanied by a comprehensive suite of tools and workflows to flexibly support diverse research settings and compute budgets.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;Model Name&lt;/th&gt; 
    &lt;th align="center"&gt;Base Model&lt;/th&gt; 
    &lt;th align="center"&gt;Max Context&lt;/th&gt; 
    &lt;th align="center"&gt;Max Tool Calls&lt;/th&gt; 
    &lt;th align="center"&gt;HF Link&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;MiroThinker-v1.5-30B&lt;/td&gt; 
    &lt;td align="center"&gt;Qwen3-30B-A3B-Thinking-2507&lt;/td&gt; 
    &lt;td align="center"&gt;256K&lt;/td&gt; 
    &lt;td align="center"&gt;400&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-v1.5-30B"&gt;ü§ó link&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;MiroThinker-v1.5-235B&lt;/td&gt; 
    &lt;td align="center"&gt;Qwen3-235B-A22B-Thinking-2507&lt;/td&gt; 
    &lt;td align="center"&gt;256K&lt;/td&gt; 
    &lt;td align="center"&gt;400&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-v1.5-235B"&gt;ü§ó link&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;p&gt;MiroThinker v1.5 demonstrates strong general-research performance across a broad range of benchmarks, achieving&amp;nbsp;39.2%,&amp;nbsp;69.8%, 71.5%, and&amp;nbsp;80.8%&amp;nbsp;on HLE-Text, BrowseComp, BrowseComp-ZH, and GAIA-Val-165, respectively. These results surpass previous open-source agents and set the new world-leading BrowseComp performance.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/assets/mirothinker_v1.5_browsecomp.png" alt="image" /&gt;&lt;/p&gt; 
&lt;h3&gt;MiroThinker-v1.0&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;üì¶ Click to expand MiroThinker-v1.0 details&lt;/summary&gt; 
 &lt;p&gt;Unlike previous agents that scale only model size or context length, MiroThinker v1.0 introduces &lt;strong&gt;interactive scaling&lt;/strong&gt; at the model level, systematically training the model to handle deeper and more frequent agent‚Äìenvironment interactions as a third dimension of performance improvement. Interactive scaling leverages environment feedback and external information acquisition to correct errors and refine trajectories.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/assets/MiroThinker_v1.0_Overall.png" alt="image" /&gt;&lt;/p&gt; 
 &lt;h3&gt;‚ú® Key Features&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;üöÄ &lt;strong&gt;256K Context Window&lt;/strong&gt;: Supports long-horizon reasoning and deep multi-step analysis&lt;/li&gt; 
  &lt;li&gt;üîß &lt;strong&gt;600 Tool Calls&lt;/strong&gt;: Handles up to 600 tool calls per task ‚Äî a substantial improvement over previous open-source research agents&lt;/li&gt; 
  &lt;li&gt;üì¶ &lt;strong&gt;Multiple Scales&lt;/strong&gt;: Released in 8B, 30B, and 72B parameter scales, accompanied by a comprehensive suite of tools and workflows to flexibly support diverse research settings and compute budgets&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;div align="center"&gt; 
  &lt;table&gt; 
   &lt;thead&gt; 
    &lt;tr&gt; 
     &lt;th align="center"&gt;Model Name&lt;/th&gt; 
     &lt;th align="center"&gt;Base Model&lt;/th&gt; 
     &lt;th align="center"&gt;Max Context&lt;/th&gt; 
     &lt;th align="center"&gt;Max Tool Calls&lt;/th&gt; 
     &lt;th align="center"&gt;HF Link&lt;/th&gt; 
    &lt;/tr&gt; 
   &lt;/thead&gt; 
   &lt;tbody&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-v1.0-8B&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-8B&lt;/td&gt; 
     &lt;td align="center"&gt;256K&lt;/td&gt; 
     &lt;td align="center"&gt;600&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-v1.0-8B"&gt;ü§ó link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-v1.0-30B&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-30B-A3B-Thinking-2507&lt;/td&gt; 
     &lt;td align="center"&gt;256K&lt;/td&gt; 
     &lt;td align="center"&gt;600&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-v1.0-30B"&gt;ü§ó link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-v1.0-72B&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen2.5-72B-Instruct&lt;/td&gt; 
     &lt;td align="center"&gt;256K&lt;/td&gt; 
     &lt;td align="center"&gt;600&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-v1.0-72B"&gt;ü§ó link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt; 
  &lt;/table&gt; 
 &lt;/div&gt; 
 &lt;p&gt;MiroThinker v1.0 demonstrates strong general-research performance across a broad range of benchmarks, achieving &lt;strong&gt;37.7%&lt;/strong&gt;, &lt;strong&gt;47.1%&lt;/strong&gt;, &lt;strong&gt;55.6%&lt;/strong&gt;, and &lt;strong&gt;81.9%&lt;/strong&gt; on HLE-Text, BrowseComp, BrowseComp-ZH, and GAIA-Text-103, respectively. These results surpass previous open-source agents and narrow the gap with commercial counterparts such as &lt;strong&gt;GPT-5-high&lt;/strong&gt;.&lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;img src="https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/assets/MiroThinker_v1.0_Performance_1.png" width="100%" alt="MiroThinker" /&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;h3&gt;MiroThinker-v0.2&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;üì¶ Click to expand MiroThinker-v0.2 details&lt;/summary&gt; 
 &lt;p&gt;In this new version, we introduced three key improvements:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;üìö &lt;strong&gt;Richer training data&lt;/strong&gt; from both English and Chinese sources, yielding significant gains in benchmark performance and generalization&lt;/li&gt; 
  &lt;li&gt;üéØ &lt;strong&gt;Unified DPO training&lt;/strong&gt; with a single preference dataset across all models&lt;/li&gt; 
  &lt;li&gt;üìè &lt;strong&gt;Extended context length&lt;/strong&gt; from 40k to 64k for more challenging multi-turn tool-use tasks&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;Compared to v0.1, MiroThinker v0.2 delivers consistent gains across benchmarks. For example, scores improved from &lt;strong&gt;57.3 ‚Üí 64.1&lt;/strong&gt; on &lt;strong&gt;GAIA-Text-103&lt;/strong&gt; and from &lt;strong&gt;17.0 ‚Üí 29.4&lt;/strong&gt; on &lt;strong&gt;BrowseComp-ZH&lt;/strong&gt;, reflecting substantial advancements in the model‚Äôs general research agent capabilities.&lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;table&gt; 
   &lt;thead&gt; 
    &lt;tr&gt; 
     &lt;th align="center"&gt;Model Name&lt;/th&gt; 
     &lt;th align="center"&gt;Base Model&lt;/th&gt; 
     &lt;th align="center"&gt;Max Context&lt;/th&gt; 
     &lt;th align="center"&gt;HF Link&lt;/th&gt; 
    &lt;/tr&gt; 
   &lt;/thead&gt; 
   &lt;tbody&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-4B-SFT-v0.2&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-4B&lt;/td&gt; 
     &lt;td align="center"&gt;64K&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-4B-SFT-v0.2"&gt;ü§ó link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-4B-DPO-v0.2&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-4B&lt;/td&gt; 
     &lt;td align="center"&gt;64K&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-4B-DPO-v0.2"&gt;ü§ó link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-8B-SFT-v0.2&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-8B&lt;/td&gt; 
     &lt;td align="center"&gt;64K&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-8B-SFT-v0.2"&gt;ü§ó link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-8B-DPO-v0.2&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-8B&lt;/td&gt; 
     &lt;td align="center"&gt;64K&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-8B-DPO-v0.2"&gt;ü§ó link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-14B-SFT-v0.2&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-14B&lt;/td&gt; 
     &lt;td align="center"&gt;64K&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-14B-SFT-v0.2"&gt;ü§ó link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-14B-DPO-v0.2&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-14B&lt;/td&gt; 
     &lt;td align="center"&gt;64K&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-14B-DPO-v0.2"&gt;ü§ó link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-32B-SFT-v0.2&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-32B&lt;/td&gt; 
     &lt;td align="center"&gt;64K&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-32B-SFT-v0.2"&gt;ü§ó link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-32B-DPO-v0.2&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-32B&lt;/td&gt; 
     &lt;td align="center"&gt;64K&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-32B-DPO-v0.2"&gt;ü§ó link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt; 
  &lt;/table&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;h3&gt;MiroThinker-v0.1&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;üì¶ Click to expand MiroThinker-v0.1 details&lt;/summary&gt; 
 &lt;div align="center"&gt; 
  &lt;img src="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/assets/gaia_text_103.png" width="98%" alt="MiroFlow Performance on GAIA-Validation" /&gt; 
  &lt;p&gt;&lt;strong&gt;Performance of Open-Source Models on GAIA-Validation Benchmark.&lt;/strong&gt;&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;p&gt;We have released the &lt;strong&gt;MiroThinker v0.1&lt;/strong&gt; series, including both SFT and DPO variants at parameter scales of &lt;strong&gt;8B&lt;/strong&gt;, &lt;strong&gt;14B&lt;/strong&gt;, and &lt;strong&gt;32B&lt;/strong&gt;. Notably, MiroThinker v0.1 achieves &lt;strong&gt;state-of-the-art performance&lt;/strong&gt; among open-source models on the &lt;a href="https://huggingface.co/datasets/gaia-benchmark/GAIA"&gt;GAIA benchmark&lt;/a&gt;, a rigorous evaluation suite for advanced agentic capabilities, demonstrating its strength in long-context, decision-intensive, and real-world task scenarios.&lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;table&gt; 
   &lt;thead&gt; 
    &lt;tr&gt; 
     &lt;th align="center"&gt;Model Name&lt;/th&gt; 
     &lt;th align="center"&gt;Base Model&lt;/th&gt; 
     &lt;th align="center"&gt;Max Context&lt;/th&gt; 
     &lt;th align="center"&gt;HF Link&lt;/th&gt; 
    &lt;/tr&gt; 
   &lt;/thead&gt; 
   &lt;tbody&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-8B-SFT-v0.1&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-8B&lt;/td&gt; 
     &lt;td align="center"&gt;40K&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-8B-SFT-v0.1"&gt;ü§ó link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-8B-DPO-v0.1&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-8B&lt;/td&gt; 
     &lt;td align="center"&gt;40K&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-8B-DPO-v0.1"&gt;ü§ó link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-14B-SFT-v0.1&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-14B&lt;/td&gt; 
     &lt;td align="center"&gt;40K&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-14B-SFT-v0.1"&gt;ü§ó link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-14B-DPO-v0.1&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-14B&lt;/td&gt; 
     &lt;td align="center"&gt;40K&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-14B-DPO-v0.1"&gt;ü§ó link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-32B-SFT-v0.1&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-32B&lt;/td&gt; 
     &lt;td align="center"&gt;40K&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-32B-SFT-v0.1"&gt;ü§ó link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-32B-DPO-v0.1&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-32B&lt;/td&gt; 
     &lt;td align="center"&gt;40K&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-32B-DPO-v0.1"&gt;ü§ó link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt; 
  &lt;/table&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;h2&gt;‚ú® Key Features&lt;/h2&gt; 
&lt;h3&gt;ü§ñ &lt;strong&gt;MiroThinker-Optimized Framework&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üîì &lt;strong&gt;Fully Open-Source Agent Framework&lt;/strong&gt;: Complete transparency with open framework and open models&lt;/li&gt; 
 &lt;li&gt;üîó &lt;strong&gt;Tool Integration&lt;/strong&gt;: Seamless integration with external tools and APIs&lt;/li&gt; 
 &lt;li&gt;üìù &lt;strong&gt;Trace Collection&lt;/strong&gt;: Comprehensive logging and analysis of agent interactions with elapsed time and estimated completion time displayed in minutes. Ready for SFT and DPO&lt;/li&gt; 
 &lt;li&gt;üìä &lt;strong&gt;Benchmark Evaluation&lt;/strong&gt;: Extensive testing across multiple benchmark datasets&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üìä &lt;strong&gt;Comprehensive Benchmark Suite&lt;/strong&gt;&lt;/h3&gt; 
&lt;details open&gt; 
 &lt;summary&gt;üìã Click to expand benchmark list&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;GAIA Validation&lt;/strong&gt;: A benchmark for General AI Assistants. (&lt;a href="https://arxiv.org/abs/2311.12983"&gt;paper&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;GAIA-Text-103&lt;/strong&gt;: A subset of GAIA Validation for text-only tasks. (&lt;a href="https://arxiv.org/abs/2505.22648"&gt;paper&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;HLE&lt;/strong&gt;: Humanity's Last Exam. (&lt;a href="https://arxiv.org/abs/2501.14249"&gt;paper&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;HLE-Text-2158&lt;/strong&gt;: A subset of HLE for text-only tasks. (&lt;a href="https://arxiv.org/abs/2501.14249"&gt;paper&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;HLE-Text-500&lt;/strong&gt;: A subset of HLE for text-only tasks, created by &lt;a href="https://arxiv.org/pdf/2504.21776"&gt;WebThinker&lt;/a&gt;. (&lt;a href="https://arxiv.org/pdf/2504.21776"&gt;paper&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;BrowseComp-EN&lt;/strong&gt;: Web browsing and comprehension tasks. (&lt;a href="https://arxiv.org/abs/2504.12516"&gt;paper&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;BrowseComp-ZH&lt;/strong&gt;: A Chinese version of BrowseComp. (&lt;a href="https://arxiv.org/abs/2504.19314"&gt;paper&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;WebWalkerQA&lt;/strong&gt;: Web navigation and question answering. (&lt;a href="https://arxiv.org/abs/2501.07572"&gt;paper&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Frames&lt;/strong&gt;: Factuality, Retrieval, And reasoning MEasurement Set. (&lt;a href="https://arxiv.org/abs/2409.12941"&gt;paper&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;XBench-DeepSearch&lt;/strong&gt;: A benchmark for deep research agents. (&lt;a href="https://xbench.org/agi/aisearch"&gt;website&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;FutureX&lt;/strong&gt;: A live benchmark designed for predicting unknown future. (&lt;a href="https://futurex-ai.github.io/"&gt;website&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;SEAL-0&lt;/strong&gt;: A benchmark for evaluating LLMs on conflicting-evidence web questions. (&lt;a href="https://arxiv.org/abs/2506.01062"&gt;paper&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;AIME2025&lt;/strong&gt;: American Invitational Mathematics Examination 2025. (&lt;a href="https://artificialanalysis.ai/evaluations/aime-2025"&gt;website&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;DeepSearchQA&lt;/strong&gt;: Google's Deep Search Question Answering benchmark. (&lt;a href="https://arxiv.org/abs/2505.20827"&gt;paper&lt;/a&gt;)&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;üìà Performance on Benchmarks&lt;/h2&gt; 
&lt;h3&gt;MiroThinker-v1.5&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;To prevent potential information leakage (e.g., searching benchmark answers from HuggingFace), access to HuggingFace has been explicitly disabled in these tools.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;We further perform canary string testing on the tool outputs of all trajectories and disregard any trajectory found to be contaminated, treating it as an incorrect answer.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;div&gt; 
 &lt;img src="https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/assets/mirothinker_v1.5_performance.png" width="100%" alt="MiroThinker" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;MiroThinker-v1.0&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;üì¶ Click to expand MiroThinker-v1.0 details&lt;/summary&gt; 
 &lt;div align="center"&gt; 
  &lt;img src="https://github.com/user-attachments/assets/108a2105-4e1d-499e-a001-4713a03fd8ac" width="100%" alt="MiroThinker" /&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;h3&gt;MiroThinker-v0.2&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;üì¶ Click to expand MiroThinker-v0.2 details&lt;/summary&gt; 
 &lt;h4&gt;Comparison with SOTA Research Agents&lt;/h4&gt; 
 &lt;div align="center"&gt; 
  &lt;img src="https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/assets/MiroThinker_v0.2_Performance_2.png" width="90%" alt="MiroThinker" /&gt; 
 &lt;/div&gt; 
 &lt;h4&gt;GAIA Benchmark&lt;/h4&gt; 
 &lt;div align="center"&gt; 
  &lt;img src="https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/assets/MiroThinker_v0.2_Performance_1.png" width="80%" alt="MiroThinker" /&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;h3&gt;MiroThinker-v0.1&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;üì¶ Click to expand MiroThinker-v0.1 details&lt;/summary&gt; 
 &lt;h4&gt;GAIA Benchmark&lt;/h4&gt; 
 &lt;div align="center"&gt; 
  &lt;table&gt; 
   &lt;thead&gt; 
    &lt;tr&gt; 
     &lt;th&gt;&lt;strong&gt;Method&lt;/strong&gt;&lt;/th&gt; 
     &lt;th align="center"&gt;Text-103&lt;br /&gt;Best Pass@1&lt;/th&gt; 
     &lt;th align="center"&gt;Text-103&lt;br /&gt;Pass@1 (Avg@8)&lt;/th&gt; 
     &lt;th align="center"&gt;Val-165&lt;br /&gt;Best Pass@1&lt;/th&gt; 
     &lt;th align="center"&gt;Val-165&lt;br /&gt;Pass@1 (Avg@8)&lt;/th&gt; 
    &lt;/tr&gt; 
   &lt;/thead&gt; 
   &lt;tbody&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;üîπ‚Äî‚Äî 7B/8B Models ‚Äî‚Äî&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;Search-o1-7B&lt;/td&gt; 
     &lt;td align="center"&gt;17.5&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;R1-Searcher-7B&lt;/td&gt; 
     &lt;td align="center"&gt;20.4&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;WebDancer-7B&lt;/td&gt; 
     &lt;td align="center"&gt;31.0&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;WebSailor-7B&lt;/td&gt; 
     &lt;td align="center"&gt;37.9&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;CK-Pro-8B&lt;/td&gt; 
     &lt;td align="center"&gt;40.3&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;32.7&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;MiroThinker-8B-SFT-v0.1&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;44.7&lt;/td&gt; 
     &lt;td align="center"&gt;40.1&lt;/td&gt; 
     &lt;td align="center"&gt;34.6&lt;/td&gt; 
     &lt;td align="center"&gt;31.8&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;+ Commercial Tools&lt;/td&gt; 
     &lt;td align="center"&gt;46.6&lt;/td&gt; 
     &lt;td align="center"&gt;42.1&lt;/td&gt; 
     &lt;td align="center"&gt;37.6&lt;/td&gt; 
     &lt;td align="center"&gt;33.9&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;MiroThinker-8B-DPO-v0.1&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;46.6&lt;/td&gt; 
     &lt;td align="center"&gt;44.8&lt;/td&gt; 
     &lt;td align="center"&gt;37.0&lt;/td&gt; 
     &lt;td align="center"&gt;35.4&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;+ Commercial Tools&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;strong&gt;50.5&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;strong&gt;46.7&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;strong&gt;38.2&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;strong&gt;35.9&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;üîπ‚Äî‚Äî 14B Models ‚Äî‚Äî&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;MiroThinker-14B-SFT-v0.1&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;47.6&lt;/td&gt; 
     &lt;td align="center"&gt;44.4&lt;/td&gt; 
     &lt;td align="center"&gt;37.0&lt;/td&gt; 
     &lt;td align="center"&gt;34.4&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;+ Commercial Tools&lt;/td&gt; 
     &lt;td align="center"&gt;49.5&lt;/td&gt; 
     &lt;td align="center"&gt;47.5&lt;/td&gt; 
     &lt;td align="center"&gt;41.8&lt;/td&gt; 
     &lt;td align="center"&gt;39.8&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;MiroThinker-14B-DPO-v0.1&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;48.5&lt;/td&gt; 
     &lt;td align="center"&gt;46.6&lt;/td&gt; 
     &lt;td align="center"&gt;42.4&lt;/td&gt; 
     &lt;td align="center"&gt;39.2&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;+ Commercial Tools&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;strong&gt;52.4&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;strong&gt;48.5&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;strong&gt;45.5&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;strong&gt;42.0&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;üîπ‚Äî‚Äî 32B Models ‚Äî‚Äî&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;Qwen3-32B&lt;/td&gt; 
     &lt;td align="center"&gt;31.1&lt;/td&gt; 
     &lt;td align="center"&gt;26.7&lt;/td&gt; 
     &lt;td align="center"&gt;29.7&lt;/td&gt; 
     &lt;td align="center"&gt;26.4&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;Search-o1-32B&lt;/td&gt; 
     &lt;td align="center"&gt;28.2&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;WebThinker-32B-RL&lt;/td&gt; 
     &lt;td align="center"&gt;48.5&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;WebDancer-QwQ-32B&lt;/td&gt; 
     &lt;td align="center"&gt;51.5&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;WebSailor-32B&lt;/td&gt; 
     &lt;td align="center"&gt;53.2&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;WebShaper-QwQ-32B&lt;/td&gt; 
     &lt;td align="center"&gt;53.3&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;MiroThinker-32B-SFT-v0.1&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;55.3&lt;/td&gt; 
     &lt;td align="center"&gt;51.3&lt;/td&gt; 
     &lt;td align="center"&gt;44.9&lt;/td&gt; 
     &lt;td align="center"&gt;42.7&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;+ Commercial Tools&lt;/td&gt; 
     &lt;td align="center"&gt;58.3&lt;/td&gt; 
     &lt;td align="center"&gt;54.2&lt;/td&gt; 
     &lt;td align="center"&gt;48.5&lt;/td&gt; 
     &lt;td align="center"&gt;45.8&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;MiroThinker-32B-DPO-v0.1&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;57.3&lt;/td&gt; 
     &lt;td align="center"&gt;54.1&lt;/td&gt; 
     &lt;td align="center"&gt;48.5&lt;/td&gt; 
     &lt;td align="center"&gt;45.9&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;+ Commercial Tools&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;strong&gt;60.2&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;strong&gt;57.9&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;strong&gt;50.9&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;strong&gt;48.9&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt; 
  &lt;/table&gt; 
 &lt;/div&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;Following the practices of WebThinker, WebAgents, and CognitiveKernel, we report the Best Pass@1, the highest score across three runs, which often reflects stronger performance, though it may exhibit some variability. To provide a more stable measure, we additionally report Pass@1 (Avg@8), which offers greater consistency at the cost of slightly lower scores.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;For consistency with prior open-source works, we evaluate GAIA-Text-103 using the WebAgents LLM-as-a-Judge template, and report results on GAIA-Val-165 using the official GAIA scorer script.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;By default, we use open-source tools wherever possible, except for the code tool &lt;a href="https://github.com/e2b-dev/E2B"&gt;E2B&lt;/a&gt; and the Google search tool &lt;a href="https://serper.dev/"&gt;Serper&lt;/a&gt;. We use &lt;a href="https://huggingface.co/openai/whisper-large-v3-turbo"&gt;Whisper&lt;/a&gt;, &lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct"&gt;Qwen2.5-VL-72B-Instruct&lt;/a&gt;, and &lt;a href="https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507"&gt;Qwen3-235B-A22B-Thinking-2507&lt;/a&gt; in our implementation. The framework can be easily extended to other open-source tools of your choice.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Replacing these open-source tools with commercial alternatives can yield performance gains. Commercial tools were mainly used for multimodal capabilities and certain complex reasoning subtasks. The majority of tasks, including planning, browsing, refinement, navigation, and more, were handled by our models.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;h4&gt;More Benchmarks&lt;/h4&gt; 
 &lt;div align="center"&gt; 
  &lt;table&gt; 
   &lt;thead&gt; 
    &lt;tr&gt; 
     &lt;th&gt;Method&lt;/th&gt; 
     &lt;th align="center"&gt;HLE&lt;br /&gt;Pass@1&lt;/th&gt; 
     &lt;th align="center"&gt;Frames&lt;br /&gt;Pass@1&lt;/th&gt; 
     &lt;th align="center"&gt;BrowseComp&lt;br /&gt;Pass@1&lt;/th&gt; 
     &lt;th align="center"&gt;BrowseComp-ZH&lt;br /&gt;Pass@1&lt;/th&gt; 
     &lt;th align="center"&gt;WebWalkerQA&lt;br /&gt;Pass@1&lt;/th&gt; 
    &lt;/tr&gt; 
   &lt;/thead&gt; 
   &lt;tbody&gt; 
    &lt;tr&gt; 
     &lt;td&gt;OpenAI Deep Research&lt;/td&gt; 
     &lt;td align="center"&gt;26.6&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;51.5&lt;/td&gt; 
     &lt;td align="center"&gt;42.9&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;Gemini Deep Research&lt;/td&gt; 
     &lt;td align="center"&gt;26.9&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;Kimi-Researcher&lt;/td&gt; 
     &lt;td align="center"&gt;26.9&lt;/td&gt; 
     &lt;td align="center"&gt;78.8&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;WebDancer-7B&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;36.0&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;WebSailor-7B&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;6.7&lt;/td&gt; 
     &lt;td align="center"&gt;14.2&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;MiroThinker-8B-SFT-v0.1&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;58.0&lt;/td&gt; 
     &lt;td align="center"&gt;5.5&lt;/td&gt; 
     &lt;td align="center"&gt;9.3&lt;/td&gt; 
     &lt;td align="center"&gt;41.3&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;MiroThinker-8B-DPO-v0.1&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;64.4&lt;/td&gt; 
     &lt;td align="center"&gt;8.7&lt;/td&gt; 
     &lt;td align="center"&gt;13.6&lt;/td&gt; 
     &lt;td align="center"&gt;45.7&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;WebThinker-32B-RL&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;46.5&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;WebDancer-QwQ-32B&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;3.8&lt;/td&gt; 
     &lt;td align="center"&gt;18.0&lt;/td&gt; 
     &lt;td align="center"&gt;47.9&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;WebSailor-32B&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;10.5&lt;/td&gt; 
     &lt;td align="center"&gt;25.5&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;WebShaper-32B&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;51.4&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;MiroThinker-32B-SFT-v0.1&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;10.2&lt;/td&gt; 
     &lt;td align="center"&gt;70.4&lt;/td&gt; 
     &lt;td align="center"&gt;10.6&lt;/td&gt; 
     &lt;td align="center"&gt;13.8&lt;/td&gt; 
     &lt;td align="center"&gt;45.7&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;MiroThinker-32B-DPO-v0.1&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;11.8&lt;/td&gt; 
     &lt;td align="center"&gt;71.7&lt;/td&gt; 
     &lt;td align="center"&gt;13.0&lt;/td&gt; 
     &lt;td align="center"&gt;17.0&lt;/td&gt; 
     &lt;td align="center"&gt;49.3&lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt; 
  &lt;/table&gt; 
 &lt;/div&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;MiroThinker‚Äôs performance was tested with this repository and open-source tools; other models‚Äô results are from their papers and official sites.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;As &lt;a href="https://huggingface.co/datasets/miromind-ai/MiroVerse-v0.1"&gt;MiroVerse-v0.1&lt;/a&gt; mainly contains English data, the model‚Äôs Chinese capability is limited. We plan to add more Chinese data to improve performance in the next version.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;h2&gt;üöÄ Quick Start&lt;/h2&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üêç &lt;strong&gt;Python 3.10+&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;üì¶ &lt;strong&gt;uv package manager&lt;/strong&gt; (&lt;a href="https://github.com/astral-sh/uv"&gt;Installation guide&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;üîë &lt;strong&gt;Required API keys&lt;/strong&gt; (see configuration section below)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone the repository
git clone https://github.com/MiroMindAI/MiroThinker
cd MiroThinker

# Setup environment
cd apps/miroflow-agent
uv sync

# Configure API keys
cp .env.example .env
# Edit .env with your API keys (SERPER_API_KEY, JINA_API_KEY, E2B_API_KEY, etc.)
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;üìù Environment Variables&lt;/strong&gt;: See &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/#tool-configuration"&gt;Tool Configuration&lt;/a&gt; section for required API keys.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Tool Configuration&lt;/h3&gt; 
&lt;h4&gt;Minimal Configuration for MiroThinker v1.5 and v1.0&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Server&lt;/th&gt; 
   &lt;th align="left"&gt;Description&lt;/th&gt; 
   &lt;th align="left"&gt;Tools Provided&lt;/th&gt; 
   &lt;th align="left"&gt;Required Environment Variables&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;&lt;code&gt;tool-python&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Execution environment and file management (E2B sandbox)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;create_sandbox&lt;/code&gt;, &lt;code&gt;run_command&lt;/code&gt;, &lt;code&gt;run_python_code&lt;/code&gt;, &lt;code&gt;upload_file_from_local_to_sandbox&lt;/code&gt;, &lt;code&gt;download_file_from_sandbox_to_local&lt;/code&gt;, &lt;code&gt;download_file_from_internet_to_sandbox&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;E2B_API_KEY&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;&lt;code&gt;search_and_scrape_webpage&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Google search via Serper API&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;google_search&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;SERPER_API_KEY&lt;/code&gt;, &lt;code&gt;SERPER_BASE_URL&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;&lt;code&gt;jina_scrape_llm_summary&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Web scraping with LLM-based information extraction&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;scrape_and_extract_info&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;JINA_API_KEY&lt;/code&gt;, &lt;code&gt;JINA_BASE_URL&lt;/code&gt;, &lt;code&gt;SUMMARY_LLM_BASE_URL&lt;/code&gt;, &lt;code&gt;SUMMARY_LLM_MODEL_NAME&lt;/code&gt;, &lt;code&gt;SUMMARY_LLM_API_KEY&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Minimal &lt;code&gt;.env&lt;/code&gt; configuration example:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Required for MiroThinker v1.5 and v1.0 (minimal setup)
SERPER_API_KEY=your_serper_key
SERPER_BASE_URL="https://google.serper.dev"
JINA_API_KEY=your_jina_key
JINA_BASE_URL="https://r.jina.ai"
E2B_API_KEY=your_e2b_key

# Required for jina_scrape_llm_summary
# Note: Summary LLM can be a small model (e.g., Qwen3-14B or GPT-5-Nano)
# The choice has minimal impact on performance, use what's most convenient
SUMMARY_LLM_BASE_URL="https://your_summary_llm_base_url/v1/chat/completions"
SUMMARY_LLM_MODEL_NAME=your_llm_model_name  # e.g., "Qwen/Qwen3-14B" or "gpt-5-nano"
SUMMARY_LLM_API_KEY=your_llm_api_key  # Optional, depends on LLM provider

# Required for benchmark evaluation (LLM-as-a-Judge)
OPENAI_API_KEY=your_openai_key  # Required for running benchmark evaluations
OPENAI_BASE_URL="https://api.openai.com/v1"  # Optional, defaults to OpenAI's API
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;üí° Why this is minimal&lt;/strong&gt;: These 3 MCP servers cover the core capabilities needed for research tasks: web search, content extraction, and code execution. All other servers are optional enhancements.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;ü§ñ Summary LLM&lt;/strong&gt;: The &lt;code&gt;SUMMARY_LLM&lt;/code&gt; can be a small model like Qwen3-14B or GPT-5-Nano. The choice has minimal impact on overall performance, use whichever is most convenient for your setup.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;üìä For Benchmark Evaluation&lt;/strong&gt;: If you plan to run benchmark evaluations, you also need &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; (and optionally &lt;code&gt;OPENAI_BASE_URL&lt;/code&gt;) for LLM-as-a-Judge functionality used in evaluation scripts.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;üñºÔ∏è For GAIA Multimodal Tasks&lt;/strong&gt;: GAIA-Val-165 includes tasks with image/audio/video files. Since MiroThinker is a text-only LLM, GPT-4o is used to pre-process these files into text descriptions. The same &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; is used for both this preprocessing and LLM-as-a-Judge.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;üìñ For more details&lt;/strong&gt;: See &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/libs/miroflow-tools/README.md"&gt;MiroFlow Tools README&lt;/a&gt; for complete documentation of all available tools.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;details&gt; 
 &lt;summary&gt;üîß Click to expand additional available tools&lt;/summary&gt; 
 &lt;p&gt;The following optional tools are available but were not used in MiroThinker v1.5 and v1.0 evaluation:&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="left"&gt;Server Name&lt;/th&gt; 
    &lt;th align="left"&gt;Type&lt;/th&gt; 
    &lt;th align="left"&gt;Description&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;&lt;code&gt;tool-vqa&lt;/code&gt;&lt;/td&gt; 
    &lt;td align="left"&gt;Commercial&lt;/td&gt; 
    &lt;td align="left"&gt;Vision processing using Claude&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;&lt;code&gt;tool-vqa-os&lt;/code&gt;&lt;/td&gt; 
    &lt;td align="left"&gt;Open-Source&lt;/td&gt; 
    &lt;td align="left"&gt;Vision processing (open-source alternative)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;&lt;code&gt;tool-transcribe&lt;/code&gt;&lt;/td&gt; 
    &lt;td align="left"&gt;Commercial&lt;/td&gt; 
    &lt;td align="left"&gt;Audio transcription using OpenAI&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;&lt;code&gt;tool-transcribe-os&lt;/code&gt;&lt;/td&gt; 
    &lt;td align="left"&gt;Open-Source&lt;/td&gt; 
    &lt;td align="left"&gt;Audio transcription using Whisper&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;&lt;code&gt;tool-reasoning&lt;/code&gt;&lt;/td&gt; 
    &lt;td align="left"&gt;Commercial&lt;/td&gt; 
    &lt;td align="left"&gt;Reasoning engine using Claude&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;&lt;code&gt;tool-reasoning-os&lt;/code&gt;&lt;/td&gt; 
    &lt;td align="left"&gt;Open-Source&lt;/td&gt; 
    &lt;td align="left"&gt;Reasoning engine (open-source alternative)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;&lt;code&gt;tool-reading&lt;/code&gt;&lt;/td&gt; 
    &lt;td align="left"&gt;Open-Source&lt;/td&gt; 
    &lt;td align="left"&gt;Document reading using MarkItDown&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;&lt;code&gt;tool-google-search&lt;/code&gt;&lt;/td&gt; 
    &lt;td align="left"&gt;Commercial&lt;/td&gt; 
    &lt;td align="left"&gt;Web search using Google + scraping&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;&lt;code&gt;tool-sougou-search&lt;/code&gt;&lt;/td&gt; 
    &lt;td align="left"&gt;Commercial&lt;/td&gt; 
    &lt;td align="left"&gt;Web search using Sougou (Chinese)&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;strong&gt;üìñ Local Deployment&lt;/strong&gt;: For instructions on deploying open-source tools (&lt;code&gt;tool-vqa-os&lt;/code&gt;, &lt;code&gt;tool-transcribe-os&lt;/code&gt;, &lt;code&gt;tool-reasoning-os&lt;/code&gt;) locally, see &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/assets/LOCAL-TOOL-DEPLOYMENT.md"&gt;Local Tool Deployment Guide&lt;/a&gt;.&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/libs/miroflow-tools/README.md"&gt;MiroFlow Tools README&lt;/a&gt; for complete documentation of all available tools.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h4&gt;Pre-configured Agent Settings&lt;/h4&gt; 
&lt;p&gt;The &lt;code&gt;apps/miroflow-agent/conf/agent/&lt;/code&gt; directory contains several pre-configured agent settings. Each configuration uses different tools and requires corresponding environment variables in your &lt;code&gt;.env&lt;/code&gt; file.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;üí° Recommended&lt;/strong&gt;: For MiroThinker v1.5, use &lt;code&gt;mirothinker_v1.5_keep5_max200&lt;/code&gt; (with context management, recommended for most tasks) or &lt;code&gt;mirothinker_v1.5_keep5_max400&lt;/code&gt; (only used for BrowseComp and BrowseComp-ZH). For v1.0, use &lt;code&gt;mirothinker_v1.0_keep5&lt;/code&gt; (with context management). All use minimal configuration with only 3 MCP servers.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Configuration&lt;/th&gt; 
   &lt;th align="left"&gt;Description&lt;/th&gt; 
   &lt;th align="left"&gt;Max Turns&lt;/th&gt; 
   &lt;th align="left"&gt;Context Retention&lt;/th&gt; 
   &lt;th align="left"&gt;Required Environment Variables&lt;/th&gt; 
   &lt;th align="left"&gt;Recommended For&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;&lt;code&gt;mirothinker_v1.5_keep5_max200&lt;/code&gt;&lt;/strong&gt; ‚≠ê&lt;/td&gt; 
   &lt;td align="left"&gt;Single-agent with context management&lt;/td&gt; 
   &lt;td align="left"&gt;200&lt;/td&gt; 
   &lt;td align="left"&gt;Keep 5 most recent&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;SERPER_API_KEY&lt;/code&gt;, &lt;code&gt;SERPER_BASE_URL&lt;/code&gt;, &lt;code&gt;JINA_API_KEY&lt;/code&gt;, &lt;code&gt;JINA_BASE_URL&lt;/code&gt;, &lt;code&gt;E2B_API_KEY&lt;/code&gt;, &lt;code&gt;SUMMARY_LLM_BASE_URL&lt;/code&gt;, &lt;code&gt;SUMMARY_LLM_MODEL_NAME&lt;/code&gt;, &lt;code&gt;SUMMARY_LLM_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;v1.5 (recommended for most tasks)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;&lt;code&gt;mirothinker_v1.5_keep5_max400&lt;/code&gt;&lt;/strong&gt; ‚≠ê&lt;/td&gt; 
   &lt;td align="left"&gt;Single-agent with context management&lt;/td&gt; 
   &lt;td align="left"&gt;400&lt;/td&gt; 
   &lt;td align="left"&gt;Keep 5 most recent&lt;/td&gt; 
   &lt;td align="left"&gt;Same as above&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;v1.5 (for BrowseComp &amp;amp; BrowseComp-ZH)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;&lt;code&gt;mirothinker_v1.5&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Single-agent for MiroThinker v1.5&lt;/td&gt; 
   &lt;td align="left"&gt;600&lt;/td&gt; 
   &lt;td align="left"&gt;Keep all results&lt;/td&gt; 
   &lt;td align="left"&gt;Same as above&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;v1.5&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;&lt;code&gt;mirothinker_v1.0_keep5&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Single-agent with context management&lt;/td&gt; 
   &lt;td align="left"&gt;600&lt;/td&gt; 
   &lt;td align="left"&gt;Keep 5 most recent&lt;/td&gt; 
   &lt;td align="left"&gt;Same as above&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;v1.0&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;&lt;code&gt;mirothinker_v1.0&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Single-agent for MiroThinker v1.0&lt;/td&gt; 
   &lt;td align="left"&gt;600&lt;/td&gt; 
   &lt;td align="left"&gt;Keep all results&lt;/td&gt; 
   &lt;td align="left"&gt;Same as above&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;v1.0&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;details&gt; 
 &lt;summary&gt;üì¶ Click to expand legacy configurations (v0.1/v0.2)&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="left"&gt;Configuration&lt;/th&gt; 
    &lt;th align="left"&gt;Description&lt;/th&gt; 
    &lt;th align="left"&gt;Max Turns&lt;/th&gt; 
    &lt;th align="left"&gt;Context Retention&lt;/th&gt; 
    &lt;th align="left"&gt;Required Environment Variables&lt;/th&gt; 
    &lt;th align="left"&gt;Recommended For&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;&lt;strong&gt;&lt;code&gt;multi_agent&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; 
    &lt;td align="left"&gt;Multi-agent with commercial tools (v0.1/v0.2)&lt;/td&gt; 
    &lt;td align="left"&gt;50&lt;/td&gt; 
    &lt;td align="left"&gt;Keep all results&lt;/td&gt; 
    &lt;td align="left"&gt;&lt;code&gt;E2B_API_KEY&lt;/code&gt;, &lt;code&gt;ANTHROPIC_API_KEY&lt;/code&gt;, &lt;code&gt;ANTHROPIC_BASE_URL&lt;/code&gt;, &lt;code&gt;OPENAI_API_KEY&lt;/code&gt;, &lt;code&gt;OPENAI_BASE_URL&lt;/code&gt;, &lt;code&gt;SERPER_API_KEY&lt;/code&gt;, &lt;code&gt;SERPER_BASE_URL&lt;/code&gt;, &lt;code&gt;JINA_API_KEY&lt;/code&gt;, &lt;code&gt;JINA_BASE_URL&lt;/code&gt;&lt;/td&gt; 
    &lt;td align="left"&gt;v0.1/v0.2&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;&lt;strong&gt;&lt;code&gt;multi_agent_os&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; 
    &lt;td align="left"&gt;Multi-agent with open-source tools (v0.1/v0.2)&lt;/td&gt; 
    &lt;td align="left"&gt;50&lt;/td&gt; 
    &lt;td align="left"&gt;Keep all results&lt;/td&gt; 
    &lt;td align="left"&gt;&lt;code&gt;E2B_API_KEY&lt;/code&gt;, &lt;code&gt;VISION_API_KEY&lt;/code&gt;, &lt;code&gt;VISION_BASE_URL&lt;/code&gt;, &lt;code&gt;VISION_MODEL_NAME&lt;/code&gt;, &lt;code&gt;WHISPER_API_KEY&lt;/code&gt;, &lt;code&gt;WHISPER_BASE_URL&lt;/code&gt;, &lt;code&gt;WHISPER_MODEL_NAME&lt;/code&gt;, &lt;code&gt;REASONING_API_KEY&lt;/code&gt;, &lt;code&gt;REASONING_BASE_URL&lt;/code&gt;, &lt;code&gt;REASONING_MODEL_NAME&lt;/code&gt;, &lt;code&gt;SERPER_API_KEY&lt;/code&gt;, &lt;code&gt;SERPER_BASE_URL&lt;/code&gt;, &lt;code&gt;JINA_API_KEY&lt;/code&gt;, &lt;code&gt;JINA_BASE_URL&lt;/code&gt;&lt;/td&gt; 
    &lt;td align="left"&gt;v0.1/v0.2&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;üí° Note&lt;/strong&gt;: All environment variables are listed in &lt;code&gt;apps/miroflow-agent/.env.example&lt;/code&gt;. Copy it to &lt;code&gt;.env&lt;/code&gt; and fill in the values for the tools you plan to use.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Creating Custom Tool Configurations&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;üîß Click to expand custom tool configuration guide&lt;/summary&gt; 
 &lt;p&gt;You can create your own YAML configuration file to freely combine MCP servers. Here's how:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;strong&gt;Create a new YAML file&lt;/strong&gt; in &lt;code&gt;apps/miroflow-agent/conf/agent/&lt;/code&gt;:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-yaml"&gt;# conf/agent/my_custom_config.yaml
defaults:
  - default
  - _self_

main_agent:
  tools:
    - tool-python                    # Execution environment
    - search_and_scrape_webpage      # Google search
    - jina_scrape_llm_summary        # Web scraping with LLM
    - tool-vqa                       # Vision processing (optional)
    - tool-transcribe                # Audio processing (optional)
    - tool-reasoning                 # Reasoning engine (optional)
    - tool-reading                   # Document reading (optional)
  max_turns: 400  # Maximum number of turns

sub_agents:
  agent-browsing:  # Optional sub-agent
    tools:
      - tool-google-search
      - tool-vqa
      - tool-reading
      - tool-python
    max_turns: 50

keep_tool_result: -1  # Context retention budget: -1 keeps all tool results, or specify K to keep only the K most recent tool responses
&lt;/code&gt;&lt;/pre&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;strong&gt;üí° Context Retention Strategy&lt;/strong&gt;: The &lt;code&gt;keep_tool_result&lt;/code&gt; parameter implements a &lt;strong&gt;recency-based context retention&lt;/strong&gt; strategy. In the standard ReAct paradigm, all tool outputs are retained in the message history, which can lead to inefficient context utilization. Empirically, we observe that the model's subsequent actions depend primarily on recent observations rather than distant ones. This strategy retains only the most recent K tool responses (where K is the &lt;code&gt;keep_tool_result&lt;/code&gt; value) while preserving the complete sequence of thoughts and actions.&lt;/p&gt; 
  &lt;p&gt;&lt;strong&gt;Benefits:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;‚úÖ Preserves the reasoning and action trace&lt;/li&gt; 
   &lt;li&gt;‚úÖ Focuses the model's attention on the most contextually relevant observations&lt;/li&gt; 
   &lt;li&gt;‚úÖ Frees additional context space for extended reasoning and deeper tool-use trajectories&lt;/li&gt; 
   &lt;li&gt;‚úÖ Does not lead to performance degradation while allowing more context space for interactive scaling&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;p&gt;&lt;strong&gt;Usage:&lt;/strong&gt; Set &lt;code&gt;keep_tool_result: -1&lt;/code&gt; to keep all tool results, or specify a positive integer K (e.g., &lt;code&gt;keep_tool_result: 5&lt;/code&gt;) to keep only the K most recent tool responses.&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;ol start="2"&gt; 
  &lt;li&gt;&lt;strong&gt;Use your custom configuration&lt;/strong&gt; when running evaluations:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;cd apps/miroflow-agent
uv run main.py llm=qwen-3 agent=my_custom_config llm.base_url=https://your_base_url/v1
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="3"&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Configure environment variables&lt;/strong&gt; in &lt;code&gt;.env&lt;/code&gt; based on the tools you use.&lt;/p&gt; &lt;p&gt;All available environment variables are listed in &lt;code&gt;apps/miroflow-agent/.env.example&lt;/code&gt;. Copy it to &lt;code&gt;.env&lt;/code&gt; and configure the variables according to your chosen configuration:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cd apps/miroflow-agent
cp .env.example .env
# Edit .env with your actual API keys
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;For MiroThinker v1.5&lt;/strong&gt; (&lt;code&gt;mirothinker_v1.5_keep5_max200.yaml&lt;/code&gt;, &lt;code&gt;mirothinker_v1.5_keep5_max400.yaml&lt;/code&gt;, or &lt;code&gt;mirothinker_v1.5.yaml&lt;/code&gt;) and &lt;strong&gt;v1.0&lt;/strong&gt; (&lt;code&gt;mirothinker_v1.0_keep5.yaml&lt;/code&gt; or &lt;code&gt;mirothinker_v1.0.yaml&lt;/code&gt;), see the &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/#minimal-configuration-for-mirothinker-v15-and-v10"&gt;Minimal Configuration&lt;/a&gt; section above for the complete configuration example.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;For other configurations&lt;/strong&gt;, refer to the &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/#pre-configured-agent-settings"&gt;Pre-configured Agent Settings&lt;/a&gt; table above to see which environment variables are required.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;üîë Click to expand optional API keys&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# API for LLM-as-a-Judge (for benchmark testing, required for benchmark evaluation)
OPENAI_API_KEY=your_openai_key
OPENAI_BASE_URL="https://api.openai.com/v1"  # Optional, defaults to OpenAI's API

# API for Open-Source Audio Transcription Tool (for benchmark testing, optional)
WHISPER_MODEL_NAME="openai/whisper-large-v3-turbo"
WHISPER_API_KEY=your_whisper_key
WHISPER_BASE_URL="https://your_whisper_base_url/v1"

# API for Open-Source VQA Tool (for benchmark testing, optional)
VISION_MODEL_NAME="Qwen/Qwen2.5-VL-72B-Instruct"
VISION_API_KEY=your_vision_key
VISION_BASE_URL="https://your_vision_base_url/v1/chat/completions"

# API for Open-Source Reasoning Tool (for benchmark testing, optional)
REASONING_MODEL_NAME="Qwen/Qwen3-235B-A22B-Thinking-2507"
REASONING_API_KEY=your_reasoning_key
REASONING_BASE_URL="https://your_reasoning_base_url/v1/chat/completions"

# API for Claude Sonnet 3.7 as Commercial Tools (optional)
ANTHROPIC_API_KEY=your_anthropic_key

# API for Sougou Search (optional)
TENCENTCLOUD_SECRET_ID=your_tencent_cloud_secret_id
TENCENTCLOUD_SECRET_KEY=your_tencent_cloud_secret_key

# API for Summary LLM (can use small models like Qwen3-14B or GPT-5-Nano)
SUMMARY_LLM_BASE_URL="https://your_summary_llm_base_url/v1/chat/completions"
SUMMARY_LLM_MODEL_NAME=your_summary_llm_model_name  # e.g., "Qwen/Qwen3-14B" or "gpt-5-nano"
SUMMARY_LLM_API_KEY=your_summary_llm_api_key
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Serve the MiroThinker Model&lt;/h3&gt; 
&lt;h4&gt;Option 1 (Recommended): Serve with SGLang or vLLM&lt;/h4&gt; 
&lt;p&gt;Use SGLang to serve MiroThinker models at port 61002:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;NUM_GPUS=4
PORT=61002

# Downloading model from HF (v1.5 recommended)
MODEL_PATH=miromind-ai/MiroThinker-v1.5-30B

# Or use v1.0
# MODEL_PATH=miromind-ai/MiroThinker-v1.0-30B

python3 -m sglang.launch_server \
    --model-path $MODEL_PATH \
    --tp $NUM_GPUS \
    --dp 1 \
    --host 0.0.0.0 \
    --port $PORT \
    --trust-remote-code
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;üìç Server URL&lt;/strong&gt;: This will start a server at &lt;code&gt;http://0.0.0.0:$PORT&lt;/code&gt;. Use this as your server base URL (e.g., &lt;code&gt;http://0.0.0.0:61002/v1&lt;/code&gt;).&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Option 2: Quantized Light-Weight Options&lt;/h4&gt; 
&lt;p&gt;We also provide comprehensive guidance for serving MiroThinker models using CPU-optimized and GPU-accelerated quantization techniques, along with detailed analysis and guidelines for deployment with llama.cpp, Ollama, SGLang, and other inference frameworks.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;üìñ Complete Guide&lt;/strong&gt;: See &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/apps/gradio-demo/"&gt;Deployment Documentation&lt;/a&gt; for detailed deployment instructions.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Run Your First Task&lt;/h3&gt; 
&lt;p&gt;After setting up the environment and starting your model server, run &lt;code&gt;main.py&lt;/code&gt; to test with a default question: &lt;em&gt;"What is the title of today's arxiv paper in computer science?"&lt;/em&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd apps/miroflow-agent

# Using MiroThinker models (requires your own model server)
uv run python main.py llm=qwen-3 agent=mirothinker_v1.5_keep5_max200 llm.base_url=http://localhost:61002/v1

# Or using Claude (requires ANTHROPIC_API_KEY in .env)
uv run python main.py llm=claude-3-7 agent=single_agent_keep5

# Or using GPT-5 (requires OPENAI_API_KEY in .env)
uv run python main.py llm=gpt-5 agent=single_agent_keep5
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;To customize your question&lt;/strong&gt;, edit &lt;code&gt;main.py&lt;/code&gt; line 32:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;task_description = "Your custom question here"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The agent will search the web, execute code if needed, and provide an answer with sources.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;üìñ More details&lt;/strong&gt;: See &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/apps/miroflow-agent/README.md"&gt;apps/miroflow-agent/README.md&lt;/a&gt; for available configurations and troubleshooting.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;üìä Benchmark Evaluation&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;For researchers who want to reproduce our benchmark results or evaluate on standard benchmarks.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Download Benchmark Data&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd MiroThinker  # Back to project root
wget https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/data_20251115_password_protected.zip
unzip data_20251115_password_protected.zip
# Password: pf4*
rm data_20251115_password_protected.zip
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Run Benchmark Evaluation&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; For MiroThinker v1.5, use &lt;code&gt;mirothinker_v1.5_keep5_max200&lt;/code&gt; (with context management), &lt;code&gt;mirothinker_v1.5_keep5_max400&lt;/code&gt; (with context management), or &lt;code&gt;mirothinker_v1.5&lt;/code&gt; configurations. For v1.0, use &lt;code&gt;mirothinker_v1.0_keep5&lt;/code&gt; (with context management) or &lt;code&gt;mirothinker_v1.0&lt;/code&gt; configurations.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Available Parameters:&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;You can customize the evaluation by setting the following environment variables before running the script:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Parameter&lt;/th&gt; 
   &lt;th align="left"&gt;Default&lt;/th&gt; 
   &lt;th align="left"&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;LLM_MODEL&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;"MiroThinker-Models"&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Model name identifier&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;BASE_URL&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;"https://your-api.com/v1"&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Base URL of your model server&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;NUM_RUNS&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Varies by benchmark&lt;/td&gt; 
   &lt;td align="left"&gt;Number of evaluation runs (3 for most benchmarks, 8 for GAIA/XBench/FutureX/SEAL-0, 32 for AIME2025)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;LLM_PROVIDER&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;"qwen"&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;LLM provider (e.g., &lt;code&gt;qwen&lt;/code&gt;, &lt;code&gt;openai&lt;/code&gt;, &lt;code&gt;anthropic&lt;/code&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;AGENT_SET&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;"mirothinker_v1.5_keep5_max200"&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Agent configuration (e.g., &lt;code&gt;mirothinker_v1.5_keep5_max200&lt;/code&gt;, &lt;code&gt;mirothinker_v1.5_keep5_max400&lt;/code&gt;, &lt;code&gt;mirothinker_v1.0_keep5&lt;/code&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;MAX_CONTEXT_LENGTH&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;262144&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Maximum context length (256K)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;MAX_CONCURRENT&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;10&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Maximum concurrent tasks&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;PASS_AT_K&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;1&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Pass@K evaluation metric&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;TEMPERATURE&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;1.0&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Sampling temperature&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;"xxx"&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;API key for the model server&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Example Usage:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Navigate to the miroflow-agent directory first
cd apps/miroflow-agent

# Basic usage with v1.5 (recommended)
NUM_RUNS=8 LLM_MODEL="MiroThinker-v1.5-30B" BASE_URL="https://your-api.com/v1" bash scripts/run_evaluate_multiple_runs_gaia-validation-text-103.sh

# Or with v1.0
# NUM_RUNS=8 LLM_MODEL="MiroThinker-v1.0-30B" BASE_URL="https://your-api.com/v1" bash scripts/run_evaluate_multiple_runs_gaia-validation-text-103.sh

# Customize number of runs and agent configuration (v1.5 with context management)
LLM_MODEL="MiroThinker-v1.5-30B" \
BASE_URL="https://your-api.com/v1" \
NUM_RUNS=8 \
AGENT_SET="mirothinker_v1.5_keep5_max200" \
bash scripts/run_evaluate_multiple_runs_gaia-validation-text-103.sh

# Or with v1.0 configuration (with context management)
# LLM_MODEL="MiroThinker-v1.0-30B" \
# BASE_URL="https://your-api.com/v1" \
# NUM_RUNS=8 \
# AGENT_SET="mirothinker_v1.0_keep5" \
# bash scripts/run_evaluate_multiple_runs_gaia-validation-text-103.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;details open&gt; 
 &lt;summary&gt;üìã Click to expand all benchmark commands&lt;/summary&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;strong&gt;‚ö†Ô∏è Important for MiroThinker v1.5&lt;/strong&gt;: To reproduce our reported results, you must set the correct &lt;code&gt;AGENT_SET&lt;/code&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;BrowseComp &amp;amp; BrowseComp-ZH&lt;/strong&gt;: Use &lt;code&gt;AGENT_SET="mirothinker_v1.5_keep5_max400"&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;All other benchmarks&lt;/strong&gt;: Use &lt;code&gt;AGENT_SET="mirothinker_v1.5_keep5_max200"&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/blockquote&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Navigate to the miroflow-agent directory first
cd apps/miroflow-agent

# HLE
NUM_RUNS=3 LLM_MODEL="xxx" BASE_URL="xxx" AGENT_SET="mirothinker_v1.5_keep5_max200" bash scripts/run_evaluate_multiple_runs_hle.sh

# HLE-Text-2158
NUM_RUNS=3 LLM_MODEL="xxx" BASE_URL="xxx" AGENT_SET="mirothinker_v1.5_keep5_max200" bash scripts/run_evaluate_multiple_runs_hle-text-2158.sh

# HLE-Text-500
NUM_RUNS=3 LLM_MODEL="xxx" BASE_URL="xxx" AGENT_SET="mirothinker_v1.5_keep5_max200" bash scripts/run_evaluate_multiple_runs_hle-text-500.sh

# GAIA-Text-103
NUM_RUNS=8 LLM_MODEL="xxx" BASE_URL="xxx" AGENT_SET="mirothinker_v1.5_keep5_max200" bash scripts/run_evaluate_multiple_runs_gaia-validation-text-103.sh

# GAIA-Validation (GAIA-Val-165)
NUM_RUNS=8 LLM_MODEL="xxx" BASE_URL="xxx" AGENT_SET="mirothinker_v1.5_keep5_max200" bash scripts/run_evaluate_multiple_runs_gaia-validation.sh

# BrowseComp-EN (‚ö†Ô∏è use max400)
NUM_RUNS=3 LLM_MODEL="xxx" BASE_URL="xxx" AGENT_SET="mirothinker_v1.5_keep5_max400" bash scripts/run_evaluate_multiple_runs_browsecomp.sh

# BrowseComp-ZH (‚ö†Ô∏è use max400)
NUM_RUNS=3 LLM_MODEL="xxx" BASE_URL="xxx" AGENT_SET="mirothinker_v1.5_keep5_max400" bash scripts/run_evaluate_multiple_runs_browsecomp_zh.sh

# WebWalkerQA
NUM_RUNS=3 LLM_MODEL="xxx" BASE_URL="xxx" AGENT_SET="mirothinker_v1.5_keep5_max200" bash scripts/run_evaluate_multiple_runs_webwalkerqa.sh

# XBench-DeepSearch
NUM_RUNS=8 LLM_MODEL="xxx" BASE_URL="xxx" AGENT_SET="mirothinker_v1.5_keep5_max200" bash scripts/run_evaluate_multiple_runs_xbench_deepsearch.sh

# FRAMES
NUM_RUNS=3 LLM_MODEL="xxx" BASE_URL="xxx" AGENT_SET="mirothinker_v1.5_keep5_max200" bash scripts/run_evaluate_multiple_runs_frames.sh

# SEAL-0
NUM_RUNS=8 LLM_MODEL="xxx" BASE_URL="xxx" AGENT_SET="mirothinker_v1.5_keep5_max200" bash scripts/run_evaluate_multiple_runs_seal-0.sh

# FutureX
NUM_RUNS=8 LLM_MODEL="xxx" BASE_URL="xxx" AGENT_SET="mirothinker_v1.5_keep5_max200" bash scripts/run_evaluate_multiple_runs_futurex.sh

# AIME2025
NUM_RUNS=32 LLM_MODEL="xxx" BASE_URL="xxx" AGENT_SET="mirothinker_v1.5_keep5_max200" bash scripts/run_evaluate_multiple_runs_aime2025.sh

# DeepSearchQA
NUM_RUNS=3 LLM_MODEL="xxx" BASE_URL="xxx" AGENT_SET="mirothinker_v1.5_keep5_max200" bash scripts/run_evaluate_multiple_runs_deepsearchqa.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h4&gt;3. &lt;strong&gt;Monitor evaluation progress&lt;/strong&gt;&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;üìä Click to expand progress monitoring commands&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Navigate to the miroflow-agent directory first
cd apps/miroflow-agent

# For HLE
python benchmarks/check_progress/check_progress_hle.py /path/to/evaluation/logs

# For HLE-Text-2158
python benchmarks/check_progress/check_progress_hle-text-2158.py /path/to/evaluation/logs

# For HLE-Text-500
python benchmarks/check_progress/check_progress_hle-text-500.py /path/to/evaluation/logs

# For BrowseComp-EN
python benchmarks/check_progress/check_progress_browsecomp.py /path/to/evaluation/logs

# For BrowseComp-ZH
python benchmarks/check_progress/check_progress_browsecomp_zh.py /path/to/evaluation/logs

# For GAIA-Validation
python benchmarks/check_progress/check_progress_gaia-validation.py /path/to/evaluation/logs

# For GAIA-Text-103
python benchmarks/check_progress/check_progress_gaia-validation-text-103.py /path/to/evaluation/logs

# For WebWalkerQA
python benchmarks/check_progress/check_progress_webwalkerqa.py /path/to/evaluation/logs

# For Frames
python benchmarks/check_progress/check_progress_frames.py /path/to/evaluation/logs

# For XBench-DeepSearch
python benchmarks/check_progress/check_progress_xbench_deepsearch.py /path/to/evaluation/logs

# For SEAL-0
python benchmarks/check_progress/check_progress_seal-0.py /path/to/evaluation/logs

# For AIME2025
python benchmarks/check_progress/check_progress_aime2025.py /path/to/evaluation/logs

# For DeepSearchQA
python benchmarks/check_progress/check_progress_deepsearchqa.py /path/to/evaluation/logs
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;üî¨ Trace Collection&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;üìã Click to expand trace collection commands&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;cd apps/collect-trace

# Collect Traces for SFT
bash scripts/collect_trace_claude37.sh
bash scripts/collect_trace_gpt5.sh

# Collect Traces for DPO
bash scripts/collect_trace_qwen3.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;‚ùì FAQ &amp;amp; Troubleshooting&lt;/h2&gt; 
&lt;h3&gt;Common Issues&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;üîß Click to expand troubleshooting guide&lt;/summary&gt; 
 &lt;h4&gt;&lt;strong&gt;Q: Which version should I use?&lt;/strong&gt;&lt;/h4&gt; 
 &lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; We recommend &lt;strong&gt;MiroThinker v1.5&lt;/strong&gt; ‚≠ê with the minimal configuration:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;v1.5&lt;/strong&gt; ‚≠ê: Latest version with 256K context, world-leading performance. Use config (with context management): 
   &lt;ul&gt; 
    &lt;li&gt;&lt;code&gt;mirothinker_v1.5_keep5_max200&lt;/code&gt; (up to 200 turns, recommended for most tasks)&lt;/li&gt; 
    &lt;li&gt;&lt;code&gt;mirothinker_v1.5_keep5_max400&lt;/code&gt; (up to 400 turns, only used for BrowseComp and BrowseComp-ZH)&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h4&gt;&lt;strong&gt;Q: How do I get API keys?&lt;/strong&gt;&lt;/h4&gt; 
 &lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; You need these keys for minimal setup:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;SERPER_API_KEY&lt;/strong&gt;: Get from &lt;a href="https://serper.dev/"&gt;Serper.dev&lt;/a&gt; (Google search API)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;JINA_API_KEY&lt;/strong&gt;: Get from &lt;a href="https://jina.ai/"&gt;Jina.ai&lt;/a&gt; (Web scraping)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;E2B_API_KEY&lt;/strong&gt;: Get from &lt;a href="https://e2b.dev/"&gt;E2B.dev&lt;/a&gt; (Code execution sandbox)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;SUMMARY_LLM_API_KEY&lt;/strong&gt;: Your LLM API credentials (for content summarization). Can be a small model like Qwen3-14B or GPT-5-Nano‚Äîthe choice has minimal impact on performance.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;OPENAI_API_KEY&lt;/strong&gt;: Get from &lt;a href="https://platform.openai.com/"&gt;OpenAI&lt;/a&gt; (Required for benchmark evaluation, used for LLM-as-a-Judge)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;OPENAI_BASE_URL&lt;/strong&gt;: Optional, defaults to &lt;code&gt;https://api.openai.com/v1&lt;/code&gt;. Can be changed to use OpenAI-compatible APIs.&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h4&gt;&lt;strong&gt;Q: Model server connection errors&lt;/strong&gt;&lt;/h4&gt; 
 &lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Common issues:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Check base URL format&lt;/strong&gt;: Should end with &lt;code&gt;/v1&lt;/code&gt; (e.g., &lt;code&gt;https://your-api.com/v1&lt;/code&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Verify API key&lt;/strong&gt;: Ensure &lt;code&gt;API_KEY&lt;/code&gt; is set correctly in environment or script&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Check server status&lt;/strong&gt;: Make sure your model server is running and accessible&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Network issues&lt;/strong&gt;: Verify firewall/network settings allow connections&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h4&gt;&lt;strong&gt;Q: Evaluation script fails to run&lt;/strong&gt;&lt;/h4&gt; 
 &lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Troubleshooting steps:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;strong&gt;Check working directory&lt;/strong&gt;: Make sure you're in &lt;code&gt;apps/miroflow-agent&lt;/code&gt; directory&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Verify environment&lt;/strong&gt;: Run &lt;code&gt;uv sync&lt;/code&gt; to ensure dependencies are installed&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Check .env file&lt;/strong&gt;: Ensure all required environment variables are set&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Review logs&lt;/strong&gt;: Check &lt;code&gt;logs/&lt;/code&gt; directory for detailed error messages&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Verify data path&lt;/strong&gt;: Ensure benchmark data is downloaded and in correct location&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;h4&gt;&lt;strong&gt;Q: Out of memory errors&lt;/strong&gt;&lt;/h4&gt; 
 &lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Solutions:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Reduce context length&lt;/strong&gt;: Set &lt;code&gt;MAX_CONTEXT_LENGTH&lt;/code&gt; to a smaller value (e.g., 131072 for 128K)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Use context management with fewer turns&lt;/strong&gt;: 
   &lt;ul&gt; 
    &lt;li&gt;For v1.5: Use &lt;code&gt;mirothinker_v1.5_keep5_max200&lt;/code&gt; or &lt;code&gt;mirothinker_v1.5_keep5_max400&lt;/code&gt; (with context management)&lt;/li&gt; 
    &lt;li&gt;For v1.0: Use &lt;code&gt;mirothinker_v1.0_keep5&lt;/code&gt; (with context management)&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Reduce concurrent tasks&lt;/strong&gt;: Set &lt;code&gt;MAX_CONCURRENT&lt;/code&gt; to a smaller number (e.g., 5)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Use smaller model&lt;/strong&gt;: 
   &lt;ul&gt; 
    &lt;li&gt;For v1.5: Try 30B instead of 235B&lt;/li&gt; 
    &lt;li&gt;For v1.0: Try 8B or 30B instead of 72B&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h4&gt;&lt;strong&gt;Q: Tool execution errors&lt;/strong&gt;&lt;/h4&gt; 
 &lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Common fixes:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;E2B errors&lt;/strong&gt;: Verify &lt;code&gt;E2B_API_KEY&lt;/code&gt; is valid and account has credits&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Serper errors&lt;/strong&gt;: Check &lt;code&gt;SERPER_API_KEY&lt;/code&gt; and rate limits&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Jina errors&lt;/strong&gt;: Verify &lt;code&gt;JINA_API_KEY&lt;/code&gt; and &lt;code&gt;JINA_BASE_URL&lt;/code&gt; are correct&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;LLM summarization errors&lt;/strong&gt;: Check &lt;code&gt;SUMMARY_LLM_*&lt;/code&gt; variables and model availability&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h4&gt;&lt;strong&gt;Q: How to monitor long-running evaluations?&lt;/strong&gt;&lt;/h4&gt; 
 &lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Use the progress monitoring scripts:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;cd apps/miroflow-agent
python benchmarks/check_progress/check_progress_&amp;lt;benchmark_name&amp;gt;.py /path/to/logs
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;The scripts show completion status, elapsed time, and estimated remaining time.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h3&gt;Getting Help&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìñ &lt;strong&gt;Documentation&lt;/strong&gt;: Check &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/libs/miroflow-tools/README.md"&gt;MiroFlow Tools README&lt;/a&gt; for tool details&lt;/li&gt; 
 &lt;li&gt;üí¨ &lt;strong&gt;Discord&lt;/strong&gt;: Join our &lt;a href="https://discord.com/invite/GPqEnkzQZd"&gt;Discord community&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üêõ &lt;strong&gt;Issues&lt;/strong&gt;: Report bugs on &lt;a href="https://github.com/MiroMindAI/MiroThinker/issues"&gt;GitHub Issues&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üìß &lt;strong&gt;Contact&lt;/strong&gt;: Visit &lt;a href="https://miromind.ai/"&gt;our website&lt;/a&gt; for more information&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìÑ License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the MIT License - see the &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h2&gt;üôè Acknowledgments&lt;/h2&gt; 
&lt;p&gt;We extend our sincere gratitude to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üèÜ &lt;strong&gt;Benchmark Contributors&lt;/strong&gt; for the comprehensive evaluation datasets&lt;/li&gt; 
 &lt;li&gt;üåç &lt;strong&gt;Open Source Community&lt;/strong&gt; for the tools and libraries that make this possible&lt;/li&gt; 
 &lt;li&gt;üë• &lt;strong&gt;All Contributors&lt;/strong&gt; who have helped make MiroThinker better&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/MiroMindAI/MiroThinker/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=MiroMindAI/MiroThinker" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;p&gt;Join our community and help us build the future of AI agents!&lt;/p&gt; 
&lt;h3&gt;References&lt;/h3&gt; 
&lt;p&gt;If you find this project useful in your research, please consider citing:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@article{miromind2025mirothinker,
  title={MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling},
  author={MiroMind Team and Bai, Song and Bing, Lidong and Chen, Carson and Chen, Guanzheng and Chen, Yuntao and Chen, Zhe and Chen, Ziyi and Dai, Jifeng and Dong, Xuan and others},
  journal={arXiv preprint arXiv:2511.11793},
  year={2025}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#MiroMindAI/MiroThinker&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=MiroMindAI/MiroThinker&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>9001/copyparty</title>
      <link>https://github.com/9001/copyparty</link>
      <description>&lt;p&gt;Portable file server with accelerated resumable uploads, dedup, WebDAV, SFTP, FTP, TFTP, zeroconf, media indexer, thumbnails++ all in one file&lt;/p&gt;&lt;hr&gt;&lt;img src="https://github.com/9001/copyparty/raw/hovudstraum/docs/logo.svg?sanitize=true" width="250" align="right" /&gt; 
&lt;h3&gt;üíæüéâ copyparty&lt;/h3&gt; 
&lt;p&gt;turn almost any device into a file server with resumable uploads/downloads using &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#browser-support"&gt;&lt;em&gt;any&lt;/em&gt;&lt;/a&gt; web browser&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;server only needs Python (2 or 3), all dependencies optional&lt;/li&gt; 
 &lt;li&gt;üîå protocols: &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#the-browser"&gt;http(s)&lt;/a&gt; // &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#webdav-server"&gt;webdav&lt;/a&gt; // &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#sftp-server"&gt;sftp&lt;/a&gt; // &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#ftp-server"&gt;ftp(s)&lt;/a&gt; // &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#tftp-server"&gt;tftp&lt;/a&gt; // &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#smb-server"&gt;smb/cifs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üì± &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#android-app"&gt;android app&lt;/a&gt; // &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#ios-shortcuts"&gt;iPhone shortcuts&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;üëâ &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#quickstart"&gt;Get started&lt;/a&gt;!&lt;/strong&gt; or visit the &lt;strong&gt;&lt;a href="https://a.ocv.me/pub/demo/"&gt;read-only demo server&lt;/a&gt;&lt;/strong&gt; üëÄ running on a nuc in my basement&lt;/p&gt; 
&lt;p&gt;üì∑ &lt;strong&gt;screenshots:&lt;/strong&gt; &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#the-browser"&gt;browser&lt;/a&gt; // &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#uploading"&gt;upload&lt;/a&gt; // &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#unpost"&gt;unpost&lt;/a&gt; // &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#thumbnails"&gt;thumbnails&lt;/a&gt; // &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#searching"&gt;search&lt;/a&gt; // &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#file-search"&gt;fsearch&lt;/a&gt; // &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#zip-downloads"&gt;zip-DL&lt;/a&gt; // &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#markdown-viewer"&gt;md-viewer&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;üé¨ &lt;strong&gt;videos:&lt;/strong&gt; &lt;a href="https://a.ocv.me/pub/demo/pics-vids/up2k.webm"&gt;upload&lt;/a&gt; // &lt;a href="https://a.ocv.me/pub/demo/pics-vids/u2cli.webm"&gt;cli-upload&lt;/a&gt; // &lt;a href="https://a.ocv.me/pub/g/nerd-stuff/cpp/2024-0418-race-the-beam.webm"&gt;race-the-beam&lt;/a&gt; // üëâ &lt;strong&gt;&lt;a href="https://a.ocv.me/pub/demo/showcase-hq.webm"&gt;feature-showcase&lt;/a&gt;&lt;/strong&gt; (&lt;a href="https://www.youtube.com/watch?v=15_-hgsX2V0"&gt;youtube&lt;/a&gt;)&lt;/p&gt; 
&lt;p&gt;built in Norway üá≥üá¥ with contributions from &lt;a href="https://github.com/9001/copyparty/graphs/contributors"&gt;not-norway&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;readme toc&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;top 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#quickstart"&gt;quickstart&lt;/a&gt; - just run &lt;strong&gt;&lt;a href="https://github.com/9001/copyparty/releases/latest/download/copyparty-sfx.py"&gt;copyparty-sfx.py&lt;/a&gt;&lt;/strong&gt; -- that's it! üéâ 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#mirrors"&gt;mirrors&lt;/a&gt; - other places to download copyparty from&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#at-home"&gt;at home&lt;/a&gt; - make it accessible over the internet&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#on-servers"&gt;on servers&lt;/a&gt; - you may also want these, especially on servers&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#features"&gt;features&lt;/a&gt; - also see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/versus.md"&gt;comparison to similar software&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#testimonials"&gt;testimonials&lt;/a&gt; - small collection of user feedback&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#motivations"&gt;motivations&lt;/a&gt; - project goals / philosophy 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#notes"&gt;notes&lt;/a&gt; - general notes&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#bugs"&gt;bugs&lt;/a&gt; - roughly sorted by chance of encounter 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#not-my-bugs"&gt;not my bugs&lt;/a&gt; - same order here too&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#breaking-changes"&gt;breaking changes&lt;/a&gt; - upgrade notes&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#FAQ"&gt;FAQ&lt;/a&gt; - "frequently" asked questions&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#accounts-and-volumes"&gt;accounts and volumes&lt;/a&gt; - per-folder, per-user permissions 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#shadowing"&gt;shadowing&lt;/a&gt; - hiding specific subfolders&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#dotfiles"&gt;dotfiles&lt;/a&gt; - unix-style hidden files/folders&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#the-browser"&gt;the browser&lt;/a&gt; - accessing a copyparty server using a web-browser 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#tabs"&gt;tabs&lt;/a&gt; - the main tabs in the ui&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#hotkeys"&gt;hotkeys&lt;/a&gt; - the browser has the following hotkeys&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#navpane"&gt;navpane&lt;/a&gt; - switching between breadcrumbs or navpane&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#thumbnails"&gt;thumbnails&lt;/a&gt; - press &lt;code&gt;g&lt;/code&gt; or &lt;code&gt;Áî∞&lt;/code&gt; to toggle grid-view instead of the file listing&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#zip-downloads"&gt;zip downloads&lt;/a&gt; - download folders (or file selections) as &lt;code&gt;zip&lt;/code&gt; or &lt;code&gt;tar&lt;/code&gt; files&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#uploading"&gt;uploading&lt;/a&gt; - drag files/folders into the web-browser to upload 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#file-search"&gt;file-search&lt;/a&gt; - dropping files into the browser also lets you see if they exist on the server&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#unpost"&gt;unpost&lt;/a&gt; - undo/delete accidental uploads&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#self-destruct"&gt;self-destruct&lt;/a&gt; - uploads can be given a lifetime&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#race-the-beam"&gt;race the beam&lt;/a&gt; - download files while they're still uploading (&lt;a href="http://a.ocv.me/pub/g/nerd-stuff/cpp/2024-0418-race-the-beam.webm"&gt;demo video&lt;/a&gt;)&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#incoming-files"&gt;incoming files&lt;/a&gt; - the control-panel shows the ETA for all incoming files&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#file-manager"&gt;file manager&lt;/a&gt; - cut/paste, rename, and delete files/folders (if you have permission)&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#shares"&gt;shares&lt;/a&gt; - share a file or folder by creating a temporary link&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#batch-rename"&gt;batch rename&lt;/a&gt; - select some files and press &lt;code&gt;F2&lt;/code&gt; to bring up the rename UI&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#rss-feeds"&gt;rss feeds&lt;/a&gt; - monitor a folder with your RSS reader&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#opds-feeds"&gt;opds feeds&lt;/a&gt; - browse and download files from your e-book reader&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#recent-uploads"&gt;recent uploads&lt;/a&gt; - list all recent uploads&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#media-player"&gt;media player&lt;/a&gt; - plays almost every audio format there is 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#playlists"&gt;playlists&lt;/a&gt; - create and play &lt;a href="https://en.wikipedia.org/wiki/M3U"&gt;m3u8&lt;/a&gt; playlists&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#creating-a-playlist"&gt;creating a playlist&lt;/a&gt; - with a standalone mediaplayer or copyparty&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#audio-equalizer"&gt;audio equalizer&lt;/a&gt; - and &lt;a href="https://en.wikipedia.org/wiki/Dynamic_range_compression"&gt;dynamic range compressor&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#fix-unreliable-playback-on-android"&gt;fix unreliable playback on android&lt;/a&gt; - due to phone / app settings&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#textfile-viewer"&gt;textfile viewer&lt;/a&gt; - with realtime streaming of logfiles and such (&lt;a href="https://a.ocv.me/pub/demo/logtail/"&gt;demo&lt;/a&gt;)&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#markdown-viewer"&gt;markdown viewer&lt;/a&gt; - and there are &lt;em&gt;two&lt;/em&gt; editors 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#markdown-vars"&gt;markdown vars&lt;/a&gt; - dynamic docs with serverside variable expansion&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#other-tricks"&gt;other tricks&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#searching"&gt;searching&lt;/a&gt; - search by size, date, path/name, mp3-tags, ...&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#server-config"&gt;server config&lt;/a&gt; - using arguments or config files, or a mix of both 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#zeroconf"&gt;zeroconf&lt;/a&gt; - announce enabled services on the LAN (&lt;a href="https://user-images.githubusercontent.com/241032/215344737-0eae8d98-9496-4256-9aa8-cd2f6971810d.png"&gt;pic&lt;/a&gt;) 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#mdns"&gt;mdns&lt;/a&gt; - LAN domain-name and feature announcer&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#ssdp"&gt;ssdp&lt;/a&gt; - windows-explorer announcer&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#qr-code"&gt;qr-code&lt;/a&gt; - print a qr-code &lt;a href="https://user-images.githubusercontent.com/241032/194728533-6f00849b-c6ac-43c6-9359-83e454d11e00.png"&gt;(screenshot)&lt;/a&gt; for quick access&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#ftp-server"&gt;ftp server&lt;/a&gt; - an FTP server can be started using &lt;code&gt;--ftp 3921&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#sftp-server"&gt;sftp server&lt;/a&gt; - goes roughly 700 MiB/s (slower than webdav and ftp)&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#webdav-server"&gt;webdav server&lt;/a&gt; - with read-write support 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#connecting-to-webdav-from-windows"&gt;connecting to webdav from windows&lt;/a&gt; - using the GUI&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#tftp-server"&gt;tftp server&lt;/a&gt; - a TFTP server (read/write) can be started using &lt;code&gt;--tftp 3969&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#smb-server"&gt;smb server&lt;/a&gt; - unsafe, slow, not recommended for wan&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#browser-ux"&gt;browser ux&lt;/a&gt; - tweaking the ui&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#opengraph"&gt;opengraph&lt;/a&gt; - discord and social-media embeds&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#file-deduplication"&gt;file deduplication&lt;/a&gt; - enable symlink-based upload deduplication&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#file-indexing"&gt;file indexing&lt;/a&gt; - enable music search, upload-undo, and better dedup 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#exclude-patterns"&gt;exclude-patterns&lt;/a&gt; - to save some time&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#filesystem-guards"&gt;filesystem guards&lt;/a&gt; - avoid traversing into other filesystems&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#periodic-rescan"&gt;periodic rescan&lt;/a&gt; - filesystem monitoring&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#upload-rules"&gt;upload rules&lt;/a&gt; - set upload rules using volflags&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#compress-uploads"&gt;compress uploads&lt;/a&gt; - files can be autocompressed on upload&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#chmod-and-chown"&gt;chmod and chown&lt;/a&gt; - per-volume filesystem-permissions and ownership&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#other-flags"&gt;other flags&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#database-location"&gt;database location&lt;/a&gt; - in-volume (&lt;code&gt;.hist/up2k.db&lt;/code&gt;, default) or somewhere else&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#metadata-from-audio-files"&gt;metadata from audio files&lt;/a&gt; - set &lt;code&gt;-e2t&lt;/code&gt; to index tags on upload&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#file-parser-plugins"&gt;file parser plugins&lt;/a&gt; - provide custom parsers to index additional tags&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#event-hooks"&gt;event hooks&lt;/a&gt; - trigger a program on uploads, renames etc (&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/bin/hooks/"&gt;examples&lt;/a&gt;) 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#zeromq"&gt;zeromq&lt;/a&gt; - event-hooks can send zeromq messages&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#upload-events"&gt;upload events&lt;/a&gt; - the older, more powerful approach (&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/bin/mtag/"&gt;examples&lt;/a&gt;)&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#handlers"&gt;handlers&lt;/a&gt; - redefine behavior with plugins (&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/bin/handlers/"&gt;examples&lt;/a&gt;)&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#ip-auth"&gt;ip auth&lt;/a&gt; - autologin based on IP range (CIDR) 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#restrict-to-ip"&gt;restrict to ip&lt;/a&gt; - limit a user to certain IP ranges (CIDR)&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#identity-providers"&gt;identity providers&lt;/a&gt; - replace copyparty passwords with oauth and such 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#generic-header-auth"&gt;generic header auth&lt;/a&gt; - other ways to auth by header&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#user-changeable-passwords"&gt;user-changeable passwords&lt;/a&gt; - if permitted, users can change their own passwords&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#using-the-cloud-as-storage"&gt;using the cloud as storage&lt;/a&gt; - connecting to an aws s3 bucket and similar&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#hiding-from-google"&gt;hiding from google&lt;/a&gt; - tell search engines you don't wanna be indexed&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#themes"&gt;themes&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#complete-examples"&gt;complete examples&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#listen-on-port-80-and-443"&gt;listen on port 80 and 443&lt;/a&gt; - become a &lt;em&gt;real&lt;/em&gt; webserver&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#reverse-proxy"&gt;reverse-proxy&lt;/a&gt; - running copyparty next to other websites 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#real-ip"&gt;real-ip&lt;/a&gt; - teaching copyparty how to see client IPs&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#reverse-proxy-performance"&gt;reverse-proxy performance&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#permanent-cloudflare-tunnel"&gt;permanent cloudflare tunnel&lt;/a&gt; - if you have a domain and want to get your copyparty online real quick&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#prometheus"&gt;prometheus&lt;/a&gt; - metrics/stats can be enabled&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#other-extremely-specific-features"&gt;other extremely specific features&lt;/a&gt; - you'll never find a use for these 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#custom-mimetypes"&gt;custom mimetypes&lt;/a&gt; - change the association of a file extension&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#GDPR-compliance"&gt;GDPR compliance&lt;/a&gt; - imagine using copyparty professionally...&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#feature-chickenbits"&gt;feature chickenbits&lt;/a&gt; - buggy feature? rip it out&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#feature-beefybits"&gt;feature beefybits&lt;/a&gt; - force-enable features with known issues on your OS/env&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#packages"&gt;packages&lt;/a&gt; - the party might be closer than you think 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#arch-package"&gt;arch package&lt;/a&gt; - &lt;code&gt;pacman -S copyparty&lt;/code&gt; (in &lt;a href="https://archlinux.org/packages/extra/any/copyparty/"&gt;arch linux extra&lt;/a&gt;)&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#fedora-package"&gt;fedora package&lt;/a&gt; - does not exist yet&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#homebrew-formulae"&gt;homebrew formulae&lt;/a&gt; - &lt;code&gt;brew install copyparty ffmpeg&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#nix-package"&gt;nix package&lt;/a&gt; - &lt;code&gt;nix profile install github:9001/copyparty&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#nixos-module"&gt;nixos module&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#browser-support"&gt;browser support&lt;/a&gt; - TLDR: yes&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#server-hall-of-fame"&gt;server hall of fame&lt;/a&gt; - unexpected things that run copyparty&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#client-examples"&gt;client examples&lt;/a&gt; - interact with copyparty using non-browser clients 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#folder-sync"&gt;folder sync&lt;/a&gt; - sync folders to/from copyparty&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#mount-as-drive"&gt;mount as drive&lt;/a&gt; - a remote copyparty server as a local filesystem&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#android-app"&gt;android app&lt;/a&gt; - upload to copyparty with one tap&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#iOS-shortcuts"&gt;iOS shortcuts&lt;/a&gt; - there is no iPhone app, but&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#performance"&gt;performance&lt;/a&gt; - defaults are usually fine - expect &lt;code&gt;8 GiB/s&lt;/code&gt; download, &lt;code&gt;1 GiB/s&lt;/code&gt; upload 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#client-side"&gt;client-side&lt;/a&gt; - when uploading files&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#security"&gt;security&lt;/a&gt; - there is a &lt;a href="https://discord.gg/25J8CdTT6G"&gt;discord server&lt;/a&gt; with announcements 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#gotchas"&gt;gotchas&lt;/a&gt; - behavior that might be unexpected&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#cors"&gt;cors&lt;/a&gt; - cross-site request config&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#filekeys"&gt;filekeys&lt;/a&gt; - prevent filename bruteforcing 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#dirkeys"&gt;dirkeys&lt;/a&gt; - share specific folders in a volume&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#password-hashing"&gt;password hashing&lt;/a&gt; - you can hash passwords&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#https"&gt;https&lt;/a&gt; - both HTTP and HTTPS are accepted&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#recovering-from-crashes"&gt;recovering from crashes&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#client-crashes"&gt;client crashes&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#firefox-wsod"&gt;firefox wsod&lt;/a&gt; - firefox 87 can crash during uploads&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#HTTP-API"&gt;HTTP API&lt;/a&gt; - see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/devnotes.md#http-api"&gt;devnotes&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#dependencies"&gt;dependencies&lt;/a&gt; - mandatory deps 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#optional-dependencies"&gt;optional dependencies&lt;/a&gt; - enable bonus features 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#dependency-chickenbits"&gt;dependency chickenbits&lt;/a&gt; - prevent loading an optional dependency&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#dependency-unvendoring"&gt;dependency unvendoring&lt;/a&gt; - force use of system modules&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#optional-gpl-stuff"&gt;optional gpl stuff&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#sfx"&gt;sfx&lt;/a&gt; - the self-contained "binary" (recommended!) 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#copypartyexe"&gt;copyparty.exe&lt;/a&gt; - download &lt;a href="https://github.com/9001/copyparty/releases/latest/download/copyparty.exe"&gt;copyparty.exe&lt;/a&gt; (win8+) or &lt;a href="https://github.com/9001/copyparty/releases/latest/download/copyparty32.exe"&gt;copyparty32.exe&lt;/a&gt; (win7+)&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#zipapp"&gt;zipapp&lt;/a&gt; - another emergency alternative, &lt;a href="https://github.com/9001/copyparty/releases/latest/download/copyparty.pyz"&gt;copyparty.pyz&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#install-on-android"&gt;install on android&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#install-on-iOS"&gt;install on iOS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#reporting-bugs"&gt;reporting bugs&lt;/a&gt; - ideas for context to include, and where to submit them&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#devnotes"&gt;devnotes&lt;/a&gt; - for build instructions etc, see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/devnotes.md"&gt;./docs/devnotes.md&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;quickstart&lt;/h2&gt; 
&lt;p&gt;just run &lt;strong&gt;&lt;a href="https://github.com/9001/copyparty/releases/latest/download/copyparty-sfx.py"&gt;copyparty-sfx.py&lt;/a&gt;&lt;/strong&gt; -- that's it! üéâ&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;‚ÑπÔ∏è the sfx is a &lt;a href="https://github.com/9001/copyparty/issues/270"&gt;self-extractor&lt;/a&gt; which unpacks an embedded &lt;code&gt;tar.gz&lt;/code&gt; into &lt;code&gt;$TEMP&lt;/code&gt; -- if this looks too scary, you can use the &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#zipapp"&gt;zipapp&lt;/a&gt; which has slightly worse performance&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;or install through &lt;a href="https://pypi.org/project/copyparty/"&gt;pypi&lt;/a&gt;: &lt;code&gt;python3 -m pip install --user -U copyparty&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;or if you cannot install python, you can use &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#copypartyexe"&gt;copyparty.exe&lt;/a&gt; instead&lt;/li&gt; 
 &lt;li&gt;or install &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#arch-package"&gt;on arch&lt;/a&gt; / &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#homebrew-formulae"&gt;homebrew&lt;/a&gt; ‚ï± &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#nixos-module"&gt;on NixOS&lt;/a&gt; ‚ï± &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#nix-package"&gt;through nix&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;or if you are on android, &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#install-on-android"&gt;install copyparty in termux&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;or maybe an iPhone or iPad? &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#install-on-iOS"&gt;install in a-Shell on iOS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;or maybe you have a &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/synology-dsm.md"&gt;synology nas / dsm&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;or if you have &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt; installed, run &lt;code&gt;uv tool run copyparty&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;or if your computer is messed up and nothing else works, &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#zipapp"&gt;try the pyz&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;or if your OS is dead, give the &lt;a href="https://a.ocv.me/pub/stuff/edcd001/enterprise-edition/"&gt;bootable flashdrive / cd-rom&lt;/a&gt; a spin&lt;/li&gt; 
 &lt;li&gt;or if you don't trust copyparty yet and want to isolate it a little, then... 
  &lt;ul&gt; 
   &lt;li&gt;...maybe &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/bin/prisonparty.sh"&gt;prisonparty&lt;/a&gt; to create a tiny &lt;a href="https://wiki.archlinux.org/title/Chroot"&gt;chroot&lt;/a&gt; (very portable),&lt;/li&gt; 
   &lt;li&gt;...or &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/bin/bubbleparty.sh"&gt;bubbleparty&lt;/a&gt; to wrap it in &lt;a href="https://github.com/containers/bubblewrap"&gt;bubblewrap&lt;/a&gt; (much better)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;or if you prefer to &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/scripts/docker/"&gt;use docker&lt;/a&gt; üêã you can do that too 
  &lt;ul&gt; 
   &lt;li&gt;docker has all deps built-in, so skip this step:&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;enable thumbnails (images/audio/video), media indexing, and audio transcoding by installing some recommended deps:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Alpine:&lt;/strong&gt; &lt;code&gt;apk add py3-pillow ffmpeg&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Debian:&lt;/strong&gt; &lt;code&gt;apt install --no-install-recommends python3-pil ffmpeg&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Fedora:&lt;/strong&gt; rpmfusion + &lt;code&gt;dnf install python3-pillow ffmpeg --allowerasing&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;FreeBSD:&lt;/strong&gt; &lt;code&gt;pkg install py39-sqlite3 py39-pillow ffmpeg&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MacOS:&lt;/strong&gt; &lt;code&gt;port install py-Pillow ffmpeg&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MacOS&lt;/strong&gt; (alternative): &lt;code&gt;brew install pillow ffmpeg&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Windows:&lt;/strong&gt; &lt;code&gt;python -m pip install --user -U Pillow&lt;/code&gt; 
  &lt;ul&gt; 
   &lt;li&gt;install &lt;a href="https://www.python.org/downloads/windows/"&gt;python&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#optional-dependencies"&gt;ffmpeg&lt;/a&gt; manually; do not use &lt;code&gt;winget&lt;/code&gt; or &lt;code&gt;Microsoft Store&lt;/code&gt; (it breaks $PATH)&lt;/li&gt; 
   &lt;li&gt;copyparty.exe comes with &lt;code&gt;Pillow&lt;/code&gt; and only needs &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#optional-dependencies"&gt;ffmpeg&lt;/a&gt; for mediatags/videothumbs&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#optional-dependencies"&gt;optional dependencies&lt;/a&gt; to enable even more features&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;running copyparty without arguments (for example doubleclicking it on Windows) will give everyone read/write access to the current folder; you may want &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#accounts-and-volumes"&gt;accounts and volumes&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;or see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#complete-examples"&gt;some usage examples&lt;/a&gt; for inspiration, or the &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/examples/windows.md"&gt;complete windows example&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;some recommended options:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;-e2dsa&lt;/code&gt; enables general &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#file-indexing"&gt;file indexing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-e2ts&lt;/code&gt; enables audio metadata indexing (needs either FFprobe or Mutagen)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-v /mnt/music:/music:r:rw,foo -a foo:bar&lt;/code&gt; shares &lt;code&gt;/mnt/music&lt;/code&gt; as &lt;code&gt;/music&lt;/code&gt;, &lt;code&gt;r&lt;/code&gt;eadable by anyone, and read-write for user &lt;code&gt;foo&lt;/code&gt;, password &lt;code&gt;bar&lt;/code&gt; 
  &lt;ul&gt; 
   &lt;li&gt;replace &lt;code&gt;:r:rw,foo&lt;/code&gt; with &lt;code&gt;:r,foo&lt;/code&gt; to only make the folder readable by &lt;code&gt;foo&lt;/code&gt; and nobody else&lt;/li&gt; 
   &lt;li&gt;see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#accounts-and-volumes"&gt;accounts and volumes&lt;/a&gt; (or &lt;a href="https://copyparty.eu/cli/#accounts-help-page"&gt;&lt;code&gt;--help-accounts&lt;/code&gt;&lt;/a&gt;) for the syntax and other permissions&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;mirrors&lt;/h3&gt; 
&lt;p&gt;other places to download copyparty from (non-github links):&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://copyparty.eu/"&gt;https://copyparty.eu/&lt;/a&gt; (hetzner, finland, official mirror): 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://copyparty.eu/py"&gt;https://copyparty.eu/py&lt;/a&gt; = &lt;a href="https://copyparty.eu/copyparty-sfx.py"&gt;https://copyparty.eu/copyparty-sfx.py&lt;/a&gt; = the sfx&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://copyparty.eu/en"&gt;https://copyparty.eu/en&lt;/a&gt; = &lt;a href="https://copyparty.eu/copyparty-en.py"&gt;https://copyparty.eu/copyparty-en.py&lt;/a&gt; = the english-only sfx&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://copyparty.eu/pyz"&gt;https://copyparty.eu/pyz&lt;/a&gt; = &lt;a href="https://copyparty.eu/copyparty.pyz"&gt;https://copyparty.eu/copyparty.pyz&lt;/a&gt; = the zipapp&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://copyparty.eu/enz"&gt;https://copyparty.eu/enz&lt;/a&gt; = &lt;a href="https://copyparty.eu/copyparty-en.pyz"&gt;https://copyparty.eu/copyparty-en.pyz&lt;/a&gt; = the enterprise pyz&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://copyparty.eu/cli"&gt;https://copyparty.eu/cli&lt;/a&gt; = online cli helptext&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;at home&lt;/h3&gt; 
&lt;p&gt;make it accessible over the internet by starting a &lt;a href="https://developers.cloudflare.com/cloudflare-one/connections/connect-networks/do-more-with-tunnels/trycloudflare/"&gt;cloudflare quicktunnel&lt;/a&gt; like so:&lt;/p&gt; 
&lt;p&gt;first download &lt;a href="https://developers.cloudflare.com/cloudflare-one/connections/connect-networks/downloads/"&gt;cloudflared&lt;/a&gt; and then start the tunnel with &lt;code&gt;cloudflared tunnel --url http://127.0.0.1:3923&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;as the tunnel starts, it will show a URL which you can share to let anyone browse your stash or upload files to you&lt;/p&gt; 
&lt;p&gt;but if you have a domain, then you probably want to skip the random autogenerated URL and instead make a &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#permanent-cloudflare-tunnel"&gt;permanent cloudflare tunnel&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;since people will be connecting through cloudflare, run copyparty with &lt;code&gt;--xff-hdr cf-connecting-ip&lt;/code&gt; to detect client IPs correctly&lt;/p&gt; 
&lt;h3&gt;on servers&lt;/h3&gt; 
&lt;p&gt;you may also want these, especially on servers:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/contrib/systemd/copyparty.service"&gt;contrib/systemd/copyparty.service&lt;/a&gt; to run copyparty as a systemd service (see guide inside)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/contrib/systemd/prisonparty.service"&gt;contrib/systemd/prisonparty.service&lt;/a&gt; to run it in a chroot (for extra security)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/contrib/podman-systemd/"&gt;contrib/podman-systemd/&lt;/a&gt; to run copyparty in a Podman container as a systemd service (see guide inside)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/contrib/openrc/copyparty"&gt;contrib/openrc/copyparty&lt;/a&gt; to run copyparty on Alpine / Gentoo&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/contrib/rc/copyparty"&gt;contrib/rc/copyparty&lt;/a&gt; to run copyparty on FreeBSD&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#nixos-module"&gt;nixos module&lt;/a&gt; to run copyparty on NixOS hosts&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/contrib/nginx/copyparty.conf"&gt;contrib/nginx/copyparty.conf&lt;/a&gt; to &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#reverse-proxy"&gt;reverse-proxy&lt;/a&gt; behind nginx (for better https)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;and remember to open the ports you want; here's a complete example including every feature copyparty has to offer:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;firewall-cmd --permanent --add-port={80,443,3921,3922,3923,3945,3990}/tcp  # --zone=libvirt
firewall-cmd --permanent --add-port=12000-12099/tcp  # --zone=libvirt
firewall-cmd --permanent --add-port={69,1900,3969,5353}/udp  # --zone=libvirt
firewall-cmd --reload
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;(69:tftp, 1900:ssdp, 3921:ftp, 3922:sftp, 3923:http/https, 3945:smb, 3969:tftp, 3990:ftps, 5353:mdns, 12000:passive-ftp)&lt;/p&gt; 
&lt;h2&gt;features&lt;/h2&gt; 
&lt;p&gt;also see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/versus.md"&gt;comparison to similar software&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;backend stuff 
  &lt;ul&gt; 
   &lt;li&gt;‚òë IPv6 + unix-sockets&lt;/li&gt; 
   &lt;li&gt;‚òë &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#performance"&gt;multiprocessing&lt;/a&gt; (actual multithreading)&lt;/li&gt; 
   &lt;li&gt;‚òë volumes (mountpoints)&lt;/li&gt; 
   &lt;li&gt;‚òë &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#accounts-and-volumes"&gt;accounts&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;‚òë &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#ftp-server"&gt;ftp server&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;‚òë &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#tftp-server"&gt;tftp server&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;‚òë &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#webdav-server"&gt;webdav server&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;‚òë &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#smb-server"&gt;smb/cifs server&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;‚òë &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#qr-code"&gt;qr-code&lt;/a&gt; for quick access&lt;/li&gt; 
   &lt;li&gt;‚òë &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#zeroconf"&gt;upnp / zeroconf / mdns / ssdp&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;‚òë &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#event-hooks"&gt;event hooks&lt;/a&gt; / script runner&lt;/li&gt; 
   &lt;li&gt;‚òë &lt;a href="https://github.com/9001/copyparty#reverse-proxy"&gt;reverse-proxy support&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;‚òë cross-platform (Windows, Linux, Macos, Android, iOS, FreeBSD, arm32/arm64, ppc64le, s390x, risc-v/riscv64, SGI IRIX)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;upload 
  &lt;ul&gt; 
   &lt;li&gt;‚òë basic: plain multipart, ie6 support&lt;/li&gt; 
   &lt;li&gt;‚òë &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#uploading"&gt;up2k&lt;/a&gt;: js, resumable, multithreaded 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;no filesize limit!&lt;/strong&gt; even on Cloudflare&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;‚òë stash: simple PUT filedropper&lt;/li&gt; 
   &lt;li&gt;‚òë filename randomizer&lt;/li&gt; 
   &lt;li&gt;‚òë write-only folders&lt;/li&gt; 
   &lt;li&gt;‚òë &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#unpost"&gt;unpost&lt;/a&gt;: undo/delete accidental uploads&lt;/li&gt; 
   &lt;li&gt;‚òë &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#self-destruct"&gt;self-destruct&lt;/a&gt; (specified server-side or client-side)&lt;/li&gt; 
   &lt;li&gt;‚òë &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#race-the-beam"&gt;race the beam&lt;/a&gt; (almost like peer-to-peer)&lt;/li&gt; 
   &lt;li&gt;‚òë symlink/discard duplicates (content-matching)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;download 
  &lt;ul&gt; 
   &lt;li&gt;‚òë single files in browser&lt;/li&gt; 
   &lt;li&gt;‚òë &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#zip-downloads"&gt;folders as zip / tar files&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;‚òë &lt;a href="https://github.com/9001/copyparty/tree/hovudstraum/bin#partyfusepy"&gt;FUSE client&lt;/a&gt; (read-only)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;browser 
  &lt;ul&gt; 
   &lt;li&gt;‚òë &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#navpane"&gt;navpane&lt;/a&gt; (directory tree sidebar)&lt;/li&gt; 
   &lt;li&gt;‚òë file manager (cut/paste, delete, &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#batch-rename"&gt;batch-rename&lt;/a&gt;)&lt;/li&gt; 
   &lt;li&gt;‚òë audio player (with &lt;a href="https://user-images.githubusercontent.com/241032/215347492-b4250797-6c90-4e09-9a4c-721edf2fb15c.png"&gt;OS media controls&lt;/a&gt; and opus/mp3 transcoding) 
    &lt;ul&gt; 
     &lt;li&gt;‚òë play video files as audio (converted on server)&lt;/li&gt; 
     &lt;li&gt;‚òë create and play &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#playlists"&gt;m3u8 playlists&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;‚òë image gallery with webm player&lt;/li&gt; 
   &lt;li&gt;‚òë &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#textfile-viewer"&gt;textfile browser&lt;/a&gt; with syntax highlighting 
    &lt;ul&gt; 
     &lt;li&gt;‚òë realtime streaming of growing files (logfiles and such)&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;‚òë &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#thumbnails"&gt;thumbnails&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;‚òë ...of images using Pillow, pyvips, or FFmpeg&lt;/li&gt; 
     &lt;li&gt;‚òë ...of RAW images using rawpy&lt;/li&gt; 
     &lt;li&gt;‚òë ...of videos using FFmpeg&lt;/li&gt; 
     &lt;li&gt;‚òë ...of audio (spectrograms) using FFmpeg&lt;/li&gt; 
     &lt;li&gt;‚òë cache eviction (max-age; maybe max-size eventually)&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;‚òë multilingual UI (english, norwegian, chinese, &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/rice/#translations"&gt;add your own&lt;/a&gt;))&lt;/li&gt; 
   &lt;li&gt;‚òë SPA (browse while uploading)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;server indexing 
  &lt;ul&gt; 
   &lt;li&gt;‚òë &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#file-search"&gt;locate files by contents&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;‚òë search by name/path/date/size&lt;/li&gt; 
   &lt;li&gt;‚òë &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#searching"&gt;search by ID3-tags etc.&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;client support 
  &lt;ul&gt; 
   &lt;li&gt;‚òë &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#folder-sync"&gt;folder sync&lt;/a&gt; (one-way only; full sync will never be supported)&lt;/li&gt; 
   &lt;li&gt;‚òë &lt;a href="https://user-images.githubusercontent.com/241032/215322619-ea5fd606-3654-40ad-94ee-2bc058647bb2.png"&gt;curl-friendly&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;‚òë &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#opengraph"&gt;opengraph&lt;/a&gt; (discord embeds)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;markdown 
  &lt;ul&gt; 
   &lt;li&gt;‚òë &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#markdown-viewer"&gt;viewer&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;‚òë editor (sure why not)&lt;/li&gt; 
   &lt;li&gt;‚òë &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#markdown-vars"&gt;variables&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;PS: something missing? post any crazy ideas you've got as a &lt;a href="https://github.com/9001/copyparty/issues/new?assignees=9001&amp;amp;labels=enhancement&amp;amp;template=feature_request.md"&gt;feature request&lt;/a&gt; or &lt;a href="https://github.com/9001/copyparty/discussions/new?category=ideas"&gt;discussion&lt;/a&gt; ü§ô&lt;/p&gt; 
&lt;h2&gt;testimonials&lt;/h2&gt; 
&lt;p&gt;small collection of user feedback&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;good enough&lt;/code&gt;, &lt;code&gt;surprisingly correct&lt;/code&gt;, &lt;code&gt;certified good software&lt;/code&gt;, &lt;code&gt;just works&lt;/code&gt;, &lt;code&gt;why&lt;/code&gt;, &lt;code&gt;wow this is better than nextcloud&lt;/code&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;UI –ø—Ä–æ—Å—Ç–æ —É–∂–∞—Å–Ω–æ. –ï—Å–ª–∏ –±—É–¥—É –æ–ø–∏—Å—ã–≤–∞—Ç—å –¥–µ—Ç–∞–ª—å–Ω–æ –Ω–µ —Å–º–æ–≥—É —É–¥–µ—Ä–∂–∞—Ç—å—Å—è –≤ —Ä–∞–º–∫–∞—Ö –ø—Ä–∏–ª–∏—á–∏–π&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;motivations&lt;/h1&gt; 
&lt;p&gt;project goals / philosophy&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;inverse unix philosophy -- do all the things, and do an &lt;em&gt;okay&lt;/em&gt; job 
  &lt;ul&gt; 
   &lt;li&gt;quick drop-in service to get a lot of features in a pinch&lt;/li&gt; 
   &lt;li&gt;some of &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/versus.md"&gt;the alternatives&lt;/a&gt; might be a better fit for you&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;run anywhere, support everything 
  &lt;ul&gt; 
   &lt;li&gt;as many web-browsers and python versions as possible 
    &lt;ul&gt; 
     &lt;li&gt;every browser should at least be able to browse, download, upload files&lt;/li&gt; 
     &lt;li&gt;be a good emergency solution for transferring stuff between ancient boxes&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;minimal dependencies 
    &lt;ul&gt; 
     &lt;li&gt;but optional dependencies adding bonus-features are ok&lt;/li&gt; 
     &lt;li&gt;everything being plaintext makes it possible to proofread for malicious code&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;no preparations / setup necessary, just run the sfx (which is also plaintext)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;adaptable, malleable, hackable 
  &lt;ul&gt; 
   &lt;li&gt;no build steps; modify the js/python without needing node.js or anything like that&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;becoming rich is specifically &lt;em&gt;not&lt;/em&gt; a motivation, but if you wanna donate then see my &lt;a href="https://github.com/9001"&gt;github profile&lt;/a&gt; regarding donations for my FOSS stuff in general (also THANKS!)&lt;/p&gt; 
&lt;h2&gt;notes&lt;/h2&gt; 
&lt;p&gt;general notes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;paper-printing is affected by dark/light-mode! use lightmode for color, darkmode for grayscale 
  &lt;ul&gt; 
   &lt;li&gt;because no browsers currently implement the media-query to do this properly orz&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;browser-specific:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;iPhone/iPad: use Firefox to download files&lt;/li&gt; 
 &lt;li&gt;Android-Chrome: increase "parallel uploads" for higher speed (android bug)&lt;/li&gt; 
 &lt;li&gt;Android-Firefox: takes a while to select files (their fix for ‚òùÔ∏è)&lt;/li&gt; 
 &lt;li&gt;Desktop-Firefox: &lt;del&gt;may use gigabytes of RAM if your files are massive&lt;/del&gt; &lt;em&gt;seems to be OK now&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;Desktop-Firefox: &lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1792598"&gt;may stop you from unplugging USB flashdrives&lt;/a&gt; until you visit &lt;code&gt;about:memory&lt;/code&gt; and click &lt;code&gt;Minimize memory usage&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;server-os-specific:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;RHEL8 / Rocky8: you can run copyparty using &lt;code&gt;/usr/libexec/platform-python&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;server notes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;pypy is supported but regular cpython is faster if you enable the database&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;bugs&lt;/h1&gt; 
&lt;p&gt;roughly sorted by chance of encounter&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;general:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;--th-ff-jpg&lt;/code&gt; may fix video thumbnails on some FFmpeg versions (macos, some linux)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;--th-ff-swr&lt;/code&gt; may fix audio thumbnails on some FFmpeg versions&lt;/li&gt; 
   &lt;li&gt;if the &lt;code&gt;up2k.db&lt;/code&gt; (filesystem index) is on a samba-share or network disk, you'll get unpredictable behavior if the share is disconnected for a bit 
    &lt;ul&gt; 
     &lt;li&gt;use &lt;code&gt;--hist&lt;/code&gt; or the &lt;code&gt;hist&lt;/code&gt; volflag (&lt;code&gt;-v [...]:c,hist=/tmp/foo&lt;/code&gt;) to place the db and thumbnails on a local disk instead&lt;/li&gt; 
     &lt;li&gt;or, if you only want to move the db (and not the thumbnails), then use &lt;code&gt;--dbpath&lt;/code&gt; or the &lt;code&gt;dbpath&lt;/code&gt; volflag&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;all volumes must exist / be available on startup; up2k (mtp especially) gets funky otherwise&lt;/li&gt; 
   &lt;li&gt;probably more, pls let me know&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;python 3.4 and older (including 2.7):&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;many rare and exciting edge-cases because &lt;a href="https://peps.python.org/pep-0475/"&gt;python didn't handle EINTR yet&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;downloads from copyparty may suddenly fail, but uploads &lt;em&gt;should&lt;/em&gt; be fine&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;python 2.7 on Windows:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;cannot index non-ascii filenames with &lt;code&gt;-e2d&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;cannot handle filenames with mojibake&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;if you have a new exciting bug to share, see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#reporting-bugs"&gt;reporting bugs&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;not my bugs&lt;/h2&gt; 
&lt;p&gt;same order here too&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://bugs.chromium.org/p/chromium/issues/detail?id=1317069"&gt;Chrome issue 1317069&lt;/a&gt; -- if you try to upload a folder which contains symlinks by dragging it into the browser, the symlinked files will not get uploaded&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://bugs.chromium.org/p/chromium/issues/detail?id=1352210"&gt;Chrome issue 1352210&lt;/a&gt; -- plaintext http may be faster at filehashing than https (but also extremely CPU-intensive)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://issues.chromium.org/issues/383568268"&gt;Chrome issue 383568268&lt;/a&gt; -- filereaders in webworkers can OOM / crash the browser-tab&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;copyparty has a workaround which seems to work well enough&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1790500"&gt;Firefox issue 1790500&lt;/a&gt; -- entire browser can crash after uploading ~4000 small files&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Android: music playback randomly stops due to &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#fix-unreliable-playback-on-android"&gt;battery usage settings&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;iPhones: the volume control doesn't work because &lt;a href="https://developer.apple.com/library/archive/documentation/AudioVideo/Conceptual/Using_HTML5_Audio_Video/Device-SpecificConsiderations/Device-SpecificConsiderations.html#//apple_ref/doc/uid/TP40009523-CH5-SW11"&gt;apple doesn't want it to&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;AudioContext&lt;/code&gt; will probably never be a viable workaround as apple introduces new issues faster than they fix current ones&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;iPhones: music volume goes on a rollercoaster during song changes&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;nothing I can do about it because &lt;code&gt;AudioContext&lt;/code&gt; is still broken in safari&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;iPhones: the preload feature (in the media-player-options tab) can cause a tiny audio glitch 20sec before the end of each song, but disabling it may cause worse iOS bugs to appear instead&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;just a hunch, but disabling preloading may cause playback to stop entirely, or possibly mess with bluetooth speakers&lt;/li&gt; 
   &lt;li&gt;tried to add a tooltip regarding this but looks like apple broke my tooltips&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;iPhones: preloaded awo files make safari log MEDIA_ERR_NETWORK errors as playback starts, but the song plays just fine so eh whatever&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;awo, opus-weba, is apple's new take on opus support, replacing opus-caf which was technically limited to cbr opus&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;iPhones: preloading another awo file may cause playback to stop&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;can be somewhat mitigated with &lt;code&gt;mp.au.play()&lt;/code&gt; in &lt;code&gt;mp.onpreload&lt;/code&gt; but that can hit a race condition in safari that starts playing the same audio object twice in parallel...&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Windows: folders cannot be accessed if the name ends with &lt;code&gt;.&lt;/code&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;python or windows bug&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Windows: msys2-python 3.8.6 occasionally throws &lt;code&gt;RuntimeError: release unlocked lock&lt;/code&gt; when leaving a scoped mutex in up2k&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;this is an msys2 bug, the regular windows edition of python is fine&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;VirtualBox: sqlite throws &lt;code&gt;Disk I/O Error&lt;/code&gt; when running in a VM and the up2k database is in a vboxsf&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;use &lt;code&gt;--hist&lt;/code&gt; or the &lt;code&gt;hist&lt;/code&gt; volflag (&lt;code&gt;-v [...]:c,hist=/tmp/foo&lt;/code&gt;) to place the db and thumbnails inside the vm instead 
    &lt;ul&gt; 
     &lt;li&gt;or, if you only want to move the db (and not the thumbnails), then use &lt;code&gt;--dbpath&lt;/code&gt; or the &lt;code&gt;dbpath&lt;/code&gt; volflag&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;also happens on mergerfs, so put the db elsewhere&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Ubuntu: dragging files from certain folders into firefox or chrome is impossible&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;due to snap security policies -- see &lt;code&gt;snap connections firefox&lt;/code&gt; for the allowlist, &lt;code&gt;removable-media&lt;/code&gt; permits all of &lt;code&gt;/mnt&lt;/code&gt; and &lt;code&gt;/media&lt;/code&gt; apparently&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;breaking changes&lt;/h1&gt; 
&lt;p&gt;upgrade notes&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;1.9.16&lt;/code&gt; (2023-11-04): 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;--stats&lt;/code&gt;/prometheus: &lt;code&gt;cpp_bans&lt;/code&gt; renamed to &lt;code&gt;cpp_active_bans&lt;/code&gt;, and that + &lt;code&gt;cpp_uptime&lt;/code&gt; are gauges&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;1.6.0&lt;/code&gt; (2023-01-29): 
  &lt;ul&gt; 
   &lt;li&gt;http-api: delete/move is now &lt;code&gt;POST&lt;/code&gt; instead of &lt;code&gt;GET&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;everything other than &lt;code&gt;GET&lt;/code&gt; and &lt;code&gt;HEAD&lt;/code&gt; must pass &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#cors"&gt;cors validation&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;1.5.0&lt;/code&gt; (2022-12-03): &lt;a href="https://github.com/9001/copyparty/commit/54e1c8d261df"&gt;new chunksize formula&lt;/a&gt; for files larger than 128 GiB 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;users:&lt;/strong&gt; upgrade to the latest &lt;a href="https://github.com/9001/copyparty/raw/hovudstraum/bin/u2c.py"&gt;cli uploader&lt;/a&gt; if you use that&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;devs:&lt;/strong&gt; update third-party up2k clients (if those even exist)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;FAQ&lt;/h1&gt; 
&lt;p&gt;"frequently" asked questions&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;CopyParty?&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;nope! the name is either copyparty (all-lowercase) or Copyparty -- it's &lt;a href="https://en.wiktionary.org/wiki/copyparty"&gt;one word&lt;/a&gt; after all :&amp;gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;what is a volflag?&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;per-volume configuration; many (not all) global-options can be set as volflags, and most (not all) volflags can be set as global-options; &lt;a href="https://copyparty.eu/cli/#flags-help-page"&gt;complete list of volflags&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;what is a volume?&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;a mapping from a URL (&lt;code&gt;/music/&lt;/code&gt;) to a folder on your server's local filesystem (&lt;code&gt;C:\Users\ed\Music&lt;/code&gt;) which can then be accessed through copyparty, depending on the permissions and options you set on it -- see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#accounts-and-volumes"&gt;accounts and volumes&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;can I change the üå≤ spinning pine-tree loading animation?&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://github.com/9001/copyparty/tree/hovudstraum/docs/rice#boring-loader-spinner"&gt;yeah...&lt;/a&gt; :-(&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;is it possible to block read-access to folders unless you know the exact URL for a particular file inside?&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;yes, using the &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#accounts-and-volumes"&gt;&lt;code&gt;g&lt;/code&gt; permission&lt;/a&gt;, see the examples there&lt;/li&gt; 
   &lt;li&gt;you can also do this with linux filesystem permissions; &lt;code&gt;chmod 111 music&lt;/code&gt; will make it possible to access files and folders inside the &lt;code&gt;music&lt;/code&gt; folder but not list the immediate contents -- also works with other software, not just copyparty&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;can I link someone to a password-protected volume/file by including the password in the URL?&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;yes, by adding &lt;code&gt;?pw=hunter2&lt;/code&gt; to the end; replace &lt;code&gt;?&lt;/code&gt; with &lt;code&gt;&amp;amp;&lt;/code&gt; if there are parameters in the URL already, meaning it contains a &lt;code&gt;?&lt;/code&gt; near the end 
    &lt;ul&gt; 
     &lt;li&gt;if you have enabled &lt;code&gt;--usernames&lt;/code&gt; then do &lt;code&gt;?pw=username:password&lt;/code&gt; instead&lt;/li&gt; 
     &lt;li&gt;&lt;code&gt;?pw&lt;/code&gt; can be disabled with &lt;code&gt;--pw-urlp=A&lt;/code&gt; but this breaks support for many clients&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;how do I stop &lt;code&gt;.hist&lt;/code&gt; folders from appearing everywhere on my HDD?&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;by default, a &lt;code&gt;.hist&lt;/code&gt; folder is created inside each volume for the filesystem index, thumbnails, audio transcodes, and markdown document history. Use the &lt;code&gt;--hist&lt;/code&gt; global-option or the &lt;code&gt;hist&lt;/code&gt; volflag to move it somewhere else; see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#database-location"&gt;database location&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;can I make copyparty download a file to my server if I give it a URL?&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;yes, using &lt;a href="https://github.com/9001/copyparty/raw/hovudstraum/bin/hooks/wget.py"&gt;hooks&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;firefox refuses to connect over https, saying "Secure Connection Failed" or "SEC_ERROR_BAD_SIGNATURE", but the usual button to "Accept the Risk and Continue" is not shown&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;firefox has corrupted its certstore; fix this by exiting firefox, then find and delete the file named &lt;code&gt;cert9.db&lt;/code&gt; somewhere in your firefox profile folder&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;the server keeps saying &lt;code&gt;thank you for playing&lt;/code&gt; when I try to access the website&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;you've gotten banned for malicious traffic! if this happens by mistake, and you're running a reverse-proxy and/or something like cloudflare, see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#real-ip"&gt;real-ip&lt;/a&gt; on how to fix this&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;copyparty seems to think I am using http, even though the URL is https&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;your reverse-proxy is not sending the &lt;code&gt;X-Forwarded-Proto: https&lt;/code&gt; header; this could be because your reverse-proxy itself is confused. Ensure that none of the intermediates (such as cloudflare) are terminating https before the traffic hits your entrypoint&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;thumbnails are broken (you get a colorful square which says the filetype instead)&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;you need to install &lt;code&gt;FFmpeg&lt;/code&gt; or &lt;code&gt;Pillow&lt;/code&gt;; see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#thumbnails"&gt;thumbnails&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;thumbnails are broken (some images appear, but other files just get a blank box, and/or the broken-image placeholder)&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;probably due to a reverse-proxy messing with the request URLs and stripping the query parameters (&lt;code&gt;?th=w&lt;/code&gt;), so check your URL rewrite rules&lt;/li&gt; 
   &lt;li&gt;could also be due to incorrect caching settings in reverse-proxies and/or CDNs, so make sure that nothing is set to ignore the query string&lt;/li&gt; 
   &lt;li&gt;could also be due to misbehaving privacy-related browser extensions, so try to disable those&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;i want to learn python and/or programming and am considering looking at the copyparty source code in that occasion&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;pre&gt;&lt;code class="language-bash"&gt; _|  _      __   _  _|_
(_| (_)     | | (_)  |_
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;accounts and volumes&lt;/h1&gt; 
&lt;p&gt;per-folder, per-user permissions - if your setup is getting complex, consider making a &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/example.conf"&gt;config file&lt;/a&gt; instead of using arguments&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;much easier to manage, and you can modify the config at runtime with &lt;code&gt;systemctl reload copyparty&lt;/code&gt; or more conveniently using the &lt;code&gt;[reload cfg]&lt;/code&gt; button in the control-panel (if the user has &lt;code&gt;a&lt;/code&gt;/admin in any volume) 
  &lt;ul&gt; 
   &lt;li&gt;changes to the &lt;code&gt;[global]&lt;/code&gt; config section requires a restart to take effect&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;a quick summary can be seen using &lt;a href="https://copyparty.eu/cli/#accounts-help-page"&gt;&lt;code&gt;--help-accounts&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;configuring accounts/volumes with arguments:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;-a usr:pwd&lt;/code&gt; adds account &lt;code&gt;usr&lt;/code&gt; with password &lt;code&gt;pwd&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-v .::r&lt;/code&gt; adds current-folder &lt;code&gt;.&lt;/code&gt; as the webroot, &lt;code&gt;r&lt;/code&gt;eadable by anyone 
  &lt;ul&gt; 
   &lt;li&gt;the syntax is &lt;code&gt;-v src:dst:perm:perm:...&lt;/code&gt; so local-path, url-path, and one or more permissions to set&lt;/li&gt; 
   &lt;li&gt;granting the same permissions to multiple accounts:&lt;br /&gt; &lt;code&gt;-v .::r,usr1,usr2:rw,usr3,usr4&lt;/code&gt; = usr1/2 read-only, 3/4 read-write&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;permissions:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;r&lt;/code&gt; (read): browse folder contents, download files, download as zip/tar, see filekeys/dirkeys&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;w&lt;/code&gt; (write): upload files, move/copy files &lt;em&gt;into&lt;/em&gt; this folder&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;m&lt;/code&gt; (move): move files/folders &lt;em&gt;from&lt;/em&gt; this folder&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;d&lt;/code&gt; (delete): delete files/folders&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;.&lt;/code&gt; (dots): user can ask to show dotfiles in directory listings&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;g&lt;/code&gt; (get): only download files, cannot see folder contents or zip/tar&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;G&lt;/code&gt; (upget): same as &lt;code&gt;g&lt;/code&gt; except uploaders get to see their own &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#filekeys"&gt;filekeys&lt;/a&gt; (see &lt;code&gt;fk&lt;/code&gt; in examples below)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;h&lt;/code&gt; (html): same as &lt;code&gt;g&lt;/code&gt; except folders return their index.html, and filekeys are not necessary for index.html&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;a&lt;/code&gt; (admin): can see upload time, uploader IPs, config-reload&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;A&lt;/code&gt; ("all"): same as &lt;code&gt;rwmda.&lt;/code&gt; (read/write/move/delete/admin/dotfiles)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;examples:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;add accounts named u1, u2, u3 with passwords p1, p2, p3: &lt;code&gt;-a u1:p1 -a u2:p2 -a u3:p3&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;make folder &lt;code&gt;/srv&lt;/code&gt; the root of the filesystem, read-only by anyone: &lt;code&gt;-v /srv::r&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;make folder &lt;code&gt;/mnt/music&lt;/code&gt; available at &lt;code&gt;/music&lt;/code&gt;, read-only for u1 and u2, read-write for u3: &lt;code&gt;-v /mnt/music:music:r,u1,u2:rw,u3&lt;/code&gt; 
  &lt;ul&gt; 
   &lt;li&gt;unauthorized users accessing the webroot can see that the &lt;code&gt;music&lt;/code&gt; folder exists, but cannot open it&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;make folder &lt;code&gt;/mnt/incoming&lt;/code&gt; available at &lt;code&gt;/inc&lt;/code&gt;, write-only for u1, read-move for u2: &lt;code&gt;-v /mnt/incoming:inc:w,u1:rm,u2&lt;/code&gt; 
  &lt;ul&gt; 
   &lt;li&gt;unauthorized users accessing the webroot can see that the &lt;code&gt;inc&lt;/code&gt; folder exists, but cannot open it&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;u1&lt;/code&gt; can open the &lt;code&gt;inc&lt;/code&gt; folder, but cannot see the contents, only upload new files to it&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;u2&lt;/code&gt; can browse it and move files &lt;em&gt;from&lt;/em&gt; &lt;code&gt;/inc&lt;/code&gt; into any folder where &lt;code&gt;u2&lt;/code&gt; has write-access&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;make folder &lt;code&gt;/mnt/ss&lt;/code&gt; available at &lt;code&gt;/i&lt;/code&gt;, read-write for u1, get-only for everyone else, and enable filekeys: &lt;code&gt;-v /mnt/ss:i:rw,u1:g:c,fk=4&lt;/code&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;c,fk=4&lt;/code&gt; sets the &lt;code&gt;fk&lt;/code&gt; (&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#filekeys"&gt;filekey&lt;/a&gt;) volflag to 4, meaning each file gets a 4-character accesskey&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;u1&lt;/code&gt; can upload files, browse the folder, and see the generated filekeys&lt;/li&gt; 
   &lt;li&gt;other users cannot browse the folder, but can access the files if they have the full file URL with the filekey&lt;/li&gt; 
   &lt;li&gt;replacing the &lt;code&gt;g&lt;/code&gt; permission with &lt;code&gt;wg&lt;/code&gt; would let anonymous users upload files, but not see the required filekey to access it&lt;/li&gt; 
   &lt;li&gt;replacing the &lt;code&gt;g&lt;/code&gt; permission with &lt;code&gt;wG&lt;/code&gt; would let anonymous users upload files, receiving a working direct link in return&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;if you want to grant access to all users who are logged in, the group &lt;code&gt;acct&lt;/code&gt; will always contain all known users, so for example &lt;code&gt;-v /mnt/music:music:r,@acct&lt;/code&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;to do the opposite, granting access to everyone who is NOT logged in. &lt;code&gt;*,-@acct&lt;/code&gt; does the trick, for example &lt;code&gt;-v /srv/welcome:welcome:r,*,-@acct&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;single users can also be subtracted from a group: &lt;code&gt;@admins,-james&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;anyone trying to bruteforce a password gets banned according to &lt;code&gt;--ban-pw&lt;/code&gt;; default is 24h ban for 9 failed attempts in 1 hour&lt;/p&gt; 
&lt;p&gt;and if you want to use config files instead of commandline args (good!) then here's the same examples as a configfile; save it as &lt;code&gt;foobar.conf&lt;/code&gt; and use it like this: &lt;code&gt;python copyparty-sfx.py -c foobar.conf&lt;/code&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;you can also &lt;code&gt;PRTY_CONFIG=foobar.conf python copyparty-sfx.py&lt;/code&gt; (convenient in docker etc)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;[accounts]
  u1: p1  # create account "u1" with password "p1"
  u2: p2  #  (note that comments must have
  u3: p3  #   two spaces before the # sign)

[groups]
  g1: u1, u2  # create a group

[/]     # this URL will be mapped to...
  /srv  # ...this folder on the server filesystem
  accs:
    r: *  # read-only for everyone, no account necessary

[/music]       # create another volume at this URL,
  /mnt/music   # which is mapped to this folder
  accs:
    r: u1, u2  # only these accounts can read,
    r: @g1     # (exactly the same, just with a group instead)
    r: @acct   # (alternatively, ALL users who are logged in)
    rw: u3     # and only u3 can read-write

[/inc]
  /mnt/incoming
  accs:
    w: u1   # u1 can upload but not see/download any files,
    rm: u2  # u2 can browse + move files out of this volume

[/i]
  /mnt/ss
  accs:
    rw: u1  # u1 can read-write,
    g: *    # everyone can access files if they know the URL
  flags:
    fk: 4   # each file URL will have a 4-character password
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;shadowing&lt;/h2&gt; 
&lt;p&gt;hiding specific subfolders by mounting another volume on top of them&lt;/p&gt; 
&lt;p&gt;for example &lt;code&gt;-v /mnt::r -v /var/empty:web/certs:r&lt;/code&gt; mounts the server folder &lt;code&gt;/mnt&lt;/code&gt; as the webroot, but another volume is mounted at &lt;code&gt;/web/certs&lt;/code&gt; -- so visitors can only see the contents of &lt;code&gt;/mnt&lt;/code&gt; and &lt;code&gt;/mnt/web&lt;/code&gt; (at URLs &lt;code&gt;/&lt;/code&gt; and &lt;code&gt;/web&lt;/code&gt;), but not &lt;code&gt;/mnt/web/certs&lt;/code&gt; because URL &lt;code&gt;/web/certs&lt;/code&gt; is mapped to &lt;code&gt;/var/empty&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;the example config file right above this section may explain this better; the first volume &lt;code&gt;/&lt;/code&gt; is mapped to &lt;code&gt;/srv&lt;/code&gt; which means &lt;a href="http://127.0.0.1:3923/music"&gt;http://127.0.0.1:3923/music&lt;/a&gt; would try to read &lt;code&gt;/srv/music&lt;/code&gt; on the server filesystem, but since there's another volume at &lt;code&gt;/music&lt;/code&gt; mapped to &lt;code&gt;/mnt/music&lt;/code&gt; then it'll go to &lt;code&gt;/mnt/music&lt;/code&gt; instead&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;‚ÑπÔ∏è this also works for single files, because files can also be volumes&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;dotfiles&lt;/h2&gt; 
&lt;p&gt;unix-style hidden files/folders by starting the name with a dot&lt;/p&gt; 
&lt;p&gt;anyone can access these if they know the name, but they normally don't appear in directory listings&lt;/p&gt; 
&lt;p&gt;a client can request to see dotfiles in directory listings if global option &lt;code&gt;-ed&lt;/code&gt; is specified, or the volume has volflag &lt;code&gt;dots&lt;/code&gt;, or the user has permission &lt;code&gt;.&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;dotfiles do not appear in search results unless one of the above is true, &lt;strong&gt;and&lt;/strong&gt; the global option / volflag &lt;code&gt;dotsrch&lt;/code&gt; is set&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;even if user has permission to see dotfiles, they are default-hidden unless &lt;code&gt;--see-dots&lt;/code&gt; is set, and/or user has enabled the &lt;code&gt;dotfiles&lt;/code&gt; option in the settings tab&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;config file example, where the same permission to see dotfiles is given in two different ways just for reference:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;[/foo]
  /srv/foo
  accs:
    r.: ed   # user "ed" has read-access + dot-access in this volume;
             # dotfiles are visible in listings, but not in searches
  flags:
    dotsrch  # dotfiles will now appear in search results too
    dots     # another way to let everyone see dotfiles in this vol
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;the browser&lt;/h1&gt; 
&lt;p&gt;accessing a copyparty server using a web-browser&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/241032/192042695-522b3ec7-6845-494a-abdb-d1c0d0e23801.png" alt="copyparty-browser-fs8" /&gt;&lt;/p&gt; 
&lt;h2&gt;tabs&lt;/h2&gt; 
&lt;p&gt;the main tabs in the ui&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;[üîé]&lt;/code&gt; &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#searching"&gt;search&lt;/a&gt; by size, date, path/name, mp3-tags ...&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[üßØ]&lt;/code&gt; &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#unpost"&gt;unpost&lt;/a&gt;: undo/delete accidental uploads&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[üöÄ]&lt;/code&gt; and &lt;code&gt;[üéà]&lt;/code&gt; are the &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#uploading"&gt;uploaders&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[üìÇ]&lt;/code&gt; mkdir: create directories&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[üìù]&lt;/code&gt; new-file: create a new textfile&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[üìü]&lt;/code&gt; send-msg: either to server-log or into textfiles if &lt;code&gt;--urlform save&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[üé∫]&lt;/code&gt; audio-player config options&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[‚öôÔ∏è]&lt;/code&gt; general client config options&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;hotkeys&lt;/h2&gt; 
&lt;p&gt;the browser has the following hotkeys (always qwerty)&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;?&lt;/code&gt; show hotkeys help&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;B&lt;/code&gt; toggle breadcrumbs / &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#navpane"&gt;navpane&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;I/K&lt;/code&gt; prev/next folder&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;M&lt;/code&gt; parent folder (or unexpand current)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;V&lt;/code&gt; toggle folders / textfiles in the navpane&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;G&lt;/code&gt; toggle list / &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#thumbnails"&gt;grid view&lt;/a&gt; -- same as &lt;code&gt;Áî∞&lt;/code&gt; bottom-right&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;T&lt;/code&gt; toggle thumbnails / icons&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ESC&lt;/code&gt; close various things&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ctrl-K&lt;/code&gt; delete selected files/folders&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ctrl-X&lt;/code&gt; cut selected files/folders&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ctrl-C&lt;/code&gt; copy selected files/folders to clipboard&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ctrl-V&lt;/code&gt; paste (move/copy)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Y&lt;/code&gt; download selected files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;F2&lt;/code&gt; &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#batch-rename"&gt;rename&lt;/a&gt; selected file/folder&lt;/li&gt; 
 &lt;li&gt;when a file/folder is selected (in not-grid-view): 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;Up/Down&lt;/code&gt; move cursor&lt;/li&gt; 
   &lt;li&gt;shift+&lt;code&gt;Up/Down&lt;/code&gt; select and move cursor&lt;/li&gt; 
   &lt;li&gt;ctrl+&lt;code&gt;Up/Down&lt;/code&gt; move cursor and scroll viewport&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;Space&lt;/code&gt; toggle file selection&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;Ctrl-A&lt;/code&gt; toggle select all&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;when a textfile is open: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;I/K&lt;/code&gt; prev/next textfile&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;S&lt;/code&gt; toggle selection of open file&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;M&lt;/code&gt; close textfile&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;when playing audio: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;J/L&lt;/code&gt; prev/next song&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;U/O&lt;/code&gt; skip 10sec back/forward&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;0..9&lt;/code&gt; jump to 0%..90%&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;P&lt;/code&gt; play/pause (also starts playing the folder)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;Y&lt;/code&gt; download file&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;when viewing images / playing videos: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;J/L, Left/Right&lt;/code&gt; prev/next file&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;Home/End&lt;/code&gt; first/last file&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;F&lt;/code&gt; toggle fullscreen&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;S&lt;/code&gt; toggle selection&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;R&lt;/code&gt; rotate clockwise (shift=ccw)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;Y&lt;/code&gt; download file&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;Esc&lt;/code&gt; close viewer&lt;/li&gt; 
   &lt;li&gt;videos: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;code&gt;U/O&lt;/code&gt; skip 10sec back/forward&lt;/li&gt; 
     &lt;li&gt;&lt;code&gt;0..9&lt;/code&gt; jump to 0%..90%&lt;/li&gt; 
     &lt;li&gt;&lt;code&gt;P/K/Space&lt;/code&gt; play/pause&lt;/li&gt; 
     &lt;li&gt;&lt;code&gt;M&lt;/code&gt; mute&lt;/li&gt; 
     &lt;li&gt;&lt;code&gt;C&lt;/code&gt; continue playing next video&lt;/li&gt; 
     &lt;li&gt;&lt;code&gt;V&lt;/code&gt; loop entire file&lt;/li&gt; 
     &lt;li&gt;&lt;code&gt;[&lt;/code&gt; loop range (start)&lt;/li&gt; 
     &lt;li&gt;&lt;code&gt;]&lt;/code&gt; loop range (end)&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;when the navpane is open: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;A/D&lt;/code&gt; adjust tree width&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;in the &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#thumbnails"&gt;grid view&lt;/a&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;S&lt;/code&gt; toggle multiselect&lt;/li&gt; 
   &lt;li&gt;shift+&lt;code&gt;A/D&lt;/code&gt; zoom&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;in the markdown editor: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;^s&lt;/code&gt; save&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;^h&lt;/code&gt; header&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;^k&lt;/code&gt; autoformat table&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;^u&lt;/code&gt; jump to next unicode character&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;^e&lt;/code&gt; toggle editor / preview&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;^up, ^down&lt;/code&gt; jump paragraphs&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;navpane&lt;/h2&gt; 
&lt;p&gt;switching between breadcrumbs or navpane&lt;/p&gt; 
&lt;p&gt;click the &lt;code&gt;üå≤&lt;/code&gt; or pressing the &lt;code&gt;B&lt;/code&gt; hotkey to toggle between breadcrumbs path (default), or a navpane (tree-browser sidebar thing)&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;[+]&lt;/code&gt; and &lt;code&gt;[-]&lt;/code&gt; (or hotkeys &lt;code&gt;A&lt;/code&gt;/&lt;code&gt;D&lt;/code&gt;) adjust the size&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[üéØ]&lt;/code&gt; jumps to the currently open folder&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[üìÉ]&lt;/code&gt; toggles between showing folders and textfiles&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[üìå]&lt;/code&gt; shows the name of all parent folders in a docked panel&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[a]&lt;/code&gt; toggles automatic widening as you go deeper&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[‚Üµ]&lt;/code&gt; toggles wordwrap&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[üëÄ]&lt;/code&gt; show full name on hover (if wordwrap is off)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;thumbnails&lt;/h2&gt; 
&lt;p&gt;press &lt;code&gt;g&lt;/code&gt; or &lt;code&gt;Áî∞&lt;/code&gt; to toggle grid-view instead of the file listing and &lt;code&gt;t&lt;/code&gt; toggles icons / thumbnails&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;can be made default globally with &lt;code&gt;--grid&lt;/code&gt; or per-volume with volflag &lt;code&gt;grid&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;enable by adding &lt;code&gt;?imgs&lt;/code&gt; to a link, or disable with &lt;code&gt;?imgs=0&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/241032/129636211-abd20fa2-a953-4366-9423-1c88ebb96ba9.png" alt="copyparty-thumbs-fs8" /&gt;&lt;/p&gt; 
&lt;p&gt;it does static images with Pillow / pyvips / FFmpeg, and uses FFmpeg for video files, so you may want to &lt;code&gt;--no-thumb&lt;/code&gt; or maybe just &lt;code&gt;--no-vthumb&lt;/code&gt; depending on how dangerous your users are&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;pyvips is 3x faster than Pillow, Pillow is 3x faster than FFmpeg&lt;/li&gt; 
 &lt;li&gt;disable thumbnails for specific volumes with volflag &lt;code&gt;dthumb&lt;/code&gt; for all, or &lt;code&gt;dvthumb&lt;/code&gt; / &lt;code&gt;dathumb&lt;/code&gt; / &lt;code&gt;dithumb&lt;/code&gt; for video/audio/images only&lt;/li&gt; 
 &lt;li&gt;for installing FFmpeg on windows, see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#optional-dependencies"&gt;optional dependencies&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;audio files are converted into spectrograms using FFmpeg unless you &lt;code&gt;--no-athumb&lt;/code&gt; (and some FFmpeg builds may need &lt;code&gt;--th-ff-swr&lt;/code&gt;)&lt;/p&gt; 
&lt;p&gt;images with the following names (see &lt;code&gt;--th-covers&lt;/code&gt;) become the thumbnail of the folder they're in: &lt;code&gt;folder.png&lt;/code&gt;, &lt;code&gt;folder.jpg&lt;/code&gt;, &lt;code&gt;cover.png&lt;/code&gt;, &lt;code&gt;cover.jpg&lt;/code&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;the order is significant, so if both &lt;code&gt;cover.png&lt;/code&gt; and &lt;code&gt;folder.jpg&lt;/code&gt; exist in a folder, it will pick the first matching &lt;code&gt;--th-covers&lt;/code&gt; entry (&lt;code&gt;folder.jpg&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;and, if you enable &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#file-indexing"&gt;file indexing&lt;/a&gt;, it will also try those names as dotfiles (&lt;code&gt;.folder.jpg&lt;/code&gt; and so), and then fallback on the first picture in the folder (if it has any pictures at all)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;enabling &lt;code&gt;multiselect&lt;/code&gt; lets you click files to select them, and then shift-click another file for range-select&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;multiselect&lt;/code&gt; is mostly intended for phones/tablets, but the &lt;code&gt;sel&lt;/code&gt; option in the &lt;code&gt;[‚öôÔ∏è] settings&lt;/code&gt; tab is better suited for desktop use, allowing selection by CTRL-clicking and range-selection with SHIFT-click, all without affecting regular clicking 
  &lt;ul&gt; 
   &lt;li&gt;the &lt;code&gt;sel&lt;/code&gt; option can be made default globally with &lt;code&gt;--gsel&lt;/code&gt; or per-volume with volflag &lt;code&gt;gsel&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;to show &lt;code&gt;/icons/exe.png&lt;/code&gt; and &lt;code&gt;/icons/elf.gif&lt;/code&gt; as the thumbnail for all &lt;code&gt;.exe&lt;/code&gt; and &lt;code&gt;.elf&lt;/code&gt; files respectively, do this: &lt;code&gt;--ext-th=exe=/icons/exe.png --ext-th=elf=/icons/elf.gif&lt;/code&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;optionally as separate volflags for each mapping; see config file example below&lt;/li&gt; 
 &lt;li&gt;the supported image formats are &lt;a href="https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Image_types"&gt;jpg, png, gif, webp, ico&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;be careful with svg; chrome will crash if you have too many unique svg files showing on the same page (the limit is 250 or so) -- showing the same handful of svg files thousands of times is ok however&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;note:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;heif/heifs/heic/heics images usually require the &lt;code&gt;libvips&lt;/code&gt; &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#optional-dependencies"&gt;optional dependency&lt;/a&gt; (available in the &lt;code&gt;iv&lt;/code&gt; docker image, &lt;code&gt;withFastThumbnails&lt;/code&gt; in nixos) 
  &lt;ul&gt; 
   &lt;li&gt;technical trivia: FFmpeg has basic support for tiled heic as of v7.0; need &lt;code&gt;-show_stream_groups&lt;/code&gt; for correct resolution&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;config file example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;[global]
  no-thumb   # disable ALL thumbnails and audio transcoding
  no-vthumb  # only disable video thumbnails

[/music]
  /mnt/nas/music
  accs:
    r: *     # everyone can read
  flags:
    dthumb   # disable ALL thumbnails and audio transcoding
    dvthumb  # only disable video thumbnails
    ext-th:  exe=/ico/exe.png  # /ico/exe.png is the thumbnail of *.exe
    ext-th:  elf=/ico/elf.gif  # ...and /ico/elf.gif is used for *.elf
    th-covers:  folder.png,folder.jpg,cover.png,cover.jpg  # the default
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;zip downloads&lt;/h2&gt; 
&lt;p&gt;download folders (or file selections) as &lt;code&gt;zip&lt;/code&gt; or &lt;code&gt;tar&lt;/code&gt; files&lt;/p&gt; 
&lt;p&gt;select which type of archive you want in the &lt;code&gt;[‚öôÔ∏è] config&lt;/code&gt; tab:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;name&lt;/th&gt; 
   &lt;th&gt;url-suffix&lt;/th&gt; 
   &lt;th&gt;description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;tar&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;?tar&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;plain gnutar, works great with &lt;code&gt;curl | tar -xv&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;pax&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;?tar=pax&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;pax-format tar, futureproof, not as fast&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;tgz&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;?tar=gz&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;gzip compressed gnu-tar (slow), for &lt;code&gt;curl | tar -xvz&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;txz&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;?tar=xz&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;gnu-tar with xz / lzma compression (v.slow)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;zip&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;?zip&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;works everywhere, glitchy filenames on win7 and older&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;zip_dos&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;?zip=dos&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;traditional cp437 (no unicode) to fix glitchy filenames&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;zip_crc&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;?zip=crc&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;cp437 with crc32 computed early for truly ancient software&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;gzip default level is &lt;code&gt;3&lt;/code&gt; (0=fast, 9=best), change with &lt;code&gt;?tar=gz:9&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;xz default level is &lt;code&gt;1&lt;/code&gt; (0=fast, 9=best), change with &lt;code&gt;?tar=xz:9&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;bz2 default level is &lt;code&gt;2&lt;/code&gt; (1=fast, 9=best), change with &lt;code&gt;?tar=bz2:9&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;hidden files (&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#dotfiles"&gt;dotfiles&lt;/a&gt;) are excluded unless account is allowed to list them 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;up2k.db&lt;/code&gt; and &lt;code&gt;dir.txt&lt;/code&gt; is always excluded&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;bsdtar supports streaming unzipping: &lt;code&gt;curl foo?zip | bsdtar -xv&lt;/code&gt; 
  &lt;ul&gt; 
   &lt;li&gt;good, because copyparty's zip is faster than tar on small files 
    &lt;ul&gt; 
     &lt;li&gt;but &lt;code&gt;?tar&lt;/code&gt; is better for large files, especially if the total exceeds 4 GiB&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;zip_crc&lt;/code&gt; will take longer to download since the server has to read each file twice 
  &lt;ul&gt; 
   &lt;li&gt;this is only to support MS-DOS PKZIP v2.04g (october 1993) and older 
    &lt;ul&gt; 
     &lt;li&gt;how are you accessing copyparty actually&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;you can also zip a selection of files or folders by clicking them in the browser, that brings up a selection editor and zip button in the bottom right&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/241032/129635374-e5136e01-470a-49b1-a762-848e8a4c9cdc.png" alt="copyparty-zipsel-fs8" /&gt;&lt;/p&gt; 
&lt;p&gt;cool trick: download a folder by appending url-params &lt;code&gt;?tar&amp;amp;opus&lt;/code&gt; or &lt;code&gt;?tar&amp;amp;mp3&lt;/code&gt; to transcode all audio files (except aac|m4a|mp3|ogg|opus|wma) to opus/mp3 before they're added to the archive&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;super useful if you're 5 minutes away from takeoff and realize you don't have any music on your phone but your server only has flac files and downloading those will burn through all your data + there wouldn't be enough time anyways&lt;/li&gt; 
 &lt;li&gt;and url-param &lt;code&gt;&amp;amp;nodot&lt;/code&gt; skips dotfiles/dotfolders; they are included by default if your account has permission to see them&lt;/li&gt; 
 &lt;li&gt;and url-params &lt;code&gt;&amp;amp;j&lt;/code&gt; / &lt;code&gt;&amp;amp;w&lt;/code&gt; produce jpeg/webm thumbnails/spectrograms instead of the original audio/video/images (&lt;code&gt;&amp;amp;p&lt;/code&gt; for audio waveforms) 
  &lt;ul&gt; 
   &lt;li&gt;can also be used to pregenerate thumbnails; combine with &lt;code&gt;--th-maxage=9999999&lt;/code&gt; or &lt;code&gt;--th-clean=0&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;uploading&lt;/h2&gt; 
&lt;p&gt;drag files/folders into the web-browser to upload&lt;/p&gt; 
&lt;p&gt;dragdrop is the recommended way, but you may also:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;select some files (not folders) in your file explorer and press CTRL-V inside the browser window&lt;/li&gt; 
 &lt;li&gt;use the &lt;a href="https://github.com/9001/copyparty/tree/hovudstraum/bin#u2cpy"&gt;command-line uploader&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;upload using &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#client-examples"&gt;curl, sharex, ishare, ...&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;when uploading files through dragdrop or CTRL-V, this initiates an upload using &lt;code&gt;up2k&lt;/code&gt;; there are two browser-based uploaders available:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;[üéà] bup&lt;/code&gt;, the basic uploader, supports almost every browser since netscape 4.0&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[üöÄ] up2k&lt;/code&gt;, the good / fancy one&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;NB: you can undo/delete your own uploads with &lt;code&gt;[üßØ]&lt;/code&gt; &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#unpost"&gt;unpost&lt;/a&gt; (and this is also where you abort unfinished uploads, but you have to refresh the page first)&lt;/p&gt; 
&lt;p&gt;up2k has several advantages:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;you can drop folders into the browser (files are added recursively)&lt;/li&gt; 
 &lt;li&gt;files are processed in chunks, and each chunk is checksummed 
  &lt;ul&gt; 
   &lt;li&gt;uploads autoresume if they are interrupted by network issues&lt;/li&gt; 
   &lt;li&gt;uploads resume if you reboot your browser or pc, just upload the same files again&lt;/li&gt; 
   &lt;li&gt;server detects any corruption; the client reuploads affected chunks&lt;/li&gt; 
   &lt;li&gt;the client doesn't upload anything that already exists on the server&lt;/li&gt; 
   &lt;li&gt;no filesize limit, even when a proxy limits the request size (for example Cloudflare)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;much higher speeds than ftp/scp/tarpipe on some internet connections (mainly american ones) thanks to parallel connections&lt;/li&gt; 
 &lt;li&gt;the last-modified timestamp of the file is preserved&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;it is perfectly safe to restart / upgrade copyparty while someone is uploading to it!&lt;br /&gt; all known up2k clients will resume just fine üí™&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/devnotes.md#up2k"&gt;up2k&lt;/a&gt; for details on how it works, or watch a &lt;a href="https://a.ocv.me/pub/demo/pics-vids/#gf-0f6f5c0d"&gt;demo video&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/241032/129635371-48fc54ca-fa91-48e3-9b1d-ba413e4b68cb.png" alt="copyparty-upload-fs8" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;protip:&lt;/strong&gt; you can avoid scaring away users with &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/contrib/plugins/minimal-up2k.js"&gt;contrib/plugins/minimal-up2k.js&lt;/a&gt; which makes it look &lt;a href="https://user-images.githubusercontent.com/241032/118311195-dd6ca380-b4ef-11eb-86f3-75a3ff2e1332.png"&gt;much simpler&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;protip:&lt;/strong&gt; if you enable &lt;code&gt;favicon&lt;/code&gt; in the &lt;code&gt;[‚öôÔ∏è] settings&lt;/code&gt; tab (by typing something into the textbox), the icon in the browser tab will indicate upload progress -- also, the &lt;code&gt;[üîî]&lt;/code&gt; and/or &lt;code&gt;[üîä]&lt;/code&gt; switches enable visible and/or audible notifications on upload completion&lt;/p&gt; 
&lt;p&gt;the up2k UI is the epitome of polished intuitive experiences:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;"parallel uploads" specifies how many chunks to upload at the same time&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[üèÉ]&lt;/code&gt; analysis of other files should continue while one is uploading&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[ü•î]&lt;/code&gt; shows a simpler UI for faster uploads from slow devices&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[üõ°Ô∏è]&lt;/code&gt; decides when to overwrite existing files on the server 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;üõ°Ô∏è&lt;/code&gt; = never (generate a new filename instead)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;üïí&lt;/code&gt; = overwrite if the server-file is older&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;‚ôªÔ∏è&lt;/code&gt; = always overwrite if the files are different&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[üé≤]&lt;/code&gt; generate random filenames during upload&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[üîé]&lt;/code&gt; switch between upload and &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#file-search"&gt;file-search&lt;/a&gt; mode 
  &lt;ul&gt; 
   &lt;li&gt;ignore &lt;code&gt;[üîé]&lt;/code&gt; if you add files by dragging them into the browser&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;and then there's the tabs below it,&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;[ok]&lt;/code&gt; is the files which completed successfully&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[ng]&lt;/code&gt; is the ones that failed / got rejected (already exists, ...)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[done]&lt;/code&gt; shows a combined list of &lt;code&gt;[ok]&lt;/code&gt; and &lt;code&gt;[ng]&lt;/code&gt;, chronological order&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[busy]&lt;/code&gt; files which are currently hashing, pending-upload, or uploading 
  &lt;ul&gt; 
   &lt;li&gt;plus up to 3 entries each from &lt;code&gt;[done]&lt;/code&gt; and &lt;code&gt;[que]&lt;/code&gt; for context&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[que]&lt;/code&gt; is all the files that are still queued&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;note that since up2k has to read each file twice, &lt;code&gt;[üéà] bup&lt;/code&gt; can &lt;em&gt;theoretically&lt;/em&gt; be up to 2x faster in some extreme cases (files bigger than your ram, combined with an internet connection faster than the read-speed of your HDD, or if you're uploading from a cuo2duo)&lt;/p&gt; 
&lt;p&gt;if you are resuming a massive upload and want to skip hashing the files which already finished, you can enable &lt;code&gt;turbo&lt;/code&gt; in the &lt;code&gt;[‚öôÔ∏è] config&lt;/code&gt; tab, but please read the tooltip on that button&lt;/p&gt; 
&lt;p&gt;if the server is behind a proxy which imposes a request-size limit, you can configure up2k to sneak below the limit with server-option &lt;code&gt;--u2sz&lt;/code&gt; (the default is 96 MiB to support Cloudflare)&lt;/p&gt; 
&lt;p&gt;if you want to replace existing files on the server with new uploads by default, run with &lt;code&gt;--u2ow 2&lt;/code&gt; (only works if users have the delete-permission, and can still be disabled with &lt;code&gt;üõ°Ô∏è&lt;/code&gt; in the UI)&lt;/p&gt; 
&lt;h3&gt;file-search&lt;/h3&gt; 
&lt;p&gt;dropping files into the browser also lets you see if they exist on the server&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/241032/129635361-c79286f0-b8f1-440e-aaf4-6e929428fac9.png" alt="copyparty-fsearch-fs8" /&gt;&lt;/p&gt; 
&lt;p&gt;when you drag/drop files into the browser, you will see two dropzones: &lt;code&gt;Upload&lt;/code&gt; and &lt;code&gt;Search&lt;/code&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;on a phone? toggle the &lt;code&gt;[üîé]&lt;/code&gt; switch green before tapping the big yellow Search button to select your files&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;the files will be hashed on the client-side, and each hash is sent to the server, which checks if that file exists somewhere&lt;/p&gt; 
&lt;p&gt;files go into &lt;code&gt;[ok]&lt;/code&gt; if they exist (and you get a link to where it is), otherwise they land in &lt;code&gt;[ng]&lt;/code&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;the main reason filesearch is combined with the uploader is cause the code was too spaghetti to separate it out somewhere else, this is no longer the case but now i've warmed up to the idea too much&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;if you have a "wark" (file-identifier/checksum) then you can also search for that in the [üîé] tab by putting &lt;code&gt;w = kFpDiztbZc8Z1Lzi&lt;/code&gt; in the &lt;code&gt;raw&lt;/code&gt; field&lt;/p&gt; 
&lt;h3&gt;unpost&lt;/h3&gt; 
&lt;p&gt;undo/delete accidental uploads using the &lt;code&gt;[üßØ]&lt;/code&gt; tab in the UI&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/241032/129635368-3afa6634-c20f-418c-90dc-ec411f3b3897.png" alt="copyparty-unpost-fs8" /&gt;&lt;/p&gt; 
&lt;p&gt;you can unpost even if you don't have regular move/delete access, however only for files uploaded within the past &lt;code&gt;--unpost&lt;/code&gt; seconds (default 12 hours) and the server must be running with &lt;code&gt;-e2d&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;config file example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;[global]
  e2d            # enable up2k database (remember uploads)
  unpost: 43200  # 12 hours (default)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;self-destruct&lt;/h3&gt; 
&lt;p&gt;uploads can be given a lifetime, after which they expire / self-destruct&lt;/p&gt; 
&lt;p&gt;the feature must be enabled per-volume with the &lt;code&gt;lifetime&lt;/code&gt; &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#upload-rules"&gt;upload rule&lt;/a&gt; which sets the upper limit for how long a file gets to stay on the server&lt;/p&gt; 
&lt;p&gt;clients can specify a shorter expiration time using the &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#uploading"&gt;up2k ui&lt;/a&gt; -- the relevant options become visible upon navigating into a folder with &lt;code&gt;lifetimes&lt;/code&gt; enabled -- or by using the &lt;code&gt;life&lt;/code&gt; &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/devnotes.md#write"&gt;upload modifier&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;specifying a custom expiration time client-side will affect the timespan in which unposts are permitted, so keep an eye on the estimates in the up2k ui&lt;/p&gt; 
&lt;h3&gt;race the beam&lt;/h3&gt; 
&lt;p&gt;download files while they're still uploading (&lt;a href="http://a.ocv.me/pub/g/nerd-stuff/cpp/2024-0418-race-the-beam.webm"&gt;demo video&lt;/a&gt;) -- it's almost like peer-to-peer&lt;/p&gt; 
&lt;p&gt;requires the file to be uploaded using up2k (which is the default drag-and-drop uploader), alternatively the command-line program&lt;/p&gt; 
&lt;h3&gt;incoming files&lt;/h3&gt; 
&lt;p&gt;the control-panel shows the ETA for all incoming files , but only for files being uploaded into volumes where you have read-access&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/fd275ffa-698c-4fca-a307-4d2181269a6a" alt="copyparty-cpanel-upload-eta-or8" /&gt;&lt;/p&gt; 
&lt;h2&gt;file manager&lt;/h2&gt; 
&lt;p&gt;cut/paste, rename, and delete files/folders (if you have permission)&lt;/p&gt; 
&lt;p&gt;file selection: click somewhere on the line (not the link itself), then:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;space&lt;/code&gt; to toggle&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;up/down&lt;/code&gt; to move&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;shift-up/down&lt;/code&gt; to move-and-select&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;ctrl-shift-up/down&lt;/code&gt; to also scroll&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;shift-click another line for range-select&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;cut: select some files and &lt;code&gt;ctrl-x&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;copy: select some files and &lt;code&gt;ctrl-c&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;paste: &lt;code&gt;ctrl-v&lt;/code&gt; in another folder&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;rename: &lt;code&gt;F2&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;you can copy/move files across browser tabs (cut/copy in one tab, paste in another)&lt;/p&gt; 
&lt;h2&gt;shares&lt;/h2&gt; 
&lt;p&gt;share a file or folder by creating a temporary link&lt;/p&gt; 
&lt;p&gt;when enabled in the server settings (&lt;code&gt;--shr&lt;/code&gt;), click the bottom-right &lt;code&gt;share&lt;/code&gt; button to share the folder you're currently in, or alternatively:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;select a folder first to share that folder instead&lt;/li&gt; 
 &lt;li&gt;select one or more files to share only those files&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;this feature was made with &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#identity-providers"&gt;identity providers&lt;/a&gt; in mind -- configure your reverseproxy to skip the IdP's access-control for a given URL prefix and use that to safely share specific files/folders sans the usual auth checks&lt;/p&gt; 
&lt;p&gt;when creating a share, the creator can choose any of the following options:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;password-protection&lt;/li&gt; 
 &lt;li&gt;expire after a certain time; &lt;code&gt;0&lt;/code&gt; or blank means infinite&lt;/li&gt; 
 &lt;li&gt;allow visitors to upload (if the user who creates the share has write-access)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;semi-intentional limitations:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;cleanup of expired shares only works when global option &lt;code&gt;e2d&lt;/code&gt; is set, and/or at least one volume on the server has volflag &lt;code&gt;e2d&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;only folders from the same volume are shared; if you are sharing a folder which contains other volumes, then the contents of those volumes will not be available&lt;/li&gt; 
 &lt;li&gt;if you change &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#password-hashing"&gt;password hashing&lt;/a&gt; settings after creating a password-protected share, then that share will stop working&lt;/li&gt; 
 &lt;li&gt;related to &lt;a href="https://github.com/9001/copyparty/raw/hovudstraum/docs/idp.md#idp-volumes-are-forgotten-on-shutdown"&gt;IdP volumes being forgotten on shutdown&lt;/a&gt;, any shares pointing into a user's IdP volume will be unavailable until that user makes their first request after a restart&lt;/li&gt; 
 &lt;li&gt;no option to "delete after first access" because tricky 
  &lt;ul&gt; 
   &lt;li&gt;when linking something to discord (for example) it'll get accessed by their scraper and that would count as a hit&lt;/li&gt; 
   &lt;li&gt;browsers wouldn't be able to resume a broken download unless the requester's IP gets allowlisted for X minutes (ref. tricky)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;specify &lt;code&gt;--shr /foobar&lt;/code&gt; to enable this feature; a toplevel virtual folder named &lt;code&gt;foobar&lt;/code&gt; is then created, and that's where all the shares will be served from&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;you can name it whatever, &lt;code&gt;foobar&lt;/code&gt; is just an example&lt;/li&gt; 
 &lt;li&gt;if you're using config files, put &lt;code&gt;shr: /foobar&lt;/code&gt; inside the &lt;code&gt;[global]&lt;/code&gt; section instead&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;users can delete their own shares in the controlpanel, and a list of privileged users (&lt;code&gt;--shr-adm&lt;/code&gt;) are allowed to see and/or delet any share on the server&lt;/p&gt; 
&lt;p&gt;after a share has expired, it remains visible in the controlpanel for &lt;code&gt;--shr-rt&lt;/code&gt; minutes (default is 1 day), and the owner can revive it by extending the expiration time there&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;security note:&lt;/strong&gt; using this feature does not mean that you can skip the &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#accounts-and-volumes"&gt;accounts and volumes&lt;/a&gt; section -- you still need to restrict access to volumes that you do not intend to share with unauthenticated users! it is not sufficient to use rules in the reverseproxy to restrict access to just the &lt;code&gt;/share&lt;/code&gt; folder.&lt;/p&gt; 
&lt;h2&gt;batch rename&lt;/h2&gt; 
&lt;p&gt;select some files and press &lt;code&gt;F2&lt;/code&gt; to bring up the rename UI&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/241032/128434204-eb136680-3c07-4ec7-92e0-ae86af20c241.png" alt="batch-rename-fs8" /&gt;&lt;/p&gt; 
&lt;p&gt;quick explanation of the buttons,&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;[‚úÖ apply rename]&lt;/code&gt; confirms and begins renaming&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[‚ùå cancel]&lt;/code&gt; aborts and closes the rename window&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[‚Ü∫ reset]&lt;/code&gt; reverts any filename changes back to the original name&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[decode]&lt;/code&gt; does a URL-decode on the filename, fixing stuff like &lt;code&gt;&amp;amp;amp;&lt;/code&gt; and &lt;code&gt;%20&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[advanced]&lt;/code&gt; toggles advanced mode&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;advanced mode: rename files based on rules to decide the new names, based on the original name (regex), or based on the tags collected from the file (artist/title/...), or a mix of both&lt;/p&gt; 
&lt;p&gt;in advanced mode,&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;[case]&lt;/code&gt; toggles case-sensitive regex&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;regex&lt;/code&gt; is the regex pattern to apply to the original filename; any files which don't match will be skipped&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;format&lt;/code&gt; is the new filename, taking values from regex capturing groups and/or from file tags 
  &lt;ul&gt; 
   &lt;li&gt;very loosely based on foobar2000 syntax&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;presets&lt;/code&gt; lets you save rename rules for later&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;available functions:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;$lpad(text, length, pad_char)&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;$rpad(text, length, pad_char)&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;two counters are available; &lt;code&gt;.n.s&lt;/code&gt; is the nth file in the selection, and &lt;code&gt;.n.d&lt;/code&gt; the nth file in the folder, for example rename-output &lt;code&gt;file(.n.d).(ext)&lt;/code&gt; gives &lt;code&gt;file5.bin&lt;/code&gt;, and &lt;code&gt;beach-$lpad((.n.s),3,0).(ext)&lt;/code&gt; is &lt;code&gt;beach-017.jpg&lt;/code&gt; and the initial value of each counter can be set in the textboxes underneath the preset dropdown&lt;/p&gt; 
&lt;p&gt;so,&lt;/p&gt; 
&lt;p&gt;say you have a file named &lt;a href="https://www.youtube.com/watch?v=-dtb0vDPruI"&gt;&lt;code&gt;meganeko - Eclipse - 07 Sirius A.mp3&lt;/code&gt;&lt;/a&gt; (absolutely fantastic album btw) and the tags are: &lt;code&gt;Album:Eclipse&lt;/code&gt;, &lt;code&gt;Artist:meganeko&lt;/code&gt;, &lt;code&gt;Title:Sirius A&lt;/code&gt;, &lt;code&gt;tn:7&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;you could use just regex to rename it:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;regex&lt;/code&gt; = &lt;code&gt;(.*) - (.*) - ([0-9]{2}) (.*)&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;format&lt;/code&gt; = &lt;code&gt;(3). (1) - (4)&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;output&lt;/code&gt; = &lt;code&gt;07. meganeko - Sirius A.mp3&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;or you could use just tags:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;format&lt;/code&gt; = &lt;code&gt;$lpad((tn),2,0). (artist) - (title).(ext)&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;output&lt;/code&gt; = &lt;code&gt;7. meganeko - Sirius A.mp3&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;or a mix of both:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;regex&lt;/code&gt; = &lt;code&gt;- ([0-9]{2})&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;format&lt;/code&gt; = &lt;code&gt;(1). (artist) - (title).(ext)&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;output&lt;/code&gt; = &lt;code&gt;07. meganeko - Sirius A.mp3&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;the metadata keys you can use in the format field are the ones in the file-browser table header (whatever is collected with &lt;code&gt;-mte&lt;/code&gt; and &lt;code&gt;-mtp&lt;/code&gt;)&lt;/p&gt; 
&lt;h2&gt;rss feeds&lt;/h2&gt; 
&lt;p&gt;monitor a folder with your RSS reader , optionally recursive&lt;/p&gt; 
&lt;p&gt;must be enabled per-volume with volflag &lt;code&gt;rss&lt;/code&gt; or globally with &lt;code&gt;--rss&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;the feed includes itunes metadata for use with podcast readers such as &lt;a href="https://antennapod.org/"&gt;AntennaPod&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;a feed example: &lt;a href="https://cd.ocv.me/a/d2/d22/?rss&amp;amp;fext=mp3"&gt;https://cd.ocv.me/a/d2/d22/?rss&amp;amp;fext=mp3&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;url parameters:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;pw=hunter2&lt;/code&gt; for password auth 
  &lt;ul&gt; 
   &lt;li&gt;if you enabled &lt;code&gt;--usernames&lt;/code&gt; then do &lt;code&gt;pw=username:password&lt;/code&gt; instead&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;nopw&lt;/code&gt; disables embedding the password (if provided) into item-URLs in the feed&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;nopw=a&lt;/code&gt; disables mentioning the password anywhere at all in the feed; may break some readers&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;recursive&lt;/code&gt; to also include subfolders&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;title=foo&lt;/code&gt; changes the feed title (default: folder name)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;fext=mp3,opus&lt;/code&gt; only include mp3 and opus files (default: all)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;nf=30&lt;/code&gt; only show the first 30 results (default: 250)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;sort=m&lt;/code&gt; sort by mtime (file last-modified), newest first (default) 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;u&lt;/code&gt; = upload-time; NOTE: non-uploaded files have upload-time &lt;code&gt;0&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;n&lt;/code&gt; = filename&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;a&lt;/code&gt; = filesize&lt;/li&gt; 
   &lt;li&gt;uppercase = reverse-sort; &lt;code&gt;M&lt;/code&gt; = oldest file first&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;opds feeds&lt;/h2&gt; 
&lt;p&gt;browse and download files from your e-book reader&lt;/p&gt; 
&lt;p&gt;enabled with the &lt;code&gt;opds&lt;/code&gt; volflag or &lt;code&gt;--opds&lt;/code&gt; global option&lt;/p&gt; 
&lt;p&gt;add &lt;code&gt;?opds&lt;/code&gt; to the end of the url you would like to browse, then input that in your opds client. for example: &lt;code&gt;https://copyparty.example/books/?opds&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;to log in with a password, enter it into either of the username or password fields in your client.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;if you've enabled &lt;code&gt;--usernames&lt;/code&gt;, then you need to enter both username and password .&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;note: some clients (e.g. Moon+ Reader) will not send the password when downloading cover images, which will cause your ip to be banned by copyparty. to work around this, you can grant the &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#accounts-and-volumes"&gt;&lt;code&gt;g&lt;/code&gt; permission&lt;/a&gt; to unauthenticated requests and enable &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#filekeys"&gt;filekeys&lt;/a&gt; to prevent guessing filenames. for example: &lt;code&gt;-vbooks&lt;span&gt;üìö&lt;/span&gt;r,ed:g:c,fk,opds&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;by default, not all file types will be listed in opds feeds. to change this, add the extension to &lt;code&gt;--opds-exts&lt;/code&gt; (volflag: &lt;code&gt;opds_exts&lt;/code&gt;), or empty the list to list everything&lt;/p&gt; 
&lt;h2&gt;recent uploads&lt;/h2&gt; 
&lt;p&gt;list all recent uploads by clicking "show recent uploads" in the controlpanel&lt;/p&gt; 
&lt;p&gt;will show uploader IP and upload-time if the visitor has the admin permission&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;global-option &lt;code&gt;--ups-when&lt;/code&gt; makes upload-time visible to all users, and not just admins&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;global-option &lt;code&gt;--ups-who&lt;/code&gt; (volflag &lt;code&gt;ups_who&lt;/code&gt;) specifies who gets access (0=nobody, 1=admins, 2=everyone), default=2&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;note that the &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#unpost"&gt;üßØ unpost&lt;/a&gt; feature is better suited for viewing &lt;em&gt;your own&lt;/em&gt; recent uploads, as it includes the option to undo/delete them&lt;/p&gt; 
&lt;p&gt;config file example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;[global]
  ups-when    # everyone can see upload times
  ups-who: 1  # but only admins can see the list,
              # so ups-when doesn't take effect
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;media player&lt;/h2&gt; 
&lt;p&gt;plays almost every audio format there is (if the server has FFmpeg installed for on-demand transcoding)&lt;/p&gt; 
&lt;p&gt;the following audio formats are usually always playable, even without FFmpeg: &lt;code&gt;aac|flac|m4a|mp3|ogg|opus|wav&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;some highlights:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;OS integration; control playback from your phone's lockscreen (&lt;a href="https://user-images.githubusercontent.com/241032/233213022-298a98ba-721a-4cf1-a3d4-f62634bc53d5.png"&gt;windows&lt;/a&gt; // &lt;a href="https://user-images.githubusercontent.com/241032/142711926-0700be6c-3e31-47b3-9928-53722221f722.png"&gt;iOS&lt;/a&gt; // &lt;a href="https://user-images.githubusercontent.com/241032/233212311-a7368590-08c7-4f9f-a1af-48ccf3f36fad.png"&gt;android&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;shows the audio waveform in the seekbar&lt;/li&gt; 
 &lt;li&gt;not perfectly gapless but can get really close (see settings + eq below); good enough to enjoy gapless albums as intended&lt;/li&gt; 
 &lt;li&gt;videos can be played as audio, without wasting bandwidth on the video&lt;/li&gt; 
 &lt;li&gt;adding &lt;code&gt;?v&lt;/code&gt; to the end of an audio/video/image link will make it open in the mediaplayer&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;click the &lt;code&gt;play&lt;/code&gt; link next to an audio file, or copy the link target to &lt;a href="https://a.ocv.me/pub/demo/music/Ubiktune%20-%20SOUNDSHOCK%202%20-%20FM%20FUNK%20TERRROR!!/#af-1fbfba61&amp;amp;t=18"&gt;share it&lt;/a&gt; (optionally with a timestamp to start playing from, like that example does)&lt;/p&gt; 
&lt;p&gt;open the &lt;code&gt;[üé∫]&lt;/code&gt; media-player-settings tab to configure it,&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;"switches": 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;[üîÅ]&lt;/code&gt; repeats one single song forever&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;[üîÄ]&lt;/code&gt; shuffles the files inside each folder&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;[preload]&lt;/code&gt; starts loading the next track when it's about to end, reduces the silence between songs&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;[full]&lt;/code&gt; does a full preload by downloading the entire next file; good for unreliable connections, bad for slow connections&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;[~s]&lt;/code&gt; toggles the seekbar waveform display&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;[/np]&lt;/code&gt; enables buttons to copy the now-playing info as an irc message&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;[üìª]&lt;/code&gt; enables buttons to create an &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#playlists"&gt;m3u playlist&lt;/a&gt; with the selected songs&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;[os-ctl]&lt;/code&gt; makes it possible to control audio playback from the lockscreen of your device (enables &lt;a href="https://developer.mozilla.org/en-US/docs/Web/API/MediaSession"&gt;mediasession&lt;/a&gt;)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;[seek]&lt;/code&gt; allows seeking with lockscreen controls (buggy on some devices)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;[art]&lt;/code&gt; shows album art on the lockscreen&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;[üéØ]&lt;/code&gt; keeps the playing song scrolled into view (good when using the player as a taskbar dock)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;[‚üé]&lt;/code&gt; shrinks the playback controls&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;"buttons": 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;[uncache]&lt;/code&gt; may fix songs that won't play correctly due to bad files in browser cache&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;"at end of folder": 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;[loop]&lt;/code&gt; keeps looping the folder&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;[next]&lt;/code&gt; plays into the next folder&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;"transcode": 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;[flac]&lt;/code&gt; converts &lt;code&gt;flac&lt;/code&gt; and &lt;code&gt;wav&lt;/code&gt; files into opus (if supported by browser) or mp3&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;[aac]&lt;/code&gt; converts &lt;code&gt;aac&lt;/code&gt; and &lt;code&gt;m4a&lt;/code&gt; files into opus (if supported by browser) or mp3&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;[oth]&lt;/code&gt; converts all other known formats into opus (if supported by browser) or mp3 
    &lt;ul&gt; 
     &lt;li&gt;&lt;code&gt;aac|ac3|aif|aiff|alac|alaw|amr|ape|au|dfpwm|dts|flac|gsm|it|m4a|mo3|mod|mp2|mp3|mpc|mptm|mt2|mulaw|ogg|okt|opus|ra|s3m|tak|tta|ulaw|wav|wma|wv|xm|xpk&lt;/code&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;"transcode to": 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;[opus]&lt;/code&gt; produces an &lt;code&gt;opus&lt;/code&gt; whenever transcoding is necessary (the best choice on Android and PCs)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;[awo]&lt;/code&gt; is &lt;code&gt;opus&lt;/code&gt; in a &lt;code&gt;weba&lt;/code&gt; file, good for iPhones (iOS 17.5 and newer) but Apple is still fixing some state-confusion bugs as of iOS 18.2.1&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;[caf]&lt;/code&gt; is &lt;code&gt;opus&lt;/code&gt; in a &lt;code&gt;caf&lt;/code&gt; file, good for iPhones (iOS 11 through 17), technically unsupported by Apple but works for the most part&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;[mp3]&lt;/code&gt; -- the myth, the legend, the undying master of mediocre sound quality that definitely works everywhere&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;[flac]&lt;/code&gt; -- lossless but compressed, for LAN and/or fiber playback on electrostatic headphones&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;[wav]&lt;/code&gt; -- lossless and uncompressed, for LAN and/or fiber playback on electrostatic headphones connected to very old equipment 
    &lt;ul&gt; 
     &lt;li&gt;&lt;code&gt;flac&lt;/code&gt; and &lt;code&gt;wav&lt;/code&gt; must be enabled with &lt;code&gt;--allow-flac&lt;/code&gt; / &lt;code&gt;--allow-wav&lt;/code&gt; to allow spending the disk space&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;"tint" reduces the contrast of the playback bar&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;playlists&lt;/h3&gt; 
&lt;p&gt;create and play &lt;a href="https://en.wikipedia.org/wiki/M3U"&gt;m3u8&lt;/a&gt; playlists -- see example &lt;a href="https://a.ocv.me/pub/demo/music/?doc=example-playlist.m3u"&gt;text&lt;/a&gt; and &lt;a href="https://a.ocv.me/pub/demo/music/#m3u=example-playlist.m3u"&gt;player&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;click a file with the extension &lt;code&gt;m3u&lt;/code&gt; or &lt;code&gt;m3u8&lt;/code&gt; (for example &lt;code&gt;mixtape.m3u&lt;/code&gt; or &lt;code&gt;touhou.m3u8&lt;/code&gt; ) and you get two choices: Play / Edit&lt;/p&gt; 
&lt;p&gt;playlists can include songs across folders anywhere on the server, but filekeys/dirkeys are NOT supported, so the listener must have read-access or get-access to the files&lt;/p&gt; 
&lt;h3&gt;creating a playlist&lt;/h3&gt; 
&lt;p&gt;with a standalone mediaplayer or copyparty&lt;/p&gt; 
&lt;p&gt;you can use foobar2000, deadbeef, just about any standalone player should work -- but you might need to edit the filepaths in the playlist so they fit with the server-URLs&lt;/p&gt; 
&lt;p&gt;alternatively, you can create the playlist using copyparty itself:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;open the &lt;code&gt;[üé∫]&lt;/code&gt; media-player-settings tab and enable the &lt;code&gt;[üìª]&lt;/code&gt; create-playlist feature -- this adds two new buttons in the bottom-right tray, &lt;code&gt;[üìªadd]&lt;/code&gt; and &lt;code&gt;[üìªcopy]&lt;/code&gt; which appear when you listen to music, or when you select a few audiofiles&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;click the &lt;code&gt;üìªadd&lt;/code&gt; button while a song is playing (or when you've selected some songs) and they'll be added to "the list" (you can't see it yet)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;at any time, click &lt;code&gt;üìªcopy&lt;/code&gt; to send the playlist to your clipboard&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;you can then continue adding more songs if you'd like&lt;/li&gt; 
   &lt;li&gt;if you want to wipe the playlist and start from scratch, just refresh the page&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;create a new textfile, name it &lt;code&gt;something.m3u&lt;/code&gt; and paste the playlist there&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;audio equalizer&lt;/h3&gt; 
&lt;p&gt;and &lt;a href="https://en.wikipedia.org/wiki/Dynamic_range_compression"&gt;dynamic range compressor&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;can also boost the volume in general, or increase/decrease stereo width (like &lt;a href="https://www.foobar2000.org/components/view/foo_dsp_meiercf"&gt;crossfeed&lt;/a&gt; just worse)&lt;/p&gt; 
&lt;p&gt;has the convenient side-effect of reducing the pause between songs, so gapless albums play better with the eq enabled (just make it flat)&lt;/p&gt; 
&lt;p&gt;not available on iPhones / iPads because AudioContext currently breaks background audio playback on iOS (15.7.8)&lt;/p&gt; 
&lt;h3&gt;fix unreliable playback on android&lt;/h3&gt; 
&lt;p&gt;due to phone / app settings, android phones may randomly stop playing music when the power saver kicks in, especially at the end of an album -- you can fix it by &lt;a href="https://user-images.githubusercontent.com/241032/235262123-c328cca9-3930-4948-bd18-3949b9fd3fcf.png"&gt;disabling power saving&lt;/a&gt; in the &lt;a href="https://user-images.githubusercontent.com/241032/235262121-2ffc51ae-7821-4310-a322-c3b7a507890c.png"&gt;app settings&lt;/a&gt; of the browser you use for music streaming (preferably a dedicated one)&lt;/p&gt; 
&lt;h2&gt;textfile viewer&lt;/h2&gt; 
&lt;p&gt;with realtime streaming of logfiles and such (&lt;a href="https://a.ocv.me/pub/demo/logtail/"&gt;demo&lt;/a&gt;) , and terminal colors work too&lt;/p&gt; 
&lt;p&gt;click &lt;code&gt;-txt-&lt;/code&gt; next to a textfile to open the viewer, which has the following toolbar buttons:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;‚úèÔ∏è edit&lt;/code&gt; opens the textfile editor&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;üì° follow&lt;/code&gt; starts monitoring the file for changes, streaming new lines in realtime 
  &lt;ul&gt; 
   &lt;li&gt;similar to &lt;code&gt;tail -f&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://a.ocv.me/pub/demo/logtail/?doc=lipsum.txt&amp;amp;tail"&gt;link directly&lt;/a&gt; to a file with tailing enabled by adding &lt;code&gt;&amp;amp;tail&lt;/code&gt; to the textviewer URL&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;markdown viewer&lt;/h2&gt; 
&lt;p&gt;and there are &lt;em&gt;two&lt;/em&gt; editors&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/241032/115978057-66419080-a57d-11eb-8539-d2be843991aa.png" alt="copyparty-md-read-fs8" /&gt;&lt;/p&gt; 
&lt;p&gt;there is a built-in extension for inline clickable thumbnails;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;enable it by adding &lt;code&gt;&amp;lt;!-- th --&amp;gt;&lt;/code&gt; somewhere in the doc&lt;/li&gt; 
 &lt;li&gt;add thumbnails with &lt;code&gt;!th[l](your.jpg)&lt;/code&gt; where &lt;code&gt;l&lt;/code&gt; means left-align (&lt;code&gt;r&lt;/code&gt; = right-align)&lt;/li&gt; 
 &lt;li&gt;a single line with &lt;code&gt;---&lt;/code&gt; clears the float / inlining&lt;/li&gt; 
 &lt;li&gt;in the case of README.md being displayed below a file listing, thumbnails will open in the gallery viewer&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;other notes,&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;the document preview has a max-width which is the same as an A4 paper when printed&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;markdown vars&lt;/h3&gt; 
&lt;p&gt;dynamic docs with serverside variable expansion to replace stuff like &lt;code&gt;{{self.ip}}&lt;/code&gt; with the client's IP, or &lt;code&gt;{{srv.htime}}&lt;/code&gt; with the current time on the server&lt;/p&gt; 
&lt;p&gt;see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/srv/expand/"&gt;./srv/expand/&lt;/a&gt; for usage and examples&lt;/p&gt; 
&lt;h2&gt;other tricks&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;you can link a particular timestamp in an audio file by adding it to the URL, such as &lt;code&gt;&amp;amp;20&lt;/code&gt; / &lt;code&gt;&amp;amp;20s&lt;/code&gt; / &lt;code&gt;&amp;amp;1m20&lt;/code&gt; / &lt;code&gt;&amp;amp;t=1:20&lt;/code&gt; after the &lt;code&gt;.../#af-c8960dab&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;enabling the audio equalizer can help make gapless albums fully gapless in some browsers (chrome), so consider leaving it on with all the values at zero&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;get a plaintext file listing by adding &lt;code&gt;?ls=t&lt;/code&gt; to a URL, or a compact colored one with &lt;code&gt;?ls=v&lt;/code&gt; (for unix terminals)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;if you are using media hotkeys to switch songs and are getting tired of seeing the OSD popup which Windows doesn't let you disable, consider &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/contrib/#media-osd-bgoneps1"&gt;./contrib/media-osd-bgone.ps1&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;click the bottom-left &lt;code&gt;œÄ&lt;/code&gt; to open a javascript prompt for debugging&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;files named &lt;code&gt;.prologue.html&lt;/code&gt; / &lt;code&gt;.epilogue.html&lt;/code&gt; will be rendered before/after directory listings unless &lt;code&gt;--no-logues&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;files named &lt;code&gt;descript.ion&lt;/code&gt; / &lt;code&gt;DESCRIPT.ION&lt;/code&gt; are parsed and displayed in the file listing, or as the epilogue if nonstandard&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;files named &lt;code&gt;README.md&lt;/code&gt; / &lt;code&gt;readme.md&lt;/code&gt; will be rendered after directory listings unless &lt;code&gt;--no-readme&lt;/code&gt; (but &lt;code&gt;.epilogue.html&lt;/code&gt; takes precedence)&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;and &lt;code&gt;PREADME.md&lt;/code&gt; / &lt;code&gt;preadme.md&lt;/code&gt; is shown above directory listings unless &lt;code&gt;--no-readme&lt;/code&gt; or &lt;code&gt;.prologue.html&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;README.md&lt;/code&gt; and &lt;code&gt;*logue.html&lt;/code&gt; can contain placeholder values which are replaced server-side before embedding into directory listings; see &lt;a href="https://copyparty.eu/cli/#exp-help-page"&gt;&lt;code&gt;--help-exp&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;searching&lt;/h2&gt; 
&lt;p&gt;search by size, date, path/name, mp3-tags, ...&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/241032/129635365-c0ff2a9f-0ee5-4fc3-8bb6-006033cf67b8.png" alt="copyparty-search-fs8" /&gt;&lt;/p&gt; 
&lt;p&gt;when started with &lt;code&gt;-e2dsa&lt;/code&gt; copyparty will scan/index all your files. This avoids duplicates on upload, and also makes the volumes searchable through the web-ui:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;make search queries by &lt;code&gt;size&lt;/code&gt;/&lt;code&gt;date&lt;/code&gt;/&lt;code&gt;directory-path&lt;/code&gt;/&lt;code&gt;filename&lt;/code&gt;, or...&lt;/li&gt; 
 &lt;li&gt;drag/drop a local file to see if the same contents exist somewhere on the server, see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#file-search"&gt;file-search&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;path/name queries are space-separated, AND'ed together, and words are negated with a &lt;code&gt;-&lt;/code&gt; prefix, so for example:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;path: &lt;code&gt;shibayan -bossa&lt;/code&gt; finds all files where one of the folders contain &lt;code&gt;shibayan&lt;/code&gt; but filters out any results where &lt;code&gt;bossa&lt;/code&gt; exists somewhere in the path&lt;/li&gt; 
 &lt;li&gt;name: &lt;code&gt;demetori styx&lt;/code&gt; gives you &lt;a href="https://www.youtube.com/watch?v=zGh0g14ZJ8I&amp;amp;list=PL3A147BD151EE5218&amp;amp;index=9"&gt;good stuff&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;the &lt;code&gt;raw&lt;/code&gt; field allows for more complex stuff such as &lt;code&gt;( tags like *nhato* or tags like *taishi* ) and ( not tags like *nhato* or not tags like *taishi* )&lt;/code&gt; which finds all songs by either nhato or taishi, excluding collabs (terrible example, why would you do that)&lt;/p&gt; 
&lt;p&gt;for the above example to work, add the commandline argument &lt;code&gt;-e2ts&lt;/code&gt; to also scan/index tags from music files, which brings us over to:&lt;/p&gt; 
&lt;h1&gt;server config&lt;/h1&gt; 
&lt;p&gt;using arguments or config files, or a mix of both:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;config files (&lt;code&gt;-c some.conf&lt;/code&gt;) can set additional commandline arguments; see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/example.conf"&gt;./docs/example.conf&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/example2.conf"&gt;./docs/example2.conf&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;kill -s USR1&lt;/code&gt; (same as &lt;code&gt;systemctl reload copyparty&lt;/code&gt;) to reload accounts and volumes from config files without restarting 
  &lt;ul&gt; 
   &lt;li&gt;or click the &lt;code&gt;[reload cfg]&lt;/code&gt; button in the control-panel if the user has &lt;code&gt;a&lt;/code&gt;/admin in any volume&lt;/li&gt; 
   &lt;li&gt;changes to the &lt;code&gt;[global]&lt;/code&gt; config section requires a restart to take effect&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;NB:&lt;/strong&gt; as humongous as this readme is, there is also a lot of undocumented features. Run copyparty with &lt;a href="https://copyparty.eu/cli/"&gt;&lt;code&gt;--help&lt;/code&gt;&lt;/a&gt; (or click that link) to see all available global options; all of those can be used in the &lt;code&gt;[global]&lt;/code&gt; section of config files, and everything listed in &lt;a href="https://copyparty.eu/cli/#flags-help-page"&gt;&lt;code&gt;--help-flags&lt;/code&gt;&lt;/a&gt; can be used in volumes as volflags (per-volume configuration).&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;if running in docker/podman, try this: &lt;code&gt;docker run --rm -it copyparty/ac --help&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;or if you prefer plaintext, &lt;a href="https://copyparty.eu/helptext.txt"&gt;https://copyparty.eu/helptext.txt&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;zeroconf&lt;/h2&gt; 
&lt;p&gt;announce enabled services on the LAN (&lt;a href="https://user-images.githubusercontent.com/241032/215344737-0eae8d98-9496-4256-9aa8-cd2f6971810d.png"&gt;pic&lt;/a&gt;) -- &lt;code&gt;-z&lt;/code&gt; enables both &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#mdns"&gt;mdns&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#ssdp"&gt;ssdp&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--z-on&lt;/code&gt; / &lt;code&gt;--z-off&lt;/code&gt; limits the feature to certain networks&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;config file example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;[global]
  z      # enable all zeroconf features (mdns, ssdp)
  zm     # only enables mdns (does nothing since we already have z)
  z-on: 192.168.0.0/16, 10.1.2.0/24  # restrict to certain subnets
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;mdns&lt;/h3&gt; 
&lt;p&gt;LAN domain-name and feature announcer&lt;/p&gt; 
&lt;p&gt;uses &lt;a href="https://en.wikipedia.org/wiki/Multicast_DNS"&gt;multicast dns&lt;/a&gt; to give copyparty a domain which any machine on the LAN can use to access it&lt;/p&gt; 
&lt;p&gt;all enabled services (&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#webdav-server"&gt;webdav&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#ftp-server"&gt;ftp&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#smb-server"&gt;smb&lt;/a&gt;) will appear in mDNS-aware file managers (KDE, gnome, macOS, ...)&lt;/p&gt; 
&lt;p&gt;the domain will be &lt;code&gt;partybox.local&lt;/code&gt; if the machine's hostname is &lt;code&gt;partybox&lt;/code&gt; unless &lt;code&gt;--name&lt;/code&gt; specifies something else&lt;/p&gt; 
&lt;p&gt;and the web-UI will be available at &lt;a href="http://partybox.local:3923/"&gt;http://partybox.local:3923/&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;if you want to get rid of the &lt;code&gt;:3923&lt;/code&gt; so you can use &lt;a href="http://partybox.local/"&gt;http://partybox.local/&lt;/a&gt; instead then see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#listen-on-port-80-and-443"&gt;listen on port 80 and 443&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ssdp&lt;/h3&gt; 
&lt;p&gt;windows-explorer announcer&lt;/p&gt; 
&lt;p&gt;uses &lt;a href="https://en.wikipedia.org/wiki/Simple_Service_Discovery_Protocol"&gt;ssdp&lt;/a&gt; to make copyparty appear in the windows file explorer on all machines on the LAN&lt;/p&gt; 
&lt;p&gt;doubleclicking the icon opens the "connect" page which explains how to mount copyparty as a local filesystem&lt;/p&gt; 
&lt;p&gt;if copyparty does not appear in windows explorer, use &lt;code&gt;--zsv&lt;/code&gt; to see why:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;maybe the discovery multicast was sent from an IP which does not intersect with the server subnets&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;qr-code&lt;/h2&gt; 
&lt;p&gt;print a qr-code &lt;a href="https://user-images.githubusercontent.com/241032/194728533-6f00849b-c6ac-43c6-9359-83e454d11e00.png"&gt;(screenshot)&lt;/a&gt; for quick access, great between phones on android hotspots which keep changing the subnet&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--qr&lt;/code&gt; enables it&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--qrs&lt;/code&gt; does https instead of http&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--qrl lootbox/?pw=hunter2&lt;/code&gt; appends to the url, linking to the &lt;code&gt;lootbox&lt;/code&gt; folder with password &lt;code&gt;hunter2&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--qrz 1&lt;/code&gt; forces 1x zoom instead of autoscaling to fit the terminal size 
  &lt;ul&gt; 
   &lt;li&gt;1x may render incorrectly on some terminals/fonts, but 2x should always work&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--qr-pin 1&lt;/code&gt; makes the qr-code stick to the bottom of the console (never scrolls away)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--qr-file qr.txt:1:2&lt;/code&gt; writes a small qr-code to &lt;code&gt;qr.txt&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--qr-file qr.txt:2:2&lt;/code&gt; writes a big qr-code to &lt;code&gt;qr.txt&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--qr-file qr.svg:1:2&lt;/code&gt; writes a vector-graphics qr-code to &lt;code&gt;qr.svg&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--qr-file qr.png:8:4:333333:ffcc55&lt;/code&gt; writes an 8x-magnified yellow-on-gray &lt;code&gt;qr.png&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--qr-file qr.png:8:4::ffffff&lt;/code&gt; writes an 8x-magnified white-on-transparent &lt;code&gt;qr.png&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;it uses the server hostname if &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#mdns"&gt;mdns&lt;/a&gt; is enabled, otherwise it'll use your external ip (default route) unless &lt;code&gt;--qri&lt;/code&gt; specifies a specific ip-prefix or domain&lt;/p&gt; 
&lt;h2&gt;ftp server&lt;/h2&gt; 
&lt;p&gt;an FTP server can be started using &lt;code&gt;--ftp 3921&lt;/code&gt;, and/or &lt;code&gt;--ftps&lt;/code&gt; for explicit TLS (ftpes)&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;based on &lt;a href="https://github.com/giampaolo/pyftpdlib"&gt;pyftpdlib&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;needs a dedicated port (cannot share with the HTTP/HTTPS API)&lt;/li&gt; 
 &lt;li&gt;uploads are not resumable -- delete and restart if necessary&lt;/li&gt; 
 &lt;li&gt;runs in active mode by default, you probably want &lt;code&gt;--ftp-pr 12000-13000&lt;/code&gt; 
  &lt;ul&gt; 
   &lt;li&gt;if you enable both &lt;code&gt;ftp&lt;/code&gt; and &lt;code&gt;ftps&lt;/code&gt;, the port-range will be divided in half&lt;/li&gt; 
   &lt;li&gt;some older software (filezilla on debian-stable) cannot passive-mode with TLS&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;login with any username + your password, or put your password in the username field 
  &lt;ul&gt; 
   &lt;li&gt;unless you enabled &lt;code&gt;--usernames&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;some recommended FTP / FTPS clients; &lt;code&gt;wark&lt;/code&gt; = example password:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://winscp.net/eng/download.php"&gt;https://winscp.net/eng/download.php&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://filezilla-project.org/"&gt;https://filezilla-project.org/&lt;/a&gt; struggles a bit with ftps in active-mode, but is fine otherwise&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://rclone.org/"&gt;https://rclone.org/&lt;/a&gt; does FTPS with &lt;code&gt;tls=false explicit_tls=true&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;lftp -u k,wark -p 3921 127.0.0.1 -e ls&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;lftp -u k,wark -p 3990 127.0.0.1 -e 'set ssl:verify-certificate no; ls'&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;curl ftp://127.0.0.1:3921/&lt;/code&gt; (plaintext ftp)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;curl --ssl-reqd ftp://127.0.0.1:3990/&lt;/code&gt; (encrypted ftps)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;config file example, which restricts FTP to only use ports 3921 and 12000-12099 so all of those ports must be opened in your firewall:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;[global]
  ftp: 3921
  ftp-pr: 12000-12099
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;sftp server&lt;/h2&gt; 
&lt;p&gt;goes roughly 700 MiB/s (slower than webdav and ftp)&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;this is &lt;strong&gt;not&lt;/strong&gt; &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#ftp-server"&gt;ftps&lt;/a&gt; (which copyparty also supports); &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#ftp-server"&gt;ftps&lt;/a&gt; is ftp-tls (think http/https), while &lt;strong&gt;sftp&lt;/strong&gt; is ssh-based and (preferably) uses ssh-keys for authentication&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;the sftp-server requires the optional dependency &lt;a href="https://pypi.org/project/paramiko/"&gt;paramiko&lt;/a&gt;;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;if you are &lt;strong&gt;not&lt;/strong&gt; using docker, then install paramiko somehow&lt;/li&gt; 
 &lt;li&gt;if you &lt;strong&gt;are&lt;/strong&gt; using docker, then use one of the following image variants: &lt;code&gt;ac&lt;/code&gt; / &lt;code&gt;im&lt;/code&gt; / &lt;code&gt;iv&lt;/code&gt; / &lt;code&gt;dj&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;enable sftpd with &lt;code&gt;--sftp 3922&lt;/code&gt; to listen on port 3922;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;use global-option &lt;code&gt;sftp-key&lt;/code&gt; to associate an ssh-key with a user; 
  &lt;ul&gt; 
   &lt;li&gt;commandline: &lt;code&gt;--sftp-key 'david ssh-ed25519 AAAAC3NzaC...'&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;config-file: &lt;code&gt;sftp-key: david ssh-ed25519 AAAAC3NzaC...&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--sftp-pw&lt;/code&gt; enables login with passwords (default is ssh-keys only)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--sftp-anon foo&lt;/code&gt; enables login with username &lt;code&gt;foo&lt;/code&gt; and no password; gives the same access/permissions as the website does when not logged in&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;see the &lt;a href="https://copyparty.eu/cli/#g-sftp"&gt;sftp section in --help&lt;/a&gt; for the other options&lt;/p&gt; 
&lt;h2&gt;webdav server&lt;/h2&gt; 
&lt;p&gt;with read-write support, supports winXP and later, macos, nautilus/gvfs ... a great way to &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#mount-as-drive"&gt;access copyparty straight from the file explorer in your OS&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;click the &lt;a href="http://127.0.0.1:3923/?hc"&gt;connect&lt;/a&gt; button in the control-panel to see connection instructions for windows, linux, macos&lt;/p&gt; 
&lt;p&gt;general usage:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;login with any username + your password, or put your password in the username field (password field can be empty/whatever) 
  &lt;ul&gt; 
   &lt;li&gt;unless you enabled &lt;code&gt;--usernames&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;on macos, connect from finder:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;[Go] -&amp;gt; [Connect to Server...] -&amp;gt; &lt;a href="http://192.168.123.1:3923/"&gt;http://192.168.123.1:3923/&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;to upload or edit files with WebDAV clients, enable the &lt;code&gt;daw&lt;/code&gt; volflag (because most WebDAV clients expect this) and give your account the delete-permission. This avoids getting several copies of the same file on the server. HOWEVER: This will also make all PUT-uploads overwrite existing files if the user has delete-access, so use with caution.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;note: if you have enabled &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#identity-providers"&gt;IdP authentication&lt;/a&gt; then that may cause issues for some/most webdav clients; see &lt;a href="https://github.com/9001/copyparty/raw/hovudstraum/docs/idp.md#connecting-webdav-clients"&gt;the webdav section in the IdP docs&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;connecting to webdav from windows&lt;/h3&gt; 
&lt;p&gt;using the GUI (winXP or later):&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;rightclick [my computer] -&amp;gt; [map network drive] -&amp;gt; Folder: &lt;code&gt;http://192.168.123.1:3923/&lt;/code&gt; 
  &lt;ul&gt; 
   &lt;li&gt;on winXP only, click the &lt;code&gt;Sign up for online storage&lt;/code&gt; hyperlink instead and put the URL there&lt;/li&gt; 
   &lt;li&gt;providing your password as the username is recommended; the password field can be anything or empty 
    &lt;ul&gt; 
     &lt;li&gt;unless you enabled &lt;code&gt;--usernames&lt;/code&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;the webdav client that's built into windows has the following list of bugs; you can avoid all of these by connecting with rclone instead:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;win7+ doesn't actually send the password to the server when reauthenticating after a reboot unless you first try to login with an incorrect password and then switch to the correct password 
  &lt;ul&gt; 
   &lt;li&gt;or just type your password into the username field instead to get around it entirely&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;connecting to a folder which allows anonymous read will make writing impossible, as windows has decided it doesn't need to login 
  &lt;ul&gt; 
   &lt;li&gt;workaround: connect twice; first to a folder which requires auth, then to the folder you actually want, and leave both of those mounted&lt;/li&gt; 
   &lt;li&gt;or set the server-option &lt;code&gt;--dav-auth&lt;/code&gt; to force password-auth for all webdav clients&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;win7+ may open a new tcp connection for every file and sometimes forgets to close them, eventually needing a reboot 
  &lt;ul&gt; 
   &lt;li&gt;maybe NIC-related (??), happens with win10-ltsc on e1000e but not virtio&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;windows cannot access folders which contain filenames with invalid unicode or forbidden characters (&lt;code&gt;&amp;lt;&amp;gt;:"/\|?*&lt;/code&gt;), or names ending with &lt;code&gt;.&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;winxp cannot show unicode characters outside of &lt;em&gt;some range&lt;/em&gt; 
  &lt;ul&gt; 
   &lt;li&gt;latin-1 is fine, hiragana is not (not even as shift-jis on japanese xp)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;tftp server&lt;/h2&gt; 
&lt;p&gt;a TFTP server (read/write) can be started using &lt;code&gt;--tftp 3969&lt;/code&gt; (you probably want &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#ftp-server"&gt;ftp&lt;/a&gt; instead unless you are &lt;em&gt;actually&lt;/em&gt; communicating with hardware from the 90s (in which case we should definitely hang some time))&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;that makes this the first RTX DECT Base that has been updated using copyparty üéâ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;based on &lt;a href="https://github.com/9001/partftpy"&gt;partftpy&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;no accounts; read from world-readable folders, write to world-writable, overwrite in world-deletable&lt;/li&gt; 
 &lt;li&gt;needs a dedicated port (cannot share with the HTTP/HTTPS API) 
  &lt;ul&gt; 
   &lt;li&gt;run as root (or see below) to use the spec-recommended port &lt;code&gt;69&lt;/code&gt; (nice)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;can reply from a predefined portrange (good for firewalls)&lt;/li&gt; 
 &lt;li&gt;only supports the binary/octet/image transfer mode (no netascii)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://datatracker.ietf.org/doc/html/rfc7440"&gt;RFC 7440&lt;/a&gt; is &lt;strong&gt;not&lt;/strong&gt; supported, so will be extremely slow over WAN 
  &lt;ul&gt; 
   &lt;li&gt;assuming default blksize (512), expect 1100 KiB/s over 100BASE-T, 400-500 KiB/s over wifi, 200 on bad wifi&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;most clients expect to find TFTP on port 69, but on linux and macos you need to be root to listen on that. Alternatively, listen on 3969 and use NAT on the server to forward 69 to that port;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;on linux: &lt;code&gt;iptables -t nat -A PREROUTING -i eth0 -p udp --dport 69 -j REDIRECT --to-port 3969&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;some recommended TFTP clients:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;curl (cross-platform, read/write) 
  &lt;ul&gt; 
   &lt;li&gt;get: &lt;code&gt;curl --tftp-blksize 1428 tftp://127.0.0.1:3969/firmware.bin&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;put: &lt;code&gt;curl --tftp-blksize 1428 -T firmware.bin tftp://127.0.0.1:3969/&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;windows: &lt;code&gt;tftp.exe&lt;/code&gt; (you probably already have it) 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;tftp -i 127.0.0.1 put firmware.bin&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;linux: &lt;code&gt;tftp-hpa&lt;/code&gt;, &lt;code&gt;atftp&lt;/code&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;atftp --option "blksize 1428" 127.0.0.1 3969 -p -l firmware.bin -r firmware.bin&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;tftp -v -m binary 127.0.0.1 3969 -c put firmware.bin&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;smb server&lt;/h2&gt; 
&lt;p&gt;unsafe, slow, not recommended for wan, enable with &lt;code&gt;--smb&lt;/code&gt; for read-only or &lt;code&gt;--smbw&lt;/code&gt; for read-write&lt;/p&gt; 
&lt;p&gt;click the &lt;a href="http://127.0.0.1:3923/?hc"&gt;connect&lt;/a&gt; button in the control-panel to see connection instructions for windows, linux, macos&lt;/p&gt; 
&lt;p&gt;dependencies: &lt;code&gt;python3 -m pip install --user -U impacket==0.13.0&lt;/code&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;newer versions of impacket will hopefully work just fine but there is monkeypatching so maybe not&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;some &lt;strong&gt;BIG WARNINGS&lt;/strong&gt; specific to SMB/CIFS, in decreasing importance:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;not entirely confident that read-only is read-only&lt;/li&gt; 
 &lt;li&gt;the smb backend is not fully integrated with vfs, meaning there could be security issues (path traversal). Please use &lt;code&gt;--smb-port&lt;/code&gt; (see below) and &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/bin/prisonparty.sh"&gt;prisonparty&lt;/a&gt; or &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/bin/bubbleparty.sh"&gt;bubbleparty&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;account passwords work per-volume as expected, and so does account permissions (read/write/move/delete), but &lt;code&gt;--smbw&lt;/code&gt; must be given to allow write-access from smb&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#shadowing"&gt;shadowing&lt;/a&gt; probably works as expected but no guarantees&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;not compatible with pw-hashing or &lt;code&gt;--usernames&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;and some minor issues,&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;clients only see the first ~400 files in big folders; 
  &lt;ul&gt; 
   &lt;li&gt;this was originally due to &lt;a href="https://github.com/SecureAuthCorp/impacket/issues/1433"&gt;impacket#1433&lt;/a&gt; which was fixed in impacket-0.12, so you can disable the workaround with &lt;code&gt;--smb-nwa-1&lt;/code&gt; but then you get unacceptably poor performance instead&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;hot-reload of server config (&lt;code&gt;/?reload=cfg&lt;/code&gt;) does not include the &lt;code&gt;[global]&lt;/code&gt; section (commandline args)&lt;/li&gt; 
 &lt;li&gt;listens on the first IPv4 &lt;code&gt;-i&lt;/code&gt; interface only (default = :: = 0.0.0.0 = all)&lt;/li&gt; 
 &lt;li&gt;login doesn't work on winxp, but anonymous access is ok -- remove all accounts from copyparty config for that to work 
  &lt;ul&gt; 
   &lt;li&gt;win10 onwards does not allow connecting anonymously / without accounts&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;python3 only&lt;/li&gt; 
 &lt;li&gt;slow (the builtin webdav support in windows is 5x faster, and rclone-webdav is 30x faster) 
  &lt;ul&gt; 
   &lt;li&gt;those numbers are specifically for copyparty's smb-server (because it sucks); other smb-servers should be similar to webdav&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;known client bugs:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;on win7 only, &lt;code&gt;--smb1&lt;/code&gt; is much faster than smb2 (default) because it keeps rescanning folders on smb2 
  &lt;ul&gt; 
   &lt;li&gt;however smb1 is buggy and is not enabled by default on win10 onwards&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;windows cannot access folders which contain filenames with invalid unicode or forbidden characters (&lt;code&gt;&amp;lt;&amp;gt;:"/\|?*&lt;/code&gt;), or names ending with &lt;code&gt;.&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;the smb protocol listens on TCP port 445, which is a privileged port on linux and macos, which would require running copyparty as root. However, this can be avoided by listening on another port using &lt;code&gt;--smb-port 3945&lt;/code&gt; and then using NAT on the server to forward the traffic from 445 to there;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;on linux: &lt;code&gt;iptables -t nat -A PREROUTING -i eth0 -p tcp --dport 445 -j REDIRECT --to-port 3945&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;authenticate with one of the following:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;username &lt;code&gt;$username&lt;/code&gt;, password &lt;code&gt;$password&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;username &lt;code&gt;$password&lt;/code&gt;, password &lt;code&gt;k&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;browser ux&lt;/h2&gt; 
&lt;p&gt;tweaking the ui&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;set default sort order globally with &lt;code&gt;--sort&lt;/code&gt; or per-volume with the &lt;code&gt;sort&lt;/code&gt; volflag; specify one or more comma-separated columns to sort by, and prefix the column name with &lt;code&gt;-&lt;/code&gt; for reverse sort 
  &lt;ul&gt; 
   &lt;li&gt;the column names you can use are visible as tooltips when hovering over the column headers in the directory listing, for example &lt;code&gt;href ext sz ts tags/.up_at tags/Circle tags/.tn tags/Artist tags/Title&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;to sort in music order (album, track, artist, title) with filename as fallback, you could &lt;code&gt;--sort tags/Circle,tags/.tn,tags/Artist,tags/Title,href&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;to sort by upload date, first enable showing the upload date in the listing with &lt;code&gt;-e2d -mte +.up_at&lt;/code&gt; and then &lt;code&gt;--sort tags/.up_at&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/rice"&gt;./docs/rice&lt;/a&gt; for more, including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;how to &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/rice/README.md#hide-ui-elements"&gt;hide ui-elements&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/rice/README.md#custom-fonts"&gt;custom fonts&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/rice/README.md#boring-loader-spinner"&gt;custom loading-spinner&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;adding stuff (css/&lt;code&gt;&amp;lt;meta&amp;gt;&lt;/code&gt;/...) &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/rice/README.md#head"&gt;to the html &lt;code&gt;&amp;lt;head&amp;gt;&lt;/code&gt; tag&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/rice/README.md#translations"&gt;adding your own translation&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;opengraph&lt;/h2&gt; 
&lt;p&gt;discord and social-media embeds&lt;/p&gt; 
&lt;p&gt;can be enabled globally with &lt;code&gt;--og&lt;/code&gt; or per-volume with volflag &lt;code&gt;og&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;note that this disables hotlinking because the opengraph spec demands it; to sneak past this intentional limitation, you can enable opengraph selectively by user-agent, for example &lt;code&gt;--og-ua '(Discord|Twitter|Slack)bot'&lt;/code&gt; (or volflag &lt;code&gt;og_ua&lt;/code&gt;)&lt;/p&gt; 
&lt;p&gt;you can also hotlink files regardless by appending &lt;code&gt;?raw&lt;/code&gt; to the url&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;WARNING: if you plan to use WebDAV, then &lt;code&gt;--og-ua&lt;/code&gt; / &lt;code&gt;og_ua&lt;/code&gt; must be configured&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;if you want to entirely replace the copyparty response with your own jinja2 template, give the template filepath to &lt;code&gt;--og-tpl&lt;/code&gt; or volflag &lt;code&gt;og_tpl&lt;/code&gt; (all members of &lt;code&gt;HttpCli&lt;/code&gt; are available through the &lt;code&gt;this&lt;/code&gt; object)&lt;/p&gt; 
&lt;h2&gt;file deduplication&lt;/h2&gt; 
&lt;p&gt;enable symlink-based upload deduplication globally with &lt;code&gt;--dedup&lt;/code&gt; or per-volume with volflag &lt;code&gt;dedup&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;by default, when someone tries to upload a file that already exists on the server, the upload will be politely declined, and the server will copy the existing file over to where the upload would have gone&lt;/p&gt; 
&lt;p&gt;if you enable deduplication with &lt;code&gt;--dedup&lt;/code&gt; then it'll create a symlink instead of a full copy, thus reducing disk space usage&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;on the contrary, if your server is hooked up to s3-glacier or similar storage where reading is expensive, and you cannot use &lt;code&gt;--safe-dedup=1&lt;/code&gt; because you have other software tampering with your files, so you want to entirely disable detection of duplicate data instead, then you can specify &lt;code&gt;--no-clone&lt;/code&gt; globally or &lt;code&gt;noclone&lt;/code&gt; as a volflag&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;warning:&lt;/strong&gt; when enabling dedup, you should also:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;enable indexing with &lt;code&gt;-e2dsa&lt;/code&gt; or volflag &lt;code&gt;e2dsa&lt;/code&gt; (see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#file-indexing"&gt;file indexing&lt;/a&gt; section below); strongly recommended&lt;/li&gt; 
 &lt;li&gt;...and/or &lt;code&gt;--hardlink-only&lt;/code&gt; to use hardlink-based deduplication instead of symlinks; see explanation below&lt;/li&gt; 
 &lt;li&gt;...and/or &lt;code&gt;--reflink&lt;/code&gt; to use CoW/reflink-based dedup (much safer than hardlink, but OS/FS-dependent)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;it will not be safe to rename/delete files if you only enable dedup and none of the above; if you enable indexing then it is not &lt;em&gt;necessary&lt;/em&gt; to also do hardlinks (but you may still want to)&lt;/p&gt; 
&lt;p&gt;by default, deduplication is done based on symlinks (symbolic links); these are tiny files which are pointers to the nearest full copy of the file&lt;/p&gt; 
&lt;p&gt;you can choose to use hardlinks instead of softlinks, globally with &lt;code&gt;--hardlink-only&lt;/code&gt; or volflag &lt;code&gt;hardlinkonly&lt;/code&gt;, and you can choose to use reflinks with &lt;code&gt;--reflink&lt;/code&gt; or volflag &lt;code&gt;reflink&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;advantages of using reflinks (CoW, copy-on-write):&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;entirely safe (when your filesystem supports it correctly); either file can be edited or deleted without affecting other copies&lt;/li&gt; 
 &lt;li&gt;only linux 5.3 or newer, only python 3.14 or newer, only some filesystems (btrfs probably ok, maybe xfs too, but zfs had bugs)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;advantages of using hardlinks:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;hardlinks are more compatible with other software; they behave entirely like regular files&lt;/li&gt; 
 &lt;li&gt;you can safely move and rename files using other file managers 
  &lt;ul&gt; 
   &lt;li&gt;symlinks need to be managed by copyparty to ensure the destinations remain correct&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;advantages of using symlinks (default):&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;each symlink can have its own last-modified timestamp, but a single timestamp is shared by all hardlinks&lt;/li&gt; 
 &lt;li&gt;symlinks make it more obvious to other software that the file is not a regular file, so this can be less dangerous 
  &lt;ul&gt; 
   &lt;li&gt;hardlinks look like regular files, so other software may assume they are safe to edit without affecting the other copies&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;warning:&lt;/strong&gt; if you edit the contents of a deduplicated file, then you will also edit all other copies of that file! This is especially surprising with hardlinks, because they look like regular files, but that same file exists in multiple locations&lt;/p&gt; 
&lt;p&gt;global-option &lt;code&gt;--xlink&lt;/code&gt; / volflag &lt;code&gt;xlink&lt;/code&gt; additionally enables deduplication across volumes, but this is probably buggy and not recommended&lt;/p&gt; 
&lt;p&gt;config file example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;[global]
  e2dsa  # scan and index filesystem on startup
  dedup  # symlink-based deduplication for all volumes

[/media]
  /mnt/nas/media
  flags:
    hardlinkonly  # this vol does hardlinks instead of symlinks
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;file indexing&lt;/h2&gt; 
&lt;p&gt;enable music search, upload-undo, and better dedup&lt;/p&gt; 
&lt;p&gt;file indexing relies on two database tables, the up2k filetree (&lt;code&gt;-e2d&lt;/code&gt;) and the metadata tags (&lt;code&gt;-e2t&lt;/code&gt;), stored in &lt;code&gt;.hist/up2k.db&lt;/code&gt;. Configuration can be done through arguments, volflags, or a mix of both.&lt;/p&gt; 
&lt;p&gt;through arguments:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;-e2d&lt;/code&gt; enables file indexing on upload&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-e2ds&lt;/code&gt; also scans writable folders for new files on startup&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-e2dsa&lt;/code&gt; also scans all mounted volumes (including readonly ones)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-e2t&lt;/code&gt; enables metadata indexing on upload&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-e2ts&lt;/code&gt; also scans for tags in all files that don't have tags yet&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-e2tsr&lt;/code&gt; also deletes all existing tags, doing a full reindex&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-e2v&lt;/code&gt; verifies file integrity at startup, comparing hashes from the db&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-e2vu&lt;/code&gt; patches the database with the new hashes from the filesystem&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-e2vp&lt;/code&gt; panics and kills copyparty instead&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;the same arguments can be set as volflags, in addition to &lt;code&gt;d2d&lt;/code&gt;, &lt;code&gt;d2ds&lt;/code&gt;, &lt;code&gt;d2t&lt;/code&gt;, &lt;code&gt;d2ts&lt;/code&gt;, &lt;code&gt;d2v&lt;/code&gt; for disabling:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;-v ~/music::r:c,e2ds,e2tsr&lt;/code&gt; does a full reindex of everything on startup&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-v ~/music::r:c,d2d&lt;/code&gt; disables &lt;strong&gt;all&lt;/strong&gt; indexing, even if any &lt;code&gt;-e2*&lt;/code&gt; are on&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-v ~/music::r:c,d2t&lt;/code&gt; disables all &lt;code&gt;-e2t*&lt;/code&gt; (tags), does not affect &lt;code&gt;-e2d*&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-v ~/music::r:c,d2ds&lt;/code&gt; disables on-boot scans; only index new uploads&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-v ~/music::r:c,d2ts&lt;/code&gt; same except only affecting tags&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;note:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;upload-times can be displayed in the file listing by enabling the &lt;code&gt;.up_at&lt;/code&gt; metadata key, either globally with &lt;code&gt;-e2d -mte +.up_at&lt;/code&gt; or per-volume with volflags &lt;code&gt;e2d,mte=+.up_at&lt;/code&gt; (will have a ~17% performance impact on directory listings) 
  &lt;ul&gt; 
   &lt;li&gt;and file checksums can be shown with global-option &lt;code&gt;-e2d -mte +w&lt;/code&gt; or volflag &lt;code&gt;e2d,mte=+w&lt;/code&gt; (always active for users with permission &lt;code&gt;a&lt;/code&gt;)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;e2tsr&lt;/code&gt; is probably always overkill, since &lt;code&gt;e2ds&lt;/code&gt;/&lt;code&gt;e2dsa&lt;/code&gt; would pick up any file modifications and &lt;code&gt;e2ts&lt;/code&gt; would then reindex those, unless there is a new copyparty version with new parsers and the release note says otherwise&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;config file example (these options are recommended btw):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;[global]
  e2dsa  # scan and index all files in all volumes on startup
  e2ts   # check newly-discovered or uploaded files for media tags
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;exclude-patterns&lt;/h3&gt; 
&lt;p&gt;to save some time, you can provide a regex pattern for filepaths to only index by filename/path/size/last-modified (and not the hash of the file contents) by setting &lt;code&gt;--no-hash '\.iso$'&lt;/code&gt; or the volflag &lt;code&gt;:c,nohash=\.iso$&lt;/code&gt;, this has the following consequences:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;initial indexing is way faster, especially when the volume is on a network disk&lt;/li&gt; 
 &lt;li&gt;makes it impossible to &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#file-search"&gt;file-search&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;if someone uploads the same file contents, the upload will not be detected as a dupe, so it will not get symlinked or rejected&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;similarly, you can fully ignore files/folders using &lt;code&gt;--no-idx [...]&lt;/code&gt; and &lt;code&gt;:c,noidx=\.iso$&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;NOTE: &lt;code&gt;no-idx&lt;/code&gt; and/or &lt;code&gt;no-hash&lt;/code&gt; prevents deduplication of those files&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;when running on macos, all the usual apple metadata files are excluded by default&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;if you set &lt;code&gt;--no-hash [...]&lt;/code&gt; globally, you can enable hashing for specific volumes using flag &lt;code&gt;:c,nohash=&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;to exclude certain filepaths from search-results, use &lt;code&gt;--srch-excl&lt;/code&gt; or volflag &lt;code&gt;srch_excl&lt;/code&gt; instead of &lt;code&gt;--no-idx&lt;/code&gt;, for example &lt;code&gt;--srch-excl 'password|logs/[0-9]'&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;config file example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;[/games]
  /mnt/nas/games
  flags:
    noidx: \.iso$  # skip indexing iso-files
    srch_excl: password|logs/[0-9]  # filter search results
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;filesystem guards&lt;/h3&gt; 
&lt;p&gt;avoid traversing into other filesystems using &lt;code&gt;--xdev&lt;/code&gt; / volflag &lt;code&gt;:c,xdev&lt;/code&gt;, skipping any symlinks or bind-mounts to another HDD for example&lt;/p&gt; 
&lt;p&gt;and/or you can &lt;code&gt;--xvol&lt;/code&gt; / &lt;code&gt;:c,xvol&lt;/code&gt; to ignore all symlinks leaving the volume's top directory, but still allow bind-mounts pointing elsewhere&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;symlinks are permitted with &lt;code&gt;xvol&lt;/code&gt; if they point into another volume where the user has the same level of access&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;these options will reduce performance; unlikely worst-case estimates are 14% reduction for directory listings, 35% for download-as-tar&lt;/p&gt; 
&lt;p&gt;as of copyparty v1.7.0 these options also prevent file access at runtime -- in previous versions it was just hints for the indexer&lt;/p&gt; 
&lt;h3&gt;periodic rescan&lt;/h3&gt; 
&lt;p&gt;filesystem monitoring; if copyparty is not the only software doing stuff on your filesystem, you may want to enable periodic rescans to keep the index up to date&lt;/p&gt; 
&lt;p&gt;argument &lt;code&gt;--re-maxage 60&lt;/code&gt; will rescan all volumes every 60 sec, same as volflag &lt;code&gt;:c,scan=60&lt;/code&gt; to specify it per-volume&lt;/p&gt; 
&lt;p&gt;uploads are disabled while a rescan is happening, so rescans will be delayed by &lt;code&gt;--db-act&lt;/code&gt; (default 10 sec) when there is write-activity going on (uploads, renames, ...)&lt;/p&gt; 
&lt;p&gt;note: folder-thumbnails are selected during filesystem indexing, so periodic rescans can be used to keep them accurate as images are uploaded/deleted (or manually do a rescan with the &lt;code&gt;reload&lt;/code&gt; button in the controlpanel)&lt;/p&gt; 
&lt;p&gt;config file example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;[global]
  re-maxage: 3600

[/pics]
  /mnt/nas/pics
  flags:
    scan: 900
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;upload rules&lt;/h2&gt; 
&lt;p&gt;set upload rules using volflags, some examples:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;:c,sz=1k-3m&lt;/code&gt; sets allowed filesize between 1 KiB and 3 MiB inclusive (suffixes: &lt;code&gt;b&lt;/code&gt;, &lt;code&gt;k&lt;/code&gt;, &lt;code&gt;m&lt;/code&gt;, &lt;code&gt;g&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;:c,df=4g&lt;/code&gt; block uploads if there would be less than 4 GiB free disk space afterwards&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;:c,vmaxb=1g&lt;/code&gt; block uploads if total volume size would exceed 1 GiB afterwards&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;:c,vmaxn=4k&lt;/code&gt; block uploads if volume would contain more than 4096 files afterwards&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;:c,nosub&lt;/code&gt; disallow uploading into subdirectories; goes well with &lt;code&gt;rotn&lt;/code&gt; and &lt;code&gt;rotf&lt;/code&gt;:&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;:c,rotn=1000,2&lt;/code&gt; moves uploads into subfolders, up to 1000 files in each folder before making a new one, two levels deep (must be at least 1)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;:c,rotf=%Y/%m/%d/%H&lt;/code&gt; enforces files to be uploaded into a structure of subfolders according to that date format 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;:c,rotf_tz=Europe/Oslo&lt;/code&gt; sets the timezone (default is UTC unless global-option &lt;code&gt;rotf-tz&lt;/code&gt; is changed)&lt;/li&gt; 
   &lt;li&gt;if someone uploads to &lt;code&gt;/foo/bar&lt;/code&gt; the path would be rewritten to &lt;code&gt;/foo/bar/2021/08/06/23&lt;/code&gt; for example&lt;/li&gt; 
   &lt;li&gt;but the actual value is not verified, just the structure, so the uploader can choose any values which conform to the format string 
    &lt;ul&gt; 
     &lt;li&gt;just to avoid additional complexity in up2k which is enough of a mess already&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;:c,lifetime=300&lt;/code&gt; delete uploaded files when they become 5 minutes old&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;you can also set transaction limits which apply per-IP and per-volume, but these assume &lt;code&gt;-j 1&lt;/code&gt; (default) otherwise the limits will be off, for example &lt;code&gt;-j 4&lt;/code&gt; would allow anywhere between 1x and 4x the limits you set depending on which processing node the client gets routed to&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;:c,maxn=250,3600&lt;/code&gt; allows 250 files over 1 hour from each IP (tracked per-volume)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;:c,maxb=1g,300&lt;/code&gt; allows 1 GiB total over 5 minutes from each IP (tracked per-volume)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;notes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;vmaxb&lt;/code&gt; and &lt;code&gt;vmaxn&lt;/code&gt; requires either the &lt;code&gt;e2ds&lt;/code&gt; volflag or &lt;code&gt;-e2dsa&lt;/code&gt; global-option&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;config file example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;[/inc]
  /mnt/nas/uploads
  accs:
    w: *    # anyone can upload here
    rw: ed  # only user "ed" can read-write
  flags:
    e2ds       # filesystem indexing is required for many of these:
    sz: 1k-3m  # accept upload only if filesize in this range
    df: 4g     # free disk space cannot go lower than this
    vmaxb: 1g  # volume can never exceed 1 GiB
    vmaxn: 4k  # ...or 4000 files, whichever comes first
    nosub      # must upload to toplevel folder
    lifetime: 300   # uploads are deleted after 5min
    maxn: 250,3600  # each IP can upload 250 files in 1 hour
    maxb: 1g,300    # each IP can upload 1 GiB over 5 minutes
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;compress uploads&lt;/h2&gt; 
&lt;p&gt;files can be autocompressed on upload, either on user-request (if config allows) or forced by server-config&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;volflag &lt;code&gt;gz&lt;/code&gt; allows gz compression&lt;/li&gt; 
 &lt;li&gt;volflag &lt;code&gt;xz&lt;/code&gt; allows lzma compression&lt;/li&gt; 
 &lt;li&gt;volflag &lt;code&gt;pk&lt;/code&gt; &lt;strong&gt;forces&lt;/strong&gt; compression on all files&lt;/li&gt; 
 &lt;li&gt;url parameter &lt;code&gt;pk&lt;/code&gt; requests compression with server-default algorithm&lt;/li&gt; 
 &lt;li&gt;url parameter &lt;code&gt;gz&lt;/code&gt; or &lt;code&gt;xz&lt;/code&gt; requests compression with a specific algorithm&lt;/li&gt; 
 &lt;li&gt;url parameter &lt;code&gt;xz&lt;/code&gt; requests xz compression&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;things to note,&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;the &lt;code&gt;gz&lt;/code&gt; and &lt;code&gt;xz&lt;/code&gt; arguments take a single optional argument, the compression level (range 0 to 9)&lt;/li&gt; 
 &lt;li&gt;the &lt;code&gt;pk&lt;/code&gt; volflag takes the optional argument &lt;code&gt;ALGORITHM,LEVEL&lt;/code&gt; which will then be forced for all uploads, for example &lt;code&gt;gz,9&lt;/code&gt; or &lt;code&gt;xz,0&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;default compression is gzip level 9&lt;/li&gt; 
 &lt;li&gt;all upload methods except up2k are supported&lt;/li&gt; 
 &lt;li&gt;the files will be indexed after compression, so dupe-detection and file-search will not work as expected&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;some examples,&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;-v inc:inc:w:c,pk=xz,0&lt;/code&gt;&lt;br /&gt; folder named inc, shared at inc, write-only for everyone, forces xz compression at level 0&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-v inc:inc:w:c,pk&lt;/code&gt;&lt;br /&gt; same write-only inc, but forces gz compression (default) instead of xz&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-v inc:inc:w:c,gz&lt;/code&gt;&lt;br /&gt; allows (but does not force) gz compression if client uploads to &lt;code&gt;/inc?pk&lt;/code&gt; or &lt;code&gt;/inc?gz&lt;/code&gt; or &lt;code&gt;/inc?gz=4&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;chmod and chown&lt;/h2&gt; 
&lt;p&gt;per-volume filesystem-permissions and ownership&lt;/p&gt; 
&lt;p&gt;by default:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;all folders are chmod 755&lt;/li&gt; 
 &lt;li&gt;files are usually chmod 644 (umask-defined)&lt;/li&gt; 
 &lt;li&gt;user/group is whatever copyparty is running as&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;this can be configured per-volume:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;volflag &lt;code&gt;chmod_f&lt;/code&gt; sets file permissions; default=&lt;code&gt;644&lt;/code&gt; (usually)&lt;/li&gt; 
 &lt;li&gt;volflag &lt;code&gt;chmod_d&lt;/code&gt; sets directory permissions; default=&lt;code&gt;755&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;volflag &lt;code&gt;uid&lt;/code&gt; sets the owner user-id&lt;/li&gt; 
 &lt;li&gt;volflag &lt;code&gt;gid&lt;/code&gt; sets the owner group-id&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;notes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;gid&lt;/code&gt; can only be set to one of the groups which the copyparty process is a member of&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;uid&lt;/code&gt; can only be set if copyparty is running as root (i appreciate your faith)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;other flags&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;:c,magic&lt;/code&gt; enables filetype detection for nameless uploads, same as &lt;code&gt;--magic&lt;/code&gt; 
  &lt;ul&gt; 
   &lt;li&gt;needs &lt;a href="https://pypi.org/project/python-magic/"&gt;https://pypi.org/project/python-magic/&lt;/a&gt; &lt;code&gt;python3 -m pip install --user -U python-magic&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;on windows grab this instead &lt;code&gt;python3 -m pip install --user -U python-magic-bin&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;cachectl&lt;/code&gt; changes how webbrowser will cache responses (the &lt;code&gt;Cache-Control&lt;/code&gt; response-header); default is &lt;code&gt;no-cache&lt;/code&gt; which will prevent repeated downloading of the same file unless necessary (browser will ask copyparty if the file has changed) 
  &lt;ul&gt; 
   &lt;li&gt;adding &lt;code&gt;?cache&lt;/code&gt; to a link will override this with "fully cache this for 69 seconds"; &lt;code&gt;?cache=321&lt;/code&gt; is 321 seconds, and &lt;code&gt;?cache=i&lt;/code&gt; is 7 days&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;database location&lt;/h2&gt; 
&lt;p&gt;in-volume (&lt;code&gt;.hist/up2k.db&lt;/code&gt;, default) or somewhere else&lt;/p&gt; 
&lt;p&gt;copyparty creates a subfolder named &lt;code&gt;.hist&lt;/code&gt; inside each volume where it stores the database, thumbnails, and some other stuff&lt;/p&gt; 
&lt;p&gt;this can instead be kept in a single place using the &lt;code&gt;--hist&lt;/code&gt; argument, or the &lt;code&gt;hist=&lt;/code&gt; volflag, or a mix of both:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--hist ~/.cache/copyparty -v ~/music::r:c,hist=-&lt;/code&gt; sets &lt;code&gt;~/.cache/copyparty&lt;/code&gt; as the default place to put volume info, but &lt;code&gt;~/music&lt;/code&gt; gets the regular &lt;code&gt;.hist&lt;/code&gt; subfolder (&lt;code&gt;-&lt;/code&gt; restores default behavior)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;by default, the per-volume &lt;code&gt;up2k.db&lt;/code&gt; sqlite3-database for &lt;code&gt;-e2d&lt;/code&gt; and &lt;code&gt;-e2t&lt;/code&gt; is stored next to the thumbnails according to the &lt;code&gt;--hist&lt;/code&gt; option, but the global-option &lt;code&gt;--dbpath&lt;/code&gt; and/or volflag &lt;code&gt;dbpath&lt;/code&gt; can be used to put the database somewhere else&lt;/p&gt; 
&lt;p&gt;if your storage backend is unreliable (NFS or bad HDDs), you can specify one or more "landmarks" to look for before doing anything database-related. A landmark is a file which is always expected to exist inside the volume. This avoids spurious filesystem rescans in the event of an outage. One line per landmark (see example below)&lt;/p&gt; 
&lt;p&gt;note:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;putting the hist-folders on an SSD is strongly recommended for performance&lt;/li&gt; 
 &lt;li&gt;markdown edits are always stored in a local &lt;code&gt;.hist&lt;/code&gt; subdirectory&lt;/li&gt; 
 &lt;li&gt;on windows the volflag path is cyglike, so &lt;code&gt;/c/temp&lt;/code&gt; means &lt;code&gt;C:\temp&lt;/code&gt; but use regular paths for &lt;code&gt;--hist&lt;/code&gt; 
  &lt;ul&gt; 
   &lt;li&gt;you can use cygpaths for volumes too, &lt;code&gt;-v C:\Users::r&lt;/code&gt; and &lt;code&gt;-v /c/users::r&lt;/code&gt; both work&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;config file example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;[global]
  hist: ~/.cache/copyparty  # put db/thumbs/etc. here by default

[/pics]
  /mnt/nas/pics
  flags:
    hist: -  # restore the default (/mnt/nas/pics/.hist/)
    hist: /mnt/nas/cache/pics/  # can be absolute path
    landmark: me.jpg  # /mnt/nas/pics/me.jpg must be readable to enable db
    landmark: info/a.txt^=ok  # and this textfile must start with "ok"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;metadata from audio files&lt;/h2&gt; 
&lt;p&gt;set &lt;code&gt;-e2t&lt;/code&gt; to index tags on upload&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;-mte&lt;/code&gt; decides which tags to index and display in the browser (and also the display order), this can be changed per-volume:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;-v ~/music::r:c,mte=title,artist&lt;/code&gt; indexes and displays &lt;em&gt;title&lt;/em&gt; followed by &lt;em&gt;artist&lt;/em&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;if you add/remove a tag from &lt;code&gt;mte&lt;/code&gt; you will need to run with &lt;code&gt;-e2tsr&lt;/code&gt; once to rebuild the database, otherwise only new files will be affected&lt;/p&gt; 
&lt;p&gt;but instead of using &lt;code&gt;-mte&lt;/code&gt;, &lt;code&gt;-mth&lt;/code&gt; is a better way to hide tags in the browser: these tags will not be displayed by default, but they still get indexed and become searchable, and users can choose to unhide them in the &lt;code&gt;[‚öôÔ∏è] config&lt;/code&gt; pane&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;-mtm&lt;/code&gt; can be used to add or redefine a metadata mapping, say you have media files with &lt;code&gt;foo&lt;/code&gt; and &lt;code&gt;bar&lt;/code&gt; tags and you want them to display as &lt;code&gt;qux&lt;/code&gt; in the browser (preferring &lt;code&gt;foo&lt;/code&gt; if both are present), then do &lt;code&gt;-mtm qux=foo,bar&lt;/code&gt; and now you can &lt;code&gt;-mte artist,title,qux&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;tags that start with a &lt;code&gt;.&lt;/code&gt; such as &lt;code&gt;.bpm&lt;/code&gt; and &lt;code&gt;.dur&lt;/code&gt;(ation) indicate numeric value&lt;/p&gt; 
&lt;p&gt;see the beautiful mess of a dictionary in &lt;a href="https://github.com/9001/copyparty/raw/hovudstraum/copyparty/mtag.py"&gt;mtag.py&lt;/a&gt; for the default mappings (should cover mp3,opus,flac,m4a,wav,aif,)&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;--no-mutagen&lt;/code&gt; disables Mutagen and uses FFprobe instead, which...&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;is about 20x slower than Mutagen&lt;/li&gt; 
 &lt;li&gt;catches a few tags that Mutagen doesn't 
  &lt;ul&gt; 
   &lt;li&gt;melodic key, video resolution, framerate, pixfmt&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;avoids pulling any GPL code into copyparty&lt;/li&gt; 
 &lt;li&gt;more importantly runs FFprobe on incoming files which is bad if your FFmpeg has a cve&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;code&gt;--mtag-to&lt;/code&gt; sets the tag-scan timeout; very high default (60 sec) to cater for zfs and other randomly-freezing filesystems. Lower values like 10 are usually safe, allowing for faster processing of tricky files&lt;/p&gt; 
&lt;h2&gt;file parser plugins&lt;/h2&gt; 
&lt;p&gt;provide custom parsers to index additional tags, also see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/bin/mtag/README.md"&gt;./bin/mtag/README.md&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;copyparty can invoke external programs to collect additional metadata for files using &lt;code&gt;mtp&lt;/code&gt; (either as argument or volflag), there is a default timeout of 60sec, and only files which contain audio get analyzed by default (see ay/an/ad below)&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;-mtp .bpm=~/bin/audio-bpm.py&lt;/code&gt; will execute &lt;code&gt;~/bin/audio-bpm.py&lt;/code&gt; with the audio file as argument 1 to provide the &lt;code&gt;.bpm&lt;/code&gt; tag, if that does not exist in the audio metadata&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-mtp key=f,t5,~/bin/audio-key.py&lt;/code&gt; uses &lt;code&gt;~/bin/audio-key.py&lt;/code&gt; to get the &lt;code&gt;key&lt;/code&gt; tag, replacing any existing metadata tag (&lt;code&gt;f,&lt;/code&gt;), aborting if it takes longer than 5sec (&lt;code&gt;t5,&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-v ~/music::r:c,mtp=.bpm=~/bin/audio-bpm.py:c,mtp=key=f,t5,~/bin/audio-key.py&lt;/code&gt; both as a per-volume config wow this is getting ugly&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;em&gt;but wait, there's more!&lt;/em&gt; &lt;code&gt;-mtp&lt;/code&gt; can be used for non-audio files as well using the &lt;code&gt;a&lt;/code&gt; flag: &lt;code&gt;ay&lt;/code&gt; only do audio files (default), &lt;code&gt;an&lt;/code&gt; only do non-audio files, or &lt;code&gt;ad&lt;/code&gt; do all files (d as in dontcare)&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;"audio file" also means videos btw, as long as there is an audio stream&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-mtp ext=an,~/bin/file-ext.py&lt;/code&gt; runs &lt;code&gt;~/bin/file-ext.py&lt;/code&gt; to get the &lt;code&gt;ext&lt;/code&gt; tag only if file is not audio (&lt;code&gt;an&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-mtp arch,built,ver,orig=an,eexe,edll,~/bin/exe.py&lt;/code&gt; runs &lt;code&gt;~/bin/exe.py&lt;/code&gt; to get properties about windows-binaries only if file is not audio (&lt;code&gt;an&lt;/code&gt;) and file extension is exe or dll&lt;/li&gt; 
 &lt;li&gt;if you want to daisychain parsers, use the &lt;code&gt;p&lt;/code&gt; flag to set processing order 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;-mtp foo=p1,~/a.py&lt;/code&gt; runs before &lt;code&gt;-mtp foo=p2,~/b.py&lt;/code&gt; and will forward all the tags detected so far as json to the stdin of b.py&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;option &lt;code&gt;c0&lt;/code&gt; disables capturing of stdout/stderr, so copyparty will not receive any tags from the process at all -- instead the invoked program is free to print whatever to the console, just using copyparty as a launcher 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;c1&lt;/code&gt; captures stdout only, &lt;code&gt;c2&lt;/code&gt; only stderr, and &lt;code&gt;c3&lt;/code&gt; (default) captures both&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;you can control how the parser is killed if it times out with option &lt;code&gt;kt&lt;/code&gt; killing the entire process tree (default), &lt;code&gt;km&lt;/code&gt; just the main process, or &lt;code&gt;kn&lt;/code&gt; let it continue running until copyparty is terminated&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;if something doesn't work, try &lt;code&gt;--mtag-v&lt;/code&gt; for verbose error messages&lt;/p&gt; 
&lt;p&gt;config file example; note that &lt;code&gt;mtp&lt;/code&gt; is an additive option so all of the mtp options will take effect:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;[/music]
  /mnt/nas/music
  flags:
    mtp: .bpm=~/bin/audio-bpm.py  # assign ".bpm" (numeric) with script
    mtp: key=f,t5,~/bin/audio-key.py  # force/overwrite, 5sec timeout
    mtp: ext=an,~/bin/file-ext.py  # will only run on non-audio files
    mtp: arch,built,ver,orig=an,eexe,edll,~/bin/exe.py  # only exe/dll
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;event hooks&lt;/h2&gt; 
&lt;p&gt;trigger a program on uploads, renames etc (&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/bin/hooks/"&gt;examples&lt;/a&gt;)&lt;/p&gt; 
&lt;p&gt;you can set hooks before and/or after an event happens, and currently you can hook uploads, moves/renames, and deletes&lt;/p&gt; 
&lt;p&gt;there's a bunch of flags and stuff, see &lt;a href="https://copyparty.eu/cli/#hooks-help-page"&gt;&lt;code&gt;--help-hooks&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;if you want to write your own hooks, see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/devnotes.md#event-hooks"&gt;devnotes&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;zeromq&lt;/h3&gt; 
&lt;p&gt;event-hooks can send zeromq messages instead of running programs&lt;/p&gt; 
&lt;p&gt;to send a 0mq message every time a file is uploaded,&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--xau zmq:pub:tcp://*:5556&lt;/code&gt; sends a PUB to any/all connected SUB clients&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--xau t3,zmq:push:tcp://*:5557&lt;/code&gt; sends a PUSH to exactly one connected PULL client&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--xau t3,j,zmq:req:tcp://localhost:5555&lt;/code&gt; sends a REQ to the connected REP client&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;the PUSH and REQ examples have &lt;code&gt;t3&lt;/code&gt; (timeout after 3 seconds) because they block if there's no clients to talk to&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;the REQ example does &lt;code&gt;t3,j&lt;/code&gt; to send extended upload-info as json instead of just the filesystem-path&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;see &lt;a href="https://github.com/9001/copyparty/raw/hovudstraum/bin/zmq-recv.py"&gt;zmq-recv.py&lt;/a&gt; if you need something to receive the messages with&lt;/p&gt; 
&lt;p&gt;config file example; note that the hooks are additive options, so all of the xau options will take effect:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;[global]
  xau: zmq:pub:tcp://*:5556`  # send a PUB to any/all connected SUB clients
  xau: t3,zmq:push:tcp://*:5557`  # send PUSH to exactly one connected PULL cli
  xau: t3,j,zmq:req:tcp://localhost:5555`  # send REQ to the connected REP cli
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;upload events&lt;/h3&gt; 
&lt;p&gt;the older, more powerful approach (&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/bin/mtag/"&gt;examples&lt;/a&gt;):&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;-v /mnt/inc:inc:w:c,e2d,e2t,mte=+x1:c,mtp=x1=ad,kn,/usr/bin/notify-send
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;that was the commandline example; here's the config file example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;[/inc]
  /mnt/inc
  accs:
    w: *
  flags:
    e2d, e2t  # enable indexing of uploaded files and their tags
    mte: +x1
    mtp: x1=ad,kn,/usr/bin/notify-send
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;so filesystem location &lt;code&gt;/mnt/inc&lt;/code&gt; shared at &lt;code&gt;/inc&lt;/code&gt;, write-only for everyone, appending &lt;code&gt;x1&lt;/code&gt; to the list of tags to index (&lt;code&gt;mte&lt;/code&gt;), and using &lt;code&gt;/usr/bin/notify-send&lt;/code&gt; to "provide" tag &lt;code&gt;x1&lt;/code&gt; for any filetype (&lt;code&gt;ad&lt;/code&gt;) with kill-on-timeout disabled (&lt;code&gt;kn&lt;/code&gt;)&lt;/p&gt; 
&lt;p&gt;that'll run the command &lt;code&gt;notify-send&lt;/code&gt; with the path to the uploaded file as the first and only argument (so on linux it'll show a notification on-screen)&lt;/p&gt; 
&lt;p&gt;note that this is way more complicated than the new &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#event-hooks"&gt;event hooks&lt;/a&gt; but this approach has the following advantages:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;non-blocking and multithreaded; doesn't hold other uploads back&lt;/li&gt; 
 &lt;li&gt;you get access to tags from FFmpeg and other mtp parsers&lt;/li&gt; 
 &lt;li&gt;only trigger on new unique files, not dupes&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;note that it will occupy the parsing threads, so fork anything expensive (or set &lt;code&gt;kn&lt;/code&gt; to have copyparty fork it for you) -- otoh if you want to intentionally queue/singlethread you can combine it with &lt;code&gt;--mtag-mt 1&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;for reference, if you were to do this using event hooks instead, it would be like this: &lt;code&gt;-e2d --xau notify-send,hello,--&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;handlers&lt;/h2&gt; 
&lt;p&gt;redefine behavior with plugins (&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/bin/handlers/"&gt;examples&lt;/a&gt;)&lt;/p&gt; 
&lt;p&gt;replace 404 and 403 errors with something completely different (that's it for now)&lt;/p&gt; 
&lt;p&gt;as for client-side stuff, there is &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/contrib/plugins/"&gt;plugins for modifying UI/UX&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ip auth&lt;/h2&gt; 
&lt;p&gt;autologin based on IP range (CIDR) , using the global-option &lt;code&gt;--ipu&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;for example, if everyone with an IP that starts with &lt;code&gt;192.168.123&lt;/code&gt; should automatically log in as the user &lt;code&gt;spartacus&lt;/code&gt;, then you can either specify &lt;code&gt;--ipu=192.168.123.0/24=spartacus&lt;/code&gt; as a commandline option, or put this in a config file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;[global]
  ipu: 192.168.123.0/24=spartacus
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;repeat the option to map additional subnets&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;be careful with this one!&lt;/strong&gt; if you have a reverseproxy, then you definitely want to make sure you have &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#real-ip"&gt;real-ip&lt;/a&gt; configured correctly, and it's probably a good idea to nullmap the reverseproxy's IP just in case; so if your reverseproxy is sending requests from &lt;code&gt;172.24.27.9&lt;/code&gt; then that would be &lt;code&gt;--ipu=172.24.27.9/32=&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;restrict to ip&lt;/h3&gt; 
&lt;p&gt;limit a user to certain IP ranges (CIDR) , using the global-option &lt;code&gt;--ipr&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;for example, if the user &lt;code&gt;spartacus&lt;/code&gt; should get rejected if they're not connecting from an IP that starts with &lt;code&gt;192.168.123&lt;/code&gt; or &lt;code&gt;172.16&lt;/code&gt;, then you can either specify &lt;code&gt;--ipr=192.168.123.0/24,172.16.0.0/16=spartacus&lt;/code&gt; as a commandline option, or put this in a config file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;[global]
  ipr: 192.168.123.0/24,172.16.0.0/16=spartacus
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;repeat the option to map additional users&lt;/p&gt; 
&lt;h2&gt;identity providers&lt;/h2&gt; 
&lt;p&gt;replace copyparty passwords with oauth and such&lt;/p&gt; 
&lt;p&gt;you can disable the built-in password-based login system, and instead replace it with a separate piece of software (an identity provider) which will then handle authenticating / authorizing of users; this makes it possible to login with passkeys / fido2 / webauthn / yubikey / ldap / active directory / oauth / many other single-sign-on contraptions&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;the regular config-defined users will be used as a fallback for requests which don't include a valid (trusted) IdP username header&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;--auth-ord&lt;/code&gt; configured auth precedence, for example to allow overriding the IdP with a copyparty password&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;the login/logout links/buttons can be replaced with links to your IdP with &lt;code&gt;--idp-login&lt;/code&gt; and &lt;code&gt;--idp-logout&lt;/code&gt; , for example &lt;code&gt;--idp-login /idp/login/?redir={dst}&lt;/code&gt; will expand &lt;code&gt;{dst}&lt;/code&gt; to the page the user was on when clicking Login&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;if your IdP-server is slow, consider &lt;code&gt;--idp-cookie&lt;/code&gt; and let requests with the cookie &lt;code&gt;cppws&lt;/code&gt; bypass the IdP; experimental sessions-based feature added for a party&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;some popular identity providers are &lt;a href="https://www.authelia.com/"&gt;Authelia&lt;/a&gt; (config-file based) and &lt;a href="https://goauthentik.io/"&gt;authentik&lt;/a&gt; (GUI-based, more complex)&lt;/p&gt; 
&lt;p&gt;there is a &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/examples/docker/idp-authelia-traefik"&gt;docker-compose example&lt;/a&gt; which is hopefully a good starting point (alternatively see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/idp.md"&gt;./docs/idp.md&lt;/a&gt; if you're the DIY type)&lt;/p&gt; 
&lt;p&gt;a more complete example of the copyparty configuration options &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/examples/docker/idp/copyparty.conf"&gt;look like this&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;but if you just want to let users change their own passwords, then you probably want &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#user-changeable-passwords"&gt;user-changeable passwords&lt;/a&gt; instead&lt;/p&gt; 
&lt;h3&gt;generic header auth&lt;/h3&gt; 
&lt;p&gt;other ways to auth by header&lt;/p&gt; 
&lt;p&gt;if you have a middleware which adds a header with a user identifier, for example tailscale's &lt;code&gt;Tailscale-User-Login: alice.m@forest.net&lt;/code&gt; then you can automatically auth as &lt;code&gt;alice&lt;/code&gt; by defining that mapping with &lt;code&gt;--idp-hm-usr '^Tailscale-User-Login^alice.m@forest.net^alice'&lt;/code&gt; or the following config file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;[global]
  idp-hm-usr: ^Tailscale-User-Login^alice.m@forest.net^alice
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;repeat the whole &lt;code&gt;idp-hm-usr&lt;/code&gt; option to add more mappings&lt;/p&gt; 
&lt;h2&gt;user-changeable passwords&lt;/h2&gt; 
&lt;p&gt;if permitted, users can change their own passwords in the control-panel&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;not compatible with &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#identity-providers"&gt;identity providers&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;must be enabled with &lt;code&gt;--chpw&lt;/code&gt; because account-sharing is a popular usecase&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;if you want to enable the feature but deny password-changing for a specific list of accounts, you can do that with &lt;code&gt;--chpw-no name1,name2,name3,...&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;to perform a password reset, edit the server config and give the user another password there, then do a &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#server-config"&gt;config reload&lt;/a&gt; or server restart&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;the custom passwords are kept in a textfile at filesystem-path &lt;code&gt;--chpw-db&lt;/code&gt;, by default &lt;code&gt;chpw.json&lt;/code&gt; in the copyparty config folder&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;if you run multiple copyparty instances with different users you &lt;em&gt;almost definitely&lt;/em&gt; want to specify separate DBs for each instance&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;if &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#password-hashing"&gt;password hashing&lt;/a&gt; is enabled, the passwords in the db are also hashed&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;...which means that all user-defined passwords will be forgotten if you change password-hashing settings&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;using the cloud as storage&lt;/h2&gt; 
&lt;p&gt;connecting to an aws s3 bucket and similar&lt;/p&gt; 
&lt;p&gt;there is no built-in support for this, but you can use FUSE-software such as &lt;a href="https://rclone.org/"&gt;rclone&lt;/a&gt; / &lt;a href="https://github.com/yandex-cloud/geesefs"&gt;geesefs&lt;/a&gt; / &lt;a href="https://juicefs.com/en/"&gt;JuiceFS&lt;/a&gt; to first mount your cloud storage as a local disk, and then let copyparty use (a folder in) that disk as a volume&lt;/p&gt; 
&lt;p&gt;if copyparty is unable to access the local folder that rclone/geesefs/JuiceFS provides (for example if it looks invisible) then you may need to run rclone with &lt;code&gt;--allow-other&lt;/code&gt; and/or enable &lt;code&gt;user_allow_other&lt;/code&gt; in &lt;code&gt;/etc/fuse.conf&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;you will probably get decent speeds with the default config, however most likely restricted to using one TCP connection per file, so the upload-client won't be able to send multiple chunks in parallel&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;before &lt;a href="https://github.com/9001/copyparty/releases/tag/v1.13.5"&gt;v1.13.5&lt;/a&gt; it was recommended to use the volflag &lt;code&gt;sparse&lt;/code&gt; to force-allow multiple chunks in parallel; this would improve the upload-speed from &lt;code&gt;1.5 MiB/s&lt;/code&gt; to over &lt;code&gt;80 MiB/s&lt;/code&gt; at the risk of provoking latent bugs in S3 or JuiceFS. But v1.13.5 added chunk-stitching, so this is now probably much less important. On the contrary, &lt;code&gt;nosparse&lt;/code&gt; &lt;em&gt;may&lt;/em&gt; now increase performance in some cases. Please try all three options (default, &lt;code&gt;sparse&lt;/code&gt;, &lt;code&gt;nosparse&lt;/code&gt;) as the optimal choice depends on your network conditions and software stack (both the FUSE-driver and cloud-server)&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;someone has also tested geesefs in combination with &lt;a href="https://nuetzlich.net/gocryptfs/"&gt;gocryptfs&lt;/a&gt; with surprisingly good results, getting 60 MiB/s upload speeds on a gbit line, but JuiceFS won with 80 MiB/s using its built-in encryption&lt;/p&gt; 
&lt;p&gt;you may improve performance by specifying larger values for &lt;code&gt;--iobuf&lt;/code&gt; / &lt;code&gt;--s-rd-sz&lt;/code&gt; / &lt;code&gt;--s-wr-sz&lt;/code&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;if you've experimented with this and made interesting observations, please share your findings so we can add a section with specific recommendations :-)&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;hiding from google&lt;/h2&gt; 
&lt;p&gt;tell search engines you don't wanna be indexed, either using the good old &lt;a href="https://www.robotstxt.org/robotstxt.html"&gt;robots.txt&lt;/a&gt; or through copyparty settings:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--no-robots&lt;/code&gt; adds HTTP (&lt;code&gt;X-Robots-Tag&lt;/code&gt;) and HTML (&lt;code&gt;&amp;lt;meta&amp;gt;&lt;/code&gt;) headers with &lt;code&gt;noindex, nofollow&lt;/code&gt; globally&lt;/li&gt; 
 &lt;li&gt;volflag &lt;code&gt;[...]:c,norobots&lt;/code&gt; does the same thing for that single volume&lt;/li&gt; 
 &lt;li&gt;volflag &lt;code&gt;[...]:c,robots&lt;/code&gt; ALLOWS search-engine crawling for that volume, even if &lt;code&gt;--no-robots&lt;/code&gt; is set globally&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;also, &lt;code&gt;--force-js&lt;/code&gt; disables the plain HTML folder listing, making things harder to parse for &lt;em&gt;some&lt;/em&gt; search engines -- note that crawlers which understand javascript (such as google) will not be affected&lt;/p&gt; 
&lt;h2&gt;themes&lt;/h2&gt; 
&lt;p&gt;you can change the default theme with &lt;code&gt;--theme 2&lt;/code&gt;, and add your own themes by modifying &lt;code&gt;browser.css&lt;/code&gt; or providing your own css to &lt;code&gt;--css-browser&lt;/code&gt;, then telling copyparty they exist by increasing &lt;code&gt;--themes&lt;/code&gt;&lt;/p&gt; 
&lt;table&gt;
 &lt;tbody&gt;
  &lt;tr&gt;
   &lt;td width="33%" align="center"&gt;&lt;a href="https://user-images.githubusercontent.com/241032/165864907-17e2ac7d-319d-4f25-8718-2f376f614b51.png"&gt;&lt;img src="https://user-images.githubusercontent.com/241032/165867551-fceb35dd-38f0-42bb-bef3-25ba651ca69b.png" /&gt;&lt;/a&gt; 0. classic dark&lt;/td&gt;
   &lt;td width="33%" align="center"&gt;&lt;a href="https://user-images.githubusercontent.com/241032/168644399-68938de5-da9b-445f-8d92-b51c74b5f345.png"&gt;&lt;img src="https://user-images.githubusercontent.com/241032/168644404-8e1a2fdc-6e59-4c41-905e-ba5399ed686f.png" /&gt;&lt;/a&gt; 2. flat pm-monokai&lt;/td&gt;
   &lt;td width="33%" align="center"&gt;&lt;a href="https://user-images.githubusercontent.com/241032/165864901-db13a429-a5da-496d-8bc6-ce838547f69d.png"&gt;&lt;img src="https://user-images.githubusercontent.com/241032/165867560-aa834aef-58dc-4abe-baef-7e562b647945.png" /&gt;&lt;/a&gt; 4. vice&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td align="center"&gt;&lt;a href="https://user-images.githubusercontent.com/241032/165864905-692682eb-6fb4-4d40-b6fe-27d2c7d3e2a7.png"&gt;&lt;img src="https://user-images.githubusercontent.com/241032/165867555-080b73b6-6d85-41bb-a7c6-ad277c608365.png" /&gt;&lt;/a&gt; 1. classic light&lt;/td&gt;
   &lt;td align="center"&gt;&lt;a href="https://user-images.githubusercontent.com/241032/168645276-fb02fd19-190a-407a-b8d3-d58fee277e02.png"&gt;&lt;img src="https://user-images.githubusercontent.com/241032/168645280-f0662b3c-9764-4875-a2e2-d91cc8199b23.png" /&gt;&lt;/a&gt; 3. flat light &lt;/td&gt;
   &lt;td align="center"&gt;&lt;a href="https://user-images.githubusercontent.com/241032/165864898-10ce7052-a117-4fcf-845b-b56c91687908.png"&gt;&lt;img src="https://user-images.githubusercontent.com/241032/165867562-f3003d45-dd2a-4564-8aae-fed44c1ae064.png" /&gt;&lt;/a&gt; 5. &lt;a href="https://blog.codinghorror.com/a-tribute-to-the-windows-31-hot-dog-stand-color-scheme/"&gt;hotdog stand&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;p&gt;the classname of the HTML tag is set according to the selected theme, which is used to set colors as css variables ++&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;each theme &lt;em&gt;generally&lt;/em&gt; has a dark theme (even numbers) and a light theme (odd numbers), showing in pairs&lt;/li&gt; 
 &lt;li&gt;the first theme (theme 0 and 1) is &lt;code&gt;html.a&lt;/code&gt;, second theme (2 and 3) is &lt;code&gt;html.b&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;if a light theme is selected, &lt;code&gt;html.y&lt;/code&gt; is set, otherwise &lt;code&gt;html.z&lt;/code&gt; is&lt;/li&gt; 
 &lt;li&gt;so if the dark edition of the 2nd theme is selected, you use any of &lt;code&gt;html.b&lt;/code&gt;, &lt;code&gt;html.z&lt;/code&gt;, &lt;code&gt;html.bz&lt;/code&gt; to specify rules&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;see the top of &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/copyparty/web/browser.css"&gt;./copyparty/web/browser.css&lt;/a&gt; where the color variables are set, and there's layout-specific stuff near the bottom&lt;/p&gt; 
&lt;p&gt;if you want to change the fonts, see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/rice/"&gt;./docs/rice/&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;complete examples&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/examples/windows.md"&gt;running on windows&lt;/a&gt; for a fancy windows setup&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;or use any of the examples below, just replace &lt;code&gt;python copyparty-sfx.py&lt;/code&gt; with &lt;code&gt;copyparty.exe&lt;/code&gt; if you're using the exe edition&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;allow anyone to download or upload files into the current folder:&lt;br /&gt; &lt;code&gt;python copyparty-sfx.py&lt;/code&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;enable searching and music indexing with &lt;code&gt;-e2dsa -e2ts&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;start an FTP server on port 3921 with &lt;code&gt;--ftp 3921&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;announce it on your LAN with &lt;code&gt;-z&lt;/code&gt; so it appears in windows/Linux file managers&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;anyone can upload, but nobody can see any files (even the uploader):&lt;br /&gt; &lt;code&gt;python copyparty-sfx.py -e2dsa -v .::w&lt;/code&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;block uploads if there's less than 4 GiB free disk space with &lt;code&gt;--df 4&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;show a popup on new uploads with &lt;code&gt;--xau bin/hooks/notify.py&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;anyone can upload, and receive "secret" links for each upload they do:&lt;br /&gt; &lt;code&gt;python copyparty-sfx.py -e2dsa -v .::wG:c,fk=8&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;anyone can browse (&lt;code&gt;r&lt;/code&gt;), only &lt;code&gt;kevin&lt;/code&gt; (password &lt;code&gt;okgo&lt;/code&gt;) can upload/move/delete (&lt;code&gt;A&lt;/code&gt;) files:&lt;br /&gt; &lt;code&gt;python copyparty-sfx.py -e2dsa -a kevin:okgo -v .::r:A,kevin&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;read-only music server:&lt;br /&gt; &lt;code&gt;python copyparty-sfx.py -v /mnt/nas/music:/music:r -e2dsa -e2ts --no-robots --force-js --theme 2&lt;/code&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;...with bpm and key scanning&lt;br /&gt; &lt;code&gt;-mtp .bpm=f,audio-bpm.py -mtp key=f,audio-key.py&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;...with a read-write folder for &lt;code&gt;kevin&lt;/code&gt; whose password is &lt;code&gt;okgo&lt;/code&gt;&lt;br /&gt; &lt;code&gt;-a kevin:okgo -v /mnt/nas/inc:/inc:rw,kevin&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;...with logging to disk&lt;br /&gt; &lt;code&gt;-lo log/cpp-%Y-%m%d-%H%M%S.txt.xz&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;listen on port 80 and 443&lt;/h2&gt; 
&lt;p&gt;become a &lt;em&gt;real&lt;/em&gt; webserver which people can access by just going to your IP or domain without specifying a port&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;if you're on windows,&lt;/strong&gt; then you just need to add the commandline argument &lt;code&gt;-p 80,443&lt;/code&gt; and you're done! nice&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;if you're on macos,&lt;/strong&gt; sorry, I don't know&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;if you're on Linux,&lt;/strong&gt; you have the following 4 options:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;option 1:&lt;/strong&gt; set up a &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#reverse-proxy"&gt;reverse-proxy&lt;/a&gt; -- this one makes a lot of sense if you're running on a proper headless server, because that way you get real HTTPS too&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;option 2:&lt;/strong&gt; NAT to port 3923 -- this is cumbersome since you'll need to do it every time you reboot, and the exact command may depend on your linux distribution:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;iptables -t nat -A PREROUTING -p tcp --dport 80 -j REDIRECT --to-port 3923
iptables -t nat -A PREROUTING -p tcp --dport 443 -j REDIRECT --to-port 3923
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;option 3:&lt;/strong&gt; disable the &lt;a href="https://www.w3.org/Daemon/User/Installation/PrivilegedPorts.html"&gt;security policy&lt;/a&gt; which prevents the use of 80 and 443; this is &lt;em&gt;probably&lt;/em&gt; fine:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;setcap CAP_NET_BIND_SERVICE=+eip $(realpath $(which python))
python copyparty-sfx.py -p 80,443
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;option 4:&lt;/strong&gt; run copyparty as root (please don't)&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;reverse-proxy&lt;/h2&gt; 
&lt;p&gt;running copyparty next to other websites hosted on an existing webserver such as nginx, caddy, or apache&lt;/p&gt; 
&lt;p&gt;you can either:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;give copyparty its own domain or subdomain (recommended)&lt;/li&gt; 
 &lt;li&gt;or do location-based proxying, using &lt;code&gt;--rp-loc=/stuff&lt;/code&gt; to tell copyparty where it is mounted -- has a slight performance cost and higher chance of bugs 
  &lt;ul&gt; 
   &lt;li&gt;if copyparty says &lt;code&gt;incorrect --rp-loc or webserver config; expected vpath starting with [...]&lt;/code&gt; it's likely because the webserver is stripping away the proxy location from the request URLs -- see the &lt;code&gt;ProxyPass&lt;/code&gt; in the apache example below&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;when running behind a reverse-proxy (this includes services like cloudflare), it is important to configure real-ip correctly, as many features rely on knowing the client's IP. The best/safest approach is to configure your reverse-proxy so it gives copyparty a header which only contains the client's true/real IP-address, and then setting &lt;code&gt;--xff-hdr theHeaderName --rproxy 1&lt;/code&gt; but alternatively, if you want/need to let copyparty handle this, look out for red and yellow log messages which explain how to do that. Basically, the log will say this:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;set &lt;code&gt;--xff-hdr&lt;/code&gt; to the name of the http-header to read the IP from (usually &lt;code&gt;x-forwarded-for&lt;/code&gt;, but cloudflare uses &lt;code&gt;cf-connecting-ip&lt;/code&gt;), and then &lt;code&gt;--xff-src&lt;/code&gt; to the IP of the reverse-proxy so copyparty will trust the xff-hdr. You will also need to configure &lt;code&gt;--rproxy&lt;/code&gt; to &lt;code&gt;1&lt;/code&gt; if the header only contains one IP (the correct one) or to a &lt;em&gt;negative value&lt;/em&gt; if it contains multiple; &lt;code&gt;-1&lt;/code&gt; being the rightmost and most trusted IP (the nearest proxy, so usually not the correct one), &lt;code&gt;-2&lt;/code&gt; being the second-closest hop, and so on&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Note that &lt;code&gt;--rp-loc&lt;/code&gt; in particular will not work at all unless you configure the above correctly&lt;/p&gt; 
&lt;p&gt;some reverse proxies (such as &lt;a href="https://caddyserver.com/"&gt;Caddy&lt;/a&gt;) can automatically obtain a valid https/tls certificate for you, and some support HTTP/2 and QUIC which &lt;em&gt;could&lt;/em&gt; be a nice speed boost, depending on a lot of factors&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;warning:&lt;/strong&gt; nginx-QUIC (HTTP/3) is still experimental and can make uploads much slower, so HTTP/1.1 is recommended for now&lt;/li&gt; 
 &lt;li&gt;depending on server/client, HTTP/1.1 can also be 5x faster than HTTP/2&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;for improved security (and a 10% performance boost) consider listening on a unix-socket with &lt;code&gt;-i unix:770:www:/dev/shm/party.sock&lt;/code&gt; (permission &lt;code&gt;770&lt;/code&gt; means only members of group &lt;code&gt;www&lt;/code&gt; can access it)&lt;/p&gt; 
&lt;p&gt;example webserver / reverse-proxy configs:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/contrib/apache/copyparty.conf"&gt;apache config&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;caddy uds: &lt;code&gt;caddy reverse-proxy --from :8080 --to unix///dev/shm/party.sock&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;caddy tcp: &lt;code&gt;caddy reverse-proxy --from :8081 --to http://127.0.0.1:3923&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/contrib/haproxy/copyparty.conf"&gt;haproxy config&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/contrib/lighttpd/subdomain.conf"&gt;lighttpd subdomain&lt;/a&gt; -- entire domain/subdomain&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/contrib/lighttpd/subpath.conf"&gt;lighttpd subpath&lt;/a&gt; -- location-based (not optimal, but in case you need it)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/contrib/nginx/copyparty.conf"&gt;nginx config&lt;/a&gt; -- recommended&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/contrib/traefik/copyparty.yaml"&gt;traefik config&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;real-ip&lt;/h3&gt; 
&lt;p&gt;teaching copyparty how to see client IPs when running behind a reverse-proxy, or a WAF, or another protection service such as cloudflare&lt;/p&gt; 
&lt;p&gt;if you (and maybe everybody else) keep getting a message that says &lt;code&gt;thank you for playing&lt;/code&gt;, then you've gotten banned for malicious traffic. This ban applies to the IP address that copyparty &lt;em&gt;thinks&lt;/em&gt; identifies the shady client -- so, depending on your setup, you might have to tell copyparty where to find the correct IP&lt;/p&gt; 
&lt;p&gt;for most common setups, there should be a helpful message in the server-log explaining what to do, but see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/xff.md"&gt;docs/xff.md&lt;/a&gt; if you want to learn more, including a quick hack to &lt;strong&gt;just make it work&lt;/strong&gt; (which is &lt;strong&gt;not&lt;/strong&gt; recommended, but hey...)&lt;/p&gt; 
&lt;h3&gt;reverse-proxy performance&lt;/h3&gt; 
&lt;p&gt;most reverse-proxies support connecting to copyparty either using uds/unix-sockets (&lt;code&gt;/dev/shm/party.sock&lt;/code&gt;, faster/recommended) or using tcp (&lt;code&gt;127.0.0.1&lt;/code&gt;)&lt;/p&gt; 
&lt;p&gt;with copyparty listening on a uds / unix-socket / unix-domain-socket and the reverse-proxy connecting to that:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;index.html&lt;/th&gt; 
   &lt;th&gt;upload&lt;/th&gt; 
   &lt;th&gt;download&lt;/th&gt; 
   &lt;th&gt;software&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;28'900 req/s&lt;/td&gt; 
   &lt;td&gt;6'900 MiB/s&lt;/td&gt; 
   &lt;td&gt;7'400 MiB/s&lt;/td&gt; 
   &lt;td&gt;no-proxy&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;18'750 req/s&lt;/td&gt; 
   &lt;td&gt;3'500 MiB/s&lt;/td&gt; 
   &lt;td&gt;2'370 MiB/s&lt;/td&gt; 
   &lt;td&gt;haproxy&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;9'900 req/s&lt;/td&gt; 
   &lt;td&gt;3'750 MiB/s&lt;/td&gt; 
   &lt;td&gt;2'200 MiB/s&lt;/td&gt; 
   &lt;td&gt;caddy&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;18'700 req/s&lt;/td&gt; 
   &lt;td&gt;2'200 MiB/s&lt;/td&gt; 
   &lt;td&gt;1'570 MiB/s&lt;/td&gt; 
   &lt;td&gt;nginx&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;9'700 req/s&lt;/td&gt; 
   &lt;td&gt;1'750 MiB/s&lt;/td&gt; 
   &lt;td&gt;1'830 MiB/s&lt;/td&gt; 
   &lt;td&gt;apache&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;9'900 req/s&lt;/td&gt; 
   &lt;td&gt;1'300 MiB/s&lt;/td&gt; 
   &lt;td&gt;1'470 MiB/s&lt;/td&gt; 
   &lt;td&gt;lighttpd&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;when connecting the reverse-proxy to &lt;code&gt;127.0.0.1&lt;/code&gt; instead (the basic and/or old-fasioned way), speeds are a bit worse:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;index.html&lt;/th&gt; 
   &lt;th&gt;upload&lt;/th&gt; 
   &lt;th&gt;download&lt;/th&gt; 
   &lt;th&gt;software&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;21'200 req/s&lt;/td&gt; 
   &lt;td&gt;5'700 MiB/s&lt;/td&gt; 
   &lt;td&gt;6'700 MiB/s&lt;/td&gt; 
   &lt;td&gt;no-proxy&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;14'500 req/s&lt;/td&gt; 
   &lt;td&gt;1'700 MiB/s&lt;/td&gt; 
   &lt;td&gt;2'170 MiB/s&lt;/td&gt; 
   &lt;td&gt;haproxy&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;11'100 req/s&lt;/td&gt; 
   &lt;td&gt;2'750 MiB/s&lt;/td&gt; 
   &lt;td&gt;2'000 MiB/s&lt;/td&gt; 
   &lt;td&gt;traefik&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;8'400 req/s&lt;/td&gt; 
   &lt;td&gt;2'300 MiB/s&lt;/td&gt; 
   &lt;td&gt;1'950 MiB/s&lt;/td&gt; 
   &lt;td&gt;caddy&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;13'400 req/s&lt;/td&gt; 
   &lt;td&gt;1'100 MiB/s&lt;/td&gt; 
   &lt;td&gt;1'480 MiB/s&lt;/td&gt; 
   &lt;td&gt;nginx&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;8'400 req/s&lt;/td&gt; 
   &lt;td&gt;1'000 MiB/s&lt;/td&gt; 
   &lt;td&gt;1'000 MiB/s&lt;/td&gt; 
   &lt;td&gt;apache&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;6'500 req/s&lt;/td&gt; 
   &lt;td&gt;1'270 MiB/s&lt;/td&gt; 
   &lt;td&gt;1'500 MiB/s&lt;/td&gt; 
   &lt;td&gt;lighttpd&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;in summary, &lt;code&gt;haproxy &amp;gt; caddy &amp;gt; traefik &amp;gt; nginx &amp;gt; apache &amp;gt; lighttpd&lt;/code&gt;, and use uds when possible (traefik does not support it yet)&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;if these results are bullshit because my config examples are bad, please submit corrections!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;permanent cloudflare tunnel&lt;/h2&gt; 
&lt;p&gt;if you have a domain and want to get your copyparty online real quick, either from your home-PC behind a CGNAT or from a server without an existing &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#reverse-proxy"&gt;reverse-proxy&lt;/a&gt; setup, one approach is to create a &lt;a href="https://developers.cloudflare.com/cloudflare-one/connections/connect-networks/get-started/"&gt;Cloudflare Tunnel&lt;/a&gt; (formerly "Argo Tunnel")&lt;/p&gt; 
&lt;p&gt;I'd recommend making a &lt;code&gt;Locally-managed tunnel&lt;/code&gt; for more control, but if you prefer to make a &lt;code&gt;Remotely-managed tunnel&lt;/code&gt; then this is currently how:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;cloudflare dashboard&lt;/code&gt; ¬ª &lt;code&gt;zero trust&lt;/code&gt; ¬ª &lt;code&gt;networks&lt;/code&gt; ¬ª &lt;code&gt;tunnels&lt;/code&gt; ¬ª &lt;code&gt;create a tunnel&lt;/code&gt; ¬ª &lt;code&gt;cloudflared&lt;/code&gt; ¬ª choose a cool &lt;code&gt;subdomain&lt;/code&gt; and leave the &lt;code&gt;path&lt;/code&gt; blank, and use &lt;code&gt;service type&lt;/code&gt; = &lt;code&gt;http&lt;/code&gt; and &lt;code&gt;URL&lt;/code&gt; = &lt;code&gt;127.0.0.1:3923&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;and if you want to just run the tunnel without installing it, skip the &lt;code&gt;cloudflared service install BASE64&lt;/code&gt; step and instead do &lt;code&gt;cloudflared --no-autoupdate tunnel run --token BASE64&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;NOTE: since people will be connecting through cloudflare, as mentioned in &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#real-ip"&gt;real-ip&lt;/a&gt; you should run copyparty with &lt;code&gt;--xff-hdr cf-connecting-ip&lt;/code&gt; to detect client IPs correctly&lt;/p&gt; 
&lt;p&gt;config file example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;[global]
  xff-hdr: cf-connecting-ip
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;prometheus&lt;/h2&gt; 
&lt;p&gt;metrics/stats can be enabled at URL &lt;code&gt;/.cpr/metrics&lt;/code&gt; for grafana / prometheus / etc (openmetrics 1.0.0)&lt;/p&gt; 
&lt;p&gt;must be enabled with &lt;code&gt;--stats&lt;/code&gt; since it reduces startup time a tiny bit, and you probably want &lt;code&gt;-e2dsa&lt;/code&gt; too&lt;/p&gt; 
&lt;p&gt;the endpoint is only accessible by &lt;code&gt;admin&lt;/code&gt; accounts, meaning the &lt;code&gt;a&lt;/code&gt; in &lt;code&gt;rwmda&lt;/code&gt; in the following example commandline: &lt;code&gt;python3 -m copyparty -a ed:wark -v /mnt/nas::rwmda,ed --stats -e2dsa&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;follow a guide for setting up &lt;code&gt;node_exporter&lt;/code&gt; except have it read from copyparty instead; example &lt;code&gt;/etc/prometheus/prometheus.yml&lt;/code&gt; below&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;scrape_configs:
  - job_name: copyparty
    metrics_path: /.cpr/metrics
    basic_auth:
      password: wark
    static_configs:
      - targets: ['192.168.123.1:3923']
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;currently the following metrics are available,&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;cpp_uptime_seconds&lt;/code&gt; time since last copyparty restart&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;cpp_boot_unixtime_seconds&lt;/code&gt; same but as an absolute timestamp&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;cpp_active_dl&lt;/code&gt; number of active downloads&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;cpp_http_conns&lt;/code&gt; number of open http(s) connections&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;cpp_http_reqs&lt;/code&gt; number of http(s) requests handled&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;cpp_sus_reqs&lt;/code&gt; number of 403/422/malicious requests&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;cpp_active_bans&lt;/code&gt; number of currently banned IPs&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;cpp_total_bans&lt;/code&gt; number of IPs banned since last restart&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;these are available unless &lt;code&gt;--nos-vst&lt;/code&gt; is specified:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;cpp_db_idle_seconds&lt;/code&gt; time since last database activity (upload/rename/delete)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;cpp_db_act_seconds&lt;/code&gt; same but as an absolute timestamp&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;cpp_idle_vols&lt;/code&gt; number of volumes which are idle / ready&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;cpp_busy_vols&lt;/code&gt; number of volumes which are busy / indexing&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;cpp_offline_vols&lt;/code&gt; number of volumes which are offline / unavailable&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;cpp_hashing_files&lt;/code&gt; number of files queued for hashing / indexing&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;cpp_tagq_files&lt;/code&gt; number of files queued for metadata scanning&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;cpp_mtpq_files&lt;/code&gt; number of files queued for plugin-based analysis&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;and these are available per-volume only:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;cpp_disk_size_bytes&lt;/code&gt; total HDD size&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;cpp_disk_free_bytes&lt;/code&gt; free HDD space&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;and these are per-volume and &lt;code&gt;total&lt;/code&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;cpp_vol_bytes&lt;/code&gt; size of all files in volume&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;cpp_vol_files&lt;/code&gt; number of files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;cpp_dupe_bytes&lt;/code&gt; disk space presumably saved by deduplication&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;cpp_dupe_files&lt;/code&gt; number of dupe files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;cpp_unf_bytes&lt;/code&gt; currently unfinished / incoming uploads&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;some of the metrics have additional requirements to function correctly,&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;cpp_vol_*&lt;/code&gt; requires either the &lt;code&gt;e2ds&lt;/code&gt; volflag or &lt;code&gt;-e2dsa&lt;/code&gt; global-option&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;the following options are available to disable some of the metrics:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--nos-hdd&lt;/code&gt; disables &lt;code&gt;cpp_disk_*&lt;/code&gt; which can prevent spinning up HDDs&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--nos-vol&lt;/code&gt; disables &lt;code&gt;cpp_vol_*&lt;/code&gt; which reduces server startup time&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--nos-vst&lt;/code&gt; disables volume state, reducing the worst-case prometheus query time by 0.5 sec&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--nos-dup&lt;/code&gt; disables &lt;code&gt;cpp_dupe_*&lt;/code&gt; which reduces the server load caused by prometheus queries&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--nos-unf&lt;/code&gt; disables &lt;code&gt;cpp_unf_*&lt;/code&gt; for no particular purpose&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;note: the following metrics are counted incorrectly if multiprocessing is enabled with &lt;code&gt;-j&lt;/code&gt;: &lt;code&gt;cpp_http_conns&lt;/code&gt;, &lt;code&gt;cpp_http_reqs&lt;/code&gt;, &lt;code&gt;cpp_sus_reqs&lt;/code&gt;, &lt;code&gt;cpp_active_bans&lt;/code&gt;, &lt;code&gt;cpp_total_bans&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;other extremely specific features&lt;/h2&gt; 
&lt;p&gt;you'll never find a use for these:&lt;/p&gt; 
&lt;h3&gt;custom mimetypes&lt;/h3&gt; 
&lt;p&gt;change the association of a file extension&lt;/p&gt; 
&lt;p&gt;using commandline args, you can do something like &lt;code&gt;--mime gif=image/jif&lt;/code&gt; and &lt;code&gt;--mime ts=text/x.typescript&lt;/code&gt; (can be specified multiple times)&lt;/p&gt; 
&lt;p&gt;in a config file, this is the same as:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;[global]
  mime: gif=image/jif
  mime: ts=text/x.typescript
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;run copyparty with &lt;code&gt;--mimes&lt;/code&gt; to list all the default mappings&lt;/p&gt; 
&lt;h3&gt;GDPR compliance&lt;/h3&gt; 
&lt;p&gt;imagine using copyparty professionally... &lt;strong&gt;TINLA/IANAL; EU laws are hella confusing&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;remember to disable logging, or configure logrotation to an acceptable timeframe with &lt;code&gt;-lo cpp-%Y-%m%d.txt.xz&lt;/code&gt; or similar&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;if running with the database enabled (recommended), then have it forget uploader-IPs after some time using &lt;code&gt;--forget-ip 43200&lt;/code&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;don't set it too low; &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#unpost"&gt;unposting&lt;/a&gt; a file is no longer possible after this takes effect&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;if you actually &lt;em&gt;are&lt;/em&gt; a lawyer then I'm open for feedback, would be fun&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;feature chickenbits&lt;/h3&gt; 
&lt;p&gt;buggy feature? rip it out by setting any of the following environment variables to disable its associated bell or whistle,&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;env-var&lt;/th&gt; 
   &lt;th&gt;what it does&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_NO_DB_LOCK&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;do not lock session/shares-databases for exclusive access&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_NO_IFADDR&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;disable ip/nic discovery by poking into your OS with ctypes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_NO_IMPRESO&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;do not try to load js/css files using &lt;code&gt;importlib.resources&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_NO_IPV6&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;disable some ipv6 support (should not be necessary since windows 2000)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_NO_LZMA&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;disable streaming xz compression of incoming uploads&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_NO_MP&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;disable all use of the python &lt;code&gt;multiprocessing&lt;/code&gt; module (actual multithreading, cpu-count for parsers/thumbnailers)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_NO_SQLITE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;disable all database-related functionality (file indexing, metadata indexing, most file deduplication logic)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_NO_TLS&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;disable native HTTPS support; if you still want to accept HTTPS connections then TLS must now be terminated by a reverse-proxy&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_NO_TPOKE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;disable systemd-tmpfilesd avoider&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_UNSAFE_STATE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;allow storing secrets into emergency-fallback locations&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;example: &lt;code&gt;PRTY_NO_IFADDR=1 python3 copyparty-sfx.py&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;feature beefybits&lt;/h3&gt; 
&lt;p&gt;force-enable features with known issues on your OS/env by setting any of the following environment variables, also affectionately known as &lt;code&gt;fuckitbits&lt;/code&gt; or &lt;code&gt;hail-mary-bits&lt;/code&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;env-var&lt;/th&gt; 
   &lt;th&gt;what it does&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_FORCE_MP&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;force-enable multiprocessing (real multithreading) on MacOS and other broken platforms&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_FORCE_MAGIC&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;use &lt;a href="https://pypi.org/project/python-magic/"&gt;magic&lt;/a&gt; on Windows (you will segfault)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;packages&lt;/h1&gt; 
&lt;p&gt;the party might be closer than you think&lt;/p&gt; 
&lt;p&gt;if your distro/OS is not mentioned below, there might be some hints in the &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#on-servers"&gt;¬´on servers¬ª&lt;/a&gt; section&lt;/p&gt; 
&lt;h2&gt;arch package&lt;/h2&gt; 
&lt;p&gt;&lt;code&gt;pacman -S copyparty&lt;/code&gt; (in &lt;a href="https://archlinux.org/packages/extra/any/copyparty/"&gt;arch linux extra&lt;/a&gt;)&lt;/p&gt; 
&lt;p&gt;it comes with a &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/contrib/systemd/copyparty@.service"&gt;systemd service&lt;/a&gt; as well as a &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/contrib/systemd/copyparty-user.service"&gt;user service&lt;/a&gt;, and expects to find a &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/contrib/systemd/copyparty.example.conf"&gt;config file&lt;/a&gt; in &lt;code&gt;/etc/copyparty/copyparty.conf&lt;/code&gt; or &lt;code&gt;~/.config/copyparty/copyparty.conf&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;after installing, start either the system service or the user service and navigate to &lt;a href="http://127.0.0.1:3923"&gt;http://127.0.0.1:3923&lt;/a&gt; for further instructions (unless you already edited the config files, in which case you are good to go, probably)&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;to start the systemd service, either do &lt;code&gt;systemctl start --user copyparty&lt;/code&gt; to start it as your own user, or &lt;code&gt;systemctl start copyparty@bob&lt;/code&gt; to use unix-user &lt;code&gt;bob&lt;/code&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;fedora package&lt;/h2&gt; 
&lt;p&gt;does not exist yet; there are rumours that it is being packaged! keep an eye on this space...&lt;/p&gt; 
&lt;h2&gt;homebrew formulae&lt;/h2&gt; 
&lt;p&gt;&lt;code&gt;brew install copyparty ffmpeg&lt;/code&gt; -- &lt;a href="https://formulae.brew.sh/formula/copyparty"&gt;https://formulae.brew.sh/formula/copyparty&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;should work on all macs (both intel and apple silicon) and all relevant macos versions&lt;/p&gt; 
&lt;p&gt;the homebrew package is maintained by the homebrew team (thanks!)&lt;/p&gt; 
&lt;h2&gt;nix package&lt;/h2&gt; 
&lt;p&gt;&lt;code&gt;nix profile install github:9001/copyparty&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;requires a &lt;a href="https://nixos.wiki/wiki/Flakes"&gt;flake-enabled&lt;/a&gt; installation of nix&lt;/p&gt; 
&lt;p&gt;some recommended dependencies are enabled by default; &lt;a href="https://github.com/9001/copyparty/raw/hovudstraum/contrib/package/nix/copyparty/default.nix#L3-L22"&gt;override the package&lt;/a&gt; if you want to add/remove some features/deps&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;ffmpeg-full&lt;/code&gt; was chosen over &lt;code&gt;ffmpeg-headless&lt;/code&gt; mainly because we need &lt;code&gt;withWebp&lt;/code&gt; (and &lt;code&gt;withOpenmpt&lt;/code&gt; is also nice) and being able to use a cached build felt more important than optimizing for size at the time -- PRs welcome if you disagree üëç&lt;/p&gt; 
&lt;h2&gt;nixos module&lt;/h2&gt; 
&lt;p&gt;for &lt;a href="https://nixos.wiki/wiki/Flakes"&gt;flake-enabled&lt;/a&gt; installations of NixOS:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-nix"&gt;{
  # add copyparty flake to your inputs
  inputs.copyparty.url = "github:9001/copyparty";

  # ensure that copyparty is an allowed argument to the outputs function
  outputs = { self, nixpkgs, copyparty }: {
    nixosConfigurations.yourHostName = nixpkgs.lib.nixosSystem {
      modules = [
        # load the copyparty NixOS module
        copyparty.nixosModules.default
        ({ pkgs, ... }: {
          # add the copyparty overlay to expose the package to the module
          nixpkgs.overlays = [ copyparty.overlays.default ];
          # (optional) install the package globally
          environment.systemPackages = [ pkgs.copyparty ];
          # configure the copyparty module
          services.copyparty.enable = true;
        })
      ];
    };
  };
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;if you don't use a flake in your configuration, you can use other dependency management tools like &lt;a href="https://github.com/andir/npins"&gt;npins&lt;/a&gt;, &lt;a href="https://github.com/nmattia/niv"&gt;niv&lt;/a&gt;, or even plain &lt;a href="https://nix.dev/manual/nix/stable/language/builtins#builtins-fetchTarball"&gt;&lt;code&gt;fetchTarball&lt;/code&gt;&lt;/a&gt;, like so:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-nix"&gt;{ pkgs, ... }:

let
  # npins example, adjust for your setup. copyparty should be a path to the downloaded repo
  # for niv, just replace the npins folder import with the sources.nix file
  copyparty = (import ./npins).copyparty;

  # or with fetchTarball:
  copyparty = fetchTarball "https://github.com/9001/copyparty/archive/hovudstraum.tar.gz";
in

{
  # load the copyparty NixOS module
  imports = [ "${copyparty}/contrib/nixos/modules/copyparty.nix" ];

  # add the copyparty overlay to expose the package to the module
  nixpkgs.overlays = [ (import "${copyparty}/contrib/package/nix/overlay.nix") ];
  # (optional) install the package globally
  environment.systemPackages = [ pkgs.copyparty ];
  # configure the copyparty module
  services.copyparty.enable = true;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;copyparty on NixOS is configured via &lt;code&gt;services.copyparty&lt;/code&gt; options, for example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-nix"&gt;services.copyparty = {
  enable = true;
  # the user to run the service as
  user = "copyparty"; 
  # the group to run the service as
  group = "copyparty"; 
  # directly maps to values in the [global] section of the copyparty config.
  # see `copyparty --help` for available options
  settings = {
    i = "0.0.0.0";
    # use lists to set multiple values
    p = [ 3210 3211 ];
    # use booleans to set binary flags
    no-reload = true;
    # using 'false' will do nothing and omit the value when generating a config
    ignored-flag = false;
  };

  # create users
  accounts = {
    # specify the account name as the key
    ed = {
      # provide the path to a file containing the password, keeping it out of /nix/store
      # must be readable by the copyparty service user
      passwordFile = "/run/keys/copyparty/ed_password";
    };
    # or do both in one go
    k.passwordFile = "/run/keys/copyparty/k_password";
  };

  # create a group
  groups = {
    # users "ed" and "k" are part of the group g1
    g1 = [ "ed" "k" ];
  };

  # create a volume
  volumes = {
    # create a volume at "/" (the webroot), which will
    "/" = {
      # share the contents of "/srv/copyparty"
      path = "/srv/copyparty";
      # see `copyparty --help-accounts` for available options
      access = {
        # everyone gets read-access, but
        r = "*";
        # users "ed" and "k" get read-write
        rw = [ "ed" "k" ];
      };
      # see `copyparty --help-flags` for available options
      flags = {
        # "fk" enables filekeys (necessary for upget permission) (4 chars long)
        fk = 4;
        # scan for new files every 60sec
        scan = 60;
        # volflag "e2d" enables the uploads database
        e2d = true;
        # "d2t" disables multimedia parsers (in case the uploads are malicious)
        d2t = true;
        # skips hashing file contents if path matches *.iso
        nohash = "\.iso$";
      };
    };
  };
  # you may increase the open file limit for the process
  openFilesLimit = 8192;
};
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;the passwordFile at /run/keys/copyparty/ could for example be generated by &lt;a href="https://github.com/ryantm/agenix"&gt;agenix&lt;/a&gt;, or you could just dump it in the nix store instead if that's acceptable&lt;/p&gt; 
&lt;h1&gt;browser support&lt;/h1&gt; 
&lt;p&gt;TLDR: yes&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/241032/118192791-fb31fe00-b446-11eb-9647-898ea8efc1f7.png" alt="copyparty-ie4-fs8" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;ie&lt;/code&gt; = internet-explorer, &lt;code&gt;ff&lt;/code&gt; = firefox, &lt;code&gt;c&lt;/code&gt; = chrome, &lt;code&gt;iOS&lt;/code&gt; = iPhone/iPad, &lt;code&gt;Andr&lt;/code&gt; = Android&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;feature&lt;/th&gt; 
   &lt;th&gt;ie6&lt;/th&gt; 
   &lt;th&gt;ie9&lt;/th&gt; 
   &lt;th&gt;ie10&lt;/th&gt; 
   &lt;th&gt;ie11&lt;/th&gt; 
   &lt;th&gt;ff 52&lt;/th&gt; 
   &lt;th&gt;c 49&lt;/th&gt; 
   &lt;th&gt;iOS&lt;/th&gt; 
   &lt;th&gt;Andr&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;browse files&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;thumbnail view&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;basic uploader&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;up2k&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;*1&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;*1&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;make directory&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;send message&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;set sort order&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;zip selection&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;file search&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;file rename&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;file cut/paste&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;unpost uploads&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;navpane&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;image viewer&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;video player&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;markdown editor&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;*2&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;*2&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;markdown viewer&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;*2&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;*2&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;*2&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;play mp3/m4a&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;play ogg/opus&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;*3&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;yep&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;= feature =&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;ie6&lt;/td&gt; 
   &lt;td&gt;ie9&lt;/td&gt; 
   &lt;td&gt;ie10&lt;/td&gt; 
   &lt;td&gt;ie11&lt;/td&gt; 
   &lt;td&gt;ff 52&lt;/td&gt; 
   &lt;td&gt;c 49&lt;/td&gt; 
   &lt;td&gt;iOS&lt;/td&gt; 
   &lt;td&gt;Andr&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;internet explorer 6 through 8 behave the same&lt;/li&gt; 
 &lt;li&gt;firefox 52 and chrome 49 are the final winxp versions&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;*1&lt;/code&gt; yes, but extremely slow (ie10: &lt;code&gt;1 MiB/s&lt;/code&gt;, ie11: &lt;code&gt;270 KiB/s&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;*2&lt;/code&gt; only able to do plaintext documents (no markdown rendering)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;*3&lt;/code&gt; iOS 11 and newer, opus only, and requires FFmpeg on the server&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;quick summary of more eccentric web-browsers trying to view a directory index:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;browser&lt;/th&gt; 
   &lt;th&gt;will it blend&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;links&lt;/strong&gt; (2.21/macports)&lt;/td&gt; 
   &lt;td&gt;can browse, login, upload/mkdir/msg&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;lynx&lt;/strong&gt; (2.8.9/macports)&lt;/td&gt; 
   &lt;td&gt;can browse, login, upload/mkdir/msg&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;w3m&lt;/strong&gt; (0.5.3/macports)&lt;/td&gt; 
   &lt;td&gt;can browse, login, upload at 100kB/s, mkdir/msg&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;netsurf&lt;/strong&gt; (3.10/arch)&lt;/td&gt; 
   &lt;td&gt;is basically ie6 with much better css (javascript has almost no effect)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;opera&lt;/strong&gt; (11.60/winxp)&lt;/td&gt; 
   &lt;td&gt;OK: thumbnails, image-viewer, zip-selection, rename/cut/paste. NG: up2k, navpane, markdown, audio&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ie4&lt;/strong&gt; and &lt;strong&gt;netscape&lt;/strong&gt; 4.0&lt;/td&gt; 
   &lt;td&gt;can browse, upload with &lt;code&gt;?b=u&lt;/code&gt;, auth with &lt;code&gt;&amp;amp;pw=wark&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ncsa mosaic&lt;/strong&gt; 2.7&lt;/td&gt; 
   &lt;td&gt;does not get a pass, &lt;a href="https://user-images.githubusercontent.com/241032/174189227-ae816026-cf6f-4be5-a26e-1b3b072c1b2f.png"&gt;pic1&lt;/a&gt; - &lt;a href="https://user-images.githubusercontent.com/241032/174189225-5651c059-5152-46e9-ac26-7e98e497901b.png"&gt;pic2&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;SerenityOS&lt;/strong&gt; (7e98457)&lt;/td&gt; 
   &lt;td&gt;hits a page fault, works with &lt;code&gt;?b=u&lt;/code&gt;, file upload not-impl&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;sony psp&lt;/strong&gt; 5.50&lt;/td&gt; 
   &lt;td&gt;can browse, upload/mkdir/msg (thx dwarf) &lt;a href="https://github.com/user-attachments/assets/9d21f020-1110-4652-abeb-6fc09c533d4f"&gt;screenshot&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;nintendo 3ds&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;can browse, upload, view thumbnails (thx bnjmn)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Nintendo Wii (Opera 9.0 "Internet Channel")&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;can browse, can't upload or download (no local storage), can view images - works best with &lt;code&gt;?b=u&lt;/code&gt;, default view broken&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p align="center"&gt;&lt;img src="https://github.com/user-attachments/assets/88deab3d-6cad-4017-8841-2f041472b853" /&gt;&lt;/p&gt; 
&lt;h1&gt;server hall of fame&lt;/h1&gt; 
&lt;p&gt;unexpected things that run copyparty:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;an old &lt;a href="https://a.ocv.me/pub/g/nerd-stuff/cpp/servers/aallwinner.jpg"&gt;allwinner&lt;/a&gt; android tv-box (ziptie-strapped to an HDD) running a firmware which flips the CPU into Big-Endian mode early during boot 
  &lt;ul&gt; 
   &lt;li&gt;copyparty is &lt;a href="https://a.ocv.me/pub/g/nerd-stuff/cpp/servers/be-ready.png"&gt;certified BE ready&lt;/a&gt; -- thanks, &lt;a href="http://ol-tele.com/"&gt;√òl Telecom&lt;/a&gt;!&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;an &lt;a href="https://a.ocv.me/pub/g/nerd-stuff/cpp/servers/sgi-o2.jpg?cache"&gt;SGI O2 (photo)&lt;/a&gt; with a grand total of 64 MiB RAM running SGI IRIX; &lt;a href="https://a.ocv.me/pub/g/nerd-stuff/cpp/servers/sgi-o2.png?cache"&gt;screenshot&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;thanks again to the wonderful people at &lt;a href="http://ol-tele.com/"&gt;√òl Telecom&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;a &lt;a href="https://a.ocv.me/pub/g/nerd-stuff/cpp/servers/clockyparty.jpg"&gt;wristwatch&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;client examples&lt;/h1&gt; 
&lt;p&gt;interact with copyparty using non-browser clients&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;javascript: dump some state into a file (two separate examples)&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;await fetch('//127.0.0.1:3923/', {method:"PUT", body: JSON.stringify(foo)});&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;var xhr = new XMLHttpRequest(); xhr.open('POST', '//127.0.0.1:3923/msgs?raw'); xhr.send('foo');&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;curl/wget: upload some files (post=file, chunk=stdin)&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;post(){ curl -F f=@"$1" http://127.0.0.1:3923/?pw=wark;}&lt;/code&gt;&lt;br /&gt; &lt;code&gt;post movie.mkv&lt;/code&gt; (gives HTML in return)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;post(){ curl -F f=@"$1" 'http://127.0.0.1:3923/?want=url&amp;amp;pw=wark';}&lt;/code&gt;&lt;br /&gt; &lt;code&gt;post movie.mkv&lt;/code&gt; (gives hotlink in return)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;post(){ curl -H pw:wark -H rand:8 -T "$1" http://127.0.0.1:3923/;}&lt;/code&gt;&lt;br /&gt; &lt;code&gt;post movie.mkv&lt;/code&gt; (randomized filename)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;post(){ wget --header='pw: wark' --post-file="$1" -O- http://127.0.0.1:3923/?raw;}&lt;/code&gt;&lt;br /&gt; &lt;code&gt;post movie.mkv&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;chunk(){ curl -H pw:wark -T- http://127.0.0.1:3923/;}&lt;/code&gt;&lt;br /&gt; &lt;code&gt;chunk &amp;lt;movie.mkv&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;curl: append to existing file with &lt;code&gt;?apnd&lt;/code&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;log(){ curl -H pw:wark -T- http://127.0.0.1:3923/logfile.txt?apnd;}&lt;/code&gt;&lt;br /&gt; &lt;code&gt;echo hey | log&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;bash: when curl and wget is not available or too boring&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;(printf 'PUT /junk?pw=wark HTTP/1.1\r\n\r\n'; cat movie.mkv) | nc 127.0.0.1 3923&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;(printf 'PUT / HTTP/1.1\r\n\r\n'; cat movie.mkv) &amp;gt;/dev/tcp/127.0.0.1/3923&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;python: &lt;a href="https://github.com/9001/copyparty/raw/hovudstraum/bin/u2c.py"&gt;u2c.py&lt;/a&gt; is a command-line up2k client &lt;a href="https://ocv.me/stuff/u2cli.webm"&gt;(webm)&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;file uploads, file-search, &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#folder-sync"&gt;folder sync&lt;/a&gt;, autoresume of aborted/broken uploads&lt;/li&gt; 
   &lt;li&gt;can be downloaded from copyparty: controlpanel -&amp;gt; connect -&amp;gt; &lt;a href="http://127.0.0.1:3923/.cpr/a/u2c.py"&gt;u2c.py&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/bin/README.md#u2cpy"&gt;./bin/README.md#u2cpy&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;FUSE: mount a copyparty server as a local filesystem&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;cross-platform python client available in &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/bin/"&gt;./bin/&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;able to mount nginx and iis directory listings too, not just copyparty&lt;/li&gt; 
   &lt;li&gt;can be downloaded from copyparty: controlpanel -&amp;gt; connect -&amp;gt; &lt;a href="http://127.0.0.1:3923/.cpr/a/partyfuse.py"&gt;partyfuse.py&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://rclone.org/"&gt;rclone&lt;/a&gt; as client can give ~5x performance, see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/rclone.md"&gt;./docs/rclone.md&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;sharex (screenshot utility): see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/contrib/#sharexsxcu"&gt;./contrib/sharex.sxcu&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;and for screenshots on macos, see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/contrib/#ishareiscu"&gt;./contrib/ishare.iscu&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;and for screenshots on linux, see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/contrib/flameshot.sh"&gt;./contrib/flameshot.sh&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://f-droid.org/en/packages/com.nyx.custom_uploader/"&gt;Custom Uploader&lt;/a&gt; (an Android app) as an alternative to copyparty's own &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#android-app"&gt;PartyUP!&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;works if you set UploadURL to &lt;code&gt;https://your.com/foo/?want=url&amp;amp;pw=hunter2&lt;/code&gt; and FormDataName &lt;code&gt;f&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;contextlet (web browser integration); see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/contrib/#send-to-cppcontextletjson"&gt;contrib contextlet&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://iglooirc.com/"&gt;igloo irc&lt;/a&gt;: Method: &lt;code&gt;post&lt;/code&gt; Host: &lt;code&gt;https://you.com/up/?want=url&amp;amp;pw=hunter2&lt;/code&gt; Multipart: &lt;code&gt;yes&lt;/code&gt; File parameter: &lt;code&gt;f&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;copyparty returns a truncated sha512sum of your PUT/POST as base64; you can generate the same checksum locally to verify uploads:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;b512(){ printf "$((sha512sum||shasum -a512)|sed -E 's/ .*//;s/(..)/\\x\1/g')"|base64|tr '+/' '-_'|head -c44;}
b512 &amp;lt;movie.mkv
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;you can provide passwords using header &lt;code&gt;PW: hunter2&lt;/code&gt;, cookie &lt;code&gt;cppwd=hunter2&lt;/code&gt;, url-param &lt;code&gt;?pw=hunter2&lt;/code&gt;, or with basic-authentication (either as the username or password)&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;for basic-authentication, all of the following are accepted: &lt;code&gt;password&lt;/code&gt; / &lt;code&gt;whatever:password&lt;/code&gt; / &lt;code&gt;password:whatever&lt;/code&gt; (the username is ignored)&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;unless you've enabled &lt;code&gt;--usernames&lt;/code&gt;, then it's &lt;code&gt;PW: usr:pwd&lt;/code&gt;, cookie &lt;code&gt;cppwd=usr:pwd&lt;/code&gt;, url-param &lt;code&gt;?pw=usr:pwd&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;NOTE: curl will not send the original filename if you use &lt;code&gt;-T&lt;/code&gt; combined with url-params! Also, make sure to always leave a trailing slash in URLs unless you want to override the filename&lt;/p&gt; 
&lt;h2&gt;folder sync&lt;/h2&gt; 
&lt;p&gt;sync folders to/from copyparty&lt;/p&gt; 
&lt;p&gt;NOTE: full bidirectional sync, like what &lt;a href="https://docs.nextcloud.com/server/latest/user_manual/sv/files/desktop_mobile_sync.html"&gt;nextcloud&lt;/a&gt; and &lt;a href="https://syncthing.net/"&gt;syncthing&lt;/a&gt; does, will never be supported! Only single-direction sync (server-to-client, or client-to-server) is possible with copyparty&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;if you want bidirectional sync, then copyparty and syncthing &lt;em&gt;should&lt;/em&gt; be entirely safe to combine; they should be able to collaborate on the same folders without causing any trouble for eachother. Many people do this, and there have been no issues so far. But, if you &lt;em&gt;do&lt;/em&gt; encounter any problems, please &lt;a href="https://github.com/9001/copyparty/issues/new/choose"&gt;file a copyparty bug&lt;/a&gt; and I'll try to help -- just keep in mind I've never used syncthing before :-)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;the commandline uploader &lt;a href="https://github.com/9001/copyparty/tree/hovudstraum/bin#u2cpy"&gt;u2c.py&lt;/a&gt; with &lt;code&gt;--dr&lt;/code&gt; is the best way to sync a folder to copyparty; verifies checksums and does files in parallel, and deletes unexpected files on the server after upload has finished which makes file-renames really cheap (it'll rename serverside and skip uploading)&lt;/p&gt; 
&lt;p&gt;if you want to sync with &lt;code&gt;u2c.py&lt;/code&gt; then:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;the &lt;code&gt;e2dsa&lt;/code&gt; option (either globally or volflag) must be enabled on the server for the volumes you're syncing into&lt;/li&gt; 
 &lt;li&gt;...but DON'T enable global-options &lt;code&gt;no-hash&lt;/code&gt; or &lt;code&gt;no-idx&lt;/code&gt; (or volflags &lt;code&gt;nohash&lt;/code&gt; / &lt;code&gt;noidx&lt;/code&gt;), or at least make sure they are configured so they do not affect anything you are syncing into&lt;/li&gt; 
 &lt;li&gt;...and u2c needs the delete-permission, so either &lt;code&gt;rwd&lt;/code&gt; at minimum, or just &lt;code&gt;A&lt;/code&gt; which is the same as &lt;code&gt;rwmd.a&lt;/code&gt; 
  &lt;ul&gt; 
   &lt;li&gt;quick reminder that &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;A&lt;/code&gt; are different permissions, and &lt;code&gt;.&lt;/code&gt; is very useful for sync&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;alternatively there is &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/rclone.md"&gt;rclone&lt;/a&gt; which allows for bidirectional sync and is &lt;em&gt;way&lt;/em&gt; more flexible (stream files straight from sftp/s3/gcs to copyparty, ...), although there is no integrity check and it won't work with files over 100 MiB if copyparty is behind cloudflare&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;starting from rclone v1.63, rclone is faster than u2c.py on low-latency connections 
  &lt;ul&gt; 
   &lt;li&gt;but this is only true for the initial upload; u2c will be faster for periodic syncing&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;mount as drive&lt;/h2&gt; 
&lt;p&gt;a remote copyparty server as a local filesystem; go to the control-panel and click &lt;code&gt;connect&lt;/code&gt; to see a list of commands to do that&lt;/p&gt; 
&lt;p&gt;alternatively, some alternatives roughly sorted by speed (unreproducible benchmark), best first:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/rclone.md"&gt;rclone-webdav&lt;/a&gt; (25s), read/WRITE (rclone v1.63 or later)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/rclone.md"&gt;rclone-http&lt;/a&gt; (26s), read-only&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/bin/#partyfusepy"&gt;partyfuse.py&lt;/a&gt; (26s), read-only&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/rclone.md"&gt;rclone-ftp&lt;/a&gt; (47s), read/WRITE&lt;/li&gt; 
 &lt;li&gt;davfs2 (103s), read/WRITE&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#webdav-server"&gt;win10-webdav&lt;/a&gt; (138s), read/WRITE&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#smb-server"&gt;win10-smb2&lt;/a&gt; (387s), read/WRITE&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;most clients will fail to mount the root of a copyparty server unless there is a root volume (so you get the admin-panel instead of a browser when accessing it) -- in that case, mount a specific volume instead&lt;/p&gt; 
&lt;p&gt;if you have volumes that are accessible without a password, then some webdav clients (such as davfs2) require the global-option &lt;code&gt;--dav-auth&lt;/code&gt; to access any password-protected areas&lt;/p&gt; 
&lt;h1&gt;android app&lt;/h1&gt; 
&lt;p&gt;upload to copyparty with one tap&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://f-droid.org/packages/me.ocv.partyup/"&gt;&lt;img src="https://ocv.me/fdroid.png" alt="Get it on F-Droid" height="50" /&gt; '' &lt;img src="https://img.shields.io/f-droid/v/me.ocv.partyup.svg?sanitize=true" alt="f-droid version info" /&gt;&lt;/a&gt; '' &lt;a href="https://github.com/9001/party-up"&gt;&lt;img src="https://img.shields.io/github/release/9001/party-up.svg?logo=github" alt="github version info" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;the app is &lt;strong&gt;NOT&lt;/strong&gt; the full copyparty server! just a basic upload client, nothing fancy yet&lt;/p&gt; 
&lt;p&gt;if you want to run the copyparty server on your android device, see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#install-on-android"&gt;install on android&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;iOS shortcuts&lt;/h1&gt; 
&lt;p&gt;there is no iPhone app, but the following shortcuts are almost as good:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.icloud.com/shortcuts/41e98dd985cb4d3bb433222bc1e9e770"&gt;upload to copyparty&lt;/a&gt; (&lt;a href="https://github.com/9001/copyparty/raw/hovudstraum/contrib/ios/upload-to-copyparty.shortcut"&gt;offline&lt;/a&gt;) (&lt;a href="https://user-images.githubusercontent.com/241032/226118053-78623554-b0ed-482e-98e4-6d57ada58ea4.png"&gt;png&lt;/a&gt;) based on the &lt;a href="https://www.icloud.com/shortcuts/ab415d5b4de3467b9ce6f151b439a5d7"&gt;original&lt;/a&gt; by &lt;a href="https://github.com/Daedren"&gt;Daedren&lt;/a&gt; (thx!) 
  &lt;ul&gt; 
   &lt;li&gt;can strip exif, upload files, pics, vids, links, clipboard&lt;/li&gt; 
   &lt;li&gt;can download links and rehost the target file on copyparty (see first comment inside the shortcut)&lt;/li&gt; 
   &lt;li&gt;pics become lowres if you share from gallery to shortcut, so better to launch the shortcut and pick stuff from there&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;if you want to run the copyparty server on your iPhone or iPad, see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#install-on-iOS"&gt;install on iOS&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;performance&lt;/h1&gt; 
&lt;p&gt;defaults are usually fine - expect &lt;code&gt;8 GiB/s&lt;/code&gt; download, &lt;code&gt;1 GiB/s&lt;/code&gt; upload&lt;/p&gt; 
&lt;p&gt;below are some tweaks roughly ordered by usefulness:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;disabling HTTP/2 and HTTP/3 can make uploads 5x faster, depending on server/client software&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;-q&lt;/code&gt; disables logging and can help a bunch, even when combined with &lt;code&gt;-lo&lt;/code&gt; to redirect logs to file&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;--hist&lt;/code&gt; pointing to a fast location (ssd) will make directory listings and searches faster when &lt;code&gt;-e2d&lt;/code&gt; or &lt;code&gt;-e2t&lt;/code&gt; is set&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;and also makes thumbnails load faster, regardless of e2d/e2t&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;--dedup&lt;/code&gt; enables deduplication and thus avoids writing to the HDD if someone uploads a dupe&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;--safe-dedup 1&lt;/code&gt; makes deduplication much faster during upload by skipping verification of file contents; safe if there is no other software editing/moving the files in the volumes&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;--no-dirsz&lt;/code&gt; shows the size of folder inodes instead of the total size of the contents, giving about 30% faster folder listings&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;--no-hash .&lt;/code&gt; when indexing a network-disk if you don't care about the actual filehashes and only want the names/tags searchable&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;if your volumes are on a network-disk such as NFS / SMB / s3, specifying larger values for &lt;code&gt;--iobuf&lt;/code&gt; and/or &lt;code&gt;--s-rd-sz&lt;/code&gt; and/or &lt;code&gt;--s-wr-sz&lt;/code&gt; may help; try setting all of them to &lt;code&gt;524288&lt;/code&gt; or &lt;code&gt;1048576&lt;/code&gt; or &lt;code&gt;4194304&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;--no-htp --hash-mt=0 --mtag-mt=1 --th-mt=1&lt;/code&gt; minimizes the number of threads; can help in some eccentric environments (like the vscode debugger)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;when running on AlpineLinux or other musl-based distro, try mimalloc for higher performance (and twice as much RAM usage); &lt;code&gt;apk add mimalloc2&lt;/code&gt; and run copyparty with env-var &lt;code&gt;LD_PRELOAD=/usr/lib/libmimalloc-secure.so.2&lt;/code&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;note that mimalloc requires special care when combined with prisonparty and/or bubbleparty/bubblewrap; you must give it access to &lt;code&gt;/proc&lt;/code&gt; and &lt;code&gt;/sys&lt;/code&gt; otherwise you'll encounter issues with FFmpeg (audio transcoding, thumbnails)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;-j0&lt;/code&gt; enables multiprocessing (actual multithreading), can reduce latency to &lt;code&gt;20+80/numCores&lt;/code&gt; percent and generally improve performance in cpu-intensive workloads, for example:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;lots of connections (many users or heavy clients)&lt;/li&gt; 
   &lt;li&gt;simultaneous downloads and uploads saturating a 20gbps connection&lt;/li&gt; 
   &lt;li&gt;if &lt;code&gt;-e2d&lt;/code&gt; is enabled, &lt;code&gt;-j2&lt;/code&gt; gives 4x performance for directory listings; &lt;code&gt;-j4&lt;/code&gt; gives 16x&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;...however it also increases the server/filesystem/HDD load during uploads, and adds an overhead to internal communication, so it is usually a better idea to don't&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;using &lt;a href="https://www.pypy.org/"&gt;pypy&lt;/a&gt; instead of &lt;a href="https://www.python.org/"&gt;cpython&lt;/a&gt; &lt;em&gt;can&lt;/em&gt; be 70% faster for some workloads, but slower for many others&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;and pypy can sometimes crash on startup with &lt;code&gt;-j0&lt;/code&gt; (TODO make issue)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;if you are running the copyparty server &lt;strong&gt;on Windows or Macos:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;--casechk=n&lt;/code&gt; makes it much faster, but also awakens &lt;a href="https://github.com/9001/copyparty/issues/781"&gt;the usual surprises&lt;/a&gt; you expect from a case-insensitive filesystem 
    &lt;ul&gt; 
     &lt;li&gt;this is the same as &lt;code&gt;casechk: n&lt;/code&gt; in a config-file&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;client-side&lt;/h2&gt; 
&lt;p&gt;when uploading files,&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;when uploading from very fast storage (NVMe SSD) with chrome/firefox, enable &lt;code&gt;[wasm]&lt;/code&gt; in the &lt;code&gt;[‚öôÔ∏è] settings&lt;/code&gt; tab to more effectively use all CPU-cores for hashing&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;don't do this on Safari (runs faster without)&lt;/li&gt; 
   &lt;li&gt;don't do this on older browsers; likely to provoke browser-bugs (browser eats all RAM and crashes)&lt;/li&gt; 
   &lt;li&gt;can be made default-enabled serverside with &lt;code&gt;--nosubtle 137&lt;/code&gt; (chrome v137+) or &lt;code&gt;--nosubtle 2&lt;/code&gt; (chrome+firefox)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;chrome is recommended (unfortunately), at least compared to firefox:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;up to 90% faster when hashing, especially on SSDs&lt;/li&gt; 
   &lt;li&gt;up to 40% faster when uploading over extremely fast internets&lt;/li&gt; 
   &lt;li&gt;but &lt;a href="https://github.com/9001/copyparty/raw/hovudstraum/bin/u2c.py"&gt;u2c.py&lt;/a&gt; can be 40% faster than chrome again&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;if you're cpu-bottlenecked, or the browser is maxing a cpu core:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;up to 30% faster uploads if you hide the upload status list by switching away from the &lt;code&gt;[üöÄ]&lt;/code&gt; up2k ui-tab (or closing it) 
    &lt;ul&gt; 
     &lt;li&gt;optionally you can switch to the lightweight potato ui by clicking the &lt;code&gt;[ü•î]&lt;/code&gt;&lt;/li&gt; 
     &lt;li&gt;switching to another browser-tab also works, the favicon will update every 10 seconds in that case&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;unlikely to be a problem, but can happen when uploading many small files, or your internet is too fast, or PC too slow&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;security&lt;/h1&gt; 
&lt;p&gt;there is a &lt;a href="https://discord.gg/25J8CdTT6G"&gt;discord server&lt;/a&gt; with announcements ; an &lt;code&gt;@everyone&lt;/code&gt; for all important updates (at the lack of better ideas)&lt;/p&gt; 
&lt;p&gt;some notes on hardening&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;set &lt;code&gt;--rproxy 0&lt;/code&gt; &lt;em&gt;if and only if&lt;/em&gt; your copyparty is directly facing the internet (not through a reverse-proxy) 
  &lt;ul&gt; 
   &lt;li&gt;cors doesn't work right otherwise&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;if you allow anonymous uploads or otherwise don't trust the contents of a volume, you can prevent XSS with volflag &lt;code&gt;nohtml&lt;/code&gt; 
  &lt;ul&gt; 
   &lt;li&gt;this returns html documents as plaintext, and also disables markdown rendering&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;when running behind a reverse-proxy, listen on a unix-socket for tighter access control (and more performance); see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#reverse-proxy"&gt;reverse-proxy&lt;/a&gt; or &lt;a href="https://copyparty.eu/cli/#bind-help-page"&gt;&lt;code&gt;--help-bind&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;safety profiles:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;option &lt;code&gt;-s&lt;/code&gt; is a shortcut to set the following options:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;--no-thumb&lt;/code&gt; disables thumbnails and audio transcoding to stop copyparty from running &lt;code&gt;FFmpeg&lt;/code&gt;/&lt;code&gt;Pillow&lt;/code&gt;/&lt;code&gt;VIPS&lt;/code&gt; on uploaded files, which is a &lt;a href="https://www.cvedetails.com/vulnerability-list.php?vendor_id=3611"&gt;good idea&lt;/a&gt; if anonymous upload is enabled&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;--no-mtag-ff&lt;/code&gt; uses &lt;code&gt;mutagen&lt;/code&gt; to grab music tags instead of &lt;code&gt;FFmpeg&lt;/code&gt;, which is safer and faster but less accurate&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;--dotpart&lt;/code&gt; hides uploads from directory listings while they're still incoming&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;--no-robots&lt;/code&gt; and &lt;code&gt;--force-js&lt;/code&gt; makes life harder for crawlers, see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#hiding-from-google"&gt;hiding from google&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;option &lt;code&gt;-ss&lt;/code&gt; is a shortcut for the above plus:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;--unpost 0&lt;/code&gt;, &lt;code&gt;--no-del&lt;/code&gt;, &lt;code&gt;--no-mv&lt;/code&gt; disables all move/delete support&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;--hardlink&lt;/code&gt; creates hardlinks instead of symlinks when deduplicating uploads, which is less maintenance 
    &lt;ul&gt; 
     &lt;li&gt;however note if you edit one file it will also affect the other copies&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;--vague-403&lt;/code&gt; returns a "404 not found" instead of "401 unauthorized" which is a common enterprise meme&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;-nih&lt;/code&gt; removes the server hostname from directory listings&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;option &lt;code&gt;-sss&lt;/code&gt; is a shortcut for the above plus:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;--no-dav&lt;/code&gt; disables webdav support&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;--no-logues&lt;/code&gt; and &lt;code&gt;--no-readme&lt;/code&gt; disables support for readme's and prologues / epilogues in directory listings, which otherwise lets people upload arbitrary (but sandboxed) &lt;code&gt;&amp;lt;script&amp;gt;&lt;/code&gt; tags&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;-lo cpp-%Y-%m%d-%H%M%S.txt.xz&lt;/code&gt; enables logging to disk&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;-ls **,*,ln,p,r&lt;/code&gt; does a scan on startup for any dangerous symlinks&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;other misc notes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;you can disable directory listings by giving permission &lt;code&gt;g&lt;/code&gt; instead of &lt;code&gt;r&lt;/code&gt;, only accepting direct URLs to files 
  &lt;ul&gt; 
   &lt;li&gt;you may want &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#filekeys"&gt;filekeys&lt;/a&gt; to prevent filename bruteforcing&lt;/li&gt; 
   &lt;li&gt;permission &lt;code&gt;h&lt;/code&gt; instead of &lt;code&gt;r&lt;/code&gt; makes copyparty behave like a traditional webserver with directory listing/index disabled, returning index.html instead 
    &lt;ul&gt; 
     &lt;li&gt;compatibility with filekeys: index.html itself can be retrieved without the correct filekey, but all other files are protected&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;gotchas&lt;/h2&gt; 
&lt;p&gt;behavior that might be unexpected&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;users without read-access to a folder can still see the &lt;code&gt;.prologue.html&lt;/code&gt; / &lt;code&gt;.epilogue.html&lt;/code&gt; / &lt;code&gt;PREADME.md&lt;/code&gt; / &lt;code&gt;README.md&lt;/code&gt; contents, for the purpose of showing a description on how to use the uploader for example&lt;/li&gt; 
 &lt;li&gt;users can submit &lt;code&gt;&amp;lt;script&amp;gt;&lt;/code&gt;s which autorun (in a sandbox) for other visitors in a few ways; 
  &lt;ul&gt; 
   &lt;li&gt;uploading a &lt;code&gt;README.md&lt;/code&gt; -- avoid with &lt;code&gt;--no-readme&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;renaming &lt;code&gt;some.html&lt;/code&gt; to &lt;code&gt;.epilogue.html&lt;/code&gt; -- avoid with either &lt;code&gt;--no-logues&lt;/code&gt; or &lt;code&gt;--no-dot-ren&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;the directory-listing embed is sandboxed (so any malicious scripts can't do any damage) but the markdown editor is not 100% safe, see below&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;markdown documents can contain html and &lt;code&gt;&amp;lt;script&amp;gt;&lt;/code&gt;s; attempts are made to prevent scripts from executing (unless &lt;code&gt;-emp&lt;/code&gt; is specified) but this is not 100% bulletproof, so setting the &lt;code&gt;nohtml&lt;/code&gt; volflag is still the safest choice 
  &lt;ul&gt; 
   &lt;li&gt;or eliminate the problem entirely by only giving write-access to trustworthy people :^)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;cors&lt;/h2&gt; 
&lt;p&gt;cross-site request config&lt;/p&gt; 
&lt;p&gt;by default, except for &lt;code&gt;GET&lt;/code&gt; and &lt;code&gt;HEAD&lt;/code&gt; operations, all requests must either:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;not contain an &lt;code&gt;Origin&lt;/code&gt; header at all&lt;/li&gt; 
 &lt;li&gt;or have an &lt;code&gt;Origin&lt;/code&gt; matching the server domain&lt;/li&gt; 
 &lt;li&gt;or the header &lt;code&gt;PW&lt;/code&gt; with your password as value&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;cors can be configured with &lt;code&gt;--acao&lt;/code&gt; and &lt;code&gt;--acam&lt;/code&gt;, or the protections entirely disabled with &lt;code&gt;--allow-csrf&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;filekeys&lt;/h2&gt; 
&lt;p&gt;prevent filename bruteforcing&lt;/p&gt; 
&lt;p&gt;volflag &lt;code&gt;fk&lt;/code&gt; generates filekeys (per-file accesskeys) for all files; users which have full read-access (permission &lt;code&gt;r&lt;/code&gt;) will then see URLs with the correct filekey &lt;code&gt;?k=...&lt;/code&gt; appended to the end, and &lt;code&gt;g&lt;/code&gt; users must provide that URL including the correct key to avoid a 404&lt;/p&gt; 
&lt;p&gt;by default, filekeys are generated based on salt (&lt;code&gt;--fk-salt&lt;/code&gt;) + filesystem-path + file-size + inode (if not windows); add volflag &lt;code&gt;fka&lt;/code&gt; to generate slightly weaker filekeys which will not be invalidated if the file is edited (only salt + path)&lt;/p&gt; 
&lt;p&gt;permissions &lt;code&gt;wG&lt;/code&gt; (write + upget) lets users upload files and receive their own filekeys, still without being able to see other uploads&lt;/p&gt; 
&lt;h3&gt;dirkeys&lt;/h3&gt; 
&lt;p&gt;share specific folders in a volume without giving away full read-access to the rest -- the visitor only needs the &lt;code&gt;g&lt;/code&gt; (get) permission to view the link&lt;/p&gt; 
&lt;p&gt;volflag &lt;code&gt;dk&lt;/code&gt; generates dirkeys (per-directory accesskeys) for all folders, granting read-access to that folder; by default only that folder itself, no subfolders&lt;/p&gt; 
&lt;p&gt;volflag &lt;code&gt;dky&lt;/code&gt; disables the actual key-check, meaning anyone can see the contents of a folder where they have &lt;code&gt;g&lt;/code&gt; access, but not its subdirectories&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;dk&lt;/code&gt; + &lt;code&gt;dky&lt;/code&gt; gives the same behavior as if all users with &lt;code&gt;g&lt;/code&gt; access have full read-access, but subfolders are hidden files (as if their names start with a dot), so &lt;code&gt;dky&lt;/code&gt; is an alternative to renaming all the folders for that purpose, maybe just for some users&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;volflag &lt;code&gt;dks&lt;/code&gt; lets people enter subfolders as well, and also enables download-as-zip/tar&lt;/p&gt; 
&lt;p&gt;if you enable dirkeys, it is probably a good idea to enable filekeys too, otherwise it will be impossible to hotlink files from a folder which was accessed using a dirkey&lt;/p&gt; 
&lt;p&gt;dirkeys are generated based on another salt (&lt;code&gt;--dk-salt&lt;/code&gt;) + filesystem-path and have a few limitations:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;the key does not change if the contents of the folder is modified 
  &lt;ul&gt; 
   &lt;li&gt;if you need a new dirkey, either change the salt or rename the folder&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;linking to a textfile (so it opens in the textfile viewer) is not possible if recipient doesn't have read-access&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;password hashing&lt;/h2&gt; 
&lt;p&gt;you can hash passwords before putting them into config files / providing them as arguments; see &lt;a href="https://copyparty.eu/cli/#pwhash-help-page"&gt;&lt;code&gt;--help-pwhash&lt;/code&gt;&lt;/a&gt; for all the details&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;--ah-alg argon2&lt;/code&gt; enables it, and if you have any plaintext passwords then it'll print the hashed versions on startup so you can replace them&lt;/p&gt; 
&lt;p&gt;optionally also specify &lt;code&gt;--ah-cli&lt;/code&gt; to enter an interactive mode where it will hash passwords without ever writing the plaintext ones to disk&lt;/p&gt; 
&lt;p&gt;the default configs take about 0.4 sec and 256 MiB RAM to process a new password on a decent laptop&lt;/p&gt; 
&lt;p&gt;when generating hashes using &lt;code&gt;--ah-cli&lt;/code&gt; for docker or systemd services, make sure it is using the same &lt;code&gt;--ah-salt&lt;/code&gt; by:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;inspecting the generated salt using &lt;code&gt;--show-ah-salt&lt;/code&gt; in copyparty service configuration&lt;/li&gt; 
 &lt;li&gt;setting the same &lt;code&gt;--ah-salt&lt;/code&gt; in both environments&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;‚ö†Ô∏è if you have enabled &lt;code&gt;--usernames&lt;/code&gt; then provide the password as &lt;code&gt;username:password&lt;/code&gt; when hashing it, for example &lt;code&gt;ed:hunter2&lt;/code&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;https&lt;/h2&gt; 
&lt;p&gt;both HTTP and HTTPS are accepted by default, but letting a &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#reverse-proxy"&gt;reverse proxy&lt;/a&gt; handle the https/tls/ssl would be better (probably more secure by default)&lt;/p&gt; 
&lt;p&gt;copyparty doesn't speak HTTP/2 or QUIC, so using a reverse proxy would solve that as well -- but note that HTTP/1 is usually faster than both HTTP/2 and HTTP/3&lt;/p&gt; 
&lt;p&gt;if &lt;a href="https://github.com/cloudflare/cfssl/releases/latest"&gt;cfssl&lt;/a&gt; is installed, copyparty will automatically create a CA and server-cert on startup&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;the certs are written to &lt;code&gt;--crt-dir&lt;/code&gt; for distribution, see &lt;code&gt;--help&lt;/code&gt; for the other &lt;code&gt;--crt&lt;/code&gt; options&lt;/li&gt; 
 &lt;li&gt;this will be a self-signed certificate so you must install your &lt;code&gt;ca.pem&lt;/code&gt; into all your browsers/devices&lt;/li&gt; 
 &lt;li&gt;if you want to avoid the hassle of distributing certs manually, please consider using a reverse proxy&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;to install cfssl on windows:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/cloudflare/cfssl/releases/latest"&gt;download&lt;/a&gt; &lt;code&gt;cfssl_windows_amd64.exe&lt;/code&gt;, &lt;code&gt;cfssljson_windows_amd64.exe&lt;/code&gt;, &lt;code&gt;cfssl-certinfo_windows_amd64.exe&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;rename them to &lt;code&gt;cfssl.exe&lt;/code&gt;, &lt;code&gt;cfssljson.exe&lt;/code&gt;, &lt;code&gt;cfssl-certinfo.exe&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;put them in PATH, for example inside &lt;code&gt;c:\windows\system32&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;recovering from crashes&lt;/h1&gt; 
&lt;h2&gt;client crashes&lt;/h2&gt; 
&lt;h3&gt;firefox wsod&lt;/h3&gt; 
&lt;p&gt;firefox 87 can crash during uploads -- the entire browser goes, including all other browser tabs, everything turns white&lt;/p&gt; 
&lt;p&gt;however you can hit &lt;code&gt;F12&lt;/code&gt; in the up2k tab and use the devtools to see how far you got in the uploads:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;get a complete list of all uploads, organized by status (ok / no-good / busy / queued):&lt;br /&gt; &lt;code&gt;var tabs = { ok:[], ng:[], bz:[], q:[] }; for (var a of up2k.ui.tab) tabs[a.in].push(a); tabs&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;list of filenames which failed:&lt;br /&gt; &lt;code&gt;‚Äãvar ng = []; for (var a of up2k.ui.tab) if (a.in != 'ok') ng.push(a.hn.split('&amp;lt;a href=\"').slice(-1)[0].split('\"&amp;gt;')[0]); ng&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;send the list of filenames to copyparty for safekeeping:&lt;br /&gt; &lt;code&gt;await fetch('/inc', {method:'PUT', body:JSON.stringify(ng,null,1)})&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;HTTP API&lt;/h1&gt; 
&lt;p&gt;see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/devnotes.md#http-api"&gt;devnotes&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;dependencies&lt;/h1&gt; 
&lt;p&gt;mandatory deps:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;jinja2&lt;/code&gt; (is built into the SFX)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;optional dependencies&lt;/h2&gt; 
&lt;p&gt;enable bonus features by installing these python-packages from pypi or so:&lt;/p&gt; 
&lt;p&gt;enable &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#password-hashing"&gt;hashed passwords&lt;/a&gt; in config: &lt;code&gt;argon2-cffi&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;enable &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#ftp-server"&gt;ftp-server&lt;/a&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;for just plaintext FTP, &lt;code&gt;pyftpdlib&lt;/code&gt; (is built into the SFX)&lt;/li&gt; 
 &lt;li&gt;with TLS encryption, &lt;code&gt;pyftpdlib pyopenssl&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;enable &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#sftp-server"&gt;sftp-server&lt;/a&gt;: &lt;code&gt;paramiko&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;enable &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#metadata-from-audio-files"&gt;music tags&lt;/a&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;either &lt;code&gt;mutagen&lt;/code&gt; (fast, pure-python, skips a few tags, makes copyparty GPL? idk)&lt;/li&gt; 
 &lt;li&gt;or &lt;code&gt;ffprobe&lt;/code&gt; (20x slower, more accurate, possibly dangerous depending on your distro and users)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;enable &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#thumbnails"&gt;thumbnails&lt;/a&gt; of...&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;images:&lt;/strong&gt; &lt;code&gt;Pillow&lt;/code&gt; and/or &lt;code&gt;pyvips&lt;/code&gt; and/or &lt;code&gt;ffmpeg&lt;/code&gt; (requires py2.7 or py3.5+)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;videos/audio:&lt;/strong&gt; &lt;code&gt;ffmpeg&lt;/code&gt; and &lt;code&gt;ffprobe&lt;/code&gt; somewhere in &lt;code&gt;$PATH&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;HEIF pictures:&lt;/strong&gt; &lt;code&gt;pyvips&lt;/code&gt; or &lt;code&gt;ffmpeg&lt;/code&gt; or &lt;code&gt;pillow-heif&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;AVIF pictures:&lt;/strong&gt; &lt;code&gt;pyvips&lt;/code&gt; or &lt;code&gt;ffmpeg&lt;/code&gt; or &lt;code&gt;pillow-avif-plugin&lt;/code&gt; or pillow v11.3+&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;JPEG XL pictures:&lt;/strong&gt; &lt;code&gt;pyvips&lt;/code&gt; or &lt;code&gt;ffmpeg&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;RAW images:&lt;/strong&gt; &lt;code&gt;rawpy&lt;/code&gt;, plus one of &lt;code&gt;pyvips&lt;/code&gt; or &lt;code&gt;Pillow&lt;/code&gt; (for some formats)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;enable sending &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#zeromq"&gt;zeromq messages&lt;/a&gt; from event-hooks: &lt;code&gt;pyzmq&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;enable &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#smb-server"&gt;smb&lt;/a&gt; support (&lt;strong&gt;not&lt;/strong&gt; recommended): &lt;code&gt;impacket==0.13.0&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pyvips&lt;/code&gt; gives higher quality thumbnails than &lt;code&gt;Pillow&lt;/code&gt; and is 320% faster, using 270% more ram&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;to install &lt;code&gt;pyvips&lt;/code&gt; on Linux: &lt;code&gt;sudo apt install libvips42 &amp;amp;&amp;amp; python3 -m pip install --user -U pyvips&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;to install &lt;code&gt;pyvips&lt;/code&gt; on windows: &lt;code&gt;pip install --user -U "pyvips[binary]"&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;to install FFmpeg on Windows, grab &lt;a href="https://www.gyan.dev/ffmpeg/builds/ffmpeg-git-full.7z"&gt;a recent build&lt;/a&gt; -- you need &lt;code&gt;ffmpeg.exe&lt;/code&gt; and &lt;code&gt;ffprobe.exe&lt;/code&gt; from inside the &lt;code&gt;bin&lt;/code&gt; folder; copy them into &lt;code&gt;C:\Windows\System32&lt;/code&gt; or any other folder that's in your &lt;code&gt;%PATH%&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;dependency chickenbits&lt;/h3&gt; 
&lt;p&gt;prevent loading an optional dependency , for example if:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;you have an incompatible version installed and it causes problems&lt;/li&gt; 
 &lt;li&gt;you just don't want copyparty to use it, maybe to save ram&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;set any of the following environment variables to disable its associated optional feature,&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;env-var&lt;/th&gt; 
   &lt;th&gt;what it does&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_NO_ARGON2&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;disable argon2-cffi password hashing&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_NO_CFSSL&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;never attempt to generate self-signed certificates using &lt;a href="https://github.com/cloudflare/cfssl"&gt;cfssl&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_NO_FFMPEG&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;audio transcoding&lt;/strong&gt; goes byebye, &lt;strong&gt;thumbnailing&lt;/strong&gt; must be handled by Pillow/libvips&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_NO_FFPROBE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;audio transcoding&lt;/strong&gt; goes byebye, &lt;strong&gt;thumbnailing&lt;/strong&gt; must be handled by Pillow/libvips, &lt;strong&gt;metadata-scanning&lt;/strong&gt; must be handled by mutagen&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_NO_MAGIC&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;do not use &lt;a href="https://pypi.org/project/python-magic/"&gt;magic&lt;/a&gt; for filetype detection&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_NO_MUTAGEN&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;do not use &lt;a href="https://pypi.org/project/mutagen/"&gt;mutagen&lt;/a&gt; for reading metadata from media files; will fallback to ffprobe&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_NO_PARAMIKO&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;disable sftp server (&lt;a href="https://www.paramiko.org/"&gt;paramiko&lt;/a&gt;-based)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_NO_PARTFTPY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;disable tftp server (&lt;a href="https://github.com/9001/partftpy"&gt;partftpy&lt;/a&gt;-based)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_NO_PIL&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;disable all &lt;a href="https://pypi.org/project/pillow/"&gt;Pillow&lt;/a&gt;-based thumbnail support; will fallback to libvips or ffmpeg&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_NO_PILF&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;disable Pillow &lt;code&gt;ImageFont&lt;/code&gt; text rendering, used for folder thumbnails&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_NO_PIL_AVIF&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;disable Pillow avif support (internal and/or &lt;a href="https://pypi.org/project/pillow-avif-plugin/"&gt;plugin&lt;/a&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_NO_PIL_HEIF&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;disable 3rd-party Pillow plugin for &lt;a href="https://pypi.org/project/pillow-heif/"&gt;HEIF support&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_NO_PIL_WEBP&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;disable use of native webp support in Pillow&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_NO_PSUTIL&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;do not use &lt;a href="https://pypi.org/project/psutil/"&gt;psutil&lt;/a&gt; for reaping stuck hooks and plugins on Windows&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_NO_PYFTPD&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;disable ftp(s) server (&lt;a href="https://pypi.org/project/pyftpdlib/"&gt;pyftpdlib&lt;/a&gt;-based)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_NO_RAW&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;disable all &lt;a href="https://pypi.org/project/rawpy/"&gt;rawpy&lt;/a&gt;-based thumbnail support for RAW images&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_NO_VIPS&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;disable all &lt;a href="https://pypi.org/project/pyvips/"&gt;libvips&lt;/a&gt;-based thumbnail support; will fallback to Pillow or ffmpeg&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;example: &lt;code&gt;PRTY_NO_PIL=1 python3 copyparty-sfx.py&lt;/code&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;PRTY_NO_PIL&lt;/code&gt; saves ram&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;PRTY_NO_VIPS&lt;/code&gt; saves ram and startup time&lt;/li&gt; 
 &lt;li&gt;python2.7 on windows: &lt;code&gt;PRTY_NO_FFMPEG&lt;/code&gt; + &lt;code&gt;PRTY_NO_FFPROBE&lt;/code&gt; saves startup time&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;dependency unvendoring&lt;/h3&gt; 
&lt;p&gt;force use of system modules instead of the vendored versions:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;env-var&lt;/th&gt; 
   &lt;th&gt;what it does&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_SYS_ALL&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;all of the below&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_SYS_DNSLIB&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;replace &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/copyparty/stolen/dnslib"&gt;stolen/dnslib&lt;/a&gt; with &lt;a href="https://pypi.org/project/dnslib/"&gt;upstream&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_SYS_IFADDR&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;replace &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/copyparty/stolen/ifaddr"&gt;stolen/ifaddr&lt;/a&gt; with &lt;a href="https://pypi.org/project/ifaddr/"&gt;upstream&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PRTY_SYS_QRCG&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;replace &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/copyparty/stolen/qrcodegen.py"&gt;stolen/qrcodegen.py&lt;/a&gt; with &lt;a href="https://github.com/nayuki/QR-Code-generator/raw/master/python/qrcodegen.py"&gt;upstream&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;to debug, run copyparty with &lt;code&gt;PRTY_MODSPEC=1&lt;/code&gt; to see where it's getting each module from&lt;/p&gt; 
&lt;h2&gt;optional gpl stuff&lt;/h2&gt; 
&lt;p&gt;some bundled tools have copyleft dependencies, see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/bin/#mtag"&gt;./bin/#mtag&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;these are standalone programs and will never be imported / evaluated by copyparty, and must be enabled through &lt;code&gt;-mtp&lt;/code&gt; configs&lt;/p&gt; 
&lt;h1&gt;sfx&lt;/h1&gt; 
&lt;p&gt;the self-contained "binary" (recommended!) &lt;a href="https://github.com/9001/copyparty/releases/latest/download/copyparty-sfx.py"&gt;copyparty-sfx.py&lt;/a&gt; will unpack itself and run copyparty, assuming you have python installed of course&lt;/p&gt; 
&lt;p&gt;if you only need english, &lt;a href="https://github.com/9001/copyparty/releases/latest/download/copyparty-en.py"&gt;copyparty-en.py&lt;/a&gt; is the same thing but smaller&lt;/p&gt; 
&lt;p&gt;you can reduce the sfx size by repacking it; see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/devnotes.md#sfx-repack"&gt;./docs/devnotes.md#sfx-repack&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;copyparty.exe&lt;/h2&gt; 
&lt;p&gt;download &lt;a href="https://github.com/9001/copyparty/releases/latest/download/copyparty.exe"&gt;copyparty.exe&lt;/a&gt; (win8+) or &lt;a href="https://github.com/9001/copyparty/releases/latest/download/copyparty32.exe"&gt;copyparty32.exe&lt;/a&gt; (win7+)&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/241032/221445946-1e328e56-8c5b-44a9-8b9f-dee84d942535.png" alt="copyparty-exe-fs8" /&gt;&lt;/p&gt; 
&lt;p&gt;can be convenient on machines where installing python is problematic, however is &lt;strong&gt;not recommended&lt;/strong&gt; -- if possible, please use &lt;strong&gt;&lt;a href="https://github.com/9001/copyparty/releases/latest/download/copyparty-sfx.py"&gt;copyparty-sfx.py&lt;/a&gt;&lt;/strong&gt; instead&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/9001/copyparty/releases/latest/download/copyparty.exe"&gt;copyparty.exe&lt;/a&gt; runs on win8 or newer, was compiled on win10, does thumbnails + media tags, and is &lt;em&gt;currently&lt;/em&gt; safe to use, but any future python/expat/pillow CVEs can only be remedied by downloading a newer version of the exe&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;on win8 it needs &lt;a href="https://www.microsoft.com/en-us/download/details.aspx?id=48145"&gt;vc redist 2015&lt;/a&gt;, on win10 it just works&lt;/li&gt; 
   &lt;li&gt;some antivirus may freak out (false-positive), possibly &lt;a href="https://www.virustotal.com/gui/file/52391a1e9842cf70ad243ef83844d46d29c0044d101ee0138fcdd3c8de2237d6/detection"&gt;Avast, AVG, and McAfee&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;dangerous: &lt;a href="https://github.com/9001/copyparty/releases/latest/download/copyparty32.exe"&gt;copyparty32.exe&lt;/a&gt; is compatible with &lt;a href="https://user-images.githubusercontent.com/241032/221445944-ae85d1f4-d351-4837-b130-82cab57d6cca.png"&gt;windows7&lt;/a&gt;, which means it uses an ancient copy of python (3.7.9) which cannot be upgraded and should never be exposed to the internet (LAN is fine)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;dangerous and deprecated: &lt;a href="https://github.com/9001/copyparty/releases/download/v1.8.7/copyparty-winpe64.exe"&gt;copyparty-winpe64.exe&lt;/a&gt; lets you &lt;a href="https://user-images.githubusercontent.com/241032/205454984-e6b550df-3c49-486d-9267-1614078dd0dd.png"&gt;run copyparty in WinPE&lt;/a&gt; and is otherwise completely useless&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;meanwhile &lt;a href="https://github.com/9001/copyparty/releases/latest/download/copyparty-sfx.py"&gt;copyparty-sfx.py&lt;/a&gt; instead relies on your system python which gives better performance and will stay safe as long as you keep your python install up-to-date&lt;/p&gt; 
&lt;p&gt;then again, if you are already into downloading shady binaries from the internet, you may also want my &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/scripts/pyinstaller#ffmpeg"&gt;minimal builds&lt;/a&gt; of &lt;a href="https://ocv.me/stuff/bin/ffmpeg.exe"&gt;ffmpeg&lt;/a&gt; and &lt;a href="https://ocv.me/stuff/bin/ffprobe.exe"&gt;ffprobe&lt;/a&gt; which enables copyparty to extract multimedia-info, do audio-transcoding, and thumbnails/spectrograms/waveforms, however it's much better to instead grab a &lt;a href="https://www.gyan.dev/ffmpeg/builds/ffmpeg-git-full.7z"&gt;recent official build&lt;/a&gt; every once ina while if you can afford the size&lt;/p&gt; 
&lt;h2&gt;zipapp&lt;/h2&gt; 
&lt;p&gt;another emergency alternative, &lt;a href="https://github.com/9001/copyparty/releases/latest/download/copyparty.pyz"&gt;copyparty.pyz&lt;/a&gt; has less features, is slow, requires python 3.7 or newer, worse compression, and more importantly is unable to benefit from more recent versions of jinja2 and such (which makes it less secure)... lots of drawbacks with this one really -- but, unlike the sfx, it is a completely normal zipfile which does not unpack any temporary files to disk, so it &lt;em&gt;may&lt;/em&gt; just work if the regular sfx fails to start because the computer is messed up in certain funky ways, so it's worth a shot if all else fails&lt;/p&gt; 
&lt;p&gt;run it by doubleclicking it, or try typing &lt;code&gt;python copyparty.pyz&lt;/code&gt; in your terminal/console/commandline/telex if that fails&lt;/p&gt; 
&lt;p&gt;it is a python &lt;a href="https://docs.python.org/3/library/zipapp.html"&gt;zipapp&lt;/a&gt; meaning it doesn't have to unpack its own python code anywhere to run, so if the filesystem is busted it has a better chance of getting somewhere&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;there is also &lt;a href="https://github.com/9001/copyparty/releases/latest/download/copyparty-en.pyz"&gt;copyparty-en.pyz&lt;/a&gt;, english-only and without smb support (enterprise-friendly)&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h1&gt;install on android&lt;/h1&gt; 
&lt;p&gt;install &lt;a href="https://termux.com/"&gt;Termux&lt;/a&gt; + its companion app &lt;code&gt;Termux:API&lt;/code&gt; (see &lt;a href="https://ocv.me/termux/"&gt;ocv.me/termux&lt;/a&gt;) and then copy-paste this into Termux (long-tap) all at once:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;yes | pkg upgrade &amp;amp;&amp;amp; termux-setup-storage &amp;amp;&amp;amp; yes | pkg install python termux-api &amp;amp;&amp;amp; python -m ensurepip &amp;amp;&amp;amp; python -m pip install --user -U copyparty &amp;amp;&amp;amp; { grep -qE 'PATH=.*\.local/bin' ~/.bashrc 2&amp;gt;/dev/null || { echo 'PATH="$HOME/.local/bin:$PATH"' &amp;gt;&amp;gt; ~/.bashrc &amp;amp;&amp;amp; . ~/.bashrc; }; }
echo $?
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;after the initial setup, you can launch copyparty at any time by running &lt;code&gt;copyparty&lt;/code&gt; anywhere in Termux -- and if you run it with &lt;code&gt;--qr&lt;/code&gt; you'll get a &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#qr-code"&gt;neat qr-code&lt;/a&gt; pointing to your external ip&lt;/p&gt; 
&lt;p&gt;if you want thumbnails (photos+videos) and you're okay with spending another 132 MiB of storage, &lt;code&gt;pkg install ffmpeg &amp;amp;&amp;amp; python3 -m pip install --user -U pillow&lt;/code&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;or if you want to use &lt;code&gt;vips&lt;/code&gt; for photo-thumbs instead, &lt;code&gt;pkg install libvips &amp;amp;&amp;amp; python -m pip install --user -U wheel &amp;amp;&amp;amp; python -m pip install --user -U pyvips &amp;amp;&amp;amp; (cd /data/data/com.termux/files/usr/lib/; ln -s libgobject-2.0.so{,.0}; ln -s libvips.so{,.42})&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;if you are suddenly unable to access storage (permission issues), try forcequitting termux, revoke all of its permissions in android settings, and run the command &lt;code&gt;termux-setup-storage&lt;/code&gt;&lt;/p&gt; 
&lt;h1&gt;install on iOS&lt;/h1&gt; 
&lt;p&gt;first install one of the following:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://apps.apple.com/us/app/a-shell-mini/id1543537943"&gt;a-Shell mini&lt;/a&gt; gives you the essential features&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://apps.apple.com/us/app/a-shell/id1473805438"&gt;a-Shell&lt;/a&gt; also enables audio transcoding and better thubmnails&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;and then copypaste the following command into &lt;code&gt;a-Shell&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;curl -L https://github.com/9001/copyparty/raw/refs/heads/hovudstraum/contrib/setup-ashell.sh | sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;if you want the latest copyparty beta, then do this instead:&lt;br /&gt; &lt;code&gt;curl -L https://copyparty.eu/beta/setup-ashell.sh | sh&lt;/code&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;what this does:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;creates a basic &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/#accounts-and-volumes"&gt;config file&lt;/a&gt; named &lt;code&gt;cpc&lt;/code&gt; which you can edit with &lt;code&gt;vim cpc&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;adds the command &lt;code&gt;cpp&lt;/code&gt; to launch copyparty with that config file&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;known issues:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;cannot run in the background; it needs to be on-screen to accept connections / uploads / downloads&lt;/li&gt; 
 &lt;li&gt;the best way to exit copyparty is to swipe away the app&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;reporting bugs&lt;/h1&gt; 
&lt;p&gt;ideas for context to include, and where to submit them&lt;/p&gt; 
&lt;p&gt;please get in touch using any of the following URLs:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/9001/copyparty/"&gt;https://github.com/9001/copyparty/&lt;/a&gt; &lt;strong&gt;(primary)&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://gitlab.com/9001/copyparty/"&gt;https://gitlab.com/9001/copyparty/&lt;/a&gt; &lt;em&gt;(mirror)&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://codeberg.org/9001/copyparty"&gt;https://codeberg.org/9001/copyparty&lt;/a&gt; &lt;em&gt;(mirror)&lt;/em&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;in general, commandline arguments (and config file if any)&lt;/p&gt; 
&lt;p&gt;if something broke during an upload (replacing FILENAME with a part of the filename that broke):&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;journalctl -aS '48 hour ago' -u copyparty | grep -C10 FILENAME | tee bug.log
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;if there's a wall of base64 in the log (thread stacks) then please include that, especially if you run into something freezing up or getting stuck, for example &lt;code&gt;OperationalError('database is locked')&lt;/code&gt; -- alternatively you can visit &lt;code&gt;/?stack&lt;/code&gt; to see the stacks live, so &lt;a href="http://127.0.0.1:3923/?stack"&gt;http://127.0.0.1:3923/?stack&lt;/a&gt; for example&lt;/p&gt; 
&lt;h1&gt;devnotes&lt;/h1&gt; 
&lt;p&gt;for build instructions etc, see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/devnotes.md"&gt;./docs/devnotes.md&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;specifically you may want to &lt;a href="https://github.com/9001/copyparty/raw/hovudstraum/docs/devnotes.md#just-the-sfx"&gt;build the sfx&lt;/a&gt; or &lt;a href="https://github.com/9001/copyparty/raw/hovudstraum/docs/devnotes.md#build-from-scratch"&gt;build from scratch&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;see &lt;a href="https://raw.githubusercontent.com/9001/copyparty/hovudstraum/docs/TODO.md"&gt;./docs/TODO.md&lt;/a&gt; for planned features / fixes / changes&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>