<rss version="2.0">
  <channel>
    <title>GitHub Rust Daily Trending</title>
    <description>Daily Trending of Rust in GitHub</description>
    <pubDate>Sun, 07 Sep 2025 01:36:57 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>solana-foundation/anchor</title>
      <link>https://github.com/solana-foundation/anchor</link>
      <description>&lt;p&gt;⚓ Solana Program Framework&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img height="170x" src="https://pbs.twimg.com/media/FVUVaO9XEAAulvK?format=png&amp;amp;name=small" /&gt; 
 &lt;h1&gt;Anchor&lt;/h1&gt; 
 &lt;p&gt; &lt;strong&gt;Solana Program Framework&lt;/strong&gt; &lt;/p&gt; 
 &lt;p&gt; &lt;a href="https://github.com/coral-xyz/anchor/actions"&gt;&lt;img alt="Build Status" src="https://github.com/coral-xyz/anchor/actions/workflows/tests.yaml/badge.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://anchor-lang.com"&gt;&lt;img alt="Tutorials" src="https://img.shields.io/badge/docs-tutorials-blueviolet" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/NHHGSXAnXk"&gt;&lt;img alt="Discord Chat" src="https://img.shields.io/discord/889577356681945098?color=blueviolet" /&gt;&lt;/a&gt; &lt;a href="https://opensource.org/licenses/Apache-2.0"&gt;&lt;img alt="License" src="https://img.shields.io/github/license/coral-xyz/anchor?color=blueviolet" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;a href="https://www.anchor-lang.com/"&gt;Anchor&lt;/a&gt; is a framework providing several convenient developer tools for writing Solana programs (sometimes called 'smart contracts').&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Rust eDSL for writing Solana programs&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Interface_description_language"&gt;IDL&lt;/a&gt; specification&lt;/li&gt; 
 &lt;li&gt;TypeScript package for generating clients from IDL&lt;/li&gt; 
 &lt;li&gt;CLI and workspace management for developing complete applications&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Anchor is the most popular framework for Solana programs.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] If you're familiar with developing in Ethereum's &lt;a href="https://docs.soliditylang.org/en/"&gt;Solidity&lt;/a&gt;, &lt;a href="https://www.trufflesuite.com/"&gt;Truffle&lt;/a&gt;, &lt;a href="https://github.com/ethereum/web3.js"&gt;web3.js&lt;/a&gt;, then using Anchor will be familiar. Although the DSL syntax and semantics are targeted at Solana, the high level flow of writing RPC request handlers, emitting an IDL, and generating clients from IDL is the same.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;For a quickstart guide and in depth tutorials, see the &lt;a href="https://book.anchor-lang.com"&gt;Anchor book&lt;/a&gt; and the &lt;a href="https://anchor-lang.com"&gt;Anchor documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To jump straight to examples, go &lt;a href="https://github.com/coral-xyz/anchor/tree/master/examples"&gt;here&lt;/a&gt;. For the latest Rust and TypeScript API documentation, see &lt;a href="https://docs.rs/anchor-lang"&gt;docs.rs&lt;/a&gt; and the &lt;a href="https://www.anchor-lang.com/docs/clients/typescript"&gt;typedoc&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Packages&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Package&lt;/th&gt; 
   &lt;th align="left"&gt;Description&lt;/th&gt; 
   &lt;th align="left"&gt;Version&lt;/th&gt; 
   &lt;th align="left"&gt;Docs&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;anchor-lang&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Rust primitives for writing programs on Solana&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://crates.io/crates/anchor-lang"&gt;&lt;img src="https://img.shields.io/crates/v/anchor-lang?color=blue" alt="Crates.io" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://docs.rs/anchor-lang"&gt;&lt;img src="https://docs.rs/anchor-lang/badge.svg?sanitize=true" alt="Docs.rs" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;anchor-spl&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;CPI clients for SPL programs on Solana&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://crates.io/crates/anchor-spl"&gt;&lt;img src="https://img.shields.io/crates/v/anchor-spl?color=blue" alt="crates" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://docs.rs/anchor-spl"&gt;&lt;img src="https://docs.rs/anchor-spl/badge.svg?sanitize=true" alt="Docs.rs" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;anchor-client&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Rust client for Anchor programs&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://crates.io/crates/anchor-client"&gt;&lt;img src="https://img.shields.io/crates/v/anchor-client?color=blue" alt="crates" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://docs.rs/anchor-client"&gt;&lt;img src="https://docs.rs/anchor-client/badge.svg?sanitize=true" alt="Docs.rs" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;@coral-xyz/anchor&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;TypeScript client for Anchor programs&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://www.npmjs.com/package/@coral-xyz/anchor"&gt;&lt;img src="https://img.shields.io/npm/v/@coral-xyz/anchor.svg?color=blue" alt="npm" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://coral-xyz.github.io/anchor/ts/index.html"&gt;&lt;img src="https://img.shields.io/badge/docs-typedoc-blue" alt="Docs" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;@coral-xyz/anchor-cli&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;CLI to support building and managing an Anchor workspace&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://www.npmjs.com/package/@coral-xyz/anchor-cli"&gt;&lt;img src="https://img.shields.io/npm/v/@coral-xyz/anchor-cli.svg?color=blue" alt="npm" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://coral-xyz.github.io/anchor/cli/commands.html"&gt;&lt;img src="https://img.shields.io/badge/docs-typedoc-blue" alt="Docs" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Note&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Anchor is in active development, so all APIs are subject to change.&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;This code is unaudited. Use at your own risk.&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;p&gt;Here's a counter program, where only the designated &lt;code&gt;authority&lt;/code&gt; can increment the count.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-rust"&gt;use anchor_lang::prelude::*;

declare_id!("Fg6PaFpoGXkYsidMpWTK6W2BeZ7FEfcYkg476zPFsLnS");

#[program]
mod counter {
    use super::*;

    pub fn initialize(ctx: Context&amp;lt;Initialize&amp;gt;, start: u64) -&amp;gt; Result&amp;lt;()&amp;gt; {
        let counter = &amp;amp;mut ctx.accounts.counter;
        counter.authority = *ctx.accounts.authority.key;
        counter.count = start;
        Ok(())
    }

    pub fn increment(ctx: Context&amp;lt;Increment&amp;gt;) -&amp;gt; Result&amp;lt;()&amp;gt; {
        let counter = &amp;amp;mut ctx.accounts.counter;
        counter.count += 1;
        Ok(())
    }
}

#[derive(Accounts)]
pub struct Initialize&amp;lt;'info&amp;gt; {
    #[account(init, payer = authority, space = 48)]
    pub counter: Account&amp;lt;'info, Counter&amp;gt;,
    pub authority: Signer&amp;lt;'info&amp;gt;,
    pub system_program: Program&amp;lt;'info, System&amp;gt;,
}

#[derive(Accounts)]
pub struct Increment&amp;lt;'info&amp;gt; {
    #[account(mut, has_one = authority)]
    pub counter: Account&amp;lt;'info, Counter&amp;gt;,
    pub authority: Signer&amp;lt;'info&amp;gt;,
}

#[account]
pub struct Counter {
    pub authority: Pubkey,
    pub count: u64,
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more, see the &lt;a href="https://github.com/coral-xyz/anchor/tree/master/examples"&gt;examples&lt;/a&gt; and &lt;a href="https://github.com/coral-xyz/anchor/tree/master/tests"&gt;tests&lt;/a&gt; directories.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Anchor is licensed under &lt;a href="https://raw.githubusercontent.com/solana-foundation/anchor/master/LICENSE"&gt;Apache 2.0&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Unless you explicitly state otherwise, any contribution intentionally submitted for inclusion in Anchor by you, as defined in the Apache-2.0 license, shall be licensed as above, without any additional terms or conditions.&lt;/p&gt; 
&lt;h2&gt;Contribution&lt;/h2&gt; 
&lt;p&gt;Thank you for your interest in contributing to Anchor! Please see the &lt;a href="https://raw.githubusercontent.com/solana-foundation/anchor/master/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; to learn how.&lt;/p&gt; 
&lt;h3&gt;Thanks ❤️&lt;/h3&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/coral-xyz/anchor/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=coral-xyz/anchor" width="100%" /&gt; &lt;/a&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>slint-ui/slint</title>
      <link>https://github.com/slint-ui/slint</link>
      <description>&lt;p&gt;Slint is an open-source declarative GUI toolkit to build native user interfaces for Rust, C++, JavaScript, or Python apps.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/slint-ui/slint/master/logo/slint-logo-full-light.svg#gh-light-mode-only" alt="Slint" /&gt; &lt;img src="https://raw.githubusercontent.com/slint-ui/slint/master/logo/slint-logo-full-dark.svg#gh-dark-mode-only" alt="Slint" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/slint-ui/slint/actions"&gt;&lt;img src="https://github.com/slint-ui/slint/workflows/CI/badge.svg?sanitize=true" alt="Build Status" /&gt;&lt;/a&gt; &lt;a href="https://api.reuse.software/info/github.com/slint-ui/slint"&gt;&lt;img src="https://api.reuse.software/badge/github.com/slint-ui/slint" alt="REUSE status" /&gt;&lt;/a&gt; &lt;a href="https://github.com/slint-ui/slint/discussions"&gt;&lt;img src="https://img.shields.io/github/discussions/slint-ui/slint" alt="Discussions" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Slint&lt;/strong&gt; is an open-source declarative GUI toolkit for building native user interfaces for embedded systems, desktops, and mobile platforms.&lt;/p&gt; 
&lt;p&gt;Write your UI once in &lt;code&gt;.slint&lt;/code&gt;, a simple markup language. Connect it to business logic written in Rust, C++, JavaScript, or Python.&lt;/p&gt; 
&lt;h2&gt;Why Slint?&lt;/h2&gt; 
&lt;p&gt;The name &lt;em&gt;Slint&lt;/em&gt; is derived from our design goals:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Scalable&lt;/strong&gt;: Slint should support responsive UI design, allow cross-platform usage across operating systems and processor architectures and support multiple programming languages.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Lightweight&lt;/strong&gt;: Slint should require minimal resources, in terms of memory and processing power, and yet deliver a smooth, smartphone-like user experience on any device.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Intuitive&lt;/strong&gt;: Designers and developers should feel productive while enjoying the GUI design and development process. The design creation tools should be intuitive to use for the designers. Similarly for the developers, the APIs should be consistent and easy to use, no matter which programming language they choose.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Native&lt;/strong&gt;: GUI built with Slint should match the end users' expectations of a native application irrespective of the platform - desktop, mobile, web or embedded system. The UI design should be compiled to machine code and provide flexibility that only a native application can offer: Access full operating system APIs, utilize all CPU and GPU cores, connect to any peripheral.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Beyond the design goals, here’s what makes Slint stand out:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Independent UI Design&lt;/strong&gt;: Use a declarative language similar to separate your UI from business logic. Designers can work in parallel with developers.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Tooling&lt;/strong&gt;: Iterate quickly with our Live Preview &amp;amp; editor integrations. Integrate from Figma with the &lt;a href="https://www.figma.com/community/plugin/1474418299182276871/figma-to-slint"&gt;Slint To Figma plugin&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Stable APIs&lt;/strong&gt;: Slint follows a stable 1.x API. We evolve carefully without breaking your code.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See what others have built: &lt;a href="https://madewithslint.com"&gt;#MadeWithSlint&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;h3&gt;Embedded&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;RaspberryPi&lt;/th&gt; 
   &lt;th&gt;STM32&lt;/th&gt; 
   &lt;th&gt;RP2040&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=_BDbNHrjK7g"&gt;Video of Slint on Raspberry Pi&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=NNNOJJsOAis"&gt;Video of Slint on STM32&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=dkBwNocItGs"&gt;Video of Slint on RP2040&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Desktop&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Windows&lt;/th&gt; 
   &lt;th&gt;macOS&lt;/th&gt; 
   &lt;th&gt;Linux&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src="https://slint.dev/resources/gallery_win_screenshot.png" alt="Screenshot of the Gallery on Windows" title="Gallery" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://slint.dev/resources/gallery_mac_screenshot.png" alt="Screenshot of the Gallery on macOS" title="Gallery" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://slint.dev/resources/gallery_linux_screenshot.png" alt="Screenshot of the Gallery on Linux" title="Gallery" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Web using WebAssembly&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Printer Demo&lt;/th&gt; 
   &lt;th&gt;Slide Puzzle&lt;/th&gt; 
   &lt;th&gt;Energy Monitor&lt;/th&gt; 
   &lt;th&gt;Widget Gallery&lt;/th&gt; 
   &lt;th&gt;Weather demo&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://slint.dev/demos/printerdemo/"&gt;&lt;img src="https://slint.dev/resources/printerdemo_screenshot.png" alt="Screenshot of the Printer Demo" title="Printer Demo" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://slint.dev/demos/slide_puzzle/"&gt;&lt;img src="https://slint.dev/resources/puzzle_screenshot.png" alt="Screenshot of the Slide Puzzle" title="Slide Puzzle" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://slint.dev/demos/energy-monitor/"&gt;&lt;img src="https://slint.dev/resources/energy-monitor-screenshot.png" alt="Screenshot of the Energy Monitor Demo" title="Energy Monitor Demo" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://slint.dev/demos/gallery/"&gt;&lt;img src="https://slint.dev/resources/gallery_screenshot.png" alt="Screenshot of the Gallery Demo" title="Gallery Demo" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://slint.dev/demos/weather-demo/"&gt;&lt;img src="https://raw.githubusercontent.com/slint-ui/slint/master/demos/weather-demo/docs/img/desktop-preview.png" alt="Screenshot of the weather Demo" title="Weather Demo" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;More examples and demos in the &lt;a href="https://raw.githubusercontent.com/slint-ui/slint/master/examples#examples"&gt;examples folder&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Get Started&lt;/h2&gt; 
&lt;h3&gt;Hello World&lt;/h3&gt; 
&lt;p&gt;The UI is defined in a Domain Specific Language that is declarative, easy to use, intuitive, and provides a powerful way to describe graphical elements, their placement, their hierarchy, property bindings, and the flow of data through the different states.&lt;/p&gt; 
&lt;p&gt;Here's the obligatory "Hello World":&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-slint"&gt;export component HelloWorld inherits Window {
    width: 400px;
    height: 400px;

    Text {
       y: parent.width / 2;
       x: parent.x + 200px;
       text: "Hello, world";
       color: blue;
    }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Documentation&lt;/h3&gt; 
&lt;p&gt;For more details, check out the &lt;a href="https://slint.dev/docs/slint"&gt;Slint Language Documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The &lt;a href="https://raw.githubusercontent.com/slint-ui/slint/master/examples"&gt;examples&lt;/a&gt; folder contains examples and demos, showing how to use the Slint markup language and how to interact with a Slint user interface from supported programming languages.&lt;/p&gt; 
&lt;p&gt;The &lt;code&gt;docs&lt;/code&gt; folder contains a lot more information, including &lt;a href="https://raw.githubusercontent.com/slint-ui/slint/master/docs/building.md"&gt;build instructions&lt;/a&gt;, and &lt;a href="https://raw.githubusercontent.com/slint-ui/slint/master/docs/development.md"&gt;internal developer docs&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Refer to the README of each language directory in the &lt;code&gt;api&lt;/code&gt; folder:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/slint-ui/slint/master/api/cpp"&gt;C++&lt;/a&gt; (&lt;a href="https://slint.dev/latest/docs/cpp"&gt;Documentation&lt;/a&gt; | &lt;a href="https://github.com/slint-ui/slint-cpp-template"&gt;Getting Started Template&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/slint-ui/slint/master/api/rs/slint"&gt;Rust&lt;/a&gt; &lt;a href="https://crates.io/crates/slint"&gt;&lt;img src="https://img.shields.io/crates/v/slint" alt="Crates.io" /&gt;&lt;/a&gt; (&lt;a href="https://slint.dev/latest/docs/rust/slint/"&gt;Documentation&lt;/a&gt; | &lt;a href="https://youtu.be/WBcv4V-whHk"&gt;Tutorial Video&lt;/a&gt; | &lt;a href="https://github.com/slint-ui/slint-rust-template"&gt;Getting Started Template&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/slint-ui/slint/master/api/node"&gt;JavaScript/NodeJS (Beta)&lt;/a&gt; &lt;a href="https://www.npmjs.com/package/slint-ui"&gt;&lt;img src="https://img.shields.io/npm/v/slint-ui" alt="npm" /&gt;&lt;/a&gt; (&lt;a href="https://slint.dev/latest/docs/node"&gt;Documentation&lt;/a&gt; | &lt;a href="https://github.com/slint-ui/slint-nodejs-template"&gt;Getting Started Template&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/slint-ui/slint/master/api/python/slint"&gt;Python (Beta)&lt;/a&gt; &lt;a href="https://pypi.org/project/slint/"&gt;&lt;img src="https://img.shields.io/pypi/v/slint" alt="pypi" /&gt;&lt;/a&gt; (&lt;a href="http://snapshots.slint.dev/master/docs/python/"&gt;Documentation&lt;/a&gt; | &lt;a href="https://github.com/slint-ui/slint-python-template"&gt;Getting Started Template&lt;/a&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Architecture&lt;/h2&gt; 
&lt;p&gt;An application is composed of the business logic written in Rust, C++, or JavaScript and the &lt;code&gt;.slint&lt;/code&gt; user interface design markup, which is compiled to native code.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://slint.dev/resources/architecture.drawio.svg?sanitize=true" alt="Architecture Overview" /&gt;&lt;/p&gt; 
&lt;h3&gt;Compiler&lt;/h3&gt; 
&lt;p&gt;The &lt;code&gt;.slint&lt;/code&gt; files are compiled ahead of time. The expressions in the &lt;code&gt;.slint&lt;/code&gt; are pure functions that the compiler can optimize. For example, the compiler could choose to "inline" properties and remove those that are constant or unchanged.&lt;/p&gt; 
&lt;p&gt;The compiler uses the typical compiler phases of lexing, parsing, optimization, and finally code generation. It provides different back-ends for code generation in the target language. The C++ code generator produces a C++ header file, the Rust generator produces Rust code, and so on. An interpreter for dynamic languages is also included.&lt;/p&gt; 
&lt;h3&gt;Runtime&lt;/h3&gt; 
&lt;p&gt;The runtime library consists of an engine that supports properties declared in the &lt;code&gt;.slint&lt;/code&gt; language. Components with their elements, items, and properties are laid out in a single memory region, to reduce memory allocations.&lt;/p&gt; 
&lt;p&gt;Rendering backends and styles are configurable at compile time:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;The &lt;code&gt;femtovg&lt;/code&gt; renderer uses OpenGL ES 2.0 for rendering.&lt;/li&gt; 
 &lt;li&gt;The &lt;code&gt;skia&lt;/code&gt; renderer uses &lt;a href="https://skia.org"&gt;Skia&lt;/a&gt; for rendering.&lt;/li&gt; 
 &lt;li&gt;The &lt;code&gt;software&lt;/code&gt; renderer uses the CPU with no additional dependencies.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;NOTE: When Qt is installed on the system, the &lt;code&gt;qt&lt;/code&gt; style becomes available, using Qt's QStyle to achieve native looking widgets.&lt;/p&gt; 
&lt;h3&gt;Tooling&lt;/h3&gt; 
&lt;p&gt;We have a few tools to help with the development of .slint files:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;A &lt;a href="https://raw.githubusercontent.com/slint-ui/slint/master/tools/lsp"&gt;&lt;strong&gt;LSP Server&lt;/strong&gt;&lt;/a&gt; that adds features like auto-complete and live preview of the .slint files to many editors.&lt;/li&gt; 
 &lt;li&gt;It is bundled in a &lt;a href="https://raw.githubusercontent.com/slint-ui/slint/master/editors/vscode"&gt;&lt;strong&gt;Visual Studio Code Extension&lt;/strong&gt;&lt;/a&gt; available from the market place.&lt;/li&gt; 
 &lt;li&gt;A &lt;a href="https://raw.githubusercontent.com/slint-ui/slint/master/tools/viewer"&gt;&lt;strong&gt;slint-viewer&lt;/strong&gt;&lt;/a&gt; tool which displays the .slint files. The &lt;code&gt;--auto-reload&lt;/code&gt; argument makes it easy to preview your UI while you are working on it (when using the LSP preview is not possible).&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://slintpad.com/"&gt;&lt;strong&gt;SlintPad&lt;/strong&gt;&lt;/a&gt;, an online editor to try out .slint syntax without installing anything (&lt;a href="https://raw.githubusercontent.com/slint-ui/slint/master/tools/slintpad"&gt;sources&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;A &lt;a href="https://www.figma.com/community/plugin/1474418299182276871/figma-to-slint"&gt;&lt;strong&gt;Figma to Slint&lt;/strong&gt;&lt;/a&gt; plugin.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Please check our &lt;a href="https://raw.githubusercontent.com/slint-ui/slint/master/editors/README.md"&gt;Editors README&lt;/a&gt; for tips on how to configure your favorite editor to work well with Slint.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;You can use Slint under &lt;em&gt;&lt;strong&gt;any&lt;/strong&gt;&lt;/em&gt; of the following licenses, at your choice:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Build proprietary desktop, mobile, or web applications for free with the &lt;a href="https://raw.githubusercontent.com/slint-ui/slint/master/LICENSES/LicenseRef-Slint-Royalty-free-2.0.md"&gt;Royalty-free License&lt;/a&gt;,&lt;/li&gt; 
 &lt;li&gt;Build open source embedded, desktop, mobile, or web applications for free with the &lt;a href="https://raw.githubusercontent.com/slint-ui/slint/master/LICENSES/GPL-3.0-only.txt"&gt;GNU GPLv3&lt;/a&gt;,&lt;/li&gt; 
 &lt;li&gt;Build proprietary embedded, desktop, mobile, or web applications with the &lt;a href="https://raw.githubusercontent.com/slint-ui/slint/master/LICENSES/LicenseRef-Slint-Software-3.0.md"&gt;Paid license&lt;/a&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;See the &lt;a href="https://slint.dev/pricing.html"&gt;Slint licensing options on the website&lt;/a&gt; and the &lt;a href="https://raw.githubusercontent.com/slint-ui/slint/master/FAQ.md#licensing"&gt;Licensing FAQ&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributions&lt;/h2&gt; 
&lt;p&gt;We welcome your contributions: in the form of code, bug reports or feedback. For contribution guidelines see &lt;a href="https://raw.githubusercontent.com/slint-ui/slint/master/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Frequently Asked Questions&lt;/h2&gt; 
&lt;p&gt;Please see our separate &lt;a href="https://raw.githubusercontent.com/slint-ui/slint/master/FAQ.md"&gt;FAQ&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;About us (SixtyFPS GmbH)&lt;/h2&gt; 
&lt;p&gt;We are passionate about software - API design, cross-platform software development and user interface components. Our aim is to make developing user interfaces fun for everyone: from Python, JavaScript, C++, or Rust developers all the way to UI/UX designers. We believe that software grows organically and keeping it open source is the best way to sustain that growth. Our team members are located remotely in Germany, Finland, and US.&lt;/p&gt; 
&lt;h3&gt;Stay up to date&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Follow &lt;a href="https://twitter.com/slint_ui"&gt;@slint_ui&lt;/a&gt; on X/Twitter.&lt;/li&gt; 
 &lt;li&gt;Follow &lt;a href="https://mastodon.social/@slint@fosstodon.org"&gt;@slint@fosstodon.org&lt;/a&gt; on Mastodon.&lt;/li&gt; 
 &lt;li&gt;Follow &lt;a href="https://www.linkedin.com/company/slint-ui/"&gt;@slint-ui&lt;/a&gt; on LinkedIn.&lt;/li&gt; 
 &lt;li&gt;Follow &lt;a href="https://bsky.app/profile/slint.dev"&gt;@slint.dev&lt;/a&gt; on Bluesky&lt;/li&gt; 
 &lt;li&gt;Subscribe to our &lt;a href="https://www.youtube.com/@Slint-UI"&gt;YouTube channel&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Contact us&lt;/h3&gt; 
&lt;p&gt;Feel free to join &lt;a href="https://github.com/slint-ui/slint/discussions"&gt;Github discussions&lt;/a&gt; for general chat or questions. Use &lt;a href="https://github.com/slint-ui/slint/issues"&gt;Github issues&lt;/a&gt; to report public suggestions or bugs.&lt;/p&gt; 
&lt;p&gt;We chat in &lt;a href="https://chat.slint.dev"&gt;our Mattermost instance&lt;/a&gt; where you are welcome to listen in or ask your questions.&lt;/p&gt; 
&lt;p&gt;You can of course also contact us privately via email to &lt;a href="mailto://info@slint.dev"&gt;info@slint.dev&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>zama-ai/fhevm</title>
      <link>https://github.com/zama-ai/fhevm</link>
      <description>&lt;p&gt;FHEVM, a full-stack framework for integrating Fully Homomorphic Encryption (FHE) with blockchain applications&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="docs/.gitbook/assets/fhevm-header-dark.png" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="docs/.gitbook/assets/fhevm-header-light.png" /&gt; 
  &lt;img width="500" alt="fhevm" /&gt; 
 &lt;/picture&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;p align="center"&gt; &lt;a href="https://raw.githubusercontent.com/zama-ai/fhevm/main/fhevm-whitepaper.pdf"&gt; 📃 Read white paper&lt;/a&gt; |&lt;a href="https://docs.zama.ai/protocol"&gt; 📒 Documentation&lt;/a&gt; | &lt;a href="https://zama.ai/community"&gt; 💛 Community support&lt;/a&gt; | &lt;a href="https://github.com/zama-ai/awesome-zama"&gt; 📚 FHE resources by Zama&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/zama-ai/fhevm/releases"&gt; &lt;img src="https://img.shields.io/github/v/release/zama-ai/fhevm?style=flat-square" /&gt;&lt;/a&gt; &lt;a href="https://github.com/zama-ai/fhevm/raw/main/LICENSE"&gt; 
  &lt;!-- markdown-link-check-disable-next-line --&gt; &lt;img src="https://img.shields.io/badge/License-BSD--3--Clause--Clear-%23ffb243?style=flat-square" /&gt;&lt;/a&gt; &lt;a href="https://github.com/zama-ai/bounty-program"&gt; 
  &lt;!-- markdown-link-check-disable-next-line --&gt; &lt;img src="https://img.shields.io/badge/Contribute-Zama%20Bounty%20Program-%23ffd208?style=flat-square" /&gt;&lt;/a&gt; &lt;a href="https://slsa.dev"&gt;&lt;img alt="SLSA 3" src="https://slsa.dev/images/gh-badge-level3.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;About&lt;/h2&gt; 
&lt;h3&gt;What is FHEVM?&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;FHEVM&lt;/strong&gt; is the core framework of the &lt;em&gt;Zama Confidential Blockchain Protocol&lt;/em&gt;. It enables confidential smart contracts on EVM-compatible blockchains by leveraging Fully Homomorphic Encryption (FHE), allowing encrypted data to be processed directly onchain.&lt;/p&gt; 
&lt;p&gt;FHEVM ensures both confidentiality and composability, with the following guarantees:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;End-to-end encryption of transactions and state:&lt;/strong&gt; Data included in transactions is encrypted and never visible to anyone.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Composability and data availability on-chain:&lt;/strong&gt; States are updated while remaining encrypted at all times.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;No impact on existing dApps and state:&lt;/strong&gt; Encrypted state co-exists alongside public one, and doesn't impact existing dApps. &lt;br /&gt;&lt;br /&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Table of contents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zama-ai/fhevm/main/#about"&gt;About&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zama-ai/fhevm/main/#what-is-fhevm"&gt;What is FHEVM?&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zama-ai/fhevm/main/#project-structure"&gt;Project structure&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zama-ai/fhevm/main/#main-features"&gt;Main features&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zama-ai/fhevm/main/#use-cases"&gt;Use cases&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zama-ai/fhevm/main/#resources"&gt;Resources&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zama-ai/fhevm/main/#working-with-fhevm"&gt;Working with FHEVM&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zama-ai/fhevm/main/#citations"&gt;Citations&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zama-ai/fhevm/main/#contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zama-ai/fhevm/main/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zama-ai/fhevm/main/#faq"&gt;FAQ&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zama-ai/fhevm/main/#support"&gt;Support&lt;/a&gt; &lt;br /&gt;&lt;br /&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Project structure&lt;/h3&gt; 
&lt;p&gt;The directories of this repository are organized in the following way:&lt;/p&gt; 
&lt;h6&gt;FHEVM Contracts&lt;/h6&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;gateway-contracts/&lt;/code&gt;&lt;/strong&gt;: Smart contracts managing the gateway between on-chain and off-chain components.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;host-contracts/&lt;/code&gt;&lt;/strong&gt;: Smart Contracts deployed on the host chain for orchestrating FHE workflows.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h6&gt;FHEVM Compute Engines&lt;/h6&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;coprocessor/&lt;/code&gt;&lt;/strong&gt;: Rust-based coprocessor implementation for FHE operations.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;kms-connector/&lt;/code&gt;&lt;/strong&gt;: Interface for integrating with Key Management Services (KMS) to handle encryption keys securely.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h6&gt;FHEVM Utilities&lt;/h6&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;charts/&lt;/code&gt;&lt;/strong&gt;: Helm charts and deployment configurations for the stack.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;golden-container-images/&lt;/code&gt;&lt;/strong&gt;: Docker golden images for Node.js and Rust environments used as base images by the stack.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;test-suite/&lt;/code&gt;&lt;/strong&gt;: Integration with docker-compose and tests covering end-to-end FHEVM stack behavior.&lt;/p&gt; &lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Main features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Privacy by design:&lt;/strong&gt; Building decentralized apps with full privacy and confidentiality on Ethereum, leveraging FHE.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Solidity integration:&lt;/strong&gt; Write FHEVM contracts like any standard Solidity contract using Solidity. Compatible with existing toolchains — such as Hardhat and Foundry (&lt;em&gt;coming soon&lt;/em&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Programmable privacy:&lt;/strong&gt; Define exactly what data is encrypted and write the access control logic directly in your smart contracts.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;High precision encrypted integers :&lt;/strong&gt; Up to 256 bits of precision for integers.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Full range of operators:&lt;/strong&gt; All typical operators are available: &lt;code&gt;+&lt;/code&gt;, &lt;code&gt;-&lt;/code&gt;, &lt;code&gt;*&lt;/code&gt;, &lt;code&gt;/&lt;/code&gt;, &lt;code&gt;&amp;lt;&lt;/code&gt;, &lt;code&gt;&amp;gt;&lt;/code&gt;, &lt;code&gt;==&lt;/code&gt;, ternary-if, boolean operations…. Consecutive FHE operations are not limited.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Security:&lt;/strong&gt; The underlying FHE crypto-scheme of FHEVM is quantum-resistant. Decryption is managed via a key management system (KMS) using multi-party computation (MPC), ensuring security even if some parties are compromised or misbehaving.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Symbolic execution of FHE computations:&lt;/strong&gt; All FHE operations are executed symbolically on the host chain, significantly reducing execution time. The actual computations on encrypted data are offloaded asynchronously to our coprocessor, allowing for faster, efficient, and scalable processing.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;em&gt;Learn more about FHEVM features in the &lt;a href="https://docs.zama.ai/protocol"&gt;documentation&lt;/a&gt; and in our &lt;a href="https://github.com/zama-ai/fhevm/raw/main/fhevm-whitepaper.pdf"&gt;whitepaper&lt;/a&gt;.&lt;/em&gt; &lt;br /&gt;&lt;br /&gt;&lt;/p&gt; 
&lt;h3&gt;Use cases&lt;/h3&gt; 
&lt;p&gt;FHEVM is built for developers to write confidential smart contracts without the need to learn cryptography. Leveraging FHEVM, you can unlock a myriad of new use cases such as DeFi, gaming, and more. For instance:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Confidential transfers&lt;/strong&gt;: Keep balances and amounts private, without using mixers.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Tokenization&lt;/strong&gt;: Swap tokens and RWAs on-chain without others seeing the amounts.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Blind auctions&lt;/strong&gt;: Bid on items without revealing the amount or the winner.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;On-chain games&lt;/strong&gt;: Keep moves, selections, cards, or items hidden until ready to reveal.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Confidential voting&lt;/strong&gt;: Prevents bribery and blackmailing by keeping votes private.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Encrypted DIDs&lt;/strong&gt;: Store identities on-chain and generate attestations without ZK.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;em&gt;Learn more use cases in the &lt;a href="https://docs.zama.ai/protocol/examples"&gt;list of examples&lt;/a&gt;.&lt;/em&gt; &lt;br /&gt;&lt;br /&gt;&lt;/p&gt; 
&lt;h2&gt;Resources&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.zama.ai/protocol"&gt;Documentation&lt;/a&gt; — Official documentation of FHEVM.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zama-ai/fhevm/main/fhevm-whitepaper.pdf"&gt;Whitepaper&lt;/a&gt; — Technical overview of FHEVM's cryptographic design.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.zama.ai/protocol/examples"&gt;Examples&lt;/a&gt; — Examples of building confidential smart contracts.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/zama-ai/awesome-zama?tab=readme-ov-file#fhevm"&gt;Awesome Zama – FHEVM&lt;/a&gt; — Curated articles, talks, and ecosystem projects.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="right"&gt; &lt;a href="https://raw.githubusercontent.com/zama-ai/fhevm/main/#about"&gt; ↑ Back to top &lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;Working with FHEVM&lt;/h2&gt; 
&lt;h3&gt;Citations&lt;/h3&gt; 
&lt;p&gt;To cite FHEVM or the whitepaper in academic papers, please use the following entries:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;@Misc{FHEVM,
title={{FHEVM: A full-stack framework for integrating Fully Homomorphic Encryption (FHE) with blockchain applications},
author={Zama},
year={2025},
note={\url{https://github.com/zama-ai/fhevm}},
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Contributing&lt;/h3&gt; 
&lt;p&gt;There are two ways to contribute to FHEVM:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/zama-ai/fhevm/issues/new/choose"&gt;Open issues&lt;/a&gt; to report bugs and typos, or to suggest new ideas&lt;/li&gt; 
 &lt;li&gt;Request to become an official contributor by emailing &lt;a href="mailto:hello@zama.ai"&gt;hello@zama.ai&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Becoming an approved contributor involves signing our Contributor License Agreement (CLA). Only approved contributors can send pull requests, so please make sure to get in touch before you do! &lt;br /&gt;&lt;br /&gt;&lt;/p&gt; 
&lt;h3&gt;License&lt;/h3&gt; 
&lt;p&gt;This software is distributed under the &lt;strong&gt;BSD-3-Clause-Clear&lt;/strong&gt; license. Read &lt;a href="https://raw.githubusercontent.com/zama-ai/fhevm/main/LICENSE"&gt;this&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h3&gt;FAQ&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Is Zama’s technology free to use?&lt;/strong&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Zama’s libraries are free to use under the BSD 3-Clause Clear license only for development, research, prototyping, and experimentation purposes. However, for any commercial use of Zama's open source code, companies must purchase Zama’s commercial patent license.&lt;/p&gt; 
 &lt;p&gt;Everything we do is open source, and we are very transparent on what it means for our users, you can read more about how we monetize our open source products at Zama in &lt;a href="https://www.zama.ai/post/open-source"&gt;this blog post&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;What do I need to do if I want to use Zama’s technology for commercial purposes?&lt;/strong&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;To commercially use Zama’s technology you need to be granted Zama’s patent license. Please contact us at &lt;a href="mailto:hello@zama.ai"&gt;hello@zama.ai&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Do you file IP on your technology?&lt;/strong&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Yes, all Zama’s technologies are patented.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Can you customize a solution for my specific use case?&lt;/strong&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;We are open to collaborating and advancing the FHE space with our partners. If you have specific needs, please email us at &lt;a href="mailto:hello@zama.ai"&gt;hello@zama.ai&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Support&lt;/h2&gt; 
&lt;a target="_blank" href="https://community.zama.ai"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="docs/.gitbook/assets/support-banner-dark.png" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="docs/.gitbook/assets/support-banner-light.png" /&gt; 
  &lt;img alt="Support" /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;p&gt;🌟 If you find this project helpful or interesting, please consider giving it a star on GitHub! Your support helps to grow the community and motivates further development.&lt;/p&gt; 
&lt;p align="right"&gt; &lt;a href="https://raw.githubusercontent.com/zama-ai/fhevm/main/#about"&gt; ↑ Back to top &lt;/a&gt; &lt;/p&gt;</description>
    </item>
    
    <item>
      <title>block/goose</title>
      <link>https://github.com/block/goose</link>
      <description>&lt;p&gt;an open source, extensible AI agent that goes beyond code suggestions - install, execute, edit, and test with any LLM&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h1&gt;codename goose&lt;/h1&gt; 
 &lt;p&gt;&lt;em&gt;a local, extensible, open source AI agent that automates engineering tasks&lt;/em&gt;&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;a href="https://opensource.org/licenses/Apache-2.0"&gt; &lt;img src="https://img.shields.io/badge/License-Apache_2.0-blue.svg?sanitize=true" /&gt; &lt;/a&gt; &lt;a href="https://discord.gg/7GaTvbDwga"&gt; &lt;img src="https://img.shields.io/discord/1287729918100246654?logo=discord&amp;amp;logoColor=white&amp;amp;label=Join+Us&amp;amp;color=blueviolet" alt="Discord" /&gt; &lt;/a&gt; &lt;a href="https://github.com/block/goose/actions/workflows/ci.yml"&gt; &lt;img src="https://img.shields.io/github/actions/workflow/status/block/goose/ci.yml?branch=main" alt="CI" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;goose is your on-machine AI agent, capable of automating complex development tasks from start to finish. More than just code suggestions, goose can build entire projects from scratch, write and execute code, debug failures, orchestrate workflows, and interact with external APIs - &lt;em&gt;autonomously&lt;/em&gt;.&lt;/p&gt; 
&lt;p&gt;Whether you're prototyping an idea, refining existing code, or managing intricate engineering pipelines, goose adapts to your workflow and executes tasks with precision.&lt;/p&gt; 
&lt;p&gt;Designed for maximum flexibility, goose works with any LLM and supports multi-model configuration to optimize performance and cost, seamlessly integrates with MCP servers, and is available as both a desktop app as well as CLI - making it the ultimate AI assistant for developers who want to move faster and focus on innovation.&lt;/p&gt; 
&lt;h1&gt;Quick Links&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://block.github.io/goose/docs/quickstart"&gt;Quickstart&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://block.github.io/goose/docs/getting-started/installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://block.github.io/goose/docs/category/tutorials"&gt;Tutorials&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://block.github.io/goose/docs/category/getting-started"&gt;Documentation&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;A Little Goose Humor 🦢&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Why did the developer choose goose as their AI agent?&lt;/p&gt; 
 &lt;p&gt;Because it always helps them "migrate" their code to production! 🚀&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h1&gt;Goose Around with Us&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://discord.gg/block-opensource"&gt;Discord&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/@blockopensource"&gt;YouTube&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.linkedin.com/company/block-opensource"&gt;LinkedIn&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://x.com/blockopensource"&gt;Twitter/X&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://bsky.app/profile/opensource.block.xyz"&gt;Bluesky&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://njump.me/opensource@block.xyz"&gt;Nostr&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>rustdesk/rustdesk</title>
      <link>https://github.com/rustdesk/rustdesk</link>
      <description>&lt;p&gt;An open-source remote desktop application designed for self-hosting, as an alternative to TeamViewer.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/rustdesk/rustdesk/master/res/logo-header.svg?sanitize=true" alt="RustDesk - Your remote desktop" /&gt;&lt;br /&gt; &lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/#raw-steps-to-build"&gt;Build&lt;/a&gt; • &lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/#how-to-build-with-docker"&gt;Docker&lt;/a&gt; • &lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/#file-structure"&gt;Structure&lt;/a&gt; • &lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/#snapshot"&gt;Snapshot&lt;/a&gt;&lt;br /&gt; [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-UA.md"&gt;Українська&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-CS.md"&gt;česky&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-ZH.md"&gt;中文&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-HU.md"&gt;Magyar&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-ES.md"&gt;Español&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-FA.md"&gt;فارسی&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-FR.md"&gt;Français&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-DE.md"&gt;Deutsch&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-PL.md"&gt;Polski&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-ID.md"&gt;Indonesian&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-FI.md"&gt;Suomi&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-ML.md"&gt;മലയാളം&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-JP.md"&gt;日本語&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-NL.md"&gt;Nederlands&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-IT.md"&gt;Italiano&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-RU.md"&gt;Русский&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-PTBR.md"&gt;Português (Brasil)&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-EO.md"&gt;Esperanto&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-KR.md"&gt;한국어&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-AR.md"&gt;العربي&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-VN.md"&gt;Tiếng Việt&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-DA.md"&gt;Dansk&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-GR.md"&gt;Ελληνικά&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-TR.md"&gt;Türkçe&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/README-NO.md"&gt;Norsk&lt;/a&gt;]&lt;br /&gt; &lt;b&gt;We need your help to translate this README, &lt;a href="https://github.com/rustdesk/rustdesk/tree/master/src/lang"&gt;RustDesk UI&lt;/a&gt; and &lt;a href="https://github.com/rustdesk/doc.rustdesk.com"&gt;RustDesk Doc&lt;/a&gt; to your native language&lt;/b&gt; &lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Caution] &lt;strong&gt;Misuse Disclaimer:&lt;/strong&gt; &lt;br /&gt; The developers of RustDesk do not condone or support any unethical or illegal use of this software. Misuse, such as unauthorized access, control or invasion of privacy, is strictly against our guidelines. The authors are not responsible for any misuse of the application.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Chat with us: &lt;a href="https://discord.gg/nDceKgxnkV"&gt;Discord&lt;/a&gt; | &lt;a href="https://twitter.com/rustdesk"&gt;Twitter&lt;/a&gt; | &lt;a href="https://www.reddit.com/r/rustdesk"&gt;Reddit&lt;/a&gt; | &lt;a href="https://www.youtube.com/@rustdesk"&gt;YouTube&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://rustdesk.com/pricing.html"&gt;&lt;img src="https://img.shields.io/badge/RustDesk%20Server%20Pro-Advanced%20Features-blue" alt="RustDesk Server Pro" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Yet another remote desktop solution, written in Rust. Works out of the box with no configuration required. You have full control of your data, with no concerns about security. You can use our rendezvous/relay server, &lt;a href="https://rustdesk.com/server"&gt;set up your own&lt;/a&gt;, or &lt;a href="https://github.com/rustdesk/rustdesk-server-demo"&gt;write your own rendezvous/relay server&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/71636191/171661982-430285f0-2e12-4b1d-9957-4a58e375304d.png" alt="image" /&gt;&lt;/p&gt; 
&lt;p&gt;RustDesk welcomes contribution from everyone. See &lt;a href="https://raw.githubusercontent.com/rustdesk/rustdesk/master/docs/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for help getting started.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/rustdesk/rustdesk/wiki/FAQ"&gt;&lt;strong&gt;FAQ&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/rustdesk/rustdesk/releases"&gt;&lt;strong&gt;BINARY DOWNLOAD&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/rustdesk/rustdesk/releases/tag/nightly"&gt;&lt;strong&gt;NIGHTLY BUILD&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://f-droid.org/en/packages/com.carriez.flutter_hbb"&gt;&lt;img src="https://f-droid.org/badge/get-it-on.png" alt="Get it on F-Droid" height="80" /&gt;&lt;/a&gt; &lt;a href="https://flathub.org/apps/com.rustdesk.RustDesk"&gt;&lt;img src="https://flathub.org/api/badge?svg&amp;amp;locale=en" alt="Get it on Flathub" height="80" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Dependencies&lt;/h2&gt; 
&lt;p&gt;Desktop versions use Flutter or Sciter (deprecated) for GUI, this tutorial is for Sciter only, since it is easier and more friendly to start. Check out our &lt;a href="https://github.com/rustdesk/rustdesk/raw/master/.github/workflows/flutter-build.yml"&gt;CI&lt;/a&gt; for building Flutter version.&lt;/p&gt; 
&lt;p&gt;Please download Sciter dynamic library yourself.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/c-smile/sciter-sdk/master/bin.win/x64/sciter.dll"&gt;Windows&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/c-smile/sciter-sdk/master/bin.lnx/x64/libsciter-gtk.so"&gt;Linux&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/c-smile/sciter-sdk/master/bin.osx/libsciter.dylib"&gt;macOS&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Raw Steps to build&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Prepare your Rust development env and C++ build env&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install &lt;a href="https://github.com/microsoft/vcpkg"&gt;vcpkg&lt;/a&gt;, and set &lt;code&gt;VCPKG_ROOT&lt;/code&gt; env variable correctly&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Windows: vcpkg install libvpx:x64-windows-static libyuv:x64-windows-static opus:x64-windows-static aom:x64-windows-static&lt;/li&gt; 
   &lt;li&gt;Linux/macOS: vcpkg install libvpx libyuv opus aom&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;run &lt;code&gt;cargo run&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;a href="https://rustdesk.com/docs/en/dev/build/"&gt;Build&lt;/a&gt;&lt;/h2&gt; 
&lt;h2&gt;How to Build on Linux&lt;/h2&gt; 
&lt;h3&gt;Ubuntu 18 (Debian 10)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;sudo apt install -y zip g++ gcc git curl wget nasm yasm libgtk-3-dev clang libxcb-randr0-dev libxdo-dev \
        libxfixes-dev libxcb-shape0-dev libxcb-xfixes0-dev libasound2-dev libpulse-dev cmake make \
        libclang-dev ninja-build libgstreamer1.0-dev libgstreamer-plugins-base1.0-dev libpam0g-dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;openSUSE Tumbleweed&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;sudo zypper install gcc-c++ git curl wget nasm yasm gcc gtk3-devel clang libxcb-devel libXfixes-devel cmake alsa-lib-devel gstreamer-devel gstreamer-plugins-base-devel xdotool-devel pam-devel
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Fedora 28 (CentOS 8)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;sudo yum -y install gcc-c++ git curl wget nasm yasm gcc gtk3-devel clang libxcb-devel libxdo-devel libXfixes-devel pulseaudio-libs-devel cmake alsa-lib-devel gstreamer1-devel gstreamer1-plugins-base-devel pam-devel
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Arch (Manjaro)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;sudo pacman -Syu --needed unzip git cmake gcc curl wget yasm nasm zip make pkg-config clang gtk3 xdotool libxcb libxfixes alsa-lib pipewire
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Install vcpkg&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;git clone https://github.com/microsoft/vcpkg
cd vcpkg
git checkout 2023.04.15
cd ..
vcpkg/bootstrap-vcpkg.sh
export VCPKG_ROOT=$HOME/vcpkg
vcpkg/vcpkg install libvpx libyuv opus aom
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Fix libvpx (For Fedora)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;cd vcpkg/buildtrees/libvpx/src
cd *
./configure
sed -i 's/CFLAGS+=-I/CFLAGS+=-fPIC -I/g' Makefile
sed -i 's/CXXFLAGS+=-I/CXXFLAGS+=-fPIC -I/g' Makefile
make
cp libvpx.a $HOME/vcpkg/installed/x64-linux/lib/
cd
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Build&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
source $HOME/.cargo/env
git clone --recurse-submodules https://github.com/rustdesk/rustdesk
cd rustdesk
mkdir -p target/debug
wget https://raw.githubusercontent.com/c-smile/sciter-sdk/master/bin.lnx/x64/libsciter-gtk.so
mv libsciter-gtk.so target/debug
VCPKG_ROOT=$HOME/vcpkg cargo run
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;How to build with Docker&lt;/h2&gt; 
&lt;p&gt;Begin by cloning the repository and building the Docker container:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;git clone https://github.com/rustdesk/rustdesk
cd rustdesk
git submodule update --init --recursive
docker build -t "rustdesk-builder" .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then, each time you need to build the application, run the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;docker run --rm -it -v $PWD:/home/user/rustdesk -v rustdesk-git-cache:/home/user/.cargo/git -v rustdesk-registry-cache:/home/user/.cargo/registry -e PUID="$(id -u)" -e PGID="$(id -g)" rustdesk-builder
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note that the first build may take longer before dependencies are cached, subsequent builds will be faster. Additionally, if you need to specify different arguments to the build command, you may do so at the end of the command in the &lt;code&gt;&amp;lt;OPTIONAL-ARGS&amp;gt;&lt;/code&gt; position. For instance, if you wanted to build an optimized release version, you would run the command above followed by &lt;code&gt;--release&lt;/code&gt;. The resulting executable will be available in the target folder on your system, and can be run with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;target/debug/rustdesk
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or, if you're running a release executable:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;target/release/rustdesk
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Please ensure that you run these commands from the root of the RustDesk repository, or the application may not find the required resources. Also note that other cargo subcommands such as &lt;code&gt;install&lt;/code&gt; or &lt;code&gt;run&lt;/code&gt; are not currently supported via this method as they would install or run the program inside the container instead of the host.&lt;/p&gt; 
&lt;h2&gt;File Structure&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rustdesk/rustdesk/tree/master/libs/hbb_common"&gt;libs/hbb_common&lt;/a&gt;&lt;/strong&gt;: video codec, config, tcp/udp wrapper, protobuf, fs functions for file transfer, and some other utility functions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rustdesk/rustdesk/tree/master/libs/scrap"&gt;libs/scrap&lt;/a&gt;&lt;/strong&gt;: screen capture&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rustdesk/rustdesk/tree/master/libs/enigo"&gt;libs/enigo&lt;/a&gt;&lt;/strong&gt;: platform specific keyboard/mouse control&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rustdesk/rustdesk/tree/master/libs/clipboard"&gt;libs/clipboard&lt;/a&gt;&lt;/strong&gt;: file copy and paste implementation for Windows, Linux, macOS.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rustdesk/rustdesk/tree/master/src/ui"&gt;src/ui&lt;/a&gt;&lt;/strong&gt;: obsolete Sciter UI (deprecated)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rustdesk/rustdesk/tree/master/src/server"&gt;src/server&lt;/a&gt;&lt;/strong&gt;: audio/clipboard/input/video services, and network connections&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rustdesk/rustdesk/tree/master/src/client.rs"&gt;src/client.rs&lt;/a&gt;&lt;/strong&gt;: start a peer connection&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rustdesk/rustdesk/tree/master/src/rendezvous_mediator.rs"&gt;src/rendezvous_mediator.rs&lt;/a&gt;&lt;/strong&gt;: Communicate with &lt;a href="https://github.com/rustdesk/rustdesk-server"&gt;rustdesk-server&lt;/a&gt;, wait for remote direct (TCP hole punching) or relayed connection&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rustdesk/rustdesk/tree/master/src/platform"&gt;src/platform&lt;/a&gt;&lt;/strong&gt;: platform specific code&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rustdesk/rustdesk/tree/master/flutter"&gt;flutter&lt;/a&gt;&lt;/strong&gt;: Flutter code for desktop and mobile&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rustdesk/rustdesk/tree/master/flutter/web/v1/js"&gt;flutter/web/js&lt;/a&gt;&lt;/strong&gt;: JavaScript for Flutter web client&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Screenshots&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://github.com/rustdesk/rustdesk/assets/28412477/db82d4e7-c4bc-4823-8e6f-6af7eadf7651" alt="Connection Manager" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/rustdesk/rustdesk/assets/28412477/9baa91e9-3362-4d06-aa1a-7518edcbd7ea" alt="Connected to a Windows PC" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/rustdesk/rustdesk/assets/28412477/39511ad3-aa9a-4f8c-8947-1cce286a46ad" alt="File Transfer" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/rustdesk/rustdesk/assets/28412477/78e8708f-e87e-4570-8373-1360033ea6c5" alt="TCP Tunneling" /&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Eventual-Inc/Daft</title>
      <link>https://github.com/Eventual-Inc/Daft</link>
      <description>&lt;p&gt;Distributed query engine providing simple and reliable data processing for any modality and scale&lt;/p&gt;&lt;hr&gt;&lt;p&gt;|Banner|&lt;/p&gt; 
&lt;p&gt;|CI| |PyPI| |Latest Tag| |Coverage| |Slack|&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;Website &amp;lt;https://www.daft.ai&amp;gt;&lt;/code&gt;_ • &lt;code&gt;Docs &amp;lt;https://docs.daft.ai&amp;gt;&lt;/code&gt;_ • &lt;code&gt;Installation &amp;lt;https://docs.daft.ai/en/stable/install/&amp;gt;&lt;/code&gt;_ • &lt;code&gt;Daft Quickstart &amp;lt;https://docs.daft.ai/en/stable/quickstart/&amp;gt;&lt;/code&gt;_ • &lt;code&gt;Community and Support &amp;lt;https://github.com/Eventual-Inc/Daft/discussions&amp;gt;&lt;/code&gt;_&lt;/p&gt; 
&lt;h1&gt;Daft: Unified Engine for Data Analytics, Engineering &amp;amp; ML/AI&lt;/h1&gt; 
&lt;p&gt;&lt;code&gt;Daft &amp;lt;https://www.daft.ai&amp;gt;&lt;/code&gt;_ is a distributed query engine for large-scale data processing using Python or SQL, implemented in Rust.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Familiar interactive API:&lt;/strong&gt; Lazy Python Dataframe for rapid and interactive iteration, or SQL for analytical queries&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Focus on the what:&lt;/strong&gt; Powerful Query Optimizer that rewrites queries to be as efficient as possible&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Data Catalog integrations:&lt;/strong&gt; Full integration with data catalogs such as Apache Iceberg&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Rich multimodal type-system:&lt;/strong&gt; Supports multimodal types such as Images, URLs, Tensors and more&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Seamless Interchange&lt;/strong&gt;: Built on the &lt;code&gt;Apache Arrow &amp;lt;https://arrow.apache.org/docs/index.html&amp;gt;&lt;/code&gt;_ In-Memory Format&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Built for the cloud:&lt;/strong&gt; &lt;code&gt;Record-setting &amp;lt;https://www.daft.ai/blog/announcing-daft-02&amp;gt;&lt;/code&gt;_ I/O performance for integrations with S3 cloud storage&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;About Daft&lt;/code&gt;_&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Getting Started&lt;/code&gt;_&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Benchmarks&lt;/code&gt;_&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Contributing&lt;/code&gt;_&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Telemetry&lt;/code&gt;_&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Related Projects&lt;/code&gt;_&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;License&lt;/code&gt;_&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;About Daft&lt;/h2&gt; 
&lt;p&gt;Daft was designed with the following principles in mind:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Any Data&lt;/strong&gt;: Beyond the usual strings/numbers/dates, Daft columns can also hold complex or nested multimodal data such as Images, Embeddings and Python objects efficiently with its Arrow based memory representation. Ingestion and basic transformations of multimodal data is extremely easy and performant in Daft.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Interactive Computing&lt;/strong&gt;: Daft is built for the interactive developer experience through notebooks or REPLs - intelligent caching/query optimizations accelerates your experimentation and data exploration.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Distributed Computing&lt;/strong&gt;: Some workloads can quickly outgrow your local laptop's computational resources - Daft integrates natively with &lt;code&gt;Ray &amp;lt;https://www.ray.io&amp;gt;&lt;/code&gt;_ for running dataframes on large clusters of machines with thousands of CPUs/GPUs.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;Installation ^^^^^^^^^^^^&lt;/p&gt; 
&lt;p&gt;Install Daft with &lt;code&gt;pip install daft&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;For more advanced installations (e.g. installing from source or with extra dependencies such as Ray and AWS utilities), please see our &lt;code&gt;Installation Guide &amp;lt;https://docs.daft.ai/en/stable/install/&amp;gt;&lt;/code&gt;_&lt;/p&gt; 
&lt;p&gt;Quickstart ^^^^^^^^^^&lt;/p&gt; 
&lt;p&gt;Check out our &lt;code&gt;quickstart &amp;lt;https://docs.daft.ai/en/stable/quickstart/&amp;gt;&lt;/code&gt;_!&lt;/p&gt; 
&lt;p&gt;In this example, we load images from an AWS S3 bucket's URLs and resize each image in the dataframe:&lt;/p&gt; 
&lt;p&gt;.. code:: python&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;import daft

# Load a dataframe from filepaths in an S3 bucket
df = daft.from_glob_path("s3://daft-public-data/laion-sample-images/*")

# 1. Download column of image URLs as a column of bytes
# 2. Decode the column of bytes into a column of images
df = df.with_column("image", df["path"].url.download().image.decode())

# Resize each image into 32x32
df = df.with_column("resized", df["image"].image.resize(32, 32))

df.show(3)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;|Quickstart Image|&lt;/p&gt; 
&lt;h2&gt;Benchmarks&lt;/h2&gt; 
&lt;p&gt;|Benchmark Image|&lt;/p&gt; 
&lt;p&gt;To see the full benchmarks, detailed setup, and logs, check out our &lt;code&gt;benchmarking page. &amp;lt;https://docs.daft.ai/en/stable/resources/benchmarks/tpch/&amp;gt;&lt;/code&gt;_&lt;/p&gt; 
&lt;p&gt;More Resources ^^^^^^^^^^^^^^&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;Daft Quickstart &amp;lt;https://docs.daft.ai/en/stable/quickstart/&amp;gt;&lt;/code&gt;_ - learn more about Daft's full range of capabilities including dataloading from URLs, joins, user-defined functions (UDF), groupby, aggregations and more.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;User Guide &amp;lt;https://docs.daft.ai/en/stable/&amp;gt;&lt;/code&gt;_ - take a deep-dive into each topic within Daft&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;API Reference &amp;lt;https://docs.daft.ai/en/stable/api/&amp;gt;&lt;/code&gt;_ - API reference for public classes/functions of Daft&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;SQL Reference &amp;lt;https://docs.daft.ai/en/stable/sql/&amp;gt;&lt;/code&gt;_ - Daft SQL reference&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We &amp;lt;3 developers! To start contributing to Daft, please read &lt;code&gt;CONTRIBUTING.md &amp;lt;https://github.com/Eventual-Inc/Daft/blob/main/CONTRIBUTING.md&amp;gt;&lt;/code&gt;_. This document describes the development lifecycle and toolchain for working on Daft. It also details how to add new functionality to the core engine and expose it through a Python API.&lt;/p&gt; 
&lt;p&gt;Here's a list of &lt;code&gt;good first issues &amp;lt;https://github.com/Eventual-Inc/Daft/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22&amp;gt;&lt;/code&gt;_ to get yourself warmed up with Daft. Comment in the issue to pick it up, and feel free to ask any questions!&lt;/p&gt; 
&lt;h2&gt;Telemetry&lt;/h2&gt; 
&lt;p&gt;To help improve Daft, we collect non-identifiable data via Scarf (&lt;a href="https://scarf.sh"&gt;https://scarf.sh&lt;/a&gt;).&lt;/p&gt; 
&lt;p&gt;To disable this behavior, set the environment variable &lt;code&gt;DO_NOT_TRACK=true&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;The data that we collect is:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Non-identifiable:&lt;/strong&gt; Events are keyed by a session ID which is generated on import of Daft&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Metadata-only:&lt;/strong&gt; We do not collect any of our users’ proprietary code or data&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;For development only:&lt;/strong&gt; We do not buy or sell any user data&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Please see our &lt;code&gt;documentation &amp;lt;https://docs.daft.ai/en/stable/resources/telemetry/&amp;gt;&lt;/code&gt;_ for more details.&lt;/p&gt; 
&lt;p&gt;.. image:: &lt;a href="https://static.scarf.sh/a.png?x-pxid=31f8d5ba-7e09-4d75-8895-5252bbf06cf6"&gt;https://static.scarf.sh/a.png?x-pxid=31f8d5ba-7e09-4d75-8895-5252bbf06cf6&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Related Projects&lt;/h2&gt; 
&lt;p&gt;+---------------------------------------------------+-----------------+---------------+-------------+-----------------+-----------------------------+-------------+ | Engine | Query Optimizer | Multimodal | Distributed | Arrow Backed | Vectorized Execution Engine | Out-of-core | +===================================================+=================+===============+=============+=================+=============================+=============+ | Daft | Yes | Yes | Yes | Yes | Yes | Yes | +---------------------------------------------------+-----------------+---------------+-------------+-----------------+-----------------------------+-------------+ | &lt;code&gt;Pandas &amp;lt;https://github.com/pandas-dev/pandas&amp;gt;&lt;/code&gt;_ | No | Python object | No | optional &amp;gt;= 2.0 | Some(Numpy) | No | +---------------------------------------------------+-----------------+---------------+-------------+-----------------+-----------------------------+-------------+ | &lt;code&gt;Polars &amp;lt;https://github.com/pola-rs/polars&amp;gt;&lt;/code&gt;_ | Yes | Python object | No | Yes | Yes | Yes | +---------------------------------------------------+-----------------+---------------+-------------+-----------------+-----------------------------+-------------+ | &lt;code&gt;Modin &amp;lt;https://github.com/modin-project/modin&amp;gt;&lt;/code&gt;_ | Yes | Python object | Yes | No | Some(Pandas) | Yes | +---------------------------------------------------+-----------------+---------------+-------------+-----------------+-----------------------------+-------------+ | &lt;code&gt;Pyspark &amp;lt;https://github.com/apache/spark&amp;gt;&lt;/code&gt;_ | Yes | No | Yes | Pandas UDF/IO | Pandas UDF | Yes | +---------------------------------------------------+-----------------+---------------+-------------+-----------------+-----------------------------+-------------+ | &lt;code&gt;Dask DF &amp;lt;https://github.com/dask/dask&amp;gt;&lt;/code&gt;_ | No | Python object | Yes | No | Some(Pandas) | Yes | +---------------------------------------------------+-----------------+---------------+-------------+-----------------+-----------------------------+-------------+&lt;/p&gt; 
&lt;p&gt;Check out our &lt;code&gt;engine comparison page &amp;lt;https://docs.daft.ai/en/stable/resources/engine_comparison/&amp;gt;&lt;/code&gt;_ for more details!&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Daft has an Apache 2.0 license - please see the LICENSE file.&lt;/p&gt; 
&lt;p&gt;.. |Quickstart Image| image:: &lt;a href="https://github.com/Eventual-Inc/Daft/assets/17691182/dea2f515-9739-4f3e-ac58-cd96d51e44a8"&gt;https://github.com/Eventual-Inc/Daft/assets/17691182/dea2f515-9739-4f3e-ac58-cd96d51e44a8&lt;/a&gt; :alt: Dataframe code to load a folder of images from AWS S3 and create thumbnails :height: 256&lt;/p&gt; 
&lt;p&gt;.. |Benchmark Image| image:: &lt;a href="https://github-production-user-asset-6210df.s3.amazonaws.com/2550285/243524430-338e427d-f049-40b3-b555-4059d6be7bfd.png"&gt;https://github-production-user-asset-6210df.s3.amazonaws.com/2550285/243524430-338e427d-f049-40b3-b555-4059d6be7bfd.png&lt;/a&gt; :alt: Benchmarks for SF100 TPCH&lt;/p&gt; 
&lt;p&gt;.. |Banner| image:: &lt;a href="https://daft.ai/images/diagram.png"&gt;https://daft.ai/images/diagram.png&lt;/a&gt; :target: &lt;a href="https://www.daft.ai"&gt;https://www.daft.ai&lt;/a&gt; :alt: Daft dataframes can load any data such as PDF documents, images, protobufs, csv, parquet and audio files into a table dataframe structure for easy querying&lt;/p&gt; 
&lt;p&gt;.. |CI| image:: &lt;a href="https://github.com/Eventual-Inc/Daft/actions/workflows/pr-test-suite.yml/badge.svg"&gt;https://github.com/Eventual-Inc/Daft/actions/workflows/pr-test-suite.yml/badge.svg&lt;/a&gt; :target: &lt;a href="https://github.com/Eventual-Inc/Daft/actions/workflows/pr-test-suite.yml?query=branch:main"&gt;https://github.com/Eventual-Inc/Daft/actions/workflows/pr-test-suite.yml?query=branch:main&lt;/a&gt; :alt: GitHub Actions tests&lt;/p&gt; 
&lt;p&gt;.. |PyPI| image:: &lt;a href="https://img.shields.io/pypi/v/daft.svg?label=pip&amp;amp;logo=PyPI&amp;amp;logoColor=white"&gt;https://img.shields.io/pypi/v/daft.svg?label=pip&amp;amp;logo=PyPI&amp;amp;logoColor=white&lt;/a&gt; :target: &lt;a href="https://pypi.org/project/daft"&gt;https://pypi.org/project/daft&lt;/a&gt; :alt: PyPI&lt;/p&gt; 
&lt;p&gt;.. |Latest Tag| image:: &lt;a href="https://img.shields.io/github/v/tag/Eventual-Inc/Daft?label=latest&amp;amp;logo=GitHub"&gt;https://img.shields.io/github/v/tag/Eventual-Inc/Daft?label=latest&amp;amp;logo=GitHub&lt;/a&gt; :target: &lt;a href="https://github.com/Eventual-Inc/Daft/tags"&gt;https://github.com/Eventual-Inc/Daft/tags&lt;/a&gt; :alt: latest tag&lt;/p&gt; 
&lt;p&gt;.. |Coverage| image:: &lt;a href="https://codecov.io/gh/Eventual-Inc/Daft/branch/main/graph/badge.svg?token=J430QVFE89"&gt;https://codecov.io/gh/Eventual-Inc/Daft/branch/main/graph/badge.svg?token=J430QVFE89&lt;/a&gt; :target: &lt;a href="https://codecov.io/gh/Eventual-Inc/Daft"&gt;https://codecov.io/gh/Eventual-Inc/Daft&lt;/a&gt; :alt: Coverage&lt;/p&gt; 
&lt;p&gt;.. |Slack| image:: &lt;a href="https://img.shields.io/badge/slack-@distdata-purple.svg?logo=slack"&gt;https://img.shields.io/badge/slack-@distdata-purple.svg?logo=slack&lt;/a&gt; :target: &lt;a href="https://join.slack.com/t/dist-data/shared_invite/zt-2e77olvxw-uyZcPPV1SRchhi8ah6ZCtg"&gt;https://join.slack.com/t/dist-data/shared_invite/zt-2e77olvxw-uyZcPPV1SRchhi8ah6ZCtg&lt;/a&gt; :alt: slack community&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ai-dynamo/dynamo</title>
      <link>https://github.com/ai-dynamo/dynamo</link>
      <description>&lt;p&gt;A Datacenter Scale Distributed Inference Serving Framework&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/docs/images/frontpage-banner.png" alt="Dynamo banner" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://opensource.org/licenses/Apache-2.0"&gt;&lt;img src="https://img.shields.io/badge/License-Apache_2.0-blue.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://github.com/ai-dynamo/dynamo/releases/latest"&gt;&lt;img src="https://img.shields.io/github/v/release/ai-dynamo/dynamo" alt="GitHub Release" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/D92uqZRjCZ"&gt;&lt;img src="https://dcbadge.limes.pink/api/server/D92uqZRjCZ?style=flat" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/ai-dynamo/dynamo"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;| &lt;strong&gt;&lt;a href="https://github.com/ai-dynamo/dynamo/issues/762"&gt;Roadmap&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href="https://github.com/ai-dynamo/dynamo/raw/main/docs/support_matrix.md"&gt;Support matrix&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href="https://docs.nvidia.com/dynamo/latest/index.html"&gt;Documentation&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href="https://github.com/ai-dynamo/dynamo/tree/main/examples"&gt;Examples&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href="https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-dynamo/collections/ai-dynamo"&gt;Prebuilt containers&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href="https://github.com/ai-dynamo/enhancements"&gt;Design Proposals&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href="https://developer.nvidia.com/blog/tag/nvidia-dynamo"&gt;Blogs&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h1&gt;NVIDIA Dynamo&lt;/h1&gt; 
&lt;p&gt;High-throughput, low-latency inference framework designed for serving generative AI and reasoning models in multi-node distributed environments.&lt;/p&gt; 
&lt;h2&gt;Latest News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;[08/05] Deploy &lt;code&gt;openai/gpt-oss-120b&lt;/code&gt; with disaggregated serving on NVIDIA Blackwell GPUs using Dynamo &lt;a href="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/components/backends/trtllm/gpt-oss.md"&gt;➡️ link&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;The Era of Multi-GPU, Multi-Node&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/docs/images/frontpage-gpu-vertical.png" alt="Multi Node Multi-GPU topology" width="600" /&gt; &lt;/p&gt; 
&lt;p&gt;Large language models are quickly outgrowing the memory and compute budget of any single GPU. Tensor-parallelism solves the capacity problem by spreading each layer across many GPUs—and sometimes many servers—but it creates a new one: how do you coordinate those shards, route requests, and share KV cache fast enough to feel like one accelerator? This orchestration gap is exactly what NVIDIA Dynamo is built to close.&lt;/p&gt; 
&lt;p&gt;Dynamo is designed to be inference engine agnostic (supports TRT-LLM, vLLM, SGLang or others) and captures LLM-specific capabilities such as:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Disaggregated prefill &amp;amp; decode inference&lt;/strong&gt; – Maximizes GPU throughput and facilitates trade off between throughput and latency.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Dynamic GPU scheduling&lt;/strong&gt; – Optimizes performance based on fluctuating demand&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;LLM-aware request routing&lt;/strong&gt; – Eliminates unnecessary KV cache re-computation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Accelerated data transfer&lt;/strong&gt; – Reduces inference response time using NIXL.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;KV cache offloading&lt;/strong&gt; – Leverages multiple memory hierarchies for higher system throughput&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/docs/images/frontpage-architecture.png" alt="Dynamo architecture" width="600" /&gt; &lt;/p&gt; 
&lt;h2&gt;Framework Support Matrix&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Feature&lt;/th&gt; 
   &lt;th&gt;vLLM&lt;/th&gt; 
   &lt;th&gt;SGLang&lt;/th&gt; 
   &lt;th&gt;TensorRT-LLM&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/docs/architecture/disagg_serving.md"&gt;&lt;strong&gt;Disaggregated Serving&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/docs/architecture/disagg_serving.md#conditional-disaggregation"&gt;&lt;strong&gt;Conditional Disaggregation&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;🚧&lt;/td&gt; 
   &lt;td&gt;🚧&lt;/td&gt; 
   &lt;td&gt;🚧&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/docs/architecture/kv_cache_routing.md"&gt;&lt;strong&gt;KV-Aware Routing&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/docs/architecture/load_planner.md"&gt;&lt;strong&gt;Load Based Planner&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;🚧&lt;/td&gt; 
   &lt;td&gt;🚧&lt;/td&gt; 
   &lt;td&gt;🚧&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/docs/architecture/sla_planner.md"&gt;&lt;strong&gt;SLA-Based Planner&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;🚧&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/docs/architecture/kvbm_architecture.md"&gt;&lt;strong&gt;KVBM&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;🚧&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;To learn more about each framework and their capabilities, check out each framework's README!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/components/backends/vllm/README.md"&gt;vLLM&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/components/backends/sglang/README.md"&gt;SGLang&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/components/backends/trtllm/README.md"&gt;TensorRT-LLM&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Built in Rust for performance and in Python for extensibility, Dynamo is fully open-source and driven by a transparent, OSS (Open Source Software) first development approach.&lt;/p&gt; 
&lt;h1&gt;Installation&lt;/h1&gt; 
&lt;p&gt;The following examples require a few system level packages. Recommended to use Ubuntu 24.04 with a x86_64 CPU. See &lt;a href="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/docs/support_matrix.md"&gt;docs/support_matrix.md&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;1. Initial setup&lt;/h2&gt; 
&lt;p&gt;The Dynamo team recommends the &lt;code&gt;uv&lt;/code&gt; Python package manager, although any way works. Install uv:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;curl -LsSf https://astral.sh/uv/install.sh | sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Install etcd and NATS (required)&lt;/h3&gt; 
&lt;p&gt;To coordinate across a data center, Dynamo relies on etcd and NATS. To run Dynamo locally, these need to be available.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://etcd.io/"&gt;etcd&lt;/a&gt; can be run directly as &lt;code&gt;./etcd&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nats.io/"&gt;nats&lt;/a&gt; needs jetstream enabled: &lt;code&gt;nats-server -js&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To quickly setup etcd &amp;amp; NATS, you can also run:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;# At the root of the repository:
docker compose -f deploy/docker-compose.yml up -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;2. Select an engine&lt;/h2&gt; 
&lt;p&gt;We publish Python wheels specialized for each of our supported engines: vllm, sglang, trtllm, and llama.cpp. The examples that follow use SGLang; continue reading for other engines.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;uv venv venv
source venv/bin/activate
uv pip install pip

# Choose one
uv pip install "ai-dynamo[sglang]"  #replace with [vllm], [trtllm], etc.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;3. Run Dynamo&lt;/h2&gt; 
&lt;h3&gt;Running an LLM API server&lt;/h3&gt; 
&lt;p&gt;Dynamo provides a simple way to spin up a local set of inference components including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;OpenAI Compatible Frontend&lt;/strong&gt; – High performance OpenAI compatible http api server written in Rust.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Basic and Kv Aware Router&lt;/strong&gt; – Route and load balance traffic to a set of workers.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Workers&lt;/strong&gt; – Set of pre-configured LLM serving engines.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;# Start an OpenAI compatible HTTP server, a pre-processor (prompt templating and tokenization) and a router.
# Pass the TLS certificate and key paths to use HTTPS instead of HTTP.
python -m dynamo.frontend --http-port 8000 [--tls-cert-path cert.pem] [--tls-key-path key.pem]

# Start the SGLang engine, connecting to NATS and etcd to receive requests. You can run several of these,
# both for the same model and for multiple models. The frontend node will discover them.
python -m dynamo.sglang.worker --model deepseek-ai/DeepSeek-R1-Distill-Llama-8B --skip-tokenizer-init
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Send a Request&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl localhost:8000/v1/chat/completions   -H "Content-Type: application/json"   -d '{
    "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
    "messages": [
    {
        "role": "user",
        "content": "Hello, how are you?"
    }
    ],
    "stream":false,
    "max_tokens": 300
  }' | jq
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Rerun with &lt;code&gt;curl -N&lt;/code&gt; and change &lt;code&gt;stream&lt;/code&gt; in the request to &lt;code&gt;true&lt;/code&gt; to get the responses as soon as the engine issues them.&lt;/p&gt; 
&lt;h3&gt;Deploying Dynamo&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Follow the &lt;a href="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/docs/guides/dynamo_deploy/README.md"&gt;Quickstart Guide&lt;/a&gt; to deploy on Kubernetes.&lt;/li&gt; 
 &lt;li&gt;Check out &lt;a href="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/components/backends"&gt;Backends&lt;/a&gt; to deploy various workflow configurations (e.g. SGLang with router, vLLM with disaggregated serving, etc.)&lt;/li&gt; 
 &lt;li&gt;Run some &lt;a href="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/examples"&gt;Examples&lt;/a&gt; to learn about building components in Dynamo and exploring various integrations.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Benchmarking Dynamo&lt;/h3&gt; 
&lt;p&gt;Dynamo provides comprehensive benchmarking tools to evaluate and optimize your deployments:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/docs/benchmarks/benchmarking.md"&gt;Benchmarking Guide&lt;/a&gt;&lt;/strong&gt; – Compare deployment topologies (aggregated vs. disaggregated vs. vanilla vLLM) using GenAI-Perf&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/docs/benchmarks/pre_deployment_profiling.md"&gt;Pre-Deployment Profiling&lt;/a&gt;&lt;/strong&gt; – Optimize configurations before deployment to meet SLA requirements&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Engines&lt;/h1&gt; 
&lt;p&gt;Dynamo is designed to be inference engine agnostic. To use any engine with Dynamo, NATS and etcd need to be installed, along with a Dynamo frontend (&lt;code&gt;python -m dynamo.frontend [--interactive]&lt;/code&gt;).&lt;/p&gt; 
&lt;h2&gt;vLLM&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;uv pip install ai-dynamo[vllm]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run the backend/worker like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python -m dynamo.vllm --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;vLLM attempts to allocate enough KV cache for the full context length at startup. If that does not fit in your available memory pass &lt;code&gt;--context-length &amp;lt;value&amp;gt;&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;To specify which GPUs to use set environment variable &lt;code&gt;CUDA_VISIBLE_DEVICES&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;SGLang&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;# Install libnuma
apt install -y libnuma-dev

uv pip install ai-dynamo[sglang]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run the backend/worker like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python -m dynamo.sglang.worker --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can pass any sglang flags directly to this worker, see &lt;a href="https://docs.sglang.ai/advanced_features/server_arguments.html"&gt;https://docs.sglang.ai/advanced_features/server_arguments.html&lt;/a&gt; . See there to use multiple GPUs.&lt;/p&gt; 
&lt;h2&gt;TensorRT-LLM&lt;/h2&gt; 
&lt;p&gt;It is recommended to use &lt;a href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch"&gt;NGC PyTorch Container&lt;/a&gt; for running the TensorRT-LLM engine.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Note] Ensure that you select a PyTorch container image version that matches the version of TensorRT-LLM you are using. For example, if you are using &lt;code&gt;tensorrt-llm==1.0.0rc6&lt;/code&gt;, use the PyTorch container image version &lt;code&gt;25.06&lt;/code&gt;. To find the correct PyTorch container version for your desired &lt;code&gt;tensorrt-llm&lt;/code&gt; release, visit the &lt;a href="https://github.com/NVIDIA/TensorRT-LLM/raw/main/docker/Dockerfile.multi"&gt;TensorRT-LLM Dockerfile.multi&lt;/a&gt; on GitHub. Switch to the branch that matches your &lt;code&gt;tensorrt-llm&lt;/code&gt; version, and look for the &lt;code&gt;BASE_TAG&lt;/code&gt; line to identify the recommended PyTorch container tag.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Important] Launch container with the following additional settings &lt;code&gt;--shm-size=1g --ulimit memlock=-1&lt;/code&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Install prerequisites&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;# Optional step: Only required for Blackwell and Grace Hopper
uv pip install torch==2.7.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128

# Required until the trtllm version is bumped to include this pinned dependency itself
uv pip install "cuda-python&amp;gt;=12,&amp;lt;13"

sudo apt-get -y install libopenmpi-dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Tip] You can learn more about these prequisites and known issues with TensorRT-LLM pip based installation &lt;a href="https://nvidia.github.io/TensorRT-LLM/installation/linux.html"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;After installing the pre-requisites above, install Dynamo&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;uv pip install ai-dynamo[trtllm]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run the backend/worker like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python -m dynamo.trtllm --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To specify which GPUs to use set environment variable &lt;code&gt;CUDA_VISIBLE_DEVICES&lt;/code&gt;.&lt;/p&gt; 
&lt;h1&gt;Developing Locally&lt;/h1&gt; 
&lt;h2&gt;1. Install libraries&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Ubuntu:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;sudo apt install -y build-essential libhwloc-dev libudev-dev pkg-config libclang-dev protobuf-compiler python3-dev cmake
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;macOS:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://brew.sh/"&gt;Homebrew&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;# if brew is not installed on your system, install it
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://developer.apple.com/xcode/"&gt;Xcode&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;brew install cmake protobuf

## Check that Metal is accessible
xcrun -sdk macosx metal
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If Metal is accessible, you should see an error like &lt;code&gt;metal: error: no input files&lt;/code&gt;, which confirms it is installed correctly.&lt;/p&gt; 
&lt;h2&gt;2. Install Rust&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
source $HOME/.cargo/env
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;3. Create a Python virtual env:&lt;/h2&gt; 
&lt;p&gt;Follow the instructions in &lt;a href="https://docs.astral.sh/uv/#installation"&gt;uv installation&lt;/a&gt; guide to install uv if you don't have &lt;code&gt;uv&lt;/code&gt; installed. Once uv is installed, create a virtual environment and activate it.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Install uv&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -LsSf https://astral.sh/uv/install.sh | sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Create a virtual environment&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv venv dynamo
source dynamo/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;4. Install build tools&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;uv pip install pip maturin
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://github.com/PyO3/maturin"&gt;Maturin&lt;/a&gt; is the Rust&amp;lt;-&amp;gt;Python bindings build tool.&lt;/p&gt; 
&lt;h2&gt;5. Build the Rust bindings&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;cd lib/bindings/python
maturin develop --uv
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;6. Install the wheel&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;cd $PROJECT_ROOT
uv pip install .
# For development, use
export PYTHONPATH="${PYTHONPATH}:$(pwd)/components/frontend/src:$(pwd)/components/planner/src:$(pwd)/components/backends/vllm/src:$(pwd)/components/backends/sglang/src:$(pwd)/components/backends/trtllm/src:$(pwd)/components/backends/llama_cpp/src:$(pwd)/components/backends/mocker/src"
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Note] Editable (&lt;code&gt;-e&lt;/code&gt;) does not work because the &lt;code&gt;dynamo&lt;/code&gt; package is split over multiple directories, one per backend.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;You should now be able to run &lt;code&gt;python -m dynamo.frontend&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Remember that nats and etcd must be running (see earlier).&lt;/p&gt; 
&lt;p&gt;Set the environment variable &lt;code&gt;DYN_LOG&lt;/code&gt; to adjust the logging level; for example, &lt;code&gt;export DYN_LOG=debug&lt;/code&gt;. It has the same syntax as &lt;code&gt;RUST_LOG&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;If you use vscode or cursor, we have a .devcontainer folder built on &lt;a href="https://code.visualstudio.com/docs/devcontainers/containers"&gt;Microsofts Extension&lt;/a&gt;. For instructions see the &lt;a href="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/.devcontainer/README.md"&gt;ReadMe&lt;/a&gt; for more details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>paradigmxyz/reth</title>
      <link>https://github.com/paradigmxyz/reth</link>
      <description>&lt;p&gt;Modular, contributor-friendly and blazing-fast implementation of the Ethereum protocol, in Rust&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;reth&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/paradigmxyz/reth/actions/workflows/bench.yml"&gt;&lt;img src="https://github.com/paradigmxyz/reth/actions/workflows/bench.yml/badge.svg?sanitize=true" alt="bench status" /&gt;&lt;/a&gt; &lt;a href="https://github.com/paradigmxyz/reth/actions/workflows/unit.yml"&gt;&lt;img src="https://github.com/paradigmxyz/reth/workflows/unit/badge.svg?sanitize=true" alt="CI status" /&gt;&lt;/a&gt; &lt;a href="https://github.com/paradigmxyz/reth/actions/workflows/lint.yml"&gt;&lt;img src="https://github.com/paradigmxyz/reth/actions/workflows/lint.yml/badge.svg?sanitize=true" alt="cargo-lint status" /&gt;&lt;/a&gt; &lt;a href="https://t.me/paradigm_reth"&gt;&lt;img src="https://img.shields.io/endpoint?color=neon&amp;amp;logo=telegram&amp;amp;label=chat&amp;amp;url=https%3A%2F%2Ftg.sumanjay.workers.dev%2Fparadigm%5Freth" alt="Telegram Chat" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Modular, contributor-friendly and blazing-fast implementation of the Ethereum protocol&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/paradigmxyz/reth/main/assets/reth-prod.png" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://paradigmxyz.github.io/reth/installation/installation.html"&gt;Install&lt;/a&gt;&lt;/strong&gt; | &lt;a href="https://reth.rs"&gt;User Docs&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/paradigmxyz/reth/main/docs"&gt;Developer Docs&lt;/a&gt; | &lt;a href="https://reth.rs/docs"&gt;Crate Docs&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;What is Reth?&lt;/h2&gt; 
&lt;p&gt;Reth (short for Rust Ethereum, &lt;a href="https://twitter.com/kelvinfichter/status/1597653609411268608"&gt;pronunciation&lt;/a&gt;) is a new Ethereum full node implementation that is focused on being user-friendly, highly modular, as well as being fast and efficient. Reth is an Execution Layer (EL) and is compatible with all Ethereum Consensus Layer (CL) implementations that support the &lt;a href="https://github.com/ethereum/execution-apis/tree/a0d03086564ab1838b462befbc083f873dcf0c0f/src/engine"&gt;Engine API&lt;/a&gt;. It is originally built and driven forward by &lt;a href="https://paradigm.xyz/"&gt;Paradigm&lt;/a&gt;, and is licensed under the Apache and MIT licenses.&lt;/p&gt; 
&lt;h2&gt;Goals&lt;/h2&gt; 
&lt;p&gt;As a full Ethereum node, Reth allows users to connect to the Ethereum network and interact with the Ethereum blockchain. This includes sending and receiving transactions/logs/traces, as well as accessing and interacting with smart contracts. Building a successful Ethereum node requires creating a high-quality implementation that is both secure and efficient, as well as being easy to use on consumer hardware. It also requires building a strong community of contributors who can help support and improve the software.&lt;/p&gt; 
&lt;p&gt;More concretely, our goals are:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Modularity&lt;/strong&gt;: Every component of Reth is built to be used as a library: well-tested, heavily documented and benchmarked. We envision that developers will import the node's crates, mix and match, and innovate on top of them. Examples of such usage include but are not limited to spinning up standalone P2P networks, talking directly to a node's database, or "unbundling" the node into the components you need. To achieve that, we are licensing Reth under the Apache/MIT permissive license. You can learn more about the project's components &lt;a href="https://raw.githubusercontent.com/paradigmxyz/reth/main/docs/repo/layout.md"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Performance&lt;/strong&gt;: Reth aims to be fast, so we use Rust and the &lt;a href="https://erigon.substack.com/p/erigon-stage-sync-and-control-flows"&gt;Erigon staged-sync&lt;/a&gt; node architecture. We also use our Ethereum libraries (including &lt;a href="https://github.com/alloy-rs/alloy/"&gt;Alloy&lt;/a&gt; and &lt;a href="https://github.com/bluealloy/revm/"&gt;revm&lt;/a&gt;) which we've battle-tested and optimized via &lt;a href="https://github.com/foundry-rs/foundry/"&gt;Foundry&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Free for anyone to use any way they want&lt;/strong&gt;: Reth is free open source software, built for the community, by the community. By licensing the software under the Apache/MIT license, we want developers to use it without being bound by business licenses, or having to think about the implications of GPL-like licenses.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Client Diversity&lt;/strong&gt;: The Ethereum protocol becomes more antifragile when no node implementation dominates. This ensures that if there's a software bug, the network does not finalize a bad block. By building a new client, we hope to contribute to Ethereum's antifragility.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Support as many EVM chains as possible&lt;/strong&gt;: We aspire that Reth can full-sync not only Ethereum, but also other chains like Optimism, Polygon, BNB Smart Chain, and more. If you're working on any of these projects, please reach out.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Configurability&lt;/strong&gt;: We want to solve for node operators that care about fast historical queries, but also for hobbyists who cannot operate on large hardware. We also want to support teams and individuals who want both sync from genesis and via "fast sync". We envision that Reth will be configurable enough and provide configurable "profiles" for the tradeoffs that each team faces.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Status&lt;/h2&gt; 
&lt;p&gt;Reth is production ready, and suitable for usage in mission-critical environments such as staking or high-uptime services. We also actively recommend professional node operators to switch to Reth in production for performance and cost reasons in use cases where high performance with great margins is required such as RPC, MEV, Indexing, Simulations, and P2P activities.&lt;/p&gt; 
&lt;p&gt;More historical context below:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;We released 1.0 "production-ready" stable Reth in June 2024. 
  &lt;ul&gt; 
   &lt;li&gt;Reth completed an audit with &lt;a href="https://sigmaprime.io/"&gt;Sigma Prime&lt;/a&gt;, the developers of &lt;a href="https://github.com/sigp/lighthouse"&gt;Lighthouse&lt;/a&gt;, the Rust Consensus Layer implementation. Find it &lt;a href="https://raw.githubusercontent.com/paradigmxyz/reth/main/audit/sigma_prime_audit_v2.pdf"&gt;here&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;Revm (the EVM used in Reth) underwent an audit with &lt;a href="https://twitter.com/guidovranken"&gt;Guido Vranken&lt;/a&gt; (#1 &lt;a href="https://ethereum.org/en/bug-bounty"&gt;Ethereum Bug Bounty&lt;/a&gt;). We will publish the results soon.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;We released multiple iterative beta versions, up to &lt;a href="https://github.com/paradigmxyz/reth/releases/tag/v0.2.0-beta.9"&gt;beta.9&lt;/a&gt; on Monday June 3, 2024,the last beta release.&lt;/li&gt; 
 &lt;li&gt;We released &lt;a href="https://github.com/paradigmxyz/reth/releases/tag/v0.2.0-beta.1"&gt;beta&lt;/a&gt; on Monday March 4, 2024, our first breaking change to the database model, providing faster query speed, smaller database footprint, and allowing "history" to be mounted on separate drives.&lt;/li&gt; 
 &lt;li&gt;We shipped iterative improvements until the last alpha release on February 28, 2024, &lt;a href="https://github.com/paradigmxyz/reth/releases/tag/v0.1.0-alpha.21"&gt;0.1.0-alpha.21&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;We &lt;a href="https://www.paradigm.xyz/2023/06/reth-alpha"&gt;initially announced&lt;/a&gt; &lt;a href="https://github.com/paradigmxyz/reth/releases/tag/v0.1.0-alpha.1"&gt;0.1.0-alpha.1&lt;/a&gt; on June 20, 2023.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Database compatibility&lt;/h3&gt; 
&lt;p&gt;We do not have any breaking database changes since beta.1, and we do not plan any in the near future.&lt;/p&gt; 
&lt;p&gt;Reth &lt;a href="https://github.com/paradigmxyz/reth/releases/tag/v0.2.0-beta.1"&gt;v0.2.0-beta.1&lt;/a&gt; includes a &lt;a href="https://github.com/paradigmxyz/reth/pull/5191"&gt;set of breaking database changes&lt;/a&gt; that makes it impossible to use database files produced by earlier versions.&lt;/p&gt; 
&lt;p&gt;If you had a database produced by alpha versions of Reth, you need to drop it with &lt;code&gt;reth db drop&lt;/code&gt; (using the same arguments such as &lt;code&gt;--config&lt;/code&gt; or &lt;code&gt;--datadir&lt;/code&gt; that you passed to &lt;code&gt;reth node&lt;/code&gt;), and resync using the same &lt;code&gt;reth node&lt;/code&gt; command you've used before.&lt;/p&gt; 
&lt;h2&gt;For Users&lt;/h2&gt; 
&lt;p&gt;See the &lt;a href="https://paradigmxyz.github.io/reth"&gt;Reth documentation&lt;/a&gt; for instructions on how to install and run Reth.&lt;/p&gt; 
&lt;h2&gt;For Developers&lt;/h2&gt; 
&lt;h3&gt;Using reth as a library&lt;/h3&gt; 
&lt;p&gt;You can use individual crates of reth in your project.&lt;/p&gt; 
&lt;p&gt;The crate docs can be found &lt;a href="https://paradigmxyz.github.io/reth/docs"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For a general overview of the crates, see &lt;a href="https://raw.githubusercontent.com/paradigmxyz/reth/main/docs/repo/layout.md"&gt;Project Layout&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Contributing&lt;/h3&gt; 
&lt;p&gt;If you want to contribute, or follow along with contributor discussion, you can use our &lt;a href="https://t.me/paradigm_reth"&gt;main telegram&lt;/a&gt; to chat with us about the development of Reth!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Our contributor guidelines can be found in &lt;a href="https://raw.githubusercontent.com/paradigmxyz/reth/main/CONTRIBUTING.md"&gt;&lt;code&gt;CONTRIBUTING.md&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;See our &lt;a href="https://raw.githubusercontent.com/paradigmxyz/reth/main/docs"&gt;contributor docs&lt;/a&gt; for more information on the project. A good starting point is &lt;a href="https://raw.githubusercontent.com/paradigmxyz/reth/main/docs/repo/layout.md"&gt;Project Layout&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Building and testing&lt;/h3&gt; 
&lt;!--
When updating this, also update:
- Cargo.toml
- .github/workflows/lint.yml
--&gt; 
&lt;p&gt;The Minimum Supported Rust Version (MSRV) of this project is &lt;a href="https://blog.rust-lang.org/2025/06/26/Rust-1.88.0/"&gt;1.88.0&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;See the docs for detailed instructions on how to &lt;a href="https://paradigmxyz.github.io/reth/installation/source"&gt;build from source&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To fully test Reth, you will need to have &lt;a href="https://geth.ethereum.org/docs/getting-started/installing-geth"&gt;Geth installed&lt;/a&gt;, but it is possible to run a subset of tests without Geth.&lt;/p&gt; 
&lt;p&gt;First, clone the repository:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;git clone https://github.com/paradigmxyz/reth
cd reth
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Next, run the tests:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;cargo nextest run --workspace

# Run the Ethereum Foundation tests
make ef-tests
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We highly recommend using &lt;a href="https://nexte.st/"&gt;&lt;code&gt;cargo nextest&lt;/code&gt;&lt;/a&gt; to speed up testing. Using &lt;code&gt;cargo test&lt;/code&gt; to run tests may work fine, but this is not tested and does not support more advanced features like retries for spurious failures.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Some tests use random number generators to generate test data. If you want to use a deterministic seed, you can set the &lt;code&gt;SEED&lt;/code&gt; environment variable.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Getting Help&lt;/h2&gt; 
&lt;p&gt;If you have any questions, first see if the answer to your question can be found in the &lt;a href="https://paradigmxyz.github.io/reth/"&gt;docs&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If the answer is not there:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Join the &lt;a href="https://t.me/paradigm_reth"&gt;Telegram&lt;/a&gt; to get help, or&lt;/li&gt; 
 &lt;li&gt;Open a &lt;a href="https://github.com/paradigmxyz/reth/discussions/new"&gt;discussion&lt;/a&gt; with your question, or&lt;/li&gt; 
 &lt;li&gt;Open an issue with &lt;a href="https://github.com/paradigmxyz/reth/issues/new?assignees=&amp;amp;labels=C-bug%2CS-needs-triage&amp;amp;projects=&amp;amp;template=bug.yml"&gt;the bug&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Security&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/paradigmxyz/reth/main/SECURITY.md"&gt;&lt;code&gt;SECURITY.md&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;p&gt;Reth is a new implementation of the Ethereum protocol. In the process of developing the node we investigated the design decisions other nodes have made to understand what is done well, what is not, and where we can improve the status quo.&lt;/p&gt; 
&lt;p&gt;None of this would have been possible without them, so big shoutout to the teams below:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ethereum/go-ethereum/"&gt;Geth&lt;/a&gt;: We would like to express our heartfelt gratitude to the go-ethereum team for their outstanding contributions to Ethereum over the years. Their tireless efforts and dedication have helped to shape the Ethereum ecosystem and make it the vibrant and innovative community it is today. Thank you for your hard work and commitment to the project.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ledgerwatch/erigon"&gt;Erigon&lt;/a&gt; (fka Turbo-Geth): Erigon pioneered the &lt;a href="https://erigon.substack.com/p/erigon-stage-sync-and-control-flows"&gt;"Staged Sync" architecture&lt;/a&gt; that Reth is using, as well as &lt;a href="https://github.com/ledgerwatch/erigon/wiki/Choice-of-storage-engine"&gt;introduced MDBX&lt;/a&gt; as the database of choice. We thank Erigon for pushing the state of the art research on the performance limits of Ethereum nodes.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/akula-bft/akula/"&gt;Akula&lt;/a&gt;: Reth uses forks of the Apache versions of Akula's &lt;a href="https://github.com/paradigmxyz/reth/pull/132"&gt;MDBX Bindings&lt;/a&gt;, &lt;a href="https://github.com/paradigmxyz/reth/pull/63"&gt;FastRLP&lt;/a&gt; and &lt;a href="https://github.com/paradigmxyz/reth/pull/80"&gt;ECIES&lt;/a&gt;. Given that these packages were already released under the Apache License, and they implement standardized solutions, we decided not to reimplement them to iterate faster. We thank the Akula team for their contributions to the Rust Ethereum ecosystem and for publishing these packages.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Warning&lt;/h2&gt; 
&lt;p&gt;The &lt;code&gt;NippyJar&lt;/code&gt; and &lt;code&gt;Compact&lt;/code&gt; encoding formats and their implementations are designed for storing and retrieving data internally. They are not hardened to safely read potentially malicious data.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>zed-industries/zed</title>
      <link>https://github.com/zed-industries/zed</link>
      <description>&lt;p&gt;Code at the speed of thought – Zed is a high-performance, multiplayer code editor from the creators of Atom and Tree-sitter.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Zed&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://zed.dev"&gt;&lt;img src="https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/zed-industries/zed/main/assets/badge/v0.json" alt="Zed" /&gt;&lt;/a&gt; &lt;a href="https://github.com/zed-industries/zed/actions/workflows/ci.yml"&gt;&lt;img src="https://github.com/zed-industries/zed/actions/workflows/ci.yml/badge.svg?sanitize=true" alt="CI" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Welcome to Zed, a high-performance, multiplayer code editor from the creators of &lt;a href="https://github.com/atom/atom"&gt;Atom&lt;/a&gt; and &lt;a href="https://github.com/tree-sitter/tree-sitter"&gt;Tree-sitter&lt;/a&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;p&gt;On macOS and Linux you can &lt;a href="https://zed.dev/download"&gt;download Zed directly&lt;/a&gt; or &lt;a href="https://zed.dev/docs/linux#installing-via-a-package-manager"&gt;install Zed via your local package manager&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Other platforms are not yet available:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Windows (&lt;a href="https://github.com/zed-industries/zed/issues/5394"&gt;tracking issue&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;Web (&lt;a href="https://github.com/zed-industries/zed/issues/5396"&gt;tracking issue&lt;/a&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Developing Zed&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zed-industries/zed/main/docs/src/development/macos.md"&gt;Building Zed for macOS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zed-industries/zed/main/docs/src/development/linux.md"&gt;Building Zed for Linux&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zed-industries/zed/main/docs/src/development/windows.md"&gt;Building Zed for Windows&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zed-industries/zed/main/docs/src/development/local-collaboration.md"&gt;Running Collaboration Locally&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Contributing&lt;/h3&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/zed-industries/zed/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for ways you can contribute to Zed.&lt;/p&gt; 
&lt;p&gt;Also... we're hiring! Check out our &lt;a href="https://zed.dev/jobs"&gt;jobs&lt;/a&gt; page for open roles.&lt;/p&gt; 
&lt;h3&gt;Licensing&lt;/h3&gt; 
&lt;p&gt;License information for third party dependencies must be correctly provided for CI to pass.&lt;/p&gt; 
&lt;p&gt;We use &lt;a href="https://github.com/EmbarkStudios/cargo-about"&gt;&lt;code&gt;cargo-about&lt;/code&gt;&lt;/a&gt; to automatically comply with open source licenses. If CI is failing, check the following:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Is it showing a &lt;code&gt;no license specified&lt;/code&gt; error for a crate you've created? If so, add &lt;code&gt;publish = false&lt;/code&gt; under &lt;code&gt;[package]&lt;/code&gt; in your crate's Cargo.toml.&lt;/li&gt; 
 &lt;li&gt;Is the error &lt;code&gt;failed to satisfy license requirements&lt;/code&gt; for a dependency? If so, first determine what license the project has and whether this system is sufficient to comply with this license's requirements. If you're unsure, ask a lawyer. Once you've verified that this system is acceptable add the license's SPDX identifier to the &lt;code&gt;accepted&lt;/code&gt; array in &lt;code&gt;script/licenses/zed-licenses.toml&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Is &lt;code&gt;cargo-about&lt;/code&gt; unable to find the license for a dependency? If so, add a clarification field at the end of &lt;code&gt;script/licenses/zed-licenses.toml&lt;/code&gt;, as specified in the &lt;a href="https://embarkstudios.github.io/cargo-about/cli/generate/config.html#crate-configuration"&gt;cargo-about book&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>huggingface/text-embeddings-inference</title>
      <link>https://github.com/huggingface/text-embeddings-inference</link>
      <description>&lt;p&gt;A blazing fast inference solution for text embeddings models&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h1&gt;Text Embeddings Inference&lt;/h1&gt; 
 &lt;a href="https://github.com/huggingface/text-embeddings-inference"&gt; &lt;img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/huggingface/text-embeddings-inference?style=social" /&gt; &lt;/a&gt; 
 &lt;a href="https://huggingface.github.io/text-embeddings-inference"&gt; &lt;img alt="Swagger API documentation" src="https://img.shields.io/badge/API-Swagger-informational" /&gt; &lt;/a&gt; 
 &lt;p&gt;A blazing fast inference solution for text embeddings models.&lt;/p&gt; 
 &lt;p&gt;Benchmark for &lt;a href="https://huggingface.co/BAAI/bge-base-en-v1.5"&gt;BAAI/bge-base-en-v1.5&lt;/a&gt; on an Nvidia A10 with a sequence length of 512 tokens:&lt;/p&gt; 
 &lt;p&gt; &lt;img src="https://raw.githubusercontent.com/huggingface/text-embeddings-inference/main/assets/bs1-lat.png" width="400" /&gt; &lt;img src="https://raw.githubusercontent.com/huggingface/text-embeddings-inference/main/assets/bs1-tp.png" width="400" /&gt; &lt;/p&gt; 
 &lt;p&gt; &lt;img src="https://raw.githubusercontent.com/huggingface/text-embeddings-inference/main/assets/bs32-lat.png" width="400" /&gt; &lt;img src="https://raw.githubusercontent.com/huggingface/text-embeddings-inference/main/assets/bs32-tp.png" width="400" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;Table of contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/text-embeddings-inference/main/#get-started"&gt;Get Started&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/text-embeddings-inference/main/#supported-models"&gt;Supported Models&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/text-embeddings-inference/main/#docker"&gt;Docker&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/text-embeddings-inference/main/#docker-images"&gt;Docker Images&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/text-embeddings-inference/main/#api-documentation"&gt;API Documentation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/text-embeddings-inference/main/#using-a-private-or-gated-model"&gt;Using a private or gated model&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/text-embeddings-inference/main/#air-gapped-deployment"&gt;Air gapped deployment&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/text-embeddings-inference/main/#using-re-rankers-models"&gt;Using Re-rankers models&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/text-embeddings-inference/main/#using-sequence-classification-models"&gt;Using Sequence Classification models&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/text-embeddings-inference/main/#using-splade-pooling"&gt;Using SPLADE pooling&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/text-embeddings-inference/main/#distributed-tracing"&gt;Distributed Tracing&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/text-embeddings-inference/main/#grpc"&gt;gRPC&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/text-embeddings-inference/main/#local-install"&gt;Local Install&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/text-embeddings-inference/main/#docker-build"&gt;Docker Build&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/text-embeddings-inference/main/#apple-m1m2-arm64-architectures"&gt;Apple M1/M2 Arm&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/text-embeddings-inference/main/#examples"&gt;Examples&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Text Embeddings Inference (TEI) is a toolkit for deploying and serving open source text embeddings and sequence classification models. TEI enables high-performance extraction for the most popular models, including FlagEmbedding, Ember, GTE and E5. TEI implements many features such as:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;No model graph compilation step&lt;/li&gt; 
 &lt;li&gt;Metal support for local execution on Macs&lt;/li&gt; 
 &lt;li&gt;Small docker images and fast boot times. Get ready for true serverless!&lt;/li&gt; 
 &lt;li&gt;Token based dynamic batching&lt;/li&gt; 
 &lt;li&gt;Optimized transformers code for inference using &lt;a href="https://github.com/HazyResearch/flash-attention"&gt;Flash Attention&lt;/a&gt;, &lt;a href="https://github.com/huggingface/candle"&gt;Candle&lt;/a&gt; and &lt;a href="https://docs.nvidia.com/cuda/cublas/#using-the-cublaslt-api"&gt;cuBLASLt&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/huggingface/safetensors"&gt;Safetensors&lt;/a&gt; weight loading&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/onnx/onnx"&gt;ONNX&lt;/a&gt; weight loading&lt;/li&gt; 
 &lt;li&gt;Production ready (distributed tracing with Open Telemetry, Prometheus metrics)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Get Started&lt;/h2&gt; 
&lt;h3&gt;Supported Models&lt;/h3&gt; 
&lt;h4&gt;Text Embeddings&lt;/h4&gt; 
&lt;p&gt;Text Embeddings Inference currently supports Nomic, BERT, CamemBERT, XLM-RoBERTa models with absolute positions, JinaBERT model with Alibi positions and Mistral, Alibaba GTE, Qwen2 models with Rope positions, MPNet, ModernBERT, and Qwen3.&lt;/p&gt; 
&lt;p&gt;Below are some examples of the currently supported models:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;MTEB Rank&lt;/th&gt; 
   &lt;th&gt;Model Size&lt;/th&gt; 
   &lt;th&gt;Model Type&lt;/th&gt; 
   &lt;th&gt;Model ID&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;8B (Very Expensive)&lt;/td&gt; 
   &lt;td&gt;Qwen3&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://hf.co/Qwen/Qwen3-Embedding-8B"&gt;Qwen/Qwen3-Embedding-8B&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;4&lt;/td&gt; 
   &lt;td&gt;0.6B&lt;/td&gt; 
   &lt;td&gt;Qwen3&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://hf.co/Qwen/Qwen3-Embedding-0.6B"&gt;Qwen/Qwen3-Embedding-0.6B&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;6&lt;/td&gt; 
   &lt;td&gt;7B (Very Expensive)&lt;/td&gt; 
   &lt;td&gt;Qwen2&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://hf.co/Alibaba-NLP/gte-Qwen2-7B-instruct"&gt;Alibaba-NLP/gte-Qwen2-7B-instruct&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;7&lt;/td&gt; 
   &lt;td&gt;0.5B&lt;/td&gt; 
   &lt;td&gt;XLM-RoBERTa&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://hf.co/intfloat/multilingual-e5-large-instruct"&gt;intfloat/multilingual-e5-large-instruct&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;14&lt;/td&gt; 
   &lt;td&gt;1.5B (Expensive)&lt;/td&gt; 
   &lt;td&gt;Qwen2&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://hf.co/Alibaba-NLP/gte-Qwen2-1.5B-instruct"&gt;Alibaba-NLP/gte-Qwen2-1.5B-instruct&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;17&lt;/td&gt; 
   &lt;td&gt;7B (Very Expensive)&lt;/td&gt; 
   &lt;td&gt;Mistral&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://hf.co/Salesforce/SFR-Embedding-2_R"&gt;Salesforce/SFR-Embedding-2_R&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;34&lt;/td&gt; 
   &lt;td&gt;0.5B&lt;/td&gt; 
   &lt;td&gt;XLM-RoBERTa&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://hf.co/Snowflake/snowflake-arctic-embed-l-v2.0"&gt;Snowflake/snowflake-arctic-embed-l-v2.0&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;40&lt;/td&gt; 
   &lt;td&gt;0.3B&lt;/td&gt; 
   &lt;td&gt;Alibaba GTE&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://hf.co/Snowflake/snowflake-arctic-embed-m-v2.0"&gt;Snowflake/snowflake-arctic-embed-m-v2.0&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;51&lt;/td&gt; 
   &lt;td&gt;0.3B&lt;/td&gt; 
   &lt;td&gt;Bert&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://hf.co/WhereIsAI/UAE-Large-V1"&gt;WhereIsAI/UAE-Large-V1&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
   &lt;td&gt;0.4B&lt;/td&gt; 
   &lt;td&gt;Alibaba GTE&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://hf.co/Alibaba-NLP/gte-large-en-v1.5"&gt;Alibaba-NLP/gte-large-en-v1.5&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
   &lt;td&gt;0.4B&lt;/td&gt; 
   &lt;td&gt;ModernBERT&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://hf.co/answerdotai/ModernBERT-large"&gt;answerdotai/ModernBERT-large&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
   &lt;td&gt;0.3B&lt;/td&gt; 
   &lt;td&gt;NomicBert&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://hf.co/nomic-ai/nomic-embed-text-v2-moe"&gt;nomic-ai/nomic-embed-text-v2-moe&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
   &lt;td&gt;0.1B&lt;/td&gt; 
   &lt;td&gt;NomicBert&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://hf.co/nomic-ai/nomic-embed-text-v1"&gt;nomic-ai/nomic-embed-text-v1&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
   &lt;td&gt;0.1B&lt;/td&gt; 
   &lt;td&gt;NomicBert&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://hf.co/nomic-ai/nomic-embed-text-v1.5"&gt;nomic-ai/nomic-embed-text-v1.5&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
   &lt;td&gt;0.1B&lt;/td&gt; 
   &lt;td&gt;JinaBERT&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://hf.co/jinaai/jina-embeddings-v2-base-en"&gt;jinaai/jina-embeddings-v2-base-en&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
   &lt;td&gt;0.1B&lt;/td&gt; 
   &lt;td&gt;JinaBERT&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://hf.co/jinaai/jina-embeddings-v2-base-code"&gt;jinaai/jina-embeddings-v2-base-code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
   &lt;td&gt;0.1B&lt;/td&gt; 
   &lt;td&gt;MPNet&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://hf.co/sentence-transformers/all-mpnet-base-v2"&gt;sentence-transformers/all-mpnet-base-v2&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;To explore the list of best performing text embeddings models, visit the &lt;a href="https://huggingface.co/spaces/mteb/leaderboard"&gt;Massive Text Embedding Benchmark (MTEB) Leaderboard&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;Sequence Classification and Re-Ranking&lt;/h4&gt; 
&lt;p&gt;Text Embeddings Inference currently supports CamemBERT, and XLM-RoBERTa Sequence Classification models with absolute positions.&lt;/p&gt; 
&lt;p&gt;Below are some examples of the currently supported models:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Task&lt;/th&gt; 
   &lt;th&gt;Model Type&lt;/th&gt; 
   &lt;th&gt;Model ID&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Re-Ranking&lt;/td&gt; 
   &lt;td&gt;XLM-RoBERTa&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/BAAI/bge-reranker-large"&gt;BAAI/bge-reranker-large&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Re-Ranking&lt;/td&gt; 
   &lt;td&gt;XLM-RoBERTa&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/BAAI/bge-reranker-base"&gt;BAAI/bge-reranker-base&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Re-Ranking&lt;/td&gt; 
   &lt;td&gt;GTE&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/Alibaba-NLP/gte-multilingual-reranker-base"&gt;Alibaba-NLP/gte-multilingual-reranker-base&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Re-Ranking&lt;/td&gt; 
   &lt;td&gt;ModernBert&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/Alibaba-NLP/gte-reranker-modernbert-base"&gt;Alibaba-NLP/gte-reranker-modernbert-base&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Sentiment Analysis&lt;/td&gt; 
   &lt;td&gt;RoBERTa&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/SamLowe/roberta-base-go_emotions"&gt;SamLowe/roberta-base-go_emotions&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Docker&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;model=Qwen/Qwen3-Embedding-0.6B
volume=$PWD/data # share a volume with the Docker container to avoid downloading weights every run

docker run --gpus all -p 8080:80 -v $volume:/data --pull always ghcr.io/huggingface/text-embeddings-inference:1.8 --model-id $model
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;And then you can make requests like&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl 127.0.0.1:8080/embed \
    -X POST \
    -d '{"inputs":"What is Deep Learning?"}' \
    -H 'Content-Type: application/json'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; To use GPUs, you need to install the &lt;a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html"&gt;NVIDIA Container Toolkit&lt;/a&gt;. NVIDIA drivers on your machine need to be compatible with CUDA version 12.2 or higher.&lt;/p&gt; 
&lt;p&gt;To see all options to serve your models:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;$ text-embeddings-router --help
Text Embedding Webserver

Usage: text-embeddings-router [OPTIONS]

Options:
      --model-id &amp;lt;MODEL_ID&amp;gt;
          The name of the model to load. Can be a MODEL_ID as listed on &amp;lt;https://hf.co/models&amp;gt; like `BAAI/bge-large-en-v1.5`. Or it can be a local directory containing the necessary files as saved by `save_pretrained(...)` methods of transformers

          [env: MODEL_ID=]
          [default: BAAI/bge-large-en-v1.5]

      --revision &amp;lt;REVISION&amp;gt;
          The actual revision of the model if you're referring to a model on the hub. You can use a specific commit id or a branch like `refs/pr/2`

          [env: REVISION=]

      --tokenization-workers &amp;lt;TOKENIZATION_WORKERS&amp;gt;
          Optionally control the number of tokenizer workers used for payload tokenization, validation and truncation. Default to the number of CPU cores on the machine

          [env: TOKENIZATION_WORKERS=]

      --dtype &amp;lt;DTYPE&amp;gt;
          The dtype to be forced upon the model

          [env: DTYPE=]
          [possible values: float16, float32]

      --pooling &amp;lt;POOLING&amp;gt;
          Optionally control the pooling method for embedding models.

          If `pooling` is not set, the pooling configuration will be parsed from the model `1_Pooling/config.json` configuration.

          If `pooling` is set, it will override the model pooling configuration

          [env: POOLING=]

          Possible values:
          - cls:        Select the CLS token as embedding
          - mean:       Apply Mean pooling to the model embeddings
          - splade:     Apply SPLADE (Sparse Lexical and Expansion) to the model embeddings. This option is only available if the loaded model is a `ForMaskedLM` Transformer model
          - last-token: Select the last token as embedding

      --max-concurrent-requests &amp;lt;MAX_CONCURRENT_REQUESTS&amp;gt;
          The maximum amount of concurrent requests for this particular deployment. Having a low limit will refuse clients requests instead of having them wait for too long and is usually good to handle backpressure correctly

          [env: MAX_CONCURRENT_REQUESTS=]
          [default: 512]

      --max-batch-tokens &amp;lt;MAX_BATCH_TOKENS&amp;gt;
          **IMPORTANT** This is one critical control to allow maximum usage of the available hardware.

          This represents the total amount of potential tokens within a batch.

          For `max_batch_tokens=1000`, you could fit `10` queries of `total_tokens=100` or a single query of `1000` tokens.

          Overall this number should be the largest possible until the model is compute bound. Since the actual memory overhead depends on the model implementation, text-embeddings-inference cannot infer this number automatically.

          [env: MAX_BATCH_TOKENS=]
          [default: 16384]

      --max-batch-requests &amp;lt;MAX_BATCH_REQUESTS&amp;gt;
          Optionally control the maximum number of individual requests in a batch

          [env: MAX_BATCH_REQUESTS=]

      --max-client-batch-size &amp;lt;MAX_CLIENT_BATCH_SIZE&amp;gt;
          Control the maximum number of inputs that a client can send in a single request

          [env: MAX_CLIENT_BATCH_SIZE=]
          [default: 32]

      --auto-truncate
          Automatically truncate inputs that are longer than the maximum supported size

          Unused for gRPC servers

          [env: AUTO_TRUNCATE=]

      --default-prompt-name &amp;lt;DEFAULT_PROMPT_NAME&amp;gt;
          The name of the prompt that should be used by default for encoding. If not set, no prompt will be applied.

          Must be a key in the `sentence-transformers` configuration `prompts` dictionary.

          For example if ``default_prompt_name`` is "query" and the ``prompts`` is {"query": "query: ", ...}, then the sentence "What is the capital of France?" will be encoded as "query: What is the capital of France?" because the prompt text will be prepended before any text to encode.

          The argument '--default-prompt-name &amp;lt;DEFAULT_PROMPT_NAME&amp;gt;' cannot be used with '--default-prompt &amp;lt;DEFAULT_PROMPT&amp;gt;`

          [env: DEFAULT_PROMPT_NAME=]

      --default-prompt &amp;lt;DEFAULT_PROMPT&amp;gt;
          The prompt that should be used by default for encoding. If not set, no prompt will be applied.

          For example if ``default_prompt`` is "query: " then the sentence "What is the capital of France?" will be encoded as "query: What is the capital of France?" because the prompt text will be prepended before any text to encode.

          The argument '--default-prompt &amp;lt;DEFAULT_PROMPT&amp;gt;' cannot be used with '--default-prompt-name &amp;lt;DEFAULT_PROMPT_NAME&amp;gt;`

          [env: DEFAULT_PROMPT=]

      --dense-path &amp;lt;DENSE_PATH&amp;gt;
          Optionally, define the path to the Dense module required for some embedding models.

          Some embedding models require an extra `Dense` module which contains a single Linear layer and an activation function. By default, those `Dense` modules are stored under the `2_Dense` directory, but there might be cases where different `Dense` modules are provided, to convert the pooled embeddings into different dimensions, available as `2_Dense_&amp;lt;dims&amp;gt;` e.g. https://huggingface.co/NovaSearch/stella_en_400M_v5.

          Note that this argument is optional, only required to be set if the path to the `Dense` module is other than `2_Dense`. And it also applies when leveraging the `candle` backend.

          [env: DENSE_PATH=]
          [default: 2_Dense]

      --hf-token &amp;lt;HF_TOKEN&amp;gt;
          Your Hugging Face Hub token

          [env: HF_TOKEN=]

      --hostname &amp;lt;HOSTNAME&amp;gt;
          The IP address to listen on

          [env: HOSTNAME=]
          [default: 0.0.0.0]

      -p, --port &amp;lt;PORT&amp;gt;
          The port to listen on

          [env: PORT=]
          [default: 3000]

      --uds-path &amp;lt;UDS_PATH&amp;gt;
          The name of the unix socket some text-embeddings-inference backends will use as they communicate internally with gRPC

          [env: UDS_PATH=]
          [default: /tmp/text-embeddings-inference-server]

      --huggingface-hub-cache &amp;lt;HUGGINGFACE_HUB_CACHE&amp;gt;
          The location of the huggingface hub cache. Used to override the location if you want to provide a mounted disk for instance

          [env: HUGGINGFACE_HUB_CACHE=]

      --payload-limit &amp;lt;PAYLOAD_LIMIT&amp;gt;
          Payload size limit in bytes

          Default is 2MB

          [env: PAYLOAD_LIMIT=]
          [default: 2000000]

      --api-key &amp;lt;API_KEY&amp;gt;
          Set an api key for request authorization.

          By default the server responds to every request. With an api key set, the requests must have the Authorization header set with the api key as Bearer token.

          [env: API_KEY=]

      --json-output
          Outputs the logs in JSON format (useful for telemetry)

          [env: JSON_OUTPUT=]

      --disable-spans
          [env: DISABLE_SPANS=]

      --otlp-endpoint &amp;lt;OTLP_ENDPOINT&amp;gt;
          The grpc endpoint for opentelemetry. Telemetry is sent to this endpoint as OTLP over gRPC. e.g. `http://localhost:4317`

          [env: OTLP_ENDPOINT=]

      --otlp-service-name &amp;lt;OTLP_SERVICE_NAME&amp;gt;
          The service name for opentelemetry. e.g. `text-embeddings-inference.server`

          [env: OTLP_SERVICE_NAME=]
          [default: text-embeddings-inference.server]

      --prometheus-port &amp;lt;PROMETHEUS_PORT&amp;gt;
          The Prometheus port to listen on

          [env: PROMETHEUS_PORT=]
          [default: 9000]

      --cors-allow-origin &amp;lt;CORS_ALLOW_ORIGIN&amp;gt;
          Unused for gRPC servers

          [env: CORS_ALLOW_ORIGIN=]

  -h, --help
          Print help (see a summary with '-h')

  -V, --version
          Print version
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Docker Images&lt;/h3&gt; 
&lt;p&gt;Text Embeddings Inference ships with multiple Docker images that you can use to target a specific backend:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Architecture&lt;/th&gt; 
   &lt;th&gt;Image&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;CPU&lt;/td&gt; 
   &lt;td&gt;ghcr.io/huggingface/text-embeddings-inference:cpu-1.8&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Volta&lt;/td&gt; 
   &lt;td&gt;NOT SUPPORTED&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Turing (T4, RTX 2000 series, ...)&lt;/td&gt; 
   &lt;td&gt;ghcr.io/huggingface/text-embeddings-inference:turing-1.8 (experimental)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ampere 80 (A100, A30)&lt;/td&gt; 
   &lt;td&gt;ghcr.io/huggingface/text-embeddings-inference:1.8&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ampere 86 (A10, A40, ...)&lt;/td&gt; 
   &lt;td&gt;ghcr.io/huggingface/text-embeddings-inference:86-1.8&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ada Lovelace (RTX 4000 series, ...)&lt;/td&gt; 
   &lt;td&gt;ghcr.io/huggingface/text-embeddings-inference:89-1.8&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Hopper (H100)&lt;/td&gt; 
   &lt;td&gt;ghcr.io/huggingface/text-embeddings-inference:hopper-1.8 (experimental)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;: Flash Attention is turned off by default for the Turing image as it suffers from precision issues. You can turn Flash Attention v1 ON by using the &lt;code&gt;USE_FLASH_ATTENTION=True&lt;/code&gt; environment variable.&lt;/p&gt; 
&lt;h3&gt;API documentation&lt;/h3&gt; 
&lt;p&gt;You can consult the OpenAPI documentation of the &lt;code&gt;text-embeddings-inference&lt;/code&gt; REST API using the &lt;code&gt;/docs&lt;/code&gt; route. The Swagger UI is also available at: &lt;a href="https://huggingface.github.io/text-embeddings-inference"&gt;https://huggingface.github.io/text-embeddings-inference&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Using a private or gated model&lt;/h3&gt; 
&lt;p&gt;You have the option to utilize the &lt;code&gt;HF_TOKEN&lt;/code&gt; environment variable for configuring the token employed by &lt;code&gt;text-embeddings-inference&lt;/code&gt;. This allows you to gain access to protected resources.&lt;/p&gt; 
&lt;p&gt;For example:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Go to &lt;a href="https://huggingface.co/settings/tokens"&gt;https://huggingface.co/settings/tokens&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Copy your cli READ token&lt;/li&gt; 
 &lt;li&gt;Export &lt;code&gt;HF_TOKEN=&amp;lt;your cli READ token&amp;gt;&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;or with Docker:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;model=&amp;lt;your private model&amp;gt;
volume=$PWD/data # share a volume with the Docker container to avoid downloading weights every run
token=&amp;lt;your cli READ token&amp;gt;

docker run --gpus all -e HF_TOKEN=$token -p 8080:80 -v $volume:/data --pull always ghcr.io/huggingface/text-embeddings-inference:1.8 --model-id $model
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Air gapped deployment&lt;/h3&gt; 
&lt;p&gt;To deploy Text Embeddings Inference in an air-gapped environment, first download the weights and then mount them inside the container using a volume.&lt;/p&gt; 
&lt;p&gt;For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# (Optional) create a `models` directory
mkdir models
cd models

# Make sure you have git-lfs installed (https://git-lfs.com)
git lfs install
git clone https://huggingface.co/Qwen/Qwen3-Embedding-0.6B

# Set the models directory as the volume path
volume=$PWD

# Mount the models directory inside the container with a volume and set the model ID
docker run --gpus all -p 8080:80 -v $volume:/data --pull always ghcr.io/huggingface/text-embeddings-inference:1.8 --model-id /data/Qwen3-Embedding-0.6B
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Using Re-rankers models&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;text-embeddings-inference&lt;/code&gt; v0.4.0 added support for CamemBERT, RoBERTa, XLM-RoBERTa, and GTE Sequence Classification models. Re-rankers models are Sequence Classification cross-encoders models with a single class that scores the similarity between a query and a text.&lt;/p&gt; 
&lt;p&gt;See &lt;a href="https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83"&gt;this blogpost&lt;/a&gt; by the LlamaIndex team to understand how you can use re-rankers models in your RAG pipeline to improve downstream performance.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;model=BAAI/bge-reranker-large
volume=$PWD/data # share a volume with the Docker container to avoid downloading weights every run

docker run --gpus all -p 8080:80 -v $volume:/data --pull always ghcr.io/huggingface/text-embeddings-inference:1.8 --model-id $model
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;And then you can rank the similarity between a query and a list of texts with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl 127.0.0.1:8080/rerank \
    -X POST \
    -d '{"query": "What is Deep Learning?", "texts": ["Deep Learning is not...", "Deep learning is..."]}' \
    -H 'Content-Type: application/json'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Using Sequence Classification models&lt;/h3&gt; 
&lt;p&gt;You can also use classic Sequence Classification models like &lt;code&gt;SamLowe/roberta-base-go_emotions&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;model=SamLowe/roberta-base-go_emotions
volume=$PWD/data # share a volume with the Docker container to avoid downloading weights every run

docker run --gpus all -p 8080:80 -v $volume:/data --pull always ghcr.io/huggingface/text-embeddings-inference:1.8 --model-id $model
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Once you have deployed the model you can use the &lt;code&gt;predict&lt;/code&gt; endpoint to get the emotions most associated with an input:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl 127.0.0.1:8080/predict \
    -X POST \
    -d '{"inputs":"I like you."}' \
    -H 'Content-Type: application/json'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Using SPLADE pooling&lt;/h3&gt; 
&lt;p&gt;You can choose to activate SPLADE pooling for Bert and Distilbert MaskedLM architectures:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;model=naver/efficient-splade-VI-BT-large-query
volume=$PWD/data # share a volume with the Docker container to avoid downloading weights every run

docker run --gpus all -p 8080:80 -v $volume:/data --pull always ghcr.io/huggingface/text-embeddings-inference:1.8 --model-id $model --pooling splade
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Once you have deployed the model you can use the &lt;code&gt;/embed_sparse&lt;/code&gt; endpoint to get the sparse embedding:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl 127.0.0.1:8080/embed_sparse \
    -X POST \
    -d '{"inputs":"I like you."}' \
    -H 'Content-Type: application/json'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Distributed Tracing&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;text-embeddings-inference&lt;/code&gt; is instrumented with distributed tracing using OpenTelemetry. You can use this feature by setting the address to an OTLP collector with the &lt;code&gt;--otlp-endpoint&lt;/code&gt; argument.&lt;/p&gt; 
&lt;h3&gt;gRPC&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;text-embeddings-inference&lt;/code&gt; offers a gRPC API as an alternative to the default HTTP API for high performance deployments. The API protobuf definition can be found &lt;a href="https://github.com/huggingface/text-embeddings-inference/raw/main/proto/tei.proto"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You can use the gRPC API by adding the &lt;code&gt;-grpc&lt;/code&gt; tag to any TEI Docker image. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;model=Qwen/Qwen3-Embedding-0.6B
volume=$PWD/data # share a volume with the Docker container to avoid downloading weights every run

docker run --gpus all -p 8080:80 -v $volume:/data --pull always ghcr.io/huggingface/text-embeddings-inference:1.8-grpc --model-id $model
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;grpcurl -d '{"inputs": "What is Deep Learning"}' -plaintext 0.0.0.0:8080 tei.v1.Embed/Embed
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Local install&lt;/h2&gt; 
&lt;h3&gt;CPU&lt;/h3&gt; 
&lt;p&gt;You can also opt to install &lt;code&gt;text-embeddings-inference&lt;/code&gt; locally.&lt;/p&gt; 
&lt;p&gt;First &lt;a href="https://rustup.rs/"&gt;install Rust&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# On x86 with ONNX backend (recommended)
cargo install --path router -F ort
# On x86 with Intel backend
cargo install --path router -F mkl
# On M1 or M2
cargo install --path router -F metal
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can now launch Text Embeddings Inference on CPU with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;model=Qwen/Qwen3-Embedding-0.6B

text-embeddings-router --model-id $model --port 8080
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; on some machines, you may also need the OpenSSL libraries and gcc. On Linux machines, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;sudo apt-get install libssl-dev gcc -y
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;CUDA&lt;/h3&gt; 
&lt;p&gt;GPUs with CUDA compute capabilities &amp;lt; 7.5 are not supported (V100, Titan V, GTX 1000 series, ...).&lt;/p&gt; 
&lt;p&gt;Make sure you have CUDA and the nvidia drivers installed. NVIDIA drivers on your device need to be compatible with CUDA version 12.2 or higher. You also need to add the nvidia binaries to your path:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;export PATH=$PATH:/usr/local/cuda/bin
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# This can take a while as we need to compile a lot of cuda kernels

# On Turing GPUs (T4, RTX 2000 series ... )
cargo install --path router -F candle-cuda-turing -F http --no-default-features

# On Ampere and Hopper
cargo install --path router -F candle-cuda -F http --no-default-features
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can now launch Text Embeddings Inference on GPU with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;model=Qwen/Qwen3-Embedding-0.6B

text-embeddings-router --model-id $model --port 8080
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Docker build&lt;/h2&gt; 
&lt;p&gt;You can build the CPU container with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;docker build .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To build the CUDA containers, you need to know the compute cap of the GPU you will be using at runtime.&lt;/p&gt; 
&lt;p&gt;Then you can build the container with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# Get submodule dependencies
git submodule update --init

# Example for Turing (T4, RTX 2000 series, ...)
runtime_compute_cap=75

# Example for A100
runtime_compute_cap=80

# Example for A10
runtime_compute_cap=86

# Example for Ada Lovelace (RTX 4000 series, ...)
runtime_compute_cap=89

# Example for H100
runtime_compute_cap=90

docker build . -f Dockerfile-cuda --build-arg CUDA_COMPUTE_CAP=$runtime_compute_cap
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Apple M1/M2 arm64 architectures&lt;/h3&gt; 
&lt;h4&gt;DISCLAIMER&lt;/h4&gt; 
&lt;p&gt;As explained here &lt;a href="https://github.com/pytorch/pytorch/issues/81224"&gt;MPS-Ready, ARM64 Docker Image&lt;/a&gt;, Metal / MPS is not supported via Docker. As such inference will be CPU bound and most likely pretty slow when using this docker image on an M1/M2 ARM CPU.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;docker build . -f Dockerfile --platform=linux/arm64
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/learn/cookbook/automatic_embedding_tei_inference_endpoints"&gt;Set up an Inference Endpoint with TEI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/plaggy/rag-containers"&gt;RAG containers with TEI&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>jdx/mise</title>
      <link>https://github.com/jdx/mise</link>
      <description>&lt;p&gt;dev tools, env vars, task runner&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h1&gt;&lt;a href="https://mise.jdx.dev"&gt; &lt;img src="https://github.com/jdx/mise/assets/216188/27a8ea18-9383-4d86-a445-305b9a6248c1" alt="mise-logo" width="400" /&gt;&lt;br /&gt; mise-en-place &lt;/a&gt;&lt;/h1&gt; 
 &lt;!-- &lt;a href="https://mise.jdx.dev"&gt;&lt;picture&gt; --&gt; 
 &lt;!--   &lt;source media="(prefers-color-scheme: dark)" width="617" srcset="./docs/logo-dark@2x.png"&gt; --&gt; 
 &lt;!--   &lt;img alt="mise logo" width="617" src="./docs/logo-light@2x.png"&gt; --&gt; 
 &lt;!-- &lt;/picture&gt;&lt;/a&gt; --&gt; 
 &lt;a href="https://crates.io/crates/mise"&gt;&lt;img alt="Crates.io" src="https://img.shields.io/crates/v/mise?style=for-the-badge" /&gt;&lt;/a&gt; 
 &lt;a href="https://github.com/jdx/mise/raw/main/LICENSE"&gt;&lt;img alt="GitHub" src="https://img.shields.io/github/license/jdx/mise?color=%2344CC11&amp;amp;style=for-the-badge" /&gt;&lt;/a&gt; 
 &lt;a href="https://github.com/jdx/mise/actions/workflows/test.yml"&gt;&lt;img alt="GitHub Workflow Status" src="https://img.shields.io/github/actions/workflow/status/jdx/mise/test.yml?style=for-the-badge" /&gt;&lt;/a&gt; 
 &lt;a href="https://discord.gg/mABnUDvP57"&gt;&lt;img alt="Discord" src="https://img.shields.io/discord/1066429325269794907?color=%23738ADB&amp;amp;style=for-the-badge" /&gt;&lt;/a&gt; 
 &lt;p&gt;&lt;em&gt;The front-end to your dev env.&lt;/em&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;What is it?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Like &lt;a href="https://asdf-vm.com"&gt;asdf&lt;/a&gt; (or &lt;a href="https://github.com/nvm-sh/nvm"&gt;nvm&lt;/a&gt; or &lt;a href="https://github.com/pyenv/pyenv"&gt;pyenv&lt;/a&gt; but for any language) it manages &lt;a href="https://mise.jdx.dev/dev-tools/"&gt;dev tools&lt;/a&gt; like node, python, cmake, terraform, and &lt;a href="https://mise.jdx.dev/registry.html"&gt;hundreds more&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Like &lt;a href="https://github.com/direnv/direnv"&gt;direnv&lt;/a&gt; it manages &lt;a href="https://mise.jdx.dev/environments/"&gt;environment variables&lt;/a&gt; for different project directories.&lt;/li&gt; 
 &lt;li&gt;Like &lt;a href="https://www.gnu.org/software/make/manual/make.html"&gt;make&lt;/a&gt; it manages &lt;a href="https://mise.jdx.dev/tasks/"&gt;tasks&lt;/a&gt; used to build and test projects.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Demo&lt;/h2&gt; 
&lt;p&gt;The following demo shows how to install and use &lt;code&gt;mise&lt;/code&gt; to manage multiple versions of &lt;code&gt;node&lt;/code&gt; on the same system. Note that calling &lt;code&gt;which node&lt;/code&gt; gives us a real path to node, not a shim.&lt;/p&gt; 
&lt;p&gt;It also shows that you can use &lt;code&gt;mise&lt;/code&gt; to install and many other tools such as &lt;code&gt;jq&lt;/code&gt;, &lt;code&gt;terraform&lt;/code&gt;, or &lt;code&gt;go&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://mise.jdx.dev/demo.html"&gt;&lt;img src="https://raw.githubusercontent.com/jdx/mise/main/docs/tapes/demo.gif" alt="demo" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;See &lt;a href="https://mise.jdx.dev/demo.html"&gt;demo transcript&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;h3&gt;Install mise&lt;/h3&gt; 
&lt;p&gt;See &lt;a href="https://mise.jdx.dev/getting-started.html"&gt;Getting started&lt;/a&gt; for more options.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh-session"&gt;$ curl https://mise.run | sh
$ ~/.local/bin/mise --version
2025.9.5 macos-arm64 (a1b2d3e 2025-09-06)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Hook mise into your shell (pick the right one for your shell):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh-session"&gt;# note this assumes mise is located at ~/.local/bin/mise
# which is what https://mise.run does by default
echo 'eval "$(~/.local/bin/mise activate bash)"' &amp;gt;&amp;gt; ~/.bashrc
echo 'eval "$(~/.local/bin/mise activate zsh)"' &amp;gt;&amp;gt; ~/.zshrc
echo '~/.local/bin/mise activate fish | source' &amp;gt;&amp;gt; ~/.config/fish/config.fish
echo '~/.local/bin/mise activate pwsh | Out-String | Invoke-Expression' &amp;gt;&amp;gt; ~/.config/powershell/Microsoft.PowerShell_profile.ps1
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Execute commands with specific tools&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh-session"&gt;$ mise exec node@22 -- node -v
mise node@22.x.x ✓ installed
v22.x.x
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Install tools&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh-session"&gt;$ mise use --global node@22 go@1
$ node -v
v22.x.x
$ go version
go version go1.x.x macos/arm64
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href="https://mise.jdx.dev/dev-tools/"&gt;dev tools&lt;/a&gt; for more examples.&lt;/p&gt; 
&lt;h3&gt;Manage environment variables&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-toml"&gt;# mise.toml
[env]
SOME_VAR = "foo"
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-sh-session"&gt;$ mise set SOME_VAR=bar
$ echo $SOME_VAR
bar
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note that &lt;code&gt;mise&lt;/code&gt; can also &lt;a href="https://mise.jdx.dev/environments/#env-directives"&gt;load &lt;code&gt;.env&lt;/code&gt; files&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Run tasks&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-toml"&gt;# mise.toml
[tasks.build]
description = "build the project"
run = "echo building..."
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-sh-session"&gt;$ mise run build
building...
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href="https://mise.jdx.dev/tasks/"&gt;tasks&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h3&gt;Example mise project&lt;/h3&gt; 
&lt;p&gt;Here is a combined example to give you an idea of how you can use mise to manage your a project's tools, environment, and tasks.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-toml"&gt;# mise.toml
[tools]
terraform = "1"
aws-cli = "2"

[env]
TF_WORKSPACE = "development"
AWS_REGION = "us-west-2"
AWS_PROFILE = "dev"

[tasks.plan]
description = "Run terraform plan with configured workspace"
run = """
terraform init
terraform workspace select $TF_WORKSPACE
terraform plan
"""

[tasks.validate]
description = "Validate AWS credentials and terraform config"
run = """
aws sts get-caller-identity
terraform validate
"""

[tasks.deploy]
description = "Deploy infrastructure after validation"
depends = ["validate", "plan"]
run = "terraform apply -auto-approve"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run it with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh-session"&gt;mise install # install tools specified in mise.toml
mise run deploy
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Find more examples in the &lt;a href="https://mise.jdx.dev/mise-cookbook/"&gt;mise cookbook&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Full Documentation&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://mise.jdx.dev"&gt;mise.jdx.dev&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Special Thanks&lt;/h2&gt; 
&lt;p&gt;We're grateful for Cloudflare's support through &lt;a href="https://www.cloudflare.com/lp/project-alexandria/"&gt;Project Alexandria&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributors&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/jdx/mise/graphs/contributors"&gt;&lt;img src="https://contrib.rocks/image?repo=jdx/mise" alt="Contributors" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>astral-sh/ruff</title>
      <link>https://github.com/astral-sh/ruff</link>
      <description>&lt;p&gt;An extremely fast Python linter and code formatter, written in Rust.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Ruff&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/astral-sh/ruff"&gt;&lt;img src="https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json" alt="Ruff" /&gt;&lt;/a&gt; &lt;a href="https://pypi.python.org/pypi/ruff"&gt;&lt;img src="https://img.shields.io/pypi/v/ruff.svg?sanitize=true" alt="image" /&gt;&lt;/a&gt; &lt;a href="https://github.com/astral-sh/ruff/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/pypi/l/ruff.svg?sanitize=true" alt="image" /&gt;&lt;/a&gt; &lt;a href="https://pypi.python.org/pypi/ruff"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/ruff.svg?sanitize=true" alt="image" /&gt;&lt;/a&gt; &lt;a href="https://github.com/astral-sh/ruff/actions"&gt;&lt;img src="https://github.com/astral-sh/ruff/workflows/CI/badge.svg?sanitize=true" alt="Actions status" /&gt;&lt;/a&gt; &lt;a href="https://discord.com/invite/astral-sh"&gt;&lt;img src="https://img.shields.io/badge/Discord-%235865F2.svg?logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://docs.astral.sh/ruff/"&gt;&lt;strong&gt;Docs&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://play.ruff.rs/"&gt;&lt;strong&gt;Playground&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;An extremely fast Python linter and code formatter, written in Rust.&lt;/p&gt; 
&lt;p align="center"&gt; 
 &lt;picture align="center"&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://user-images.githubusercontent.com/1309177/232603514-c95e9b0f-6b31-43de-9a80-9e844173fd6a.svg" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://user-images.githubusercontent.com/1309177/232603516-4fb4892d-585c-4b20-b810-3db9161831e4.svg" /&gt; 
  &lt;img alt="Shows a bar chart with benchmark results." src="https://user-images.githubusercontent.com/1309177/232603516-4fb4892d-585c-4b20-b810-3db9161831e4.svg?sanitize=true" /&gt; 
 &lt;/picture&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;i&gt;Linting the CPython codebase from scratch.&lt;/i&gt; &lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;⚡️ 10-100x faster than existing linters (like Flake8) and formatters (like Black)&lt;/li&gt; 
 &lt;li&gt;🐍 Installable via &lt;code&gt;pip&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;🛠️ &lt;code&gt;pyproject.toml&lt;/code&gt; support&lt;/li&gt; 
 &lt;li&gt;🤝 Python 3.13 compatibility&lt;/li&gt; 
 &lt;li&gt;⚖️ Drop-in parity with &lt;a href="https://docs.astral.sh/ruff/faq/#how-does-ruffs-linter-compare-to-flake8"&gt;Flake8&lt;/a&gt;, isort, and &lt;a href="https://docs.astral.sh/ruff/faq/#how-does-ruffs-formatter-compare-to-black"&gt;Black&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;📦 Built-in caching, to avoid re-analyzing unchanged files&lt;/li&gt; 
 &lt;li&gt;🔧 Fix support, for automatic error correction (e.g., automatically remove unused imports)&lt;/li&gt; 
 &lt;li&gt;📏 Over &lt;a href="https://docs.astral.sh/ruff/rules/"&gt;800 built-in rules&lt;/a&gt;, with native re-implementations of popular Flake8 plugins, like flake8-bugbear&lt;/li&gt; 
 &lt;li&gt;⌨️ First-party &lt;a href="https://docs.astral.sh/ruff/editors"&gt;editor integrations&lt;/a&gt; for &lt;a href="https://github.com/astral-sh/ruff-vscode"&gt;VS Code&lt;/a&gt; and &lt;a href="https://docs.astral.sh/ruff/editors/setup"&gt;more&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;🌎 Monorepo-friendly, with &lt;a href="https://docs.astral.sh/ruff/configuration/#config-file-discovery"&gt;hierarchical and cascading configuration&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Ruff aims to be orders of magnitude faster than alternative tools while integrating more functionality behind a single, common interface.&lt;/p&gt; 
&lt;p&gt;Ruff can be used to replace &lt;a href="https://pypi.org/project/flake8/"&gt;Flake8&lt;/a&gt; (plus dozens of plugins), &lt;a href="https://github.com/psf/black"&gt;Black&lt;/a&gt;, &lt;a href="https://pypi.org/project/isort/"&gt;isort&lt;/a&gt;, &lt;a href="https://pypi.org/project/pydocstyle/"&gt;pydocstyle&lt;/a&gt;, &lt;a href="https://pypi.org/project/pyupgrade/"&gt;pyupgrade&lt;/a&gt;, &lt;a href="https://pypi.org/project/autoflake/"&gt;autoflake&lt;/a&gt;, and more, all while executing tens or hundreds of times faster than any individual tool.&lt;/p&gt; 
&lt;p&gt;Ruff is extremely actively developed and used in major open-source projects like:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/apache/airflow"&gt;Apache Airflow&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/apache/superset"&gt;Apache Superset&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/tiangolo/fastapi"&gt;FastAPI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/huggingface/transformers"&gt;Hugging Face&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pandas-dev/pandas"&gt;Pandas&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/scipy/scipy"&gt;SciPy&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;...and &lt;a href="https://raw.githubusercontent.com/astral-sh/ruff/main/#whos-using-ruff"&gt;many more&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Ruff is backed by &lt;a href="https://astral.sh"&gt;Astral&lt;/a&gt;. Read the &lt;a href="https://astral.sh/blog/announcing-astral-the-company-behind-ruff"&gt;launch post&lt;/a&gt;, or the original &lt;a href="https://notes.crmarsh.com/python-tooling-could-be-much-much-faster"&gt;project announcement&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Testimonials&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://twitter.com/tiangolo/status/1591912354882764802"&gt;&lt;strong&gt;Sebastián Ramírez&lt;/strong&gt;&lt;/a&gt;, creator of &lt;a href="https://github.com/tiangolo/fastapi"&gt;FastAPI&lt;/a&gt;:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Ruff is so fast that sometimes I add an intentional bug in the code just to confirm it's actually running and checking the code.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;a href="https://twitter.com/schrockn/status/1612615862904827904"&gt;&lt;strong&gt;Nick Schrock&lt;/strong&gt;&lt;/a&gt;, founder of &lt;a href="https://www.elementl.com/"&gt;Elementl&lt;/a&gt;, co-creator of &lt;a href="https://graphql.org/"&gt;GraphQL&lt;/a&gt;:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Why is Ruff a gamechanger? Primarily because it is nearly 1000x faster. Literally. Not a typo. On our largest module (dagster itself, 250k LOC) pylint takes about 2.5 minutes, parallelized across 4 cores on my M1. Running ruff against our &lt;em&gt;entire&lt;/em&gt; codebase takes .4 seconds.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;a href="https://github.com/bokeh/bokeh/pull/12605"&gt;&lt;strong&gt;Bryan Van de Ven&lt;/strong&gt;&lt;/a&gt;, co-creator of &lt;a href="https://github.com/bokeh/bokeh/"&gt;Bokeh&lt;/a&gt;, original author of &lt;a href="https://docs.conda.io/en/latest/"&gt;Conda&lt;/a&gt;:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Ruff is ~150-200x faster than flake8 on my machine, scanning the whole repo takes ~0.2s instead of ~20s. This is an enormous quality of life improvement for local dev. It's fast enough that I added it as an actual commit hook, which is terrific.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;a href="https://twitter.com/timothycrosley/status/1606420868514877440"&gt;&lt;strong&gt;Timothy Crosley&lt;/strong&gt;&lt;/a&gt;, creator of &lt;a href="https://github.com/PyCQA/isort"&gt;isort&lt;/a&gt;:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Just switched my first project to Ruff. Only one downside so far: it's so fast I couldn't believe it was working till I intentionally introduced some errors.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;a href="https://github.com/astral-sh/ruff/issues/465#issuecomment-1317400028"&gt;&lt;strong&gt;Tim Abbott&lt;/strong&gt;&lt;/a&gt;, lead developer of &lt;a href="https://github.com/zulip/zulip"&gt;Zulip&lt;/a&gt;:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;This is just ridiculously fast... &lt;code&gt;ruff&lt;/code&gt; is amazing.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;!-- End section: Overview --&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;p&gt;For more, see the &lt;a href="https://docs.astral.sh/ruff/"&gt;documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/astral-sh/ruff/main/#getting-started"&gt;Getting Started&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/astral-sh/ruff/main/#configuration"&gt;Configuration&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/astral-sh/ruff/main/#rules"&gt;Rules&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/astral-sh/ruff/main/#contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/astral-sh/ruff/main/#support"&gt;Support&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/astral-sh/ruff/main/#acknowledgements"&gt;Acknowledgements&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/astral-sh/ruff/main/#whos-using-ruff"&gt;Who's Using Ruff?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/astral-sh/ruff/main/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Getting Started&lt;a id="getting-started"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;For more, see the &lt;a href="https://docs.astral.sh/ruff/"&gt;documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;p&gt;Ruff is available as &lt;a href="https://pypi.org/project/ruff/"&gt;&lt;code&gt;ruff&lt;/code&gt;&lt;/a&gt; on PyPI.&lt;/p&gt; 
&lt;p&gt;Invoke Ruff directly with &lt;a href="https://docs.astral.sh/uv/"&gt;&lt;code&gt;uvx&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;uvx ruff check   # Lint all files in the current directory.
uvx ruff format  # Format all files in the current directory.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or install Ruff with &lt;code&gt;uv&lt;/code&gt; (recommended), &lt;code&gt;pip&lt;/code&gt;, or &lt;code&gt;pipx&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# With uv.
uv tool install ruff@latest  # Install Ruff globally.
uv add --dev ruff            # Or add Ruff to your project.

# With pip.
pip install ruff

# With pipx.
pipx install ruff
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Starting with version &lt;code&gt;0.5.0&lt;/code&gt;, Ruff can be installed with our standalone installers:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# On macOS and Linux.
curl -LsSf https://astral.sh/ruff/install.sh | sh

# On Windows.
powershell -c "irm https://astral.sh/ruff/install.ps1 | iex"

# For a specific version.
curl -LsSf https://astral.sh/ruff/0.12.12/install.sh | sh
powershell -c "irm https://astral.sh/ruff/0.12.12/install.ps1 | iex"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also install Ruff via &lt;a href="https://formulae.brew.sh/formula/ruff"&gt;Homebrew&lt;/a&gt;, &lt;a href="https://anaconda.org/conda-forge/ruff"&gt;Conda&lt;/a&gt;, and with &lt;a href="https://docs.astral.sh/ruff/installation/"&gt;a variety of other package managers&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Usage&lt;/h3&gt; 
&lt;p&gt;To run Ruff as a linter, try any of the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;ruff check                          # Lint all files in the current directory (and any subdirectories).
ruff check path/to/code/            # Lint all files in `/path/to/code` (and any subdirectories).
ruff check path/to/code/*.py        # Lint all `.py` files in `/path/to/code`.
ruff check path/to/code/to/file.py  # Lint `file.py`.
ruff check @arguments.txt           # Lint using an input file, treating its contents as newline-delimited command-line arguments.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or, to run Ruff as a formatter:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;ruff format                          # Format all files in the current directory (and any subdirectories).
ruff format path/to/code/            # Format all files in `/path/to/code` (and any subdirectories).
ruff format path/to/code/*.py        # Format all `.py` files in `/path/to/code`.
ruff format path/to/code/to/file.py  # Format `file.py`.
ruff format @arguments.txt           # Format using an input file, treating its contents as newline-delimited command-line arguments.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Ruff can also be used as a &lt;a href="https://pre-commit.com/"&gt;pre-commit&lt;/a&gt; hook via &lt;a href="https://github.com/astral-sh/ruff-pre-commit"&gt;&lt;code&gt;ruff-pre-commit&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;- repo: https://github.com/astral-sh/ruff-pre-commit
  # Ruff version.
  rev: v0.12.12
  hooks:
    # Run the linter.
    - id: ruff-check
      args: [ --fix ]
    # Run the formatter.
    - id: ruff-format
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Ruff can also be used as a &lt;a href="https://github.com/astral-sh/ruff-vscode"&gt;VS Code extension&lt;/a&gt; or with &lt;a href="https://docs.astral.sh/ruff/editors/setup"&gt;various other editors&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Ruff can also be used as a &lt;a href="https://github.com/features/actions"&gt;GitHub Action&lt;/a&gt; via &lt;a href="https://github.com/astral-sh/ruff-action"&gt;&lt;code&gt;ruff-action&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;name: Ruff
on: [ push, pull_request ]
jobs:
  ruff:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: astral-sh/ruff-action@v3
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Configuration&lt;a id="configuration"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Ruff can be configured through a &lt;code&gt;pyproject.toml&lt;/code&gt;, &lt;code&gt;ruff.toml&lt;/code&gt;, or &lt;code&gt;.ruff.toml&lt;/code&gt; file (see: &lt;a href="https://docs.astral.sh/ruff/configuration/"&gt;&lt;em&gt;Configuration&lt;/em&gt;&lt;/a&gt;, or &lt;a href="https://docs.astral.sh/ruff/settings/"&gt;&lt;em&gt;Settings&lt;/em&gt;&lt;/a&gt; for a complete list of all configuration options).&lt;/p&gt; 
&lt;p&gt;If left unspecified, Ruff's default configuration is equivalent to the following &lt;code&gt;ruff.toml&lt;/code&gt; file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-toml"&gt;# Exclude a variety of commonly ignored directories.
exclude = [
    ".bzr",
    ".direnv",
    ".eggs",
    ".git",
    ".git-rewrite",
    ".hg",
    ".ipynb_checkpoints",
    ".mypy_cache",
    ".nox",
    ".pants.d",
    ".pyenv",
    ".pytest_cache",
    ".pytype",
    ".ruff_cache",
    ".svn",
    ".tox",
    ".venv",
    ".vscode",
    "__pypackages__",
    "_build",
    "buck-out",
    "build",
    "dist",
    "node_modules",
    "site-packages",
    "venv",
]

# Same as Black.
line-length = 88
indent-width = 4

# Assume Python 3.9
target-version = "py39"

[lint]
# Enable Pyflakes (`F`) and a subset of the pycodestyle (`E`) codes by default.
select = ["E4", "E7", "E9", "F"]
ignore = []

# Allow fix for all enabled rules (when `--fix`) is provided.
fixable = ["ALL"]
unfixable = []

# Allow unused variables when underscore-prefixed.
dummy-variable-rgx = "^(_+|(_+[a-zA-Z0-9_]*[a-zA-Z0-9]+?))$"

[format]
# Like Black, use double quotes for strings.
quote-style = "double"

# Like Black, indent with spaces, rather than tabs.
indent-style = "space"

# Like Black, respect magic trailing commas.
skip-magic-trailing-comma = false

# Like Black, automatically detect the appropriate line ending.
line-ending = "auto"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note that, in a &lt;code&gt;pyproject.toml&lt;/code&gt;, each section header should be prefixed with &lt;code&gt;tool.ruff&lt;/code&gt;. For example, &lt;code&gt;[lint]&lt;/code&gt; should be replaced with &lt;code&gt;[tool.ruff.lint]&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Some configuration options can be provided via dedicated command-line arguments, such as those related to rule enablement and disablement, file discovery, and logging level:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;ruff check --select F401 --select F403 --quiet
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The remaining configuration options can be provided through a catch-all &lt;code&gt;--config&lt;/code&gt; argument:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;ruff check --config "lint.per-file-ignores = {'some_file.py' = ['F841']}"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To opt in to the latest lint rules, formatter style changes, interface updates, and more, enable &lt;a href="https://docs.astral.sh/ruff/rules/"&gt;preview mode&lt;/a&gt; by setting &lt;code&gt;preview = true&lt;/code&gt; in your configuration file or passing &lt;code&gt;--preview&lt;/code&gt; on the command line. Preview mode enables a collection of unstable features that may change prior to stabilization.&lt;/p&gt; 
&lt;p&gt;See &lt;code&gt;ruff help&lt;/code&gt; for more on Ruff's top-level commands, or &lt;code&gt;ruff help check&lt;/code&gt; and &lt;code&gt;ruff help format&lt;/code&gt; for more on the linting and formatting commands, respectively.&lt;/p&gt; 
&lt;h2&gt;Rules&lt;a id="rules"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;!-- Begin section: Rules --&gt; 
&lt;p&gt;&lt;strong&gt;Ruff supports over 800 lint rules&lt;/strong&gt;, many of which are inspired by popular tools like Flake8, isort, pyupgrade, and others. Regardless of the rule's origin, Ruff re-implements every rule in Rust as a first-party feature.&lt;/p&gt; 
&lt;p&gt;By default, Ruff enables Flake8's &lt;code&gt;F&lt;/code&gt; rules, along with a subset of the &lt;code&gt;E&lt;/code&gt; rules, omitting any stylistic rules that overlap with the use of a formatter, like &lt;code&gt;ruff format&lt;/code&gt; or &lt;a href="https://github.com/psf/black"&gt;Black&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you're just getting started with Ruff, &lt;strong&gt;the default rule set is a great place to start&lt;/strong&gt;: it catches a wide variety of common errors (like unused imports) with zero configuration.&lt;/p&gt; 
&lt;!-- End section: Rules --&gt; 
&lt;p&gt;Beyond the defaults, Ruff re-implements some of the most popular Flake8 plugins and related code quality tools, including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/autoflake/"&gt;autoflake&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/eradicate/"&gt;eradicate&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/flake8-2020/"&gt;flake8-2020&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/flake8-annotations/"&gt;flake8-annotations&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/flake8-async"&gt;flake8-async&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/flake8-bandit/"&gt;flake8-bandit&lt;/a&gt; (&lt;a href="https://github.com/astral-sh/ruff/issues/1646"&gt;#1646&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/flake8-blind-except/"&gt;flake8-blind-except&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/flake8-boolean-trap/"&gt;flake8-boolean-trap&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/flake8-bugbear/"&gt;flake8-bugbear&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/flake8-builtins/"&gt;flake8-builtins&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/flake8-commas/"&gt;flake8-commas&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/flake8-comprehensions/"&gt;flake8-comprehensions&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/flake8-copyright/"&gt;flake8-copyright&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/flake8-datetimez/"&gt;flake8-datetimez&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/flake8-debugger/"&gt;flake8-debugger&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/flake8-django/"&gt;flake8-django&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/flake8-docstrings/"&gt;flake8-docstrings&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/flake8-eradicate/"&gt;flake8-eradicate&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/flake8-errmsg/"&gt;flake8-errmsg&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/flake8-executable/"&gt;flake8-executable&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/flake8-future-annotations/"&gt;flake8-future-annotations&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/flake8-gettext/"&gt;flake8-gettext&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/flake8-implicit-str-concat/"&gt;flake8-implicit-str-concat&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/joaopalmeiro/flake8-import-conventions"&gt;flake8-import-conventions&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/flake8-logging/"&gt;flake8-logging&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/flake8-logging-format/"&gt;flake8-logging-format&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/flake8-no-pep420"&gt;flake8-no-pep420&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/flake8-pie/"&gt;flake8-pie&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/flake8-print/"&gt;flake8-print&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/flake8-pyi/"&gt;flake8-pyi&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/flake8-pytest-style/"&gt;flake8-pytest-style&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/flake8-quotes/"&gt;flake8-quotes&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/flake8-raise/"&gt;flake8-raise&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/flake8-return/"&gt;flake8-return&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/flake8-self/"&gt;flake8-self&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/flake8-simplify/"&gt;flake8-simplify&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/flake8-slots/"&gt;flake8-slots&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/flake8-super/"&gt;flake8-super&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/flake8-tidy-imports/"&gt;flake8-tidy-imports&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/flake8-todos/"&gt;flake8-todos&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/flake8-type-checking/"&gt;flake8-type-checking&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/flake8-use-pathlib/"&gt;flake8-use-pathlib&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/flynt/"&gt;flynt&lt;/a&gt; (&lt;a href="https://github.com/astral-sh/ruff/issues/2102"&gt;#2102&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/isort/"&gt;isort&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/mccabe/"&gt;mccabe&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/pandas-vet/"&gt;pandas-vet&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/pep8-naming/"&gt;pep8-naming&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/pydocstyle/"&gt;pydocstyle&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pre-commit/pygrep-hooks"&gt;pygrep-hooks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/pylint-airflow/"&gt;pylint-airflow&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/pyupgrade/"&gt;pyupgrade&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/tryceratops/"&gt;tryceratops&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/yesqa/"&gt;yesqa&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For a complete enumeration of the supported rules, see &lt;a href="https://docs.astral.sh/ruff/rules/"&gt;&lt;em&gt;Rules&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;a id="contributing"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;Contributions are welcome and highly appreciated. To get started, check out the &lt;a href="https://docs.astral.sh/ruff/contributing/"&gt;&lt;strong&gt;contributing guidelines&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You can also join us on &lt;a href="https://discord.com/invite/astral-sh"&gt;&lt;strong&gt;Discord&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Support&lt;a id="support"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;Having trouble? Check out the existing issues on &lt;a href="https://github.com/astral-sh/ruff/issues"&gt;&lt;strong&gt;GitHub&lt;/strong&gt;&lt;/a&gt;, or feel free to &lt;a href="https://github.com/astral-sh/ruff/issues/new"&gt;&lt;strong&gt;open a new one&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You can also ask for help on &lt;a href="https://discord.com/invite/astral-sh"&gt;&lt;strong&gt;Discord&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Acknowledgements&lt;a id="acknowledgements"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;Ruff's linter draws on both the APIs and implementation details of many other tools in the Python ecosystem, especially &lt;a href="https://github.com/PyCQA/flake8"&gt;Flake8&lt;/a&gt;, &lt;a href="https://github.com/PyCQA/pyflakes"&gt;Pyflakes&lt;/a&gt;, &lt;a href="https://github.com/PyCQA/pycodestyle"&gt;pycodestyle&lt;/a&gt;, &lt;a href="https://github.com/PyCQA/pydocstyle"&gt;pydocstyle&lt;/a&gt;, &lt;a href="https://github.com/asottile/pyupgrade"&gt;pyupgrade&lt;/a&gt;, and &lt;a href="https://github.com/PyCQA/isort"&gt;isort&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;In some cases, Ruff includes a "direct" Rust port of the corresponding tool. We're grateful to the maintainers of these tools for their work, and for all the value they've provided to the Python community.&lt;/p&gt; 
&lt;p&gt;Ruff's formatter is built on a fork of Rome's &lt;a href="https://github.com/rome/tools/tree/main/crates/rome_formatter"&gt;&lt;code&gt;rome_formatter&lt;/code&gt;&lt;/a&gt;, and again draws on both API and implementation details from &lt;a href="https://github.com/rome/tools"&gt;Rome&lt;/a&gt;, &lt;a href="https://github.com/prettier/prettier"&gt;Prettier&lt;/a&gt;, and &lt;a href="https://github.com/psf/black"&gt;Black&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Ruff's import resolver is based on the import resolution algorithm from &lt;a href="https://github.com/microsoft/pyright"&gt;Pyright&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Ruff is also influenced by a number of tools outside the Python ecosystem, like &lt;a href="https://github.com/rust-lang/rust-clippy"&gt;Clippy&lt;/a&gt; and &lt;a href="https://github.com/eslint/eslint"&gt;ESLint&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Ruff is the beneficiary of a large number of &lt;a href="https://github.com/astral-sh/ruff/graphs/contributors"&gt;contributors&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Ruff is released under the MIT license.&lt;/p&gt; 
&lt;h2&gt;Who's Using Ruff?&lt;a id="whos-using-ruff"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;Ruff is used by a number of major open-source projects and companies, including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/albumentations-team/albumentations"&gt;Albumentations&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Amazon (&lt;a href="https://github.com/aws/serverless-application-model"&gt;AWS SAM&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://apps.ankiweb.net/"&gt;Anki&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Anthropic (&lt;a href="https://github.com/anthropics/anthropic-sdk-python"&gt;Python SDK&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/apache/airflow"&gt;Apache Airflow&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;AstraZeneca (&lt;a href="https://github.com/AstraZeneca/magnus-core"&gt;Magnus&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/python-babel/babel"&gt;Babel&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Benchling (&lt;a href="https://github.com/benchling/refac"&gt;Refac&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/bokeh/bokeh"&gt;Bokeh&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Capital One (&lt;a href="https://github.com/capitalone/datacompy"&gt;datacompy&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;CrowdCent (&lt;a href="https://github.com/crowdcent/numerblox"&gt;NumerBlox&lt;/a&gt;) 
  &lt;!-- typos: ignore --&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pyca/cryptography"&gt;Cryptography (PyCA)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;CERN (&lt;a href="https://getindico.io/"&gt;Indico&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/iterative/dvc"&gt;DVC&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/dagger/dagger"&gt;Dagger&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/dagster-io/dagster"&gt;Dagster&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Databricks (&lt;a href="https://github.com/mlflow/mlflow"&gt;MLflow&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/langgenius/dify"&gt;Dify&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/tiangolo/fastapi"&gt;FastAPI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/godotengine/godot"&gt;Godot&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/gradio-app/gradio"&gt;Gradio&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/great-expectations/great_expectations"&gt;Great Expectations&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/encode/httpx"&gt;HTTPX&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pypa/hatch"&gt;Hatch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/home-assistant/core"&gt;Home Assistant&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Hugging Face (&lt;a href="https://github.com/huggingface/transformers"&gt;Transformers&lt;/a&gt;, &lt;a href="https://github.com/huggingface/datasets"&gt;Datasets&lt;/a&gt;, &lt;a href="https://github.com/huggingface/diffusers"&gt;Diffusers&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;IBM (&lt;a href="https://github.com/Qiskit/qiskit"&gt;Qiskit&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;ING Bank (&lt;a href="https://github.com/ing-bank/popmon"&gt;popmon&lt;/a&gt;, &lt;a href="https://github.com/ing-bank/probatus"&gt;probatus&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ibis-project/ibis"&gt;Ibis&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/unifyai/ivy"&gt;ivy&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/jax-ml/jax"&gt;JAX&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/jupyter-server/jupyter_server"&gt;Jupyter&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://kraken.tech/"&gt;Kraken Tech&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hwchase17/langchain"&gt;LangChain&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://litestar.dev/"&gt;Litestar&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/jerryjliu/llama_index"&gt;LlamaIndex&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Matrix (&lt;a href="https://github.com/matrix-org/synapse"&gt;Synapse&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/oxsecurity/megalinter"&gt;MegaLinter&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Meltano (&lt;a href="https://github.com/meltano/meltano"&gt;Meltano CLI&lt;/a&gt;, &lt;a href="https://github.com/meltano/sdk"&gt;Singer SDK&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;Microsoft (&lt;a href="https://github.com/microsoft/semantic-kernel"&gt;Semantic Kernel&lt;/a&gt;, &lt;a href="https://github.com/microsoft/onnxruntime"&gt;ONNX Runtime&lt;/a&gt;, &lt;a href="https://github.com/microsoft/LightGBM"&gt;LightGBM&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;Modern Treasury (&lt;a href="https://github.com/Modern-Treasury/modern-treasury-python"&gt;Python SDK&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;Mozilla (&lt;a href="https://github.com/mozilla/gecko-dev"&gt;Firefox&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/python/mypy"&gt;Mypy&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/nautobot/nautobot"&gt;Nautobot&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Netflix (&lt;a href="https://github.com/Netflix/dispatch"&gt;Dispatch&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/neondatabase/neon"&gt;Neon&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nokia.com/"&gt;Nokia&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/nonebot/nonebot2"&gt;NoneBot&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pyro-ppl/numpyro"&gt;NumPyro&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/onnx/onnx"&gt;ONNX&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenBB-finance/OpenBBTerminal"&gt;OpenBB&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Open-Wine-Components/umu-launcher"&gt;Open Wine Components&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pdm-project/pdm"&gt;PDM&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/PaddlePaddle/Paddle"&gt;PaddlePaddle&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pandas-dev/pandas"&gt;Pandas&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/python-pillow/Pillow"&gt;Pillow&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/python-poetry/poetry"&gt;Poetry&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pola-rs/polars"&gt;Polars&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/PostHog/posthog"&gt;PostHog&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Prefect (&lt;a href="https://github.com/PrefectHQ/prefect"&gt;Python SDK&lt;/a&gt;, &lt;a href="https://github.com/PrefectHQ/marvin"&gt;Marvin&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pyinstaller/pyinstaller"&gt;PyInstaller&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pymc-devs/pymc/"&gt;PyMC&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pymc-labs/pymc-marketing"&gt;PyMC-Marketing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pytest-dev/pytest"&gt;pytest&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pytorch/pytorch"&gt;PyTorch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pydantic/pydantic"&gt;Pydantic&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/PyCQA/pylint"&gt;Pylint&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pyvista/pyvista"&gt;PyVista&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/reflex-dev/reflex"&gt;Reflex&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/online-ml/river"&gt;River&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://rippling.com"&gt;Rippling&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/sansyrox/robyn"&gt;Robyn&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/saleor/saleor"&gt;Saleor&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Scale AI (&lt;a href="https://github.com/scaleapi/launch-python-client"&gt;Launch SDK&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/scipy/scipy"&gt;SciPy&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Snowflake (&lt;a href="https://github.com/Snowflake-Labs/snowcli"&gt;SnowCLI&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/sphinx-doc/sphinx"&gt;Sphinx&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/DLR-RM/stable-baselines3"&gt;Stable Baselines3&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/encode/starlette"&gt;Starlette&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/streamlit/streamlit"&gt;Streamlit&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/TheAlgorithms/Python"&gt;The Algorithms&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/altair-viz/altair"&gt;Vega-Altair&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://weblate.org/"&gt;Weblate&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;WordPress (&lt;a href="https://github.com/WordPress/openverse"&gt;Openverse&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/zenml-io/zenml"&gt;ZenML&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/zulip/zulip"&gt;Zulip&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pypa/build"&gt;build (PyPA)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pypa/cibuildwheel"&gt;cibuildwheel (PyPA)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/delta-io/delta-rs"&gt;delta-rs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/alteryx/featuretools"&gt;featuretools&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mesonbuild/meson-python"&gt;meson-python&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/wntrblm/nox"&gt;nox&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pypa/pip"&gt;pip&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Show Your Support&lt;/h3&gt; 
&lt;p&gt;If you're using Ruff, consider adding the Ruff badge to your project's &lt;code&gt;README.md&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-md"&gt;[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;...or &lt;code&gt;README.rst&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-rst"&gt;.. image:: https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json
    :target: https://github.com/astral-sh/ruff
    :alt: Ruff
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;...or, as HTML:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-html"&gt;&amp;lt;a href="https://github.com/astral-sh/ruff"&amp;gt;&amp;lt;img src="https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json" alt="Ruff" style="max-width:100%;"&amp;gt;&amp;lt;/a&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;License&lt;a id="license"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;This repository is licensed under the &lt;a href="https://github.com/astral-sh/ruff/raw/main/LICENSE"&gt;MIT License&lt;/a&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a target="_blank" href="https://astral.sh" style="background:none"&gt; &lt;img src="https://raw.githubusercontent.com/astral-sh/ruff/main/assets/svg/Astral.svg?sanitize=true" alt="Made by Astral" /&gt; &lt;/a&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>microsoft/edit</title>
      <link>https://github.com/microsoft/edit</link>
      <description>&lt;p&gt;We all edit.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;img src="https://raw.githubusercontent.com/microsoft/edit/main/assets/edit.svg?sanitize=true" alt="Application Icon for Edit" /&gt; Edit&lt;/h1&gt; 
&lt;p&gt;A simple editor for simple needs.&lt;/p&gt; 
&lt;p&gt;This editor pays homage to the classic &lt;a href="https://en.wikipedia.org/wiki/MS-DOS_Editor"&gt;MS-DOS Editor&lt;/a&gt;, but with a modern interface and input controls similar to VS Code. The goal is to provide an accessible editor that even users largely unfamiliar with terminals can easily use.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/microsoft/edit/main/assets/edit_hero_image.png" alt="Screenshot of Edit with the About dialog in the foreground" /&gt;&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://repology.org/project/microsoft-edit/versions"&gt;&lt;img src="https://repology.org/badge/vertical-allrepos/microsoft-edit.svg?exclude_unsupported=1" alt="Packaging status" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;You can also download binaries from &lt;a href="https://github.com/microsoft/edit/releases/latest"&gt;our Releases page&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Windows&lt;/h3&gt; 
&lt;p&gt;You can install the latest version with WinGet:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;winget install Microsoft.Edit
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Build Instructions&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.rust-lang.org/tools/install"&gt;Install Rust&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Install the nightly toolchain: &lt;code&gt;rustup install nightly&lt;/code&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Alternatively, set the environment variable &lt;code&gt;RUSTC_BOOTSTRAP=1&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Clone the repository&lt;/li&gt; 
 &lt;li&gt;For a release build, run: &lt;code&gt;cargo build --config .cargo/release.toml --release&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Build Configuration&lt;/h3&gt; 
&lt;p&gt;During compilation you can set various environment variables to configure the build. The following table lists the available configuration options:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Environment variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;EDIT_CFG_ICU*&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;See &lt;a href="https://raw.githubusercontent.com/microsoft/edit/main/#icu-library-name-soname"&gt;ICU library name (SONAME)&lt;/a&gt; for details.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;EDIT_CFG_LANGUAGES&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;A comma-separated list of languages to include in the build. See &lt;a href="https://raw.githubusercontent.com/microsoft/edit/main/i18n/edit.toml"&gt;i18n/edit.toml&lt;/a&gt; for available languages.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Notes to Package Maintainers&lt;/h2&gt; 
&lt;h3&gt;Package Naming&lt;/h3&gt; 
&lt;p&gt;The canonical executable name is "edit" and the alternative name is "msedit". We're aware of the potential conflict of "edit" with existing commands and recommend alternatively naming packages and executables "msedit". Names such as "ms-edit" should be avoided. Assigning an "edit" alias is recommended, if possible.&lt;/p&gt; 
&lt;h3&gt;ICU library name (SONAME)&lt;/h3&gt; 
&lt;p&gt;This project &lt;em&gt;optionally&lt;/em&gt; depends on the ICU library for its Search and Replace functionality. By default, the project will look for a SONAME without version suffix:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Windows: &lt;code&gt;icuuc.dll&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;macOS: &lt;code&gt;libicuuc.dylib&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;UNIX, and other OS: &lt;code&gt;libicuuc.so&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If your installation uses a different SONAME, please set the following environment variable at build time:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;EDIT_CFG_ICUUC_SONAME&lt;/code&gt;: For instance, &lt;code&gt;libicuuc.so.76&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;EDIT_CFG_ICUI18N_SONAME&lt;/code&gt;: For instance, &lt;code&gt;libicui18n.so.76&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Additionally, this project assumes that the ICU exports are exported without &lt;code&gt;_&lt;/code&gt; prefix and without version suffix, such as &lt;code&gt;u_errorName&lt;/code&gt;. If your installation uses versioned exports, please set:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;EDIT_CFG_ICU_CPP_EXPORTS&lt;/code&gt;: If set to &lt;code&gt;true&lt;/code&gt;, it'll look for C++ symbols such as &lt;code&gt;_u_errorName&lt;/code&gt;. Enabled by default on macOS.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;EDIT_CFG_ICU_RENAMING_VERSION&lt;/code&gt;: If set to a version number, such as &lt;code&gt;76&lt;/code&gt;, it'll look for symbols such as &lt;code&gt;u_errorName_76&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Finally, you can set the following environment variables:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;EDIT_CFG_ICU_RENAMING_AUTO_DETECT&lt;/code&gt;: If set to &lt;code&gt;true&lt;/code&gt;, the executable will try to detect the &lt;code&gt;EDIT_CFG_ICU_RENAMING_VERSION&lt;/code&gt; value at runtime. The way it does this is not officially supported by ICU and as such is not recommended to be relied upon. Enabled by default on UNIX (excluding macOS) if no other options are set.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To test your settings, run &lt;code&gt;cargo test&lt;/code&gt; again but with the &lt;code&gt;--ignored&lt;/code&gt; flag. For instance:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;cargo test -- --ignored
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>nautechsystems/nautilus_trader</title>
      <link>https://github.com/nautechsystems/nautilus_trader</link>
      <description>&lt;p&gt;A high-performance algorithmic trading platform and event-driven backtester&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;img src="https://github.com/nautechsystems/nautilus_trader/raw/develop/assets/nautilus-trader-logo.png" width="500" /&gt;&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://codecov.io/gh/nautechsystems/nautilus_trader"&gt;&lt;img src="https://codecov.io/gh/nautechsystems/nautilus_trader/branch/master/graph/badge.svg?token=DXO9QQI40H" alt="codecov" /&gt;&lt;/a&gt; &lt;a href="https://codspeed.io/nautechsystems/nautilus_trader"&gt;&lt;img src="https://img.shields.io/endpoint?url=https://codspeed.io/badge.json" alt="codspeed" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/pypi/pyversions/nautilus_trader" alt="pythons" /&gt; &lt;img src="https://img.shields.io/pypi/v/nautilus_trader" alt="pypi-version" /&gt; &lt;img src="https://img.shields.io/pypi/format/nautilus_trader?color=blue" alt="pypi-format" /&gt; &lt;a href="https://pepy.tech/project/nautilus-trader"&gt;&lt;img src="https://pepy.tech/badge/nautilus-trader" alt="Downloads" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/NautilusTrader"&gt;&lt;img src="https://img.shields.io/badge/Discord-%235865F2.svg?logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Branch&lt;/th&gt; 
   &lt;th align="left"&gt;Version&lt;/th&gt; 
   &lt;th align="left"&gt;Status&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;master&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://packages.nautechsystems.io/simple/nautilus-trader/index.html"&gt;&lt;img src="https://img.shields.io/endpoint?url=https%3A%2F%2Fraw.githubusercontent.com%2Fnautechsystems%2Fnautilus_trader%2Fmaster%2Fversion.json" alt="version" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml"&gt;&lt;img src="https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml/badge.svg?branch=nightly" alt="build" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;nightly&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://packages.nautechsystems.io/simple/nautilus-trader/index.html"&gt;&lt;img src="https://img.shields.io/endpoint?url=https%3A%2F%2Fraw.githubusercontent.com%2Fnautechsystems%2Fnautilus_trader%2Fnightly%2Fversion.json" alt="version" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml"&gt;&lt;img src="https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml/badge.svg?branch=nightly" alt="build" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;develop&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://packages.nautechsystems.io/simple/nautilus-trader/index.html"&gt;&lt;img src="https://img.shields.io/endpoint?url=https%3A%2F%2Fraw.githubusercontent.com%2Fnautechsystems%2Fnautilus_trader%2Fdevelop%2Fversion.json" alt="version" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml"&gt;&lt;img src="https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml/badge.svg?branch=develop" alt="build" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Platform&lt;/th&gt; 
   &lt;th align="left"&gt;Rust&lt;/th&gt; 
   &lt;th align="left"&gt;Python&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;Linux (x86_64)&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;1.89.0&lt;/td&gt; 
   &lt;td align="left"&gt;3.11-3.13&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;Linux (ARM64)&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;1.89.0&lt;/td&gt; 
   &lt;td align="left"&gt;3.11-3.13&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;macOS (ARM64)&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;1.89.0&lt;/td&gt; 
   &lt;td align="left"&gt;3.11-3.13&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;Windows (x86_64)&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;1.89.0&lt;/td&gt; 
   &lt;td align="left"&gt;3.11-3.13&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Docs&lt;/strong&gt;: &lt;a href="https://nautilustrader.io/docs/"&gt;https://nautilustrader.io/docs/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Website&lt;/strong&gt;: &lt;a href="https://nautilustrader.io"&gt;https://nautilustrader.io&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Support&lt;/strong&gt;: &lt;a href="mailto:support@nautilustrader.io"&gt;support@nautilustrader.io&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;NautilusTrader is an open-source, high-performance, production-grade algorithmic trading platform, providing quantitative traders with the ability to backtest portfolios of automated trading strategies on historical data with an event-driven engine, and also deploy those same strategies live, with no code changes.&lt;/p&gt; 
&lt;p&gt;The platform is &lt;em&gt;AI-first&lt;/em&gt;, designed to develop and deploy algorithmic trading strategies within a highly performant and robust Python-native environment. This helps to address the parity challenge of keeping the Python research/backtest environment consistent with the production live trading environment.&lt;/p&gt; 
&lt;p&gt;NautilusTrader's design, architecture, and implementation philosophy prioritizes software correctness and safety at the highest level, with the aim of supporting Python-native, mission-critical, trading system backtesting and live deployment workloads.&lt;/p&gt; 
&lt;p&gt;The platform is also universal, and asset-class-agnostic — with any REST API or WebSocket feed able to be integrated via modular adapters. It supports high-frequency trading across a wide range of asset classes and instrument types including FX, Equities, Futures, Options, Crypto, DeFi, and Betting, enabling seamless operations across multiple venues simultaneously.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/nautechsystems/nautilus_trader/raw/develop/assets/nautilus-trader.png" alt="nautilus-trader" title="nautilus-trader" /&gt;&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Fast&lt;/strong&gt;: Core is written in Rust with asynchronous networking using &lt;a href="https://crates.io/crates/tokio"&gt;tokio&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Reliable&lt;/strong&gt;: Rust-powered type- and thread-safety, with optional Redis-backed state persistence.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Portable&lt;/strong&gt;: OS independent, runs on Linux, macOS, and Windows. Deploy using Docker.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible&lt;/strong&gt;: Modular adapters mean any REST API or WebSocket feed can be integrated.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Advanced&lt;/strong&gt;: Time in force &lt;code&gt;IOC&lt;/code&gt;, &lt;code&gt;FOK&lt;/code&gt;, &lt;code&gt;GTC&lt;/code&gt;, &lt;code&gt;GTD&lt;/code&gt;, &lt;code&gt;DAY&lt;/code&gt;, &lt;code&gt;AT_THE_OPEN&lt;/code&gt;, &lt;code&gt;AT_THE_CLOSE&lt;/code&gt;, advanced order types and conditional triggers. Execution instructions &lt;code&gt;post-only&lt;/code&gt;, &lt;code&gt;reduce-only&lt;/code&gt;, and icebergs. Contingency orders including &lt;code&gt;OCO&lt;/code&gt;, &lt;code&gt;OUO&lt;/code&gt;, &lt;code&gt;OTO&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Customizable&lt;/strong&gt;: Add user-defined custom components, or assemble entire systems from scratch leveraging the &lt;a href="https://nautilustrader.io/docs/latest/concepts/cache"&gt;cache&lt;/a&gt; and &lt;a href="https://nautilustrader.io/docs/latest/concepts/message_bus"&gt;message bus&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Backtesting&lt;/strong&gt;: Run with multiple venues, instruments and strategies simultaneously using historical quote tick, trade tick, bar, order book and custom data with nanosecond resolution.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Live&lt;/strong&gt;: Use identical strategy implementations between backtesting and live deployments.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-venue&lt;/strong&gt;: Multiple venue capabilities facilitate market-making and statistical arbitrage strategies.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;AI Training&lt;/strong&gt;: Backtest engine fast enough to be used to train AI trading agents (RL/ES).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://github.com/nautechsystems/nautilus_trader/raw/develop/assets/nautilus-art.png" alt="Alt text" title="nautilus" /&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;nautilus - from ancient Greek 'sailor' and naus 'ship'.&lt;/em&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;The nautilus shell consists of modular chambers with a growth factor which approximates a logarithmic spiral. The idea is that this can be translated to the aesthetics of design and architecture.&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Why NautilusTrader?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Highly performant event-driven Python&lt;/strong&gt;: Native binary core components.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Parity between backtesting and live trading&lt;/strong&gt;: Identical strategy code.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Reduced operational risk&lt;/strong&gt;: Enhanced risk management functionality, logical accuracy, and type safety.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Highly extendable&lt;/strong&gt;: Message bus, custom components and actors, custom data, custom adapters.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Traditionally, trading strategy research and backtesting might be conducted in Python using vectorized methods, with the strategy then needing to be reimplemented in a more event-driven way using C++, C#, Java or other statically typed language(s). The reasoning here is that vectorized backtesting code cannot express the granular time and event dependent complexity of real-time trading, where compiled languages have proven to be more suitable due to their inherently higher performance, and type safety.&lt;/p&gt; 
&lt;p&gt;One of the key advantages of NautilusTrader here, is that this reimplementation step is now circumvented - as the critical core components of the platform have all been written entirely in &lt;a href="https://www.rust-lang.org/"&gt;Rust&lt;/a&gt; or &lt;a href="https://cython.org/"&gt;Cython&lt;/a&gt;. This means we're using the right tools for the job, where systems programming languages compile performant binaries, with CPython C extension modules then able to offer a Python-native environment, suitable for professional quantitative traders and trading firms.&lt;/p&gt; 
&lt;h2&gt;Why Python?&lt;/h2&gt; 
&lt;p&gt;Python was originally created decades ago as a simple scripting language with a clean straightforward syntax. It has since evolved into a fully fledged general purpose object-oriented programming language. Based on the TIOBE index, Python is currently the most popular programming language in the world. Not only that, Python has become the &lt;em&gt;de facto lingua franca&lt;/em&gt; of data science, machine learning, and artificial intelligence.&lt;/p&gt; 
&lt;p&gt;developer/user communities. However, Python has performance and typing limitations for large-scale, latency-sensitive systems. Cython addresses many of these issues by introducing static typing into Python's rich ecosystem of libraries and communities.&lt;/p&gt; 
&lt;h2&gt;Why Rust?&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.rust-lang.org/"&gt;Rust&lt;/a&gt; is a multi-paradigm programming language designed for performance and safety, especially safe concurrency. Rust is "blazingly fast" and memory-efficient (comparable to C and C++) with no garbage collector. It can power mission-critical systems, run on embedded devices, and easily integrates with other languages.&lt;/p&gt; 
&lt;p&gt;Rust’s rich type system and ownership model guarantees memory-safety and thread-safety deterministically — eliminating many classes of bugs at compile-time.&lt;/p&gt; 
&lt;p&gt;The project increasingly utilizes Rust for core performance-critical components. Python bindings are implemented via Cython and &lt;a href="https://pyo3.rs"&gt;PyO3&lt;/a&gt;—no Rust toolchain is required at install time.&lt;/p&gt; 
&lt;p&gt;This project makes the &lt;a href="https://raphlinus.github.io/rust/2020/01/18/soundness-pledge.html"&gt;Soundness Pledge&lt;/a&gt;:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;“The intent of this project is to be free of soundness bugs. The developers will do their best to avoid them, and welcome help in analyzing and fixing them.”&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;MSRV:&lt;/strong&gt; NautilusTrader relies heavily on improvements in the Rust language and compiler. As a result, the Minimum Supported Rust Version (MSRV) is generally equal to the latest stable release of Rust.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Integrations&lt;/h2&gt; 
&lt;p&gt;NautilusTrader is modularly designed to work with &lt;em&gt;adapters&lt;/em&gt;, enabling connectivity to trading venues and data providers by translating their raw APIs into a unified interface and normalized domain model.&lt;/p&gt; 
&lt;p&gt;The following integrations are currently supported; see &lt;a href="https://nautilustrader.io/docs/latest/integrations/"&gt;docs/integrations/&lt;/a&gt; for details:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Name&lt;/th&gt; 
   &lt;th align="left"&gt;ID&lt;/th&gt; 
   &lt;th align="left"&gt;Type&lt;/th&gt; 
   &lt;th align="left"&gt;Status&lt;/th&gt; 
   &lt;th align="left"&gt;Docs&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://betfair.com"&gt;Betfair&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;BETFAIR&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Sports Betting Exchange&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/betfair.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://binance.com"&gt;Binance&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;BINANCE&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Exchange (CEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/binance.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://binance.us"&gt;Binance US&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;BINANCE&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Exchange (CEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/binance.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://www.binance.com/en/futures"&gt;Binance Futures&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;BINANCE&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Exchange (CEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/binance.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://www.bitmex.com"&gt;BitMEX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;BITMEX&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Exchange (CEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/building-orange" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/bitmex.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://www.bybit.com"&gt;Bybit&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;BYBIT&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Exchange (CEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/bybit.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://www.coinbase.com/en/international-exchange"&gt;Coinbase International&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;COINBASE_INTX&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Exchange (CEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/coinbase_intx.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://databento.com"&gt;Databento&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;DATABENTO&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Data Provider&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/databento.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://dydx.exchange/"&gt;dYdX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;DYDX&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Exchange (DEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/dydx.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://hyperliquid.xyz"&gt;Hyperliquid&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;HYPERLIQUID&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Exchange (DEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/building-orange" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/hyperliquid.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://www.interactivebrokers.com"&gt;Interactive Brokers&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;INTERACTIVE_BROKERS&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Brokerage (multi-venue)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/ib.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://okx.com"&gt;OKX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;OKX&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Exchange (CEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/beta-yellow" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/okx.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://polymarket.com"&gt;Polymarket&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;POLYMARKET&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Prediction Market (DEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/polymarket.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://tardis.dev"&gt;Tardis&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;TARDIS&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Data Provider&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/tardis.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ID&lt;/strong&gt;: The default client ID for the integrations adapter clients.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Type&lt;/strong&gt;: The type of integration (often the venue type).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Status&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;building&lt;/code&gt;: Under construction and likely not in a usable state.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;beta&lt;/code&gt;: Completed to a minimally working state and in a beta testing phase.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;stable&lt;/code&gt;: Stabilized feature set and API, the integration has been tested by both developers and users to a reasonable level (some bugs may still remain).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See the &lt;a href="https://nautilustrader.io/docs/latest/integrations/index.html"&gt;Integrations&lt;/a&gt; documentation for further details.&lt;/p&gt; 
&lt;h2&gt;Versioning and releases&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;NautilusTrader is still under active development&lt;/strong&gt;. Some features may be incomplete, and while the API is becoming more stable, breaking changes can occur between releases. We strive to document these changes in the release notes on a &lt;strong&gt;best-effort basis&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;We aim to follow a &lt;strong&gt;bi-weekly release schedule&lt;/strong&gt;, though experimental or larger features may cause delays.&lt;/p&gt; 
&lt;h3&gt;Branches&lt;/h3&gt; 
&lt;p&gt;We aim to maintain a stable, passing build across all branches.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;master&lt;/code&gt;: Reflects the source code for the latest released version; recommended for production use.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;nightly&lt;/code&gt;: Daily snapshots of the &lt;code&gt;develop&lt;/code&gt; branch for early testing; merged at &lt;strong&gt;14:00 UTC&lt;/strong&gt; or on demand.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;develop&lt;/code&gt;: Active development branch for contributors and feature work.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;Our &lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/ROADMAP.md"&gt;roadmap&lt;/a&gt; aims to achieve a &lt;strong&gt;stable API for version 2.x&lt;/strong&gt; (likely after the Rust port). Once this milestone is reached, we plan to implement a formal deprecation process for any API changes. This approach allows us to maintain a rapid development pace for now.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Precision mode&lt;/h2&gt; 
&lt;p&gt;NautilusTrader supports two precision modes for its core value types (&lt;code&gt;Price&lt;/code&gt;, &lt;code&gt;Quantity&lt;/code&gt;, &lt;code&gt;Money&lt;/code&gt;), which differ in their internal bit-width and maximum decimal precision.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;High-precision&lt;/strong&gt;: 128-bit integers with up to 16 decimals of precision, and a larger value range.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Standard-precision&lt;/strong&gt;: 64-bit integers with up to 9 decimals of precision, and a smaller value range.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;By default, the official Python wheels &lt;strong&gt;ship&lt;/strong&gt; in high-precision (128-bit) mode on Linux and macOS. On Windows, only standard-precision (64-bit) is available due to the lack of native 128-bit integer support. For the Rust crates, the default is standard-precision unless you explicitly enable the &lt;code&gt;high-precision&lt;/code&gt; feature flag.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;See the &lt;a href="https://nautilustrader.io/docs/latest/getting_started/installation"&gt;Installation Guide&lt;/a&gt; for further details.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Rust feature flag&lt;/strong&gt;: To enable high-precision mode in Rust, add the &lt;code&gt;high-precision&lt;/code&gt; feature to your Cargo.toml:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-toml"&gt;[dependencies]
nautilus_model = { version = "*", features = ["high-precision"] }
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;We recommend using the latest supported version of Python and installing &lt;a href="https://pypi.org/project/nautilus_trader/"&gt;nautilus_trader&lt;/a&gt; inside a virtual environment to isolate dependencies.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;There are two supported ways to install&lt;/strong&gt;:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Pre-built binary wheel from PyPI &lt;em&gt;or&lt;/em&gt; the Nautech Systems package index.&lt;/li&gt; 
 &lt;li&gt;Build from source.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP]&lt;/p&gt; 
 &lt;p&gt;We highly recommend installing using the &lt;a href="https://docs.astral.sh/uv"&gt;uv&lt;/a&gt; package manager with a "vanilla" CPython.&lt;/p&gt; 
 &lt;p&gt;Conda and other Python distributions &lt;em&gt;may&lt;/em&gt; work but aren’t officially supported.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;From PyPI&lt;/h3&gt; 
&lt;p&gt;To install the latest binary wheel (or sdist package) from PyPI using Python's pip package manager:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -U nautilus_trader
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;From the Nautech Systems package index&lt;/h3&gt; 
&lt;p&gt;The Nautech Systems package index (&lt;code&gt;packages.nautechsystems.io&lt;/code&gt;) complies with &lt;a href="https://peps.python.org/pep-0503/"&gt;PEP-503&lt;/a&gt; and hosts both stable and development binary wheels for &lt;code&gt;nautilus_trader&lt;/code&gt;. This enables users to install either the latest stable release or pre-release versions for testing.&lt;/p&gt; 
&lt;h4&gt;Stable wheels&lt;/h4&gt; 
&lt;p&gt;Stable wheels correspond to official releases of &lt;code&gt;nautilus_trader&lt;/code&gt; on PyPI, and use standard versioning.&lt;/p&gt; 
&lt;p&gt;To install the latest stable release:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -U nautilus_trader --index-url=https://packages.nautechsystems.io/simple
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Development wheels&lt;/h4&gt; 
&lt;p&gt;Development wheels are published from both the &lt;code&gt;nightly&lt;/code&gt; and &lt;code&gt;develop&lt;/code&gt; branches, allowing users to test features and fixes ahead of stable releases.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Wheels from the &lt;code&gt;develop&lt;/code&gt; branch are only built for the Linux x86_64 platform to save time and compute resources, while &lt;code&gt;nightly&lt;/code&gt; wheels support additional platforms as shown below.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Platform&lt;/th&gt; 
   &lt;th align="left"&gt;Nightly&lt;/th&gt; 
   &lt;th align="left"&gt;Develop&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;Linux (x86_64)&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;✓&lt;/td&gt; 
   &lt;td align="left"&gt;✓&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;Linux (ARM64)&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;✓&lt;/td&gt; 
   &lt;td align="left"&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;macOS (ARM64)&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;✓&lt;/td&gt; 
   &lt;td align="left"&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;Windows (x86_64)&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;✓&lt;/td&gt; 
   &lt;td align="left"&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;This process also helps preserve compute resources and ensures easy access to the exact binaries tested in CI pipelines, while adhering to &lt;a href="https://peps.python.org/pep-0440/"&gt;PEP-440&lt;/a&gt; versioning standards:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;develop&lt;/code&gt; wheels use the version format &lt;code&gt;dev{date}+{build_number}&lt;/code&gt; (e.g., &lt;code&gt;1.208.0.dev20241212+7001&lt;/code&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;nightly&lt;/code&gt; wheels use the version format &lt;code&gt;a{date}&lt;/code&gt; (alpha) (e.g., &lt;code&gt;1.208.0a20241212&lt;/code&gt;).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING]&lt;/p&gt; 
 &lt;p&gt;We do not recommend using development wheels in production environments, such as live trading controlling real capital.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Installation commands&lt;/h4&gt; 
&lt;p&gt;By default, pip will install the latest stable release. Adding the &lt;code&gt;--pre&lt;/code&gt; flag ensures that pre-release versions, including development wheels, are considered.&lt;/p&gt; 
&lt;p&gt;To install the latest available pre-release (including development wheels):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -U nautilus_trader --pre --index-url=https://packages.nautechsystems.io/simple
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To install a specific development wheel (e.g., &lt;code&gt;1.208.0a20241212&lt;/code&gt; for December 12, 2024):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install nautilus_trader==1.208.0a20241212 --index-url=https://packages.nautechsystems.io/simple
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Available versions&lt;/h4&gt; 
&lt;p&gt;You can view all available versions of &lt;code&gt;nautilus_trader&lt;/code&gt; on the &lt;a href="https://packages.nautechsystems.io/simple/nautilus-trader/index.html"&gt;package index&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To programmatically fetch and list available versions:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -s https://packages.nautechsystems.io/simple/nautilus-trader/index.html | grep -oP '(?&amp;lt;=&amp;lt;a href=")[^"]+(?=")' | awk -F'#' '{print $1}' | sort
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Branch updates&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;develop&lt;/code&gt; branch wheels (&lt;code&gt;.dev&lt;/code&gt;): Build and publish continuously with every merged commit.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;nightly&lt;/code&gt; branch wheels (&lt;code&gt;a&lt;/code&gt;): Build and publish daily when we automatically merge the &lt;code&gt;develop&lt;/code&gt; branch at &lt;strong&gt;14:00 UTC&lt;/strong&gt; (if there are changes).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Retention policies&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;develop&lt;/code&gt; branch wheels (&lt;code&gt;.dev&lt;/code&gt;): We retain only the most recent wheel build.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;nightly&lt;/code&gt; branch wheels (&lt;code&gt;a&lt;/code&gt;): We retain only the 30 most recent wheel builds.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;From Source&lt;/h3&gt; 
&lt;p&gt;It's possible to install from source using pip if you first install the build dependencies as specified in the &lt;code&gt;pyproject.toml&lt;/code&gt;.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Install &lt;a href="https://rustup.rs/"&gt;rustup&lt;/a&gt; (the Rust toolchain installer):&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;Linux and macOS:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;curl https://sh.rustup.rs -sSf | sh
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Windows:&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Download and install &lt;a href="https://win.rustup.rs/x86_64"&gt;&lt;code&gt;rustup-init.exe&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;Install "Desktop development with C++" with &lt;a href="https://visualstudio.microsoft.com/thank-you-downloading-visual-studio/?sku=BuildTools&amp;amp;rel=16"&gt;Build Tools for Visual Studio 2019&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Verify (any system): from a terminal session run: &lt;code&gt;rustc --version&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Enable &lt;code&gt;cargo&lt;/code&gt; in the current shell:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;Linux and macOS:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;source $HOME/.cargo/env
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Windows:&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Start a new PowerShell&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install &lt;a href="https://clang.llvm.org/"&gt;clang&lt;/a&gt; (a C language frontend for LLVM):&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;Linux:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt-get install clang
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Windows:&lt;/p&gt; 
    &lt;ol&gt; 
     &lt;li&gt; &lt;p&gt;Add Clang to your &lt;a href="https://visualstudio.microsoft.com/thank-you-downloading-visual-studio/?sku=BuildTools&amp;amp;rel=16"&gt;Build Tools for Visual Studio 2019&lt;/a&gt;:&lt;/p&gt; 
      &lt;ul&gt; 
       &lt;li&gt;Start | Visual Studio Installer | Modify | C++ Clang tools for Windows (12.0.0 - x64…) = checked | Modify&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;Enable &lt;code&gt;clang&lt;/code&gt; in the current shell:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-powershell"&gt;[System.Environment]::SetEnvironmentVariable('path', "C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\VC\Tools\Llvm\x64\bin\;" + $env:Path,"User")
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
    &lt;/ol&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Verify (any system): from a terminal session run: &lt;code&gt;clang --version&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install uv (see the &lt;a href="https://docs.astral.sh/uv/getting-started/installation"&gt;uv installation guide&lt;/a&gt; for more details):&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;curl -LsSf https://astral.sh/uv/install.sh | sh
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Clone the source with &lt;code&gt;git&lt;/code&gt;, and install from the project's root directory:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone --branch develop --depth 1 https://github.com/nautechsystems/nautilus_trader
cd nautilus_trader
uv sync --all-extras
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;The &lt;code&gt;--depth 1&lt;/code&gt; flag fetches just the latest commit for a faster, lightweight clone.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ol start="6"&gt; 
 &lt;li&gt; &lt;p&gt;Set environment variables for PyO3 compilation (Linux and macOS only):&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Set the library path for the Python interpreter (in this case Python 3.13.4)
export LD_LIBRARY_PATH="$HOME/.local/share/uv/python/cpython-3.13.4-linux-x86_64-gnu/lib:$LD_LIBRARY_PATH"

# Set the Python executable path for PyO3
export PYO3_PYTHON=$(pwd)/.venv/bin/python
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;Adjust the Python version and architecture in the &lt;code&gt;LD_LIBRARY_PATH&lt;/code&gt; to match your system. Use &lt;code&gt;uv python list&lt;/code&gt; to find the exact path for your Python installation.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;See the &lt;a href="https://nautilustrader.io/docs/latest/getting_started/installation"&gt;Installation Guide&lt;/a&gt; for other options and further details.&lt;/p&gt; 
&lt;h2&gt;Redis&lt;/h2&gt; 
&lt;p&gt;Using &lt;a href="https://redis.io"&gt;Redis&lt;/a&gt; with NautilusTrader is &lt;strong&gt;optional&lt;/strong&gt; and only required if configured as the backend for a &lt;a href="https://nautilustrader.io/docs/latest/concepts/cache"&gt;cache&lt;/a&gt; database or &lt;a href="https://nautilustrader.io/docs/latest/concepts/message_bus"&gt;message bus&lt;/a&gt;. See the &lt;strong&gt;Redis&lt;/strong&gt; section of the &lt;a href="https://nautilustrader.io/docs/latest/getting_started/installation#redis"&gt;Installation Guide&lt;/a&gt; for further details.&lt;/p&gt; 
&lt;h2&gt;Makefile&lt;/h2&gt; 
&lt;p&gt;A &lt;code&gt;Makefile&lt;/code&gt; is provided to automate most installation and build tasks for development. Some of the targets include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;make install&lt;/code&gt;: Installs in &lt;code&gt;release&lt;/code&gt; build mode with all dependency groups and extras.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make install-debug&lt;/code&gt;: Same as &lt;code&gt;make install&lt;/code&gt; but with &lt;code&gt;debug&lt;/code&gt; build mode.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make install-just-deps&lt;/code&gt;: Installs just the &lt;code&gt;main&lt;/code&gt;, &lt;code&gt;dev&lt;/code&gt; and &lt;code&gt;test&lt;/code&gt; dependencies (does not install package).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make build&lt;/code&gt;: Runs the build script in &lt;code&gt;release&lt;/code&gt; build mode (default).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make build-debug&lt;/code&gt;: Runs the build script in &lt;code&gt;debug&lt;/code&gt; build mode.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make build-wheel&lt;/code&gt;: Runs uv build with a wheel format in &lt;code&gt;release&lt;/code&gt; mode.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make build-wheel-debug&lt;/code&gt;: Runs uv build with a wheel format in &lt;code&gt;debug&lt;/code&gt; mode.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make cargo-test&lt;/code&gt;: Runs all Rust crate tests using &lt;code&gt;cargo-nextest&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make clean&lt;/code&gt;: Deletes all build results, such as &lt;code&gt;.so&lt;/code&gt; or &lt;code&gt;.dll&lt;/code&gt; files.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make distclean&lt;/code&gt;: &lt;strong&gt;CAUTION&lt;/strong&gt; Removes all artifacts not in the git index from the repository. This includes source files which have not been &lt;code&gt;git add&lt;/code&gt;ed.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make docs&lt;/code&gt;: Builds the documentation HTML using Sphinx.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make pre-commit&lt;/code&gt;: Runs the pre-commit checks over all files.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make ruff&lt;/code&gt;: Runs ruff over all files using the &lt;code&gt;pyproject.toml&lt;/code&gt; config (with autofix).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make pytest&lt;/code&gt;: Runs all tests with &lt;code&gt;pytest&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make test-performance&lt;/code&gt;: Runs performance tests with &lt;a href="https://codspeed.io"&gt;codspeed&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP]&lt;/p&gt; 
 &lt;p&gt;Run &lt;code&gt;make help&lt;/code&gt; for documentation on all available make targets.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP]&lt;/p&gt; 
 &lt;p&gt;See the &lt;a href="https://github.com/nautechsystems/nautilus_trader/raw/develop/crates/infrastructure/TESTS.md"&gt;crates/infrastructure/TESTS.md&lt;/a&gt; file for running the infrastructure integration tests.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;p&gt;Indicators and strategies can be developed in both Python and Cython. For performance and latency-sensitive applications, we recommend using Cython. Below are some examples:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/nautilus_trader/examples/indicators/ema_python.py"&gt;indicator&lt;/a&gt; example written in Python.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/nautilus_trader/indicators/"&gt;indicator&lt;/a&gt; examples written in Cython.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/nautilus_trader/examples/strategies/"&gt;strategy&lt;/a&gt; examples written in Python.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/examples/backtest/"&gt;backtest&lt;/a&gt; examples using a &lt;code&gt;BacktestEngine&lt;/code&gt; directly.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Docker&lt;/h2&gt; 
&lt;p&gt;Docker containers are built using the base image &lt;code&gt;python:3.12-slim&lt;/code&gt; with the following variant tags:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;nautilus_trader:latest&lt;/code&gt; has the latest release version installed.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;nautilus_trader:nightly&lt;/code&gt; has the head of the &lt;code&gt;nightly&lt;/code&gt; branch installed.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;jupyterlab:latest&lt;/code&gt; has the latest release version installed along with &lt;code&gt;jupyterlab&lt;/code&gt; and an example backtest notebook with accompanying data.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;jupyterlab:nightly&lt;/code&gt; has the head of the &lt;code&gt;nightly&lt;/code&gt; branch installed along with &lt;code&gt;jupyterlab&lt;/code&gt; and an example backtest notebook with accompanying data.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can pull the container images as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker pull ghcr.io/nautechsystems/&amp;lt;image_variant_tag&amp;gt; --platform linux/amd64
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can launch the backtest example container by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker pull ghcr.io/nautechsystems/jupyterlab:nightly --platform linux/amd64
docker run -p 8888:8888 ghcr.io/nautechsystems/jupyterlab:nightly
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then open your browser at the following address:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;http://127.0.0.1:8888/lab
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING]&lt;/p&gt; 
 &lt;p&gt;NautilusTrader currently exceeds the rate limit for Jupyter notebook logging (stdout output). Therefore, we set the &lt;code&gt;log_level&lt;/code&gt; to &lt;code&gt;ERROR&lt;/code&gt; in the examples. Lowering this level to see more logging will cause the notebook to hang during cell execution. We are investigating a fix that may involve either raising the configured rate limits for Jupyter or throttling the log flushing from Nautilus.&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://github.com/jupyterlab/jupyterlab/issues/12845"&gt;https://github.com/jupyterlab/jupyterlab/issues/12845&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/deshaw/jupyterlab-limit-output"&gt;https://github.com/deshaw/jupyterlab-limit-output&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Development&lt;/h2&gt; 
&lt;p&gt;We aim to provide the most pleasant developer experience possible for this hybrid codebase of Python, Cython and Rust. See the &lt;a href="https://nautilustrader.io/docs/latest/developer_guide/index.html"&gt;Developer Guide&lt;/a&gt; for helpful information.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP]&lt;/p&gt; 
 &lt;p&gt;Run &lt;code&gt;make build-debug&lt;/code&gt; to compile after changes to Rust or Cython code for the most efficient development workflow.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Testing with Rust&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://nexte.st"&gt;cargo-nextest&lt;/a&gt; is the standard Rust test runner for NautilusTrader. Its key benefit is isolating each test in its own process, ensuring test reliability by avoiding interference.&lt;/p&gt; 
&lt;p&gt;You can install cargo-nextest by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cargo install cargo-nextest
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP]&lt;/p&gt; 
 &lt;p&gt;Run Rust tests with &lt;code&gt;make cargo-test&lt;/code&gt;, which uses &lt;strong&gt;cargo-nextest&lt;/strong&gt; with an efficient profile.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Thank you for considering contributing to NautilusTrader! We welcome any and all help to improve the project. If you have an idea for an enhancement or a bug fix, the first step is to open an &lt;a href="https://github.com/nautechsystems/nautilus_trader/issues"&gt;issue&lt;/a&gt; on GitHub to discuss it with the team. This helps to ensure that your contribution will be well-aligned with the goals of the project and avoids duplication of effort.&lt;/p&gt; 
&lt;p&gt;Before getting started, be sure to review the &lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/ROADMAP.md#open-source-scope"&gt;open-source scope&lt;/a&gt; outlined in the project’s roadmap to understand what’s in and out of scope.&lt;/p&gt; 
&lt;p&gt;Once you're ready to start working on your contribution, make sure to follow the guidelines outlined in the &lt;a href="https://github.com/nautechsystems/nautilus_trader/raw/develop/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; file. This includes signing a Contributor License Agreement (CLA) to ensure that your contributions can be included in the project.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;Pull requests should target the &lt;code&gt;develop&lt;/code&gt; branch (the default branch). This is where new features and improvements are integrated before release.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Thank you again for your interest in NautilusTrader! We look forward to reviewing your contributions and working with you to improve the project.&lt;/p&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;p&gt;Join our community of users and contributors on &lt;a href="https://discord.gg/NautilusTrader"&gt;Discord&lt;/a&gt; to chat and stay up-to-date with the latest announcements and features of NautilusTrader. Whether you're a developer looking to contribute or just want to learn more about the platform, all are welcome on our Discord server.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING]&lt;/p&gt; 
 &lt;p&gt;NautilusTrader does not issue, promote, or endorse any cryptocurrency tokens. Any claims or communications suggesting otherwise are unauthorized and false.&lt;/p&gt; 
 &lt;p&gt;All official updates and communications from NautilusTrader will be shared exclusively through &lt;a href="https://nautilustrader.io"&gt;https://nautilustrader.io&lt;/a&gt;, our &lt;a href="https://discord.gg/NautilusTrader"&gt;Discord server&lt;/a&gt;, or our X (Twitter) account: &lt;a href="https://x.com/NautilusTrader"&gt;@NautilusTrader&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;If you encounter any suspicious activity, please report it to the appropriate platform and contact us at &lt;a href="mailto:info@nautechsystems.io"&gt;info@nautechsystems.io&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;The source code for NautilusTrader is available on GitHub under the &lt;a href="https://www.gnu.org/licenses/lgpl-3.0.en.html"&gt;GNU Lesser General Public License v3.0&lt;/a&gt;. Contributions to the project are welcome and require the completion of a standard &lt;a href="https://github.com/nautechsystems/nautilus_trader/raw/develop/CLA.md"&gt;Contributor License Agreement (CLA)&lt;/a&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;NautilusTrader™ is developed and maintained by Nautech Systems, a technology company specializing in the development of high-performance trading systems. For more information, visit &lt;a href="https://nautilustrader.io"&gt;https://nautilustrader.io&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;© 2015-2025 Nautech Systems Pty Ltd. All rights reserved.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/nautechsystems/nautilus_trader/raw/develop/assets/ns-logo.png" alt="nautechsystems" title="nautechsystems" /&gt; &lt;img src="https://github.com/nautechsystems/nautilus_trader/raw/develop/assets/ferris.png" width="128" /&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>pola-rs/polars</title>
      <link>https://github.com/pola-rs/polars</link>
      <description>&lt;p&gt;Dataframes powered by a multithreaded, vectorized query engine, written in Rust&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt; &lt;a href="https://pola.rs"&gt; &lt;img src="https://raw.githubusercontent.com/pola-rs/polars-static/master/banner/polars_github_banner.svg?sanitize=true" alt="Polars logo" /&gt; &lt;/a&gt; &lt;/h1&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://crates.io/crates/polars"&gt; &lt;img src="https://img.shields.io/crates/v/polars.svg?sanitize=true" alt="crates.io Latest Release" /&gt; &lt;/a&gt; 
 &lt;a href="https://pypi.org/project/polars/"&gt; &lt;img src="https://img.shields.io/pypi/v/polars.svg?sanitize=true" alt="PyPi Latest Release" /&gt; &lt;/a&gt; 
 &lt;a href="https://www.npmjs.com/package/nodejs-polars"&gt; &lt;img src="https://img.shields.io/npm/v/nodejs-polars.svg?sanitize=true" alt="NPM Latest Release" /&gt; &lt;/a&gt; 
 &lt;a href="https://community.r-multiverse.org/polars"&gt; &lt;img src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fcommunity.r-multiverse.org%2Fapi%2Fpackages%2Fpolars&amp;amp;query=%24.Version&amp;amp;label=r-multiverse" alt="R-multiverse Latest Release" /&gt; &lt;/a&gt; 
 &lt;a href="https://doi.org/10.5281/zenodo.7697217"&gt; &lt;img src="https://zenodo.org/badge/DOI/10.5281/zenodo.7697217.svg?sanitize=true" alt="DOI Latest Release" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;p align="center"&gt; &lt;b&gt;Documentation&lt;/b&gt;: &lt;a href="https://docs.pola.rs/api/python/stable/reference/index.html"&gt;Python&lt;/a&gt; - &lt;a href="https://docs.rs/polars/latest/polars/"&gt;Rust&lt;/a&gt; - &lt;a href="https://pola-rs.github.io/nodejs-polars/index.html"&gt;Node.js&lt;/a&gt; - &lt;a href="https://pola-rs.github.io/r-polars/index.html"&gt;R&lt;/a&gt; | &lt;b&gt;StackOverflow&lt;/b&gt;: &lt;a href="https://stackoverflow.com/questions/tagged/python-polars"&gt;Python&lt;/a&gt; - &lt;a href="https://stackoverflow.com/questions/tagged/rust-polars"&gt;Rust&lt;/a&gt; - &lt;a href="https://stackoverflow.com/questions/tagged/nodejs-polars"&gt;Node.js&lt;/a&gt; - &lt;a href="https://stackoverflow.com/questions/tagged/r-polars"&gt;R&lt;/a&gt; | &lt;a href="https://docs.pola.rs/"&gt;User guide&lt;/a&gt; | &lt;a href="https://discord.gg/4UfP5cfBE7"&gt;Discord&lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;Polars: Blazingly fast DataFrames in Rust, Python, Node.js, R, and SQL&lt;/h2&gt; 
&lt;p&gt;Polars is a DataFrame interface on top of an OLAP Query Engine implemented in Rust using &lt;a href="https://arrow.apache.org/docs/format/Columnar.html"&gt;Apache Arrow Columnar Format&lt;/a&gt; as the memory model.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Lazy | eager execution&lt;/li&gt; 
 &lt;li&gt;Multi-threaded&lt;/li&gt; 
 &lt;li&gt;SIMD&lt;/li&gt; 
 &lt;li&gt;Query optimization&lt;/li&gt; 
 &lt;li&gt;Powerful expression API&lt;/li&gt; 
 &lt;li&gt;Hybrid Streaming (larger-than-RAM datasets)&lt;/li&gt; 
 &lt;li&gt;Rust | Python | NodeJS | R | ...&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To learn more, read the &lt;a href="https://docs.pola.rs/"&gt;user guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Python&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;&amp;gt;&amp;gt;&amp;gt; import polars as pl
&amp;gt;&amp;gt;&amp;gt; df = pl.DataFrame(
...     {
...         "A": [1, 2, 3, 4, 5],
...         "fruits": ["banana", "banana", "apple", "apple", "banana"],
...         "B": [5, 4, 3, 2, 1],
...         "cars": ["beetle", "audi", "beetle", "beetle", "beetle"],
...     }
... )

# embarrassingly parallel execution &amp;amp; very expressive query language
&amp;gt;&amp;gt;&amp;gt; df.sort("fruits").select(
...     "fruits",
...     "cars",
...     pl.lit("fruits").alias("literal_string_fruits"),
...     pl.col("B").filter(pl.col("cars") == "beetle").sum(),
...     pl.col("A").filter(pl.col("B") &amp;gt; 2).sum().over("cars").alias("sum_A_by_cars"),
...     pl.col("A").sum().over("fruits").alias("sum_A_by_fruits"),
...     pl.col("A").reverse().over("fruits").alias("rev_A_by_fruits"),
...     pl.col("A").sort_by("B").over("fruits").alias("sort_A_by_B_by_fruits"),
... )
shape: (5, 8)
┌──────────┬──────────┬──────────────┬─────┬─────────────┬─────────────┬─────────────┬─────────────┐
│ fruits   ┆ cars     ┆ literal_stri ┆ B   ┆ sum_A_by_ca ┆ sum_A_by_fr ┆ rev_A_by_fr ┆ sort_A_by_B │
│ ---      ┆ ---      ┆ ng_fruits    ┆ --- ┆ rs          ┆ uits        ┆ uits        ┆ _by_fruits  │
│ str      ┆ str      ┆ ---          ┆ i64 ┆ ---         ┆ ---         ┆ ---         ┆ ---         │
│          ┆          ┆ str          ┆     ┆ i64         ┆ i64         ┆ i64         ┆ i64         │
╞══════════╪══════════╪══════════════╪═════╪═════════════╪═════════════╪═════════════╪═════════════╡
│ "apple"  ┆ "beetle" ┆ "fruits"     ┆ 11  ┆ 4           ┆ 7           ┆ 4           ┆ 4           │
│ "apple"  ┆ "beetle" ┆ "fruits"     ┆ 11  ┆ 4           ┆ 7           ┆ 3           ┆ 3           │
│ "banana" ┆ "beetle" ┆ "fruits"     ┆ 11  ┆ 4           ┆ 8           ┆ 5           ┆ 5           │
│ "banana" ┆ "audi"   ┆ "fruits"     ┆ 11  ┆ 2           ┆ 8           ┆ 2           ┆ 2           │
│ "banana" ┆ "beetle" ┆ "fruits"     ┆ 11  ┆ 4           ┆ 8           ┆ 1           ┆ 1           │
└──────────┴──────────┴──────────────┴─────┴─────────────┴─────────────┴─────────────┴─────────────┘
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;SQL&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;&amp;gt;&amp;gt;&amp;gt; df = pl.scan_csv("docs/assets/data/iris.csv")
&amp;gt;&amp;gt;&amp;gt; ## OPTION 1
&amp;gt;&amp;gt;&amp;gt; # run SQL queries on frame-level
&amp;gt;&amp;gt;&amp;gt; df.sql("""
...	SELECT species,
...	  AVG(sepal_length) AS avg_sepal_length
...	FROM self
...	GROUP BY species
...	""").collect()
shape: (3, 2)
┌────────────┬──────────────────┐
│ species    ┆ avg_sepal_length │
│ ---        ┆ ---              │
│ str        ┆ f64              │
╞════════════╪══════════════════╡
│ Virginica  ┆ 6.588            │
│ Versicolor ┆ 5.936            │
│ Setosa     ┆ 5.006            │
└────────────┴──────────────────┘
&amp;gt;&amp;gt;&amp;gt; ## OPTION 2
&amp;gt;&amp;gt;&amp;gt; # use pl.sql() to operate on the global context
&amp;gt;&amp;gt;&amp;gt; df2 = pl.LazyFrame({
...    "species": ["Setosa", "Versicolor", "Virginica"],
...    "blooming_season": ["Spring", "Summer", "Fall"]
...})
&amp;gt;&amp;gt;&amp;gt; pl.sql("""
... SELECT df.species,
...     AVG(df.sepal_length) AS avg_sepal_length,
...     df2.blooming_season
... FROM df
... LEFT JOIN df2 ON df.species = df2.species
... GROUP BY df.species, df2.blooming_season
... """).collect()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;SQL commands can also be run directly from your terminal using the Polars CLI:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# run an inline SQL query
&amp;gt; polars -c "SELECT species, AVG(sepal_length) AS avg_sepal_length, AVG(sepal_width) AS avg_sepal_width FROM read_csv('docs/assets/data/iris.csv') GROUP BY species;"

# run interactively
&amp;gt; polars
Polars CLI v0.3.0
Type .help for help.

&amp;gt; SELECT species, AVG(sepal_length) AS avg_sepal_length, AVG(sepal_width) AS avg_sepal_width FROM read_csv('docs/assets/data/iris.csv') GROUP BY species;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Refer to the &lt;a href="https://github.com/pola-rs/polars-cli"&gt;Polars CLI repository&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;Performance 🚀🚀&lt;/h2&gt; 
&lt;h3&gt;Blazingly fast&lt;/h3&gt; 
&lt;p&gt;Polars is very fast. In fact, it is one of the best performing solutions available. See the &lt;a href="https://www.pola.rs/benchmarks.html"&gt;PDS-H benchmarks&lt;/a&gt; results.&lt;/p&gt; 
&lt;h3&gt;Lightweight&lt;/h3&gt; 
&lt;p&gt;Polars is also very lightweight. It comes with zero required dependencies, and this shows in the import times:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;polars: 70ms&lt;/li&gt; 
 &lt;li&gt;numpy: 104ms&lt;/li&gt; 
 &lt;li&gt;pandas: 520ms&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Handles larger-than-RAM data&lt;/h3&gt; 
&lt;p&gt;If you have data that does not fit into memory, Polars' query engine is able to process your query (or parts of your query) in a streaming fashion. This drastically reduces memory requirements, so you might be able to process your 250GB dataset on your laptop. Collect with &lt;code&gt;collect(engine='streaming')&lt;/code&gt; to run the query streaming. (This might be a little slower, but it is still very fast!)&lt;/p&gt; 
&lt;h2&gt;Setup&lt;/h2&gt; 
&lt;h3&gt;Python&lt;/h3&gt; 
&lt;p&gt;Install the latest Polars version with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip install polars
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We also have a conda package (&lt;code&gt;conda install -c conda-forge polars&lt;/code&gt;), however pip is the preferred way to install Polars.&lt;/p&gt; 
&lt;p&gt;Install Polars with all optional dependencies.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip install 'polars[all]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also install a subset of all optional dependencies.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip install 'polars[numpy,pandas,pyarrow]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See the &lt;a href="https://docs.pola.rs/user-guide/installation/#feature-flags"&gt;User Guide&lt;/a&gt; for more details on optional dependencies&lt;/p&gt; 
&lt;p&gt;To see the current Polars version and a full list of its optional dependencies, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;pl.show_versions()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Releases happen quite often (weekly / every few days) at the moment, so updating Polars regularly to get the latest bugfixes / features might not be a bad idea.&lt;/p&gt; 
&lt;h3&gt;Rust&lt;/h3&gt; 
&lt;p&gt;You can take latest release from &lt;code&gt;crates.io&lt;/code&gt;, or if you want to use the latest features / performance improvements point to the &lt;code&gt;main&lt;/code&gt; branch of this repo.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-toml"&gt;polars = { git = "https://github.com/pola-rs/polars", rev = "&amp;lt;optional git tag&amp;gt;" }
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Requires Rust version &lt;code&gt;&amp;gt;=1.80&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Want to contribute? Read our &lt;a href="https://docs.pola.rs/development/contributing/"&gt;contributing guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Python: compile Polars from source&lt;/h2&gt; 
&lt;p&gt;If you want a bleeding edge release or maximal performance you should compile Polars from source.&lt;/p&gt; 
&lt;p&gt;This can be done by going through the following steps in sequence:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install the latest &lt;a href="https://www.rust-lang.org/tools/install"&gt;Rust compiler&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Install &lt;a href="https://maturin.rs/"&gt;maturin&lt;/a&gt;: &lt;code&gt;pip install maturin&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;cd py-polars&lt;/code&gt; and choose one of the following: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;make build&lt;/code&gt;, slow binary with debug assertions and symbols, fast compile times&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;make build-release&lt;/code&gt;, fast binary without debug assertions, minimal debug symbols, long compile times&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;make build-nodebug-release&lt;/code&gt;, same as build-release but without any debug symbols, slightly faster to compile&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;make build-debug-release&lt;/code&gt;, same as build-release but with full debug symbols, slightly slower to compile&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;make build-dist-release&lt;/code&gt;, fastest binary, extreme compile times&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;By default the binary is compiled with optimizations turned on for a modern CPU. Specify &lt;code&gt;LTS_CPU=1&lt;/code&gt; with the command if your CPU is older and does not support e.g. AVX2.&lt;/p&gt; 
&lt;p&gt;Note that the Rust crate implementing the Python bindings is called &lt;code&gt;py-polars&lt;/code&gt; to distinguish from the wrapped Rust crate &lt;code&gt;polars&lt;/code&gt; itself. However, both the Python package and the Python module are named &lt;code&gt;polars&lt;/code&gt;, so you can &lt;code&gt;pip install polars&lt;/code&gt; and &lt;code&gt;import polars&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Using custom Rust functions in Python&lt;/h2&gt; 
&lt;p&gt;Extending Polars with UDFs compiled in Rust is easy. We expose PyO3 extensions for &lt;code&gt;DataFrame&lt;/code&gt; and &lt;code&gt;Series&lt;/code&gt; data structures. See more in &lt;a href="https://github.com/pola-rs/polars/tree/main/pyo3-polars"&gt;https://github.com/pola-rs/polars/tree/main/pyo3-polars&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Going big...&lt;/h2&gt; 
&lt;p&gt;Do you expect more than 2^32 (~4.2 billion) rows? Compile Polars with the &lt;code&gt;bigidx&lt;/code&gt; feature flag or, for Python users, install &lt;code&gt;pip install polars-u64-idx&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Don't use this unless you hit the row boundary as the default build of Polars is faster and consumes less memory.&lt;/p&gt; 
&lt;h2&gt;Legacy&lt;/h2&gt; 
&lt;p&gt;Do you want Polars to run on an old CPU (e.g. dating from before 2011), or on an &lt;code&gt;x86-64&lt;/code&gt; build of Python on Apple Silicon under Rosetta? Install &lt;code&gt;pip install polars-lts-cpu&lt;/code&gt;. This version of Polars is compiled without &lt;a href="https://en.wikipedia.org/wiki/Advanced_Vector_Extensions"&gt;AVX&lt;/a&gt; target features.&lt;/p&gt; 
&lt;h2&gt;Sponsors&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.jetbrains.com"&gt;&lt;img src="https://www.jetbrains.com/company/brand/img/jetbrains_logo.png" height="50" alt="JetBrains logo" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>launchbadge/sqlx</title>
      <link>https://github.com/launchbadge/sqlx</link>
      <description>&lt;p&gt;🧰 The Rust SQL Toolkit. An async, pure Rust SQL crate featuring compile-time checked queries without a DSL. Supports PostgreSQL, MySQL, and SQLite.&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt;SQLx&lt;/h1&gt; 
&lt;div align="center"&gt; 
 &lt;strong&gt; 🧰 The Rust SQL Toolkit &lt;/strong&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;!-- Github Actions --&gt; 
 &lt;a href="https://github.com/launchbadge/sqlx/actions/workflows/sqlx.yml?query=branch%3Amain"&gt; &lt;img src="https://img.shields.io/github/actions/workflow/status/launchbadge/sqlx/sqlx.yml?branch=main&amp;amp;style=flat-square" alt="actions status" /&gt;&lt;/a&gt; 
 &lt;!-- Version --&gt; 
 &lt;a href="https://crates.io/crates/sqlx"&gt; &lt;img src="https://img.shields.io/crates/v/sqlx.svg?style=flat-square" alt="Crates.io version" /&gt;&lt;/a&gt; 
 &lt;!-- Discord --&gt; 
 &lt;a href="https://discord.gg/uuruzJ7"&gt; &lt;img src="https://img.shields.io/discord/665528275556106240?style=flat-square" alt="chat" /&gt;&lt;/a&gt; 
 &lt;!-- Docs --&gt; 
 &lt;a href="https://docs.rs/sqlx"&gt; &lt;img src="https://img.shields.io/badge/docs-latest-blue.svg?style=flat-square" alt="docs.rs docs" /&gt;&lt;/a&gt; 
 &lt;!-- Downloads --&gt; 
 &lt;a href="https://crates.io/crates/sqlx"&gt; &lt;img src="https://img.shields.io/crates/d/sqlx.svg?style=flat-square" alt="Download" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;h4&gt; &lt;a href="https://raw.githubusercontent.com/launchbadge/sqlx/main/#install"&gt; Install &lt;/a&gt; &lt;span&gt; | &lt;/span&gt; &lt;a href="https://raw.githubusercontent.com/launchbadge/sqlx/main/#usage"&gt; Usage &lt;/a&gt; &lt;span&gt; | &lt;/span&gt; &lt;a href="https://docs.rs/sqlx"&gt; Docs &lt;/a&gt; &lt;span&gt; | &lt;/span&gt; &lt;a href="https://github.com/launchbadge/sqlx/wiki/Ecosystem"&gt; Ecosystem &lt;/a&gt; &lt;span&gt; | &lt;/span&gt; &lt;a href="https://discord.gg/uuruzJ7"&gt; Discord &lt;/a&gt; &lt;/h4&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;small&gt;Built with ❤️ by &lt;a href="https://launchbadge.com"&gt;The LaunchBadge team&lt;/a&gt;&lt;/small&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;h5&gt;Have a question? Be sure to &lt;a href="https://raw.githubusercontent.com/launchbadge/sqlx/main/FAQ.md"&gt;check the FAQ first!&lt;/a&gt;&lt;/h5&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;p&gt;SQLx is an async, pure Rust&lt;sub&gt;†&lt;/sub&gt; SQL crate featuring compile-time checked queries without a DSL.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Truly Asynchronous&lt;/strong&gt;. Built from the ground-up using async/await for maximum concurrency.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Compile-time checked queries&lt;/strong&gt; (if you want). See &lt;a href="https://raw.githubusercontent.com/launchbadge/sqlx/main/#sqlx-is-not-an-orm"&gt;SQLx is not an ORM&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Database Agnostic&lt;/strong&gt;. Support for &lt;a href="http://postgresql.org/"&gt;PostgreSQL&lt;/a&gt;, &lt;a href="https://www.mysql.com/"&gt;MySQL&lt;/a&gt;, &lt;a href="https://www.mariadb.org/"&gt;MariaDB&lt;/a&gt;, &lt;a href="https://sqlite.org/"&gt;SQLite&lt;/a&gt;.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://www.microsoft.com/en-us/sql-server"&gt;MSSQL&lt;/a&gt; was supported prior to version 0.7, but has been removed pending a full rewrite of the driver as part of our &lt;a href="https://github.com/launchbadge/sqlx/discussions/1616"&gt;SQLx Pro initiative&lt;/a&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Pure Rust&lt;/strong&gt;. The Postgres and MySQL/MariaDB drivers are written in pure Rust using &lt;strong&gt;zero&lt;/strong&gt; unsafe&lt;sub&gt;††&lt;/sub&gt; code.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Runtime Agnostic&lt;/strong&gt;. Works on different runtimes (&lt;a href="https://github.com/async-rs/async-std"&gt;&lt;code&gt;async-std&lt;/code&gt;&lt;/a&gt; / &lt;a href="https://github.com/tokio-rs/tokio"&gt;&lt;code&gt;tokio&lt;/code&gt;&lt;/a&gt; / &lt;a href="https://github.com/actix/actix-net"&gt;&lt;code&gt;actix&lt;/code&gt;&lt;/a&gt;) and TLS backends (&lt;a href="https://crates.io/crates/native-tls"&gt;&lt;code&gt;native-tls&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://crates.io/crates/rustls"&gt;&lt;code&gt;rustls&lt;/code&gt;&lt;/a&gt;).&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;small&gt;&lt;small&gt;&lt;/small&gt;&lt;/small&gt;&lt;/p&gt;
&lt;small&gt;&lt;small&gt; &lt;p&gt;† The SQLite driver uses the libsqlite3 C library as SQLite is an embedded database (the only way we could be pure Rust for SQLite is by porting &lt;em&gt;all&lt;/em&gt; of SQLite to Rust).&lt;/p&gt; &lt;p&gt;†† SQLx uses &lt;code&gt;#![forbid(unsafe_code)]&lt;/code&gt; unless the &lt;code&gt;sqlite&lt;/code&gt; feature is enabled. The SQLite driver directly invokes the SQLite3 API via &lt;code&gt;libsqlite3-sys&lt;/code&gt;, which requires &lt;code&gt;unsafe&lt;/code&gt;.&lt;/p&gt; &lt;/small&gt;&lt;/small&gt;
&lt;p&gt;&lt;small&gt;&lt;small&gt;&lt;/small&gt;&lt;/small&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Cross-platform. Being native Rust, SQLx will compile anywhere Rust is supported.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Built-in connection pooling with &lt;code&gt;sqlx::Pool&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Row streaming. Data is read asynchronously from the database and decoded on demand.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Automatic statement preparation and caching. When using the high-level query API (&lt;code&gt;sqlx::query&lt;/code&gt;), statements are prepared and cached per connection.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Simple (unprepared) query execution including fetching results into the same &lt;code&gt;Row&lt;/code&gt; types used by the high-level API. Supports batch execution and returns results from all statements.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Transport Layer Security (TLS) where supported (&lt;a href="https://www.mysql.com/"&gt;MySQL&lt;/a&gt;, &lt;a href="https://www.mariadb.org/"&gt;MariaDB&lt;/a&gt; and &lt;a href="http://postgresql.org/"&gt;PostgreSQL&lt;/a&gt;).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Asynchronous notifications using &lt;code&gt;LISTEN&lt;/code&gt; and &lt;code&gt;NOTIFY&lt;/code&gt; for &lt;a href="http://postgresql.org/"&gt;PostgreSQL&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Nested transactions with support for save points.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Any&lt;/code&gt; database driver for changing the database driver at runtime. An &lt;code&gt;AnyPool&lt;/code&gt; connects to the driver indicated by the URL scheme.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Install&lt;/h2&gt; 
&lt;p&gt;SQLx is compatible with the &lt;a href="https://github.com/async-rs/async-std"&gt;&lt;code&gt;async-std&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://github.com/tokio-rs/tokio"&gt;&lt;code&gt;tokio&lt;/code&gt;&lt;/a&gt;, and &lt;a href="https://github.com/actix/actix-net"&gt;&lt;code&gt;actix&lt;/code&gt;&lt;/a&gt; runtimes; and, the &lt;a href="https://crates.io/crates/native-tls"&gt;&lt;code&gt;native-tls&lt;/code&gt;&lt;/a&gt; and &lt;a href="https://crates.io/crates/rustls"&gt;&lt;code&gt;rustls&lt;/code&gt;&lt;/a&gt; TLS backends. When adding the dependency, you must choose a runtime feature that is &lt;code&gt;runtime&lt;/code&gt; + &lt;code&gt;tls&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-toml"&gt;# Cargo.toml
[dependencies]
# PICK ONE OF THE FOLLOWING:

# tokio (no TLS)
sqlx = { version = "0.8", features = [ "runtime-tokio" ] }
# tokio + native-tls
sqlx = { version = "0.8", features = [ "runtime-tokio", "tls-native-tls" ] }
# tokio + rustls with ring and WebPKI CA certificates
sqlx = { version = "0.8", features = [ "runtime-tokio", "tls-rustls-ring-webpki" ] }
# tokio + rustls with ring and platform's native CA certificates
sqlx = { version = "0.8", features = [ "runtime-tokio", "tls-rustls-ring-native-roots" ] }
# tokio + rustls with aws-lc-rs
sqlx = { version = "0.8", features = [ "runtime-tokio", "tls-rustls-aws-lc-rs" ] }

# async-std (no TLS)
sqlx = { version = "0.8", features = [ "runtime-async-std" ] }
# async-std + native-tls
sqlx = { version = "0.8", features = [ "runtime-async-std", "tls-native-tls" ] }
# async-std + rustls with ring and WebPKI CA certificates
sqlx = { version = "0.8", features = [ "runtime-async-std", "tls-rustls-ring-webpki" ] }
# async-std + rustls with ring and platform's native CA certificates
sqlx = { version = "0.8", features = [ "runtime-async-std", "tls-rustls-ring-native-roots" ] }
# async-std + rustls with aws-lc-rs
sqlx = { version = "0.8", features = [ "runtime-async-std", "tls-rustls-aws-lc-rs" ] }
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Cargo Feature Flags&lt;/h4&gt; 
&lt;p&gt;For backward-compatibility reasons, the runtime and TLS features can either be chosen together as a single feature, or separately.&lt;/p&gt; 
&lt;p&gt;For forward compatibility, you should use the separate runtime and TLS features as the combination features may be removed in the future.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;runtime-async-std&lt;/code&gt;: Use the &lt;code&gt;async-std&lt;/code&gt; runtime without enabling a TLS backend.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;runtime-tokio&lt;/code&gt;: Use the &lt;code&gt;tokio&lt;/code&gt; runtime without enabling a TLS backend.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Actix-web is fully compatible with Tokio and so a separate runtime feature is no longer needed.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;tls-native-tls&lt;/code&gt;: Use the &lt;code&gt;native-tls&lt;/code&gt; TLS backend (OpenSSL on *nix, SChannel on Windows, Secure Transport on macOS).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;tls-rustls&lt;/code&gt;: Use the &lt;code&gt;rustls&lt;/code&gt; TLS backend (cross-platform backend, only supports TLS 1.2 and 1.3).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;postgres&lt;/code&gt;: Add support for the Postgres database server.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;mysql&lt;/code&gt;: Add support for the MySQL/MariaDB database server.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;mssql&lt;/code&gt;: Add support for the MSSQL database server.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;sqlite&lt;/code&gt;: Add support for the self-contained &lt;a href="https://sqlite.org/"&gt;SQLite&lt;/a&gt; database engine with SQLite bundled and statically-linked.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;sqlite-unbundled&lt;/code&gt;: The same as above (&lt;code&gt;sqlite&lt;/code&gt;), but link SQLite from the system instead of the bundled version.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Allows updating SQLite independently of SQLx or using forked versions.&lt;/li&gt; 
   &lt;li&gt;You must have SQLite installed on the system or provide a path to the library at build time. See &lt;a href="https://github.com/rusqlite/rusqlite?tab=readme-ov-file#notes-on-building-rusqlite-and-libsqlite3-sys"&gt;the &lt;code&gt;rusqlite&lt;/code&gt; README&lt;/a&gt; for details.&lt;/li&gt; 
   &lt;li&gt;May result in link errors if the SQLite version is too old. Version &lt;code&gt;3.20.0&lt;/code&gt; or newer is recommended.&lt;/li&gt; 
   &lt;li&gt;Can increase build time due to the use of bindgen.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;sqlite-preupdate-hook&lt;/code&gt;: enables SQLite's &lt;a href="https://sqlite.org/c3ref/preupdate_count.html"&gt;preupdate hook&lt;/a&gt; API.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Exposed as a separate feature because it's generally not enabled by default.&lt;/li&gt; 
   &lt;li&gt;Using this feature with &lt;code&gt;sqlite-unbundled&lt;/code&gt; may cause linker failures if the system SQLite version does not support it.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;any&lt;/code&gt;: Add support for the &lt;code&gt;Any&lt;/code&gt; database driver, which can proxy to a database driver at runtime.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;derive&lt;/code&gt;: Add support for the derive family macros, those are &lt;code&gt;FromRow&lt;/code&gt;, &lt;code&gt;Type&lt;/code&gt;, &lt;code&gt;Encode&lt;/code&gt;, &lt;code&gt;Decode&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;macros&lt;/code&gt;: Add support for the &lt;code&gt;query*!&lt;/code&gt; macros, which allows compile-time checked queries.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;migrate&lt;/code&gt;: Add support for the migration management and &lt;code&gt;migrate!&lt;/code&gt; macro, which allow compile-time embedded migrations.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;uuid&lt;/code&gt;: Add support for UUID.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;chrono&lt;/code&gt;: Add support for date and time types from &lt;code&gt;chrono&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;time&lt;/code&gt;: Add support for date and time types from &lt;code&gt;time&lt;/code&gt; crate (alternative to &lt;code&gt;chrono&lt;/code&gt;, which is preferred by &lt;code&gt;query!&lt;/code&gt; macro, if both enabled)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;bstr&lt;/code&gt;: Add support for &lt;code&gt;bstr::BString&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;bigdecimal&lt;/code&gt;: Add support for &lt;code&gt;NUMERIC&lt;/code&gt; using the &lt;code&gt;bigdecimal&lt;/code&gt; crate.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;rust_decimal&lt;/code&gt;: Add support for &lt;code&gt;NUMERIC&lt;/code&gt; using the &lt;code&gt;rust_decimal&lt;/code&gt; crate.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;ipnet&lt;/code&gt;: Add support for &lt;code&gt;INET&lt;/code&gt; and &lt;code&gt;CIDR&lt;/code&gt; (in postgres) using the &lt;code&gt;ipnet&lt;/code&gt; crate.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;ipnetwork&lt;/code&gt;: Add support for &lt;code&gt;INET&lt;/code&gt; and &lt;code&gt;CIDR&lt;/code&gt; (in postgres) using the &lt;code&gt;ipnetwork&lt;/code&gt; crate.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;json&lt;/code&gt;: Add support for &lt;code&gt;JSON&lt;/code&gt; and &lt;code&gt;JSONB&lt;/code&gt; (in postgres) using the &lt;code&gt;serde_json&lt;/code&gt; crate.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Offline mode is now always enabled. See &lt;a href="https://raw.githubusercontent.com/launchbadge/sqlx/main/sqlx-cli/README.md#enable-building-in-offline-mode-with-query"&gt;sqlx-cli/README.md&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;SQLx is not an ORM!&lt;/h2&gt; 
&lt;p&gt;SQLx supports &lt;strong&gt;compile-time checked queries&lt;/strong&gt;. It does not, however, do this by providing a Rust API or DSL (domain-specific language) for building queries. Instead, it provides macros that take regular SQL as input and ensure that it is valid for your database. The way this works is that SQLx connects to your development DB at compile time to have the database itself verify (and return some info on) your SQL queries. This has some potentially surprising implications:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Since SQLx never has to parse the SQL string itself, any syntax that the development DB accepts can be used (including things added by database extensions)&lt;/li&gt; 
 &lt;li&gt;Due to the different amount of information databases let you retrieve about queries, the extent of SQL verification you get from the query macros depends on the database&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;If you are looking for an (asynchronous) ORM,&lt;/strong&gt; you can check out our new &lt;a href="https://github.com/launchbadge/sqlx/wiki/Ecosystem#orms"&gt;Ecosystem wiki page&lt;/a&gt;!&lt;/p&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;p&gt;See the &lt;code&gt;examples/&lt;/code&gt; folder for more in-depth usage.&lt;/p&gt; 
&lt;h3&gt;Quickstart&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-rust"&gt;use sqlx::postgres::PgPoolOptions;
// use sqlx::mysql::MySqlPoolOptions;
// etc.

#[async_std::main] // Requires the `attributes` feature of `async-std`
// or #[tokio::main]
// or #[actix_web::main]
async fn main() -&amp;gt; Result&amp;lt;(), sqlx::Error&amp;gt; {
    // Create a connection pool
    //  for MySQL/MariaDB, use MySqlPoolOptions::new()
    //  for SQLite, use SqlitePoolOptions::new()
    //  etc.
    let pool = PgPoolOptions::new()
        .max_connections(5)
        .connect("postgres://postgres:password@localhost/test").await?;

    // Make a simple query to return the given parameter (use a question mark `?` instead of `$1` for MySQL/MariaDB)
    let row: (i64,) = sqlx::query_as("SELECT $1")
        .bind(150_i64)
        .fetch_one(&amp;amp;pool).await?;

    assert_eq!(row.0, 150);

    Ok(())
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Connecting&lt;/h3&gt; 
&lt;p&gt;A single connection can be established using any of the database connection types and calling &lt;code&gt;connect()&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-rust"&gt;use sqlx::Connection;

let conn = SqliteConnection::connect("sqlite::memory:").await?;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Generally, you will want to instead create a connection pool (&lt;code&gt;sqlx::Pool&lt;/code&gt;) for the application to regulate how many server-side connections it's using.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-rust"&gt;let pool = MySqlPool::connect("mysql://user:pass@host/database").await?;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Querying&lt;/h3&gt; 
&lt;p&gt;In SQL, queries can be separated into prepared (parameterized) or unprepared (simple). Prepared queries have their query plan &lt;em&gt;cached&lt;/em&gt;, use a binary mode of communication (lower bandwidth and faster decoding), and utilize parameters to avoid SQL injection. Unprepared queries are simple and intended only for use where a prepared statement will not work, such as various database commands (e.g., &lt;code&gt;PRAGMA&lt;/code&gt; or &lt;code&gt;SET&lt;/code&gt; or &lt;code&gt;BEGIN&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;SQLx supports all operations with both types of queries. In SQLx, a &lt;code&gt;&amp;amp;str&lt;/code&gt; is treated as an unprepared query, and a &lt;code&gt;Query&lt;/code&gt; or &lt;code&gt;QueryAs&lt;/code&gt; struct is treated as a prepared query.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-rust"&gt;// low-level, Executor trait
conn.execute("BEGIN").await?; // unprepared, simple query
conn.execute(sqlx::query("DELETE FROM table")).await?; // prepared, cached query
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We should prefer to use the high-level &lt;code&gt;query&lt;/code&gt; interface whenever possible. To make this easier, there are finalizers on the type to avoid the need to wrap with an executor.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-rust"&gt;sqlx::query("DELETE FROM table").execute(&amp;amp;mut conn).await?;
sqlx::query("DELETE FROM table").execute(&amp;amp;pool).await?;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The &lt;code&gt;execute&lt;/code&gt; query finalizer returns the number of affected rows, if any, and drops all received results. In addition, there are &lt;code&gt;fetch&lt;/code&gt;, &lt;code&gt;fetch_one&lt;/code&gt;, &lt;code&gt;fetch_optional&lt;/code&gt;, and &lt;code&gt;fetch_all&lt;/code&gt; to receive results.&lt;/p&gt; 
&lt;p&gt;The &lt;code&gt;Query&lt;/code&gt; type returned from &lt;code&gt;sqlx::query&lt;/code&gt; will return &lt;code&gt;Row&amp;lt;'conn&amp;gt;&lt;/code&gt; from the database. Column values can be accessed by ordinal or by name with &lt;code&gt;row.get()&lt;/code&gt;. As the &lt;code&gt;Row&lt;/code&gt; retains an immutable borrow on the connection, only one &lt;code&gt;Row&lt;/code&gt; may exist at a time.&lt;/p&gt; 
&lt;p&gt;The &lt;code&gt;fetch&lt;/code&gt; query finalizer returns a stream-like type that iterates through the rows in the result sets.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-rust"&gt;// provides `try_next`
use futures_util::TryStreamExt;
// provides `try_get`
use sqlx::Row;

let mut rows = sqlx::query("SELECT * FROM users WHERE email = ?")
    .bind(email)
    .fetch(&amp;amp;mut conn);

while let Some(row) = rows.try_next().await? {
    // map the row into a user-defined domain type
    let email: &amp;amp;str = row.try_get("email")?;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To assist with mapping the row into a domain type, one of two idioms may be used:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-rust"&gt;let mut stream = sqlx::query("SELECT * FROM users")
    .map(|row: PgRow| {
        // map the row into a user-defined domain type
    })
    .fetch(&amp;amp;mut conn);
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-rust"&gt;#[derive(sqlx::FromRow)]
struct User { name: String, id: i64 }

let mut stream = sqlx::query_as::&amp;lt;_, User&amp;gt;("SELECT * FROM users WHERE email = ? OR name = ?")
    .bind(user_email)
    .bind(user_name)
    .fetch(&amp;amp;mut conn);
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Instead of a stream of results, we can use &lt;code&gt;fetch_one&lt;/code&gt; or &lt;code&gt;fetch_optional&lt;/code&gt; to request one required or optional result from the database.&lt;/p&gt; 
&lt;h3&gt;Compile-time verification&lt;/h3&gt; 
&lt;p&gt;We can use the macro, &lt;code&gt;sqlx::query!&lt;/code&gt; to achieve compile-time syntactic and semantic verification of the SQL, with an output to an anonymous record type where each SQL column is a Rust field (using raw identifiers where needed).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-rust"&gt;let countries = sqlx::query!(
        "
SELECT country, COUNT(*) as count
FROM users
GROUP BY country
WHERE organization = ?
        ",
        organization
    )
    .fetch_all(&amp;amp;pool) // -&amp;gt; Vec&amp;lt;{ country: String, count: i64 }&amp;gt;
    .await?;

// countries[0].country
// countries[0].count
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Differences from &lt;code&gt;query()&lt;/code&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;The input (or bind) parameters must be given all at once (and they are compile-time validated to be the right number and the right type).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;The output type is an anonymous record. In the above example the type would be similar to:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-rust"&gt;{ country: String, count: i64 }
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;The &lt;code&gt;DATABASE_URL&lt;/code&gt; environment variable must be set at build time to a database which it can prepare queries against; the database does not have to contain any data but must be the same kind (MySQL, Postgres, etc.) and have the same schema as the database you will be connecting to at runtime.&lt;/p&gt; &lt;p&gt;For convenience, you can use &lt;a href="https://github.com/dotenv-rs/dotenv#examples"&gt;a &lt;code&gt;.env&lt;/code&gt; file&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt; to set DATABASE_URL so that you don't have to pass it every time:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;DATABASE_URL=mysql://localhost/my_database
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The biggest downside to &lt;code&gt;query!()&lt;/code&gt; is that the output type cannot be named (due to Rust not officially supporting anonymous records). To address that, there is a &lt;code&gt;query_as!()&lt;/code&gt; macro that is mostly identical except that you can name the output type.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-rust"&gt;// no traits are needed
struct Country { country: String, count: i64 }

let countries = sqlx::query_as!(Country,
        "
SELECT country, COUNT(*) as count
FROM users
GROUP BY country
WHERE organization = ?
        ",
        organization
    )
    .fetch_all(&amp;amp;pool) // -&amp;gt; Vec&amp;lt;Country&amp;gt;
    .await?;

// countries[0].country
// countries[0].count
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To avoid the need of having a development database around to compile the project even when no modifications (to the database-accessing parts of the code) are done, you can enable "offline mode" to cache the results of the SQL query analysis using the &lt;code&gt;sqlx&lt;/code&gt; command-line tool. See &lt;a href="https://raw.githubusercontent.com/launchbadge/sqlx/main/sqlx-cli/README.md#enable-building-in-offline-mode-with-query"&gt;sqlx-cli/README.md&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Compile-time verified queries do quite a bit of work at compile time. Incremental actions like &lt;code&gt;cargo check&lt;/code&gt; and &lt;code&gt;cargo build&lt;/code&gt; can be significantly faster when using an optimized build by putting the following in your &lt;code&gt;Cargo.toml&lt;/code&gt; (More information in the &lt;a href="https://doc.rust-lang.org/cargo/reference/profiles.html"&gt;Profiles section&lt;/a&gt; of The Cargo Book)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-toml"&gt;[profile.dev.package.sqlx-macros]
opt-level = 3
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt; The &lt;code&gt;dotenv&lt;/code&gt; crate itself appears abandoned as of &lt;a href="https://github.com/dotenv-rs/dotenv/issues/74"&gt;December 2021&lt;/a&gt; so we now use the &lt;code&gt;dotenvy&lt;/code&gt; crate instead. The file format is the same.&lt;/p&gt; 
&lt;h2&gt;Safety&lt;/h2&gt; 
&lt;p&gt;This crate uses &lt;code&gt;#![forbid(unsafe_code)]&lt;/code&gt; to ensure everything is implemented in 100% Safe Rust.&lt;/p&gt; 
&lt;p&gt;If the &lt;code&gt;sqlite&lt;/code&gt; feature is enabled, this is downgraded to &lt;code&gt;#![deny(unsafe_code)]&lt;/code&gt; with &lt;code&gt;#![allow(unsafe_code)]&lt;/code&gt; on the &lt;code&gt;sqlx::sqlite&lt;/code&gt; module. There are several places where we interact with the C SQLite API. We try to document each call for the invariants we're assuming. We absolutely welcome auditing of, and feedback on, our unsafe code usage.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Licensed under either of&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Apache License, Version 2.0 (&lt;a href="https://raw.githubusercontent.com/launchbadge/sqlx/main/LICENSE-APACHE"&gt;LICENSE-APACHE&lt;/a&gt; or &lt;a href="http://www.apache.org/licenses/LICENSE-2.0"&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;MIT license (&lt;a href="https://raw.githubusercontent.com/launchbadge/sqlx/main/LICENSE-MIT"&gt;LICENSE-MIT&lt;/a&gt; or &lt;a href="http://opensource.org/licenses/MIT"&gt;http://opensource.org/licenses/MIT&lt;/a&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;at your option.&lt;/p&gt; 
&lt;h2&gt;Contribution&lt;/h2&gt; 
&lt;p&gt;Unless you explicitly state otherwise, any Contribution intentionally submitted for inclusion in the work by you, as defined in the Apache-2.0 license, shall be dual licensed as above, without any additional terms or conditions.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>MystenLabs/sui</title>
      <link>https://github.com/MystenLabs/sui</link>
      <description>&lt;p&gt;Sui, a next-generation smart contract platform with high throughput, low latency, and an asset-oriented programming model powered by the Move programming language&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/MystenLabs/sui/refs/heads/main/docs/site/static/img/logo.svg?sanitize=true" alt="Logo" width="100" height="100" /&gt; &lt;/p&gt; 
&lt;h1&gt;Welcome to Sui&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/MystenLabs/sui/releases/latest"&gt;&lt;img src="https://img.shields.io/github/v/release/MystenLabs/sui.svg?sort=semver" alt="Github release" /&gt;&lt;/a&gt; &lt;a href="https://github.com/MystenLabs/sui/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/MystenLabs/sui" alt="License" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://sui.io"&gt;Sui&lt;/a&gt; is a next-generation smart contract platform with high throughput, low latency, and an asset-oriented programming model powered by the &lt;a href="https://github.com/MystenLabs/awesome-move"&gt;Move programming language&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Sui Highlights&lt;/h2&gt; 
&lt;p&gt;Sui offers the following benefits and capabilities:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Unmatched scalability, instant settlement&lt;/li&gt; 
 &lt;li&gt;A safe smart contract language accessible to mainstream developers&lt;/li&gt; 
 &lt;li&gt;Ability to define rich and composable on-chain assets&lt;/li&gt; 
 &lt;li&gt;Better user experience for web3 apps&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Sui is the only blockchain today that can scale with the growth of web3 while achieving industry-leading performance, cost, programmability, and usability. As Sui approaches Mainnet launch, it will demonstrate capacity beyond the transaction processing capabilities of established systems – traditional and blockchain alike. Sui is the first internet-scale programmable blockchain platform, a foundational layer for web3.&lt;/p&gt; 
&lt;h2&gt;Sui Architecture&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-mermaid"&gt;flowchart LR
    CC(CLI Client) --&amp;gt; ClientService
    RC(Rest Client) --&amp;gt; ClientService
    RPCC(RPC Client) --&amp;gt; ClientService
    ClientService --&amp;gt; AuthorityAggregator
    AuthorityAggregator --&amp;gt; AC1[AuthorityClient] &amp;amp; AC2[AuthorityClient]
    subgraph Authority1
      AS[AuthorityState]
    end
    subgraph Authority2
      AS2[AuthorityState]
    end
    AC1 &amp;lt;==&amp;gt;|Network TCP| Authority1
    AC2 &amp;lt;==&amp;gt;|Network TCP| Authority2
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Sui Overview&lt;/h2&gt; 
&lt;p&gt;Sui is a smart contract platform maintained by a permissionless set of authorities that play a role similar to validators or miners in other blockchain systems.&lt;/p&gt; 
&lt;p&gt;Sui offers scalability and unprecedented low-latency for common use cases. Sui makes the vast majority of transactions processable in parallel, which makes better use of processing resources, and offers the option to increase throughput with more resources. Sui forgoes consensus to instead use simpler and lower-latency primitives for common use cases, such as payment transactions and asset transfers. This is unprecedented in the blockchain world and enables a number of new latency-sensitive distributed applications, ranging from gaming to retail payment at physical points of sale.&lt;/p&gt; 
&lt;p&gt;Sui is written in &lt;a href="https://www.rust-lang.org"&gt;Rust&lt;/a&gt; and supports smart contracts written in the &lt;a href="https://github.com/move-language/move"&gt;Move programming language&lt;/a&gt; to define assets that may have an owner. Move programs define operations on these assets including custom rules for their creation, the transfer of these assets to new owners, and operations that mutate assets.&lt;/p&gt; 
&lt;p&gt;Sui has a native token called SUI, with a fixed supply. The SUI token is used to pay for gas, and is also used as &lt;a href="https://learn.bybit.com/blockchain/delegated-proof-of-stake-dpos/"&gt;delegated stake on authorities&lt;/a&gt; within an epoch. The voting power of authorities within this epoch is a function of this delegated stake. Authorities are periodically reconfigured according to the stake delegated to them. In any epoch, the set of authorities is &lt;a href="https://pmg.csail.mit.edu/papers/osdi99.pdf"&gt;Byzantine fault tolerant&lt;/a&gt;. At the end of the epoch, fees collected through all transactions processed are distributed to authorities according to their contribution to the operation of the system. Authorities can in turn share some of the fees as rewards to users that delegated stakes to them.&lt;/p&gt; 
&lt;p&gt;Sui is supported by several cutting-edge &lt;a href="https://github.com/MystenLabs/sui/raw/main/docs/content/concepts/research-papers.mdx"&gt;peer-reviewed studies&lt;/a&gt; and extensive years of open-source development.&lt;/p&gt; 
&lt;h2&gt;More About Sui&lt;/h2&gt; 
&lt;p&gt;Use the following links to learn more about Sui and the Sui ecosystem:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Learn more about working with Sui in the &lt;a href="https://docs.sui.io/"&gt;Sui Documentation&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Join the Sui community on &lt;a href="https://discord.gg/sui"&gt;Sui Discord&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Find out more about the Sui ecosystem on the &lt;a href="https://sui.io/resources/"&gt;Sui Resources&lt;/a&gt; page.&lt;/li&gt; 
 &lt;li&gt;Review information about Sui governance, &lt;a href="https://suifoundation.org/decentralization"&gt;decentralization&lt;/a&gt;, and &lt;a href="https://sui.io/grants-hub"&gt;Developer Grants Program&lt;/a&gt; on the &lt;a href="https://sui.io/about"&gt;Sui Foundation&lt;/a&gt; site.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;How to Contribute&lt;/h2&gt; 
&lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/MystenLabs/sui/main/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt; for details on how to contribute to Sui.&lt;/p&gt; 
&lt;h2&gt;Code of Conduct&lt;/h2&gt; 
&lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/MystenLabs/sui/main/CODE_OF_CONDUCT.MD"&gt;Code of Conduct&lt;/a&gt; for details on our code of conduct.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/MystenLabs/sui/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for more details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>tensorzero/tensorzero</title>
      <link>https://github.com/tensorzero/tensorzero</link>
      <description>&lt;p&gt;TensorZero is an open-source stack for industrial-grade LLM applications. It unifies an LLM gateway, observability, optimization, evaluation, and experimentation.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;
 &lt;picture&gt;
  &lt;img src="https://github.com/user-attachments/assets/47d67430-386d-4675-82ad-d4734d3262d9" alt="TensorZero Logo" width="128" height="128" /&gt;
 &lt;/picture&gt;&lt;/p&gt; 
&lt;h1&gt;TensorZero&lt;/h1&gt; 
&lt;p&gt;
 &lt;picture&gt;
  &lt;img src="https://www.tensorzero.com/github-trending-badge.svg?sanitize=true" alt="#1 Repository Of The Day" /&gt;
 &lt;/picture&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;TensorZero is an open-source stack for &lt;em&gt;industrial-grade LLM applications&lt;/em&gt;:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Gateway:&lt;/strong&gt; access every LLM provider through a unified API, built for performance (&amp;lt;1ms p99 latency)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Observability:&lt;/strong&gt; store inferences and feedback in your database, available programmatically or in the UI&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Optimization:&lt;/strong&gt; collect metrics and human feedback to optimize prompts, models, and inference strategies&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Evaluation:&lt;/strong&gt; benchmark individual inferences or end-to-end workflows using heuristics, LLM judges, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Experimentation:&lt;/strong&gt; ship with confidence with built-in A/B testing, routing, fallbacks, retries, etc.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Take what you need, adopt incrementally, and complement with other tools.&lt;/p&gt; 
&lt;p&gt;
 &lt;video src="https://github.com/user-attachments/assets/04a8466e-27d8-4189-b305-e7cecb6881ee"&gt;&lt;/video&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p align="center"&gt; &lt;b&gt;&lt;a href="https://www.tensorzero.com/" target="_blank"&gt;Website&lt;/a&gt;&lt;/b&gt; · &lt;b&gt;&lt;a href="https://www.tensorzero.com/docs" target="_blank"&gt;Docs&lt;/a&gt;&lt;/b&gt; · &lt;b&gt;&lt;a href="https://www.x.com/tensorzero" target="_blank"&gt;Twitter&lt;/a&gt;&lt;/b&gt; · &lt;b&gt;&lt;a href="https://www.tensorzero.com/slack" target="_blank"&gt;Slack&lt;/a&gt;&lt;/b&gt; · &lt;b&gt;&lt;a href="https://www.tensorzero.com/discord" target="_blank"&gt;Discord&lt;/a&gt;&lt;/b&gt; &lt;br /&gt; &lt;br /&gt; &lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/quickstart" target="_blank"&gt;Quick Start (5min)&lt;/a&gt;&lt;/b&gt; · &lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/deployment" target="_blank"&gt;Deployment Guide&lt;/a&gt;&lt;/b&gt; · &lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/api-reference" target="_blank"&gt;API Reference&lt;/a&gt;&lt;/b&gt; · &lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/deployment" target="_blank"&gt;Configuration Reference&lt;/a&gt;&lt;/b&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td width="30%" valign="top"&gt;&lt;b&gt;What is TensorZero?&lt;/b&gt;&lt;/td&gt; 
   &lt;td width="70%" valign="top"&gt;TensorZero is an open-source stack for industrial-grade LLM applications. It unifies an LLM gateway, observability, optimization, evaluation, and experimentation.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="30%" valign="top"&gt;&lt;b&gt;How is TensorZero different from other LLM frameworks?&lt;/b&gt;&lt;/td&gt; 
   &lt;td width="70%" valign="top"&gt; 1. TensorZero enables you to optimize complex LLM applications based on production metrics and human feedback.&lt;br /&gt; 2. TensorZero supports the needs of industrial-grade LLM applications: low latency, high throughput, type safety, self-hosted, GitOps, customizability, etc.&lt;br /&gt; 3. TensorZero unifies the entire LLMOps stack, creating compounding benefits. For example, LLM evaluations can be used for fine-tuning models alongside AI judges. &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="30%" valign="top"&gt;&lt;b&gt;Can I use TensorZero with ___?&lt;/b&gt;&lt;/td&gt; 
   &lt;td width="70%" valign="top"&gt;Yes. Every major programming language is supported. You can use TensorZero with our Python client, any OpenAI SDK or OpenAI-compatible client, or our HTTP API.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="30%" valign="top"&gt;&lt;b&gt;Is TensorZero production-ready?&lt;/b&gt;&lt;/td&gt; 
   &lt;td width="70%" valign="top"&gt;Yes. Here's a case study: &lt;b&gt;&lt;a href="https://www.tensorzero.com/blog/case-study-automating-code-changelogs-at-a-large-bank-with-llms"&gt;Automating Code Changelogs at a Large Bank with LLMs&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="30%" valign="top"&gt;&lt;b&gt;How much does TensorZero cost?&lt;/b&gt;&lt;/td&gt; 
   &lt;td width="70%" valign="top"&gt;Nothing. TensorZero is 100% self-hosted and open-source. There are no paid features.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="30%" valign="top"&gt;&lt;b&gt;Who is building TensorZero?&lt;/b&gt;&lt;/td&gt; 
   &lt;td width="70%" valign="top"&gt;Our technical team includes a former Rust compiler maintainer, machine learning researchers (Stanford, CMU, Oxford, Columbia) with thousands of citations, and the chief product officer of a decacorn startup. We're backed by the same investors as leading open-source projects (e.g. ClickHouse, CockroachDB) and AI labs (e.g. OpenAI, Anthropic). See our &lt;b&gt;&lt;a href="https://www.tensorzero.com/blog/tensorzero-raises-7-3m-seed-round-to-build-an-open-source-stack-for-industrial-grade-llm-applications/"&gt;$7.3M seed round announcement&lt;/a&gt;&lt;/b&gt; and &lt;b&gt;&lt;a href="https://venturebeat.com/ai/tensorzero-nabs-7-3m-seed-to-solve-the-messy-world-of-enterprise-llm-development/" rel="nofollow" target="_blank"&gt;coverage from VentureBeat&lt;/a&gt;&lt;/b&gt;. We're &lt;b&gt;&lt;a href="https://www.tensorzero.com/jobs" rel="nofollow" target="_blank"&gt;hiring in NYC&lt;/a&gt;&lt;/b&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="30%" valign="top"&gt;&lt;b&gt;How do I get started?&lt;/b&gt;&lt;/td&gt; 
   &lt;td width="70%" valign="top"&gt;You can adopt TensorZero incrementally. Our &lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/quickstart"&gt;Quick Start&lt;/a&gt;&lt;/b&gt; goes from a vanilla OpenAI wrapper to a production-ready LLM application with observability and fine-tuning in just 5 minutes.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;h3&gt;🌐 LLM Gateway&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Integrate with TensorZero once and access every major LLM provider.&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Access every major LLM provider (API or self-hosted) through a single unified API&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Infer with streaming, tool use, structured generation (JSON mode), batch, embeddings, multimodal (VLMs), file inputs, caching, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Define prompt templates and schemas to enforce a consistent, typed interface between your application and the LLMs&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Satisfy extreme throughput and latency needs, thanks to 🦀 Rust: &amp;lt;1ms p99 latency overhead at 10k+ QPS&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Integrate using our Python client, any OpenAI SDK or OpenAI-compatible client, or our HTTP API (use any programming language)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Ensure high availability with routing, retries, fallbacks, load balancing, granular timeouts, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Soon: rate limits, spend tracking and budgeting, service accounts&lt;/li&gt; 
&lt;/ul&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt;&lt;/tr&gt; 
  &lt;!-- flip highlight order --&gt; 
  &lt;tr&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;b&gt;Model Providers&lt;/b&gt;&lt;/td&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;b&gt;Features&lt;/b&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%" align="left" valign="top"&gt; &lt;p&gt; The TensorZero Gateway natively supports: &lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/anthropic"&gt;Anthropic&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/aws-bedrock"&gt;AWS Bedrock&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/aws-sagemaker"&gt;AWS SageMaker&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/azure"&gt;Azure OpenAI Service&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/deepseek"&gt;DeepSeek&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/fireworks"&gt;Fireworks&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/gcp-vertex-ai-anthropic"&gt;GCP Vertex AI Anthropic&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/gcp-vertex-ai-gemini"&gt;GCP Vertex AI Gemini&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/google-ai-studio-gemini"&gt;Google AI Studio (Gemini API)&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/groq"&gt;Groq&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/hyperbolic"&gt;Hyperbolic&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/mistral"&gt;Mistral&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/openai"&gt;OpenAI&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/openrouter"&gt;OpenRouter&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/sglang"&gt;SGLang&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/tgi"&gt;TGI&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/together"&gt;Together AI&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/vllm"&gt;vLLM&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/xai"&gt;xAI (Grok)&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;p&gt; &lt;em&gt; Need something else? Your provider is most likely supported because TensorZero integrates with &lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/providers/openai-compatible"&gt;any OpenAI-compatible API (e.g. Ollama)&lt;/a&gt;&lt;/b&gt;. &lt;/em&gt; &lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%" align="left" valign="top"&gt; &lt;p&gt; The TensorZero Gateway supports advanced features like: &lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/retries-fallbacks"&gt;Retries &amp;amp; Fallbacks&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations"&gt;Inference-Time Optimizations&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/prompt-templates-schemas"&gt;Prompt Templates &amp;amp; Schemas&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/experimentation/"&gt;Experimentation (A/B Testing)&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/configuration-reference"&gt;Configuration-as-Code (GitOps)&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/batch-inference"&gt;Batch Inference&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/multimodal-inference"&gt;Multimodal Inference (VLMs)&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/inference-caching"&gt;Inference Caching&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/metrics-feedback"&gt;Metrics &amp;amp; Feedback&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/episodes"&gt;Multi-Step LLM Workflows (Episodes)&lt;/a&gt;&lt;/b&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;em&gt;&amp;amp; a lot more...&lt;/em&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;p&gt; The TensorZero Gateway is written in Rust 🦀 with &lt;b&gt;performance&lt;/b&gt; in mind (&amp;lt;1ms p99 latency overhead @ 10k QPS). See &lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/benchmarks"&gt;Benchmarks&lt;/a&gt;&lt;/b&gt;.&lt;br /&gt; &lt;/p&gt; &lt;p&gt; You can run inference using the &lt;b&gt;TensorZero client&lt;/b&gt; (recommended), the &lt;b&gt;OpenAI client&lt;/b&gt;, or the &lt;b&gt;HTTP API&lt;/b&gt;. &lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;br /&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;b&gt;Usage: Python — TensorZero Client (Recommended)&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;You can access any provider using the TensorZero Python client.&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;code&gt;pip install tensorzero&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;Optional: Set up the TensorZero configuration.&lt;/li&gt; 
  &lt;li&gt;Run inference:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from tensorzero import TensorZeroGateway  # or AsyncTensorZeroGateway


with TensorZeroGateway.build_embedded(clickhouse_url="...", config_file="...") as client:
    response = client.inference(
        model_name="openai::gpt-4o-mini",
        # Try other providers easily: "anthropic::claude-3-7-sonnet-20250219"
        input={
            "messages": [
                {
                    "role": "user",
                    "content": "Write a haiku about artificial intelligence.",
                }
            ]
        },
    )
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;See &lt;strong&gt;&lt;a href="https://www.tensorzero.com/docs/quickstart"&gt;Quick Start&lt;/a&gt;&lt;/strong&gt; for more information.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Usage: Python — OpenAI Client&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;You can access any provider using the OpenAI Python client with TensorZero.&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;code&gt;pip install tensorzero&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;Optional: Set up the TensorZero configuration.&lt;/li&gt; 
  &lt;li&gt;Run inference:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from openai import OpenAI  # or AsyncOpenAI
from tensorzero import patch_openai_client

client = OpenAI()

patch_openai_client(
    client,
    clickhouse_url="http://chuser:chpassword@localhost:8123/tensorzero",
    config_file="config/tensorzero.toml",
    async_setup=False,
)

response = client.chat.completions.create(
    model="tensorzero::model_name::openai::gpt-4o-mini",
    # Try other providers easily: "tensorzero::model_name::anthropic::claude-3-7-sonnet-20250219"
    messages=[
        {
            "role": "user",
            "content": "Write a haiku about artificial intelligence.",
        }
    ],
)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;See &lt;strong&gt;&lt;a href="https://www.tensorzero.com/docs/quickstart"&gt;Quick Start&lt;/a&gt;&lt;/strong&gt; for more information.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Usage: JavaScript / TypeScript (Node) — OpenAI Client&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;You can access any provider using the OpenAI Node client with TensorZero.&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Deploy &lt;code&gt;tensorzero/gateway&lt;/code&gt; using Docker. &lt;strong&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/deployment"&gt;Detailed instructions →&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;li&gt;Set up the TensorZero configuration.&lt;/li&gt; 
  &lt;li&gt;Run inference:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-ts"&gt;import OpenAI from "openai";

const client = new OpenAI({
  baseURL: "http://localhost:3000/openai/v1",
});

const response = await client.chat.completions.create({
  model: "tensorzero::model_name::openai::gpt-4o-mini",
  // Try other providers easily: "tensorzero::model_name::anthropic::claude-3-7-sonnet-20250219"
  messages: [
    {
      role: "user",
      content: "Write a haiku about artificial intelligence.",
    },
  ],
});
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;See &lt;strong&gt;&lt;a href="https://www.tensorzero.com/docs/quickstart"&gt;Quick Start&lt;/a&gt;&lt;/strong&gt; for more information.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Usage: Other Languages &amp;amp; Platforms — HTTP API&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;TensorZero supports virtually any programming language or platform via its HTTP API.&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Deploy &lt;code&gt;tensorzero/gateway&lt;/code&gt; using Docker. &lt;strong&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/deployment"&gt;Detailed instructions →&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;li&gt;Optional: Set up the TensorZero configuration.&lt;/li&gt; 
  &lt;li&gt;Run inference:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;curl -X POST "http://localhost:3000/inference" \
  -H "Content-Type: application/json" \
  -d '{
    "model_name": "openai::gpt-4o-mini",
    "input": {
      "messages": [
        {
          "role": "user",
          "content": "Write a haiku about artificial intelligence."
        }
      ]
    }
  }'
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;See &lt;strong&gt;&lt;a href="https://www.tensorzero.com/docs/quickstart"&gt;Quick Start&lt;/a&gt;&lt;/strong&gt; for more information.&lt;/p&gt; 
&lt;/details&gt; 
&lt;br /&gt; 
&lt;h3&gt;🔍 LLM Observability&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Zoom in to debug individual API calls, or zoom out to monitor metrics across models and prompts over time — all using the open-source TensorZero UI.&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Store inferences and feedback (metrics, human edits, etc.) in your own database&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Dive into individual inferences or high-level aggregate patterns using the TensorZero UI or programmatically&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Build datasets for optimization, evaluation, and other workflows&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Replay historical inferences with new prompts, models, inference strategies, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Export OpenTelemetry (OTLP) traces to your favorite general-purpose observability tool&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Soon: AI-assisted debugging and root cause analysis; AI-assisted data labeling&lt;/li&gt; 
&lt;/ul&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt;&lt;/tr&gt; 
  &lt;!-- flip highlight order --&gt; 
  &lt;tr&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;b&gt;Observability » UI&lt;/b&gt;&lt;/td&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;b&gt;Observability » Programmatic&lt;/b&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;
    &lt;video src="https://github.com/user-attachments/assets/a23e4c95-18fa-482c-8423-6078fb4cf285"&gt;&lt;/video&gt;&lt;/td&gt; 
   &lt;td width="50%" align="left" valign="middle"&gt; &lt;pre&gt;&lt;code class="language-python"&gt;t0.experimental_list_inferences(
  function_name="sales_agent",
  variant_name="qwen3-promptv2",
  filters=BooleanMetricFilter(
      metric_name="converted_sale",
      value=True,
  ),
  order_by=[OrderBy(by="timestamp", direction="DESC")],
  limit=100_000,
  # ... and more ...
)
&lt;/code&gt;&lt;/pre&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;br /&gt; 
&lt;h3&gt;📈 LLM Optimization&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Send production metrics and human feedback to easily optimize your prompts, models, and inference strategies — using the UI or programmatically.&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Optimize your models with supervised fine-tuning, RLHF, and other techniques&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Optimize your prompts with automated prompt engineering algorithms like MIPROv2&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Optimize your inference strategy with dynamic in-context learning, chain of thought, best/mixture-of-N sampling, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Enable a feedback loop for your LLMs: a data &amp;amp; learning flywheel turning production data into smarter, faster, and cheaper models&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Soon: synthetic data generation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Model Optimization&lt;/h4&gt; 
&lt;p&gt;Optimize closed-source and open-source models using supervised fine-tuning (SFT) and preference fine-tuning (DPO).&lt;/p&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt;&lt;/tr&gt; 
  &lt;!-- flip highlight order --&gt; 
  &lt;tr&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;b&gt;Supervised Fine-tuning — UI&lt;/b&gt;&lt;/td&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;b&gt;Preference Fine-tuning (DPO) — Jupyter Notebook&lt;/b&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;
    &lt;video src="https://github.com/user-attachments/assets/82f76be7-5e02-4ada-b503-69dfa209a442"&gt;&lt;/video&gt;&lt;/td&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;img src="https://github.com/user-attachments/assets/a67a0634-04a7-42b0-b934-9130cb7cdf51" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h4&gt;Inference-Time Optimization&lt;/h4&gt; 
&lt;p&gt;Boost performance by dynamically updating your prompts with relevant examples, combining responses from multiple inferences, and more.&lt;/p&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt;&lt;/tr&gt; 
  &lt;!-- flip highlight order --&gt; 
  &lt;tr&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#best-of-n-sampling"&gt;Best-of-N Sampling&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#mixture-of-n-sampling"&gt;Mixture-of-N Sampling&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;img src="https://github.com/user-attachments/assets/c0edfa4c-713c-4996-9964-50c0d26e6970" /&gt;&lt;/td&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;img src="https://github.com/user-attachments/assets/75b5bf05-4c1f-43c4-b158-d69d1b8d05be" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#dynamic-in-context-learning-dicl"&gt;Dynamic In-Context Learning (DICL)&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#chain-of-thought-cot"&gt;Chain-of-Thought (CoT)&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;img src="https://github.com/user-attachments/assets/d8489e92-ce93-46ac-9aab-289ce19bb67d" /&gt;&lt;/td&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;img src="https://github.com/user-attachments/assets/ea13d73c-76a4-4e0c-a35b-0c648f898311" height="320" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;p&gt;&lt;em&gt;More coming soon...&lt;/em&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;h4&gt;Prompt Optimization&lt;/h4&gt; 
&lt;p&gt;Optimize your prompts programmatically using research-driven optimization techniques.&lt;/p&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt;&lt;/tr&gt; 
  &lt;!-- flip highlight order --&gt; 
  &lt;tr&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;b&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#best-of-n-sampling"&gt;MIPROv2&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;b&gt;&lt;a href="https://github.com/tensorzero/tensorzero/tree/main/examples/gsm8k-custom-recipe-dspy"&gt;DSPy Integration&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;img src="https://github.com/user-attachments/assets/d81a7c37-382f-4c46-840f-e6c2593301db" alt="MIPROv2 diagram" /&gt;&lt;/td&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt; TensorZero comes with several optimization recipes, but you can also easily create your own. This example shows how to optimize a TensorZero function using an arbitrary tool — here, DSPy, a popular library for automated prompt engineering. &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;p&gt;&lt;em&gt;More coming soon...&lt;/em&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;h3&gt;📊 LLM Evaluation&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Compare prompts, models, and inference strategies using evaluations powered by heuristics and LLM judges.&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Evaluate individual inferences with &lt;em&gt;static evaluations&lt;/em&gt; powered by heuristics or LLM judges (≈ unit tests for LLMs)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Evaluate end-to-end workflows with &lt;em&gt;dynamic evaluations&lt;/em&gt; with complete flexibility (≈ integration tests for LLMs)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Optimize LLM judges just like any other TensorZero function to align them to human preferences&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Soon: more built-in evaluators; headless evaluations&lt;/li&gt; 
&lt;/ul&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt;&lt;/tr&gt; 
  &lt;!-- flip highlight order --&gt; 
  &lt;tr&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;b&gt;Evaluation » UI&lt;/b&gt;&lt;/td&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;b&gt;Evaluation » CLI&lt;/b&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%" align="center" valign="middle"&gt;&lt;img src="https://github.com/user-attachments/assets/f4bf54e3-1b63-46c8-be12-2eaabf615699" /&gt;&lt;/td&gt; 
   &lt;td width="50%" align="left" valign="middle"&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;docker compose run --rm evaluations \
  --evaluation-name extract_data \
  --dataset-name hard_test_cases \
  --variant-name gpt_4o \
  --concurrency 5&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;Run ID: 01961de9-c8a4-7c60-ab8d-15491a9708e4
Number of datapoints: 100
██████████████████████████████████████ 100/100
exact_match: 0.83 ± 0.03
semantic_match: 0.98 ± 0.01
item_count: 7.15 ± 0.39&lt;/code&gt;&lt;/pre&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;🧪 LLM Experimentation&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Ship with confidence with built-in A/B testing, routing, fallbacks, retries, etc.&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Ship with confidence with built-in A/B testing for models, prompts, providers, hyperparameters, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Enforce principled experiments (RCTs) in complex workflows, including multi-turn and compound LLM systems&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Soon: multi-armed bandits; AI-managed experiments&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&amp;amp; more!&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Build with an open-source stack well-suited for prototypes but designed from the ground up to support the most complex LLM applications and deployments.&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Build simple applications or massive deployments with GitOps-friendly orchestration&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Extend TensorZero with built-in escape hatches, programmatic-first usage, direct database access, and more&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Integrate with third-party tools: specialized observability and evaluations, model providers, agent orchestration frameworks, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Iterate quickly by experimenting with prompts interactively using the Playground UI&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Demo&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Watch LLMs get better at data extraction in real-time with TensorZero!&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#dynamic-in-context-learning-dicl"&gt;Dynamic in-context learning (DICL)&lt;/a&gt;&lt;/strong&gt; is a powerful inference-time optimization available out of the box with TensorZero. It enhances LLM performance by automatically incorporating relevant historical examples into the prompt, without the need for model fine-tuning.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/4df1022e-886e-48c2-8f79-6af3cdad79cb"&gt;https://github.com/user-attachments/assets/4df1022e-886e-48c2-8f79-6af3cdad79cb&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Get Started&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Start building today.&lt;/strong&gt; The &lt;strong&gt;&lt;a href="https://www.tensorzero.com/docs/quickstart"&gt;Quick Start&lt;/a&gt;&lt;/strong&gt; shows it's easy to set up an LLM application with TensorZero.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Questions?&lt;/strong&gt; Ask us on &lt;strong&gt;&lt;a href="https://www.tensorzero.com/slack"&gt;Slack&lt;/a&gt;&lt;/strong&gt; or &lt;strong&gt;&lt;a href="https://www.tensorzero.com/discord"&gt;Discord&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Using TensorZero at work?&lt;/strong&gt; Email us at &lt;strong&gt;&lt;a href="mailto:hello@tensorzero.com"&gt;hello@tensorzero.com&lt;/a&gt;&lt;/strong&gt; to set up a Slack or Teams channel with your team (free).&lt;/p&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;p&gt;We are working on a series of &lt;strong&gt;complete runnable examples&lt;/strong&gt; illustrating TensorZero's data &amp;amp; learning flywheel.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/tensorzero/tensorzero/tree/main/examples/data-extraction-ner"&gt;Optimizing Data Extraction (NER) with TensorZero&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;This example shows how to use TensorZero to optimize a data extraction pipeline. We demonstrate techniques like fine-tuning and dynamic in-context learning (DICL). In the end, an optimized GPT-4o Mini model outperforms GPT-4o on this task — at a fraction of the cost and latency — using a small amount of training data.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/tensorzero/tensorzero/tree/main/examples/rag-retrieval-augmented-generation/simple-agentic-rag/"&gt;Agentic RAG — Multi-Hop Question Answering with LLMs&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;This example shows how to build a multi-hop retrieval agent using TensorZero. The agent iteratively searches Wikipedia to gather information, and decides when it has enough context to answer a complex question.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/tensorzero/tensorzero/tree/main/examples/haiku-hidden-preferences"&gt;Writing Haikus to Satisfy a Judge with Hidden Preferences&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;This example fine-tunes GPT-4o Mini to generate haikus tailored to a specific taste. You'll see TensorZero's "data flywheel in a box" in action: better variants leads to better data, and better data leads to better variants. You'll see progress by fine-tuning the LLM multiple times.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/tensorzero/tensorzero/tree/main/examples/multimodal-vision-finetuning"&gt;Image Data Extraction — Multimodal (Vision) Fine-tuning&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;This example shows how to fine-tune multimodal models (VLMs) like GPT-4o to improve their performance on vision-language tasks. Specifically, we'll build a system that categorizes document images (screenshots of computer science research papers).&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/tensorzero/tensorzero/tree/main/examples/chess-puzzles/"&gt;Improving LLM Chess Ability with Best-of-N Sampling&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;This example showcases how best-of-N sampling can significantly enhance an LLM's chess-playing abilities by selecting the most promising moves from multiple generated options.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/tensorzero/tensorzero/tree/main/examples/gsm8k-custom-recipe-dspy"&gt;Improving Math Reasoning with a Custom Recipe for Automated Prompt Engineering (DSPy)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;TensorZero provides a number of pre-built optimization recipes covering common LLM engineering workflows. But you can also easily create your own recipes and workflows! This example shows how to optimize a TensorZero function using an arbitrary tool — here, DSPy.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;em&gt;&amp;amp; many more on the way!&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Automattic/harper</title>
      <link>https://github.com/Automattic/harper</link>
      <description>&lt;p&gt;Offline, privacy-first grammar checker. Fast, open-source, Rust-powered&lt;/p&gt;&lt;hr&gt;&lt;div id="header" align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/Automattic/harper/master/logo.svg?sanitize=true" width="400px" /&gt; 
 &lt;h1&gt;Harper&lt;/h1&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;a href="https://github.com/automattic/harper/actions/workflows/build_harper_binaries.yml"&gt;&lt;img src="https://github.com/automattic/harper/actions/workflows/build_harper_binaries.yml/badge.svg?sanitize=true" alt="Harper Binaries" /&gt;&lt;/a&gt; &lt;a href="https://github.com/automattic/harper/actions/workflows/build_web.yml"&gt;&lt;img src="https://github.com/automattic/harper/actions/workflows/build_web.yml/badge.svg?sanitize=true" alt="Website" /&gt;&lt;/a&gt; &lt;a href="https://github.com/automattic/harper/actions/workflows/precommit.yml"&gt;&lt;img src="https://github.com/automattic/harper/actions/workflows/precommit.yml/badge.svg?sanitize=true" alt="Precommit" /&gt;&lt;/a&gt; &lt;a href="https://crates.io/crates/harper-ls"&gt;&lt;img src="https://img.shields.io/crates/v/harper-ls" alt="Crates.io" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/npm/v/harper.js" alt="NPM Version" /&gt;&lt;/p&gt; 
&lt;p&gt;Harper is an English grammar checker designed to be &lt;em&gt;just right.&lt;/em&gt; I created it after years of dealing with the shortcomings of the competition.&lt;/p&gt; 
&lt;p&gt;Grammarly was too expensive and too overbearing. Its suggestions lacked context, and were often just plain &lt;em&gt;wrong&lt;/em&gt;. Not to mention: it's a privacy nightmare. Everything you write with Grammarly is sent to their servers. Their privacy policy claims they don't sell the data, but that doesn't mean they don't use it to train large language models and god knows what else. Not only that, but the round-trip-time of the network request makes revising your work all the more tedious.&lt;/p&gt; 
&lt;p&gt;LanguageTool is great, if you have gigabytes of RAM to spare and are willing to download the ~16GB n-gram dataset. Besides the memory requirements, I found LanguageTool too slow: it would take several seconds to lint even a moderate-size document.&lt;/p&gt; 
&lt;p&gt;That's why I created Harper: it is the grammar checker that fits my needs. Not only does it take milliseconds to lint a document, take less than 1/50th of LanguageTool's memory footprint, but it is also completely private.&lt;/p&gt; 
&lt;p&gt;Harper is even small enough to load via &lt;a href="https://writewithharper.com"&gt;WebAssembly.&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Language Support&lt;/h2&gt; 
&lt;p&gt;Harper currently only supports English, but the core is extensible to support other languages, so we welcome contributions that allow for other language support.&lt;/p&gt; 
&lt;h2&gt;Performance Issues&lt;/h2&gt; 
&lt;p&gt;We consider long lint times bugs. If you encounter any significant performance issues, please create an issue on the topic.&lt;/p&gt; 
&lt;p&gt;If you find a fix to any performance issue, we would appreciate the contribution. Just please make sure to read &lt;a href="https://github.com/automattic/harper/raw/master/CONTRIBUTING.md"&gt;our contribution guidelines first.&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Links&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://writewithharper.com/docs/faq"&gt;Frequently Asked Questions&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://writewithharper.com/docs/integrations/obsidian"&gt;Obsidian Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://writewithharper.com/docs/integrations/language-server"&gt;&lt;code&gt;harper-ls&lt;/code&gt; Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Supported Editors' Documentation 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://writewithharper.com/docs/integrations/visual-studio-code"&gt;Visual Studio Code&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://writewithharper.com/docs/integrations/neovim"&gt;Neovim&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://writewithharper.com/docs/integrations/helix"&gt;Helix&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://writewithharper.com/docs/integrations/emacs"&gt;Emacs&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://writewithharper.com/docs/integrations/zed"&gt;Zed&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://writewithharper.com/docs/harperjs/introduction"&gt;&lt;code&gt;harper.js&lt;/code&gt; Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://discord.com/invite/JBqcAaKrzQ"&gt;Official Discord Server&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Huge Thanks&lt;/h2&gt; 
&lt;p&gt;This project would not be possible without the hard work from those who &lt;a href="https://writewithharper.com/docs/contributors/introduction"&gt;contribute&lt;/a&gt;.&lt;/p&gt; 
&lt;a href="https://github.com/automattic/harper/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=automattic/harper" /&gt; &lt;/a&gt; 
&lt;p&gt;Harper's logo was designed by &lt;a href="https://lukaswerner.com/"&gt;Lukas Werner&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>astral-sh/uv</title>
      <link>https://github.com/astral-sh/uv</link>
      <description>&lt;p&gt;An extremely fast Python package and project manager, written in Rust.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;uv&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/astral-sh/uv"&gt;&lt;img src="https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/uv/main/assets/badge/v0.json" alt="uv" /&gt;&lt;/a&gt; &lt;a href="https://pypi.python.org/pypi/uv"&gt;&lt;img src="https://img.shields.io/pypi/v/uv.svg?sanitize=true" alt="image" /&gt;&lt;/a&gt; &lt;a href="https://pypi.python.org/pypi/uv"&gt;&lt;img src="https://img.shields.io/pypi/l/uv.svg?sanitize=true" alt="image" /&gt;&lt;/a&gt; &lt;a href="https://pypi.python.org/pypi/uv"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/uv.svg?sanitize=true" alt="image" /&gt;&lt;/a&gt; &lt;a href="https://github.com/astral-sh/uv/actions"&gt;&lt;img src="https://github.com/astral-sh/uv/actions/workflows/ci.yml/badge.svg?sanitize=true" alt="Actions status" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/astral-sh"&gt;&lt;img src="https://img.shields.io/badge/Discord-%235865F2.svg?logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;An extremely fast Python package and project manager, written in Rust.&lt;/p&gt; 
&lt;p align="center"&gt; 
 &lt;picture align="center"&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://github.com/astral-sh/uv/assets/1309177/03aa9163-1c79-4a87-a31d-7a9311ed9310" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://github.com/astral-sh/uv/assets/1309177/629e59c0-9c6e-4013-9ad4-adb2bcf5080d" /&gt; 
  &lt;img alt="Shows a bar chart with benchmark results." src="https://github.com/astral-sh/uv/assets/1309177/629e59c0-9c6e-4013-9ad4-adb2bcf5080d" /&gt; 
 &lt;/picture&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;i&gt;Installing &lt;a href="https://trio.readthedocs.io/"&gt;Trio&lt;/a&gt;'s dependencies with a warm cache.&lt;/i&gt; &lt;/p&gt; 
&lt;h2&gt;Highlights&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;🚀 A single tool to replace &lt;code&gt;pip&lt;/code&gt;, &lt;code&gt;pip-tools&lt;/code&gt;, &lt;code&gt;pipx&lt;/code&gt;, &lt;code&gt;poetry&lt;/code&gt;, &lt;code&gt;pyenv&lt;/code&gt;, &lt;code&gt;twine&lt;/code&gt;, &lt;code&gt;virtualenv&lt;/code&gt;, and more.&lt;/li&gt; 
 &lt;li&gt;⚡️ &lt;a href="https://github.com/astral-sh/uv/raw/main/BENCHMARKS.md"&gt;10-100x faster&lt;/a&gt; than &lt;code&gt;pip&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;🗂️ Provides &lt;a href="https://raw.githubusercontent.com/astral-sh/uv/main/#projects"&gt;comprehensive project management&lt;/a&gt;, with a &lt;a href="https://docs.astral.sh/uv/concepts/projects/layout#the-lockfile"&gt;universal lockfile&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;❇️ &lt;a href="https://raw.githubusercontent.com/astral-sh/uv/main/#scripts"&gt;Runs scripts&lt;/a&gt;, with support for &lt;a href="https://docs.astral.sh/uv/guides/scripts#declaring-script-dependencies"&gt;inline dependency metadata&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;🐍 &lt;a href="https://raw.githubusercontent.com/astral-sh/uv/main/#python-versions"&gt;Installs and manages&lt;/a&gt; Python versions.&lt;/li&gt; 
 &lt;li&gt;🛠️ &lt;a href="https://raw.githubusercontent.com/astral-sh/uv/main/#tools"&gt;Runs and installs&lt;/a&gt; tools published as Python packages.&lt;/li&gt; 
 &lt;li&gt;🔩 Includes a &lt;a href="https://raw.githubusercontent.com/astral-sh/uv/main/#the-pip-interface"&gt;pip-compatible interface&lt;/a&gt; for a performance boost with a familiar CLI.&lt;/li&gt; 
 &lt;li&gt;🏢 Supports Cargo-style &lt;a href="https://docs.astral.sh/uv/concepts/projects/workspaces"&gt;workspaces&lt;/a&gt; for scalable projects.&lt;/li&gt; 
 &lt;li&gt;💾 Disk-space efficient, with a &lt;a href="https://docs.astral.sh/uv/concepts/cache"&gt;global cache&lt;/a&gt; for dependency deduplication.&lt;/li&gt; 
 &lt;li&gt;⏬ Installable without Rust or Python via &lt;code&gt;curl&lt;/code&gt; or &lt;code&gt;pip&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;🖥️ Supports macOS, Linux, and Windows.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;uv is backed by &lt;a href="https://astral.sh"&gt;Astral&lt;/a&gt;, the creators of &lt;a href="https://github.com/astral-sh/ruff"&gt;Ruff&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;Install uv with our standalone installers:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# On macOS and Linux.
curl -LsSf https://astral.sh/uv/install.sh | sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# On Windows.
powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or, from &lt;a href="https://pypi.org/project/uv/"&gt;PyPI&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# With pip.
pip install uv
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Or pipx.
pipx install uv
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If installed via the standalone installer, uv can update itself to the latest version:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv self update
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See the &lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;installation documentation&lt;/a&gt; for details and alternative installation methods.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;uv's documentation is available at &lt;a href="https://docs.astral.sh/uv"&gt;docs.astral.sh/uv&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Additionally, the command line reference documentation can be viewed with &lt;code&gt;uv help&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;h3&gt;Projects&lt;/h3&gt; 
&lt;p&gt;uv manages project dependencies and environments, with support for lockfiles, workspaces, and more, similar to &lt;code&gt;rye&lt;/code&gt; or &lt;code&gt;poetry&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;$ uv init example
Initialized project `example` at `/home/user/example`

$ cd example

$ uv add ruff
Creating virtual environment at: .venv
Resolved 2 packages in 170ms
   Built example @ file:///home/user/example
Prepared 2 packages in 627ms
Installed 2 packages in 1ms
 + example==0.1.0 (from file:///home/user/example)
 + ruff==0.5.0

$ uv run ruff check
All checks passed!

$ uv lock
Resolved 2 packages in 0.33ms

$ uv sync
Resolved 2 packages in 0.70ms
Audited 1 package in 0.02ms
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See the &lt;a href="https://docs.astral.sh/uv/guides/projects/"&gt;project documentation&lt;/a&gt; to get started.&lt;/p&gt; 
&lt;p&gt;uv also supports building and publishing projects, even if they're not managed with uv. See the &lt;a href="https://docs.astral.sh/uv/guides/publish/"&gt;publish guide&lt;/a&gt; to learn more.&lt;/p&gt; 
&lt;h3&gt;Scripts&lt;/h3&gt; 
&lt;p&gt;uv manages dependencies and environments for single-file scripts.&lt;/p&gt; 
&lt;p&gt;Create a new script and add inline metadata declaring its dependencies:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;$ echo 'import requests; print(requests.get("https://astral.sh"))' &amp;gt; example.py

$ uv add --script example.py requests
Updated `example.py`
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then, run the script in an isolated virtual environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;$ uv run example.py
Reading inline script metadata from: example.py
Installed 5 packages in 12ms
&amp;lt;Response [200]&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See the &lt;a href="https://docs.astral.sh/uv/guides/scripts/"&gt;scripts documentation&lt;/a&gt; to get started.&lt;/p&gt; 
&lt;h3&gt;Tools&lt;/h3&gt; 
&lt;p&gt;uv executes and installs command-line tools provided by Python packages, similar to &lt;code&gt;pipx&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Run a tool in an ephemeral environment using &lt;code&gt;uvx&lt;/code&gt; (an alias for &lt;code&gt;uv tool run&lt;/code&gt;):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;$ uvx pycowsay 'hello world!'
Resolved 1 package in 167ms
Installed 1 package in 9ms
 + pycowsay==0.0.0.2
  """

  ------------
&amp;lt; hello world! &amp;gt;
  ------------
   \   ^__^
    \  (oo)\_______
       (__)\       )\/\
           ||----w |
           ||     ||
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Install a tool with &lt;code&gt;uv tool install&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;$ uv tool install ruff
Resolved 1 package in 6ms
Installed 1 package in 2ms
 + ruff==0.5.0
Installed 1 executable: ruff

$ ruff --version
ruff 0.5.0
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See the &lt;a href="https://docs.astral.sh/uv/guides/tools/"&gt;tools documentation&lt;/a&gt; to get started.&lt;/p&gt; 
&lt;h3&gt;Python versions&lt;/h3&gt; 
&lt;p&gt;uv installs Python and allows quickly switching between versions.&lt;/p&gt; 
&lt;p&gt;Install multiple Python versions:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;$ uv python install 3.10 3.11 3.12
Searching for Python versions matching: Python 3.10
Searching for Python versions matching: Python 3.11
Searching for Python versions matching: Python 3.12
Installed 3 versions in 3.42s
 + cpython-3.10.14-macos-aarch64-none
 + cpython-3.11.9-macos-aarch64-none
 + cpython-3.12.4-macos-aarch64-none
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Download Python versions as needed:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;$ uv venv --python 3.12.0
Using Python 3.12.0
Creating virtual environment at: .venv
Activate with: source .venv/bin/activate

$ uv run --python pypy@3.8 -- python --version
Python 3.8.16 (a9dbdca6fc3286b0addd2240f11d97d8e8de187a, Dec 29 2022, 11:45:30)
[PyPy 7.3.11 with GCC Apple LLVM 13.1.6 (clang-1316.0.21.2.5)] on darwin
Type "help", "copyright", "credits" or "license" for more information.
&amp;gt;&amp;gt;&amp;gt;&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Use a specific Python version in the current directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;$ uv python pin 3.11
Pinned `.python-version` to `3.11`
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See the &lt;a href="https://docs.astral.sh/uv/guides/install-python/"&gt;Python installation documentation&lt;/a&gt; to get started.&lt;/p&gt; 
&lt;h3&gt;The pip interface&lt;/h3&gt; 
&lt;p&gt;uv provides a drop-in replacement for common &lt;code&gt;pip&lt;/code&gt;, &lt;code&gt;pip-tools&lt;/code&gt;, and &lt;code&gt;virtualenv&lt;/code&gt; commands.&lt;/p&gt; 
&lt;p&gt;uv extends their interfaces with advanced features, such as dependency version overrides, platform-independent resolutions, reproducible resolutions, alternative resolution strategies, and more.&lt;/p&gt; 
&lt;p&gt;Migrate to uv without changing your existing workflows — and experience a 10-100x speedup — with the &lt;code&gt;uv pip&lt;/code&gt; interface.&lt;/p&gt; 
&lt;p&gt;Compile requirements into a platform-independent requirements file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;$ uv pip compile docs/requirements.in \
   --universal \
   --output-file docs/requirements.txt
Resolved 43 packages in 12ms
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Create a virtual environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;$ uv venv
Using Python 3.12.3
Creating virtual environment at: .venv
Activate with: source .venv/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Install the locked requirements:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;$ uv pip sync docs/requirements.txt
Resolved 43 packages in 11ms
Installed 43 packages in 208ms
 + babel==2.15.0
 + black==24.4.2
 + certifi==2024.7.4
 ...
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See the &lt;a href="https://docs.astral.sh/uv/pip/index/"&gt;pip interface documentation&lt;/a&gt; to get started.&lt;/p&gt; 
&lt;h2&gt;Platform support&lt;/h2&gt; 
&lt;p&gt;See uv's &lt;a href="https://docs.astral.sh/uv/reference/platforms/"&gt;platform support&lt;/a&gt; document.&lt;/p&gt; 
&lt;h2&gt;Versioning policy&lt;/h2&gt; 
&lt;p&gt;See uv's &lt;a href="https://docs.astral.sh/uv/reference/versioning/"&gt;versioning policy&lt;/a&gt; document.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We are passionate about supporting contributors of all levels of experience and would love to see you get involved in the project. See the &lt;a href="https://github.com/astral-sh/uv/raw/main/CONTRIBUTING.md"&gt;contributing guide&lt;/a&gt; to get started.&lt;/p&gt; 
&lt;h2&gt;FAQ&lt;/h2&gt; 
&lt;h4&gt;How do you pronounce uv?&lt;/h4&gt; 
&lt;p&gt;It's pronounced as "you - vee" (&lt;a href="https://en.wikipedia.org/wiki/Help:IPA/English#Key"&gt;&lt;code&gt;/juː viː/&lt;/code&gt;&lt;/a&gt;)&lt;/p&gt; 
&lt;h4&gt;How should I stylize uv?&lt;/h4&gt; 
&lt;p&gt;Just "uv", please. See the &lt;a href="https://raw.githubusercontent.com/astral-sh/uv/main/STYLE.md#styling-uv"&gt;style guide&lt;/a&gt; for details.&lt;/p&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;p&gt;uv's dependency resolver uses &lt;a href="https://github.com/pubgrub-rs/pubgrub"&gt;PubGrub&lt;/a&gt; under the hood. We're grateful to the PubGrub maintainers, especially &lt;a href="https://github.com/Eh2406"&gt;Jacob Finkelman&lt;/a&gt;, for their support.&lt;/p&gt; 
&lt;p&gt;uv's Git implementation is based on &lt;a href="https://github.com/rust-lang/cargo"&gt;Cargo&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Some of uv's optimizations are inspired by the great work we've seen in &lt;a href="https://pnpm.io/"&gt;pnpm&lt;/a&gt;, &lt;a href="https://github.com/orogene/orogene"&gt;Orogene&lt;/a&gt;, and &lt;a href="https://github.com/oven-sh/bun"&gt;Bun&lt;/a&gt;. We've also learned a lot from Nathaniel J. Smith's &lt;a href="https://github.com/njsmith/posy"&gt;Posy&lt;/a&gt; and adapted its &lt;a href="https://github.com/njsmith/posy/tree/main/src/trampolines/windows-trampolines/posy-trampoline"&gt;trampoline&lt;/a&gt; for Windows support.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;uv is licensed under either of&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Apache License, Version 2.0, (&lt;a href="https://raw.githubusercontent.com/astral-sh/uv/main/LICENSE-APACHE"&gt;LICENSE-APACHE&lt;/a&gt; or &lt;a href="https://www.apache.org/licenses/LICENSE-2.0"&gt;https://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;MIT license (&lt;a href="https://raw.githubusercontent.com/astral-sh/uv/main/LICENSE-MIT"&gt;LICENSE-MIT&lt;/a&gt; or &lt;a href="https://opensource.org/licenses/MIT"&gt;https://opensource.org/licenses/MIT&lt;/a&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;at your option.&lt;/p&gt; 
&lt;p&gt;Unless you explicitly state otherwise, any contribution intentionally submitted for inclusion in uv by you, as defined in the Apache-2.0 license, shall be dually licensed as above, without any additional terms or conditions.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a target="_blank" href="https://astral.sh" style="background:none"&gt; &lt;img src="https://raw.githubusercontent.com/astral-sh/uv/main/assets/svg/Astral.svg?sanitize=true" alt="Made by Astral" /&gt; &lt;/a&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>vectordotdev/vector</title>
      <link>https://github.com/vectordotdev/vector</link>
      <description>&lt;p&gt;A high-performance observability data pipeline.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="https://github.com/vectordotdev/vector/actions/workflows/nightly.yml"&gt;&lt;img src="https://github.com/vectordotdev/vector/actions/workflows/nightly.yml/badge.svg?sanitize=true" alt="Nightly" /&gt;&lt;/a&gt; &lt;a href="https://github.com/vectordotdev/vector/actions/workflows/e2e.yml"&gt;&lt;img src="https://github.com/vectordotdev/vector/actions/workflows/e2e.yml/badge.svg?sanitize=true" alt="E2E Test Suite" /&gt;&lt;/a&gt; &lt;a href="https://github.com/vectordotdev/vector/actions/workflows/component_features.yml"&gt;&lt;img src="https://github.com/vectordotdev/vector/actions/workflows/component_features.yml/badge.svg?sanitize=true" alt="Component Features" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/vectordotdev/vector/master/website/static/img/diagram.svg?sanitize=true" alt="Vector" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;strong&gt; &lt;a href="https://vector.dev/docs/setup/quickstart/"&gt;Quickstart&lt;/a&gt;&amp;nbsp;&amp;nbsp;•&amp;nbsp;&amp;nbsp; &lt;a href="https://vector.dev/docs/"&gt;Docs&lt;/a&gt;&amp;nbsp;&amp;nbsp;•&amp;nbsp;&amp;nbsp; &lt;a href="https://vector.dev/guides/"&gt;Guides&lt;/a&gt;&amp;nbsp;&amp;nbsp;•&amp;nbsp;&amp;nbsp; &lt;a href="https://vector.dev/components/"&gt;Integrations&lt;/a&gt;&amp;nbsp;&amp;nbsp;•&amp;nbsp;&amp;nbsp; &lt;a href="https://chat.vector.dev"&gt;Chat&lt;/a&gt;&amp;nbsp;&amp;nbsp;•&amp;nbsp;&amp;nbsp; &lt;a href="https://vector.dev/releases/latest/download/"&gt;Download&lt;/a&gt;&amp;nbsp;&amp;nbsp;•&amp;nbsp;&amp;nbsp; &lt;a href="https://rust-doc.vector.dev/"&gt;Rust Crate Docs&lt;/a&gt; &lt;/strong&gt; &lt;/p&gt; 
&lt;h2&gt;What is Vector?&lt;/h2&gt; 
&lt;p&gt;Vector is a high-performance, end-to-end (agent &amp;amp; aggregator) observability data pipeline that puts you in control of your observability data. &lt;a href="https://vector.dev/docs/reference/configuration/sources/"&gt;Collect&lt;/a&gt;, &lt;a href="https://vector.dev/docs/reference/configuration/transforms/"&gt;transform&lt;/a&gt;, and &lt;a href="https://vector.dev/docs/reference/configuration/sinks/"&gt;route&lt;/a&gt; all your logs and metrics to any vendors you want today and any other vendors you may want tomorrow. Vector enables dramatic cost reduction, novel data enrichment, and data security where you need it, not where it is most convenient for your vendors. Additionally, it is open source and up to 10x faster than every alternative in the space.&lt;/p&gt; 
&lt;p&gt;To get started, follow our &lt;a href="https://vector.dev/docs/setup/quickstart/"&gt;&lt;strong&gt;quickstart guide&lt;/strong&gt;&lt;/a&gt; or &lt;a href="https://vector.dev/docs/setup/installation/"&gt;&lt;strong&gt;install Vector&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Vector is maintained by Datadog's &lt;a href="https://opensource.datadoghq.com/about/#the-community-open-source-engineering-team"&gt;Community Open Source Engineering team&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Principles&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Reliable&lt;/strong&gt; - Built in &lt;a href="https://www.rust-lang.org/"&gt;Rust&lt;/a&gt;, Vector's primary design goal is reliability.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;End-to-end&lt;/strong&gt; - Deploys as an &lt;a href="https://vector.dev/docs/setup/deployment/roles/#agent"&gt;agent&lt;/a&gt; or &lt;a href="https://vector.dev/docs/setup/deployment/roles/#aggregator"&gt;aggregator&lt;/a&gt;. Vector is a complete platform.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Unified&lt;/strong&gt; - &lt;a href="https://vector.dev/docs/architecture/data-model/log/"&gt;Logs&lt;/a&gt;, &lt;a href="https://vector.dev/docs/architecture/data-model/metric/"&gt;metrics&lt;/a&gt; (beta), and traces (coming soon). One tool for all of your data.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Use cases&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Reduce total observability costs.&lt;/li&gt; 
 &lt;li&gt;Transition vendors without disrupting workflows.&lt;/li&gt; 
 &lt;li&gt;Enhance data quality and improve insights.&lt;/li&gt; 
 &lt;li&gt;Consolidate agents and eliminate agent fatigue.&lt;/li&gt; 
 &lt;li&gt;Improve overall observability performance and reliability.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Community&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Vector is relied on by startups and enterprises like &lt;strong&gt;Atlassian&lt;/strong&gt;, &lt;strong&gt;T-Mobile&lt;/strong&gt;, &lt;strong&gt;Comcast&lt;/strong&gt;, &lt;strong&gt;Zendesk&lt;/strong&gt;, &lt;strong&gt;Discord&lt;/strong&gt;, &lt;strong&gt;Fastly&lt;/strong&gt;, &lt;strong&gt;CVS&lt;/strong&gt;, &lt;strong&gt;Trivago&lt;/strong&gt;, &lt;strong&gt;Tuple&lt;/strong&gt;, &lt;strong&gt;Douban&lt;/strong&gt;, &lt;strong&gt;Visa&lt;/strong&gt;, &lt;strong&gt;Mambu&lt;/strong&gt;, &lt;strong&gt;Blockfi&lt;/strong&gt;, &lt;strong&gt;Claranet&lt;/strong&gt;, &lt;strong&gt;Instacart&lt;/strong&gt;, &lt;strong&gt;Forcepoint&lt;/strong&gt;, and &lt;a href="https://github.com/vectordotdev/vector/issues/790"&gt;many more&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Vector is &lt;strong&gt;downloaded over 100,000 times per day&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;Vector's largest user &lt;strong&gt;processes over 500TB daily&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;Vector has &lt;strong&gt;over 500 contributors&lt;/strong&gt; and growing.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;All user documentation is available at &lt;strong&gt;&lt;a href="https://vector.dev/docs"&gt;vector.dev/docs&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;Other Resources:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://calendar.vector.dev"&gt;&lt;strong&gt;Vector Calendar&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Policies&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://github.com/vectordotdev/vector/raw/master/CODE_OF_CONDUCT.md"&gt;&lt;strong&gt;Code of Conduct&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/vectordotdev/vector/raw/master/CONTRIBUTING.md"&gt;&lt;strong&gt;Contributing&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/vectordotdev/vector/raw/master/PRIVACY.md"&gt;&lt;strong&gt;Privacy&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/vectordotdev/vector/raw/master/RELEASES.md"&gt;&lt;strong&gt;Releases&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/vectordotdev/vector/raw/master/VERSIONING.md"&gt;&lt;strong&gt;Versioning&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/vectordotdev/vector/security/policy"&gt;&lt;strong&gt;Security&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Comparisons&lt;/h2&gt; 
&lt;h3&gt;Performance&lt;/h3&gt; 
&lt;p&gt;The following performance tests demonstrate baseline performance between common protocols with the exception of the Regex Parsing test.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Test&lt;/th&gt; 
   &lt;th&gt;Vector&lt;/th&gt; 
   &lt;th&gt;Filebeat&lt;/th&gt; 
   &lt;th&gt;FluentBit&lt;/th&gt; 
   &lt;th&gt;FluentD&lt;/th&gt; 
   &lt;th&gt;Logstash&lt;/th&gt; 
   &lt;th&gt;SplunkUF&lt;/th&gt; 
   &lt;th&gt;SplunkHF&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/vectordotdev/vector-test-harness/tree/master/cases/tcp_to_blackhole_performance"&gt;TCP to Blackhole&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;em&gt;&lt;strong&gt;86mib/s&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt; 
   &lt;td&gt;n/a&lt;/td&gt; 
   &lt;td&gt;64.4mib/s&lt;/td&gt; 
   &lt;td&gt;27.7mib/s&lt;/td&gt; 
   &lt;td&gt;40.6mib/s&lt;/td&gt; 
   &lt;td&gt;n/a&lt;/td&gt; 
   &lt;td&gt;n/a&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/vectordotdev/vector-test-harness/tree/master/cases/file_to_tcp_performance"&gt;File to TCP&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;em&gt;&lt;strong&gt;76.7mib/s&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt; 
   &lt;td&gt;7.8mib/s&lt;/td&gt; 
   &lt;td&gt;35mib/s&lt;/td&gt; 
   &lt;td&gt;26.1mib/s&lt;/td&gt; 
   &lt;td&gt;3.1mib/s&lt;/td&gt; 
   &lt;td&gt;40.1mib/s&lt;/td&gt; 
   &lt;td&gt;39mib/s&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/vectordotdev/vector-test-harness/tree/master/cases/regex_parsing_performance"&gt;Regex Parsing&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;13.2mib/s&lt;/td&gt; 
   &lt;td&gt;n/a&lt;/td&gt; 
   &lt;td&gt;&lt;em&gt;&lt;strong&gt;20.5mib/s&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt; 
   &lt;td&gt;2.6mib/s&lt;/td&gt; 
   &lt;td&gt;4.6mib/s&lt;/td&gt; 
   &lt;td&gt;n/a&lt;/td&gt; 
   &lt;td&gt;7.8mib/s&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/vectordotdev/vector-test-harness/tree/master/cases/tcp_to_http_performance"&gt;TCP to HTTP&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;em&gt;&lt;strong&gt;26.7mib/s&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt; 
   &lt;td&gt;n/a&lt;/td&gt; 
   &lt;td&gt;19.6mib/s&lt;/td&gt; 
   &lt;td&gt;&amp;lt;1mib/s&lt;/td&gt; 
   &lt;td&gt;2.7mib/s&lt;/td&gt; 
   &lt;td&gt;n/a&lt;/td&gt; 
   &lt;td&gt;n/a&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/vectordotdev/vector-test-harness/tree/master/cases/tcp_to_tcp_performance"&gt;TCP to TCP&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;69.9mib/s&lt;/td&gt; 
   &lt;td&gt;5mib/s&lt;/td&gt; 
   &lt;td&gt;67.1mib/s&lt;/td&gt; 
   &lt;td&gt;3.9mib/s&lt;/td&gt; 
   &lt;td&gt;10mib/s&lt;/td&gt; 
   &lt;td&gt;&lt;em&gt;&lt;strong&gt;70.4mib/s&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt; 
   &lt;td&gt;7.6mib/s&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;To learn more about our performance tests, please see the &lt;a href="https://github.com/vectordotdev/vector-test-harness/"&gt;Vector test harness&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Correctness&lt;/h3&gt; 
&lt;p&gt;The following correctness tests are not exhaustive, but they demonstrate fundamental differences in quality and attention to detail:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Test&lt;/th&gt; 
   &lt;th&gt;Vector&lt;/th&gt; 
   &lt;th&gt;Filebeat&lt;/th&gt; 
   &lt;th&gt;FluentBit&lt;/th&gt; 
   &lt;th&gt;FluentD&lt;/th&gt; 
   &lt;th&gt;Logstash&lt;/th&gt; 
   &lt;th&gt;Splunk UF&lt;/th&gt; 
   &lt;th&gt;Splunk HF&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/vectordotdev/vector-test-harness/tree/master/cases/disk_buffer_persistence_correctness"&gt;Disk Buffer Persistence&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;✓&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;⚠&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/vectordotdev/vector-test-harness/tree/master/cases/file_rotate_create_correctness"&gt;File Rotate (create)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;✓&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/vectordotdev/vector-test-harness/tree/master/cases/file_rotate_truncate_correctness"&gt;File Rotate (copytruncate)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;✓&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/vectordotdev/vector-test-harness/tree/master/cases/file_truncate_correctness"&gt;File Truncation&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;✓&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/vectordotdev/vector-test-harness/tree/master/cases/sighup_correctness"&gt;Process (SIGHUP)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;✓&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;⚠&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/vectordotdev/vector-test-harness/tree/master/cases/wrapped_json_correctness"&gt;JSON (wrapped)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;✓&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;To learn more about our correctness tests, please see the &lt;a href="https://github.com/vectordotdev/vector-test-harness/"&gt;Vector test harness&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Features&lt;/h3&gt; 
&lt;p&gt;Vector is an end-to-end, unified, open data platform.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Vector&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;Beats&lt;/th&gt; 
   &lt;th&gt;Fluentbit&lt;/th&gt; 
   &lt;th&gt;Fluentd&lt;/th&gt; 
   &lt;th&gt;Logstash&lt;/th&gt; 
   &lt;th&gt;Splunk UF&lt;/th&gt; 
   &lt;th&gt;Splunk HF&lt;/th&gt; 
   &lt;th&gt;Telegraf&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;End-to-end&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;✓&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Agent&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;✓&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Aggregator&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;✓&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Unified&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;✓&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Logs&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;✓&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Metrics&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;✓&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;⚠&lt;/td&gt; 
   &lt;td&gt;⚠&lt;/td&gt; 
   &lt;td&gt;⚠&lt;/td&gt; 
   &lt;td&gt;⚠&lt;/td&gt; 
   &lt;td&gt;⚠&lt;/td&gt; 
   &lt;td&gt;⚠&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Traces&lt;/td&gt; 
   &lt;td&gt;🚧&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Open&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;✓&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Open-source&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;✓&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Vendor-neutral&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;✓&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Reliability&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;✓&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Memory-safe&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;✓&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Delivery guarantees&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;✓&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Multi-core&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;✓&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;⚠ = Not interoperable, metrics are represented as structured logs&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p align="center"&gt; Developed with ❤️ by &lt;strong&gt;&lt;a href="https://datadoghq.com"&gt;Datadog&lt;/a&gt;&lt;/strong&gt; - &lt;a href="https://github.com/vectordotdev/vector/security/policy"&gt;Security Policy&lt;/a&gt; - &lt;a href="https://github.com/vectordotdev/vector/raw/master/PRIVACY.md"&gt;Privacy Policy&lt;/a&gt; &lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>